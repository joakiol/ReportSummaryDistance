Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 49?58,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsSentiment Analysis of Political Tweets: Towards an Accurate ClassifierAkshat Bakliwal1, Jennifer Foster2, Jennifer van der Puil3?,Ron O?Brien4, Lamia Tounsi2 and Mark Hughes51Search and Information Extraction Lab, IIIT-Hyderabad, India2NCLT/CNGL, School of Computing, Dublin City University, Ireland3Department of Computer Science and Statistics, Trinity College, Ireland4Quiddity, Dublin, Ireland5CLARITY, School of Computing, Dublin City University, Ireland1akshat.bakliwal@research.iiit.ac.in2,5{jfoster,ltounsi,mhughes}@computing.dcu.ie3jvanderp@tcd.ie4ron@quiddity.ieAbstractWe perform a series of 3-class sentiment clas-sification experiments on a set of 2,624 tweetsproduced during the run-up to the Irish Gen-eral Elections in February 2011.
Even thoughtweets that have been labelled as sarcastichave been omitted from this set, it still rep-resents a difficult test set and the highestaccuracy we achieve is 61.6% using super-vised learning and a feature set consistingof subjectivity-lexicon-based scores, Twitter-specific features and the top 1,000 most dis-criminative words.
This is superior to variousnaive unsupervised approaches which use sub-jectivity lexicons to compute an overall senti-ment score for a <tweet,political party> pair.1 IntroductionSupervised machine learning using minimal featureengineering has been shown to work well in binarypositive/negative sentiment classification tasks onwell-behaved datasets such as movie reviews (Panget al 2002).
In this paper we describe sentimentanalysis experiments in a more complicated setup:the task is three-class positive/negative/neutral clas-sification, the sentiment being classified is not at thegeneral document level but rather directed towards atopic, the documents are tweets, and the topic is poli-tics, specifically the Irish General Election of Febru-ary 2011.?Akshat Bakliwal and Jennifer van der Puil carried out theirpart of this work while employed as summer interns at the Cen-tre for Next Generation Localisation(CNGL) in the School ofComputing, DCU.The dataset used in the experiments containstweets which were collected in the run up to the elec-tion and which were subsequently doubly annotatedas positive, negative or neutral towards a particularpolitical party or party leader.
The annotators alsomarked a tweet as sarcastic if its literal sentimentwas different to its actual sentiment.
Before explor-ing the thorny issue of sentiment classification in theface of sarcasm, we simplify the problem by first try-ing to establish some sentiment analysis baselinesfor those tweets which were not deemed to be sar-castic.We first explore a naive approach in which a sub-jectivity lexicon is used as the primary source of in-formation in determining whether sentiment towardsa political party or party leader is positive, negativeor neutral.
The best version of this method achievesan accuracy of 58.9, an absolute improvement of 4.9points over the majority baseline (54%) in which alltweets are classified as neutral.
When these lexi-con scores are combined with bag-of-word featuresand some Twitter-specific features in a supervisedmachine learning setup, this accuracy increases to61.6%.The paper is organised as follows: related workis described in Section 2, followed by a brief dis-cussion of the 2011 Irish General Election in Sec-tion 3, a description of the dataset in Section 4and a description of the natural language processingtools and resources employed in Section 5.
In Sec-tion 6, the unsupervised lexicon-based approach ispresented and its limitations discussed.
Section 7 de-scribes the machine-learning-based experiments andSection 8 concludes and provides hints towards fu-49ture work with this new dataset.2 Previous WorkThe related work can be divided into two groups,general sentiment analysis research and researchwhich is devoted specifically to the political domain.2.1 General Sentiment AnalysisResearch in the area of sentiment mining startedwith product (Turney, 2002) and movie (Pang et al2002) reviews.
Turney (2002) used Pointwise Mu-tual Information (PMI) to estimate the sentiment ori-entation of phrases.
Pang et al(2002) employedsupervised learning with various set of n-gram fea-tures, achieving an accuracy of almost 83% with un-igram presence features on the task of document-level binary sentiment classification.
Research onother domains and genres including blogs (Chesley,2006) and news (Godbole et al 2007) followed.Early sentiment analysis research focused onlonger documents such as movie reviews and blogs.Microtext on the other hand restricts the writer to amore concise expression of opinion.
Smeaton andBermingham (2010) tested the hypothesis that it iseasier to classify sentiment in microtext as comparedto longer documents.
They experimented with mi-crotext from Twitter, microreviews from blippr, blogposts and movie reviews and concluded that it is eas-ier to identify sentiment from microtext.
However,as they move from contextually sparse unigrams tohigher n-grams, it becomes more difficult to improvethe performance of microtext sentiment classifica-tion, whereas higher-order information makes it eas-ier to perform classification of longer documents.There has been some research on the use of pos-itive and negative emoticons and hashtags in tweetsas a proxy for sentiment labels (Go et al 2009; Pakand Paroubek, 2010; Davidov et al 2010; Bora,2012).
Bakliwal et al(2012) emphasized the im-portance of preprocessing and proposed a set offeatures to extract maximum sentiment informationfrom tweets.
They used unigram and bigram fea-tures along with features which are more associatedwith tweets such as emoticons, hashtags, URLs, etc.and showed that combining linguistic and Twitter-specific features can boost the classification accu-racy.2.2 Political Sentiment AnalysisIn recent years, there has been growing interest inmining online political sentiment in order to pre-dict the outcome of elections.
One of the most in-fluential papers is that of Tumasjan et al(2010)who focused on the 2009 German federal electionand investigated whether Twitter can be used to pre-dict election outcomes.
Over one hundred thousandtweets dating from August 13 to September 19, 2009containing the names of the six parties representedin the German parliament were collected.
LIWC2007 (Pennebaker et al 2007) was then used to ex-tract sentiment from the tweets.
LIWC is a text anal-ysis software developed to assess emotional, cog-nitive and structural components of text samplesusing a psychometrically validated internal dictio-nary.
Tumasjan et alconcluded that the number oftweets/mentions of a party is directly proportional tothe probability of winning the elections.O?Connor et al(2010) investigated the extent towhich public opinion polls were correlated with po-litical sentiment expressed in tweets.
Using the Sub-jectivity Lexicon (Wilson et al 2005), they estimatethe daily sentiment scores for each entity.
A tweet isdefined as positive if it contains a positive word andvice versa.
A sentiment score for that day is calcu-lated as the ratio of the positive count over the neg-ative count.
They find that their sentiment scoreswere correlated with opinion polls on presidentialjob approval but less strongly with polls on electoraloutcome.Choy et al(2011) discuss the application of on-line sentiment detection to predict the vote percent-age for each of the candidates in the Singapore pres-idential election of 2011.
They devise a formula tocalculate the percentage vote each candidate will re-ceive using census information on variables such asage group, sex, location, etc.
They combine thiswith a sentiment-lexicon-based sentiment analysisengine which calculates the sentiment in each tweetand aggregates the positive and negative sentimentfor each candidate.
Their model was able to predictthe narrow margin between the top two candidatesbut failed to predict the correct winner.Wang et al(2012) proposed a real-time sentimentanalysis system for political tweets which was basedon the U.S. presidential election of 2012.
They col-50lected over 36 million tweets and collected the sen-timent annotations using Amazon Mechanical Turk.Using a Naive Bayes model with unigram features,their system achieved 59% accuracy on the four-category classification.Bermingham and Smeaton (2011) are also con-cerned with predicting electoral outcome, in partic-ular, the outcome of the Irish General Election of2011 (the same election that we focused on).
Theyanalyse political sentiment in tweets by means of su-pervised classification with unigram features and anannotated dataset different to and larger than the onewe present, achieving 65% accuracy on the task ofpositive/negative/neutral classification.
They con-clude that volume is a stronger indicator of electionoutcome than sentiment, but that sentiment still hasa role to play.Gayo-Avello (2012) calls into question the use ofTwitter for election outcome prediction.
Previousworks which report positive results on this task usingdata from Twitter are surveyed and shortcomings intheir methodology and/or assumptions noted.
In thispaper, our focus is not the (non-) predictive nature ofpolitical tweets but rather the accurate identificationof any sentiment expressed in the tweets.
If the ac-curacy of sentiment analysis of political tweets canbe improved (or its limitations at least better under-stood) then this will likely have a positive effect onits usefulness as an alternative or complement to tra-ditional opinion polling.3 #ge11: The Irish General Election 2011The Irish general elections were held on February25, 2011.
165 representatives were elected across 43constituencies for the Da?il, the main house of parlia-ment.
Eight parties nominated their candidates forelection and a coalition (Fine Gael and Labour) gov-ernment was formed.
The parties in the outgoingcoalition government, Fianna Fa?il and the Greens,suffered disastrous defeats, the worst defeat of a sit-ting government since the foundatation of the Statein 1922.Gallagher and Marsh (2011, chapter 5) discuss theuse of social media by parties, candidates and vot-ers in the 2011 election and conclude that it had amuch more important role to play in this electionthan in the previous one in 2007.
On the role of Twit-ter in particular, they report that ?Twitter was lesswidespread among candidates [than Facebook], butit offered the most diverse source of citizen coverageduring the election, and it has been integrated intoseveral mainstream media?.
They estimated that 7%of the Irish population had a Twitter account at thetime of the election.4 DatasetWe compiled a corpus of tweets using the Twittersearch API between 20th and the 25th of January2011 (one month before the election).
We selectedthe main political entities (the five biggest politi-cal parties ?
Fianna Fa?il, Fine Gael, Labour, SinnFe?in and the Greens ?
and their leaders) and per-form query-based search to collect the tweets relat-ing to these entities.
The resulting dataset contains7,916 tweets of which 4,710 are retweets or dupli-cates, leaving a total of 3,206 tweets.The tweets were annotated by two Irish annota-tors with a knowledge of the Irish political land-scape.
Disagreements between the two annotatorswere studied and resolved by a third annotator.
Theannotators were asked to identify the sentiment as-sociated with the topic (or entity) of the tweet.
An-notation was performed using the following 6 labels:?
pos: Tweets which carry positive sentiment to-wards the topic?
neg: Tweets which carry negative sentiment to-wards the topic?
mix: Tweets which carry both positive and neg-ative sentiment towards the topic?
neu: Tweets which do not carry any sentimenttowards the topic?
nen: Tweets which were written in languagesother than English.?
non: Tweets which do not have any mentionor relation to the topic.
These represent searcherrors.In addition to the above six classes, annotators wereasked to flag whether a tweet was sarcastic.The dataset which we use for the experimentsdescribed in this paper contains only those tweets51Positive Tweets 256 9.75%Negative Tweets 950 36.22%Neutral Tweets 1418 54.03%Total Tweets 2624Table 1: Class Distributionthat have been labelled as either positive, negativeor neutral, i.e.
non-relevant, mixed-sentiment andnon-English tweets are discarded.
We also simplifyour task by omitting those tweets which have beenflagged as sarcastic by one or both of the annotators,leaving a set of 2,624 tweets with a class distributionas shown in Table 1.5 Tools and ResourcesIn the course of our experiments, we use two differ-ent subjectivity lexicons, one part-of-speech taggerand one parser.
For part-of-speech tagging we usea tagger (Gimpel et al 2011) designed specificallyfor tweets.
For parsing, we use the Stanford parser(Klein and Manning, 2003).
To identify the senti-ment polarity of a word we use:1.
Subjectivity Lexicon (SL) (Wilson et al2005): This lexicon contains 8,221 words(6,878 unique forms) of which 3,249 are adjec-tives, 330 are adverbs, 1,325 are verbs, 2,170are nouns and remaining (1,147) words aremarked as anypos.
There are many wordswhich occur with two or more different part-of-speech tags.
We extend SL with 341 domain-specific words to produce an extended SL.2.
SentiWordNet 3.0 (SWN) (Baccianella et al2010): With over 100+ thousand words, SWNis far larger than SL but is likely to be noisiersince it has been built semi-automatically.
Eachword in the lexicon is associated with both apositive and negative score, and an objectivescore given by (1), i.e.
the positive, negativeand objective score sum to 1.ObjScore = 1?PosScore?NegScore (1)6 Naive Lexicon-based ClassificationIn this section we describe a naive approach to sen-timent classification which does not make use of la-belled training data but rather uses the informationin a sentiment lexicon to deduce the sentiment ori-entation towards a political party in a tweet (seeLiu (2010) for an overview of this unsupervisedlexicon-based approach).
In Section 6.1, we presentthe basic method along with some variants whichimprove on the basic method by making use of infor-mation about part-of-speech, negation and distancefrom the topic.
In Section 6.2, we examine someof the cases which remain misclassified by our bestlexicon-based method.
In Section 6.3, we discussbriefly those tweets that have been labelled as sar-castic.6.1 Method and ResultsOur baseline lexicon-based approach is as follows:we look up each word in our sentiment lexicon andsum up the scores to corresponding scalars.
The re-sults are shown in Table 2.
Note that the most likelyestimated class prediction is neutral with a probabil-ity of .5403 (1418/2624).6.1.1 Which Subjectivity Lexicon?The first column shows the results that we obtainwhen the lexicon we use is our extended version ofthe SL lexicon.
The results in the second columnare those that result from using SWN.
In the thirdcolumn, we combine the two lexicons.
We definea combination pattern of Extended-SL and SWN inwhich we prioritize Extended-SL because it is man-ually checked and some domain-specific words areadded.
For the words which were missing fromExtended-SL (SWN), we assign them the polarity ofSWN (Extended-SL).
Table 3 explains exactly howthe scores from the two lexicons are combined.
Al-though SWN slightly outperforms Extended-SL forthe baseline lexicon-based approach (first row of Ta-ble 2), it is outperformed by Extended-SL and thecombinaton of the two lexicons for all the variants.We can conclude from the full set of results in Ta-ble 2 that SWN is less useful than Extended-SL orthe combination of SWN and Extended-SL.6.1.2 Filtering by Part-of-SpeechThe results in the first row of Table 2 representour baseline experiment in which each word in thetweet is looked up in the sentiment lexicon andits sentiment score added to a running total.
Weachieve a classification accuracy of 52.44% with the52Method Extended-SL SWN Combined3-Class Classification (Pos vsNeg vs Neu)Correct Accuracy Correct Accuracy Correct AccuracyBaseline 1376 52.44% 1379 52.55% 1288 49.09%Baseline + Adj 1457 55.53% 1449 55.22% 1445 55.07%Baseline + Adj + S 1480 56.40% 1459 55.60% 1481 56.44%Baseline + Adj + S + Neg 1495 56.97% 1462 55.72% 1496 57.01%Baseline + Adj + S + Neg +Phrases1511 57.58% 1479 56.36% 1509 57.51%Baseline + Adj + S + Neg +Phrases + Than1533 58.42% 1502 57.24% 1533 58.42%Distance Based Scoring:Baseline + Adj + S + Neg +Phrases + Than1545 58.88% 1506 57.39% 1547 58.96%Sarcastic Tweets 87/344 25.29% 81/344 23.55% 87/344 25.29%Table 2: 3-class classification using the naive lexicon-based approach.
The majority baseline is 54.03%.Extended-SLpolaritySWNPolarityCombinationPolarity-1 -1 -2-1 0 -1-1 1 -10 -1 -0.50 0 00 1 0.51 -1 11 0 11 1 2Table 3: Combination Scheme of extended-SL and SWN.Here 0 represents either a neutral word or a word missingfrom the lexicon.Extended-SL lexicon.
We speculate that this lowaccuracy is occurring because too many words thatappear in the sentiment lexicon are included in theoverall sentiment score without actually contribut-ing to the sentiment towards the topic.
To refine ourapproach one step further, we use part-of-speech in-formation and consider only adjectives for the clas-sification of tweets since adjectives are strong in-dicators of sentiment (Hatzivassiloglou and Wiebe,2000).
We achieve an accuracy improvement of ap-proximately three absolute points, and this improve-ment holds true for both sentiment lexicons.
Thissupports our hypothesis that we are using irrelevantinformation for classification in the baseline system.Our next improvement (third row of Table 2)comes from mapping all inflected forms to theirstems (using the Porter stemmer).
Examples of in-flected forms that are reduced to their stems are de-lighted or delightful.
Using stemming with adjec-tives over the baseline, we achieve an accuracy of56.40% with Extended-SL.6.1.3 Negation?Negation is a very common linguistic construc-tion that affects polarity and, therefore, needs tobe taken into consideration in sentiment analysis?
(Councill et al 2010).
We perform negation han-dling in tweets using two different approaches.
Inthe first approach, we first identify negation words53and reverse the polarity of sentiment-bearing wordswithin a window of three words.
In the second ap-proach, we try to resolve the scope of the negationusing syntactic parsing.
The Stanford dependencyscheme (de Marneffe and Manning, 2008) has a spe-cial relation (neg) to indicate negation.
We reversethe sentiment polarity of a word marked via the negrelation as being in the scope of a negation.
Usingthe first approach, we see an improvement of 0.6%in the classification accuracy with the Extended-SLlexicon.
Using the second approach, we see animprovement of 0.5%.
Since there appears to bevery little difference between the two approaches tonegation-handling and in order to reduce the compu-tational burden of running the Stanford parser eachtime to obtain the dependencies, we continue furtherexperiments with the first method only.
Using base-line + stemming + adjectives + neg we achieve anaccuracy of 56.97% with the Extended-SL lexicon.6.1.4 Domain-specific idiomsIn the context of political tweets we see manysentiment-bearing idioms and fixed expressions, e.g.god save us, X for Taoiseach1, wolf in sheep?s cloth-ing, etc.
In our study, we had a total of 89 phrases.When we directly account for these phrases, weachieve an accuracy of 57.58% (an absolute im-provement of 0.6 points over the last step).6.1.5 Comparative ExpressionsAnother form of expressing an opinion towardsan entity is by comparing the entity with some otherentity.
For example consider the tweet:Fast Food sounds like a better vote than Fianna Fail.
(2)In this tweet, an indirect negative sentiment is ex-pressed towards the political party Fianna Fa?il.
Inorder to take into account such constructions, thefollowing procedure is applied: we divide the tweetinto two parts, left and right.
The left part containsthe text which comes before the than and the rightpart contains the text which comes after than, e.g.Tweet: ?X is better than Y?Left: ?X is better?Right: ?Y?.1The term Taoiseach refers to the Irish Prime Minister.We then use the following strategy to calculate thepolarity of the tweet oriented towards the entity:S left = sentiment score of Left.S right = sentiment score of Right.Ent pos left = if entity is left of?than?, then 1, otherwise ?
1.Ent pos right = if entity is right of?than?, then 1, otherwise ?
1.S(tweet) = Ent pos left ?
S left +Ent pos right ?
S right.
(3)So in (2) above the entity, Fianna Fa?il, is to theright of than meaning that its Ent pos right valueis 1 and its Ent pos left value is -1.
This has theeffect of flipping the polarity of the positive wordbetter.
By including the ?than?
comparison, we seean improvement of absolute 0.8% (third last row ofTable 2).6.1.6 Distance ScoringTo emphasize the topic-oriented nature of our sen-timent classification, we also define a distance-basedscoring function where we define the overall scoreof the tweet as given in (4).
Here dis(word) is de-fined as number of words between the topic (i.e.
thepolitical entity) and the sentiment word.S(tweet) =n?i=1S(wordi)/dis(wordi).
(4)The addition of the distance information further en-hanced our system accuracy by 0.45%, taking it to58.88% (second last row of Table 2).
Our highestoverall accuracy (58.96) is achieved in this settingusing the combined lexicon.It should be noted that this lexicon-based ap-proach is overfitting to our dataset since the list ofdomain-specific phrases and the form of the com-parative constructions have been obtained from thedataset itself.
This means that we are making astrong assumption about the representativeness ofthis dataset and accuracy on a held-out test set islikely to be lower.6.2 Error AnalysisIn this section we discuss pitfalls of the naivelexicon-based approach with the help of some exam-ples (see Table 4).
Consider the first example from54the table, @username and u believe people in fiannafail .
What are you a numbskull or a journalist ?In this tweet, we see that negative sentiment is im-parted by the question part of the tweet, but actuallythere are no sentiment adjectives.
The word numb-skull is contributing to the sentiment but is tagged asa noun and not as an adjective.
This tweet is taggedas negative by our annotators and as neutral by ourlexicon-based classifier.Consider the second example from Table 4,@username LOL .
A guy called to our house tonightselling GAA tickets .
His first words were : I?mnot from Fianna Fail .
This is misclassified becausethere are no sentiment bearing words according tothe sentiment lexicon.
The last tweet in the table rep-resents another example of the same problem.
Notehowever that the emoticon :/ in the last tweet and theweb acronym LOL in the second tweet are providinghints which our system is not making use of.In the third example from Table 4, @usernameSuch scary words .?
Sinn Fein could top the poll ?in certain constituencies .
I feel sick at the thoughtof it .
?
In this example, we have three sentimentbearing words: scary, top and sick.
Two of the threewords are negative and one word is positive.
Theword scary is stemmed incorrectly as scari whichmeans that it is out of the scope of our lexicons.If we just count the number of sentiment words re-maining, then this tweet is labelled as neutral but ac-tually is negative with respect to the party Sinn Fe?in.We proposed the use of distance as a measure of re-latedness to the topic and we observed a minor im-provement in classification accuracy.
However, forthis example, the distance-based approach does notwork.
The word top is just two words away from thetopic and thus contributes the maximum, resulting inthe whole tweet being misclassified as positive.6.3 Sarcastic Tweets?Political discouse is plagued with humor, doubleentendres, and sarcasm; this makes determining po-litical preference of users hard and inferring votingintention even harder.?
(Gayo-Avello, 2012)As part of the annotation process, annotators wereasked to indicate whether they thought a tweet ex-hibited sarcasm.
Some examples of tweets that wereannotated as sarcastic are shown in Table 5.We made the decision to omit these tweets fromthe main sentiment classification experiments underthe assumption that they constituted a special casewhich would be better handled by a different clas-sifier.
This decision is vindicated by the results inthe last row of Table 2 which show what happenswhen we apply our best classifier (Distance-basedScoring: Baseline+Adj+S+Neg+Phrases+Than) tothe sarcastic tweets ?
only a quarter of them are cor-rectly classified.
Even with a very large and highlydomain-tuned lexicon, the lexicon-based approachon its own will struggle to be of use for cases suchas these, but the situation might be improved werethe lexicon to be used in conjunction with possiblesarcasm indicators such as exclamation marks.7 Supervised Machine LearningAlthough our dataset is small, we investigatewhether we can improve over the lexicon-based ap-proach by using supervised machine learning.
Asour learning algorithm, we employ support vectormachines in a 5-fold cross validation setup.
The toolwe use is SVMLight (Joachims, 1999).We explore two sets of features.
The first are thetried-and-tested unigram presence features whichhave been used extensively not only in sentimentanalysis but in other text classification tasks.
As wehave only 2,624 training samples, we performed fea-ture selection by ranking the features using the Chi-squared metric.The second feature set consists of 25 featureswhich are inspired by the work on lexicon-basedclassification described in the previous section.These are the counts of positive, negative, objec-tive words according to each of the three lexiconsand the corresponding sentiment scores for the over-all tweets.
In total there are 19 such features.
Wealso employ six Twitter-related presence features:positive emoticons, negative emoticons, URLs, pos-itive hashtags, negative hashtags and neutral hash-tags.
For further reference we call this second set offeatures our ?hand-crafted?
features.The results are shown in Table 6.
We can seethat using the hand-crafted features alone barely im-proves over the majority baseline of 54.03 but it doesimprove over our baseline lexicon-based approach(see first row of Table 2).
Encouragingly, we seesome benefit from using these features in conjunc-55Tweet TopicManualPolar-ityCalculatedPolarityReason formisclassifica-tion@username and u believe people in fianna fail .What are you a numbskull or a journalist ?FiannaFa?ilneg neuFocus only onadjectives@username LOL .
A guy called to our housetonight selling GAA tickets .
His first words were :I?m not from Fianna Fail .FiannaFa?ilneg neuNo sentimentwords@username Such scary words .?
Sinn Fein couldtop the poll ?
in certain constituencies .
I feel sickat the thought of it .SinnFe?inneg posStemmingand worddistance order@username more RTE censorship .
Why are theyso afraid to let Sinn Fein put their position across .Certainly couldn?t be worse than ffSinnFe?inpos negcontributionof afraidBased on this programme the winners will be SinnFein & Gilmore for not being there #rteflSinnFe?inpos neuFocus only onadjectives#thefrontline pearce Doherty is a spoofer !
Votesinn fein and we loose more jobsSinnFe?inneg posFocus only onadjectives &contributionof phrase VoteX@username Tread carefully Conor .
BNPendorsing Sinn Fin etc .
etc .SinnFe?inneg neuNo sentimentwords@username ah dude .
You made me go to the finegael web site !
:/FineGaelneg neuNo sentimentwordsTable 4: Misclassification ExamplesFeature Set # Features Accuracy# samples = 2624 SVM LightHand-crafted 25 54.76Unigram7418 55.22Top 1000 58.92Top 100 56.86Unigram + Hand-crafted7444 54.73Top 1000 61.62Top 100 59.53Table 6: Results of 3-Class Classification using Super-vised Machine Learningtion with the unigram features.
Our best overall re-sult of 61.62% is achieved by using the Top 1000 un-igram features together with these hand-crafted fea-tures.
This result seems to suggest that, even withonly a few thousand training instances, employingsupervised machine learning is still worthwhile.8 ConclusionWe have introduced a new dataset of political tweetswhich will be made available for use by other re-searchers.
Each tweet in this set has been annotatedfor sentiment towards a political entity, as well asfor the presence of sarcasm.
Omitting the sarcastictweets from our experiments, we show that we canclassify a tweet as being positive, negative or neutraltowards a particular political party or party leaderwith an accuracy of almost 59% using a simple ap-proach based on lexicon lookup.
This improves overthe majority baseline by almost 5 absolute percent-age points but as the classifier uses information fromthe test set itself, the result is likely to be lower ona held-out test set.
The accuracy increases slightlywhen the lexicon-based information is encoded asfeatures and employed together with bag-of-wordfeatures in a supervised machine learning setup.Future work involves carrying out further exper-56Sarcastic TweetsAh bless Brian Cowen?s little cotton socks!
He?s staying on as leader of FF because its better for thecountry.
How selfless!So now Brian Cowen is now Minister for foreign affairs and Taoiseach?
Thats exactly what he needsmore responsibilities http://bbc.in/hJI0hbMary Harney is going.
Surprise surprise!
Brian Cowen is going to be extremely busy with all theseportfolios to administer.
Super hero!Now in its darkest hour Fianna Fail needs.
.
.
Ivor!Labour and Fine Gael have brought the election forward by 16 days Crisis over Ireland is SAVED!!
#vinb@username Maybe one of those nice Sinn Fein issue boiler suits?
#rteflI WILL vote for Fine Gael if they pledge to dress James O?Reilly as a leprechaun and send himto the White House for Paddy?s Day.Table 5: Examples of tweets which have been flagged as sarcasticiments on those tweets that have been annotated assarcastic, exploring the use of syntactic dependencypaths in the computation of distance between a wordand the topic, examining the role of training set classbias on the supervised machine learning results andexploring the use of distant supervision to obtainmore training data for this domain.AcknowledgementsThanks to Emmet O Briain, Lesley Ni Bhriain andthe anonymous reviewers for their helpful com-ments.
This research has been supported by En-terprise Ireland (CFTD/2007/229) and by ScienceFoundation Ireland (Grant 07/CE/ I1142) as part ofthe CNGL (www.cngl.ie) at the School of Comput-ing, DCU.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10).Akshat Bakliwal, Piyush Arora, Senthil Madhappan,Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.2012.
Mining sentiments from tweets.
In Proceedingsof the WASSA?12 in conjunction with ACL?12.Adam Bermingham and Alan F. Smeaton.
2010.
Clas-sifying sentiment in microblogs: is brevity an advan-tage?
In Proceedings of the 19th ACM internationalconference on Information and Knowledge Manage-ment.Adam Bermingham and Alan Smeaton.
2011.
On usingTwitter to monitor political sentiment and predict elec-tion results.
In Proceedings of the Workshop on Sen-timent Analysis where AI meets Psychology (SAAIP2011).Nibir Nayan Bora.
2012.
Summarizing public opinionsin tweets.
In Journal Proceedings of CICLing 2012.Paula Chesley.
2006.
Using verbs and adjectives to au-tomatically classify blog sentiment.
In Proceedingsof AAAI-CAAW-06, the Spring Symposia on Computa-tional Approaches.Murphy Choy, Michelle L. F. Cheong, Ma Nang Laik,and Koo Ping Shung.
2011.
A sentiment analysisof Singapore Presidential Election 2011 using Twitterdata with census correction.
CoRR, abs/1108.5520.Isaac G. Councill, Ryan McDonald, and Leonid Ve-likovich.
2010.
What?s great and what?s not: learn-ing to classify the scope of negation for improved sen-timent analysis.
In Proceedings of the Workshop onNegation and Speculation in Natural Language Pro-cessing, NeSp-NLP ?10.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using Twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Proceedings of the COLING Workshopon Cross-Framework and Cross-Domain Parser Eval-uation.Michael Gallagher and Michael Marsh.
2011.
How Ire-land Voted 2011: The Full Story of Ireland?s Earth-quake Election.
Palgrave Macmillan.Daniel Gayo-Avello.
2012.
?I wanted to predict elec-tions with Twitter and all I got was this lousy paper?.57A balanced survey on election prediction using Twitterdata.
CoRR, abs/1204.6441.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers - Volume 2.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twittersentiment classification using distant supervision.
InCS224N Project Report, Stanford University.Namrata Godbole, Manjunath Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis fornews and blogs.
In Proceedings of the InternationalConference on Weblogs and Social Media (ICWSM).Vasileios Hatzivassiloglou and Janyce M. Wiebe.
2000.Effects of adjective orientation and gradability on sen-tence subjectivity.
In Proceedings of COLING.Thorsten Joachims.
1999.
Advances in kernel meth-ods.
chapter Making large-scale support vector ma-chine learning practical, pages 169?184.
MIT Press,Cambridge, MA, USA.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1.Bing Liu.
2010.
Handbook of natural language pro-cessing.
chapter Sentiment Analysis and Subjectivity.Chapman and Hall.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.
Fromtweets to polls: Linking text sentiment to public opin-ion time series.
In Proceedings of the InternationalConference on Weblogs and Social Media (ICWSM).Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InProceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the con-ference on Empirical Methods in Natural LanguageProcessing - Volume 10.James W. Pennebaker, Cindy K. Chung, Molly Ireland,Amy Gonzales, and Roger J. Booth.
2007.
The de-velopment and psychometric properties of liwc2007.Technical report, Austin,Texas.Andranik Tumasjan, Timm Oliver Sprenger, Philipp G.Sandner, and Isabell M. Welpe.
2010.
Predicting elec-tions with Twitter: What 140 characters reveal aboutpolitical sentiment.
In Proceedings of the Interna-tional Conference on Weblogs and Social Media.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics.Hao Wang, Dogan Can, Abe Kazemzadeh, Franc?ois Bar,and Shrikanth Narayanan.
2012.
A system for real-time Twitter sentiment analysis of 2012 U.S. presiden-tial election cycle.
In ACL (System Demonstrations).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing.58
