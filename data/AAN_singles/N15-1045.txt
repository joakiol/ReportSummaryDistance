Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 420?429,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsAn In-depth Analysis of the Effect of Text Normalization in Social MediaTyler Baldwin?baldwin.tyler.s@gmail.comYunyao LiIBM Research - Almaden650 Harry RoadSan Jose, CA 95120, USAyunyaoli@us.ibm.comAbstractRecent years have seen increased interest intext normalization in social media, as the in-formal writing styles found in Twitter andother social media data often cause problemsfor NLP applications.
Unfortunately, mostcurrent approaches narrowly regard the nor-malization task as a ?one size fits all?
task ofreplacing non-standard words with their stan-dard counterparts.
In this work we build ataxonomy of normalization edits and present astudy of normalization to examine its effect onthree different downstream applications (de-pendency parsing, named entity recognition,and text-to-speech synthesis).
The results sug-gest that how the normalization task should beviewed is highly dependent on the targeted ap-plication.
The results also show that normal-ization must be thought of as more than wordreplacement in order to produce results com-parable to those seen on clean text.1 IntroductionThe informal writing style employed by authors ofsocial media data is problematic for many naturallanguage processing (NLP) tools, which are gener-ally trained on clean, formal text such as newswiredata.
One possible solution to this problem is nor-malization, in which the informal text is convertedinto a more standard formal form.
Because of this,the rise of social media data has coincided with arise in interest in the normalization problem.Unfortunately, while many approaches to theproblem exist, there are notable limitations to the?Work was done while at IBM Research - Almaden.way in which normalization is examined.
First,although social media normalization is universallymotivated by pointing to its role in helping down-stream applications, most normalization work giveslittle to no insight into the effect of the normalizationprocess on the downstream application of interest.Further, the normalization process is generally seento be agnostic of the downstream application, adopt-ing a ?one size fits all?
view of how normalizationshould be performed.
This view seems intuitivelyproblematic, as different information is likely to beof importance for different tasks.
For instance, whilecapitalization is important for resolving named enti-ties, it is less important for other tasks, such as de-pendency parsing.Some recent work has given credence to the ideathat application-targeted normalization is appropri-ate (Wang and Ng, 2013; Zhang et al, 2013).
How-ever, how certain normalization actions influence theoverall performance of these applications is not wellunderstood.
To address this, we design a taxonomyof possible normalization edits based on inspirationfrom previous work and an examination of anno-tated data.
We then use this taxonomy to examinethe importance of individual normalization actionson three different downstream applications: depen-dency parsing, named entity recognition, and text-to-speech synthesis.
The results suggest that the im-portance of a given normalization edit is highly de-pendent on the task, making the ?one size fits all?approach inappropriate.
The results also show that anarrow view of normalization as word replacementis insufficient, as many often-ignored normalizationactions prove to be important for certain tasks.420In the next section, we give an overview of previ-ous work on the normalization problem.
We thenintroduce our taxonomy of normalization edits inSection 3.
In Section 4, we present our evaluationmethodology and present results over the three ap-plications, using Twitter data as a representative do-main.
Finally, we discuss our results in Section 5and conclude in Section 6.2 Related WorkTwitter and other social media data is littered withnon-standard word forms and other informal usagepatterns, making it difficult for many NLP tools toproduce results comparable to what is seen on for-mal datasets.
There are two approaches proposedin the literature to handle this problem (Eisenstein,2013).
One approach is to tailor a specific NLP tooltowards the data, by using training data from the do-main to help the tool learn its specific idiosyncrasies.This approach has been applied with reasonable suc-cess on named entity recognition (Liu et al, 2011b;Ritter et al, 2011) as well as on parsing and part-of-speech tagging (Foster et al, 2011).The other approach is normalization.
Rather thantailoring a NLP tool towards the data, normalizationseeks to tailor the data towards the tool.
This isaccomplished by transforming the data into a formmore akin to the formal text that NLP tools are gen-erally trained on.
While normalization is often morestraightforward and more easily applied in instancesin which retraining is difficult or impractical, it haspotential disadvantages as well, such as the potentialloss of pragmatic nuance (Baldwin and Chai, 2011).Prior to the rise of social media, the normalizationprocess was primarily seen as one of standardizingnon-standard tokens found in otherwise clean text,such as numbers, dates, and acronyms (Sproat et al,2001).
However, the current popularity of Twitterand other informal texts has caused the normaliza-tion task to take on a broader meaning in these con-texts, where the goal is to convert informal text intoformal text that downstream applications expect.Many different approaches to social media nor-malization have been undertaken.
These approachesoften draw inspiration from other tasks such as ma-chine translation (Pennell and Liu, 2011; Aw et al,2006), spell checking (Choudhury et al, 2007) orspeech recognition (Kobus et al, 2008).
Other ap-proaches include creating automatic abbreviationsvia a maximum entropy classifier (Pennell and Liu,2010), creating word association graphs (Sonmezand Ozgur, 2014), and incorporating both rules andstatistical models (Beaufort et al, 2010).
Whilemost initial approaches used supervised methods,unsupervised methods have recently become popu-lar (Cook and Stevenson, 2009; Liu et al, 2011a;Yang and Eisenstein, 2013; Li and Liu, 2014).
Somework has chosen to focus on specific aspects of thenormalization process, such as providing good cov-erage (Liu et al, 2012) or building normalizationdictionaries (Han et al, 2012).In all of the work mentioned above, the normal-ization task was seen primarily as one of convert-ing non-standard tokens into an equivalent standardform.
Similarly, many of these works defined theproblem even more narrowly such that punctuation,capitalization, and multi-word replacements wereignored.
However, two pieces of recent work havesuggested that this understanding of the normaliza-tion task is too narrow, as it ignores many otherhallmarks of informal writing that are prevalent insocial media data.
Wang and Ng (2013) present abeam search based approach designed to handle ma-chine translation which incorporates attempts to cor-rect mistaken punctuation and add missing words,such as forms of the verb to be.
Similarly, Zhang etal.
(2013) attempt to perform all actions necessaryto create a formal text.
In both instances the workwas motivated by, and evaluated with respect to, aspecific downstream application (machine transla-tion and dependency parsing, respectively).
How-ever, not every study that tied the output to an ap-plication chose a broad interpretation of the normal-ization problem (Beaufort et al, 2010; Kaji and Kit-suregawa, 2014).3 Taxonomy of Normalization EditsIn order to understand the impact of individual nor-malization edits on downstream applications, wefirst need to define the space of possible normaliza-tion edits.
While it is not uncommon for normaliza-tion work to present some analysis of the data, theseanalyses are often quite specific to the domain anddatasets of interest.
Because there is no agreed upon421ReplacementPunctuation WordInsertionBeSubj.
Det.
OtherPunctuation WordSlang Contraction OtherCapitalizationRemovalPunctuation WordTwitter OtherEditFigure 1: Taxonomy of normalization editstaxonomy of normalization token or edit types, dif-ferent analyses often look at different edit types andat different levels of granularity.
In an attempt tohelp future work converge on a common understand-ing of normalization edits, in this section we presentour taxonomy of normalization edits at several dif-ferent levels of granularity.
While it would be diffi-cult for a taxonomy of normalization edits to be uni-versal enough to be appropriate over all datasets anddomains, we attempt to provide a taxonomy generalenough to give future work a meaningful initial pointof reference.3.1 MethodologyOur taxonomy draws inspiration from both previouswork and an examination of our own dataset (Sec-tion 3.3).
In doing so, it attempts to cover normal-ization edits broadly, including cases that are uni-versally understood to be important, such as slangreplacement, as well as cases that are frequently ig-nored, such as capitalization correction.One of the guiding principles in the design of ourtaxonomy was that categories should not be dividedso narrowly such that the phenomenon they captureappeared very infrequently in the data.
One exam-ple of this is our decision not to divide punctuationedits at the lowest level of granularity.
While certainclear categories exist (e.g., emoticons), these casesappeared in a small enough percentage of tokens thatthey would be difficult to examine and likely have anegligible effect on overall performance.3.2 TaxonomyOur taxonomy of normalization edits is shown inFigure 1.
As can be seen, we categorize edits at threelevels of granularity.Level One.
The primary goal of the level one seg-mentation is to separate token replacements whichare most centrally thought of as part of the normal-ization task from other instances that may requireadditional pragmatic inference.
Specifically, we sep-arate edits coarsely into three categories:?
Token Replacements.
Replacing one or moreexisting tokens with one or more new tokens(e.g., replacing wanna with want to).?
Token Additions.
Adding a token that doesnot replace an existing token (e.g., adding inmissing subjects).?
Token Removals.
Removing a token withoutreplacing it with an equivalent (e.g., removinglaughter words such as lol and hahaha).Level Two.
The next level of granularity separatesnormalization edits over word tokens from thoseover punctuation:?
Word.
Replacing, adding, or removing wordtokens (depending on parent).?
Punctuation.
Replacing, adding, or removingpunctuation tokens (depending on parent).Level Three.
At the final level, we subdivide wordedits into groups as appropriate for the edit type.Rather than attempting to keep consistent groupsacross all leaf nodes, we selected the grouping basedon the data distribution.
For instance, Twitter-specific tokens (e.g., retweets) are often removedduring normalization, so examining the removal ofthese words as a group is warranted.
In contrast,these tokens are never added, so different segmenta-tion is appropriate when examining word addition.At the lowest level of the taxonomy, word replace-ments were subdivided as follows:422?
Contraction Replacements.
Unrolling stan-dard contractions (don?t), common informalcases (wanna), and non-standard variationsproduced via apostrophe omission (dont).?
Slang Replacements.
Replacing slang terms,such as slang shortenings and word elongation.?
Capitalization Replacements.
Correcting thecapitalization of words.
The replaced word dif-fers from its replacement by only capitalization.?
Other Replacements.
Correcting uninten-tional typographic mistakes, such as mis-spelling and word concatenation.When segmenting word additions, we note thatwords that need to be added in a normalization editwere often consciously dropped by the user in theoriginal text.
Our categorization reflects this by ex-amining syntactic categories that are often droppedin informal writing:?
Subject Addition.
Adding in omitted subjects.?
Determiner Addition.
Adding in omitted de-terminers (e.g., ?
[The] front row is so close?).?
Be-verb Addition.
Adding in omitted forms ofthe verb to be.?
Other Addition.
All word additions not cov-ered by the other categories.Finally, word removals are subdivided into justtwo categories:?
Dataset-specific Removals.
Removing tokensthat do not appear outside of the dataset inquestion (e.g., for Twitter: hashtags, @replies,and retweets).?
Other Removals.
Removing interjections,laughter words, and other expression of emo-tion (e.g., ugh).Note that we are not suggesting here that dataset-specific words should be removed in all cases.
Whilein many cases they may be removed if they do nothave a formal equivalent, they may also be replacedor retained as is, depending on the context.3.3 DatasetTo facilitate our experiments, we collected and an-notated a dataset of Twitter posts (tweets) from theTREC Twitter Corpus1.
The TREC Twitter corpusis a collection of 16 million tweets posted in Jan-uary and February of 2011.
The corpus is designedto be a representative sample of Twitter usage, andas such includes both regular and spam tweets.
Tobuild our dataset, we sampled 600 posts at randomfrom the corpus.
The tweets were then manually fil-tered such that tweets that were not in English werereplaced with those in English.To produce our gold standard, two oDesk2con-tractors were asked to manually normalize eachtweet in the dataset to its fully grammatical form, aswould be found in formal text.
Annotation guide-lines stipulated that twitter-specific tokens shouldbe retained if important to understanding the sen-tence, but modified or removed otherwise.
Asnoted, most previous work often stopped short ofrequiring full grammaticality.
However, Zhang etal.
(2013) argued that grammaticality should bethe ideal end goal of normalization since the mod-els used in downstream applications are typicallytrained on well-formed sentences.
We adopt thismethodology here both because we agree with thisassertion and because a fully grammatical form isappropriate for all of the downstream applications ofinterest, allowing for a single unified gold standardthat can aid comparison across applications.During gold standard creation, each normaliza-tion edit was labeled with its type, according to theabove taxonomy.
The distribution of normalizationedits in the dataset is given in Table 1.
As shown,normalization edits accounted for about 29% of alltokens.
Token replacements accounted for just overhalf of all edits (53%), while token addition (29%)was more common than token removal (18%).
Oneinteresting observation is non-capitalization wordreplacement accounted for only 25% of all normal-ization edits, intuitively indicating potential draw-backs for the common definition of normalization asone of simple word replacement which ignores cap-italization and punctuation.1http://trec.nist.gov/data/tweets/2https://www.odesk.com/423Configuration CountNo edit 8479All edits 3411ADDITION 993PUNCT 437WORD 556BEVERB 137DETERMINER 103OTHER 141SUBJECT 175REPLACEMENT 1797PUNCT 312WORD 1485CAPITALIZATION 634CONTRACTION 246OTHER 176SLANG 429REMOVAL 621PUNCT 120WORD 501OTHER 172TWITTER 329Table 1: Token counts for each type of normalization edit.4 EvaluationIn this section, we present our examination of the ef-fect of normalization edits on downstream NLP ap-plications.
To get a broad understanding of theseeffects, we examine three very different cases: de-pendency parsing, named entity recognition (NER),and text-to-speech (TTS) synthesis.
We chose thesetasks because they each require the extraction ofdifferent information from the text.
For instance,named entity recognition requires only a shallowsyntactic analysis, in contrast to the deeper under-standing required for dependency parsing.
Simi-larly, only speech synthesis requires phoneme pro-duction, while the other tasks do not.
Despite theirdifferences, each of these tasks is relevant to largerapplications that would benefit from improved per-formance on Twitter data, and each has garnered at-tention in the normalization and Twitter-adaptationliterature (Beaufort et al, 2010; Liu et al, 2011b;Zhang et al, 2013).Although the differences in these tasks also dic-tates that they be evaluated somewhat differently, weexamine them within a common evaluation struc-ture.
In all cases, to examine the effects of each nor-malization edit we model our analyses as ablationstudies.
That is, for every category in the taxonomy,we examine the effect of performing all normaliza-tion edits except the relevant case.
This allows us tomeasure the drop in performance solely attributableto each category; the greater the performance dropobserved when a given normalization edit is not per-formed, the greater the importance of performingthat edit.To aid analysis, results are presented in two ways:1) as raw performance numbers, and 2) as an er-ror rate per-token.
These metrics give two differentviews of the relevance of each edit type.
The rawnumbers give a sense of the overall impact of a givencategory, and as such may be impacted by the size ofthe category, with common edits becoming more im-portant simply by virtue of their frequency.
In con-trast, the per-token error rate highlights the cost offailing to perform a single instance of a given nor-malization edit, independent of the frequency of theedit.
Both of these measures are likely to be relevantwhen attempting to improve the performance of anormalization system.
Note that since the first mea-sure is one of overall performance, smaller numbersreflect larger performance drops when removing agiven type of edit, so that the smaller the numberthe more critical the need to perform the given typeof normalization.
In contrast, the latter judgment isone of error rate, and thus interpretation is reversed;the larger the error rate when it is removed, the morecritical the normalization edit.Another commonality among the analyses is thatperformance is measured relative to the top perfor-mance of the tool, not the task.
That is, followingZhang et al (2013), we consider the output pro-duced by the tool (e.g., the dependency parser) onthe grammatically correct data to be gold standardperformance.
This means that some output basedon our gold standard may in fact be incorrect rel-ative to human judgment, simply because the toolused does not have perfect performance even if thetext if fully grammatical.
Since the goal is to un-derstand how normalization edits impact the perfor-mance, this style of evaluation is appropriate; it con-siders mistakes attributable to normalization edits aserroneous, but ignores those mistakes attributable tothe limitations of the tool.Finally, to maximize the relevance of the analyses424given here, in each case we employ publicly avail-able and widely used tools.4.1 Parser EvaluationTo examine the effect of normalization on depen-dency parsing, we employ the Stanford dependencyparser3(Marneffe et al, 2006).
To produce the goldstandard dependencies for comparison, the manu-ally grammaticalized tweets (Section 3.3) were runthrough the parser.
To compare the ablation resultsto the gold standard parses, we adopt a variation ofthe evaluation method used by Zhang et al (2013).Given dependency parses from the gold standard anda candidate normalization, we define precision andrecall as follows:precisionsov=|SOV ?
SOVgold||SOV |(1)recallsov=|SOV ?
SOVgold||SOVgold|(2)Where SOV and SOVgoldare the sets of subject,object, and verb dependencies in the candidate nor-malization and gold standard, respectively.
WhileZhang et al chose to examine subjects and objectsseparately from verbs, we employ a unified metricto simplify interpretation.4.1.1 ResultsResults of the ablation study are summarized inTable 2.
As shown, the performance of a com-plex task such as dependency parsing is broadly im-pacted by a variety of normalization edits.
Basedon the raw F-measure, the more common word re-placements proved to be the most critical, althoughfailing to handle token addition and removal editsalso resulted in substantial drops in performance.
Atthe lowest level in the taxonomy, slang replacementsand subject addition were the most critical edits.Although many replacement tasks were importantin aggregate, on a per-token basis the most importantedits were those that required token removal and ad-dition.
Perhaps unsurprisingly, failing to add sub-jects and verbs resulted in the largest issues, as theparser has little chance of identifying these depen-dencies if the terms simply do not appear in the sen-tence.
However, not all word additions proved crit-3Version 2.0.5Per-tokenConfiguration F-measure Error Rate-ADDITION 0.790 0.00021-PUNCT 0.919 0.00019-WORD 0.842 0.00028-BEVERB 0.948 0.00038-DETERMINER 0.980 0.00019-OTHER 0.959 0.00029-SUBJECT 0.903 0.00055-REPLACEMENT 0.710 0.00016-PUNCT 0.907 0.00030-WORD 0.754 0.00017-CAPITALIZATION 0.950 0.00008-CONTRACTION 0.945 0.00023-OTHER 0.947 0.00030-SLANG 0.872 0.00030-REMOVAL 0.866 0.00022-PUNCT 0.959 0.00034-WORD 0.887 0.00023-OTHER 0.952 0.00028-TWITTER 0.925 0.00023Table 2: Dependency Parser Results.ical, as failing to add in a missing determiner gen-erally had little impact on the overall performance.Similarly, failing to correct capitalization did notcause substantial problems for the parser.
Someword replacements did prove to be important, withslang and other word replacements showing some ofthe largest per-token error rates.
Removing mislead-ing punctuation or changing non-standard punctua-tion both proved important, but the per-token effectof punctuation addition was modest.In general, the results suggest that a complextask such as dependency parsing suffers substan-tially when the input data differs from formal textin any number of ways.
With the exception of cap-italization correction, performing almost every nor-malization edit is necessary to achieve results com-mensurate with those seen on formal text.4.2 NER EvaluationIn this section, we examine the effect of each nor-malization edit on a somewhat more shallow inter-pretation task, named entity recognition.
Unlike de-pendency parsing which requires an understandingof every token in the text, NER must only determinewhether a given token is a named entity, and if so,discover its associated entity type.425The setup for evaluation of normalization edits onnamed entity recognition closely follows that of de-pendency parsing.
We once again employ a toolfrom the suite of Stanford NLP tools, the Stanfordnamed entity recognizer4(Finkel et al, 2005).
Wealso define precision and recall in a similar manner:precisionner=|ENT ?
ENTgold||ENT |(3)recallner=|ENT ?
ENTgold||ENTgold|(4)Where ENT and ENTgoldare the sets of enti-ties identified over the candidate normalization andgold standard sentences, respectively.
Entities werelabeled as one of three classes (person, location,or organization), and two entities were only con-sidered a match if they both selected the same entityand the same entity class.4.2.1 ResultsTable 3 shows the results of the NER ablationstudy.
Unlike dependency parsing, only word re-placement edits proved to be critically important forNER tasks, as adding and subtracting words had lit-tle impact on the overall performance.
Capitaliza-tion, which is generally an important feature for theidentification of named entities, was unsurprisinglyimportant.
Similarly, the replacement of word typesother than slang and contraction was important, be-cause many of these instances may come from mis-spelled named entities.
Slang and contractions wereless important, as they were generally not used toreference named entities.
As the words droppedby Twitter users tend to be function words that arenot critical to understanding the sentence they arerarely named entities and have only a small effecton named entity recognition.
Similarly, terms thatare removed during normalization also tend to not benamed entities, and thus has minor overall impact.A similar phenomenon is observed in the per-token evaluation, where unintentionally produced,non-slang, non-contraction word replacement wasseen to be of paramount importance.
Punctuationremoval was also important on a per-token basis, de-spite having little impact in aggregate.Overall, the results given in Table 3 indicate that afocused approach to normalization for named entity4Version 1.2.8Per-tokenConfiguration F-measure Error Rate-ADDITION 0.955 0.00005-PUNCT 0.973 0.00006-WORD 0.974 0.00005-BEVERB 0.998 0.00001-DETERMINER 0.989 0.00011-OTHER 0.989 0.00008-SUBJECT 0.998 0.00001-REPLACEMENT 0.827 0.00010-PUNCT 0.962 0.00012-WORD 0.849 0.00010-CAPITALIZATION 0.921 0.00012-CONTRACTION 0.977 0.00009-OTHER 0.931 0.00039-SLANG 0.945 0.00013-REMOVAL 0.956 0.00007-PUNCT 0.970 0.00025-WORD 0.960 0.00008-OTHER 0.973 0.00015-TWITTER 0.962 0.00012Table 3: NER Results.recognition is warranted.
Unlike dependency pars-ing that required a broad approach involving tokenaddition and removal, the replacement-centric nor-malization approach typically employed by previouswork is likely to be sufficient when the goal is to im-prove entity recognition.4.3 TTS EvaluationUnlike the previous two tasks, the TTS problem iscomplicated by its need for speech production.
Sim-ilarly, evaluation of speech synthesis is more diffi-cult, as it requires human judgment about the over-all quality of the output (Black and Tokuda, 2005).While speech synthesis evaluations often rate perfor-mance on a 5 point scale, we adopt a more restrictedmethod, based on the comparison to gold standardmethodology used in the previous evaluations.
Foreach tweet and each round of ablation, a synthesizedaudio file was produced from both the gold stan-dard and ablated version of the tweet.
These audiosnippets were randomized and presented to humanjudges who were asked to make a binary judgmentas to whether the meaning and understandability ofthe ablated case was comparable to the gold stan-dard.
The accuracy of a given round of ablation isthen calculated to be the percentage of tweets judged426Per-tokenConfiguration F-measure Error Rate-ADDITION 0.713 0.00029-PUNCT 0.920 0.00018-WORD 0.723 0.00050-BEVERB 0.903 0.00071-DETERMINER 0.937 0.00061-OTHER 0.910 0.00064-SUBJECT 0.853 0.00084-REPLACEMENT 0.550 0.00025-PUNCT 0.877 0.00040-WORD 0.590 0.00028-CAPITALIZATION 0.860 0.00022-CONTRACTION 0.910 0.00037-OTHER 0.883 0.00066-SLANG 0.783 0.00051-REMOVAL 0.580 0.00068-PUNCT 0.880 0.00100-WORD 0.600 0.00080-OTHER 0.837 0.00095-TWITTER 0.710 0.00088Table 4: Text-To-Speech Synthesis Results.to be similar to the gold standard.The eSpeak speech synthesizer5was used to pro-duce audio files for all tweet variations in the abla-tion study.
As is common for speech synthesizers,eSpeak does perform some amount of TTS-specificnormalization natively.
While this does influencethe normalizations produced, the comparison to goldstandard methodology employed in this study helpsus to focus on differences that are primarily at-tributable to the normalization edits we wish to ex-amine, not those produced natively.
To obtain thegold standard, two native-English speaking judgeswere recruited via oDesk.
Inter-annotator agreementwas moderate, ?
= 0.48.4.3.1 ResultsTable 4 shows the results of the speech synthesisstudy.
As shown, the removal of non-standard or outof place tokens was most critical to the productionof a normalization that is clearly understandable tohuman listeners.
The aggregate results for token re-movals were comparable to or better than those ofreplacements at all levels of the taxonomy, in con-trast to the results from the other two tasks, wherethe larger number of replacements led to the largest5Version 1.47.11, http://espeak.sourceforge.net/performance hits.
Meanwhile, word addition provedto be less essential overall.At the token level, the importance of token re-moval is even more stark; the per-token error rateof every category of removal is greater than thatof all other categories at the same taxonomy level.Although most word additions had a comparativelysmall effect on performance overall, they were im-portant on a per-token basis.
Most notably, sub-ject adding had high per-token importance.
In con-trast, failing to add missing punctuation was not of-ten marked as erroneous by human judges, nor wasfailing to normalize capitalization or contractions.Similar to those on dependency parsing, the re-sults on speech synthesis suggest that a broad ap-proach that considers several different types of nor-malization edit is necessary to produce results com-parable to those seen on clean text.
However, at ahigh level there is a clear divide in importance be-tween normalization types, where the greatest per-formance gains can be obtained by focusing on thecomparatively small number of token removals.5 DiscussionThe results presented in Section 4 are consistentwith the hypothesis that a ?one size fits all?
approachto Twitter normalization is problematic, as the im-portance of a given normalization edit was highlydependent on the intended downstream task.
Differ-ences in which edits had the most substantial effectwere present at all levels of scrutiny.
Adding sub-jects and other words that a Twitter author droppedcan be vitally important if the goal is to improveparsing performance, but can mostly be ignored ifthe goal is NER.
Removing twitter-specific or other-wise non-standard words showed a gradation of im-portance over the three tasks, with little importancefor NER, moderate importance for parsing, and crit-ical importance for speech synthesis.
Capitalizationcorrection had negligible impact on the parser orsynthesizer, but was helpful for NER.The importance of different edit types can be seeneven at the most coarse level of examination.
Whilenormalization for speech synthesis is primarily de-pendent on removing unknown tokens, normaliza-tion that targets name entity recognition would bebetter served focusing on replacing non-standard to-427kens with their standard forms.
In contrast, parser-targeted normalization must attend to both of thetasks, as well as the task of restoring dropped tokens.Despite the differences, there are a few commonthreads that appear in each evaluation.
Most no-tably, the results suggest that the decision of most re-cent Twitter normalization work to focus on word re-placement was not entirely without merit, as the highfrequency of token replacements translated into highoverall importance for all tasks.
Similarly, the focuson slang was also somewhat reasonable, as failing tohandle slang terms had a significant impact on pars-ing and speech synthesis, though it had little impacton entity recognition.
Nonetheless, the results inSection 4 clearly suggest that handling these casesrepresent only a small fraction of the actions nec-essary to produce performance comparable to whatwould be seen on formal text.Another similarity among all instances was thelack of importance of certain categories.
For in-stance, punctuation addition was not important forany of the three tasks.
While Zhang et al had hy-pothesized that punctuation addition would be im-portant for dependency parsing, the results givenhere suggest that the overall impact is minor.
Sim-ilarly, contraction standardization was not shown tobe important in any of the evaluations.
Contractionnormalization is more representative of how the nor-malization task was seen prior to the rise of socialmedia normalization, as it represents a fairly minornormalizing action that might still be performed onformal text.
Since contractions likely appear in a va-riety of forms in the data used to train NLP tools, itis unsurprising that these tools are comparatively ro-bust to contraction differences than to cases that areless typically encountered.6 ConclusionIn this work, we presented an in-depth look at theeffects of the normalization of Twitter data.
To doso, we introduced a taxonomy of normalization ed-its based on an examination of our Twitter datasetand inspiration from previous work.
The taxonomyallowed for normalization edits to be examined sys-tematically at different levels of granularity, and en-abled an examination of the effects of not only tokenreplacements, but the token additions and removalsthat recent work has suggested may have been un-justly ignored.To understand the effects of each edit, we con-ducted ablation studies that examined results onthree different downstream tasks: dependency pars-ing, named entity recognition, and text-to-speechsynthesis.
We found that while some normaliza-tion edits were universally important (or unimpor-tant) for the production of accurate results, manydifferences persist.
These results suggest that, forbest results, how the normalization task is performedshould not be agnostic of the downstream applica-tion.
Further, our results support the suggestion thatin order for downstream applications to produce ac-curate results, in most cases it is necessary to take abroad view of the normalization task the looks be-yond simple word replacements.AcknowledgmentsThe authors would like to thank Benny Kimelfeldfor his comments on an early draft of this work.We also thank our anonymous reviewers for theirconstructive comments and feedback, and StephanieMcneish, Lacy Corlis, and Kaila Milos C. Factolerinfor their assistance with annotation and evaluation.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for sms text normaliza-tion.
In ACL, pages 33?40.Tyler Baldwin and Joyce Chai.
2011.
Beyond normal-ization: Pragmatics of word form in text messages.In IJCNLP, pages 1437?1441, Chiang Mai, Thailand,November.
Asian Federation of Natural Language Pro-cessing.Richard Beaufort, Sophie Roekhaut, Louise-Am?elieCougnon, and C?edrick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing sms messages.
In ACL, pages 770?779.Alan W. Black and Keiichi Tokuda.
2005.
The blizzardchallenge - 2005: evaluating corpus-based speech syn-thesis on common datasets.
In INTERSPEECH, pages77?80.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
IJDAR, 10(3-4):157?174.428Paul Cook and Suzanne Stevenson.
2009.
An unsuper-vised model for text message normalization.
In CALC,pages 71?78.Jacob Eisenstein.
2013.
What to do about bad languageon the internet.
In NAACL-HLT, pages 359?369, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In ACL, pages 363?370, Ann Arbor, Michigan, June.Association for Computational Linguistics.Jennifer Foster,?Ozlem C?etinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011. hard-toparse: Pos tagging and parsing the twitterverse.
vol-ume WS-11-05 of AAAI Workshops.
AAAI.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Auto-matically constructing a normalisation dictionary formicroblogs.
In EMNLP-CoNLL, pages 421?432.Nobuhiro Kaji and Masaru Kitsuregawa.
2014.
Accu-rate word segmentation and pos tagging for japanesemicroblogs: Corpus annotation and joint modelingwith lexical normalization.
In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 99?109, Doha,Qatar, October.
Association for Computational Lin-guistics.Catherine Kobus, Franc?ois Yvon, and G?eraldineDamnati.
2008.
Normalizing SMS: are two metaphorsbetter than one?
In COLING, pages 441?448.Chen Li and Yang Liu.
2014.
Improving text nor-malization via unsupervised model and discriminativereranking.
In Proceedings of the ACL 2014 StudentResearch Workshop, pages 86?93, Baltimore, Mary-land, USA, June.
Association for Computational Lin-guistics.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011a.
Insertion, deletion, or substitution?
normal-izing text messages without pre-categorization nor su-pervision.
In ACL, pages 71?76.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011b.
Recognizing named entities in tweets.In NAACL-HLT, pages 359?367, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
A broad-coverage normalization system for social media lan-guage.
In ACL, pages 1035?1044.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC, pages 449?454.Deana Pennell and Yang Liu.
2010.
Normalization oftext messages for text-to-speech.
In ICASSP, pages4842?4845.Deana Pennell and Yang Liu.
2011.
A character-level machine translation approach for normalizationof SMS abbreviations.
In IJCNLP, pages 974?982.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in Tweets: An ex-perimental study.
In EMNLP, pages 1524?1534.Cagil Sonmez and Arzucan Ozgur.
2014.
A graph-basedapproach for contextual text normalization.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages313?324, Doha, Qatar, October.
Association for Com-putational Linguistics.Richard Sproat, Alan W. Black, Stanley F. Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of non-standard words.
Com-puter Speech & Language, 15(3):287?333.Pidong Wang and Hwee Tou Ng.
2013.
A beam-searchdecoder for normalization of social media text withapplication to machine translation.
In NAACL-HLT,pages 471?481, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Yi Yang and Jacob Eisenstein.
2013.
A log-linear modelfor unsupervised text normalization.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 61?72, Seattle, Wash-ington, USA, October.
Association for ComputationalLinguistics.Congle Zhang, Tyler Baldwin, Howard Ho, BennyKimelfeld, and Yunyao Li.
2013.
Adaptive parser-centric text normalization.
In ACL, Sofia, Bulgaria,August.
Association for Computational Linguistics.429
