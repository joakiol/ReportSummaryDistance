Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 10?19,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsGenerating Natural-Language Video DescriptionsUsing Text-Mined KnowledgeNiveda Krishnamoorthy ?UT Austinniveda@cs.utexas.eduGirish Malkarnenkar ?UT Austingirish@cs.utexas.eduRaymond MooneyUT Austinmooney@cs.utexas.eduKate SaenkoUMass Lowellsaenko@cs.uml.eduSergio GuadarramaUC Berkeleysguada@eecs.berkeley.eduAbstractWe present a holistic data-driven techniquethat generates natural-language descriptionsfor videos.
We combine the output of state-of-the-art object and activity detectors with ?real-world?
knowledge to select the most proba-ble subject-verb-object triplet for describing avideo.
We show that this knowledge, automat-ically mined from web-scale text corpora, en-hances the triplet selection algorithm by pro-viding it contextual information and leads toa four-fold increase in activity identification.Unlike previous methods, our approach canannotate arbitrary videos without requiring theexpensive collection and annotation of a sim-ilar training video corpus.
We evaluate ourtechnique against a baseline that does not usetext-mined knowledge and show that humansprefer our descriptions 61% of the time.1 IntroductionCombining natural-language processing (NLP) withcomputer vision to to generate English descriptionsof visual data is an important area of active research(Farhadi et al 2010; Motwani and Mooney, 2012;Yang et al 2011).
We present a novel approach togenerating a simple sentence for describing a shortvideo that:1.
Identifies the most likely subject, verb andobject (SVO) using a combination of visualobject and activity detectors and text-minedknowledge to judge the likelihood of SVOtriplets.
From a natural-language generation?Indicates equal contribution(NLG) perspective, this is the content planningstage.2.
Given the selected SVO triplet, it uses a simpletemplate-based approach to generate candidatesentences which are then ranked using a statis-tical language model trained on web-scale datato obtain the best overall description.
This isthe surface realization stage.Figure 1 shows sample system output.
Our ap-proach can be viewed as a holistic data-driven three-step process where we first detect objects and ac-tivities using state-of-the-art visual recognition al-gorithms.
Next, we combine these often noisy de-tections with an estimate of real-world likelihood,which we obtain by mining SVO triplets from large-scale web corpora.
Finally, these triplets are used togenerate candidate sentences which are then rankedfor plausibility and grammaticality.
The resultingnatural-language descriptions can be usefully em-ployed in applications such as semantic video searchand summarization, and providing video interpreta-tions for the visually impaired.Using vision models alone to predict the best sub-ject and object for a given activity is problematic,especially while dealing with challenging real-worldYouTube videos as shown in Figures 4 and 5, asit requires a large annotated video corpus of simi-lar SVO triplets (Packer et al 2012).
We are in-terested in annotating arbitrary short videos usingoff-the-shelf visual detectors, without the engineer-ing effort required to build domain-specific activitymodels.
Our main contribution is incorporating thepragmatics of various entities?
likelihood of being10Figure 1: Content planning and surface realizationthe subject/object of a given activity, learned fromweb-scale text corpora.
For example, animate ob-jects like people and dogs are more likely to be sub-jects compared to inanimate objects like balls or TVmonitors.
Likewise, certain objects are more likelyto function as subjects/objects of certain activities,e.g., ?riding a horse?
vs. ?riding a house.
?Selecting the best verb may also require recog-nizing activities for which no explicit training datahas been provided.
For example, consider a videowith a man walking his dog.
The object detectorsmight identify the man and dog; however the actiondetectors may only have the more general activity,?move,?
in their training data.
In such cases, real-world pragmatics is very helpful in suggesting that?walk?
is best used to describe a man ?moving?
withhis dog.
We refer to this process as verb expansion.After describing the details of our approach, wepresent experiments evaluating it on a real-worldcorpus of YouTube videos.
Using a variety of meth-ods for judging the output of the system, we demon-strate that it frequently generates useful descriptionsof videos and outperforms a purely vision-based ap-proach that does not utilize text-mined knowledge.2 Background and Related WorkAlthough there has been a lot of interesting workdone in natural language generation (Bangalore andRambow, 2000; Langkilde and Knight, 1998), weuse a simple template for generating our sentencesas we found it to work well for our task.Most prior work on natural-language descrip-tion of visual data has focused on static images(Felzenszwalb et al 2008; Kulkarni et al 2011;Kuznetsova et al 2012; Laptev et al 2008; Li et al2011; Yao et al 2010).
The small amount of exist-ing work on videos (Ding et al 2012; Khan and Go-toh, 2012; Kojima et al 2002; Lee et al 2008; Yaoand Fei-Fei, 2010) uses hand-crafted templates orrule-based systems, works in constrained domains,and does not exploit text mining.
Barbu et al(2012)produce sentential descriptions for short video clipsby using an interesting dynamic programming ap-proach combined with Hidden Markov Models forobtaining verb labels for each video.
However, theydo not use any text mining to improve the quality oftheir visual detections.Our work differs in that we make extensive use oftext-mined knowledge to select the best SVO tripleand generate coherent sentences.
We also evaluateour approach on a generic, large and diverse set ofchallenging YouTube videos that cover a wide rangeof activities.
Motwani and Mooney (2012) explorehow object detection and text mining can aid activityrecognition in videos; however, they do not deter-mine a complete SVO triple for describing a videonor generate a full sentential description.With respect to static image description, Li et al(2011) generate sentences given visual detections ofobjects, visual attributes and spatial relationships;however, they do not consider actions.
Farhadi et al(2010) propose a system that maps images and thecorresponding textual descriptions to a ?meaning?space which consists of an object, action and scenetriplet.
However, they assume a single object perimage and do not use text-mining to determine thelikelihood of objects matching different verbs.
Yanget al(2011) is the most similar to our approach inthat it uses text-mined knowledge to generate sen-tential descriptions of static images after performingobject and scene detection.
However, they do notperform activity recognition nor use text-mining toselect the best verb.3 ApproachOur overall approach is illustrated in Figure 2 andconsists of visual object and activity recognition fol-lowed by content-planning to generate the best SVOtriple and surface realization to generate the finalsentence.11Figure 2: Summary of our approach3.1 DatasetWe used the English portion of the YouTube datacollected by Chen et al(2010), consisting of shortvideos each with multiple natural-language descrip-tions.
This data was previously used by Motwaniand Mooney (2012), and like them, we ensured thatthe test data only contained videos in which we canpotentially detect objects.
We used the object de-tector by Felzenszwalb et al(2008) as it achievesthe state-of-the-art performance on the PASCAL Vi-sual Object Classes (VOC) Challenge.
As such, weselected test videos whose subjects and objects be-long to the 20 VOC object classes - aeroplane, car,horse, sheep, bicycle, cat, sofa, bird, chair, motor-bike, train, boat, cow, person, tv monitor, bottle,dining table, bus, dog, potted plant.
During thisfiltering, we also allow synonyms of these objectnames by including all words with a Lesk similar-ity (as implemented by Pedersen et al(2004)) of atleast 0.5.1 Using this approach, we chose 235 po-tential test videos; the remaining 1,735 videos werereserved for training.All the published activity recognition methodsthat work on datasets such as KTH (Schuldt et al2004), Drinking and Smoking (Laptev and Perez,2007) and UCF50 (Reddy and Shah, 2012) havea very limited recognition vocabulary of activityclasses.
Since we did not have explicit activity la-1Empirically, this method worked better than using WordNetsynsets.Figure 3: Activity clusters discovered by HACbels for our YouTube videos, we followed Motwaniand Mooney (2012)?s approach to automatically dis-cover activity clusters.
We first parsed the train-ing descriptions using Stanford?s dependency parser(De Marneffe et al 2006) to obtain the set of verbsdescribing each video.
We then clustered these verbsusing Hierarchical Agglomerative Clustering (HAC)using the res metric from WordNet::Similarity byPedersen et al(2004) to measure the distance be-tween verbs.
By manually cutting the resulting hi-erarchy at a desired level (ensuring that each clus-ter has at least 9 videos), we discovered the 58 ac-tivity clusters shown in Figure 3.
We then filteredthe training and test sets to ensure that all verbs be-longed to these 58 activity clusters.
The final datacontains 185 test and 1,596 training videos.3.2 Object DetectionWe used Felzenszwalb et al(2008)?sdiscriminatively-trained deformable parts mod-els to detect the most likely objects in each video.Since these object detectors were designed forstatic images, each video was split into frames atone-second intervals.
For each frame, we ran theobject detectors and selected the maximum scoreassigned to each object in any of the frames.
Weconverted the detection scores, f(x), to estimatedprobabilities p(x) using a sigmoid p(x) = 11+e?f(x).3.3 Activity RecognitionIn order to get an initial probability distribution foractivities detected in the videos, we used the motiondescriptors developed by Laptev et al(2008).
Theirapproach extracts spatio-temporal interest points(STIPs) from which it computes HoG (Histograms12Corpora Size of textBritish National Corpus (BNC) 1.5GBWaCkypedia EN 2.6GBukWaC 5.5GBGigaword 26GBGoogleNgrams 1012 wordsTable 1: Corpora used to Mine SVO Tripletsof Oriented Gradients) and HoF (Histograms of Op-tical Flow) features over a 3-dimensional space-timevolume.
These descriptors are then randomly sam-pled and clustered to obtain a ?bag of visual words,?and each video is then represented as a histogramover these clusters.
We experimented with differentclassifiers such as LIBSVM (Chang and Lin, 2011)to train a final activity detector using these features.Since we achieved the best classification accuracy(still only 8.65%) using an SVM with the intersec-tion kernel, we used this approach to obtain a prob-ability distribution over the 58 activity clusters foreach test video.
We later experimented with DenseTrajectories (Wang et al 2011) for activity recogni-tion but there was only a minor improvement.3.4 Text MiningWe improve these initial probability distributionsover objects and activities by incorporating the like-lihood of different activities occuring with particularsubjects and objects using two different approaches.In the first approach, using the Stanford dependencyparser, we parsed 4 different text corpora covering awide variety of text: English Gigaword, British Na-tional Corpus (BNC), ukWac and WaCkypedia EN.In order to obtain useful estimates, it is essential tocollect text that approximates all of the written lan-guage in scale and distribution.
The sizes of thesecorpora (after preprocessing) are shown in Table 1.Using the dependency parses for these corpora,we mined SVO triplets.
Specifically, we looked forsubject-verb relationships using nsubj dependenciesand verb-object relationships using dobj and prepdependencies.
The prep dependency ensures thatwe account for intransitive verbs with prepositionalobjects.
Synonyms of subjects and objects and con-jugations of verbs were reduced to their base forms(20 object classes, 58 activity clusters) while form-ing triplets.
If a subject, verb or object not belongingto these base forms is encountered, it is ignored dur-ing triplet construction.These triplets are then used to train a backoff lan-guage model with Kneser-Ney smoothing (Chen andGoodman, 1999) for estimating the likelihood of anSVO triple.
In this model, if we have not seen train-ing data for a particular SVO trigram, we ?back-off?to the Subject-Verb and Verb-Object bigrams to co-herently estimate its probability.
This results in asophisticated statistical model for estimating tripletprobabilities using the syntactic context in whichthe words have previously occurred.
This allows usto effectively determine the real-world plausibilityof any SVO using knowledge automatically minedfrom raw text.
We call this the ?SVO LanguageModel?
approach (SVO LM).In a second approach to estimating SVO prob-abilities, we used BerkeleyLM (Pauls and Klein,2011) to train an n-gram language model on theGoogleNgram corpus (Lin et al 2012).
This sim-ple model does not consider synonyms, verb con-jugations, or SVO dependencies but only looks atword sequences.
Given an SVO triplet as an in-put sequence, it estimates its probability based onn-grams.
We refer to this as the ?Language Model?approach (LM).3.5 Verb ExpansionAs mentioned earlier, the top activity detections areexpanded with their most similar verbs in order togenerate a larger set of potential words for describ-ing the action.
We used the WUP metric from Word-Net::Similarity to expand each activity cluster to in-clude all verbs with a similarity of at least 0.5.
Forexample, we expand the verb ?move?
with go 1.0,walk 0.8, pass 0.8, follow 0.8, fly 0.8, fall 0.8, come0.8, ride 0.8, run 0.67, chase 0.67, approach 0.67,where the number is the WUP similarity.3.6 Content PlanningTo combine the vision detection and NLP scores anddetermine the best overall SVO, we use simple lin-ear interpolation as shown in Equation 1.
Whencomputing the overall vision score, we make a con-ditional independence assumption and multiply theprobabilities of the subject, activity and object.
Toaccount for expanded verbs, we additionally mul-tiply by the WUP similarity between the original13(Vorig) and expanded (Vsim) verbs.
The NLP scoreis obtained from either the ?SVO Language Model?or the ?Language Model?
approach, as previouslydescribed.score = w1 ?
vis score + w2 ?
nlp score (1)(2)vis score = P (S|vid) ?
P (Vorig|vid)?
Sim(Vsim, Vorig) ?
P (O|vid)After determining the top n=5 object detectionsand top k=10 verb detections for each video, wegenerate all possible SVO triplets from these nounsand verbs, including all potential verb expansions.Each resulting SVO is then scored using Equation 1,and the best is selected.
We compare this approachto a ?pure vision?
baseline where the subject is thehighest scored object detection (which empiricallyis more likely to be the subject than the object), theobject is the second highest scored object detection,and the verb is the activity cluster with the highestdetection probability.3.7 Surface RealizationFinally, the subject, verb and object from the top-scoring SVO are used to produce a set of candi-date sentences, which are then ranked using a lan-guage model.
The text corpora in Table 1 are minedagain to get the top three prepositions for every verb-object pair.
We use a template-based approach inwhich each sentence is of the form:?Determiner (A,The) - Subject - Verb (Present,Present Continuous) - Preposition (optional) - De-terminer (A,The) - Object.
?Using this template, a set of candidate sentences aregenerated and ranked using the BerkeleyLM lan-guage model trained on the GoogleNgram corpus.The top sentence is then used to describe the video.This surface realization technique is used for boththe vision baseline triplet and our proposed triplet.In addition to the one presented here, we tried al-ternative ?pure vision?
baselines, but they are notincluded since they performed worse.
We tried anon-parametric approach similar to Ordonez et al(2011), which computes global similarity of thequery to a large captioned dataset and returns thenearest neighbor?s description.
To compute the sim-ilarity we used an RBF-Chi2 kernel over bag-of-words STIP features.
However, as noted by Ordonezet al(2011), who used 1 million Flickr images, ourdataset is likely not large enough to produce goodmatches.
In an attempt to combine information fromboth object and activity recognition, we also triedcombining object detections from 20 PASCAL ob-ject detectors (Felzenszwalb et al 2008) and fromObject Bank (Li et al 2010) using a multi-channelapproach as proposed in (Zhang et al 2007), with aRBF-Chi2 kernel for the STIP features and a RBF-Correlation Distance kernel for object detections.4 Experimental Results4.1 Content PlanningWe first evaluated the ability of the system to iden-tify the best SVO content.
From the ?
50 humandescriptions available for each video, we identifiedthe SVO for each description and then determinedthe ground-truth SVO for each of the 185 test videosusing majority vote.
These verbs were then mappedback to their 58 activity clusters.
For the results pre-sented in Tables 2 and 3, we assigned the visionscore a weight of 0 (w1 = 0) and the NLP scorea weight of 1 (w2 = 1) since these weights gave usthe best performance for thresholds of 5 and 10 forthe objects and activity detections respectively.
Notethat while the vision score is given a weight of zero,the vision detections still play a vital role in the de-termination of the final triplet since our model onlyconsiders the objects and activities with the highestvision detection scores.To evaluate the accuracy of SVO identification,we used two metrics.
The first is a binary metric thatrequires exactly matching the gold-standard subject,verb and object.
We also evaluate the overall tripletaccuracy.
Note that the verb accuracy in the visionbaseline is not word-based and is measured on the58 activity classes.
Its results are shown in Table 2,where VE and NVE stand for ?verb expansion?and ?no verb expansion?
respectively.
However,the binary evaluation can be unduly harsh.
If weincorrectly choose ?bicycle?
instead of a ?motor-bike?
as the object, it should be considered betterthan choosing ?dog.?
Similarly, predicting ?chop?instead of ?slice?
is better than choosing ?go?.14Method Subject% Verb% Object% All%Vision Baseline 71.35 8.65 29.19 1.62LM(VE) 71.35 8.11 10.81 0.00SVO LM(NVE) 85.95 16.22 24.32 11.35SVO LM(VE) 85.95 36.76 33.51 23.78Table 2: SVO Triplet accuracy: Binary metricMethod Subject% Verb% Object% All%Vision Baseline 87.76 40.20 61.18 63.05LM(VE) 85.77 53.32 61.54 66.88SVO LM(NVE) 94.90 63.54 69.39 75.94SVO LM(VE) 94.90 66.36 72.74 78.00Table 3: SVO Triplet accuracy: WUP metricIn order to account for such similarities, we alsomeasure the WUP similarity between the predictedand correct items.
For the examples above, the rel-evant scores are: wup(motorbike,bicycle)=0.7826,wup(motorbike,dog)=0.1, wup(slice,chop)=0.8,wup(slice,go)=0.2857.
The results for the WUPmetric are shown in Table 3.4.2 Surface RealizationFigures 4 and 5 show examples of good and bad sen-tences generated by our method compared to the vi-sion baseline.4.2.1 Automatic MetricsTo automatically compare the sentences gener-ated for the test videos to ground-truth human de-scriptions, we employed the BLEU and METEORmetrics used to evaluate machine-translation output.METEOR was designed to fix some of the prob-lems with the more popular BLEU metric.
Theyboth measure the number of matching n-grams (forvarious values of n) between the automatic and hu-man generated sentences.
METEOR takes stem-ming and synonymy into consideration.
We usedthe SVO Language Model (with verb expansion) ap-proach since it gave us the best results for triplets.The results are given in Table 4.4.2.2 Human Evaluation using MechanicalTurkGiven the limitations of metrics like BLEU andMETEOR, we also asked human judges to evalu-ate the quality of the sentences generated by our ap-Figure 4: Examples where we outperform the baselineFigure 5: Examples where we underperform the baselineproach compared to those generated by the baselinesystem.
For each of the 185 test videos, we asked 9unique workers (with >95% HIT approval rate andwho had worked on more than 1000 HITs) on Ama-zon Mechanical Turk to pick which sentence betterdescribed the video.
We also gave them a ?noneof the above two sentences?
option in case neitherof the sentences were relevant to the video.
Qual-ity was controlled by also including in each HIT agold-standard example generated from the humandescriptions, and discarding judgements of workerswho incorrectly answered this gold-standard item.Overall, when they expressed a preference, hu-mans picked our descriptions to that of the baselineMethod BLEU score METEOR scoreVision Baseline 0.37?0.05 0.25?0.08SVO LM(VE) 0.45?0.05 0.36?0.27Table 4: Automatic evaluation of sentence quality1561.04% of the time.
Out of the 84 videos where themajority of judges had a clear preference, they choseour descriptions 65.48% of the time.5 DiscussionOverall, the results consistently show the advantageof utilizing text-mined knowledge to improve the se-lection of an SVO that best describes a video.
Belowwe discuss various specific aspects of the results.Vision Baseline: For the vision baseline, the sub-ject accuracy is quite high compared to the objectand activity accuracies.
This is likely because theperson detector has higher recall and confidencethan the other object detectors.
Since most testvideos have a person as the subject, this works infavor of the vision baseline, as typically the top ob-ject detection is ?person?.
Activity (verb) accuracyis quite low (8.65% binary accuracy).
This is be-cause there are 58 activity clusters, some with verylittle training data.
Object accuracy is not as highas subject accuracy because the true object, whileusually present in the top object detections, is notalways the second-highest object detection.
By al-lowing ?partial credit?, the WUP metric increasesthe verb and object accuracies to 40.2% and 61.18%,respectively.Language Model(VE): The Language Model ap-proach performs even worse than the vision baselineespecially for object identification.
This is becausewe consider the language model score directly forthe SVO triplet without any verb conjugations andpresence of determiners between the verb and ob-ject.
For example, while the GoogleNgram corpusis likely to contain many instances of a sentence like?A person is walking with a dog?, it will probablynot contain many instances of ?person walk dog?,resulting in lower scores.SVO Language Model(NVE): The SVO Lan-guage Model (without verb expansion) improvesverb accuracy from 8.65% to 16.22%.
For the WUPmetric, we see an improvement in accuracy in allcases.
This indicates that we are getting semanti-cally closer to the right object compared to the ob-ject predicted by the vision baseline.SVO Language Model(VE): When used withverb expansion, the SVO Language Model approachresults in a dramatic improvement in verb accu-racy, causing it to jump to 36.76%.
The WUPscore increase for verbs between SVO LanguageModel(VE) and SVO Language Model(NVE) is mi-nor, probably because even without verb expansion,semantically similar verbs are selected but not theone used in most human descriptions.
So, the jumpin verb accuracy for the binary metric is much morethan the one for WUP.Importance of verb expansion: Verb expansionclearly improves activity accuracy.
This idea couldbe extended to a scenario where the test set containsmany activities for which we do not have any ex-plicit training data.
As such, we cannot train activ-ity classifiers for these ?missing?
classes.
However,we can train a ?coarse?
activity classifier using thetraining data that is available, get the top predictionsfrom this coarse classifier and then refine them byusing verb expansion.
Thus, we can even detect anddescribe activities that were unseen at training timeby using text-mined knowledge to determine the de-scription of an activity that best fits the detected ob-jects.Effect of different training corpora: As men-tioned earlier, we used a variety of textual cor-pora.
Since they cover newswire articles, web pages,Wikipedia pages and neutral content, we comparedtheir individual effect on the accuracy of triplet se-lection.
The results of this ablation study are shownin Tables 5 and 6 for the binary and WUP met-ric respectively.
We also show results for trainingthe SVO model on the descriptions of the trainingvideos.
The WaCkypedia EN corpus gives us thebest overall results, probably because it covers awide variety of topics, unlike Gigaword which is re-stricted to the news domain.
Also, using our SVOLanguage Model approach on the triplets from thedescriptions of the training videos is not sufficient.This is because of the relatively small size and nar-row domain of the training descriptions in compari-son to the other textual corpora.Effect of changing the weight of the NLP scoreWe experimented with different weights for the Vi-sion and NLP scores (in Equation 1).
These resultscan be seen in Figure 6 for the binary-metric evalu-ation.
The WUP-metric evaluation graph is qualita-tively similar.
A general trend seems to be that thesubject and activity accuracies increase with increas-ing weights of the NLP score.
There is a significant16Method Subject% Verb% Object% All%Vision Baseline 71.35 8.65 29.19 1.62Train Desc.
85.95 16.22 16.22 8.65Gigaword 85.95 32.43 20.00 14.05BNC 85.95 17.30 29.73 14.59ukWaC 85.95 34.05 32.97 22.16WaCkypedia EN 85.95 35.14 40.00 28.11All 85.95 36.76 33.51 23.78Table 5: Effect of training corpus on SVO binary accu-racyMethod Subject% Verb% Object% All%Vision Baseline 87.76 40.20 61.18 63.05Train Desc.
94.95 45.12 61.43 67.17Gigaword 94.90 63.99 65.71 74.87BNC 94.88 51.48 73.93 73.43ukWaC 94.86 60.59 72.83 76.09WaCkypedia EN 94.90 62.52 76.48 77.97All 94.90 66.36 72.74 78.00Table 6: Effect of training corpus on SVO WUP accuracyimprovement in verb accuracy as the NLP weight isincreased towards 1.
However, for objects we noticea slight increase in accuracy until the weight for theNLP component is 0.9 after which there is a slightdip.
We hypothesize that this dip is caused by theloss of vision-based information about the objectswhich provide some guidance for the NLP system.BLEU and METEOR results: From the resultsin Table 4, it is clear that the sentences generatedby our approach outperform those generated by thevision baseline, using both the BLEU and METEORevaluation metrics.MTurk results: The Mechanical Turk resultsshow that human judges generally prefer our sys-tem?s sentences to those of the vision baseline.
Aspreviously seen, our method improves verbs farmore than it improves subjects or objects.
We hy-pothesize that the reason we do not achieve a simi-larly large jump in performance in the MTurk evalu-ation is because people seem to be more influencedby the object than the verb when both options arepartially irrelevant.
For example, in a video of a per-son riding his bike onto the top of a car, our pro-posed sentence was ?A person is a riding a motor-bike?
while the vision sentence was ?A person playsFigure 6: Effect of increasing NLP weights (Binary met-ric)a car?, and most workers selected the vision sen-tence.Drawback of Using YouTube Videos: YouTubevideos often depict unusual and ?interesting?
events,and these might not agree with the statistics on typ-ical SVOs mined from text corpora.
For instance,the last video in Figure 5 shows a person dragging acat on the floor.
Since sentences describing peoplemoving or dragging cats around are not common intext corpora, our system actually down-weights thecorrect interpretation.6 ConclusionThis paper has introduced a holistic data-drivenapproach for generating natural-language descrip-tions of short videos by identifying the best subject-verb-object triplet for describing realistic YouTubevideos.
By exploiting knowledge mined from largecorpora to determine the likelihood of various SVOcombinations, we improve the ability to select thebest triplet for describing a video and generate de-scriptive sentences that are prefered by both au-tomatic and human evaluation.
From our experi-ments, we see that linguistic knowledge significantlyimproves activity detection, especially when train-ing and test distributions are very different, one ofthe advantages of our approach.
Generating morecomplex sentences with adjectives, adverbs, andmultiple objects and multi-sentential descriptions oflonger videos with multiple activities are areas forfuture research.177 AcknowledgementsThis work was funded by NSF grant IIS1016312and DARPA Minds Eye grant W911NF-10-2-0059.Some of our experiments were run on the MastodonCluster (NSF Grant EIA-0303609).ReferencesBangalore, S. and Rambow, O.
(2000), Exploitinga probabilistic hierarchical model for generation,in ?Proceedings of the 18th conference on Com-putational linguistics-Volume 1?, Association forComputational Linguistics, pp.
42?48.Barbu, A., Bridge, A., Burchill, Z., Coroian, D.,Dickinson, S., Fidler, S., Michaux, A., Mussman,S., Narayanaswamy, S., Salvi, D. et al(2012),Video in sentences out, in ?Proceedings of the28th Conference on Uncertainty in Artificial In-telligence (UAI)?, pp.
102?12.Chang, C. and Lin, C. (2011), ?LIBSVM: a li-brary for support vector machines?, ACM Trans-actions on Intelligent Systems and Technology(TIST) 2(3), 27.Chen, D., Dolan, W., Raghavan, S., Huynh, T.,Mooney, R., Blythe, J., Hobbs, J., Domingos, P.,Kate, R., Garrette, D. et al(2010), ?Collectinghighly parallel data for paraphrase evaluation?,Journal of Artificial Intelligence Research (JAIR)37, 397?435.Chen, S. and Goodman, J.
(1999), ?An empiricalstudy of smoothing techniques for language mod-eling?, Computer Speech & Language 13(4), 359?393.De Marneffe, M., MacCartney, B. and Manning, C.(2006), Generating typed dependency parses fromphrase structure parses, in ?Proceedings of the In-ternational Conference on Language Resourcesand Evaluation (LREC)?, Vol.
6, pp.
449?454.Ding, D., Metze, F., Rawat, S., Schulam, P., Burger,S., Younessian, E., Bao, L., Christel, M. andHauptmann, A.
(2012), Beyond audio and videoretrieval: towards multimedia summarization, in?Proceedings of the 2nd ACM International Con-ference on Multimedia Retrieval?.Farhadi, A., Hejrati, M., Sadeghi, M., Young, P.,Rashtchian, C., Hockenmaier, J. and Forsyth, D.(2010), ?Every picture tells a story: Generatingsentences from images?, Computer Vision?European Conference on Computer Vision(ECCV) pp.
15?29.Felzenszwalb, P., McAllester, D. and Ramanan,D.
(2008), A discriminatively trained, multi-scale, deformable part model, in ?IEEE Confer-ence on Computer Vision and Pattern Recognition(CVPR)?, pp.
1?8.Khan, M. and Gotoh, Y.
(2012), ?Describing videocontents in natural language?, European Chapterof the Association for Computational Linguistics(EACL) .Kojima, A., Tamura, T. and Fukunaga, K. (2002),?Natural language description of human activitiesfrom video images based on concept hierarchy ofactions?, International Journal of Computer Vi-sion (IJCV) 50(2), 171?184.Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi,Y., Berg, A. and Berg, T. (2011), Baby talk:Understanding and generating simple image de-scriptions, in ?IEEE Conference on Computer Vi-sion and Pattern Recognition (CVPR)?, pp.
1601?1608.Kuznetsova, P., Ordonez, V., Berg, A. C., Berg,T.
L. and Choi, Y.
(2012), Collective genera-tion of natural image descriptions, in ?Proceed-ings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics: Long Papers-Volume 1?, Association for Computational Lin-guistics, pp.
359?368.Langkilde, I. and Knight, K. (1998), Generation thatexploits corpus-based statistical knowledge, in?Proceedings of the 17th international conferenceon Computational linguistics-Volume 1?, Associ-ation for Computational Linguistics, pp.
704?710.Laptev, I., Marszalek, M., Schmid, C. and Rozen-feld, B.
(2008), Learning realistic human actionsfrom movies, in ?IEEE Conference on ComputerVision and Pattern Recognition (CVPR)?, pp.
1?8.Laptev, I. and Perez, P. (2007), Retrieving actions inmovies, in ?Proceedings of the 11th IEEE Interna-tional Conference on Computer Vision (ICCV)?,pp.
1?8.Lee, M., Hakeem, A., Haering, N. and Zhu, S.18(2008), Save: A framework for semantic anno-tation of visual events, in ?IEEE Computer Visionand Pattern Recognition Workshops (CVPR-W)?,pp.
1?8.Li, L., Su, H., Xing, E. and Fei-Fei, L. (2010), ?Ob-ject bank: A high-level image representation forscene classification and semantic feature sparsifi-cation?, Advances in Neural Information Process-ing Systems (NIPS) 24.Li, S., Kulkarni, G., Berg, T., Berg, A. and Choi,Y.
(2011), Composing simple image descriptionsusing web-scale n-grams, in ?Proceedings of theFifteenth Conference on Computational NaturalLanguage Learning (CoNLL)?, Association forComputational Linguistics (ACL), pp.
220?228.Lin, Y., Michel, J., Aiden, E., Orwant, J., Brockman,W.
and Petrov, S. (2012), Syntactic annotationsfor the google books ngram corpus, in ?Proceed-ings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (ACL)?.Motwani, T. and Mooney, R. (2012), Improvingvideo activity recognition using object recogni-tion and text mining, in ?European Conference onArtificial Intelligence (ECAI)?.Ordonez, V., Kulkarni, G. and Berg, T. (2011),Im2text: Describing images using 1 millioncaptioned photographs, in ?Proceedings of Ad-vances in Neural Information Processing Systems(NIPS)?.Packer, B., Saenko, K. and Koller, D. (2012), Acombined pose, object, and feature model for ac-tion understanding, in ?IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR)?,pp.
1378?1385.Pauls, A. and Klein, D. (2011), Faster and smallern-gram language models, in ?Proceedings of the49th annual meeting of the Association for Com-putational Linguistics: Human Language Tech-nologies?, Vol.
1, pp.
258?267.Pedersen, T., Patwardhan, S. and Michelizzi, J.
(2004), Wordnet:: Similarity: measuring the re-latedness of concepts, in ?Demonstration Papersat Human Language Technologies-NAACL?, As-sociation for Computational Linguistics, pp.
38?41.Reddy, K. and Shah, M. (2012), ?Recognizing 50human action categories of web videos?, MachineVision and Applications pp.
1?11.Schuldt, C., Laptev, I. and Caputo, B.
(2004), Rec-ognizing human actions: A local SVM approach,in ?Proceedings of the 17th International Con-ference on Pattern Recognition (ICPR)?, Vol.
3,pp.
32?36.Wang, H., Klaser, A., Schmid, C. and Liu, C.-L.(2011), Action recognition by dense trajectories,in ?IEEE Conference on Computer Vision andPattern Recognition (CVPR)?, pp.
3169?3176.Yang, Y., Teo, C. L., Daume?, III, H. and Aloimonos,Y.
(2011), Corpus-guided sentence generation ofnatural images, in ?Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP)?, Association for Computa-tional Linguistics, pp.
444?454.Yao, B. and Fei-Fei, L. (2010), Modeling mutualcontext of object and human pose in human-object interaction activities, in ?IEEE Confer-ence on Computer Vision and Pattern Recognition(CVPR)?.Yao, B., Yang, X., Lin, L., Lee, M. and Zhu, S.(2010), ?I2t: Image parsing to text description?,Proceedings of the IEEE 98(8), 1485?1508.Zhang, J., Marsza?ek, M., Lazebnik, S. and Schmid,C.
(2007), ?Local features and kernels for classi-fication of texture and object categories: A com-prehensive study?, International Journal of Com-puter Vision (IJCV) 73(2), 213?238.19
