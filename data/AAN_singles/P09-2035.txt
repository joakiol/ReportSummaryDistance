Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137?140,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPSub-Sentence Division for Tree-Based Machine TranslationHao Xiong*, Wenwen Xu+, Haitao Mi*, Yang Liu* and Qun Liu**Key Lab.
of Intelligent Information Processing+Key Lab.
of Computer System and ArchitectureInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{xionghao,xuwenwen,htmi,yliu,liuqun}@ict.ac.cnAbstractTree-based statistical machine translationmodels have made significant progress in re-cent years, especially when replacing 1-besttrees with packed forests.
However, as theparsing accuracy usually goes down dramati-cally with the increase of sentence length,translating long sentences often takes longtime and only produces degenerate transla-tions.
We propose a new method named sub-sentence division that reduces the decodingtime and improves the translation quality fortree-based translation.
Our approach divideslong sentences into several sub-sentences byexploiting tree structures.
Large-scale ex-periments on the NIST 2008 Chinese-to-English test set show that our approachachieves an absolute improvement of 1.1BLEU points over the baseline system in50% less time.1 IntroductionTree-based statistical machine translationmodels in days have witness promising progressin recent years, such as tree-to-string models (Liuet al, 2006; Huang et al, 2006), tree-to-treemodels (Quirk et al,2005;Zhang et al, 2008).Especially, when incorporated with forest, thecorrespondent forest-based tree-to-string models(Mi et al, 2008; Zhang et al, 2009), tree-to-treemodels (Liu et al, 2009) have achieved a prom-ising improvements over correspondent tree-based systems.
However, when we translate longsentences, we argue that two major issues will beraised.
On one hand, parsing accuracy will belower as the length of sentence grows.
It will in-evitably hurt the translation quality (Quirk andCorston-Oliver, 2006; Mi and Huang, 2008).
Onthe other hand, decoding on long sentences willbe time consuming, especially for forest ap-proaches.
So splitting long sentences into sub-Figure 1.
Main framework of our methodsentences becomes a natural way in MT litera-ture.A simple way is to split long sentences bypunctuations.
However, without concerningabout the original whole tree structures, this ap-proach will result in ill-formed sub-trees whichdon?t respect to original structures.
In this paper,we present a new approach, which pays moreattention to parse trees on the long sentences.
Wefirstly parse the long sentences into trees, andthen divide them accordingly into sub-sentences,which will be translated independently (Section3).
Finally, we combine sub translations into afull translation (Section 4).
Large-scale experi-ments (Section 5) show that the BLEU scoreachieved by our approach is 1.1 higher than di-rect decoding and 0.3 higher than always split-ting on commas on the 2008 NIST MT Chinese-English test set.
Moreover, our approach has re-duced decoding time significantly.2 FrameworkOur approach works in following steps.
(1) Split a long sentence into sub-sentences.
(2) Translate all the sub-sentences respectively.
(3) Combine the sub-translations.Figure 1 illustrates the main idea of our ap-proach.
The crucial issues of our method are howto divide long sentences and how to combine thesub-translations.3 Sub Sentence DivisionLong sentences could be very complicated ingrammar and sentence structure, thereby creatingan obstacle for translation.
Consequently, weneed to break them into shorter and easierclauses.
To divide sentences by punctuation is137Figure 2.
An undividable parse treeFigure 3.
A dividable parse treeone of the most commonly used methods.
How-ever, simply applying this method might damagethe accuracy of parsing.
As a result, the strategywe proposed is to operate division while con-cerning the structure of parse tree.As sentence division should not influence theaccuracy of parsing, we have to be very cautiousabout sentences whose division might decreasethe accuracy of parsing.
Figure 2(a) shows anexample of the parse tree of an undividable sen-tence.As can be seen in Figure 2, when we dividethe sentence by comma, it would break the struc-ture of ?VP?
sub-tree and result in a ill-formedsub-tree ?VP?
(right sub-tree), which don?t havea subject and don?t respect to original tree struc-tures.Consequently, the key issue of sentence divi-sion is finding the sentences that can be dividedwithout loosing parsing accuracy.
Figure 2(b)shows the parse tree of a sentence that can bedivided by punctuation, as sub-sentences dividedby comma are independent.
The reference trans-lation of the sentence in figure 3 isLess than two hours earlier, a Palestinian tookon a shooting spree on passengers in the town ofKfar Saba in northern Israel.Pseudocode 1 Check Sub Sentence Divi-sion Algorithm1: procedure CheckSubSentence(sent)2: for each word i in sent3:    if(i is a comma)4:       left={words in left side of i};//words between last comma and cur-rent comma i5:       right={words in right side of i};//words between i and next comma orsemicolon, period, question mark6:       isDividePunct[i]=true;7:       for each j in left8:          if(( LCA(j, i)!=parent[i])9:             isDividePunct[i]=false;10:           break;11:     for each j in right12:        if(( LCA(j, i)!=parent[i])13:           isDividePunct[i]=false;14:           break;15: function LCA(i, j)16:    return lowest common ancestor(i, j);It demonstrates that this long sentence can bedivided into two sub-sentences, providing a goodsupport to our division.In addition to dividable sentences and non-dividable sentences, there are sentences contain-ing more than one comma, some of which aredividable and some are not.
However, this doesnot prove to be a problem, as we process eachcomma independently.
In other words, we onlysplit the dividable part of this kind of sentences,leaving the non-dividable part unchanged.To find the sentences that can be divided, wepresent a new method and provide its pseudocode.
Firstly, we divide a sentence by its commas.For each word in the sub-sentence on the leftside of a comma, we compute its lowest commonancestor (LCA) with the comma.
And we processthe words in the sub-sentence on the right side ofthe comma in the same way.
Finally, we check ifall the LCA we have computed are comma?s par-ent node.
If all the LCA are the comma?s parentnode, the sub-sentences are independent.As shown in figure 3, the LCA (AD ??
,PU ?
),  is ?IP?
,which is the parent node of?PU ??
; and the LCA (NR ???
, PU ?)
isalso ?IP?.
Till we have checked all the LCA ofeach word and comma, we finally find that allthe LCA are ?IP?.
As a result, this sentence canbe divided without loosing parsing accuracy.LCA can be computed by using union-set (Tar-jan, 1971) in lineal time.
Concerning the138sub-sentence 1: ???
?Translation 1: Johndroe said                   A1Translation 2: Johndroe pointed out       A2Translation 3: Qiang Zhuo said              A3comma 1: ,Translation: punctuation translation (whitespace, that ?
)sub-sentence 2: ?????????????????????????
?Translation 1: the two presidents also wel-comed the US-South Korea free tradeagreement that was signed yesterday       B1Translation 2: the two presidents also ex-pressed welcome to the US ?
South Koreafree trade agreement signed yesterday     B2comma 2: ,Translation: punctuation translation (whitespace, that ?
)sub-sentence 3:????????????????
?Translation 1: and would work to ensurethat the congresses of both countries ap-prove this agreement.
C1Translation 2: and will make efforts to en-sure the Congress to approve this agreementof the two countries.
C2Table 1.
Sub translation exampleimplementation complexity, we have reduced theproblem to range minimum query problem(Bender et al, 2005) with a time complexity of(1)?
for querying.Above all, our approach for sub sentenceworks as follows:(1)Split a sentence by semi-colon if there isone.
(2)Parse a sentence if it contains a comma,generating k-best parses (Huang Chiang, 2005)with k=10.
(3)Use the algorithm in pseudocode 1 tocheck the sentence and divide it if there aremore than 5 parse trees indicates that the sen-tence is dividable.4 Sub Translation CombiningFor sub translation combining, we mainly use thebest-first expansion idea from cube pruning(Huang and Chiang, 2007) to combine sub-translations and generate the whole k-best trans-lations.
We first select the best translation fromsub translation sets, and then use an interpolationTest Set 02 05 08No Sent Division 34.56 31.26 24.53Split by Comma 34.59 31.23 25.39Our Approach 34.86 31.23 25.69Table 2.
BLEU results (case sensitive)Test Set 02 05 08No Sent Division 28 h 36 h 52 hSplit by Comma 18h 23h 29hOur Approach 18 h 22 h 26 hTable 3.
Decoding time of our experiments(h means hours)language model for rescoring (Huang and Chiang,2007).For example, we split the following sentence ?????,??????????????????????????,??????????????????
into three sub-sentences and generatesome translations, and the results are displayed inTable 1.As seen in Table 1, for each sub-sentence,there are one or more versions of translation.
Forconvenience, we label the three translation ver-sions of sub-sentence 1 as A1, A2, and A3, re-spectively.
Similarly, B1, B2, C1, C2 are alsolabels of translation.
We push the A1, whitespace, B1, white space, C1 into the cube, andthen generate the final translation.According to cube pruning algorithm, we willgenerate other translations until we get the bestlist we need.
Finally, we rescore the k-best listusing interpolation language model and find thebest translation which is A1 that B1 white spaceC1.5 Experiments5.1 Data preparationWe conduct our experiments on Chinese-Englishtranslation, and use the Chinese parser of Xionget al (2005) to parse the source sentences.
Andour decoder is based on forest-based tree-to-string translation model (Mi et al 2008).Our training corpus consists of 2.56 millionsentence pairs.
Forest-based rule extractor (Miand Huang 2008) is used with a pruning thresh-old p=3.
And we use SRI Language ModelingToolkit (Stolcke, 2002) to train two 5-gram lan-guage models with Kneser-Ney smoothing on theEnglish side of the training corpus and the Xin-hua portion of Gigaword corpora respectively.139We use 2006 NIST MT Evaluation test set asdevelopment set, and 2002, 2005 and 2008 NISTMT Evaluation test sets as test sets.
We also useminimum error-rate training (Och, 2003) to tuneour feature weights.
We evaluate our results withcase-sensitive BLEU-4 metric (Papineni et al,2002).
The pruning threshold p for parse forest indecoding time is 12.5.2 ResultsThe final BLEU results are shown in Table 2, ourapproach has achieved a BLEU score that is 1.1higher than direct decoding and 0.3 higher thanalways splitting on commas.The decoding time results are presented in Ta-ble 3.
The search space of our experiment is ex-tremely large due to the large pruning threshold(p=12), thus resulting in a long decoding time.However, our approach has reduced the decodingtime by 50% over direct decoding, and 10% overalways splitting on commas.6 Conclusion & Future WorkWe have presented a new sub-sentence divisionmethod and achieved some good results.
In thefuture, we will extend our work from decoding totraining time, where we divide the bilingual sen-tences accordingly.AcknowledgementThe authors were supported by National NaturalScience Foundation of China, Contracts 0873167and 60736014, and 863 State Key ProjectNo.2006AA010108.
We thank Liang Huang forhis insightful suggestions.ReferencesBender, Farach-Colton, Pemmasani, Skiena, Sumazin,Lowest common ancestors in trees and di-rected acyclic graphs.
J. Algorithms 57(2), 75?94 (2005)Liang Huang and David Chiang.
2005.
Better kbestParsing.
In Proceedings of IWPT-2005.Liang Huang and David Chiang.
2007.
Forest res-coring: Fast decoding with integrated lan-guage models.
In Proceedings of ACL.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with ex-tended domain of locality.
In Proceedings ofAMTAPhilipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL 2003, pages 127-133.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String alignments template for statistical ma-chine translation.
In Proceedings of ACL.Yang Liu, Yajuan Lv and Qun Liu.2009.
ImprovingTree-to-Tree Translation with Packed Forests.Toappear in Proceedings of ACL/IJCNLP..Daniel Marcu, Wei Wang, AbdessamadEchihabi, andKevin Knight.
2006.
Statistical Machine Trans-lation with syntactifiedtarget languagephrases.
In Proceedings of EMNLP.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT.Haitao Mi and Liang Huang.
2008.
Forest-basedtranslation rule extraction.
In Proceedings ofEMNLP.Franz J. Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceed-ings of ACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
InProceedings of ACL, pages 311?318,.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntacticallyinformed phrasal SMT.
In Proceedings of ACL.Chris Quirk and Simon Corston-Oliver.
2006.
Theimpact of parse quality on syntactically-informed statistical machine translation.
InProceedings of EMNLP.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings ofICSLP, volume 30, pages 901?904.Georgianna Tarjan, Depth First Search and LinearGraph Algorithms.
SIAM J. Comp.
1:2, pp.
146?160, 1972.Deyi Xiong, Shuanglong Li, Qun Liu, and ShouxunLin.2005.
Parsing the Penn Chinese Treebankwith semantic knowledge.
In Proceedings ofIJCNLP.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree transla-tion model.
In Proceedings of ACL.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw andChew Lim Tan.
2009.
Forest-based Tree Sequenceto String Translation Model.
To appear in Proceed-ings of ACL/IJCNLP140
