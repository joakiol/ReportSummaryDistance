Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 160?167,New York, June 2006. c?2006 Association for Computational LinguisticsMultilingual Dependency Parsing using Bayes Point MachinesSimon Corston-OliverMicrosoft ResearchOne Microsoft WayRedmond, WA 98052simonco@microsoft.comAnthony AueMicrosoft ResearchOne Microsoft WayRedmond, WA 98052anthaue@microsoft.comKevin DuhDept.
of Electrical Eng.Univ.
of WashingtonSeattle, WA 98195duh@ee.washington.eduEric RinggerComputer Science Dept.Brigham Young Univ.Provo, UT 84602ringger@cs.byu.eduAbstractWe develop dependency parsers for Ara-bic, English, Chinese, and Czech usingBayes Point Machines, a training algo-rithm which is as easy to implement asthe perceptron yet competitive with largemargin methods.
We achieve results com-parable to state-of-the-art in English andCzech, and report the first directed depen-dency parsing accuracies for Arabic andChinese.
Given the multilingual nature ofour experiments, we discuss some issuesregarding the comparison of dependencyparsers for different languages.1 IntroductionDependency parsing is an alternative to constituencyanalysis with a venerable tradition going back atleast two millenia.
The last century has seen at-tempts to formalize dependency parsing, particu-larly in the Prague School approach to linguistics(Tesnie`re, 1959; Melc?uk, 1988).In a dependency analysis of syntax, words directlymodify other words.
Unlike constituency analysis,there are no intervening non-lexical nodes.
We usethe terms child and parent to denote the dependentterm and the governing term respectively.Parsing has many potential applications, rang-ing from question answering and information re-trieval to grammar checking.
Our intended ap-plication is machine translation in the MicrosoftResearch Treelet Translation System (Quirk et al,2005; Menezes and Quirk, 2005).
This system ex-pects an analysis of the source language in whichwords are related by directed, unlabeled dependen-cies.
For the purposes of developing machine trans-lation for several language pairs, we are interested independency analyses for multiple languages.The contributions of this paper are two-fold: First,we present a training algorithm called Bayes PointMachines (Herbrich et al, 2001; Harrington et al,2003), which is as easy to implement as the per-ceptron, yet competitive with large margin meth-ods.
This algorithm has implications for anyoneinterested in implementing discriminative trainingmethods for any application.
Second, we developparsers for English, Chinese, Czech, and Arabic andprobe some linguistic questions regarding depen-dency analyses in different languages.
To the best ofour knowledge, the Arabic and Chinese results arethe first reported results to date for directed depen-dencies.
In the following, we first describe the data(Section 2) and the basic parser architecture (Section3).
Section 4 introduces the Bayes Point Machinewhile Section 5 describes the features for each lan-guage.
We conclude with experimental results anddiscussions in Sections 6 and 7.2 DataWe utilize publicly available resources in Arabic,Chinese, Czech, and English for training our depen-dency parsers.For Czech we used the Prague Dependency Tree-bank version 1.0 (LDC2001T10).
This is a corpusof approximately 1.6 million words.
We dividedthe data into the standard splits for training, devel-160opment test and blind test.
The Prague Czech De-pendency Treebank is provided with human-editedand automatically-assigned morphological informa-tion, including part-of-speech labels.
Training andevaluation was performed using the automatically-assigned labels.For Arabic we used the Prague Arabic De-pendency Treebank version 1.0 (LDC2004T23).Since there is no standard split of the data intotraining and test sections, we made an approxi-mate 70%/15%/15% split for training/developmenttest/blind test by sampling whole files.
The Ara-bic Dependency Treebank is considerably smallerthan that used for the other languages, with approx-imately 117,000 tokens annotated for morphologi-cal and syntactic relations.
The relatively small sizeof this corpus, combined with the morphologicalcomplexity of Arabic and the heterogeneity of thecorpus (it is drawn from five different newspapersacross a three-year time period) is reflected in therelatively low dependency accuracy reported below.As with the Czech data, we trained and evaluated us-ing the automatically-assigned part-of-speech labelsprovided with the data.Both the Czech and the Arabic corpora are anno-tated in terms of syntactic dependencies.
For En-glish and Chinese, however, no corpus is availablethat is annotated in terms of dependencies.
We there-fore applied head-finding rules to treebanks thatwere annotated in terms of constituency.For English, we used the Penn Treebank version3.0 (Marcus et al, 1993) and extracted dependencyrelations by applying the head-finding rules of (Ya-mada and Matsumoto, 2003).
These rules are asimplification of the head-finding rules of (Collins,1999).
We trained on sections 02-21, used section24 for development test and evaluated on section23.
The English Penn Treebank contains approxi-mately one million tokens.
Training and evaluationagainst the development test set was performed us-ing human-annotated part-of-speech labels.
Evalu-ation against the blind test set was performed us-ing part-of-speech labels assigned by the tagger de-scribed in (Toutanova et al, 2003).For Chinese, we used the Chinese Treebank ver-sion 5.0 (Xue et al, 2005).
This corpus containsapproximately 500,000 tokens.
We made an approx-imate 70%/15%/15% split for training/developmenttest/blind test by sampling whole files.
As with theEnglish Treebank, training and evaluation againstthe development test set was performed usinghuman-annotated part-of-speech labels.
For evalu-ation against the blind test section, we used an im-plementation of the tagger described in (Toutanovaet al, 2003).
Trained on the same training sectionas that used for training the parser and evaluated onthe development test set, this tagger achieved a to-ken accuracy of 92.2% and a sentence accuracy of63.8%.The corpora used vary in homogeneity from theextreme case of the English Penn Treebank (a largecorpus drawn from a single source, the Wall StreetJournal) to the case of Arabic (a relatively smallcorpus?approximately 2,000 sentences?drawn frommultiple sources).
Furthermore, each languagepresents unique problems for computational analy-sis.
Direct comparison of the dependency parsingresults for one language to the results for anotherlanguage is therefore difficult, although we do at-tempt in the discussion below to provide some basisfor a more direct comparison.
A common questionwhen considering the deployment of a new languagefor machine translation is whether the natural lan-guage components available are of sufficient qualityto warrant the effort to integrate them into the ma-chine translation system.
It is not feasible in everyinstance to do the integration work first and then toevaluate the output.Table 1 summarizes the data used to train theparsers, giving the number of tokens (excludingtraces and other empty elements) and counts of sen-tences.13 Parser ArchitectureWe take as our starting point a re-implementationof McDonald?s state-of-the-art dependency parser(McDonald et al, 2005a).
Given a sentence x, thegoal of the parser is to find the highest-scoring parsey?
among all possible parses y ?
Y :y?
= arg maxy?Ys(x, y) (1)1The files in each partition of the Chinese and Arabic dataare given at http://research.microsoft.com/?simonco/HLTNAACL2006.161Language Total Training Development BlindTokens Sentences Sentences SentencesArabic 116,695 2,100 446 449Chinese 527,242 14,735 1,961 2,080Czech 1,595,247 73,088 7,319 7,507English 1,083,159 39,832 1,346 2,416Table 1: Summary of data used to train parsers.For a given parse y, its score is the sum of the scoresof all its dependency links (i, j) ?
y:s(x, y) = ?
(i,j)?yd(i, j) = ?
(i,j)?yw ?
f(i, j) (2)where the link (i, j) indicates a head-child depen-dency between the token at position i and the tokenat position j.
The score d(i, j) of each dependencylink (i, j) is further decomposed as the weightedsum of its features f(i, j).This parser architecture naturally consists of threemodules: (1) a decoder that enumerates all possi-ble parses y and computes the argmax; (2) a train-ing algorithm for adjusting the weights w given thetraining data; and (3) a feature representation f(i, j).Two decoders will be discussed here; the training al-gorithm and feature representation are discussed inthe following sections.A good decoder should satisfy several proper-ties: ideally, it should be able to search through allvalid parses of a sentence and compute the parsescores efficiently.
Efficiency is a significant issuesince there are usually an exponential number ofparses for any given sentence, and the discrimina-tive training methods we will describe later requirerepeated decoding at each training iteration.
We re-implemented Eisner?s decoder (Eisner, 1996), whichsearches among all projective parse trees, and theChu-Liu-Edmonds?
decoder (Chu and Liu, 1965;Edmonds, 1967), which searches in the space ofboth projective and non-projective parses.
(A pro-jective tree is a parse with no crossing dependencylinks.)
For the English and Chinese data, the head-finding rules for converting from Penn Treebankanalyses to dependency analyses creates trees thatare guaranteed to be projective, so Eisner?s algo-rithm suffices.
For the Czech and Arabic corpora,a non-projective decoder is necessary.
Both algo-rithms are O(N3), where N is the number of wordsin a sentence.2 Refer to (McDonald et al, 2005b)for a detailed treatment of both algorithms.4 Training: The Bayes Point MachineIn this section, we describe an online learning al-gorithm for training the weights w. First, we ar-gue why an online learner is more suitable than abatch learner like a Support Vector Machine (SVM)for this task.
We then review some standard on-line learners (e.g.
perceptron) before presenting theBayes Point Machine (BPM) (Herbrich et al, 2001;Harrington et al, 2003).4.1 Online LearningAn online learner differs from a batch learner in thatit adjusts w incrementally as each input sample isrevealed.
Although the training data for our pars-ing problem exists as a batch (i.e.
all input sam-ples are available during training), we can applyonline learning by presenting the input samples insome sequential order.
For large training set sizes,a batch learner may face computational difficultiessince there already exists an exponential number ofparses per input sentence.
Online learning is moretractable since it works with one input at a time.A popular online learner is the perceptron.
It ad-justs w by updating it with the feature vector when-ever a misclassification on the current input sampleoccurs.
It has been shown that such updates con-verge in a finite number of iterations if the data is lin-early separable.
The averaged perceptron (Collins,2002) is a variant which averages the w across alliterations; it has demonstrated good generalizationespecially with data that is not linearly separable,as in many natural language processing problems.2The Chu-Liu-Edmonds?
decoder, which is based on a maxi-mal spanning tree algorithm, can run in O(N2), but our simplerimplementation of O(N3) was sufficient.162Recently, the good generalization properties of Sup-port Vector Machines have prompted researchers todevelop large margin methods for the online set-ting.
Examples include the margin perceptron (Dudaet al, 2001), ALMA (Gentile, 2001), and MIRA(which is used to train the parser in (McDonald et al,2005a)).
Conceptually, all these methods attempt toachieve a large margin and approximate the maxi-mum margin solution of SVMs.4.2 Bayes Point MachinesThe Bayes Point Machine (BPM) achieves goodgeneralization similar to that of large margin meth-ods, but is motivated by a very different philoso-phy of Bayesian learning or model averaging.
Inthe Bayesian learning framework, we assume a priordistribution over w. Observations of the trainingdata revise our belief of w and produce a poste-rior distribution.
The posterior distribution is usedto create the final wBPM for classification:wBPM = Ep(w|D)[w] =|V (D)|?i=1p(wi|D) wi (3)where p(w|D) is the posterior distribution of theweights given the data D and Ep(w|D) is the expec-tation taken with respect to this distribution.
Theterm |V (D)| is the size of the version space V (D),which is the set of weights wi that is consistent withthe training data (i.e.
the set of wi that classifies thetraining data with zero error).
This solution achievesthe so-called Bayes Point, which is the best approx-imation to the Bayes optimal solution given finitetraining data.In practice, the version space may be large, so weapproximate it with a finite sample of size I .
Further,assuming a uniform prior over weights, we get thefollowing equation:wBPM = Ep(w|D)[w] ?I?i=11I wi (4)Equation 4 can be computed by a very simple al-gorithm: (1) Train separate perceptrons on differentrandom shuffles of the entire training data, obtaininga set of wi.
(2) Take the average (arithmetic mean)of the weights wi.
It is well-known that perceptrontraining results in different weight vector solutionsInput: Training set D = ((x1, y1), (x2, y2), .
.
.
, (xT , yT ))Output: wBPMInitialize: wBPM = 0for i = 1 to I; doRandomly shuffle the sequential order of samples in DInitialize: wi = 0for t = 1 to T; doy?t = wi ?
xtif (y?t != yt) thenwi = wi + ytxtdonewBPM = wBPM + 1IwidoneFigure 1: Bayes Point Machine pseudo-code.if the data samples are presented sequentially in dif-ferent orders.
Therefore, random shuffles of the dataand training a perceptron on each shuffle is effec-tively equivalent to sampling different models (wi)in the version space.
Note that this averaging op-eration should not be confused with ensemble tech-niques such as Bagging or Boosting?ensemble tech-niques average the output hypotheses, whereas BPMaverages the weights (models).The BPM pseudocode is given in Figure 1.
Theinner loop is simply a perceptron algorithm, so theBPM is very simple and fast to implement.
Theouter loop is easily parallelizable, allowing speed-ups in training the BPM.
In our specific implemen-tation for dependency parsing, the line of the pseu-docode corresponding to [y?t = wi ?
xt] is replacedby Eq.
1 and updates are performed for each in-correct dependency link.
Also, we chose to averageeach individual perceptron (Collins, 2002) prior toBayesian averaging.Finally, it is important to note that the definition ofthe version space can be extended to include weightswith non-zero training error, so the BPM can handledata that is not linearly separable.
Also, although weonly presented an algorithm for linear classifiers (pa-rameterized by the weights), arbitrary kernels can beapplied to BPM to allow non-linear decision bound-aries.
Refer to (Herbrich et al, 2001) for a compre-hensive treatment of BPMs.5 FeaturesDependency parsers for all four languages weretrained using the same set of feature types.
Thefeature types are essentially those described in (Mc-Donald et al, 2005a).
For a given pair of tokens,163where one is hypothesized to be the parent and theother to be the child, we extract the word of the par-ent token, the part of speech of the parent token, theword of the child token, the part of speech of thechild token and the part of speech of certain adjacentand intervening tokens.
Some of these atomic fea-tures are combined in feature conjunctions up to fourlong, with the result that the linear classifiers de-scribed below approximate polynomial kernels.
Forexample, in addition to the atomic features extractedfrom the parent and child tokens, the feature [Par-entWord, ParentPOS, ChildWord, ChildPOS] is alsoadded to the feature vector representing the depen-dency between the two tokens.
Additional featuresare created by conjoining each of these features withthe direction of the dependency (i.e.
is the parent tothe left or right of the child) and a quantized measureof the distance between the two tokens.
Every tokenhas exactly one parent.
The root of the sentence hasa special synthetic token as its parent.Like McDonald et al we add features that con-sider the first five characters of words longer thanfive characters.
This truncated word crudely approx-imates stemming.
For Czech and English the addi-tion of these features improves accuracy.
For Chi-nese and Arabic, however, it is clear that we need adifferent backoff strategy.For Chinese, we truncate words longer than a sin-gle character to the first character.3 Experimentalresults on the development test set suggested that analternative strategy, truncation of words longer thantwo characters to the first two characters, yieldedslightly worse results.The Arabic data is annotated with gold-standardmorphological information, including informationabout stems.
It is also annotated with the outputof an automatic morphological analyzer, so that re-searchers can experiment with Arabic without firstneeding to build these components.
For Arabic, wetruncate words to the stem, using the value of thelemma attribute.All tokens are converted to lowercase, and num-bers are normalized.
In the case of English, Czechand Arabic, all numbers are normalized to a sin-3There is a near 1:1 correspondence between charactersand morphemes in contemporary Mandarin Chinese.
However,most content words consist of more than one morpheme, typi-cally two.gle token.
In Chinese, months are normalized to aMONTH token, dates to a DATE token, years to aYEAR token.
All other numbers are normalized to asingle NUMBER token.The feature types were instantiated using all or-acle combinations of child and parent tokens fromthe training data.
It should be noted that when thefeature types are instantiated, we have considerablymore features than McDonald et al For example,for English we have 8,684,328 whereas they report6,998,447 features.
We suspect that this is mostlydue to differences in implementation of the featuresthat backoff to stems.The averaged perceptrons were trained on theone-best parse, updating the perceptron for everyedge and averaging the accumulated perceptrons af-ter every sentence.
Experiments in which we up-dated the perceptron based on k-best parses tendedto produce worse results.
The Chu-Liu-Edmonds al-gorithm was used for Czech.
Experiments with thedevelopment test set suggested that the Eisner de-coder gave better results for Arabic than the Chu-Liu-Edmonds decoder.
We therefore used the Eisnerdecoder for Arabic, Chinese and English.6 ResultsTable 2 presents the accuracy of the dependencyparsers.
Dependency accuracy indicates for howmany tokens we identified the correct head.
Root ac-curacy, i.e.
for how many sentences did we identifythe correct root or roots, is reported as F1 measure,since sentences in the Czech and Arabic corpora canhave multiple roots and since the parsing algorithmscan identify multiple roots.
Complete match indi-cates how many sentences were a complete matchwith the oracle dependency parse.A convention appears to have arisen when report-ing dependency accuracy to give results for Englishexcluding punctuation (i.e., ignoring punctuation to-kens in the output of the parser) and to report resultsfor Czech including punctuation.
In order to facil-itate comparison of the present results with previ-ously published results, we present measures includ-ing and excluding punctuation for all four languages.We hope that by presenting both sets of measure-ments, we also simplify one dimension along whichpublished results of parse accuracy differ.
A direct164Including punctuation Excluding punctuationLanguage Dependency Root Complete Dependency Root CompleteAccuracy Accuracy Match Accuracy Accuracy MatchArabic 79.9 90.0 9.80 79.8 87.8 10.2Chinese 71.2 66.2 17.5 73.3 66.2 18.2Czech 84.0 88.8 30.9 84.3 76.2 32.2English 90.0 93.7 35.1 90.8 93.7 37.6Table 2: Bayes Point Machine accuracy measured on blind test set.comparison of parse results across languages is stilldifficult for reasons to do with the different natureof the languages, the corpora and the differing stan-dards of linguistic detail annotated, but a compar-ison of parsers for two different languages whereboth results include punctuation is at least preferableto a comparison of results including punctuation toresults excluding punctuation.The results reported here for English and Czechare comparable to the previous best published num-bers in (McDonald et al, 2005a), as Table 3 shows.This table compares McDonald et al?s results for anaveraged perceptron trained for ten iterations withno check for convergence (Ryan McDonald, pers.comm.
), MIRA, a large margin classifier, and thecurrent Bayes Point Machine results.
To determinestatistical significance we used confidence intervalsfor p=0.95.
For the comparison of English depen-dency accuracy excluding punctuation, MIRA andBPM are both statistically significantly better thanthe averaged perceptron result reported in (McDon-ald et al, 2005a).
MIRA is significantly betterthan BPM when measuring dependency accuracyand root accuracy, but BPM is significantly betterwhen measuring sentences that match completely.From the fact that neither MIRA nor BPM clearlyoutperforms the other, we conclude that we havesuccessfully replicated the results reported in (Mc-Donald et al, 2005a) for English.For Czech we also determined significance usingconfidence intervals for p=0.95 and compared re-sults including punctuation.
For both dependencyaccuracy and root accuracy, MIRA is statisticalltysignificantly better than averaged perceptron, andBPM is statistically significantly better than MIRA.Measuring the number of sentences that match com-pletely, BPM is statistically significantly better thanaveraged perceptron, but MIRA is significantly bet-ter than BPM.
Again, since neither MIRA nor BPMoutperforms the other on all measures, we concludethat the results constitute a valiation of the resultsreported in (McDonald et al, 2005a).For every language, the dependency accuracy ofthe Bayes Point Machine was greater than the ac-curacy of the best individual perceptron that con-tributed to that Bayes Point Machine, as Table 4shows.
As previously noted, when measuringagainst the development test set, we used human-annotated part-of-speech labels for English and Chi-nese.Although the Prague Czech Dependency Tree-bank is much larger than the English Penn Treebank,all measurements are lower than the correspondingmeasurements for English.
This reflects the fact thatCzech has considerably more inflectional morphol-ogy than English, leading to data sparsity for the lex-ical features.The results reported here for Arabic are, to ourknowledge, the first published numbers for depen-dency parsing of Arabic.
Similarly, the results forChinese are the first published results for the depen-dency parsing of the Chinese Treebank 5.0.4 Sincethe Arabic and Chinese numbers are well short ofthe numbers for Czech and English, we attemptedto determine what impact the smaller corpora usedfor training the Arabic and Chinese parsers mighthave.
We performed data reduction experiments,training the parsers on five random samples at eachsize smaller than the entire training set.
Figure 2shows the dependency accuracy measured on thecomplete development test set when training withsamples of the data.
The graph shows the average4(Wang et al, 2005) report numbers for undirected depen-dencies on the Chinese Treebank 3.0.
We cannot meaningfullycompare those numbers to the numbers here.165Language Algorithm DA RA CMEnglish Avg.
Perceptron 90.6 94.0 36.5(exc punc) MIRA 90.9 94.2 37.5Bayes Point Machine 90.8 93.7 37.6Czech Avg.
Perceptron 82.9 88.0 30.3(inc punc) MIRA 83.3 88.6 31.3Bayes Point Machine 84.0 88.8 30.9Table 3: Comparison to previous best published results reported in (McDonald et al, 2005a).Arabic Chinese Czech EnglishBayes Point Machine 78.4 83.8 84.5 91.2Best averaged perceptron 77.9 83.1 83.5 90.8Worst averaged perceptron 77.4 82.6 83.3 90.5Table 4: Bayes Point Machine accuracy vs. averaged perceptrons, measured on development test set, ex-cluding punctuation.dependency accuracy for five runs at each samplesize up to 5,000 sentences.
English and Chineseaccuracies in this graph use oracle part-of-speechtags.
At all sample sizes, the dependency accu-racy for English exceeds the dependency accuracyof the other languages.
This difference is perhapspartly attributable to the use of oracle part-of-speechtags.
However, we suspect that the major contribu-tor to this difference is the part-of-speech tag set.The tags used in the English Penn Treebank encodetraditional lexical categories such as noun, prepo-sition, and verb.
They also encode morphologicalinformation such as person (the VBZ tag for exam-ple is used for verbs that are third person, presenttense?typically with the suffix -s), tense, numberand degree of comparison.
The part-of-speech tagsets used for the other languages encode lexical cat-egories, but do not encode morphological informa-tion.5 With small amounts of data, the perceptronsdo not encounter sufficient instances of each lexicalitem to calculate reliable weights.
The perceptronsare therefore forced to rely on the part-of-speech in-formation.It is surprising that the results for Arabic and Chi-nese should be so close as we vary the size of the5For Czech and Arabic we followed the convention estab-lished in previous parsing work on the Prague Czech Depen-dency Treebank of using the major and minor part-of-speechtags but ignoring other morphological information annotated oneach node.training data (Figure 2) given that Arabic has richmorphology and Chinese very little.
One possibleexplanation for the similarity in accuracy is that therather poor root accuracy in Chinese indicates parsesthat have gone awry.
Anecdotal inspection of parsessuggests that when the root is not correctly identi-fied, there are usually cascading related errors.Czech, a morphologically complex language inwhich root identification is far from straightfor-ward, exhibits the worst performance at small sam-ple sizes.
But (not shown) as the sample size in-creases, the accuracy of Czech and Chinese con-verge.7 ConclusionsWe have successfully replicated the state-of-the-artresults for dependency parsing (McDonald et al,2005a) for both Czech and English, using BayesPoint Machines.
Bayes Point Machines have the ap-pealing property of simplicity, yet are competitivewith online wide margin methods.We have also presented first results for depen-dency parsing of Arabic and Chinese, together withsome analysis of the performance on those lan-guages.In future work we intend to explore the discrim-inative reranking of n-best lists produced by theseparsers and the incorporation of morphological fea-tures.166606570758085900 500 1000 1500 2000 2500 3000 3500 4000 4500 5000Sample sizeDependencyAccuracyEnglishChineseArabicCzechFigure 2: Dependency accuracy at various samplesizes.
Graph shows average of five samples at eachsize and measures accuracy against the developmenttest set.AcknowledgementsWe would like to thank Ryan McDonald, OtakarSmrz?
and Hiroyasu Yamada for help in variousstages of the project.ReferencesY.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.Michael John Collins.
1999.
Head-Driven StatisticalModels for Natural Language Processing.
Ph.D. the-sis, University of Pennsylvania.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proceedings of EMNLP.R.
O. Duda, P. E. Hart, and D. G. Stork.
2001.
PatternClassification.
John Wiley & Sons, Inc.: New York.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of COLING 1996, pages 340?345.Claudio Gentile.
2001.
A new approximate maximalmargin classification algorithm.
Journal of MachineLearning Research, 2:213?242.Edward Harrington, Ralf Herbrich, Jyrki Kivinen,John C. Platt, and Robert C. Williamson.
2003.
On-line bayes point machines.
In Proc.
7th Pacific-AsiaConference on Knowledge Discovery and Data Min-ing, pages 241?252.Ralf Herbrich, Thore Graepel, and Colin Campbell.2001.
Bayes point machines.
Journal of MachineLearning Research, pages 245?278.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of english: The PennTreebank.
Computational Linguistics, 19(2):313?330.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting ofthe Assocation for Computational Linguistics.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005b.
Online large-margin training of dependencyparsers.
Technical Report MS-CIS-05-11, Dept.
ofComputer and Information Science, Univ.
of Pennsyl-vania.Igor A. Melc?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Arul Menezes and Chris Quirk.
2005.
Microsoft re-search treelet translation system: IWSLT evaluation.In Proceedings of the International Workshop on Spo-ken Language Translation.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd annual meet-ing of the Association for Computational Linguistics.Lucien Tesnie`re.
1959.
E?le?ments de syntaxe structurale.Librairie C. Klincksieck.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL 2003, pages 252?259.Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005.Strictly lexical dependency parsing.
In Proceedingsof the Ninth International Workshop on Parsing Tech-nologies, pages 152?159.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2).Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT, pages 195?206.167
