Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544?1555,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsJointly Learning Word Representations and Composition FunctionsUsing Predicate-Argument StructuresKazuma Hashimoto?, Pontus Stenetorp?, Makoto Miwa?, and Yoshimasa Tsuruoka?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan{hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp?Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japanmakoto-miwa@toyota-ti.ac.jpAbstractWe introduce a novel compositional lan-guage model that works on Predicate-Argument Structures (PASs).
Our modeljointly learns word representations andtheir composition functions using bag-of-words and dependency-based con-texts.
Unlike previous word-sequence-based models, our PAS-based model com-poses arguments into predicates by usingthe category information from the PAS.This enables our model to capture long-range dependencies between words andto better handle constructs such as verb-object and subject-verb-object relations.We verify this experimentally using twophrase similarity datasets and achieve re-sults comparable to or higher than the pre-vious best results.
Our system achievesthese results without the need for pre-trained word vectors and using a muchsmaller training corpus; despite this, forthe subject-verb-object dataset our modelimproves upon the state of the art by asmuch as ?10% in relative performance.1 IntroductionStudies on embedding single words in a vectorspace have made notable successes in capturingtheir syntactic and semantic properties (Turneyand Pantel, 2010).
These embeddings have alsobeen found to be a useful component for NaturalLanguage Processing (NLP) systems; for exam-ple, Turian et al.
(2010) and Collobert et al.
(2011)demonstrated how low-dimensional word vectorslearned by Neural Network Language Models(NNLMs) are beneficial for a wide range of NLPtasks.Recently, the main focus of research on vectorspace representation is shifting from word repre-sentations to phrase representations (Baroni andZamparelli, 2010; Grefenstette and Sadrzadeh,2011; Mitchell and Lapata, 2010; Socher et al.,2012).
Combining the ideas of NNLMs and se-mantic composition, Tsubaki et al.
(2013) intro-duced a novel NNLM incorporating verb-objectdependencies.
More recently, Levy and Goldberg(2014) presented a NNLM that integrated syntac-tic dependencies.
However, to the best of ourknowledge, there is no previous work on integrat-ing a variety of syntactic and semantic dependen-cies into NNLMs in order to learn compositionfunctions as well as word representations.
The fol-lowing question thus arises naturally:Can a variety of dependencies be used tojointly learn both stand-alone word vectorsand their compositions, embedding them inthe same vector space?In this work, we bridge the gap betweenpurely context-based (Levy and Goldberg, 2014;Mikolov et al., 2013b; Mnih and Kavukcuoglu,2013) and compositional (Tsubaki et al., 2013)NNLMs by using the flexible set of categoriesfrom Predicate-Argument-Structures (PASs).More specifically, we propose a CompositionalLog-Bilinear Language Model using PASs (PAS-CLBLM), an overview of which is shown inFigure 1.
The model is trained by maximizingthe accuracy of predicting target words from theirbag-of-words and dependency-based context,which provides information about selectionalpreference.
As shown in Figure 1 (b), one of theadvantages of the PAS-CLBLM is that the modelcan treat not only word vectors but also composedvectors as contexts.
Since the composed vectors1544Figure 1: An overview of the proposed model: PAS-CLBLM.
(a) The PAS-LBLM predicts target wordsfrom their bag-of-words and dependency-based context words.
(b) The PAS-CLBLM predicts targetwords using not only context words but also composed vector representations derived from another levelof predicate-argument structures.
Underlined words are target words and we only depict the bag-of-words vector for the PAS-CLBLM.are treated as input to the language model inthe same way as word vectors, these composedvectors are expected to become similar to wordvectors for words with similar meanings.Our empirical results demonstrate that the pro-posed model has the ability to learn meaning-ful representations for adjective-noun, noun-noun,and (subject-) verb-object dependencies.
On threetasks of measuring the semantic similarity be-tween short phrases (adjective-noun, noun-noun,and verb-object), the learned composed vectorsachieve scores (Spearman?s rank correlation ?
)comparable to or higher than those of previ-ous models.
On a task involving more complexphrases (subject-verb-object), our learned com-posed vectors achieve state-of-the-art performance(?
= 0.50) with a training corpus that is an orderof magnitude smaller than that used by previouswork (Tsubaki et al., 2013; Van de Cruys et al.,2013).
Moreover, the proposed model does notrequire any pre-trained word vectors produced byexternal models, but rather induces word vectorsjointly while training.2 Related WorkThere is a large body of work on how to representthe meaning of a word in a vector space.
Distri-butional approaches assume that the meaning ofa word is determined by the contexts in which itappears (Firth, 1957).
The context of a word is of-ten defined as the words appearing in a windowof fixed-length (bag-of-words) and a simple ap-proach is to treat the co-occurrence statistics of aword w as a vector representation for w (Mitchelland Lapata, 2008; Mitchell and Lapata, 2010); al-ternatively, dependencies between words can beused to define contexts (Goyal et al., 2013; Erkand Pado?, 2008; Thater et al., 2010).In contrast to distributional representations,NNLMs represent words in a low-dimensionalvector space (Bengio et al., 2003; Collobert et al.,2011).
Recently, Mikolov et al.
(2013b) and Mnihand Kavukcuoglu (2013) proposed highly scalablemodels to learn high-dimensional word vectors.Levy and Goldberg (2014) extended the model ofMikolov et al.
(2013b) by treating syntactic depen-dencies as contexts.Mitchell and Lapata (2008) investigated a vari-ety of compositional operators to combine wordvectors into phrasal representations.
Among theseoperators, simple element-wise addition and mul-tiplication are now widely used to represent shortphrases (Mitchell and Lapata, 2010; Blacoe andLapata, 2012).
The obvious limitation with thesesimple approaches is that information about wordorder and syntactic relations is lost.To incorporate syntactic information into com-position functions, a variety of compositionalmodels have been proposed.
These include recur-sive neural networks using phrase-structure trees(Socher et al., 2012; Socher et al., 2013b) andmodels in which words have a specific form ofparameters according to their syntactic roles andcomposition functions are syntactically dependenton the relations of input words (Baroni and Zam-parelli, 2010; Grefenstette and Sadrzadeh, 2011;Hashimoto et al., 2013; Hermann and Blunsom,2013; Socher et al., 2013a).More recently, syntactic dependency-based1545compositional models have been proposed (Pa-perno et al., 2014; Socher et al., 2014; Tsub-aki et al., 2013).
One of the advantages of thesemodels is that they are less restricted by word or-der.
Among these, Tsubaki et al.
(2013) intro-duced a novel compositional NNLM mainly fo-cusing on verb-object dependencies and achievedstate-of-the-art performance for the task of mea-suring the semantic similarity between subject-verb-object phrases.3 PAS-CLBLM: A CompositionalLog-Bilinear Language Model UsingPredicate-Argument StructuresIn some recent studies on representing words asvectors, word vectors are learned by solving wordprediction tasks (Mikolov et al., 2013a; Mnih andKavukcuoglu, 2013).
More specifically, given tar-get words and their context words, the basic ideais to train classifiers to discriminate between eachtarget word and artificially induced negative tar-get words.
The feature vector of the classifiers arecalculated using the context word vectors whosevalues are optimized during training.
As a result,vectors of words in similar contexts become simi-lar to each other.Following these studies, we propose a novelmodel to jointly learn representations for wordsand their compositions by training word predic-tion classifiers using PASs.
In this section, wefirst describe the predicate-argument structures asthey serve as the basis of our model.
We thenintroduce a Log-Bilinear Language Model us-ing Predicate-Argument Structures (PAS-LBLM)to learn word representations using both bag-of-words and dependency-based contexts.
Finally,we propose integrating compositions of words intothe model.
Figure 1 (b) shows the overview of theproposed model.3.1 Predicate-Argument StructuresDue to advances in deep parsing technologies,syntactic parsers that can produce predicate-argument structures are becoming accurate andfast enough to be used for practical applications.In this work, we use the probabilistic HPSGparser Enju (Miyao and Tsujii, 2008) to obtain thepredicate-argument structures of individual sen-tences.
In its grammar, each word in a sentenceis treated as a predicate of a certain category withzero or more arguments.
Table 1 shows some ex-Category predicate arg1 arg2adj arg1 heavy rainnoun arg1 car accidentverb arg12 cause rain accidentprep arg12 at eat restaurantTable 1: Examples of predicates of different cate-gories from the grammar of the Enju parser.
arg1and arg2 denote the first and second arguments.amples of predicates of different categories.1 Forexample, a predicate of the category verb arg12expresses a verb with two arguments.
A graph canbe constructed by connecting words in predicate-argument structures in a sentence; in general, thesegraphs are acyclic.One of the merits of using predicate-argumentstructures is that they can capture dependenciesbetween more than two words, while standard syn-tactic dependency structures are limited to depen-dencies between two words.
For example, one ofthe predicates in the phrase ?heavy rain caused caraccidents?
is the verb ?cause?, and it has two ar-guments (?rain?
and ?accident?).
Furthermore, ex-actly the same predicate-argument structure (pred-icate: cause, first argument: rain, second argu-ment: accident) is extracted from the passive formof the above phrase: ?car accidents were causedby heavy rain?.
This is helpful when capturingsemantic dependencies between predicates and ar-guments, and in extracting facts or relations de-scribed in a sentence, such as who did what towhom.3.2 A Log-Bilinear Language Model UsingPredicate-Argument Structures3.2.1 PAS-based Word PredictionThe PAS-LBLM predicts a target word given itsPAS-based context.
We assume that each wordw in the vocabulary V is represented with a d-dimensional vector v(w).
When a predicate ofcategory c is extracted from a sentence, the PAS-LBLM computes the predicted d-dimensional vec-tor p(wt) for the target word wtfrom its contextwords w1, w2, .
.
.
, wm:p(wt) = f(m?i=1hci?
v(wi)), (1)1The categories of the predicates in the Enju parser aresummarized at http://kmcs.nii.ac.jp/?yusuke/enju/enju-manual/enju-output-spec.html.1546where hci?
Rd?1 are category-specific weightvectors and ?
denotes element-wise multiplica-tion.
f is a non-linearity function; in this workwe define f as tanh.As an example following Figure 1 (a), whenthe predicate ?cause?
is extracted with its firstand second arguments ?rain?
and ?accident?, thePAS-LBLM computes p(cause) ?
Rd followingEq.
(1):p(cause) = f(hverb arg12arg1?
v(rain)+hverb arg12arg2?
v(accident)).
(2)In Eq.
(2), the predicate is treated as the targetword, and its arguments are treated as the con-text words.
In the same way, an argument can betreated as a target word:p(rain) = f(hverb arg12verb?
v(cause)+hverb arg12arg2?
v(accident)).
(3)Relationship to previous work.
If we omit thethe category-specific weight vectors hciin Eq.
(1),our model is similar to the CBOW model inMikolov et al.
(2013a).
CBOW predicts a tar-get word given its surrounding bag-of-words con-text, while our model uses its PAS-based context.To incorporate the PAS information in our modelmore efficiently, we use category-specific weightvectors.
Similarly, the vLBL model of Mnih andKavukcuoglu (2013) uses different weight vec-tors depending on the position relative to the tar-get word.
As with previous neural network lan-guage models (Collobert et al., 2011; Huang et al.,2012), our model and vLBL can use weight ma-trices rather than weight vectors.
However, as dis-cussed by Mnih and Teh (2012), using weight vec-tors makes the training significantly faster than us-ing weight matrices.
Despite the simple formula-tion of the element-wise operations, the category-specific weight vectors efficiently propagate PAS-based context information as explained next.3.2.2 Training Word VectorsTo train the PAS-LBLM, we use a scoring functionto evaluate how well the target word wtfits thegiven context:s(wt, p(wt)) = v?
(wt)Tp(wt), (4)where v?
(wt) ?
Rd?1 is the scoring weight vectorfor wt.
Thus, the model parameters in the PAS-LBLM are (V, ?V ,H).
V is the set of word vec-tors v(w), and ?V is the set of scoring weight vec-tors v?(w).
H is the set of the predicate-category-specific weight vectors hci.Based on the objective in the model of Collobertet al.
(2011), the model parameters are learned byminimizing the following hinge loss:N?n=1max(1?
s(wt, p(wt)) + s(wn, p(wt)), 0),(5)where the negative sample wnis a randomly sam-pled word other than wt, and N is the numberof negative samples.
In our experiments we setN = 1.
Following Mikolov et al.
(2013b), nega-tive samples were drawn from the distribution overunigrams that we raise to the power 0.75 and thennormalize to once again attain a probability distri-bution.
We minimize the loss function in Eq.
(5)using AdaGrad (Duchi et al., 2011).
For furthertraining details, see Section 4.5.Relationship to softmax regression models.The model parameters can be learned by maximiz-ing the log probability of the target word wtbasedon the softmax function:p(wt|context) =exp(s(wt, p(wt)))?|V|i=1exp(s(wi, p(wt))).
(6)This is equivalent to a softmax regression model.However, when the vocabulary V is large, com-puting the softmax function in Eq.
(6) is compu-tationally expensive.
If we do not need probabil-ity distributions over words, we are not necessar-ily restricted to using the probabilistic expressions.Recently, several methods have been proposed toefficiently learn word representations rather thanaccurate language models (Collobert et al., 2011;Mikolov et al., 2013b; Mnih and Kavukcuoglu,2013), and our objective follows the work of Col-lobert et al.
(2011).
Mikolov et al.
(2013b) andMnih and Kavukcuoglu (2013) trained their mod-els using word-dependent scoring weight vectorswhich are the arguments of our scoring functionin Eq.
(4).
During development we also trainedour model using the negative sampling techniqueof Mikolov et al.
(2013b); however, we did not ob-serve any significant performance difference.Intuition behind the PAS-LBLM.
Here webriefly explain how each class of the model pa-rameters of the PAS-LBLM contributes to learningword representations at each stochastic gradient1547decent step.
The category-specific weight vectorsprovide the PAS information for context word vec-tors which we would like to learn.
During train-ing, context word vectors having the same PAS-based syntactic roles are updated similarly.
Theword-dependent scoring weight vectors propagatethe information on which words should, or shouldnot, be predicted.
In effect, context word vectorsmaking similar contributions to word predictionsare updated similarly.
The non-linear function fprovides context words with information on theother context words in the same PAS.
In this way,word vectors are expected to be learned efficientlyby the PAS-LBLM.3.3 Learning Composition FunctionsAs explained in Section 3.1, predicate-argumentstructures inherently form graphs whose nodes arewords in a sentence.
Using the graphs, we can in-tegrate relationships between multiple predicate-argument structures into our model.When the context word wiin Eq.
(1), excludingpredicate words, has another predicate-argumentof category c?
as a dependency, we replace v(wi)with the vector produced by the composition func-tion for the predicate category c?.
For example,as shown in Figure 1 (b), when the first argument?rain?
of the predicate ?cause?
is also the argu-ment of the predicate ?heavy?, we first computethe d-dimensional composed vector representationfor ?heavy?
and ?rain?:gc?
(v(heavy), v(rain)), (7)where c?
is the category adj arg1, and gc?
is a func-tion to combine input vectors for the predicate-category c?.
We can use any composition func-tion that produces a representation of the samedimensionality as its inputs, such as element-wise addition/multiplication (Mitchell and Lap-ata, 2008) or neural networks (Socher et al.,2012).
We then replace v(rain) in Eq.
(2) withgc?
(v(heavy), v(rain)).
When the second argu-ment ?accident?
in Eq.
(2) is also the argumentof the predicate ?car?, v(accident) is replacedwith gc??
(v(car), v(accident)).
c??
is the predi-cate category noun arg1.
These multiple relation-ships of predicate-argument structures should pro-vide richer context information.
We refer to thePAS-LBLM with composition functions as PAS-CLBLM.3.4 Bag-of-Words Sensitive PAS-CLBLMBoth the PAS-LBLM and PAS-CLBLM can takemeaningful relationships between words into ac-count.
However, at times, the number of contextwords can be limited and the ability of other mod-els to take ten or more words from a fixed con-text in a bag-of-words (BoW) fashion could com-pensate for this sparseness.
Huang et al.
(2012)combined local and global contexts in their neuralnetwork language models, and motivated by theirwork, we integrate bag-of-words vectors into ourmodels.
Concretely, we add an additional inputterm to Eq.
(1):p(wt) = f(m?i=1hci?
v(wi) + hcBoW?
v(BoW)),(8)where hcBoW?
Rd?1 are additional weight vec-tors, and v(BoW) ?
Rd?1 is the average of theword vectors in the same sentence.
To constructthe v(BoW) for each sentence, we average theword vectors of nouns and verbs in the same sen-tence, excluding the target and context words.4 Experimental Settings4.1 Training CorpusWe used the British National Corpus (BNC) as ourtraining corpus, extracted 6 million sentences fromthe original BNC files, and parsed them using theEnju parser described in Section 3.1.4.2 Word Sense Disambiguation UsingPart-of-Speech TagsIn general, words can have multiple syntactic us-ages.
For example, the word cause can be anoun or a verb depending on its context.
Mostof the previous work on learning word vectorsignores this ambiguity since word sense disam-biguation could potentially be performed after theword vectors have been trained (Huang et al.,2012; Kartsaklis and Sadrzadeh, 2013).
Some re-cent work explicitly assigns an independent vec-tor for each word usage according to its part-of-speech (POS) tag (Hashimoto et al., 2013; Kart-saklis and Sadrzadeh, 2013).
Alternatively, Baroniand Zamparelli (2010) assigned different forms ofparameters to adjectives and nouns.In our experiments, we combined each wordwith its corresponding POS tags.
We used thebase-forms provided by the Enju parser rather than1548Figure 2: Two PAS-CLBLM training samples.the surface-forms, and used the first two charac-ters of the POS tags.
For example, VB, VBP,VBZ, VBG, VBD, VBN were all mapped to VB.This resulted in two kinds of cause: cause NN andcause VB and we used the 100,000 most frequentlowercased word-POS pairs in the BNC.4.3 Selection of Training Samples Based onCategories of PredicatesTo train the PAS-LBLM and PAS-CLBLM, wecould use all predicate categories.
However, ourpreliminary experiments showed that these cate-gories covered many training samples which arenot directly relevant to our experimental setting,such as determiner-noun dependencies.
We thusmanually selected the categories used in our ex-periments.
The selected predicates are listed inTable 1: adj arg1, noun arg1, prep arg12, andverb arg12.
These categories should providemeaningful information on selectional preference.For example, the prep arg12 denotes prepositionswith two arguments, such as ?eat at restaurant?which means that the verb ?eat?
is related to thenoun ?restaurant?
by the preposition ?at?.
Prepo-sitions are one of the predicates whose argumentscan be verbs, and thus prepositions are importantin training the composition functions for (subject-)verb-object dependencies as described in the nextparagraph.Another point we had to consider was howto construct the training samples for the PAS-CLBLM.
We constructed compositional trainingsamples as explained in Section 3.3 when c?
wasadj arg1, noun arg1, or verb arg12.
Figure 2shows two examples in addition to the examplein Figure 1 (b).
Using such training samples, thePAS-CLBLM could, for example, recognize fromthe two predicate-argument structures, ?eat food?and ?eat at restaurant?, that eating foods is an ac-tion that occurs at restaurants.Model Composition FunctionAddlv(w1) + v(w2)Addnltanh(v(w1) + v(w2))Waddlmcadj?
v(w1) + mcarg1?
v(w2)Waddnltanh(mcadj?v(w1)+mcarg1?v(w2))Table 2: Composition functions used in this work.The examples are shown as the adjective-noun de-pendency between w1=?heavy?
and w2=?rain?.4.4 Selection of Composition FunctionsAs described in Section 3.3, we are free to se-lect any composition functions in Eq.
(7).
Tomaintain the fast training speed of the PAS-LBLM, we avoid dense matrix-vector multiplica-tion in our composition functions.
In Table 2,we list the composition functions used for thePAS-CLBLM.
Addlis element-wise addition andAddnlis element-wise addition with the non-linear function tanh.
The subscripts l and nl de-note the words linear and non-linear.
Similarly,Waddlis element-wise weighted addition andWaddnlis element-wise weighted addition withthe non-linear function tanh.
The weight vec-tors mci?
Rd?1 in Table 2 are predicate-category-specific parameters which are learned during train-ing.
We investigate the effects of the non-linearfunction tanh for these composition functions.In the formulations of the backpropagation algo-rithm, non-linear functions allow the input vectorsto weakly interact with each other.4.5 Initialization and Optimization of ModelParametersWe assigned a 50-dimensional vector for eachword-POS pair described in Section 4.2 and ini-tialized the vectors and the scoring weight vec-tors using small random values.
In part inspiredby the initialization method of the weight matricesin Socher et al.
(2013a), we initialized all valuesin the compositional weight vectors of the Waddland Waddnlas 1.0.
The context weight vectorswere initialized using small random values.We minimized the loss function in Eq.
(5) us-ing mini-batch SGD and AdaGrad (Duchi et al.,2011).
Using AdaGrad, the SGD?s learning rateis adapted independently for each model parame-ter.
This is helpful in training the PAS-LBLM andPAS-CLBLM, as they have conditionally depen-dent model parameters with varying frequencies.1549The mini-batch size was 32 and the learning ratewas 0.05 for each experiment, and no regulariza-tion was used.
To verify the semantics captured bythe proposed models during training and to tunethe hyperparameters, we used the WordSim-3532word similarity data set (Finkelstein et al., 2001).5 Evaluation on Phrase Similarity Tasks5.1 Evaluation SettingsThe learned models were evaluated on four tasksof measuring the semantic similarity betweenshort phrases.
We performed evaluation using thethree tasks (AN, NN, and VO) in the dataset3 pro-vided by Mitchell and Lapata (2010), and the SVOtask in the dataset4 provided by Grefenstette andSadrzadeh (2011).The datasets include pairs of short phrases ex-tracted from the BNC.
AN, NN, and VO con-tain 108 phrase pairs of adjective-noun, noun-noun, and verb-object.
SVO contains 200 pairs ofsubject-verb-object phrases.
Each phrase pair hasmultiple human-ratings: the higher the rating is,the more semantically similar the phrases.
For ex-ample, the subject-verb-object phrase pair of ?stu-dent write name?
and ?student spell name?
has ahigh rating.
The pair ?people try door?
and ?peo-ple judge door?
has a low rating.For evaluation we used the Spearman?s rankcorrelation ?
between the human-ratings and thecosine similarity between the composed vectorpairs.
We mainly used non-averaged human-ratings for each pair, and as described in Section5.3, we also used averaged human-ratings for theSVO task.
Each phrase pair in the datasets was an-notated by more than two annotators.
In the caseof averaged human ratings, we averaged multiplehuman-ratings for each phrase pair, and in the caseof non-averaged human-ratings, we treated eachhuman-rating as a separate annotation.With the PAS-CLBLM, we represented eachphrase using the composition functions listed inTable 2.
When there was no composition present,we represented the phrase using element-wise ad-dition.
For example, when we trained the PAS-CLBLM with the composition function Waddnl,2http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/3http://homepages.inf.ed.ac.uk/s0453356/share4http://www.cs.ox.ac.uk/activities/compdistmeaning/GS2011data.txtModel AN NN VOPAS-CLBLM (Addl) 0.52 0.44 0.35PAS-CLBLM (Addnl) 0.52 0.46 0.45PAS-CLBLM (Waddl) 0.48 0.39 0.34PAS-CLBLM (Waddnl) 0.48 0.40 0.39PAS-LBLM 0.41 0.44 0.39word2vec 0.52 0.48 0.42BL w/ BNC 0.48 0.50 0.35HB w/ BNC 0.41 0.44 0.34KS w/ ukWaC n/a n/a 0.45K w/ BNC n/a n/a 0.41Human agreement 0.52 0.49 0.55Table 3: Spearman?s rank correlation scores ?
forthe three tasks: AN, NN, and VO.the composed vector for each phrase was com-puted using the Waddnlfunction, and when wetrained the PAS-LBLM, we used the element-wiseaddition function.
To compute the composed vec-tors using the Waddland Waddnlfunctions, weused the categories of the predicates adj arg1,noun arg1, and verb arg12 listed in Table 1.As a strong baseline, we trained the Skip-grammodel of Mikolov et al.
(2013b) using the pub-licly available word2vec5 software.
We fed thePOS-tagged BNC into word2vec since our modelsutilize POS tags and trained 50-dimensional wordvectors using word2vec.
For each phrase we thencomputed the representation using vector addition.5.2 AN, NN, and VO TasksTable 3 shows the correlation scores ?
for the AN,NN, and VO tasks.
Human agreement denotes theinter-annotator agreement.
The word2vec baselineachieves unexpectedly high scores for these threetasks.
Previously these kinds of models (Mikolovet al., 2013b; Mnih and Kavukcuoglu, 2013) havemainly been evaluated for word analogy tasks and,to date, there has been no work using these wordvectors for the task of measuring the semantic sim-ilarity between phrases.
However, this experimen-tal result suggests that word2vec can serve as astrong baseline for these kinds of tasks, in addi-tion to word analogy tasks.In Table 3, BL, HB, KS, and K denote the workof Blacoe and Lapata (2012), Hermann and Blun-som (2013), Kartsaklis and Sadrzadeh (2013), andKartsaklis et al.
(2013) respectively.
Among these,5https://code.google.com/p/word2vec/1550Averaged Non-averagedModel Corpus SVO-SVO SVO-V SVO-SVO SVO-VPAS-CLBLM (Addl) 0.29 0.34 0.24 0.28PAS-CLBLM (Addnl) 0.27 0.32 0.24 0.28PAS-CLBLM (Waddl) BNC 0.25 0.26 0.21 0.23PAS-CLBLM (Waddnl) 0.42 0.50 0.34 0.41PAS-LBLM 0.21 0.06 0.18 0.08word2vec BNC 0.12 0.32 0.12 0.28Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/aTsubaki et al.
(2013) ukWaC n/a 0.47 n/a n/aVan de Cruys et al.
(2013) ukWaC n/a n/a 0.32 0.37Human agreement 0.75 0.62Table 4: Spearman?s rank correlation scores ?
for the SVO task.
Averaged denotes the ?
calculated byaveraged human ratings, and Non-averaged denotes the ?
calculated by non-averaged human ratings.only Kartsaklis and Sadrzadeh (2013) used theukWaC corpus (Baroni et al., 2009) which is an or-der of magnitude larger than the BNC.
As we cansee in Table 3, the PAS-CLBLM (Addnl) achievesscores comparable to and higher than those of thebaseline and the previous state-of-the-art results.In relation to these results, the Waddland Waddnlvariants of the PAS-CLBLM do not achieve greatimprovements in performance.
This indicates thatsimple word vector addition can be sufficient tocompose representations for phrases consisting ofword pairs.5.3 SVO TaskTable 4 shows the correlation scores ?
for the SVOtask.
The scores ?
for this task are reported forboth averaged and non-averaged human ratings.This is due to a disagreement in previous workregarding which metric to use when reporting re-sults.
Hence, we report the scores for both settingsin Table 4.
Another point we should consider isthat some previous work reported scores based onthe similarity between composed representations(Grefenstette and Sadrzadeh, 2011; Van de Cruyset al., 2013), and others reported scores based onthe similarity between composed representationsand word representations of landmark verbs fromthe dataset (Tsubaki et al., 2013; Van de Cruys etal., 2013).
For completeness, we report the scoresfor both settings: SVO-SVO and SVO-V in Table 4.The results show that the weighted additionmodel with the non-linear function tanh (PAS-CLBLM (Waddnl)) is effective for the more com-plex phrase task.
While simple vector addition issufficient for phrases consisting of word pairs, it isclear from our experimental results that they fallshort for more complex structures such as thoseinvolved in the SVO task.Our PAS-CLBLM (Waddnl) model outperformsthe previous state-of-the-art scores for the SVOtask as reported by Tsubaki et al.
(2013) andVan de Cruys et al.
(2013).
As such, there are threekey points that we would like to emphasize:(1) the difference of the training corpus size,(2) the necessity of the pre-trained word vectors,(3) the modularity of deep learning models.Tsubaki et al.
(2013) and Van de Cruys et al.
(2013) used the ukWaC corpus.
This means ourmodel works better, despite using a considerablysmaller corpora.
It should also be noted that, likeus, Grefenstette and Sadrzadeh (2011) used theBNC corpus.The model of Tsubaki et al.
(2013) is based onneural network language models which use syn-tactic dependencies between verbs and their ob-jects.
While their novel model, which incorpo-rates the idea of co-compositionality, works wellwith pre-trained word vectors produced by exter-nal models, it is not clear whether the pre-trainedvectors are required to achieve high scores.
Incontrast, we have achieved state-of-the-art resultswithout the use of pre-trained word vectors.Despite our model?s scalability, we trained 50-dimensional vector representations for words andtheir composition functions and achieved highscores using this low dimensional vector space.1551model d AN NN VO SVOAddl50 0.52 0.44 0.35 0.241000 0.51 0.51 0.43 0.31Addnl50 0.52 0.46 0.45 0.241000 0.51 0.50 0.45 0.31Waddl50 0.48 0.39 0.34 0.211000 0.50 0.49 0.43 0.32Waddnl50 0.48 0.40 0.39 0.341000 0.51 0.48 0.48 0.34Table 5: Comparison of the PAS-CLBLM betweend = 50 and d = 1000.This maintains the possibility to incorporate re-cently developed deep learning composition func-tions into our models, such as recursive neuraltensor networks (Socher et al., 2013b) and co-compositional neural networks (Tsubaki et al.,2013).
While such complex composition functionsslow down the training of compositional models,richer information could be captured during train-ing.5.4 Effects of the DimensionalityTo see how the dimensionality of the word vectorsaffects the scores, we trained the PAS-CLBLM foreach setting using 1,000-dimensional word vectorsand set the learning rate to 0.01.
Table 5 showsthe scores for all four tasks.
Note that we only re-port the scores for the setting non-averaged SVO-SVO here.
As shown in Table 5, the scores consis-tently improved with a few exceptions.
The scores?
= 0.51 for the NN task and ?
= 0.48 for theVO task are the best results to date.
However, thescore ?
= 0.34 for the SVO task did not improveby increasing the dimensionality.
This means thatsimply increasing the dimensionality of the wordvectors does not necessarily lead to better resultsfor complex phrases.5.5 Effects of Bag-of-Words ContextsLastly, we trained the PAS-CLBLM without thebag-of-words contexts described in Section 3.4and used 50-dimensional word vectors.
As can beseen in Table 6, large score improvements wereobserved only for the VO and SVO tasks by in-cluding the bag-of-words contexts and the non-linearity function.
It is likely that the results de-pend on how the bag-of-words contexts are con-structed.
However, we leave this line of analysisas future work.
Both adjective-noun and noun-model BoW AN NN VO SVOAddlw/ 0.52 0.44 0.35 0.24w/o 0.48 0.46 0.38 0.23Addnlw/ 0.52 0.46 0.45 0.24w/o 0.50 0.47 0.41 0.15Waddlw/ 0.48 0.39 0.34 0.21w/o 0.47 0.39 0.38 0.21Waddnlw/ 0.48 0.40 0.39 0.34w/o 0.52 0.42 0.33 0.26Table 6: Scores of the PAS-CLBLM with andwithout BoW contexts.noun phrase are noun phrases, and (subject-) verb-object phrases can be regarded as complete sen-tences.
Therefore, different kinds of context infor-mation might be required for both groups.6 Qualitative Analysis on ComposedVectorsAn open question that remains is to what ex-tent composition affects the representations pro-duced by our PAS-CLBLM model.
To evalu-ate this we assigned a vector for each composedrepresentation.
For example, the adjective-noundependency ?heavy rain?
would be assigned anindependent vector.
We added the most fre-quent 100,000 adjective-noun, noun-noun, and(subject-) verb-object tuples to the vocabulary andthe resulting vocabulary contained 400,000 to-kens (100,000+3?100,000).
A similar methodfor treating frequent neighboring words as singlewords was introduced by Mikolov et al.
(2013b).However, some dependencies, such as (subject-)verb-object phrases, are not always captured whenconsidering only neighboring words.Table 7 (No composition) shows some examplesof predicate-argument dependencies with theirclosest neighbors in the vector space accordingto the cosine similarity.
The table shows that thelearned vectors of multiple words capture seman-tic similarity.
For example, the vector of ?heavyrain?
is close to the vectors of words which ex-press the phenomena heavily raining.
The vectorof ?new york?
captures the concept of a major city.The vectors of (subject-) verb-object dependenciesalso capture the semantic similarity, which is themain difference to previous approaches, such asthat of Mikolov et al.
(2013b), which only considerneighboring words.
These results suggest that thePAS-CLBLM can learn meaningful composition1552Query No composition Compositionrain rain(AN) thunderstorm sunshineheavy downpour stormrain blizzard drizzlemuch rain chillgeneral manager executive(AN) vice president directorchief executive director representativeexecutive project manager officermanaging director administratorsecond war war(NN) plane crash worldworld riot racewar last war holocaustgreat war warfareoslo york(NN) paris torontonew birmingham parisyork moscow edinburghmadrid glasgowmake order make(VO) carry survey allowmake pay tax demandpayment pay produceimpose tax bringachieve objective solve(VO) bridge gap alleviatesolve improve quality overcomeproblem deliver information resolveencourage development circumventhold meeting take(SVO) event take place getmeeting end season wintake discussion take place putplace do work gainTable 7: Nearest neighbor vectors for multiplewords.
POS-tags are not shown for simplicity.category predicate arg1 arg2adj arg1 2.38 6.55 -noun arg1 3.37 5.60 -verb arg12 6.78 2.57 2.18Table 8: L2-norms of the 50-dimensional weightvectors of the composition function Waddnl.functions since the composition layers receive thesame error signal via backpropagation.We then trained the PAS-CLBLM using Waddnlto learn composition functions.
Table 7 (Compo-sition) shows the nearest neighbor words for eachcomposed vector, and as we can see, the learnedcomposition function emphasizes the head wordsand captures some sort of semantic similarity.
Wethen inspected the L2-norms of the weight vectorsof the composition function.
As shown in Table 8,head words are strongly emphasized.
Emphasiz-ing head words is helpful in representing com-posed meanings, but in the case of verbs it maynot always be sufficient.
This can be observed inTable 3 and Table 4, which demonstrates that verb-related tasks are more difficult than noun-phrasetasks.While No composition captures the seman-tic similarity well using independent parameters,there is the issue of data sparseness.
As the size ofthe vocabulary increases, the number of tuples ofword dependencies increases rapidly.
In this ex-periment, we used only the 300,000 most frequenttuples.
In contrast to this, the learned composi-tion functions can capture similar information us-ing only word vectors and a small set of predicatecategories.7 Conclusion and Future WorkWe have presented a compositional log-bilinearlanguage model using predicate-argument struc-tures that incorporates both bag-of-words anddependency-based contexts.
In our experimentsthe learned composed vectors achieve state-of-the-art scores for the task of measuring the semanticsimilarity between short phrases.
For the subject-verb-object phrase task, the result is achievedwithout any pre-trained word vectors using a cor-pus an order of magnitude smaller than that of theprevious state of the art.
For future work, we willinvestigate how our models and the resulting vec-tor representations can be helpful for other unsu-pervised and/or supervised tasks.AcknowledgmentsWe thank the anonymous reviewers for their help-ful comments and suggestions.
This work wassupported by JSPS KAKENHI Grant Number13F03041.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky Wide Web:A Collection of Very Large Linguistically ProcessedWeb-Crawled Corpora.
Language Resources andEvaluation, 43(3):209?226.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A Neural Probabilistic Lan-1553guage Model.
Journal of Machine Learning Re-search, 3:1137?1155.William Blacoe and Mirella Lapata.
2012.
A Com-parison of Vector-based Representations for Seman-tic Composition.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 546?556.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural Language Processing (Almost) fromScratch.
Journal of Machine Learning Research,12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
Journal of MachineLearning Research, 12:2121?2159.Katrin Erk and Sebastian Pado?.
2008.
A StructuredVector Space Model for Word Meaning in Context.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages897?906.Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-pin Eytan.
2001.
Placing Search in Context: TheConcept Revisited.
In Proceedings of the Tenth In-ternational World Wide Web Conference.John Rupert Firth.
1957.
A synopsis of linguistictheory 1930-55.
In Studies in Linguistic Analysis,pages 1?32.Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-maya Sachan, Shashank Srivastava, and EduardHovy.
2013.
A Structured Distributional Seman-tic Model : Integrating Structure with Semantics.
InProceedings of the Workshop on Continuous VectorSpace Models and their Compositionality, pages 20?29.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental Support for a Categorical Composi-tional Distributional Model of Meaning.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 1394?1404.Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-ruoka, and Takashi Chikayama.
2013.
Simple Cus-tomization of Recursive Neural Networks for Se-mantic Relation Classification.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1372?1376.Karl Moritz Hermann and Phil Blunsom.
2013.
TheRole of Syntax in Vector Space Models of Composi-tional Semantics.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 894?904.Eric Huang, Richard Socher, Christopher Manning,and Andrew Ng.
2012.
Improving Word Represen-tations via Global Context and Multiple Word Proto-types.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 873?882.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2013.Prior Disambiguation of Word Tensors for Con-structing Sentence Vectors.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 1590?1601.Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and StephenPulman.
2013.
Separating Disambiguation fromComposition in Distributional Semantics.
In Pro-ceedings of 17th Conference on Natural LanguageLearning (CoNLL), pages 114?123.Omer Levy and Yoav Goldberg.
2014.
Dependency-Based Word Embeddings.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages302?308.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient Estimation of Word Repre-sentations in Vector Space.
In Proceedings of Work-shop at the International Conference on LearningRepresentations.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed Represen-tations of Words and Phrases and their Composition-ality.
In Advances in Neural Information ProcessingSystems 26, pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings of46th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 236?244.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
CognitiveScience, 34(8):1388?1439.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In Advances in Neural Information Pro-cessing Systems 26, pages 2265?2273.Andriy Mnih and Yee Whye Teh.
2012.
A fastand simple algorithm for training neural probabilis-tic language models.
In John Langford and JoellePineau, editors, Proceedings of the 29th Interna-tional Conference on Machine Learning (ICML-12),ICML ?12, pages 1751?1758.Denis Paperno, Nghia The Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.
In1554Proceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 90?99.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Compo-sitionality through Recursive Matrix-Vector Spaces.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 1201?1211.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013a.
Parsing with Compo-sitional Vector Grammars.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages455?465.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013b.
Recursive Deep Mod-els for Semantic Compositionality Over a SentimentTreebank.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1631?1642.Richard Socher, Quoc V. Le, Christopher D. Manning,and Andrew Y. Ng.
2014.
Grounded CompositionalSemantics for Finding and Describing Images withSentences.
Transactions of the Association for Com-putational Linguistics, 2:207?218.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing Semantic RepresentationsUsing Syntactically Enriched Vector Models.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 948?957.Masashi Tsubaki, Kevin Duh, Masashi Shimbo, andYuji Matsumoto.
2013.
Modeling and Learning Se-mantic Co-Compositionality through Prototype Pro-jections and Neural Networks.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 130?140.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and GeneralMethod for Semi-Supervised Learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2013.
A Tensor-based Factorization Model ofSemantic Compositionality.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1142?1151.1555
