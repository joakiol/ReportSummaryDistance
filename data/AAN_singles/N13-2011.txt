Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 77?83,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsHelpfulness-Guided Review SummarizationWenting XiongUniversity of Pittsburgh210 South Bouquet Street, Pittsburgh, PA 15260wex12@cs.pitt.eduAbstractReview mining and summarization has beena hot topic for the past decade.
A lot of ef-fort has been devoted to aspect detection andsentiment analysis under the assumption thatevery review has the same utility for relatedtasks.
However, reviews are not equally help-ful as indicated by user-provided helpfulnessassessment associated with the reviews.
Inthis thesis, we propose a novel review sum-marization framework which summarizes re-view content under the supervision of auto-mated assessment of review helpfulness.
Thishelpfulness-guided framework can be easilyadapted to traditional review summarizationtasks, for a wide range of domains.1 IntroductionNowadays, as reviews thrive on the web, more andmore people wade through these online resourcesto inform their own decision making.
Due to therapid growth of the review volume, the ability ofautomatically summarizing online reviews becomescritical to allowing people to make use of them.This makes review mining and summarization anincreasingly hot topic over the past decade.
Gen-erally speaking, there are two main paradigms inreview summarization.
One is aspect-based opin-ion summarization, which aims to differentiate andsummarize opinions regarding specific subject as-pects.
It usually involves fine-grained analysis ofboth review topics and review sentiment.
The otheris more summarization-oriented, prior work underthis category either assumes a shared topic or aims toproduce general summaries.
In this case, the focusis the summarization, extracting salient informationfrom reviews and organizing them properly.
Com-pared with traditional text summarizers, sentiment-informed summarizers generally perform better asshown by human evaluation results (Carenini et al2006; Lerman et al 2009).However, one implicit assumption shared by mostprior work is that all reviews are of the same util-ity in review summarization tasks, while reviewsthat comment on the same aspect and are associ-ated with the same rating may have difference in-fluence to users, as indicated by user-provided help-fulness assessment (e.g.
?helpful?
votes on Ama-zon.com).
We believe that user-generated helpful-ness votes/ratings suggest people?s point of interestin review exploration.
Intuitively, when users re-fer to online reviews for guidance, reviews that areconsidered helpful by more people naturally receivemore attention and credit, and thus should be givenmore weight in review summarization.
Followingthis intuition, we hypothesize that introducing re-view helpfulness information into review summa-rization can yield more useful review summaries.In addition, we are also motivated by the chal-lenges that we faced when summarizing educationalpeer reviews in which the review entity is also text.In the peer-review domain, traditional algorithmsof identifying review aspects may suffer as reviewscontain both reviewers?
evaluations of a paper andreviewers?
references to the paper.
Such heteroge-neous sources of review content bring challenges toaspect identification, and the educational perspectiveof peer review directly affects the characteristics of77desired summaries, which has not ye been taken intoconsideration in any of the current summarizationtechniques.
We expect the helpfulness assessmentof peer reviews can identify important informationthat should be captured in peer-review summaries.2 Related workThe proposed work is grounded in the followingareas: review-helpfulness analysis, review summa-rization and supervised topic modeling.
In this sec-tion, we will discuss existing work in the literatureand explain how the proposed work relates to them.2.1 Review-helpfulness analysisIn the literature, most researchers take a supervisedapproach in modeling review helpfulness.
They ei-ther aggregate binary helpfulness votes for each re-view into a numerical score, or directly use numer-ical helpfulness ratings.
Kim et.
al (2006) took thefirst attempt, using regression to model review help-fulness based on various linguistic features.
Theyreported that the combination of review length, re-view unigrams and product rating statistics per-formed best.
Along this line, other studies showedthe perceived review helpfulness depends not onlyon the review content, but also on some other fac-tors.
Ghose et.
al (2008) found that the reviewer?sreviewing history also matters.
However, they ob-served that review-subjectivity, review-readabilityand other reviewer-related features are interchange-able for predicting review helpfulness.
In addition,the empirical study on Amazon reviews conductedby Danescu-Niculescu-Mizil et.
al (2009) revealedthat the perceived helpfulness is also affected byhow a review relates to the other reviews of the sameproduct.
However, given our goal of using reviewhelpfulness assessment to guide summarization to-wards generating more useful summaries rather thanto explain each individual helpfulness rating, wewill ignore the interaction of helpfulness assessmentamong reviews of the same target.Furthermore, the utility of features in modelingreview helpfulness may vary with the review do-main.
Mudambi et.
al (2010) showed that forproduct reviews, the product type moderates boththe product ratings and review length on the per-ceived review helpfulness.
For educational peer re-views, in X (2011) we showed that cognitive con-structs which predict feedback implementation canfurther improve our helpfulness model upon generallinguistic features.
These findings seem to suggestthat the review helpfulness model should be domain-dependent, due to the specific semantics of ?helpful-ness?
defined in context of the domain.2.2 Review summarizationOne major paradigm of review summarization isaspect-based summarization, which is based onidentifying aspects and associating opinion senti-ment with them.
(Although this line of work isclosely related to sentiment analysis, it is not thefocus of this proposed work.)
While initially peo-ple use information retrieval techniques to recog-nize aspect terms and opinion expressions (Hu andLiu, 2004; Popescu and Etzioni, 2005), recent workseems to favor generative statistical models more(Mei et al 2007; Lu and Zhai, 2008; Titov and Mc-Donald, 2008b; Titov and McDonald, 2008a; Bleiand McAuliffe, 2010; Brody and Elhadad, 2010;Mukherjee and Liu, 2012; Sauper and Barzilay,2013).
One typical problem with these models isthat many discovered aspects are not meaningful toend-users.
Some of these studies focus on distin-guishing aspects in terms of sentiment variation bymodeling aspects together with sentiment (Titov andMcDonald, 2008a; Lu and Zhai, 2008; Mukherjeeand Liu, 2012; Sauper and Barzilay, 2013).
How-ever, little attention is given to differentiating reviewcontent directly regarding their utilities in reviewexploration.
Mukherjee and Liu (2012) attemptedto address this issue by introducing user-providedaspect terms as seeds for learning review aspects,though this approach might not be easily generalizedto other domains, as users?
point of interest couldvary with the review domain.Another paradigm of review summarization ismore summarization-oriented.
In contrast, such ap-proaches do not require the step of identifying as-pects, instead, they either assume the input text sharethe same aspect or aim to produce general sum-maries.
These studies are closely related to the tra-ditional NLP task of text summarization.
Generallyspeaking, the goal of text summarization is to retainthe most important points of the input text within ashorter length.
Either extractively or abstractively,78one important task is to determine the informative-ness of a text element.
In addition to reducing in-formation redundancy, different heuristics were pro-posed within the context of opinion summarization.Stoyanov and Cardie (2008) focused on identifyingopinion entities (opinion, source, target) and pre-senting them in a structured way (templates or di-agrams).
Lerman et.
al (2009) reported that userspreferred sentiment informed summaries based ontheir analysis of human evaluation of various sum-marization models, while Kim and Zhai (2009) fur-ther considered an effective review summary as rep-resentative contrastive opinion pairs.
Different fromall above, Ganesan et.
al (2010) represented textinput as token-based graphs based on the token or-der in the string.
They rank summary candidates byscoring paths after removing redundant informationfrom the graph.
For any summarization frameworkdiscussed above, the helpfulness of the review ele-ments (e.g.
sentences, opinion entities, or words),which can be derived from the review overall help-fulness, captures informativeness from another di-mension that has not been taken into account yet.2.3 Supervised content modelingAs review summarization is meant to help users ac-quire useful information effectively, what and howto summarize may vary with user needs.
To discoveruser preferences, Ando and Ishizaki (2012) man-ually analyzed travel reviews to identify the mostinfluential review sentences objectively and subjec-tively, while Mukherjee and Liu (2012) extract andcategorize review aspects through semi-supervisedmodeling using user-provided seeds (categories ofterms).
In contrast, we are interested in using user-provided helpfulness ratings for guidance.
As thesehelpfulness ratings are existing meta data of reviews,we will need no additional input from users.
Specif-ically, we propose to use supervised LDA (Blei andMcAuliffe, 2010) to model review content under thesupervision of review helpfulness ratings.
Similarapproach is widely adopted in sentiment analysis,where review aspects are learned in the presenceof sentiment predictions (Blei and McAuliffe, 2010;Titov and McDonald, 2008a).
Furthermore, Brana-van et.
al (2009) showed that joint modeling of textand user annotations benefits extractive summariza-tion.
Therefore, we hypothesize modeling reviewcontent together with review helpfulness is benefi-cial to review summarization as well.3 DataWe plan to experiment on three representative re-view domains: product reviews, book reviews andpeer reviews.
The first one is mostly studied, whilethe later two types are more complex, as the reviewcontent consists of both reviewer?s evaluations of thetarget and reviewer?s references to the target, whichis also text.
This property makes review summariza-tion more challenging.For product reviews and book reviews, we planto use Amazon reviews provided by Jindal and Liu(2008), which is a widely used data set in reviewmining and sentiment analysis.
We consider thehelpfulness assessment of an Amazon review as theratio of ?helpful?
votes over all votes (Kim et al2006).
For educational peer reviews, we plan to usean annotated corpus (Nelson and Schunn, 2009) col-lected from an online peer-review reciprocal system,which we used in our prior work (Xiong and Litman,2011).
Two experts (a writing instructor and a con-tent instructor) were asked to rate the helpfulness ofeach peer review on a scale from one to five (Pearsoncorrelation r = 0.425, p ?
0.01).
For our study, weconsider the average ratings given by the two experts(which roughly follow a normal distribution) as thegold standard of review helpfulness ratings.
To beconsistent with the other review domains, we nor-malize peer-review helpfulness ratings in the rangebetween 0 and 1.4 Proposed workThe proposed thesis work consists of three parts:1) review content analysis using user-provided help-fulness ratings, 2) automatically predicting reviewhelpfulness and 3) a helpfulness-guided review sum-marization framework.4.1 Review content analysisBefore advocating the proposed idea, we would testour two hypothesis: 1) user-provided review help-fulness assessment reflects review content differ-ence.
2) Considering review content in terms of in-ternal content (e.g.
reviewers?
opinions) vs. exter-nal content (e.g.
book content), the internal content79influences the perceived review helpfulness morethan the external content.We propose to use two kind of instruments, one isLinguistic Inquiry Word Count (LIWC)1, which isa manually created dictionary of words; the other isthe set of review topics learned by Latent Dirich-let Allocation (LDA) (Blei et al 2003; Blei andMcAuliffe, 2010).
LIWC analyzes text input basedon language usages both syntactically and semanti-cally, which reveals review content patterns at a highlevel; LDA can be used to model sentence-level re-view topics which are domain specific.For the LIWC-based analysis, we test whethereach category count has a significant effect on thenumerical helpfulness ratings using paired T-test.For LDA-based analysis, we demonstrate the dif-ference by show how the learned topics vary whenhelpfulness information is introduced as supervi-sion.
Specifically, by comparing the topics learnedfrom the unsupervised LDA and those learned fromthe supervised LDA (with helpfulness ratings), weexpect to show that the supervision of helpfulnessratings can yield more meaningful aspect clusters.It is important to note that in both approachesa review is considered as a bag of words, whichmight be problematic if the review has both internaland external content.
Considering this, we hypoth-esize that the content difference captured by user-provided helpfulness ratings is mainly in the review-ers?
evaluation rather than in the content of externalsources (hypothesis 2).
We plan to test this hypoth-esis on both book reviews and peer reviews by ana-lyzing review content in two conditions: in the firstcondition (the control condition), all content is pre-served; in the second condition, the external contentis excluded.
If we observe more content variancein the second condition than the first one, the sec-ond hypothesis is true.
Thus we will separate reviewinternal and external content in the later summariza-tion step.
For simplification, in the second condi-tion, we only consider the topic words of the exter-nal content; we plan to use a corpus-based approachto identify these topic terms and filter them out toreduce the impact of external content.1Url: http://www.liwc.net.
We are using LIWC2007.4.2 Automated review helpfulness assessmentConsidering how review usefulness would be inte-grated in the proposed summarization framework,we propose two models for predicting review help-fulness at different levels of granularity.A discriminative model to learn review globalhelpfulness.
Previously we (2011) built a discrim-inative model for predicting the helpfulness of ed-ucational peer reviews based on prior work of au-tomatically predicting review helpfulness of prod-uct reviews (Kim et al 2006).
We considered bothdomain-general features and domain-specific fea-tures.
The domain-general features include structurefeatures (e.g.
review length), semantic features, anddescriptive statistics of the product ratings (Kim etal., 2006); the domain-specific features include thepercentage of external content in reviews and cog-nitive and social science features that are specificto the peer-review domain.
To extend this idea toother types of reviews: for product reviews, we con-sider product aspect-related terms as the topic wordsof the external content; for book reviews, we takeinto account author?s profile information (numberof books, the mean average book ratings).
As weshowed that replacing review unigrams with manu-ally crafted keyword categories can further improvethe helpfulness model of peer reviews, we plan toinvestigate whether review unigrams are generallyreplaceable by review LIWC features for modelingreview helpfulness.A generative model to learn review local help-fulness.
In order to utilize user-provided helpfulnessinformation in a decomposable fashion, we proposeto use sLDA (Blei and McAuliffe, 2010) to modelreview content with review helpfulness informationat the review level, so that the learned latent topicswill be predictive of review helpfulness.
In additionto evaluating the model?s predictive power and thequality of the learned topics, we will also investi-gate the extent to which the model?s performance isaffected by the size of the training set, as we mayneed to use automatically predicted review helpful-ness instead, if user-provided helpfulness informa-tion is not available.804.3 Helpfulness-guided review summarizationIn the proposed work, we plan to investigate variousmethods of supervising an extractive review summa-rizer using the proposed helpfulness models.
Thesimplest method (M1) is to control review helpful-ness of the summarization input by removing re-views that are predicted of low helpfulness.
A sim-ilar method (M2) is to use post-processing ratherthan pre-processing ?
reorder the selected summarycandidates (e.g.
sentences) based on their predictedhelpfulness.
The helpfulness of a summary sentencecan be either inferred from the local-helpfulnessmodel (sLDA), or aggregated from review-levelhelpfulness ratings of the review(s) from which thesentence is extracted.
The third one (M3) workstogether with a specific summarization algorithm,interpolating traditional informativeness assessmentwith novel helpfulness metrics based on the pro-posed helpfulness models.For demonstration, we plan to prototype the pro-posed framework based on MEAD* (Carenini et al2006), which is an extension of MEAD (an open-source framework for multi-document summariza-tion (Radev et al 2004)) for summarizing evalu-ative text.
MEAD* defines sentence informative-ness based on features extracted through standardaspect-based review mining (Hu and Liu, 2004).
Asa human-centric design, we plan to evaluate the pro-posed framework in a user study in terms of pair-wise comparison of the reviews generated by differ-ent summarizers (M1, M2, M3 and MEAD*).
Al-though fully automated summarization metrics areavailable (e.g.
Jensen-Shannon Divergence (Louisand Nenkova, 2009)), they favor summaries thathave a similar word distribution to the input and thusdo not suit our task of review summarization.To show the generality of the proposed ideas, weplan to evaluate the utility of introducing reviewhelpfulness in aspect ranking as well, which is animportant sub-task of review opinion analysis.
Ifour hypothesis (1) is true, we would expect aspectranking based on helpfulness-involved metrics out-performing the baseline which does not use reviewhelpfulness (Yu et al 2011).
This evaluation willbe done on product reviews and peer reviews, as theprevious work was based on product reviews, whilepeer reviews tend to have an objective aspect rank-ing (provided by domain experts).5 ContributionsThe proposed thesis mainly contributes to reviewmining and summarization.1.
Investigate the impact of the source of reviewcontent on review helpfulness.
While a lot ofstudies focus on product reviews, we based ouranalysis on a wider range of domains, includingpeer reviews, which have not been well studiedbefore.2.
Propose two models to automatically assess re-view helpfulness at different levels of granu-larity.
While the review-level global helpful-ness model takes into account domain-specificsemantics of helpfulness of reviews, the lo-cal helpfulness model learns review helpfulnessjointly with review topics.
This local helpful-ness model allows us to decompose overall re-view helpfulness into small elements, so thatreview helpfulness can be easily combined withmetrics of other dimensions in assessing theimportance of summarization candidates.3.
Propose a user-centric review summarizationframework that utilizes user-provided helpful-ness assessment as supervision.
Comparedwith previous work, we take a data driven ap-proach in modeling review helpfulness as wellas helpfulness-related topics, which requiresno extra human input of user-preference andcan be adapted to typical review summarizationtasks such as aspect selection/ranking, sum-mary sentence ordering, etc.ReferencesM.
Ando and S. Ishizaki.
2012.
Analysis of travel re-view data from readers point of view.
WASSA 2012,page 47.D.M.
Blei and J.D.
McAuliffe.
2010.
Supervised topicmodels.
arXiv preprint arXiv:1003.0783.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alcation.
the Journal of machine Learningresearch, 3:993?1022.SRK Branavan, H. Chen, J. Eisenstein, and R. Barzilay.2009.
Learning document-level semantic properties81from free-text annotations.
Journal of Artificial Intel-ligence Research, 34(2):569.S.
Brody and N. Elhadad.
2010.
An unsupervised aspect-sentiment model for online reviews.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 804?812.
Associa-tion for Computational Linguistics.G.
Carenini, R. Ng, and A. Pauls.
2006.
Multi-documentsummarization of evaluative text.
In In Proceedingsof the 11st Conference of the European Chapter of theAssociation for Computational Linguistics.
Citeseer.Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,Jon Kleinber g, and Lillian Lee.
2009.
How opin-ions are received by online communities: A case studyon Amazon .com helpfulness votes.
In Proceedings ofWWW, pages 141?150.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conference onComputational Linguistics, COLING ?10, pages 340?348, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Anindya Ghose and Panagiotis G. Ipeirotis.
2008.
Esti-mating the socio-economic impact of product reviews.In NYU Stern Research Working Paper CeDER.M.
Hu and B. Liu.
2004.
Mining and summarizingcustomer reviews.
In Proceedings of the tenth ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 168?177.
ACM.N.
Jindal and B. Liu.
2008.
Opinion spam and analysis.In Proceedings of the international conference on Websearch and web data mining, pages 219?230.H.D.
Kim and C.X.
Zhai.
2009.
Generating comparativesummaries of contradictory opinions in text.
In Pro-ceedings of the 18th ACM conference on Informationand knowledge management, pages 385?394.
ACM.Soo-Min Kim, Patrick Pantel, Tim Chklovski, and MarcoPennacchiotti.
2006.
Automatically assessing reviewhelpfulness.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP2006), pages 423?430, Sydney, Australia,July.K.
Lerman, S. Blair-Goldensohn, and R. McDonald.2009.
Sentiment summarization: Evaluating andlearning user preferences.
In Proceedings of the 12thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 514?522.Association for Computational Linguistics.Annie Louis and Ani Nenkova.
2009.
Automaticallyevaluating content selection in summarization withouthuman models.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing: Volume 1-Volume 1, pages 306?314.
Associ-ation for Computational Linguistics.Y.
Lu and C. Zhai.
2008.
Opinion integration throughsemi-supervised topic modeling.
In Proceedings ofthe 17th international conference on World Wide Web,pages 121?130.
ACM.Q.
Mei, X. Ling, M. Wondra, H. Su, and C.X.
Zhai.
2007.Topic sentiment mixture: modeling facets and opin-ions in weblogs.
In Proceedings of the 16th interna-tional conference on World Wide Web, pages 171?180.ACM.S.M.
Mudambi and D. Schuff.
2010.
What makes ahelpful online review?
a study of customer reviewson amazon.
com.
MIS quarterly, 34(1):185?200.A.
Mukherjee and B. Liu.
2012. aspect extractionthrough semi-supervised modeling.
In Proceedings of50th anunal meeting of association for computationalLinguistics (acL-2012)(accepted for publication).Melissa M. Nelson and Christian D. Schunn.
2009.
Thenature of feedback: how different types of peer feed-back affect writing performance.
In Instructional Sci-ence, volume 37, pages 375?401.A.M.
Popescu and O. Etzioni.
2005.
Extracting productfeatures and opinions from reviews.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 339?346.
Association for Computational Lin-guistics.D.
Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer,A.
Celebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam,D.
Liu, et al2004.
Mead-a platform for multidocu-ment multilingual text summarization.
In Proceedingsof LREC, volume 2004.C.
Sauper and R. Barzilay.
2013.
Automatic aggregationby joint modeling of aspects and values.
Journal ofArtificial Intelligence Research, 46:89?127.V.
Stoyanov and C. Cardie.
2008.
Topic identificationfor fine-grained opinion analysis.
In Proceedings ofthe 22nd International Conference on ComputationalLinguistics-Volume 1, pages 817?824.
Association forComputational Linguistics.I.
Titov and R. McDonald.
2008a.
A joint model of textand aspect ratings for sentiment summarization.
Ur-bana, 51:61801.I.
Titov and R. McDonald.
2008b.
Modeling online re-views with multi-grain topic models.
In Proceedingsof the 17th international conference on World WideWeb, pages 111?120.
ACM.Wenting Xiong and Diane Litman.
2011.
Automaticallypredicting peer-review helpfulness.
In Proceedings of82the 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 502?507.J.
Yu, Z.J.
Zha, M. Wang, and T.S.
Chua.
2011.
Aspectranking: identifying important product aspects fromonline consumer reviews.
Computational Linguistics,pages 1496?1505.83
