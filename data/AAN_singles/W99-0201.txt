Cross-Document Event Coreference: Annotations, Experiments,and ObservationsAmit  BaggaGenera l  E lect r i c  Company CRDPO Box  8Schenectady ,  NY  12309bagga@crd .ge .com518-387-7077Breck  BaldwinIRCS,  Un ivers i ty  of  Pennsy lvan ia3401 Walnut  S t reet ,  #400CPh i lade lph ia ,  PA  19104breck@unag i .c i s .upenn.edu215-898-03291 Abst ractWe have developed cross document event trackingtechnology that extends our earlier efforts in crossdocument person coreference.
The software takesclass of events, like "resignations" and clusters doc-uments that mention resignations into equivalenceclasses.
Documents belong to the same equivalenceclass if they mention the same "resignation" event,i.e.
resignations involving the same person, time,and organization.
Other events evaluated include"elections" and "espionage" events.
Results rangefrom 45-90% F-measure scores and we present a briefinterannotator study for the "elections" data set.2 IntroductionEvents form the backbone of the reasons why peo-ple communicate o one another.
News is interestingand important because it describes actions, changesof state and new relationships between individuals.While the communicative importance of describedevents is evident, the phenomenon has proved diffi-cult to recognize and manipulate in automated ways(example: MUC information extraction efforts).We began this research program by developingalgorithms to determine whether two mentions ofa name, example "John Smith", in different docu-ments actually referred to the same individual in theworld.
The system that we built was quite success-ful at resolving cross-document entoty coreference(Bagga, 98b).
We, therefore, decided to extend thesystem so that it could handle events as well.
Ourgoal was to determine whether events in separatedocuments, example "resignations", referred to thesame event in the world (is it the same person re-signing from the same company at the same time).This new classof coreference has proved to be morechallenging.Below we will present our approach and resultsas follows: First we discuss how this research is dif-ferent from Information Extraction and Topic De-tection and Tracking.
Then we present he core al-gorithm for cross document person coreference andour method of scoring the the system's output.
Themethod for determining event reference follows withpresentation and discussion of results.
We finishwith an interannotator agreement experiment andfuture work.3 D i f fe rences  between CrossDocument  Event  Reference  and  IEand TDTBefore proceeding further, it should be emphasizedthat cross-document event reference is a distinct goalfrom Information Extraction (IE) and Topic Detec-tion and Tracking (TDT).Our approach differs from both IE and TDT inthat it takes a very abstract definition of an eventas a starting place, for instance the initial set of doc-uments for resignation events consists of documentsthat have "resign" as a sub-string.
This is even lessinformation than information retrieval evaluationslike TREC.
IE takes as an event description largehand built event recognizers that are typically finitestate machines.
TDT starts with rather verbose de-scriptions of events.
In addition to differences inwhat these technologies take as input to describethe event, the goal of the technologies differ as well.Information Extraction focuses on mapping fromfree text into structured ata formats like databaseentries.
Two separate instances of an event intwo documents would be mapped into the databasestructures without consideration whether they werethe same event or not.
In fact, cross-document eventtracking could well help information extraction sys-tems by identifying sets of documents that describethe same event, and giving the patterns multiplechances to find a match.Topic Detection and Tracking seeks to classify astream of documents into "bins" based on a descrip-tion of the bins.
Looking at the tasks from the TDT-2 evaluation, there are examples that are more gen-eral and tasks that are more specific than our an-notation.
For example, the topic "Asian bailouts bythe IMF" clusters documents into the same bin ir-respective of which country is being bailed out.
Ourapproach would try to more finely individuate thedocuments by distinguishing between countrieS andtimes.
Another TDT topic involved the Texas Cat-John Perry, of Weston Golf Club, an-nounced his resignation yesterday.
He wasthe President of the Massachusetts GolfAssociation.
During his two years in of-rice, Perry guided the MGA into a closerrelationship with the Women's Golf Asso-ciation of Massachusetts.Oliver "Biff" Kelly of Weymouth suc-ceeds John Perry as president of the Mas-sachusetts Golf Association.
"We will havecontinued growth in the future," said Kelly,who will serve for two years.
"There's beena lot of changes and there will be continuedchanges as we head into the year 2000.
"Figure 2: Extract from doc.36IGIFigure 3: Coreference Chains for doc.36tlemen's Association lawsuit against Oprah Winfrey.Given "lawsuits" as an event, we would seek to putdocuments mentioning that lawsuit into the sameequivalent class, but would also form equivalenceclasses of for other lawsuits.
In addition, our even-tual goal is to provide generic ross-document coref-erence for all entities/events in a document i.e.
wewant to resolve cross-docuemtn coreferences for allentities and events mentioned in a document.
Thisgoal is significantly different from TDT's goal of clas-sifying a stream of documents into "bins".4 Cross -Document  Core ference  fo rI nd iv idua lsThe primary technology that drives this research iscross-document coreference.
Until recently, cross-document coreference had been thought o be a hardproblem to solve (Grishman, 94).
However, pre-liminary results in (Bagga, 98a) and (Bagga, 98b)show that high quality cross-document coreferenceis achievable.Figure 1 shows the architecture of the cross-document system built.
Details about each of themain steps of the cross-document coreference algo-rithm are given below.?
First, for each article, the within documentcoreference module of the University of Penn-sylvania's CAMP system is run on that article.It produces coreference chains for all the enti-ties mentioned in the article.
For example, con-sider the two extracts in Figures 2 and 4.
Thecoreference chains output by CAMP for the twoextracts are shown in Figures 3 and 5.?
Next, for the coreference hain of interest withineach article (for example, the coreference chainFigure 4: Extract from doc.38J I rI I I,IIFigure 5: Coreference Chains for doc.38that contains "John Perry"), the Sentence Ex-tractor module extracts all the sentences thatcontain the noun phrases which form the coref-erence chain.
In other words, the SentenceEx-tractor module produces a "summary" of the ar-ticle with respect to the entity of interest.
Thesesummaries are a special case of the query sensi-tive techniques being developed at Penn usingCAMP.
Therefore, for doc.36 (Figure 2), sinceat least one of the three noun phrases ("JohnPerry," "he," and "Perry") in the coreferencechain of interest appears in each of the threesentences in the extract, the summary producedby SentenceExtractor is the extract itself.
Onthe other hand, the summary produced by Sen-tenceExtractor for the coreference chain of in-terest in doc.38 is only the first sentence of theextract because the only element of the corefer-ence chain appears in this sentence.Finally, for each article, the VSM-Disambiguatemodule uses the summary extracted by the Sen-tenceExtractor and computes its similarity withthe summaries extracted from each of the otherarticles.
The VSM-Disambiguate module uses astandard vector space model (used widely in in-formation retrieval) (Salton, 89) to compute thesimilarities between the summaries.
Summarieshaving similarity above a certain threshold areconsidered to be regarding the same entity.4.1 ScoringIn order to score the cross-document coreferencechains output by the system, we had to map thecross-document coreference scoring problem to awithin-document coreference scoring problem.
Thiswas done by creating a meta document consistingof the file names of each of the documents that thesystem was run on.
Assuming that each of the doc-2Coreference Chains for doc.O 1~ U:~ v;/; ihtY~\[r P ef:n syl: asnyi~'~ s m Core~nc~hai~for doc.02 ~\ [Cross-Document Coreference ChainsI II iIiJ t VSM- I Disambiguate~ summary.Ol \]I summary.nn IFigure 1: Architecture of the Cross-Document Coreference Systemuments in the data sets was about a single entity, orabout a single event, the cross-document coreferencechains produced by the system could now be evalu-ated by scoring the corresponding within-documentcoreference chains in the meta document.We used two different scoring algorithms for scor-ing the output.
The first was the standard algo-r ithm for within-document coreference chains whichwas used for the evaluation of the systems partic-ipating in the MUC-6 and the MUC-7 coreferencetasks.
This algorithm computes precision and recallstatistics by looking at the number of links identifiedby a system compared to the links in an answer key.The shortcomings of the MUC scoring algorithmwhen used for the cross-document coreference taskforced us to develop a second algorithm - the B-CUBED algorithm - which is described in detail be-low.
Full details about both these algorithms (in-cluding the shortcoming of the MUC scoring algo-rithm) can be found in (Bagga, 98).4.1.1 The  B -CUBED A lgor i thmFor an entity, i, we define the precision and recallwith respect o that entity in Figure 6.The final precision and recall numbers are com-puted by the following two formulae:NFinal Precision = ~ wi * Precisionii=lN= ~ wi * Recalli Final Recalli----1where N is the number of entities in the document,and wi is the weight assigned to entity i in the docu-ment.
For the results discussed in this paper, equalweights were assigned to each entity in the meta doc-ument.
In other words, wi = -~ for all i.5 Cross -Document  Coreference forEventsIn order to extend our systems, as described ear-lier, so that it was able to handle events, we neededt o figure out a method to capture all the informa-tion about an event in a document.
Previously, withnamed entities, it was possible to use the within-document coreference chain regarding the entity toextract a "summary" with respect to that entity.However, since CAMP does not annotate within-document coreference chains for events, it was notpossible to use the same approach.The updated version of the system builds "sum-maries" with respect o the event of interest by ex-tracting all the sentences in the article that containeither the verb describing the event or one of itsnominalizations.
Currently, sentences that containsynonyms of the verb are not extracted.
However',we did conduct an experiment (described later in thepaper) where the system extracted sentences con-3taining one of three pre-specified synonyms to theverb.The new version of the system was tested on sev-eral data sets.5.1 Ana lys i s  o f  DataFigure 7 gives some insight into the data sets usedfor the experiments described later in the paper.
Inthe figure, Column 1 shows the number of articlesin the data set.
The second column shows the av-erage number of sentences in the summary for theentity/event of interest constructed for each article.Column 3 shows, for each summary, the averagenumber of words that were found in at least oneother summary (in the same data set).
The condi-tions when measuring the overlap should be notedhere:?
the summaries are filtered for stop words?
all within-document coreference chains passingthrough the summaries are expanded and theresulting additional noun phrases are attachedto the summariesThe fourth column shows for each such overlappingword, the average number of summaries (in the samedata set) that it is found in.
Column 5 which is theproduct of the numbers in Columns 3 and 4 shows,for each summary, the average number of summaries,in the data set, it shares a word with (the amount ofoverlap).
We hypothesize here that the higher theamount of overlap, the higher is the ambiguity in thedomain.
We will return to this hypothesis later inthe paper.Figure 7 shows that the "resign" and the "espi-onage" data sets are remarkably similar.
They havevery similar numbers for the number of sentences persummary, the average number of overlapping wordsper summary, and the average number of summariesthat each of the overlapping words occur in.
A closerlook at several of the summaries from each data setyielded the following properties that the two datasets shared:?
The summaries usually consisted of a single sen-tence from the article.?
The "players" involved in the events (people,places, companies, positions, etc.)
were usuallyreferenced in the sentences which were in thesummaries.However, the "election" data set is very differentfrom the other two sets.
This data set has almosttwice as many sentences per summary (2.38).
Inaddition, the number of overlapping words in eachsummary is also comparatively high although theaverage number of summaries that an overlappingwords occurs in is similar to that of the other twodata sets.
But, "elections" has a very high overlap8,?
:oO.10090 \[80706050403020100Precision/Recall vs Threshold~.-o"\]~', MUC AIg: Precision --o--j ,,El.,, ' MUC AIg: Recall -~-- - "+ "'o MUC AIg: F-Measure -o--\ O.."o ."
,  "0  ""ET.
O .
.E \ ] .~k  "E\] --E}-- O-  .0 .
G. .E\] .
.
.0 .
0""-I,, "0I I I I I I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 8: Results for the "John Smith" data set us-ing the MUC scorer100908070g 6og_ 4o3020100Precision/Recall vs Threshold= '"*, Our AIg.~ Precision',,~, Our AIg: Recall -+---~ r  Ig."
F-~easum -o- --  f :il.i \" "'~"~"G--O-~ - O- -E \ ] - - (~ - El--E}- O.
DI I I I !
I !
I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 9: Results for the "John Smith" data set us-ing the B-CUBED scorernumber (22.41) which is about 30% more than theother data sets.
From our hypothesis it follows thatthis data set is comparatively much more ambiguous;a fact which is verified later in the paper.Assuming our hypothesis i true, the overlap num-ber also gives an indication of the optimal thresholdwhich, when chosen, will result in the best precisionand recall numbers for the data set.
It seems a rea-sonable conjecture that the optimal threshold variesinversely with the overlap number i.e.
the higher theoverlap number, the higher the ambiguity, and lowerthe optimal threshold.5.2 Exper iments  and Resu l tsWe tested our cross-document coreference system onseveral data sets.
The goal was to identify cross-document coreference chains about the same event.Figures 8 - 15 shows the results from the ex-periments we conducted.
For each experiment con-ducted, the following conditions hold:4E (D ,o,?_.Precision/Recall vs Threshold100 I I ~ .
, , x ^ ?
^ , , ," .
M'~'b ,~lg~Pr~cisio~"~ %N~N / 9O I- m'~('~'~B"r~.
MUG AIg: Recall -+---80 L .,",v/~ ""w,'?
"B._ MUC AIg: F-Measure -o--.L.. 7 ",,.
-% 70~ / ~'-, 19.
/ ~ ", "s - .B .60 I-7 ~',, '~-~.~.
'E~'13.. ~ .
"4" ---1% "B.
"4~ .+.
"O .
.O"k"  "1" - .,+.
,,{..50403020100 I I I I I I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thresho ldFigure 10: Results for the "resign" data set usingthe MUC scorer1009080706050403020100(Df l .Precision/Recall vs Threshold"'*, ~ Our AIg: Precision",~,'B--B.~ Our Ale: Recall -+---m:7 "'~, ='~'B.
.~.
Our AIg: F-~Aeasure -B--.
, , ' ?  "'
i"'"~..,.W.
~ \ [ \ ]  ID -O- - i~- .
.O ._G.
.E \ ] .
, .E} .
(3 .
.E  )I I I I !
I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thresho ldFigure 11: Results for the "resign" data set usingthe B-CUBED scorer?
Figure 7 shows, for each data set, the numberof articles chosen for the experiment.?
All of the articles in the data sets were chosenrandomly from the 1996 and 1997 editions of theNew York Times.
The sole criterion used whenchoosing an article was the presence/ absenceof the event of interest in the data set.
For ex-ample, an article containing the word "election"would be put in the elections data set.?
The answer keys for each data set were con-structed manually, although scoring was auto-mated.Figure 16 shows for each data set, the optimalthreshold, and the best precision, recall, and F-Measure obtained at that threshold.5.3  Ana lys i s  o f  Resu l tsWe had mentioned earlier that we expected the opti-mal threshold value to vary inversely with the over-c a)o $1009080706050403020100Prec is ion /Reca l l  vs  Thresho ld?
, , ~ : ; v ;.
: ; : ; : ; :\[", f MUC AIg: Precision ~ ol '~- / MUC AIg: Recall -~-.I - "  A /  MUC AIg: F-Measure "B'-L,i ,",?..a0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thresho ldFigure 12: Results for the "elections" data set usingthe MUC scorerf::(1) p1009080706050403020100Precision/Recall vs Thresho ld, , ~ : ; : ; .
; :  ; : ; o ?o".
', /~ Our AIg: Precision', ~ / Our  AIg: Recall -+--- i / ' ~  Our AIg: F-Measure -B---, , "+- -~-~:~- ' - '~- ' - -~  .
.
.
.
.
,~ m ,~0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thresho ldFigure 13: Results for the "elections" data set usingthe B-CUBED scorerlap number.
Figure 16 verifies this - the optimalthresholds decline for the "espionage", resign", andthe "election" data sets (which have increasing over-lap numbers).
In addition, the results for the "elec-tion" data set alo verify our hypothesis that datasets with large overlap numbers are more ambiguous.There are several different factors which can affectthe performance of the system.
We describe some ofthe more important ones below.expans ion  of  core ference  cha ins :  Expandingthe coreference chains that pass throughthe sentences contained in a summary andappending the coreferent noun phrases to thesummary results in approximately a 5 pointincrease in F-Measure for each data set.use  o f  synonyms:  For the "election" data set, theuse of three synonyms (poll, vote, and cam-paign) to extract additional sentences for thesummaries helped in increasing the performance5(D?1) o#.10090807060504030201000Precision/Recall vs Threshold2 :  MUC AIg: Precision -e.--/~-.s..~ MUC AIg: Recall -+---"--GI(."
~ - "u'-e~ MUC AIg: F-Measure -s--J .
,?
.
_ .
.
i .
i .  "
.
.
.
.
.
"'4, E\] \[\] ID O--O..{D..O..13...O..O.. 0I I I I I I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 14: Results for the "espionage" data set usingthe MUC scorerPrecision/Recall vs Threshold 10o90 ~- " ~: Our AIg: PrecisionI '~ \] Our AIg."
Recall -~--80 ~- ~ , .
~ l g :  F-Measure -\[\]--30201000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 15: Results for the "espionage" data set usingthe B-CUBED scorerof the system by 3 F-measure points.
The re-sulting increase in performance implies that thesentences containing the term "election" did notcontain sufficient information for disambiguat-ing all the elections.
Some of the disambigua-tion information (example: the "players" in-volved in the event) was mentioned in the ad-ditional sentences.
This also strengthens ourobservation that this data set is more compar-atively more ambiguous.p resence  of  a single, large coreference chain:The presence of a single, large cross-documentcoreference chain in the test set affects theperformance of a system with respect tothe scoring algorithm used.
For example,the "election" data set consisted of a verylarge coreference chain - the coreference chainconsisting of articles regarding the 1996 USGeneral (Congressional and Presidential)elections.
This chain consisted of 36 of the 73links in the data set.
The B-CUBED algorithmpenalizes systems severely for precision andrecall errors in such a scenario.
The differencein the results reported by the two scoringalgorithms for this data set is glaring.
TheMUC scorer reports a 71 point F-Measurewhile the B-CUBED scorer reports only a 43point F-Measure.5.4 The "election" Data SetSince the results for the "election" data set weresignificantly lower than other results, we decided toanalyze this data set in more detail.
The followingfactors makes this data set harder to deal with:p resence  of  sub-events:  The presence of sub-events that correspond to a single event makesthe task harder.
The "election" data set of-ten mentioned election events which consistedof more than one actual election.
For example,the data set contained articles which mentionedthe 1996 US General Elections which comprisedof the US Congressional elections and the USPresidential elections.
In addition, there werearticles which only mentioned the sub-electionswithout mentioning the 'more general event.
"p layers"  are the same: Elections is one eventwhere the players involved are often the same.For example, elections are about the same po-sitions, in the same places, and very often in-volving the same people making the task veryambiguous.
Very often the only disambiguatingfactor is the year (temporal information) of theelection and this too has to be inferred.
For ex-ample, articles will mention an election in thefollowing ways: "the upcoming November elec-tions," "next years elections," last fall's elec-tions," etc.descriptions are very  s imi lar:  Another very im-portant factor that makes the "elections" taskharder is the fact that most election issues(across elections in different countries) are verysimilar.
For example: crime rates, inflation, un-employment, etc.6 In terannotator  AgreementWhen comparing machine performance against a hu-man annotation, it is important o understand howconsistently two humans can perform the same task.If people cannot replicate one other, then there maybe serious problems with the task definition thatquestion the wisdom of developing automated meth-ods for the task.Both authors independently annotated the "elec-tions" data set with no agreed upon annotation stan-dard in contrast o how data sets were annotatedin the MUC-6/7 coreference task.
Instead, we used6whatever mutual understanding we had on what thegoal of our annotation was from phone calls over thecourse of a few months.
We did not develop an an-notation standard because we have not considered asufficiently broad range of events to write down nec-essary and sufficient conditions for event coreference.For now our understanding is:Any two events are in the same equivalence classif they are of the same generic class, ie "elections"or "resignations", and the principle actors, entities,and times are the same.This definition does not cover the specificity ofevent descriptions, i.e.
the difference between thegeneral November 96 elections and a particular elec-tion in a district (at the same time).
We left thisdecision up to human judgment rather than tryingto codify the decision at this early stage.Interannotator agreement was evaluated in twophases, a completely independent phase and a con-sensus phase where we compared annotations andcorrected obvious errors and attentional lapses butallowed differences of opinion when there was roomfor judgment.
The results for the completely in-dependent annotation were 87% precision and 87%recall as determined by treating one annotation astruth and the other as a systems output with theMUC scorer.
Perfect agreement between the annota-tors would result in 100% precision and recall.
Theseresults are quite high given the lack of a clear anno-tation standard in combination with the ambiguityof the task.After adjudication, the agreement increased sig-nificantly to 95% precision and recall which indi-cates that there was genuine disagreement for 5% ofthe links found across two annotators.
Using the B-CUBED scorer the results were 80% for the indepen-dent case and 93% for the consensus phase.
Thesefigures establish an upper bound on possible ma-chine performance and suggest hat cross documentevent coreference is a fairly natural phenomenon forpeople to recognize.7 Future  ResearchThe goal of this research as been to gain experiencein cross document reference across a range of enti-ties/events.
We have focused on simple techniques(the vector space model) over rich data structures(within document coreference annotated text) as ameans to better understanding of where to furtherexplore the phenomenon.It is worth exploring alternatives to the vectorspace model since there are areas where it couldbe improved.
One possibility would be to explic-itly identify the individuating factors of events, i.e.the "players" of an event, and then individuate bycomparing these factors.
This would be particularlyhelpful when there is only one individuating factorlike a date that differentiates two events.The benefit of cross document entity referencecenters around nove.1 interfaces to large data collec-tions, so we are focusing on potential applicationsthat include link visualization (Bagga, 98c), questionanswering, and multi-document summarization.8 Conc lus ionsWe have shown that it is possible to extend ourearlier work with cross document person referenceto include cross document event reference.
This isachieved by using the vector space model to formequivalence classes of "summaries" about the eventsin question.
These summaries are generated by in-cluding sentences that have coreference into the coreevent sentence as well as sentences that fit within asynonymy class for the event in question.
Our resultsare encouraging with performance ranging from 45%f-score to 90% f-score.
We also have established thathuman annotators agree on cross document eventreference around 95% of the time.ReferencesBagga, Amit, and Breck Baldwin.
Algorithms forScoring Coreference Chains.
Proceedings of theLinguistic Coreference Workshop at The FirstInternational Conference on Language Resourcesand Evaluation, May 1998.Bagga, Amit, and Breck Baldwin.
How Much Pro-cessing is Required for Cross-Document Coref-erence?
Proceedings of the Linguistic Corefer-ence Workshop at The First International Confer-ence on Language Resources and Evaluation, May1998.Bagga, Amit, and Breck Baldwin.
Entity-BasedCross-Document Coreferencing Using the VectorSpace Model.
In Proceedings of the 36th An-nual Meeting of the Association for Computa-tional Linguistics and the 17th International Con-ferende on Computational Linguistics (COLING-ACL'98), pp.
79-85, August 1998.Bagga, Amit, and Breck Baldwin.
Coreference asthe Foundations for Link Analysis Over FreeText Databases.
In Proceedings of the COLING-A CL'98 Content Visualization and IntermediaRepresentations Workshop (CVIR'98), pp.
19-24,August 1998.Grishman, Ralph.
Whither Written Language Eval-uation?, Proceedings of the Human LanguageTechnology Workshop, pp.
120-125, March 1994,San Francisco: Morgan Kaufmann.Salton, Gerard.
AUtomatic Text Processing: TheTransformation, Analysis, and Retrieval of In-formation by Computer, 1989, Reading, MA:Addison-Wesley.number of correct elements in the output chain containing entity~Precisioni =Recalli =number of elements in the output chain containing entityinumber of correct elements in the output chain containing entityinumber of elements in the truth chain containing entity~Figure 6: Definitions for Precision and Recall for an Entity idata setJohn Smith# ofarticles197avg # ofsentencesper summary1.16avg # ofoverlappingwords in summary2.46avg # of summariesthat overlappingwords occur in5.74amount ofoverlapper summary14.13resign 219 1.35 4.35 3.99 17.36elections 135 2.38 5.66 3.96 22.411.28 espionage 3.62 184 4.57 16.54Figure 7: Analysis of the Data SetsData SetJohn SmithresignelectionsespionageScorerMUCB-CUBEDMUCB-CUBEDMUCB-CUBEDMUCB-CUBEDOptimal Threshold0.150.150.200.200.080.100.250.25F-Measure8884.69088.271438682Precision Recall98 8093.3 77.392 8889.6 86.871 7150 3796 7796 71Figure 16: Analysis of the Data Sets8
