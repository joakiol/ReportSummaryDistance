Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 657?665,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsHead-driven Transition-based Parsing with Top-down PredictionKatsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Institute of Science and TechnologyIkoma, Nara, 630-0192, Japan?National Institute of Information and Communications TechnologySorakugun, Kyoto, 619-0289, Japan?National Institute for Japanese Language and LinguisticsTachikawa, Tokyo, 190-8561, Japankatsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jpmasayu-a@ninjal.ac.jp, matsu@is.naist.jpAbstractThis paper presents a novel top-down head-driven parsing algorithm for data-driven pro-jective dependency analysis.
This algorithmhandles global structures, such as clause andcoordination, better than shift-reduce or otherbottom-up algorithms.
Experiments on theEnglish Penn Treebank data and the ChineseCoNLL-06 data show that the proposed algo-rithm achieves comparable results with otherdata-driven dependency parsing algorithms.1 IntroductionTransition-based parsing algorithms, such as shift-reduce algorithms (Nivre, 2004; Zhang and Clark,2008), are widely used for dependency analysis be-cause of the efficiency and comparatively good per-formance.
However, these parsers have one majorproblem that they can handle only local information.Isozaki et al (2004) pointed out that the drawbacksof shift-reduce parser could be resolved by incorpo-rating top-down information such as root finding.This work presents an O(n2) top-down head-driven transition-based parsing algorithm which canparse complex structures that are not trivial for shift-reduce parsers.
The deductive system is very similarto Earley parsing (Earley, 1970).
The Earley predic-tion is tied to a particular grammar rule, but the pro-posed algorithm is data-driven, following the currenttrends of dependency parsing (Nivre, 2006; McDon-ald and Pereira, 2006; Koo et al, 2010).
To do theprediction without any grammar rules, we introducea weighted prediction that is to predict lower nodesfrom higher nodes with a statistical model.To improve parsing flexibility in deterministicparsing, our top-down parser uses beam search al-gorithm with dynamic programming (Huang andSagae, 2010).
The complexity becomes O(n2 ?
b)where b is the beam size.
To reduce prediction er-rors, we propose a lookahead technique based on aFIRST function, inspired by the LL(1) parser (Ahoand Ullman, 1972).
Experimental results show thatthe proposed top-down parser achieves competitiveresults with other data-driven parsing algorithms.2 Definition of Dependency GraphA dependency graph is defined as follows.Definition 2.1 (Dependency Graph) Given an in-put sentence W = n0 .
.
.nn where n0 is a spe-cial root node $, a directed graph is defined asGW = (VW , AW ) where VW = {0, 1, .
.
.
, n} is aset of (indices of) nodes and AW ?
VW ?
VW is aset of directed arcs.
The set of arcs is a set of pairs(x, y) where x is a head and y is a dependent of x.x ??
l denotes a path from x to l. A directed graphGW = (VW , AW ) is well-formed if and only if:?
There is no node x such that (x, 0) ?
AW .?
If (x, y) ?
AW then there is no node x?
suchthat (x?, y) ?
AW and x?
?= x.?
There is no subset of arcs {(x0, x1), (x1, x2),.
.
.
, (xl?1, xl)} ?
AW such that x0 = xl.These conditions are refered to ROOT, SINGLE-HEAD, and ACYCLICITY, and we call an well-formed directed graph as a dependency graph.Definition 2.2 (PROJECTIVITY) A dependencygraph GW = (VW , AW ) is projective if and only if,657input: W = n0 .
.
.nnaxiom(p0): 0 : ?1, 0, n + 1,n0?
: ?predx:state p?
??
??
: ?i, h, j, sd|...|s0?
:?
+ 1 : ?i, k, h, sd?1|...|s0|nk?
: {p}?k : i ?
k < hpredy:state p?
??
??
: ?i, h, j, sd|...|s0?
:?
+ 1 : ?i, k, j, sd?1|...|s0|nk?
: {p}?k : i ?
k < j ?
h < iscan:?
: ?i, h, j, sd|...|s0?
: pi?
+ 1 : ?i + 1, h, j, sd|...|s0?
: pii = hcomp:state q?
??
?
: ?
, h?, j?, s?d|...|s?0?
: pi?state p?
??
??
: ?i, h, j, sd|...|s0?
: pi?
+ 1 : ?i, h?, j?, s?d|...|s?1|s?0ys0?
: pi?q ?
pi, h < igoal: 3n : ?n + 1, 0, n + 1, s0?
: ?Figure 1: The non-weighted deductive system of top-down dependency parsing algorithm: means ?take anything?.for every arc (x, y) ?
AW and node l in x < l < yor y < l < x, there is a path x ??
l or y ??
l.The proposed algorithm in this paper is for projec-tive dependency graphs.
If a projective dependencygraph is connected, we call it a dependency tree,and if not, a dependency forest.3 Top-down Parsing AlgorithmOur proposed algorithm is a transition-based algo-rithm, which uses stack and queue data structures.This algorithm formally uses the following state:?
: ?i, h, j, S?
: piwhere ?
is a step size, S is a stack of trees sd|...|s0where s0 is a top tree and d is a window size forfeature extraction, i is an index of node on the topof the input node queue, h is an index of root nodeof s0, j is an index to indicate the right limit (j ?1 inclusive) of predy, and pi is a set of pointers topredictor states, which are states just before puttingthe node in h onto stack S. In the deterministic case,pi is a singleton set except for the initial state.This algorithm has four actions, predictx(predx),predicty(predy), scan and complete(comp).
Thedeductive system of the top-down algorithm isshown in Figure 1.
The initial state p0 is a state ini-tialized by an artificial root node n0.
This algorithmapplies one action to each state selected from appli-cable actions in each step.
Each of three kinds ofactions, pred, scan, and comp, occurs n times, andthis system takes 3n steps for a complete analysis.Action predx puts nk onto stack S selected fromthe input queue in the range, i ?
k < h, which isto the left of the root nh in the stack top.
Similarly,action predy puts a node nk onto stack S selectedfrom the input queue in the range, h < i ?
k < j,which is to the right of the root nh in the stack top.The node ni on the top of the queue is scanned if itis equal to the root node nh in the stack top.
Actioncomp creates a directed arc (h?, h) from the root h?of s?0 on a predictor state q to the root h of s0 on acurrent state p if h < i 1.The precondition i < h of action predx meansthat the input nodes in i ?
k < h have not beenpredicted yet.
Predx, scan and predy do not con-flict with each other since their preconditions i < h,i = h and h < i do not hold at the same time.However, this algorithm faces a predy-comp con-flict because both actions share the same precondi-tion h < i, which means that the input nodes in1 ?
k ?
h have been predicted and scanned.
This1In a single root tree, the special root symbol $0 has exactlyone child node.
Therefore, we do not apply comp action to astate if its condition satisfies s1.h = n0 ?
?
?= 3n?
1.658step state stack queue action state information0 p0 $0 I1 saw2 a3 girl4 ?
?1, 0, 5?
: ?1 p1 $0|saw2 I1 saw2 a3 girl4 predy ?1, 2, 5?
: {p0}2 p2 saw2|I1 I1 saw2 a3 girl4 predx ?1, 1, 2?
: {p1}3 p3 saw2|I1 saw2 a3 girl4 scan ?2, 1, 2?
: {p1}4 p4 $0|I1xsaw2 saw2 a3 girl4 comp ?2, 2, 5?
: {p0}5 p5 $0|I1xsaw2 a3 girl4 scan ?3, 2, 5?
: {p0}6 p6 I1xsaw2|girl4 a3 girl4 predy ?3, 4, 5?
: {p5}7 p7 girl4|a3 a3 girl4 predx ?3, 3, 4?
: {p6}8 p8 girl4|a3 girl4 scan ?4, 3, 4?
: {p6}9 p9 I1xsaw2|a3xgirl4 girl4 comp ?4, 4, 5?
: {p5}10 p10 I1xsaw2|a3xgirl4 scan ?5, 4, 5?
: {p5}11 p11 $0|I1xsaw2ygirl4 comp ?5, 2, 5?
: {p0}12 p12 $0ysaw2 comp ?5, 0, 5?
: ?Figure 2: Stages of the top-down deterministic parsing process for a sentence ?I saw a girl?.
We follow a conventionand write the stack with its topmost element to the right, and the queue with its first element to the left.
In this example,we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1.parser constructs left and right children of a headnode in a left-to-right direction by scanning the headnode prior to its right children.
Figure 2 shows anexample for parsing a sentence ?I saw a girl?.4 CorrectnessTo prove the correctness of the system in Figure1 for the projective dependency graph, we use theproof strategy of (Nivre, 2008a).
The correct deduc-tive system is both sound and complete.Theorem 4.1 The deductive system in Figure 1 iscorrect for the class of dependency forest.Proof 4.1 To show soundness, we show that Gp0 =(VW , ?
), which is a directed graph defined by theaxiom, is well-formed and projective, and that everytransition preserves this property.?
ROOT: The node 0 is a root in Gp0 , and thenode 0 is on the top of stack of p0.
The two predactions put a word onto the top of stack, andpredict an arc from root or its descendant tothe child.
The comp actions add the predictedarcs which include no arc of (x, 0).?
SINGLE-HEAD: Gp0 is single-head.
A nodey is no longer in stack and queue after a compaction creates an arc (x, y).
The node y cannotmake any arc (x?, y) after the removal.?
ACYCLICITY: Gp0 is acyclic.
A cycle is cre-ated only if an arc (x, y) is added when thereis a directed path y ??
x.
The node x is nolonger in stack and queue when the directedpath y ??
x was made by adding an arc (l, x).There is no chance to add the arc (x, y) on thedirected path y ??
x.?
PROJECTIVITY: Gp0 is projective.
Projec-tivity is violated by adding an arc (x, y) whenthere is a node l in x < l < y or y < l < xwith the path to or from the outside of the spanx and y.
When predy creates an arc relationfrom x to y, the node y cannot be scanned be-fore all nodes l in x < l < y are scanned andcompleted.
When predx creates an arc rela-tion from x to y, the node y cannot be scannedbefore all nodes k in k < y are scanned andcompleted, and the node x cannot be scannedbefore all nodes l in y < l < x are scannedand completed.
In those processes, the node lin x < l < y or y < l < x does not make apath to or from the outside of the span x and y,and a path x ??
l or y ??
l is created.
2To show completeness, we show that for any sen-tence W , and dependency forest GW = (VW , AW ),there is a transition sequence C0,m such that Gpm =GW by an inductive method.?
If |W | = 1, the projective dependency graphfor W is GW = ({0}, ?)
and Gp0 = GW .?
Assume that the claim holds for sentences withlength less or equal to t, and assume that|W | = t + 1 and GW = (VW , AW ).
The sub-graph GW ?
is defined as (VW ?
t, A?t) where659...s2 h... .
.. ....s1 h.. .
... .
.
...s1.l.. .
.
... .
... .
.
...s1.r.
.. .
....s0 h.. .
... .
.
...s0.l.. .
.
... .
... .
.
...s0.r.
.. .
.Figure 3: Feature window of trees on stack S: The win-dow size d is set to 2.
Each x.h, x.l and x.r denotes root,left and right child nodes of a stack element x.A?t = AW ?
{(x, y)|x = t?
y = t}.
If GW isa dependency forest, then GW ?
is also a depen-dency forest.
It is obvious that there is a transi-tion sequence for constructing GW except arcswhich have a node t as a head or a dependent2.There is a state pq = q : ?i, x, t + 1?
:for i and x (0 ?
x < i < t + 1).
Whenx is the head of t, predy to t creates a statepq+1 = q + 1 : ?i, t, t + 1?
: {pq}.
At least onenode y in i ?
y < t becomes the dependent oft by predx and there is a transition sequencefor constructing a tree rooted by y.
After con-structing a subtree rooted by t and spaned fromi to t, t is scaned, and then comp creates anarc from x to t. It is obvious that the remainingtransition sequence exists.
Therefore, we canconstruct a transition sequence C0,m such thatGpm = GW .
2The deductive sysmtem in Figure 1 is both sound andcomplete.
Therefore, it is correct.
25 Weighted Parsing Model5.1 Stack-based ModelThe proposed algorithm employs a stack-basedmodel for scoring hypothesis.
The cost of the modelis defined as follows:cs(i, h, j, S) = ?s ?
fs,act(i, h, j, S) (1)where ?s is a weight vector, fs is a feature function,and act is one of the applicable actions to a state ?
:?i, h, j, S?
: pi.
We use a set of feature templates of(Huang and Sagae, 2010) for the model.
As shownin Figure 3, left children s0.l and s1.l of trees on2This transition sequence is defined for GW ?
, but it is pos-sible to be regarded as the definition for GW as long as thetransition sequence is indifferent from the node t.Algorithm 1 Top-down Parsing with Beam Search1: input W = n0, .
.
.
,nn2: start?
?1, 0, n + 1,n0?3: buf [0]?
{start}4: for ??
1 .
.
.
3n do5: hypo?
{}6: for each state in buf [??
1] do7: for act?applicableAct(state) do8: newstates?actor(act, state)9: addAll newstates to hypo10: add top b states to buf [?]
from hypo11: return best candidate from buf [3n]stack for extracting features are different from thoseof Huang and Sagae (2010) because in our parser theleft children are generated from left to right.As mentioned in Section 1, we apply beam searchand Huang and Sagae (2010)?s DP techniques toour top-down parser.
Algorithm 1 shows the ourbeam search algorithm in which top most b statesare preserved in a buffer buf [?]
in each step.
Inline 10 of Algorithm 1, equivalent states in the step?
are merged following the idea of DP.
Two states?i, h, j, S?
and ?i?, h?, j?, S??
in the step ?
are equiv-alent, notated ?i, h, j, S?
?
?i?, h?, j?, S?
?, ifffs,act(i, h, j, S) = fs,act(i?, h?, j?, S?).
(2)When two equivalent predicted states are merged,their predictor states in pi get combined.
For fur-ther details about this technique, readers may referto (Huang and Sagae, 2010).5.2 Weighted PredictionThe step 0 in Figure 2 shows an example of predic-tion for a head node ?$0?, where the node ?saw2?
isselected as its child node.
To select a probable childnode, we define a statistical model for the prediction.In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) whichdirectly models dependency links.
The cost of the1st-order model is defined as the relation between achild node c and a head node h:cp(h, c) = ?p ?
fp(h, c) (3)where ?p is a weight vector and fp is a features func-tion.
Using the cost cp, the top-down parser selectsa probable child node in each prediction step.When we apply beam search to the top-downparser, then we no longer use ?
but ?
on predx and660...h..l1 .
.. .
.
.
..ll .
..r1 .
.. .
.
.
..rmFigure 4: An example of tree structure: Each h, l and rdenotes head, left and right child nodes.predy in Figure 1.
Therefore, the parser may predictmany nodes as an appropriate child from a singlestate, causing many predicted states.
This may causethe beam buffer to be filled only with the states, andthese may exclude other states, such as scanned orcompleted states.
Thus, we limit the number of pre-dicted states from a single state by prediction sizeimplicitly in line 10 of Algorithm 1.To improve the prediction accuracy, we introducea more sophisticated model.
The cost of the sibling2nd-order model is defined as the relationship be-tween c, h and a sibling node sib:cp(h, sib, c) = ?p ?
fp(h, sib, c).
(4)The 1st- and sibling 2nd-order models are the sameas McDonald and Pereira (2006)?s definitions, ex-cept the cost factors of the sibling 2nd-order model.The cost factors for a tree structure in Figure 4 aredefined as follows:cp(h,?, l1) +l?1?y=1cp(h, ly, ly+1)+cp(h,?, r1) +m?1?y=1cp(h, ry, ry+1).This is different from McDonald and Pereira (2006)in that the cost factors for left children are calcu-lated from left to right, while those in McDonald andPereira (2006)?s definition are calculated from rightto left.
This is because our top-down parser gener-ates left children from left to right.
Note that thecost of weighted prediction model in this section isincrementally calculated by using only the informa-tion on the current state, thus the condition of statemerge in Equation 2 remains unchanged.5.3 Weighted Deductive SystemWe extend deductive system to a weighted one, andintroduce forward cost and inside cost (Stolcke,1995; Huang and Sagae, 2010).
The forward cost isthe total cost of a sequence from an initial state to theend state.
The inside cost is the cost of a top tree s0in stack S. We define these costs using a combina-tion of stack-based model and weighted predictionmodel.
The forward and inside costs of the combi-nation model are as follows:{cfw = cfws + cfwpcin = cins + cinp(5)where cfws and cins are a forward cost and an insidecost for stack-based model, and cfwp and cinp are a for-ward cost and an inside cost for weighted predictionmodel.
We add the following tuple of costs to a state:(cfws , cins , cfwp , cinp ).For each action, we define how to efficiently cal-culate the forward and inside costs3 , following Stol-cke (1995) and Huang and Sagae (2010)?s works.
Ineither case of predx or predy,(cfws , , cfwp , )(cfws + ?, 0, cfwp + cp(s0.h,nk), 0)where?
={?s ?
fs,predx(i, h, j, S) if predx?s ?
fs,predy(i, h, j, S) if predy(6)In the case of scan,(cfws , cins , cfwp , cinp )(cfws + ?, cins + ?, cfwp , cinp )where?
= ?s ?
fs,scan(i, h, j, S).
(7)In the case of comp,(c?fws , c?ins , c?fwp , c?inp ) (cfws , cins , cfwp , cinp )(c?fws + cins + ?, c?ins + cins + ?,c?fwp + cinp + cp(s?0.h, s0.h),c?inp + cinp + cp(s?0.h, s0.h))where?
= ?s ?
fs,comp(i, h, j, S) + ?s ?
fs,pred ( , h?, j?, S?).
(8)3For brevity, we present the formula not by 2nd-order modelas equation 4 but a 1st-order one for weighted prediction.661Pred takes either predx or predy.
Beam search isperformed based on the following linear order forthe two states p and p?
at the same step, which have(cfw, cin) and (c?fw, c?in) respectively:p ?
p?
iff cfw < c?fw or cfw = c?fw ?
cin < c?in.
(9)We prioritize the forward cost over the inside costsince forward cost pertains to longer action sequenceand is better suited to evaluate hypothesis states thaninside cost (Nederhof, 2003).5.4 FIRST Function for LookaheadTop-down backtrack parser usually reduces back-tracking by precomputing the set FIRST(?)
(Aho andUllman, 1972).
We define the set FIRST(?)
for ourtop-down dependency parser:FIRST(t?)
= {ld.t|ld ?
lmdescendant(Tree, t?
)Tree ?
Corpus} (10)where t?
is a POS-tag, Tree is a correct depen-dency tree which exists in Corpus, a functionlmdescendant(Tree, t?)
returns the set of the leftmostdescendant node ld of each nodes in Tree whosePOS-tag is t?, and ld.t denotes a POS-tag of ld.Though our parser does not backtrack, it looks aheadwhen selecting possible child nodes at the predictionstep by using the function FIRST.
In case of predx:?k : i ?
k < h ?
ni.t ?
FIRST(nk.t)state p?
??
??
: ?i, h, j, sd|...|s0?
:?
+ 1 : ?i, k, h, sd?1|...|s0|nk?
: {p}where ni.t is a POS-tag of the node ni on the top ofthe queue, and nk.t is a POS-tag in kth position ofan input nodes.
The case for predy is the same.
Ifthere are no nodes which satisfy the condition, ourtop-down parser creates new states for all nodes, andpushes them into hypo in line 9 of Algorithm 1.6 Time ComplexityOur proposed top-down algorithm has three kindsof actions which are scan, comp and predict.
Eachscan and comp actions occurs n times when parsinga sentence with the length n. Predict action also oc-curs n times in which a child node is selected froma node sequence in the input queue.
Thus, the algo-rithm takes the following times for prediction:n + (n?
1) + ?
?
?
+ 1 =n?ii = n(n + 1)2 .
(11)As n2 for prediction is the most dominant factor, thetime complexity of the algorithm is O(n2) and thatof the algorithm with beam search is O(n2 ?
b).7 Related WorkAlshawi (1996) proposed head automaton whichrecognizes an input sentence top-down.
Eisnerand Satta (1999) showed that there is a cubic-timeparsing algorithm on the formalism of the headautomaton grammars, which are equivalently con-verted into split-head bilexical context-free gram-mars (SBCFGs) (McAllester, 1999; Johnson, 2007).Although our proposed algorithm does not employthe formalism of SBCFGs, it creates left childrenbefore right children, implying that it does not havespurious ambiguities as well as parsing algorithmson the SBCFGs.
Head-corner parsing algorithm(Kay, 1989) creates dependency tree top-down, andin this our algorithm has similar spirit to it.Yamada and Matsumoto (2003) applied a shift-reduce algorithm to dependency analysis, which isknown as arc-standard transition-based algorithm(Nivre, 2004).
Nivre (2003) proposed anothertransition-based algorithm, known as arc-eager al-gorithm.
The arc-eager algorithm processes right-dependent top-down, but this does not involve theprediction of lower nodes from higher nodes.
There-fore, the arc-eager algorithm is a totally bottom-upalgorithm.
Zhang and Clark (2008) proposed a com-bination approach of the transition-based algorithmwith graph-based algorithm (McDonald and Pereira,2006), which is the same as our combination modelof stack-based and prediction models.8 ExperimentsExperiments were performed on the English PennTreebank data and the Chinese CoNLL-06 data.
Forthe English data, we split WSJ part of it into sections02-21 for training, section 22 for development andsection 23 for testing.
We used Yamada and Mat-sumoto (2003)?s head rules to convert phrase struc-ture to dependency structure.
For the Chinese data,662time accuracy complete rootMcDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 ?Koo10 (Koo and Collins, 2010) ?
93.04 ?
?Hayashi11 (Hayashi et al, 2011) 0.3 92.89 ?
?2nd-MST?
0.13 92.3 43.7 96.0Goldberg10 (Goldberg and Elhadad, 2010) ?
89.7 37.5 91.5Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) ?
91.3 41.7 ?Zhang08 (Sh beam 64) ?
91.4 41.8 ?Zhang08 (Sh+Graph beam 64) ?
92.1 45.4 ?Huang10 (beam+DP) 0.04 92.1 ?
?Huang10?
(beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1Zhang11 (beam 64) (Zhang and Nivre, 2011) ?
93.07 49.59 ?top-down?
(beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2top-down?
(beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6Table 1: Results for test data: Time measures the parsing time per sentence in seconds.
Accuracy is an unlabeledattachment score, complete is a sentence complete rate, and root is a correct root rate.
?
indicates our experiments.00.20.40.60.810  10  20  30  40  50  60  70parsingtime(cpu sec)length of input sentence"shift-reduce""2nd-mst""top-down"Figure 5: Scatter plot of parsing time against sentencelength, comparing with top-down, 2nd-MST and shift-reduce parsers (beam size: 8, pred size: 5)we used the information of words and fine-grainedPOS-tags for features.
We also implemented and ex-perimented Huang and Sagae (2010)?s arc-standardshift-reduce parser.
For the 2nd-order Eisner-Sattaalgorithm, we used MSTParser (McDonald, 2012).We used an early update version of averaged per-ceptron algorithm (Collins and Roark, 2004) fortraining of shift-reduce and top-down parsers.
Aset of feature templates in (Huang and Sagae, 2010)were used for the stack-based model, and a set offeature templates in (McDonald and Pereira, 2006)were used for the 2nd-order prediction model.
Theweighted prediction and stack-based models of top-down parser were jointly trained.8.1 Results for English DataDuring training, we fixed the prediction size andbeam size to 5 and 16, respectively, judged by pre-accuracy complete rootoracle (sh+mst) 94.3 52.3 97.7oracle (top+sh) 94.2 51.7 97.6oracle (top+mst) 93.8 50.7 97.1oracle (top+sh+mst) 94.9 55.3 98.1Table 2: Oracle score, choosing the highest accuracyparse for each sentence on test data from results of top-down (beam 8, pred 5) and shift-reduce (beam 8) andMST(2nd) parsers in Table 1.accuracy complete roottop-down (beam:8, pred:5) 90.9 80.4 93.0shift-reduce (beam:8) 90.8 77.6 93.52nd-MST 91.4 79.3 94.2oracle (sh+mst) 94.0 85.1 95.9oracle (top+sh) 93.8 84.0 95.6oracle (top+mst) 93.6 84.2 95.3oracle (top+sh+mst) 94.7 86.5 96.3Table 3: Results for Chinese Data (CoNLL-06)liminary experiments on development data.
After25 iterations of perceptron training, we achieved92.94 unlabeled accuracy for top-down parser withthe FIRST function and 93.01 unlabeled accuracyfor shift-reduce parser on development data by set-ting the beam size to 8 for both parsers and the pre-diction size to 5 in top-down parser.
These trainedmodels were used for the following testing.We compared top-down parsing algorithm withother data-driven parsing algorithms in Table 1.Top-down parser achieved comparable unlabeled ac-curacy with others, and outperformed them on thesentence complete rate.
On the other hand, top-down parser was less accurate than shift-reduce663No.717 Little Lily , as Ms. Cunningham calls7 herself in the book , really was14 n?t ordinary .shift-reduce 2 7 2 2 6 4 14 7 7 11 9 7 14 0 14 14 142nd-MST 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14top-down 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14correct 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14No.127 resin , used to make garbage bags , milk jugs , housewares , toys and meat packaging25 , among other items .shift-reduce 25 9 9 13 11 15 13 25 18 25 25 25 25 25 25 25 7 25 25 29 27 42nd-MST 29 9 9 13 11 15 13 29 18 29 29 29 29 25 25 25 29 25 25 29 7 4top-down 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4correct 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4Table 4: Two examples on which top-down parser is superior to two bottom-up parsers: In correct analysis, the boxedportion is the head of the underlined portion.
Bottom-up parsers often mistake to capture the relation.parser on the correct root measure.
In step 0, top-down parser predicts a child node, a root node ofa complete tree, using little syntactic information,which may lead to errors in the root node selection.Therefore, we think that it is important to seek moresuitable features for the prediction in future work.Figure 5 presents the parsing time against sen-tence length.
Our proposed top-down parser is the-oretically slower than shift-reduce parser and Fig-ure 5 empirically indicates the trends.
The domi-nant factor comes from the score calculation, andwe will leave it for future work.
Table 2 showsthe oracle score for test data, which is the scoreof the highest accuracy parse selected for each sen-tence from results of several parsers.
This indicatesthat the parses produced by each parser are differ-ent from each other.
However, the gains obtained bythe combination of top-down and 2nd-MST parsersare smaller than other combinations.
This is becausetop-down parser uses the same features as 2nd-MSTparser, and these are more effective than those ofstack-based model.
It is worth noting that as shownin Figure 5, our O(n2?b) (b = 8) top-down parser ismuch faster than O(n3) Eisner-Satta CKY parsing.8.2 Results for Chinese Data (CoNLL-06)We also experimented on the Chinese data.
Fol-lowing English experiments, shift-reduce parser wastrained by setting beam size to 16, and top-downparser was trained with the beam size and the predic-tion size to 16 and 5, respectively.
Table 3 shows theresults on the Chinese test data when setting beamsize to 8 for both parsers and prediction size to 5 intop-down parser.
The trends of the results are almostthe same as those of the English results.8.3 Analysis of ResultsTable 4 shows two interesting results, on which top-down parser is superior to either shift-reduce parseror 2nd-MST parser.
The sentence No.717 containsan adverbial clause structure between the subjectand the main verb.
Top-down parser is able to han-dle the long-distance dependency while shift-reudceparser cannot correctly analyze it.
The effectivenesson the clause structures implies that our head-drivenparser may handle non-projective structures well,which are introduced by Johansonn?s head rule (Jo-hansson and Nugues, 2007).
The sentence No.127contains a coordination structure, which it is diffi-cult for bottom-up parsers to handle, but, top-downparser handles it well because its top-down predic-tion globally captures the coordination.9 ConclusionThis paper presents a novel head-driven parsing al-gorithm and empirically shows that it is as practi-cal as other dependency parsing algorithms.
Ourhead-driven parser has potential for handling non-projective structures better than other non-projectivedependency algorithms (McDonald et al, 2005; At-tardi, 2006; Nivre, 2008b; Koo et al, 2010).
We arein the process of extending our head-driven parserfor non-projective structures as our future work.AcknowledgmentsWe would like to thank Kevin Duh for his helpfulcomments and to the anonymous reviewers for giv-ing valuable comments.664ReferencesA.
V. Aho and J. D. Ullman.
1972.
The Theory of Pars-ing, Translation and Compiling, volume 1: Parsing.Prentice-Hall.H.
Alshawi.
1996.
Head automata for speech translation.In Proc.
the ICSLP.G.
Attardi.
2006.
Experiments with a multilanguagenon-projective dependency parser.
In Proc.
the 10thCoNLL, pages 166?170.M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proc.
the 42nd ACL.J.
Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the Association for Com-puting Machinery, 13(2):94?102.J.
M. Eisner and G. Satta.
1999.
Efficient parsing forbilexical context-free grammars and head automatongrammars.
In Proc.
the 37th ACL, pages 457?464.Y.
Goldberg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In Proc.
the HLT-NAACL, pages 742?750.K.
Hayashi, T. Watanabe, M. Asahara, and Y. Mat-sumoto.
2011.
The third-order variational rerank-ing on packed-shared dependency forests.
In Proc.EMNLP, pages 1479?1488.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proc.
the 48thACL, pages 1077?1086.H.
Isozaki, H. Kazawa, and T. Hirao.
2004.
A determin-istic word dependency analyzer enhanced with prefer-ence learning.
In Proc.
the 21st COLING, pages 275?281.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for english.
InProc.
NODALIDA.M.
Johnson.
2007.
Transforming projective bilexicaldependency grammars into efficiently-parsable CFGswith unfold-fold.
In Proc.
the 45th ACL, pages 168?175.M.
Kay.
1989.
Head driven parsing.
In Proc.
the IWPT.K.
Kitagawa and K. Tanaka-Ishii.
2010.
Tree-based de-terministic dependency parsing ?
an application tonivre?s method ?.
In Proc.
the 48th ACL 2010 ShortPapers, pages 189?193, July.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
the 48th ACL, pages 1?11.T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In Proc.
EMNLP, pages1288?1298.D.
McAllester.
1999.
A reformulation of eisner andsatta?s cubic time parser for split head automata gram-mars.
http://ttic.uchicago.edu/ dmcallester/.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.EACL, pages 81?88.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
HLT-EMNLP, pages 523?530.R.
McDonald.
2012.
Minimum spanning tree parser.http://www.seas.upenn.edu/ strctlrn/MSTParser.M.-J.
Nederhof.
2003.
Weighted deductive parsingand knuth?s algorithm.
Computational Linguistics,29:135?143.J.
Nivre.
2003.
An efficient algorithm for projective de-pendency parsing.
In Proc.
the IWPT, pages 149?160.J.
Nivre.
2004.
Incrementality in deterministic depen-dency parsing.
In Proc.
the ACL Workshop Incremen-tal Parsing: Bringing Engineering and Cognition To-gether, pages 50?57.J.
Nivre.
2006.
Inductive Dependency Parsing.
Springer.J.
Nivre.
2008a.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34:513?553.J.
Nivre.
2008b.
Sorting out dependency parsing.
InProc.
the CoTAL, pages 16?27.A.
Stolcke.
1995.
An efficient probabilistic context-freeparsing algorithm that computes prefix probabilities.Computational Linguistics, 21(2):165?201.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.the IWPT, pages 195?206.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: In-vestigating and combining graph-based and transition-based dependency parsing using beam-search.
InProc.
EMNLP, pages 562?571.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Proc.the 49th ACL, pages 188?193.665
