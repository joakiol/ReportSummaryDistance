Data-Or iented  Trans lat ionArjen PoutsmaDepartment ofComputational LinguisticsUniversity of Amsterdamthe Nether landspout sma@wins, uva.
nlAbstractIn this allicle, we present a statistical approach tomachine translation that is based on Data-OrientedParsing: l)ata-Oriented Translation (DOT).
In DOT,we use linked subtree lmirs for creating a derivationof a source sentence.
Each linked subhee pair has acertain probability, and consists of two trees: onein the source language and one in the target lan-guage.
When a derbation has been formed withthese subtree pairs, we can create a translation fromthis deriwition.
Since there are typically many dif-ferent derivations of tile same sentence in the sourcelanguage, there can be as many dilTemnt ranslationsfor it.
The probability of a translation can be calcu-lated as the total probability of all tile derivationsthat form this translation.
We give the computa-tional aspects for Ibis model, show tlmt we can con-vert each subtree imir into a productive rewrite rule,and that tile most probable translation can be com-imted by means of Monte Carlo disambiguation.
H-nally, we discuss some pilot experiments with theVerbmobil COl\]mS.1 IntroductionThe Data-Oriented Parsing model has been pre-sented as a promising paradigm for natural hmguageprocessing (Scha, 1990; Bod, 1995; Bod, 1998).It has been shown that DOP has the ability to lo-cate syntactic and semantic dependencies, both ofwhich are quite important for machine translation.We hope that, by basing our model on DOP, we caninherit these advantages, thus obtaining a new andinteresting way to perform machine translation.In section 2, we describe this novel model byidentifying its parameters, in section 3, we describeits comlmtational spects; in section 4, we discusssome pilot experiments with this model; and finally,in section 5, we give some issues open for filtumresearch.2 The Data-Oriented 35ranslation ModelIn this section, we will give the instantiation of amodel that uses DOP for MT purposes, which wewill call Data-Oriented Translation (DOT).
t Thismodel is largely based on DOPI (Bod, 1998, chapt.2).In DOT, we use linked subtree pairs as combi-national flagments, a Each linked subtree pair hasa certain probability, and consists of a trec in thesource language and a tree in the target language.By combining these fragments to form an an analy-sis of the soume sentence, we automatically gener-ate a translation, i.e.
we form a derivation of bothsource sentence and target sentence.
Since theream typically many different derivations which con-tain the same source sentence, there can be equallymany different ranslations t\~r it.
Tile probability ofa translation can be calculated as the total probabil-ity of all the derivations that form this translation.Tile model presented here is capable of translat-ing between two hmguages only.
This lilnitation isby no means a property of the model itself, but ischosen for simplicity and readability reasons only.The following parameters should be specified fora DOP-like approach to MT:1. tile representations of sentences that are as-sl imed,2.
the fragments of these representations that canbe used for generating new representations,3.
the operator that is used to combine the flag-ments to form a translation, andI This is actually the second instantiation of such a frame-work.
The original model (Poutsma, 1998; l)outsnm, 2000) hada major flaw, which resulted in translations that were simply in-correct, as pointed out by Way (1999).2Links between tree nodes were introduced for TAG trees,in (Schieber and Schabes, 1990), and put to use for MachineTranslation by Abeilld et al (1990).635, I  fNP VPICharles V NPI Ilikes Anne"SNP " -.
VP\ Anne V -.
PPI / '> .pla~t P NPI Ia CharlesFigure 1: A linked tree pair (T,., T~).4. the model that is used for determining theprobability of a target sentence given a sourcesentence.In the explanation that follows, we will use a sub-script s to denote an element of the source language,and a subscript t to denote one of the target lan-guage.2.1 RepresentationsIn DOT, we basically use the same utterance-analysis as in DOPI (i.e.
syntactically labeledphrase structure trees).
To allow for translation ca-pabilities in tiffs model, we will use pairs of treesthat incorporate semantic infonnation.
The amounlof semantic information eed not be very detailed,since all we are interested in is semantic equiva-lence.
Two trees 7\] and T2 are said to be semanticequivalents (denoted as TI "" 7~) iff TI can be re-placed with T2 without loss of meaning.We can now introduce the notion of links: a linksymbolizes a semantic equivalence between twotrees, or part of trees.
It can occur at any level inthe tree structure, except for the terminal level.
3The representation used in DOT is a 3-tuple(T,, Tt, ?
), where ~ is a tree in the somce language,Tt is a tree in the target language, and ?
is a functionthat maps between semantic equivalent parts in bothtrees.
In the rest of this article, we will refer to this3-tuple as tile pair (T,, g) .Because of the semantic equivalence, a link nmstexist at the top level of the tree pair (Ts, Tt).
Figure 1shows an example of two linked trees, the links aredepicted graphically as dashed lines.3Links cannot occur at the terminal evel, since we mapbetween semantic equivalent parts on the level of syntacticcategories.2.2 FragmentsLikewise, we will use linked subtrees as our flag-ments.
Given a pair of linked trees (T~, Tt), a linkedsubtree pair of (T~, Tt) consists of two connectedand linked subgraphs (t~, 6) of (77~, 7}) such that:1. for every pair of linked nodes in (t.,.,6), it holdsthat:(a) both nodes in (ts,lt} have either zerodaughter nodes,or(b) both nodes have all the daughter nodes ofthe corresponding nodes in (T,, Tt)and2.
every non-linked node in either t~.
(or 6) has allthe daughter nodes of the corresponding nodein T, (T,),and3.
both t, and ~ consist of more than one node.This definition has a number of consequences.First of all, it is morn restrictive than the DOPI def-inition for subtrees, thus resulting in a smaller orequal amount of subtrees per tree.
Secondly, it de-fines a possible pair of linked subtl'ees.
Typically,there are many pairs of linked subtrees for each setof linked trees.
Thirdly, the linked tree pair itselfis also a valid linked subtree pair.
Finally, accord-ing to this definition, all the linked subtree pairs aresemantic equivalents, since the semantic daughternodes of the original tree are removed or retained si-multaneously (clause 1).
The nodes for which a se-mantic equivalent does not exist are always retained(clause 2).We can now define the bag of linked subtreepailw, which we will use as a grammar.
Given acorpus of linked trees C, the bag of linked subtreepairs of C is the bag in which linked subtree pairsoccur exactly as often as they can be identified inC. 4 Figure 2 show the bag of linked subtree pairsfor the linked tree pair (T,, Tt).2.3 Composition operatorIn DOT, we use the leftmost substitution opera-tor for forming combinations of grammar ules.The composition of tile linked tree pair {ts,6) and4The similarity between Example-based MT (Nagao, 1984)and DOT is clear: EBMT uses a database ofexamples toforma translation, whereas DOT uses a bag of structured trees.636,q~ .
.
.
.
SNP VP NP ~ - VPI ~ ~"  I L?~Charles V NP Anne V \ -.
PPlikes Anne l;Iaft P NPI I?~ Charles/ \NP VP NP "- VPl ~ , "  >Q- - .
.Charles V NP V \ PPlikes plait P NPI I?t Charles?
\S - - -  SNP VP NP - VP~ ~ " I "v>~'-...\pV NP Anne Plikes Anne pla) P NP I0/ \NP VP NP " .
.
VP/V NP V \ PPlikes" pla) P NPIgtNP NP NP NPI I I ICharles Charles Anne AnneFigure 2: The bag of liuked subtree pairs of (T,, 7~)(us,u,), written as (ts,tt)o (u.,.,u,), is deiined iff|he label of lhe leftmost nonterlninal \]inked fi'ontieruocle and the label of its linked counterpart are iden-tical to the labels of the root nodes of (u.~., ur).
If thiscomposition is defined, it yields a copy of (t,.,tt), inwhich a copy of u.,.
has been substituted on t.,.
's left-most nonterminal linked frontier node, and a copyof ut has been substituted on the node's linked coun-terpart.
The colnposition operation is illustrated infigure 3.Given a bag of linked subtree pairs B, a se-quence of compositions (ts~ , it, ) o .
.
.
o {t.~N, bN ), with(t.~i,b~) E B yielding a tree pair (T,,Tt) without non-terminal leaves is called a derivation D of (7~., 7~).2.4 Probabi l i ty  calculat ionTo compute the probability of the target composi-tion, we make the same statistical assumptions as inDOPI with regard to independence and representa-tiou of the subtrees (Bed, 1998, p. 16).The probability of selecting a subtree pair (ts~blis calculated by dividing the frequency of the sub-tree pair in the bag by the number of snbtrees thathave the same root node labels in this bag.
In otherwords, let I(t.,,t,)l be the number of times the sub-tree pair (t,.,tr} occurs in the bag of subtree pairs,and r(t) be the root node categories of t, then theprobability assigned to (is,b) isp((t,,,t,)) - I(l.,,t,)lEl,,,,,,,> :~(,,,.)=,-(,,.
)~,-(,,,):,-(,,)I (",  ", )1(1)Given the assumptions that all subtree pairsare independent, Ihe probability of a derivation(ts~ ,hi) o .
.
.
o (GN,ttN) is equal to the product of theprobabilities of the used subtree pairs.P(0.~,,t,,) o .
.
.o  (ts,,t,N>) = l-\[p((t,,,t, i))i(2)The translation generated by a derivation is equalto the sentence yielded by the target trees of thederivation.
Typically, a translation can be generatedby a large number of different deriwltions, each ofwhich has its own probability.
Therefore, the prob-ability of a translation ws ~ wt is the sum of theprobabilities of its derivations:P(w~, w, ) :  ~_P(D(ws,w,}) (3)637/ \/ \~ S ~ -  ~ SNP VP NP \ VP)V NP V \ PPlikes plait P NPI71S SNP VP NPNP'NP I\] \] = Anne V NP'AtmeAnne \]likesVPV PPplaft P NPI Ia AnneFigure 3: The composition operationThe justification of this last equation is quite triv-ial.
As in any statistical MT system, we wish tochoose the target sentence w~ so as to maximizeP(wtlw,) (Brown et al, 1990, p. 79).
if we take thesum over all possible derivations that wele formedfrom Ws and derive wt, we can rewrite this as equa-tion 4, as seen below.
Since both ws and wt arecontained in Dlw,,w, ), we can remove them both andarrive at equation 5, which--as we maximize overwt--is equivalent to equation 3 above.maxP(wtlWs) =Wt= max Ewr D{ws,wt)11qax Ewt D(ws.Wt)P(w,, D<w,,w,)lw.d (4)P(D(w,,w,)) (5)3 Computational AspectsWhen translating using the DOT model, we can dis-tinguish between three computational stages:I. parsing: the formation of a derivation forest,2.
translation: the transfer of the derivation for-est from the source language to the target lan-guage,3.
disambiguation: the selection of the mostprobable translation from the derivation forest.3.1 ParsingIn DOT, every subtlee pair (t~,tt) can be seenas a productive rewrite rule: (root(t~),root(tt))(frontier(ts), frontier(tt)), where all linkage in thefrontier nodes is retained.
The linked non-terminalsin the yield constitute the symbol pairs to which newroles (subtlee pairs) are applied.
For instance, therightmost subtree pair in tigure 3 can be rewritten as\ \[(S, S) --+ ((Anne, likes, NP), (NP, pla~t, ~'t, Aune))This rule can then be combined with nfles that havethe root pair (NP, NP), and so on.If we only consider the left-side part of this rule,we can use algorithms that exist for context-freegrammars, so that we can parse a sentence of nwords with a time complexity which is polynomialin n. These algorithms give as output a chart-likederivation forest (Sima'an et al, 1994), which con-tains the tree pairs of all the derivations that can beformed.3.2 TranslationSince every tree pair in the derivation forest containsa tree for the target language, the translation of thisfolest is trivial.3.3 DisambiguationIn order to select he most probable translation, it isnot efficient o compare all translations, ince therecan be exponentially many of them.
Furthermore,it has been shown that the Viterbi algorithm cannotbe used to make the most probable selection from aDOP-like derivation forest (Sima'an, 1996).Instead, we use a random selection lnethodto generate derivations from the target derivationforest, otherwise known as Monte Carlo sam-pling (Bod, 1998, p. 4649).
In this method,the random choices of derivations ale based on theprobabilities of the nnderlying subderivations.
If wegenerate a large number of samples, we can esti-mate the most probable translation as the translationwhich results most often.
The most probable trans-lation can be estimated as accurately as desired bymaking the number of random samples ufficientlylarge.4 Pilot ExperimentsIn order to test the DOT-model, we did some pi-lot experiments with a small part of the Verbmo-bil corpus.
This corpus consists of transliteratedspoken appointment dialogues in German, English,638and Japanese.
We only used the German and En-glish datasets, which were aligned at sentence level,and syntactically annotated using different annota-tion schemes.
5Naturally, the tree pairs in the corpus did not con-tain any links, so-- in order to make it useful forl )OT- -we had to analyze each tree pair, and placelinks where necessary.
We also corrected tree pairsthat were not aligned correctly.
Figure 4 shows anexample of a corrected and linked tree from our co lrection of the Verbmobil corpus.We used a blind testing method, dividing the 266trees of our corpus into an 85% training set of 226tree pairs, and a 15% test set of 40 tree pairs.
Wecarried out three experiments, in both directions,each using a different split of training and test set.The 226 training set tree pairs were converted intofragments (i.e.
subtree pairs), and were enrichedwith their corpus probabilities.
The 40 sentencesfrom the lest set served as input sentences: theywere translated with the fragments from the train-ing set using a bottom-up chart parser, and disam-biguated by the Monte Carlo algorithm.
The mostprobable translations were estinmted from probabil-ity distributions of 1500 sampled erivations, whichaccounts for a standard deviation ?5 < 0.013.
Fi-nally, we compared the resulting trauslations wilhthe original translation as given in the test set.
Wealso fed tile tes!
sentences inlo another MT-system:AltaVista's Babelfish, which is based on Systran.
64.1 EvahmtionIn a manner similar to (Brown et al, 1990, p. 83),we assigned each of the resulting sentences a cate-gory according to the following criteria.
If the pro-duced sentence was exactly the stone as the actualVerbmobil translation, we assigned it the exact cat-ego W. If it was a legitimate translation of the sourcesentence but in different words, we assigned it thealternale category.
If it made sense as a sentence,but could not be interpreted as a valid translation ofthe source sentence, we assigned it the wrong cat-egory.
If the translation only yielded a part of thesource sentence, we assigned it the partial category:either partial exact if it was a part of the actual Verb-mobil translation, or partial alternate if it was partof an alternate translation.
Finally, if no translation5The Penn Treebank scheme for English; the Tiibingenschelne for Gernlan.6This service is available on the lnte,'net via ht tp : / /babel fish.
al tavista, com.ExaclVcrbmobil:Translated as:That woukl be very interesting.I)as wiire sehr inte,essant.l)as w~h'e sehr interessant.AlternaleVerbmobil:Translated as:WrongVerbmobil:Translated as:Parlial ExactVerbmobil:I will book the trains.ich buche die Zfige.Ich werdc (tie Ziige reservieren.Es ist ja keine Behgrde.It is not an administrative officeyou know.There is not an administrative officeyou know.Translated as:And as said 1 think the location of thebranch office is posh.Und wit gesagt ich denke die Lage zurFiliale spricht Biinde ist.ich denke die Lagel'artial Alternatelch habe Preise veto Parkhotelltannover da.Verbmobil: 1 have got prices for HatmoverParkhotel here.Translated as: for Parkhotel HannoverFigure 5: Translation and classification examples.was given,  ? "
we assigned it tile none category.
Tile re-suits we obtained from Systran were also evaluatedusing this procedure.
Figure 5 gives some classiIi-cation examples.The method of evaluation is very strict: even ifore" model generated a translation that had a betterquality than the given Verbmobil translation, we stillassigned it the (partial) alternate category.
This canbe seen in the second example in figure 5.4.2 Resu l tsThe results that we obtained can be seen in table 1and 2.
In both our experiments, the number ofexact translations was somewhat higher tlmn Sys-trmfs, but Systran excelled at the number of al-ternate translations.
This can be explained by thefact that Systran has a much larger lexicon, thus al-lowing it to form much more alternate translations.While it is meaningless to compare results obtainedfrom different corpora, it may be interesting to notethat Brown et al (1990) report a 5% exact matchin experiments with the Hansard corpus, indicatingthat an exact match is very hard to achieve.The number of ungrammatical translations in our639J - .S SLK 1V~F" ~ _ -MD: -NP= - _ VPI - Imachen NX NX ADVX / shall we VB NP ADVPI I I t I \[wir es dann do it thenFigure 4: Example of a linked tree pair in VerbmobilCorpus Categorical ccuracyMax.
Size Correct Incorrect PartialDepth Exact Alternate Ungn Wrong Exact Alternate1 1263 16.22% 2 .70% t8.92% 18.92% 18.92% 24.32%2 2733 16.22% 2 .70% 32.43% 5.41% 27.03% 16.22%24.32% 13.51% 3 82284 1419218.92% 5.41%18.92% 5.41%32.43% 5.41%32.43% 5.41% 24.32% 13.51%5 22147 18.92% 5 .41% 32.43% 5.41% 24.32% 13.51%6 27039 18.92% 5 .41% 32.43% 5.41% 27.03% 10.8t%18.92% 5.41% 33479Systran32.43% 5.41%18.92% 35.14% 8.11% 37.84%24.32% 13.51%0% 0%Table 1: Results of English to German translation experimentsEnglish to German experiment were much higherthan Systran's (32% versus Systran's 19%); vice-versa it was much lower (13% versus Systran's21%).
Since the German grammar is more complexthan the English grammar, this result could be ex-pected.
It is simpler to map a complex grammar toa simpler than vice-versa.The partial translations, which are quite useflfl forforming the basis of a post-edited, manual trans-lation, varied around 38% in our English to Ger-man experiments, and around 55% when translatingfrom German to English.
Systran is incapable offorming partial translations.As can be seen from the tables, we experimentedwith the maxinmm depth of the tree pairs used.
Weexpected that the performance of the model wouldincrease when we used deeper subtree pairs, sincedeeper structures allow for more complex struc-tures, and therefore better translations.
Our exper-iments showed, however, that there was very littleincrease of performance as we increased the maxi-mum tree depth.
A possible explanation is that thetrees in our corpus contained a lot of lexical context(i.e.
terminals) at very small tree depths.
Insteadof varying the maximum tree depth, we should ex-periment with varying the maximum tree width.
Weplan to perform such experiments in the future.5 Future workThough the findings presented in this article coverthe most important issues regarding DOT, there arestill some topics open for future research.As we stated in the previous section, we wishto see whether DOT's performance increases as wevary the maximum width of a tree.In the experiments it became clear that DOT lacksa large lexicon, thus resulting in less alternate trans-lations than Systran.
By using an external lexicon,we can form a part-of-speech sequences fiom thesource sentence, and use this sequence as input forDOT.
The resulting target part-of-speech sequencecan then be reformed into a target sentence.The experiments discussed in this article are pilotexperiments, and do not account for much.
In orderto find more about DOT and its (dis)abilities, moreexperiments on larger corpora are required.6 ConclusionIn this article, we have presented a new approach tomachine translation: the Data-Oriented Translationmodel.
This method uses linked subtree pairs forcreating a derivation of a sentence.
Each subtree-pair consists of two trees: one in the source lan-guage and one in the target language.
Using thesesubtree pairs, we can form a derivation of a givensource sentence, which can then be used to form atarget sentence.
The probability of a translation can640Corpus - Categorical ccuracyMax.
Size Correctl)epth Exact Alternate1 12632 27333 822815.38% 2.56%12.82% 7.69%12.82% 1{).26%IncorrectUngr.
Wrong12.82% 12.82%12.82% 12.82%l'artialExact Alternate41.03% 15.38%35.90% 17.95%38.46% 17.95% 12.82% 7.69%4 14192 15.38% 7.69% 12.82% 10.26% 35.90% 17.95%5 22147 15.38% 5.13% 12.82% 12.82% 35.90% 17.95%6 27039 15.38% 5.13% 12.82% 10.26% 38.46% 17.95%oo 33479 15.38% 7.69% 12.82% 7.69% 38.46% 17.95%Systran 12.82% 25.64% 2(/.51% 41.03% 0% 1)%Table 2: Results of German to English translation experimentsthen be calculated as the total probability of all thederivations that form tiffs translation.The computational aspects of DOT have been dis-cussed, where we introduced a way to reform eachsubtree pair into a productive rewrite role so thatwell-known parsing algorithms can be used.
We de-l:ermine the best translation by Monte Carlo sam-pling.We have discussed the results of some pilot ex-periments with a part of the Verbmobil corpus, andshowed a method of evaluating them.
The ewflua-tion showed that DOT produces less correct rans-lation than Systran, but also less incorrect ransla-tions.
We expected to see an increase in perfor-mance as we increased the depth of subtree pairsused, but this was not the case.Finally, we supplied some topics which art openl'or future research.ReferencesA.
Abeill6, Y. Schabes, and A.K.
Joshi.
1990.
Us-ing lexicalized tags for machine translation.
InProceedings of the 13th international col~\[erenceon computational linguistics, volume 3, pages 1-6, Helsinki.R.
Bod.
1995.
Enriching linguistics with statistics:Pelformance models of natural language.
Num-ber 1995-14 in ILLC Dissertation Series.
Institutefor Logic, Language and Computation, Amster-dam.R.
Bod.
1998.
Beyond grammar: an exl)erience-based theory of language.
Number 88 in CSLIlecture notes.
CSLI Publications, Stanford, Cali-fornia.J.
Brown, J. Cocke, S. Della Pietra, V. Della Pietra,E Jelinek, J. Lafferty, R. Mercer, and R Roossin.1990.
A statistical approach to machine transla-tion.
Computational Linguistics, 16(2):79-86.M.
Nagao.
1984.
A framework of a mechani-cal translation between Japanese and English byanalogy principle.
In A. Elithom and R. Banmji,editors, Artificial and Human Intelligence, chap-ter 11, pages 173-180.
North-Holland, Amster-dam.A.
Poulsma.
1998.
Data-Oriented Translation.
InNinth Conference of Computational Linguisticsin the Netherlands, Leuven, Belgium.
Confer-ence presentation.A.
Poutsma.
2000.
Data-Oriented Translation: Us-ing the DOP framework for MT.
Master's the-sis, Faculty of Mathematics, Computer Science,Physics and Astronomy, University of Amster-dam, the Netherlands.R.
Scha.
1990.
Taaltheorie n taaltechnologie;competence en performance.
In Q.A.M.
de Kortand G.L.J.
Leerdam, editors, Coml)utertoel)assiu-gen in de Neerlandistiek.
Landelijke Verenigmgvan Neerlandici, Ahnere, the Netherlands.S.M.
Schieber and Y. Sehabes.
1990.
Synchronoustree-adjoining grammars.
In Proceedings of the13th international cor(ference on computationallinguistics, volume 3, pages 253-258, Helsinki.K.
Sima'an, R. Bod, S. Krauwer, and R. Scha.1994.
Efficient disambiguation by means ofstochastic tree substitution grammars.
In Pro-ceedings International Crmference ou New Meth-ods in Language Processing, Manchestel; UK.UMIST.K.
Sima'an.
1996.
Computational Complexity ofProbabilistic Disambiguation by means of TreeGrammars.
In Proceedings COLING-96, Copen-hagen, Denmark.A.
Way.
1999.
A hybrid amhitectum for robust MTusing LFG-DOP.
Journal of Experhnental andTheoretical Artificial Intelligence, 11(3).641
