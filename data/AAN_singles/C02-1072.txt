A Comparative Evaluation of Data-driven Models in TranslationSelection of Machine Translation?Yu-Seop Kim ?
Jeong-Ho Chang ?Byoung-Tak Zhang?
Ewha Institute of Science and Technology, Ewha Woman?s Univ.Seoul 120-750 Korea, yskim01@ewha.ac.kr??
Schools of Computer Science and Engineering, Seoul National Univ.Seoul 151-742 Korea, {jhchang, btzhang}@bi.snu.ac.kr?AbstractWe present a comparative evaluation of twodata-driven models used in translation selec-tion of English-Korean machine translation.
La-tent semantic analysis(LSA) and probabilisticlatent semantic analysis (PLSA) are applied forthe purpose of implementation of data-drivenmodels in particular.
These models are able torepresent complex semantic structures of givencontexts, like text passages.
Grammatical rela-tionships, stored in dictionaries, are utilized intranslation selection essentially.
We have usedk-nearest neighbor (k-NN) learning to select anappropriate translation of the unseen instancesin the dictionary.
The distance of instances ink-NN is computed by estimating the similar-ity measured by LSA and PLSA.
For experi-ments, we used TREC data(AP news in 1988)for constructing latent semantic spaces of twomodels and Wall Street Journal corpus for eval-uating the translation accuracy in each model.PLSA selected relatively more accurate transla-tions than LSA in the experiment, irrespectiveof the value of k and the types of grammaticalrelationship.1 IntroductionConstruction of language associated resourceslike thesaurus, annotated corpora, machine-readable dictionary and etc.
requires high de-gree of cost, since they need much of human-effort, which is also dependent heavily upon hu-man intuition.
A data-driven model, however,does not demand any of human-knowledge,knowledge bases, semantic thesaurus, syntac-tic parser or the like.
This model represents?
He is supported by Brain Korea 21 project performedby Ewha Institute of Science and Technology.?
They are supported by Brain Tech.
project controlledby Korean Ministry of Science and Technology.latent semantic structure of contexts like textpassages.
Latent semantic analysis (LSA) (Lan-dauer et al, 1998) and probabilistic latent se-mantic analysis (PLSA) (Hofmann, 2001) fallunder the model.LSA is a theory and method for extractingand representing the contextual-usage meaningof words.
This method has been mainly usedfor indexing and relevance estimation in infor-mation retrieval area (Deerwester et al, 1990).And LSA could be utilized to measure the co-herence of texts (Foltz et al, 1998).
By applyingthe basic concept, a vector representation anda cosine computation, to estimate relevance ofa word and/or a text and coherence of texts,we could also estimate the semantic similaritybetween words.
It is claimed that LSA repre-sents words of similar meaning in similar ways(Landauer et al, 1998).Probabilistic LSA (PLSA) is based on proba-bilistic mixture decomposition while LSA is on alinear algebra and singular value decomposition(SVD) (Hofmann, 1999b).
In contrast to LSA,PLSA?s probabilistic variant has a sound statis-tical foundation and defines a proper generativemodel of the data.
Both two techniques havea same idea which is to map high-dimensionalvectors representing text documents, to a lowerdimensional representation, called a latent se-mantic space (Hofmann, 1999a).Dagan (Dagan et al, 1999) performed a com-parative analysis of several similarity measures,which based mainly on conditional probabilitydistribution.
And the only elements in the dis-tribution are words, which appeared in texts.However, LSA and PLSA expressed the latentsemantic structures, called a topic of the con-text.In this paper, we comparatively evaluatedthese two techniques performed in translationselection of English-Korean machine transla-tion.
First, we built a dictionary storing tu-ples representing the grammatical relationshipof two words, like subject-verb, object-verb, andmodifier-modified.
Second, with an input tuple,in which an input word would be translated andthe other would be used as an argument word,translation is performed by searching the dic-tionary with the argument word.
Third, if theargument word is not listed in the dictionary,we used k-nearest neighbor learning method todetermine which class of translation is appro-priate for the translation of an input word.
Thedistance used in discovering the nearest neigh-bors was computed by estimating the similaritymeasured on above latent semantic spaces.In the experiment, we used 1988 AP newscorpus from TREC-7 data (Voorhees and Har-man, 1998) for building latent semantic spacesand Wall Street Journal (WSJ) corpus for con-structing a dictionary and test sets.
We ob-tained 11-20% accuracy improvement, compar-ing to a simple dictionary search method.
AndPLSA has shown that its ability to select an ap-propriate translation is superior to LSA as anextent of up to 3%, without regard to the valueof k and grammatical relationship.In section 2, we discuss two of data-drivenmodels, LSA and PLSA.
Section 3 describesways of translation with a grammatical rela-tion dictionary and k-nearest neighbor learningmethod.
Experiment is explained in Section 4and concluding remarks are presented in Section5.2 Data-Driven ModelFor the data-driven model which does not re-quire additional human-knowledge in acquiringinformation, Latent Semantic Analysis (LSA)and Probabilistic LSA (PLSA) are applied to es-timate semantic similarity among words.
Nexttwo subsections will explain how LSA and PLSAare to be adopted to measuring semantic simi-larity.2.1 Latent Semantic AnalysisThe basic idea of LSA is that the aggregate of allthe word contexts in which a given word doesand does not appear provides a set of mutualconstraints that largely determines the similar-ity of meaning of words and sets of words to eachother (Landauer et al, 1998)(Gotoh and Renals,1997).
LSA also extracts and infers relations ofexpected contextual usage of words in passagesof discourse.
It uses no human-made dictionar-ies, knowledge bases, semantic thesaurus, syn-tactic parser or the like.
Only raw text parsedinto unique character strings is needed for itsinput data.The first step is to represent the text as amatrix in which each row stands for a uniqueword and each column stands for a text passageor other context.
Each cell contains the occur-rence frequency of a word in the text passage.Next, LSA applies singular value decomposi-tion (SVD) to the matrix.
SVD is a form offactor analysis and is defined asA = U?V T (1),where ?
is a diagonal matrix composed ofnonzero eigen values of AAT or ATA, and Uand V are the orthogonal eigenvectors associ-ated with the r nonzero eigenvalues of AAT andATA, respectively.
One component matrix (U)describes the original row entities as vectors ofderived orthogonal factor value, another (V ) de-scribes the original column entities in the sameway, and the third (?)
is a diagonal matrix con-taining scaling values when the three compo-nents are matrix-multiplied, the original matrixis reconstructed.The singular vectors corresponding to thek(k ?
r) largest singular values are then usedto define k-dimensional document space.
Usingthese vectors,m?k and n?k matrices Uk and Vkmay be redefined along with k?k singular valuematrix ?k.
It is known that Ak = Uk?kV Tk isthe closest matrix of rank k to the original ma-trix.LSA can represent words of similar meaningin similar ways.
This can be claimed by the factthat one compares words with similar vectors asderived from large text corpora.
The term-to-term similarity is based on the inner productsbetween two row vectors of A, AAT = U?2UT .One might think of the rows of U?
as definingcoordinates for terms in the latent space.
Tocalculate the similarity of coordinates, V1 andV2, cosine computation is used:cos?
= V1 ?V2?
V1 ?
?
?
V2 ?
(2)2.2 Probabilistic Latent SemanticAnalysisProbabilistic latent semantic analysis (PLSA)is a statistical technique for the analysis of two-mode and co-occurrence data, and has producedsome meaningful results in such applicationsas language modelling (Gildea and Hofmann,1999) and document indexing in information re-trieval (Hofmann, 1999b).
PLSA is based onaspect model where each observation of the co-occurrence data is associated with a latent classvariable z ?
Z = {z1, z2, .
.
.
, zK} (Hofmann,1999a).
For text documents, the observation isan occurrence of a word w ?
W in a documentd ?
D, and each possible state z of the latentclass represents one semantic topic.A word-document co-occurrence event,(d,w), is modelled in a probabilistic way whereit is parameterized as inP (d,w) =?zP (z)P (d,w|z)=?zP (z)P (w|z)P (d|z).
(3)Here, w and d are assumed to be condition-ally independent given a specific z. P (w|z) andP (d|z) are topic-specific word distribution anddocument distribution, respectively.
The three-way decomposition for the co-occurrence datais similar to that of SVD in LSA.
But the ob-jective function of PLSA, unlike that of LSA,is the likelihood function of multinomial sam-pling.
And the parameters P (z), P (w|z), andP (d|z) are estimated by maximization of thelog-likelihood functionL =?d?D?w?Wn(d,w) logP (d,w), (4)and this maximization is performed using theEM algorithm as for most latent variable mod-els.
Details on the parameter estimation arereferred to (Hofmann, 1999a).
To computethe similarity of w1 and w2, P (zk|w1)P (zk|w2)should be approximately computed with beingderived fromP (zk|w) =P (zk)P (w|zk)?zk?Z P (zk)P (w|zk)(5)And we can evaluate similarities with thelow-dimensional representation in the semantictopic space P (zk|w1) and P (zk|w2).3 Translation with GrammaticalRelationship3.1 Grammatical RelationshipWe used grammatical relations stored in theform of a dictionary for translation of words.The structure of the dictionary is as follows(Kim and Kim, 1998):T (Si) =??????
?T1 if Cooc(Si, S1)T2 if Cooc(Si, S2).
.
.Tn otherwise,(6)where Cooc(Si, Sj) denotes grammatical co-occurrence of source words Si and Sj , which onemeans an input word to be translated and theother means an argument word to be used intranslation, and Tj is the translation result ofthe source word.
T (?)
denotes the translationprocess.Table 1 shows a grammatical relationship dic-tionary for an English verb Si =?build?
and itsobject nouns as an input word and an argumentword, respectively.
The dictionary shows thatthe word ?build?
is translated into five differenttranslated words in Korean, depending on thecontext.
For example, ?build?
is translated into?geon-seol-ha-da?
(?construct?)
when its objectnoun is a noun ?plant?
(=?factory?
), into ?che-chak-ha-da?
(?produce?)
when co-occurring withthe object noun ?car?, and into ?seol-lip-ha-da?(?establish?)
in the context of object noun ?com-pany?
(Table 2).One of the fundamental difficulties in co-occurrence-based approaches to word sense dis-ambiguation (translation selection in this case)is the problem of data sparseness or unseenwords.
For example, for an unregistered objectnoun like ?vehicle?
in the dictionary, the correcttranslation of the verb cannot be selected us-ing the dictionary described above.
In the nextsubsection, we will present k-nearest neighbormethod that resolves this problem.3.2 k-Nearest Neighbor Learning forTranslation SelectionThe similarity between two words on latent se-mantic spaces is required when performing k-NN search to select the translation of a word.The nearest instance of a given word is decidedby selecting a word with the highest similarityto the given word.Table 1: Examples of co-occurrence word lists for a verb ?build?
in the dictionaryMeaning of ?build?
in Korean (Tj) Collocated Object Noun (Sj)?geon-seol-ha-da?
(= ?construct?)
plant facility network .
.
.?geon-chook-ha-da?
(= ?design?)
house center housing .
.
.?che-chak-ha-da?
(= ?produce?)
car ship model .
.
.?seol-lip-ha-da?
(= ?establish?)
company market empire .
.
.?koo-chook-ha-da?
(= ?develop?)
system stake relationship .
.
.Table 2: Examples of translation of ?build?source words translated words (in Korean) sense of the verb?build a plant?
?
?gong-jang-eul geon-seol-ha-da?
?construct?
?build a car?
?
?ja-dong-cha-reul che-chak-ha-da?
?produce?
?build a company?
?
?hoi-sa-reul seol-lip-ha-da?
?establish?The k-nearest neighbor learning algorithm(Cover and Hart, 1967)(Aha et al, 1991) as-sumes all instances correspond to points in then-dimensional space Rn.
We mapped the n-dimensional space into the n-dimensional vectorof a word for an instance.
The nearest neigh-bors of an instance are defined in terms of thestandard Euclidean distance.Then the distance between two instances xiand xj , D(xi, xj), is defined to beD(xi, xj) =?(a(xi)?
a(xj))2 (7)and a(xi) denotes the value of instance xi, sim-ilarly to cosine computation between two vec-tors.
Let us consider learning discrete-valuedtarget functions of the form f : Rn ?
V ,where V is the finite set {v1, .
.
.
, vs}.
The k-nearest neighbor algorithm for approximating adiscrete-valued target function is given in Table3.The value f?
(xq) returned by this algorithmas its estimate of f(xq) is just the most com-mon value of f among the k training examplesnearest to xq.4 Experiment and Evaluation4.1 Data for Latent Space andDictionaryIn the experiment, we used two kinds of cor-pus data, one for constructing LSA and PLSAspaces and the other for building a dictionarycontaining grammatical relations and a test set.79,919 texts in 1988 AP news corpus fromTREC-7 data was indexed with a stemming tooland 19,286 words with the frequency of above 20Table 3: The k-nearest neighbor learning algo-rithm.?
Training?
For each training example ?x, f(x)?,add the example to the listtraining examples.?
Classification?
Given a query instance xq to be clas-sified,?
Let x1, .
.
.
, xk denote the k in-stances from training examplesthat are nearest to xq.?
Returnf?(xq)?
argmaxv?Vk?i=1?
(v, f(xi)) ,where ?
(a, b) = 1 if a = b and?
(a, b) = 0 otherwise.are extracted.
We built 200 dimensions in SVDof LSA and 128 latent dimensions of PLSA.
Thedifference of the numbers was caused from thedegree of computational complexity in learningphase.
Actually, PLSA of 128 latent factors re-quired 50-fold time as much as LSA hiring 200eigen-vector space during building latent spaces.This was caused by 50 iterations which madethe log likelihood maximized.
We utilized a sin-Figure 1: The accuracy ration of verb-objectgle vector lanczos algorithm derived from SVD-PACK when constructing LSA space.
(Berryet al, 1993).
We generated both of LSA andPLSA spaces, with each word having a vectorof 200 and 128 dimensions, respectively.
Thesimilarity of any two words could be estimatedby performing cosine computation between twovectors representing coordinates of the words inthe spaces.Table 4 shows 5 most similar words of ran-domly selected words from 3,443 examples.
Weextracted 3,443 example sentences containinggrammatical relations, like verb-object, subject-verb and adjective-noun, from Wall Street Jour-nal corpus of 220,047 sentences and othernewspapers corpus of 41,750 sentences, totally261,797 sentences.
We evaluated the accu-racy performance of each grammatical rela-tion.
2,437, 188, and 818 examples were uti-lized for verb-object, subject-verb, and adjective-noun, respectively.
The selection accuracy wasmeasured using 5-fold cross validation for eachgrammatical relation.
Sample sentences of eachgrammatical relation were divided into five dis-joint samples and each sample became a testsample once in the experiment and the remain-ing four samples were combined to make up acollocation dictionary.4.2 Experimental ResultTable 5 and figure 1-3 show the results oftranslation selection with respect to the appliedmodel and to the value of k. As shown in Table5, similarity based on data-driven model couldimprove the selection accuracy up to 20% asFigure 2: The accuracy ration of subject-verbFigure 3: The accuracy ration of adjective-nouncontrasted with the direct matching method.We could obtain the result that PLSA couldimprove the accuracy more than LSA in almostall cases.
The amount of improvement is variedfrom -0.12% to 2.96%.As figure 1-3 show, the value of k had affec-tion to the translation accuracy in PLSA, how-ever, not in LSA.
From this, we could not de-clare whether the value of k and translation ac-curacy have relationship of each other or notin the data-driven models described in this pa-per.
However, we could also find that the degreeof accuracy was raised in accordance with thevalue of k in PLSA.
From this, we consequentlyinferred that the latent semantic space gener-ated by PLSA had more sound distribution withreflection of well-structured semantic structurethan LSA.
Only one of three grammatical re-Table 4: Lists of 5 most semantically similar words for randomly selected words generated fromLSA, and PLSA.
The words are stems of original words.
The first row of each selected word standsfor the most similar words in LSA semantic space and the second row stands for those in the PLSAspace.selected words most similar wordsplant westinghous isocyan shutdown zinc manurradioact hanford irradi tritium biodegradcar buick oldsmobil chevrolet sedan corollahighwai volkswagen sedan vehicular vehiclehome parapleg broccoli coconut liverpool jamalmemori baxter hanlei corwin headstonbusiness entrepreneur corpor custom ventur firmdigit compat softwar blackston zayrship vessel sail seamen sank sailordestroy frogmen maritim skipper vesselTable 5: Translation accuracy in various case.
The first column stands for each grammatical relationand the second column stands for the used models, LSA or PLSA.
And other three columns standfor the accuracy ratio (rm) with respect to the value of k. The numbers in parenthesis of the firstcolumn show the translation accuracy ratio of simple dictionary search method (rs).
And numbersin the other parenthesis were obtained by rm ?
rs.grammatical used k = 1 k = 5 k = 10relations modelverb-object LSA 84.41(1.17) 83.01(1.16) 84.24(1.17)(71.85) PLSA 84.53(1.18) 85.35(1.19) 86.05(1.20)subject-verb LSA 83.99(1.11) 84.62(1.11) 84.31(1.11)(75.93) PLSA 86.85(1.14) 87.49(1.15) 87.27(1.15)adjective-noun LSA 80.93(1.15) 80.32(1.14) 80.93(1.15)(70.54) PLSA 80.81(1.15) 82.27(1.17) 82.76(1.17)lations, subj-verb, showed an exceptional case,which seemed to be caused by the small size ofexamples, 188.Selection errors taking place in LSA andPLSA models were caused mainly by the fol-lowing reasons.
First of all, the size of vocab-ulary should be limited by computation com-plexity.
In this experiment, we acquired below20,000 words for the vocabulary, which couldnot cover a section of corpus data.
Second, thestemming algorithm was not robust for an in-dexing.
For example, ?house?
and ?housing?
areregarded as a same word as ?hous?.
This factbrought about hardness in reflecting the seman-tic structure more precisely.
And finally, themeaning of similar word is somewhat varied inthe machine translation field and the informa-tion retrieval field.
The selectional restrictiontends to depend a little more upon semantictype like human-being, place and etc., than onthe context in a document.5 ConclusionThis paper describes a comparative evaluationof the accuracy performance in translation se-lection based on data-driven models.
LSA andPLSA were utilized for implementation of themodels, which are mainly used in estimatingsimilarity between words.
And a manually-built grammatical relation dictionary was usedfor the purpose of appropriate translation se-lection of a word.
To break down the datasparseness problem occurring when the dictio-nary is used, we utilized similarity measure-ments schemed out from the models.
When anargument word is not included in the dictionary,the most k similar words to the word are discov-ered in the dictionary, and then the meaning ofthe grammatically-related class for the majorityof the k words is selected as the translation ofan input word.We evaluated the accuracy ratio of LSA andPLSA comparatively and classified the exper-iments with criteria of the values of k andthe grammatical relations.
We acquired up to20% accuracy improvement, compared to directmatching to a collocation dictionary.
PLSAshowed the ability to select translation betterthan LSA, up to 3%.
The value of k is stronglyrelated with PLSA in translation accuracy, nottoo with LSA.
That means the latent semanticspace of PLSA has more sound distribution oflatent semantics than that of LSA.
Even thoughlonger learning time than LSA, PLSA is benefi-cial in translation accuracy and distributionalsoundness.
A distributional soundness is ex-pected to have better performance as the sizeof examples is growing.However, we should resolve several problemsraised during the experiment.
First, a robuststemming tool should be exploited for more ac-curate morphology analysis.
Second, the opti-mal value of k should be obtained, according tothe size of examples.
Finally, we should discovermore specific contextual information suited tothis type of problem.
While simple text couldbe used properly in IR, MT should require an-other type of information.The data-driven models could be applied toother sub-fields related with semantics in ma-chine translation.
For example, to-infinitivephrase and preposition phrase attachment dis-ambiguation problem can also apply these mod-els.
And syntactic parser could apply the mod-els for improvement of accurate analysis by us-ing semantic information generated by the mod-els.ReferencesD.
Aha, D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37?66.M.
Berry, T. Do, G. O?Brien, V. Krishna, andS.
Varadhan.
1993.
Svdpackc: Version 1.0user?s guide.
Technical Report CS?93?194,University of Tennessee, Knoxville, TN.T.
Cover and P. Hart.
1967.
Nearest neighborpattern classification.
IEEE Transactions onInformation Theory, 13:21?27.I.
Dagan, L. Lee, and F. Fereira.
1999.Similarity-based models of word cooccurrenceprobabilities.
Machine Learning, 34:43?69.S.
Deerwester, S. Dumais, G. Furnas, T. Lan-dauer, and R. Harshman.
1990.
Indexingby latent semantic analysis.
Journal of theAmerican Society for Information Science,41:391?407.P.
Foltz, W. Kintsch, and T. Landauer.
1998.The mesurement of textual coherence with la-tent semantic analysis.
Discourse Processes,25:285?307.D.
Gildea and T. Hofmann.
1999.
Topic basedlanguage models using em.
In Proceedings ofthe 6th European Conference on Speech Com-munication and Technology (Eurospeech99).D.
Gotoh and S. Renals.
1997.
Documentspace models using latent semantic analysis.In Proceedings of Eurospeech-97, pages 1443?1446.T.
Hofmann.
1999a.
Probabilistic latent se-mantic analysis.
In Proceedings of the Fif-teenth Conference on Uncertainty in Artifi-cial Intelligence (UAI?99).T.
Hofmann.
1999b.
Probabilistic latent se-mantic indexing.
In Proceedings of the 22thAnnual International ACM SIGIR conferenceon Research and Developement in Informa-tion Retrieval (SIGIR99), pages 50?57.T.
Hofmann.
2001.
Unsupervised learning byprobabilistic latent semantic analysis.
Ma-chine Learning Journal, 42(1):177?196.Y.
Kim and Y. Kim.
1998.
Semantic implemen-tation based on extended idiom for english tokorean machine translation.
The Asia-PacificAssociation for Machine Translation Journal,21:23?39.T.
K. Landauer, P. W. Foltz, and D. Laham.1998.
An introduction to latent semanticanalysis.
Discourse Processes, 25:259?284.E.
Voorhees and D. Harman.
1998.
Overview ofthe seventh text retrieval conference (trec-7).In Proceedings of the Seventh Text REtrievalConference (TREC-7), pages 1?24.
