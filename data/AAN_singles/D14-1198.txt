Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846?1851,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsConstructing Information Networks Using One Single ModelQi Li?Heng Ji?Yu Hong?
?Sujian Li?
?Computer Science Department, Rensselaer Polytechnic Institute, USA?School of Computer Science and Technology, Soochow University, China?Key Laboratory of Computational Linguistics, Peking University, MOE, China?
{liq7,hongy2,jih}@rpi.edu,?lisujian@pku.edu.cnAbstractIn this paper, we propose a new frame-work that unifies the output of three infor-mation extraction (IE) tasks - entity men-tions, relations and events as an informa-tion network representation, and extractsall of them using one single joint modelbased on structured prediction.
This novelformulation allows different parts of theinformation network fully interact witheach other.
For example, many rela-tions can now be considered as the re-sultant states of events.
Our approachachieves substantial improvements overtraditional pipelined approaches, and sig-nificantly advances state-of-the-art end-to-end event argument extraction.1 IntroductionInformation extraction (IE) aims to discover entitymentions, relations and events from unstructuredtexts, and these three subtasks are closely inter-dependent: entity mentions are core componentsof relations and events, and the extraction of rela-tions and events can help to accurately recognizeentity mentions.
In addition, the theory of eventu-alities (D?olling, 2011) suggested that relations canbe viewed as states that events start from and resultin.
Therefore, it is intuitive but challenging to ex-tract all of them simultaneously in a single model.Some recent research attempted to jointly modelmultiple IE subtasks (e.g., (Roth and Yih, 2007;Riedel and McCallum, 2011; Yang and Cardie,2013; Riedel et al., 2009; Singh et al., 2013; Li etal., 2013; Li and Ji, 2014)).
For example, Roth andYih (2007) conducted joint inference over entitymentions and relations; Our previous work jointlyextracted event triggers and arguments (Li et al.,2013), and entity mentions and relations (Li andJi, 2014).
However, a single model that can ex-tract all of them has never been studied so far.Asif Mohammed Hanif detonated explosives in Tel AvivAttackPerson Weapon Geopolitical EntityPlaceInstrumentAttackerAgent-ArtifactPhysicalx1 x2 x3 x4 x5 x6 x7 x8x:y:Figure 1: Information Network Representation.Information nodes are denoted by rectangles.
Ar-rows represent information arcs.For the first time, we uniformly represent the IEoutput from each sentence as an information net-work, where entity mentions and event triggers arenodes, relations and event-argument links are arcs.We apply a structured perceptron framework witha segment-based beam-search algorithm to con-struct the information networks (Collins, 2002; Liet al., 2013; Li and Ji, 2014).
In addition to the per-ceptron update, we also apply k-best MIRA (Mc-Donald et al., 2005), which refines the perceptronupdate in three aspects: it is flexible in using var-ious loss functions, it is a large-margin approach,and it can use mulitple candidate structures to tunefeature weights.In an information network, we can capture theinteractions among multiple nodes by learningjoint features during training.
In addition to thecross-component dependencies studied in (Li etal., 2013; Li and Ji, 2014), we are able to cap-ture interactions between relations and events.
Forexample, in Figure 1, if we know that the Personmention ?Asif Mohammed Hanif ?
is an Attackerof the Attack event triggered by ?detonated?, andthe Weapon mention ?explosives?
is an Instrument,we can infer that there exists an Agent-Artifactrelation between them.
Similarly we can inferthe Physical relation between ?Asif MohammedHanif ?
and ?Tel Aviv?.However, in practice many useful interactionsare missing during testing because of the data spar-1846sity problem of event triggers.
We observe that21.5% of event triggers appear fewer than twice inthe ACE?051training data.
By using only lexicaland syntactic features we are not able to discoverthe corresponding nodes and their connections.
Totackle this problem, we use FrameNet (Baker andSato, 2003) to generalize event triggers so thatsemantically similar triggers are clustered in thesame frame.The following sections will elaborate the de-tailed implementation of our new framework.2 ApproachWe uniformly represent the IE output from eachsentence as an information network y = (V,E).Each node vi?
V is represented as a triple?ui, vi, ti?
of start index ui, end index vi, and nodetype ti.
A node can be an entity mention or anevent trigger.
A particular type of node is ?
(nei-ther entity mention nor event trigger), whose max-imal length is always 1.
Similarly, each infor-mation arc ej?
E is represented as ?uj, vj, rj?,where ujand vjare the end offsets of the nodes,and rjis the arc type.
For instance, in Fig-ure 1, the event trigger ?detonated?
is representedas ?4, 4, Attack?, the entity mention ?Asif Mo-hammed Hanif ?
is represented as ?1, 3, Person?,and their argument arc is ?4, 3, Attacker?.
Ourgoal is to extract the whole information network yfor a given sentence x.2.1 Decoding AlgorithmOur joint decoding algorithm is based on ex-tending the segment-based algorithm described inour previous work (Li and Ji, 2014).
Let x =(x1, ..., xm) be the input sentence.
The decoderperforms two types of actions at each token xifrom left to right:?
NODEACTION(i, j): appends a new node?j, i, t?
ending at the i-th token, where i?
dt<j ?
i, and dtis the maximal length of type-tnodes in training data.?
ARCACTION(i, j): for each j < i, incremen-tally creates a new arc between the nodes endingat the j-th and i-th tokens respectively: ?i, j, r?.After each action, the top-k hypotheses are se-lected according to their features f(x, y?)
and1http://www.itl.nist.gov/iad/mig//tests/aceweights w:bestky?
?bufferf(x, y?)
?wSince a relation can only occur between a pair ofentity mentions, an argument arc can only occurbetween an entity mention and an event trigger,and each edge must obey certain entity type con-straints, during the search we prune invalid AR-CACTIONs by checking the types of the nodesending at the j-th and the i-th tokens.
Finally, thetop hypothesis in the beam is returned as the finalprediction.
The upper-bound time complexity ofthe decoding algorithm is O(d ?
b ?
m2), where dis the maximum size of nodes, b is the beam size,and m is the sentence length.
The actual executiontime is much shorter, especially when entity typeconstraints are applied.2.2 Parameter EstimationFor each training instance (x, y), the structuredperceptron algorithm seeks the assignment withthe highest model score:z = argmaxy?
?Y(x)f(x, y?)
?wand then updates the feature weights by using:wnew= w + f(x, y)?
f(x, z)We relax the exact inference problem by the afore-mentioned beam-search procedure.
The stan-dard perceptron will cause invalid updates be-cause of inexact search.
Therefore we apply early-update (Collins and Roark, 2004), an instance ofviolation-fixing methods (Huang et al., 2012).
Inthe rest of this paper, we override y and z to denoteprefixes of structures.In addition to the simple perceptron update, wealso apply k-best MIRA (McDonald et al., 2005),an online large-margin learning algorithm.
Duringeach update, it keeps the norm of the change tofeature weights w as small as possible, and forcesthe margin between y and the k-best candidate zgreater or equal to their loss L(y, z).
It is formu-lated as a quadratic programming problem:min ?wnew?w?s.t.
wnewf(x, y)?wnewf(x, z) ?
L(y, z)?z ?
bestk(x,w)We employ the following three loss functionsfor comparison:1847Freq.
Relation Type Event Type Arg-1 Arg-2 Example159 Physical Transport Artifact Destination He(arg-1)was escorted(trigger)into Iraq(arg-2).46 Physical Attack Target Place Many people(arg-1)were in the cafe(arg-2)during the blast(trigger).42 Agent-Artifact Attack Attacker Instrument Terrorists(arg-1)might use(trigger)the devices(arg-2)as weapons.41 Physical Transport Artifact Origin The truck(arg-1)was carrying(trigger)Syrians fleeing the war in Iraq(arg-2).33 Physical Meet Entity Place They(arg-1)have reunited(trigger)with their friends in Norfolk(arg-2).32 Physical Die Victim Place Two Marines(arg-1)were killed(trigger)in the fighting in Kut(arg-2).28 Physical Attack Attacker Place Protesters(arg-1)have been clashing(trigger)with police in Tehran(arg-2).26 ORG-Affiliation End-Position Person Entity NBC(arg-2)is terminating(trigger)freelance reporter Peter Arnett(arg-1).Table 1: Frequent overlapping relation and event types in the training set.?
The first one is F1loss:L1(y, z) = 1?2 ?
|y ?
z||y|+ |z|When counting the numbers, we treat each nodeand arc as a single unit.
For example, in Fig-ure 1, |y| = 6.?
The second one is 0-1 loss:L2(y, z) ={1 y 6= z0 y = zIt does not discriminate the extent to which zdeviates from y.?
The third loss function counts the difference be-tween y and z:L3(y, z) = |y|+ |z| ?
2 ?
|y ?
z|Similar to F1loss function, it penalizes bothmissing and false-positive units.
The differenceis that it is sensitive to the size of y and z.2.3 Joint Relation-Event FeaturesBy extracting three core IE components in a jointsearch space, we can utilize joint features overmultiple components in addition to factorized fea-tures in pipelined approaches.
In addition to thefeatures as described in (Li et al., 2013; Li andJi, 2014), we can make use of joint features be-tween relations and events, given the fact thatrelations are often ending or starting states ofevents (D?olling, 2011).
Table 1 shows the mostfrequent overlapping relation and event types inour training data.
In each partial structure y?dur-ing the search, if both arguments of a relation par-ticipate in an event, we compose the correspond-ing argument roles and relation type as a joint fea-ture for y?.
For example, for the structure in Fig-ure 1, we obtain the following joint relation-eventfeatures:Attacker InstrumentAgent-ArtifactAttacker PlacePhysicalSplit Sentences Mentions Relations Triggers ArgumentsTrain 7.2k 25.7k 4.8k 2.8k 4.5kDev 1.7k 6.3k 1.2k 0.7k 1.1kTest 1.5k 5.3k 1.1k 0.6k 1.0kTable 2: Data set0 20 40 60 80 100Number of instances02468101214FrequencyTrigger WordsFrame IDsFigure 2: Distribution of triggers and their frames.2.4 Semantic Frame FeaturesOne major challenge of constructing informationnetworks is the data sparsity problem in extract-ing event triggers.
For instance, in the sen-tence: ?Others were mutilated beyond recogni-tion.?
The Injure trigger ?mutilated?
does not oc-cur in our training data.
But there are some sim-ilar words such as ?stab?
and ?smash?.
We uti-lize FrameNet (Baker and Sato, 2003) to solvethis problem.
FrameNet is a lexical resource forsemantic frames.
Each frame characterizes a ba-sic type of semantic concept, and contains a num-ber of words (lexical units) that evoke the frame.Many frames are highly related with ACE events.For example, the frame ?Cause harm?
is closelyrelated with Injure event and contains 68 lexicalunits such as ?stab?, ?smash?
and ?mutilate?.Figure 2 compares the distributions of triggerwords and their frame IDs in the training data.
Wecan clearly see that the trigger word distributionsuffers from the long-tail problem, while Framesreduce the number of triggers which occur only1848MethodsEntity Mention (%)Relation (%)Event Trigger (%)Event Argument (%)P R F1P R F1P R F1P R F1Pipelined Baseline83.6 75.7 79.568.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7Pipeline + Li et al.
(2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/AJoint w/ Avg.
Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6Joint w/ MIRA w/ F1Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9Joint w/ MIRA w/ L3Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8Table 3: Overall performance on test set.once in the training data from 100 to 60 and al-leviate the sparsity problem.
For each token, weexploit the frames that contain the combination ofits lemma and POS tag as features.
For the aboveexample, ?Cause harm?
will be a feature for ?mu-tilated?.
We only consider tokens that appear inat most 2 frames, and omit the frames that occurfewer than 20 times in our training data.3 Experiments3.1 Data and EvaluationWe use ACE?05 corpus to evaluate our methodwith the same data split as in (Li and Ji, 2014).
Ta-ble 2 summarizes the statistics of the data set.
Wereport the performance of extracting entity men-tions, relations, event triggers and arguments sep-arately using the standard F1measures as definedin (Ji and Grishman, 2008; Chan and Roth, 2011):?
An entity mention is correct if its entity type (7in total) and head offsets are correct.?
A relation is correct if its type (6 in total) and thehead offsets of its two arguments are correct.?
An event trigger is correct if its event subtype(33 in total) and offsets are correct.?
An argument link is correct if its event subtype,offsets and role match those of any of the refer-ence argument mentions.In this paper we focus on entity arguments whiledisregard values and time expressions becausethey can be most effectively extracted by hand-crafted patterns (Chang and Manning, 2012).3.2 ResultsBased on the results of our development set, wetrained all models with 21 iterations and chose thebeam size to be 8.
For the k-best MIRA updates,we set k as 3.
Table 3 compares the overall perfor-mance of our approaches and baseline methods.Our joint model with perceptron update out-performs the state-of-the-art pipelined approachin (Li et al., 2013; Li and Ji, 2014), and furtherimproves the joint event extraction system in (Liet al., 2013) (p < 0.05 for entity mention extrac-tion, and p < 0.01 for other subtasks, accord-ing to Wilcoxon Signed RankTest).
For the k-best MIRA update, the L3loss function achievedbetter performance than F1loss and 0-1 loss onall sub-tasks except event argument extraction.
Italso significantly outperforms perceptron updateon relation extraction and event argument extrac-tion (p < 0.01).
It is particularly encouraging tosee the end output of an IE system (event argu-ments) has made significant progress (12.2% ab-solute gain over traditional pipelined approach).3.3 Discussions3.3.1 Feature StudyRank Feature Weight1 Frame=Killing Die 0.802 Frame=Travel Transport 0.613 Physical(Artifact, Destination) 0.604 w1=?home?
Transport 0.595 Frame=Arriving Transport 0.546 ORG-AFF(Person, Entity) 0.487 Lemma=charge Charge-Indict 0.458 Lemma=birth Be-Born 0.449 Physical(Artifact,Origin) 0.4410 Frame=Cause harm Injure 0.43Table 4: Top Features about Event Triggers.Table 4 lists the weights of the most significantfeatures about event triggers.
The 3rd, 6th, and9throws are joint relation-event features.
For in-stance, Physical(Artifact, Destination) means thearguments of a Physical relation participate in aTransport event as Artifact and Destination.
Wecan see that both the joint relation-event features1849and FrameNet based features are of vital impor-tance to event trigger labeling.
We tested the im-pact of each type of features by excluding them inthe experiments of ?MIRA w/ L3loss?.
We foundthat FrameNet based features provided 0.8% and2.2% F1gains for event trigger and argument la-beling respectively.
Joint relation-event featuresalso provided 0.6% F1gain for relation extraction.3.3.2 Remaining ChallengesEvent trigger labeling remains a major bottleneck.In addition to the sparsity problem, the remain-ing errors suggest to incorporate external worldknowledge.
For example, some words act as trig-gers for some certain types of events only whenthey appear together with some particular argu-ments:?
?Williams picked up the child again and thistime, threwAttackher out the window.
?The word ?threw?
is used as an Attack eventtrigger because the Victim argument is a ?child?.?
?Ellison to spend $10.3 billion to getMerge Orghis company.?
The common word ?get?
istagged as a trigger of Merge Org, because itsobject is ?company?.?
?We believe that the likelihood of themusingAttackthose weapons goes up.
?The word ?using?
is used as an Attack eventtrigger because the Instrument argument is?weapons?.Another challenge is to distinguish physical andnon-physical events.
For example, in the sentence:?
?we are paying great attention to their ability todefendAttackon the ground.
?,our system fails to extract ?defend?
as an Attacktrigger.
In the training data, ?defend?
appears mul-tiple times, but none of them is tagged as Attack.For instance, in the sentence:?
?North Korea could do everything to defend it-self.
??defend?
is not an Attack trigger since it does notrelate to physical actions in a war.
This challengecalls for deeper understanding of the contexts.Finally, some pronouns are used to refer to ac-tual events.
Event coreference is necessary to rec-ognize them correctly.
For example, in the follow-ing two sentences from the same document:?
?It?s important that people all over the worldknow that we don?t believe in the warAttack.?,?
?Nobody questions whether thisAttackis rightor not.??this?
refers to ?war?
in its preceding contexts.Without event coreference resolution, it is difficultto tag it as an Attack event trigger.4 ConclusionsWe presented the first joint model that effectivelyextracts entity mentions, relations and eventsbased on a unified representation: informationnetworks.
Experiment results on ACE?05 cor-pus demonstrate that our approach outperformspipelined method, and improves event-argumentperformance significantly over the state-of-the-art.In addition to the joint relation-event features, wedemonstrated positive impact of using FrameNetto handle the sparsity problem in event trigger la-beling.Although our primary focus in this paper is in-formation extraction in the ACE paradigm, we be-lieve that our framework is general to improveother tightly coupled extraction tasks by capturingthe inter-dependencies in the joint search space.AcknowledgmentsWe thank the three anonymous reviewers for theirinsightful comments.
This work was supported bythe U.S. Army Research Laboratory under Coop-erative Agreement No.
W911NF-09-2-0053 (NS-CTA), U.S. NSF CAREER Award under GrantIIS-0953149, U.S. DARPA Award No.
FA8750-13-2-0041 in the Deep Exploration and Filteringof Text (DEFT) Program, IBM Faculty Award,Google Research Award, Disney Research Awardand RPI faculty start-up grant.
The views and con-clusions contained in this document are those ofthe authors and should not be interpreted as rep-resenting the official policies, either expressed orimplied, of the U.S. Government.
The U.S. Gov-ernment is authorized to reproduce and distributereprints for Government purposes notwithstandingany copyright notation here on.ReferencesCollin F. Baker and Hiroaki Sato.
2003.
The framenetdata and software.
In Proc.
ACL, pages 161?164.Yee Seng Chan and Dan Roth.
2011.
Exploitingsyntactico-semantic structures for relation extrac-tion.
In Proc.
ACL, pages 551?560.1850Angel X. Chang and Christopher Manning.
2012.
Su-time: A library for recognizing and normalizing timeexpressions.
In Proc.
LREC, pages 3735?3740.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Proc.ACL, pages 111?118.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proc.
EMNLP,pages 1?8.Johannes D?olling.
2011.
Aspectual coercion and even-tuality structure.
pages 189?226.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.HLT-NAACL, pages 142?151.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through cross-document inference.
In Proc.ACL.Qi Li and Heng Ji.
2014.
Incremental joint extractionof entity mentions and relations.
In Proc.
ACL.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In Proc.
ACL, pages 73?82.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proc.
ACL, pages 91?98.Sebastian Riedel and Andrew McCallum.
2011.
Fastand robust joint models for biomedical event extrac-tion.
In Proc.
EMNLP.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic ap-proach to bio-molecular event extraction.
In Proc.the Workshop on Current Trends in Biomedical Nat-ural Language Processing: Shared Task.Dan Roth and Wen-tau Yih.
2007.
Global inferencefor entity and relation identification via a lin- earprogramming formulation.
In Introduction to Sta-tistical Relational Learning.
MIT.Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-ing Zheng, and Andrew McCallum.
2013.
Jointinference of entities, relations, and coreference.
InProc.
CIKM Workshop on Automated KnowledgeBase Construction.Bishan Yang and Claire Cardie.
2013.
Joint inferencefor fine-grained opinion extraction.
In Proc.
ACL,pages 1640?1649.1851
