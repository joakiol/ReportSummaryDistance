Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1222?1232,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSPred: Large-scale Harvesting of Semantic PredicatesTiziano Flati and Roberto NavigliDipartimento di InformaticaSapienza Universita` di Roma{flati,navigli}@di.uniroma1.itAbstractWe present SPred, a novel method for thecreation of large repositories of semanticpredicates.
We start from existing colloca-tions to form lexical predicates (e.g., break?)
and learn the semantic classes that bestfit the ?
argument.
To do this, we extractall the occurrences in Wikipedia whichmatch the predicate and abstract its argu-ments to general semantic classes (e.g.,break BODY PART, break AGREEMENT,etc.).
Our experiments show that we areable to create a large collection of seman-tic predicates from the Oxford AdvancedLearner?s Dictionary with high precisionand recall, and perform well against themost similar approach.1 IntroductionAcquiring semantic knowledge from text automat-ically is a long-standing issue in ComputationalLinguistics and Artificial Intelligence.
Over thelast decade or so the enormous abundance of in-formation and data that has become available hasmade it possible to extract huge amounts of pat-terns and named entities (Etzioni et al, 2005), se-mantic lexicons for categories of interest (Thelenand Riloff, 2002; Igo and Riloff, 2009), large do-main glossaries (De Benedictis et al, 2013) andlists of concepts (Katz et al, 2003).
Recently,the availability of Wikipedia and other collabora-tive resources has considerably boosted researchon several aspects of knowledge acquisition (Hovyet al, 2013), leading to the creation of severallarge-scale knowledge resources, such as DBPe-dia (Bizer et al, 2009), BabelNet (Navigli andPonzetto, 2012), YAGO (Hoffart et al, 2013),MENTA (de Melo and Weikum, 2010), to namebut a few.
This wealth of acquired knowledgeis known to have a positive impact on importantfields such as Information Retrieval (Chu-Carrolland Prager, 2007), Information Extraction (Krauseet al, 2012), Question Answering (Ferrucci et al,2010) and Textual Entailment (Berant et al, 2012;Stern and Dagan, 2012).Not only are these knowledge resources ob-tained by acquiring concepts and named entities,but they also provide semantic relations betweenthem.
These relations are extracted from unstruc-tured or semi-structured text using ontology learn-ing from scratch (Velardi et al, 2013) and OpenInformation Extraction techniques (Etzioni et al,2005; Yates et al, 2007; Wu and Weld, 2010;Fader et al, 2011; Moro and Navigli, 2013) whichmainly stem from seminal work on is-a relationacquisition (Hearst, 1992) and subsequent devel-opments (Girju et al, 2003; Pasca, 2004; Snow etal., 2004, among others).However, these knowledge resources still lacksemantic information about language units suchas phrases and collocations.
For instance, whichsemantic classes are expected as a direct objectof the verb break?
What kinds of noun does theadjective amazing collocate with?
Recognition ofthe need for systems that are aware of the selec-tional restrictions of verbs and, more in general, oftextual expressions, dates back to several decades(Wilks, 1975), but today it is more relevant thanever, as is testified by the current interest in se-mantic class learning (Kozareva et al, 2008) andsupertype acquisition (Kozareva and Hovy, 2010).These approaches leverage lexico-syntactic pat-terns and input seeds to recursively learn the se-mantic classes of relation arguments.
However,they require the manual selection of one or moreseeds for each pattern of interest, and this selec-tion influences the amount and kind of semanticclasses to be learned.
Furthermore, the learnedclasses are not directly linked to existing resourcessuch as WordNet (Fellbaum, 1998) or Wikipedia.The goal of our research is to create a large-scale repository of semantic predicates whose lex-ical arguments are replaced by their semanticclasses.
For example, given the textual expres-sion break a toe we want to create the correspond-1222ing semantic predicate break a BODY PART, whereBODY PART is a class comprising several lexicalrealizations, such as leg, arm, foot, etc.This paper provides three main contributions:?
We propose SPred, a novel approach whichharvests predicates from Wikipedia and gen-eralizes them by leveraging core conceptsfrom WordNet.?
We create a large-scale resource made up ofsemantic predicates.?
We demonstrate the high quality of our se-mantic predicates, as well as the generalityof our approach, also in comparison with ourclosest competitor.2 PreliminariesWe introduce two preliminary definitions whichwe use in our approach.Definition 1 (lexical predicate).
A lexical pred-icate w1 w2 .
.
.
wi ?
wi+1 .
.
.
wn is a regularexpression, where wj are tokens (j = 1, .
.
.
, n), ?matches any sequence of one or more tokens, andi ?
{0, .
.
.
, n}.
We call the token sequence whichmatches ?
the filling argument of the predicate.For example, a * of milk matches occurrencessuch as a full bottle of milk, a glass of milk, a car-ton of milk, etc.
While in principle * could matchany sequence of words, since we aim at general-izing nouns, in what follows we allow ?
to matchonly noun phrases (e.g., glass, hot cup, very bigbottle, etc.
).Definition 2 (semantic predicate).
A semanticpredicate is a sequence w1 w2 .
.
.
wi c wi+1.
.
.
wn, where wj are tokens (j = 1, .
.
.
, n),c ?
C is a semantic class selected from a fixedset C of classes, and i ?
{0, .
.
.
, n}.As an example, consider the semantic predicatecup of BEVERAGE,1 where BEVERAGE is a se-mantic class representing beverages.
This pred-icate matches phrases like cup of coffee, cup oftea, etc., but not cup of sky.
Other examples in-clude: MUSICAL INSTRUMENT is played by, aCONTAINER of milk, break AGREEMENT, etc.Semantic predicates mix the lexical informationof a given lexical predicate with the explicit se-mantic modeling of its argument.
Importantly, thesame lexical predicate can have different classes asits argument, like cup of FOOD vs. cup of BEVER-AGE.
Note, however, that different classes mightconvey different semantics for the same lexical1In what follows we denote the SEMANTIC CLASS insmall capitals and the lexical predicate in italics.predicate, such as cup of COUNTRY, referring tocup as a prize instead of cup as a container.3 Large-Scale Harvesting of SemanticPredicatesThe goal of this paper is to provide a fully auto-matic approach for the creation of a large repos-itory of semantic predicates in three phases.
Foreach lexical predicate of interest (e.g., break ?):1.
We extract all its possible filling argumentsfrom Wikipedia, e.g., lease, contract, leg,arm, etc.
(Section 3.1).2.
We disambiguate as many filling argumentsas possible using Wikipedia, obtaining aset of corresponding Wikipedia pages, e.g.,Lease, Contract, etc.
(Section 3.2).3.
We create the semantic predicates by general-izing the Wikipedia pages to their most suit-able semantic classes, e.g., break AGREE-MENT, break LIMB, etc.
(Section 3.3).We can then exploit the learned semantic predi-cates to assign the most suitable semantic class tonew filling arguments for the given lexical predi-cate (Section 3.4).3.1 Extraction of Filling ArgumentsLet pi be an input lexical predicate (e.g., break ?
).We search the English Wikipedia for all the to-ken sequences which match pi, resulting in a listof noun phrases filling the ?
argument.
We showan excerpt of the output obtained when searchingWikipedia for the arguments of the lexical predi-cate a * of milk in Table 1.
As can be seen, a widerange of noun phrases are extracted, from quanti-ties such as glass and cup to other aspects, such asbrand and constituent.The output of this first step is a set Lpi of triples(a, s, l) of filling arguments a matching the lexi-cal predicate pi in a sentence s of the Wikipediacorpus, with a potentially linked to a page l (e.g.,see the top 3 rows in Table 1; l =  if no link isprovided, see bottom rows of the Table).2 Notethat Wikipedia is the only possible corpus that canbe used here for at least two reasons: first, in or-der to extract relevant arguments, we need a largecorpus of a definitional nature; second, we needwide-coverage semantic annotations of filling ar-guments.3.2 Disambiguation of Filling ArgumentsThe objective of the second step is to disambiguateas many arguments in Lpi as possible for the lex-2We will also refer to l as the sense of a in sentence s.1223a full [[bottle]] of milka nice hot [[cup]] of milka cold [[glass]] of milka very big bottle of milka brand of milka constituent of milkTable 1: An excerpt of the token sequenceswhich match the lexical predicate a * of milk inWikipedia (filling argument shown in the secondcolumn; following the Wikipedia convention weprovide links in double square brackets).ical predicate pi.
We denote Dpi = {(a, s, l) :l 6= } ?
Lpi as the set of those arguments origi-nally linked to the corresponding Wikipedia page(like the top three linked arguments in Table 1).Therefore, in the rest of this section we will focusonly on the remaining triples (a, s, ) ?
Upi, whereUpi = Lpi \Dpi, i.e., those triples whose argumentsare not semantically annotated.
Our goal is to re-place  with an appropriate sense, i.e., page, for a.For each such triple (a, s, ) ?
Upi, we apply thefollowing disambiguation heuristics:?
One sense per page: if another occurrenceof a in the same Wikipedia page (indepen-dent of the lexical predicate) is linked to apage l, then remove (a, s, ) from Upi and add(a, s, l) to Dpi.
In other words, we propa-gate an existing annotation of a in the sameWikipedia page and apply it to our ambigu-ous item.
For instance, cup of coffee appearsin the Wikipedia page Energy drink in thesentence ?[.
.
. ]
energy drinks contain morecaffeine than a strong cup of coffee?, but thisoccurrence of coffee is not linked.
How-ever the second paragraph contains the sen-tence ?
[[Coffee]], tea and other naturally caf-feinated beverages are usually not consideredenergy drinks?, where coffee is linked to theCoffee page.
This heuristic naturally reflectsthe broadly known assumption about lexi-cal ambiguity presented in (Yarowsky, 1995),namely the one-sense-per-discourse heuris-tic.?
One sense per lexical predicate: if?
(a, s?, l) ?
Dpi, then remove (a, s, ) fromUpi and add (a, s, l) to Dpi.
If multiple sensesof a are available, choose the most frequentone in Dpi.
For example, in the page Singa-porean cuisine the occurrence of coffee in thesentence ?[.
.
. ]
combined with a cup of cof-fee and a half-boiled egg?
is not linked, butwe have collected many other occurrences,all linked to the Coffee page, so this linkgets propagated to our ambiguous item aswell.
This heuristic mimes the one-sense-per-collocation heuristic presented in (Yarowsky,1995).?
Trust the inventory: if Wikipedia providesonly one sense for a, i.e., only one page titlewhose lemma is a, link a to that page.
Con-sider the instance ?At that point, Smith threwdown a cup of Gatorade?
in page JimmyClausen; there is only one sense for Gatoradein Wikipedia, so we link the unannotated oc-currence to it.As a result, the initial set of disambiguated ar-guments in Dpi is augmented with all those triplesfor which any of the above three heuristics apply.Note that Dpi might contain the same argumentseveral times, occurring in different sentences andlinked many times to the same page or to differ-ent pages.
Notably, the discovery of new links ismade through one scan of Wikipedia per heuristic.The three disambiguation strategies, applied in thesame order as presented above, contribute to pro-moting the most relevant sense for a given word.Finally, let A be the set of arguments in Dpi,i.e., A := {a : ?
(a, s, l) ?
Dpi}.
For each argu-ment a ?
Awe select the majority sense sense(a)of a and collect the corresponding set of sen-tences sent(a) marked with that sense.
Formally,sense(a) := argmaxl |{(x, y, z) ?
Dpi : x =a?z = l}| and sent(a) := {s : (a, s, sense(a)) ?Dpi}.3.3 Generalization to Semantic ClassesOur final objective is to generalize the annotatedarguments to semantic classes picked out from afixed set C of classes.
As explained below, we as-sume the set C to be made up of representativesynsets from WordNet.
We perform this in twosubsteps: we first link all our disambiguated argu-ments to WordNet (Section 3.3.1) and then lever-age the WordNet taxonomy to populate the seman-tic classes in C (Section 3.3.2).3.3.1 Linking to WordNetSo far the arguments in Dpi have been semanti-cally annotated with the Wikipedia pages they re-fer to.
However, using Wikipedia as our sense in-ventory is not desirable; in fact, contrarily to othercommonly used lexical-semantic networks suchas WordNet, Wikipedia is not formally organizedin a structured, taxonomic hierarchy.
While it istrue that attached to each Wikipedia page there areone or more categories, these categories just pro-vide shallow information about the class the page1224belongs to.
Indeed, categories are not ideal forrepresenting the semantic classes of a Wikipediapage for at least three reasons: i) many cate-gories do not express taxonomic information (e.g.,the English page Albert Einstein provides cate-gories such as DEATHS FROM ABDOMINAL AOR-TIC ANEURYSM and INSTITUTE FOR ADVANCEDSTUDY FACULTY); ii) categories are mostly struc-tured in a directed acyclic graph with multipleparents per category (even worse, cycles are pos-sible in principle); iii) there is no clear way ofidentifying core semantic classes from the largeset of available categories.
Although efforts to-wards the automatic taxonomization of Wikipediacategories do exist in the literature (Ponzetto andStrube, 2011; Nastase and Strube, 2013), the re-sults are of a lower quality than a hand-built lexicalresource.
Therefore, as was done in previous work(Mihalcea and Moldovan, ; Ciaramita and Altun,2006; Izquierdo et al, 2009; Erk and McCarthy,2009; Huang and Riloff, 2010), we pick out oursemantic classes C from WordNet and leverage itsmanually-curated taxonomy to associate our argu-ments with the most suitable class.
This way weavoid building a new taxonomy and shift the prob-lem to that of projecting the Wikipedia pages ?associated with annotated filling arguments ?
tosynsets in WordNet.
We address this problem intwo steps:Wikipedia-WordNet mapping.
We exploit anexisting mapping implemented in BabelNet (Nav-igli and Ponzetto, 2012), a wide-coveragemultilingual semantic network that integratesWikipedia and WordNet.3 Based on a disam-biguation algorithm, BabelNet establishes a map-ping ?
: Wikipages ?
Synsets which linksabout 50,000 pages to their most suitable Word-Net senses.4Mapping extension.
Nevertheless, BabelNet isable to solve the problem only partially, because itstill leaves the vast majority of the 4 million En-glish Wikipedia pages unmapped.
This is mainlydue to the encyclopedic nature of most pages,which do not have a counterpart in the WordNetdictionary.
To address this issue, for each un-mapped Wikipedia page p we obtain its textualdefinition as the first sentence of the page.5 Next,3http://babelnet.org4We follow (Navigli, 2009) and denote with wip the i-thsense of w in WordNet with part of speech p.5According to the Wikipedia guidelines, ?The articleshould begin with a short declarative sentence, answer-ing two questions for the nonspecialist reader: What (orwho) is the subject?
and Why is this subject notable?
?,extracted from http://en.wikipedia.org/wiki/we extract the hypernym from the textual defini-tion of p by applying Word-Class Lattices (Navigliand Velardi, 2010, WCL6), a domain-independenthypernym extraction system successfully appliedto taxonomy learning from scratch (Velardi et al,2013) and freely available online (Faralli and Nav-igli, 2013).
If a hypernym h is successfully ex-tracted and h is linked to a Wikipedia page p?for which ?(p?)
is defined, then we extend themapping by setting ?
(p) := ?(p?).
For instance,the mapping provided by BabelNet does not pro-vide any link for the page Peter Spence; thanks toWCL, though, we are able to set the page Jour-nalist as its hypernym, and link it to the WordNetsynset journalist1n.This way our mapping extension now covers539,954 pages, i.e., more than an order of mag-nitude greater than the number of pages originallycovered by the BabelNet mapping.3.3.2 Populating the Semantic ClassesWe now proceed to populating the semanticclasses in C with the annotated arguments ob-tained for the lexical predicate pi.Definition 3 (semantic class of a synset).
Thesemantic class for a WordNet synset S is the classc among those in C which is the most specific hy-pernym of S according to the WordNet taxonomy.For instance, given the synset tap water1n, its se-mantic class is water1n (while the other more gen-eral subsumers in C are not considered, e.g., com-pound2n, chemical1n, liquid3n, etc).For each argument a ?
A for which aWikipedia-to-WordNet mapping ?
(sense(a))could be established as a result of the linkingprocedure described above, we associate a withthe semantic class of ?(sense(a)).
For example,consider the case in which a is equal to tap waterand sense(a) is equal to the Wikipedia page Tapwater, in turn mapped to tap water1n via ?
; wethus associate tap water with its semantic classwater1n.
If more than one class can be found weadd a to each of them.7Ultimately, for each class c ?
C, we obtaina set support(c) made up of all the argumentsa ?
A associated with c. For instance, sup-port(beverage1n) = { chinese tea, 3.2% beer, hotcocoa, cider, .
.
.
, orange juice }.
Note that, thanksto our extended mapping (cf.
Section 3.3.1), thesupport of a class can also contain arguments notcovered in WordNet (e.g., hot cocoa and tejuino).Wikipedia:Writing_better_articles.6http://lcl.uniroma1.it/wcl7This can rarely happen due to multiple hypernyms avail-able in WordNet for the same synset.1225Pclass(c|pi) c support(c)0.1896 wine1n wine, sack, white wine, red wine, wine in china, madeira wine, claret, kosher wine0.1805 coffee1n turkish coffee, drip coffee, espresso, coffee, cappucino, caffe` latte, decaffeinated coffee, latte0.1143 herb2n green tea, indian tea, black tea, orange pekoe tea, tea0.1104 water1n water, seawater0.0532 beverage1n chinese tea, 3.2% beer, orange soda, boiled water, hot chocolate, hot cocoa, tejuino, cider,beverage, cocoa, coffee milk, lemonade, orange juice0.0403 milk1n skim milk, milk, cultured buttermilk, whole milk0.0351 beer1n 3.2% beer, beer0.0273 alcohol1n mead, umeshu, kava, rice wine, ja?germeister, kvass, sake, gin, rum0.0182 poison1n poisonTable 2: Highest-probability semantic classes for the lexical predicate pi = cup of *, according to our setC of semantic classes.Since not all classes are equally relevant to thelexical predicate pi, we estimate the conditionalprobability of each class c ?
C given pi on thebasis of the number of sentences which contain anargument in that class.
Formally:Pclass(c|pi) =?a?support(c) |sent(a)|Z , (1)where Z is a normalization factor calculated asZ =?c??C?a?support(c?)
|sent(a)|.
As an ex-ample, in Table 2 we show the highest-probabilityclasses for the lexical predicate cup of ?.As a result of the probabilistic association ofeach semantic class c with a target lexical predi-cate w1 w2 .
.
.
wi ?
wi+1 .
.
.
wn, we obtain asemantic predicate w1 w2 .
.
.
wi c wi+1 .
.
.
wn.3.4 Classification of new argumentsOnce the semantic predicates for the input lexicalpredicate pi have been learned, we can classify anew filling argument a of pi.
However, the classprobabilities calculated with Formula 1 might notprovide reliable scores for several classes, includ-ing unseen ones whose probability would be 0.To enable wide coverage we estimate a secondconditional probability based on the distributionalsemantic profile of each class.
To do this, we per-form three steps:1.
For each WordNet synset S we create a dis-tributional vector ~S summing the noun occur-rences within all the Wikipedia pages p suchthat ?
(p) = S. Next, we create a distribu-tional vector for each class c ?
C as follows:~c =?S?desc(c) ~S,where desc(c) is the set of all synsetswhich are descendants of the semantic classc in WordNet.
As a result we obtain apredicate-independent distributional descrip-tion for each semantic class in C.2.
Now, given an argument a of a lexical predi-cate pi, we create a distributional vector ~a bysumming the noun occurrences of all the sen-tences s such that (a, s, l) ?
Lpi (cf.
Section3.1).3.
Let Ca be the set of candidate semanticclasses for argument a, i.e., Ca contains thesemantic classes for the WordNet synsets ofa as well as the semantic classes associatedwith ?
(p) for all Wikipedia pages p whoselemma is a.
For each candidate class c ?
Ca,we determine the cosine similarity betweenthe distributional vectors ~c and ~a as follows:sim(~c,~a) = ~c ?
~a||~c|| ||~a|| .Then, we determine the most suitable seman-tic class c ?
Ca of argument a as the classwith the highest distributional probability, es-timated as:Pdistr(c|pi, a) =sim(~c,~a)?c?
?Ca sim(~c ?,~a).
(2)We can now choose the most suitable class c ?Ca for argument a which maximizes the proba-bility mixture of the distributional probability inFormula 2 and the class probability in Formula 1:P (c|pi, a) = ?Pdistr(c|pi, a)+(1??
)Pclass(c|pi),(3)where ?
?
[0, 1] is an interpolation factor.We now illustrate the entire process of our al-gorithm on a real example.
Given a textual ex-pression such as virus replicate, we: (i) extractall the filling arguments of the lexical predicate* replicate; (ii) link and disambiguate the ex-tracted filling arguments; (iii) query our system forthe available virus semantic classes (i.e., {virus1n,virus3n}); (iv) build the distributional vectors for1226the candidate semantic classes and the given in-put argument; (v) calculate the probability mix-ture.
As a result we obtain the following rank-ing, virus1n:0.250, virus3n:0.000894, so that the firstsense of virus in WordNet 3.0 is preferred, beingan ?ultramicroscopic infectious agent that repli-cates itself only within cells of living hosts?.4 Experiment 1: Oxford LexicalPredicatesWe evaluate on the two forms of output producedby SPred: (i) the top-ranking semantic classes of alexical predicate, as obtained with Formula 1, and(ii) the classification of a lexical predicate?s argu-ment with the most suitable semantic class, as pro-duced using Formula 3.
For both evaluations, weuse a lexical predicate dataset built from the Ox-ford Advanced Learner?s Dictionary (Crowther,1998).4.1 Set of Semantic ClassesThe selection of which semantic classes to includein the set C is of great importance.
In fact, hav-ing too many classes will end up in an overly fine-grained inventory of meanings, whereas an exces-sively small number of classes will provide lit-tle discriminatory power.
As our set C of seman-tic classes we selected the standard set of 3,299core nominal synsets available in WordNet.8 How-ever, our approach is flexible and can be used withclasses of an arbitrary level of granularity.4.2 DatasetsThe Oxford Advanced Learner?s Dictionary pro-vides usage notes that contain typical predicates invarious semantic domains in English, e.g., Travel-ing.9 Each predicate is made up of a fixed part(e.g., a verb) and a generalizable part which con-tains one or more nouns.Examples include fix an election/the vote, bac-teria/microbes/viruses spread, spend money/sav-ings/a fortune.
In the case that more than onenoun was provided, we split the textual expres-sion into as many items as the number of nouns.For instance, from spend money/savings/a fortunewe created three items in our dataset, i.e., spendmoney, spend savings, spend a fortune.
The split-ting procedure generated 6,220 instantiated lexicalpredicate items overall.8http://wordnetcode.princeton.edu/standoff-files/core-wordnet.txt9http://oald8.oxfordlearnersdictionaries.com/usage_notes/unbox_colloc/k Prec@k Correct Total1 0.94 46 492 0.87 85 983 0.86 124 1454 0.83 160 1925 0.82 194 2376 0.81 228 2827 0.80 261 3268 0.78 288 3709 0.77 318 41410 0.76 349 45811 0.75 379 50212 0.75 411 54613 0.75 445 59014 0.76 479 63415 0.75 510 67816 0.75 544 72117 0.76 577 76318 0.76 612 80619 0.76 643 84920 0.75 671 892Table 3: Precision@k for ranking the semanticclasses of lexical predicates.4.3 Evaluating the Semantic Class RankingDataset.
Given the above dataset, we general-ized each item by pairing its fixed verb part with *(i.e., we keep ?verb predicates?
only, since theyare more informative).
For instance, the threeitems bacteria/microbes/viruses spread were gen-eralized into the lexical predicate * spread.
The to-tal number of different lexical predicates obtainedwas 1,446, totaling 1,429 distinct verbs (note thatthe dataset might contain the lexical predicate *spread as well as spread *).10Methodology.
For each lexical predicate we cal-culated the conditional probability of each seman-tic class using Formula 1, resulting in a rankingof semantic classes.
To evaluate the top rankingclasses, we calculated precision@k, with k rang-ing from 1 to 20, by counting all applicable classesas correct, e.g., location1n is a valid semantic classfor travel to * while emotion1n is not.Results.
We show in Table 3 the precision@kcalculated over a random sample of 50 lexicalpredicates.11 As can be seen, while the classesquality is pretty high with low values of k, per-formance gradually degrades as we let k increase.This is mostly due to the highly polysemous natureof the predicates selected (e.g., occupy *, leave *,help *, attain *, live *, etc.).
We note that high per-formance, attaining above 80%, can be achieved10The low number of items per predicate is due to the orig-inal Oxford resource.11One lexical predicate did not have any semantic classranking.1227by focusing up to the first 7 classes output by oursystem, with a 94% precision@1.4.4 Evaluating Classification PerformanceDataset.
Starting from the lexical predicateitems obtained as described in Section 4.2, we se-lected those items belonging to a random sampleof 20 usage notes among those provided by theOxford dictionary, totaling 3,245 items.
We thenmanually tagged each item?s argument (e.g., virusin viruses spread) with the most suitable seman-tic class (e.g., virus1n), obtaining a gold standarddataset for the evaluation of our argument classifi-cation algorithm (cf.
Section 3.4).Methodology.
In this second evaluation wemeasure the accuracy of our method at assigningthe most suitable semantic class to the argumentof a lexical predicate item in our gold standard.We use three customary measures to determine thequality of the acquired semantic classes, i.e., pre-cision, recall and F1.
Precision is the number ofitems which are assigned the correct class (as eval-uated by a human) over the number of items whichare assigned a class by the system.
Recall is thenumber of items which are assigned the correctclass over the number of items to be classified.
F1is the harmonic mean of precision and recall.Tuning.
The only parameter to be tuned is thefactor ?
that we use to mix the two probabilitiesin Formula 3 (cf.
Section 3.4).
For tuning ?
weused a held-out set of 8 verbs, randomly sampledfrom the lexical predicates not used in the dataset.We created a tuning set using the annotated argu-ments in Wikipedia for these verbs: we trained themodel on 80% of the annotated lexical predicatearguments (i.e., the class probability estimates inFormula 1) and then applied the probability mix-ture (i.e., Formula 3) for classifying the remain-ing 20% of arguments.
Finally, we calculated theperformance in terms of precision, recall and F1with 11 different values of ?
?
{0, 0.1, .
.
.
, 1.0},achieving optimal performance with ?
= 0.2.Results.
Table 4 shows the results on the seman-tic class assignments.
Our system shows very highprecision, above 85%, while at the same time at-taining an adequate 68% recall.
We also comparedagainst a random baseline that randomly selectsone out of all the candidate semantic classes foreach item, achieving only moderate results.
A sub-sequent error analysis revealed the common typesof error produced by our system: terms for whichwe could not provide (1) any WordNet conceptMethod Precision Recall F1SPred 85.61 68.01 75.80Random 40.96 40.96 40.96Table 4: Performance on semantic class assign-ment.
(e.g., political corruption) or (2) any candidate se-mantic class (e.g., immune system).4.5 Disambiguation heuristics impactAs a follow-up analysis, for each dataset we con-sidered the impact of each disambiguation heuris-tic described in Section 3.2 according to how manytimes it was triggered.
Starting from the entire setof 1,446 lexical predicates from the Oxford dictio-nary (see Section 4.3), we counted the number ofargument triples (a, s, l) already disambiguated inWikipedia (i.e., l 6= ) and those disambiguatedthanks to our disambiguation strategies.
Table5 shows the statistics.
We note that, while theamount of originally linked arguments is very low(about 2.5% of total), our strategies are able toconsiderably increase the size of the initial set oflinked instances.
The most effective strategies ap-pear to be the One sense per page and the Trust theinventory, which contribute 26.16% and 31.33%of the total links, respectively.Even though most of the triples (i.e., 68 out ofalmost 74 million) remain unlinked, the ratio ofdistinct arguments which we linked to WordNetis considerably higher, calculated as 3,723,979linked arguments over 12,431,564 distinct argu-ments, i.e., about 30%.5 Experiment 2: Comparison withKozareva & Hovy (2010)Due to the novelty of the task carried out by SPred,the resulting output may be compared with only alimited number of existing approaches.
The mostsimilar approach is that of Kozareva and Hovy(2010, K&H) who assign supertypes to the argu-ments of arbitrary relations, a task which resem-bles our semantic predicate ranking.
We thereforeperformed a comparison on the quality of the mosthighly-ranked supertypes (i.e., semantic classes)using their dataset of 24 relation patterns (i.e., lex-ical predicates).Dataset.
The dataset contained 14 lexical pred-icates (e.g., work for * or * fly to), 10 of whichwere expanded in order to semantify their left- andright-side arguments (e.g., * work for and workfor *); for the remaining 4 predicates just a single1228Total Linked in One sense One sense per Trust the Nottriples Wikipedia per page lexical predicate inventory linked73,843,415 1,795,608 1,433,634 533,946 1,716,813 68,363,414Table 5: Statistics on argument triple linking for all the lexical predicates in the Oxford dataset.k Prec@k Correct Total1 0.88 21 242 0.90 43 483 0.88 63 724 0.89 85 965 0.91 109 1206 0.91 131 1447 0.92 154 1688 0.91 175 1929 0.92 198 21610 0.92 221 24011 0.92 242 26412 0.92 264 28813 0.91 284 31214 0.90 304 33615 0.91 327 36016 0.91 348 38417 0.90 367 40818 0.89 386 43219 0.89 407 45620 0.89 429 480Table 6: Precision@k for the semantic classes ofthe relations of Kozareva and Hovy (2010).side was generalized (e.g., * dress).
While most ofthe relations apply to persons as a supertype, ourmethod could find arguments for each of them.Methodology.
We carried out the same evalua-tion as in Section 4.3.
We calculated precision@kof the semantic classes obtained for each relationin the dataset of K&H.
Because the set of appli-cable classes was potentially unbounded, we werenot able to report recall directly.Results.
K&H reported an overall accuracy ofthe top-20 supertypes of 92%.
As can be seen inTable 6 we exhibit very good performance with in-creasing values of k. A comparison of Table 3 withTable 6 shows considerable differences in perfor-mance between the two datasets.
We attribute thisdifference to the higher average WordNet poly-semy of the verbal component of the Oxford pred-icates (on average 2.64 senses for K&H against6.52 for the Oxford dataset).Although we cannot report recall, we list thenumber of Wikipedia arguments and associatedclasses in Table 7, which provides an estimate ofthe extraction capability of SPred.
The large num-ber of classes found for the arguments demon-strates the ability of our method to generalize toa variety of semantic classes.Predicate Number of args Number of classescause * 181,401 1,339live in * 143,628 600go to * 134,712 867* cause 92,160 1,244work in * 79,444 770* go to 71,794 746* live in 61,074 541work on * 58,760 840work for * 58,332 681work at * 31,904 511* work in 24,933 528* celebrate 23,333 408Table 7: Number of arguments and associatedclasses for the 12 most frequent lexical predicatesof Kozareva and Hovy (2010) extracted by SPredfrom Wikipedia.6 Related workThe availability of Web-scale corpora has led tothe production of large resources of relations (Et-zioni et al, 2005; Yates et al, 2007; Wu and Weld,2010; Carlson et al, 2010; Fader et al, 2011).However, these resources often operate purely atthe lexical level, providing no information on thesemantics of their arguments or relations.
Severalstudies have examined adding semantics throughgrouping relations into sets (Yates and Etzioni,2009), ontologizing the arguments (Chklovski andPantel, 2004), or ontologizing the relations them-selves (Moro and Navigli, 2013).
However, analy-sis has largely been either limited to ontologizinga small number of relation types with a fixed in-ventory, which potentially limits coverage, or hasused implicit definitions of semantic categories(e.g., clusters of arguments), which limits inter-pretability.
For example, Mohamed et al (2011)use the semantic categories of the NELL system(Carlson et al, 2010) to learn roughly 400 validontologized relations from over 200M web pages,whereas WiSeNet (Moro and Navigli, 2012) lever-ages Wikipedia to acquire relation synsets for anopen set of relations.
Despite these efforts, nolarge-scale resource has existed to date that con-tains ontologized lexical predicates.
In contrast,the present work provides a high-coverage methodfor learning argument supertypes from a broad-coverage ontology (WordNet), which can poten-tially be leveraged in relation extraction to ontolo-1229gize relation arguments.Our method for identifying the different seman-tic classes of predicate arguments is closely relatedto the task of identifying selectional preferences.The most similar approaches to it are taxonomy-based ones, which leverage the semantic typesof the relations arguments (Resnik, 1996; Li andAbe, 1998; Clark and Weir, 2002; Pennacchiottiand Pantel, 2006).
Nevertheless, despite their highquality sense-tagged data, these methods have of-ten suffered from lack of coverage.
As a result,alternative approaches have been proposed that es-chew taxonomies in favor of rating the quality ofpotential relation arguments (Erk, 2007; Cham-bers and Jurafsky, 2010) or generating probabil-ity distributions over the arguments (Rooth et al,1999; Pantel et al, 2007; Bergsma et al, 2008;Ritter et al, 2010; Se?aghdha, 2010; Bouma, 2010;Jang and Mostow, 2012) in order to obtain highercoverage of preferences.In contrast, we overcome the data sparsity ofclass-based models by leveraging the large quan-tity of collaboratively-annotated Wikipedia text inorder to connect predicate arguments with theirsemantic class in WordNet using BabelNet (Nav-igli and Ponzetto, 2012); because we map directlyto WordNet synsets, we provide a more readily-interpretable collocation preference model thanmost similarity-based or probabilistic models.Verb frame extraction (Green et al, 2004) andpredicate-argument structure analysis (Surdeanuet al, 2003; Yakushiji et al, 2006) are two areasthat are also related to our work.
But their gener-ality goes beyond our intentions, as we focus onsemantic predicates, which is much simpler andfree from syntactic parsing.Another closely related work is that of Hanks(2013) concerning the Theory of Norms and Ex-ploitations, where norms (exploitations) representexpected (unexpected) classes for a given lexicalpredicate.
Although our semantified predicates do,indeed, provide explicit evidence of norms ob-tained from collective intelligence and would pro-vide support for this theory, exploitations presenta more difficult task, different from the one ad-dressed here, due to its focus on identifying prop-erty transfer between the semantic class and theexploited instance.The closest technical approach to ours is thatof Kozareva and Hovy (2010), who use recursivepatterns to induce semantic classes for the argu-ments of relational patterns.
Whereas their ap-proach requires both a relation pattern and oneor more seeds, which bias the types of semanticclasses that are learned, our proposed method re-quires only the pattern itself, and as a result is ca-pable of learning an unbounded number of differ-ent semantic classes.7 ConclusionsIn this paper we present SPred, a novel approachto large-scale harvesting of semantic predicates.In order to semantify lexical predicates we ex-ploit the wide coverage of Wikipedia to extractand disambiguate lexical predicate occurrences,and leverage WordNet to populate the semanticclasses with suitable predicate arguments.
As a re-sult, we are able to ontologize lexical predicate in-stances like those available in existing dictionaries(e.g., break a toe) into semantic predicates (suchas break a BODY PART).For each lexical predicate (such as break ?
),our method produces a probability distributionover the set of semantic classes (thus covering thedifferent expected meanings for the filling argu-ments) and is able to classify new instances withthe most suitable class.
Our experiments showgenerally high performance, also in comparisonwith previous work on argument supertyping.We hope that our semantic predicates will en-able progress in different Natural Language Pro-cessing tasks such as Word Sense Disambigua-tion (Navigli, 2009), Semantic Role Labeling(Fu?rstenau and Lapata, 2012) or even Textual En-tailment (Stern and Dagan, 2012) ?
each of whichis in urgent need of reliable semantics.
While wefocused on semantifying lexical predicates, as fu-ture work we will apply our method to the ontol-ogization of large amounts of sequences of words,such as phrases or textual relations (e.g., consid-ering Google n-grams appearing in Wikipedia).Notably, our method should, in principle, gener-alize to any semantically-annotated corpus (e.g.,Wikipedias in other languages), provided lexicalpredicates can be extracted with associated seman-tic classes.In order to support future efforts we are releas-ing our semantic predicates as a freely availableresource.12AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.Thanks go to David A. Jurgens, Silvia Necs?ulescu,Stefano Faralli and Moreno De Vincenzi for theirhelp.12http://lcl.uniroma1.it/spred1230ReferencesJonathan Berant, Ido Dagan, and Jacob Goldberger.2012.
Learning entailment relations by global graphstructure optimization.
Computational Linguistics,38(1):73?111.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Discriminative learning of selectional prefer-ence from unlabeled text.
In Proc.
of EMNLP, pages59?68, Stroudsburg, PA, USA.Christian Bizer, Jens Lehmann, Georgi Kobilarov,So?ren Auer, Christian Becker, Richard Cyganiak,and Sebastian Hellmann.
2009.
DBpedia - a crystal-lization point for the Web of Data.
Web Semantics,7(3):154?165.Gerlof Bouma.
2010.
Collocation Extraction beyondthe Independence Assumption.
In Proc.
of ACL,Short Papers, pages 109?114, Uppsala, Sweden.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka, and Tom M. Mitchell.2010.
Toward an architecture for never-ending lan-guage learning.
In Proc.
of AAAI, pages 1306?1313,Atlanta, Georgia.Nathanael Chambers and Dan Jurafsky.
2010.
Improv-ing the use of pseudo-words for evaluating selec-tional preferences.
In Proc.
of ACL, pages 445?453,Stroudsburg, PA, USA.Tim Chklovski and Patrick Pantel.
2004.
VerbOcean:Mining the Web for fine-grained semantic verb rela-tions.
In Proc.
of EMNLP, pages 33?40, Barcelona,Spain.Jennifer Chu-Carroll and John Prager.
2007.
An exper-imental study of the impact of information extractionaccuracy on semantic search performance.
In Proc.of CIKM, pages 505?514, Lisbon, Portugal.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-Coverage Sense Disambiguation and Infor-mation Extraction with a Supersense Sequence Tag-ger.
In Proc.
of EMNLP, pages 594?602, Sydney,Australia.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.Jonathan Crowther, editor.
1998.
Oxford AdvancedLearner?s Dictionary.
Cornelsen & Oxford, 5th edi-tion.Flavio De Benedictis, Stefano Faralli, and RobertoNavigli.
2013.
GlossBoot: Bootstrapping multilin-gual domain glossaries from the Web.
In Proc.
ofACL, Sofia, Bulgaria.Gerard de Melo and Gerhard Weikum.
2010.
MENTA:Inducing Multilingual Taxonomies from Wikipedia.In Proc.
of CIKM, pages 1099?1108, New York, NY,USA.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proc.
of EMNLP, pages 440?449, Stroudsburg, PA, USA.Katrin Erk.
2007.
A Simple, Similarity-based Modelfor Selectional Preferences.
In Proc.
of ACL, pages216?223, Prague, Czech Republic.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:an experimental study.
Artificial Intelligence,165(1):91?134.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying Relations for Open InformationExtraction.
In Proc.
of EMNLP, pages 1535?1545,Edinburgh, UK.Stefano Faralli and Roberto Navigli.
2013.
A Javaframework for multilingual definition and hypernymextraction.
In Proc.
of ACL, Comp.
Volume, Sofia,Bulgaria.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyan-pur, Adam Lally, J. William Murdock, Eric Nyberg,John M. Prager, Nico Schlaefer, and Christopher A.Welty.
2010.
Building Watson: an overview of theDeepQA project.
AI Magazine, 31(3):59?79.Hagen Fu?rstenau and Mirella Lapata.
2012.
Semi-supervised semantic role labeling via structuralalignment.
Computational Linguistics, 38(1):135?171.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In Proc.
ofHLT-NAACL, pages 1?8, Edmonton, Canada.Rebecca Green, Bonnie J. Dorr, and Philip Resnik.2004.
Inducing Frame Semantic Verb Classes fromWordNet and LDOCE.
In Proc.
of ACL, pages 375?382, Barcelona, Spain.Patrick Hanks.
2013.
Lexical Analysis: Norms andExploitations.
University Press Group Limited.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of COL-ING, pages 539?545, Nantes, France.Johannes Hoffart, Fabian M. Suchanek, KlausBerberich, and Gerhard Weikum.
2013.
Yago2: Aspatially and temporally enhanced knowledge basefrom wikipedia.
Artificial Intelligence, 194:28?61.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structured content and artificial intelligence: Thestory so far.
Artificial Intelligence, 194:2?27.Ruihong Huang and Ellen Riloff.
2010.
InducingDomain-Specific Semantic Class Taggers from (Al-most) Nothing.
In Proc.
of ACL, pages 275?285,Uppsala, Sweden.Sean P. Igo and Ellen Riloff.
2009.
Corpus-based se-mantic lexicon induction with Web-based corrobo-ration.
In Proc.
of UMSLLS, pages 18?26, Strouds-burg, PA, USA.Rube?n Izquierdo, Armando Sua?rez, and German Rigau.2009.
An Empirical Study on Class-Based WordSense Disambiguation.
In Proc.
of EACL, pages389?397, Athens, Greece.Hyeju Jang and Jack Mostow.
2012.
Inferring se-lectional preferences from part-of-speech n-grams.In Proc.
of EACL, pages 377?386, Stroudsburg, PA,USA.1231Boris Katz, Jimmy J. Lin, Daniel Loreto, Wesley Hilde-brandt, Matthew W. Bilotti, Sue Felshin, AaronFernandes, Gregory Marton, and Federico Mora.2003.
Integrating Web-based and Corpus-basedTechniques for Question Answering.
In Proc.
ofTREC, pages 426?435, Gaithersburg, Maryland.Zornitsa Kozareva and Eduard Hovy.
2010.
LearningArguments and Supertypes of Semantic RelationsUsing Recursive Patterns.
In Proc.
of ACL, pages1482?1491, Uppsala, Sweden.Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy.2008.
Semantic Class Learning from the Webwith Hyponym Pattern Linkage Graphs.
In Proc.ACL/HLT, pages 1048?1056, Columbus, Ohio.Sebastian Krause, Hong Li, Hans Uszkoreit, and FeiyuXu.
2012.
Large-scale learning of relation-extraction rules with distant supervision from theweb.
In Proc.
of ISWC 2012, Part I, pages 263?278,Boston, MA.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the MDL principle.Computational Linguistics, 24(2):217?244.Rada Mihalcea and Dan Moldovan.
eXtended Word-Net: Progress report.
In Proceedings of the NAACL-01 Workshop on WordNet and Other Lexical Re-sources, Pittsburgh, Penn.Thahir Mohamed, Estevam Hruschka, and TomMitchell.
2011.
Discovering Relations betweenNoun Categories.
In Proc.
of EMNLP, pages 1447?1455, Edinburgh, Scotland, UK.Andrea Moro and Roberto Navigli.
2012.
WiSeNet:Building a Wikipedia-based semantic network withontologized relations.
In Proc.
of CIKM, pages1672?1676, Maui, HI, USA.Andrea Moro and Roberto Navigli.
2013.
IntegratingSyntactic and Semantic Analysis into the Open In-formation Extraction Paradigm.
In Proc.
of IJCAI,Beijing, China.Vivi Nastase and Michael Strube.
2013.
Transform-ing wikipedia into a large scale multilingual conceptnetwork.
Artificial Intelligence, 194:62?85.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli and Paola Velardi.
2010.
LearningWord-Class Lattices for Definition and HypernymExtraction.
In Proc.
of ACL, pages 1318?1327, Up-psala, Sweden.Roberto Navigli.
2009.
Word Sense Disambiguation:A survey.
ACM Computing Surveys, 41(2):1?69.Patrick Pantel, Rahul Bhagat, Timothy Chklovski, andEduard Hovy.
2007.
ISP: learning inferential selec-tional preferences.
In Proc.
of NAACL, pages 564?571, Rochester, NY.Marius Pasca.
2004.
Acquisition of categorized namedentities for web search.
In Proc.
of CIKM, pages137?145, New York, NY, USA.Marco Pennacchiotti and Patrick Pantel.
2006.
On-tologizing semantic relations.
In Proc.
of COLING,pages 793?800, Sydney, Australia.Simone Paolo Ponzetto and Michael Strube.
2011.Taxonomy induction based on a collaboratively builtknowledge repository.
Artificial Intelligence, 175(9-10):1737?1756.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61(1):127?159.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet alocation method for selectional pref-erences.
In Proc.
of ACL, pages 424?434, Uppsala,Sweden.
ACL.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a seman-tically annotated lexicon via EM-based clustering.In Proc.
of ACL, pages 104?111, Stroudsburg, PA,USA.Diarmuid O Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proc.
of ACL, pages435?444, Uppsala, Sweden.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2004.Learning Syntactic Patterns for Automatic Hyper-nym Discovery.
In NIPS, pages 1297?1304, Cam-bridge, Mass.Asher Stern and Ido Dagan.
2012.
Biutee: A mod-ular open-source system for recognizing textual en-tailment.
In Proc.
of ACL 2012, System Demonstra-tions, pages 73?78, Jeju Island, Korea.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argumentstructures for information extraction.
In Proc.
ACL,pages 8?15, Stroudsburg, PA, USA.M.
Thelen and E. Riloff.
2002.
A BootstrappingMethod for Learning Semantic Lexicons using Ex-traction Pattern Contexts.
In Proc.
of EMNLP, pages214?221, Salt Lake City, UT, USA.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
OntoLearn Reloaded: A graph-based algo-rithm for taxonomy induction.
Computational Lin-guistics, 39(3).Yorick Wilks.
1975.
A preferential, pattern-seeking,semantics for natural language inference.
ArtificialIntelligence, 6(1):53?74.Fei Wu and Daniel S. Weld.
2010.
Open InformationExtraction Using Wikipedia.
In Proc.
of ACL, pages118?127, Uppsala, Sweden.Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, YukaTateisi, and Jun?ichi Tsujii.
2006.
Automatic con-struction of predicate-argument structure patternsfor biomedical information extraction.
In Proc.
ofEMNLP, pages 284?292, Stroudsburg, PA, USA.David Yarowsky.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
InProc.
of ACL, pages 189?196, Cambridge, MA,USA.Alexander Yates and Oren Etzioni.
2009.
Unsuper-vised methods for determining object and relationsynonyms on the web.
Journal of Artificial Intelli-gence Research, 34(1):255.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
TextRunner: open informa-tion extraction on the web.
In Proc.
of NAACL-Demonstrations, pages 25?26, Stroudsburg, PA,USA.1232
