Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 155?163,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsText Mining Techniques for Leveraging Positively Labeled DataLana Yeganova*, Donald C. Comeau, Won Kim, W. John WilburNational Center for Biotechnology Information, NLM, NIH, Bethesda, MD 20894 USA{yeganova, comeau, wonkim, wilbur}@mail.nih.gov* Corresponding author.
Tel.
:+1 301 402 0776AbstractSuppose we have a large collection ofdocuments most of which are unlabeled.
Supposefurther that we have a small subset of thesedocuments which represent a particular class ofdocuments we are interested in, i.e.
these arelabeled as positive examples.
We may have reasonto believe that there are more of these positiveclass documents in our large unlabeled collection.What data mining techniques could help us findthese unlabeled positive examples?
Here weexamine machine learning strategies designed tosolve this problem.
We find that a proper choice ofmachine learning method as well as trainingstrategies can give substantial improvement inretrieving, from the large collection, data enrichedwith positive examples.
We illustrate the principleswith a real example consisting of multiwordUMLS phrases among a much larger collection ofphrases from Medline.1 IntroductionGiven a large collection of documents, a few ofwhich are labeled as interesting, our task is toidentify unlabeled documents that are alsointeresting.
Since the labeled data represents thedata we are interested in, we will refer to it as thepositive class and to the remainder of the data asthe negative class.
We use the term negative class,however, documents in the negative class are notnecessarily negative, they are simply unlabeled andthe negative class may contain documents relevantto the topic of interest.
Our goal is to retrieve theseunknown relevant documents.A na?ve approach to this problem would simplytake the positive examples as the positive class andthe rest of the collection as the negative class andapply machine learning to learn the difference andrank the negative class based on the resultingscores.
It is reasonable to expect that the top of thisranking would be enriched for the positive class.But an appropriate choice of methods can improveover the na?ve approach.One issue of importance would be choosing themost appropriate machine learning method.
Ourproblem can be viewed from two differentperspectives: the problem of learning fromimbalanced data as well as the problem ofrecommender systems.
In terms of learning fromimbalanced data, our positive class is significantlysmaller than the negative, which is the remainderof the collection.
Therefore we are learning fromimbalanced data.
Our problem is also arecommender problem in that based on a fewexamples found of interest to a customer we seeksimilar positive examples amongst a largecollection of unknown status.
Our bias is to usesome form of wide margin classifier for ourproblem as such classifiers have given goodperformance for both the imbalanced data problemand the recommender problem (Zhang and Iyengar2002; Abkani, Kwek et al 2004; Lewis, Yang etal.
2004).Imbalanced data sets arise very frequently intext classification problems.
The issue withimbalanced learning is that the large prevalence ofnegative documents dominates the decisionprocess and harms classification performance.Several approaches have been proposed to dealwith the problem including sampling methods andcost-sensitive learning methods and are describedin (Chawla, Bowyer et al 2002; Maloof 2003;Weiss, McCarthy et al 2007).
These studies haveshown that there is no clear advantage of oneapproach versus another.
Elkan (2001) points outthat cost-sensitive methods and sampling methodsare related in the sense that altering the classdistribution of training data is equivalent toaltering misclassification cost.
Based on thesestudies we examine cost-sensitive learning inwhich the cost on the positive set is increased, as auseful approach to consider when using an SVM.In order to show how cost-sensitive learning foran SVM is formulated, we write the standardequations for an SVM following (Zhang 2004).155Given training data ?
??
?,i ix y  where iy  is 1 or ?1depending on whether the data pointix  isclassified as positive or negative, an SVM seeksthat vectoriw  which minimizes?
?
2( )     (1)2i ii h y x w w???
?
?
?where the loss function is defined by?
?
1 ,  1           (2) 0,         z>1.z zh z ?
???
?
?The cost-sensitive version modifies (1) to becomeand now we can choose r?
and r?
to magnify thelosses appropriately.
Generally we take r?
to be 1,and r?
to be some factor larger than 1.
We refer tothis formulation as CS-SVM.
Generally, the samealgorithms used to minimize (1) can be used tominimize (3).Recommender systems use historical data onuser preferences, purchases and other availabledata to predict items of interest to a user.
Zhangand Iyengar (2002) propose a wide marginclassifier with a quadratic loss function as veryeffective for this purpose (see appendix).
It is usedin (1) and requires no adjustment in cost betweenpositive and negative examples.
It is proposed as abetter method than varying costs because it doesnot require searching for the optimal costrelationship between positive and negativeexamples.
We will use for our wide marginclassifier the modified Huber loss function (Zhang2004).
The modified Huber loss function isquadratic where this is important and has the form?
?
?
?24 ,     1(4)1 ,  -1 10,     z>1.z zh z z z?
?
?
????
?
?
???
?We also use it in (1).
We refer to this approach asthe Huber method (Zhang 2004) as opposed toSVM.
We compare it with SVM and CS-SVM.
Weused our own implementations for SVM, CS-SVM,and Huber that use gradient descent to optimize theobjective function.The methods we develop are related to semi-supervised learning approaches (Blum andMitchell 1998; Nigam, McCallum et al 1999) andactive learning (Roy and McCallum 2001; Tongand Koller 2001).
Our method differs from activelearning in that active learning seeks thoseunlabeled examples for which labels prove mostinformative in improving the classifier.
Typicallythese examples are the most uncertain.
Some semi-supervised learning approaches start with labeledexamples and iteratively seek unlabeled examplesclosest to already labeled data and impute theknown label to the nearby unlabeled examples.
Ourgoal is simply to retrieve plausible members for thepositive class with as high a precision as possible.Our method has value even in cases where humanreview of retrieved examples is necessary.
Theimbalanced nature of the data and the presence ofpositives in the negative class make this achallenging problem.In Section 2 we discuss additional strategiesproposed in this work, describe the data used anddesign of experiments, and provide the evaluationmeasure used.
In Section 3 we present our results,in Sections 4 and 5 we discuss our approach anddraw conclusions.2 Methods2.1 Cross TrainingLet D  represent our set of documents, and C?those documents that are known positives in D .Generally C?
would be a small fraction of D  andfor the purposes of learning we assume that\C D C?
??
.We are interested in the case when some of thenegatively labeled documents actually belong tothe positive class.
We will apply machine learningto learn the difference between the documents inthe class C?
and documents in the class C?
anduse the weights obtained by training to score thedocuments in the negative class C?
.
The highestscoring documents in set C?
are candidatemislabeled documents.
However, there may be aproblem with this approach, because the classifieris based on partially mislabeled data.
Candidate?
?
?
?
2( ) ( )  (3)2i i i ii C i Cr h y x w r h y x w w??
??
??
??
??
?
?
?
?
?
?
??
?156mislabeled documents are part of the C?
class.
Inthe process of training, the algorithm purposelylearns to score them low.
This effect can bemagnified by any overtraining that takes place.
Itwill also be promoted by a large number offeatures, which makes it more likely that anypositive point in the negative class is in someaspect different from any member of C?
.Another way to set up the learning is byexcluding documents from directly participating inthe training used to score them.
We first divide thenegative set into disjoint pieces1 2C Z Z?
?
?Then train documents in C?
versus documents in1Z  to rank documents in 2Z  and train documentsin C?
versus documents in 2Z  to rank documentsin1Z .
We refer to this method as cross training(CT).
We will apply this approach and show that itconfers benefit in ranking the false negatives inC?
.2.2 Data Sources and PreparationThe databases we studied are MeSH25, Reuters,20NewsGroups, and MedPhrase.MeSH25.
We selected 25 MeSH?
terms withoccurrences covering a wide frequency range: from1,000 to 100,000 articles.
A detailed explanation ofMeSH can be found athttp://www.nlm.nih.gov/mesh/.For a given MeSH term m, we treat the recordsassigned that MeSH term m as positive.
Theremaining MEDLINE?
records do not have massigned as a MeSH and are treated as negative.Any given MeSH term generally appears in a smallminority of the approximately 20 million MEDLINEdocuments making the data highly imbalanced forall MeSH terms.Reuters.
The data set consists of 21,578 Reutersnewswire articles in 135 overlapping topiccategories.
We experimented on the 23 mostpopulated classes.For each of these 23 classes, the articles in theclass of interest are positive, and the rest of 21,578articles are negative.
The most populous positiveclass contains 3,987 records, and the leastpopulous class contains 112 records.20NewsGroups.
The dataset is a collection ofmessages from twenty different newsgroups withabout one thousand messages in each newsgroup.We used each newsgroup as the positive class andpooled the remaining nineteen newsgroups as thenegative class.Text in the MeSH25 and Reuters databases hasbeen preprocessed as follows: all alphabeticcharacters were lowercased, non-alphanumericcharacters replaced by blanks, and no stemmingwas done.
Features in the MeSH25 dataset are allsingle nonstop terms and all pairs of adjacentnonstop terms that are not separated bypunctuation.
Features in the Reuters database aresingle nonstop terms only.
Features in the20Newsgroups are extracted using the Rainbowtoolbox (McCallum 1996).MedPhrase.
We process MEDLINE to extract allmultiword UMLS?
(http://www.nlm.nih.gov/research/umls/) phrasesthat are present in MEDLINE.
From the resultingset of strings, we drop the strings that containpunctuation marks or stop words.
The remainingstrings are normalized (lowercased, redundantwhite space is removed) and duplicates areremoved.
We denote the resulting set of 315,679phrases byphrasesU .For each phrase in ,phrasesU  we randomlysample, as available, up to 5 MEDLINE sentencescontaining it.
We denote the resulting set of728,197 MEDLINE sentences byphrasesS .
FromphrasesS  we extract all contiguous multiwordexpressions that are not present inphrasesU .
Wecall them n-grams, where n>1.
N-grams containingpunctuation marks and stop words are removedand remaining n-grams are normalized andduplicates are dropped.
The result is 8,765,444 n-grams that we refer to as .ngramM  We believe thatngramM contains many high quality biologicalphrases.
We usephrasesU  , a known set of highquality biomedical phrases, as the positive class,andngramMas the negative class.In order to apply machine learning we need todefine features for each n-gram.
Given an n-gramgrm that is composed of n  words,1 2 ngrm w w w?
, we extract a set of 11 numbers157?
?111i if ?
associated with the n-gram grm.
These areas follows:f1: number of occurrences of grm throughoutMedline;f2: -(number of occurrences of w2?wn notfollowing w1 in documents that contain grm)/ f1;f3: -(number of occurrences of w1?wn-1 notpreceding wn in documents that contain grm)/ f1;f4: number of occurrences of (n+1)-grams of theform xw1?wn throughout Medline;f5: number of occurrences of (n+1)-grams  ofthe form w1?wn x throughout Medline;f6: ?
?
?
??
??
??
?
?
?1 2 1 21 2 1 2| 1 |log 1 | |p w w p w wp w w p w w?
??
??
??
??
??
?f7: mutual information between w1 and w2;f8: ?
?
?
??
??
??
?
?
?1 11 1| 1 |log 1 | |n n n nn n n np w w p w wp w w p w w?
??
??
??
??
??
??
?f9: mutual information between wn-1 and wn;f10: -(number of different multiword expressionsbeginning with w1 in Medline);f11: -(number of different multiword expressionsending with wn in Medline).We discretize the numeric values of the ?
?111i if ?into categorical values.In addition to these features, for every n-gramgrm, we include the part of speech tags predictedby the MedPost tagger (Smith, Rindflesch et al2004).
To obtain the tags for a given n-gram grmwe randomly select a sentence fromphrasesScontaining grm, tag the sentence, and consider thetags0 1 2 1 1n n nt t t t t t?
?
where 0t is the tag of theword preceding word1w in n-gram grm, 1t  is thetag of word1w  in n-gram grm, and so on.
Weconstruct the featuresThese features emphasize the left and right ends ofthe n-gram and include parts-of-speech in themiddle without marking their position.
Theresulting features are included with ?
?111i if ?
torepresent the n-gram.2.3 Experimental DesignA standard way to measure the success of aclassifier is to evaluate its performance on acollection of documents that have been previouslyclassified as positive or negative.
This is usuallyaccomplished by randomly dividing up the datainto training and test portions which are separate.The classifier is then trained on the trainingportion, and is tested on test portion.
This can bedone in a cross-validation scheme or by randomlyre-sampling train and test portions repeatedly.We are interested in studying the case whereonly some of the positive documents are labeled.We simulate that situation by taking a portion ofthe positive data and including it in the negativetraining set.
We refer to that subset of positivedocuments as tracer data (Tr).
The tracer data isthen effectively mislabeled as negative.
Byintroducing such an artificial supplement to thenegative training set we are not only certain thatthe negative set contains mislabeled positiveexamples, but we know exactly which ones theyare.
Our goal is to automatically identify thesemislabeled documents in the negative set andknowing their true labels will allow us to measurehow successful we are.
Our measurements will becarried out on the negative class and for thispurpose it is convenient to write the negative classas composed of true negatives and tracer data(false negatives)'C C Tr?
??
?
.When we have trained a classifier, we evaluateperformance by ranking 'C?
and measuring howwell tracer data is moved to the top ranks.
Thechallenge is that Tr appears in the negative classand will interact with the training in some way.2.4 EvaluationWe evaluate performance using Mean AveragePrecision (MAP) (Baeza-Yates and Ribeiro-Neto1999).
The mean average precision is the meanvalue of the average precisions computed for alltopics in each of the datasets in our study.
Averageprecision is the average of the precisions at eachrank that contains a true positive document.?
?
?
??
??
??
??
?
?
??
??
??
?0 1 1 2 10 1 1if 2 :  ,1 , , 2 ,3 ,4 , ,...,otherwise: ,1 , , 2 ,3 ,4 .n nn nn t t t t t tt t t t?
???
????
?158Table 1: MAP scores trained with three levels of tracer data introduced to the negative training set.No Cross Training No Tracer Data Tr20 in training Tr50 in trainingMeSH Terms Huber SVM Huber SVM Huber SVMceliac disease 0.694 0.677 0.466 0.484 0.472 0.373lactose intolerance 0.632 0.635 0.263 0.234 0.266 0.223myasthenia gravis 0.779 0.752 0.632 0.602 0.562 0.502carotid stenosis 0.466 0.419 0.270 0.245 0.262 0.186diabetes mellitus 0.181 0.181 0.160 0.129 0.155 0.102rats, wistar 0.241 0.201 0.217 0.168 0.217 0.081myocardial infarction 0.617 0.575 0.580 0.537 0.567 0.487blood platelets 0.509 0.498 0.453 0.427 0.425 0.342serotonin 0.514 0.523 0.462 0.432 0.441 0.332state medicine 0.158 0.164 0.146 0.134 0.150 0.092urinary bladder 0.366 0.379 0.312 0.285 0.285 0.219drosophila melanogaster 0.553 0.503 0.383 0.377 0.375 0.288tryptophan 0.487 0.480 0.410 0.376 0.402 0.328laparotomy 0.186 0.173 0.138 0.101 0.136 0.066crowns 0.520 0.497 0.380 0.365 0.376 0.305streptococcus mutans 0.795 0.738 0.306 0.362 0.218 0.306infectious mononucleosis 0.622 0.614 0.489 0.476 0.487 0.376blood banks 0.283 0.266 0.170 0.153 0.168 0.115humeral fractures 0.526 0.495 0.315 0.307 0.289 0.193tuberculosis, lymph node 0.385 0.397 0.270 0.239 0.214 0.159mentors 0.416 0.420 0.268 0.215 0.257 0.137tooth discoloration 0.499 0.499 0.248 0.215 0.199 0.151pentazocine 0.710 0.716 0.351 0.264 0.380 0.272hepatitis e 0.858 0.862 0.288 0.393 0.194 0.271genes, p16 0.278 0.313 0.041 0.067 0.072 0.058Avg 0.491 0.479 0.321 0.303 0.303 0.2383 Results3.1 MeSH25, Reuters, and 20NewsGroupsWe begin by presenting results for the MeSH25dataset.
Table 1 shows the comparison betweenHuber and SVM methods.
It also compares theperformance of the classifiers with different levelsof tracer data in the negative set.
We set aside 50%of C?
to be used as tracer data and used theremaining 50% of C?
as the positive set fortraining.
We describe three experiments where wehave different levels of tracer data in the negativeset at training time.
These sets are ,C?
20 ,C Tr?
?and50  C Tr?
?
representing no tracer data, 20% ofC?
as tracer data and 50% of C?
as tracer data,respectively.
The test set20C Tr?
?
is the same forall of these experiments.
Results indicate that onaverage Huber outperforms SVM on these highlyimbalanced datasets.
We also observe thatperformance of both methods deteriorates withincreasing levels of tracer data.Table 2 shows the performance of Huber andSVM methods on negative training sets with tracerdata20C Tr?
?
and 50  C Tr?
?
as in Table 1, butwith cross training.
As mentioned in the Methodssection, we first divide each negative training setinto two disjoint pieces1Z  and 2Z .
We then traindocuments in the positive training set versusdocuments in1Z  to score documents in 2Z  andtrain documents in the positive training set versusdocuments in2Z  to score documents in 1Z .
Wethen merge1Z  and 2Z  as scored sets and reportmeasurements on the combined ranked set ofdocuments.
Comparing with Table 1, we see asignificant improvement in the MAP when usingcross training.159Table 2: MAP scores for Huber and SVM trained with two levels of tracer data introduced to thenegative training set using cross training technique.2-fold Cross Training Tr20 in training Tr50 in trainingMeSH Terms Huber SVM Huber SVMceliac disease 0.550 0.552 0.534 0.521lactose intolerance 0.415 0.426 0.382 0.393myasthenia gravis 0.652 0.643 0.623 0.631carotid stenosis 0.262 0.269 0.241 0.241diabetes mellitus 0.148 0.147 0.144 0.122rats, wistar 0.212 0.186 0.209 0.175myocardial infarction 0.565 0.556 0.553 0.544blood platelets 0.432 0.435 0.408 0.426serotonin 0.435 0.447 0.417 0.437state medicine 0.135 0.136 0.133 0.132urinary bladder 0.295 0.305 0.278 0.280drosophila melanogaster 0.426 0.411 0.383 0.404tryptophan 0.405 0.399 0.390 0.391laparotomy 0.141 0.128 0.136 0.126crowns 0.375 0.376 0.355 0.353streptococcus mutans 0.477 0.517 0.448 0.445infectious mononucleosis 0.519 0.514 0.496 0.491blood banks 0.174 0.169 0.168 0.157humeral fractures 0.335 0.335 0.278 0.293tuberculosis, lymph node 0.270 0.259 0.262 0.244mentors 0.284 0.278 0.275 0.265tooth discoloration 0.207 0.225 0.209 0.194pentazocine 0.474 0.515 0.495 0.475hepatitis e 0.474 0.499 0.482 0.478genes, p16 0.102 0.101 0.083 0.093Avg 0.350 0.353 0.335 0.332We performed similar experiments with theReuters and 20NewsGroups datasets, where 20%and 50% of the good set is used as tracer data.
Wereport MAP scores for these datasets in Tables 3and 4.3.2 Identifying high quality biomedicalphrases in the MEDLINE DatabaseWe illustrate our findings with a real exampleof detecting high quality biomedical phrasesamong ,ngramM a large collection of multiwordexpressions from Medline.
We believe thatngramMcontains many high quality biomedical phrases.These examples are the counterpart of themislabeled positive examples (tracer data) in theprevious tests.Table 3: MAP scores for Huber and SVMtrained with 20% and 50% tracer data introduced tothe negative training set for Reuters dataset.ReutersTr20 in training Tr50 in trainingHuber SVM Huber SVMNo CT 0.478 0.451 0.429 0.4032-Fold CT 0.662 0.654 0.565 0.555Table 4: MAP scores for Huber and SVMtrained with 20% and 50% tracer data introduced tothe negative training set for 20NewsGroups dataset.20NewsGroupsTr20 in training Tr50 in trainingHuber SVM Huber SVMNo CT 0.492 0.436 0.405 0.3502-Fold CT 0.588 0.595 0.502 0.512160To identify these examples, we learn thedifference between the phrases inphrasesUand.ngramM  Based on the training we rank the n-gramsin .ngramMWe expect the n-grams that cannot beseparated from UMLS phrases are high qualitybiomedical phrases.
In our experiments, weperform 3-fold cross validation for training andtesting.
This insures we obtain any possible benefitfrom cross training.
The results shown in figure 1are MAP values for these 3 folds.Figure 1.
Huber, CS-SVM, and na?ve Bayesclassifiers applied to the MedPhrase dataset.We trained na?ve Bayes, Huber, and CS-SVMwith a range of different cost factors.
The resultsare presented in Figure 1.
We observe that theHuber classifier performs better than na?ve Bayes.CS-SVM with the cost factor of 1 (standard SVM)is quite ineffective.
As we increase the cost factor,the performance of CS-SVM improves until it iscomparable to Huber.
We believe that the qualityof ranking is better when the separation ofphrasesUfromngramM  is better.Because we have no tracer data we have nodirect way to evaluate the ranking of .ngramMHowever, we selected a random set of 100 n-gramsfrom ,ngramM  which score as high as top-scoring10% of phrases inphrasesU .
Two reviewersmanually reviewed that list and identified that 99of these 100 n-grams were high quality biomedicalphrases.
Examples are: aminoshikimate pathway,berberis aristata, dna hybridization, subcellulardistribution, acetylacetoin synthase, etc.
One false-positive example in that list was congestive heart.4 DiscussionWe observed that the Huber classifier performsbetter than SVM on imbalanced data with no crosstraining (see appendix).
The improvement ofHuber over SVM becomes more marked as thepercentage of tracer data in the negative trainingset is increased.
However, the results also showthat cross training, using either SVM or Huber(which are essentially equivalent), is better thanusing Huber without cross training.
This isdemonstrated in our experiments using the tracerdata.
The results are consistent over the range ofdifferent data sets.
We expect cross training tohave benefit in actual applications.Where does cost-sensitive learning fit into thispicture?
We tested cost-sensitive learning on all ofour corpora using the tracer data.
We observedsmall and inconsistent improvements (data notshown).
The optimal cost factor varied markedlybetween cases in the same corpus.
We could notconclude this was a useful approach and insteadsaw better results simply using Huber.
Thisconclusion is consistent with (Zhang and Iyengar2002) which recommend using a quadratic lossfunction.
It is also consistent with results reportedin (Lewis, Yang et al 2004) where CS-SVM iscompared with SVM on multiple imbalanced textclassification problems and no benefit is seen usingCS-SVM.
Others have reported a benefit with CS-SVM (Abkani, Kwek et al 2004; Eitrich and Lang2005).
However, their datasets involve relativelyfew features and we believe this is an importantaspect where cost-sensitive learning has proveneffective.
We hypothesize that this is the casebecause with few features the positive data is morelikely to be duplicated in the negative set.
In ourcase, the MedPhrase dataset involves relativelyfew features (410) and indeed we see a dramaticimprovement of CS-SVM over SVM.One approach to dealing with imbalanced datais the artificial generation of positive examples asseen with the SMOTE algorithm (Chawla, Bowyeret al 2002).
We did not try this method and do notknow if this approach would be beneficial for1 11 21 31 41Cost  Factor r+0.200.220.240.260.28AveragePrecisionComparison of different Machine Learning MethodsHuberCS-SVMBayes161textual data or data with many features.
This is anarea for possible future research.Effective methods for leveraging positivelylabeled data have several potential applications:?
Given a set of documents discussing aparticular gene, one may be interested infinding other documents that talk about thesame gene but use an alternate form of thegene name.?
Given a set of documents that are indexed witha particular MeSH term, one may want to findnew documents that are candidates for beingindexed with the same MeSH term.?
Given a set of papers that describe a particulardisease, one may be interested in otherdiseases that exhibit a similar set of symptoms.?
One may identify incorrectly tagged webpages.These methods can address both removingincorrect labels and adding correct ones.5 ConclusionsGiven a large set of documents and a small setof positively labeled examples, we study how bestto use this information in finding additionalpositive examples.
We examine the SVM andHuber classifiers and conclude that the Huberclassifier provides an advantage over the SVMclassifier on such imbalanced data.
We introduce atechnique which we term cross training.
When thistechnique is applied we find that the SVM andHuber classifiers are essentially equivalent andsuperior to applying either method without crosstraining.
We confirm this on three differentcorpora.
We also analyze an example where cost-sensitive learning is effective.
We hypothesize thatwith datasets having few features, cost-sensitivelearning can be beneficial and comparable to usingthe Huber classifier.Appendix: Why Huber Loss Function worksbetter for problems with Unbalanced ClassDistributions.The drawback of the standard SVM for theproblem with an unbalanced class distributionresults from the shape of ( )h z  in (2).
Consider theinitial condition at 0w ?
and also imagine that there isa lot more C?
training data than C?
training data.
Inthis case, by choosing 1?
?
?
, we can achieve theminimum value of the loss function in (1) for the initialcondition 0w ?
.
Under these conditions, all C?
pointsyield 1z ?
and ( ) 0h z ?
and all C?
points yield1z ?
?
and ( ) 2h z ?
.
The change of the loss function( )h z?
in (2) with a change w?
is given byIn order to reduce the loss at a C?data point ( , )i ix y ,we must choose w?
such that 0.ix w??
?
But weassume that there are significantly more C?
classdata points than C?
and many such points x?
aremislabeled and close toix  such that 0.x w?
??
?Then ( )h z  is likely be increased by ( 0)x w?
??
?for these mislabeled points.
Clearly, if there aresignificantly more C?
class data than those of  C?class and the C?
set  contains a lot of mislabeledpoints, it may be difficult to find w?
that canresult in a net effect of decreasing the right handside of (2).
The above analysis shows why thestandard support vector machine formulation in (2)is vulnerable to an unbalanced and noisy trainingdata set.
The problem is clearly caused by the factthat the SVM loss function ( )h z  in (2) has aconstant slope for 1z ?
.
In order to alleviate thisproblem, Zhang and Iyengar (2002) proposed theloss function 2 ( )h z  which is a smooth non-increasing function with slope 0 at 1z ?
.
Thisallows the loss to decrease while the positivepoints move a small distance away from the bulkof the negative points and take mislabeled pointswith them.
The same argument applies to theHuber loss function defined in (4).AcknowledgmentsThis research was supported by the Intramural ResearchProgram of the NIH, National Library of Medicine.?
?
( )      (5).w i idh zh z z w y x wdz?
?
?
??
?
?
?
?162ReferencesAbkani, R., S. Kwek, et al (2004).
Applying SupportVector Machines to Imballanced Datasets.
ECML.Baeza-Yates, R. and B. Ribeiro-Neto (1999).
ModernInformation Retrieval.
New York, ACM Press.Blum, A. and T. Mitchell (1998).
"Combining Labeledand Unlabeled Data with Co-Training."
COLT:Proceedings of the Workshop on ComputationalLearning Theory: 92-100.Chawla, N. V., K. W. Bowyer, et al (2002).
"SMOTE:Synthetic Minority Over-sampling Technique."
Journalof Artificial Intelligence Research 16: 321-357.Eitrich, T. and B. Lang (2005).
"Efficient optimizationof support vector machine learning parameters forunbalanced datasets."
Journal of Computational andApplied Mathematics 196(2): 425-436.Elkan, C. (2001).
The Foundations of Cost SensitiveLearning.
Proceedings of the Seventeenth InternationalJoint Conference on Artificial Intelligence.Lewis, D. D., Y. Yang, et al (2004).
"RCV1: A NewBenchmark Collection for Text CategorizationResearch."
Journal of Machine Learning Research 5:361-397.Maloof, M. A.
(2003).
Learning when data sets areimbalanced and when costs are unequal and unknown.ICML 2003, Workshop on Imballanced Data Sets.McCallum, A. K. (1996).
"Bow: A toolkit for statisticallanguage modeling, text retrieval, classification andclustering.
http://www.cs.cmu.edu/~mccallum/bow/.
"Nigam, K., A. K. McCallum, et al (1999).
"TextClassification from Labeled and Unlabeled Documentsusing EM."
Machine Learning: 1-34.Roy, N. and A. McCallum (2001).
Toward OptimalActive Learning through Sampling Estimation of ErrorReduction.
Eighteenth International Conference onMachine Learning.Smith, L., T. Rindflesch, et al (2004).
"MedPost: A partof speech tagger for biomedical text."
Bioinformatics20: 2320-2321.Tong, S. and D. Koller (2001).
"Support vector machineactive learning with applications to text classification.
"Journal of Machine Learning Research 2: 45-66.Weiss, G., K. McCarthy, et al (2007).
Cost-SensitiveLearning vs. Sampling: Which is Best for HandlingUnbalanced Classes with Unequal Error Costs?Proceedings of the 2007 International Conference onData Mining.Zhang, T. (2004).
Solving large scale linear predictionproblems using stochastic gradient descent algorithms.Twenty-first International Conference on Machinelearning, Omnipress.Zhang, T. and V. S. Iyengar (2002).
"RecommenderSystems Using Linear Classifiers."
Journal of MachineLearning Research 2: 313-334.163
