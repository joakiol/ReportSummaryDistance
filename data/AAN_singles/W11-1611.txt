Workshop on Monolingual Text-To-Text Generation, pages 91?97,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91?97,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsEvaluating sentence compression: Pitfalls and suggested remediesCourtney Napoles1 and Benjamin Van Durme1,2 and Chris Callison-Burch11Department of Computer Science2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractThis work surveys existing evaluationmethodologies for the task of sentencecompression, identifies their shortcomings,and proposes alternatives.
In particular,we examine the problems of evaluatingparaphrastic compression and comparing theoutput of different models.
We demonstratethat compression rate is a strong predictorof compression quality and that perceivedimprovement over other models is often a sideeffect of producing longer output.1 IntroductionSentence compression is the natural language gen-eration (NLG) task of automatically shortening sen-tences.
Because good compressions should be gram-matical and retain important meaning, they must beevaluated along these two dimensions.
Evaluation isa difficult problem for NLG, and many of the prob-lems identified in this work are relevant for othergeneration tasks.
Shared tasks are popular in manyareas as a way to compare system performance in anunbiased manner.
Unlike other tasks, such as ma-chine translation, there is no shared-task evaluationfor compression, even though some compressionsystems are indirectly evaluated as a part of DUC.The benefits of shared-task evaluation have been dis-cussed before (e.g., Belz and Kilgarriff (2006) andReiter and Belz (2006)), and they include compar-ing systems fairly under the same conditions.One difficulty in evaluating compression systemsfairly is that an unbiased automatic metric is hardto define.
Automatic evaluation relies on a com-parison to a single gold standard at a predeterminedlength, which greatly limits the types of compres-sions that can be fairly judged.
As we will discussin Section 2.1.1, automatic evaluation assumes thatdeletions are independent, considers only a singlegold standard, and cannot handle compressions withparaphrasing.
Like for most areas in NLG, humanevaluation is preferable.
However, as we discuss inSection 2.2, there are some subtleties to appropri-ate experiment design, which can give misleadingresults if not handled properly.This work identifies the shortcomings of widelypracticed evaluation methodologies and proposes al-ternatives.
We report on the effect of compressionrate on perceived quality and suggest ways to controlfor this dependency when evaluating across differentsystems.
In this work we:?
highlight the importance of comparing systemswith similar compression rates,?
argue that comparisons in many previous pub-lications are invalid,?
provide suggestions for unbiased evaluation.While many may find this discussion intuitive, thesepoints are not addressed in much of the existing re-search, and therefore it is crucial to enumerate themin order to improve the scientific validity of the task.2 Current PracticesBecause it was developed in support of extractivesummarization (Knight and Marcu, 2000), com-pression has mostly been framed as a deletion task(e.g., McDonald (2006), Galanis and Androutsopou-los (2010), Clarke and Lapata (2008), and Galley91Words Sentence31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for trans-porting bombs and bomb parts from Montana to California and mailing them to victims .17 Kaczynski faces charges naming him responsible for transporting bombs to California and mailing them to victims .18 Kaczynski faces charges naming him responsible for transporting bombs and bomb parts and mailing them to victims .18 Kaczynski faces a 10-count federal indictment for transporting bombs and bomb parts and mailing them to victims .Table 1: Three acceptable compressions of a sentence created by different annotators (the first is the original).and McKeown (2007)).
In this context, a compres-sion is an extracted subset of words from a longsentence.
There are limited compression corporabecause, even when an aligned corpus exists, thenumber of extractive sentence pairs will be few andtherefore gold-standard compressions must be man-ually annotated.
The most popular corpora are theZiff-Davis corpus (Knight and Marcu, 2000), whichcontains a small set of 1067 extracted sentencesfrom article/abstract pairs, and the manually anno-tated Clarke and Lapata (2008) corpus, consisting ofnearly 3000 sentences from news articles and broad-cast news transcripts.
These corpora contain onegold standard for each sentence.2.1 Automatic TechniquesOne of the most widely used automatic metrics is theF1 measure over grammatical relations of the gold-standard compressions (Riezler et al, 2003).
Thismetric correlates significantly with human judg-ments and is better than Simple String Accuracy(Bangalore et al, 2000) for judging compressionquality (Clarke and Lapata, 2006).
F1 has also beenused over unigrams (Martins and Smith, 2009) andbigrams (Unno et al, 2006).
Unno et al (2006)compared the F1 measures to BLEU scores (usingthe gold standard as a single reference) over vary-ing compression rates, and found that BLEU be-haves similarly to both F1 measures.
A syntacticapproach considers the alignment over parse trees(Jing, 2000), and a similar technique has been usedwith dependency trees to evaluate the quality of sen-tence fusions (Marsi and Krahmer, 2005).The only metric that has been shown to correlatewith human judgments is F1 (Clarke and Lapata,2006), but even this is not entirely reliable.
F1 overgrammatical relations also depends on parser accu-racy and the type of dependency relations used.11For example, the RASP parser uses 16 grammatical depen-2.1.1 Pitfalls of Automatic EvaluationAutomatic evaluation operates under three oftenincorrect assumptions:Deletions are independent.
The dependencystructure of a sentence may be unaltered when de-pendent words are not deleted as a unit.
Examplesof words that should be treated as a single unit in-clude negations and negative polarity items or cer-tain multi-word phrases (such as deleting Latin andleaving America).
F1 treats all deletions equally,when in fact errors of this type may dramatically al-ter the meaning or the grammaticality of a sentenceand should be penalized more than less serious er-rors, such as deleting an article.The gold standard is the single best compres-sion.
Automatic evaluation considers a singlegold-standard compression.
This ignores the pos-sibility of different length compressions and equallygood compressions of the same length, where mul-tiple non-overlapping deletions are acceptable.
Foran example, see Table 1.Having multiple gold standards would providereferences at different compression lengths and re-flect different deletion choices (see Section 3).
Sinceno large corpus with multiple gold standards existsto our knowledge, systems could instead report thequality of compressions at several different com-pression rates, as Nomoto (2008) did.
Alternatively,systems could evaluate compressions that are of asimilar length as the gold standard compression, tofix a length for the purpose of evaluation.
Outputlength is controlled for evaluation in some other ar-eas, notably DUC.Systems compress by deletion and not substitu-tion.
More recent approaches to compression in-troduce reordering and paraphrase operations (e.g.,dencies (Briscoe, 2006) while there are over 50 Stanford De-pendencies (de Marneffe and Manning, 2008).92Cohn and Lapata (2008), Woodsend et al (2010),and Napoles et al (2011)).
For paraphrastic com-pressions, manual evaluation alone reliably deter-mines the compression quality.
Because automaticevaluation metrics compare shortened sentences toextractive gold standards, they cannot be applied toparaphrastic compression.To apply automatic techniques to substitution-based compression, one would need a gold-standardset of paraphrastic compressions.
These are rare.Cohn and Lapata (2008) created an abstractive cor-pus, which contains word reordering and paraphras-ing in addition to deletion.
Unfortunately, this cor-pus is small (575 sentences) and only includes onepossible compression for each sentence.Other alternatives include deriving such corporafrom existing corpora of multi-reference transla-tions.
The longest reference translation can bepaired with the shortest reference to represent along sentence and corresponding paraphrased gold-standard compression.Similar to machine translation or summarization,automatic translation of paraphrastic compressionswould require multiple references to capture allow-able variation, since there are often many equallyvalid ways of compressing an input.
ROUGEor BLEU could be applied to a set of multiple-reference compressions, although BLEU is not with-out its own shortcomings (Callison-Burch et al,2006).
One benefit of both ROUGE and BLEU isthat they are based on n-gram recall and precision(respectively) instead of word-error rate, so reorder-ing and word substitutions can be evaluated.
Dorr etal.
(2003) used BLEU for evaluation in the contextof headline generation, which uses rewording andis related to sentence compression.
Alternatively,manual evalation can be adapted from other NLGdomains, such as the techniques described in the fol-lowing section.2.2 Manual EvaluationIn order to determine semantic and syntactic suit-ability, manual evaluation is preferable over au-tomatic techniques whenever possible.
The mostwidely practiced manual evaluation methodologywas first used by Knight and Marcu (2002).
Judgesgrade each compressed sentence against the originaland make two separate decisions: how grammaticalis the compression and how much of the meaningfrom the original sentence is preserved.
Decisionsare rated along a 5-point scale (LDC, 2005).Most compression systems consider sentences outof context (a few exceptions exist, e.g., Daume?
IIIand Marcu (2002), Martins and Smith (2009), andLin (2003)).
Contextual cues and discourse struc-ture may not be a factor to consider if the sentencesare generated for use out of context.
An exampleof a context-aware approach considered the sum-maries formed by shortened sentences and evalu-ated the compression systems based on how wellpeople could answer questions about the originaldocument from the summaries (Clarke and Lapata,2007).
This technique has been used before forevaluating summarization and text comprehension(Mani et al, 2002; Morris et al, 1992).2.2.1 Pitfalls of Manual EvaluationGrammar judgments decrease when the compres-sion is presented alongside the original sentence.Figure 1 shows that the mean grammar rating for thesame compressions is on average about 0.3 pointshigher when the compression is judged in isolation.Researchers should be careful to state when gram-mar is judged on compressions lacking referencesentences.Another factor is the group of judges.
Obvi-ously different studies will rely on different judges,so whenever possible the sentences from an exist-ing model should be re-evaluated alongside the newmodel.
The ?McD?
entries in Table 2 represent a setof sentences generated from the exact same modelevaluated by two different sets of judges.
The meangrammar and meaning ratings in each evaluationsetup differ by 0.5?0.7 points.3 Compression Rate Predicts PerformanceThe dominant assumption in compression researchis that the system makes the determination about theoptimal compression length.
For this reason, com-pression rates can vary drastically across systems.
Inorder to get unbiased evaluations, systems should becompared only when they are compressing at similarrates.Compression rate is defined as:# of tokens in compressed sentence# of tokens in original sentence?
100 (1)93CRMeaning12345l ll ll ll l0 20 40 60 80 100Modell DeletionGoldCRGrammar12345l ll lll ll0 20 40 60 80 100Modell DeletionGold.1Gold.2Figure 1: Compression rate strongly correlates with human judgments of meaning and grammaticality.
Gold representsgold-standard compression and Deletion the results of a leading deletion model.
Gold.1 grammar judgments weremade alongside the original sentence and Gold.2 were made in isolation.It seems intuitive that sentence quality diminishesin relation to the compression rate.
Each worddeleted increases the probability that errors are intro-duced.
To verify this notion, we generated compres-sions at decreasing compression rates of 250 sen-tences randomly chosen from the written corpus ofClarke and Lapata (2008), generated by our imple-mentation of a leading extractive compression sys-tem (Clarke and Lapata, 2008).
We collected hu-man judgments using the 5-point scales of meaningand grammar described above.
Both quality judg-ments decreased linearly with the compression rate(see ?Deletion?
in Figure 1).As this behavior could have been an artifact ofthe particular model employed, we next developeda unique gold-standard corpus for 50 sentences se-lected at random from the same corpus describedabove.
The authors manually compressed each sen-tence at compression rates ranging from less than10 to 100.
Using the same setup as before, wecollected human judgments of these gold standardsto determine an upper bound of perceived qualityat a wide range of compression rates.
Figure 1demonstrates that meaning and grammar ratings de-cay more drastically at compression rates below 40(see ?Gold?).
Analysis suggests that humans are of-ten able to practice ?creative deletion?
to tighten asentence up to a certain point, before hitting a com-pression barrier, shortening beyond which leads tosignificant meaning and grammatically loss.4 Mismatched ComparisonsWe have observed that a difference in compressionrates as small as 5 percentage points can influencethe quality ratings by as much as 0.1 points andconclude: systems must be compared using simi-lar levels of compression.
In particular, if systemA?s output is higher quality, but longer than systemB?s, then it is not necessarily the case that A is betterthan B. Conversely, if B has results at least as goodas system A, one can claim that B is better, since B?soutput is shorter.Here are some examples in the literature of mis-matched comparisons:?
Nomoto (2009) concluded their system signif-icantly outperformed that of Cohn and Lapata(2008).
However, the compression rate of theirsystem ranged from 45 to 74, while the com-pression rate of Cohn and Lapata (2008) was35.
This claim is unverifiable without furthercomparison.?
Clarke and Lapata (2007), when comparingagainst McDonald (2006), reported signifi-cantly better results at a 5-point higher com-pression rate.
At first glance, this does notseem like a remarkable difference.
However,94Model Meaning Grammar CompRC&L 3.83 3.66 64.1McD 3.94 3.87 64.2C&L 3.76?
3.53?
78.4?McD 3.50?
3.17?
68.5?Table 2: Mean quality ratings of two competing mod-els once the compression rates have been standardized,and as reported in the original work (denoted ?).
Thereis no significant improvement, but the numerically bettermodel changes.the study evaluated the quality of summariescontaining automatically shortened sentences.The average document length in the test set was20 sentences, and with approximately 24 wordsper sentence, a typical 65.4% compressed doc-ument would have 80 more words than a typical60.1% McDonald compression.
The aggregateloss from 80 words can be considerable, whichsuggests that this comparison is inconclusive.We re-evaluated the model described in Clarkeand Lapata (2008) (henceforth C&L) against theMcDonald (2006) model with global constraints, butfixed the compression rates to be equal.
We ran-domly selected 100 sentences from that same cor-pus and generated compressions with the same com-pression rate as the sentences generated by the Mc-Donald model (McD), using our implementation ofC&L.
Although not statistically significant, this newevaluation reversed the polarity of the results re-ported by Clarke and Lapata (Table 2).
This againstresses the importance of using similar compressionrates to draw accurate conclusions about differentmodels.An example of unbiased evaluation is found inCohn and Lapata (2009).
In this work, their modelachieved results significantly better than a compet-ing system (McDonald, 2006).
Recognizing thattheir compression rate was about 15 percentagepoints higher than the competing system, they fixedthe target compression rate to one similar to McDon-ald?s output, and still found significantly better per-formance using automatic measures.
This work isone of the few that controls their output length inorder to make an objective comparison (another ex-ample is found in McDonald (2006)), and this typeof analysis should be emulated in the future.5 SuggestionsModels should be tested on the same corpus, be-cause different corpora will likely have different fea-tures that make them easier or harder to compress.
Inorder to make non-vacuous comparisons of differentmodels, a system also needs to be constrained to pro-duce the same length output as another system, orreport results at least as good for shorter compres-sions.
Using the multi-reference gold-standard col-lection described in Section 3, relative performancecould be estimated through comparison to the gold-standard curve.
The reference set we have annotatedis yet small, but this is an area for future work basedon feedback from the community.2Other methods for limiting quality disparities in-troduced by the compression rate include fixing thetarget length to that of the gold standard (e.g., Unnoet al (2006)).
Alternately, results for a system atvarying compression levels can be reported,3 allow-ing for comparisons at similar lengths.
This is apractice to be emulated, if possible, because systemsthat cannot control output length can make compar-isons against the appropriate compression rate.In conclusion, we have provided justification forthe following practices in evaluating compressions:?
Compare systems at similar compression rates.?
Provide results across multiple compressionrates when possible.?
Report that system A surpasses B iff: A andB have the same compression rate and A doesbetter than B, or A produces shorter output thanB and A does at least as well B.?
New corpora for compression should have mul-tiple gold standards for each sentence.AcknowledgmentsWe are very grateful to James Clarke for helping usobtain the results of existing systems and to the re-viewers for their helpful comments and recommen-dations.
The first author was supported by the JHUHuman Language Technology Center of Excellence.This research was funded in part by the NSF undergrant IIS-0713448.
The views and findings are theauthors?
alone.2This data is available on request.3For example, Nomoto (2008) reported results ranging overcompression rates: 0.50?0.70.95ReferencesSrinivas Bangalore, Owen Rambow, and Steve Whittaker.2000.
Evaluation metrics for generation.
In Proceed-ings of the first international conference on Naturallanguage generation-Volume 14, pages 1?8.
Associa-tion for Computational Linguistics.A.
Belz and A. Kilgarriff.
2006.
Shared-task eval-uations in HLT: Lessons for NLG.
In Proceedingsof the Fourth International Natural Language Gen-eration Conference, pages 133?135.
Association forComputational Linguistics.Ted Briscoe.
2006.
An introduction to tag sequencegrammars and the RASP system parser.
ComputerLaboratory Technical Report, 662.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of Bleu in ma-chine translation research.
In Proceedings of EACL,Trento, Italy.James Clarke and Mirella Lapata.
2006.
Models forsentence compression: A comparison across domains,training requirements and evaluation measures.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages 377?384.
Association for Computational Lin-guistics.James Clarke and Mirella Lapata.
2007.
Modelling com-pression with discourse constraints.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages1?11.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:399?429.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofCOLING.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of Artificial In-telligence Research, 34:637?674.Hal Daume?
III and Daniel Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, pages 449?456.
Association forComputational Linguistics.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Bonnie Dorr, David Zajic, and Richard Schwartz.
2003.Hedge trimmer: A parse-and-trim approach to head-line generation.
In Proceedings of the HLT-NAACLWorkshop on Text summarization Workshop.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Proceedings of NAACL.Michel Galley and Kathleen R. McKeown.
2007.
Lex-icalized Markov grammars for sentence compression.the Proceedings of NAACL/HLT.Shudong Huang, David Graff, and George Doddington.2002.
Multiple-Translation Chinese Corpus.
Linguis-tic Data Consortium.Hongyan Jing.
2000.
Sentence reduction for automatictext summarization.
In Proceedings of the sixth con-ference on Applied natural language processing, pages310?315.
Association for Computational Linguistics.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization ?
Step one: Sentence compression.
InProceedings of AAAI.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139:91?107.LDC.
2005.
Linguistic data annotation specification:Assessment of fluency and adequacy in translations.Revision 1.5.Chin-Yew Lin.
2003.
Improving summarization per-formance by sentence compression: a pilot study.
InProceedings of the sixth international workshop on In-formation retrieval with Asian languages-Volume 11,pages 1?8.
Association for Computational Linguistics.Inderjeet Mani, Gary Klein, David House, LynetteHirschman, Therese Firmin, and Beth Sundheim.2002.
SUMMAC: a text summarization evaluation.Natural Language Engineering, 8(01):43?68.Erwin Marsi and Emiel Krahmer.
2005.
Explorationsin sentence fusion.
In Proceedings of the EuropeanWorkshop on Natural Language Generation, pages 8?10.Andre?
F. T. Martins and Noah A. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop on In-teger Linear Programming for Natural Langauge Pro-cessing.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic constraints.
In In Proceed-ings of EACL.Andrew H. Morris, George M. Kasper, and Dennis A.Adams.
1992.
The effects and limitations of auto-mated text condensing on reading comprehension per-formance.
INFORMATION SYSTEMS RESEARCH,3(1):17?35.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011.
Paraphrasticsentence compression with a character-based metric:Tightening without deletion.
In Proceedings of ACL,Workshop on Monolingual Text-To-Text Generation.96Tadashi Nomoto.
2008.
A generic sentence trimmer withCRFs.
Proceedings of ACL-08: HLT, pages 299?307.Tadashi Nomoto.
2009.
A comparison of model free ver-sus model intensive approaches to sentence compres-sion.
In Proceedings of EMNLP.E.
Reiter and A. Belz.
2006.
GENEVAL: A proposalfor shared-task evaluation in NLG.
In Proceedingsof the Fourth International Natural Language Gen-eration Conference, pages 136?138.
Association forComputational Linguistics.Stefan Riezler, Tracy H. King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensationusing ambiguity packing and stochastic disambigua-tion methods for lexical-functional grammar.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology-Volume 1,pages 118?125.
Association for Computational Lin-guistics.Yuya Unno, Takashi Ninomiya, Yusuke Miyao, andJun?ichi Tsujii.
2006.
Trimming CFG parse treesfor sentence compression using machine learning ap-proaches.
In Proceedings of the COLING/ACL onMain conference poster sessions, pages 850?857.
As-sociation for Computational Linguistics.Kristian Woodsend, Yansong Feng, and Mirella Lapata.2010.
Generation with quasi-synchronous grammar.In Proceedings of EMNLP.97
