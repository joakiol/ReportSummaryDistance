Auditory-based Acoustic Distinctive Features and Spectral Cues for RobustAutomatic Speech Recognition in Low-SNR Car EnvironmentsSid-Ahmed SelouaniUniversite?
de Moncton218 bvd.
J.-D.-Gauthier,Shippagan, E8S 1P6, Canadaselouani@umcs.caHesham TolbaINRS-Te?le?communications800 de la Gauchetie`re Ouest,Montre?al, H5A 1K6, Canada{tolba,dougo}@inrs-telecom.uquebec.caDouglas O?ShaughnessyINRS-Te?le?communications800 de la Gauchetie`re Ouest,Montre?al, H5A 1K6, CanadaAbstractIn this paper, a multi-stream paradigm is pro-posed to improve the performance of auto-matic speech recognition (ASR) systems in thepresence of highly interfering car noise.
Itwas found that combining the classical MFCCswith some auditory-based acoustic distinctivecues and the main formant frequencies of aspeech signal using a multi-stream paradigmleads to an improvement in the recognition per-formance in noisy car environments.1 IntroductionIn general, the performance of existing speech recogni-tion systems, whose designs are predicated on relativelynoise-free conditions, degrades rapidly in the presence ofa high level of adverse conditions.
However, a recognizercan provide good performance even in very noisy back-ground conditions if the exact testing condition is usedto provide the training material from which the referencepatterns of the vocabulary are obtained, which is practi-cally not always the case.
In order to cope with the ad-verse conditions, different approaches could be used.
Theapproaches that have been studied for achieving noise ro-bustness can be summarized into two fundamentally dif-ferent approaches.
The first approach attempts to prepro-cess the corrupted speech input signal prior to the patternmatching in an attempt to enhance the SNR.
The secondapproach attempts to modify the pattern matching itselfin order to account for the effects of noise.
For more de-tails see (O?Shaughnessy, 2000).In a previous work, we introduced an auditory-basedmulti-stream paradigm for ASR (Tolba et al, 2002).Within this multi-stream paradigm, we merge differentsources of information about the speech signal that couldbe lost when using only the MFCCs to recognize utteredspeech.
Our experiments showed that the use of someauditory-based features and formant cues via a multi-stream paradigm approach leads to an improvement of therecognition performance.
This proves that the MFCCsloose some information relevant to the recognition pro-cess despite the popularity of such coefficients in all cur-rent ASR systems.
In our experiments, we used a 3-stream feature vector.
The First stream vector consists ofthe classical MFCCs and their first derivatives, whereasthe second stream vector consists of acoustic cues derivedfrom hearing phenomena studies.
Finally, the magnitudesof the main resonances of the spectrum of the speech sig-nal were used as the elements of the third stream vector.In this paper, we extend our work presented in (Tolba etal., 2002) to evaluate the robustness of the proposed fea-tures (the acoustic distinctive cues and the spectral cues)using a multi- stream paradigm for ASR in noisy car en-vironments.
As mentioned above, the first stream con-sists of the MFCCs and their first derivatives, whereasthe second stream vector consists of the acoustic cues arecomputed from an auditory-based analysis applied to thespeech signal modeled using the Caelen Model (Caelen,1985).
Finally, the values of the main peaks of the spec-trum of the speech signal were used as the elements of thethird stream vector.
The magnitudes of the main peakswere obtained through an LPC analysis.The outline of this paper is as follows.
In section 2, anoverview on the auditory Caelen Model is given.
Next,we describe briefly in section 3 the statistical frameworkof the multi-stream paradigm.
Then in section 4, we pro-ceed with the evaluation of the proposed approach forASR.
Finally, in section 5 we conclude and discuss ourresults.2 The Auditory-based ProcessingIt was shown through several studies that the use ofhuman hearing properties provides insight into defin-ing a potentially useful front-end speech representa-tion (O?Shaughnessy, 2000).
However, the performanceof current ASR systems is far from the performanceachieved by humans.
In an attempt to improve the ASRperformance in noisy environments, we evaluate in thiswork the use of the hearing/perception knowledge forASR in noisy car environments.
This is accomplishedthrough the use of the auditory-based acoustic distinctivefeatures and the formant frequencies for robust ASR.2.1 The Caelen?s Auditory ModelCaelen?s auditory model (Caelen, 1985) consists of threeparts which simulate the behavior of the ear.
The exter-nal and middle ear are modeled using a bandpass filterthat can be adjusted to signal energy to take into accountthe various adaptive motions of ossicles.
The next partof the model simulates the behavior of the basilar mem-brane (BM), the most important part of the inner ear, thatacts substantially as a non-linear filter bank.
Due to thevariability of its stiffness, different places along the BMare sensitive to sounds with different spectral content.
Inparticular, the BM is stiff and thin at the base, but lessrigid and more sensitive to low frequency signals at theapex.
Each location along the BM has a characteristic fre-quency, at which it vibrates maximally for a given inputsound.
This behavior is simulated in the model by a cas-cade filter bank.
The bigger the number of these filters themore accurate is the model.
In front of these stages thereis another stage that simulates the effects of the outer andmiddle ear (pre-emphasis).
In our experiments we haveconsidered 24 filters.
This number depends on the sam-pling rate of the signals (16 kHz) and on other param-eters of the model such as the overlapping factor of thebands of the filters, or the quality factor of the resonantpart of the filters.
The final part of the model deals withthe electro-mechanical transduction of hair-cells and af-ferent fibers and the encoding at the level of the synapticendings.
For more details see (Caelen, 1985).2.2 Acoustic Distinctive CuesThe acoustic distinctive cues are calculated starting fromthe spectral data using linear combinations of the ener-gies taken in various channels.
It was shown in (Jakob-son et al, 1951) that 12 acoustic cues are sufficient tocharacterize acoustically all languages.
However, it isnot necessary to use all of these cues to characterize aspecific language.
In our study, we choose 7 cues to bemerged in a multi-stream feature vector in an attempt toimprove the performance of ASR.
These cues are basedon the Caelen ear model described above, which doesnot correspond exactly to Jakobson?s cues.
Each cue iscomputed based on the output of the 24 channel filters ofthe above-mentioned ear model.
These seven normalizedacoustic cues are: acute/grave (AG), open/closed (OC),diffuse/compact (DC), sharp/flat (SF), mat/strident (MS),continuous/discontinuous (CD) and tense/lax (TL).3 Multi-stream Statistical FrameworkMost recognizers use typically left-to-right HMMs,which consist of an arbitrary number of states N(O?Shaughnessy, 2000).
The output distribution associ-ated with each state is dependent on one or more statisti-cally independent streams.
Assuming an observation se-quence O composed of S input streams Os possibly ofdifferent lengths, representing the utterance to be recog-nized, the probability of the composite input vector Ot ata time t in state j can be written as follows:bj(Ot) =S?s=1[bjs(Ost)]?s , (1)where Ost is the input observation vector in stream s attime t and ?s is the stream weight.
Each individual streamprobability bjs(Ost) is represented by a multivariate mix-ture Gaussian.
To investigate the multi-stream paradigmusing the proposed features for ASR, we have performeda number of experiments in which we merged differentsources of information about the speech signal that couldbe lost with the cepstral analysis.4 Experiments & ResultsIn the following experiments the TIMIT database wasused.
The TIMIT corpus contains broadband record-ings of a total of 6300 sentences, 10 sentences spokenby each of 630 speakers from 8 major dialect regionsof the United States, each reading 10 phonetically richsentences.
To simulate a noisy environment, car noisewas added artificially to the clean speech.
Throughoutall experiments the HTK-based speech recognition plat-form system described in (Cambridge University SpeechGroup, 1997) has been used.
The toolkit was designed tosupport continuous-density HMMs with any numbers ofstate and mixture components.In order to evaluate the use of the proposed featuresfor ASR in noisy car environments, we repeated the sameexperiments performed in our previous study (Tolba etal., 2002) using the subsets dr1 & dr2 of a noisy ver-sion of the TIMIT database at different values of SNRwhich varies from 16 dB to -4 dB.
In all our experi-ments, 12 MFCCs were calculated on a 30-msec Ham-ming window advanced by 10 msec each frame.
More-over, the normalized log energy is also found, which isadded to the 12 MFCCs to form a 13-dimensional (static)vector.
This static vector is then expanded to produce a26-dimensional (static+dynamic) vector.
This latter wasexpanded by adding the seven acoustic distinctive cuesthat were computed based on the Caelen model analysis.This was followed by the computation of the main spec-tral peak magnitudes, which were added to the MFCCsand the acoustic cues to form a 37-dimensional vector16 dB 8dB 4 dB 0 dB -4 dBMFCCEDA 81.67 58.02 48.02 33.44 22.81MFCCEDE 87.60 50.83 38.23 27.29 17.29MFCCEDP 89.69 69.58 60.73 40.31 27.50MFCCEDEP 89.38 55.31 41.88 28.44 17.40[a] %CWrd using 1-mixture triphone models.16 dB 8dB 4 dB 0 dB -4 dBMFCCEDA 83.85 60.31 49.58 36.56 25.21MFCCEDE 88.12 51.98 39.58 28.02 16.56MFCCEDP 90.21 71.35 59.06 42.92 27.19MFCCEDEP 89.79 55.73 42.92 29.06 18.12[b] %CWrd using 2-mixture triphone models.16 dB 8dB 4 dB 0 dB -4 dBMFCCEDA 84.58 62.40 51.77 35.73 26.25MFCCEDE 89.06 53.85 42.29 29.38 17.71MFCCEDP 89.69 71.67 59.79 42.81 27.81MFCCEDEP 89.27 58.65 43.75 29.27 19.38[c] %CWrd using 4-mixture triphone models.16 dB 8dB 4 dB 0 dB -4 dBMFCCEDA 85.42 63.54 52.60 40.10 28.75MFCCEDE 89.38 53.33 41.46 29.27 17.92MFCCEDP 90.62 70.94 58.85 42.19 28.85MFCCEDEP 91.35 57.92 43.85 28.75 18.33[d] %CWrd using 8-mixture triphone models.Table 1: Comparison of the percent word recognition performance (%CWrd) of the MFCCEDA-, MFCCEDE-MFCCEDP- and MFCCEDEP-based HTK ASR systems to the baseline HTK using (a) 2-mixture, (b) 4-mixtureand (c) 8-mixture triphone models and the dr1 & dr2 subsets of the TIMIT database when contaminated by additivecar noise for different values of SNR.upon which the hidden Markov models (HMMs), thatmodel the speech subword units, were trained.
The mainspectral peak magnitudes were computed based on anLPC analysis using 12 poles followed by a peak pickingalgorithm.
The proposed system used for the recogni-tion task uses tri-phone Gaussian mixture HMM system.Three different sets of experiments has been carried outon the noisy version of the TIMIT database.
In the firstset of these experiments, we tested our recognizer usinga 30-dimensional feature vector (MFCCEDP), in whichwe combined the magnitudes of the main spectral peaksto the classical MFCCs and their first derivatives to formtwo streams that have been used to perform the recogni-tion process.
We found through experiments that the useof these two streams leads to an improvement in the ac-curacy of the word recognition rate compared to the oneobtained when we used the classical MFCCEDA featurevector, Table 1.
These tests were repeated using the 2-stream feature vector, in which we combined the acous-tic distinctive cues to the classical MFCCs and their firstderivatives to form two streams (MFCCEDE).
Again, us-ing these two streams, an improvement in the accuracyof the word recognition rate has been obtained when wetested our recognizer using N mixture Gaussian HMMsusing triphone models for different values of SNR, Table1.
We repeated these tests using the proposed featureswhich combines the MFCCs with the acoustic distinctivecues and the formant frequencies to form a three-streamfeature vector (MFCCEDEP).
Again, using these com-bined features, an improvement in the accuracy of theword recognition rate was obtained, Table 1.5 ConclusionWe have proposed in this paper a multi-stream paradigmto improve the performance of ASR systems in noisycar environments.
Results showed that combining theclassical MFCCs with the main formant frequencies ofa speech signal using a multi- stream paradigm leads toan improvement in the recognition performance in noisycar environments for a wide range of SNR values varyingfrom 16 dB to -4 dB.
These results show that the formantfrequencies are relevant for the recognition process notonly for clean speech, but also for noisy speech, even atvery low SNR values.
On the other hand, results showedalso that the use of the auditory-based acoustic distinctivecues improves the performance of the recognition processin noisy car environments with respect to the use of onlythe MFCCs, their first and second derivatives at high SNRvalues, but not for low SNR values.ReferencesHesham Tolba, Sid-Ahmed Selouani and DouglasO?Shaughnessy.
2002.
Auditory-based Acoustic Dis-tinctive Features and Spectral Cues for AutomaticSpeech Recognition Using a Multi-Stream Paradigm.IEEE-ICASSP?2002: 837-840.Jean Caelen.
1985.
Space/Time Data-Information in theARIAL Project Ear Model.
Speech Communication,4(1&2): 251-267.Douglas O?Shaughnessy.
2000.
Speech Communication:Human and Machine.
IEEE Press.Roman Jakobson, Gunnar Fant and Morris Halle.
1951.Preliminaries to Speech Analysis: The Distinctive Fea-tures and their Correlates.
MIT Press, Cambridge.Cambridge University Speech Group.
1997.
The HTKBook (Version 2.1.1).
Cambridge University Group.
