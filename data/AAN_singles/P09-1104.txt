Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923?931,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPBetter Word Alignments with Supervised ITG ModelsAria Haghighi, John Blitzer, John DeNero and Dan KleinComputer Science Division, University of California at Berkeley{ aria42,blitzer,denero,klein }@cs.berkeley.eduAbstractThis work investigates supervised word align-ment methods that exploit inversion transduc-tion grammar (ITG) constraints.
We con-sider maximum margin and conditional like-lihood objectives, including the presentationof a new normal form grammar for canoni-calizing derivations.
Even for non-ITG sen-tence pairs, we show that it is possible learnITG alignment models by simple relaxationsof structured discriminative learning objec-tives.
For efficiency, we describe a set of prun-ing techniques that together allow us to alignsentences two orders of magnitude faster thannaive bitext CKY parsing.
Finally, we intro-duce many-to-one block alignment features,which significantly improve our ITG models.Altogether, our method results in the best re-ported AER numbers for Chinese-English anda performance improvement of 1.1 BLEU overGIZA++ alignments.1 IntroductionInversion transduction grammar (ITG) con-straints (Wu, 1997) provide coherent structuralconstraints on the relationship between a sentenceand its translation.
ITG has been extensivelyexplored in unsupervised statistical word align-ment (Zhang and Gildea, 2005; Cherry andLin, 2007a; Zhang et al, 2008) and machinetranslation decoding (Cherry and Lin, 2007b;Petrov et al, 2008).
In this work, we investigatelarge-scale, discriminative ITG word alignment.Past work on discriminative word alignmenthas focused on the family of at-most-one-to-onematchings (Melamed, 2000; Taskar et al, 2005;Moore et al, 2006).
An exception to this is thework of Cherry and Lin (2006), who discrim-inatively trained one-to-one ITG models, albeitwith limited feature sets.
As they found, ITGapproaches offer several advantages over generalmatchings.
First, the additional structural con-straint can result in superior alignments.
We con-firm and extend this result, showing that one-to-one ITG models can perform as well as, or betterthan, general one-to-one matching models, eitherusing heuristic weights or using rich, learned fea-tures.A second advantage of ITG approaches is thatthey admit a range of training options.
As withgeneral one-to-one matchings, we can optimizemargin-based objectives.
However, unlike withgeneral matchings, we can also efficiently com-pute expectations over the set of ITG derivations,enabling the training of conditional likelihoodmodels.
A major challenge in both cases is thatour training alignments are often not one-to-oneITG alignments.
Under such conditions, directlytraining to maximize margin is unstable, and train-ing to maximize likelihood is ill-defined, since thetarget algnment derivations don?t exist in our hy-pothesis class.
We show how to adapt both marginand likelihood objectives to learn good ITG align-ers.In the case of likelihood training, two innova-tions are presented.
The simple, two-rule ITGgrammar exponentially over-counts certain align-ment structures relative to others.
Because of this,Wu (1997) and Zens and Ney (2003) introduced anormal form ITG which avoids this over-counting.We extend this normal form to null productionsand give the first extensive empirical comparisonof simple and normal form ITGs, for posterior de-coding under our likelihood models.
Additionally,we show how to deal with training instances wherethe gold alignments are outside of the hypothesisclass by instead optimizing the likelihood of a setof minimum-loss alignments.Perhaps the greatest advantage of ITG mod-els is that they straightforwardly permit block-923structured alignments (i.e.
phrases), which gen-eral matchings cannot efficiently do.
The need forblock alignments is especially acute in Chinese-English data, where oracle AERs drop from 10.2without blocks to around 1.2 with them.
Indeed,blocks are the primary reason for gold alignmentsbeing outside the space of one-to-one ITG align-ments.
We show that placing linear potential func-tions on many-to-one blocks can substantially im-prove performance.Finally, to scale up our system, we give a com-bination of pruning techniques that allows us tosum ITG alignments two orders of magnitudefaster than naive inside-outside parsing.All in all, our discriminatively trained, blockITG models produce alignments which exhibitthe best AER on the NIST 2002 Chinese-Englishalignment data set.
Furthermore, they result ina 1.1 BLEU-point improvement over GIZA++alignments in an end-to-end Hiero (Chiang, 2007)machine translation system.2 Alignment FamiliesIn order to structurally restrict attention to rea-sonable alignments, word alignment models mustconstrain the set of alignments considered.
In thissection, we discuss and compare alignment fami-lies used to train our discriminative models.Initially, as in Taskar et al (2005) and Mooreet al (2006), we assume the score a of a potentialalignment a) decomposes ass(a) = ?
(i,j)?asij +?i/?asi +?j /?asj (1)where sij are word-to-word potentials and si andsj represent English null and foreign null poten-tials, respectively.We evaluate our proposed alignments (a)against hand-annotated alignments, which aremarked with sure (s) and possible (p) alignments.The alignment error rate (AER) is given by,AER(a, s,p) = 1?
|a ?
s|+ |a ?
p||a|+ |s|2.1 1-to-1 MatchingsThe class of at most 1-to-1 alignment match-ings, A1-1, has been considered in several works(Melamed, 2000; Taskar et al, 2005; Moore et al,2006).
The alignment that maximizes a set of po-tentials factored as in Equation (1) can be foundin O(n3) time using a bipartite matching algo-rithm (Kuhn, 1955).1 On the other hand, summingover A1-1 is #P -hard (Valiant, 1979).Initially, we consider heuristic alignment poten-tials given by Dice coefficientsDice(e, f) = 2CefCe + Cfwhere Cef is the joint count of words (e, f) ap-pearing in aligned sentence pairs, and Ce and Cfare monolingual unigram counts.We extracted such counts from 1.1 millionFrench-English aligned sentence pairs of Hansardsdata (see Section 6.1).
For each sentence pair inthe Hansards test set, we predicted the alignmentfrom A1-1 which maximized the sum of Dice po-tentials.
This yielded 30.6 AER.2.2 Inversion Transduction GrammarWu (1997)?s inversion transduction grammar(ITG) is a synchronous grammar formalism inwhich derivations of sentence pairs correspond toalignments.
In its original formulation, there is asingle non-terminal X spanning a bitext cell withan English and foreign span.
There are three ruletypes: Terminal unary productions X ?
?e, f?,where e and f are an aligned English and for-eign word pair (possibly with one being null);normal binary rules X ?
X(L)X(R), where theEnglish and foreign spans are constructed fromthe children as ?X(L)X(R), X(L)X(R)?
; and in-verted binary rules X ; X(L)X(R), where theforeign span inverts the order of the children?X(L)X(R), X(R)X(L)?.2 In general, we will calla bitext cell a normal cell if it was constructed witha normal rule and inverted if constructed with aninverted rule.Each ITG derivation yields some alignment.The set of such ITG alignments,AITG, are a strictsubset of A1-1 (Wu, 1997).
Thus, we will viewITG as a constraint on A1-1 which we will ar-gue is generally beneficial.
The maximum scor-ing alignment from AITG can be found in O(n6)time with synchronous CFG parsing; in practice,we can make ITG parsing efficient using a varietyof pruning techniques.
One computational advan-tage of AITG over A1-1 alignments is that sum-mation overAITG is tractable.
The corresponding1We shall use n throughout to refer to the maximum offoreign and English sentence lengths.2The superscripts on non-terminals are added only to in-dicate correspondence of child symbols.924Indonesia'sparliamentspeakerarraignedincourt???????????????????
?Indonesia'sparliamentspeakerarraignedincourt(a) Max-Matching Alignment (b) Block ITG AlignmentFigure 1: Best alignments from (a) 1-1 matchings and (b) block ITG (BITG) families respectively.
The 1-1matching is the best possible alignment in the model family, but cannot capture the fact that Indonesia is renderedas two words in Chinese or that in court is rendered as a single word in Chinese.dynamic program allows us to utilize likelihood-based objectives for learning alignment models(see Section 4).Using the same heuristic Dice potentials onthe Hansards test set, the maximal scoring align-ment from AITG yields 28.4 AER?2.4 betterthan A1-1 ?indicating that ITG can be beneficialas a constraint on heuristic alignments.2.3 Block ITGAn important alignment pattern disallowed byA1-1 is the many-to-one alignment block.
Whilenot prevalent in our hand-aligned French Hansardsdataset, blocks occur frequently in our hand-aligned Chinese-English NIST data.
Figure 1contains an example.
Extending A1-1 to includeblocks is problematic, because finding a maximal1-1 matching over phrases is NP-hard (DeNeroand Klein, 2008).With ITG, it is relatively easy to allow contigu-ous many-to-one alignment blocks without addedcomplexity.3 This is accomplished by adding ad-ditional unary terminal productions aligning a for-eign phrase to a single English terminal or viceversa.
We will use BITG to refer to this blockITG variant and ABITG to refer to the alignmentfamily, which is neither contained in nor containsA1-1.
For this alignment family, we expand thealignment potential decomposition in Equation (1)to incorporate block potentials sef and sef whichrepresent English and foreign many-to-one align-ment blocks, respectively.One way to evaluate alignment families is to3In our experiments we limited the block size to 4.consider their oracle AER.
In the 2002 NISTChinese-English hand-aligned data (see Sec-tion 6.2), we constructed oracle alignment poten-tials as follows: sij is set to +1 if (i, j) is a sureor possible alignment in the hand-aligned data, -1 otherwise.
All null potentials (si and sj) areset to 0.
A max-matching under these potentials isgenerally a minimal loss alignment in the family.The oracle AER computed in this was is 10.1 forA1-1 and 10.2 for AITG.
The ABITG alignmentfamily has an oracle AER of 1.2.
These basic ex-periments show that AITG outperforms A1-1 forheuristic alignments, and ABITG provide a muchcloser fit to true Chinese-English alignments thanA1-1.3 Margin-Based TrainingIn this and the next section, we discuss learningalignment potentials.
As input, we have a trainingset D = (x1,a?1), .
.
.
, (xn,a?n) of hand-aligneddata, where x refers to a sentence pair.
We will as-sume the score of a alignment is given as a linearfunction of a feature vector ?(x,a).
We will fur-ther assume the feature representation of an align-ment, ?
(x,a) decomposes as in Equation (1),?
(i,j)?a?ij(x) +?i/?a?i(x) +?j /?a?j(x)In the framework of loss-augmented marginlearning, we seek a w such that w ?
?(x,a?)
islarger than w ?
?
(x,a) + L(a,a?)
for all a in analignment family, where L(a,a?)
is the loss be-tween a proposed alignment a and the gold align-ment a?.
As in Taskar et al (2005), we utilize a925loss that decomposes across alignments.
Specif-ically, for each alignment cell (i, j) which is nota possible alignment in a?, we incur a loss of 1when aij 6= a?ij ; note that if (i, j) is a possiblealignment, our loss is indifferent to its presence inthe proposal alignment.A simple loss-augmented learning pro-cedure is the margin infused relaxed algo-rithm (MIRA) (Crammer et al, 2006).
MIRAis an online procedure, where at each time stept+ 1, we update our weights as follows:wt+1 = argminw||w ?wt||22 (2)s.t.
w ?
?(x,a?)
?
w ?
?
(x, a?)
+ L(a?,a?
)where a?
= argmaxa?Awt ?
?
(x,a)In our data sets, many a?
are not in A1-1 (andthus not in AITG), implying the minimum in-family loss must exceed 0.
Since MIRA oper-ates in an online fashion, this can cause severestability problems.
On the Hansards data, thesimple averaging technique described by Collins(2002) yields a reasonable model.
On the ChineseNIST data, however, where almost no alignmentis in A1-1, the update rule from Equation (2) iscompletely unstable, and even the averaged modeldoes not yield high-quality results.We instead use a variant of MIRA similar toChiang et al (2008).
First, rather than updatetowards the hand-labeled alignment a?, we up-date towards an alignment which achieves mini-mal loss within the family.4 We call this best-in-class alignment a?p.
Second, we perform loss-augmented inference to obtain a?.
This yields themodified QP,wt+1 = argminw||w ?wt||22 (3)s.t.
w ?
?
(x,a?p) ?
w ?
?
(x, a?)
+ L(a,a?p)where a?
= argmaxa?Awt ?
?
(x,a) + ?L(a,a?p)By setting ?
= 0, we recover the MIRA updatefrom Equation (2).
As ?
grows, we increase ourpreference that a?
have high loss (relative to a?p)rather than high model score.
With this change,MIRA is stable, but still performs suboptimally.The reason is that initially the score for all align-ments is low, so we are biased toward only usingvery high loss alignments in our constraint.
Thisslows learning and prevents us from finding a use-ful weight vector.
Instead, in all the experiments4There might be several alignments which achieve thisminimal loss; we choose arbitrarily among them.we report here, we begin with ?
= 0 and slowlyincrease it to ?
= 0.5.4 Likelihood ObjectiveAn alternative to margin-based training is a likeli-hood objective, which learns a conditional align-ment distribution Pw(a|x) parametrized as fol-lows,logPw(a|x)=w??(x,a)?log?a??Aexp(w??(x,a?
))where the log-denominator represents a sum overthe alignment family A.
This alignment probabil-ity only places mass on members ofA.
The likeli-hood objective is given by,maxw?(x,a?
)?AlogPw(a?|x)Optimizing this objective with gradient methodsrequires summing over alignments.
ForAITG andABITG, we can efficiently sum over the set of ITGderivations inO(n6) time using the inside-outsidealgorithm.
However, for the ITG grammar pre-sented in Section 2.2, each alignment has multiplegrammar derivations.
In order to correctly sumover the set of ITG alignments, we need to alterthe grammar to ensure a bijective correspondencebetween alignments and derivations.4.1 ITG Normal FormThere are two ways in which ITG derivations dou-ble count alignments.
First, n-ary productions arenot binarized to remove ambiguity; this results inan exponential number of derivations for diagonalalignments.
This source of overcounting is con-sidered and fixed by Wu (1997) and Zens and Ney(2003), which we briefly review here.
The result-ing grammar, which does not handle null align-ments, consists of a symbol N to represent a bi-text cell produced by a normal rule and I for a cellformed by an inverted rule; alignment terminalscan be either N or I .
In order to ensure uniquederivations, we stipulate that a N cell can be con-structed only from a sequence of smaller invertedcells I .
Binarizing the rule N ?
I2+ introducesthe intermediary symbolN (see Figure 2(a)).
Sim-ilarly for inverse cells, we insist an I cell only bebuilt by an inverted combination of N cells; bina-rization of I ; N2+ requires the introduction ofthe intermediary symbol I (see Figure 2(b)).Null productions are also a source of doublecounting, as there are many possible orders in926N ?
I2+N ?
INN ?
I}N ?
INIIINNN(a) Normal Domain Rules} I !
N2+I !
NII !
NII !
N NNNIII(b) Inverted Domain RulesN11 ?
?
?, f?N11N11 ?
N10N10 ?
N10?e, ?
?N10 ?
N00}N11 ?
?
?, f?
?N10}N10 ?
N00?e, ???
}N00 ?
I11NN ?
I11NN ?
I00N00 ?
I+11I00N00 N10 N10N11NNI11I11I00N00N11(c) Normal Domain with Null Rules}}}I11 !
?
?, f?I11I11 !
I10 I11 !
?
?, f?
?I10I10 !
I10?e, ?
?I10 !
I00 I10 !
I00?e, ??
?I00 !
N+11N00 IIN00N11N11I00 !
N11II !
N11II !
N00I00I00 I10 I10I11I11(d) Inverted Domain with Null RulesFigure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammarwithout nulls (presented in Wu (1997) and Zens and Ney (2003)).
In (c) and (d), we present a normal form grammarthat accounts for null alignments.which to attach null alignments to a bitext cell;we address this by adapting the grammar to forcea null attachment order.
We introduce symbolsN00, N10, and N11 to represent whether a normalcell has taken no nulls, is accepting foreign nulls,or is accepting English nulls, respectively.
We alsointroduce symbols I00, I10, and I11 to representinverse cells at analogous stages of taking nulls.As Figures 2 (c) and (d) illustrate, the directionsin which nulls are attached to normal and inversecells differ.
The N00 symbol is constructed byone or more ?complete?
inverted cells I11 termi-nated by a no-null I00.
By placing I00 in the lowerright hand corner, we allow the larger N00 to un-ambiguously attach nulls.
N00 transitions to theN10 symbol and accepts any number of ?e, ??
En-glish terminal alignments.
Then N10 transitions toN11 and accepts any number of ?
?, f?
foreign ter-minal alignments.
An analogous set of grammarrules exists for the inverted case (see Figure 2(d)for an illustration).
Given this normal form, wecan efficiently compute model expectations overITG alignments without double counting.5 To ourknowledge, the alteration of the normal form toaccommodate null emissions is novel to this work.5The complete grammar adds sentinel symbols to the up-per left and lower right, and the root symbol is constrained tobe a N00.4.2 Relaxing the Single Target AssumptionA crucial obstacle for using the likelihood objec-tive is that a given a?
may not be in the alignmentfamily.
As in our alteration to MIRA (Section 3),we could replace a?
with a minimal loss in-classalignment a?p.
However, in contrast to MIRA, thelikelihood objective will implicitly penalize pro-posed alignments which have loss equal to a?p.
Weopt instead to maximize the probability of the setof alignmentsM(a?)
which achieve the same op-timal in-class loss.
Concretely, let m?
be the min-imal loss achievable relative to a?
in A.
Then,M(a?)
= {a ?
A|L(a,a?)
= m?
}When a?
is an ITG alignment (i.e., m?
is 0),M(a?)
consists only of alignments which have allthe sure alignments in a?, but may have some sub-set of the possible alignments in a?.
See Figure 3for a specific example where m?
= 1.Our modified likelihood objective is given by,maxw?(x,a?
)?Dlog ?a?M(a?
)Pw(a|x)Note that this objective is no longer convex, as itinvolves a logarithm of a summation, however westill utilize gradient-based optimization.
Summingand obtaining feature expectations over M(a?
)can be done efficiently using a constrained variant927MIRA Likelihood1-1 ITG ITG-S ITG-NFeatures P R AER P R AER P R AER P R AERDice,dist 85.9 82.6 15.6 86.7 82.9 15.0 89.2 85.2 12.6 87.8 82.6 14.6+lex,ortho 89.3 86.0 12.2 90.1 86.4 11.5 92.0 90.6 8.6 90.3 88.8 10.4+joint HMM 95.8 93.8 5.0 96.0 93.2 5.2 95.5 94.2 5.0 95.6 94.0 5.1Table 1: Results on the French Hansards dataset.
Columns indicate models and training methods.
The rowsindicate the feature sets used.
ITG-S uses the simple grammar (Section 2.2).
ITG-N uses the normal form grammar(Section 4.1).
For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.ThatisnotgoodenoughSeneestpassuffisanta?Gold Alignment Target AlignmentsM(a?
)Figure 3: Often, the gold alignment a?
isn?t in ouralignment family, here ABITG.
For the likelihood ob-jective (Section 4.2), we maximize the probability ofthe setM(a?)
consisting of alignments ABITG whichachieve minimal loss relative to a?.
In this example,the minimal loss is 1, and we have a choice of remov-ing either of the sure alignments to the English wordnot.
We also have the choice of whether to include thepossible alignment, yielding 4 alignments inM(a?
).of the inside-outside algorithm where sure align-ments not present in a?
are disallowed, and thenumber of missing sure alignments is appended tothe state of the bitext cell.6One advantage of the likelihood-based objec-tive is that we can obtain posteriors over individualalignment cells,Pw((i, j)|x) =?a?A:(i,j)?aPw(a|x)We obtain posterior ITG alignments by includingall alignment cells (i, j) such that Pw((i, j)|x) ex-ceeds a fixed threshold t. Posterior thresholdingallows us to easily trade-off precision and recall inour alignments by raising or lowering t.5 Dynamic Program PruningBoth discriminative methods require repeatedmodel inference: MIRA depends upon loss-augmented Viterbi parsing, while conditional like-6Note that alignments that achieve the minimal loss wouldnot introduce any alignments not either sure or possible, so itsuffices to keep track only of the number of sure recall errors.lihood uses the inside-outside algorithm for com-puting cell posteriors.
Exhaustive computationof these quantities requires an O(n6) dynamicprogram that is prohibitively slow even on smallsupervised training sets.
However, most of thesearch space can safely be pruned using posteriorpredictions from a simpler alignment models.
Weuse posteriors from two jointly estimated HMMmodels to make pruning decisions during ITG in-ference (Liang et al, 2006).
Our first pruning tech-nique is broadly similar to Cherry and Lin (2007a).We select high-precision alignment links from theHMM models: those word pairs that have a pos-terior greater than 0.9 in either model.
Then, weprune all bitext cells that would invalidate morethan 8 of these high-precision alignments.Our second pruning technique is to prune allone-by-one (word-to-word) bitext cells that have aposterior below 10?4 in both HMM models.
Prun-ing a one-by-one cell also indirectly prunes largercells containing it.
To take maximal advantage ofthis indirect pruning, we avoid explicitly attempt-ing to build each cell in the dynamic program.
In-stead, we track bounds on the spans for which wehave successfully built ITG cells, and we only iter-ate over larger spans that fall within those bounds.The details of a similar bounding approach appearin DeNero et al (2009).In all, pruning reduces MIRA iteration timefrom 175 to 5 minutes on the NIST Chinese-English dataset with negligible performance loss.Likelihood training time is reduced by nearly twoorders of magnitude.6 Alignment Quality ExperimentsWe present results which measure the quality ofour models on two hand-aligned data sets.
Ourfirst is the English-French Hansards data set fromthe 2003 NAACL shared task (Mihalcea and Ped-ersen, 2003).
Here we use the same 337/100train/test split of the labeled data as Taskar et al928MIRA Likelihood1-1 ITG BITG BITG-S BITG-NFeatures P R AER P R AER P R AER P R AER P R AERDice, dist,blcks, dict, lex 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1+HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4Table 2: Word alignment results on Chinese-English.
Each column is a learning objective paired with an alignmentfamily.
The first row represents our best model without external alignment models and the second row includesfeatures from the jointly trained HMM.
Under likelihood, BITG-S uses the simple grammar (Section 2.2).
BITG-Nuses the normal form grammar (Section 4.1).
(2005); we compute external features from thesame unlabeled data, 1.1 million sentence pairs.Our second is the Chinese-English hand-alignedportion of the 2002 NIST MT evaluation set.
Thisdataset has 491 sentences, which we split into atraining set of 150 and a test set of 191.
When wetrained external Chinese models, we used the sameunlabeled data set as DeNero and Klein (2007), in-cluding the bilingual dictionary.For likelihood based models, we set the L2 reg-ularization parameter, ?2, to 100 and the thresh-old for posterior decoding to 0.33.
We report re-sults using the simple ITG grammar (ITG-S, Sec-tion 2.2) where summing over derivations dou-ble counts alignments, as well as the normal formITG grammar (ITG-N,Section 4.1) which doesnot double count.
We ran our annealed loss-augmented MIRA for 15 iterations, beginningwith ?
at 0 and increasing it linearly to 0.5.
Wecompute Viterbi alignments using the averagedweight vector from this procedure.6.1 French Hansards ResultsThe French Hansards data are well-studied datasets for discriminative word alignment (Taskar etal., 2005; Cherry and Lin, 2006; Lacoste-Julienet al, 2006).
For this data set, it is not clearthat improving alignment error rate beyond that ofGIZA++ is useful for translation (Ganchev et al,2008).
Table 1 illustrates results for the Hansardsdata set.
The first row uses dice and the same dis-tance features as Taskar et al (2005).
The firsttwo rows repeat the experiments of Taskar et al(2005) and Cherry and Lin (2006), but adding ITGmodels that are trained to maximize conditionallikelihood.
The last row includes the posterior ofthe jointly-trained HMM of Liang et al (2006)as a feature.
This model alone achieves an AERof 5.4.
No model significantly improves over theHMM alone, which is consistent with the resultsof Taskar et al (2005).6.2 Chinese NIST ResultsChinese-English alignment is a much harder taskthan French-English alignment.
For example, theHMM aligner achieves an AER of 20.7 when us-ing the competitive thresholding heuristic of DeN-ero and Klein (2007).
On this data set, our blockITG models make substantial performance im-provements over the HMM, and moreover theseresults do translate into downstream improve-ments in BLEU score for the Chinese-English lan-guage pair.
Because of this, we will briefly de-scribe the features used for these models in de-tail.
For features on one-by-one cells, we con-sider Dice, the distance features from (Taskar etal., 2005), dictionary features, and features for the50 most frequent lexical pairs.
We also trained anHMM aligner as described in DeNero and Klein(2007) and used the posteriors of this model as fea-tures.
The first two columns of Table 2 illustratethese features for ITG and one-to-one matchings.For our block ITG models, we include all ofthese features, along with variants designed formany-to-one blocks.
For example, we include theaverage Dice of all the cells in a block.
In addi-tion, we also created three new block-specific fea-tures types.
The first type comprises bias featuresfor each block length.
The second type comprisesfeatures computed from N-gram statistics gatheredfrom a large monolingual corpus.
These includefeatures such as the number of occurrences of thephrasal (multi-word) side of a many-to-one block,as well as pointwise mutual information statisticsfor the multi-word parts of many-to-one blocks.These features capture roughly how ?coherent?
themulti-word side of a block is.The final block feature type consists of phraseshape features.
These are designed as follows: Foreach word in a potential many-to-one block align-ment, we map an individual word to X if it is notone of the 25 most frequent words.
Some examplefeatures of this type are,929?
English Block: [the X, X], [in X of, X]?
Chinese Block: [ X, X] [X|, X]For English blocks, for example, these featurescapture the behavior of phrases such as in spiteof or in front of that are rendered as one word inChinese.
For Chinese blocks, these features cap-ture the behavior of phrases containing classifierphrases like?
orP, which are rendered asEnglish indefinite determiners.The right-hand three columns in Table 2 presentsupervised results on our Chinese English data setusing block features.
We note that almost all ofour performance gains (relative to both the HMMand 1-1 matchings) come from BITG and blockfeatures.
The maximum likelihood-trained nor-mal form ITG model outperforms the HMM, evenwithout including any features derived from theunlabeled data.
Once we include the posteriorsof the HMM as a feature, the AER decreases to14.4.
The previous best AER result on this data setis 15.9 from Ayan and Dorr (2006), who trainedstacked neural networks based on GIZA++ align-ments.
Our results are not directly comparable(they used more labeled data, but did not have theHMM posteriors as an input feature).6.3 End-To-End MT ExperimentsWe further evaluated our alignments in an end-to-end Chinese to English translation task using thepublicly available hierarchical pipeline JosHUa(Li and Khudanpur, 2008).
The pipeline extractsa Hiero-style synchronous context-free grammar(Chiang, 2007), employs suffix-array based ruleextraction (Lopez, 2007), and tunes model pa-rameters with minimum error rate training (Och,2003).
We trained on the FBIS corpus using sen-tences up to length 40, which includes 2.7 millionEnglish words.
We used a 5-gram language modeltrained on 126 million words of the Xinhua sectionof the English Gigaword corpus, estimated withSRILM (Stolcke, 2002).
We tuned on 300 sen-tences of the NIST MT04 test set.Results on the NIST MT05 test set appear inTable 3.
We compared four sets of alignments.The GIZA++ alignments7 are combined across di-rections with the grow-diag-final heuristic, whichoutperformed the union.
The joint HMM align-ments are generated from competitive posterior7We used a standard training regimen: 5 iterations ofmodel 1, 5 iterations of HMM, 3 iterations of Model 3, and 3iterations of Model 4.Alignments TranslationsModel Prec Rec Rules BLEUGIZA++ 62 84 1.9M 23.22Joint HMM 79 77 4.0M 23.05Viterbi ITG 90 80 3.8M 24.28Posterior ITG 81 83 4.2M 24.32Table 3: Results on the NIST MT05 Chinese-Englishtest set show that our ITG alignments yield improve-ments in translation quality.thresholding (DeNero and Klein, 2007).
The ITGViterbi alignments are the Viterbi output of theITG model with all features, trained to maximizelog likelihood.
The ITG Posterior alignmentsresult from applying competitive thresholding toalignment posteriors under the ITG model.
Oursupervised ITG model gave a 1.1 BLEU increaseover GIZA++.7 ConclusionThis work presented the first large-scale applica-tion of ITG to discriminative word alignment.
Weempirically investigated the performance of con-ditional likelihood training of ITG word alignersunder simple and normal form grammars.
Weshowed that through the combination of relaxedlearning objectives, many-to-one block alignmentpotential, and efficient pruning, ITG models canyield state-of-the art word alignments, even whenthe underlying gold alignments are highly non-ITG.
Our models yielded the lowest published er-ror for Chinese-English alignment and an increasein downstream translation performance.ReferencesNecip Fazil Ayan and Bonnie Dorr.
2006.
Goingbeyond AER: An extensive analysis of word align-ments and their impact on MT.
In ACL.Colin Cherry and Dekang Lin.
2006.
Soft syntacticconstraints for word alignment through discrimina-tive training.
In ACL.Colin Cherry and Dekang Lin.
2007a.
Inversion trans-duction grammar for joint phrasal translation mod-eling.
In NAACL-HLT 2007.Colin Cherry and Dekang Lin.
2007b.
A scalable in-version transduction grammar for joint phrasal trans-lation modeling.
In SSST Workshop at ACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In EMNLP.930David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP.Koby Crammer, Ofer Dekel, Shai S. Shwartz, andYoram Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In ACL.John DeNero and Dan Klein.
2008.
The complexityof phrase alignment problems.
In ACL Short PaperTrack.John DeNero, Mohit Bansal, Adam Pauls, and DanKlein.
2009.
Efficient parsing for transducer gram-mars.
In NAACL.Kuzman Ganchev, Joao Graca, and Ben Taskar.
2008.Better alignments = better translations?
In ACL.H.
W. Kuhn.
1955.
The Hungarian method for the as-signment problem.
Naval Research Logistic Quar-terly.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael Jordan.
2006.
Word alignment viaquadratic assignment.
In NAACL.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
InSSST Workshop at ACL.Percy Liang, Dan Klein, and Dan Klein.
2006.
Align-ment by agreement.
In NAACL-HLT.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In EMNLP.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics.Rada Mihalcea and Ted Pedersen.
2003.
An evalua-tion exercise for word alignment.
In HLT/NAACLWorkshop on Building and Using Parallel Texts.Robert C. Moore, Wen tau Yih, and Andreas Bode.2006.
Improved discriminative bilingual wordalignment.
In ACL-COLING.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL.Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation usinglanguage projections.
In Empirical Methods in Nat-ural Language Processing.Andreas Stolcke.
2002.
Srilm: An extensible languagemodeling toolkit.
In ICSLP 2002.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In NAACL-HLT.L.
G. Valiant.
1979.
The complexity of computing thepermanent.
Theoretical Computer Science, 8:189?201.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In ACL.Hao Zhang and Dan Gildea.
2005.
Stochastic lexical-ized inversion transduction grammar for alignment.In ACL.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InACL.931
