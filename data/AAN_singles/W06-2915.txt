Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 109?116, New York City, June 2006. c?2006 Association for Computational LinguisticsWhich Side are You on?
Identifying Perspectives at the Document andSentence LevelsWei-Hao LinLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213whlin@cs.cmu.eduTheresa Wilson, Janyce WiebeIntelligent Systems ProgramUniversity of PittsburghPittsburgh, PA 15260{twilson,wiebe}@cs.pitt.eduAlexander HauptmannSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213alex@cs.cmu.eduAbstractIn this paper we investigate a new problemof identifying the perspective from whicha document is written.
By perspective wemean a point of view, for example, fromthe perspective of Democrats or Repub-licans.
Can computers learn to identifythe perspective of a document?
Not everysentence is written strongly from a per-spective.
Can computers learn to identifywhich sentences strongly convey a partic-ular perspective?
We develop statisticalmodels to capture how perspectives areexpressed at the document and sentencelevels, and evaluate the proposed mod-els on articles about the Israeli-Palestinianconflict.
The results show that the pro-posed models successfully learn how per-spectives are reflected in word usage andcan identify the perspective of a documentwith high accuracy.1 IntroductionIn this paper we investigate a new problem of au-tomatically identifying the perspective from whicha document is written.
By perspective we meana ?subjective evaluation of relative significance, apoint-of-view.
?1 For example, documents about thePalestinian-Israeli conflict may appear to be aboutthe same topic but reveal different perspectives:1The American Heritage Dictionary of the English Lan-guage, 4th ed.
(1) The inadvertent killing by Israeli forces ofPalestinian civilians ?
usually in the course ofshooting at Palestinian terrorists ?
isconsidered no different at the moral and ethicallevel than the deliberate targeting of Israelicivilians by Palestinian suicide bombers.
(2) In the first weeks of the Intifada, for example,Palestinian public protests and civiliandemonstrations were answered brutally byIsrael, which killed tens of unarmed protesters.Example 1 is written from an Israeli perspective;Example 2 is written from a Palestinian perspec-tive.
Anyone knowledgeable about the issues ofthe Israeli-Palestinian conflict can easily identify theperspectives from which the above examples werewritten.
However, can computers learn to identifythe perspective of a document given a training cor-pus?When an issue is discussed from different per-spectives, not every sentence strongly reflects theperspective of the author.
For example, the follow-ing sentences were written by a Palestinian and anIsraeli.
(3) The Rhodes agreements of 1949 set them asthe ceasefire lines between Israel and the Arabstates.
(4) The green line was drawn up at the RhodesArmistice talks in 1948-49.Examples 3 and 4 both factually introduce the back-ground of the issue of the ?green line?
without ex-pressing explicit perspectives.
Can we develop a109system to automatically discriminate between sen-tences that strongly indicate a perspective and sen-tences that only reflect shared background informa-tion?A system that can automatically identify the per-spective from which a document is written will bea valuable tool for people analyzing huge collec-tions of documents from different perspectives.
Po-litical analysts regularly monitor the positions thatcountries take on international and domestic issues.Media analysts frequently survey broadcast news,newspapers, and weblogs for differing viewpoints.Without the assistance of computers, analysts haveno choice but to read each document in order to iden-tify those from a perspective of interest, which is ex-tremely time-consuming.
What these analysts needis to find strong statements from different perspec-tives and to ignore statements that reflect little or noperspective.In this paper we approach the problem of learningindividual perspectives in a statistical framework.We develop statistical models to learn how perspec-tives are reflected in word usage, and we treat theproblem of identifying perspectives as a classifica-tion task.
Although our corpus contains document-level perspective annotations, it lacks sentence-levelannotations, creating a challenge for learning theperspective of sentences.
We propose a novel sta-tistical model to overcome this problem.
The ex-perimental results show that the proposed statisti-cal models can successfully identify the perspectivefrom which a document is written with high accu-racy.2 Related WorkIdentifying the perspective from which a documentis written is a subtask in the growing area of au-tomatic opinion recognition and extraction.
Sub-jective language is used to express opinions, emo-tions, and sentiments.
So far, research in automaticopinion recognition has primarily addressed learn-ing subjective language (Wiebe et al, 2004; Riloffet al, 2003), identifying opinionated documents (Yuand Hatzivassiloglou, 2003) and sentences (Yu andHatzivassiloglou, 2003; Riloff et al, 2003), and dis-criminating between positive and negative language(Pang et al, 2002; Morinaga et al, 2002; Yu andHatzivassiloglou, 2003; Turney and Littman, 2003;Dave et al, 2003; Nasukawa and Yi, 2003; Popescuand Etzioni, 2005; Wilson et al, 2005).
While by itsvery nature we expect much of the language that isused when presenting a perspective or point-of-viewto be subjective, labeling a document or a sentenceas subjective is not enough to identify the perspec-tive from which it is written.
Moreover, the ideol-ogy and beliefs authors possess are often expressedin ways other than positive or negative language to-ward specific targets.Research on the automatic classification of movieor product reviews as positive or negative (e.g.,(Pang et al, 2002; Morinaga et al, 2002; Turneyand Littman, 2003; Nasukawa and Yi, 2003; Mullenand Collier, 2004; Beineke et al, 2004; Hu and Liu,2004)) is perhaps the most similar to our work.
Aswith review classification, we treat perspective iden-tification as a document-level classification task, dis-criminating, in a sense, between different types ofopinions.
However, there is a key difference.
A pos-itive or negative opinion toward a particular movieor product is fundamentally different from an overallperspective.
One?s opinion will change from movieto movie, whereas one?s perspective can be seen asmore static, often underpinned by one?s ideology orbeliefs about the world.There has been research in discourse analysis thatexamines how different perspectives are expressedin political discourse (van Dijk, 1988; Pan et al,1999; Geis, 1987).
Although their research mayhave some similar goals, they do not take a compu-tational approach to analyzing large collections ofdocuments.
To the best of our knowledge, our ap-proach to automatically identifying perspectives indiscourse is unique.3 CorpusOur corpus consists of articles published on thebitterlemonswebsite2.
The website is set up to?contribute to mutual understanding [between Pales-tinians and Israelis] through the open exchange ofideas.
?3 Every week an issue about the Israeli-Palestinian conflict is selected for discussion (e.g.,2http://www.bitterlemons.org3http://www.bitterlemons.org/about/about.html110?Disengagement: unilateral or coordinated??
), anda Palestinian editor and an Israeli editor each con-tribute one article addressing the issue.
In addition,the Israeli and Palestinian editors invite one Israeliand one Palestinian to express their views on theissue (sometimes in the form of an interview), re-sulting in a total of four articles in a weekly edi-tion.
We choose the bitterlemons website fortwo reasons.
First, each article is already labeledas either Palestinian or Israeli by the editors, allow-ing us to exploit existing annotations.
Second, thebitterlemons corpus enables us to test the gen-eralizability of the proposed models in a very real-istic setting: training on articles written by a smallnumber of writers (two editors) and testing on arti-cles from a much larger group of writers (more than200 different guests).We collected a total of 594 articles published onthe website from late 2001 to early 2005.
The dis-tribution of documents and sentences are listed inTable 1.
We removed metadata from all articles, in-Palestinian IsraeliWritten by editors 148 149Written by guests 149 148Total number of documents 297 297Average document length 740.4 816.1Number of sentences 8963 9640Table 1: The basic statistics of the corpuscluding edition numbers, publication dates, topics,titles, author names and biographic information.
Weused OpenNLP Tools4 to automatically extract sen-tence boundaries, and reduced word variants usingthe Porter stemming algorithm.We evaluated the subjectivity of each sentence us-ing the automatic subjective sentence classifier from(Riloff and Wiebe, 2003), and find that 65.6% ofPalestinian sentences and 66.2% of Israeli sentencesare classified as subjective.
The high but almostequivalent percentages of subjective sentences in thetwo perspectives support our observation in Sec-tion 2 that a perspective is largely expressed usingsubjective language, but that the amount of subjec-tivity in a document is not necessarily indicative of4http://sourceforge.net/projects/opennlp/its perspective.4 Statistical Modeling of PerspectivesWe develop algorithms for learning perspectives us-ing a statistical framework.
Denote a training corpusas a set of documents Wn and their perspectives la-bels Dn, n = 1, .
.
.
,N , where N is the total numberof documents in the corpus.
Given a new documentW?
with a unknown document perspective, the per-spective D?
is calculated based on the following con-ditional probability.P (D?|W?
, {Dn,Wn}Nn=1) (5)We are also interested in how strongly each sen-tence in a document conveys perspective informa-tion.
Denote the intensity of the m-th sentence ofthe n-th document as a binary random variable Sm,n.To evaluate Sm,n, how strongly a sentence reflectsa particular perspective, we calculate the followingconditional probability.P (Sm,n|{Dn,Wn}Nn=1) (6)4.1 Na?
?ve Bayes ModelWe model the process of generating documents froma particular perspective as follows:pi ?
Beta(?pi, ?pi)?
?
Dirichlet(??
)Dn ?
Binomial(1, pi)Wn ?
Multinomial(Ln, ?d)First, the parameters pi and ?
are sampled once fromprior distributions for the whole corpus.
Beta andDirichlet are chosen because they are conjugate pri-ors for binomial and multinomial distributions, re-spectively.
We set the hyperparameters ?pi, ?pi, and??
to one, resulting in non-informative priors.
Adocument perspective Dn is then sampled from a bi-nomial distribution with the parameter pi.
The valueof Dn is either d0 (Israeli) or d1 (Palestinian).
Wordsin the document are then sampled from a multino-mial distribution, where Ln is the length of the doc-ument.
A graphical representation of the model isshown in Figure 1.111pi ?Dn WnNFigure 1: Na?
?ve Bayes ModelThe model described above is commonly knownas a na?
?ve Bayes (NB) model.
NB models havebeen widely used for various classification tasks,including text categorization (Lewis, 1998).
TheNB model is also a building block for the modeldescribed later that incorporates sentence-level per-spective information.To predict the perspective of an unseen documentusing na?
?ve Bayes , we calculate the posterior distri-bution of D?
in (5) by integrating out the parameters,?
?P (D?, pi, ?|{(Dn,Wn)}Nn=1, W?
)dpid?
(7)However, the above integral is difficult to compute.As an alternative, we use Markov Chain MonteCarlo (MCMC) methods to obtain samples from theposterior distribution.
Details about MCMC meth-ods can be found in Appendix A.4.2 Latent Sentence Perspective ModelWe introduce a new binary random variable, S, tomodel how strongly a perspective is reflected at thesentence level.
The value of S is either s1 or s0,where s1 indicates a sentence is written stronglyfrom a perspective while s0 indicates it is not.
Thewhole generative process is modeled as follows:pi ?
Beta(?pi, ?pi)?
?
Beta(??
, ??
)?
?
Dirichlet(??
)Dn ?
Binomial(1, pi)Sm,n ?
Binomial(1, ?
)Wm,n ?
Multinomial(Lm,n, ?
)The parameters pi and ?
have the same semantics asin the na?
?ve Bayes model.
S is naturally modeled asa binomial variable, where ?
is the parameter of S.S represents how likely it is that a sentence stronglyconveys a perspective.
We call this model the La-tent Sentence Perspective Model (LSPM) because Sis not directly observed.
The graphical model repre-sentation of LSPM is shown in Figure 2.pi ?
?DnSm,n Wm,nNMnFigure 2: Latent Sentence Perspective ModelTo use LSPM to identify the perspective of a newdocument D?
with unknown sentence perspectives S?,we calculate posterior probabilities by summing outpossible combinations of sentence perspective in thedocument and parameters.?
?
?
?Sm,n?S?P (D?, Sm,n, S?, pi, ?, ?| (8){(Dn,Wn)}Nn=1, W?
)dpid?d?As before, we resort to MCMC methods to samplefrom the posterior distributions, given in Equations(5) and (6).As is often encountered in mixture models, thereis an identifiability issue in LSPM.
Because the val-ues of S can be permuted without changing the like-lihood function, the meanings of s0 and s1 are am-biguous.
In Figure 3a, four ?
values are used to rep-resent the four possible combinations of documentperspective d and sentence perspective intensity s. Ifwe do not impose any constraints, s1 and s0 are ex-changeable, and we can no longer strictly interprets1 as indicating a strong sentence-level perspectiveand s0 as indicating that a sentence carries little orno perspective information.
The other problem ofthis parameterization is that any improvement fromLSPM over the na?
?ve Bayes model is not necessarily112d0?d0,s0s0?d0,s1s1d1?d1,s0s0?d0,s0s1(a) s0 and s1 are not identifiables1?d0,s1d0?d1,s1d1 ?s0s0(b) sharing ?d1,s0 and?d0,s0Figure 3: Two different parameterization of ?due to the explicit modeling of sentence-level per-spective.
S may capture aspects of the documentcollection that we never intended to model.
For ex-ample, s0 may capture the editors?
writing styles ands1 the guests?
writing styles in the bitterlemonscorpus.We solve the identifiability problem by forcing?d1,s0 and ?d0,s0 to be identical and reducing thenumber of ?
parameters to three.
As shown in Fig-ure 3b, there are separate ?
parameters conditionedon the document perspective (left branch of the tree,d0 is Israeli and d1 is Palestinian), but there is single?
parameter when S = s0 shared by both document-level perspectives (right branch of the tree).
We as-sume that the sentences with little or no perspectiveinformation, i.e., S = s0, are generated indepen-dently of the perspective of a document.
In otherwords, sentences that are presenting common back-ground information or introducing an issue and thatdo not strongly convey any perspective should looksimilar whether they are in Palestinian or Israeli doc-uments.
By forcing this constraint, we become moreconfident that s0 represents sentences of little per-spectives and s1 represents sentences of strong per-spectives from d1 and d0 documents.5 Experiments5.1 Identifying Perspective at the DocumentLevelWe evaluate three different models for the taskof identifying perspective at the document level:two na?
?ve Bayes models (NB) with different infer-ence methods and Support Vector Machines (SVM)(Cristianini and Shawe-Taylor, 2000).
NB-B usesfull Bayesian inference and NB-M uses Maximuma posteriori (MAP).
We compare NB with SVM notonly because SVM has been very effective for clas-sifying topical documents (Joachims, 1998), but alsoto contrast generative models like NB with discrimi-native models like SVM.
For training SVM, we rep-resent each document as a V -dimensional featurevector, where V is the vocabulary size and each co-ordinate is the normalized term frequency within thedocument.
We use a linear kernel for SVM andsearch for the best parameters using grid methods.To evaluate the statistical models, we train themon the documents in the bitterlemons corpusand calculate how accurately each model predictsdocument perspective in ten-fold cross-validationexperiments.
Table 2 reports the average classi-fication accuracy across the the 10 folds for eachmodel.
The accuracy of a baseline classifier, whichrandomly assigns the perspective of a document asPalestinian or Israeli, is 0.5, because there are equiv-alent numbers of documents from the two perspec-tives.Model Data Set Accuracy ReductionBaseline 0.5SVM Editors 0.9724NB-M Editors 0.9895 61%NB-B Editors 0.9909 67%SVM Guests 0.8621NB-M Guests 0.8789 12%NB-B Guests 0.8859 17%Table 2: Results for Identifying Perspectives at theDocument LevelThe last column of Table 2 is error reductionrelative to SVM.
The results show that the na?
?veBayes models and SVM perform surprisingly wellon both the Editors and Guests subsets of thebitterlemons corpus.
The na?
?ve Bayes mod-els perform slightly better than SVM, possibly be-cause generative models (i.e., na?
?ve Bayes models)achieve optimal performance with a smaller num-ber of training examples than discriminative models(i.e., SVM) (Ng and Jordan, 2002), and the size ofthe bitterlemons corpus is indeed small.
NB-B,which performs full Bayesian inference, improves113on NB-M, which only performs point estimation.The results suggest that the choice of words madeby the authors, either consciously or subconsciously,reflects much of their political perspectives.
Statis-tical models can capture word usage well and canidentify the perspective of documents with high ac-curacy.Given the performance gap between Editors andGuests, one may argue that there exist distinct edit-ing artifacts or writing styles of the editors andguests, and that the statistical models are capturingthese things rather than ?perspectives.?
To test if thestatistical models truly are learning perspectives, weconduct experiments in which the training and test-ing data are mismatched, i.e., from different subsetsof the corpus.
If what the SVM and na?
?ve Bayesmodels learn are writing styles or editing artifacts,the classification performance under the mismatchedconditions will be considerably degraded.Model Training Testing AccuracyBaseline 0.5SVM Guests Editors 0.8822NB-M Guests Editors 0.9327 43%NB-B Guests Editors 0.9346 44%SVM Editors Guests 0.8148NB-M Editors Guests 0.8485 18%NB-B Editors Guests 0.8585 24%Table 3: Identifying Document-Level Perspectiveswith Different Training and Testing SetsThe results on the mismatched training and test-ing experiments are shown in Table 3.
Both SVMand the two variants of na?
?ve Bayes perform wellon the different combinations of training and testingdata.
As in Table 2, the na?
?ve Bayes models per-form better than SVM with larger error reductions,and NB-B slightly outperforms NB-M.
The high ac-curacy on the mismatched experiments suggests thatstatistical models are not learning writing styles orediting artifacts.
This reaffirms that document per-spective is reflected in the words that are chosen bythe writers.We list the most frequent words (excluding stop-words) learned by the the NB-M model in Ta-ble 4.
The frequent words overlap greatly be-tween the Palestinian and Israeli perspectives, in-cluding ?state,?
?peace,?
?process,?
?secure?
(?se-curity?
), and ?govern?
(?government?).
This is incontrast to what we expect from topical text classi-fication (e.g., ?Sports?
vs.
?Politics?
), in which fre-quent words seldom overlap.
Authors from differ-ent perspectives often choose words from a simi-lar vocabulary but emphasize them differently.
Forexample, in documents that are written from thePalestinian perspective, the word ?palestinian?
ismentioned more frequently than the word ?israel.
?It is, however, the reverse for documents that arewritten from the Israeli perspective.
Perspectivesare also expressed in how frequently certain people(?sharon?
v.s.
?arafat?
), countries (?international?v.s.
?america?
), and actions (?occupation?
v.s.
?set-tle?)
are mentioned.
While one might solicit thesecontrasting word pairs from domain experts, our re-sults show that statistical models such as SVM andna?
?ve Bayes can automatically acquire them.5.2 Identifying Perspectives at the SentenceLevelIn addition to identifying the perspective of a docu-ment, we are interested in knowing which sentencesof the document strongly conveys perspective in-formation.
Sentence-level perspective annotationsdo not exist in the bitterlemons corpus, whichmakes estimating parameters for the proposed La-tent Sentence Perspective Model (LSPM) difficult.The posterior probability that a sentence stronglycovey a perspective (Example (6)) is of the most in-terest, but we can not directly evaluate this modelwithout gold standard annotations.
As an alterna-tive, we evaluate how accurately LSPM predicts theperspective of a document, again using 10-fold crossvalidation.
Although LSPM predicts the perspec-tive of both documents and sentences, we will doubtthe quality of the sentence-level predictions if thedocument-level predictions are incorrect.The experimental results are shown in Table 5.We include the results for the na?
?ve Bayes modelsfrom Table 3 for easy comparison.
The accuracy ofLSPM is comparable or even slightly better than thatof the na?
?ve Bayes models.
This is very encouragingand suggests that the proposed LSPM closely cap-tures how perspectives are reflected at both the doc-ument and sentence levels.
Examples 1 and 2 fromthe introduction were predicted by LSPM as likely to114Palestinian palestinian, israel, state, politics, peace, international, people, settle, occupation, sharon,right, govern, two, secure, end, conflict, process, side, negotiateIsraeli israel, palestinian, state, settle, sharon, peace, arafat, arab, politics, two, process, secure,conflict, lead, america, agree, right, gaza, governTable 4: The top twenty most frequent stems learned by the NB-M model, sorted by P (w|d)Model Training Testing AccuracyBaseline 0.5NB-M Guests Editors 0.9327NB-B Guests Editors 0.9346LSPM Guests Editors 0.9493NB-M Editors Guests 0.8485NB-B Editors Guests 0.8585LSPM Editors Guests 0.8699Table 5: Results for Perspective Identification at theDocument and Sentence Levelscontain strong perspectives, i.e., large Pr(S?
= s1).Examples 3 and 4 from the introduction were pre-dicted by LSPM as likely to contain little or no per-spective information, i.e., high Pr(S?
= s0).The comparable performance between the na?
?veBayes models and LSPM is in fact surprising.
Wecan train a na?
?ve Bayes model directly on the sen-tences and attempt to classify a sentence as reflect-ing either a Palestinian or Israeli perspective.
A sen-tence is correctly classified if the predicted perspec-tive for the sentence is the same as the perspectiveof the document from which it was extracted.
Us-ing this model, we obtain a classification accuracy ofonly 0.7529, which is much lower than the accuracypreviously achieved at the document level.
Identify-ing perspectives at the sentence level is thus moredifficult than identifying perspectives at the docu-ment level.
The high accuracy at the document levelshows that LSPM is very effective in pooling evi-dence from sentences that individually contain littleperspective information.6 ConclusionsIn this paper we study a new problem of learning toidentify the perspective from which a text is writtenat the document and sentence levels.
We show thatmuch of a document?s perspective is expressed inword usage, and statistical learning algorithms suchas SVM and na?
?ve Bayes models can successfullyuncover the word patterns that reflect author per-spective with high accuracy.
In addition, we developa novel statistical model to estimate how stronglya sentence conveys perspective, in the absence ofsentence-level annotations.
By introducing latentvariables and sharing parameters, the Latent Sen-tence Perspective Model is shown to capture wellhow perspectives are reflected at the document andsentence levels.
The small but positive improvementdue to sentence-level modeling in LSPM is encour-aging.
In the future, we plan to investigate how con-sistently LSPM sentence-level predictions are withhuman annotations.AcknowledgmentThis material is based on work supported bythe Advanced Research and Development Activity(ARDA) under contract number NBCHC040037.A Gibbs SamplersBased the model specification described in Sec-tion 4.2 we derive the Gibbs samplers (Chen et al,2000) for the Latent Sentence Perspective Model asfollows,pi(t+1) ?
Beta(?pi +N?n=1dn + d?
(t+1),?pi + N ?N?n=1dn + 1 ?
d?(t+1))?
(t+1) ?
Beta(??
+N?n=1Mn?m=1sm,n +M??m=1s?m,??
+N?n=1Mn ?N?n=1Mn?m=1sm,n + M?
?M??m=1s?m)115?
(t+1) ?
Dirichlet(??
+N?n=1Mn?m=1wm,n)Pr(S(t+1)n,m = s1) ?
P (Wm,n|Sm,n = 1, ?
(t))Pr(S(t+1)m,n = 1|?,Dn)Pr(D?
(t+1) = d1) ?M??m=1dbinom(?
(t+1)d )M??m=1dmultinom(?d,m?
(t))dbinom(pi(t))where dbinom and dmultinom are the density func-tions of binomial and multinomial distributions, re-spectively.
The superscript t indicates that a sampleis from the t-th iteration.
We run three chains andcollect 5000 samples.
The first half of burn-in sam-ples are discarded.ReferencesPhilip Beineke, Trevor Hastie, and ShivakumarVaithyanathan.
2004.
The sentimental factor:Improving review classification via human-providedinformation.
In Proceedings of ACL-2004.Ming-Hui Chen, Qi-Man Shao, and Joseph G. Ibrahim.2000.
Monte Carlo Methods in Bayesian Computa-tion.
Springer-Verlag.Nello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Univer-sity Press.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
In Pro-ceedings of WWW-2003.Michael L. Geis.
1987.
The Language of Politics.Springer.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Proceedings of KDD-2004.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
In Proceedings of ECML-1998.David D. Lewis.
1998.
Naive (Bayes) at forty: The inde-pendence assumption in information retrieval.
In Pro-ceedings of ECML-1998.S.
Morinaga, K. Yamanishi, K. Tateishi, andT.
Fukushima.
2002.
Mining product reputations onthe web.
In Proceedings of KDD-2002.Tony Mullen and Nigel Collier.
2004.
Sentiment analy-sis using support vector machines with diverse infor-mation sources.
In Proceedings of EMNLP-2004.T.
Nasukawa and J. Yi.
2003.
Sentiment analysis: Cap-turing favorability using natural language processing.In Proceedings of K-CAP 2003.Andrew Y. Ng and Michael Jordan.
2002.
On discrim-inative vs. generative classifiers: A comparison of lo-gistic regression and naive bayes.
In NIPS-2002, vol-ume 15.Zhongdang Pan, Chin-Chuan Lee, Joseph Man Chen, andClement Y.K.
So.
1999.
One event, three stories: Me-dia narratives of the handover of hong kong in culturalchina.
Gazette, 61(2):99?112.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of EMNLP-2002.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of HLT/EMNLP-2005, pages 339?346.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of EMNLP-2003.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of CoNLL-2003.Peter Turney and Michael L. Littman.
2003.
Measuringpraise and criticism: Inference of semantic orientationfrom association.
ACM TOIS, 21(4):315?346.T.A.
van Dijk.
1988.
News as Discourse.
LawrenceErlbaum, Hillsdale, NJ.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Computational Linguistics, 30(3).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of HLT/EMNLP-2005.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of EMNLP-2003.116
