Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 335?340,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsModeling Wisdom of Crowds UsingLatent Mixture of Discriminative ExpertsDerya Ozkan and Louis-Philippe MorencyInstitute for Creative TechnologiesUniversity of Southern California{ozkan,morency}@ict.usc.eduAbstractIn many computational linguistic scenarios,training labels are subjectives making it nec-essary to acquire the opinions of multiple an-notators/experts, which is referred to as ?wis-dom of crowds?.
In this paper, we propose anew approach for modeling wisdom of crowdsbased on the Latent Mixture of Discrimina-tive Experts (LMDE) model that can automat-ically learn the prototypical patterns and hid-den dynamic among different experts.
Experi-ments show improvement over state-of-the-artapproaches on the task of listener backchannelprediction in dyadic conversations.1 IntroductionIn many real life scenarios, it is hard to collectthe actual labels for training, because it is expen-sive or the labeling is subjective.
To address thisissue, a new direction of research appeared in thelast decade, taking full advantage of the ?wisdom ofcrowds?
(Surowiecki, 2004).
In simple words, wis-dom of crowds enables parallel acquisition of opin-ions from multiple annotators/experts.In this paper, we propose a new method to fusewisdom of crowds.
Our approach is based on theLatent Mixture of Discriminative Experts (LMDE)model originally introduced for multimodal fu-sion (Ozkan et al, 2010).
In our Wisdom-LMDEmodel, a discriminative expert is trained for eachcrowd member.
The key advantage of our compu-tational model is that it can automatically discoverthe prototypical patterns of experts and learn the dy-namic between these patterns.
An overview of ourapproach is depicted in Figure 1.We validate our model on the challenging task oflistener backchannel feedback prediction in dyadicconversations.
Backchannel feedback includes thenods and paraverbals such as ?uh-huh?
and ?mm-hmm?
that listeners produce as they are speaking.Backchannels play a significant role in determiningthe nature of a social exchange by showing rapportand engagement (Gratch et al, 2007).
When thesesignals are positive, coordinated and reciprocated,they can lead to feelings of rapport and promotebeneficial outcomes in diverse areas such as nego-tiations and conflict resolution (Drolet and Morris,2000), psychotherapeutic effectiveness (Tsui andSchultz, 1985), improved test performance in class-rooms (Fuchs, 1987) and improved quality of childcare (Burns, 1984).
Supporting such fluid interac-tions has become an important topic of virtual hu-man research.
In particular, backchannel feedbackhas received considerable interest due to its perva-siveness across languages and conversational con-texts.
By correctly predicting backchannel feed-back, virtual agent and robots can have strongersense of rapport.What makes backchannel prediction task well-suited for our model is that listener feedback variesbetween people and is often optional (listeners canalways decide to give feedback or not).
A successfulcomputational model of backchannel must be ableto learn these variations among listeners.
Wisdom-LMDE is a generic approach designed to integrateopinions from multiple listeners.In our experiments, we validate the performanceof our approach using a dataset of 43 storytellingdyadic interactions.
Our analysis suggests three pro-335Latent Mixture ofDiscriminative Expertsh1 h2 h3 hny2y1 y3 ynx1x1Wisdom of crowds(listener backchannel)Speakerx1 x2 x3 xnPitchWordsGaze Look  at listenerh1TimeFigure 1: Left: Our approach applied to backchannel prediction: (1) multiple listeners experience the same series ofstimuli (pre-recorded speakers) and (2) a Wisdom-LMDE model is learned using this wisdom of crowds, associatingone expert for each listener.
Right: Baseline models used in our experiments: a) Conditional Random Fields (CRF),b) Latent Dynamic Conditional Random Fields (LDCRF), c) CRF Mixture of Experts (no latent variable)totypical patterns for backchannel feedback.
Byautomatically identifying these prototypical pat-terns and learning the dynamic, our Wisdom-LMDEmodel outperforms the previous approaches for lis-tener backchannel prediction.1.1 Previous WorkSeveral researchers have developed models to pre-dict when backchannel should happen.
Ward andTsukahara (2000) propose a unimodal approachwhere backchannels are associated with a region oflow pitch lasting 110ms during speech.
Nishimura etal.
(2007) present a unimodal decision-tree approachfor producing backchannels based on prosodic fea-tures.
Cathcart et al (2003) propose a unimodalmodel based on pause duration and trigram part-of-speech frequency.Wisdom of crowds was first defined and used inbusiness world by Surowiecki (2004).
Later, it hasbeen applied to other research areas as well.
Raykaret.
al.
(2010) proposed a probabilistic approach forsupervised learning tasks for which multiple annota-tors provide labels but not an absolute gold standard.Snow et.
al.
(2008) show that using non-expert la-bels for training machine learning algorithms can beas effective as using a gold standard annotation.In this paper, we present a computational ap-proach for listener backchannel prediction that ex-ploits multiple listeners.
Our model takes into ac-count the differences in people?s reactions, and au-tomatically learns the hidden structure among them.The rest of the paper is organized as follows.
InSection 2, we present the wisdom acquisition pro-cess.
Then, we describe our Wisdom-LMDE modelin Section 3.
Experimentals are presented in Sec-tion 4.
Finally, we conclude with discussions andfuture works in Section 5.2 Wisdom AcquisitionIt is known that culture, age and gender affect peo-ple?s nonverbal behaviors (Linda L. Carli and Loe-ber, 1995; Matsumoto, 2006).
Therefore, theremight be variations among people?s reactions evenwhen experiencing the same situation.
To effi-ciently acquire responses from multiple listeners, weemploy the Parasocial Consensus Sampling (PCS)paradigm (Huang et al, 2010), which is based on thetheory that people behave similarly when interact-ing through a media (e.g., video conference).
Huanget al (2010) showed that a virtual human driven byPCS approach creates significantly more rapport andis perceived as more believable than the virtual hu-man driven by face-to-face interaction data (from ac-tual listener).
This result indicates that the parasocialparadigm is a viable source of information for wis-dom of crowds.In practice, PCS is applied by having participantswatch pre-recorded speaker videos drawn from a336Listener1 Listener2 Listener3 Listener4 Listener5 Listener6 Listener7 Listener8 Listener9pauselabel:subPOS:NNPOS:NNpauselabel:pmodpausePOS:NNlabel:nmodpausePOS:NNlow pitchpausedirdist:L1low pitchPOS:NNpauselow pitchEyebrow updirdist:L8+POS:NNeye gazedirdist:R1POS:JJlownesseye gazepauseTable 1: Most predictive features for each listener from our wisdom dataset.
This analysis suggests three prototypicalpatterns for backchannel feedback.dyadic story-telling dataset.
In our experiments,we used 43 video-recorded dyadic interactions fromthe RAPPORT1 dataset (Gratch et al, 2006).
Thisdataset was drawn from a study of face-to-facenarrative discourse (?quasi-monologic?
storytelling).The videos of the actual listeners were manually an-notated for backchannel feedback.
For PCS wis-dom acquisition, we recruited 9 participants, whowere told to pretend they are an active listener andpress the keyboard whenever they felt like provid-ing backchannel feedback.
This provides us the re-sponses from multiple listeners all interacting withthe same speaker, hence the wisdom necessary tomodel the variability among listeners.3 Modeling Wisdom of CrowdsGiven the wisdom of multiple listeners, our goal is tocreate a computational model of backchannel feed-back.
Although listener responses vary among indi-viduals, we expect some patterns in these responses.Therefore, we first analyze the most predictive fea-tures for each listener and search for prototypicalpatterns (in Section 3.1).
Then, we present ourWisdom-LMDE that allows to automatically learnthe hidden structure within listener responses.3.1 Wisdom AnalysisWe analyzed our wisdom data to see the most rel-evant speaker features when predicting responsesfrom each individual listener.
(The complete list ofspeaker features are described in Section 4.1.)
Weused a feature ranking scheme based on a sparseregularization technique, as described in (Ozkan andMorency, 2010).
It allows us to identify the speakerfeatures most predictive of each listener backchan-nel feedback.
The top 3 features for all 9 listenersare listed in Table 1.This analysis suggests three prototypical patterns.For the first 3 listeners, pause in speech and syntac-1http://rapport.ict.usc.edu/tic information (POS:NN) are more important.
Thenext 3 experts include a prosodic feature, low pitch,which is coherent with earlier findings (Nishimuraet al, 2007; Ward and Tsukahara, 2000).
It is inter-esting to see that the last 3 experts incorporate visualinformation when predicting backchannel feedback.This is in line with Burgoon et al (Burgoon et al,1995) work showing that speaker gestures are of-ten correlated with listener feedback.
These resultsclearly suggest that variations be present among lis-teners and some prototypical patterns may exist.Based on these observations, we propose new com-putational model for listener backchannel.3.2 Computational Model: Wisdom-LMDEThe goals of our computational model are to au-tomatically discover the prototypical patterns ofbackchannel feedback and learn the dynamic be-tween these patterns.
This will allow the compu-tational model to accurately predict the responses ofa new listener even if he/she changes her backchan-nel patterns in the middle of the interaction.
It willalso improve generalization by allowing mixtures ofthese prototypical patterns.To achieve these goals, we propose a variant of theLatent Mixture of Discriminative Experts (Ozkan etal., 2010) which takes full advantage of the wisdomof crowds.
Our Wisdom-LMDE model is based ona two step process: a Conditional Random Field(CRF, see Figure 1a) is learned for each wisdomlistener, and the outputs of these expert models areused as input to a Latent Dynamic Conditional Ran-dom Field (LDCRF, see Figure 1b) model, which iscapable of learning the hidden structure within theexperts.
In our Wisdom-LMDE, each expert cor-responds to a different listener from the wisdom ofcrowds.
More details about training and inference ofLMDE can be found in Ozkan et al (2010).3374 ExperimentsTo confirm the validity of our Wisdom-LMDEmodel, we compare its performance with compu-tational models previously proposed.
As motivatedearlier, we focus our experiments on predicting lis-tener backchannel since it is a well-suited task wherevariability exists among listeners.4.1 Multimodal Speaker FeaturesThe speaker videos were transcribed and annotatedto extract the following features:Lexical: Some studies have suggested an asso-ciation between lexical features and listener feed-back (Cathcart et al, 2003).
Therefore, we use allthe words (i.e., unigrams) spoken by the speaker.Syntactic structure: Using a CRF part-of-speech(POS) tagger and a data-driven left-to-right shift-reduce dependency parser (Sagae and Tsujii, 2007)we extract four types of features from a syntactic de-pendency structure corresponding to the utterance:POS tags and grammatical function for each word,POS tag of the syntactic head, distance and directionfrom each word to its syntactic head.Prosody: Prosody refers to the rhythm, pitch andintonation of speech.
Several studies have demon-strated that listener feedback is correlated witha speaker?s prosody (Ward and Tsukahara, 2000;Nishimura et al, 2007).
Following this, we usedownslope in pitch, pitch regions lower than 26thpercentile, drop/rise and fast drop/rise in energy ofspeech, vowel volume, pause.Visual gestures: Gestures performed by the speakerare often correlated with listener feedback (Burgoonet al, 1995).
Eye gaze, in particular, has often beenimplicated as eliciting listener feedback.
Thus, weencode the following contextual features: speakerlooking at listener, smiling, moving eyebrows upand frowning.Although our current method for extracting thesefeatures requires that the entire utterance to be avail-able for processing, this provides us with a firststep towards integrating information about syntac-tic structure in multimodal prediction models.
Manyof these features could in principle be computed in-crementally with only a slight degradation in accu-racy, with the exception of features that require de-pendency links where a word?s syntactic head is tothe right of the word itself.
We leave an investiga-tion that examines only syntactic features that can beproduced incrementally in real time as future work.4.2 Baseline ModelsConsensus Classifier In our first baseline model, weuse consensus labels to train a CRF model, whichare constructed by a similar approach presentedin (Huang et al, 2010).
The consensus threshold isset to 3 (at least 3 listeners agree to give feedback ata point) so that it contains approximately the samenumber of head nods as the actual listener.
See Fig-ure 1 for a graphical representation of CRF model.CRF Mixture of Experts To show the importanceof latent variable in our Wisdom-LMDE model, wetrained a CRF-based mixture of discriminative ex-perts.
This model is similar to the LogarithmicOpinion Pool (LOP) CRF suggested by Smith etal.
(2005).
Similar to our Wisdom-LMDE model,the training is performed in two steps.
A graphicalrepresentation of a CRF Mixture of experts is givenin the Figure 1.Actual Listener (AL) Classifiers This baseline modelconsists of two models: CRF and LDCRF chains(See Figure 1).
To train these models, we use thelabels of the ?Actual Listeners?
(AL) from the RAP-PORT dataset.Multimodal LMDE In this baseline model, we com-pare our Wisdom LMDE to a multimodal LMDE,where each expert refers to one of 5 different set ofmultimodal features as presented in (Ozkan et al,2010): lexical, prosodic, part-of-speech, syntactic,and visual.Random Classifier Our last baseline model is a ran-dom backchannel generator as desribed by Wardand Tsukahara (2000).
This model randomly gener-ates backchannels whenever some pre-defined con-ditions in the prosody of the speech is purveyed.4.3 MethodolgyWe performed hold-out testing on a randomly se-lected subset of 10 interactions.
The training setcontains the remaining 33 interactions.
Model pa-rameters were validated by using a 3-fold cross-validation strategy on the training set.
Regulariza-338Table 2: Comparison of our Wisdom-LMDE model with previously proposed models.
The last column shows thepaired one tailed t-test results comparing Wisdom LMDE to each model.tion values used are 10k for k = -1,0,..,3.
Numbersof hidden states used in the LDCRF models were2, 3 and 4.
We use the hCRF library2 for trainingof CRFs and LDCRFs.
Our Wisdom-LMDE modelwas implemented in Matlab based on the hCRF li-brary.
Following (Morency et al, 2008), we usean encoding dictionary to represent our features.The performance is measured by using the F-score,which is the weighted harmonic mean of precisionand recall.
A backchannel is predicted correctly ifa peak happens during an actual listener backchan-nel with high enough probability.
The threshold wasselected automatically during validation.4.4 Results and DiscussionBefore reviewing the prediction results, is it impor-tant to remember that backchannel feedback is anoptional phenomena, where the actual listener mayor may not decide on giving feedback (Ward andTsukahara, 2000).
Therefore, results from predic-tion tasks are expected to have lower accuracies asopposed to recognition tasks where labels are di-rectly observed (e.g., part-of-speech tagging).Table 2 summarizes our experiments comparingour Wisdom-LMDE model with state-of-the-art ap-proaches for behavior prediction (see Section 4.2).Our Wisdom-LMDE model achieves the best F1score.
Statistical t-test analysis show that Wisdom-LMDE is significantly better than Consensus Clas-sifier, AL Classifier (LDCRF), Multimodel LMDEand Random Classifier.The second best F1 score is achieved by CRFMixture of experts, which is the only model amongother baseline models that combines different lis-tener labels in a late fusion manner.
This result2http://sourceforge.net/projects/hrcf/supports our claim that wisdom of clouds improveslearning of prediction models.
CRF Mixture modelis a linear combination of the experts, whereasWisdom-LMDE enables different weighting of ex-perts at different point in time.
By using hiddenstates, Wisdom-LMDE can automatically learn theprototypical patterns between listeners.One really interesting result is that the optimalnumber of hidden states in the Wisdom-LMDEmodel (after cross-validation) is 3.
This is coherentwith our qualitative analysis in Section 3.1, wherewe observed 3 prototypical patterns.5 ConclusionsIn this paper, we proposed a new approach calledWisdom-LMDE for modeling wisdom of crowds,which automatically learns the hidden structure inlistener responses.
We applied this method onthe task of listener backchannel feedback predic-tion, and showed improvement over previous ap-proaches.
Both our qualitative analysis and exper-imental results suggest that prototypical patterns ex-ist when predicting listener backchannel feedback.The Wisdom-LMDE is a generic model applicableto multiple sequence labeling tasks (such as emotionanalysis and dialogue intent recognition), where la-bels are subjective (i.e.
small inter-coder reliability).AcknowledgementsThis material is based upon work supported bythe National Science Foundation under Grant No.0917321 and the U.S. Army Research, Develop-ment, and Engineering Command (RDE-COM).The content does not necessarily reflect the positionor the policy of the Government, and no official en-dorsement should be inferred.339ReferencesJudee K. Burgoon, Lesa A. Stern, and Leesa Dillman.1995.
Interpersonal adaptation: Dyadic interactionpatterns.
Cambridge University Press, Cambridge.M.
Burns.
1984.
Rapport and relationships: The basis ofchild care.
Journal of Child Care, 4:47?57.N.
Cathcart, Jean Carletta, and Ewan Klein.
2003.
Ashallow model of backchannel continuers in spokendialogue.
In European Chapter of the Association forComputational Linguistics.
51?58.Aimee L. Drolet and Michael W. Morris.
2000.
Rap-port in conflict resolution: Accounting for how face-to-face contact fosters mutual cooperation in mixed-motive conflicts.
Journal of Experimental Social Psy-chology, 36(1):26?50.D.
Fuchs.
1987.
Examiner familiarity effects on test per-formance: Implications for training and practice.
Top-ics in Early Childhood Special Education, 7:90?104.J.
Gratch, A. Okhmatovskaia, F. Lamothe, S. Marsella,M.
Morales, R.J. Werf, and L.-P. Morency.
2006.
Vir-tual rapport.
Proceedings of International Conferenceon Intelligent Virtual Agents (IVA), Marina del Rey,CA.Jonathan Gratch, Ning Wang, Jillian Gerten, and EdwardFast.
2007.
Creating rapport with virtual agents.
InIVA.L.
Huang, L.-P. Morency, and J. Gratch:.
2010.
Paraso-cial consensus sampling: combining multiple perspec-tives to learn virtual human behavior.
In Interna-tional Conference on Autonomous Agents and Multi-agent Systems (AAMAS).Suzanne J. LaFleur Linda L. Carli and Christopher C.Loeber.
1995.
Nonverbal behavior, gender, and influ-ence.
Journal of Personality and Social Psychology.68, 1030-1041.D.
Matsumoto.
2006.
Culture and Nonverbal Behav-ior.
The Sage Handbook of Nonverbal Communica-tion, Sage Publications Inc.L.-P. Morency, I. de Kok, and J. Gratch.
2008.
Predict-ing listener backchannels: A probabilistic multimodalapproach.
In Proceedings of the Conference on Intel-ligent Virutal Agents (IVA).Ryota Nishimura, Norihide Kitaoka, and Seiichi Naka-gawa.
2007.
A spoken dialog system for chat-likeconversations considering response timing.
Interna-tional Conference on Text, Speech and Dialog.
599-606.D.
Ozkan and L.-P. Morency.
2010.
Concensus of self-features for nonverbal behavior analysis.
In HumanBehavior Understanding in conjucion with Interna-tional Conference in Pattern Recognition.D.
Ozkan, K. Sagae, and L.-P. Morency.
2010.
La-tent mixture of discriminative experts for multimodalprediction modeling.
In International Conference onComputational Linguistics (COLING).Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-ardo Hermosillo Valadez, Charles Florin, Luca Bo-goni, Linda Moy, and David Blei.
2010.
Learningfrom crowds.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 1044?1050,Prague, Czech Republic, June.
Association for Com-putational Linguistics.A.
Smith, T. Cohn, and M. Osborne.
2005.
Logarithmicopinion pools for conditional random fields.
In ACL,pages 18?25.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2008.Cheap and fast - but is it good?
Evaluating non-expertannotations for natural language tasks.James Surowiecki.
2004.
The Wisdom of Crowds: Whythe Many Are Smarter Than the Few and How Col-lective Wisdom Shapes Business, Economies, Societiesand Nations.
Doubleday.P.
Tsui and G.L.
Schultz.
1985.
Failure of rapport: Whypsychotheraputic engagement fails in the treatment ofasian clients.
American Journal of Orthopsychiatry,55:561?569.N.
Ward and W. Tsukahara.
2000.
Prosodic fea-tures which cue back-channel responses in english andjapanese.
Journal of Pragmatics.
23, 1177?1207.340
