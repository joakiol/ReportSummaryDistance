UDel: Refining a Method of Named Entity GenerationCharles F. Greenbacker, Nicole L. Sparks, Kathleen F. McCoy, and Che-Yu KuoDepartment of Computer and Information SciencesUniversity of DelawareNewark, Delaware, USA[charlieg|sparks|mccoy|kuo]@cis.udel.eduAbstractThis report describes the methods and re-sults of a system developed for the GRECNamed Entity Challenge 2010.
We de-tail the refinements made to our 2009 sub-mission and present the output of the self-evaluation on the development data set.1 IntroductionThe GREC Named Entity Challenge 2010 (NEG)is an NLG shared task whereby submitted systemsmust select a referring expression from a list ofoptions for each mention of each person in a text.The corpus is a collection of 2,000 introductorysections from Wikipedia articles about individualpeople in which all mentions of person entitieshave been annotated.
An in-depth description ofthe task, along with the evaluation results from theprevious year, is provided by Belz et al (2009).Our 2009 submission (Greenbacker and Mc-Coy, 2009a) was an extension of the system wedeveloped for the GREC Main Subject Refer-ence Generation Challenge (MSR) (Greenbackerand McCoy, 2009b).
Although our system per-formed reasonably-well in predicting REG08-Type in the NEG task, our string accuracy scoreswere disappointingly-low, especially when com-pared to the other competing systems and our ownperformance in the MSR task.
As suggested by theevaluators (Belz et al, 2009), this was due in largepart to our reliance on the list of REs being in aparticular order, which had changed for the NEGtask.2 MethodThe first improvement we made to our existingmethods related to the manner by which we se-lected the specific RE to employ.
In 2009, wetrained a series of decision trees to predict REG08-Type based on our psycholinguistically-inspiredfeature set (described in (Greenbacker and Mc-Coy, 2009c)), and then simply chose the first op-tion in the list of REs matching the predicted type.For 2010, we incorporated the case of each REinto our target attribute so that the decision treeclassifier would predict both the type and case forthe given reference.
Then, we applied a seriesof rules governing the length of initial and sub-sequent REs involving a person?s name (followingNenkova and McKeown (2003)), as well as ?back-offs?
if the predicted type or case were not avail-able.Another improvement we made involved ourmethod of determining whether the use of a pro-noun would introduce ambiguity in a given con-text.
Previously, we searched for references toother people entities since the most recent mentionof the entity at hand, and if any were found, weassumed these would cause the use of a pronounto be ambiguous.
However, this failed to accountfor the fact that personal pronouns in English aregender-specific (ie.
the mention of a male individ-ual would not make the use of ?she?
ambiguous).So, we refined this by determining the gender ofeach named entity (by seeing which personal pro-nouns were associated with it in the list of REs),and only noting ambiguity when the current entityand candidate interfering antecedent were of thesame gender.Other small changes from 2009 include an ex-panded abbreviation set in the sentence segmenter,separate decision trees for the main subject andother entities, and fixing how we handled embed-ded REF elements with unspecified mention IDs.3 ResultsScores for REG08-Type precision & recall, stringaccuracy, and string-edit distance are presented inFigure 1.
These were computed on the entire de-velopment set, as well as the three subsets, us-ing the geval.pl self-evaluation tool provided in theNEG participants?
pack.While we were able to achieve an improvementof nearly 50% over our 2009 scores in string ac-curacy, we saw less than a 1% gain in overallREG08-Type performance.Metric ScoreType Precision/Recall 0.757995735607676String Accuracy 0.650496141124587Mean Edit Distance 0.875413450937156Normalized Distance 0.319266300067796(a) Scores on the entire development set.Metric ScoreType Precision/Recall 0.735294117647059String Accuracy 0.623287671232877Mean Edit Distance 0.839041095890411Normalized Distance 0.345490867579909(b) Scores on the ?Chefs?
subset.Metric ScoreType Precision/Recall 0.790769230769231String Accuracy 0.683544303797468Mean Edit Distance 0.882911392405063Normalized Distance 0.279837251356239(c) Scores on the ?Composers?
subset.Metric ScoreType Precision/Recall 0.745928338762215String Accuracy 0.642140468227425Mean Edit Distance 0.903010033444816Normalized Distance 0.335326519731057(d) Scores on the ?Inventors?
subset.Figure 1: Scores on the development set obtainedvia the geval.pl self-evaluation tool.
REG08-Typeprecision and recall were equal in all four sets.4 ConclusionsThe fact that our string accuracy scores improvedover our 2009 submission far more than REG08-Type prediction is hardly surprising.
Our effortsduring this iteration of the NEG task were primar-ily focused on enhancing our methods of choosingthe best RE once the reference type was selected.We remain several points below the best-performing team from 2009 (ICSI-Berkeley), pos-sibly due to the inclusion of additional items intheir feature set, or the use of Conditional Ran-dom Fields as their learning technique (Favre andBohnet, 2009).5 Future WorkMoving forward, we hope to expand our featureset by including the morphology of words immedi-ately surrounding the reference, as well as a moreextensive reference history, as suggested by (Favreand Bohnet, 2009).
We suspect that these featuresmay play a significant role in determining the typeof referenced used, the prediction of which acts asa ?bottleneck?
in generating exact REs.We would also like to compare the efficacy ofseveral different machine learning techiques as ap-plied to our feature set and the NEG task.ReferencesAnja Belz, Eric Kow, and Jette Viethen.
2009.
TheGREC named entity generation challenge 2009:Overview and evaluation results.
In Proceedingsof the 2009 Workshop on Language Generation andSummarisation (UCNLG+Sum 2009), pages 88?98,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Benoit Favre and Bernd Bohnet.
2009.
ICSI-CRF: The generation of references to the mainsubject and named entities using conditional ran-dom fields.
In Proceedings of the 2009 Workshopon Language Generation and Summarisation (UC-NLG+Sum 2009), pages 99?100, Suntec, Singapore,August.
Association for Computational Linguistics.Charles Greenbacker and Kathleen McCoy.
2009a.UDel: Extending reference generation to multipleentities.
In Proceedings of the 2009 Workshopon Language Generation and Summarisation (UC-NLG+Sum 2009), pages 105?106, Suntec, Singa-pore, August.
Association for Computational Lin-guistics.Charles Greenbacker and Kathleen McCoy.
2009b.UDel: Generating referring expressions guided bypsycholinguistc findings.
In Proceedings of the2009 Workshop on Language Generation and Sum-marisation (UCNLG+Sum 2009), pages 101?102,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Charles F. Greenbacker and Kathleen F. McCoy.2009c.
Feature selection for reference generation asinformed by psycholinguistic research.
In Proceed-ings of the CogSci 2009 Workshop on Production ofReferring Expressions (PRE-Cogsci 2009), Amster-dam, July.Ani Nenkova and Kathleen McKeown.
2003.
Improv-ing the coherence of multi-document summaries:a corpus study for modeling the syntactic realiza-tion of entities.
Technical Report CUCS-001-03,Columbia University, Computer Science Depart-ment.
