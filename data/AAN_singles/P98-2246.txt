Neural Network Recognition of Spelling ErrorsMark LewellenComputational Linguistics, Georgetown UniversityWashington DC, 20057-1051mlewellen@apptek.comAbstractOne area in which artificial neural networks(ANNs) may strengthen NLP systems is in theidentification of words under noisy conditions.
Inorder to achieve this benefit when spelling errorsor spelling variants are present, variable-lengthstrings of symbols must be converted to ANNinput/output form--fixed-length arrays ofnumbers.
A common view in the neural networkcommunity has been that different forms ofinput/output representations have negligible ffecton ANN performance.
This paper, however,shows that input/output representations can in factaffect the performance of ANNs in the case ofnatural anguage words.
Minimum properties foran adequate word representation are proposed, aswell as new methods of word representation.To test the hypothesis that word representationssignificantly affect ANN performance, traditionaland new word representations are evaluated fortheir ability to recognize words in the presence offour types of typographical noise: substitutions,insertions, deletions and reversals of letters.
Theresults indicate that word representations have asignificant effect on ANN performance.Additionally, different types of wordrepresentation are shown to perform better ondifferent types of error.IntroductionANNs are a promising technology for NLP, sincea strength of ANNs is their "common sense"ability to make reasonable decisions even whenfaced with novel data, while a weakness of NLPapplications i brittleness in the face of ambiguoussituations.
One area in which much ambiguityoccurs is the identification of words: words maybe misspelled, they may have valid spellingvariants, and they can be homographic.
Robustword recognition capabilities can improveapplications which involve text understanding,and are the central component of applicationssuch as spell-checking and name searching.In order for ANNs to recognize variant andhomographic forms of words, however, wordsmust be transformed to a form that is meaningfulto ANNs.
The interface to ANNs is input andoutput layers each composed of fixed numbers ofnodes.
Each node is associated with a numericalvalue, typically between 0 and 1.
Thus, words--variable-length strings of symbolsmneed to beconverted to fixed-length arrays of numbers inorder to be processed by ANNs.
The resultingword representations should ideally:1) be in a form which enables an ANN toidentify spelling similarities and differences;2) represent all the letters of words;3) be concise enough to allow processing of alarge number of words in a reasonable time.To date, research in ANNs has ignored these low-level input issues, even though they criticallyaffect "higher-level" processing.
A common viewhas been that different representation methods donot significantly impact ANN performance.
Thispaper, however, presents word representationsthat significantly enhance ANN performance onnatural anguage words.1 Word RepresentationsTo represent words for ANNs, symbols need to beconverted to numbers and variable length must beconverted to fixed length, ideally under the threeconstraints listed above.To handle the variable length of words, recurrentANNs have sometimes been used.
In a recurrentANN, the values of nodes in the output or hiddenlayers are recycled to a portion of the input layernodes.
Input to the network thus consists of aletter representation plus the state of the networkafter all previous letters.
Recurrent ANNs have1490several drawbacks, though: they require muchmore training time and use part of their processingcapability for the development of representations,rather than the problem to which the network isapplied, hnportantly, such designs suffer from aprimacy effect: the initial letters of a wordreceive greater emphasis, so that errors at thebeginning of words cause much greater problemsthan errors at the etad of words.1.1 Fixed-Length Letter BuffersThe most common method of representing lettersis in a buffer containing a set number of letterrepresentations.
For example, space might beallocated for up to 14 letters; if 26 nodes are usedto represent each letter, then the input buffer usesa total of 364 nodes.
In such "fixed-length letterbuffers" (FLLBs), letters are traditionally placedin the buffer one by one from the left, as inwriting from left to right.
These left-alignedFLLBs suffer from the primacy effect discussedabove.
To correct his problem, two new FLLBstructures are proposed: split and bi-directional.A split FLLB splits the word in two, left-justifying tim first half, and right-justifying thesecond half, in order to halve the effect of errorswhich cause position shifts in subsequent letters.A bi-directional FLLB is similar to a split FLLB,but uses all available space in the FLLB.
Insteadof leaving certain letter positions blank, as in asplit representation, extra letter positions are filledby continuing to add letters from the beginningand end.
Such a scheme tends to weight themiddle of words more heavily, as that portion of aword is more likely to be represented twice.Examples of FLLBs for the word "knight" are:Le f t  k n i g h tSplit k n i g h tBi-D k n i g h n i g h t i1.2 Local vs.
Distributed RepresentationsEach letter of an FLLB needs to be converted to anumeric representation.
Letters are symbols,which are adequately represented by binary, ratherthan continuous values.
Consequently, represen-tations become much larger, which may placel iraitations on the choice of word representation.Each letter may be represented in a "local" or"distributed" manner.
In a local letter represen-tation, 26 nodes could be utilized, one for eachletter of the alphabet.
Only the node corres-ponding to a particular letter is assigned a value ofl, while the rest of the nodes have a value of 0.in a distributed representation, several nodescombine to represent one letter; each node mayalso participate in different letter representations.Distributed representations are more biologicallyplausible, and are particularly desirable for theircompressive characteristics, as well as theincreased error-tolerance of having several, ratherthan one, nodes contribute to a representation.2 Test Design and ResultsTesting of alternative representations wasperformed with two variables, FLLB type andlocal/distributed, for each of four types of error.A test corpus of similarly-spelled words wasdeveloped from a list of American English homo-phones (Antworth 1993).
Homophone groupscontaining words with apostrophes were removed,yielding a list of 1449 words.
Each word wasrandomly assigned an arbitrary 4-digit symbol.The training set for the ANN consists of 1449word/symbol pairs.
The words were presented tothe network in a 14-letter FLLB, composed in sixmethods (left-aligned J split J hi-directional Xlocal J distributed).
The local method uses l of 26nodes for each letter (total of 364 nodes), whilethe distributed method uses 4 of I 1 nodes for eachletter (total of 154 nodes), with no more than twonodes pennitted to overlap with any other letterrepresentation.
The output of the network is a4-digit symbol, represented by four 9-nodedistributed representations (total of 36 nodes).Four test sets were developed from the word list,each roughly 10% of the list size, resulting in onetest set of 150 words for each type of error--substitution, reversal, insertion and deletion.
Theerrors were created by han& evenly distributedthrough the beginning, middle and end of words.Training and testing were performed with anARTMAP-ICMM ANN, a variant of ARTMAP-IC (Carpenter & Markuzon 1996) specialized fordata sets containing many-to-many mappings.The testing phase of ARTMAP-ICMM outputs arank-ordered list of potential mappings, with therank of the desired output returned as a score.
Ascore of 1 is optimal; in this case, the worst scoreis 1449.
As scores become larger, they becomeless meaningful; for example, a difference of 10 is1491much more significant between 5 and 15 thanbetween 100 and 110.To evaluate network performance for a test set,measures of central tendency are computed for therank scores of the test set.
Since large scoresbecome increasingly arbitrary, it is desirable tolimit their effect on measures of central tendency.A measure that often fulfills this criterion is themedian; however it is somewhat inexact for thispurpose.
The squared mean root (analagous to thequadratic mean) lessens the influence of largescores while remaining more discriminating:N /N  ,or  4"x ?/=1The squared mean root is presented first, as aprimary indicator, with the median following forcomparison.
The test results are presented alongboth test variables for each of four error types.Substitution Left Split Bi-DLocal 1.7/I  1.7 /1 1.5 / IDistributed 1.8/1 1.7 / I 1.6 / !Reversal Left Split Bi-DLocal 4.8 / 3 5.8 / 4 3.4 / 2Distributed 5.9 /2  7.0 /3 4.3 /2Insertion Left Split Bi-DLocal 99 /19 5.9 /2  4.7 /3Distributed 97 /24 7.8 /3 7.5 /5.5Deletion Left Split B i- DLocal 125/64 7.3 /3 21.7/21.5Distributed 152 / 97 13.5 / 4 38.8/39.53 Position-maintaining and position-altering errorsThe results for the four types of error can be usedto create two groupings: position-maintaining andposition-altering errors.
The position-maintainingerrors are substitution and reversal errors, whichdo not cause other letters to shift to differentpositions.
The position-altering errors (insertionsand deletions), however, do cause such a shift.The scores demonstrate that for FLLBrepresentations, position-altering errors causegreater difficulty than position-maintaining errors.The traditional left-aligned FLLB performeddramatically worse on position-altering errors(scores of 99197 and 1251152) than on position-maintaining errors (1.711.8 and 4.815.9).
Both thesplit and bi-directional FLLBs display much-improved performance on the position-alteringerrors.
The bi-directional FLLB, however, stillhas substantially more difficulty with deletionerrors than does the split FLLB.
The split FLLBthus demonstrated the best overall performance ofthe three FLLB representations.Along the local/distributed variable, the localrepresentations consistently equal or surpass theperformance of the distributed representations.The advantage, however, is relatively minor,unlike the clear distinctions between FLLB type.ConclusionThis paper has found that word and letterrepresentations can have a significant effect onANN recognition of spelling errors.
It hasspecifically found that:?
Methods of word representation call havesubstantial and measureable effects on ANNperformance.?
Position-altering (insertion and deletion) andposition-maintaining errors (substitution andreversal) have different effects on ANNrecognition of spelling errors.?
An FLLB may, in addition to a traditionalleft-aligned representation, be organized illsplit and bi-directional structures.
These newFLLBs result in improved performance onposition-altering errors, with tile splitrepresentation ffering the best performance.Research in progress includes development ofother ANN word representation methods andtesting with data from other languages.AcknowledgementsThank-you to Gall Carpenter for suggesting theapplicability of ARTMAP-IC, and Donald Loritzand anonymous reviewers for their helpful advice.ReferencesAntworth E., ed.
(1993) List of homophones inGeneral American English.
Consortium for LexicalResearch.
27 Jan. 1998 <ftp://crl.nmsu.edu/CLR/lexica/homophones/>.Carpenter G. A. and Markuzon N. (1996) ARTMAP-ICand Medical Diagnosis: Instance Counting andInconsistent Cases.
Technical Report CAS/CNS TR-96-017.
Boston University, Boston, MA.1492
