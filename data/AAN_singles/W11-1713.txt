Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 104?110,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsAutomatic Emotion Classification for Interpersonal CommunicationFrederik VaassenCLiPS - University of AntwerpS.L.202, Lange Winkelstraat 40-42B-2000 Antwerpen, Belgiumfrederik.vaassen@ua.ac.beWalter DaelemansCLiPS - University of AntwerpS.L.
203, Lange Winkelstraat 40-42B-2000 Antwerpen, Belgiumwalter.daelemans@ua.ac.beAbstractWe introduce a new emotion classificationtask based on Leary?s Rose, a framework forinterpersonal communication.
We present asmall dataset of 740 Dutch sentences, out-line the annotation process and evaluate an-notator agreement.
We then evaluate the per-formance of several automatic classificationsystems when classifying individual sentencesaccording to the four quadrants and the eightoctants of Leary?s Rose.
SVM-based classi-fiers achieve average F-scores of up to 51% for4-way classification and 31% for 8-way clas-sification, which is well above chance level.We conclude that emotion classification ac-cording to the Interpersonal Circumplex is achallenging task for both humans and ma-chine learners.
We expect classification per-formance to increase as context informationbecomes available in future versions of ourdataset.1 IntroductionWhile sentiment and opinion mining are popular re-search topics, automatic emotion classification oftext is a relatively novel ?and difficult?
natural lan-guage processing task.
Yet it immediately speaksto the imagination.
Being able to automaticallyidentify and classify user emotions would open upa whole range of interesting applications, from in-depth analysis of user reviews and comments to en-riching social network environments according tothe user?s emotions.Most experiments in emotion classification focuson a set of basic emotions such as ?happiness?, ?sad-ness?, ?fear?, ?anger?, ?surprise?
and ?disgust?.
Theinterpretation of ?emotion?
we?re adopting in thispaper, however, is slightly more specific.
We con-centrate on the emotions that are at play in interper-sonal communication, more specifically in the dy-namics between participants in a conversation: isone of the participants taking on a dominant role?Are the speakers working towards a common goal,or are they competing?
Being able to automati-cally identify these power dynamics in interpersonalcommunication with sufficient accuracy would openup interesting possibilities for practical applications.This technology would be especially useful in e-learning, where virtual agents that accept (and inter-pret) natural language input could be used by playersto practice their interpersonal communication skillsin a safe environment.The emotion classification task we present in thispaper involves classifying individual sentences intothe quadrants and octants of Leary?s Rose, a frame-work for interpersonal communication.We give a brief overview of related work in sec-tion 2 and the framework is outlined in section 3.Section 4 introduces the dataset we used for clas-sification.
Section 5 outlines the methodology weapplied, and the results of the different experimentsare reported on in section 6.
We discuss these resultsand draw conclusions in section 7.
Finally, section 8gives some pointers for future research.2 Related WorkThe techniques that have been used for emotion clas-sification can roughly be divided into pattern-basedmethods and machine-learning methods.
An often-104used technique in pattern-based approaches is to usepre-defined lists of keywords which help determinean instance?s overall emotion contents.
The AESOPsystem by Goyal et al (2010), for instance, attemptsto analyze the affective state of characters in fablesby identifying affective verbs and by using a set ofprojection rules to calculate the verbs?
influence ontheir patients.
Another possible approach ?whichwe subscribe to?
is to let a machine learner deter-mine the appropriate emotion class.
Mishne (2005)and Keshtkar and Inkpen (2009), for instance, at-tempt to classify LiveJournal posts according to theirmood using Support Vector Machines trained withfrequency features, length-related features, semanticorientation features and features representing specialsymbols.
Finally, Rentoumi et al (2010) posit thatcombining the rule-based and machine learning ap-proaches can have a positive effect on classificationperformance.
By classifying strongly figurative ex-amples using Hidden Markov Models while relyingon a rule-based system to classify the mildly figura-tive ones, the overall performance of the classifica-tion system is improved.Whereas emotion classification in general is arelatively active domain in the field of computa-tional linguistics, little research has been done re-garding the automatic classification of text accord-ing to frameworks for interpersonal communication.We have previously carried out a set of classifica-tion experiments using Leary?s Rose on a smallerdataset (Vaassen and Daelemans, 2010), only tak-ing the quadrants of the Rose into account.
To ourknowledge, this is currently the only other work con-cerning automatic text classification using any real-ization of the Interpersonal Circumplex.
We expandon this work by using a larger dataset which we eval-uate for reliability.
We attempt 8-way classificationinto the octants of the Rose, and we also evaluate abroader selection of classifier setups, including one-vs-all and error-correcting systems.3 Leary?s RoseThough several frameworks have been developedto describe the dynamics involved in interpersonalcommunication (Wiggins, 2003; Benjamin, 2006),we have chosen to use the Interpersonal Circum-plex, better known as ?Leary?s Rose?
(Leary, 1957).Figure 1: Leary?s RoseLeary?s Rose (Figure 1) is defined by two axes: theabove-below axis (vertical), which tells us whetherthe speaker is being dominant or submissive towardsthe listener; and the together-opposed axis (horizon-tal), which says something about the speaker?s will-ingness to co-operate with the listener.
The axes di-vide the Rose into four quadrants, and each quadrantcan again be divided into two octants.What makes the Circumplex especially interest-ing for interpersonal communication training is thatit also allows one to predict (to some extent) whatposition the listener is most likely going to takein reaction to the way the speaker positions him-self.
Two types of interactions are at play in Leary?sRose, one of complementarity and one of similar-ity.
Above-behavior triggers a (complementary) re-sponse from the below zone and vice versa, whiletogether-behavior triggers a (similar) response fromthe together zone and opposed-behavior triggers a(similar) response from the opposed area of theRose.
The speaker can thus influence the listener?semotions (and consequently, his response) by con-sciously positioning himself in the quadrant that willlikely trigger the desired reaction.4 DatasetTo evaluate how difficult it is to classify sentences?both manually and automatically?
according toLeary?s Rose, we used an expanded version of thedataset described in Vaassen and Daelemans (2010).105The dataset1 contains a total of 740 Dutch sentenceslabeled according to their position on the Interper-sonal Circumplex.
The majority of the sentenceswere gathered from works specifically designed toteach the use of Leary?s Rose (van Dijk, 2000; vanDijk and Moes, 2005).
The remaining sentenceswere specifically written by colleagues at CLiPS andby e-learning company Opikanoba.
31 sentencesthat were labeled as being purely neutral were re-moved from the dataset for the purposes of this clas-sification experiment, leaving a set of 709 Dutchsentences divided across the octants and quadrantsof the Interpersonal Circumplex.
Table 1 shows theclass distribution within the dataset and also lists thestatistical random baselines for both 8-class and 4-class classification tasks.709 sentencesTOG A: 165 sentences leading: 109 sentenceshelping: 56 sentencesTOG B: 189 sentences co-operative: 92 sentencesdependent: 97 sentencesOPP B: 189 sentences withdrawn: 73 sentencesdefiant: 116 sentencesOPP A: 166 sentences aggressive: 71 sentencescompetitive: 95 sentencesBaseline 25.4% 13.1%Table 1: Distribution of classes within the dataset2Below are a few example sentences with their cor-responding position on the Rose.?
Please have a seat and we?ll go over the optionstogether.
- helping (TOG A)?
So what do you think I should do now?
- de-pendent (TOG B)?
That?s not my fault, administration?s not my re-sponsibility!
- defiant (OPP B)?
If you had done your job this would never havehappened!
- aggressive (OPP A)4.1 Agreement ScoresPlacing sentences on Leary?s Rose is no easy task,not even for human annotators.
An added complica-tion is that the sentences in the dataset lack any formof textual or situational context.
We therefore expectagreement between annotators to be relatively low.1Dataset available on request.2?TOG?
and ?OPP?
stand for together and opposed respec-tively, while ?A?
and ?B?
stand for above and below.To measure the extent of inter-annotator disagree-ment, we had four annotators label the same randomsubset of 50 sentences.
The annotators were given ashort introduction to the workings of Leary?s Rose,and were then instructed to label each of the sen-tences according to the octants of the Rose using thefollowing set of questions:?
Is the current sentence task-oriented (opposed)or relationship-oriented (together)??
Does the speaker position himself as the dom-inant partner in the conversation (above) or isthe speaker submissive (below)??
Which of the above two dimensions (affinity ordominance) is most strongly present?Annotators were also given the option to label a sen-tence as being purely neutral should no emotionalcharge be present.Table 2 shows Fleiss?
kappa scores calculated for4 and 8-class agreement.# of classes ?4 0.378 0.29Table 2: Inter-annotator agreement, 4 annotatorsThough the interpretation of kappa scores is initself subjective, scores between 0.20 and 0.40 areusually taken to indicate ?fair agreement?.The full dataset was also annotated a second timeby the initial rater six months after the first annota-tion run.
This yielded the intra-annotator scores inTable 3.
A score of 0.5 is said to indicate ?moderateagreement?.# of classes ?4 0.508 0.37Table 3: Intra-annotator agreementThe relatively low kappa scores indicate that theclassification of isolated sentences into the quad-rants or octants of Leary?s Rose is a difficult taskeven for humans.As an upper baseline for automatic classification,we take the average of the overlaps between the106main annotator and each of the other annotators onthe random subset of 50 sentences.
This gives us anupper baseline of 51.3% for 4-way classification and36.0% for the 8-class task.5 MethodologyOur approach falls within the domain of automatictext categorization (Sebastiani, 2002), which fo-cuses on the classification of text into predefined cat-egories.
Starting from a training set of sentenceslabeled with their position on the Rose, a machinelearner should be able to pick up on cues that will al-low the classification of new sentences into the cor-rect emotion class.
Since there are no easily identi-fiable keywords or syntactic structures that are con-sistently used with a position on Leary?s Rose, usinga machine learning approach is a logical choice forthis emotion classification task.5.1 Feature ExtractionThe sentences in our dataset were first syntacti-cally parsed using the Frog parser for Dutch (Vanden Bosch et al, 2007).
From the parsed out-put, we extracted token, lemma, part-of-speech, syn-tactic and dependency features using a ?bag-of-ngrams?
approach, meaning that for each n-gram(up to trigrams) of one of the aforementioned fea-ture types present in the training data, we countedhow many times it occurred in the current instance.We also introduced some extra features, includingaverage word and sentence length, features for spe-cific punctuation marks (exclamation points, ques-tion marks...) and features relating to (patterns of)function and content words.Due to efficiency and memory considerations, wedid not use all of the above feature types in the sameexperiment.
Instead, we ran several experiments us-ing combinations of up to three feature types.5.2 Feature Subset SelectionWhereas some machine learners (e.g.
Support Vec-tor Machines) deal relatively well with large num-bers of features, others (e.g.
memory-based learn-ers) struggle to achieve good classification accuracywhen too many uninformative features are present.For these learners, we go through an extra featureselection step where the most informative featuresare identified using a filter metric (see also Vaassenand Daelemans (2010)), and where only the top nfeatures are selected to be included in the featurevectors.5.3 ClassificationWe compared the performance of different classifiersetups on both the 4-way and 8-way classificationtasks.
We evaluated a set of native multiclass clas-sifiers: the memory-based learner TiMBL (Daele-mans and van den Bosch, 2005), a Na?
?ve Bayesclassifier and SVM Multiclass (Tsochantaridis et al,2005), a multiclass implementation of Support Vec-tor Machines.
Further experiments were run usingSVM light classifiers (Joachims, 1999) in a one-vs-all setup and in an Error-Correcting Output Codesetup (ECOCs are introduced in more detail in sec-tion 5.3.1).
Parameters for SVM Multiclass andSVM light were determined using Paramsearch?stwo-fold pseudo-exhaustive search (Van den Bosch,2004) on vectors containing only token unigrams.The parameters for TiMBL were determined usinga genetic algorithm designed to search through theparameter space3.5.3.1 Error-Correcting Output CodesThere are several ways of decomposing multiclassproblems into binary classification problems.
Error-Correcting Output Codes (ECOCs) (Dietterich andBakiri, 1995) are one of these techniques.
Inspiredby distributed output coding in signal processing(Sejnowski and Rosenberg, 1987), ECOCs assigna distributed output code ?or ?codeword??
to eachclass in the multiclass problem.
These codewords,when taken together, form a code matrix (Table 4).Class 1 0 1 0 1 0 1 0Class 2 0 0 0 0 1 1 1Class 3 1 1 1 1 1 1 1Class 4 0 0 1 1 0 0 1Table 4: Example code matrixEach column of this code matrix defines a binaryclassification task, with a 0 indicating that the in-stances with the corresponding class label shouldbe part of a larger negative class, and a 1 indicat-3The fitness factor driving evolution was the classificationaccuracy of the classifier given a set of parameters, using tokenunigram features in a 10-fold cross-validation experiment.107ing the positive class.
A binary classifier (or ?di-chotomizer?)
is trained for each column.
When anew instance is to be classified, it is first classified byeach of these dichotomizers, which each return theirpredicted class (1 or 0).
The combined output fromeach dichotomizer forms a new codeword.
The finalclass is determined by choosing the codeword in thecode matrix that has the smallest distance (accordingto some distance metric) to the predicted codeword.This method offers one important advantage com-pared to other, simpler ensemble methods: becausethe final class label is determined by calculating thedistance between the predicted codeword and theclass codewords, it is possible to correct a certainnumber of bits in the predicted codeword if the dis-tance between the class codewords is large enough.Formally, a set of ECOCs can correct bd?12 c bits,where d is the minimum Hamming distance (thenumber of differing bits) between codewords in thecode matrix.
The error-correcting capacity of anECOC setup is thus entirely dependent on the codematrix used, and a great deal of attention has beendevoted to the different ways of constructing suchcode matrices (Ghani, 2000; Zhang et al, 2003;A?lvarez et al, 2007).In our ECOC classification setup, we used codematrices artificially constructed to maximize theirerror-correcting ability while keeping the number ofclassifiers within reasonable bounds.
For 4-classclassification, we constructed 7-bit codewords us-ing the exhaustive code construction technique de-scribed in Dietterich and Bakiri (1995).
For the 8-class classification problem, we used a Hadamardmatrix of order 8 (Zhang et al, 2003), which hasoptimal row (and column) separation for the givennumber of columns.
Both matrices have an error-correcting capacity of 1 bit.6 ResultsAll results in this section are based on 10-fold cross-validation experiments.
Table 5 shows accuracyscores and average F-scores for both 4-way and 8-way classification using classifiers trained on to-ken unigrams only, using optimal learner parame-ters.
For TiMBL, the number of token unigrams waslimited to the 1000 most predictive according to theGini coe?fficient4.
All other learners used the fullrange of token unigram features.
The Na?
?ve Bayesapproach performed badly on the 8-way classifica-tion task, wrongly classifying all instances of someclasses, making it impossible to calculate an F-score.4-class 8-classaccuracy F-score accuracy F-scoreSVM Multiclass 47.3% 46.8% 31.6% 28.3%Na?
?ve Bayes 42.6% 40.1% 26.1% NaNTiMBL 41.3% 41.3% 23.6% 22.9%SVM / one-vs-all 46.0% 45.4% 29.3% 27.2%SVM / ECOCs 48.1% 47.8% 31.3% 26.3%Random baseline 25.4% 13.1%Upper baseline 51.3% 36.0%Table 5: Accuracy and average F-scores - token unigramsAll classifiers performed better than the randombaseline (25.4% for 4-class classification, 13.1% forclassification into octants) to a very significant de-gree.
We therefore take these token unigram scoresas a practical baseline.feature types accuracy avg.
F-scoreSVM Multiclass w1, l3, awl 49.4% 49.4%TiMBL w1, w2, l1 42.0% 42.0%SVM / one-vs-all l2, fw3, c3 51.1% 51.0%SVM / ECOCs l2, c3 52.1% 51.2%Table 6: Best feature type combinations - quadrants5feature types accuracy avg.
F-scoreSVM / one-vs-all w1, l1, c1 34.0% 30.9%SVM / ECOCs w2, fw3, c3 34.8% 30.2%Table 7: Best feature type combinations - octantsWe managed to improve the performance of someof the classifier systems by including more and dif-ferent features types.
Tables 6 and 7 show perfor-mance for 4-way and 8-way classification respec-tively, this time using the best possible combination4The filter metric and number of retained features was de-termined by testing the different options using 10-fold CV andby retaining the best-scoring combination (Vaassen and Daele-mans, 2010).5The ?feature types?
column indicates the types of featuresthat were used, represented as a letter followed by an integerindicating the size of the n-gram: w: word tokens, l: lemmas,fw: function words, c: characters, awl: average word length(based on the number of characters)108of up to three feature types6 for every classifier setupwhere an improvement was noted.We used McNemar?s test (Dietterich, 1998) tocompare the token unigram scores with the best fea-ture combination scores for each of the above clas-sifiers.
For both 4-way and 8-way classification, theone-vs-all and ECOC approaches produced signif-icantly different results7.
The improvement is lesssignificant for TiMBL and SVM Multiclass in the4-way classification experiments.Note that for classification into quadrants, the per-formance of the SVM-based classifiers is very closeto the upper baseline of 50.3% we defined earlier.It is unlikely that performance on this task will im-prove much more unless we add context informationto our interpersonal communication dataset.
The 8-way classification results also show promise, withscores up to 30%, but there is still room for improve-ment before we reach the upper baseline of 36%.In terms of classifiers, the SVM-based systemsperform better than their competitors.
Na?
?ve Bayesespecially seems to be struggling, performing signif-icantly worse for the 4-class classification task andmaking grave classification errors in the 8-way clas-sification task.
The memory-based learner TiMBLfares slightly better on the 8-class task, but isn?t ableto keep up with the SVM-based approaches.When we examine the specific features that areidentified as being the most informative, we see thatmost of them seem instinctively plausible as impor-tant cues related to positions on Leary?s Rose.
Ques-tion marks and exclamation marks, for instance, areamongst the 10 most relevant features.
So too arethe Dutch personal pronouns ?u?, ?je?
and ?we?
??u?
being a second person pronoun marking polite-ness, while ?je?
is the unmarked form, and ?we?
be-ing the first person plural pronoun.
Of course, noneof these features on their own are strong enough toaccurately classify the sentences in our dataset.
Itis only through complex interactions between manyfeatures that the learners are able to identify the cor-rect class for each sentence.6The best feature type combination for each setup was de-termined experimentally by running a 10-fold cross-validationtest for each of the possible combinations.74-class SVM one-vs-all: P=0.0014, 4-class SVM ECOCs:P=0.0170, 8-class SVM one-vs-all: P=0.0045, 8-class SVMECOCs: P=0.00927 ConclusionsWe have introduced a new emotion classificationtask based on the Interpersonal Circumplex or?Leary?s Rose?, a framework for interpersonal com-munication.
The goal of the classification task is toclassify individual sentences (outside of their textualor situational context), into one of the four quad-rants or eight octants of Leary?s Rose.
We have out-lined the annotation process of a small corpus of 740Dutch sentences, and have shown the classificationtask to be relatively difficult, even for human anno-tators.
We evaluated several classifier systems in atext classification approach, and reached the best re-sults using SVM-based systems.
The SVM learnersachieved F-scores around 51% on the 4-way classi-fication task, which is close to the upper baseline(based on inter-annotator agreement), and perfor-mance on 8-class classification reached F-scores ofalmost 31%.8 Future ResearchThe initial results of the emotion classification tasksdescribed in this paper are promising, but there isa clear sense that without some contextual informa-tion, it is simply too difficult to correctly classifysentences according to their interpersonal emotionalcharge.
For this reason, we are currently developinga new version of the dataset, which will no longercontain isolated sentences, but which will insteadconsist of full conversations.
We expect that havingthe sentences in their textual context will make theclassification task easier for both human annotatorsand machine learners.
It will be interesting to see ifand how the classification performance improves onthis new dataset.AcknowledgmentsThis study was made possible through financial sup-port from the IWT (the Belgian government agencyfor Innovation by Science and Technology, TETRA-project deLearyous).
Many thanks go out to our col-leagues at the e-Media Lab (Groep T, Leuven, Bel-gium) and Opikanoba, partners in the deLearyousproject.
We would also like to thank the WASSA2.011 reviewers for their helpful feedback.109ReferencesVictor A?lvarez, Jose A. Armario, Maria D. Frau, ElenaMartin and Amparo Osuna.
2007.
Error CorrectingCodes from Quasi-Hadamard Matrices.
Lecture Notesin Computer Science, volume 4547/2007.Lorna S. Benjamin, Jeffrey C. Rothweiler and KennethL.
Critchfield.
2006.
The Use of Structural Analy-sis of Social Behavior (SASB) as an Assessment Tool.Annual Review of Clinical Psychology, Vol.
2, No.
1.Walter Daelemans and Antal van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press, Cambridge, UK.Thomas G. Dietterich and Ghulum Bakiri.
1995.
SolvingMulticlass Learning Problems via Error-CorrectingOutput Codes.
Journal of Artificial Intelligence Re-search.Thomas G. Dietterich.
1998.
Approximate StatisticalTests for Comparing Supervised Classification Learn-ing Algorithms.
Neural Computing, volume 10.Rayid Ghani.
2000.
Using Error-Correcting Codes forText Classification.
Proceedings of the Seventeenth In-ternational Conference on Machine Learning.Amit Goyal, Ellen Riloff, Hal Daume III and NathanGilbert.
2010.
Toward Plot Units: Automatic Af-fect State Analysis.
Workshop on Computational Ap-proaches to Analysis and Generation of Emotion inText.Thorston Joachims.
1999.
Making large-scale supportvector machine learning practical.
MIT Press, Cam-bridge, MA.Fazel Keshtkar and Diana Inkpen.
2009.
Using Senti-ment Orientation Features for Mood Classification inBlogs.
Proceedings of the IEEE International Confer-ence on Natural Language Processing and KnowledgeEngineering (IEEE NLP-KE 2009).Timothy Leary.
1957.
Interpersonal Diagnosis of Per-sonality: Functional Theory and Methodology for Per-sonality Evaluation.
Ronald Press Company, NewYork.Kim Luyckx.
2011.
The Effect of Author Set Size andData Size in Authorship Attribution.
Literary and Lin-guistic Computing, volume 26/1.Francesco Masulli and Giorgio Valentini.
2004.
An Ex-perimental Analysis of the Dependence Among Code-word Bit Errors in ECOC Learning Machines.
Neuro-computing, volume 57.Gilad Mishne.
2005.
Experiments with Mood Classifi-cation in Blog Posts.
Proceedings of the 1st Workshopon Stylistic Analysis of Text for Information Access.Vassiliki Rentoumi, Stefanos Petrakis, Manfred Klen-ner, George A. Vouros and Vangelis Karkaletsis.2010.
United we Stand: Improving Sentiment Anal-ysis by Joining Machine Learning and Rule BasedMethods.
Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10).Fabrizio Sebastiani.
2002.
Machine Learning in Auto-mated Text Categorization.
ACM Comput.
Surv., vol-ume 34/1.Terrence J. Sejnowski and Charles R. Rosenberg.
1987.Parallel Networks that Learn to Pronounce EnglishText.
Complex Systems.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann and Yasemin Altun.
2005.
Large MarginMethods for Structured and Interdependent OutputVariables.
Journal of Machine Learning Research,6:1453-1484 (2005).Frederik Vaassen and Walter Daelemans.
2010.
EmotionClassification in a Serious Game for Training Com-munication Skills.
Computational Linguistics in theNetherlands 2010: selected papers from the twentiethCLIN meeting.Antal van den Bosch.
2004.
Wrapped Progressive Sam-pling Search for Optimizing Learning Algorithm Pa-rameters.
Proceedings of the 16th Belgian-Dutch Con-ference on Artificial Intelligence (BNAIC2004).Antal van den Bosch, Bertjan Busser, Walter Daelemansand Sander Canisius.
2007.
An Efficient Memory-based Morphosyntactic Tagger and Parser for Dutch.Selected Papers of the 17th Computational Linguisticsin the Netherlands Meeting (CLIN17).Bert van Dijk.
2000.
Be?
?nvloed anderen, begin bijjezelf.
Over gedrag en de Roos van Leary, 4th edition.Thema.Bert van Dijk and Fenno Moes.
2005.
Het grotebe??nvloedingsspel.
Thema.Jerry S. Wiggins.
2003.
Paradigms of Personality As-sessment.
Guilford Press.Aijun Zhang, Zhi-Li Wu, Chun-Hung Li and Kai-TaiFang.
2003.
On Hadamard-Type Output Coding inMulticlass Learning.
Lecture Notes in Computer Sci-ence, volume 2690/2003.
Springer Berlin / Heidel-berg.110
