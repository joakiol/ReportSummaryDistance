Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 958?967,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsBuilding Event Threads out of Multiple News ArticlesXavier TannierLIMSI-CNRSUniv.
Paris-SudOrsay, Francexavier.tannier@limsi.frVe?ronique MoriceauLIMSI-CNRSUniv.
Paris-SudOrsay, Francemoriceau@limsi.frAbstractWe present an approach for building multi-document event threads from a large corpusof newswire articles.
An event thread is basi-cally a succession of events belonging to thesame story.
It helps the reader to contextual-ize the information contained in a single arti-cle, by navigating backward or forward in thethread from this article.
A specific effort isalso made on the detection of reactions to aparticular event.In order to build these event threads, we use acascade of classifiers and other modules, tak-ing advantage of the redundancy of informa-tion in the newswire corpus.We also share interesting comments con-cerning our manual annotation procedure forbuilding a training and testing set1.1 IntroductionIn this paper, we explore a new way of dealingwith temporal relations between events.
Our taskis somewhat between multidocument summariza-tion and classification of temporal relations betweenevents.
We work with a large collection of En-glish newswire articles, where each article relatesan event: the main topic of the article is a specificevent, and other older events are mentioned in orderto put it into perspective.
Thus, we consider that anevent is associated with an article and that definingtemporal relations between articles is a way to definetemporal relations between events.1This work has been partially funded by French NationalResearch Agency (ANR) under project Chronolines (ANR-10-CORD-010).
We would like to thank the French News Agency(AFP) for providing us with the corpus.The task is to build a temporal graph of arti-cles, linked between each other by the following re-lations:?
Same event, when two documents relate thesame event, or when a document is an updateof another one.?
Continuation, when an event is the continua-tion or the consequence of a previous one.We also define a subset of continuation, calledreaction, concerning a document relating the reac-tion of someone to another event.Some examples of these three classes will begiven in Section 3.These relations can be represented by a directedgraph where documents are vertices and relationsare edges (as illustrated in all figures of this article).Figure 1 shows an example of such a graph.Press articles, and especially newswire articles,are characterized by an important redundancy of re-lated events.
An important event2 is likely to betreated by several successive articles, which willgive more and more details and update some num-bers (mainly, tragedy casualty updates, as shown inFigure 2).
On the one hand, this redundancy is anissue since a system must not show duplicate infor-mation to the user; on the other hand, we show inthis article that it can also be of great help in theprocess of extracting temporal graphs.In what follows, we first review some of the re-lated work in Section 2.
Section 3 presents the anno-tation procedure and the resulting annotated corpus2Note that we do not focus intentionally on ?important?events.
However, the fact is that minor events do hardly leadto dense temporal graphs.958Figure 1: Example of ?temporal graph?
: around thePope?s death.
The associated text is the title of each ar-ticle.
Relations that can be obtained by transitivity havebeen hidden for clarity?s sake.used for developing, learning and evaluating the sys-tem.
The simple modules used to predict the sameevent, continuation and, possibly, reaction relationsare described in Section 4, and results are given inSection 5.We also propose an end-user application to thiswork.
When a user reads an article, the system willthen be able to provide her with a thread of eventshaving occurred before or after, helping her to con-textualize the information she is reading.
This appli-cation is described in Section 6.2 Related workThe identification of temporal relations betweenevents in texts has been the focus of increasing atten-tion because of its importance in NLP applicationssuch as information extraction, question-answeringor summarization.
The evaluation campaigns Tem-pEval 2007 (Verhagen et al 2007) and TempEval2010 (Verhagen et al 2010) focused on temporalrelation identification, mainly on temporal relationsbetween events and times in the same sentence orin consecutive sentences and between events and thecreation time of documents.
In this context, the goalis to identify the type of a temporal relation which isFigure 2: Example of ?temporal graph?
: Madrid attacks,with many updates of the initial information.
Note thatarticles gathered in this main pool of articles can be pos-terior to the continuations and reactions to the describedevent.known to be present.
Systems having the best results(accuracy about 0.6) use statistical learning basedon temporal features (modality, tense, aspect, etc.
)(Mani et al 2006; Chambers et al 2007).
More re-cently, Mirroshandel and Ghassem-Sani (2012) pro-posed a new method for temporal relation extractionby using a bootstrapping method on annotated dataand have a better accuracy than state-of-the-art sys-tems.
Their method is based on the assumption thatsimilar event pairs in topically related documentsare likely to have the same temporal relations.
Forthis work, the authors had already some collectionsof topically related documents and did not need toidentify them.In the 2012 i2b2 challenge (i2b, 2012), theproblem was not only to identify the type of tempo-ral relations, but also to decide whether a temporalrelation existed or not between two elements, eitherclinical concepts or temporal expressions.
But, asin TempEval, the temporal analysis were only to beperformed within a single document.Other works focus on event ordering.
For ex-ample, Fujiki et al(2003) and Talukdar et al(2012)proposed methods for automatic acquisition of eventsequences from texts.
They did not use tempo-ral information present in texts and extracted se-quences of events (e.g.
arrest/escape) from sen-tences which were already arranged in chronologi-959cal order.
Chambers and Jurafsky (2008) proposeda method to learn narrative chains of events relatedto a protagonist in a single document.
The firststep consists in detecting narrative relations betweenevents sharing coreferring arguments.
Then, a tem-poral classifier orders partially the connected eventswith the before relation.Concerning the identification of the reaction re-lation, to our knowledge, there is no work on thedetection of reaction between several documents.Pouliquen et al(2007), Krestel et al(2008) andBalahur et al(2009) focused on the identification ofreported speech or opinions in quotations in a docu-ment, but not on the identification of an event whichis the source of a reaction and which can possibly bein another document.As we can see, all these approaches, as well astraditional information extraction approaches, leanon information contained by a single document, andconsider an event as a word or a phrase.
However,Ahmed et al(2011) proposed a framework to grouptemporally and tocipally related news articles intosame story clusters in order to reveal the temporalevolution of stories.
But in these topically relatedclusters of documents, no temporal relation is de-tected between articles or events except chronologi-cal order.
On this point of view, our task is closerto what is done in multidocument summarization,where a system has to detect redundant excerptsfrom various texts on the same topic and presentresults in a relevant chronological order.
For ex-ample, Barzilay et al(2002) propose a system formultidocument summarization from newswire arti-cles describing the same event.
First, similar textunits from different documents are identified usingstatistical techniques and shallow text analysis andgrouped into thematic clusters.
Then, in each theme,sentences which are selected as part of the summaryare ordered using the publication date of the first oc-currence of events to order sentences.3 ResourcesWe built an annotated collection of English articles,taken from newswire texts provided by the Frenchnews agency (AFP), spreading over the period 2004-2012.
The entire collection contains about 1.5 mil-lion articles.
Each document is an XML file contain-ing a title, a creation time (DCT), a set of keywordsand textual content split into paragraphs.3.1 Selection of Article PairsPairs of documents were automatically selected ac-cording to the following constraints:?
The article describes an event.
Articles suchas timelines, fact files, agendas or summarieswere discarded (all these kinds of articles weretagged by specific keywords, making the filter-ing easy).?
The distance between the two DCTs does notexceed 7 days.?
There are at least 2 words in common in the setof keywords and/or 2 proper nouns in commonin the first paragraph of each article.These last two restrictions are important, butnecessary, in order to give annotators a chance tofind some related articles.
Pure random selection ofpairs over a collection of 1.5 million articles wouldbe impractical.We assume that the title and the first paragraphdescribe the event associated with the document.This is a realistic hypothesis, since the basic rulesof journalism impose that the first sentence shouldsummarize the event by informing on the ?5 Ws?
(What, Who, When, Where, Why).
However, readingmore than the first paragraph is sometimes necessaryto determine whether a relation exists between twoevents.3.2 Relation AnnotationTwo annotators were asked to attribute the followingrelations between each pair of articles presented bythe annotation interface system.In a first annotation round, 7 types of relationswere annotated:?
Three relations concerning cases where the twoarticles relate the same event or an update:?
number update, when a document is anupdate of numerical data (see top of Fig-ure 5),?
form update, when the second documentbrings only minor corrections,960Figure 3: Examples of relation continuation between twodocuments.Figure 4: Examples of relation continuation-reaction be-tween two documents.?
details, when the second document givesmore details about the events (see bottomof Figure 5).?
development of same story, when the two docu-ments relate two events which are included intoa third one;?
continuation, when an event is the continuationor the consequence of a previous one.
Figure 3shows two examples of such a relation.
It isimportant to make clear that a continuation re-lation is more than a simple thematic relation,it implies a natural prolongation between twoevents.
For example, two sport events of thesame Olympic Games, or two different attacksin Iraq, shall not be linked together unless a di-rect link between both is specified in the arti-cles.?
reaction, a subset of continuation, when a doc-ument relates the reaction of someone to an-other event, as illustrated by the example inFigure 4.Figure 5: Example of relations same-event between twodocuments: update on casualties (top) or details (bottom).?
nil, when no relation can be identified betweenthe two documents.The inter-annotator agreement was calculatedwith Cohen?s Kappa measure (Cohen, 1960) across150 pairs: ?
?
0.68.
The agreement was low forthe first 4 types of relations mostly because the dif-ference between relations was not clear enough.
Wetherefore aggregated the number update, form up-date and details relations into a more generic andconsensual same-event relation (see Figure 5).
Wealso discarded the development of same story rela-tion, leaving only same-event, continuation and re-action.Annotation guidelines were modified and a sec-ond annotation round was carried out: only thesame-event, continuation, reaction and nil relationswere annotated.
Inter-annotator agreement across150 pairs was then ?
?
0.83, which is a good agree-ment.3.3 Relation Set ExtensionThis manual annotation would have led to verysparse temporal graphs without the two followingadditional processes:?
When the annotator attributed a ?non-nil?
rela-tion to a pair of documents, the annotation sys-tem suggested other pairs to annotate aroundthe concerned articles.?
Same-event and continuation relations are tran-sitive: if A same-event B and B same-eventC, then A same-event C (and respectively for961Pair number Learning EvaluationSame event 762 458 304Continuation 1134 748 386Reaction 182 123 59Nil 918 614 304TOTAL 2996 1943 1053Table 1: Characteristics of the corpus.continuation).
Then, when the annotation wasdone, a transitive closure was performed on theentire graph, in order to get more relations withlow effort (and to detect and correct some in-consistencies in the annotations).Finally, almost 3,000 relations were annotated.2{3 of the annotated pairs were used for developmentand learning phases, while 1{3 were kept for evalua-tion purpose (cf.
Table 1).4 Building Temporal GraphsAs we explained in the introduction, the main pur-pose of this paper is to show that it is possible to ex-tract temporal graphs of events from multiple docu-ments in a news corpus.
This is achieved with thehelp of redundancy of information in this corpus.Therefore, we will use a cascade of classifiers andother modules, each of them using the relations de-duced by the previous one.
All modules predict arelation between two documents (i.e., two events).We did not focus on complex algorithms orclassifiers for tuning our results, and most of our fea-tures are very simple.
The idea here is to show thatgood results can be obtained in this original and use-ful task.
The process can be separated into 3 mainstages, illustrated in Figure 6:A. Filtering out pairs that have no relation at all, i.e.classifying between nil and non-nil relations;B.
Classifying between same-event and continua-tion relations;C. Extracting reactions from the set of continuationrelations.All described classifiers use SMO (Platt, 1998),the SVM optimization implemented into Weka (Hallet al 2009), with logistic models fitting (option ?-M?).
With this option, the confidence score of eachFigure 6: A 3-step classification.prediction can be used, while SMO alone provides aconstant probability for all instances.From now on, when considering a pair of doc-uments, we will refer to the older document as doc-ument 1, and to the more recent one as document 2.The relations found between documents will be rep-resented by a directed graph where documents arevertices and relations are edges.4.1 A. Nils versus non-nilsWe first aim at separating nil relations (no relationbetween two events) from other relations.
This stepis achieved by two successive classifiers: the firstone (A.1) uses mainly similarity measures betweendocuments, while the second one (A.2) uses the re-lations obtained by the first one.4.1.1 Step A.1: Nil classifier, level 1Features provided to the SMO classifier at thisfirst step are based on 3 different similarity measuresapplied to pairs of titles, pairs of first sentences,and pairs of entire documents: cosine similarity (asimplemented by Lucene search engine3), inclusionsimilarity (rate of words from element 1 present inelement 2) and overlap similarity (number of wordspresent in both elements).
This classifier is thereforebased on only 9 features.4.1.2 Step A.2: Nil classifier, level 2Finding relations on a document implies that thedescribed event is important enough to be addressedby several articles (same-event) or to have conse-quences (continuation).
Consequently, if we findsuch relations concerning a document, we are morelikely to find more of them, because this means that3http://lucene.apache.org962the document has some importance.
A typical exam-ple is shown in Figure 7, where an event describedby several documents (on the left) has many contin-uations.
For this reason, we build a second classifierA.2 using additional features related to the relationsfound at step A.1:?
Number of non-nil edges, incoming to or out-going from document 1 (2 features); the sum ofboth numbers (1 extra feature);?
Number of non-nil edges, incoming to or out-going from document 2 (2 features); the sum ofboth numbers (1 extra feature);?
Number of non-nil edges found involving oneof the two documents (i.e., the sum of all edgesdescribed above ?
1 feature).These figures have been computed on trainingset for training, and on result of step A.1 classifierfor testing.
This new information will basically helpthe classifier to be more optimistic toward non-nilrelations for documents having already non-nil rela-tions.4.2 B. Same-event versus ContinuationWe are now working only with non-nil relations(even if some relations may switch between nil andnon-nil during the transitive closure).4.2.1 Step B.1: Relation classifier, level 1Distinction between same-event and continua-tion is made by the following sets of features:?
Date features:?
Difference between the two document cre-ation times (DCTs): difference in days, inhours, in minutes (3 features);?
Whether the creation time of doc.
1 ismentioned in doc.
2.
For this purpose,we use the date normalization system de-scribed in Kessler et al(2012).?
Cosine similarity between the first sen-tence of doc.
1 and sentences of doc.
2containing the DCT of doc.
1.?
Cosine similarity between the first sen-tence of doc.
1 and the most similar sen-tence of doc.
2.Figure 7: An example of highly-connected subgraph,corresponding to the development of an important story.Same events are grouped by cliques (see Section 4.2.3)and some redundant relations are not shown for clarity?ssake.These last three features come from theidea that a continuation relation can bemade explicit in text by mentioning thefirst event in the second document.?
Temporal features: whether words introducingtemporal relations occur in document 1 or doc-ument 2.
These manually-collected words canbe prepositions (after, before, etc.)
or verbs(follow, confirm, prove, etc.).?
Reaction features: whether verbs introducingreactions occur in document 1 or document 2(25 manually-collected verbs as approve, ac-cept, welcome, vow, etc.).?
Opinion features: whether opinion words occurin document 1 or document 2.
The list of opin-ion words comes from the MPQA subjectivitylexicon (Wilson et al 2005).Only same-event relations classified with morethan 90% confidence by the classifier are kept, inorder to ensure a high precision (recall will be im-proved at next step).
This threshold has been set upon development set.4.2.2 Step B.2: Relation classifier, level 2As for step A.2, a second classifier is im-plemented, using the results of step B.1 with thesame manner as A.2 uses A.1 (collecting numbersof same-event and continuation relations that havebeen found by the previous classifier).4.2.3 Steps B.3 and B.4: Transitive closure byvoteAs already stated, same-event and continuationrelations are transitive.
Same-event is also symmet-ric (A same-event B ?
B same-event A).
In the963graph formed by documents (vertices) and relations(edges), it is then possible to find all cliques, i.e.
sub-sets of vertices such that every two vertices in thesubset are connected by a same-event relation, as il-lustrated by Figure 7.This step does not involve any learning phase.Starting from the result of last step, we find all same-event cliques in the graph by using the Bron andKerbosch (1973) algorithm.
The transitive closureprocess is then illustrated by Figure 8.
If the classi-fier proposed a relation between some documents ofa clique and some other documents (as D1, D2 andD3), then a vote is necessary:?
If the document is linked to half or more of theclique, then all missing links are created (Fig-ure 8.a);?
Otherwise, the document is entirely discon-nected from the clique (Figure 8.b).This vote is done for same-event and contin-uation relations (resp.
steps B.3 and B.4).
Onlycliques containing at least 3 nodes are used.
A draw-back of this voting procedure is that the final resultmay not be independent of the voting order, in somecases.
However, it is assured that the result is con-sistent, i.e.
that no document will sit in two differentcliques, or that two documents from the same cliquewill not have two different relations toward a thirddocument.Note that this vote leads to improvements onlyif the precision of the initial classifier is sufficientlygood.
As we will see in Section 5.2, this is the casein our situation, but one must keep in mind that avote leaning on too imprecise material would lead toeven worse results.
Some experiments on the devel-opment set show us that at least 70% precision wasnecessary.
Another way to ensure robustness of thevote would be to apply the transitive closure onlyon bigger cliques (e.g., containing more than 3 or4 nodes).4.3 C. Continuation versus ReactionThe approach for reaction extraction is different.
Wefirst try to determine which documents describe re-actions, regardless of which event it is a reaction to.In the training set, all documents having at least oneincoming reaction edge are considered as reaction?
?Figure 8: Vote for same-event transitive closure.
At thetop (a.
), four nodes from the 5-node clique are linked todocument D1, which is enough to add D1 to the clique.At the bottom (b.
), only two nodes from the clique arelinked to documents D2 and D3, which is not enough toadd them into the clique.
All edges from the clique to D2and D3 are then deleted.documents, all others are not.
This distinction isthen learned with the same model and features asfor step B.1 (Section 4.2.1).Once reaction documents have been selected,the question is how to decide to which other doc-ument(s) it must be linked.
For example, in Fig-ure 1, ?Queen Elizabeth expresses deep sorrow?
isa reaction to pope?s death, not to other documents inthe temporal thread (for example, not to other reac-tions or to ?Pope in serious condition?).
We did notmanage to build any classifier leading to satisfyingresults at this point.
We then proposed the two fol-lowing basic heuristics, applied on all continuationrelations found after step B:?
A reaction reacts to only one event.?
A reaction reacts to an important event.
Then,among all continuation edges incoming tothe reaction document, we choose the biggestsame-event clique and create reaction edgesinstead of continuations.
If there is noclique (only single nodes) or several same-sizecliques, all of them are tagged as reactions.This module is called step C.1.
Finally, a transitiveclosure is performed for reactions (C.2).964Relation Precision Recall F1NIL 0.754 0.821 0.786same-event 0.832 0.812 0.822continuation 0.736 0.696 0.715?
reaction 0.273 0.077 0.120Table 2: Results obtained by the baseline system.
Con-tinuation scores do not consider reactions, only the lastrow makes the distinction.5 Results5.1 BaselineAs a baseline, we propose a single classifier deter-mining all classes at once, based on the same SMOclassifier with the exact same parameters and allsimilarity-based features (on titles, first sentencesand entire documents) described in Section 4.1.1.Table 2 shows results for this baseline.
Unsur-prisingly, same-event relations are quite well clas-sified by this baseline, since similarity is the majorclue for this class.
Continuation is much lower andonly 3 reactions are well detected.5.2 System EvaluationResults for all successive steps described in previoussection are shown in Figure 3.
The final result of theentire system is the last one.
The first observationis that redundancy-based steps improve performancein a significant manner:?
Classifiers A.2 and B.2, using the number ofincoming or outgoing edges found at previoussteps, lead to very significant improvement.?
Among transitivity closure algorithms (B.3,B.4, C.2), only same-event transitivity B.3leads to significant improvement.
Furthermore,as we already noticed, these algorithms must beused only when a good precision is guaranteedat previous step.
Otherwise, there is a risk ofinferring mostly bad relations.
This is why webiased classifier at step B.1 towards precision.Finally, if this condition on precision is true,transitivity closure is a robust way to get newrelations for free.Results also tell that classification of relationssame-event and continuation is encouraging.
Reac-tion level gets a fair precision but a bad recall.
ThisStep Relation Precision Recall F1A.
NIL vs non-NIL classifierA.1 NIL 0.764 0.815 0.788non-NIL 0.921 0.896 0.910A.2 NIL 0.907 0.811 0.857???
non-NIL 0.925 0.966 0.945B.
Same-event vs continuation classifierB.1 NIL 0.907 0.811 0.857same-event 0.870 0.553 0.676continuation 0.664 0.867 0.752B.2 NIL 0.947 0.831 0.885???
same-event 0.894 0.724 0.800continuation 0.744 0.911 0.819B.3 NIL 0.884 0.831 0.857??
same-event 0.943 0.819 0.877continuation 0.797 0.906 0.848B.4 NIL 0.890 0.831 0.860?
same-event 0.943 0.819 0.877continuation 0.798 0.911 0.851C.
Reaction vs continuationC.1 NIL 0.890 0.831 0.860C.2 same-event 0.943 0.819 0.877continuation 0.798 0.911 0.851?
reaction 0.778 0.359 0.491Table 3: Results obtained at each step of the classifica-tion process.
The significance of the improvement wrtprevious step (when relevant) is indicated by the Studentt-test (?
: non significant; ??
: p ?
0.05 (significant); ???
:p ?
0.01 (highly significant)).
Steps C.1 and C.2 areaggregated, since their results are exactly the same.is not catastrophic since most of the missed reactionsare tagged as continuation, which is still true (only10% of the reaction relations are mistagged as same-event).
However, there is big room for improvementon this point.6 ApplicationAs we showed in previous section, results for classi-fication of same-event and continuation relations be-tween documents are good enough to use this systemin an application that builds ?event threads?
aroundan input document.
The use case is the following:?
The reader reads an article (let?s say, about thedeath of John Paul II, article published on Feb.4th, 2005 (UT) ?
see Figure 1).?
A link in the page suggests the user to visualizethe event thread around this article.965Figure 9: An example of temporal thread obtained on the death of John Paul II for user visualization (see correspondingrelation graph in Figure 1).?
All articles within a period of 7 days aroundthe event, sharing at least two keywords withthe current document, are collected.
All pairsare given to the system4.?
When same-event cliques are found, only thelongest article (often, the most recent one) ofeach clique is presented to the user.
However,the date and time presented to the user are thoseof the first article relating the event.?
This leads to a graph with only continuationand reaction relations.
Edges are ?cleaned?
sothat a unique thread is visible: relations that canbe obtained by transitivity are removed, edgesbetween two documents are kept only if no doc-ument can be inserted in-between.?
Nodes are presented in chronological order.The user can visualize and navigate throughthis graph (the event thread shows only titlesbut full articles can be accessed by clicking onthe node).?
When found, reactions are isolated from themain thread.?
Such a temporal thread is potentially infinite.
Ifthe user navigates through the end of the 7-daywindow, the system must be run again on thenext time span.4In case of very important events where ?all pairs?
would betoo much, the temporal window is restrained.
However, there isno real time performance issue in this system.Figure 9 presents the result of this process onthe partial temporal graph shown in Figure 1.7 ConclusionThis article presents a task of multidocument tem-poral graph building.
We make the assumption thateach news article (after filtering) relates an event,and we present a system extracting relations be-tween articles.
This system uses simple features andalgorithms but takes advantage of the important re-dundancy of information in a news corpus, by in-corporating redundancy information in a cascade ofclassifiers, and by using transitivity of relations toinfer new links.Finally, we present an application presenting?event threads?
to the user, in order to contextual-ize the information and recomposing the story of anevent.Now that the task is well defined and that en-couraging results have been obtained, we envisage toenrich classifiers by more fine-grained temporal andlexical information, such as narrative chains (Cham-bers and Jurafsky, 2008) for continuation relationor event clustering (Barzilay et al 2002) for same-event relation.
There is no doubt that reaction de-tection can be improved a lot, by going beyond sim-ple lexical features and discovering specific patterns.We also intend to adapt the described system to otherlanguages than English.966ReferencesA.
Ahmed, Q. Ho, J. Eisenstein, E.P.
Xing, A.J.
Smola,and C.H.
Teo.
2011.
Unified Analysis of StreamingNews.
In Proceedings of WWW, Hyderabad, India.A.
Balahur, R. Steinberger, E. van der Goot,B.
Pouliquen, and M. Kabadjov.
2009.
OpinionMining on Newspaper Quotations.
In Proceedingsof International Joint Conference on Web Intelli-gence and Intelligent Agent Technologies, Milano,Italy.R.
Barzilay, N. Elhadad, and K.R.
McKeown.
2002.
In-ferring Strategies for Sentence Ordering in Multi-document News Summarization.
Journal of Artifi-cial Intelligence Research, 17:35?55.C.
Bron and J. Kerbosch.
1973.
Algorithm 457: findingall cliques of an undirected graph.
Communicationsof the ACM, 16(9):575?577.N.
Chambers and D. Jurafsky.
2008.
UnsupervisedLearning of Narrative Event Chains.
In Proceedingsof the 46th Annual Meeting of the ACL, Columbus,USA.N.
Chambers, S. Wang, and D. Jurafsky.
2007.
Classify-ing temporal relations between events.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, Prague,Czech Republic, June.J.
Cohen.
1960.
A Coefficient of Agreement for Nom-inal Scales.
Educational and Psychological Mea-surement, 43(6):551?558.T.
Fujiki, H. Nanba, and M. Okumura.
2003.
AutomaticAcquisition of Script Knowledge from a Text Col-lection.
In Proceedings of EACL, Budapest, hun-gary.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The WEKA DataMining Software: An Update.
SIGKDD Explo-rations, 11(1).2012.
Proceedings of i2b2/VA Shared-Tasks and Work-shop on Challenges in Natural Language Process-ing for Clinical Data, Chicago, USA.R.
Kessler, X. Tannier, C. Hage`ge, V. Moriceau, andA.
Bittar.
2012.
Finding Salient Dates for Build-ing Thematic Timelines.
In Proceedings of the 50thAnnual Meeting of the ACL, Jeju Island, Republic ofKorea.R.
Krestel, S. Bergler, and R. Witte.
2008.
Minding theSource: Automatic Tagging of Reported Speech inNewspaper Articles.
In Proceedings of LREC, Mar-rakech, Morocco.I.
Mani, M. Verhagen, B. Wellner, C. Lee, and J. Puste-jovsky.
2006.
Machine learning of temporal rela-tions.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the ACL, Sydney, Australia.S.A.
Mirroshandel and G. Ghassem-Sani.
2012.
TowardsUnsupervised Learning of Temporal Relations be-tween Events.
In Journal of Artificial IntelligenceResearch, volume 45.J.C.
Platt, 1998.
Advances in Kernel Methods - SupportVector Learning, chapter Fast Training of SupportVector Machines Using Sequential Minimal Opti-mization.
MIT Press.B.
Pouliquen, R. Steinberger, and C. Best.
2007.
Auto-matic Detection of Quotations in Multilingual News.In Proceedings of RANLP, Borovets, Bulgaria.P.P.
Talukdar, D. Wijaya, and T. Mitchell.
2012.
Ac-quiring Temporal Constraints between Relations.
InProceedings of the 21st ACM international confer-ence on Information and knowledge management,Hawaii.M.
Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,G.
Katz, and J. Pustejovsky.
2007.
SemEval-2007 -15: TempEval Temporal Relation Identification.
InProceedings of SemEval workshop at ACL, Prague,Czech Republic.M.
Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky.2010.
SemEval-2010 - 13: TempEval-2.
In Pro-ceedings of SemEval workshop at ACL, Uppsala,Sweden.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing Contextual Polarity in Phrase-Level SentimentAnalysis.
In Proceedings of HLT-EMNLP.967
