Comparing Several Aspects of Human-Computer andHuman-Human DialoguesChristine Doran, John Aberdeen, Laurie Damianos and Lynette HirschmanThe MITRE Corporation202 Burlington RoadBedford, MA 01730 USA{cdoran,aberdeen,laurie,lynette}@mitre.orgAbstractWhile researchers have many intuitionsabout the differences between human-computer and human-human interac-tions, most of these have not previouslybeen subject to empirical scrutiny.
Thiswork presents some initial experimentsin this direction, with the ultimate goalbeing to use what we learn to improvecomputer dialogue systems.
Workingwith data from the air travel domain,we identified a number of striking dif-ferences between the human-human andhuman-computer interactions.1 IntroductionIn our initial experiments comparing human-human (HH) and human-computer (HC) inter-action we have annotated dialogues from the airtravel domain with several sets of tags: dialogueact, initiative and unsolicited information.
Ouraim is to begin an empirical exploration of howthese aspects of the dialogue shed light on dif-ferences between HH and HC interactions.
Wefound striking differences between the human-human and human-computer interactions.
Withmany of the issues we examine here, researchershave voiced strong intuitions about the differencesbetween HH and HC communication, but these in-tuitions have not previously been subject to em-pirical scrutiny.Why do we want to compare HH and HC in-teractions?
We believe that an examination ofthe differences between HH and HC dialogues canhelp those working on the HC interactions to im-prove their systems.
This will not necessarilymean making the HC interactions ?more like?
HHinteractions; rather, we believe that such analy-sis can give us insights about the appropriatenessand success of various communicative approachesin different settings.
We are also interested inquantifying what it means for a dialogue to be?mixed-initiative?.
There is liberal use of thisterm in work on human-computer dialogues, butthere does not seem to be a clear sense of what itreally means and how to define it.This work is being done in the context of theCommunicator Travel task.
Communicator is aDARPA-funded program involving major indus-try and academic sites, established to provide thenext generation of intelligent conversational inter-faces to distributed information.
The goal of theprogram is to move beyond the current systeminitiated voice menu style of dialogue interactiontowards a more flexible strategy of shared con-versational control between human and system.Work up to the fall of 2000 concentrated on thetravel task, but groups are now moving into otherdomains with their dialogue systems.2 Our DataWe have tagged 20 HH dialogues and 40HC dialogues; the HH dialogues consist of25,208 words and 1742 turns, and the HCdialogues consist of 8313 words and 957turns.
The HH dialogues are from the SRIATIS collection (available at www.ai.sri.com/?communic/amex/amex.html) and the CMU Traveldata (www.speech.cs.cmu.edu).
The HC dia-logues are from a subset of the systems partici-pating in the DARPA Communicator data collec-tion which was run in June and July of 2000.1Subjects were selected by the U.S. National In-stitute of Standards and Technology (NIST) andeach subject was paid to call all of the 9 partic-ipating sites with either a fixed or open travelitinerary.
The fixed itineraries were intended tobalance flight types (one-way, round-trip, multi-leg) and destinations (large US airport, small for-1Ideally, of course, we would want HH dialoguescollected under similar circumstances to the HC dia-logues.
However, no such data are available and col-lecting new HH data is not currently a feasible en-deavor for us.eign airport, etc.
).2 We made no attempt to bal-ance the subset of data we analyzed; we merelyselected 10 random users.
We then identified foursites whose systems we expected to differ quitewidely (based on researchers?
descriptions of theirsystems), and extracted the 10 users?
dialogues foreach site.
Each dialogue was tagged with threesets of annotations?for dialogue acts, for initia-tive and for provision of unsolicited informationby users (HC only).
Sections 2.1, 2.2 and 3.5 de-scribe these in more detail.
The passage belowis excerpted from one of the human-computer in-teractions, using the transcribed versions of theuser?s turns.3User said: i would like to fly from miamiflorida to sydney australia on wednesdayoctober eleventh pleaseSystem said: okay, from Miami to Sydney onWednesday October 11.
Can you provide theapproximate departure time or airline?User said: [um] i prefer american airlinesand i can leave anytimeSystem said: There are no nonstop flights.I have a flight connecting in Los Angelesleaving at 7:15 a.m. a flight connectingin Los Angeles leaving at 5:20 p.m. and aflight connecting in Los Angeles leavingat 7:30 p.m. Do you like any of these?User said: yes how about the morning oneSystem said: American flight 912 connectingto American flight 7362 in Los Angeles hasbeen added to your itinerary.
you departMiami at 7:15 am and arrive in Sydney at9:40 pm the following day.
Where would youlike to go next?2.1 Dialogue Act TaggingWe have done two rounds of tagging.
Our start-ing point was a 20-tag subset of the CSTAR Con-sortium tags (Consortium, 1999).
We found theCSTAR tag set to be the most suitable for ourtask of the tagging schemes we considered, basedon information provided in the survey of dialogueact annotation schemes assembled by the MATEproject (Klein et al, 1998).
We picked the CSTARtags because they had been designed for task-2It is important to note that the fixed itinerarieswere assigned, i.e.
not real trips, and it was also evi-dent from the transcripts that few if any of the openitineraries represented real trips.
This may well havehad some impact on the data that were collected.3Both human-human and human-system dialoguetranscripts are reproduced with their native format-ting, i.e.
all caps, no punctuation, etc.
The systemside contains punctuation resulting from the genera-tion process.oriented dialogues, the tag categories were suffi-ciently clear and simple that we believed we wouldbe able to tag the data reliably and, finally, thecategories captured the sorts of distinctions we be-lieved would be relevant.
We rejected the DAMSLtag set (Core and Allen, 1997; Core et al, 1999)on the grounds that is was too sophisticated forour purposes, covering many aspects of dialoguestructure that were not necessarily relevant for ourtask such as intentionality, grounding and contexttracking.
In addition, the interannotator agree-ment levels reported for this scheme are quite low.Some of the other tag sets we considered were(Carletta et al, 1995; Nakatani et al, 1995; vanVark et al, 1996; Di Eugenio et al, 1998; Jurafskyet al, 1997).In collaboration with AT&T, we arrived at aset of changes to our tag set that would makeit compatible with their efforts to tag system ut-terances automatically (Walker and Passonneau,2001), in the hopes of being able to share re-sults with them more easily.
We added a sit-uation/conversation/task distinction to a num-ber of our tags (e.g.
give-information splitinto give-task-info, give-situation-info andgive-conversation-info).
We also added anot-understand tag and collapsed some orig-inal tags into super-categories.
Our revised tagset had 26 tags, and two people (one who hadalso done the first round of tagging) tagged thesame data set.
The situation/conversation/taskdistinction turned out to be extremely difficult forthe taggers to make; we believe that revisions tothe tagging guidelines could lead to some improve-ment on this front, but without enumerating thekinds of utterances which fall into each category,this will remain a difficult task.We tagged each utterance that contained somespeech, i.e.
was not composed entirely of non-speech annotation like *pause* or [click], andwe split turns4 into utterances using guidelinesthat had been developed internally for anotherpurpose.
Utterances on this definition wereroughly clause-sized units, and possibly fragmen-tary.5 This meant that there were often multi-ple dialogue acts (DAs) per turn, and where therewere multiple sequential DAs of the same type, wecollapsed them under a single tag on the assump-tion that they were combining to ?perform?
thatDA.
We initially split some of the CSTAR tags4Chunk of text labelled with either User said orExpert said.
It was possible for a single speaker to havemore than one sequential turn, i.e.
turn 6= speakerchange.5In hindsight, it would have been preferable to seg-ment the dialogues in a separate step.into implicit and explicit versions, but foundthat the implicit cases were so hard to identifythat we were not using those tags, and they weredropped from the tag set.Tables 1 and 2 show roughly parallel sub-dialogues from the HH and HC data.6 Each turnis tagged with its DA, and the first expert turnin Table 2 shows multiple DAs within a turn, agive-information followed by an offer.Expert:WHAT TIME DO [req-task-info]YOU NEED TO DEPARTUser:AS SOON AS [give-task-info]POSSIBLE AFTER FIVE P.M.Expert:THE FIRST FLIGHT [give-task-info]AFTER FIVE P.M. ON THAT DATE ISAT FIVE THIRTY FIVE P.M. ARRIVINGIN CHICAGO AT SIX OH SIX P.M.ON U.S. AIRUser: IS THAT O?HARE [req-task-info]Table 1: DA tagging in an HH ExchangeExpert: i have an American [give-task-info]Airlines flight departing Seattle attwelve fifty five p.m., arrives Tokyoat three p.m. the next day.Is that OK?
[offer]User: yes I?ll take it [accept]Expert: Will you return to seattle[req-task-info]from tokyo?User: what airport [req-task-info]Expert: Will you return to seattle[req-task-info]from tokyo?Table 2: DA tagging in an HC ExchangeWith our first tag set, our Kappa score forinterannotator agreement on these dialogues is0.90 (with two annotators).
Not surprisingly, ourKappa score on the second, more complex tag set(cf.
Table 10 for a list of the tags) was lower,0.71 (0.74 on the HC data and 0.66 on the HHdata).
Both scores are in line with scores re-ported in similar tagging tasks (Klein et al, 1998):0.56 for DAMSL (overall average), 0.83 for Map-task (experienced coders), 0.8-0.84 for Switch-board DAMSL and 0.83 for VerbMobil.
The dropin score between our two tag sets emphasizes anissue which we continue to wrestle with?the trade-off between tag set complexity and tagging accu-racy.
At what point is it more useful to have re-6Throughout the paper, we will use expert to referto either the human or the computer travel agent, sys-tem to refer exclusively to the computer travel agent,and user to refer to the travelers.liable results from an impoverished tag set thanresults of questionable value from a sophisticatedtag set?2.2 Initiative TaggingThere is not a clearly agreed upon definition of ini-tiative in the literature on dialogue analysis (butsee e.g., (Chu-Carroll and Brown, 1998; Jordanand Di Eugenio, 1997; Flammia and Zue, 1997)),despite the fact the terms initiative and mixed-initiative are widely used.
Intuitively, it seemsthat control rests with the participant who is mov-ing a conversation ahead at a given point, or se-lecting new topics for conversation.After experimenting with several tagging meth-ods, we concluded that the approach presentedin Walker and Whittaker (1990) adopted from(Whittaker and Stenton, 1988) best captured theaspects of the dialogue we were interested in and,as with the DAs, could be tagged reliably on ourdata.Each turn is tagged with which participant hascontrol at the end of that turn, based on the utter-ance type.
Again, we did not tag turns composedentirely of non-speech annotation, and we also ex-cluded conventional openings and closings, follow-ing Walker and Whittaker.
Below, we list therules for tagging each utterance type; a promptis an utterance ?which did not express proposi-tional content, such as Yeah, Okay, Uh-huh, .
.
.
.?
(Op cit, p. 3) The classification refers to the il-locutionary force of the item, rather than to itsparticular syntactic form.Assertion: speaker has initiative unless it is aresponse to a question or command7Question: speaker has initiative unless it is a re-sponse to a question or commandCommand: speaker has initiativePrompt: hearer has initiativeTables 3 and 4 show the same passages usedabove, but this time tagged for initiative.
To givea sense of how the tagging rules are applied, let usstep through the HC example (Table 4).
Turn (1)is assigned expert-initiative, because it is anassertion which is not a response to any precedingquestion or command.
Turn (2) is still expert-initiative, because it is an answer to the ques-tion Is that OK?.
The third turn is a questionand expert-initiative, but turn (4) is user-initiative because it is a question that is not aresponse to the previous question.
The system7Italics show our modification to the rule.does not address the user?s question, but ratherrepeats its own question, so the final turn (5) isexpert-initiative.Expert:WHAT TIME DO YOU [exp-init]NEED TO DEPARTUser:AS SOON AS POSSIBLE [exp-init]AFTER FIVE P.M.Expert:THE FIRST FLIGHT AFTER [exp-init]FIVE P.M. ON THAT DATE IS ATFIVE THIRTY FIVE P.M.ARRIVING IN CHICAGO ATSIX OH SIX P.M. ON U.S. AIRUser:IS THAT O?HARE [user-init]Table 3: Initiative tagging in an HH Exchange(1)Expert: i have an American [exp-init]Airlines flight departing Seattle attwelve fifty five p.m. , arrives Tokyoat three p.m. the next day.Is that OK?
(2)User: yes I?ll take it [exp-init](3)Expert: Will you return to seattle [exp-init]from tokyo?
(4)User: what airport [user-init](5)Expert: Will you return to seattle [exp-init]from tokyo?Table 4: Initiative tagging in an HC ExchangeOur Kappa scores for interannotator agreementon the initiative tagging were somewhat lowerthan for DA tagging.
Here, ?=0.68.
In fact, ouragreement was rather high, at 87%, but becausethere were so few instances of user initiative inthe HC dialogues, our agreement would have tobe exceptional to be reflected in a higher Kappascore.
While we had believed this to be the easiertask, with quite clear guidelines and only a binarytagging choice, it in fact proved to be quite diffi-cult.
We still believe that this tag set can giveus useful insights into our data, but we wouldbe interested in attempting further revisions tothe tagging guidelines, particularly as regards thedefinition of an ?answer?, i.e.
when an answer isresponsive and when it is not.3 AnalysisWe found a number of interesting differences be-tween the HH and HC dialogues.
While we havenot yet been able to test our hypotheses aboutwhy these differences appear, we will discuss ourideas about them and what sorts of further workwe would like to do to subject those ideas to em-pirical validation.3.1 Initiative DistributionBased on researchers?
descriptions of their systems(i.e.
for the most part, ?highly mixed-initiative?
),we had expected to find some variance in the dis-tribution of initiative across systems.
As is ev-ident from Table 5, the HC systems do not dif-fer much from each other, but taken as whole,the dialogues differ dramatically from the HH di-alogues.
In the HH dialogues, users and expertshare the initiative relatively equitably, while inthe HC data the experts massively dominate intaking the initiative.
Here, we are simply countingthe number of turns tagged as user-initiative orexpert-initiative.8We also show turns to completion and overalluser satisfaction scores for each system as a refer-ence point.
User satisfaction was calculated fromfive questions asked of each user after each dia-logue.
The questions use a 5-point Likert scale.Turns to completion measures the total numberof on-task turns.
We found no significant corre-lations here, but cf.
Walker et al (2001) whichprovides more detailed analyses of the Communi-cator dialogues using user satisfaction and othermetrics, within the PARADISE framework.
It isworth noting, however, that the HC D has boththe highest percentage of expert initiative and thehighest satisfaction scores, so we should not con-clude that more initiative will necessarily lead tohappier users.% Exp % User Turns to UserInit Init Comp SatHC A 86.8% 13.2% 40.5 60.0%HC B 89.9% 10.1% 41.4 71.5%HC C 90.6% 9.4% 36.0 68.5%HC D 93.7% 6.3% 43.9 82.8%HH SRI 48.3% 51.7% N/A N/AHH CMU 54.0% 46.0% N/A N/ATable 5: Percentages of User and Expert Initiativein HH and HC DialoguesIn the HC dialogues, we also see a difference insuccess rate for user-initiative turns.
By our defi-nition, the user ?succeeds?
in taking the initiativein the dialogue if the system responds to the initia-tive on the first possible turn.
The rate of success8A cautionary note is warranted here.
We arenot suggesting that more user-initiative is intrinsicallypreferable; it may well turn out to be the case thata completely system-directed dialogue is more pleas-ant/efficient/etc.
Rather, we are seeking to quantifyand assess what it means to be ?mixed-initiative?
sothat we can better evaluate the role of initiative ineffective (task-oriented) dialogues.is the ratio of successful user-initiatives attemptsto total user-initiatives attempts.
There appearsto be a negative relationship between number ofinitiative attempts and their success rate.
SeeFigure 1, below.
HC D has a high success ratefor a relatively small number of user-initiative at-tempts.
HC A has many more occurrences of userinitiative, but does not incorporate them as well.Figure 1: User-Initiative and Success Rate perSystemThere is no determinable relationship betweenuser experience (i.e., the number of calls per sys-tems) and either the amount of user-initiative orthe success rate of user-initiative.We also looked at user-initiative with re-spect to dialogue act type.
Most user-initiativesare request-action (26%) and request-information (19%).
Request-informationdialogue acts (e.g., What cities do you know inTexas?, Are there any other flights?, Which air-port is that?)
are handled well by the systems(83% success rate) while request-action dia-logue acts (e.g., start over, scratch that, book thatflight) are not (48%).
Most of the user-initiativesthat are request-action dialogue acts are thestart over command (16% of the total user-initiatives).
Corrections to flight information pre-sented by the systems consist of 20% of the totaluser-initiatives.3.2 Overall VerbosityIn counting the number of words used, we findthat the computer experts are much more verbosethan their human users, and are relatively moreverbose than their human travel agent counter-parts.
In the HH dialogues, experts average 10.1words/turn, while users average 7.2.
In the HC di-alogues on average, system have from 16.65-33.1words/turn vs. the users?
2.8-4.8 words/turn.
Fig-ure 2 shows these differences for each of the foursystems and for the combined HH data.Figure 2: Words per turn for users and experts inthe HH and HC dialogues3.2.1 Short vs. Long ConfirmationsOne DA which is a basic conversational tool andtherefore an interesting candidate for analysis isthe use of confirmations.
Instances of short con-firmation, typically back-channel utterances suchas okay and uh huh were tagged as acknowl-edge, while instances of long confirmation, aswhen one participant explicitly repeats somethingthat the other participant has said, were taggedas verify-X, where X=conversation-action,task-information and task-action, This tag-ging allows us to easily calculate the distributionof short and long confirmations.Overall we found in the HC dialogues a ratherdifferent confirmation profile from the HH dia-logues.
In the HC dialogues, the systems use bothtypes of confirmation far more than the users do(246 total system, 8 total user).
Moreover, sys-tems use long confirmation about five times moreoften (210 vs. 36) than they use short confirma-tion.
In contrast, the experts in the HH dialoguesuse somewhat more confirmations than users (247vs.
173), but both parties use far more short thanlong confirmations (340 vs. 80), just the reverseof the HC situation.
This difference partially ac-counts for the total word count differences we sawin the previous section.
Tables 6 and 7 show thebreakdowns in these numbers for each system andfor the two sets of HH data, and begin to quantifythe striking contrasts between human and com-puter confirmation strategies.3.3 Number of Dialogue ActsAnother observation is that the computer expertsappear to be trying to do more.
They have sig-nificantly more DAs per turn than do their hu-man users, whereas in the HH dialogues, the twoparticipants have nearly the same number of DAsper turn (just over 1.3).
In the HC dialogues, sys-Site Expert User TotalHC A 3 (0.5%) 4 (0.7%) 7 (1.2%)HC B 13 (1.9%) 0 (0.0%) 13 (1.9%)HC C 20 (3.1%) 3 (0.5%) 23 (3.6%)HC D 0 (0.0%) 0 (0.0%) 0 (0.0%)HH SRI 95 (16.1%) 79 (13.3%) 174 (29.4%)HH CMU 94 (12.1%) 72 (9.3%) 166 (21.4%)Table 6: Number of short confirmations, i.e.
ac-knowledge (percentage of total dialogue acts)Site Expert User TotalHC A 32 (5.7%) 0 (0.0%) 32 (5.7%)HC B 74 (10.6%) 0 (0.0%) 74 (10.6%)HC C 59 (9.2%) 1 (0.2%) 60 (9.4%)HC D 45 (8.6%) 0 (0.0%) 45 (8.6%)HH SRI 11 (1.9%) 11 (1.9%) 22 (3.7%)HH CMU 47 (6.1%) 11 (1.4%) 58 (7.5%)Table 7: Number of long confirmations i.e.verify-X (percentage of total dialogue acts)tems have, on average 1.6 DAs per turn whereusers have just 1.0, as Figure 3 shows.
If we takea DA as representing a single dialogue ?move?,then users in the HC dialogues are managing onemove per turn, where the systems have at least oneand often more.
A common sequence for the com-puter experts is a verify-task-information fol-lowed by a request-task-information, such asA flight to Atlanta.
What city are you departingfrom?.Figure 3: Dialogue acts per turn for users andexperts in the HH and HC dialogues3.4 Types of Dialogue ActsOne of our main questions going into this workwas whether there would be interestingly differ-ent distributions of DAs in the HH and HC dia-logues, and whether different distributions of DAsacross systems would be correlated with user sat-isfaction.
Unfortunately, we do not have user sat-isfaction scores for the HH data, but if new datawere to be collected, this would be an essentialaddition.Tables 8 and 9 illustrate some of the main dif-ferences between the HH and HC dialogues, andas regards our first research question, definitelygive an interesting view of the differences betweenthe HH and HC conversations.DA Overall Expert UserGiveTaskInfo 27.7% 29.7% 25.5%Acknowledge 24.9% 26.9% 22.7%RequestTaskInfo 11.0% 10.7% 11.4%VerifyTaskInfo 5.4% 7.5% 3.2%Affirm 4.8% 4.3% 5.4%Table 8: Five most frequent DAs in Human-Human dialogues, by percent of total DAs for col-umnDA Overall Expert UserGiveTaskInfo 23.7% 12.9% 46.3%RequestTaskInfo 15.3% 22.1% 1.3%Offer 7.7% 11.5% 0.0%VerifyTaskInfo 7.1% 10.5% 0.1%Apology 4.5% 6.6% 0.1%Table 9: Five most frequent DAs in Human-Computer dialogues, by percent of total DAs forcolumnAs expected in this domain, all DAs involvingexchange of task information (give-task-info,request-task-info, and verify-task-info arefrequent in both sets of dialogues.
However, in theHH dialogues, acknowledge (e.g.
the tag forback-channel responses and general confirmationssuch as right, uh huh and okay) is the second mostcommon DA, and does not even appear in the topfive for the HC dialogues.
The DA for positive re-sponses, affirm, is also in the top ranking for theHH dialogues, but does not appear in the list forthe HC dialogues.
Finally, offer and apologyappear frequently in the HC dialogues and not inthe top HH DAs.
The appearance of these two is aclear indication that the systems are doing thingsquite differently from their human counterparts.Turning to differences between experts andusers in these top categories, we can see that hu-man users and experts are about equally likelyto ask for or give task-related information (give-task-info and request-task-info).
In con-trast, in the HC dialogues nearly half of the users?DAs are giving task information and hardly anyare requesting such information, while almost aquarter of expert DAs are requesting information.There is some inequity in the use of verify-task-info in the HH dialogues, where experts performabout twice as many verifications as users; how-ever, in the HC dialogues, virtually all verificationis done by the expert.
All of these patterns rein-force our finding about initiative distribution; inthe HC dialogues, one disproportionately finds theexpert doing the asking and verification of task in-formation, and the user doing the answering, whilein the HH dialogues the exchange of informationis much more balanced.DA HC A HC B HC C HC Daccept 3.9% 3.1% 4.8% 3.4%acknowledge 1.2% 1.9% 3.6% 0.0%affirm 1.8% 2.4% 0.8% 9.5%apologize 4.6% 3.7% 8.9% 0.0%demand-conv-info 1.1% 0.0% 0.0% 0.0%demand-sit-info 0.0% 1.6% 1.4% 1.3%demand-task-info 3.4% 0.3% 0.0% 1.3%give-sit-info 5.7% 6.3% 4.7% 1.9%give-task-info 34.8% 16.0% 24.8% 20.8%negate 2.1% 1.7% 0.8% 5.2%not-understand 2.5% 3.7% 7.2% 0.0%offer 3.5% 8.4% 9.4% 9.4%open-close 2.3% 3.1% 4.8% 3.4%please-wait 0.0% 6.2% 1.6% 3.1%reject 1.1% 4.1% 0.3% 2.5%req-conv-action 2.7% 4.4% 2.5% 1.0%req-sit-action 1.1% 1.4% 0.2% 1.9%req-sit-info 0.0% 3.3% 0.2% 3.2%req-task-action 1.1% 1.4% 0.3% 0.2%req-task-info 17.9% 12.6% 10.9% 21.6%suggest-conv-action 1.6% 0.1% 2.0% 0.0%thank 2.1% 3.4% 1.4% 1.7%verify-conv-action 0.7% 0.7% 0.0% 0.0%verify-task-action 2.5% 0.4% 1.9% 0.0%verify-task-info 2.5% 9.4% 7.5% 8.6%user satisfaction9 60.0% 71.5% 68.5% 82.8%Table 10: Distribution of DAs by SystemTable 10 gives an interesting snapshot of eachsystem, in terms of its overall distribution of DAs.These numbers are reflective of the system design-ers?
decisions for their systems, and that means allDAs are not going to be used by all systems (i.e.0.0% may mean that that DA is not part of thesystem?s repertoire).We will concentrate here on the best and worst9This figure combines the scores on five user satis-faction questions.
A perfect score is 100%.received systems in terms of their overall user sat-isfaction, HC D and HC A; the relevant numbersare boldfaced.
They also have very different di-alogue strategies, and that is partially reflectedin the table.
HC D?s dialogue strategy does notmake use of the ?social nicety?
DAs employed byother systems (acknowledge, apologize, not-understand), and yet it still had the highest usersatisfaction of the four.
This system also has thehighest proportion of affirm (more than threetimes as many as the next highest system) andreq-task-info DAs, which suggests that quite alot of information is being solicited and the users(because we know from Table 9 that it is primarilythe users responding) are more often than averageresponding affirmatively.
The fact that the per-centage of give-task-infos is somewhere in themiddle of the range and affirms is so high mayindicate that the HC D uses more yes/no thancontent questions.Looking at the lower scoring system, HC A, wesee very different patterns.
HC A has most ofthe demand-task-infos, the second highest per-centage of req-task-infos and by far the mostgive-task-infos, so its dialogue strategy mustinvolve a large number of attempts to extract in-formation from the user, and yet it has the fewestoffer DAs, so these don?t appear to be resultingin suggestions of particular travel options.Turning to correlations between DA use byexpert and user (combined across systems) anduser satisfaction, we see some expected resultsbut also some rather surprising correlations.Not unexpectedly, apologies and signals of non-understanding by the system are highly negativelycorrelated with satisfaction (-0.7 and -0.9, respec-tively).
While it may seem counter-intuitive thatopen-close by the user is negatively correlated(at -0.8), those familiar with this data will un-doubtedly have noticed that users often try to sayGoodbye repeatedly to try to end a dialogue thatis going badly.
Discussion of situational informa-tion (e.g.
phone use) by the expert is highly neg-atively correlated, but by the user, the DA req-situation-info is perfectly positively correlated.We cannot account for this finding.3.5 Unsolicited InformationIn the HC data we noticed that users oftenprovided more information than was explicitlysolicited?we call this ?unsolicited information?.For example, when a system asks for one pieceof information, On what day would you be depart-ing Portland?, the user might respond with ad-ditional information such as, Thursday, October5th before six pm from Portland back to Seattle.78% of that unsolicited information is offered inresponse to open-ended questions (e.g., How can Ihelp you?
or What are your travel plans?).
Whileour initiative tagging partially captures this, thereare cases where the answer may be considered re-sponsive (i.e.
initiative does not shift away fromthe participant asking the question) and yet un-solicited information has been offered.
Thus, thiscategory is somewhat orthogonal to our charac-terization of initiative, although it is clearly oneway of seizing control of the conversation.10To get at this information, we developed a thirdtagging scheme for annotating unsolicited infor-mation.
We began examining just the HC doc-uments, because the phenomenon is prevalent inthese data; we hope to perform a similar analysison the HH data as well.
We found that the sys-tems we examined in general handle unsolicited in-formation well.
70% of all unsolicited informationis handled correctly by the systems, 22% is han-dled incorrectly, and the rest could not be accu-rately classified.
Information offered in responseto open-ended questions is handled correctly moreoften by the systems than unsolicited informationoffered at other points in the dialogue (74% versus56%).
The former figure is not surprising, sincethe systems are designed to handle ?unsolicited?information following open-prompts.
However, wewere surprised the systems did as well as they didon unsolicited information in contexts where itwas not expected.
Figure 4 shows the relationshipbetween frequency of various types of unsolicitedinformation and how well the system incorporatesthat information.
There appears to be some cor-relation between the frequency of unsolicited in-formation and the rate of success, but we do nothave enough data to make a stronger claim.Furthermore, systems vary in response delay topieces of unsolicited information.
We define re-sponse delay as the number of system turns ittakes before the information is acknowledged bythe system (either correctly or incorrectly.)
If asystem responds immediately to the unsolicitedinformation, a count of zero turns is recorded.Figure 5 shows the difference among systems in re-sponding to unsolicited information.
We graphedboth the average total number of system turns aswell as the average number of turns minus rep-etitions.
HC B responds almost immediately to10This issue may also be related to where in thedialogue errors occur.
We are pursuing another lineof research which looks at automatic error detection,described in (Aberdeen et al, 2001).
We believe wemay also be able to detect unsolicited information au-tomatically, as well as to see whether it is likely totrigger errors by the system.Figure 4: Unsolicited Fields vs.
Success Rate ofIncorporationunsolicited information while HCs A and C takemore turns to respond.
HC D has trouble under-standing the unsolicited information, and eitherkeeps asking for clarification or continues to ig-nore the human and prompts for some other pieceof information multiple times.Figure 5: Variation of System Response to Unso-licited InformationFigure 6 shows the different rates at which sys-tems acknowledge unsolicited information for dif-ferent fields.
For example, departure city is recog-nized and validated almost immediately.
Returndate and flight type are incorporated fairly quicklywhen the system understands what is being said.If we look at the effects of experience onthe amount of unsolicited information offered, asshown in Figure 7, we can see that users tendto provide more unsolicited information over time(i.e., as they make more calls to the systems).This effect may be the result of increased userconfidence in the systems at handling unsolicitedinformation.
It also may be attributed to userboredom; as time goes on, users may be tryingto finish the task as quickly as possible.
Even ifthis is true, however, it demonstrates attempts byusers to take more control of the interactions asFigure 6: System Response to Different Types ofUnsolicited Informationthey become more experienced.Figure 7: Effect of Experience on Unsolicited In-formationOur data also show that the success rate of in-corporating unsolicited information improves withuser experience.
The ratio of successes to failuresincreases in later calls to the systems (Figure 8).4 DiscussionThis was a relatively small study, but many ofthe results are sufficiently striking that we expectthem to hold over large sets of dialogues.
First,it is clear that (for our definition of the term) ini-tiative is skewed towards the computer expert inthe human-computer dialogues, despite claims ofdevelopers to the contrary.
Whether this is de-sirable or not is a separate issue, but we believeit is a move forward to be able to quantify thisdifference.
Second, there are clear differences indialogue act patterns between the HH and HC di-alogues.
When the DAs correspond to basic di-alogue moves, like questions or signals of agree-ment, we can begin to see how the dialogue dy-namic is different in the human computer situa-Figure 8: Experience versus Success Rate of In-corporating Unsolicited Informationtion.
In general, the conversation was much morebalanced between traveler and expert in the HHsetting, in terms of amount of speech, types of di-alogue acts and with respect to initiative.
In theHC conversations, the system dominated, in num-ber of words and dialogue acts and in initiative.We are very interested in the selection of the?right?
tag set for a given task.
As we noted inour discussion of DA tagging, we had very dif-ferent outcomes with two closely related tag sets.Clearly the choice of tag set is highly dependenton the use the tagged data will be put to, howeasily the task can be characterized in the set oftagging guidelines, and what trade-offs in accu-racy vs. richness of representation are acceptable.A central question we are left with is ?Whydon?t the users talk more in HC dialogues??
Isit that they are happy to just give short, specificanswers to very directed questions?
Or do they?learn?
that longer answers are likely to cause thesystems problems?
Or perhaps users have pre-conceived notions (often justified) that the com-puter will not understand long utterances?
Wemay speculate that poor speech recognition per-formance is a major factor shaping this behav-ior, leading system designers to attempt to con-strain what users can say, while simultneously at-tempting to hold onto the initiative.
(Walker etal.
(2001) found sentence accuracy to be one ofthe significant predictors of user satisfaction in theSummer 2000 DARPA Communicator data collec-tion.)
There are some cases where the experts inthe HC dialogues say things their human counter-parts need not.
One obvious case, which appearsin even the small example dialogues we are us-ing here, is that the systems tend to repeat utter-ances when there is some processing difficulty.
Inthe same vein, errors and misunderstandings aremore frequent in the HC data, resulting in (somefairly verbose) efforts by the systems to identifythe problem and get the conversation back ontrack.5 Future WorkWe are currently working with other Communica-tor sites who are also looking at dialogue issues.In addition, we are beginning to look at two newaspects of these dialogues: task complexity andconversational failure analysis (at the turn level,(Aberdeen et al, 2001)).
We are also interestedin examining patterns of initiative tags, i.e.
con-trol shift types and length of initiative runs, andat relations between DAs and user satisfaction.6 AcknowledgmentsThanks to Lori Levin and Alon Lavie at CMU forsharing the CSTAR tagging guidelines and theirsample tagged corpus.ReferencesJ.
Aberdeen, C. Doran, L. Damianos, S. Bayer, andL.
Hirschman.
2001.
Finding errors automaticallyin semantically tagged dialogues.
In Notebook Pro-ceedings of the First International Conference onHuman Language Technology Research, San Diego,CA, March.J.
C. Carletta, A. Isard, S. Isard, J. Kowtko,G.
Doherty-Sneddon, and A. Anderson.
1995.The coding of dialogue structure in a corpus.
InJ.
A. Andernach, S. P. van de Burgt, and G. F.van der Hoeven, editors, Proceedings of the TwenteWorkshop on Language Technology: Corpus-basedapproaches to dialogue modelling, Enschede, TheNetherlands.
Universiteit Twente.Jennifer Chu-Carroll and Michael K. Brown.
1998.An evidential model for tracking initiative in col-laborative dialogue interactions.
User Modeling andUser-Adapted Interaction, 8(3-4):215?253.CSTAR Consortium.
1999.
Dialogue act annotation.Unpublished Manuscript, October.Mark Core and James Allen.
1997.
Coding dialogswith the damsl annotation scheme.
In Proceed-ings of the AAAI Fall Symposium on Communica-tive Action in Humans and Machines, Boston, MA,November.Mark Core, Masato Ishizaki, Johanna Moore, Chris-tine Nakatani, Nobert Reithinger, David Traum,and Syun Tutiya, editors.
1999.
The Report of TheThird Workshop of the Discourse Resource Initia-tive, Chiba Univeristy.
Technical Report No.3 CC-TR-99-1.Barbara Di Eugenio, Pamela W. Jordan, and LiinaPylkknen.
1998.
The COCONUT project: dialogueannotation manual.
Technical Report ISP Techni-cal Report 98-1, University of Pittsburgh, Decem-ber.Giovanni Flammia and Victor Zue.
1997.
Learn-ing the structure of mixed initiative dialogues us-ing a corpus of annotated conversations.
In Proc.Eurospeech 97, pages 1871?1874, Rhodes, Greece,September.Pamela W. Jordan and Barbara Di Eugenio.
1997.Control and initiative in collaborative problem solv-ing dialogues.
In AAAI Spring Symposium on Com-putational Models for Mixed Initiative Interaction,Stanford, CA.Daniel Jurafsky, Elizabeth Shriberg, and De-bra Biasca.
1997.
Switchboard swbd-damslshallow-discourse-function annotation coders man-ual.
Technical Report Technical Report 97-02, Uni-versity of Colorado Institute of Cognitive Science,August.Marion Klein, Niels Ole Bernsen, Sarah Davies, LailaDybkj?r, Juanma Garrido, Henrik Kasch, An-dreas Mengel, Vito Pirelli, Massimo Poesio, Sil-via Quazza, and Claudia Soria, 1998.
SupportedCoding Schemes, MATE Deliverable D1.1, July.http://mate.nis.sdu.dk/.Christine H. Nakatani, Barbara J. Grosz, David D.Ahn, and Julia Hirschberg.
1995.
Instructions forannotating discourse.
Technical Report TR-21-95,Harvard University.R.J.
van Vark, J.P.M.
de Vreught, and L.J.M.Rothkrantz.
1996.
Analysing ovr dialogues, codingscheme 1.0.
Technical Report 96-137, Delft Univer-sity of Technology.Marilyn Walker and Rebecca Passonneau.
2001.
Di-alogue act tags as qualitative dialogue metrics forspoken dialogue systems.
In Notebook Proceedingsof the First International Conference on HumanLanguage Technology Research, San Diego, CA,March.Marilyn Walker and Steve Whittaker.
1990.
Mixedinitiative in dialogue: An investigation into dis-course segmentation.
In Proceedings of ACL90.M.
Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-folo, L. Hirschman, A.
Le, S. Lee, S. Narayan,K.
Papineni, B. Pellom, J. Polifroni, A. Potamianos,P.
Prabhu, A. Rudnicky, G. Sanders, S. Seneff,D.
Stallard, and S. Whittaker.
2001.
DARPA Com-municator Dialog Travel Planning Systems: TheJune 2000 Data Collection.
Submitted., April.Steve Whittaker and Phil Stenton.
1988.
Cues andcontrol in expert client dialogues.
In Proceedingsof the 26th Annual Meeting of the Association forComputational Linguistics (ACL88), pages 123?130.
