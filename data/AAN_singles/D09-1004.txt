Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30?39,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPSemantic Dependency Parsing of NomBank and PropBank:An Efficient Integrated Approach via a Large-scale Feature Selection?Hai Zhao(??
)?, Wenliang Chen(???
)?, Chunyu Kit?(???
)?Department of Chinese, Translation and LinguisticsCity University of Hong KongTat Chee Avenue, Kowloon, Hong Kong, China?Language Infrastructure Group, MASTAR ProjectNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289haizhao@cityu.edu.hk, chenwl@nict.go.jpAbstractWe present an integrated dependency-based semantic role labeling system forEnglish from both NomBank and Prop-Bank.
By introducing assistant argumentlabels and considering much more fea-ture templates, two optimal feature tem-plate sets are obtained through an effec-tive feature selection procedure and helpconstruct a high performance single SRLsystem.
From the evaluations on the dateset of CoNLL-2008 shared task, the per-formance of our system is quite close tothe state of the art.
As to our knowl-edge, this is the first integrated SRL sys-tem that achieves a competitive perfor-mance against previous pipeline systems.1 IntroductionWe investigate the possibility to construct an effec-tive integrated system for dependency-based se-mantic role labeling (SRL) task.
This means inthis work that a single system handles all thesesub-tasks, predicate identification/disambiguationand argument identification/classification, regard-less of whether the predicate is verbal or nominal.Traditionally, a SRL task, either dependencyor constituent based, is implemented as two sub-tasks, namely, argument identification and clas-sification.
If the predicate is unknown, then apredicate identification or disambiguation subtaskshould be additionally considered.
A pipelineframework is usually adopted to handle all thesesub-tasks.
The reason to divide the whole task?This study is partially supported by CERG grant9040861 (CityU 1318/03H), CityU Strategic Research Grant7002037.into multiple stages is two-fold, one is each sub-task asks for its favorable features, the other isat the consideration of computational efficiency.Generally speaking, a joint system is slower thana pipeline system in training.
(Xue and Palmer,2004) fount out that different features suited fordifferent sub-tasks of SRL, i.e.
argument identifi-cation and classification.
The results from CoNLLshared tasks in 2005 and 2008 (Carreras and Mar-quez, 2005; Koomen et al, 2005; Surdeanu et al,2008; Johansson and Nugues, 2008), further showthat SRL pipeline may be one of the standard toachieve a state-of-the-art performance in practice.In the recent years, most works on SRL, includ-ing two CoNLL shared task in 2004 and 2005,focus on verbal predicates with the availabilityof PropBank (Palmer et al, 2005).
As a com-plement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their cor-responding semantic roles using similar semanticframework as PropBank.
Though SRL for nomi-nal predicates offers more challenge, it draws rel-atively little attention (Jiang and Ng, 2006).
(Pustejovsky et al, 2005) discussed the issue ofmerging various treebanks, including PropBank,NomBank, and others.
The idea of merging thesetwo different treebanks was implemented in theCoNLL-2008 shared task (Surdeanu et al, 2008).However, few empirical studies support the ne-cessity of an integrated learning strategy fromNomBank and PropBank.
Though aiming at Chi-nese SRL, (Xue, 2006) reported that their exper-iments show that simply adding the verb data tothe training set of NomBank and extracting thesame features from the verb and noun instanceswill hurt the overall performance.
From the re-sults of CoNLL-2008 shared task, the top systemby (Johansson and Nugues, 2008) also used two30different subsystems to handle verbal and nominalpredicates, respectively.Despite all the above facts, an integrated SRLsystem still holds some sort of merits, being eas-ier to implement, a single-stage feature selectionbenefiting the whole system, an all-in-one modeloutputting all required semantic role informationand so on.The shared tasks at the CoNLL 2008 and 2009are devoted to the joint learning of syntactic andsemantic dependencies, which show that SRL canbe well performed using only dependency syn-tax input.
Using data and evaluation settingsof the CoNLL-2008 shared task, this work willonly focus on semantic dependency parsing andcompares the best-performing SRL system in theCoNLL-2009 shared Task (Zhao et al, 2009b)with those in the CoNLL-2008 shared task (Sur-deanu et al, 2008; Haji?c et al, 2009)1.Aiming at main drawbacks of an integrated ap-proach, two key techniques will be applied.
1)Assistant argument labels are introduced for thefurther improvement of argument pruning.
Thishelps the development of a fast and lightweightSRL system.
2) Using a greedy feature selec-tion algorithm, a large-scale feature engineering isperformed on a much larger feature template setthan that in previous work.
This helps us find fea-tures that may be of benefit to all SRL sub-tasks aslong as possible.
As two optimal feature templatesets have been proven available, for the first timewe report that an integrated SRL system may pro-vide a result close to the state-of-the-art achievedby those SRL pipelines or individual systems forsome specific predicates.2 Adaptive Argument PruningA word-pair classification is used to formulate se-mantic dependency parsing as in (Zhao and Kit,2008).
As for predicate identification or disam-biguation, the first word is set as a virtual root(which is virtually set before the beginning of thesentence.)
and the second as a predicate candi-date.
As for argument identification/classification,the first word in a word pair is specified as a predi-1CoNLL-2008 is an English-only task, while CoNLL-2009 is a multilingual one.
Though the English corpus inCoNLL-2009 is almost identical to the corpus in the CoNLL-2008 shared task evaluation, the latter holds more sophisti-cated input structure as in (Surdeanu et al, 2008).
The mostdifference for these two tasks is that the identification of se-mantic predicates is required in the task of CoNLL-2008 butnot in CoNLL-2009.cate candidate and the second as an argument can-didate.
In either of case, the first word is called asemantic head, and noted as p in our feature rep-resentation, the second is called a semantic depen-dent and noted as a.Word pairs are collected for the classifier insuch order.
The first word of the pair is set to thevirtual root at first, the second word is then spec-ified as a predicate candidate.
According to theresult that the predicate candidate is classified orproven to be non-predicate, 1) the second word isreset to next predicate candidate if the answer isnon-predicate, otherwise, 2) the first word of thepair is reset to the predicate that is just determined,and the second is set to every argument candidatesone by one.
The classifier will scan the input sen-tence from left to right to check if each word is atrue predicate.Without any constraint, all word pairs in an in-put sequence must be considered by the classifier,leading to poor computational efficiency and un-necessary performance loss.
Thus, the trainingsample for SRL task needs to be pruned properly.We use a simple strategy to prune predicate can-didates, namely, only verbs and nouns are chosenin this case.There are two paths to collect argument candi-dates over the sequence.
One is based on an inputsyntactic dependency tree, the other is based ona linear path of the sentence.
As for the former(hereafter it is referred to synPth), we continue touse a dependency version of the pruning algorithmof (Xue and Palmer, 2004).
The pruning algorithmis readdressed as the following.Initialization: Set the given predicate as thecurrent node;(1) The current node and all of its syntacticchildren are selected as argument candidates(children are traversed from left to right.).
(2) Reset the current node to its syntactic headand repeat step (1) until the root is reached.Note that this pruning algorithm is slightly dif-ferent from that of (Xue and Palmer, 2004), thepredicate itself is also included in the argumentcandidate list as the nominal predicate sometimestakes itself as its argument.The above pruning algorithm has been showneffective.
However, it is still inefficient for a SRL31system that needs to tackle argument identifica-tion/classification in a single stage.
Assuming thatarguments trend to surround their predicate, an as-sistant argument label ?
NoMoreArgument?
is in-troduced for further pruning.
If an argument can-didate in the above algorithm is assigned to sucha label, then the pruning algorithm will end im-mediately.
In training, this assistant label meansno more samples will be generated for the currentpredicate, while in test, the decoder will not searcharguments any more.
It will be seen that this adap-tive technique more effectively prunes argumentcandidates without missing more true arguments.Along the linear path (hereafter referred tolinPth), the classifier will search all words beforeand after the predicate.
Similar to the pruningalgorithm for synPth, we also introduce two as-sistant argument labels ?
noLeft?
and ?
noRight?to adaptively prune words too far away from thepredicate.To show how assistant argument labels actuallywork, we give an example for linP th.
Suppose aninput sequence with argument labels for a predi-cate isa b c d e f g h .A1 A0Note that c and g are two boundary words as nomore arguments appear before or after them.
Aftertwo assistant argument labels are added, it will bea b c d e f g h .noLeft A1 A0 noRightTraining samples will generated from c to g ac-cording to the above sequence.We use a Maximum Entropy classifier with atunable Gaussian prior as usual.
Our implemen-tation of the model adopts L-BFGS algorithm forparameter optimization.3 Feature Templates3.1 Elements for Feature GenerationMotivated by previous works, we carefully con-sider those factors from a wide range of featuresthat can help semantic role labeling for both predi-cate disambiguation, argument?s identification andclassification as the predicate is either verbal ornominal.
These works include (Gildea and Juraf-sky, 2002; Carreras and Marquez, 2005; Koomenet al, 2005; Marquez et al, 2005; Dang andPalmer, 2005; Pradhan et al, 2005; Toutanova etal., 2005; Jiang and Ng, 2006; Liu and Ng, 2007;Surdeanu et al, 2007; Johansson and Nugues,2008; Che et al, 2008).
Most feature templatesthat we will adopt for this work will come fromvarious combinations or integrations of the follow-ing basic elements.Word Property.
This type of elements includeword form (form and its split form, spForm)2,lemma (lemma,spLemma), and part-of-speech tag(pos, spPos), syntactic dependency label (dprel),and semantic dependency label (semdprel)3.Syntactic Connection.
This includes syn-tactic head (h), left(right) farthest(nearest) child(lm, ln, rm, and rn), and high(low) supportverb or noun.
We explain the last item, sup-port verb(noun).
From a given word to thesyntactic root along the syntactic tree, the firstverb/noun/preposition that is met is called as itslow support verb/noun/preposition, and the near-est one to the root is called as its high supportverb/noun/preposition.
The concept of supportverb was broadly used (Toutanova et al, 2005;Xue, 2006; Jiang and Ng, 2006)4, we here extendit to nouns and prepositions.
In addition, we intro-duce a slightly modified syntactic head, pphead,it returns the left most sibling of a given word ifthe word is headed by a preposition, otherwise itreturns the original head.Path.
There are two basic types of path betweenthe predicate and the argument candidates.
Oneis the linear path (linePath) in the sequence, theother is the path in the syntactic parsing tree (dp-Path).
For the latter, we further divide it into foursub-types with respect to the syntactic root, dp-Path is the full path in the syntactic tree.
Leadingtwo paths to the root from the predicate and theargument, respectively, the common part of thesetwo paths will be dpPathShare.
Assume that dp-PathShare starts from a node r?, then dpPathPredis from the predicate to r?, and dpPathArgu is fromthe argument to r?.Family.
Two types of children sets for the pred-icate or argument candidate are considered, the2In CoNLL-2008, Treebank tokens are split at the positionthat a hyphen (-) or a forward slash (/) occurs.
This leads totwo types of feature columns, non-split and split.3Lemma and pos for either training or test are from auto-matically pre-analyzed columns in the input files.4Note that the meaning of support verb is slightly differentbetween (Toutanova et al, 2005) and (Xue, 2006; Jiang andNg, 2006)32first includes all syntactic children (children), thesecond also includes all but excludes the left mostand the right most children (noFarChildren).Concatenation of Elements.
For all collectedelements according to linePath, children and soon, we use three strategies to concatenate all thosestrings to produce the feature value.
The first isseq, which concatenates all collected strings with-out doing anything.
The second is bag, whichremoves all duplicated strings and sort the rest.The third is noDup, which removes all duplicatedneighbored strings.We address some other elements that are not in-cluded by the above description as the following.dpTreeRelation.
It returns the relationship of aand p in the input syntactic tree.
The possible val-ues for this feature include parent, siblingetc.isCurPred.
It judges if a given word is the cur-rent predicate.
If the word is the predicate, then itreturns the predicate itself, otherwise it returns adefault value.existCross.
It judges if a forthcoming depen-dency relation that is between a given word pairmay cause any cross with all existing dependencyrelations.distance.
It counts the number of words along agiven path, either dpPath or linePath.existSemdprel.
It checks if the given argumentlabel for other predicates has been assigned to agiven word.voice.
This feature returns Active or Passive forverbs, and a default value for nouns.baseline.
Two types of semantic role baselineoutputs are used for features from (Carreras andMarquez, 2005)5. baseline Ax tags the head ofthe first NP before the predicate as A0 and thehead of the first NP after the predicate as A1.baseline Mod tags the dependant of the predicateas AM-MOD as it is a modal verb.We show some feature template examples de-rived from the above mentioned items.a.lm.lemma The lemma of the left most child ofthe argument candidate.p.h.dprel The dependant label of the syntactichead of the predicate candidate.p?1.pos+p.pos pos of the previous word of thepredicate and PoS of the predicate itself.a:p|dpPath.lemma.bag Collect all lemmas5These baseline rules were developed by Erik Tjong KimSang, from the University of Antwerp, Belgium.along the syntactic tree path from the argumentto the predicate, then removed all duplicatedones and sort the rest, finally concatenate all as afeature string.a:p.highSupportNoun|linePath.dprel.seq Col-lect all dependant labels along with the line pathfrom the argument to the high support noun of thepredicate, then concatenate all as a feature string.3.2 Feature Template SelectionBased on the above mentioned elements, 781 fea-ture templates (hereafter the set of these templatesis referred to FT )6are initially considered.
Fea-ture templates in this initial set are constructed ina generalized way.
For example, if we find thata feature template a.lm.lemma was once used insome existing work, then such three templates,a.rm.lemma, a.rn.lemma, a.ln.lemma will be alsoadded into the set.As an optimal feature template subset cannot beexpected to be extracted from so large a set byhand, a greedy feature selection similar to that in(Jiang and Ng, 2006; Ding and Chang, 2008) is ap-plied.
The detailed algorithm is described in Algo-rithm 1.
Assuming that the number of feature tem-plates in a given set is n, the algorithm of (Dingand Chang, 2008) requires O(n2) times of train-ing/test routines, it cannot handle a set that con-sists of hundreds of templates.
As the time com-plexity of Algorithm 1 is only O(n), it permits alarge scale feature selection accomplished by pay-ing a reasonable time cost.
Though the time com-plexity of the algorithm given by (Jiang and Ng,2006) is also linear, it should assume all featuretemplates in the initial selected set ?good?
enoughand handles other feature template candidates in astrict incremental way.
However, these two con-straints are not easily satisfied in our case, whileAlgorithm 1 may release these two constraints.Choosing the first 1/10 templates in FT asthe initial selected set S, the feature selection isperformed for two argument candidate traverseschemes, synPth and linP th, respectively.
4686machine learning routines run for the former,while 6248 routines for the latter.
Two featuretemplate sets, FTsynand FTlin, are obtained atlast.
These two sets are given in Table 1-3.
We seethat two sets share 30 identical feature templatesas in Table 1.
FTsynholds 51 different templates6This set with detailed explanation will be available at ourwebsite.33p.lm.dprelp.rm.dprelp.spFormp?1.spLemmap.spLemmap?1.spLemma+p.spLemmap.spLemma + p1.spLemmap.spLemma + p.h.spFormp.spLemma + p.currentSensep.lemmap.lemma + p1.lemmap?1.pos+p.posa.isCurPred.lemmaa?2.isCurPred.lemma + a?1.isCurPred.lemmaa.isCurPred.spLemmaa?1.isCurPred.spLemma + a.isCurPred.spLemmaa.isCurPred.spLemma + a1.isCurPred.spLemmaa.children.dprel.baga?1.spLemma + a.spLemmaa?1.spLemma + a.dprela?1.spLemma + a.dprel + a.h.spLemmaa.lm?1.spLemmaa.rm?1.dprel + a.spPosa?1.lemma + a.dprel + a.h.lemmaa.lemma + p.lemmaa.pos + p.posa.spLemma + p.spLemmaa:p|dpPath.dprela:p|dpPathArgu.dprela:p|dpPathPred.spPosTable 1: Feature templates for both synPth andlinP thas in Table 2 and FTlinholds 57 different tem-plates as in Table 3.
In these tables, the subscripts -2(or -1) and 1(or 2) stand for the previous and nextwords, respectively.
For example, a.lm?1.lemmareturns the lemma of the previous word of the ar-gument?s left most child.4 DecodingAfter the predicate sense is disambiguated, an op-timal argument structure for each predicate is de-termined by the following maximal probability.Sp= argmax?iP (ai|ai?1, ai?2, ...), (1)where Spis the argument structure, P (ai|ai?1...)is the conditional probability to determine the la-bel of the i-th argument candidate label.
A beamsearch algorithm is used to find the optimal argu-ment structure.5 Evaluation ResultsOur evaluation is performed on the standardtraining/development/test corpus of CoNLL-2008shared task.
The data is derived by merging a de-pendency version of the Penn Treebank with Prop-Bank and NomBank.
More details on the data areAlgorithm 1 Greedy Feature SelectionInput:The set of all feature templates: FTThe set of selected feature templates: S0Output:The set of selected feature templates: SProcedure:Let the counter i = 1Let Si= S0and C = FT ?
Siwhile doTrain a model with features according to Si,test on development set and the result is pi.Let Cr= null.for each feature template fjin set SidoLet S?= Si?
fj.Train a model with features according toS?, test on development set and the resultis p?.if p?> pithenCr= Cr+ fj.end ifend forC = C + CrSi= Si?
CrLet S?i= SiTrain a model with features according to S?i,test on development set and the result is qi.Let Cr= nullfor each feature template fjin set C doLet C?= S?i+ fj.Train a model with features according toC?, test on development set and the resultis p?.if p?> qithenCr= Cr+ fj.end ifend forC = C ?
CrS?i= S?i+ Crif Si= Si?1(No feature templates are addedor removed) or, neither pinor qiis larger thanpi?1and qi?1thenOutput S = argmaxpi,qi{Si, S?i} and thealgorithm ends.elseLet i = i+ 1, Si=Si?1and C = FT ?
Siend ifend while34p?1.lemma + p.lemmap?2.posp.posp?2.spForm + p?1.spFormp1.spFormp.spForm + p.children.dprel.noDupp.lm.spPosp.spForm + p.lm.spPos+ p.noFarChildren.spPos.bag + p.rm.spPosp.dprelp.children.dprel.bagp.children.pos.seqp.dprel = OBJ ?aa.dprela?1.lemma + a1.lemmaa1.lemmaa?1.posa1.spPosa.h.lemmaa.h.spLemmaa.pphead.lemmaa.pphead.spLemmaa.lm.dprel + a.spPosa.rm?1.posa.spLemma + a.h.spPosa.existSemdprel A1a.dprel = OBJ ?a.form + a.children.pos.seqa.children.adv.bagba:p|linePath.distancea:p|dpPath.distancea:p|existCrossa:p|dpPath.dprel.baga:p|dpPathPred.dprel.baga:p|dpPath.spForm.seqa:p|dpPathArgu.spForm.seqa:p|dpPathPred.spForm.baga:p|dpPath.spLemma.seqa:p|dpPathArgu.spLemma.seqa:p|dpPathArgu.spLemma.baga:p|dpPathPred.spLemma.baga:p|dpPath.spPos.baga:p|dpPathPred.spPos.bag(a:p|dpPath.dprel.seq) + p.spPos(a:p|dpTreeRelation) + a.spPos(a:p|dpTreeRelation) + p.spPos(a.highSupportVerb:p|dpTreeRelation) + a.spPosa.highSupportNoun:p|dpPath.dprel.seqa.lowSupportVerb:p|dpPath.dprel.seqa:p|linePath.spForm.baga:p|linePath.spLemma.baga:p|linePath.spLemma.seqaThis feature checks if the dependant type is OBJ.badv means all adverbs.Table 2: Feature templates only for synPthp.currentSense + a.spLemmap.currentSense + a.spPosp.voice + (a:p|direction)p.rm.dprelp.children.dprel.noDupp.rm.formp.lowSupportNoun.spFormp.lowSupportProp:p|dpTreeRelationp?2.form + p?1.formp.voicep.form + p.children.dprel.noDupp.pos + p.dprelp.spForm + p.children.dprel.baga.voice + (a:p|direction)a?1.isCurPred.lemmaa1.isCurPred.lemmaa?1.isCurPred.lemma + a.isCurPred.lemmaa.isCurPred.lemma + a1.isCurPred.lemmaa1.isCurPred.spLemmaa?2.isCurPred.spLemma + a?1.isCurPred.spLemmaa.baseline Ax + a.voice + (a:p|direction)a.baseline Moda.h.children.dprel.baga.lm.dprel + a.dprela.lm.dprel + a.posa.lm?1.lemmaa.lm.lemmaa.lm1.lemmaa.lm.pos + a.posa.lm.spForma.lm?1.spPosa.lm.spPosa.ln.dprel + a.posa.noFarChildren.spPos.bag + a.rm.spPosa.children.spPos.seq + p.children.spPos.seqa.rm.dprel + a.posa.rm?1.spPosa.rm.spPosa.rm1.spPosa.rn.dprel + a.spPosa.forma.form + a1.forma.form + a.posa?1.lemmaa?1.lemma + a.lemmaa?2.posa.spForm + a1.spForma.spForm + a.spPosa.spLemma + a1.spLemmaa.spForm + a.children.spPos.seqa.spForm + a.children.spPos.baga.spLemma + a.h.spForma.spLemma + a.pphead.spForma.existSemdprel A2a:p|dpPathArgu.pos.seqa:p|dpPathPred.dprel.seqa:p|dpTreeRelationTable 3: Feature templates only for linPth35in (Surdeanu et al, 2008).
Note that CoNLL-2008shared task is essentially a joint learning task forboth syntactic and semantic dependencies, how-ever, we will focus on semantic part of this task.The main semantic measure that we adopt is se-mantic labeled F1score (Sem-F1).
In addition, themacro labeled F1scores (Macro-F1), which wasused for the ranking of the participating systems ofCoNLL-2008, the ratio between labeled F1scorefor semantic dependencies and the LAS for syn-tactic dependencies (Sem-F1/LAS), are also givenfor reference.5.1 Syntactic Dependency ParsersWe consider three types of syntactic informationto feed the SRL task.
One is gold-standard syn-tactic input, and other two are based on automati-cally parsing results of two parsers, the state-of-the-art syntactic parser described in (Johanssonand Nugues, 2008)7(it is referred to Johansson)and an integrated parser described as the follow-ing (referred to MSTME).The parser is basically based on the MSTParser8using all the features presented by (McDonald etal., 2006) with projective parsing.
Moreover, weexploit three types of additional features to im-prove the parser.
1) Chen et al (2008) used fea-tures derived from short dependency pairs basedon large-scale auto-parsed data to enhance depen-dency parsing.
Here, the same features are used,though all dependency pairs rather than short de-pendency pairs are extracted along with the de-pendency direction from training data rather thanauto-parsed data.
2) Koo et al (2008) presentednew features based on word clusters obtained fromlarge-scale unlabeled data and achieved large im-provement for English and Czech.
Here, the samefeatures are also used as word clusters are gen-erated only from the training data.
3) Nivre andMcDonald (2008) presented an integrating methodto provide additional information for graph-basedand transition-based parsers.
Here, we representfeatures based on dependency relations predictedby transition-based parsers for the MSTParer.
Forthe sake of efficiency, we use a fast transition-7It is a 2-order maximum spanning tree parser withpseudo-projective techniques.
A syntactic-semantic rerank-ing was performed to output the final results according to (Jo-hansson and Nugues, 2008).
However, only 1-best outputs ofthe parser before reranking are used for our evaluation.
Notethat the reranking may slightly improve the syntactic perfor-mance according to (Johansson and Nugues, 2008).8It?s freely available at http://mstparser.sourceforge.net.Parser Path Adaptive Pruning Coverage/wo /w RateGold synPth 2.13M 1.05M 98.4%(49.30%)linP th 5.29M 1.57M 100.0%(29.68%)Johansson synPth 2.15M 1.06M 95.4%(49.30%)linP th 5.28M 1.57M 100.0%(29.73%)MSTMEsynPth 2.15M 1.06M 95.0%(49.30%)linP th 5.29M 1.57M 100.0%(29.68%)Table 4: The number of training samples on argu-ment candidatessynPth+FTsynlinPth+FTlinSyn-Parser LAS Sem Sem-F1Sem Sem-F1F1/LAS F1/LASMSTME88.39 80.53 91.10 79.83 90.31Johansson 89.28 80.94 90.66 79.84 89.43Gold 100.00 84.57 84.57 83.34 83.34Table 5: Semantic Labeled F1based parser based on maximum entropy as inZhao and Kit (2008).
We still use the similar fea-ture notations of that work.5.2 The ResultsAt first, we report the effectiveness of the proposedadaptive argument pruning.
The numbers of argu-ment candidates are in Table 4.
The statistics isconducted on three different syntactic inputs.
Thecoverage rate in the table means the ratio of howmany true arguments are covered by the selectedpruning scheme.
Note that the adaptive pruningof argument candidates using assistant labels doesnot change this rate.
This ratio only depends onwhich path, either synPth or linP th, is chosen,and how good the syntactic input is (if synPthis the case).
From the results, we see that morethan a half of argument candidates can be effec-tively pruned for synPth and even 2/3 for linP th.As mentioned by (Pradhan et al, 2004), argumentidentification plays a bottleneck role in improvingthe performance of a SRL system.
The effective-ness of the proposed additional pruning techniquesmay be seen as a significant improvement over theoriginal algorithm of (Xue and Palmer, 2004).
Theresults also indicate that such an assumption holdsthat arguments trend to close with their predicate,at either type of distance, syntactic or linear.Based on different syntactic inputs, we obtaindifferent results on semantic dependency parsing36as shown in Table 5.
These results on differ-ent syntactic inputs also give us a chance to ob-serve how semantic performance varies accordingto syntactic performance.
The fact from the re-sults is that the ratio Sem-F1/LAS becomes rela-tively smaller as the syntactic input becomes bet-ter.
Though not so surprised, the results do showthat the argument traverse scheme synPth alwaysoutperforms the other linP th.
The result of thiscomparison partially shows that an integrated se-mantic role labeler is sensitive to the order of howargument candidates are traversed to some extent.The performance given by synPth is com-pared to some other systems that participated inthe CoNLL-2008 shared task.
They were cho-sen among the 20 participating systems either be-cause they held better results (the first four partic-ipants) or because they used some joint learningtechniques (Henderson et al, 2008).
The results of(Titov et al, 2009) that use the similar joint learn-ing technique as (Henderson et al, 2008) are alsoincluded9.
Results of these evaluations on the testset are in Table 6.
Top three systems of CoNLL-2008, (Johansson and Nugues, 2008; Ciaramita etal., 2008; Che et al, 2008), used SRL pipelines.In this work, we partially use the similartechniques (synPth) for our participation in theshared tasks of CoNLL-2008 and 2009 (Zhao andKit, 2008; Zhao et al, 2009b; Zhao et al, 2009a).Here we report that all SRL sub-tasks are tackledin one integrated model, while the predicate dis-ambiguation sub-task was performed individuallyin both of our previous systems.
Therefore, this isour first attempt at a full integrated SRL system.
(Titov et al, 2009) reported the best result byusing joint learning technique up to now.
Thecomparison indicates that our integrated systemoutputs a result quite close to the state-of-the-artby the pipeline system of (Johansson and Nugues,2008) as the same syntactic structure input isadopted.
It is worth noting that our system actu-ally competes with two independent sub-systemsof (Johansson and Nugues, 2008), one for verbalpredicates, the other for nominal predicates.
In ad-dition, the results of our system is obtained with-out using additional joint learning technique likesyntactic-semantic reranking.
It indicates that oursystem is expected to obtain some further perfor-mance improvement by using such techniques.9In addition, the work of (Henderson et al, 2008) and(Titov et al, 2009) jointly considered syntactic and semanticdependencies, that is significantly different from the others.6 ConclusionWe have described a dependency-based semanticrole labeling system for English from NomBankand PropBank.
From the evaluations, the result ofour system is quite close to the state of the art.
Asto our knowledge, it is the first integrated SRL sys-tem that achieves such a competitive performanceagainst previous pipeline systems.According to the path that the word-pair classi-fier traverses argument candidates, two integrationschemes are presented.
Argument candidate prun-ing and feature selection are performed on them,respectively.
These two schemes are more thanproviding a trivial comparison.
As assistant la-beled are introduced to help further argument can-didate pruning, and this techniques work well forboth schemes, it support the assumption that argu-ments trend to surround their predicate.
The pro-posed feature selection procedure also work forboth schemes and output quite different two fea-ture template sets, and either of the sets helps thesystem obtain a competitive performance, this factsuggests that the feature selection procedure is ro-bust and effective, too.Either of the presented integrated systems canprovide a competitive performance.
This conclu-sion about basic learning scheme for SRL is somedifferent from previous literatures.
However, ac-cording to our results, there does exist a ?harmony?feature template set that is helpful to both predi-cate and argument identification/classification, orSRL for both verbal and nominal predicates.
Weattribute this different conclusion to two main fac-tors, 1) much more feature templates (for example,ten times more than those used by Xue et al) thanprevious that are considered for a successful fea-ture engineering, 2) a maximum entropy classifiermakes it possible to accept so many various fea-tures in one model.
Note that maximum entropy isnot so sensitive to those (partially) overlapped fea-tures, while SVM and other margin-based learnersare not so.AcknowledgementsOur thanks give to Dr. Richard Johansson, whokindly provided the syntactic output for his partic-ipation in the CoNLL-2008 shared task.37SystemsaLAS Sem-F1Macro Sem-F1pred-F1bargu-F1cVerb-F1dNomi-F1eF1/LASJohansson:2008*f89.32 81.65 85.49 91.41 87.22 79.04 84.78 77.12Ours:Johansson 89.28 80.94 85.12 90.66 86.57 78.30 83.66 76.93Ours:MSTME88.39 80.53 84.93 91.10 86.80 77.60 82.77 77.23Johansson:2008 89.32 80.37 84.86 89.98 85.40 78.02 84.45 74.32Ciaramita:2008* 87.37 78.00 82.69 89.28 83.46 75.35 80.93 73.80Che:2008 86.75 78.52 82.66 90.51 85.31 75.27 80.46 75.18Zhao:2008* 87.68 76.75 82.24 87.53 78.52 75.93 78.81 73.59Ciaramita:2008 86.60 77.50 82.06 89.49 83.46 74.56 80.15 73.17Titov:2009 87.50 76.10 81.80 86.97 ?
?
?
?Zhao:2008 86.66 76.16 81.44 87.88 78.26 75.18 77.67 73.28Henderson:2008* 87.64 73.09 80.48 83.40 81.42 69.10 75.84 68.90Henderson:2008 86.91 70.97 79.11 81.66 79.60 66.83 73.80 66.26Ours:Gold 100.0 84.57 92.20 84.57 87.67 83.15 88.71 78.39aRanking according to Sem-F1bLabeled F1for predicate identification and classificationcLabeled F1for argument identification and classificationdLabeled F1for verbal predicateseLabeled F1for nominal predicatesf* means post-evaluation results, which are available at the official website of CoNLL-2008 shared task,http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.Table 6: Comparison of the best existing systemsReferencesXavier Carreras and Lluis Marquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
In Proceedings of CoNLL-2005, pages 152?164, Ann Arbor, Michigan, USA.Wanxiang Che, Zhenghua Li, Yuxuan Hu, YongqiangLi, Bing Qin, Ting Liu, and Sheng Li.
2008.
Acascaded syntactic and semantic dependency pars-ing system.
In Proceedings of CoNLL-2008, pages238?242, Manchester, England, August.Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-moto, Yujie Zhang, and Hitoshi Isahara.
2008.
De-pendency parsing with short dependency relationsin unlabeled data.
In Proceedings of IJCNLP-2008,Hyderabad, India, January 8-10.Massimiliano Ciaramita, Giuseppe Attardi, FeliceDell?Orletta, and Mihai Surdeanu.
2008.
Desrl: Alinear-time semantic role labeling system.
In Pro-ceedings of CoNLL-2008, pages 258?262, Manch-ester, England, August.Hoa Trang Dang and Martha Palmer.
2005.
The roleof semantic roles in disambiguating verb senses.
InProceedings of ACL-2005, pages 42?49, Ann Arbor,USA.Weiwei Ding and Baobao Chang.
2008.
Improvingchinese semantic role classification with hierarchi-cal feature selection strategy.
In Proceedings ofEMNLP-2008, pages 324?323, Honolulu, USA.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, pages 1?18, Boulder, Colorado, USA.James Henderson, Paola Merlo, Gabriele Musillo, andIvan Titov.
2008.
A latent variable model of syn-chronous parsing for syntactic and semantic depen-dencies.
In Proceedings of CoNLL-2008, pages178?182, Manchester, England, August.Zheng Ping Jiang and Hwee Tou Ng.
2006.
Seman-tic role labeling of nombank: A maximum entropyapproach.
In Proceedings of EMNLP-2006, pages138?145, Sydney, Australia.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith propbank and nombank.
In Proceedings ofCoNLL-2008, page 183?187, Manchester, UK.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL-08: HLT, pages 595?603,Columbus, Ohio, USA, June.Peter Koomen, Vasin Punyakanok, Dan Roth, and Wentau Yih.
2005.
Generalized inference with multi-ple semantic role labeling systems.
In Proceedingsof CoNLL-2005, pages 181?184, Ann Arbor, Michi-gan, USA.Chang Liu and Hwee Tou Ng.
2007.
Learning pre-dictive structures for semantic role labeling of nom-bank.
In Proceedings of ACL-2007, pages 208?215,Prague, Czech.38Lluis Marquez, Mihai Surdeanu, Pere Comas, andJordi Turmo.
2005.
A robust combination strat-egy for semantic role labeling.
In Proceedingsof HLT/EMNLP-2005, page 644?651, Vancouver,Canada.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with atwo-stage discriminative parser.
In Proceedings ofCoNLL-X, New York City, June.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The nombank project:An interim report.
In Proceedings of HLT/NAACLWorkshop on Frontiers in Corpus Annotation, pages24?31, Boston, Massachusetts, USA, May 6.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
2004.
Shallowsemantic parsing using support vector machines.
InProceedings of HLT/NAACL-2004, pages 233?240,Boston, Massachusetts, USA.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Daniel Jurafsky.
2005.
Se-mantic role labeling using different syntactic views.In Proceedings of ACL-2005, pages 581?588, AnnArbor, USA.James Pustejovsky, Adam Meyers, Martha Palmer, andMassimo Poesio.
2005.
Merging propbank, nom-bank, timebank, penn discourse treebank and coref-erence.
In Proceedings of the Workshop on Frontiersin Corpus Annotations II: Pie in the Sky, pages 5?12,Ann Arbor, USA.Mihai Surdeanu, Lluis Marquez, Xavier Carreras, andPere R. Comas.
2007.
Combination strategies forsemantic role labeling.
Journal of Artificial Intelli-gence Research, 29:105?151.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofCoNLL-2008, pages 159?177, Manchester, UK.Ivan Titov, James Henderson, Paola Merlo, andGabriele Musillo.
2009.
Online graph planarisationfor synchronous parsing of semantic and syntacticdependencies.
In IJCAI-2009, Pasadena, California,USA.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of ACL-2005, pages589?596, Ann Arbor, USA.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof EMNLP-2004, pages 88?94, Barcelona, Spain,July 25-26.Nianwen Xue.
2006.
Semantic role labeling of nom-inalized predicates in chinese.
In Proceedings ofNAACL-2006, pages 431?438, New York City, USA,June.Hai Zhao and Chunyu Kit.
2008.
Parsing syntactic andsemantic dependencies with two single-stage max-imum entropy models.
In Proceeding of CoNLL-2008, pages 203?207, Manchester, UK.Hai Zhao, Wenliang Chen, Jun?ichi Kazama, KiyotakaUchimoto, and Kentaro Torisawa.
2009a.
Multilin-gual dependency learning: Exploiting rich featuresfor tagging syntactic and semantic dependencies.
InProceedings of the 13th Conference on Computa-tional Natural Language Learning (CoNLL-2009),June 4-5, pages 61?66, Boulder, Colorado, USA.Hai Zhao, Wenliang Chen, Chunyu Kit, and GuodongZhou.
2009b.
Multilingual dependency learning:A huge feature engineering method to semantic de-pendency parsing.
In Proceedings of CoNLL-2009,pages 55?60, Boulder, Colorado, USA.39
