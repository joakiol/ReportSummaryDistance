Proceedings of the SIGDIAL 2013 Conference, pages 173?182,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsInterpreting Situated Dialogue Utterances:an Update Model that Uses Speech, Gaze, and Gesture InformationCasey KenningtonCITEC, Bielefeld Universityckennington1Spyros KousidisBielefeld Universityspyros.kousidis21@cit-ec.uni-bielefeld.de2@uni-bielefeld.deDavid SchlangenBielefeld Universitydavid.schlangen2AbstractIn situated dialogue, speakers share timeand space.
We present a statistical modelfor understanding natural language thatworks incrementally (i.e., in real, sharedtime) and is grounded (i.e., links to en-tities in the shared space).
We describeour model with an example, then estab-lish that our model works well on non-situated, telephony application-type utter-ances, show that it is effective in ground-ing language in a situated environment,and further show that it can make good useof embodied cues such as gaze and point-ing in a fully multi-modal setting.1 IntroductionSpeech by necessity unfolds over time, and in spo-ken conversation, this time is shared between theparticipants.
Speakers are also by necessity lo-cated, and in face-to-face conversation, they sharetheir (wider) location (that is, they are co-located).The constraints that arise from this set of facts areoften ignored in computational research on spokendialogue, and where they are addressed, typicallyonly one of the two is addressed.Here, we present a model that computes in anincremental fashion an intention representation fordialogue acts that may comprise both spoken lan-guage and embodied cues such as gestures andgaze, where these representations are grounded inrepresentations of the shared visual context.
Themodel is trained on conversational data and can beused as an understanding module in an incremen-tal, situated dialogue system.Our paper begins with related work and back-ground and then specifies in an abstract way thetask of the model.
We describe our model formallyin Section 4, followed by three experiments withthe model, the first establishing it with a traditionalspoken language understanding (SLU) setting, thesecond to show that our model works well undersituated conditions, and the third shows that ourmodel can make use of embodied cues.
We fin-ish the paper with a general discussion and futurework.2 Related Work and BackgroundThe work presented in this paper connects and ex-tends several areas of research: grounded seman-tics (Roy, 2005; Hsiao et al 2008; Liu et al2012), which aims to connect language with theworld, but typically does not work incrementally;semantic parsing / statistical natural language un-derstanding (NLU), which aims to map an utter-ance to its meaning representation (using vari-ous routes and approaches, such as logical forms(Zettlemoyer and Collins, 2007; Zettlemoyer andCollins, 2009), dependency-based compositionalsemantics (Liang et al 2011), neural networks(Huang and Er, 2010), Markov Logic Networks(MLN) (Meurs et al 2008; Meza-Ruiz et al2008), and dynamic Bayesian networks (Meurset al 2009); see also overviews in (De Mori etal., 2008; Wang et al 2011)), but typically nei-ther provides situated interpretations nor incre-mental specifications of the representations; incre-mental NLU (DeVault et al 2009; DeVault et al2011; Aist et al 2007; Schlangen and Skantze,2009), which focuses on incrementality, but noton situational grounding; integration of gaze intolanguage understanding (Prasov and Chai, 2010),which was not incremental.We move beyond this work in that we present amodel that is incremental, uses a form of groundedsemantics, can easily incorporate multi-modal in-formation sources, and finally on which inferencecan be performed quickly, satisfying the demandsof real-time dialogue.
The model brings togetheraspects we?ve previously looked into separately:grounded semantics in (Siebert and Schlangen,1732008); incremental interpretation (reference res-olution) in (Schlangen et al 2009); incrementalgeneral NLU in (Heintze et al 2010); and a moresophisticated approach that handled all of these us-ing markov logic networks, but did not work inreal-time or with multi-modal input (Kenningtonand Schlangen, 2012).3 The TaskThe task for our model is as follows: to compute atany moment a distribution over possible intentions(expressed as semantic frames), given the unfold-ing utterance and possibly information about thestate of the world in which the utterance is hap-pening.
The slots of these frames are to be filledwith semantic constants, that is, they are uniquelyresolved; if appropriate, to objects in the sharedenvironment.This is illustrated in Figure 1, where forthree successive incremental units (Schlangen andSkantze, 2009) (that is, successively available bitsof information pertaining to the same act, such aswords of an utterance, or information about speechaccompanying gesture) three distributions over in-tentions are shown.1[   ]fe: a[   ]fe: b[   ]fe: aIU1IU2IU3Donnerstag, 2.
Mai 2013Figure 1: Schematic Illustration of Task4 Our ModelMore formally, the goal of the model is to recoverI , the intention of the speaker behind her utter-ance, in an incremental fashion, that is, word byword.
We make the assumption that the set ofpossible intentions is finite, and that they consistof (combinations of) entities (where however evenactions like taking are considered ?entities?
; moreon this below).
We observe U , the current wordthat the speaker uttered as part of their utterance(and features derived from that).
We also assumethat there is an unobserved mediating variable R,1Here, no links between these intention representationsare shown.
The model we present in the next section isan update model, that is, it builds the representation at steptn based on that at tn?1; other possibilities are explored in(Heintze et al 2010) and (Kennington and Schlangen, 2012).which represents the (visual or abstract) proper-ties of the (visually present, or abstract) objectof the intention.
So, what we need to calculateis P (I|U,R), even though ultimately we?re inter-ested only in P (I|U).
By definition of conditionalprobability, P (I|U,R) = P (I, U,R)?P (U,R)?1.We factorise P (I, U,R) as indicated in the follow-ing:P (I|R,U) = P (R|I)P (I)P (U |R)P (U,R) (1)That is, we make the assumption that R is con-ditional only on I , and U is conditional only onR.
Marginalizing over R gets us the model we?reinterested in (and it amounts to a not uncommontagging model with a hidden layer):P (I|U) = P (I)?r?RP (U |R = r)P (R = r|I)P (U,R = r)(2)Where we can move P (I) out of the summation,as it is not dependent on R. Hence, we need threemodels, P (I), P (U |R) and P (R|I), to computeP (I|U).
Figure 2 shows how these three modelsinteract over time.It?2Rt?2Ut?2It?1Rt?1Ut?1ItRtUtFigure 2: Our model represented as an unrolledDBN over three words.Each sub-model will now be explained.P(I) At the beginning of the computation for anincoming sentence, we set the prior P (I) to a uni-form distribution (or, if there is reason to do so, adifferent distribution to encode initial expectationsabout intentions; i.e., prior gaze information).
Forlater words, it is set to the posteriori of the pre-vious step, and so this constitutes a Bayesian up-dating of belief (with a trivial, constant transitionmodel that equates P (It?1) and P (It)).22In that sense, our incremental understanding could becalled ?intra-sentential belief tracking,?
in analogy to the cur-rent effort to track system belief about user intentions acrossturns (Ma et al 2012; Williams, 2010).174The other models represent knowledge aboutlinks between intentions and object properties,P (R|I), and knowledge about language use,P (U |R).
We now explain how this knowledge isacquired.P(R|I) The model P (R|I) provides the link be-tween objects (as occurring in the intentions) andtheir properties.
Here we follow, to our knowl-edge, a novel approach, by deriving this distribu-tion directly from the scene representation.
Thisis best explained by looking at the overall modelin a generative way.
First, the intention is gener-ated, P (I), then based on that a property, P (R|I).We assume that with equal probability one of theproperties that the intended object actually has ispicked to be verbalised, leaving zero probabilityfor the ones that it does not have.
This in a way isa rationality assumption: a rational speaker will, ifat all, mention properties that are realised and notothers (at least in non-negative contexts).P(U|R), learned directly The other model,P (U |R), can be learned directly from data by(smoothed) Maximum Likelihood estimation.
Fortraining, we assume that the property R that ispicked out for verbalisation is actually observable.In our data, we know which properties the refer-ent actually has, and so we can simply count howoften a word (and its derived features) co-ocurredwith a given property, out of all cases where thatproperty was present.P(U|R), via P(R|U) Instead of directly learn-ing a model of the data, we can learn a discrimina-tive model that connects words and properties.In Equation 2, we can rewrite P (U |R) usingBayes?
Rule:P (I|U) = P (I)?r?RP (U)P (R = r|U)P (R = r|I)P (R = r)P (U,R = r) (3)P (U) is a constant when computing P (I|U) forall possible values of I whose actual value doesnot change the rank of each intention, and so canbe dropped.
P (R) can be approximated with auniform distribution, and can also be dropped,yielding:P (I|U) = P (I)?r?RP (R = r|U)P (R = r|I)P (U,R = r)(4)Other models could also be learned here; we chosea discriminative model to show that our modelworks under varied circumstances.word red round square greenthe 0.03 0.02 0.02 0.02red 0.82 0.009 0.09 0.01ball 0.02 0.9 0.02 0.07Table 1: P (U |R) for our toy domain for somevalues of U and R; we assume that this model islearned from data (columns are excerpted from adistribution over a larger vocabulary).int.
red round square greenobj1 0.5 0.5 0 0obj2 0.5 0 0.5 0Table 2: P (R|I), for our example domain.Properties An important part of our model isthe set of properties.
Properties can be visualproperties such as color or shape or spatial prop-erties (left-of, below, etc.).
Though not the fo-cus of this paper, they could also be concep-tual properties (the verb run can have the proper-ties of movement, use of legs, and quick).Another example, New York has the property ofbeing New York.
(That is generally sufficientenough to denote New York, but note that descrip-tive properties (e.g., ?location of the Empire StateBuilding?)
could be used as well.)
The purposeof the properties is to ground intentions with lan-guage in a more fine-grained way than the wordsalone.We will now give an example of the generativeapproach as in Equation 2 (it is straight-forward todo the same for the discriminative model).4.1 ExampleThe task is reference resolution in a shared visualcontext: there is an intention to refer to a visibleobject.
For this example, there are two objectsobj1 and obj2, and four properties to describethose objects, red, round, square and green.The utterance for which we want to track a dis-tribution over possible referents, going word-by-word, is the red ball.
obj1 happens to be a redball, with properties red and round; obj2 is ared cube, with the properties red and square.We now need the models P (U |R) and P (R|I).We assume the former is learned from data, andfor the four properties and three words gives us re-sults as shown in Table 1 (that is, P (U = the|R =red) = 0.03).
The model P (R|I) can be read offthe representation of the scene: if you intend to175refer to object obj1 (I = obj1), you can eitherpick the property red or the property round, soboth get a probability of 0.5 and all others 0; sim-ilar for obj2 and red and square (Table 2).Table 3 now shows an application of the fullmodel to our example utterance.
The cellsin the columns labeled with properties showP (U |R)P (R|I) for the appropriate properties andintentions (objects), the column ?
shows resultsafter marginalizing over R. The final column thenfactors in P (I) with a uniform prior for the firstword, and the respectively previous distributionfor all others, and normalises.I U red rnd.
sq.
?
P (I|U)obj1 the .015 .01 0 .025 .5obj2 .015 0 .01 .025 .5obj1 red .41 .0045 0 .41 .47obj2 .41 0 .045 .46 .53obj1 ball .01 .45 0 .46 .96obj2 .01 0 .01 .02 .04Table 3: Application of utterance the red ball,where obj1 is the referred objectAs these numbers show, the model behaves asexpected: up until ball, the utterance does notgive enough information to decide for either ob-ject probabilities are roughly equal, once ball isuttered obj1 is the clear winner.This illustrated how the model works in princi-ple and showed that it yields the expected resultsin a simple toy domain.
In the next section we willshow that this works in more realistic domains.5 ExperimentsOur model?s task is to predict a semantic frame,where the required slots of the frame are knownbeforehand and each slot value is predicted us-ing a separate model P (I|U).
We realise P (U |R)as a Naive Bayes classifier (NB) which counts co-occurrences of utterance features (words, bigrams,trigrams; so U is actually a tuple, not a single vari-able) and properties (but naively treats features asindependent), and which is smoothed using add-one smoothing.
As explained earlier, P (I) repre-sents a uniform distribution at the beginning of anutterance, and the posteriori of the previous step,for later words.
We also train a discriminativemodel, P (R|U), using a maximum entropy classi-fier (ME) using the same features as NB to classifyproperties.33http://opennlp.apache.org/5.1 A Non-Situated Baseline using ATISWe performed an initial test of our model usinga corpus in traditional NLU: the air travel infor-mation system (ATIS) corpus (Dahl et al 1994)using the pre-processed corpus as in (Meza-Ruizet al 2008).
In ATIS, the main task is to predictthe slot attributes (the values were simply wordsfrom the utterance); however, the GOAL slot (rep-resenting the overall utterance intent) was was al-ways present, the value of which required a predic-tion.
We tested our model?s ability to predict theGOAL slot (using very simple properties; the prop-erty of a GOAL intention is itself, i.e., the propertyof flight is flight) and found encouraging re-sults (the GOAL slot baseline is 71.6%, see (Tur etal., 2010); our NB and ME models obtained scoresof 77% and 77.9% slot value prediction accura-cies, respectively).
How our model works undermore complicated settings will now be explained.5.2 Puzzle Domain: Speech-OnlyFigure 3: ExamplePentomino Board??
?ACTION rotateOBJECT object-4RESULT clockwise??
?Figure 4: Pentoframe exampleData and Task The Pentomino domain(Ferna?ndez et al 2007) contains task-orientedconversational data; more specifically, we workedwith the corpus also used recently in (Heintze etal., 2010; Peldszus et al 2012; Kennington andSchlangen, 2012).
This corpus was collected ina Wizard-of-Oz study, where the user goal wasto instruct the computer to pick up, delete, rotateor mirror puzzle tiles on a rectangular board (asin Figure 3), and place them onto another one.For each utterance, the corpus records the state ofthe game board before the utterance, the immedi-ately preceding system action, and the intendedinterpretation of the utterance (as understoodby the Wizard) in the form of a semantic framespecifying action-type and arguments, wherethose arguments are objects occurring in thedescription of the state of the board.
The languageof the corpus is German.
An example frame isgiven in Figure 4.176The task that we want our model to perform isas follows: given information about the state ofthe world (i.e., game board), previous system ac-tion, and the ongoing utterance, predict the valuesof the frame.
To this end, three slot values needto be predicted, one of which links to the visualscene.
Each slot value will be predicted by an in-dividual instantiation of our model (i.e., each hasa different I to predict).
Generally, we want ourmodel to learn how language connects to the world(given discourse context, visual context, domaincontext, etc.).
We used a combination of visualproperties (color, shape, and board position), andsimple properties to ground the utterance with I .Our model gives probability distributions overall possible slot values, but as we are interestedin single best candidates (or the special valueunknown if no guess can be made yet), we ap-plied an additional decision rule to the output ofour model.
If the probability of the highest candi-date is below a threshold, unknown is returned,otherwise that candidate is returned.
Ties are bro-ken by random selection.
The thresholds for eachslot value were determined empirically on held-out data so that a satisfactory trade-off betweenletting through wrong predictions and changingcorrect results to unknown was achieved.Procedure All results were obtained by aver-aging the results of a 10-fold validation on 1489Pento boards (i.e., utterances+context, as in (Ken-nington and Schlangen, 2012)).
We used a sep-arate set of 168 boards for small-scale, held-outexperiments.
As this data set has been usedin previous work, we use previous results asbaselines/comparisons.
For incremental process-ing, we used InproTK (Baumann and Schlangen,2012).4On the incremental level, we followed(Schlangen et al 2009) and (Kennington andSchlangen, 2012) for evaluation, but use a subsetof their incremental metrics, with a modificationon the edit overhead:first correct: how deep into the utterance do wemake the first correct guess?first final: how deep into the utterance do wemake the correct guess, and don?t subsequentlychange our minds?edit overhead: what is the ratio of unnecessaryedits / sentence length, where the only necessaryedit is that going from unknown to the final,4http://sourceforge.net/projects/inprotk/correct result anywhere in the sentence)?Results The results for full utterances are givenin Table 4.
Both of our model types work betterthan (Heintze et al 2010) which used support vec-tor machines and conditional random fields, and(Peldszus et al 2012) which was rule-based (butdid not include utterances with pronouns like wedo here).
The NB version did not work well incomparison to (Kennington and Schlangen, 2012)which used MLN, but the ME version did in mostmetrics.
Overall these are nice results as theyare achieved using a more straightforward modelwith rather simple features (with room for exten-sions).
Another welcome result is performancefrom noisy data (trained and evaluated on automat-ically transcribed speech; ASR); the ME version ofour model is robust and performs well in compar-ison to previous work.NB ME K H Pfscore 81.16 92.26 92.18 76.9(74.5) (89.4) (86.8)slot 73.62 88.91 88.88(66.4) (85.1) (81.6)frame 42.57 74.08 74.76(34.2) (67.2) (61.2)action 80.05 93.62 92.62object 76.27 90.79 84.71 64.3result 64.4 82.34 86.65Table 4: Comparison of results from Pento: NaiveBayes NB, Maximum Entropy ME, (Kenningtonand Schlangen, 2012) K, (Heintze et al 2010)H, (Peldszus et al 2012) P; values in parenthe-ses denote results from automatically transcribedspeech.A big difference between our current modeland MLN is the way incrementality is realised:MLN was restart incremental in that at each incre-ment, features from the full utterance prefix wereused, not just the latest word; the present model isfully incremental in that a prior belief is updatedbased only on the new information.
This, how-ever, seems to lead our model to perform with lessaccuracy for the result slot, which usually oc-curs at the end of the sentence.Incremental Table 5 shows the incrementalresults in the same way as (Kennington andSchlangen, 2012).
Utterances are binned intoshort, normal, and long utterance lengths (1-6,7-8, 9-17 words, respectively) as determined bylooking at the distribution of utterance lengths,which appeared as a normal distribution with 7 and177das graue Teil in der ersten Reihe nehmenFigure 5: Example of reference resolution for the utterance: das graue Teil in der ersten Reihe nehmen /the gray piece in the first row take; lighter cell background means higher probability assigned to piece.8-word utterances having highest representation.In comparison with (Kennington and Schlangen,2012), our model generally takes longer to cometo a first correct for action, but is earlier for theother two slots.
For first final, our model alwaystakes longer, albeit with lower edit overhead.
Thistells us that our model is more careful than theMLN one; it waits longer before making a final de-cision and it doesn?t change its mind as much inthe process, which arguably is desired behaviourfor incremental systems.action 1-6 7-8 9-14first correct (% into utt.)
5.78 2.56 3.64first final (% into utt.)
38.26 36.10 30.84edit overhead 2.37object 1-6 7-8 9-14first correct (% into utt.)
7.39 7.5 10.11first final (% into utt.)
44.7 44.18 35.55edit overhead 4.6result 1-6 7-8 9-14first correct (% into utt.)
15.16 23.23 20.88first final (% into utt.)
42.55 40.57 35.21edit overhead 10.19Table 5: Incremental Results for Pento slots withvarying sentence lengths.Figure 5 illustrates incremental performance byshowing the distribution over the pieces (using theME model; lighter means higher probability) forthe utterance das graue Teil in der ersten Reihenehmen (the gray piece in the first row take / takethe gray piece in the first row) for each word inthe utterance.
When the first word, das is uttered,it already assigns probabilities to the pieces withsome degree of confidence (note that in German,das (the) denotes the neuter gender, and the pieceon the right with the lowest probability is often re-ferred to by a noun (Treppe) other than neuter).Once graue (gray) is uttered, the distribution isnow more even upon the three gray pieces, whichremains largely the same when Teil (piece) is ut-tered.
The next two words, in der (in the) givemore probability to the left gray piece, but once er-sten Reihe (first row) is uttered, the most probablepiece becomes the correct one, the second piecefrom the left on the top.5.3 Puzzle Domain: Speech, Gaze and DeixisData and Task Our final experiment uses newlycollected data (Kousidis et al 2013), again fromthe Pentomino domain.
In this Wizard-of-Ozstudy, the participant was confronted with a Pentogame board containing 15 pieces in random col-ors, shapes, and positions, where the pieces weregrouped in the four corners of the screen (exam-ple in Figure 6).
The users were seated at a tablein front of the screen.
Their gaze was then cali-brated with an eye tracker (Seeingmachines Face-Lab) placed above the screen and their arm move-ments (captured by a Microsoft Kinect, also abovethe screen) were calibrated by pointing to eachcorner of the screen, then the middle of the screen.They were then given task instructions: (silently)choose a Pento tile on the screen and then instructthe computer game system to select this piece bydescribing and pointing to it.
When a piece was se-lected (by the wizard), the participant had to uttera confirmation (or give negative feedback) and anew board was generated and the process repeated(each instance is denoted as an episode).
The ut-terances, board states, arm movements, and gazeinformation were recorded, as in (Kousidis et al2012).
The wizard was instructed to elicit point-ing gestures by waiting to select the participant-referred piece by several seconds, unless a point-ing action by the participant had already occurred.When the wizard misunderstood, or a technicalproblem arose, the wizard had an option to flagthe episode.
In total, 1214 episodes were recordedfrom 8 participants (all university students).
Allbut one were native speakers; the non-native spokeproficient German (see Appendix for a set of ran-dom example utterances).The task in this experiment was reference res-olution (i.e., filling a single-slot frame).
The in-formation available to our model for these datainclude the utterance (ASR-transcribed and repre-sented as words, bigrams, and trigrams), the vi-178Figure 6: Example Pento board for gaze and deixisexperiment; yellow piece in the top-right quadranthas been ?selected?
by the wizard after the partic-ipant utterance.sual context (game board), gaze information, anddeixis (pointing) information, where a rule-basedclassifier predicted from the motion capture datathe quadrant of the screen at which the participantwas pointing.
These data were very noisy (andhence, realistic) despite the constrained conditionsof the task: the participants were not required tosay things a certain way (as long as it was under-stood by the wizard); their hand movements poten-tially covered their faces which interfered with theeye tracker; each participant had a different way ofpointing (each had their own gesture space, hand-edness, distance of hand from body when point-ing, alignment of hand with face, etc.).
Also, theepisodes were not split into individual utterances,but rather interpreted as one; this indicates that themodel can deal with belief tracking over whole in-teractions (here, if the wizard did not respond, theparticipant had to clarify her intent in some way,producing a new utterance).Procedure Removing the flagged utterances andthe utterances of one of the participants (who hadmisunderstood the task) left us with a total of 1051utterances.
We used 951 for development (fine-tuning of parameters, see below), and 100 for eval-uation.
Evaluation was leave-one-out (i.e., 100fold cross validation) where the training data wereall other 1050 utterances.
For this experiment, weonly used the ME model as it performed much bet-ter in the previous experiment.
We give resultsas resolution accuracy.
We incorporate gaze anddeixis information in two ways: (1) We computedthe distribution over tiles gazed at, and quadrantof the screen pointed at during the interval beforeand during an utterance.
The distributions werethen combined at the end of the utterance with theNLU distribution (denoted as Gaze and Point); thatis, Gaze and Point had their own P (I) which wereevenly interpolated with the INLU P (I|U), and (2)we incrementally computed properties to be pro-vided to our INLU model; i.e., a tile has a prop-erty in R of being looked at if it is gazed at forsome interval of time, or tiles in a quadrant of thescreen have the property of being pointed at.These models are denoted as Gaze-F and Point-F.As an example, Figure 7 shows an example utter-ance, gaze, and gesture activity over time and howthey are reflected in the model (the utterance is theobserved U , where the gaze and gesture becomeproperties in R for the tiles that they affect).
Ourbaseline model is the NLU without using gaze ordeixis information; random accuracy is 7%.We also include the percentage of the timethe gold tile is in the top 2 and top 4 rankings(out of 15); situations in which a dialogue sys-tem could at least provide alternatives in a clar-ification request (if it could detect that it shouldhave low confidence in the best prediction; whichwe didn?t investigate here).
Importantly, these re-sults are achieved with automatically transcribedutterances; hand transcriptions do not yet exist forthese data.
For gaze, we also make the naive as-sumption that over the utterance the participant(who in this case is the speaker) will gaze at hischosen intended tile most of the time.Figure 7: Human activity (top) aligned with howmodalities are reflected in the model for Gaze-Fand Point-F (bottom) over time for example utter-ance: take the yellow tile.Results See Table 6 for results.
The models thathave access to gaze and pointing gestures can re-solve better than those that do not.
Our findingsare consistent in that referential success with gazealone approaches 20% (a rate found by (Pfeiffer,2010) in a different setting).
Another interest-ing result is that the Gaze-F and Point-F variants,that continuously integrate multi-modal informa-tion, perform the same as or better than their non-incremental counterparts (where the distributionsare weighted once at the end of the utterance).179Version Acc Top 2 Top 4Gaze 18%(baseline) NLU 50% 59% 77%NLU + Gaze 53% 62% 80%NLU + Point 52% 65% 90%NLU + Gaze + Point 53% 70% 91%NLU + Gaze-F 53% 65% 78%NLU + Point-F 57% 68% 88%NLU+Gaze-F+Point-F 56% 69% 85%Table 6: Accuracies for reference resolution taskwhen considering NLU, gaze and pointing infor-mation before and during the utterance (Gaze andPoint), and gaze and pointing information whenconsidered as properties to the NLU model (Gaze-F and Point-F).Incremental We also include incremental re-sults when using gaze and deixis.
We binned thesentences in the same way as in the previous ex-periment (the distribution of sentence lengths wassimilar).
Figure 8 shows how the NLU model base-line, the (NLU+) Gaze-F, Point-F, and Gaze-F +Point-F models perform incrementally for utter-ances of lengths 7-8.
All models increase mono-tonically, except for Point-F at one point in the ut-terance and Gaze-F at the end.
It would appear thatthe gaze as an information source is a good earlyindicator of speaker intent, but should be trustedless as the utterance progresses.
Deixis is moretrustworthy overall, and the two taken together of-fer a more stable model.
Table 7 shows the re-sults using the previously explained incrementalmetrics.
All models have little edit overhead, butdon?t make the correct final decision until well intothe utterances.
This was expected due to the noisydata.
A consumer of the output of these modelswould need to wait longer to trust the results givenby the models (though the number of words of theutterance can never be known beforehand).6 Discussion and ConclusionsWe presented a model for the interpretation ofutterances in situated dialogue that a) works in-crementally and b) can ground meanings in theshared context.
Taken together, the three experi-ments we?ve reported give good evidence that ourmodel has the potential to be used as a success-ful NLU component of an interactive dialogue sys-tem.
Our model can process at a speed which isfaster than the ongoing utterance, which will al-low it to be useful in real-time, interactive exper-iments.
And, crucially, our model is able to inte-Figure 8: Incremental process for referential accu-racy; comparing NLU, Gaze-F, Point-F, and Gaze-F + Point-F for utterances of length 7-8.NLU 1-6 7-8 9-14first correct (% into utt.)
22.2 37.2 30first final (% into utt.)
82.4 82.4 74.8edit overhead 2.95Gaze-F 1-6 7-8 9-14first correct (% into utt.)
23 32 31.1first final (% into utt.)
84.1 81.5 75.4edit overhead 2.89Point-F 1-6 7-8 9-14first correct (% into utt.)
21.4 30 23.3first final (% into utt.)
83.5 80 72.3edit overhead 2.59Gaze-F + Point-F 1-6 7-8 9-14first correct (% into utt.)
16.7 31 28first final (% into utt.)
81.5 81 73.9edit overhead 2.67Table 7: Incremental results for Pento slots withvarying sentence lengths.grate information from various sources, includinggaze and deixis.
We expect the model to scale tolarger domains; the number of computations thatare required grows with |I| ?
|R|.Our model makes use of properties which areused to connect an utterance to an intention.Knowing which properties to use requires empir-ical testing to determine which ones are useful.We are working on developing principled meth-ods for selecting such properties and their con-tribution (i.e., properties should not be uniform).Future work also includes better use of linguistics(instead of just n-grams), building a more sophis-ticated DBN model that has fewer independenceassumptions, e.g.
tracking properties as well bymaking Rt depended on Rt?1.
We are also inthe process of using the model interactively; as aproof-of-concept, we were trivially able to plug itinto an existing dialogue manager for Pento do-mains (see (Bu?
et al 2010)).180Acknowledgements: Thanks to the anony-mous reviewers for their useful comments andfeedback.
This work was partially funded througha DFG Emmy Noether grant.Appendix A: Example Utterances (PentoSpeech)1. nimm die Bru?cke in der oberen Reihe2.
nimm das Teil in der mittleren Reihe das zweiteTeil in der mittleren Reihe3.
und setz ihn in die Mitte links4.
dreh das nach links5.
a?hm und setz ihn oben links in die Ecke6.
nimm bitte den gelben Winkel oben7.
bewege das Ka?stchen die Treppe unten links8.
lo?sche das Teil in der Mitte9.
nimm die gelbe Kru?cke aus der zweiten Reiheoben10.
und verschiebe es in die erste Zeile dritteSpalteAppendix B: Example Utterances (Speech,Gaze and Deixis)(as recognised by the ASR)1. dieses teil genau st es oben links t2.
das t mit vier rechts oben ist d es direkt hierrechts3.
gru?ne von rechts uh fla?che4.
das obere gru?ne za?hl hm so es obersten hohlese rechts oben ecke5.
a?hm das hintere kreuz unten links rechts rechts6.
a?h das einzige blaue symbol oben rechts7.
das einzige gru?n okay oben rechts8.
hm innerhalb diesem blauen striche vorne hmso genau in die genau rechts9.
und das sind dann nehmen diese fu?nf zeichenoben na?mlich genau das in der mitte so10.
oben links is die untereReferencesGregory Aist, James Allen, Ellen Campana, Car-los Gomez Gallo, Scott Stoness, Mary Swift, andMichael K Tanenhaus.
2007.
Incremental under-standing in human-computer dialogue and experi-mental evidence for advantages over nonincremen-tal methods.
In Proceedings of Decalog (Semdial2007), Trento, Italy.Timo Baumann and David Schlangen.
2012.
The In-proTK 2012 Release.
In NAACL.Okko Bu?
Timo Baumann, and David Schlangen.2010.
Collaborating on Utterances with a SpokenDialogue System Using an ISU-based Approach toIncremental Dialogue Management.
In Proceedingsof SIGdial, pages 233?236.Deborah A Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the ATIStask: the ATIS-3 corpus.
In Proceedings of theworkshop on Human Language Technology, HLT?94, pages 43?48, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Renato De Mori, Frederic Be?chet, Dilek Hakkani-tu?r,Michael Mctear, Giuseppe Riccardi, and GokhanTur.
2008.
Spoken Language Understanding.
IEEESignal Processing Magazine, pages 50?58, May.David DeVault, Kenji Sagae, and David Traum.
2009.Can I finish?
: learning when to respond to incremen-tal interpretation results in interactive dialogue.
InProceedings of the 10th SIGdial, pages 11?20.
As-sociation for Computational Linguistics.David DeVault, Kenji Sagae, and David Traum.
2011.Incremental Interpretation and Prediction of Utter-ance Meaning for Interactive Dialogue.
Dialogue &Discourse, 2(1):143?170.Raquel Ferna?ndez, Tatjana Lucht, and DavidSchlangen.
2007.
Referring under restrictedinteractivity conditions.
In Proceedings of the 8thSIGdial, pages 136?139.Silvan Heintze, Timo Baumann, and David Schlangen.2010.
Comparing local and sequential models forstatistical incremental natural language understand-ing.
In Proceedings of the 11th SIGdial, pages 9?16.Association for Computational Linguistics.Kai-yuh Hsiao, Soroush Vosoughi, Stefanie Tellex,Rony Kubat, and Deb Roy.
2008.
Object schemasfor grounding language in a responsive robot.
Con-nection Science2, 20(4):253?276.Guangpu Huang and Meng Joo Er.
2010.
A HybridComputational Model for Spoken Language Under-standing.
In 11th International Conference on Con-trol, Automation, Robotics, and Vision, pages 7?10,Singapore.
IEEE.Casey Kennington and David Schlangen.
2012.Markov Logic Networks for Situated IncrementalNatural Language Understanding.
In Proceedingsof the 13th SIGdial, pages 314?323, Seoul, SouthKorea, July.
Association for Computational Linguis-tics.Spyros Kousidis, Thies Pfeiffer, Zofia Malisz, PetraWagner, and David Schlangen.
2012.
Evaluat-ing a minimally invasive laboratory architecture forrecording multimodal conversational data.
In Proc.of the Interdisciplinary Workshop on Feedback Be-haviours in Dialogue.181Spyros Kousidis, Casey Kennington, and DavidSchlangen.
2013.
Investigating speaker gaze andpointing behaviour in human-computer interactionwith the mint.tools collection.
In Proceedings of the14th SIGdial.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning Dependency-Based Compositional Se-mantics.
In Proceedings of the 49th ACLHLT, pages590?599, Portland, Oregon.
Association for Compu-tational Linguistics.Changsong Liu, Rui Fang, and Joyce Chai.
2012.
To-wards Mediating Shared Perceptual Basis in Situ-ated Dialogue.
In Proceedings of the 13th SIGdial,pages 140?149, Seoul, South Korea, July.
Associa-tion for Computational Linguistics.Yi Ma, Antoine Raux, Deepak Ramachandran, andRakesh Gupta.
2012.
Landmark-Based LocationBelief Tracking in a Spoken Dialog System.
In Pro-ceedings of the 13th SIGdial, pages 169?178, Seoul,South Korea, July.
Association for ComputationalLinguistics.Marie-Jean Meurs, Frederic Duvert, Fabrice Lefevre,and Renato De Mori.
2008.
Markov Logic Net-works for Spoken Language Interpretation.
Infor-mation Systems Journal, pages 535?544.Marie-Jean Meurs, Fabrice Lefe`vre, and Renato DeMori.
2009.
Spoken Language Interpretation: Onthe Use of Dynamic Bayesian Networks for Seman-tic Composition.
In IEEE International Conferenceon Acoustics, Speech, and Signal Processing, pages4773?4776.Ivan Meza-Ruiz, Sebastian Riedel, and Oliver Lemon.2008.
Accurate Statistical Spoken Language Un-derstanding from Limited Development Resources.In IEEE International Conference on Acoustics,Speech, and Signal Processing, pages 5021?5024.IEEE.Andreas Peldszus, Okko Bu?, Timo Baumann, andDavid Schlangen.
2012.
Joint Satisfaction of Syn-tactic and Pragmatic Constraints Improves Incre-mental Spoken Language Understanding.
In Pro-ceedings of the 13th EACL, pages 514?523, Avi-gnon, France, April.
Association for ComputationalLinguistics.Thies Pfeiffer.
2010.
Understanding multimodal deixiswith gaze and gesture in conversational interfaces.Ph.D.
thesis, Bielefeld University.Zahar Prasov and Joyce Y Chai.
2010.
Fusing EyeGaze with Speech Recognition Hypotheses to Re-solve Exophoric References in Situated Dialogue.In EMNLP 2010, number October, pages 471?481.Deb Roy.
2005.
Grounding words in perception andaction: computational insights.
Trends in CognitiveSciences, 9(8):389?396, August.David Schlangen and Gabriel Skantze.
2009.
A Gen-eral, Abstract Model of Incremental Dialogue Pro-cessing.
In Proceedings of the 10th EACL, pages710?718, Athens, Greece.
Association for Compu-tational Linguistics.David Schlangen, Timo Baumann, and Michaela At-terer.
2009.
Incremental Reference Resolution: TheTask, Metrics for Evaluation, and a Bayesian Filter-ing Model that is Sensitive to Disfluencies.
In Pro-ceedings of the 10th SIGdial, pages 30?37, London,UK.
Association for Computational Linguistics.Alexander Siebert and David Schlangen.
2008.
A Sim-ple Method for Resolution of Definite Reference ina Shared Visual Context.
In Proceedings of the 9thSIGdial, pages 84?87, Columbus, Ohio.
Associationfor Computational Linguistics.Gokhan Tur, Dilek Hakkani-tu?r, and Larry Heck.
2010.What Is Left to Be Understood by ATIS?
In IEEEWorkshop on Spoken Language Technologies, pages19?24, Berkeley, California.
IEEE.Ye-Yi Wang, Li Deng, and Alex Acero.
2011.
Seman-tic Frame-based Spoken Language Understanding.Wiley.Jason D Williams.
2010.
Incremental partition re-combination for efficient tracking of multiple dia-log states.
Acoustics Speech and Signal ProcessingICASSP 2010, pages 5382?5385.Luke S Zettlemoyer and Michael Collins.
2007.
On-line Learning of Relaxed CCG Grammars for Pars-ing to Logical Form.
Computational Linguistics,pages 678?687.Luke S Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
Proceedings of the JointConference of the 47th ACL and the 4th AFNLP:Volume 2 - ACL-IJCNLP ?09, 2:976.182
