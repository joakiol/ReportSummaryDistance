Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 110?117,COLING 2010, Beijing, August 2010.New Parameterizations and Features for PSCFG-Based MachineTranslationAndreas Zollmann Stephan VogelLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University{zollmann,vogel}@cs.cmu.eduAbstractWe propose several improvements to thehierarchical phrase-based MT model ofChiang (2005) and its syntax-based exten-sion by Zollmann and Venugopal (2006).We add a source-span variance modelthat, for each rule utilized in a prob-abilistic synchronous context-free gram-mar (PSCFG) derivation, gives a confi-dence estimate in the rule based on thenumber of source words spanned by therule and its substituted child rules, withthe distributions of these source span sizesestimated during training time.We further propose different methods ofcombining hierarchical and syntax-basedPSCFG models, by merging the grammarsas well as by interpolating the translationmodels.Finally, we compare syntax-augmentedMT, which extracts rules based on target-side syntax, to a corresponding variantbased on source-side syntax, and experi-ment with a model extension that jointlytakes source and target syntax into ac-count.1 IntroductionThe Probabilistic Synchronous Context FreeGrammar (PSCFG) formalism suggests an intu-itive approach to model the long-distance and lex-ically sensitive reordering phenomena that oftenoccur across language pairs considered for statis-tical machine translation.
As in monolingual pars-ing, nonterminal symbols in translation rules areused to generalize beyond purely lexical opera-tions.
Labels on these nonterminal symbols areoften used to enforce syntactic constraints in thegeneration of bilingual sentences and imply con-ditional independence assumptions in the statis-tical translation model.
Several techniques havebeen recently proposed to automatically iden-tify and estimate parameters for PSCFGs (or re-lated synchronous grammars) from parallel cor-pora (Galley et al, 2004; Chiang, 2005; Zollmannand Venugopal, 2006; Liu et al, 2006; Marcu etal., 2006).In this work, we propose several improvementsto the hierarchical phrase-based MT model ofChiang (2005) and its syntax-based extension byZollmann and Venugopal (2006).
We add a sourcespan variance model that, for each rule utilizedin a probabilistic synchronous context-free gram-mar (PSCFG) derivation, gives a confidence es-timate in the rule based on the number of sourcewords spanned by the rule and its substituted childrules, with the distributions of these source spansizes estimated during training (i.e., rule extrac-tion) time.We further propose different methods of com-bining hierarchical and syntax-based PSCFGmodels, by merging the grammars as well as byinterpolating the translation models.Finally, we compare syntax-augmented MT,which extracts rules based on target-side syntax,to a corresponding variant based on source-sidesyntax, and experiment with a model extensionbased on source and target syntax.We evaluate the different models on theNIST large resource Chinese-to-English transla-tion task.1102 Related workChiang et al (2008) introduce structural dis-tortion features into a hierarchical phrase-basedmodel, aimed at modeling nonterminal reorderinggiven source span length, by estimating for eachpossible source span length ` a Bernoulli distribu-tion p(R|`) where R takes value one if reorder-ing takes place and zero otherwise.
Maximum-likelihood estimation of the distribution amountsto simply counting the relative frequency of non-terminal reorderings over all extracted rule in-stances that incurred a substitution of span length`.
In a more fine-grained approach they add a sep-arate binary feature ?R, `?
for each combination ofreordering truth value R and span length ` (whereall ` ?
10 are merged into a single value), andthen tune the feature weights discriminatively on adevelopment set.
Our approach differs from Chi-ang et al (2008) in that we estimate one sourcespan length distribution for each substitution siteof each grammar rule, resulting in unique distri-butions for each rule, estimated from all instancesof the rule in the training data.
This enables ourmodel to condition reordering range on the in-dividual rules used in a derivation, and even al-lows to distinguish between two rules r1 and r2that both reorder arguments with identical meanspan lengths `, but where the span lengths encoun-tered in extracted instances of r1 are all close to `,whereas span length instances for r2 vary widely.Chen and Eisele (2010) propose a hypbrid ap-proach between hierarchical phrase based MTand a rule based MT system, reporting improve-ment over each individual model on an English-to-German translation task.
Essentially, the rulebased system is converted to a single-nonterminalPSCFG, and hence can be combined with thehierarchical model, another single-nonterminalPSCFG, by taking the union of the rule setsand augmenting the feature vectors, adding zero-values for rules that only exist in one of the twogrammars.
We face the challenge of combiningthe single-nonterminal hierarchical grammar witha multi-nonterminal syntax-augmented grammar.Thus one hierarchical rule typically correspondsto many syntax-augmented rules.
The SAMT sys-tem used by Zollmann et al (2008) adds hierar-chical rules separately to the syntax-augmentedgrammar, resulting in a backbone grammar ofwell-estimated hierarchical rules supporting thesparser syntactic rules.
They allow the modelpreference between hierarchical and syntax rulesto be learned from development data by addingan indicator feature to all rules, which is onefor hierarchical rules and zero for syntax rules.However, no empirical comparison is given be-tween the purely syntax-augmented and the hy-brid grammar.
We aim to fill this gap by experi-menting with both models, and further refine thehybrid approach by adding interpolated probabil-ity models to the syntax rules.Chiang (2010) augments a hierarchical phrase-based MT model with binary syntax features rep-resenting the source and target syntactic con-stituents of a given rule?s instantiations duringtraining, thus taking source and target syntaxinto account while avoiding the data-sparsenessand decoding-complexity problems of multi-nonterminal PSCFG models.
In our approach, thesource- and target-side syntax directly determinesthe grammar, resulting in a nonterminal set de-rived from the labels underlying the source- andtarget-language treebanks.3 PSCFG-based translationGiven a source language sentence f , statisticalmachine translation defines the translation task asselecting the most likely target translation e undera model P (e|f), i.e.:e?
(f) = argmaxeP (e|f) = argmaxem?i=1hi(e, f)?iwhere the argmax operation denotes a searchthrough a structured space of translation outputsin the target language, hi(e, f) are bilingual fea-tures of e and f and monolingual features ofe, and weights ?i are typically trained discrim-inatively to maximize translation quality (basedon automatic metrics) on held out data, e.g., us-ing minimum-error-rate training (MERT) (Och,2003).In PSCFG-based systems, the search space isstructured by automatically extracted rules thatmodel both translation and re-ordering operations.111Most large scale systems approximate the searchabove by simply searching for the most likelyderivation of rules, rather than searching for themost likely translated output.
There are efficientalgorithms to perform this search (Kasami, 1965;Chappelier and Rajman, 1998) that have been ex-tended to efficiently integrate n-gram languagemodel features (Chiang, 2007; Venugopal et al,2007; Huang and Chiang, 2007; Zollmann et al,2008; Petrov et al, 2008).In this work we experiment with PSCFGsthat have been automatically learned from word-aligned parallel corpora.
PSCFGs are defined by asource terminal set (source vocabulary) TS , a tar-get terminal set (target vocabulary) TT , a sharednonterminal set N and rules of the form: X ??
?, ?,w?
where?
X ?
N is a labeled nonterminal referred to asthe left-hand-side of the rule.?
?
?
(N ?
TS)?
is the source side of the rule.?
?
?
(N ?
TT )?
is the target side of the rule.?
w ?
[0,?)
is a non-negative real-valuedweight assigned to the rule; in our model, w isthe exponential function of the inner product offeatures h and weights ?.3.1 Hierarchical phrase-based MTBuilding upon the success of phrase-based meth-ods, Chiang (2005) presents a PSCFG model oftranslation that uses the bilingual phrase pairsof phrase-based MT as starting point to learnhierarchical rules.
For each training sentencepair?s set of extracted phrase pairs, the set of in-duced PSCFG rules can be generated as follows:First, each phrase pair is assigned a generic X-nonterminal as left-hand-side, making it an initialrule.
We can now recursively generalize each al-ready obtained rule (initial or including nontermi-nals)N ?
f1 .
.
.
fm/e1 .
.
.
enfor which there is an initial ruleM ?
fi .
.
.
fu/ej .
.
.
evwhere 1 ?
i < u ?
m and 1 ?
j < v ?
n, toobtain a new ruleN ?
f i?11 Xkfmu+1/ej?11 Xkenv+1where e.g.
f i?11 is short-hand for f1 .
.
.
fi?1, andwhere k is an index for the nonterminal X thatindicates the one-to-one correspondence betweenthe new X tokens on the two sides (it is not inthe space of word indices like i, j, u, v,m, n).
Therecursive form of this generalization operation al-lows the generation of rules with multiple nonter-minal pairs.Chiang (2005) uses features analogous to theones used in phrase-based translation: a lan-guage model neg-log probability, a ?rule givensource-side?
neg-log-probability, a ?rule giventarget-side?
neg-log-probability, source- and tar-get conditioned ?lexical?
neg-log-probabilitiesbased on word-to-word co-occurrences (Koehn etal., 2003), as well as rule, target word, and glueoperation counters.
We follow Venugopal andZollmann (2009) to further add a rareness penalty,1/ count(r)where count(r) is the occurrence count of ruler in the training corpus, allowing the system tolearn penalization of low-frequency rules, as wellas three indicator features firing if the rule hasone, two unswapped, and two swapped nontermi-nal pairs, respectively.13.2 Syntax Augmented MTSyntax Augmented MT (SAMT) (Zollmann andVenugopal, 2006) extends Chiang (2005) to in-clude nonterminal symbols from target languagephrase structure parse trees.
Each target sentencein the training corpus is parsed with a stochas-tic parser to produce constituent labels for targetspans.
Phrase pairs (extracted from a particularsentence pair) are assigned left-hand-side nonter-minal symbols based on the target side parse treeconstituent spans.Phrase pairs whose target side corresponds toa constituent span are assigned that constituent?slabel as their left-hand-side nonterminal.
If thetarget side of the phrase pair is not spanned bya single constituent in the corresponding parsetree, we use the labels of subsuming, subsumed,and neighboring parse tree constituents to assign1Penalization or reward of purely-lexical rules can be in-directly learned by trading off these features with the rulecounter feature.112an extended label of the form C1 + C2, C1/C2,or C2\C1 (the latter two being motivated fromthe operations in combinatory categorial gram-mar (CCG) (Steedman, 2000)), indicating that thephrase pair?s target side spans two adjacent syn-tactic categories (e.g., she went: NP+VB), a par-tial syntactic category C1 missing aC2 at the right(e.g., the great: NP/NN), or a partial C1 missinga C2 at the left (e.g., great wall: DT\NP), respec-tively.
The label assignment is attempted in the or-der just described, i.e., assembling labels based on?+?
concatenation of two subsumed constituents ispreferred, as smaller constituents tend to be moreaccurately labeled.
If no label is assignable by ei-ther of these three methods, a default label ?FAIL?is assigned.In addition to the features used in hierarchicalphrase-based MT, SAMT introduces a relative-frequency estimated probability of the rule givenits left-hand-side nonterminal.4 Modeling Source Span Length ofPSCFG Rule Substitution SitesExtracting a rule with k right-hand-side nonter-minal pairs, i.e., substitution sites, (from now oncalled order-k rule) by the method described inSection 3 involves k + 1 phrase pairs: one phrasepair used as initial rule and k phrase pairs that aresub phrase pairs of the first and replaced by non-terminal pairs.
Conversely, during translation, ap-plying this rule amounts to combining k hypothe-ses from k different chart cells, each representedby a source span and a nonterminal, to form a newhypothesis and file it into a chart cell.
Intuitively,we want the source span lengths of these k + 1chart cells to be close to the source side lengths ofthe k+1 phrase pairs from the training corpus thatwere involved in extracting the rule.
Of course,each rule generally was extracted from multipletraining corpus locations, with different involvedphrase pairs of different lengths.
We thereforemodel k + 1 source span length distributions foreach order-k rule in the grammar.Ignoring the discreteness of source span lengthfor the sake of easier estimation, we assume thedistribution to be log-normal.
This is motivatedby the fact that source span length is positive andthat we expect its deviation between instances ofthe same rule to be greater for long phrase pairsthan for short ones.We can now add k?
+ 1 features to the transla-tion framework, where k?
is the maximum num-ber of PSCFG rule nonterminal pairs, in our casetwo.
Each feature is computed during translationtime.
Ideally, it should represent the probabil-ity of the hypothesized rule given the respectivechart cell span length.
However, as each com-peting rule underlies a different distribution, thiswould require a Bayesian setting, in which priorsover distributions are specified.
In this prelimi-nary work we take a simpler approach: Based onthe rule?s span distribution, we compute the prob-ability that a span length no likelier than the oneencountered was generated from the distribution.This probability thus yields a confidence estimatefor the rule.
More formally, let ?
be the mean and?
the standard deviation of the logarithm of thespan length random variableX concerned, and letx be the span length encountered during decoding.Then the computed confidence estimate is givenbyP (| ln(X)?
?| ?
| ln(x)?
?|)= 2 ?
Z (?
(| ln(x)?
?|)/?
)where Z is the cumulative density function of thenormal distribution with mean zero and varianceone.The confidence estimate is one if the encoun-tered span length is equal to the mean of the dis-tribution, and decreases as the encountered spanlength deviates further from the mean.
The sever-ity of that decline is determined by the distributionvariance: the higher the variance, the less a devia-tion from the mean is penalized.Mean and variance of log source span length aresufficient statistics of the log-normal distribution.As we extract rules in a distributed fashion, weuse a straightforward parallelization of the onlinealgorithm of Welford (1962) and its improvementby West (1979) to compute the sample varianceover all instances of a rule.1135 Merging a Hierarchical and aSyntax-Based ModelWhile syntax-based grammars allow for more re-fined statistical models and guide the search byconstraining substitution possibilitites in a gram-mar derivation, grammar sizes tend to be muchgreater than for hierarchical grammars.
Thereforethe average occurrence count of a syntax rule ismuch lower than that of a hierarchical rule, andthus estimated probabilitites are less reliable.We propose to augment the syntax-based ?rulegiven source side?
and ?rule given target side?
dis-tributions by hierarchical counterparts obtained bymarginalizing over the left-hand-side and right-hand-side rule nonterminals.
For example, thehierarchical equivalent of the ?rule given sourceside?
probability is obtained by summing occur-rence counts over all rules that have the samesource and target terminals and substitution posi-tions but possibly differ in the left- and/or right-hand side nonterminal labels, divided by the sumof occurrence counts of all rules that have thesame source side terminals and source side substi-tution positions.
Similarly, an alternative rarenesspenalty based on the combined frequency of allrules with the same terminals and substitution po-sitions is obtained.Using these syntax and hierarchical featuresside by side amounts to interpolation of the re-spective probability models in log-space, withminimum-error-rate training (MERT) determiningthe optimal interpolation coefficient.
We also addrespective models interpolated with coefficient .5in probability-space as additional features to thesystem.We further experiment with adding hierarchicalrules separately to the syntax-augmented gram-mar, as proposed in Zollmann et al (2008), withthe respective syntax-specific features set to zero.A ?hierarchical-indicator?
feature is added to allrules, which is one for hierarchical rules and zerofor syntax rules, allowing the joint model to tradeof hierarchical against syntactic rules.
Duringtranslation, the hierarchical and syntax worlds arebridged by glue rules, which allow monotonicconcatenation of hierarchical and syntactic partialsentence hypotheses.
We separate the glue featureused in hierarchical and syntax-augmented trans-lation into a glue feature that only fires when a hi-erarchical rule is glued, and a distinct glue featurefiring when gluing a syntax-augmented rule.6 Extension of SAMT to a bilinguallyparsed corpusSyntax-based MT models have been proposedboth based on target-side syntactic annotations(Galley et al, 2004; Zollmann and Venugopal,2006) as well source-side annotations (Liu et al,2006).
Syntactic annotations for both source andtarget language are available for popular languagepairs such as Chinese-English.
In this case, ourgrammar extraction procedure can be easily ex-tended to impose both source and target con-straints on the eligible substitutions simultane-ously.Let Nf be the nonterminal label that would beassigned to a given initial rule when utilizing thesource-side parse tree, and Ne the assigned labelaccording to the target-side parse.
Then our bilin-gual model assigns ?Nf + Ne?
to the initial rule.The extraction of complex rules proceeds as be-fore.
The number of nonterminals in this model,based on a source-model label set of size s and atarget label set of size t, is thus given by st.7 ExperimentsWe evaluate our approaches by comparing trans-lation quality according to the IBM-BLEU (Pap-ineni et al, 2002) metric on the NIST Chinese-to-English translation task using MT04 as devel-opment set to train the model parameters ?, andMT05, MT06 and MT08 as test sets.We perform PSCFG rule extraction and de-coding using the open-source ?SAMT?
system(Venugopal and Zollmann, 2009), using the pro-vided implementations for the hierarchical andsyntax-augmented grammars.
For all systems, weuse the bottom-up chart parsing decoder imple-mented in the SAMT toolkit with a reorderinglimit of 15 source words, and correspondingly ex-tract rules from initial phrase pairs of maximumsource length 15.
All rules have at most two non-terminal symbols, which must be non-consecutiveon the source side, and rules must contain at least114one source-side terminal symbol.For parameter tuning, we use the L0-regularized minimum-error-rate training tool pro-vided by the SAMT toolkit.The parallel training data comprises of 9.6Msentence pairs (206M Chinese Words, 228M En-glish words).
The source and target languageparses for the syntax-augmented grammar weregenerated by the Stanford parser (Klein and Man-ning, 2003).The results are given in Table 1.
The sourcespan models (indicated by +span) achieve smalltest set improvements of 0.15 BLEU points on av-erage for the hierarchical and 0.26 BLEU pointsfor the syntax-augmented system, but these arenot statistically significant.Augmenting a syntax-augmented grammarwith hierarchical features (?Syntax+hiermodels?
)results in average test set improvements of 0.5BLEU points.
These improvements are not sta-tistically significant either, but persist across allthree test sets.
This demonstrates the benefit ofmore reliable feature estimation.
Further aug-menting the hierarchical rules to the grammar(?Syntax+hiermodels+hierrules?)
does not yieldadditional improvements.The use of bilingual syntactic parses (?Syn-tax/src&tgt?)
turns out detrimental to translationquality.
We assume this is due to the huge numberof nonterminals in these grammars and the greatamount of badly-estimated low-occurrence-countrules.
Perhaps merging this grammar with a regu-lar syntax-augmented grammar could yield betterresults.We also experimented with a source-parsebased model (?Syntax/src?).
While not being ableto match translation quality of its target-basedcounterpart, the model still outperforms the hier-archical system on all test sets.8 ConclusionWe proposed several improvements to the hierar-chical phrase-based MT model of Chiang (2005)and its syntax-based extension by Zollmann andVenugopal (2006).
We added a source span lengthmodel that, for each rule utilized in a probabilis-tic synchronous context-free grammar (PSCFG)derivation, gives a confidence estimate in the rulebased on the number of source words spanned bythe rule and its substituted child rules, resulting insmall improvements for hierarchical phrase-basedas well as syntax-augmented MT.We further demonstrated the utility of combin-ing hierarchical and syntax-based PSCFG modelsand grammars.Finally, we compared syntax-augmented MT,which extracts rules based on target-side syntax,to a corresponding variant based on source-sidesyntax, showing that target syntax is more ben-efitial, and unsuccessfully experimented with amodel extension that jointly takes source and tar-get syntax into account.Hierarchical phrase-based MT suffers fromspurious ambiguity: A single translation for agiven source sentence can usually be accom-plished by many different PSCFG derivations.This problem is exacerbated by syntax-augmentedMT with its thousands of nonterminals, and madeeven worse by its joint source-and-target exten-sion.
Future research should apply the work ofBlunsom et al (2008) and Blunsom and Osborne(2008), who marginalize over derivations to findthe most probable translation rather than the mostprobable derivation, to these multi-nonterminalgrammars.All source code underlying this work is avail-able under the GNU Lesser General Public Li-cense as part of the ?SAMT?
system at:www.cs.cmu.edu/?zollmann/samtAcknowledgementsThis work is in part supported by NSF un-der the Cluster Exploratory program (grant NSF0844507), and in part by the US DARPA GALEprogram.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of NSF or DARPA.ReferencesBlunsom, Phil and Miles Osborne.
2008.
Probabilisticinference for machine translation.
In EMNLP ?08:Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 215?223, Morristown, NJ, USA.
Association for Com-putational Linguistics.115Dev (MT04) MT05 MT06 MT08 TestAvg TimeHierarchical 38.63 36.51 33.26 25.77 31.85 14.3Hier+span 39.03 36.44 33.29 26.26 32.00 16.7Syntax 39.17 37.17 33.87 26.81 32.62 59Syntax+hiermodels 39.61 37.74 34.30 27.30 33.11 68.4Syntax+hiermodels+hierrules 39.69 37.56 34.66 26.93 33.05 34.6Syntax+span+hiermodels+hierrules 39.81 38.02 34.50 27.41 33.31 39.6Syntax/src+span+hiermodels+hierrules 39.62 37.25 33.99 26.44 32.56 20.1Syntax/src&tgt+span+hiermodels+hierrules 39.15 36.92 33.70 26.24 32.29 17.5Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length) fordifferent systems on Chinese-English NIST-large translation tasks.
?TestAvg?
shows the average score over the three test sets.?Time?
is the average decoding time per sentence in seconds on one CPU.Blunsom, Phil, Trevor Cohn, and Miles Osborne.2008.
A discriminative latent variable model forstatistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics (ACL).Brown, Peter F., Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19(2).Chappelier, J.C. and M. Rajman.
1998.
A general-ized CYK algorithm for parsing stochastic CFG.
InProceedings of Tabulation in Parsing and Deduction(TAPD), pages 133?137, Paris.Chen, Yu and Andreas Eisele.
2010.
Hierarchical hy-brid translation between english and german.
InHansen, Viggo and Francois Yvon, editors, Pro-ceedings of the 14th Annual Conference of the Eu-ropean Association for Machine Translation, pages90?97.
EAMT, EAMT, 5.Chiang, David, Yuval Marton, and Philip Resnik.2008.
Online large-margin training of syntactic andstructural translation features.
In Proceedings ofthe 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 224?233, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Chiang, David.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL).Chiang, David.
2007.
Hierarchical phrase based trans-lation.
Computational Linguistics, 33(2).Chiang, David.
2010.
Learning to translate withsource and target syntax.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1443?1452, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Galley, Michael, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics Confer-ence (HLT/NAACL).Huang, Liang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics (ACL).Kasami, T. 1965.
An efficient recognitionand syntax-analysis algorithm for context-free lan-guages.
Technical report, Air Force Cambridge Re-search Lab.Klein, Dan and Christoper Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics (ACL).Koehn, Philipp, Franz J. Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the Human Language Technology Con-ference of the North American Chapter of the As-sociation for Computational Linguistics Conference(HLT/NAACL).Liu, Yang, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Compu-tational Linguistics.Marcu, Daniel, Wei Wang, Abdessamad Echihabi,and Kevin Knight.
2006.
SPMT: Statistical ma-chine translation with syntactified target languagephrases.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Sydney, Australia.116Och, Franz J.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics (ACL).Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL).Petrov, Slav, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation usinglanguage projections.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP).Steedman, Mark.
2000.
The Syntactic Process.
MITPress.Venugopal, Ashish and Andreas Zollmann.
2009.Grammar based statistical MT on Hadoop: An end-to-end toolkit for large scale PSCFG based MT.The Prague Bulletin of Mathematical Linguistics,91:67?78.Venugopal, Ashish, Andreas Zollmann, and StephanVogel.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In Pro-ceedings of the Human Language Technology Con-ference of the North American Chapter of the As-sociation for Computational Linguistics Conference(HLT/NAACL).Welford, B. P. 1962.
Note on a method for calculatingcorrected sums of squares and products.
Techno-metrics, 4(3):419?420.West, D. H. D. 1979.
Updating mean and varianceestimates: an improved method.
Commun.
ACM,22(9):532?535.Zollmann, Andreas and Ashish Venugopal.
2006.Syntax augmented machine translation via chartparsing.
In Proceedings of the Workshop on Sta-tistical Machine Translation, HLT/NAACL.Zollmann, Andreas, Ashish Venugopal, Franz J. Och,and Jay Ponte.
2008.
A systematic comparisonof phrase-based, hierarchical and syntax-augmentedstatistical MT.
In Proceedings of the Conference onComputational Linguistics (COLING).117
