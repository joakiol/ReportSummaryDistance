Proceedings of NAACL HLT 2009: Short Papers, pages 277?280,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsFast decoding for open vocabulary spoken term detection1B.
Ramabhadran,1A.
Sethy, 2J.
Mamou?1 B. Kingsbury, 1 U. Chaudhari1IBM T. J. Watson Research CenterYorktown Heights,NY2IBM Haifa Research LabsMount Carmel,HaifaAbstractInformation retrieval and spoken-term detec-tion from audio such as broadcast news, tele-phone conversations, conference calls, andmeetings are of great interest to the academic,government, and business communities.
Mo-tivated by the requirement for high-quality in-dexes, this study explores the effect of usingboth word and sub-word information to findin-vocabulary and OOV query terms.
It alsoexplores the trade-off between search accu-racy and the speed of audio transcription.
Wepresent a novel, vocabulary independent, hy-brid LVCSR approach to audio indexing andsearch and show that using phonetic confu-sions derived from posterior probabilities es-timated by a neural network in the retrievalof OOV queries can help in reducing misses.These methods are evaluated on data sets fromthe 2006 NIST STD task.1 IntroductionIndexing and retrieval of speech content in vari-ous forms such as broadcast news, customer caredata and on-line media has gained a lot of interestfor a wide range of applications from market in-telligence gathering, to customer analytics and on-line media search.
Spoken term detection (STD) isa key information retrieval technology which aimsopen vocabulary search over large collections ofspoken documents.
An approach for solving the out-of-vocabulary (OOV) issues (Saraclar and Sproat,2004) consists of converting speech into phonetic,?TThe work done by J. Mamou was partially funded by theEU projects SAPIR and HERMESsyllabic or word-fragment transcripts and represent-ing the query as a sequence of phones, syllables orword-fragments respectively.
Popular approachesinclude subword decoding (Clements et al, 2002;Mamou et al, 2007; Seide et al, 2004; Siohan andBacchiani, 2005) and representations enhanced withphone confusion probabilities and approximate sim-ilarity measures (Chaudhari and Picheny, 2007).2 Fast Decoding ArchitectureThe first step in converting speech to a searchable in-dex involves the use of an ASR system that producesword, word-fragment or phonetic transcripts.
Inthis paper, the LVCSR system is a discriminativelytrained speaker-independent recognizer using PLP-derived features and a quinphone acoustic modelwith approximately 1200 context dependent statesand 30000 Gaussians.
The acoustic model is trainedon 430 hours of audio from the 1996 and 1997 En-glish Broadcast News Speech corpus (LDC97S44,LDC98S71) and the TDT4 Multilingual BroadcastNews Speech corpus (LDC2005S11).The language model used for decoding is a tri-gram model with 84087 words trained on a collec-tion of 335M words from the following data sources:Hub4 Language Model data, EARS BN03 closedcaptions and GALE Broadcast news and conversa-tions data.
A word-fragment language model is builton this same data after tokenizing the text to frag-ments using a fragment inventory of size 21000.
Agreedy search algorithm assigns the longest possi-ble matching fragment first and iteratively uses thenext longest possible fragment until the entire pro-nunciation of the OOV term has been represented2770 5 10 15 20 25 3030405060708090Real Time FactorWERFigure 1: Speed vs WERby sub-word units.The speed and accuracy of the decoding are con-trolled using two forms of pruning.
The first is thestandard likelihood-based beam pruning that is usedin many Viterbi decoders.
The second is a formof Gaussian shortlisting in which the Gaussians inthe acoustic model are clustered into 1024 clusters,each of which is represented by a single Gaussian.When the decoder gets a new observation vector, itcomputes the likelihood of the observation under all1024 cluster models and then ranks the clusters bylikelihood.
Observation likelihoods are then com-puted only for those mixture components belongingto the top maxL1 clusters; for components outsidethis set a default, low likelihood is used.
To illus-trate the trade-offs in speed vs. accuracy that canbe achieved by varying the two pruning parame-ters, we sweep through different values for the pa-rameters and measure decoding accuracy, reportedas word error rate (WER), and decoding speed, re-ported as times faster than real time (xfRT).
For ex-ample, a system that operates at 20xfRT will requireone minute of time (measured as elapsed time) toprocess 20 minutes of speech.
Figure 1 illustratesthis effect on the NIST 2006 Spoken Term Detec-tion Dev06 test set.3 Lucene Based Indexing and SearchThe main difficulty with retrieving information fromspoken data is the low accuracy of the transcription,particularly on terms of interest such as named en-tities and content words.
Generally, the accuracyof a transcript is measured by its word error rate(WER), which is characterized by the number ofsubstitutions, deletions, and insertions with respectto the correct audio transcript.
Mamou (Mamouet al, 2007) presented the enhancement in recalland precision by searching on word confusion net-works instead of considering only the 1-best pathword transcript.
We used this model for searchingin-vocabulary queries.To handle OOV queries, a combination ofword and phonetic search was presented byMamou (Mamou et al, 2007).
In this paper, we ex-plore fuzzy phonetic search extending Lucene1, anApache open source search library written in Java,for indexing and search.
When searching for theseOOVs in word-fragment indexes, they are repre-sented phonetically (and subsequently using word-fragments) using letter-to-phoneme (L2P) rules.3.1 IndexingEach transcript is composed of basic units (e.g.,word, word-fragment, phones) associated with a be-gin time, duration and posterior probability.
Aninverted index is used in a Lucene-based indexingscheme.
Each occurrence of a unit of indexing u ina transcript D is indexed on its timestamp.
If theposterior probability is provided, we store the confi-dence level of the occurrence of u at the time t thatis evaluated by its posterior probability Pr(u|t,D).Otherwise, we consider its posterior probability tobe one.
This representation allows the indexing ofdifferent types of transcripts into a single index.3.2 RetrievalSince the vocabulary of the ASR system used to gen-erate the word transcripts is known, we can easilyidentify IV and OOV parts of the query.
We presenttwo different algorithms, namely, exact and fuzzysearch on word-fragment transcripts.
For searchon word-fragment or phonetic transcripts, the queryterms are converted to their word-fragment or pho-netic representation.Candidate lists of each query unit are extractedfrom the inverted index.
For fuzzy search, we re-trieve several fuzzy matches from the inverted in-dex for each unit of the query using the edit distanceweighted by the substitution costs provided by theconfusion matrix.
Only the matches whose weighted1http://lucene.apache.org/278edit distance is below a given threshold are returned.We use a dynamic programming algorithm to incor-porate the confusion costs specified in the matrixin the distance computation.
Our implementation isfail-fast since the procedure is aborted if it is discov-ered that the minimal cost between the sequences isgreater than a certain threshold.The score of each occurrence aggregates the pos-terior probability of each indexed unit.
The occur-rence of each unit is also weighted (user definedweight) according to its type, for example, a higherweight can be assigned to word matches instead ofword-fragment or phonetic matches.
Given the na-ture of the index, a match for any query term cannotspan across two consecutively indexed units.3.3 Hybrid WordFragment IndexingFor the hybrid system we limited the word portionof the ASR system?s lexicon to the 21K most fre-quent (frequency greater than 5) words in the acous-tic training data.
This resulted in roughly 11M(3.1%) OOV tokens in the hybrid LM training setand 1127(2.5%) OOV tokens in the evaluation set.A relative entropy criterion described in (Siohan andBacchiani, 2005) based on a 5-gram phone languagemodel was used to identify fragments.
We selected21K fragments to complement the 21K words result-ing in a composite 42K vocabulary.
The languagemodel text (11M (3.1%) fragment tokens and 320Mword tokens) was tokenized to contain words andword-fragments (for the OOVs) and the resulting hy-brid LM was used in conjunction with the acousticmodels described in Section 2.4 Neural Network Based Posteriors forFuzzy SearchIn assessing the match of decoded transcripts withsearch queries, recognition errors must be accountedfor.
One method relies on converting both the de-coded transcripts and queries into phonetic represen-tations and modeling the confusion between phones,typically represented as a confusion matrix.
In thiswork, we derive this matrix from broadcast news de-velopment data.
In particular, two systems: HMMbased automatic speech recognition (ASR) (Chaud-hari and Picheny, 2007) and a neural network basedacoustic model (Kingsbury, 2009), are used to ana-lyze the data and the results are compared to produceconfusion estimates.Let X = {xt} represent the input feature framesand S the set of context dependent HMM states.Associated with S is a many to one map M fromeach member sj ?
S to a phone in the phone setpk ?
P. This map collapses the beginning, mid-dle, and end context dependent states to the centralphone identity.
The ASR system is used to generatea state based alignment of the development data tothe training transcripts.
This results in a sequenceof state labels (classes) {st}, st ?
S , one for eachframe of the input data.
Note that the aligned statesare collapsed to the phone identity with M, so theframe class labels are given by {ct}, ct ?
P.Corresponding to each frame, we also use thestate posteriors derived from the output of a Neu-ral Network acoustic model and the prior probabil-ities computed on the training set.
Define Xt ={.
.
.
, xt, .
.
.}
to be the sub-sequence of the inputspeech frames centered around time index t. Theneural network takes Xt as input and produceslt(sj) = y(sj|Xt)?
l(sj), sj ?
Swhere y is the neural network output and l is theprior probability, both in the log domain.
Again, thestate labels are mapped using M, so the above pos-terior is interpreted as that for the collapsed phone:lt(sj) ?
lt(M(sj)) = lt(pj), pj = M(sj).The result of both analyses gives the following set ofassociations:c0 ?
l0(p0), l0(p1), l0(p2), .
.
.c1 ?
l1(p0), l1(p1), l1(p2), .
.
...ct ?
lt(p0), lt(p1), lt(p2), .
.
.Each log posterior li(pj) is converted into a countni,j = ceil[N ?
eli(pj)],where N is a large constant, i ranges over thetime index, and j ranges over the context dependentstates.
From the counts, the confusion matrix entriesare computed.
The total count for each state isnj(k) =?i:ci=pjni,k,279where k is an index over the states.????
?n1(1) n1(2) .
.
.n2(1) n2(2) .
.
...????
?The rows of the above matrix correspond to the ref-erence and the columns to the observations.
By nor-malizing the rows, the entries can be interpreted as?probability?
of an observed phone (indicated by thecolumn) given the true phone.5 Experiments and ResultsThe performance of a spoken term detection systemis measured using DET curves that plot the trade-offbetween false alarms (FAs) and misses.
This NISTSTD 2006 evaluation metric used Actual/MaximumTerm Weighted Value (ATWV/MTWV) that allowsone to weight FAs and Misses per the needs of thetask at hand (NIST, 2006).Figure 2 illustrates the effect of speed on ATWVon the NIST STD 2006 Dev06 data set using 1107query terms.
As the speed of indexing is increased tomany times faster than real time, the WER increases,which in turn decreases the ATWV measure.
It canbe seen that the use of word-fragments improvesthe performance on OOV queries thus making thecombined search better than simple word search.The primary advantage of using a hybrid decodingscheme over a separate word and fragment baseddecoding scheme is the speed of transforming theaudio into indexable units.
The blue line in the fig-ure illustrates that when using a hybrid setup, thesame performance can be achieved at speeds twiceas fast.
For example, with the combined searchon two different decodes, an ATWV of 0.1 can beachieved when indexing at a speed 15 times fasterthan real time, but with a hybrid system, the sameperformance can be reached at an indexing speed 30times faster than real time.
The ATWV on the hybridsystem also degrades gracefully with faster speedswhen compared to separate word and word-fragmentsystems.
Preliminary results indicate that fuzzysearch on one best output gives the same ATWVperformance as exact search (Figure 2) on consen-sus output.
Also, a closer look at the retrieval resultsof OOV terms revealed that many more OOVs areretrieved with the fuzzy search.0 5 10 15 20 25 30 35?0.4?0.200.20.40.60.81Real Time FactorATWVexactWordexactWordAndFragexactHybridFigure 2: Effect of WER on ATWV.
Note that the cuvesfor exactWord and exactWordAndFrag lie on top of eachother.6 CONCLUSIONIn this paper, we have presented the effect of rapiddecoding on a spoken term detection task.
Wehave demonstrated that hybrid systems perform welland fuzzy search with phone confusion probabilitieshelp in OOV retrieval.ReferencesU.
V. Chaudhari and M. Picheny.
2007.
Improvements inphone based audio search via constrained match withhigh order confusion estimates.
In Proc.
of ASRU.M.
Clements, S. Robertson, and M. S. Miller.
2002.Phonetic searching applied to on-line distance learningmodules.
In Proc.
of IEEE Digital Signal ProcessingWorkshop.B.
Kingsbury.
2009.
Lattice-based optimizationof sequence classification criteria for neural-networkacoustic modeling.
In Proc.
of ICASSP.J.
Mamou, B. Ramabhadran, and O. Siohan.
2007.
Vo-cabulary independent spoken term detection.
In Proc.of ACM SIGIR.NIST.
2006.
The spoken term de-tection (STD) 2006 evaluation plan.http://www.nist.gov/speech/tests/std/docs/std06-evalplan-v10.pdf.M.
Saraclar and R. Sproat.
2004.
Lattice-based searchfor spoken utterance retrieval.
In Proc.
HLT-NAACL.F.
Seide, P. Yu, C. Ma, and E. Chang.
2004.
Vocabulary-independent search in spontaneous speech.
In Proc.
ofICASSP.O.
Siohan and M. Bacchiani.
2005.
Fast vocabulary in-dependent audio search using path based graph index-ing.
In Proc.
of Interspeech.280
