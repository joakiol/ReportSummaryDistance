Coling 2010: Poster Volume, pages 162?170,Beijing, August 2010Global topology of word co-occurrence networks:Beyond the two-regime power-lawMonojit ChoudhuryMicrosoft Research Lab Indiamonojitc@microsoft.comDiptesh ChatterjeeIndian Institute of Technology Kharagpurdiptesh.chh.1987@gmail.comAnimesh MukherjeeComplex Systems Lagrange Lab, ISI Foundationanimesh.mukherjee@isi.itAbstractWord co-occurrence networks are oneof the most common linguistic networksstudied in the past and they are knownto exhibit several interesting topologicalcharacteristics.
In this article, we inves-tigate the global topological properties ofword co-occurrence networks and, in par-ticular, present a detailed study of theirspectrum.
Our experiments reveal cer-tain universal trends found across the net-works for seven different languages fromthree different language families, whichare neither reported nor explained by anyof the previous studies and models ofword-cooccurrence networks.
We hy-pothesize that since word co-occurrencesare governed by syntactic properties ofa language, the network has much con-strained topology than that predicted bythe previously proposed growth model.
Adeeper empirical and theoretical investiga-tion into the evolution of these networksfurther suggests that they have a core-periphery structure, where the core hardlyevolves with time and new words are onlyattached to the periphery of the network.These properties are fundamental to thenature of word co-occurrence across lan-guages.1 IntroductionIn a natural language, words interact among them-selves in different ways ?
some words co-occurwith certain words at a very high probabilitythan other words.
These co-occurrences are non-trivial, as in their patterns cannot be inferred fromthe frequency distribution of the individual words.Understanding the structure and the emergence ofthese patterns can present us with important cluesand insights about how we evolved this extremelycomplex phenomenon, that is language.In this paper, we present an in-depth study ofthe word co-occurrence patterns of a language inthe framework of complex networks.
The choiceof this framework is strongly motivated by itssuccess in explaining various properties of wordco-occurrences previously (Ferrer-i-Cancho andSole?, 2001; Ferrer-i-Cancho et al 2007; Kapustinand Jamsen, 2007).
Local properties, such asthe degree distribution and clustering coefficientof the word co-occurrence networks, have beenthoroughly studied for a few languages (Ferrer-i-Cancho and Sole?, 2001; Ferrer-i-Cancho et al2007; Kapustin and Jamsen, 2007) and many in-teresting conclusions have been drawn.
For in-stance, it has been found that these networks aresmall-world in nature and are characterized by atwo regime power-law degree distribution.
Effortshave also been made to explain the emergence ofsuch a two regime degree distribution through net-work growth models (Dorogovstev and Mendes,2001).
Although it is tempting to believe that alot is known about word co-occurrences, in or-der to obtain a deeper insight into how these co-occurrence patterns emerged there are many otherinteresting properties that need to be investigated.One such property is the spectrum of the word co-162occurrence network which can provide importantinformation about its global organization.
In fact,the application of this powerful mathematical ma-chinery to infer global patterns in linguistic net-works is rarely found in the literature (few excep-tions are (Belkin and Goldsmith, 2002; Mukher-jee et al 2009)).
However, note that spectral anal-ysis has been quite successfully applied in theanalysis of biological and social networks (Baner-jee and Jost, 2007; Farkas et al 2001).The aim of the present work is to investigatethe spectral properties of a word co-occurrencenetwork in order to understand its global struc-ture.
In particular, we study the properties ofseven different languages namely Bangla (Indo-European family), English (Indo-European fam-ily), Estonian (Finno-Ugric family), French (Indo-European family), German (Indo-European fam-ily), Hindi (Indo-European family) and Tamil(Dravidian family).
Quite importantly, as we shallsee, the most popular growth model proposed byDorogovtsev and Mendes (DM) (Dorogovstev andMendes, 2001) for explaining the degree distribu-tion of such a network is not adequate to repro-duce the spectrum of the network.
This observa-tion holds for all the seven different languages un-der investigation.
We shall further attempt to iden-tify the precise (linguistic) reasons behind this dif-ference in the spectrum of the empirical networkand the one reproduced by the model.
Finally, asan additional objective, we shall present a hithertounreported deeper analysis of this popular modeland show how its most important parameter is cor-related to the size of the corpus from which theempirical network is constructed.The rest of the paper is laid out as follows.In section 2, we shall present a brief review ofthe previous works on word co-occurrence net-works.
This is followed by a short primer to spec-tral analysis.
In section 4, we outline the construc-tion methodology of the word co-occurrence net-works and present the experiments comparing thespectrum of these real networks with those gen-erated by the DM model.
Section 5 shows howthe most important parameter of the DM modelvaries with the size of the corpus from which theco-occurrence networks are constructed.
Finally,we conclude in section 6 by summarizing our con-tributions and pointing out some of the implica-tions of the current work.2 Word Co-occurrence NetworksIn this section, we present a short review of theearlier works on word co-occurrence networks,where the nodes are the words and an edge be-tween two words indicate that the words have co-occurred in a language in certain context(s).
Themost basic and well studied form of word co-occurrence networks are the word collocation net-works, where two words are linked by an edge ifthey are neighbors (i.e., they collocate) in a sen-tence (Ferrer-i-Cancho and Sole?, 2001).In (Ferrer-i-Cancho and Sole?, 2001), the au-thors study the properties of two types of col-location networks for English, namely the unre-stricted and the restricted ones.
While in the unre-stricted network, all the collocation edges are pre-served, in the restricted one only those edges arepreserved for which the probability of occurrenceof the edge is higher than the case when the twowords collocate independently.
They found thatboth the networks exhibit small-world properties;while the average path length between any twonodes in these networks is small (between 2 and3), the clustering coefficients are high (0.69 for theunrestricted and 0.44 for the restricted networks).Nevertheless, the most striking observation aboutthese networks is that the degree distributions fol-low a two regime power-law.
The degree distribu-tion of the 5000 most connected words (i.e., thekernel lexicon) follow a power-law with an expo-nent ?3.07, which is very close to that predictedby the Baraba?si-Albert growth model (Baraba?siand Albert, 1999).
These findings led the au-thors to argue that the word usage of the humanlanguages is preferential in nature, where the fre-quency of a word defines the comprehensibilityand production capability.
Thus, higher the us-age frequency of a word, higher is the probabilitythat the speakers will be able to produce it eas-ily and the listeners will comprehend it fast.
Thisidea is closely related to the recency effect in lin-guistics (Akmajian, 1995).Properties of word collocation networks havealso been studied for languages other than En-glish (Ferrer-i-Cancho et al 2007; Kapustin and163Jamsen, 2007).
The basic topological characteris-tics of all these networks (e.g., scale-free, smallworld, assortative) are similar across languagesand thus, point to the fact that like Zipf?s law,these are also linguistic universals whose emer-gence and existence call for a non-trivial psycho-linguistic account.In order to explain the two regime power-law in word collocation networks, Dorogovtsevand Mendes (Dorogovstev and Mendes, 2001)proposed a preferential attachment based growthmodel (henceforth referred to as the DM model).In this model, at every time step t, a new word(i.e., a node) enters the language (i.e., the net-work) and connects itself preferentially to one ofthe pre-existing nodes.
Simultaneously, ct (wherec is a positive constant and a parameter of themodel) new edges are grown between pairs ofold nodes that are chosen preferentially.
Throughmathematical analysis and simulations, the au-thors successfully establish that this model givesrise to a two regime power-law with exponentsvery close to those observed in (Ferrer-i-Canchoand Sole?, 2001).
In fact, for English, the val-ues kcross (i.e., the point where the two powerlaw regimes intersect) and kcut (i.e., the pointwhere the degree distribution cuts the x-axis) ob-tained from the model are in perfect agreementwith those observed for the empirical network.Although the DM model is capable of explain-ing the local topological properties of the wordcollocation network, as we shall see in the forth-coming sections, it is unable to reproduce theglobal properties (e.g., the spectrum) of the net-work.3 A Primer to Spectral AnalysisSpectral analysis1 is a powerful mathematicalmethod capable of revealing the global structuralpatterns underlying an enormous and complicatedenvironment of interacting entities.
Essentially, itrefers to the systematic investigation of the eigen-values and the eigenvectors of the adjacency ma-trix of the network of these interacting entities.In this section, we shall briefly outline the basic1The term spectral analysis is also used in the contextof signal processing, where it refers to the study of the fre-quency spectrum of a signal.concepts involved in spectral analysis and discusssome of its applications (see (Chung, 1994) fordetails).A network consisting of n nodes (labeled as1 through n) can be represented by an n ?
nsquare matrix A, where the entry aij representsthe weight of the edge from node i to node j. Notethat A, which is known as the adjacency matrix,is symmetric for an undirected graph and havebinary entries for an unweighted graph.
?
is aneigenvalue of A if there is an n-dimensional vec-tor x such thatAx = ?xAny real symmetric matrix A has n (possibly non-distinct) eigenvalues ?0 ?
?1 ?
.
.
.
?
?n?1,and corresponding n eigenvectors that are mutu-ally orthogonal.
The spectrum of a network isthe set of the distinct eigenvalues of the graph andtheir corresponding multiplicities.
It is a distribu-tion usually represented in the form of a plot withthe eigenvalues in x-axis and their multiplicities inthe y-axis.The spectrum of real and random networks dis-play several interesting properties.
Banerjee andJost (Banerjee and Jost, 2007) report the spectrumof several biological networks and show that theseare significantly different from the spectrum of ar-tificially generated networks.
It is worthwhile tomention here that spectral analysis is also closelyrelated to Principal Component Analysis and Mul-tidimensional Scaling.
If the first few (say d)eigenvalues of a matrix are much higher than therest of the eigenvalues, then one can conclude thatthe rows of the matrix can be approximately rep-resented as linear combinations of d orthogonalvectors.
This further implies that the correspond-ing graph has a few motifs (subgraphs) that are re-peated a large number of time to obtain the globalstructure of the graph (Banerjee and Jost, 2009).In the next section, we shall present a thoroughstudy of the spectrum of the word co-occurrencenetworks across various languages.4 Experiments and ResultsFor the purpose of our experiments, we con-struct word collocation networks for seven dif-ferent languages namely, Bangla, English, Esto-164Figure 1: Cumulative degree distributions for Bangla, English, Estonian, French, German, Hindi andTamil respectively.
Each red line signifies the degree distribution for the empirical network while eachblue line signifies the one obtained from the DM model.Lang.
Tokens (Mill.)
Words KLD c Max.
Eig.
(Real) Max.
Eig.
(DM)English 32.5 97144 0.21 5.0e-4 849.1 756.8Hindi 20.2 99210 0.32 2.3e-4 472.5 329.5Bangla 12.7 100000 0.29 2.0e-3 326.2 245.0German 5.0 159842 0.19 6.3e-5 192.3 110.7Estonian 4.0 100000 0.25 1.1e-4 158.6 124.0Tamil 2.3 75929 0.24 9.9e-4 116.4 73.06French 1.8 100006 0.44 8.0e-5 236.1 170.1Table 1: Summary of results comparing the structural properties of the empirical networks for the sevenlanguages and the corresponding best fits (in terms of KLD) obtained from the DM model.nian, French, German, Hindi and Tamil.
We usedthe corpora available in the Lipezig Corpora Col-lection (http://corpora.informatik.uni-leipzig.de/)for English, Estonian, French and German.
TheHindi, Bangla and Tamil corpora were collectedby crawling some online newspapers.
In these net-works, each distinct word corresponds to a ver-tex and two vertices are connected by an edgeif the corresponding two words are adjacent inone or more sentences in the corpus.
We assumethe network to be undirected and unweighted (asin (Ferrer-i-Cancho and Sole?, 2001)).As a following step, we simulate the DM modeland reproduce the degree distribution of the col-location networks for the seven languages.
Wevary the parameter c in order to minimize the KL165divergence (KLD) (Kullback and Leibler, 1951)between the empirical and the synthesized dis-tributions and, thereby, obtain the best match.The results of these experiments are summarizedthrough Figure 1 and Table 1.
The results clearlyshow that the DM model is indeed capable of gen-erating the degree distribution of the collocationnetworks to a very close approximation for cer-tain values of the parameter c (see Table 1 for thevalues of c and the corresponding KLD).Subsequently, for the purpose of spectral anal-ysis, we construct subgraphs induced by the top5000 nodes for each of the seven empirical net-works as well as those generated by the DM model(i.e., those for which the degree distribution fitsbest in terms of KLD with the real data).
We thencompute and compare the spectrum of the realand the synthesized networks (see Figure 2 andTable 1).
It is quite apparent from these resultsthat the spectra of the empirical networks are sig-nificantly different from those obtained using theDM model.
In general, the spectral plots indicatethat the adjacency matrices for networks obtainedfrom the DM model have a higher rank than thosefor the empirical networks.
Further, in case of thesynthesized networks, the first eigenvalue is sig-nificantly larger than the second whereas for theempirical networks the top 3 to 4 eigenvalues arefound to dominate.
Interestingly, this property isobserved across all the languages under investiga-tion.We believe that the difference in the spectra isdue to the fact that the ordering of the words ina sentence are strongly governed by the grammaror the syntax of the language.
Words belong toa smaller set of lexico-syntactic categories, whichare more commonly known as the parts-of-speech(POS).
The co-occurrence patterns of the wordsare influenced, primarily, by its POS category.
Forinstance, nouns are typically preceded by articlesor adjectives, whereas verbs might be preceded byauxiliary verbs, adverbs or nouns, but never ar-ticles or adjectives.
Therefore, the words ?car?and ?camera?
are more likely to be structurallysimilar in the word co-occurrence network, than?car?
and ?jumped?.
In general, the local neigh-borhoods of the words belonging to a particularPOS is expected to be very similar, which meansthat several rows in the adjacency matrix will bevery similar to each other.
Thus, the matrix is ex-pected to have low rank.In fact, this property is not only applicable tosyntax, but also semantics.
For instance, eventhough adjectives are typically followed by nouns,semantic constraints make certain adjective-nounco-occurrences (e.g., ?green leaves?)
much morelikely than some others (e.g., ?green dreams?
or?happy leaves?).
These notions are at the core oflatent semantics and vector space models of se-mantics (see, for instance, Turney and Pantel (Tur-ney and Pantel, 2010) for a recent study).
The DMmodel, on the other hand, is based on the recencyeffect that says that the words which are producedmost recently are easier to remember and there-fore, easier to produce in the future.
Preferentialattachment models the recency effect in word pro-duction, which perhaps is sufficient to replicatethe degree distribution of the networks.
However,the model fails to explain the global properties,precisely because it does not take into accountthe constraints that govern the distribution of thewords.It is quite well known that the spectrum of a net-work can be usually obtained by iteratively pow-ering the adjacency matrix of the network (akapower iteration method).
Note that if the adja-cency matrices of the empirical and the synthe-sized networks are powered even once (i.e., theyare squared)2, their degree distributions match nolonger (see Figure 3).
This result further cor-roborates that although the degree distribution ofa word co-occurrence network is quite appropri-ately reproduced by the DM model, more globalstructural properties remain unexplained.
We be-lieve that word association in human languagesis not arbitrary and therefore, a model which ac-counts for the clustering of words around theirPOS categories might possibly turn out to presenta more accurate explanation of the spectral prop-erties of the co-occurrence networks.166Figure 2: The spectrum for Bangla, English, Estonian, French, German, Hindi and Tamil respectively.The last plot shows a portion of the spectrum for English magnified around 0 for better visualization.All the curves are binned distributions with bin size = 100.
The blue line in each case is the spectrumfor the network obtained from the DM model while each red line corresponds to the spectrum for theempirical network.5 Reinvestigating the DM ModelIn this section, we shall delve deeper into explor-ing the properties of the DM model since it is oneof the most popular and well accepted models forexplaining the emergence of word associations ina language.
In particular, we shall investigate theinfluence of the model parameter c on the emer-gent results.If we plot the value of the parameter c (fromTable 1) versus the size of the corpora (from Ta-ble 1) used to construct the empirical networks forthe different languages we find that the two arehighly correlated (see Figure 4).2Note that this squared network is weighted in nature.
Wethreshold all edges below the weight 0.07 so that the resultantnetwork is neither too dense nor too sparse.
The value of thethreshold is chosen based on the inspection of the data.In order to further check the dependence of con the corpus size we perform the following ex-periment.
We draw samples of varying corpussize and construct empirical networks from eachof them.
We then simulate the DM model and at-tempt to reproduce the degree distribution for eachof these empirical networks.
In each case, we notethe value c for which the KLD between the empir-ical and the corresponding synthesized network isminimum.
Figure 5 shows the result of the aboveexperiment for English.
The figure clearly indi-cates that as the corpus size increases the value ofthe parameter c decreases.
Similar trends are ob-served for all the other languages.In general, one can mathematically prove thatthe parameter c is equal to the rate of change ofthe average degree of the network with respect to167Figure 3: Cumulative degree distribution for thesquared version of the networks for English.
Thered line is the degree distribution for the squaredversion of the empirical network while the blueline is degree distribution of the squared versionof the network obtained from the DM model.
Thetrends are similar for all the other languages.the time t. The proof is as follows.At every time step t, the number of new edgesformed is (1+ct).
Since each edge contributes toa total degree of 2 to the network, the sum of thedegrees of all the nodes in the network (ktot) isktot = 2T?t=1(1 + ct) = 2T + cT (T + 1) (1)At every time step, only one new node is addedto the network and therefore the total number ofnodes at the end of time T is exactly equal to T .Thus the average degree of the network is?k?
= 2T + cT (T + 1)T = 2 + c(T + 1) (2)The rate of change of average degree isd?k?dT = c (3)and this completes the proof.In fact, it is also possible to make a preciseempirical estimate of the value of the parameterc.
One can express the average degree of the co-occurrence networks as the ratio of twice the bi-gram frequency (i.e., twice the number of edgesin the network) to the unigram frequency (i.e., the0 5 10 15 20 25 30 350.511.522.533.544.55 x 10?4Corpus Size(Across Languages)cFigure 4: The parameter c versus the corpus sizefor the seven languages.Figure 5: The parameter c versus the corpus sizefor English.number of nodes or unique words in the network).Therefore, if we can estimate this ratio we can eas-ily estimate the value of c using equation 3.
Letus denote the total number of distinct bigrams andunigrams after processing a corpus of size N byB(N) and W (N) respectively.
Hence we have?k?
= 2B(N)W (N) (4)Further, the number of distinct new unigrams afterLanguage B(N) W (N) cEnglish 29.2N.67 59.3N.43 .009N?.20Hindi 26.2N.66 49.7N.46 .009N?.26Tamil 1.9N.91 6.4N.71 .207N?.50Table 2: Summary of expressions for B(N),W (N) and c for English, Hindi and Tamil.168Figure 6: Variation of B(N) and W (N) with Nfor English (in doubly-logarithmic scale).
Theblue dots correspond to variation of B(N) whilethe red dots correspond to the variation of W (N).processing a corpus of size N is equivalent to Tand thereforeT = W (N) (5)Sampling experiments across different languagesdemonstrate that W (N) and B(N) are of the form?N?
(?
< 1) where ?
and ?
are constants.
Forinstance, Figure 6 shows in doubly-logarithmicscale how B(N) and W (N) varies with N forEnglish.
The R2 values obtained as a result offitting the B(N) versus N and the W (N) ver-sus N plots using equations of the form ?N?
forEnglish, Hindi and Tamil are greater than 0.99.This reflects the high accuracy of the fits.
Similartrends are observed for all the other languages.Finally, using equations 3, 4 and 5 we havec = d?k?dT =d?k?dNdNdT (6)and plugging the values of B(N) and W (N) inequation 6 we find that c has the form ?N??
(?
<1) where ?
and ?
are language dependent positiveconstants.
The values of c obtained in this wayfor three different languages English, Hindi andTamil are noted in Table 5.Thus, we find that as N ?
?, c ?
0.
Inother words, as the corpus size grows the numberof distinct new bigrams goes on decreasing andultimately reaches (almost) zero for a very largesized corpus.
Now, if one plugs in the values of cand T obtained above in the expressions for kcrossand kcut in (Dorogovstev and Mendes, 2001), oneobserves that limN??
kcrosskcut = 0.
This impliesthat as the corpus size becomes very large, thetwo-regime power law (almost) converges to a sin-gle regime with an exponent equal to -3 as is ex-hibited by the Baraba?si-Albert model (Baraba?siand Albert, 1999).
Therefore, it is reasonable toconclude that although the DM model provides agood explanation of the degree distribution of aword co-occurrence network built from a mediumsized corpora, it does not perform well for verysmall or very large sized corpora.6 ConclusionsIn this paper, we have tried to investigate in de-tail the co-occurrence properties of words in alanguage.
Some of our important observationsare: (a) while the DM model is able to reproducethe degree distributions of the word co-occurrencenetworks, it is not quite appropriate for explainingthe spectrum of these networks; (b) the parameterc in the DM model signifies the rate of change ofthe average degree of the network with respect totime; and (c) the DM model does not perform wellin explaining the degree distribution of a word co-occurrence network when the corpus size is verylarge.It is worthwhile to mention here that our analy-sis of the DM model leads us to a very importantobservation.
As N grows, the value of kcut growsat a much faster rate than the value of kcross andin the limit N ??
the value of kcut is so high ascompared to kcross that the ratio kcrosskcut becomes(almost) zero.
In other words, the kernel lexicon,formed of the words in the first regime of the tworegime power-law and required to ?say everythingor almost everything?
(Ferrer-i-Cancho and Sole?,2001) in a language, grows quite slowly as newwords creep into the language.
In contrast, the pe-ripheral lexicon making the other part of the tworegime grows very fast as new words enter the lan-guage.
Consequently, it may be argued that sincethe kernel lexicon remains almost unaffected, theeffort to learn and retain a language by its speak-ers increases only negligibly as new words creepinto the language.169ReferencesA.
Akmajian.
Linguistics: An introduction to Lan-guage and Communication.
MIT Press, Cambridge,MA, 1995.A.
Banerjee and J. Jost.
Spectral plots and the repre-sentation and interpretation of biological data.
The-ory in Biosciences, 126(1), 15-21, 2007.A.
Banerjee and J. Jost.
Graph spectra as a system-atic tool in computational biology.
Discrete AppliedMathematics, 157(10), 2425?2431, 2009.A.-L. Baraba?si and R. Albert.
Emergence of scaling inrandom networks.
Science, 286, 509-512, 1999.M.
Belkin and J. Goldsmith.
Using eigenvectors ofthe bigram graph to infer morpheme identity.
InProceedings of Morphological and PhonologicalLearning, Association for Computational Linguis-tics, 41-47, 2002.F.
R. K. Chung.
Spectral Graph Theory.
Number 2 inCBMS Regional Conference Series in Mathematics,American Mathematical Society, 1994.S.
N. Dorogovstev and J. F .F.
Mendes.
Language as anevolving word Web.
Proceedings of the Royal Soci-ety of London B, 268, 2603-2606, 2001.I.
J. Farkas, I. Dere?nyi, A.
-L. Baraba?si and T. Vicsek.Spectra of ?real-world?
graphs: Beyond the semi-circle law, Physical Review E, 64, 026704, 2001.R.
Ferrer-i-Cancho and R. V. Sole?.
The small-world ofhuman language.
Proceedings of the Royal Societyof London B, 268, 2261?2266, 2001.R.
Ferrer-i-Cancho, A. Mehler, O. Pustylnikov andA.
D??az-Guilera.
Correlations in the organizationof large-scale syntactic dependency networks.
InProceedings of TextGraphs-2: Graph-Based Algo-rithms for Natural Language Processing, 65-72, As-sociation for Computational Linguistics, 2007.V.
Kapustin and A. Jamsen.
Vertex degree distributionfor the graph of word co-occurrences in Russian.
InProceedings of TextGraphs-2: Graph-Based Algo-rithms for Natural Language Processing, 89-92, As-sociation for Computational Linguistics, 2007.S.
Kullback and R. A. Leibler.
On information andsufficiency.
Annals of Mathematical Statistics 22(1),79-86, 1951.A.
Mukerjee, M. Choudhury and R. Kannan.
Discov-ering global patterns in linguistic networks throughspectral analysis: A case study of the consonant in-ventories.
In Proceedings of EACL, 585?593, Asso-ciation for Computational Linguistics, 2009.P.
D. Turney and P. Pantel.
From frequency to meaning:Vector space models of semantics.
In JAIR, 37, 141-188, 2010.170
