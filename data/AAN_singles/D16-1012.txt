Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 118?127,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDeep Multi-Task Learning with Shared MemoryPengfei Liu Xipeng Qiu?
Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{pfliu14,xpqiu,xjhuang}@fudan.edu.cnAbstractNeural network based models have achievedimpressive results on various specific tasks.However, in previous works, most models arelearned separately based on single-task su-pervised objectives, which often suffer frominsufficient training data.
In this paper, wepropose two deep architectures which can betrained jointly on multiple related tasks.
Morespecifically, we augment neural model with anexternal memory, which is shared by severaltasks.
Experiments on two groups of text clas-sification tasks show that our proposed archi-tectures can improve the performance of a taskwith the help of other related tasks.1 IntroductionNeural network based models have been shown toachieved impressive results on various NLP tasks ri-valing or in some cases surpassing traditional mod-els, such as text classification (Kalchbrenner et al,2014; Socher et al, 2013; Liu et al, 2015a), seman-tic matching (Hu et al, 2014; Liu et al, 2016a),parser (Chen and Manning, 2014) and machinetranslation (Bahdanau et al, 2014).Usually, due to the large number of parametersthese neural models need a large-scale corpus.
It ishard to train a deep neural model that generalizeswell with size-limited data, while building the largescale resources for some NLP tasks is also a chal-lenge.
To overcome this problem, these models ofteninvolve an unsupervised pre-training phase.
The fi-nal model is fine-tuned on specific task with respect?
Corresponding author.to a supervised training criterion.
However, mostpre-training methods are based on unsupervised ob-jectives (Collobert et al, 2011; Turian et al, 2010;Mikolov et al, 2013), which is effective to improvethe final performance, but it does not directly opti-mize the desired task.Multi-task learning is an approach to learn multi-ple related tasks simultaneously to significantly im-prove performance relative to learning each task in-dependently.
Inspired by the success of multi-tasklearning (Caruana, 1997), several neural networkbased models (Collobert and Weston, 2008; Liu etal., 2015b) are proposed for NLP tasks, which uti-lized multi-task learning to jointly learn several taskswith the aim of mutual benefit.
The characteristicof these multi-task architectures is they share somelower layers to determine common features.
Afterthe shared layers, the remaining layers are split intomultiple specific tasks.In this paper, we propose two deep architecturesof sharing information among several tasks in multi-task learning framework.
All the related tasks are in-tegrated into a single system which is trained jointly.More specifically, inspired by Neural Turing Ma-chine (NTM) (Graves et al, 2014) and memorynetwork (Sukhbaatar et al, 2015), we equip task-specific long short-term memory (LSTM) neuralnetwork (Hochreiter and Schmidhuber, 1997) withan external shared memory.
The external memoryhas capability to store long term information andknowledge shared by several related tasks.
Differentwith NTM, we use a deep fusion strategy to integratethe information from the external memory into task-specific LSTM, in which a fusion gate controls the118information flowing flexibly and enables the modelto selectively utilize the shared information.We demonstrate the effectiveness of our architec-tures on two groups of text classification tasks.
Ex-perimental results show that jointly learning of mul-tiple related tasks can improve the performance ofeach task relative to learning them independently.Our contributions are of three-folds:?
We proposed a generic multi-task framework,in which different tasks can share informationby an external memory and communicate bya reading/writing mechanism.
Two proposedmodels are complementary to prior multi-taskneural networks.?
Different with Neural Turing Machine andmemory network, we introduce a deep fu-sion mechanism between internal and externalmemories, which helps the LSTM units keepthem interacting closely without being con-flated.?
As a by-product, the fusion gate enables usto better understand how the external sharedmemory helps specific task.2 Neural Memory Models for Specific TaskIn this section, we briefly describe LSTM model,and then propose an external memory enhancedLSTM with deep fusion.2.1 Long Short-term MemoryLong short-term memory network (LSTM) (Hochre-iter and Schmidhuber, 1997) is a type of recurrentneural network (RNN) (Elman, 1990), and specifi-cally addresses the issue of learning long-term de-pendencies.
LSTM maintains an internal memorycell that updates and exposes its content only whendeemed necessary.Architecturally speaking, the memory state andoutput state are explicitly separated by activationgates (Wang and Cho, 2015).
However, the limita-tion of LSTM is that it lacks a mechanism to indexits memory while writing and reading (Danihelka etal., 2016).While there are numerous LSTM variants, herewe use the LSTM architecture used by (Jozefowiczet al, 2015), which is similar to the architecture of(Graves, 2013) but without peep-hole connections.We define the LSTM units at each time step t tobe a collection of vectors in Rd: an input gate it, aforget gate ft, an output gate ot, a memory cell ctand a hidden state ht.
d is the number of the LSTMunits.
The elements of the gating vectors it, ft andot are in [0, 1].The LSTM is precisely specified as follows.????c?totitft????
=????tanh???????
(Wp[xtht?1]+ bp), (1)ct = c?t  it + ct?1  ft, (2)ht = ot  tanh (ct) , (3)where xt ?
Rm is the input at the current time step;W ?
R4h?
(d+m) and bp ?
R4h are parameters ofaffine transformation; ?
denotes the logistic sigmoidfunction and  denotes elementwise multiplication.The update of each LSTM unit can be written pre-cisely as follows:(ht, ct) = LSTM(ht?1, ct?1,xt, ?p).
(4)Here, the function LSTM(?, ?, ?, ?)
is a shorthandfor Eq.
(1-3), and ?p represents all the parametersof LSTM.2.2 Memory Enhanced LSTMLSTM has an internal memory to keep useful in-formation for specific task, some of which may bebeneficial to other tasks.
However, it is non-trivial toshare information stored in internal memory.Recently, there are some works to augment LSTMwith an external memory, such as neural Turingmachine (Graves et al, 2014) and memory net-work (Sukhbaatar et al, 2015), called memory en-hanced LSTM (ME-LSTM).
These models enhancethe low-capacity internal memory to have a capabil-ity of modelling long pieces of text (Andrychowiczand Kurach, 2016).Inspired by these models, we introduce an ex-ternal memory to share information among severaltasks.
To better control shared information and un-derstand how it is utilized from external memory, wepropose a deep fusion strategy for ME-LSTM.119tanhtanh?
??
?tx1th ?
1tc ?
trtf ti to tg thtcMemory1tk ?1te ?1ta ?
?tanhtanhFigure 1: Graphical illustration of the proposed ME-LSTM unit with deep fusion of internal and externalmemories.As shown in Figure 1, ME-LSTM consists theoriginal LSTM and an external memory which ismaintained by reading and writing operations.
TheLSTM not only interacts with the input and outputinformation but accesses the external memory usingselective read and write operations.The external memory and corresponding opera-tions will be discussed in detail below.External Memory The form of external memoryis defined as a matrix M ?
RK?M , where K isthe number of memory segments, and M is thesize of each segment.
Besides, K and M are gener-ally instance-independent and pre-defined as hyper-parameters.At each step t, LSTM emits output ht and threekey vectors kt, et and at simultaneously.
kt, et andat can be computed as??ktetat??
=??tanh?tanh??
(Wmht + bm) (5)where Wm and bm are parameters of affine trans-formation.Reading The read operation is to read informationrt ?
RM from memory Mt?1.rt = ?tMt?1, (6)where rt denotes the reading vector and ?t ?
RKrepresents a distribution over the set of segments ofmemory Mt?1, which controls the amount of infor-mation to be read from and written to the memory.Each scalar ?t,k in attention distribution ?t can beobtained as:?t,k = softmax(g(Mt?1,k,kt?1)) (7)where Mt?1,k represents the k-th row memory vec-tor, and kt?1 is a key vector emitted by LSTM.Here g(x,y) (x ?
RM ,y ?
RM ) is a alignfunction for which we consider two different alter-natives:g(x,y) ={vT tanh(Wa[x;y])cosine(x, y)(8)where v ?
RM is a parameter vector.In our current implementation, the similarity mea-sure is cosine similarity.Writing The memory can be written by two oper-ations: erase and add.Mt = Mt?1(1?
?teTt ) + ?taTt , (9)where et,at ?
RM represent erase and add vectorsrespectively.To facilitate the following statements, we re-writethe writing equation as:Mt = fwrite(Mt?1, ?t,ht).
(10)Deep Fusion between External and InternalMemories After we obtain the information fromexternal memory, we need a strategy to comprehen-sively utilize information from both external and in-ternal memory.To better control signals flowing from externalmemory, inspired by (Wang and Cho, 2015), we pro-pose a deep fusion strategy to keep internal and ex-ternal memories interacting closely without beingconflated.In detail, the state ht of LSTM at step t dependson both the read vector rt from external memory,and internal memory ct, which is computed byht = ot  tanh(ct + gt  (Wfrt)), (11)where Wf is parameter matrix, and gt is a fusiongate to select information from external memory,which is computed bygt = ?
(Wrrt + Wcct), (12)120x1 x2 x3 xTh(m)1 h(m)2 h(m)3 ?
?
?
h(m)T softmax1 y(m)M(s)0 M(s)1 M(s)2 ?
?
?
M(s)T?1h(n)1 h(n)2 h(n)3 ?
?
?
h(n)T softmax2 y(n)x1 x2 x3 xT(a) Global Memory Architecturex1 x2 x3 xTh(m)1 h(m)2 h(m)3 ?
?
?
h(m)T softmax1 y(m)M(m)1 M(m)2 M(m)3M(s)0 M(s)1 M(s)2 M(s)T?1M(n)1 M(n)2 M(n)3h(n)1 h(n)2 h(n)3 ?
?
?
h(n)T softmax2 y(n)x1 x2 x3 xT(b) Local-Global Hybrid Memory ArchitectureFigure 2: Two architectures for modelling text withmulti-task learning.where Wr and Wc are parameter matrices.Finally, the update of external memory enhancedLSTM unit can be written precisely as(ht,Mt, ct) = ME-LSTM(ht?1,Mt?1, ct?1,xt, ?p, ?q), (13)where ?p represents all the parameters of LSTM in-ternal structure and ?q represents all the parametersto maintain the external memory.Overall, the external memory enables ME-LSTMto have larger capability to store more information,thereby increasing the ability of ME-LSTM.
Theread and write operations allow ME-LSTM to cap-ture complex sentence patterns.3 Deep Architectures with SharedMemory for Multi-task LearningMost existing neural network methods are basedon supervised training objectives on a single task(Collobert et al, 2011; Socher et al, 2013; Kalch-brenner et al, 2014).
These methods often sufferfrom the limited amounts of training data.
To dealwith this problem, these models often involve anunsupervised pre-training phase.
This unsupervisedpre-training is effective to improve the final perfor-mance, but it does not directly optimize the desiredtask.Motivated by the success of multi-task learning(Caruana, 1997), we propose two deep architectureswith shared external memory to leverage superviseddata from many related tasks.
Deep neural model iswell suited for multi-task learning since the featureslearned from a task may be useful for other tasks.Figure 2 gives an illustration of our proposed archi-tectures.ARC-I: Global Shared Memory In ARC-I, theinput is modelled by a task-specific LSTM and ex-ternal shared memory.
More formally, given an inputtext x, the task-specific output h(m)t of taskm at stept is defined as(h(m)t ,M(s)t , c(m)t ) = ME-LSTM(h(m)t?1,M(s)t?1, c(m)t?1,xt, ?
(m)p , ?
(s)q ), (14)where xt represents word embeddings of wordxt; the superscript s represents the parameters areshared across different tasks; the superscript m rep-resents that the parameters or variables are task-specific for task m.Here all tasks share single global memory M(s),meaning that all tasks can read information fromit and have the duty to write their shared or task-specific information into the memory.M(s)t = fwrite(M(s)t?1, ?
(s)t ,h(m)t ) (15)After calculating the task-specific representation oftext h(m)T for task m, we can predict the probabilitydistribution over classes.ARC-II: Local-Global Hybrid Memory InARC-I, all tasks share a global memory, but canalso record task-specific information besides sharedinformation.
To address this, we allocate each taska local task-specific external memory, which canfurther write shared information to a global memoryfor all tasks.More generally, for task m, we assign each task-specific LSTM with a local memory M(m), followedby a global memory M(s), which is shared acrossdifferent tasks.The read and write operations of the local andglobal memory are defined asr(m)t = ?
(m)t M(m)t , (16)121Dataset Type Train Size Dev.
Size Test Size Class Avg.
Length Vocabulary SizeMovieSST-1 Sen. 8544 1101 2210 5 19 18KSST-2 Sen. 6920 872 1821 2 18 15KSUBJ Sen. 9000 - 1000 2 21 21KIMDB Doc.
25,000 - 25,000 2 294 392KProductBooks Doc.
1400 200 400 2 181 27KDVDs Doc.
1400 200 400 2 197 29KElectronics Doc.
1400 200 400 2 117 14KKitchen Doc.
1400 200 400 2 98 12KTable 1: Statistics of two multi-task datasets.
Each dataset consists of four related tasks.M(m)t = fwrite(M(m)t?1, ?
(m)t ,h(m)t ), (17)r(s)t = ?
(s)t?1M(s)t?1, (18)M(s)t = fwrite(M(s)t?1, ?
(s)t , r(m)t ), (19)where the superscript s represents the parametersare shared across different tasks; the superscript mrepresents that the parameters or variables are task-specific for task m.In ARC-II, the local memories enhance the capac-ity of memorizing, while global memory enables theinformation flowing from different tasks to interactsufficiently.4 TrainingThe task-specific representation h(m), emitted bythe deep muti-task architectures, is ultimately fedinto the corresponding task-specific output layers.y?
(m) = softmax(W(m)h(m) + b(m)), (20)where y?
(m) is prediction probabilities for task m.Given M related tasks, our global cost function isthe linear combination of cost function for all tasks.?
=M?m=1?mL(y?
(m), y(m)) (21)where ?m is the weights for each task m respec-tively.Computational Cost Compared with vanillaLSTM, our proposed two models do not cause muchextra computational cost while converge faster.
Inour experiment, the most complicated ARC-II, costs2 times as long as vanilla LSTM.MovieReviewsProductReviewsEmbedding dimension 100 100Hidden layer size 100 100External memory size (50,20) (50,20)Initial learning rate 0.01 0.1Regularization 0 1E?5Table 2: Hyper-parameters of our models.5 ExperimentIn this section, we investigate the empirical perfor-mances of our proposed architectures on two multi-task datasets.
Each dataset contains several relatedtasks.5.1 DatasetsThe used multi-task datasets are briefly described asfollows.
The detailed statistics are listed in Table 1.Movie Reviews The movie reviews dataset con-sists of four sub-datasets about movie reviews.?
SST-1 The movie reviews with five classes inthe Stanford Sentiment Treebank1 (Socher etal., 2013).?
SST-2 The movie reviews with binary classes.It is also from the Stanford Sentiment Tree-bank.?
SUBJ The movie reviews with labels of sub-jective or objective (Pang and Lee, 2004).?
IMDB The IMDB dataset2 consists of 100,000movie reviews with binary classes (Maas et al,2011).
One key aspect of this dataset is thateach movie review has several sentences.1http://nlp.stanford.edu/sentiment.2http://ai.stanford.edu/?amaas/data/sentiment/122Model SST-1 SST-2 SUBJ IMDB Avg?Single Task LSTM 45.9 85.8 91.6 88.5 -ME-LSTM 46.4 85.5 91.0 88.7 -Multi-taskARC-I 48.6 87.0 93.8 89.8 +(1.8/1.9)ARC-II 49.5 87.8 95.0 91.2 +(2.9/3.0)MT-CNN 46.7 86.1 92.2 88.4 -MT-DNN 44.5 84.0 90.1 85.6 -NBOW 42.4 80.5 91.3 83.6 -RAE (Socher et al, 2011) 43.2 82.4 - - -MV-RNN (Socher et al, 2012) 44.4 82.9 - - -RNTN (Socher et al, 2013) 45.7 85.4 - - -DCNN (Kalchbrenner et al, 2014) 48.5 86.8 - 89.3 -CNN-multichannel (Kim, 2014) 47.4 88.1 93.2 - -Tree-LSTM (Tai et al, 2015) 50.6 86.9 - - -Table 3: Accuracies of our models on movie reviews tasks against state-of-the-art neural models.
The lastcolumn gives the improvements relative to LSTM and ME-LSTM respectively.
NBOW: Sums up the wordvectors and applies a non-linearity followed by a softmax classification layer.
RAE: Recursive Autoencoderswith pre-trained word vectors from Wikipedia (Socher et al, 2011).
MV-RNN: Matrix-Vector RecursiveNeural Network with parse trees (Socher et al, 2012).
RNTN: Recursive Neural Tensor Network withtensor-based feature function and parse trees (Socher et al, 2013).
DCNN: Dynamic Convolutional NeuralNetwork with dynamic k-max pooling (Kalchbrenner et al, 2014; Denil et al, 2014).
CNN-multichannel:Convolutional Neural Network (Kim, 2014).
Tree-LSTM: A generalization of LSTMs to tree-structurednetwork topologies (Tai et al, 2015).Product Reviews This dataset3, constructed byBlitzer et al (2007), contains Amazon product re-views from four different domains: Books, DVDs,Electronics and Kitchen appliances.
The goal ineach domain is to classify a product review as ei-ther positive or negative.
The datasets in each do-main are partitioned randomly into training data, de-velopment data and testing data with the proportionof 70%, 20% and 10% respectively.5.2 Competitor Methods for Multi-taskLearningThe multi-task frameworks proposed by previousworks are various while not all can be applied to thetasks we focused.
Nevertheless, we chose two mostrelated neural models for multi-task learning and im-plement them as strong competitor methods .?
MT-CNN: This model is proposed by Collobertand Weston (2008) with convolutional layer, inwhich lookup-tables are shared partially whileother layers are task-specific.3https://www.cs.jhu.edu/?mdredze/datasets/sentiment/?
MT-DNN: The model is proposed by Liu etal.
(2015b) with bag-of-words input and multi-layer perceptrons, in which a hidden layer isshared.5.3 Hyperparameters and TrainingThe networks are trained with backpropagation andthe gradient-based optimization is performed usingthe Adagrad update rule (Duchi et al, 2011).The word embeddings for all of the models areinitialized with the 100d GloVe vectors (840B tokenversion, (Pennington et al, 2014)) and fine-tunedduring training to improve the performance.
Theother parameters are initialized by randomly sam-pling from uniform distribution in [?0.1, 0.1].
Themini-batch size is set to 16.For each task, we take the hyperparameters whichachieve the best performance on the developmentset via an small grid search over combinations ofthe initial learning rate [0.1, 0.01], l2 regularization[0.0, 5E?5, 1E?5].
For datasets without develop-ment set, we use 10-fold cross-validation (CV) in-stead.
The final hyper-parameters are set as Table 2.1235.4 Multi-task Learning of Movie ReviewsWe first compare our proposed models with thebaseline system for single task classification.
Table3 shows the classification accuracies on the moviereviews dataset.
The row of ?Single Task?
shows theresults of LSTM and ME-LSTM for each individ-ual task.
With the help of multi-task learning, theperformances of these four tasks are improved by1.8% (ARC-I) and 2.9% (ARC-II) on average rela-tive to LSTM.
We can find that the architecture oflocal-global hybrid external memory has better per-formances.
The reason is that the global memory inARC-I could store some task-specific informationbesides shared information, which maybe noisy toother tasks.
Moreover, both of our proposed mod-els outperform MT-CNN and MT-DNN, which indi-cates the effectiveness of our proposed shared mech-anism.
To give an intuitive evaluation of these re-sults, we also list the following state-of-the-art neu-ral models.
With the help of utilizing the shared in-formation of several related tasks, our results out-perform most of state-of-the-art models.
AlthoughTree-LSTM outperforms our method on SST-1, itneeds an external parser to get the sentence topologi-cal structure.
It is worth noticing that our models aregeneric and compatible with the other LSTM basedmodels.
For example, we can easily extend our mod-els to incorporate the Tree-LSTM model.5.5 Multi-task Learning of Product ReviewsTable 4 shows the classification accuracies on thetasks of product reviews.
The row of ?Single Task?shows the results of the baseline for each individ-ual task.
With the help of global shared memory(ARC-I), the performances of these four tasks areimproved by an average of 2.9%(2.6%) comparedwith LSTM(ME-LSTM).
ARC-II achieves best per-formances on three sub-tasks, and its average im-provement is 3.7%(3.5%).
Compared with MT-CNNand MT-DNN, our models achieve a better perfor-mance.
We think the reason is that our models cannot only share lexical information but share compli-cated patterns of sentences by reading/writing op-erations of external memory.
Furthermore, these re-sults on product reviews are consistent with that onmovie reviews, which shows our architectures arerobust.5.6 Case StudyTo get an intuitive understanding of what is happen-ing when we use shared memory to predict the classof text, we design an experiment to compare and an-alyze the difference between our models and vanillaLSTM, thereby demonstrating the effectiveness ofour proposed architectures.We sample two sentences from the SST-2 valida-tion dataset, and the changes of the predicted sen-timent score at different time steps are shown inFigure 3, which are obtained by vanilla LSTM andARC-I respectively.
Additionally, both models arebidirectional for better visualization.
To get moreinsights into how the shared external memory in-fluences the specific task, we plot and observe theevolving activation of fusion gates through time,which controls signals flowing from a shared exter-nal memory to task-specific output, to understandthe behaviour of neurons.For the sentence ?It is a cookie-cutter movie, acut-and-paste job.
?, which has a negative sentiment,while the standard LSTM gives a wrong predic-tion due to not understanding the informative words?cookie-cutter?
and ?cut-and-paste?.In contrast, our model makes a correct predictionand the reason can be inferred from the activation offusion gates.
As shown in Figure 3-(c), we can seeclearly the neurons are activated much when theytake input as ?cookie-cutter?
and ?cut-and-paste?,which indicates much information in shared mem-ory has be passed into LSTM, therefore enabling themodel to give a correct prediction.Another case ?If you were not nearly moved totears by a couple of scenes , you ?ve got ice water inyour veins?, a subjunctive clause introduced by ?if ?,has a positive sentiment.As shown in Figure 3-(b,d), vanilla LSTM failedto capture the implicit meaning behind the sentence,while our model is sensitive to the pattern ?If ... werenot ...?
and has an accurate understanding of thesentence, which indicates the shared memory mech-anism can not only enrich the meaning of certainwords, but teach some information of sentence struc-ture to specific task.124Model Books DVDs Electronics Kitchen Avg?Single Task LSTM 78.0 79.5 81.2 81.8 -ME-LSTM 77.5 80.2 81.5 82.1 -Multi-taskARC-I 81.2 82.0 84.5 84.3 +(2.9/2.6)ARC-II 82.8 83.0 85.5 84.0 +(3.7/3.5)MT-CNN 80.2 81.0 83.4 83.0 -MT-DNN 79.7 80.5 82.5 82.8 -Table 4: Accuracies of our models on product reviews dataset.
The last column gives the improvementrelative to LSTM and ME-LSTM respectively.It is a cookie-cutter movie cut-and-paste job .00.20.40.60.8LSTMOurs(a)If you were not nearly moved tears by couple scenes you ?ve got ice water your veins0.20.40.60.81LSTMOurs(b)It is a cookie?cutter movie cut?and?paste job .20406080100(c)If you were not nearly moved tears by couplescenes you ve got ice water your veins20406080(d)Figure 3: (a)(b) The change of the predicted sentiment score at different time steps.
Y-axis represents thesentiment score, while X-axis represents the input words in chronological order.
The red horizontal line givesa border between the positive and negative sentiments.
(c)(d) Visualization of the fusion gate?s activation.6 Related WorkNeural networks based multi-task learning has beenproven effective in many NLP problems (Collobertand Weston, 2008; Glorot et al, 2011; Liu et al,2015b; Liu et al, 2016b).
In most of these models,the lower layers are shared across all tasks, while toplayers are task-specific.Collobert and Weston (2008) used a shared rep-resentation for input words and solved different tra-ditional NLP tasks within one framework.
However,only one lookup table is shared, and the other lookuptables and layers are task-specific.Liu et al (2015b) developed a multi-task DNN forlearning representations across multiple tasks.
Theirmulti-task DNN approach combines tasks of queryclassification and ranking for web search.
But theinput of the model is bag-of-word representation,which loses the information of word order.More recently, several multi-task encoder-decoder networks were also proposed for neuralmachine translation (Dong et al, 2015; Luong etal., 2015; Firat et al, 2016), which can make use ofcross-lingual information.Unlike these works, in this paper we design twoneural architectures with shared memory for multi-task learning, which can store useful informationacross the tasks.
Our architectures are relativelyloosely coupled, and therefore more flexible to ex-pand.
With the help of shared memory, we can ob-tain better task-specific sentence representation byutilizing the knowledge obtained by other relatedtasks.7 Conclusion and Future WorkIn this paper, we introduce two deep architecturesfor multi-task learning.
The difference with the pre-vious models is the mechanisms of sharing infor-mation among several tasks.
We design an external125memory to store the knowledge shared by several re-lated tasks.
Experimental results show that our mod-els can improve the performances of several relatedtasks by exploring common features.In addition, we also propose a deep fusion strat-egy to integrate the information from the externalmemory into task-specific LSTM with a fusion gate.In future work, we would like to investigate theother sharing mechanisms of neural network basedmulti-task learning.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir valuable comments.
This work was partiallyfunded by National Natural Science Foundation ofChina (No.
61532011 and 61672162), the NationalHigh Technology Research and Development Pro-gram of China (No.
2015AA015408).ReferencesMarcin Andrychowicz and Karol Kurach.
2016.
Learn-ing efficient algorithms with hierarchical attentivememory.
arXiv preprint arXiv:1602.03218.D.
Bahdanau, K. Cho, and Y. Bengio.
2014.
Neural ma-chine translation by jointly learning to align and trans-late.
ArXiv e-prints, September.John Blitzer, Mark Dredze, Fernando Pereira, et al 2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InACL, volume 7, pages 440?447.Rich Caruana.
1997.
Multitask learning.
Machine learn-ing, 28(1):41?75.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 740?750.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalch-brenner, and Alex Graves.
2016.
Associative longshort-term memory.
CoRR, abs/1602.03032.Misha Denil, Alban Demiraj, Nal Kalchbrenner, PhilBlunsom, and Nando de Freitas.
2014.
Modelling,visualising and summarising documents with a sin-gle convolutional neural network.
arXiv preprintarXiv:1406.3830.Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, andHaifeng Wang.
2015.
Multi-task learning for multi-ple language translation.
In Proceedings of the ACL.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Jeffrey L Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016.Multi-way, multilingual neural machine translationwith a shared attention mechanism.
arXiv preprintarXiv:1601.01073.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proceed-ings of the 28th International Conference on MachineLearning (ICML-11), pages 513?520.Alex Graves, Greg Wayne, and Ivo Danihelka.2014.
Neural turing machines.
arXiv preprintarXiv:1410.5401.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Advances inNeural Information Processing Systems.Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.2015.
An empirical exploration of recurrent networkarchitectures.
In Proceedings of The 32nd Interna-tional Conference on Machine Learning.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of ACL.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
arXiv preprint arXiv:1408.5882.PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, andXuanjing Huang.
2015a.
Multi-timescale long short-term memory neural network for modelling sentencesand documents.
In Proceedings of the Conference onEMNLP.Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,Kevin Duh, and Ye-Yi Wang.
2015b.
Representa-tion learning using multi-task deep neural networks forsemantic classification and information retrieval.
InNAACL.126Pengfei Liu, Xipeng Qiu, Jifan Chen, and XuanjingHuang.
2016a.
Deep fusion LSTMs for text seman-tic matching.
In Proceedings of Annual Meeting of theAssociation for Computational Linguistics.Pengfei Liu, Xipeng Qiu, and Xuanjing Huang.
2016b.Recurrent neural network for text classification withmulti-task learning.
In Proceedings of InternationalJoint Conference on Artificial Intelligence.Minh-Thang Luong, Quoc V Le, Ilya Sutskever, OriolVinyals, and Lukasz Kaiser.
2015.
Multi-tasksequence to sequence learning.
arXiv preprintarXiv:1511.06114.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Pro-ceedings of the ACL, pages 142?150.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In Proceedings of ACL.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
Proceedings of the Empiricial Meth-ods in Natural Language Processing (EMNLP 2014),12:1532?1543.Richard Socher, Jeffrey Pennington, Eric H Huang, An-drew Y Ng, and Christopher D Manning.
2011.
Semi-supervised recursive autoencoders for predicting sen-timent distributions.
In Proceedings of EMNLP.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of EMNLP.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advances inNeural Information Processing Systems, pages 2431?2439.Kai Sheng Tai, Richard Socher, and Christopher D Man-ning.
2015.
Improved semantic representationsfrom tree-structured long short-term memory net-works.
arXiv preprint arXiv:1503.00075.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of ACL.Tian Wang and Kyunghyun Cho.
2015.
Larger-context language modelling.
arXiv preprintarXiv:1511.03729.127
