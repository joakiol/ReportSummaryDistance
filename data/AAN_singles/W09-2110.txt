Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 64?72,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing First and Second Language Models to Correct Preposition Errorsin Second Language AuthoringMatthieu Hermet Alain D?siletsSchool of Information Technology andEngineeringInstitute for Information TechnologyUniversity of Ottawa National Research Council of Canada800, King Edward, Ottawa,CanadaBldg M-50, Montreal Road, Ottawa, K1A 0R6,Canadamhermet@site.uottawa.ca alain.desilets@nrc-cnrc.gc.caAbstractIn this paper, we investigate a novel approachto correcting grammatical  and lexical  errorsin texts written by second language authors.Contrary to previous approaches which tendto use unilingual models of the user's secondlanguage (L2), this new approach uses a sim-ple  roundtrip  Machine  Translation  methodwhich  leverages  information about  both theauthor?s first (L1) and second languages.
Wecompare  the  repair  rate  of  this  roundtriptranslation approach to that of an existing ap-proach based on a unilingual L2 model withshallow  syntactic  pruning,  on  a  series  ofpreposition choice errors.
We find no statisti-cally significant  difference between the twoapproaches,  but find that a hybrid combina-tion of both does perform significantly betterthan either one in isolation.
Finally, we illus-trate how the translation approach has the po-tential  of  repairing  very  complex  errorswhich would be hard to treat without leverag-ing knowledge of the author's L1.1 IntroductionIn this paper, we investigate a novel approach tocorrecting grammatical  and lexical errors in textswritten  by  second  language  learners  or  authors.Contrary to previous approaches which tend to useunilingual  models  of  the  user's  second  language(L2), this new approach uses a translation modelbased on both the user's first (L1) and second lan-guages.
It has the advantage of being able to modellinguistic  interference  phenomena,  that  is,  errorswhich are produced through literal translation fromthe author's first language.
Although we apply thismethod in the context of French-as-a-Second-Lan-guage, its principles are largely independent of lan-guage, and could also be extended to other classesof errors.
Note that this is preliminary work which,in a first step, focuses on error correction, and ig-nores for now the preliminary step of error detec-tion which is left for future research.This work is of interest to applications in Comput-er-Assisted-Language-Learning (CALL) and Intel-ligent Tutoring Systems (ITS), where tutoring ma-terial  often  consists  of  drills  such  as  fill-in-the-blanks or multiple-choice-questions.
These requirevery little  use  of  a learner's  language productioncapacities, and in order to support richer free-textassessment capabilities, ITS systems thus need touse  error  detection  and  correction  functionalities(Heift and Schulze, 2007).Editing Aids (EA) are tools which assist a user inproducing written compositions.
They typically userules  for  grammar  checking  as  well  as  lexicalheuristics to suggest stylistic tips, synonyms or fal-lacious collocations.
Advanced examples  of  such64tools include Antidote1 for French and StyleWriter2for English.
Text Editors like MS Word and WordPerfect  also include grammar  checkers,  but  theirstyle checking capabilities tend to be limited.
Allthese tools can provide useful assistance to editingstyle,  but  they  were  not  designed  to  assist  withmany errors found typically in highly non-idiomat-ic sentences produced by L2 authors.Recent work in the field of error correction, espe-cially as applied to English in the context of En-glish as  a  Second Language (ESL),  show an in-creasing  use  of  corpora  and  language  models.These have the advantage of offering a model  ofcorrectness based on common usage, independent-ly of any meta-information on correctness.
Corpus-based approaches  are  also able  to  correct  higherlevel lexical-syntactic errors, such as the choice ofpreposition which is  often semantically governedby other parts of the sentence.The reminder of this paper is organized as follows.In  section  2,  we  give  a  detailed  account  of  theproblem of  preposition  errors  in  a  Second  Lan-guage Learning (SLL) context.
Related work is re-viewed in section 3 and the algorithmic frameworkis  presented  in  section  4.
An  evaluation  is  dis-cussed in section 5, and conclusions and directionsfor future research are presented in section 6.2 The Preposition ProblemPrepositions constitute 14% of all tokens producedin most languages (Fort & Guillaume 2007).
Theyare  reported as  yielding  among  the  highest  errorclass rates across various languages (Izumi, 2004,for Japanese, Granger et al, 2001, for French).
Intheir analysis of a small corpus of advanced-inter-mediate French as a Second Language (FSL) learn-ers,  Hermet  et  al.
(2008)  found  that  prepositionchoice accounted for 17.2 % of all errors.
Preposi-tions can be seen as a special class of cognates, inthe sense that the same L1 preposition used in dif-ferent L1 sentences, could translate to several dif-ferent L2 prepositions.Automatic error detection/correction methods oftenprocess prepositions and determiners in the sameway because they both fall in the class of function-1 www.druide.com2 www.stylewriter-usa.comwords.
However, one can make the argument thatpreposition errors  deserve a  different  and deeperkind of treatment, because they tend to be more se-mantically motivated (event hough some preposi-tions governed by verbs draw a purely functionalrelation).
In contrast, determiners are not semanti-cally motivated  and only vary on the  register  ofquantity (or genre in some languages).For example, there are 37 determiners in French,most of which can be used interchangeably withoutsignificantly affecting the syntax of a sentence, andoften,  not  even  its  meaning  ("I'll  have  onecoffee"/"I'll  have  a  coffee"/"I'll  have  somecoffee"/"I'll have my coffee"/"I'll have coffee" areall rather alike).
Comparatively, there are 85 sim-ple prepositions and 222 compounds ones and theycannot  be  used  interchangeably  without  signifi-cantly modifying the sense of an utterance, exceptfor cases of synonymy.In this paper, we focus our attention on prepositioncorrection only, as it seems to be a more complexproblem than determiners.
While in principle themethods  described  here  could  handle  determinererrors, we feel that our framework, which involvesparsing in combination with a very large languagemodel and Machine Translation, constitutes heav-ier  machinery than  is  warranted  for  that  simplerproblem.There are two major causes of preposition errors ina SLL context.
The first kind is caused by lexicalconfusion  within  the  second language  itself.
Forexample, a L2 author writing in English may erro-neously use a location preposition like "at" whereanother location preposition like  "in" would havebeen more appropriate.
The second kind involveslinguistic interference between prepositions in L1and prepositions in L2 (Granger et al, 2001).
Forexample, a Second Language Learner who wantsto render the following two English sentences inFrench "I go to Montreal" and "I go to Argentina",might use the same French preposition "?"
for "to",when in fact, French usage dictates that you write"?
Montr?al?,  and  "en Argentine?.
Note  that  thesituation varies greatly from language to language.The same two English sentences rendered in Italianand German would in fact employ a same preposi-tion,  whereas  in  Spanish,  different  prepositionswould also be required as in French.65Studies have found that the majority of errors madeby L2 authors (especially intermediate to advancedones)  are  caused  by  such  linguistic  interference(Wang and Garigliano, 1992, Cowan, 1983, p 109).Note that this kind of linguistic interference can of-ten lead to much more severe and hard to repair er-rors, as illustrated by the following example, takenfrom an actual SLL corpus.
Say a native Englishauthor wants to render "Police arrived at the sceneof the crime" into French (her L2).
Because she isnot fluent in French, she translates the last part ofthe sentence to "?
la sc?ne de la crime".
This liter-al translation turns out to be highly unidiomatic inFrench, and should instead be written as  "sur leslieux du crime" (which in English, would translateliterally to "on the location of the crime").One  might  suspect  that  preposition  errors  of  thefirst  type  would be solvable  using unilingual  L2language models,  but  that  the second type  mightbenefit from a language model which also takes L1into account.
This is the main question investigatedin this paper.3 Related WorkHistorically, grammatical error correction has beendone  through  parsing-based  techniques  such  assyntactic  constraint-relaxation  (L'haire  &  Vande-venter-Feltin,  2003),  or  mal-rules  modeling(Schneider and McCoy, 1998).
But generating therule-bases needed by these types of approaches in-volves a lot of manual work, and may still in theend be too imprecise to convey information on thenature and solution of an error.
Recently, more ef-fort has been put in methods that rely on automati-cally built language models.
Typically, this kind ofwork will focus either on a restricted class of errorsor on specific domains.
Seneff and Lee (2006) pro-pose  a  two-phased  generation-based  frameworkwhere  a n-gram model  re-ranked by a stochasticcontext-free-grammar model is used to correct sen-tence-level errors in the language domain of flightreservation.
Brockett  et  al.
(2006)  used a Brownnoise channel translation model to record patternsof  determiner  error  correction  on  a  small  set  ofmass-nouns,  and  reducing  the  error  spectrum  inboth class and semantic domain, but adding detec-tion  capabilities.
Note  that  although  they  use  atranslation model, it processes only text that is inone  language.
More  specifically,  the  systemlearned to "translate" from poorly written Englishinto correctly written English.Chodorow et al (2007) employed a maximum en-tropy  model  to  estimate  the  probability  of  34prepositions  based  on  25  local  context  featuresranging from words  to  NP/VP chunks.
They uselemmatization  as  a  means  of  generalization  andtrained  their  model  over  7  million  prepositionalcontexts,  achieving results  of  84% precision and19% recall in preposition error detection in the bestof the system's configurations.
Gamon et al (2008)worked on a  similar  approach using only taggedtrigram left and right contexts: a model of preposi-tions uses serves to identify preposition errors andthe Web provides examples of correct form.
Theyevaluate their framework on the task of prepositionidentification and report results ranging from 74 to45% precision on a set of 13 prepositions.Yi et al (2008) use the Web as corpus and sendsegments of sentences of varying length as bag-of-constituents  queries  to  retrieve  occurrence  con-texts.
The number of the queried segments is a PoScondition of "check-points" sensitive to typical er-rors  made  by L2 authors.
The contexts  retrievedare in turn analyzed for correspondence with theoriginal input.
The detection and correction meth-ods differ according to the class of the error.
Deter-miner errors call for distinct detection and correc-tion  procedures  while  collocation  errors  use  thesame procedure for both.
Determiner errors are dis-covered by thresholds ratios on search hits statis-tics,  taking  into  account  probable  ambiguities,since multiple forms of determiners can be valid ina  single  context.
Collocation  errors  on  the  otherhand, are assessed only by a threshold on absolutecounts, that is, a form different from the input au-tomatically signals an error and provides its correc-tion.
This  suggests  that  detection  and  correctionprocedures coincide when the error ceases to bearon a function word.Similarly, Hermet et al (2008) use a Web as cor-pus  based  approach  to  address  the  correction  ofpreposition  errors  in  a  French-as-a-Second-Lan-guage  (FSL)  context.
Candidate  prepositions  aresubstituted for erroneous ones following a taxono-my of semantic classes, which produces a set of al-66ternate sentences for each error.
The main interestof their study is the use of a syntax-based sentencegeneralization method to maximize the likelihoodthat  at  least  one  of  the  alternatives  will  have  atleast one hits on the Web.
They achieve accuracyof 69% in error repair  (no error detection),  on asmall set of clauses written by FSL Learners.Very little work has been done to actually exploitknowledge of a L2 author's first language, in cor-recting  errors.
Several  authors  (Wang  andGarigliano,  1992,   Anderson,  1995,  La  Torre,1999, Somers, 2001) have suggested that studentsmay learn by analyzing erroneous sentences pro-duced by a MT system, and reflecting on the prob-able cause of errors, especially in terms of interfer-ence between the  two languages.
In  this  contexthowever, the MT system is used only to generateexercises,  as opposed to helping the student  findand correct errors in texts that he produces.Although it is not based on an MT model, Wangand Garigliano propose an algorithm which uses ahand-crafted,  domain-specific,  mixed  L1  and  L2grammar, in order to identify L1 interference errorsin L2 sentences.
L2 sentences are parsed with thismixed  grammar,  giving priority to  L2 rules,  andonly employing L1 rules as a last resort.
Parts ofthe sentence which required the user of  L1 rulesare  labeled  as  errors  caused  by  L1  interference.The paper does not present an actual evaluation ofthe algorithm.Finally, a patent by Dymetman and Isabelle (2005)describes  several  ways  in  which  MT technologycould  be  used  to  correct  L2  errors,  but  to  ourknowledge,  none of  them has  been implementedand evaluated yet.4 Algorithmic FrameworkAs discussed in section 2, L2 authoring errors canbe caused by confusions within the L2 itself, or bylinguistic interference between L1 and L2.
In orderto account for this duality, we investigate the useof two correction strategies, one which is based onunilingual models of L2, and one which is basedon translation models between L1 and L2.The first approach, called the  Unilingual strategy,is illustrated by the example in Figure 1.
It  uses aweb search engine (Yahoo) as a simple, unilinguallanguage  model,  where  the  probability  of  a  L2phrase is estimated simply by counting its numberof occurrences in Web pages of that language.
Asevere limitation of this kind of model is that it canonly estimate the probability of phrases that appearat least once on the Web.
In contrast, an N-grammodel (for example) is able to estimate the proba-bility of phrases that it has never seen in the train-ing corpus.
In  order  to  deal  with this  limitation,syntactic pruning is therefore applied to the phrasebefore it is sent to the search engine, in order toeliminate parts which are not core to the context ofuse of  the  preposition,   thus  increasing the oddsthat the pruned sentence will have at least one oc-currence on the Web.This pruning and generalization is done by carry-ing  out  syntactic  analysis  with  the  Xerox  Incre-mental Parser for the syntactic analysis (Ref XIP).XIP is an error robust, symbolic, dependency pars-er, which outputs syntactic information at the con-stituency and dependency levels.
Its ability to pro-duce syntactic analyses in the presence of errors isInput SentenceIl y a une grande fen?tre qui permet au soleil <?>entrer(there is a large window which lets the sun come in)Syntactic Pruning and Lemmatizationpermettre <?> entrer(let come in)Generation of alternate prepositionssemantically related:  dans, en, chez, sur, sous,au, dans, apr?s, avant, en, versmost common: de, avec, par, pourQuery and sort alternative phrasespermettre d'entrer: 119 000 hitspermettre avant entrer: 12 hitspermettre ?
entrer: 4 hitspermettre en entrer: 2 hits...?
preposition <d'> is returned as correctionFigure 1.
Typical  processing carried out by the  Unilingualapproach.67particularly  interesting  in  the  context  of  secondlanguage authoring where the sentences producedby the authors can be quite far from grammaticalcorrectness.
The input sentence is fed to the parseras two segments split at error point (in this case, atthe location of the erroneous preposition).
This en-sures that the parses are correct and not affected atdependency  level  by  the  presence  of  error.
Thesyntactic analyses are needed to perform syntacticpruning, which is a crucial step in our framework,following  Hermet  et.
al  (2008).
Pruning  is  per-formed by way of chunking heuristics, which arecontrolled  by  grammatical  features,  provided  byXIP's  morphological  analysis  (PoS  tagger).
Theheuristics  are  designed  to  suppress  syntacticallyextraneous  material  in  the  sentence,  such  as  ad-verbs, some adjectives and some NPs.
Adverbs areremoved in all cases, while adjectives are only re-moved when they are not in a position to govern aPrepositional  Phrase.
NPs are suppressed in con-trolled cases, based on the verb sub-categorizationframe, when a PP can be attached directly to thepreceding verb.
In case of ambiguity in the attach-ment  of the PP,  two versions of the pruned sen-tence can be produced reflecting two different PPattachments.
Lemmatization  of  verbs  is  also car-ried out in the pruning step.After pruning, the right and left sides of the sen-tences are re-assembled with alternate prepositions.The replacement  of  prepositions  is  controlled byway of semantics.
Since prepositions are richer insense than strict function words, they can thereforebe categorized according to semantics.
Saint-Dizier(2007)  proposes  such  a  taxonomy,  and  in  ourframework,  prepositions  have  been  grouped in  7non-exclusive categories.
Table 1 provides detailsof  this  categorization.
The  input  preposition  ismapped  to  all  the  sets  it  belongs  to,  and  corre-sponding alternates are retrieved as correction can-didates.
The  6  most  frequent  French  prepositionare also added automatically to the candidates list.The resulting sentences are then sent to the YahooSearch Engine and hits are counted.
The number ofhits returned by each of the queries is used as deci-sion criteria, and the preposition contained in thequery with the most hits is selected as the correc-tion candidate.While  the above  Unilingual strategy might  workfor simple cases of L1 interference, one would notexpect it to work as well  in more complex caseswhere both the preposition and its governing partshave been translated too literally.
For example, inthe case of the example from section 2, while theUnilingual strategy might be able to effect correc-tion  ?sur la sc?ne du crime?
which is marginallybetter than the original ??
la sc?ne du crime?
(12Khits versus 1K), it cannot address the root of theproblem,  that  is,  the  unidiomatic  expression?sc?ne  du  crime?
which  should  instead  be  ren-dered as ?lieux du crime?
(38K hits).
In this partic-ular case, it is not really an issue because it so hap-pens that ?sur?
is the correct preposition to use forboth  ?lieux du crime?
and  ?sc?ne du crime?, butin our experience, that is not always the case.
Notealso  that  the  Unilingual approach  can  only  dealwith preposition errors (although it would be easyenough  to  extend  it  to  other  kinds  of  functionwords),  and  cannot  deal  with  more  semanticallydeep L1 interference.To address these issues,  we experimented with asecond  strategy  which  we  will  refer  to  as  theRoundtrip  Machine  Translation approach  (orRoundtrip MT for short).
Note that our approach isdifferent from that of Brockett et al (2006), as wedo  make  use  of  a  truly  multi-lingual  translationmodel.
In  contrast,  Brockett?s  translation  modelwas trained on texts that were written in the samelanguage, with the sources being ill-written text inthe  same  language as  the  properly-formed  targettexts.
One drawback of our approach however isCategory PrepositionsLocalization in front, behind, after, before, above,in, at, on, below, above...Temporal at, in, after, before, for, during,since...Cause for, because ofGoal for, atManner in, by, with, according to...Material in, ofPossession/Rela-tionto, at, with respect to...Most common to, at, on, with, by, forTable 1.
Categories of prepositions ?
the list is given in En-glish, and  non exhaustive for space reasons.68that it may require different translation models forspeakers with different first languages.There  are  many  ways  in  which  error-correctioncould be carried out using MT techniques.
Severalof these have been described in a patent by Dymet-man  and Isabelle  (2005),  but  to  our  knowledge,none of them have yet been implemented and eval-uated.
In this paper, we use the simplest possibleimplementation of this concept, namely,  we carryout a single round-trip translation.
Given a poten-tially erroneous L2 sentence written by a secondlanguage author, we translate it to the author's L1language, and then back to L2.
Even with this sim-ple approach, we often find that errors which werepresent in the original L2 sentence have been re-paired  in  the  roundtrip  version.
This  may soundsurprising,  since  one  would  expect  the  roundtripsentence to be worse than the original, on accountof the "Chinese Whisper" effect.
Our current theo-ry for why this is not the case in practice goes asfollows.
In the course of translating the original L2sentence to L1, when the MT system encounters apart  that  is  ill-formed,  it  will  tend  to  use  singleword entries from its phrase table, because longerphrases will not have been represented in the well-formed L2 training data.
In other words, the systemtends to generate a word for word translation of ill-formed parts,  which mirrors exactly what L2 au-thors do when they write poorly formed L2 sen-tences  by  translating  too  literally  from their  L1thought.
As a result, the L1 sentence produced bythe MT system is often well formed for that lan-guage.
Subsequently, when the MT system tries totranslate that well-formed L1 sentence back to L2,it  is  therefore able to use longer entries from itsphrase table, and hence produce a better L2 trans-lation of that part than what the author originallyproduced.We use Google Translate as a translation enginefor matter of simplicity.
A drawback of using suchan online service is that it  is essentially a closedbox, and we therefore have little control over thetranslation process,  and no access  to  lower  leveldata generated by the system in the course of trans-lation (e.g.
phrase alignments between source andtarget sentences).
In particular, this means that wecan only generate one alternative L2 sentence, andhave no way of assessing which parts of this singlealternative have a high probability of being betterthan their  corresponding parts  in  the  original  L2sentence written by the author.
In other words, wehave no way of telling which changes are likely tobe false positives, and which changes are likely tobe true positives.
This is the main reason why wefocus only on error repair in this preliminary work.The  roundtrip  sentences  generated  with  GoogleTranslate often differ significantly from the origi-nal L2 sentence, and in more ways than just the er-roneous preposition used by the author.
For exam-ple, the (pruned) clause "avoir du succ?s en le re-crutement" ("to be successful in recruiting") mightcome back as as "r?ussir ?
recruter" ("to succeedin recruiting").
Here, the translation is acceptable,but the preposition used by the MT system is notappropriate for use in the original sentence as writ-ten  by  the  L2  author.
Conversely,  a  roundtriptranslation can be ill-formed, yet use a prepositionwhich would be correct in the original L2 sentence.For example, "regarder ?
des films" ("look at somemovies") might come back as "inspecter des films"("inspect some films").
Here, the original meaningis somewhat lost, but the system correctly suggest-ed that there should be no preposition before ?desfilms?.Hence,  in  the  context  of  the  Roundtrip  MT ap-proach, we need two ways of measuring appropri-ateness  of  the  suggested  corrections  for  givenclauses.
The  first  approach,  which  we  call  theClause criteria, looks at whether or not the wholeclause  has  been  restored  to  a  correct  idiomaticform (including correct use of preposition) whichalso preserves the meaning intended by the authorof the original sentence.
Hence, according to thisapproach, an MT alternative may be deemed cor-rect, even if it chooses a preposition which wouldhave been incorrect if substituted in the original L2sentence as is.
In the second approach, called thePrep criteria, we only look at whether the preposi-tion used by the MT system in the roundtrip trans-lation, corresponds to the correct preposition to beused in the original L2 clause.
Hence, with this ap-proach, an MT alternative may be deemed correct,even if the preposition chosen by the MT system isactually inappropriate in the context of the generat-ed  roundtrip  translation,  or,  even  worse,  if  theroundtrip modified the clause to a point where itactually means something different than what theauthor actually intended.69Of course, in the case of the Prep evaluation crite-ria, having the MT system return a sentence whichemploys the proper preposition to use in the con-text of the original L2 sentence is not the end ofthe  process.
In  an  error  correction  context,  onemust also isolate the correct preposition and insertit in the appropriate place in the original L2 sen-tence.
This part of the processing chain is not cur-rently implemented, but would be easy to do, if weused an MT system that provided us with the align-ment information between the source sentence andthe target sentence generated.
The accuracy figureswhich  we  present  in  this  paper  assume  that  thismapping has been implemented and that this par-ticular part of the process can be done with 100%accuracy  (a  claim  which,  while  plausible,  stillneeds to be demonstrated in future work).We also investigate a third strategy called Hybrid,which uses the Roundtrip MT approach as a back-up for cases where the Unilingual approach is un-able  to  distinguish  between  different  choices  ofpreposition.
The  latter  typically  occurs  when thesystem is not able to sufficiently prune and gener-alize the phrase, resulting in a situation where allpruned variants yield zero hits on the Web, no mat-ter what preposition is used.
One could of coursealso use the  Unilingual approach as a backup forthe  Roundtrip  MT approach,  but  this  would  beharder to implement since the MT system alwaysreturns  an  answer,  and  our  use  of  the  onlineGoogle Translate system precludes any attempt toestimate the confidence level of that answer.In conclusion to this section, we use three preposi-tion  correction  strategies:  Unilingual,  RoundtripMT and  Hybrid, and in the case of the  RoundtripMT approach,  appropriateness  of  the  correctionscan  be  evaluated  using  two  criteria:  Prep andClause.5 Evaluation and Results5.1 Corpus and Evaluation MetricFor  evaluation,  we  extracted  clauses  containingpreposition  errors  from  a  small  corpus  of  textswritten by advanced-intermediate French as a Sec-ond Language (FSL) student in the course of onesemester.
The  corpus  contained  about  50,  000words  and  133  unique  preposition  errors.
Whilerelatively  small,  we  believe  this  set  to  be  suffi-ciently rich to test the approach.
Most clauses alsopresented  other  errors,  including  orthographic,tense,  agreement,  morphologic  and  auxiliary  er-rors, of which only the last two affect parsing.
Theclauses were fed as is to the correction algorithms,without first fixing the other types of errors.
But toour surprise, XIP's robust parsing has proven resis-tant in that it produced enough information to en-able correct pruning based on chunking informa-tion, and we report no pruning errors.
Chodorow etal.
(2008) stress the importance of agreement be-tween  annotators  when  retrieving  or  correctingpreposition errors.
In our case, our policy has beento only retain errors reported by both authors ofthis paper, and correction of these errors has raisedlittle matter of dispute.We evaluated the various algorithms in terms of re-pair rate, that is, the percentage of times that the al-gorithm proposed an appropriate fix (the absenceof a suggestion was taken to be an inappropriatefix).
These figures are reported in Table 2.5.2 DiscussionANOVA of the data summarized in Table 2 revealsa statistically significant (p < 0.001) effect of thealgorithm on repair rate.
Although  Roundtrip MTperformed slightly worse than  Unilingual  (66.4%versus 68.7%), this difference was not found to bestatistically  significant.
On  one  hand,  we  foundthat  round-trip  translation  sometimes  result  inspectacular restorations of long and clumsy phrasescaused by complex linguistic interference.
Howev-er, too often the Chinese whispers effect destroyedthe sense of the original phrase, resulting in inap-propriate suggestions.
This is evidenced by the factthat repair rate of the Roundtrip MT approach wassignificantly  lower  (p  <  0.001)  when  using  theAlgorithm Repair rate (%)Unilingual 68.7Roundtrip MT (Clause) 44.8Roundtrip MT (Prep) 66.4Hybrid (Prep) 82.1Table 2.
Results for 3 algorithms on 133 sentences.70Clause criteria (44.8%) than when using the  Prepcriteria (66.4%).
It seems that, in the case of prepo-sition correction,  roundtrip MT is best  used as away to to generate an L2 alternative from which tomine  the  correct  preposition.
Indeed,  flawed  asthey are,  these  distorted  roundtrip  segments  cor-rected prepositions  errors  in  66.4% of  the  cases.However, for a full picture, the approach should betried on more data, and on other classes of errors.Particularly,  we  currently  lack  sufficient  data  totest the hypothesis that the approach could addressthe correction of more complex literal translationsby SL Learners.In the Unilingual approach, the Yahoo Web searchengine proved to be an insufficient language modelfor   31 cases  out  of  133,  meaning that  even thepruned and generalized phrases  got  zero hits,  nomatter  what  alternative  preposition  was  used.
Inthose cases,  the  Hybrid approach would then at-tempt  correction  using  MT  Roundtrip approach.This turned out to work quite well, since it resultedin an overall accuracy of 82.1%.
ANOVA on thedata for  Hybrid and the two pure approaches re-veals a significant effect (p < 0.001) of the algo-rithm factor.
Individual t-tests between the Hybridapproach and each of the two pure approaches alsoreveal  statistically  significant  differences  (p  <0.001).
The improvements provided by the hybridapproach are fairly substantial, and represent rela-tive gains of 19.5% over the pure  Unilingual ap-proach, and 23.6% over the pure Roundtrip MT ap-proach.
The  success  of  this  combined  approachmight  be attributable to the fact that the two ap-proaches  follow  different  paradigms.
RoundtripMT uses a model  of controlled incorrectness (er-rors of anglicism) and  Unilingual a model of cor-rectness (occurrences of correct forms).
In this re-spect,  the  relatively  low  agreement  between  thetwo approaches (65.4%) is not surprising.6 Conclusion and Future WorkIn this paper,  we have demonstrated for the firsttime that a bilingual Machine Translation approachcan be used to good effect to correct errors in textswritten by Second Language Authors or Learners.In the case of preposition error correction we foundthat,  while  the MT approach on its  own did notperform significantly better  than a unilingual  ap-proach,  a  hybrid  combination  of  both  performedmuch  better  than  the  unilingual  approach  alone.More work needs to be carried out in order to fullyevaluate the potential of the MT approach.
In par-ticular, we plan to experiment with this kind of ap-proach to deal with more complex cases of L1 in-terference  which  result  in  severely  damaged  L2sentences.In this paper, we compared the bilingual MT ap-proach to a unilingual baseline which used a rela-tively simple  Web as  a corpus algorithm,  whoseaccuracy is comparable to that reported in the liter-ature for a similar preposition correction algorithm(Yi et al 2008).
Notwithstanding the fact that suchsimple  Web  as  a  corpus  approaches  have  oftenbeen shown to be competitive with (if not betterthan)  more  complex  algorithms  which  cannotleverage the full extent of the web (Halevy et al,2009), it would be interesting to compare the bilin-gual MT approach to more sophisticated unilingualalgorithms  for  preposition  correction,  many  ofwhich are referenced in section 3.Error detection is another area for future research.In this paper, we limited ourselves to error correc-tion, since it could be solved through a very simpleround-trip translation, without requiring a detailedcontrol of the MT system, or access to lower levelinformation generated by the system in the courseof translation (for example, intermediate hypothe-ses  with  probabilities  and  alignment  informationbetween source and target sentences).
In contrast,we  believe  that  error  detection  with  an  MT ap-proach will require this kind of finer control andaccess to the guts of the MT system.
We plan to in-vestigate  this  using  the  PORTAGE  MT  system(Ueffing et al, 2007).
Essentially, we plan to usethe  MT  system's  internal  information  to  assignconfidence  scores  to  various  segments  of  theroundtrip translation, and label them as correctionsif this confidence is above a certain threshold.
Indoing this, we will be following in the footsteps ofYi et al (2008) who use the same algorithm for er-ror detection and error correction.
The process ofdetecting  an  error  is  simply  one  of  determiningwhether the system's topmost alternative is differ-ent  from what  appeared in  the original  sentence,and whether the system's confidence in that alter-native is sufficiently high to take the risk of pre-senting it to the user as a suggested correction.71AcknowledgmentsThe authors are indebted to the following people(all from NRC) for helpful advice on how to bestexploit MT for second language correction: PierreIsabelle, George Foster and Eric Joanis.ReferencesAnderson, D. D. 1995.
Machine Translation As a Toolin Second Language Learning.
CALICO Journal, v13n1 p68-97.Brockett C., Dolan W. B., and Gamon M.. 2006.
Cor-recting ESL errors using phrasal SMT techniques.
InProc.
21st International Conf.
On Computational Lin-guistics and the 44th annual meeting of the ACL, p.249?256, Sydney, Australia.Chodorow  M.,  Tetreault  J.  R.  and  Han  N.-R..  2007.Detection of Grammatical Errors Involving Preposi-tions.
In Proc.
ACL-SIGSEM Workshop on Preposi-tions.
Prague, Czech Republic.Cowan, J. R. 1983.
Towards a Psychological Theory ofInterference in Second Language Learning.
In  Sec-ond Language Learning: Contrastive Analysis, ErrorAnalysis, and Related Aspects, edited by B. W. Robi-nett, J. Schachter, pp 109-119, The Univ.
of Michi-gan Press.Dymetman M., Isabelle, P. 2007.
Second language writ-ing advisor.
US Patent #20070033002 , Feb 8, 2007.Fort  K.,  Guillaume B.
2007.
PrepLex: un lexique despr?positions  du  fran?ais  pour  l'analyse  syntaxique.TALN 2007, Toulouse, June 5-8.Gamon  M.,  Gao  J.  F.,  Brockett  C.,  Klementiev  A.,Dolan W. B., and Vanderwende L. 2008.
Using con-textual speller techniques and language modeling forESL  error  correction.
In  Proceedings  of  IJCNLP2008, Hyderabad, India, January.Granger  S.,  Vandeventer  A.
&  Hamel  M.  J.
2001.Analyse de corpus d'apprenants  pour l'ELAO bas?sur le TAL.
TAL 42(2), 609-621.Halevy,  A., Norvig,  P., Pereira,  F. 2009.
"The Unrea-sonable  Effectiveness  of  Data.
",  IEEE  IntelligentSystems, March/April 2009, pp 8-12.Heift, T. & Schulze, M. 2007.
Errors and Intelligencein  Computer-Assisted  Language  Learning:  Parsersand Pedagogues.
Routledge.Hermet, M., D?silets, A., Szpakowicz, S. 2008.
Usingthe Web as a Linguistic Resource to AutomaticallyCorrect  Lexico-Syntactic  Errors.
In  Proceedings  ofthe LREC'08.
Marrakech, Morroco.Izumi,  E.,  K.  Uchimoto,  and  H.  Isahara.
2004.
Theoverview of the sst speech corpus of Japanese learn-er English and evaluation through the experiment onautomatic detection of learners?
errors.
In LREC.La Torre, M. D. 1999.
A web-based resource to imprivetranslation skills.
ReCALL, Vol 11, No3, pp.
41-49.Lee J. and Seneff S. 2006.
Automatic grammar correc-tion  for  second-language  learners.
In  Interspeech.ICSLP.
p. 1978-1981.
Pittsburgh.L'haire S. & Vandeventer Faltin A.
2003.
Error diagno-sis in the FreeText project.
CALICO 20(3), 481-495,special Issue Error Analysis and Error Correction inComputer-Assisted  Language  Learning,  T.  Heift  &M. Schulze (eds.
).Schneider, D. and McCoy, K. F. 1998.
Recognizing syn-tactic errors in the writing of second language learn-ers.
In Proceedings of COLING/ACL 98.Saint-Dizier, P. 2007.
Regroupement des Pr?positionspar sens.
Undated Report.
IRIT.
Toulouse.http://www.irit.fr/recherches/ILPL/Site-Equipe/publi_fichier/prepLCS.docSomers, Harold.
2001.
Three Perspectives on MT in theClassroom, MT SUMMIT VIII Workshop onTeaching Machine Translation, Santiago deCompostela, pages 25-29.Tetreault  J.  and  Chodorow  M.  2008.
The  Ups  andDowns  of  Preposition  Error  Detection.
COLING,Manchester.Ueffing,  N.,  Simard,  M.,  Larkin,  S.,  Johnson,  J.  H.(2007),  NRC's  PORTAGE system for  WMT 2007,ACL-2007 Workshop on SMT, Prague,  Czech Re-public 2007.Wang, Y. and Garigliano, R. 1992.
An Intelligent Lan-guage Tutoring System for Handling Errors causedby Transfer.
In Proceedings of ITS-92, pp.
395-404.Yi X., Gao J. F., Dolan W. B., 2008.
A Web-based En-glish Proofing System for English as a Second Lan-guage Users.
In Proceedings of IJCNLP 2008, Hy-derabad, India, January.72
