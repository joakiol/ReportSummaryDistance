Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 43?46,Athens, Greece, 30 March, 2009. c?2009 Association for Computational LinguisticsThe Interaction of Syntactic Theory and Computational PsycholinguisticsFrank KellerSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKkeller@inf.ed.ac.uk1 IntroductionTypically, current research in psycholinguisticsdoes not rely heavily on results from theoreticallinguistics.
In particular, most experimental workstudying human sentence processing makes verystraightforward assumptions about sentence struc-ture; essentially only a simple context-free gram-mar is assumed.
The main text book in psycholin-guistics, for instance, mentions Minimalism in itschapter on linguistic description (Harley, 2001,ch.
2), but does not provide any details, and all theexamples in this chapter, as well as in the chapterson sentence processing and language production(Harley, 2001, chs.
9, 12), only use context-freesyntactic structures with uncontroversial phrasemarkers (S, VP, NP, etc.).
The one exception istraces, which the textbook discusses in the contextof syntactic ambiguity resolution.Harley?s (2001) textbook is typical of experi-mental psycholinguistics, a field in which mostof the work is representationally agnostic, i.e., as-sumptions about syntactic structure left implicit,or limited to uncontroversial ones.
However, thesituation is different in computational psycholin-guistics, where researchers built computationallyimplemented models of human language process-ing.
This typically involves making one?s theoret-ical assumptions explicit, a prerequisite for beingable to implement them.
For example, Crocker?s(1996) model explicitly implements assumptionsfrom the Principles and Parameters framework,while Hale (2006) uses probabilistic Minimalistgrammars, or Mazzei et al (2007) tree-adjoininggrammars.Here, we will investigate how evidence regard-ing human sentence processing can inform our as-sumptions about syntactic structure, at least in sofar as this structure is used in computational mod-els of human parsing.2 Prediction and IncrementalityEvidence from psycholinguistic research suggeststhat language comprehension is largely incremen-tal, i.e., that comprehenders build an interpretationof a sentence on a word-by-word basis.
Evidencefor incrementality comes from speech shadow-ing, self-paced reading, and eye-tracking studies(Marslen-Wilson, 1973; Konieczny, 2000; Tanen-haus et al, 1995): as soon as a reader or listenerperceives a word in a sentence, they integrate it asfully as possible into a representation of the sen-tence thus far.
They experience differential pro-cessing difficulty during this integration process,depending on the properties of the word and its re-lationship to the preceding context.There is also evidence for full connectivity inhuman language processing (Sturt and Lombardo,2005).
Full connectivity means that all wordsare connected by a single syntactic structure; theparser builds no unconnected tree fragments, evenfor the incomplete sentences (sentence prefixes)that arise during incremental processing.Furthermore, there is evidence that readers orlisteners make predictions about upcoming mate-rial on the basis of sentence prefixes.
Listenerscan predict an upcoming post-verbal element, de-pending on the semantics of the preceding verb(Kamide et al, 2003).
Prediction effects can alsobe observed in reading.
Staub and Clifton (2006)showed that following the word either readers pre-dict the conjunction or and the complement thatfollows it; processing was facilitated compared tostructures that include or without either.
In anERP study, van Berkum et al (1999) found thatlisteners use contextual information to predict spe-cific lexical items and experience processing diffi-culty if the input is incompatible with the predic-tion.The concepts of incrementality, connectedness,and prediction are closely related: in order to guar-43antee that the syntactic structure of a sentence pre-fix is fully connected, it may be necessary to buildphrases whose lexical anchors (the words that theyrelate to) have not been encountered yet.
Full con-nectedness is required to ensure that a fully inter-pretable structure is available at any point duringincremental sentence processing.Here, we explore how these key psycholinguis-tic concepts (incrementality, connectedness, andprediction) can be realized within a new version oftree-adjoining grammar, which we call Psycholin-guistically Motivated TAG (PLTAG).
We proposea formalization of PLTAG and a linking theory thatderives predictions of processing difficulty from it.We then present an implementation of this modeland evaluate it against key experimental data re-lating to incrementality and prediction.
This ap-proach is described in more detail in Demberg andKeller (2008) and Demberg and Keller (2009).3 Modeling Explicit PredictionWe propose a theory of sentence processingguided by the principles of incrementality, con-nectedness, and prediction.
The core assumptionof our proposal is that a sentence processor thatmaintains explicit predictions about the upcomingstructure has to validate these predictions againstthe input it encounters.
Using this assumption, wecan naturally combine the forward-looking aspectof Hale?s (2001) surprisal (sentence structures arecomputed incrementally and unexpected continu-ations cause difficulty) with the backward-lookingintegration view of Gibson?s (1998) dependencylocality theory (previously predicted structures areverified against new evidence, leading to process-ing difficulty as predictions decay with time).In order to build a model that implements thistheory, we require an incremental parser that is ca-pable of building fully connected structures andgenerating explicit predictions from which we canthen derive a measure of processing difficulty.
Ex-isting parsers and grammar formalism do not meetthis specification.
While there is substantial previ-ous work on incremental parsing, none of the ex-isting model observes full connectivity.
One likelyreason for this is that full connectivity cannot beachieved using canonical linguistic structures asassumed in standard grammar formalisms suchas CFG, CCG, TAG, LFG, or HPSG.
Instead, astack has to be used to store partial structuresand retrieve them later when it has become clear(through additional input) how to combine them.We therefore propose a new variant of the tree-adjoining grammar (TAG) formalism which real-izes full connectedness.
The key idea is that incases where new input cannot be combined im-mediately with the existing structure, we need topredict additional syntactic material, which needsto be verified against future input later on.4 Incremental Processing with PLTAGPLTAG extends normal LTAG in that it specifiesnot only the canonical lexicon containing lexical-ized initial and auxiliary trees, but also a predictivelexicon which contains potentially unlexicalizedtrees, which we will call prediction trees.
Eachnode in a prediction tree is annotated with indicesof the forms js j , where inner nodes have two iden-tical indices, root nodes only have a lower indexand foot and substitution nodes only have an up-per index.
The reason for only having half of theindices is that these nodes (root, foot, and substitu-tion nodes) still need to combine with another treein order to build a full node.
If an initial tree substi-tutes into a substitution node, the node where theyare integrated becomes a full node, with the upperhalf contributed by the substitution node and thelower half contributed by the root node.Prediction trees have the same shape as treesfrom the normal lexicon, with the difference thatthey do not contain substitution nodes to the rightof their spine (the spine is the path from the rootnode to the anchor), and that their spine does nothave to end with a lexical item.
The reason forthe missing right side of the spine and the miss-ing lexical item are considerations regarding thegranularity of prediction.
This way, for example,we avoid predicting verbs with specific subcate-gorization frames (or even a specific verb).
In gen-eral, we only predict upcoming structure as faras we need it, i.e., as required by connectivity orsubcategorization.
(However, this is a preliminaryassumption, the optimal prediction grain size re-mains an open research question.
)PLTAG allows the same basic operations (sub-stitution and adjunction) as normal LTAG, the onlydifference is that these operations can also be ap-plied to prediction trees.
In addition, we assumea verification operation, which is needed to val-idate previously integrated prediction trees.
Thetree against which verification happens has to al-ways match the predicted tree in shape (i.e., the44verification tree must contain all the nodes witha unique, identical index that were present in theprediction tree, and in the same order; any addi-tional nodes present in the verification tree mustbe below the prediction tree anchor or to the rightof its spine).
This means that the verification op-eration does not introduce any tree configurationsthat would not be allowed by normal LTAG.
Notethat substitution or adjunction with a predictivetree and the verification of that tree always occurpairwise, since each predicted node has to be veri-fied.
A valid parse for a sentence must not containany nodes that are still annotated as being predic-tive ?
all of them have to be validated through ver-ification by the end of the sentence.5 Modeling Processing DifficultyOur variant of TAG is designed to implement aspecific set of assumptions about human languageprocessing (strong incrementality with full con-nectedness, prediction, ranked parallel process-ing).
The formalism forms the basis for the pro-cessing theory, which uses the parser states to de-rive estimates of processing difficulty.
In addition,we need a linking theory that specifies the mathe-matical relationship between parser states and pro-cessing difficulty in our model.During processing, the elementary tree of eachnew word is integrated with any previous struc-ture, and a set of syntactic expectations is gener-ated (these expectations can be easily read off thegenerated tree in the form of predicted trees).
Eachof these predicted trees has a time-stamp that en-codes when it was first predicted, or last activated(i.e., accessed).
Based on the timestamp, a tree?sdecay at verification time is calculated, under theassumption that recently-accessed structures areeasier to integrate.In our model, processing difficulty is thus in-curred during the construction of the syntacticanalyses, as calculated from the probabilities ofthe elementary trees (this directly corresponds toHaleian surprisal calculated over PLTAG struc-tures instead of over CFG structures).
In additionto this, processing difficulty has a second com-ponent, the cost of verifying earlier predictions,which is subject to decay.The verification cost component bears similari-ties to DLT integration costs, but we do not calcu-late distance in terms of number of discourse refer-ents intervening between a dependent and its head.Rather verification cost is determined by the num-ber of words intervening between a prediction andits verification, subject to decay.
This captures theintuition that a prediction becomes less and lessuseful the longer ago it was made, as it decaysfrom memory with increasing distance.6 EvaluationIn Demberg and Keller (2009), we present an im-plementation of the PLTAG model, including alexicon induction procedure, a parsing algorithm,and a probability model.
We show that the result-ing framework can capture experimental resultsfrom the literature, and can explain both localityand prediction effects, which standard models ofsentence processing like DLT and surprisal are un-able to account for simultaneously.Our model therefore constitutes a step towardsa unified theory of human parsing that potentiallycaptures a broad range of experimental findings.This work also demonstrates that (computational)psycholinguistics cannot afford to be representa-tionally agnostic ?
a comprehensive, computation-ally realistic theory of human sentence process-ing needs to make explicit assumptions about syn-tactic structure.
Here, we showed how the factthat human parsing is incremental and predic-tive necessitates certain assumptions about syntac-tic structure (such as full connectedness), whichcan be implemented by augmenting an existinggrammar formalism, viz., tree-adjoining grammar.Note, however, that it is difficult to show that thisapproach is the only one that is able to realize therequired representational assumptions; other solu-tions using different grammar formalisms are pre-sumably possible.7 AcknowledgmentsThis abstract reports joint work with Vera Dem-berg, described in more detail in Demberg andKeller (2008) and Demberg and Keller (2009).The research was supported by EPSRC grantEP/C546830/1 Prediction in Human Parsing.ReferencesCrocker, Matthew W. 1996.
Computational Psy-cholinguistics: An Interdisciplinary Approachto the Study of Language.
Kluwer, Dordrecht.Demberg, Vera and Frank Keller.
2008.
A psy-cholinguistically motivated version of TAG.
In45Proceedings of the 9th International Workshopon Tree Adjoining Grammars and Related For-malisms.
Tu?bingen.Demberg, Vera and Frank Keller.
2009.
A com-putational model of prediction in human pars-ing: Unifying locality and surprisal effects.Manuscript under review.Gibson, Edward.
1998.
Linguistic complexity:locality of syntactic dependencies.
Cognition68:1?76.Hale, John.
2001.
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chapterof the Association for Computational Linguis-tics.
Association for Computational Linguistics,Pittsburgh, PA, volume 2, pages 159?166.Hale, John.
2006.
Uncertainty about the rest of thesentence.
Cognitive Science 30(4):609?642.Harley, Trevor.
2001.
The Psychology of Lan-guage: From Data to Theory.
PsychologyPress,, Hove, 2 edition.Kamide, Yuki, Christoph Scheepers, and GerryT.
M. Altmann.
2003.
Integration of syntacticand semantic information in predictive process-ing: Cross-linguistic evidence from german andenglish.
Journal of Psycholinguistic Research32:37?55.Konieczny, Lars.
2000.
Locality and parsing com-plexity.
Journal of Psycholinguistic Research29(6):627?645.Marslen-Wilson, William D. 1973.
Linguisticstructure and speech shadowing at very short la-tencies.
Nature 244:522?523.Mazzei, Alessandro, Vicenzo Lombardo, andPatrick Sturt.
2007.
Dynamic TAG and lexi-cal dependencies.
Research on Language andComputation 5(3):309?332.Staub, Adrian and Charles Clifton.
2006.
Syntac-tic prediction in language comprehension: Evi-dence from either .
.
.
or.
Journal of Experimen-tal Psychology: Learning, Memory, and Cogni-tion 32:425?436.Sturt, Patrick and Vincenzo Lombardo.
2005.Processing coordinated structures: Incremen-tality and connectedness.
Cognitive Science29(2):291?305.Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C.Sedivy.
1995.
Integration of visual and linguis-tic information in spoken language comprehen-sion.
Science 268:1632?1634.van Berkum, J. J.
A., C. M. Brown, and Peter Ha-goort.
1999.
Early referential context effectsin sentence processing: Evidence from event-related brain potentials.
Journal of Memory andLanguage 41:147?182.46
