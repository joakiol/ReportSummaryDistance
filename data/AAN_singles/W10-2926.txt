Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 223?233,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsOn Reverse Feature Engineering of Syntactic Tree KernelsDaniele PighinFBK-irst, DISI, University of TrentoVia di Sommarive, 14I-38123 Povo (TN) Italydaniele.pighin@gmail.comAlessandro MoschittiDISI, University of TrentoVia di Sommarive, 14I-38123 Povo (TN) Italymoschitti@disi.unitn.itAbstractIn this paper, we provide a theoreticalframework for feature selection in tree ker-nel spaces based on gradient-vector com-ponents of kernel-based machines.
Weshow that a huge number of features canbe discarded without a significant decreasein accuracy.
Our selection algorithm is asaccurate as and much more efficient thanthose proposed in previous work.
Com-parative experiments on three interestingand very diverse classification tasks, i.e.Question Classification, Relation Extrac-tion and Semantic Role Labeling, supportour theoretical findings and demonstratethe algorithm performance.1 IntroductionKernel functions are very effective at modelingdiverse linguistic phenomena by implicitly rep-resenting data in high dimensional spaces, e.g.
(Cumby and Roth, 2003; Culotta and Sorensen,2004; Kudo et al, 2005; Moschitti et al, 2008).However, the implicit nature of the kernel spacecauses two major drawbacks: (1) high computa-tional costs for learning and classification, and (2)the impossibility to identify the most importantfeatures.
A solution to both problems is the ap-plication of feature selection techniques.In particular, the problem of feature selectionin Tree Kernel (TK) spaces has already been ad-dressed by previous work in NLP, e.g.
(Kudoand Matsumoto, 2003; Suzuki and Isozaki, 2005).However, these approaches lack a theoretical char-acterization of the problem that could support andjustify the design of more effective algorithms.In (Pighin and Moschitti, 2009a) and (Pighinand Moschitti, 2009b) (P&M), we presented aheuristic framework for feature selection in kernelspaces that selects features based on the compo-nents of the weight vector, ~w, optimized by Sup-port Vector Machines (SVMs).
This method ap-pears to be very effective, as the model accuracydoes not significantly decrease even when a largenumber of features are filtered out.
Unfortunately,we could not provide theoretical or intuitive moti-vations to justify our proposed apporach.In this paper, we present and empirically val-idate a theory which aims at filling the above-mentioned gaps.
In particular we provide: (i) aproof of the equation for the exact computation offeature weights induced by TK functions (Collinsand Duffy, 2002); (ii) a theoretical characteriza-tion of feature selection based on ?~w?.
We showthat if feature selection does not sensibly reduces?~w?, the margin associated with ~w does not sen-sibly decrease as well.
Consequently, the theoret-ical upperbound to the probability error does notsensibly increases; (iii) a proof that the convolu-tive nature of TK allows for filtering out an expo-nential number of features with a small ?~w?
de-crease.
The combination of (ii) with (iii) suggeststhat an extremely aggressive feature selection canbe applied.
We describe a greedy algorithm thatexploits these results.
Compared to the one pro-posed in P&M, the new version of the algorithmhas only one parameter (instead of 3), it is moreefficient and can be more easily connected with theamount of gradient norm that is lost after featureselection.In the remainder: Section 2 briefly reviewsSVMs and TK functions; Section 3 describes theproblem of selecting and projecting features fromvery high onto lower dimensional spaces, and pro-vides the theoretical foundation to our approach;Section 4 presents a selection of related work; Sec-tion 5 describes our approach to tree fragment se-lection; Section 6 details the outcome of our ex-periments; finally, in Section 7 we draw our con-clusions.2232 Fragment Weights in TK SpacesThe critical step for feature selection in tree ker-nel spaces is the computation of the weights offeatures (tree fragments) in the kernel machines?gradient.
The basic parameters are the fragmentfrequencies which are combined with a decay fac-tor used to downscale the weight of large sub-trees (Collins and Duffy, 2002).
In this section, af-ter introducing basic kernel concepts, we describea theorem that establishes the correct weight1 offeatures in the STK space.2.1 Kernel Based-MachinesTypically, a kernel machine is a linear classifierwhose decision function can be expressed as:c(~x) = ~w ?
~x+ b =?`i=1?iyi ~xi ?
~x+ b (1)where ~x ?
<N is a classifying example and~w ?
<N and b ?
< are the separating hyper-plane?s gradient and its bias, respectively.
Thegradient is a linear combination of ` trainingpoints ~xi ?
<N multiplied by their labelsyi ?
{?1,+1} and their weights ?i ?
<+.Different optimizers use different strategies tolearn the gradient.
For instance, an SVM learnsto maximize the distance between positive andnegative examples, i.e.
the margin ?.
Applyingthe so-called kernel trick, it is possible to replacethe scalar product with a kernel function definedover pairs of objects, which can more efficientlycompute it:c(o) =?`i=1?iyik(oi, o) + b,where k(oi, o) = ?
(oi) ?
?
(o), with the advantagethat we do not need to provide an explicit mapping?
: O ?
<N of our example objects O in a vec-tor space.
In the next section, we show a kerneldirectly working on syntactic trees.2.2 Syntactic Tree Kernel (STK)Tree Kernel (TK) functions are convolution ker-nels (Haussler, 1999) defined over pairs of trees.Different TKs are characterized by alternativefragment definitions, e.g.
(Collins and Duffy,2002; Kashima and Koyanagi, 2002; Moschitti,2006).
We will focus on the syntactic tree kerneldescribed in (Collins and Duffy, 2002), which re-lies on a fragment definition that does not allow to1In P&M we provided an approximation of the realweight.break production rules (i.e.
if any child of a node isincluded in a fragment, then also all the other chil-dren have to).
As such, it is especially indicatedfor tasks involving constituency parsed texts.Tree kernels compute the number of commonsubstructures between two trees T1 and T2without explicitly considering the whole feature(fragment) space.
Let F = {f1, f2, .
.
.
, f|F|}be the set of tree fragments, i.e.
the explicitrepresentation for the components of the fragmentspace, and ?i(n) be an indicator function2, equalto 1 if the target fi is rooted at node n and equalto 0 otherwise.
A tree kernel function over T1 andT2 is defined asTK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2), (2)whereNT1 andNT2 are the sets of nodes in T1 andT2, respectively and?
(n1, n2) =|F|?i=1?i(n1)?i(n2).
(3)The ?
function counts the number of commonsubtrees rooted in n1 and n2 and weighs themaccording to their size.
It can be evaluated asfollows (Collins and Duffy, 2002):1. if the productions at n1 and n2 are different,then ?
(n1, n2) = 0;2. if the productions at n1 and n2 are the same,and n1 and n2 have only leaf children (i.e.
theyare pre-terminal symbols) then ?
(n1, n2) = ?;3.
if the productions at n1 and n2 are the same,and n1 and n2 are not pre-terminals then?
(n1, n2) = ?l(n1)?j=1(1 + ?
(cjn1 , cjn2)), (4)where l(n1) is the number of children of n1, cjnis the j-th child of node n and ?
is a decay factorpenalizing larger structures.2.3 Tree Fragment WeightsEq.
3 shows that ?
counts the shared fragmentsrooted in n1 and n2 in the form of scalar product,as evaluated by Eq.
2.
However, when ?
is used in?
as in Eq.
4, it changes the weight of the product?i(n1)?i(n2).
As ?
multiplies ?
in each recur-sion step, we may be induced to assume3 that the2We will consider it as a weighting function.3In (Collins and Duffy, 2002), there is a short note aboutthe correct value weight of lambda for each product compo-nents (i.e.
pairs of fragments).
This is in line with the formu-lation we provide.224weight of a fragment is ?d, where d is the depth ofthe fragment.
On the contrary, we show the actualweight by providing the following:Theorem 1.
Let T and f be a tree and one ofits fragments, respectively, induced by STK.
Theweight of f accounted by STK is ?s(f)2 , wherelf (n) is the number of children of n in f ands(f) = |{n ?
T : lf (n) > 0}| is the numberof nodes that have active productions in the frag-ment, i.e.
the size of the fragment.In other words, the exponent of ?
is the numberof fragment nodes that have at least one child (i.e.active productions), divided by 2.Proof.
The thesis can be proven by induction onthe depth d of f .
The base case is f of depth1.
Fragments of depth 1 are matched by step 2of ?
(n1, n2) computation, which assigns a value?
= ?i(n1)?i(n2) independently of the number ofchildren (where fi = f ).
It follows that the weightof f is ?i(n1) = ?i(n2) = ?1/2.Suppose that the thesis is valid for depth d andlet us consider a fragment f of depth d+ 1, rootedin r. Without loss of generality, we can assumethat f is in the set of the fragments rooted in n1and n2, as evaluated by Eq.
4.
It follows thatthe production rules associated with n1 and n2 areidentical to the production rule in r. Let us con-sider M = {i ?
{1, .., l(n1)} : l(cir) > 0},i.e.
the set of child indices of r which have atleast a child.
Thus, for j ?
M , cir has a pro-duction shared by cjn1 and cjn2 .
Conversely, forj /?
M , there is no match and ?
(cjn1 , cjn2) = 0.Therefore, the product in Eq.
4 can be rewrit-ten as ?
?j?M ?
(cjn1 , cjn2), where the term 1 in(1 + ?
(cjn1 , cjn2)) is not considered since it ac-counts for those cases where there are no commonproductions in the children, i.e.
cjn1 6= cjn2?j ?M .We can now substitute ?
(cjn1 , cjn2) with theweight of the subtree tj of f rooted in cjr (and ex-tended until its leaves), which is ?s(tj) by induc-tive hypothesis (since tj has depth lower than d).Thus, the weight of f is s(f) = ?
?j?M ?s(tj) =?1+?j?Ms(tj), where?j?M s(tj) is the num-ber of nodes in f ?s subtrees rooted in r?s chil-dren and having at least one child; by adding1, i.e.
the root of f , we obtain s(f).
Finally,?s(f) = ?i(n1)?i(n2), which satisfies our thesis:?i(n1) = ?i(n2) = ?s(f)2 .2.4 Weights in Feature VectorsIn the light of this result, we can use the definitionof a TK function to project a tree t onto a linearspace by recognizing that t can be represented as avector ~xi = [x(1)i , .
.
.
, x(N)i ] whose attributes arethe counts of the occurrences for each fragment,weighed with respect to the decay factor ?.For a normalized STK kernel, the value of thej-th attribute of the example ~xi is therefore:x(j)i =ti,j?s(fj)2?~xi?=ti,j?s(fj)2?
?Nk=1 t2i,k?s(fk)(5)where: ti,j is the number of occurrences of thefragment fj , associated with the j-th dimensionof the feature space, in the tree ti.
It follows thatthe components of ~w (see Eq.
1) can be rewrittenas:w(j) =?`i=1?iyix(j)i =?`i=1?iyiti,j?s(fj)2?
?Nk=1 t2i,k?s(fk)(6)3 Projecting Exponentially Large SpacesIn order to provide a theoretical background to ourfeature selection technique and to develop effec-tive algorithms, we want to relate our approach tostatistical learning and, in particular, support vec-tor classification theory.
Since we select featureswith respect to their weight w(j), we can use thefollowing theorem that establishes a general boundfor margin-based classifiers.Theorem 2.
(Bartlett and Shawe-Taylor, 1998)Let C = {~x ?
~w ?
~x : ?~w?
?
1, ?~x?
?
R}be the class of real-valued functions defined in aball of radius R in <N .
Then there is a con-stant k such that ?c ?
C having a margin ?, i.e.|~w ?
~x| ?
?,?~x ?
X (training set), the error of cis bounded by b/` +?k`(R2?2 log2`+ log 1?
)witha probability 1 ?
?, where ` = |X | and b is thenumber of examples with margin less than ?.In other words, if X is separated with a margin?
by a linear classifier, then the error has a bounddepending on ?.
Another conclusion is that a fea-ture selection algorithm that wants to preserve theaccuracy of the original space should not affect themargin.Since we would like to exploit the availabilityof the initial gradient ~w derived by the applica-tion of SVMs, it makes sense to try to quantify thepercentage of ?
reduction after feature selection,which we indicate by ?.
We found out that ?
is225linked to the reduction of ||~w||, as illustrated bythe next lemma.Lemma 1.
Let X be a set of points in a vectorspace and ~w be the gradient vector which sepa-rates them with a margin ?.
If the selection de-creases ||~w|| of a ?
rate, then the resulting hyper-plane separates X by a margin larger than ?in =?
?
?R||~w||.Proof.
Let ~w = ~win+ ~wout, where ~win and ~wout ?<N are constituted by the components of ~w thatare selected in and out, respectively, and have zerovalues in the remaining positions.
By hypothesis|~w ?
~x| ?
?
; without loss of generality, we canconsider just the case ~w ?
~x ?
?, and write ~w ?~x = ~win ?
~x + ~wout ?
~x ?
?
?
~win ?
~x ?
?
?~wout ?
~x ?
?
?
|~wout ?
~x| ?
?
?
||~wout|| ?
||~x||,where the last inequality holds owing to Cauchy-Schwarz inequality.
The margin associated with~win, i.e.
?in, is therefore ?
?
||~wout|| ?
||~x|| ??
?
||~wout||R = ?
?
?R||~w||.Remark 1.
The lemma suggests that, even in caseof very aggressive feature selection, if a small per-centage ?
of ||~w|| is lost, the margin reduction issmall.
Consequently, through Theorem 2, we canconclude that the accuracy of the model is by andlarge preserved.Remark 2.
We prefer to show the lemma in themore general form, but if we use normalized ~x andclassifiers with ||~w|| ?
1, then ?in = ??||~w||?
>?
?
?.The last result that we present justifies our se-lection approach as it demonstrates that most ofthe gradient norm is concentrated in relatively fewfeatures, with respect to the huge space inducedby tree kernels.
The selection of these few fea-tures allows us to preserve most of the norm andthe margin.Lemma 2.
Let ~w be a linear separator of a set ofpoints X , where each ~xi ?
X is an explicit vectorrepresentations of a tree ti in the space induced bySTK and let ?
be the largest s(ti), i.e.
the max-imum tree size.
Then, if we discard fragments ofsize greater than ?, ||~wout|| ?
??2?(??)??(??)?1???
.Proof.
By applying simple norm proper-ties, ||~wout|| =???
?`i=1 ?iyi~xouti???
?
?`i=1||?iyi~xouti || =?`i=1 ?i||~xouti ||.
To evaluatethe latter, we first re-organize the summation inEq.
5 (with no normalization) such that ?~xi?2=?
?k=1?j:s(fj)=k t2i,j?s(fj).
Since a fragmentfj can be at maximum rooted in ?
nodes, thenti,j ?
?.
Therefore, by replacing the number oftrees of size k with the upperbound ?k, we have?~xi?
<??
?k=1 ?2?k?k =??
?k=1 ?2(??
)k =?
?2 1???1??
, where we applied geometric seriessummation.
Now if we assume that our algorithmselects out (i.e.
discards) fragments with sizes(f) > ?, ?~xouti?
<?
?2 ?????1??
.
It follows that||~wout|| <?`i=1 ?i?
?2 ?????1??
.
In case of hard-margin SVMs, we have?`i=1 ?i = 1/?2.
Thus,||~wout|| < ??2??????1??
=??2?(??)??(??)?1???
.Remark 3.
The lemma shows that for an enoughlarge ?
and ?
< 1/?, ||~wout|| can be very small,even though it includes an exponential number offeatures, i.e.
all the subtrees whose size rangesfrom ?
to ?.
Therefore, according to Lemma 1 andTheorem 2, we can discard an exponential numberof features with a limited loss in accuracy.Remark 4.
Regarding the proposed norm bound,we observe that ?k is a rough overestimation of thethe real number of fragments having size k rootedin the nodes of the target tree t. This suggests thatwe don?t really need ?
< 1/?.
Moreover, in caseof soft-margin SVMs, we can bound ?i with thevalue of the trade-off parameter C.4 Previous WorkInitial work on feature selection for text, e.g.
(Yang and Pedersen, 1997), has shown that it mayimprove the accuracy or, at least, improve effi-ciency while preserving accuracy.
Our context forfeature selection is different for several importantreasons: (i) we focus on structured features witha syntactic nature, which show different behaviourfrom lexical ones, e.g.
they tend to be more sparse;(ii) in the TK space, the a-priori weights are veryskewed, and large fragments receive exponentiallylower scores than small ones; (iii) there is high re-dundancy and inter-dependency between such fea-tures; (iv) we want to be able to observe the mostrelevant features automatically generated by TKs;and (v) the huge number of features makes it im-possible to evaluate the weight of each feature in-dividually.Guyon and Elisseeff (2003) carries out a veryinformative survey of feature selection techniques.Non-filter approaches for SVMs and kernel ma-chines are often concerned with polynomial and226Gaussian kernels, e.g.
(Weston et al, 2001; Neu-mann et al, 2005).
In (Kudo and Matsumoto,2003), an extension of the PrefixSpan algo-rithm (Pei et al, 2001) is used to efficiently minethe features in a low degree polynomial kernelspace.
The authors discuss an approximationof their method that allows them to handle highdegree polynomial kernels.
Suzuki and Isozaki(2005) present an embedded approach to featureselection for convolution kernels based on ?2-driven relevance assessment.
With respect to theirwork, the main differences in the approach that wepropose are that we want to exploit the SVM op-timizer to select the most relevant features, and tobe able to observe the relevant fragments.Regarding work that may directly benefit fromreverse kernel engineering is worthwhile mention-ing: (Cancedda et al, 2003; Shen et al, 2003;Daume?
III and Marcu, 2004; Giuglea and Mos-chitti, 2004; Toutanova et al, 2004; Kazama andTorisawa, 2005; Titov and Henderson, 2006; Kateand Mooney, 2006; Zhang et al, 2006; Bloehdornet al, 2006; Bloehdorn and Moschitti, 2007; Mos-chitti and Zanzotto, 2007; Surdeanu et al, 2008;Moschitti, 2008; Moschitti and Quarteroni, 2008;Martins et al, 2009; Nguyen et al, 2009a)5 Mining Fragments EfficientlyThe high-level description of our feature selectiontechnique is as follows: we start by learning anSTK model and we greedily explore the supportvectors in search for the most relevant fragments.We store them in an index, and then we decode (orlinearize) all the trees in the dataset, i.e.
we repre-sent them as vectors in a linear space where only avery small subset of the fragments in the originalspace are accounted for.
These vectors are thenemployed for learning and classification in the lin-ear space.To explore the fragment space defined by a setof support vectors, we adopt the greedy strategydescribed in Algorithm 5.1.
Its arguments are amodel M , and the threshold factor L. The greedyalgorithm explores the fragment space in a small tolarge fashion.
The first step is the generation of theall base fragments F encoded in each tree, i.e.
thesmallest possible fragments according to the defi-nition of the kernel function.
For STK, such frag-ments are all those consisting of a node and all itsdirect children (i.e.
production rules of the gram-mar).
We assess the cumulative relevance of eachAlgorithm 5.1: GREEDY MODEL MINER(M,L)B ?
BASE FRAGS(model)B ?
REL(BEST(B))?
?
B/LDprev ?
FILTER(B, ?
)UPDATE(Dprev)while Dprev 6= ?do????????????????????????????????????????
?Dnext ?
??
?
1/ ?
widthfactor ?
/Wprev ?
DprevwhileWprev 6= ?do??????????????????????
?Wnext ?
?for each f ?
Wprevdo??????????
?Ef ?
EXPAND(f, ?
)F ?
FILTER(Ef , ?
)if F 6= ?then{Wnext ?Wnext ?
{f}Dnext ?
Dnext ?
FUPDATE(F )?
?
?
+ 1Wprev ?WnextDprev ?
Dnextreturn (result)base fragment according to Eq.
6 and then use therelevanceB of the heaviest fragment, i.e.
the frag-ment with the highest relevance in absolute value,as a criterion to set our fragment mining threshold?
to B/L.
We then apply the FILTER(?)
operatorwhich discards all the fragments whose cumula-tive score is less than ?.
Then, the UPDATE(?)
op-erator stores the ramaining fragments in the index.The exploration of the kernel space is carriedout via the process of fragment expansion, bywhich each fragment retained at the previous stepis incrementally grown to span more levels of thetree and to include more nodes at each level.
Thesetwo directions of growth are controlled by theouter and the inner while loops, respectively.
Frag-ment expansion is realized by the EXPAND(f, n)operator, that grows the fragment f by includingthe children of n expandable nodes in the frag-ment.
Expandable nodes are nodes which areleaves in f but that have children in the tree thatoriginated f .After each expansion, the FILTER(?)
operator isinvoked on the set of generated fragments.
If thefiltered set is empty, i.e.
no fragments more rele-vant than ?
have been found during the previousiteration, then the loop is terminated.Unlike previous attempts, this algorithm relieson just one parameter, i.e.
L. As it revolves aroundthe weight of the most relevant fragment, it oper-ates according to the norm-preservation principledescribed in the previous sections.
In fact, if wecall N the number of fragments mined for a givenvalue of L, the norm after feature selection can be227bounded by BL?N ?
?win?
?
B?N .The choice of B, i.e.
the highest relevance ofa base fragment, as an upper bound for fragmentrelevance is motivated as follows.
In Eq.
6, we canidentify a term Ti = ?iyi/?ti?
that is the same forall the fragments in the tree ti.
For 0 < ?
?
1,if fj is an expansion of fk, then from our defini-tion of fragment expansion it follows that ?s(fj)2 <?s(fk)2 .
It can also be observed that ti,j ?
ti,k.
In-deed, if ti,k is a subset of ti,j , then it will occur atleast as many times as its expansion ti,k, possiblyoccurring as a seed fragment for different expan-sions in other parts of the tree as well.
Therefore,if Ef is the set of expansions of f , for every twofragments fi,j , fi,k coming from the same tree ti,we can conclude that x(j)i < x(k)i ?fi,j ?
Efi,k .
Inother words, for each tree in the model, base frag-ments are the most relevant, and we can assumethat the relevance of the heaviest fragment is anupper bound for the relevance of any fragment 4.6 ExperimentsWe ran a set of thorough experiments to sup-port our claims with empirical evidence.
Weshow our results on three very different bench-marks: Question Classification (QC) using TREC10 data (Voorhees, 2001), Relation Extraction(RE) based on the newswire and broadcast newsdomain of the ACE 2004 English corpus (Dod-dington et al, 2004) and Semantic Role Labeling(SRL) on the CoNLL 2005 shared task data (Car-reras and Ma`rquez, 2005).
In the next sections weelaborate on the setup and outcome of each setof experiments.
As a supervised learning frame-work we used SVM-Light-TK5, which extends theSVM-Light optimizer (Joachims, 2000) with sup-port for tree kernel functions.Unless differently stated, all the classifiers areparametrized for optimal Precision and Recall ona development set, obtained by selecting one ex-ample in ten from the training set with the samepositive-to-negative example ratio.
The resultsthat we show are obtained on the test sets by usingall the available data for training.
For multi-classscenarios, the classifiers are arranged in a one vs.4In principle, the weight of some fragment encoded in themodel M may be greater than B.
However, as an empiricaljustification, we report that in all our experiments we havenever been able to observe such case.
Thus, with a certainprobability, we can assume that the highest weight will beobtained from the heaviest of the base fragments.5http://disi.unitn.it/?moschitt/Tree-Kernel.htmall configuration, where each sentence is a positiveexample for one of the classes, and negative forthe others.
While binary classifiers are evaluatedin terms of F1 measure, for multi-class classifierswe show the final accuracy.The next paragraphs describe the datasets usedfor the experiments.Question Classification (QC) Given a question,the task consists in selecting the most appropriateexpected answer type from a given set of possibil-ities.
We adopted the question taxonomy knownas coarse grained, which has been describedin (Zhang and Lee, 2003) and (Li and Roth, 2006),consisting of six non overlapping classes: Abbre-viations (ABBR), Descriptions (DESC, e.g.
def-initions or explanations), Entity (ENTY, e.g.
an-imal, body or color), Human (HUM, e.g.
groupor individual), Location (LOC, e.g.
cities or coun-tries) and Numeric (NUM, e.g.
amounts or dates).The TREC 10 QA data set accounts for 6,000questions.
For each question, we generate thefull parse of the sentence and use it to train ourmodels.
Automatic parses are obtained with theStanford parser6 (Klein and Manning, 2003), andwe actually have only 5,953 sentences in our dataset due to parsing issues.
During preliminary ex-periments, we observed an uneven distribution ofexamples in the traditional training/test split (thesame used in P&M).
Therefore, we used a ran-dom selection to generate an unbiased split, with5,468 sentences for training and 485 for testing.The resulting data set is available for downloadat http://danielepighin.net/cms/research/QC_dataset.tgz.Relation Extraction (RE) The corpusconsists of 348 documents, and containsseven relation classes defined over pairs ofmentions: Physical, Person/Social, Employ-ment/Membership/Subsidiary, Agent-Artifact,PER/ORG Affiliation, GPE Affiliation, andDiscourse.
There are 4,400 positive and 38,696negative examples when the potential relationsare generated using all the entity/mention pairs inthe same sentence.Documents are parsed using the StanfordParser, where the nodes of the entities are enrichedwith information about the entity type.
Overall,we used the setting and data defined in (Nguyen etal., 2009b).6http://nlp.stanford.edu/software/lex-parser.shtml228Semantic Role Labeling (SRL) SRL can be de-composed into two tasks: boundary detection,where the word sequences that are arguments ofa predicate word w are identified, and role clas-sification, where each argument is assigned theproper role.
For these experiments we concen-trated on this latter task and used exactly the samesetup as P&M.
We considered all the argumentnodes of any of the six PropBank (Palmer et al,2005) core roles7 (i.e.
A0, .
.
.
, A5) from all theavailable training sections, i.e.
2 through 21, for atotal of 179,091 training instances.
Similarly, wecollected 9,277 test instances from the annotationsof Section 23.6.1 Model ComparisonTo show the validity of Lemma 1 in practical sce-narios, we compare the accuracy of our linearizedmodels against vanilla STK classifiers.
We de-signed two types of classifiers:LIN, a linearized STK model, which uses theweights estimated by the learner in the STK spaceand linearized examples; in other words LIN uses~wIN .
It allows us to measure exactly the loss inaccuracy with respect to the reduction of ||~w||.OPT, a linearized STK model that is re-optimized in the linear space, i.e.
for which weretrained an SVM using the linearized training ex-amples as input data.
Since the LIN solution ispart of the candidate solutions from which OPT isselected, we always expect higher accuracy fromit.Additionally, we compare selection based ongradient ~w (as detailed in Section 2.4) against to?2 selection, which evaluates the relevance of fea-tures, in a similar way to (Suzuki and Isozaki,2005).
The relevance of a fragment is calculatedas?2 =N(yN ?Mx)2x(N ?
x)M(N ?M),where N is the number of support vectors, M isthe number of positive vectors (i.e.
?i > 0), and xand y are the fractions ofN andM where the frag-ment is instantiated, respectively.
We specify theselection models by means of Grad for the formerand Chi for the latter.
For example, a model calledOPT/Grad is a re-trained model using the featuresselected according the highest gradient weights,while LIN/Chi would be a linearized tree kernelmodel using ?2 for feature selection.7We do not consider adjuncts because we preferred thenumber of classes to be similar across the three benchmarks.0.20.250.30.350.40.450.50.550.60.650.71  10  100  1000  10000Number of fragments (log)ABBRDESCENTYHUMLOCNUM1?
?Figure 1: Percentage of gradient Norm, i.e.
1?
?,according to the number of selected fragments, fordifferent QC classifiers.STK LinearizedLIN OPTF1 ||~w|| Frags F1 ||~win|| F1A 80.00 11.77 566 66.67 7.13 90.91D 86.26 41.33 5161 81.87 25.10 83.72E 76.86 51.71 5,702 73.03 31.06 75.56H 84.92 43.61 5,232 80.47 26.20 77.08L 81.69 38.73 1,732 78.87 24.27 82.89N 92.31 37.65 1,015 85.07 24.53 87.07Table 1: Per-class comparison between STK andthe LIN/Grad and OPT/Grad models on the QCtask.
Each class is identified by its initial (e.g.A=ABBR).
For each class, we considered a valueof the threshold factor parameter L so as to retainat least 60% of the gradient norm after feature se-lection.6.2 ResultsThe plots in Figure 1 show, for each class, the per-centage of the gradient norm (i.e.
1 ?
?, see Sec-tion 3) retained when including a different num-ber of fragments.
This graph empirically validatesLemma 2 since it clearly demonstrates that after1,000-10,000 features the percentage of the normreaches a plateau (around 60-65%).
This meansthat after such threshold, which interestingly gen-eralizes across all classifiers, a huge number offeatures is needed for a small increase of the norm.We recall that the maximum reachable norm isaround 70% since we apriori filter out fragmentsof frequency lower than three.Table 1 shows the F1 of the binary question clas-sifiers learned with STK, LIN/Grad and OPT/Gradmodels.
It also shows the norm of the gradi-ent before, ||~w||, and after, ||~win||, feature selec-2295055606570758085900.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7F1(LOC)LIN/GradOPT/GradLIN/ChiOPT/ChiSTK1?
?01020304050607080900.1  0.2  0.3  0.4  0.5  0.6F1(DESC)LIN/GradOPT/GradLIN/ChiOPT/ChiSTK1?
?Figure 2: F1-measure of LOC and DESC wrt dif-ferent 1?
?
values.tion along with the number of selected fragments,Frags.
Instead of selecting an optimal number offragments on a validation set, we investigated the60% value suggested by the previous plot.
Thus,for each category we selected the feature set reach-ing approximately 60% of ||~w||.
The table showsthat the accuracy of the OPT/Grad model is inline with STK.
In some cases, e.g.
ABBR, theprojected model is more accurate, i.e.
90.91 vs.80.00, whereas in others, e.g.
HUM, STK per-forms better, i.e.
84.92 vs. 77.08.
It is interestingto see how the empirical results clearly comple-ment the theoretical findings of the previous sec-tions.
For example, the LOC classifier uses only1,732 of the ?
1012 features encoded by the cor-responding STK model, but since only 40% of thenorm of ~w is lost, classification accuracy is af-fected only marginally.As mentioned above, the selected number offeatures is not optimal for every class.
Fig-ure 2 plots the accuracy of the LIN/Grad andOPT/Grad for different numbers of fragments ontwo classes 8.
These show that the former, with8The other classes, which show similar behaviour, areomitted due to lack of space.2030405060708090100  1000  10000  100000MulticlassaccuracyNumber of fragments (log)OPT/GradOPT/ChiSTKFigure 3: Multiclass accuracy obtained by includ-ing a growing number of fragments.more than 60% of the norm, approaches STKwhereas the latter requires less fragments.
Theplots also show the comparison against the samefragment mining algorithm and learning frame-work when using ?2-based selection.
This alsoprovides similar good results, as far as the reduc-tion of ||~w|| is kept under control, i.e.
as far as weselect the components of the gradient that mostlyaffect its norm.To concretely assess the benefits of our modelsfor QC, Figure 3 plots the accuracy of OPT/Gradand OPT/Chi on the multiclass QC problem wrtthe number of fragments employed.
The resultsfor the multi-class classifier are less biased by thebinary Precision/Recall classifiers thus they aremore stable and clearly show how, after selectingthe optimal number of fragments (1,000-10,000i.e.
60-65% of the norm), the accuracy of the OPTand CHI classifiers stabilize around levels of accu-racy which are in line with STK.STK OPT/GradF1 F1 FragsQC 83.70 84.12 ?2kRE 67.53 66.31 ?10kSRL 87.56 88.17 ?300kTable 2: Multiclass classification accuracy onthree benchmarks.Finally, Table 2 shows the best results that weachieved on the three multi-class classificationtasks, i.e.
QC, RE9 and SRL, and compares themagainst the STK 10.
For all the tasks OPT/Grad9For RE, we show lower accuracy than in (Nguyen et al,2009b) since, to have a closer comparison with STK, we donot combine structural features with manual designed fea-tures.10We should point out that this models are only partially230produces the best results for all the tests, eventhough the difference with OPT/Chi is generallynot statistically significant.
Out of three tasks,OPT/Grad manages to slightly improve two ofthem, i.e.
QC (84.12 vs. 83.7) and SRL (88.17vs.
87.56), while STK is more accurate on RE, i.e.67.53 vs. 66.31.6.3 Comparison with P&MThe results on SRL can be compared againstthose that we presented in (Pighin and Moschitti,2009a), where we measured an accuracy of 87.13exactly on the same benchmark.
As we can see inTable 2, our model improves the classification ac-curacy of about 1 point, i.e.
88.17.
On the otherhand, such comparison is not really fair since thealgorithms rely on different parameter sets, and itis almost impossible to find matching configura-tions for the different versions of the algorithmsthat would result in exactly the same number offragments.
In a projected space with approxi-mately 103 or 104 fragments, including a few hun-dred more features can produce noticeably differ-ent accuracy readings.Generally speaking, the current model canachieve comparable accuracy with P&M whileconsidering a smaller number of fragments.
Forexample, in (Pighin and Moschitti, 2009b) thebest model for the A1 binary classifier of theSRL benchmark was obtained by including 50,000fragments, achieving an F1 score of 89.04.
Withthe new algorithm, using approximately half thefragments the accuracy of the linearized A1 clas-sifier is 90.09.
In P&M, the algorithm would onlyconsider expansions of a fragment f where at mostm nodes are expanded.
Consequently, the set ofmined fragments may include some small struc-tures which can be less relevant than larger ones.Conversely, the new algorithm (see Alg.
5.1) mayinclude larger but more relevant structures, thusaccounting for a larger fraction of the gradientnorm with a smaller number of fragments.Concerning efficiency, the complexity of bothmining algorithms is proportional to the numberof fragments that they generate.
Therefore, we canconclude that the new implementation is more effi-cient by considering that we can achieve the sameaccuracy with less fragments.
As for the complex-optimized, as we evaluated them by using the same thresholdfactor parameter L for all the classes.
Better performancescould be achieved by selecting an optimal value of L for in-dividual classes when building the multi-class classifier.ity of decoding, i.e.
providing explicit vector rep-resentations of the input trees, in P&M, we useda very naive approach, i.e.
the generation of allthe fragments encoded in the tree and then look upeach fragment in the index.
This solution has ex-ponential complexity with the number of nodes inthe tree.
Conversely, the new implementation hasapproximately linear complexity.
The approach isbased on the idea of an FST-like index, that wecan query with a tree node.
Every time the treematches one of the fragments, the index increasesthe count of that fragment for the tree.
The reduc-tion in time complexity is made possible by en-coding in the index the sequence of expansion op-erations that produced each indexed fragment, andby considering only those expansions at decodingtime.7 ConclusionsAvailable feature selection frameworks for veryhigh dimensional kernel families, such as tree ker-nels, suffer from the lack of a theory that couldjustify the very aggressive selection strategies nec-essary to cope with the exceptionally high dimen-sional feature space.In this paper, we have provided a theoreticalfoundation in the context of margin classifiers by(i) linking the reduction of the gradient norm to thetheoretical error bound and (ii) by proving that thenorm is mostly concentrated in a relatively smallnumber of features.
The two properties suggestthat we can apply an extremely aggressive fea-ture selection by keeping the same accuracy.
Wedescribed a very efficient algorithm to carry outsuch strategy in the fragment space.
Our experi-ments empirically support our theoretical findingson three very different NLP tasks.AcknowledgementsWe would like to thank Truc-Vien T. Nguyen forproviding us with the SVM learning and test filesof the Relation Extraction dataset.
Many thanks tothe anonymous reviewers for their valuable sug-gestions.This research has been partially supported by theEC project, EternalS: ?Trustworthy Eternal Sys-tems via Evolving Software, Data and Knowl-edge?, project number FP7 247758.231ReferencesP.
Bartlett and J. Shawe-Taylor, 1998.
Advances in KernelMethods ?
Support Vector Learning, chapter Generaliza-tion Performance of Support Vector Machines and otherPattern Classifiers.
MIT Press.Stephan Bloehdorn and Alessandro Moschitti.
2007.
Struc-ture and semantics for expressive text kernels.
In In Pro-ceedings of CIKM ?07.Stephan Bloehdorn, Roberto Basili, Marco Cammisa, andAlessandro Moschitti.
2006.
Semantic kernels for textclassification based on topological measures of featuresimilarity.
In Proceedings of ICDM 06, Hong Kong, 2006.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, 3:1059?1082.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction tothe CoNLL-2005 Shared Task: Semantic Role Labeling.In Proceedings of CoNLL?05.Michael Collins and Nigel Duffy.
2002.
New Ranking Al-gorithms for Parsing and Tagging: Kernels over DiscreteStructures, and the Voted Perceptron.
In Proceedings ofACL?02.Aron Culotta and Jeffrey Sorensen.
2004.
DependencyTree Kernels for Relation Extraction.
In Proceedings ofACL?04.Chad Cumby and Dan Roth.
2003.
Kernel Methods for Re-lational Learning.
In Proceedings of ICML 2003.Hal Daume?
III and Daniel Marcu.
2004.
Np bracketing bymaximum entropy tagging and SVM reranking.
In Pro-ceedings of EMNLP?04.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
The Auto-matic Content Extraction (ACE) Program?Tasks, Data,and Evaluation.
Proceedings of LREC 2004, pages 837?840.Ana-Maria Giuglea and Alessandro Moschitti.
2004.Knowledge Discovering using FrameNet, VerbNet andPropBank.
In In Proceedings of the Workshop on On-tology and Knowledge Discovering at ECML 2004, Pisa,Italy.Isabelle Guyon and Andre?
Elisseeff.
2003.
An introduc-tion to variable and feature selection.
Journal of MachineLearning Research, 3:1157?1182.David Haussler.
1999.
Convolution kernels on discrete struc-tures.
Technical report, Dept.
of Computer Science, Uni-versity of California at Santa Cruz.T.
Joachims.
2000.
Estimating the generalization perfor-mance of a SVM efficiently.
In Proceedings of ICML?00.Hisashi Kashima and Teruo Koyanagi.
2002.
Kernels forsemi-structured data.
In Proceedings of ICML?02.Rohit J. Kate and Raymond J. Mooney.
2006.
Using string-kernels for learning semantic parsers.
In Proceedings ofthe 21st ICCL and 44th Annual Meeting of the ACL, pages913?920, Sydney, Australia, July.
Association for Compu-tational Linguistics.Jun?ichi Kazama and Kentaro Torisawa.
2005.
Speeding uptraining with tree kernels for node relation labeling.
InProceedings of HLT-EMNLP?05.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL?03, pages423?430.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.
Boosting-based parse reranking with subtree features.
In Proceed-ings of ACL?05.Xin Li and Dan Roth.
2006.
Learning question classifiers:the role of semantic information.
Natural Language En-gineering, 12(3):229?249.Andre?
F. T. Martins, Noah A. Smith, Eric P. Xing, PedroM.
Q. Aguiar, and Ma?rio A. T. Figueiredo.
2009.
Nonex-tensive information theoretic kernels on measures.
J.Mach.
Learn.
Res., 10:935?975.Alessandro Moschitti and Silvia Quarteroni.
2008.
Kernelson linguistic structures for answer extraction.
In Proceed-ings of ACL-08: HLT, Short Papers, Columbus, Ohio.Alessandro Moschitti and Fabio Massimo Zanzotto.
2007.Fast and effective kernels for relational learning fromtexts.
In Zoubin Ghahramani, editor, Proceedings of the24th Annual International Conference on Machine Learn-ing (ICML 2007).Alessandro Moschitti, Daniele Pighin, and Roberto Basili.2008.
Tree kernels for semantic role labeling.
Compu-tational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006.
Efficient convolution kernelsfor dependency and constituent syntactic trees.
In Pro-ceedings of ECML?06, pages 318?329.Alessandro Moschitti.
2008.
Kernel methods, syntax andsemantics for relational text categorization.
In Proceedingof CIKM ?08, NY, USA.Julia Neumann, Christoph Schnorr, and Gabriele Steidl.2005.
Combined SVM-Based Feature Selection and Clas-sification.
Machine Learning, 61(1-3):129?150.Truc-Vien T. Nguyen, Alessandro Moschitti, and GiuseppeRiccardi.
2009a.
Convolution kernels on constituent, de-pendency and sequential structures for relation extraction.In Proceedings of EMNLP.Truc-Vien T. Nguyen, Alessandro Moschitti, and GiuseppeRiccardi.
2009b.
Convolution kernels on constituent,dependency and sequential structures for relation extrac-tion.
In EMNLP ?09: Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Processing,pages 1378?1387, Morristown, NJ, USA.
Association forComputational Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of semanticroles.
Comput.
Linguist., 31(1):71?106.J.
Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,and M. C. Hsu.
2001.
PrefixSpan Mining Sequential Pat-terns Efficiently by Prefix Projected Pattern Growth.
InProceedings of ICDE?01.232Daniele Pighin and Alessandro Moschitti.
2009a.
Efficientlinearization of tree kernel functions.
In Proceedings ofCoNLL?09.Daniele Pighin and Alessandro Moschitti.
2009b.
Reverseengineering of tree kernel feature spaces.
In Proceedingsof EMNLP, pages 111?120, Singapore, August.
Associa-tion for Computational Linguistics.Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
2003.
Us-ing LTAG Based Features in Parse Reranking.
In Proceed-ings of EMNLP?06.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to rank answers on large onlineQA collections.
In Proceedings of ACL-08: HLT, Colum-bus, Ohio.Jun Suzuki and Hideki Isozaki.
2005.
Sequence and TreeKernels with Statistical Feature Mining.
In Proceedingsof NIPS?05.Ivan Titov and James Henderson.
2006.
Porting statisti-cal parsers with data-defined kernels.
In Proceedings ofCoNLL-X.Kristina Toutanova, Penka Markova, and Christopher Man-ning.
2004.
The Leaf Path Projection View of ParseTrees: Exploring String Kernels for HPSG Parse Selec-tion.
In Proceedings of EMNLP 2004.Ellen M. Voorhees.
2001.
Overview of the trec 2001 ques-tion answering track.
In In Proceedings of the Tenth TextREtrieval Conference (TREC, pages 42?51.Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-iano Pontil, Tomaso Poggio, and Vladimir Vapnik.
2001.Feature Selection for SVMs.
In Proceedings of NIPS?01.Yiming Yang and Jan O. Pedersen.
1997.
A comparativestudy on feature selection in text categorization.
In Dou-glas H. Fisher, editor, Proceedings of ICML-97, 14th In-ternational Conference on Machine Learning, pages 412?420, Nashville, US.
Morgan Kaufmann Publishers, SanFrancisco, US.Dell Zhang and Wee Sun Lee.
2003.
Question classifica-tion using support vector machines.
In Proceedings of SI-GIR?03, pages 26?32.Min Zhang, Jie Zhang, and Jian Su.
2006.
Exploring Syntac-tic Features for Relation Extraction using a Convolutiontree kernel.
In Proceedings of NAACL.233
