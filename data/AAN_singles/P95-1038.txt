Evaluation of Semantic ClustersRajeev AgarwalMississippi State UniversityMississippi State, MS 39762USAra jeev@cs.msstate.eduAbst rac tSemantic clusters of a domain form animportant feature that can be useful forperforming syntactic and semantic disam-biguation.
Several attempts have beenmade to extract he semantic lusters of adomain by probabilistic or taxonomic tech-niques.
However, not much progress hasbeen made in evaluating the obtained se-mantic clusters.
This paper focuses on anevaluation mechanism that can be used toevaluate semantic lusters produced by asystem against hose provided by humanexperts.1 In t roduct ion  1Most natural language processing (NLP) systems aredesigned to work on certain specific domains andporting them to other domains is often a very time-consuming and human-intenslve process.
As theneed for applying NLP systems to more and var-ied domains grows, it becomes increasingly impor-tant that some techniques be used to make thesesystems more portable.
Several researchers (Langand Hirschman, 1988; Rau et al, 1989; Pustejovsky,1992; Grishman and Sterling, 1993; Basili et al,1994), either directly or indirectly, have addressedissues that assist in making it easier to move anNLP system from one domain to another.
One ofthe reasons for the lack of portability is the need fordomain-specific semantic features that such systemsoften use for lexical, syntactic, and semantic disam-biguation.
One such feature is the knowledge of thesemantic lusters in a domain.Since semantic lasses are often domain-specific,their automatic acquisition is not trivial.
Suchclasses can be derived either by distributional meansor from existing taxonomies, knowledge bases, dic-tionaries, thesauruses, and so on.
A prime exam-ple of the latter is WordNet which has been used to1The author is currently at Texas Instruments and allinquiries hould be addressed to rajeev@csc.ti.com.provide such semantic lasses (Resnik, 1993; Basiliet al, 1994) to assist in text understanding.
Ourefforts to obtain such semantic lusters with limitedhuman intervention have been described elsewhere(Agarwal, 1995).
This paper concentrates on theaspect of evahiating the obtained clusters againstclasses provided by human experts.2 The  NeedAlthough there has been a lot of work done in ex-tracting semantic lasses of a given domain, rela-tively little attention has been paid to the task ofevaluating the generated classes.
In the absence ofan evaluation scheme, the only way to decide if thesemantic lasses produced by a system are "reason-able" or not is by having an expert analyze them byinspection.
Such informal evaluations make it verydifficult to compare one set of classes against an-other and are also not very reliable estimates of thequality of a set of classes.
It is clear that a formalevaluation scheme would be of great help.Hatzivassiloglou and McKeown (1993) duster ad-jectives into partitions and present an interest-ing evaluation to compare the generated adjectiveclasses against hose provided by an expert.
Theirevaluation scheme bases the comparison betweentwo classes on the presence or absence of pairs ofwords in them.
Their approach involves filling in aYES-NO contingency table based on whether a pairof words (adjectives, in their case) is classified in thesame class by the human expert and by the system.This method works very well for partitions.
How-ever, if it is used to evaluate sets of classes where theclasses may be potentiaily overlapping, their tech-nique yields a weaker measure since the same wordpair could possibly be present in more than one class.An ideal scheme used to evaluate semantic lassesshould be able to handle overlapping classes (as o1>.posed to partitions) as well as hierarchies.
The tech-nique proposed by Hatzivassiloglou and McKeowndoes not do a good job of evaluating either of these.In this paper, we present an evaluation methodologywhich makes it possible to properly evaluate over-284Table 1: Two Example ClassesClass A Class B(System) (Expert)catdogstomachpigCOWhaircattlegoathorseCOWcatpiglambdogsheepmarecattleswinegoatlapping classes.
Our scheme is also capable of in-corporating hierarchies provided by an expert intothe evaluation, but still lacks the ability to comparehierarchies against hierarchies.In the discussion that follows, the word "cluster-ing" is used to refer to the set of classes that maybe either provided by an expert or generated by thesystem, and the word "class" is used to refer to asingle class in the clustering.3 Eva luat ion  ApproachAs mentioned above, we intend to be able to com-pare a clustering enerated by a system against oneprovided by an expert.
Since a word can occur inmore than one class, it is important o find somekind of mapping between the classes generated bythe system and the classes given by the expert.
Sucha mapping tells us which class in the system's clus-tering maps to which one in the expert's clustering,and an overall comparison of the clusterings i  basedon the comparison of the mutually mapping classes.Before we delve deeper into the evaluation pro-cess, we must decide on some measure of "closeness"between a pair of classes.
We have adopted theF-measure (Hatzivassiloglou and McKeown, 1993;Chincor, 1992).
In our computation of the F-measure, we construct a contingency table basedon the presence or absence of individual elementsin the two classes being compared, as opposed tobasing it on pairs of words.
For example, supposethat Class A is generated by the system and Class Bis provided by an expert (as shown in Table 1).
Thecontingency table obtained for this pair of classes isshown in Table 2.The three main steps in the evaluation process arethe acquisition of "correct" classes from domain ex-perts, mapping the experts' clustering to that gener-ated by the system, and generating an overall mea-sure that represents the system's performance whencompared against the expert.Table 2: Contingency Table for Classes A and BSystem- NO 5 03.1 Knowledge Acquisition from ExpertsThe objective of this step is to get human experts toundertake the same task that the system performs,i.e., classifying a set of words into several potentiallyoverlapping classes.
The classes produced by a sys-tem are later compared to these "correct" classifica-tions provided by the expert.3.2 Mapp ing  A lgor i thmIn order to determine pairwise mappings betweenthe clustering enerated by the system and one pro-vided by an expert, a table of F-measures is con-structed, with a row for each class generated by thesystem, and a column for every class provided by theexpert.
Note that since the expert actually providesa hierarchy, there is one column corresponding toevery individual class and subclass provided by theexpert.
This allows the system's classes to map toa class at any level in the expert's hierarchy.
Thistable gives an estimate of how well each class gen-erated by the system maps to the ones provided bythe expert.The algorithm used to compute the actual map-pings from the F-measure table is briefly describedhere.
In each row of the table, mark the cell with thehighest F-measure as a potential mapping.
In gen-eral, conflicts arise when more than one class gener-ated by the system maps to a given class providedby the expert.
In other words, whenever a columnin the table has more than one cell marked as a po-tential mapping, a conflict is said to exist.
To re-solve a conflict, one of the system classes must bere-mapped.
The heuristic used here is that the classfor which such a re-mapping results in minimal ossof F-measure is the one that must be re-mapped.Several such conflicts may exist, and re-mappingmay lead to further conflicts.
The mapping algo-rithm iteratively searches for conflicts and resolvesthem till no more conflicts exist.
Note also that asystem class may map to an expert class only if theF-measure between them exceeds a certain thresholdvalue.
This ensures that a certain degree of similar-ity must exist between two classes for them to mapto each other.
We have used a threshold value of0.20.
This value is obtained purely by observationsmade on the F-measures between different pairs ofclasses with varying degrees of similarity.285Table 3: Noun Clustering ResultsExpert SystemPrecision I Recall I F-measureExpert A 75.38 29.09 0.42Expert B 77.08 25.23 0.38Expert C 73.85 37.88 0.503.3 Computat ion  of the Overall F-measureOnce the mappings have been determined betweenthe clusterings of the system and the expert, the nextstep is to compute the F-measure between the twoclusterings.
Rather than populating separate con-tingency tables for every pair of classes, constructa single contingency table.
For every pairwise map-ping found for the classes in these two clusterings,populate the YES-YES, YES-NO, and NO-YES cellsof the contingency table appropriately (see Table 2).Once all the mapped classes have been incorporatedinto this contingency table, add every element of allunmapped classes generated by the system to theYES-NO cell and every element of all unmappedclasses provided by the expert to the NO-YES cellof this table.
Once all classes in the two clusteringshave been accounted for, calculate the precision, re-call, and F-measure as explained in (Hatzivassiloglouand McKeown, 1993).4 Resu l t s  and  D iscuss ionIn one of our experiments, the 400 most frequentnouns in the Merck Veterinary Manual were clus-tered.
Three experts were used to evaluate the gen-erated noun clusters.
Some examples of the classesthat were generated by the system for the veteri-nary medicine domain are PROBLEM, TREAT-MENT, ORGAN, DIET, ANIMAL, MEASURE-MENT, PROCESS, and so on.
The results obtainedby comparing these noun classes to the clusteringsprovided by three different experts are shown in Ta-ble 3.
We have also experimented with the use ofWordNet to improve the classes obtained by a dis-tributional technique.
Some initial experiments haveshown that WordNet consistently improves the F-measures for these noun classes by about 0.05 on anaverage.
Details of these experiments can be foundin (Agarwal, 1995).It is our belief that the evaluation scheme pre-sented in this paper is useful for comparing differentclusterings produced by the same system or thoseproduced by different systems against one providedby an expert.
The resulting precision, recall, andF-measure should not be treated as a kind of "goldstandard" to represent the quality of these classesin some absolute sense.
It has been our experiencethat, as semantic lustering is a highly subjectivetask, evaluating a given clustering against differentexperts may yield numbers that vary considerably.However, when different clusterings generated by asystem are compared against he same expert (orthe same set of experts), such relative comparisonsare useful.The evaluation scheme presented here still suffersfrom one major limitation - -  it is not capable ofevaluating a hierarchy generated by a system againstone provided by an expert.
Such evaluations getcomplicated because of the restriction of one-to-onemapping.
More work definitely needs to be done inthis area.Re ferencesRajeev Agarwal.
1995.
Semantic feature eztractionfrom technical tezts with limited human interven-tion.
Ph.D. thesis, Mississippi State University,May.Roberto Basili, Maria Pazienza, and Paola Velardi.1994.
The noisy channel and the braying donkey.In Proceedings of the ACL Balancing Act Work-shop, pages 21-28, Las Cruces, New Mexico, July.Nancy Chincor.
1992.
MUC-4 evaluation metrics.In Proceedings of the Fourth Message Understand-ing Conference (MUC-4).Ralph Grishman and John Sterling.
1993.
Smooth-ing of automatically generated selectional con-straints.
In Proceedings of the ARPA Workshopon Human Language Technology.
Morgan Kauf-mann Publishers, Inc., March.Vasileios Hatzivassiloglou and Kathleen R. McKe-own.
1993.
Towards the automatic identifica-tion of adjectival scales: Clustering adjectives ac-cording to meaning.
In Proceedings of the 31stAnnual Meeting of the Association for Computa-tional Linguistics, pages 172-82.Francois-Michel Lang and Lynette Hirschman.
1988.Improved portability and parsing through interac-tive acquisition of semantic information.
In Pro-ceedings of the Second Conference on Applied Nat-ural Language Processing, pages 49-57, February.James Pustejovsky.
1992.
The acquisition of lex-ical semantic knowledge from large corpora.
InProceedings of the Speech and Natural LanguageWorkshop, pages 243--48, Harriman, N.Y., Febru-ary.Lisa Rau, Paul Jacobs, and Uri Zernik.
1989.
In-formation extraction and text summarization us-ing linguistic knowledge acquisition.
InformationProcessing and Management, 25(4):419-28.Philip Resnik.
1993.
Selection and Information:A Class-Based Approach to Lezical Relationships.Ph.D.
thesis, University of Pennsylvania, Decem-ber.
(Institute for Research in Cognitive Sciencereport IRCS-93-42).286
