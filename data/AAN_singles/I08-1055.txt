Corpus-based Question Answering for why-QuestionsRyuichiro Higashinaka and Hideki IsozakiNTT Communication Science Laboratories, NTT Corporation2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan{rh,isozaki}@cslab.kecl.ntt.co.jpAbstractThis paper proposes a corpus-based ap-proach for answering why-questions.
Con-ventional systems use hand-crafted patternsto extract and evaluate answer candidates.However, such hand-crafted patterns arelikely to have low coverage of causal expres-sions, and it is also difficult to assign suit-able weights to the patterns by hand.
In ourapproach, causal expressions are automati-cally collected from corpora tagged with se-mantic relations.
From the collected expres-sions, features are created to train an an-swer candidate ranker that maximizes theQA performance with regards to the corpusof why-questions and answers.
NAZEQA, aJapanese why-QA system based on our ap-proach, clearly outperforms a baseline thatuses hand-crafted patterns with a Mean Re-ciprocal Rank (top-5) of 0.305, making itpresumably the best-performing fully imple-mented why-QA system.1 IntroductionFollowing the trend of non-factoid QA, we areseeing the emergence of work on why-QA; e.g.,answering generic ?why X??
questions (Verberne,2006).
However, since why-QA is an inherently dif-ficult problem, there have only been a small numberof fully implemented systems dedicated to solvingit.
Recent systems at NTCIR-61 Question Answer-ing Challenge (QAC-4) can handle why-questions(Fukumoto et al, 2007).
However, their perfor-mance is much lower (Mori et al, 2007) than thatof factoid QA systems (Fukumoto et al, 2004;Voorhees and Dang, 2005).We consider that this low performance is due tothe great amount of hand-crafting involved in the1http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.htmlsystems.
Currently, most of the systems rely onhand-crafted patterns to extract and evaluate answercandidates (Fukumoto et al, 2007).
Such patternsinclude typical cue phrases and POS-tag sequencesrelated to causality, such as ?because of?
and ?byreason of.?
However, as noted in (Inui and Okumura,2005), causes are expressed in various forms, andit is difficult to cover all such expressions by hand.Hand-crafting is also very costly.
Some patternsmay be more indicative of causes than others.
There-fore, it may be useful to assign different weights tothe patterns for better answer candidate extraction,but currently this must be done by hand (Mori et al,2007).
It is not clear whether theweights determinedby hand are suitable.In this paper, we propose a corpus-based approachfor why-QA in order to reduce this hand-craftingeffort.
We automatically collect causal expressionsfrom corpora to improve the coverage of causal ex-pressions, and utilize a machine learning techniqueto train a ranker of answer candidates on the ba-sis of features created from the expressions togetherwith other possible features related to causality.
Theranker is trained to maximize the QA performancewith regards to a corpus of why-questions and an-swers, automatically tuning the weights of the fea-tures.This paper is organized as follows: Section 2 de-scribes previous work onwhy-QA, and Section 3 de-scribes our approach.
Section 4 describes the imple-mentation of our approach, and Section 5 presentsthe evaluation results.
Section 6 summarizes andmentions future work.2 Previous WorkAlthough systems that can answer why-questionsare emerging, they tend to have limitations in thatthey can answer questions only with causal verbs(Girju, 2003), in specific domains (Khoo et al,4182000), or questions covered by a specific knowl-edge base (Curtis et al, 2005).
Recently, Verberne(2006; 2007a) has been intensively working on why-QA based on the Rhetorical Structure Theory (RST)(Mann and Thompson, 1988).
However, her ap-proach requires manually annotated corpora withRST relations.When we look for fully implemented systems forgeneric ?why X??
questions, we only find a smallnumber of such systems.
Since why-QA would bea challenging task when tackled straightforwardly,requiring common-sense knowledge and semanticinterpretation of questions and answer candidates,current systems place higher priority on achiev-ability and therefore use hand-crafted patterns andheuristics to extract causal expressions as answercandidates and use conventional sentence similaritymetrics for answer candidate evaluation (Fukumoto,2007; Mori et al, 2007).
We argue, in this paper,that this hand-crafting is the cause of the currentlow performance levels.
Recently, (Shima and Mi-tamura, 2007) applied a machine learning approachto why-QA, but they also rely on manually selectedcue words to create their features.Semantic Role Labeling (SRL) techniques can beused to automatically detect causal expressions.
Inthe CoNLL-2005 shared task (SRL for English), thebest system found causal adjuncts with a reasonableaccuracy of 65% (Ma`rquez et al, 2005).
However,when we analyzed the data, we found that more thanhalf of the causal adjuncts contain explicit cues suchas ?because.?
Since causes are reported to be ex-pressed by a wide variety of linguistic phenomena,not just explicit cues (Inui and Okumura, 2005), fur-ther verification is needed before SRL can be safelyused for why-QA.Why-questions are a subset of non-factoid ques-tions.
Since non-factoid questions are observedin many FAQ sites, such sites have been regardedas valuable resources for the development of non-factoid QA systems.
Examples include Burke et al(1997), who used FAQ corpora to analyze questionsto achieve accurate question-type matching; Soricutand Brill (2006), who used them to train statisticalmodels for answer evaluation and formulation; andMizuno et al (2007), who used them to train clas-sifiers of question and answer-types.
However, theydo not focus on why-questions and do not use anycausal knowledge, which is considered to be usefulfor explicit why-questions (Soricut and Brill, 2006).3 ApproachIn this paper, we propose a corpus-based approachfor why-QA in order to reduce the hand-crafting ef-fort that is currently necessary.
We first automat-ically collect causal expressions from corpora anduse them to create features to represent an answercandidate.
The features are then used to train an an-swer candidate ranker that maximizes the QA per-formance with regards to a corpus of why-questionsand answers.
We also enumerate possible featuresthat may be useful for why-QA to be incorporatedin the training to improve the QA performance.Following the systems at QAC-4 (Fukumoto,2007) and the answer analysis in (Verberne, 2007b;Verberne et al, 2007), we consider the task of why-QA to be a sentence/paragraph extraction task.
Wealso assume that a document retrieval module of asystem returns top-N documents for a question onthe basis of conventional IR-related metrics and allsentences/paragraphs extracted from them are re-garded as answer candidates.
Hence, the task be-comes the ranking of given sentences/paragraphs.For an answer candidate (a sentence or a para-graph) to be the correct answer, the candidate should(1) have an expression indicating a cause and (2)be similar to the question in content, and (3) somecausal relation should be observed between the can-didate and the question.
For example, an answercandidate ?X was arrested for fraud.?
is likely tobe a correct answer to the question ?Why was Xarrested??
because ?for fraud?
expresses a cause,the question and the answer are both about the sameevent (X being arrested), and ?fraud?
and ?arrest?
in-dicate a causal relation between the question and thecandidate.
Condition (3) would be especially use-ful when the candidates do not have obvious cuesor topically similar words/phrases to the question;it may be worthwhile to rely on some prior causalknowledge to select one over others.
Although cur-rent working systems (Fukumoto, 2007; Mori et al,2007) do not explicitly state these conditions, theycan be regarded as using hand-crafted patterns for(1) and (3).2 Lexical similarity metrics, such as co-sine similarity and n-gram overlaps, are generallyused for (2).We represent each answer candidate with causalexpression, content similarity, and causal relation2(3) is dealt with in a manner similar to the treatment of?cause of death?
in (Smith et al, 2005).419features that encode how it complies with the threeconditions.
Here, the causal expression features arethose based on the causal expressions we aim to col-lect automatically.
For the other two types of fea-tures, we turn to the existing similarity metrics anddictionaries to derive features that would be usefulfor why-QA.
To train a ranker, we create a corpus ofwhy-questions and answers and adopt one of thema-chine learning algorithms for ranking.
The follow-ing sections describe the three types of features, thecorpus creation, and the ranker training.
The actualinstances of the features, the corpus, and the rankerwill be presented in Section 4.3.1 Causal Expression FeaturesWith the increasing attention paid to SRL, we cur-rently have a number of corpora, such as PropBank(Palmer, 2005) and FrameNet (Baker et al, 1998),that are tagged with semantic relations including acausal relation.
Since text spans for such relationsare annotated in the corpora, we can simply col-lect the spans marked by a causal relation as causalexpressions.
Since an answer candidate that has amatching expression for one of the collected causalexpressions is likely to be expressing a cause aswell, we can make the existence of each expressiona feature.
Although the collected causal expressionswithout any modification might be used to createfeatures, for generality, it would be better to abstractthem into syntactic patterns.
From m causal expres-sions/patterns automatically extracted from corpora,we can create m binary features.In addition, some why-QA systems may alreadypossess some good hand-crafted patterns to detectcausal expressions.
Since there is no reason not touse them if we know they are useful for why-QA,we can create a feature indicatingwhether an answercandidate matches existing hand-crafted patterns.3.2 Content Similarity FeaturesIn general, if a question and an answer candidateshare many words, it is likely that they are aboutthe same content.
From this assumption, we cre-ate a feature that encodes the lexical similarity of ananswer candidate to the question.
To calculate itsvalue, existing sentence similarity metrics, such ascosine similarity or n-gram overlaps, can be used.Even if a question and an answer candidate do notshare the same words, they may still be about thesame content.
One such case is when they are aboutthe same topic.
To express this case as a feature, wecan use the similarity of the question and the docu-ment in which the answer candidate is found.
Sincethe documents from which we extract answer candi-dates typically have scores output by an IR enginethat encode their relevance to the question, we canuse this score or simply the rank of the retrieved doc-ument as a feature.A question and an answer candidate may be se-mantically expressing the same content with differ-ent expressions.
The simplest case is when syn-onyms are used to describe the same content; e.g.,when ?arrest?
is used instead of ?apprehend.?
Forsuch cases, we can exploit existing thesauri.
Wecan create a feature encoding whether synonyms ofwords in the question are found in the answer can-didate.
We could also use the value of semanticsimilarity and relatedness measures (Pedersen et al,2004) or the existence of hypernym or hyponym re-lations as features.3.3 Causal Relation FeaturesThere are semantic lexicons where a semantic re-lation between concepts is indicated.
For example,the EDR dictionary3 shows whether a causal relationholds between two concepts; e.g., between ?murder?and ?arrest.?
Using such dictionaries, we can createpairs of expressions, one indicating a cause and theother its effect.
If we find an expression for a causein the answer candidate and that for an effect in thequestion, it is likely that they hold a causal relation.Therefore, we can create a feature encoding whetherthis is the case.
In cases where such semantic lex-icons are not available, they may be automaticallyconstructed, although with noise, using causal min-ing techniques such as (Marcu and Echihabi, 2002;Girju, 2003; Chang and Choi, 2004).3.4 Creating a QA CorpusFor ranker training, we need a corpus of why-questions and answers.
Because we regard thetask of why-QA as a ranking of given sen-tences/paragraphs, it is best to prepare the corpus inthe same setting.
Therefore, we use the followingprocedure to create the corpus: (a) create a question,(b) use an IR engine to retrieve documents for thequestion, (c) select among all sentences/paragraphsin the retrieved documents those that contain the an-swer to the question, and (d) store the question and a3http://www2.nict.go.jp/r/r312/EDR/index.html420set of selected sentences/paragraphs with their doc-ument IDs as answers.3.5 Training a RankerHaving created the QA corpus, we can apply exist-ing machine learning algorithms for ranking, suchas RankBoost (Freund et al, 2003) or RankingSVM (Joachims, 2002), so that the selected sen-tences/paragraphs are preferred to non-selected oneson the basis of their features.
Good ranking wouldresult in goodMean Reciprocal Rank (MRR), whichis one of the most commonly used measures in QA.4 ImplementationUsing our approach, we implemented a Japanesewhy-QA system, NAZEQA (?Naze?
means ?why?in Japanese).
The system was built as an extensionto our factoid QA system, SAIQA (Isozaki, 2004;Isozaki, 2005), and works as follows:1.
The question is analyzed by a rule-based ques-tion analysis component to derive a questiontype; ?REASON?
for a why-question.2.
The document retrieval engine extracts n-bestdocuments from Mainichi newspaper articles(1998?2001) using DIDF (Isozaki, 2005), avariant of the IDF metric.
We chose 20 as n.All sentences/paragraphs in the n documentsare extracted as answer candidates.
Whetherto use sentences or paragraphs as answer can-didates is configurable.3.
The feature extraction component produces, foreach answer candidate, causal expression, con-tent similarity, and causal relation features en-coding how it satisfies conditions (1)?
(3) de-scribed in Section 3.4.
The SVM ranker trained by a QA corpus ranksthe answer candidates based on the features.5.
The top-N answer candidates are presented tothe user as answers.In the following sections, we describe the features(399 in all), the QA corpus, and the ranker.4.1 Causal Expression Features(F1?F394: AUTO-Causal Expression) We au-tomatically extracted causal expressions from theEDR dictionary.
The EDR dictionary is a suiteof corpora and dictionaries and includes the EDRcorpus, the EDR concept dictionary (hierarchy ofword senses), and the EDR Japanese word dictio-nary (sense to word mappings).
The EDR corpusis a collection of independent Japanese sentencestaken from various sources, such as newspaper ar-ticles, magazines, and dictionary glosses.
The cor-pus is annotated with semantic relations including acausal relation in a manner similar to PropBank andFrameNet corpora.
We extracted regions marked by?cause?
tags and abstracted them by leaving onlythe functional words (auxiliary verbs and case, as-pect, tense markers) and replacing others with wild-cards ?*.?
For example, a causal expression ?ar-rested for fraud?
would be abstracted to ?
*-PASSfor *.?
We used CaboCha4 as a morphological ana-lyzer.
From 8,747 regions annotated with ?cause,?we obtained 394 causal expression patterns after fil-tering out those that occurred only once.
Finally, wehave 394 binary features representing the existenceof each abstracted causal expression pattern.
(F395: MAN-Causal Expression) We emulate themanually created patterns described in (Fukumoto,2007) and create a binary feature indicating whetheran answer candidate is matched by the patterns.4.2 Content Similarity Features(F396: Question-Candidate Cosine Similarity)We use the cosine similarity between a question andan answer candidate using the word frequency vec-tors of the content words.
We chose nouns, verbs,and adjectives as content words.
(F397: Question-Document Relevance) We use,as a feature, the inverse of the rank of the documentwhere the answer candidate is found.
(F398: Synonym Pair) This is a binary feature thatindicates whether a word and its synonym appearin an answer candidate and a question, respectively.We use the combination of the EDR concept dictio-nary and the EDR Japanese word dictionary as a the-saurus to collect synonym pairs.
We have 133,486synonym pairs.4.3 Causal Relation Feature(F399: Cause-Effect Pair) This is a binary fea-ture that indicates whether a word representing acause and a word corresponding to its effect ap-pear in an answer candidate and a question, respec-tively.
We used the EDR concept dictionary to findpairs of word senses holding a causal relation and4http://chasen.org/?taku/software/cabocha/421Q13: Why are pandas on the verge of extinction?
(000217262)A:000217262,L2 Since pandas are not good at raisingtheir offspring, the Panda Preservation Center inSichuan Province is promoting artificial insemina-tion as well as the training of mother pandas.A:000217262,L3 A mother panda often gives birth totwo cubs, but when there are two cubs, one is dis-carded, and young mothers sometimes crush theirbabies to death.A:000406060,L6 However, because of the recent devel-opment in the midland, they are becoming extinct.A:010219075,L122 The most common cause of the ex-tinction for mammals, birds, and plants is degrada-tion and destruction of habitat, followed by huntingand poaching for mammals and the impact of alienspecies for birds.Figure 1: An excerpt from the WHYQA collection.The number in parentheses is the ID of the docu-ment used to come up with the question.
The an-swers were headed by the document ID and the linenumber where the sentence is found in the docu-ment.
(N.B.
The above sentences were translated bythe authors.
)expanded the senses to corresponding words usingthe EDR Japanese word dictionary to create cause-effect word pairs.
We have 355,641 cause-effectword pairs.4.4 WHYQA CollectionSince QAC-4 does not provide official answer setsand their questions include only a small numberof why-questions, we created a corpus of why-questions and answers on our own.An expert, who specializes in text analysis andis not one of authors, created questions from arti-cles randomly extracted from Mainichi newspaperarticles (1998?2001).
Then, for each question, shecreated sentence-level answers by selecting the sen-tences that she considered to fully include the an-swer from a list of sentences from top-20 documentsreturned from the text retrieval engine with the ques-tion as input.
Paragraph-level answers were auto-matically created from the sentence-level answersby selecting the paragraphs containing the answersentences.The analyst was instructed not to create ques-tions by simply converting existing declarative sen-tences into interrogatives.
It took approximately fivemonths to create 1,000 question and answer sets(called the WHYQA collection).
All questions areguaranteed to have answers.
Figure 1 lists an exam-ple question and answer sentences in the collection.4.5 Training a Ranker by Ranking SVMUsing the WHYQA collection, we trained rank-ing models using the ranking SVM (Joachims,2002) (with a linear kernel) that minimizes thepairwise ranking error among the answer candi-dates.
In the training data, the answers were la-beled ?+1?
and non-answers ??1.?
When using sen-tences as answers, there are 4,849 positive exam-ples and 521,177 negative examples.
In the case ofparagraphs, there are 4,371 positive examples and261,215 negative examples.5 EvaluationFor evaluation, we compared the proposed system(NAZEQA) with two baselines.
Baseline-1 (COS)simply uses, for answer candidate evaluation, the co-sine similarity between an answer candidate and aquestion based on frequency vectors of their con-tent words.
The aim of having this baseline is to seehow the system performs without any use of causalknowledge.
Baseline-2 (FK) uses hand-crafted pat-terns described in (Fukumoto, 2007) to narrow downthe answer candidates to those having explicit causalexpressions, which are then ranked by the cosinesimilarity to the question.
NAZEQA and the twobaselines used the same document retrieval engineto obtain the top-20 documents and ranked the sen-tences or paragraphs in these documents.5.1 ResultsWe made each system output the top-1, 5, 10, and 20answer sentences and paragraphs for all 1,000 ques-tions in the WHYQA collection.
We used the MRRand coverage as the evaluation metrics.
Coveragemeans the rate of questions that can be answeredby the top-N answer candidates.
Table 1 shows theMRRs and coverage for the baselines and NAZEQA.A 10-fold cross validation was used for the evalua-tion of NAZEQA.We can see from the table that NAZEQA is bet-ter in all comparisons.
A statistical test (a signtest that compares the number of times one sys-tem places the correct answer before the other)showed that NAZEQA is significantly better thanFK for the top-5, 10, and 20 answers in the sen-tence and paragraph-levels (p<0.01).
Although thesentence-level MRR for NAZEQA is rather low, theparagraph-level MRR for the top-5 answers is 0.305,which is reasonably high for a non-factoid QA sys-tem (Mizuno et al, 2007).
The coverage is also422MRR Coveragetop-N COS FK NZQ COS FK NZQSentences as answer candidates:top-1 0.036 0.091+ 0.113 3.6% 9.1% 11.3%top-5 0.086 0.139+ 0.196* 19.1% 23.1% 35.4%top-10 0.102 0.149+ 0.216* 31.3% 30.7% 50.4%top-20 0.115 0.152 0.227* 51.4% 35.5% 66.6%Paragraphs as answer candidates:top-1 0.065 0.152+ 0.186 6.5% 15.2% 18.6%top-5 0.140 0.245+ 0.305* 29.2% 41.6% 53.1%top-10 0.166 0.257+ 0.328* 48.8% 50.5% 70.3%top-20 0.181 0.262+ 0.339* 70.7% 56.4% 85.6%Table 1: Mean Reciprocal Rank (MRR) and cov-erage for the baselines (COS and FK) and the pro-posed NAZEQA (NZQ in the table) system for theentire WHYQA collection.
The top-1, 5, 10, and20 mean the numbers of topmost candidates usedto calculate MRR and coverage.
Asterisks indicateNAZEQA?s statistical significance (p<0.01) overFK, and ?+?
FK?s over COS.Feature Set Sent.
Para.All features (NAZEQA) 0.181 0.287w/o F1?F394 (AUTO-Causal Exp.)
0.138* 0.217*w/o F395 (MAN-Causal Exp.)
0.179 0.286w/o F396 (Q-Cand.
Cosine Similarity) 0.131* 0.188*w/o F397 (Doc.-Q Relevance ) 0.161 0.275w/o F398 (Synonym Pair) 0.180 0.282w/o F399 (Cause-Effect Pair) 0.184 0.287Table 2: Performance changes in MRR (top-5) whenwe exclude one of the feature sets.
Asterisks indi-cate a statistically significant drop in performancefrom NAZEQA.
In this experiment, we used a two-fold cross validation to reduce computational cost.high for NAZEQA, making it possible to find an-swers within the top-10 sentences and top-5 para-graphs for more than 50% of the questions.
Becausethere are no why-QA systems known to be betterthan NAZEQA in MRR and coverage and becauseNAZEQA clearly outperforms a competitive base-line (FK), we conclude that NAZEQA has one ofthe best performance levels for why-QA.It is interesting to know how each of the featuresets (e.g., AUTO-Causal Expression Features) con-tributes to the QA performance.
Table 2 shows howthe performance in MRR (top-5) changes when oneof the feature sets is excluded in the training.
Al-though the drop in performance by removing theQuestion-Candidate Cosine Similarity feature is un-derstandable, the performance also drops signifi-cantly from NAZEQA when we exclude AUTO-Causal Expression features, showing the effective-ness of our automatically collected causal patterns.Rank Feature Name Weight1 Question-Candidate Cosine Similarity 4.662 Exp.
[de (by) * wo (-ACC) * teshimai (-PERF)] 1.863 Exp.
[no (of) * niyote wa (according to)] 1.444 Exp.
[no (of) * na (AUX) * no (of) * de (by)] 1.425 Exp.
[no (of) * ya (or) * niyotte (by)] 1.356 Exp.
[no (of) * ya (or) * no (of) * de (by)] 1.307 Exp.
[na (AUX) * niyotte (by)] 1.238 Exp.
[koto niyotte (by the fact that)] 1.229 Exp.
[to (and) * no (of) * niyotte (by)] 1.2010 Document-Question Relevance 0.89...27 Synonym Pair 0.40102 MAN-Causal Expression 0.16127 Cause-Effect Pair 0.15Table 3: Weights of features learned by the rank-ing SVM.
?AUTO-Causal Expression?
is denoted as?Exp.?
for lack of space.
AUX means an auxiliaryverb.
The abstracted causal expression patterns areshown in square brackets with their English transla-tions in parentheses.The MAN-Causal Expression, Synonym Pair, andCause-Effect Pair features, do not seem to contributemuch to the performance.
One of the reasons forthe small contribution of the MAN-Causal Expres-sion feature may be that the manual patterns used tocreate this feature overlap greatly with the automat-ically collected causal expression patterns, loweringthe impact of the MAN-Causal Expression feature.The small contribution of the Synonym Pair featureis probably attributed to the way the answers werecreated in the creation of the WHYQA Collection.Since the answer candidates from which the expertchose the answers were those retrieved by a text re-trieval engine that uses lexical similarity to retrieverelevant documents, it is possible that the answersthat contain synonyms had already been filtered outin the beginning, making the Synonym Pair featureless effective.
Without the Cause-Effect Pair feature,the performance does not change or even improvesa little when sentences are used as answers.
Thereason for this may be that the syntactically well-formed sentences of the newspaper articles mighthave made causal cues and patterns more effectivethan prior causal knowledge.
We need to investigatethe difference between the manually created causalpatterns and the automatically collected ones.
Wealso need to investigate whether the Synonym Pairand Cause-Effect Pair features could be useful inother conditions; e.g., when answers are created indifferent ways.
We also need to examine the qualityof our synonym and cause-effect word pairs because4230204060801001201401601802001  2  3  4  5  6  7  8  9  10Number of questionsRank of the first correct answerBaseline-1 (COS)Baseline-2 (FK)NAZEQAFigure 2: Distribution of the ranks of first correctanswers.
Paragraphs were used as answers.
A 10-fold cross validationwas used to evaluate NAZEQA.00.050.10.150.20.250.30.350.40.45100  200  300  400  500  600  700  800  900MRRNumber of training samplestop-1top-5top-10top-20Figure 3: Learning curve: Performance changeswhen answering Q1?Q100 with different sizes oftraining samples.
Paragraphs are used as answercandidates.their quality itself may be to blame.Furthermore, analyzing the trained ranking mod-els allows us to calculate the weights given to thefeatures (Hirao et al, 2002).
Table 3 shows theweights of the top-10 features.
We also include inthe table the weights of the Synonym Pair, MAN-Causal Expression and Cause Effect Pair features sothat the role of all three types of features in our ap-proach can be shown.
The analyzed model was theone trained with all 1,000 questions in the WHYQAcollection with paragraphs as answers.
Just as sug-gested by Table 2, the Question-Candidate CosineSimilarity feature plays a key role, followed by au-tomatically collected causal expression features.Figure 2 shows the distribution of the ranks ofthe first correct answers for all questions in theWHYQA collection for COS, FK, and NAZEQA.The distribution of COS is almost uniform, indicat-ing that lexical similarity cannot be directly trans-lated into causality.
The figure also shows thatNAZEQA consistently outperforms FK.It may be useful to know how much training datais needed to train a ranker.
We therefore fixed thetest set to Q1?Q100 in the WHYQA collection andtrained rankers with nine different sizes of train-ing data (100?900) created from Q101?
{Q200 ?
?
?Q1000}.
Figure 3 shows the learning curve.
Natu-rally, the performance improves as we increase thedata.
However, the performance gains begin to de-crease relatively early, possibly indicating the limi-tation of our approach.
Since our approach heavilyrelies on surface patterns, the use of syntactic andsemantic features may be necessary.6 Summary and Future WorkThis paper proposed corpus-based QA for why-questions.
We automatically collected causal ex-pressions from semantically tagged corpora andused them to create features to train an answer can-didate ranker that maximizes the QA performancewith regards to the corpus of why-questions and an-swers.
The implemented system NAZEQA outper-formed baselines with an MRR (top-5) of 0.305 andthe coverage was also high, making NAZEQA pre-sumably the best-performing system as a fully im-plemented why-QA system.As future work, we are planning to investigateother features that may be useful for why-QA.
Wealso need to examine how QA performance and theweights of the features differ when we use othersources for answer retrieval.
In this work, we fo-cused only on the ?cause?
relation in the EDR cor-pus to obtain causal expressions.
However, there areother relations, such as ?purpose,?
that may alsobe related to causality (Verberne, 2006).Although we believe our approach is language-independent, it would be worth verifying it by creat-ing an English version of NAZEQA based on causalexpressions that can be derived from PropBank andFrameNet.
Finally, we are planning to make publicsome of the WHYQA collection at the authors?
web-page so that various why-QA systems can be com-pared.AcknowledgmentsWe thank Jun Suzuki, Kohji Dohsaka, Masaaki Na-gata, and all members of the Knowledge Processing424Research Group for helpful discussions and com-ments.
We also thank the anonymous reviewers fortheir valuable suggestions.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998.The Berkeley FrameNet Project.
In Proc.
COLING-ACL,pages 86?90.Robin Burke, Kristian Hammond, Vladimir Kulyukin, SteveLytinen, Noriko Tomuro, and Scott Schoenberg.
1997.Question answering from frequently asked question files:Experiences with the FAQFinder system.
AI Magazine,18(2):57?66.Du-Seong Chang and Key-Sun Choi.
2004.
Causal relationextraction using cue phrase and lexical pair probabilities.
InProc.
IJCNLP, pages 61?70.Jon Curtis, Gavin Matthews, and David Baxter.
2005.
On theeffective use of Cyc in a question answering system.
In Proc.IJCAI Workshop on Knowledge and Reasoning for Answer-ing Questions, pages 61?70.Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer.2003.
An efficient boosting algorithm for combining prefer-ences.
Journal of Machine Learning Research, 4:933?969.Jun?ichi Fukumoto, Tsuneaki Kato, and Fumito Masui.
2004.Question answering challenge for five ranked answers andlist answers ?
overview of NTCIR4 QAC2 subtask 1 and 2?.
In Proc.
NTCIR, pages 283?290.Jun?ichi Fukumoto, Tsuneaki Kato, Fumito Masui, andTsunenori Mori.
2007.
An overview of the 4th question an-swering challenge (QAC-4) at NTCIR workshop 6.
In Proc.NTCIR, pages 483?440.Jun?ichi Fukumoto.
2007.
Question answering system for non-factoid type questions and automatic evaluation based on BEmethod.
In Proc.
NTCIR, pages 441?447.Roxana Girju.
2003.
Automatic detection of causal relationsfor question answering.
In Proc.
ACL 2003 Workshop onMultilingual Summarization and Question Answering, pages76?83.Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-sumoto.
2002.
Extracting important sentences with supportvector machines.
In Proc.
19th COLING, pages 342?348.Takashi Inui and Manabu Okumura.
2005.
Investigating thecharacteristics of causal relations in Japanese text.
In Proc.ACL 2005 Workshop on Frontiers in Corpus Annotation II:Pie in the Sky.Hideki Isozaki.
2004.
NTT?s question answering system forNTCIR QAC2.
In Proc.
NTCIR, pages 326?332.Hideki Isozaki.
2005.
An analysis of a high-performanceJapanese question answering system.
ACM Transactions onAsian Language Information Processing (TALIP), 4(3):263?279.Thorsten Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proc.
KDD, pages 133?142.Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000.
Ex-tracting causal knowledge from a medical database usinggraphical patterns.
In Proc.
38th ACL, pages 336?343.W.
Mann and S. Thompson.
1988.
Rhetorical structure theory:Toward a functional theory of text organization.
In Text, vol-ume 8, pages 243?281.Daniel Marcu and Abdessamad Echihabi.
2002.
In Proc.
40thACL, pages 368?375.Llu?
?s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus Catala`.2005.
Semantic role labeling as sequential tagging.
In Proc.CoNLL, pages 193?196.Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, and KatunobuItou.
2007.
Non-factoid question answering experimentsat NTCIR-6: Towards answer type detection for realworldquestions.
In Proc.
NTCIR, pages 487?492.Tatsunori Mori, Mitsuru Sato, Madoka Ishioroshi, YugoNishikawa, Shigenori Nakano, and Kei Kimura.
2007.
Amonolithic approach and a type-by-type approach for non-factoid question-answering ?
YokohamaNational Universityat NTCIR-6 QAC ?.
In Proc.
NTCIR, pages 469?476.Martha Palmer.
2005.
The proposition bank: An annotatedcorpus of semantic roles.
Comp.
Ling., 31(1):71?106.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.2004.
Wordnet::Similarity - Measuring the Relatedness ofConcepts.
In Proc.
HLT-NAACL (Demonstration Papers),pages 38?41.Hideki Shima and Teruko Mitamura.
2007.
JAVELIN III: An-swering non-factoid questions in Japanese.
In Proc.
NTCIR,pages 464?468.Troy Smith, Thomas M. Repede, and Steven L. Lytinen.
2005.Determining the plausibility of answers to questions.
InProc.
AAAI Workshop on Inference for Textual Question An-swering, pages 52?58.Radu Soricut and Eric Brill.
2006.
Automatic question answer-ing using the web: Beyond the factoid.
Journal of Informa-tion Retrieval, 9:191?206.Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-ArnoCoppen.
2007.
Evaluating discourse-based answer extrac-tion for why-question answering.
In Proc.
SIGIR (Postersand Demonstrations), pages 735?736.Suzan Verberne.
2006.
Developing an approach for why-question answering.
In Proc.
11th European Chapter ofACL, pages 39?46.Suzan Verberne.
2007a.
Evaluating answer extraction for why-QA using RST-annotated Wikipedia texts.
In Proc.
12thESSLLI Student Session, pages 255?266.Suzan Verberne.
2007b.
Paragraph retrieval for why-questionanswering.
In Proc.
Doctoral Consortium Workshop atSIGIR-2007, page 922.Ellen M. Voorhees and Hoa Trang Dang.
2005.
Overview ofthe TREC 2005 question answering track.
In Proc.
TREC.425
