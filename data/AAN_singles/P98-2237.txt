Using Chunk Based Partial Parsingof Spontaneous Speech in Unrestricted Domains forReducing Word Error Rate in Speech RecognitionKlaus  Zechner  and A lex  Waibe lLanguage Technologies Ins t i tu teCarnegie Mellon Univers i ty5000 Forbes AvenueP i t tsburgh,  PA 15213, USA{zechner ,  ahw}@cs, cmu.
eduAbst ractIn this paper, we present a chunk based partial pars-ing system for spontaneous, conversational speechin unrestricted domains.
We show that the chunkparses produced by this parsing system can be use-fully applied to the task of reranking Nbest listsfrom a speech recognizer, using a combination ofchunk-based n-gram model scores and chunk cov-erage scores.The input for the system is Nbest lists generatedfrom speech recognizer lattices.
The hypothesesfrom the Nbest lists are tagged for part of speech,"cleaned up" by a preprocessing pipe, parsed bya part of speech based chunk parser, and rescoredusing a backpropagation neural net trained on thechunk based scores.
Finally, the reranked Nbest listsare generated.The results of a system evaluation are promising inthat a chunk accuracy of 87.4% is achieved and thebest performance on a randomly selected test set isa decrease in word error rate of 0.3 percent (abso-lute), measured on the new first hypotheses in thereranked Nbest lists.1 In t roduct ionIn the area of parsing spontaneous speech, mostwork so far has primarily focused on dealing withtexts within a narrow, well-defined omain.
Fullscale parsers for spontaneous speech face severe dif-ficulties due to the intrinsic nature of spoken lan-guage (e.g., false starts, hesitations, ungrammati-calities), in addition to the well-known complexitiesof large coverage parsing systems in general (Lavie,1996; Light, 1996).An even more serious problem is the imper-fect word accuracy of speech recognizers, particu-larly when faced with spontaneous speech over alarge vocabulary and over a low bandwidth channel.This is particularly the case for the SWITCHBOARDdatabase (Godfrey et al, 1992) which we mainlyused for development, testing, and evaluation of oursystem.
Current state-of-the-art recognizers exhibitword error rates (WER 1) for this corpus of approx-IThe word error rate (WEFt in %) is defined as follows:imately 30%-40% (Finke et al, 1997).
This meansthat in fact about every third word in an input utter-ance will be misrecognized.
Thus, any parser whichis too restrictive with respect o the input it acceptswill likely fail to find a parse for most of these ut-terances.When the domain is restricted, sufficient cover-age can be achieved using semantically guided ap-proaches that allow skipping of unparsable words orsegments (Ward, 1991; Lavie, 1996).Since we cannot build on semantic knowledge forconstructing parsers in the way it is done for lim-ited domains when attempting to parse spontaneousspeech in unrestricted domains, we argue that moreshallow approaches have to be employed to reach asufficient reliability with a reasonable amount of ef-fort.In this paper, we present a chunk based partialparser, following ideas from (Abney, 1996), whichis used to to generate shallow syntactic structuresfrom speech recognizer output.
These representa-tions then serve as the basis for scores used in thetask of reranking Nbest lists.The organization of this paper is as follows: Insection 2 we introduce the concept of chunk.pars-ing and how we interpret and use it in our system.Section 3 deals with the issue of reranking Nbestlists and the question of why we consider it appro-priate to use chunk representations for this task.
Insection 4, the system architecture is described, andthen the results from an evaluation of the system arepresented and discussed (sections 5 and 6).
Finally,we give the results of a small study with human sub-jects on an analogous task (section 7), before point-ing out directions for future research (section 8) andsummarizing our work (section 9).2 Chunk  Pars ingThere have been recent developments which encour-age the investigation of the possibility of parsingspeech in unrestricted omains.
It was demon-strated that parsing natural language 2 can be han-WER ----- 100.0.  substitutiona-~d~leticms-~insertionscorrectt  ~ubstitutiollsJrd?|?tion$2mostly of the written, but also of the spoken type1453dled by very simple, even finite-state approaches ifone adheres to the principle of "chunking" the inputinto small and hence easily manageable constituents(Abney, 1996; Light, 1996).We use the notion of a chunk similar to (Abney,1996), namely a contiguous, non-recursive phrase.Chunk phrases mostly correspond to traditional no-tions of syntactic onstituents, uch as NPs or PPs,but there are exceptions, e.g.
VCs ("verb complexphrases"), which are not used in most traditionallinguistic paradigms.
3 Unlike in (Abney, 1996), ourgoal was not to build a multi-stage, cascaded sys-tem to result in full sentence parses, but to confineourselves to parsing of "basic chunks".A strong rationale for following this simple ap-proach is the nature of the ill-formed input due to(i) spontaneous speech dysfluencies, and (ii) errorsin the hypotheses of the speech recognizer.To get an intuitive feel about the output of thechunk parser, we present a short example here: 4\[conj BUT\] \[np HE\] \[vc DOESN'T REALLY LIKE\]\[np HIS HISTORY TEACHER\] \[advp VERY MUCH\]3 Rerank ing  o f  Speech  Recogn izerNbest  L i s tsState-of-the-art speech recognizers, such as theJANUS recognizer (Waibel et al, 1996) whose outputwe used for our system, typically generate lattices ofword hypotheses.
From these lattices, Nbest listscan be computed automatically, such that it is en-sured that the ordering of hypotheses in these listscorresponds to the internal ranking of the speechrecognizer.As an example, we present a reference utterance(i.e., "what was actually said") and two hypothesesfrom the Nbest list, given with their rank:KEF: YOU WEREN'T BORN JUST TO SOAK UP SUN1: YOU WF.JtEN'T BORN JUSTICE SO CUPS ON190: YOU WEREN'T BORN JUST TO SOAK UP SUNThis is a typical example, in that it is frequentlythe case that hypotheses which are ranked furtherdown the list, are actually closer to the true (ref-erence) utterance (i.e., the WER would be lower).
5So, if we had an oracle that could tell the speechrecognizer to always pick the hypothesis with thelowest WER from the Nbest list (instead of the top3A VC-chunk is a contiguous verbal segment of an utter-ance, whereas a VP usually comprises this verbal segment andi ts arguments together.4conj=conjunction chunk, np=noun phrase chunk,vc=verb complex chunk, advp--adverbial phrase chunk5In this case, hypothesis 190 is completely correct; gener-ally it is not the case, particularly for longer utterances, tofind the correct hypothesis in the lattice.ranked hypothesis), the global performance could beimproved significantly.
6In the speech recognizer architecture, the searchmodule is guided mostly by very local phenomena,both in the acoustic models (a context of severalphones), and in the language models (a context ofseveral words).
Also, the recognizer does not makeuse of any syntactic (or constituent-based) howl-edge.Thus, the intuitive idea is to generate represen-tations that allow for a discriminative judgment be-tween different hypotheses in the Nbest list, so thateventually a more plausible candidate can be iden-tified, if, as it is the case in the following example,the resulting chunk structure is more likely to bewell-formed than that of the first ranked hypothesis:1: \[np YOU\] \[vc ~.J~.$I'T BORN\] \[np JUSTICE\]\[advp SO\] \[np CUPS\] \[advp ON\]190: \[np YOU\] \[vc WFJtEN'T BORN\]\[advp JUST\] \[vc TO SOAK UP\] \[np SUN\]We use two main scores to assess this plausibility:(i) a chunk coverage score (percentage ofinput stringwhich gets parsed), and (ii) a chunk language modelscore, which is using a standard n-gram model basedon the chunk sequences.
The latter should giveworse scores in cases like hypothesis (1) in our exam-ple, where we encounter the vc-np-advp-np-advpsequence, as opposed to hypothesis (190) with themore natural vc-advp-vc-np sequence.4 Sys tem Arch i tec ture4.1 OverviewFigure 1 shows the global system architecture.The Nbest lists are generated from lattices that areproduced by the JANUS speech recognizer (Walbelet al, 1996).
First, the hypothesis duplicates withrespect o silence and noise words are removed fromthe Nbest lists 7, next the word stream is tagged withBrill's part of speech (POS) tagger (Brill, 1994),Version 1.14, adapted to the SWITCHBOARD Cor-pus.
Then, the token stream is "cleaned up" in thepreprocessing pipe, which then serves as the inputof the POS based chunk parser.
Finally, the chunkrepresentations generated by the parser are used tocompute scores which are the basis of the rescoringcomponent that eventually generates new rerankedNbest lists.In the following, we describe the major compo-nents of the system in more detail.6On our data, from WER.--43.5~ to WER=30.4%, usingthe top 300 hypotheses of each utterance (see Table I).7since we are ignoring these pieces of information in laterstages of processing1454input utlemncesspeech recognizert wordlattices\] duplicate filter I.........................
I ............................ i IIt oh- ,--tlichunk sequenceNbest rescorer.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
?reranked Nbest listsFigure 1: Global system architecture4.2 Preprocesslng P ipeThis preprocessing pipe consists of a number of fil-ter components hat serve the purpose of simplify-ing the input for subsequent components, withoutloss of essential information.
Multiple word repeti-tions and non-content interjections or adverbs (e.g.,"actually") are removed from the input, some shortforms are expanded (e.g., "we'll" -+ "we will"), andfrequent word sequences are combined into a singletoken (e.g., % lot of" --~ "a_lot_of").
Longer turnsare segmented into short clauses, which are definedas consisting of at least a subject and an inflectedverbal form.4.3 Chunk ParserThe chunk parser is a chart based context freeparser, originally developed for the purpose of se-mantic frame parsing (Ward, 1991).
For our pur-poses, we define the chunks to be the relevant con-cepts in the underlying rammar.
We use 20 differ-ent chunks that consist of part of speech sequences(there are 40 different POS tags in the version ofBrill's tagger that we are using).
Since the grammaris non-recursive, no attachments of constituents aremade, and, also due to its small size, parsing is ex-tremely fast (more than 2000 tokens per second), sThe parser takes the POS sequence from the taggedinput, parses it in chunks, and finally, these POS-chunks are combined again with the words from theinput stream.4.4 Nbest RescorerThe rescorer's task is to take an Nbest list generatedfrom the speech recognizer and to label each elementin this list (=hypothesis) with a new score whichshould correspond to the true WER of the respectivehypothesis; these new scores are then used for thereranking of the Nbest list.
Thus, in the optimalcase, the hypothesis with lowest WER would moveto the top of the reranked Nbest list.The three main components of the rescorer are:1.
Score Calculation:There are three types of scores used:(a) normalized score from the recognizer (withrespect o the acoustic and language mod-els used internally): highest score = lowestrank number in the original Nbest list(b) chunk coverage scores: derived from therelative coverage of the chunk parser foreach hypothesis: highest score = completecoverage, no skipped words in the hypoth-esis(c) chunk language model score: this is a stan-dard n-gram score, derived from the se-quence of chunks in each hypothesis (asopposed to the sequence of words in therecognizer): high score = high probabilityfor the chunk sequence; the chunk languagemodel was computed on the chunk parsesof the LDC 9 SWITCHBOARD transcripts(about 3 million words total; we computedstandard 3-gram and 5-gram backoff mod-els).2.
Reranking Neural  Network: We are usinga standard three layer backpropagation neuralnetwork.
The input units are the scores de-scribed here, the output unit should be a goodpredictor of the true WER of the hypothesis.For training of the neural net, the data was splitrandomly into a training and a test set.3.
Cutoff  Fi lter: Initial experiments and dataanalysis howed clearly that in short utterances(less than 5-10 words) the potential reductionin WER is usually low: many of these utter-ances are (almost) correctly recognized in theSDEC Alpha, 200MHz9Linguistic Data Consortium1455data set Utts.
true opt.WER WERtrain 271 1"45.05 30.75test 103 40.50 29.83Total 374 43.51 30.41Table 1: Characteristics of train and test sets(WER in %)first place.
For this reason, this filter preventsapplication of reranking to these short utter-ances.5 Experiment:  System Performance5.1 DataThe data we used for system training, testing,and evaluation were drawn from the SWITCHBOARDand CALLHOME LVCSR 1?
evaluation in spring 1996(Finke and Zeppenfeld, 1996).
In total, 374 utter-ances were used that were randomly split to form atraining and test set.
For these utterances, Nbestlists of length 300 were created from speech recog-nizer lattices.
11 The word error rates (WER) ofthese sets are given in Table 1.
While the trueWER corresponds to the WER of the first hypoth-esis (--top ranked), the optimal WER is computedunder the assumption that an oracle would alwayspick the hypothesis with the lowest WER in everyNbest list.
The difference between the average trueWER and the optimal WER is 13.1%; this givesthe maximum margin of improvement that rerank-ing can possibly achieve on this data set.
Anotherinteresting figure is the expected WER gain, whena random process would rerank the Nbest lists andjust pick any hypothesis to be the (new) top one.For the test set, this expected WER gain is -4.9%(i.e., the WER would drop by 4.9%).5.2 Global Sys tem SpeedThe system runtime, starting from the POS-taggerthrough all components up to the final evaluation ofWER gain for the 103 utterances of the test set (ca.8400 hypotheses, 145000 tokens) is less than 10 min-utes on a DEC Alpha workstation (200 MHz, 192MBRAM), i.e., the throughput is more than 10 utter-ances per minute (or 840 hypotheses per minute).5.3 Par t  Of  Speech TaggerWe are using Brill's part of speech tagger as animportant preprocessing component of our system(Brill, 1994).
As our evaluations prove, the perfor-mance of this component isquite crucial to the wholel?Large Vocabulary Continuous Speech RecognitionII Short utterances tend to have small lattices and thereforenot all Nbest lists comprise the  max imum of 300 hypotheses.test set words miss.
wrong sup.ft, error \]20utts  372 33 13 1 12.6% I20ut ts -cor r  372 10 0 1 3.0% \]Table 2: Performance of the chunk parser ondifferent est setssystem's performance, in particular to the segmen-tation module and to the POS based chunk parser.Since the original tagger was trained on writ-ten corpora (Wall Street Journal, Brown corpus),we had to adapt it and retrain it on SWITCH-BOARD data.
The tagset was slightly modified andadapted, to accommodate phenomena ofspoken lan-guage (e.g., hesitation words, fillers), and to facili-tate the task of the segmentation module (e.g., bytagging clausal and non-clausal coordinators differ-ently).
After the adaptive training, the POS accu-racy is 91.2% on general SWITCHBOARD 12 and 88.3%on a manually tagged subset of the training data weused for our experiments.
13Fortunately, some of these tagging errors are irrel-evant with respect o the POS based chunk gram-mar: the tagger's performance with respect o thisgrammar is 92.8% on general SWITCHBOARD, and90.6% for the manually tagged subset from our train-ing set.5.4 Chunk ParserThe evaluation of the chunk parser's accuracy wasdone on the following data sets: (i) 20 utterances(5 references and 15 speech recognizer hypothe-ses) (20utts); (ii) the same data, but with manualcorrections of POS tags and short clause segmentboundaries (20utts-corr) .For each word appearing in the chunk parser's out-put (including the skipped words14), it was deter-mined, whether it belonged to the correct chunk, orwhether it had to be classified into one of these threeerror categories:?
"missing": either not parsed or wrongfully in-corporated in another chunk;?
"wrong": belongs to the wrong type of chunk;?
"superfluous": parsed as a chunk that shouldnot be there (because it should be a part ofanother chunk)12The original.LDC transcripts not used in our rescoringevaluations.13These numbers are significantly lower than those achiev-able by taggers for written language~ we conjecture that onereason for this lower performance is due to the more refinedtagset we use which causes a higher amount of ambiguity forsome frequent words.14Skipped words are words that  could not be parsed intoany chunks.1456data seteval21testbest expectedperformance WER gain+2.0 +0.5+0.3 -4.9Table 3: WER gain: best results in neuralnet experiments for two test sets (in absolute%)The results of this evaluation are given in Table 2.We see that an optimally preprocessed input is in-deed crucial for the accuracy of the parser: it in-creases from 87.4% to 97.0%.
155.5 Nbest  RescorerThe task of the Nbest list rescorer is performed bya neural net, trained on chunk coverage, chunk lan-guage model, and speech recognizer scores, with thetrue WER as target value.
We ran experiments totest various combinations of the following param-eters: type of chunk language model (3-gram vs.5-gram); chunk score parameters (e.g., penalty fac-tors for skipped words, length normalization param-eters); hypothesis length cutoffs (for the cutoff fil-ter); number of hidden units; number of trainingepochs.The net with the best performance on the test sethas one hidden unit, and is trained for 10 epochs.
Alength cutoff of 8 words is used, i.e., only hypothe-ses whose average length was >_ 8 are actually con-sidered as reranking candidates.
A 3-gram chunklanguage model proved to be slightly better than a5-gram model.Table 3 gives the results for the entire test setand a subset of 21 hypotheses (eval21) which hadat least a potential gain of three word errors (whencomparing the first ranked hypothesis with the hy-pothesis which has the fewest errors), leWe also calculated the cumulative average WERbefore and after reranking, over the size of the Nbestlist for various hypotheses.
17 Figure 2 shows theplots of these two graphs for the example utterancein section 3 ("you weren't born just to soak up sun").We see very clearly, that in this example not onlyhas the new first hypothesis a significant WER gaincompared to the old one, but that in general hy-potheses with lower WER moved towards the top ofthe Nbest list.Is (Abney, 1996) reports a comparable per word accuracy ofhis CASS2 chunk parser (92.1%).1aWhile the latter set was obtained post hoc (using theknown WEB.
), it is conceivable to approximate this biased se-lection, when fairly reliable confidence annotations from thespeech recognizer are available (Chase, 1997).17Average of the WEB.
from hypotheses 1 to k in the Nbestilst.100IN)I 6O!|20A~ge lo:#m~?1~Kl WER t l lo~ ~ | I~ ~ ronm 4belo.m NN m,'mn~~ NN m~m.m.ldngf lI I I I I i I I~e~Nbe~l is tFigure 2: Cumulative average WER beforeand after reranking for an example utterancerank/nr .1/12/33/1894/1905/2146/269/2738/296hypothes isyou  weren't born justice so cups onyou weren't born just to  sew cups onyou weren't born justice vocal songyou weren ' t  born  jus t  to  soak up  sunyou  weren't foreign just to sew cups onyou weren't born justice so courts onyou weren't born just to sew carp songyou weren't boring just to soak  up sonTable 4: Recognizer hypotheses from anexample utterance (hypothesis nr.
190exactly corresponds to the reference)A more detailed account of 8 hypotheses from thesame example utterance is given in tables 4 (whichlists the recognizer hypotheses) and 5 (where variousscores, WER, and the ranks before and after thereranking procedure are provided).
It can be seenthat while the new first best hypothesis i  not theone with the lowest WER, it does have a lower WEB,than the originally first ranked hypothesis (25.0% vs.62.5%).6 DiscussionUsing the neural net with the characteristics de-scribed in the previous ection, we were able to geta positive effect in WER reduction on a non-biasedtest set.
While this effect is quite small, one hasto keep in mind that the (constituent-like) chunkrepresentations were the only source of informationfor our reranking system, in addition to the internalscores of the speech recognizer.
It can be expectedthat including more sources of knowledge, like theplausibility of correct verb-argument structures (thecorrect match of subcategorization frames), and thelikelihood of selectional restrictions between the ver-bal heads and their head noun arguments would fur-ther improve these results.1457Hypo-RankNew/OldI/82/73/44/35/66/57/18/2Table 5: Scores, WER,  andTrue WER Chunk-Cov.
Skipped Chunk-LM Norm.SRin % Score Words Score Score25.0 0.875 0 0.984 0.9337.5 0.625 0 0.865 0.940.0 0.75 0 0.954 0.9762.5 0.5 0 0.618 0.9862.5 0.625 0.125 0.715 0.9550.0 0.75 0.125 1.056 0.9662.5 0.625 0.125 0.715 1.037.5 0.625 0.125 1.032 0.99ranks before and after reranking of 8 hypotheses from an example utteranceThe second observation we make when looking atthe markedly positive results of the eval21 set con-cerns the potential benefit of selecting good candi-dates for reranking in the first place.7 Compar i son :  Human StudyOne of our motivations for using syntactic represen-tations for the task of Nbest list reranking was theintuition that frequently, by just reading through thelist of hypotheses, one can eliminate highly implau-sible candidates or favor more plausible ones.To put this intuition to test, we conducted a smallexperiment where human subjects were asked to lookat pairs of speech recognizer hypotheses drawn fromthe Nbest lists and to decide which of these they con-sidered to be "more well-formed".
Well-formednesswas judged in terms of (i) structure (syntax) and(ii) meaning (semantics).
128 hypothesis pairs wereextracted from the training set (the top ranked hy-pothesis and the hypothesis with lowest WER) ,  andpresented in random order to the subjects.4 subjects participated in the study and table 6gives the results of its evaluation: WER gain ismeasured the same way as in our system evalua-tion -- here, it corresponds to the average reductionin WER,  when the well-formedness judgements ofthe human subjects were to be used to rerank therespective hypothesis-pairs.While the max imum WER gain for these 128hypothesis-pairs is 15.2%, the expected WER gain(i.e., the WER gain of a random process) is 7.6%.Whereas the difference between both methods toa random choice is highly significant (syntax: a =0.01,t = 9.036, df = 3; semantics: a = 0.01,t =11.753,df = 3) TM , the difference between thesetwo methods is not (a = 0.05,t = -1.273,df =6) 19 .
The latter is most likely due to the fact thatthere were only few hypotheses that were judgeddifferently in terms of syntactic or semantic well-formedness by one subject: on average, only 6% of18These results were obtained using the one-sided t-test.tOTwo-sided t-test.SubjectA 10.0B 10.0C 9.1D 10.2Total Avg.
9.810.310.29.710.810.2Table 6: Human Performance (WER gain in %)the hypothesis-pairs received a different judgementby one subject.8 Future  WorkFrom our results and experiments, we conclude thatthere are several directions of future work which arepromising to pursue:?
improvement of the POS tagger: Since the per-formance of this component was shown to beof essential importance for later stages of thesystem, we expect o see benefits from puttingefforts into further training.?
alternative language models: An idea for im-provement here is to integrate skipped wordsinto the LM (similar to the modeling of noisein speech).
In this way we get rid of the skip-ping penalties we were using so far and whichblurred the statistical nature of the model.?
identifying ood reranking candidates: So far,the only and exclusive heuristics we are usingfor determining when to rerank and when notto, is to use the length-cutoff filter to excludeshort utterances from being considered in the fi-nal reranking procedure.
(Chase, 1997) showedthat there are a number of potentially useful"features" from various ources within the rec-ognizer which can predict, at least to a cer-tain extent, the "confidence" that the recognizerhas about a particular hypothesis.
Hypotheses1458which have a higher WER on average also ex-hibit a higher word gain potential, and there-fore these predictions appear to be promisingindeed.?
adding argument structure representations: Thechunk representation in our system only givesan idea about which constituents here are ina clause and what their ordering is.
A richermodel has to include also the dependencies be-tween these chunks.
Exploiting statistics aboutsubcategorization frames of verbs and selec-tional restrictions would be a way to enhancethe available representations.9 SummaryIn this paper we have shown that it is feasible to pro-duce chunk based representations for spontaneousspeech in unrestricted domains with a high level ofaccuracy.The chunk representations are used to generatescores for an Nbest list reranking component.The results are promising, in that the best perfor-mance on a randomly selected test set is an absolutedecrease in word error rate of 0.3 percent, measuredon the new first hypotheses in the reranked Nbestlists.10 AcknowledgementsThe authors are grateful for valuable discussionsand suggestions from many people in the InteractiveSystems Laboratories, CMU, in particular to AlonLavie, Klaus PLies, Marsal GavMd~, Torsten Zeppen-feld, and Michael Finke.
Also, we wish to thankMarsal Gavald~, Maria Lapata, Alon Lavie, and thethree anonymous reviewers for their comments onearlier drafts of this paper.More details about the work reported here can befound in the first author's master's thesis (Zechner,1997).This work was funded in part by grants of the Aus-trian Ministry for Science and Research (BMWF),the Verbmobil project of the Federal Republic ofGermany, ATR - Interpreting TelecommunicationsResearch Laboratories of Japan, and the US Depart-ment of Defense.ReferencesSteven Abney.
1996.
Partial parsing via finite-statecascades.
In Workshop on Robust Parsing, 8thEuropean Summer School in Logic, Language andInformation, Prague, Czech Republic, pages 8-15.Eric Brill.
1994.
Some advances in transformation-based part of speech tagging.
In Proceeedings ofAAAI-94.Lin Chase.
1997.
Error-responsive f edback mech-anisms for speech recognizers.
Ph.D. thesis,Carnegie Mellon University, Pittsburgh, PA.Michael Finke, Jilrgen Fritsch, Petra Geutner, KlausRies and Torsten Zeppenfeld.
1997.
The Janus-RTk SWITCHBOARD//CALLHOME 1997 EvaluationSystem.
In Proceedings of LVCSR HubS-e Work-shop, May 13-I5, Baltimore, Maryland.Michael Finke and Torsten Zeppenfeld.
1996.LVCSR SWITCHBOARD April 1996 Evaluation Re-port.
In Proceedings of the LVCSR Hub 5 Work-shop, April ~9 - May 1, 1996 Maritime Instituteof Technology, Linthicum Heights, Maryland.J.
J. Godfrey, E. C. Holliman, and J. McDaniel.1992.
SWITCHBOARD: telephone speech corpusfor research and development.
In Proceedings ofthe ICASSP-9$, volume 1, pages 517-520.Alon Lavie.
1996.
GLR*: A Robust Grammar.Focused Parser for Spontaneously Spoken Lan-guage.
Ph.D. thesis, Carnegie Mellon University,Pittsburgh, PA.Marc Light.
1996.
CHUMP: Partial parsing andunderspecified representations.
In Proceedings ofthe l~th European Conference on Artificial Intel-ligence (ECAI-96), Budapest, Hungary.Alex Waibel, Michael Finke, Donna Gates, MarsalGavaldh, Thomas Kemp, Alon Lavie, Lori Levin,Martin Maier, Laura Maytleld, Arthur McNair,Ivica Rogina, Kaori Shima, Tilo Sloboda, MonikaWoszczyna, Torsten Zeppenfeld, and PumingZhan.
1996.
JANUS-II - advances inspeech recog-nition.
In Proceedings of the ICASSP-96.Wayne Ward.
1991.
Understanding spontaneousspeech: The PHOENIX system.
In Proceedingsof ICASSP-91, pages 365-367.Klaus Zechner.
1997.
Building chunk level rep-resentations for spontaneous peech in unre-stricted domains: The CHUNKY system andits application to reranking Nbest lists of aspeech recognizer.
M.S.
Project Report, CMU,Department of Philosophy.
Available fromhttp://w~, eontrib, andrew, cmu.
edu/'zechner/publ icat ions.
html1459
