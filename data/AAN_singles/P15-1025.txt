Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 250?259,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Continuous Word Embedding with Metadata for QuestionRetrieval in Community Question AnsweringGuangyou Zhou1, Tingting He1, Jun Zhao2, and Po Hu11School of Computer, Central China Normal University, Wuhan 430079, China2National Laboratory of Pattern Recognition, CASIA, Beijing 100190, China{gyzhou,tthe,phu}@mail.ccnu.edu.cn jzhao@nlpr.ia.ac.cnAbstractCommunity question answering (cQA)has become an important issue due to thepopularity of cQA archives on the web.This paper is concerned with the problemof question retrieval.
Question retrievalin cQA archives aims to find the exist-ing questions that are semantically equiv-alent or relevant to the queried questions.However, the lexical gap problem bringsabout new challenge for question retrievalin cQA.
In this paper, we propose to learncontinuous word embeddings with meta-data of category information within cQApages for question retrieval.
To deal withthe variable size of word embedding vec-tors, we employ the framework of fisherkernel to aggregated them into the fixed-length vectors.
Experimental results onlarge-scale real world cQA data set showthat our approach can significantly out-perform state-of-the-art translation modelsand topic-based models for question re-trieval in cQA.1 IntroductionOver the past few years, a large amount of user-generated content have become an important in-formation resource on the web.
These includethe traditional Frequently Asked Questions (FAQ)archives and the emerging community questionanswering (cQA) services, such as Yahoo!
An-swers1, Live QnA2, and Baidu Zhidao3.
The con-tent in these web sites is usually organized as ques-tions and lists of answers associated with meta-data like user chosen categories to questions andaskers?
awards to the best answers.
This data made1http://answers.yahoo.com/2http://qna.live.com/3http://zhidao.baidu.com/cQA archives valuable resources for various taskslike question-answering (Jeon et al, 2005; Xue etal., 2008) and knowledge mining (Adamic et al,2008), etc.One fundamental task for reusing content incQA is finding similar questions for queried ques-tions, as questions are the keys to accessing theknowledge in cQA.
Then the best answers ofthese similar questions will be used to answer thequeried questions.
Many studies have been donealong this line (Jeon et al, 2005; Xue et al, 2008;Duan et al, 2008; Lee et al, 2008; Bernhard andGurevych, 2009; Cao et al, 2010; Zhou et al,2011; Singh, 2012; Zhang et al, 2014a).
One bigchallenge for question retrieval in cQA is the lexi-cal gap between the queried questions and the ex-isting questions in the archives.
Lexical gap meansthat the queried questions may contain words thatare different from, but related to, the words in theexisting questions.
For example shown in (Zhanget al, 2014a), we find that for a queried question?how do I get knots out of my cats fur?
?, thereare good answers under an existing question ?howcan I remove a tangle in my cat?s fur??
in Yahoo!Answers.
Although the two questions share fewwords in common, they have very similar mean-ings, it is hard for traditional retrieval models (e.g.,BM25 (Robertson et al, 1994)) to determine theirsimilarity.
This lexical gap has become a majorbarricade preventing traditional IR models (e.g.,BM25) from retrieving similar questions in cQA.To address the lexical gap problem in cQA, pre-vious work in the literature can be divided into twogroups.
The first group is the translation models,which leverage the question-answer pairs to learnthe semantically related words to improve tradi-tional IR models (Jeon et al, 2005; Xue et al,2008; Zhou et al, 2011).
The basic assumption isthat question-answer pairs are ?parallel texts?
andrelationship of words (or phrases) can be estab-lished through word-to-word (or phrase-to-phrase)250translation probabilities (Jeon et al, 2005; Xueet al, 2008; Zhou et al, 2011).
Experimentalresults show that translation models obtain state-of-the-art performance for question retrieval incQA.
However, questions and answers are far from?parallel?
in practice, questions and answers arehighly asymmetric on the information they con-tain (Zhang et al, 2014a).
The second group isthe topic-based models (Cai et al, 2011; Ji et al,2012), which learn the latent topics aligned acrossthe question-answer pairs to alleviate the lexicalgap problem, with the assumption that a questionand its paired answers share the same topic distri-bution.
However, questions and answers are het-erogeneous in many aspects, they do not share thesame topic distribution in practice.Inspired by the recent success of continuousspace word representations in capturing the se-mantic similarities in various natural languageprocessing tasks, we propose to incorporate anembedding of words in a continuous space forquestion representations.
Due to the ability ofword embeddings, we firstly transform words ina question into continuous vector representationsby looking up tables.
These word embeddings arelearned in advance using a continuous skip-grammodel (Mikolov et al, 2013), or other continuousword representation learning methods.
Once thewords are embedded in a continuous space, onecan view a question as a Bag-of-Embedded-Words(BoEW).
Then, the variable-cardinality BoEWwill be aggregated into a fixed-length vector byusing the Fisher kernel (FK) framework of (Clin-chant and Perronnin, 2013; Sanchez et al, 2013).Through the two steps, the proposed approach canmap a question into a length invariable compactvector, which can be efficiently and effectively forlarge-scale question retrieval task in cQA.We test the proposed approach on large-scaleYahoo!
Answers data and Baidu Zhidao data.
Ya-hoo!
Answers and Baidu Zhidao represent thelargest and most popular cQA archives in Englishand Chinese, respectively.
We conduct both quan-titative and qualitative evaluations.
Experimentalresults show that our approach can significantlyoutperform state-of-the-art translation models andtopic-based models for question retrieval in cQA.Our contribution in this paper are three-fold: (1)we represent a question as a bag-of-embedded-words (BoEW) in a continuous space; (2) we in-troduce a novel method to aggregate the variable-cardinality BoEW into a fixed-length vector by us-ing the FK.
The FK is just one possible way to sub-sequently transform this bag representation intoa fixed-length vector which is more amenable tolarge-scale processing; (3) an empirical verifica-tion of the efficacy of the proposed framework onlarge-scale English and Chinese cQA data.The rest of this paper is organized as follows.Section 2 summarizes the related work.
Section 3describes our proposed framework for question re-trieval.
Section 4 reports the experimental results.Finally, we conclude the paper in Section 5.2 Related Work2.1 Question Retrieval in cQASignificant research efforts have been conductedover the years in attempt to improve question re-trieval in cQA (Jeon et al, 2005; Xue et al, 2008;Lee et al, 2008; Duan et al, 2008; Bernhard andGurevych, 2009; Cao et al, 2010; Zhou et al,2011; Singh, 2012; Zhang et al, 2014a).
Mostof these works focus on finding similar questionsfor the user queried questions.
The major chal-lenge for question retrieval in cQA is the lexicalgap problem.
Jeon et al (2005) proposed a word-based translation model for automatically fixingthe lexical gap problem.
Xue et al (2008) pro-posed a word-based translation language modelfor question retrieval.
Lee et al (2008) tried tofurther improve the translation probabilities basedon question-answer pairs by selecting the most im-portant terms to build compact translation mod-els.
Bernhard and Gurevych (2009) proposed touse as a parallel training data set the definitionsand glosses provided for the same term by differ-ent lexical semantic resources.
In order to improvethe word-based translation model with some con-textual information, Riezler et al (2007) and Zhouet al (2011) proposed a phrase-based translationmodel for question and answer retrieval.
Thephrase-based translation model can capture somecontextual information in modeling the transla-tion of phrases as a whole, thus the more accuratetranslations can better improve the retrieval per-formance.
Singh (2012) addressed the lexical gapissues by extending the lexical word-based trans-lation model to incorporate semantic information(entities).In contrast to the works described above that as-sume question-answer pairs are ?parallel text?, ourpaper deals with the lexical gap by learning con-251tinuous word embeddings in capturing the simi-larities without any assumptions, which is muchmore reasonable in practice.Besides, some other studies model the semanticrelationship between questions and answers withdeep linguistic analysis (Duan et al, 2008; Wanget al, 2009; Wang et al, 2010; Ji et al, 2012;Zhang et al, 2014a) or a learning to rank strat-egy (Surdeanu et al, 2008; Carmel et al, 2014).Recently, Cao et al (2010) and Zhou et al (2013)exploited the category metadata within cQA pagesto further improve the performance.
On the con-trary, we focus on the representation learning forquestions, with a different solution with those pre-vious works.2.2 Word Embedding LearningRepresentation of words as continuous vectors hasattracted increasing attention in the area of nat-ural language processing (NLP).
Recently, a se-ries of works applied deep learning techniques tolearn high-quality word representations.
Bengioet al (2003) proposed a probabilistic neural net-work language model (NNLM) for word represen-tations.
Furthermore, Mikolov et al (2013) pro-posed efficient neural network models for learn-ing word representations, including the continu-ous skip-gram model and the continuous bag-of-word model (CBOW), both of which are unsu-pervised models learned from large-scale text cor-pora.
Besides, there are also a large number ofworks addressing the task of learning word repre-sentations (Huang et al, 2012; Maas et al, 2011;Turian et al, 2010).Nevertheless, since most the existing workslearned word representations mainly based onthe word co-occurrence information, the obtainedword embeddings cannot capture the relationshipbetween two syntactically or semantically similarwords if either of them yields very little context in-formation.
On the other hand, even though amountof context could be noisy or biased such that theycannot reflect the inherent relationship betweenwords and further mislead the training process.Most recently, Yu et al (2014) used semantic priorknowledge to improve word representations.
Xuet al (2014) used the knowledge graph to advancethe learning of word embeddings.
In contrast toall the aforementioned works, in this paper, wepresent a general method to leverage the metadataof category information within cQA pages to fur-ther improve the word embedding representations.To our knowledge, it is the first work to learn wordembeddings with metadata on cQA data set.3 Our ApproachIn this Section, we describe the proposed ap-proach: learning continuous word embedding withmetadata for question retrieval in cQA.
The pro-posed framework consists of two steps: (1) wordembedding learning step: given a cQA data collec-tion, questions are treated as the basic units.
Foreach word in a question, we firstly transform it to acontinuous word vector through the looking up ta-bles.
Once the word embeddings are learned, eachquestion is represented by a variable-cardinalityword embedding vector (also called BoEW); (2)fisher vector generation step: which uses a genera-tive model in the FK framework to generate fishervectors (FVs) by aggregating the BoEWs for allthe questions.
Question retrieval can be performedthrough calculating the similarity between the FVsof a queried question and an existing question inthe archive.From the framework, we can see that althoughthe word embedding learning computations andgenerative model estimation are time consuming,they can run only once in advance.
Meanwhile, thecomputational requirements of FV generation andsimilarity calculation are limited.
Hence, the pro-posed framework can efficiently achieve the large-scale question retrieval task.3.1 Word Embedding LearningIn this paper, we consider a context-aware pre-dicting model, more specifically, the Skip-grammodel (Mikolov et al, 2013) for learning wordembeddings, since it is much more efficient as wellas memory-saving than other approaches.4Skip-gram is recently proposed for learning word rep-resentations using a neural network model, whoseunderlying idea is that similar words should havesimilar contexts.
In the Skip-gram model (see Fig-ure 1), a sliding window is employed on the inputtext stream to generate the training data, and l in-dicates the context window size to be 2l + 1.
Ineach slide window, the model aims to use the cen-tral word wkas input to predict the context words.Let Md?Ndenote the learned embedding matrix,4Note that although we use the skip-gram model as an ex-ample to illustrate our approach, the similar framework canbe developed on the basis of any other word embedding mod-els.252???
?word embedding ofFigure 1: The continuous skip-gram model.where N is the vocabulary size and d is the di-mension of word embeddings.
Each column of Mrepresents the embedding of a word.
Letwkis firstmapped to its embedding ewkby selecting the cor-responding column vector of M .
The probabilityof its context word wk+jis then computed using alog-linear softmax function:p(wk+j|wk; ?)
=exp(eTwk+jewk)?Nw=1exp(eTwewk)(1)where ?
are the parameters we should learned, k =1 ?
?
?
d, and j ?
[?l, l].
Then, the log-likelihoodover the entire training data can be computed as:J(?)
=?
(wk,wk+j)logp(wk+j|wk; ?)
(2)To calculate the prediction errors for back prop-agation, we need to compute the derivative ofp(wk+j|wk; ?
), whose computation cost is pro-portional to the vocabulary size N .
As N is of-ten very large, it is difficult to directly computethe derivative.
To deal this problem, Mikolovet al (2013) proposed a simple negative sam-pling method, which generates r noise samplesfor each input word to estimate the target word,in which r is a very small number compared withN .
Therefore, the training time yields linear scaleto the number of noise samples and it becomesindependent of the vocabulary size.
Suppose thefrequency of word w is u(w), then the proba-bility of sampling w is usually set to p(w) ?u(w)3/4(Mikolov et al, 2013).3.2 Metadata Powered ModelAfter briefing the skip-gram model, we introducehow we equip it with the metadata information.In cQA sites, there are several metadata, such as?category?,?voting?
and so on.
In this paper, weonly consider the metadata of category informa-tion for word embedding learning.
All questionsin cQA are usually organized into a hierarchy ofcategories.
When an user asks a question, the usertypically required to choose a category label forthe question from a predefined hierarchy of cate-gories (Cao et al, 2010; Zhou et al, 2013).
Pre-vious work in the literature has demonstrated theeffectiveness of the category information for ques-tion retrieval (Cao et al, 2010; Zhou et al, 2013).On the contrary, we argue that the category infor-mation benefits the word embedding learning inthis work.
The basic idea is that category informa-tion encodes the attributes or properties of words,from which we can group similar words accordingto their categories.
Here, a word?s category is as-signed based on the questions it appeared in.
Forexample, a question ?What are the security issueswith java??
is under the category of ?Computers& Internet ?
Security?, we simply put the cate-gory of a word java as ?Computers & Internet?Security?.
Then, we may require the representa-tions of words that belong to the same category tobe close to each other.Let s(wk, wi) be the similarity score betweenwkand wi.
Under the above assumption, weuse the following heuristic to constrain the simi-lar scores:s(wk, wi) ={1 if c(wk) = c(wi)0 otherwise(3)where c(wk) denotes the category of wk.
If thecentral word wkshares the same category with theword wi, their similarity score will become 1, oth-erwise, we set to 0.
Then we encode the categoryinformation using a regularization function Ec:Ec=N?k=1N?i=1s(wk, wi)d(wk, wi) (4)where d(wk, wi) is the distance for the words inthe embedding space and s(wk, wi) serves as aweighting function.
Again, for simplicity, we de-fine d(wk, wi) as the Euclidean distance betweenwkand wi.We combine the skip-gram objective functionand the regularization function derived from themetadata of category information, we get the fol-lowing combined objective Jcthat incorporatescategory information into the word representationlearning process:Jc= J(?)
+ ?Ec(5)where ?
is the combination coefficient.
Our goalis to maximize the combined objective Jc, which253????
?Figure 2: The continuous skip-gram model withmetadata of category information, called M-NET.can be optimized using back propagation neuralnetworks.
We call this model as metadata poweredmodel (see Figure 2), and denote it by M-NET foreasy of reference.In the implementation, we optimize the regu-larization function derived from the metadata ofcategory information along with the training pro-cess of the skip-gram model.
During the pro-cedure of learning word representations from thecontext words in the sliding window, if the centralword wkhits the category information, the cor-responding optimization process of the metadatapowered regularization function will be activated.Therefore, we maximize the weighted Euclideandistance between the representation of the centralword and that of its similar words according to theobjective function in Equation (5).3.3 Fisher Vector GenerationOnce the word embeddings are learned, ques-tions can be represented by variable length setsof word embedding vectors, which can be viewedas BoEWs.
Semantic level similarities betweenqueried questions and the existing questions rep-resented by BoEWs can be captured more accu-rately than previous bag-of-words (BoW) meth-ods.
However, since BoEWs are variable-size setsof word embeddings and most of the index meth-ods in information retrieval field are not suitablefor this kinds of issues, BoEWs cannot be directlyused for large-scale question retrieval task.Given a cQA data collection Q = {qi, 1 ?
i ?|Q|}, where qiis the ith question and |Q| is thenumber of questions in the data collection.
The ithquestion qiis composed by a sequence of wordswi= {wij, 1 ?
j ?
Ni}, where Nidenotes thelength of qi.
Through looking up table (word em-bedding matrix) of M , the ith question qican berepresented by Ewi= {ewij, 1 ?
j ?
Ni}, whereewijis the word embedding of wij.
According tothe framework of FK (Clinchant and Perronnin,2013; Sanchez et al, 2013; Zhang et al, 2014b),questions are modeled by a probability densityfunction.
In this work, we use Gaussian mixturemodel (GMM) to do it.
We assume that the con-tinuous word embedding Ewifor question qihavebeen generated by a ?universal?
(e.g., question-independent) probability density function (pdf).As is a common practice, we choose this pdf to bea GMM since any continuous distribution can beapproximated with arbitrary precision by a mix-ture of Gaussian.
In what follows, the pdf is de-noted u?where ?
= {?i, ?i,?i, i = 1 ?
?
?K}is the set of parameters of the GMM.
?i, ?iand?idenote respectively the mixture weight, meanvector and covariance matrix of Gaussian i. Forcomputational reasons, we assume that the covari-ance matrices are diagonal and denote ?2ithe vari-ance vector of Gaussian i, e.g., ?2i= diag(?i).In real applications, the GMM is estimated of-fline with a set of continuous word embeddingsextracted from a representative set of questions.The parameters ?
are estimated through the op-timization of a Maximum Likelihood (ML) crite-rion using the Expectation-Maximization (EM) al-gorithm.
In the following, we follow the notationsused in (Sanchez et al, 2013).Given u?, one can characterize the question qiusing the following score function:Gqi?= 5Ni?logu?
(qi) (6)where Gqi?is a vector whose size depends only onthe number of parameters in ?.
Assuming that theword embedding ewijis iid (a simplifying assump-tion), we get:Gqi?=Ni?j=15?logu?
(ewij) (7)Following the literature (Sanchez et al, 2013),we propose to measure the similarity between twoquestions qiand qjusing the FK:K(qi, qj) = GqTi?F?1?Gqj?
(8)where F?is the Fisher Information Matrix (FIM)of u?
:F?= Eqi?u?[Gqi?GqTi?
](9)Since F?is symmetric and positive definite,F?1?can be transformed to LT?L?based on theCholesky decomposition.
Hence,KFK(qi, qj) canrewritten as follows:KFK(qi, qj) = GqTi?Gqj?
(10)254whereGqi?= L?Gqi?= L?5?logu?
(qi) (11)In (Sanchez et al, 2013), Gqi?refers to as theFisher Vector (FV) of qi.
The dot product betweenFVs can be used to calculate the semantic simi-larities.
Based on the specific probability densityfunction, GMM, FV of qiis respect to the mean ?and standard deviation ?
of all the mixed Gaussiandistributions.
Let ?j(k) be the soft assignment ofthe jth word embedding ewijin qito Guassian k(uk):?j(k) = p(k|ewij)?iuk(ewij)?Kj=1?kuk(ewij)(12)Mathematical derivations lead to:Gqi?,k=1Ni??iNi?j=1?j(k)[ewij?
?k?k](13)Gqi?,k=1Ni?2?iNi?j=1?j(k)[(ewij?
?k)2?2k?
1]The division by the vector ?kshould be under-stood as a term-by-term operation.
The final gradi-ent vector Gqi?is the concatenation of the Gqi?,kandGqi?,kvectors for k = 1 ?
?
?K.
Let d denote the di-mensionality of the continuous word embeddingsand K be the number of Gaussians.
The finalfisher vector Gqi?is therefore 2Kd-dimensional.4 ExperimentsIn this section, we present the experiments to eval-uate the performance of the proposed method forquestion retrieval.4.1 Data Set and Evaluation MetricsWe collect the data sets from Yahoo!
Answersand Baidu Zhidao.
Yahoo!
Answers and BaiduZhidao represent the largest and the most popu-lar cQA archives in English and Chinese, respec-tively.
More specifically, we utilized the resolvedquestions at Yahoo!
Answers and Baidu Zhidao.The questions include 10 million items from Ya-hoo!
Answers and 8 million items from BaiduZhidao (also called retrieval data).
Each resolvedquestion consists of three fields: ?title?, ?descrip-tion?
and ?answers?, as well as some metadata,such as ?category?.
For question retrieval, we useonly the ?title?
field and ?category?
metadata.
It#queries #candidate #relevantYahoo data 1,000 13,000 2,671Baidu data 1,000 8,000 2,104Table 1: Statistics on the manually labeled data.is assumed that the titles of questions already pro-vide enough semantic information for understand-ing users?
information needs (Duan et al, 2008).We develop two test sets, one for ?Yahoo data?,and the other for ?Baidu data?.
In order to createthe test sets, we collect some extra questions thathave been posted more recently than the retrievaldata, and randomly sample 1, 000 questions forYahoo!
Answers and Baidu Zhidao, respectively.We take those questions as queries.
All questionsare lowercased and stemmed.
Stopwords5are alsoremoved.We separately index all data from Yahoo!
An-swers and Baidu Zhidao using an open sourceLucene with the BM25 scoring function6.
Foreach query from Yahoo!
Answers and Baidu Zhi-dao, we retrieve the several candidate questionsfrom the corresponding indexed data by using theBM25 ranking algorithm in Lucene.
On average,each query from Yahoo!
Answers has 13 candi-date questions and the average number of candi-date questions for Baidu Zhidao is 8.We recruit students to label the relevance ofthe candidate questions regarding to the queries.Specifically, for each type of language, we letthree native students.
Given a candidate question,a student is asked to label it with ?relevant?
or ?ir-relevant?.
If a candidate question is consideredsemantically similar to the query, the student willlabel it as ?relevant?
; otherwise, the student willlabel it as ?irrelevant?.
As a result, each candi-date question gets three labels and the majority ofthe label is taken as the final decision for a query-candidate pair.
We randomly split each of the twolabeled data sets into a validation set and a test setwith a ration 1 : 3.
The validation set is used fortuning parameters of different models, while thetest set is used for evaluating how well the modelsranked relevant candidates in contrast to irrelevantcandidates.
Table 1 presents the manually labeleddata.Please note that rather than evaluate both re-trieval and ranking capability of different meth-5http://truereader.com/manuals/onix/stopwords1.html6We use the BM25 implementation provided by ApacheLucene (http://lucene.apache.org/), using the default parame-ter setting (k1= 1.2, b = 0.75)255ods like the existing work (Cao et al, 2010), wecompare them in a ranking task.
This may loserecall for some methods, but it can enable large-scale evaluation.In order to evaluate the performance of dif-ferent models, we employ Mean Average Preci-sion (MAP), Mean Reciprocal Rank (MRR), R-Precision (R-Prec), and Precision at K (P@5) asevaluation measures.
These measures are widelyused in the literature for question retrieval incQA (Cao et al, 2010).4.2 Parameter SettingIn our experiments, we train the word embeddingson another large-scale data set from cQA sites.
ForEnglish, we train the word embeddings on the Ya-hoo!
Webscope dataset7.
For Chinese, we train theword embeddings on a data set with 1 billion webpages from Baidu Zhidao.
These two data sets donot intersect with the above mentioned retrievaldata.
Little pre-processing is conducted for thetraining of word embeddings.
The resulting text istokenized using the Stanford tokenizer,8, and ev-ery word is converted to lowercase.
Since the pro-posed framework has no limits in using which ofthe word embedding learning methods, we onlyconsider the following two representative meth-ods: Skip-gram (baseline) and M-NET.
To train theword embedding using these two methods, we ap-ply the same setting for their common parameters.Specifically, the count of negative samples r is setto 3; the context window size l is set to 5; eachmodel is trained through 1 epoch; the learning rateis initialized as 0.025 and is set to decrease linearlyso that it approached zero at the end of training.Besides, the combination weight ?
used in M-NET also plays an important role in producinghigh quality word embedding.
Overemphasizingthe weight of the original objective of Skip-grammay result in weakened influence of metadata,while putting too large weight on metadata pow-ered objective may hurt the generality of learnedword embedding.
Based on our experience, it isa better way to decode the objective combinationweight of the Skip-gram model and metadata in-formation based on the scale of their respectivederivatives during optimization.
Finally, we set?
= 0.001 empirically.
Note that if the parameter7The Yahoo!
Webscope dataset Yahoo answers com-prehensive questions and answers version 1.0.2, available athttp://reseach.yahoo.com/Academic Relations.8http://nlp.stanford.edu/software/tokenizer.shtmlis optimized on the validation set, the final perfor-mance can be further improved.For parameter K used in FV, we do an exper-iment on the validation data set to determine thebest value among 1, 2, 4, ?
?
?
, 64 in terms of MAP.As a result, we set K = 16 in the experimentsempirically as this setting yields the best perfor-mance.4.3 Main ResultsIn this subsection, we present the experimental re-sults on the test sets of Yahoo data and Baidu data.We compare the baseline word embedding trainedby Skip-gram against this trained by M-NET.
Thedimension of word embedding is set as 50,100 and300.
Since the motivation of this paper attempts totackle the lexical gap problem for queried ques-tions and questions in the archive, we also com-pare them with the two groups of methods whichalso address the lexical gap in the literature.
Thefirst group is the translation models: word-basedtranslation model (Jeon et al, 2005), word-basedtranslation language model (Xue et al, 2008),and phrase-based translation model (Zhou et al,2011).
We implement those three translation mod-els based on the original papers and train thosemodels with (question, best answer) pairs from theYahoo!
Webscope dataset Yahoo answers and the1 billion web pages of Baidu Zhidao for Englishand Chinese, respectively.
Training the translationmodels with different pairs (e.g., question-best an-swer, question-description, question-answer) mayachieve inconsistent performance on Yahoo dataand Baidu data, but its comparison and analysisare beyond the scope of this paper.
The secondgroup is the topic-based methods: unsupervisedquestion-answer topic model (Ji et al, 2012) andsupervised question-answer topic model (Zhang etal., 2014a).
We re-implement these two topic-based models and tune the parameter settings onour data set.
Besides, we also introduce a baselinelanguage model (LM) (Zhai and Lafferty, 2001)for comparison.Table 2 shows the question retrieval perfor-mance by using different evaluation metrics.
Fromthis table, we can see that learning continu-ous word embedding representations (Skip-gram+ FV, M-NET + FV) for question retrieval canoutperform the translation-based approaches andtopic-based approaches on all evaluation metrics.We conduct a statistical test (t-test), the results256Model dimYahoo data Baidu dataMAP MRR R-Prec P@5 MAP MRR R-Prec P@5LM (baseline) - 0.435 0.472 0.381 0.305 0.392 0.413 0.325 0.247(Jeon et al, 2005) - 0.463 0.495 0.396 0.332 0.414 0.428 0.341 0.256(Xue et al, 2008) - 0.518 0.560 0.423 0.346 0.431 0.435 0.352 0.264(Zhou et al, 2011) - 0.536 0.587 0.439 0.361 0.448 0.450 0.367 0.273(Ji et al, 2012) - 0.508 0.544 0.405 0.324 0.425 0.431 0.349 0.258(Zhang et al, 2014a) - 0.527 0.572 0.433 0.350 0.443 0.446 0.358 0.265Skip-gram + FV50 0.532 0.583 0.437 0.358 0.447 0.450 0.366 0.272100 0.544 0.605?0.440 0.363 0.454 0.457 0.373 0.274300 0.550?0.619?0.444 0.365 0.460?0.464?0.374 0.277M-NET + FV50 0.548?0.612?0.441 0.363 0.459?0.462?0.374 0.276100 0.562?0.628?0.452?0.367?0.468?0.471 0.378?0.280?300 0.571?0.643?0.455?0.374?0.475?0.477?0.385?0.283?Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of theword embeddings.
The bold formate indicates the best results for question retrieval.
?
indicates thatthe difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and othermethods are mildly significant with p < 0.08 under a t-test; ?
indicates the comparisons are statisticallysignificant with p < 0.05.show that the improvements between the pro-posed M-NET + FV and the two groups of com-pared methods (translation-based approaches andtopic-based approaches) are statistically signifi-cant (p < 0.05), while the improvements be-tween Skip-gram + FV and the translation-basedapproaches are mildly significant (p < 0.08).Moreover, the metadata of category informationpowered model (M-NET + FV) outperforms thebaseline skip-gram model (Skip-gram + FV) andyields the largest improvements.
These results canimply that the metadata powered word embeddingis of higher quality than the baseline model withno metadata information regularization.
Besides,we also note that setting higher dimension bringsmore improvements for question retrieval task.Translation-based methods significantly outper-form LM, which demonstrate that matching ques-tions with the semantically related translationwords or phrases from question-answer pairs caneffectively address the word lexical gap problem.Besides, we also note that phrase-based translationmodel is more effective because it captures somecontextual information in modeling the transla-tion of phrases as a whole.
More precise transla-tion can be determined for phrases than for words.Similar observation has also been found in the pre-vious work (Zhou et al, 2011).On both data sets, topic-based models achievecomparable performance with the translation-based models and but they perform better thanLM.
The results demonstrate that learning thelatent topics aligned across the question-answerpairs can be an alternative for bridging lexical gapproblem for question retrieval.5 ConclusionThis paper proposes to learn continuous vectorrepresentations for question retrieval in cQA.
Wefirstly introduce a new metadata powered wordembedding method, called M-NET, to leverage thecategory information within cQA pages to obtainword representations.
Once the words are embed-ded in a continuous space, we treat each ques-tion as a BoEW.
Then, the variable size BoEWsare aggregated into fixed-length vectors by usingFK.
Finally, the dot product between FVs are usedto calculate the semantic similarities for questionretrieval.
Experiments on large-scale real worldcQA data demonstrate that the efficacy of the pro-posed approach.
For the future work, we willexplore how to incorporate more types of meta-data information, such as the user ratings, like sig-nals and Poll and Survey signals, into the learningprocess to obtain more powerful word representa-tions.AcknowledgmentsThis work was supported by the National Natu-ral Science Foundation of China (No.
61303180,257No.
61272332 and 61402191), the Beijing Natu-ral Science Foundation (No.
4144087), the Ma-jor Project of National Social Science Found (No.12&2D223), the Fundamental Research Funds forthe Central Universities (No.
CCNU15ZD003),and also Sponsored by CCF-Tencent Open Re-search Fund.
We thank the anonymous reviewersfor their insightful comments.ReferencesLada A. Adamic, Jun Zhang, Eytan Bakshy, andMark S. Ackerman.
2008.
Knowledge sharing andyahoo answers: Everyone knows something.
In Pro-ceedings of WWW, pages 665?674.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
J. Mach.
Learn.
Res., 3.Delphine Bernhard and Iryna Gurevych.
2009.
Com-bining lexical semantic resources with question &answer archives for translation-based answer find-ing.
In Proceedings of ACL-IJCNLP.Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.2011.
Learning the latent topics for question re-trieval in community qa.
In Proceedings of IJCNLP,pages 273?281.Xin Cao, Gao Cong, Bin Cui, and Christian S. Jensen.2010.
A generalized framework of exploring cate-gory information for question retrieval in commu-nity question answer archives.
In Proceedings ofWWW, pages 201?210.David Carmel, Avihai Mejer, Yuval Pinter, and IdanSzpektor.
2014.
Improving term weighting for com-munity question answering search using syntacticanalysis.
In Proceedings of CIKM, pages 351?360.Stephane Clinchant and Florent Perronnin.
2013.
Ag-gregating continuous word embeddings for informa-tion retrieval.
In Proceedings of the Workshop onContinuous Vector Space Models and their Compo-sitionality, pages 100?109.Huizhong Duan, Yunbo Cao, Chin yew Lin, and YongYu.
2008.
Searching questions by identifying ques-tion topic and question focus.
In Proceedings ofACL.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of ACL, pages 873?882.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of CIKM.Zongcheng Ji, Fei Xu, Bin Wang, and Ben He.
2012.Question-answer topic model for question retrievalin community question answering.
In Proceedingsof CIKM, pages 2471?2474.Jung-Tae Lee, Sang-Bum Kim, Young-In Song, andHae-Chang Rim.
2008.
Bridging lexical gaps be-tween queries and questions on large online q&a col-lections with compact translation models.
In Pro-ceedings of EMNLP, pages 410?418.Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning word vectors for sentiment analysis.In Proceedings of ACL, pages 142?150.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119.Stefan Riezler, Er Vasserman, Ioannis Tsochantaridis,Vibhu Mittal, and Yi Liu.
2007.
Statistical machinetranslation for query expansion in answer retrieval.In Proceedings of ACL.S.
Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.
1994.
Okapi at trec-3.In Proceedings of TREC, pages 109?126.Jorge Sanchez, Florent Perronnin, Thomas Mensink,and Jakob J. Verbeek.
2013.
Image classificationwith the fisher vector: Theory and practice.
Interna-tional Journal of Computer Vision, pages 222?245.A.
Singh.
2012.
Entity based q&a retrieval.
In Pro-ceedings of EMNLP, pages 1266?1277.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2008.Learning to rank answers on large online qa collec-tions.
In Proceedings of ACL, pages 719?727.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of ACL.Kai Wang, Zhaoyan Ming, and Tat-Seng Chua.
2009.A syntactic tree matching approach to finding sim-ilar questions in community-based qa services.
InProceedings of SIGIR, pages 187?194.B.
Wang, X. Wang, C. Sun, B. Liu, and L. Sun.
2010.Modeling semantic relevance for question-answerpairs in web social communities.
In ACL.Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, GangWang, Xiaoguang Liu, and Tie-Yan Liu.
2014.
Rc-net: A general framework for incorporating knowl-edge into word representations.
In Proceedings ofCIKM, pages 1219?1228.Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.
2008.Retrieval models for question and answer archives.In Proceedings of SIGIR, pages 475?482.258Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proceedingsof ACL, pages 545?550.Chengxiang Zhai and John Lafferty.
2001.
A studyof smoothing methods for language models appliedto ad hoc information retrieval.
In Proceedings ofSIGIR, pages 334?342.Kai Zhang, Wei Wu, Haocheng Wu, Zhoujun Li, andMing Zhou.
2014a.
Question retrieval with highquality answers in community question answering.In Proceedings of CIKM, pages 371?380.Qi Zhang, Jihua Kang, Jin Qian, and Xuanjing Huang.2014b.
Continuous word embeddings for detect-ing local text reuses at the semantic level.
In Pro-ceedings of the 37th International ACM SIGIR Con-ference on Research & Development in InformationRetrieval, SIGIR ?14, pages 797?806.Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.2011.
Phrase-based translation model for questionretrieval in community question answer archives.
InProceedings of ACL, pages 653?662.Guangyou Zhou, Yubo Chen, Daojian Zeng, and JunZhao.
2013.
Towards faster and better retrievalmodels for question search.
In Proceedings ofCIKM, pages 2139?2148.259
