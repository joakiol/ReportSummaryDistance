Comparison of Two Interactive Search Refinement TechniquesOlga VechtomovaDepartment of Management SciencesUniversity of Waterloo200 University Avenue West, Waterloo,Canadaovechtom@engmail.uwaterloo.caMurat KaramuftuogluDepartment of Computer EngineeringBilkent University06800 Bilkent Ankara,Turkeyhmk@cs.bilkent.edu.trAbstractThe paper presents two approaches tointeractively refining user search formulationsand their evaluation in the new HighAccuracy Retrieval from Documents (HARD)track of TREC-12.
One method consists ofasking the user to select a number ofsentences that may represent relevantdocuments, and then using the documents,whose sentences were selected for queryexpansion.
The second method consists ofshowing to the user a list of noun phrases,extracted from the initial document set, andthen expanding the query with the terms fromthe phrases selected by the user.1  IntroductionQuery expansion following relevance feedback is a wellestablished technique in information retrieval, whichaims at improving user search performance.
It combinesuser and system effort towards selecting and addingextra terms to the original query.
The traditional modelof query expansion following relevance feedback is asfollows: the user reads a representation of a retrieveddocument, typically its full-text or abstract, andprovides the system with a binary relevance judgement.After that the system extracts query expansion termsfrom the document, which are then added to the queryeither manually by the searcher ?
interactive queryexpansion, or automatically ?
automatic queryexpansion.
Intuitively interactive query expansionshould produce better results than automatic, howeverthis is not consistently so  (Beaulieu 1997, Koenemannand Belkin 1996, Ruthven 2003).In this paper we present two new approaches toautomatic and interactive query expansion, which wedeveloped and tested within the framework of the HighAccuracy Retrieval from Documents (HARD) track ofTREC (Text Retrieval Conference).1.1 HARD trackThe main goal of the new HARD track in TREC-12 isto explore what techniques could be used to improvesearch results by using two types of information:1.
Extra-linguistic contextual information about theuser and the information need, which was provided bytrack organisers in the form of metadata.
It specifies thefollowing: Genre ?
the type of documents that the searcher islooking for.
It has the following values:- Overview (general news related to the topic);- Reaction (news commentary on the topic);- I-Reaction (as above, but about non-UScommentary)- Any. Purpose of the user?s search, which has one of thefollowing values:- Background (the searcher is interested in thebackground information for the topic);- Details (the searcher is interested in the details ofthe topic);- Answer (the searcher wants to know the answerto a specific question);- Any. Familiarity of the user with the topic on a five-point scale. Granularity ?
the amount of text the user isexpecting in response to the query.
It has thefollowing values: Document, Passage, Sentence,Phrase, Any. Related text ?
sample relevant text found by theusers from any source, except the evaluationcorpus.2.
Relevance feedback given by the user in responseto topic clarification questions.
This information waselicited by each site by means of a (manually orautomatically) composed set of clarification forms pertopic.
The forms are filled in by the users (annotators),and provide additional search criteria.In more detail the HARD track evaluation scenarioconsists of the following steps:1) The track organisers invite annotators (users), eachof whom formulates one or more topics.
An example ofa typical HARD topic is given below:Title: Red Cross activitiesDescription: What has been the Red Cross'sinternational role in the last year?Narrative: Articles concerning the Red Cross's activitiesaround the globe are on topic.
Has the RC's rolechanged?
Information restricted to international reliefefforts that do not include the RC are off-topic.Purpose: DetailsGenre: OverviewGranularity: SentenceFamiliarity: 22) Participants receive Title, Description and Narrativesections of the topics, and use any information fromthem to produce one or more baseline runs.3) Participants produce zero or more clarification formswith the purpose of obtaining feedback from theannotators.
Only two forms were guaranteed to be filledout.4) All clarification forms for one topic are filled out bythe annotator, who has composed that topic.5) Participants receive the topic metadata and theannotators?
responses to clarification forms, and use anydata from them to produce one or more final runs.6) Two runs per site (baseline and final) are judged bythe annotators.
Top 75 documents, retrieved for eachtopic in each of these runs, are assigned binaryrelevance judgement by the annotator ?
author of thetopic.7) The annotators?
relevance judgements are then usedto calculate the performance metrics (see section 4).The evaluation corpus used in the HARD trackconsists of 372,219 documents, and includes threenewswire corpora (New York Times,  Associated PressWorldstream and Xinghua English) and twogovernmental corpora (The Congressional Record andFederal Register).
The overall size of the corpus is1.7Gb.The primary goal of our participation in the trackwas to investigate how to achieve high retrievalaccuracy through relevance feedback.
The secondarygoal was to study ways of reducing the amount of timeand effort the user spends on making a relevancejudgement, and at the same time assisting the user tomake a correct judgement.We evaluated the effectiveness of two differentapproaches to eliciting information from the users.
Thefirst approach is to represent each top-ranked retrieveddocument by means of one sentence containing thehighest proportion of query terms, and ask the user toselect those sentences, which possibly represent relevantdocuments.
The second method extracts noun phrasesfrom top-ranked retrieved documents and asks the userto select those, which might be useful in retrievingrelevant documents.
Both approaches aim to minimisethe amount of text the user has to read, and to focus theuser?s attention on the key information clues from thedocuments.Traditionally in bibliographical and library IRsystems the hitlist of retrieved documents is representedin the form of the titles and/or the first few sentences ofeach document.
Based on this information the user hasto make initial implicit relevance judgements: whetherto refer to the full text document or not.
Explicitrelevance feedback is typically requested by IR systemsafter the user has seen the full text document, anexample of such IR system is Okapi (Robertson et al2000, Beaulieu 1997).
Reference to full text documentsis obviously time-consuming, therefore it is important torepresent documents in the hitlist in such a form, thatwould enable the users to reliably judge their relevancewithout referring to the full text.
Arguably, the title andthe first few sentences of the document are frequentlynot sufficient to make correct relevance judgement.Query-biased summaries, usually constructed throughthe extraction of sentences that contain higherproportion of query terms than the rest of the text ?
maycontain more relevance clues than generic documentrepresentations.
Tombros and Sanderson (1998)compared query-biased summaries with the titles plusthe first few sentences of the documents by how manytimes the users have to request full-text documents toverify their relevance/non-relevance.
They discoveredthat subjects using query-biased summaries refer to thefull text of only 1.32% documents, while subjects usingtitles and first few sentences refer to 23.7% ofdocuments.
This suggests that query-biasedrepresentations are likely to contain more relevanceclues than generic document representations.The remainder of this paper is organised as follows:sections 2 and 3 present the two documentrepresentation and query expansion methods wedeveloped,  section 4 discusses their evaluation, andsection 5 concludes the paper and outlines futureresearch directions.2  Query expansion method 1According to the HARD track specifications, aclarification form for each topic must fit into a screenwith 1152 x 900 pixels resolution, and the user mayspend no more than 3 minutes filling out each form.The goal that we aim to achieve with the aid of theclarification form is to have the users judge as manyrelevant documents as possible on the basis of onesentence representation of a document.
The questionsexplored here were: What is the error rate in selectingrelevant documents on the basis of one sentencerepresentation of its content?
How does sentence-levelrelevance feedback affect retrieval performance?2.1 Sentence selectionThe sentence selection algorithm consists of thefollowing steps:We take N top-ranked documents, retrieved inresponse to query terms from the topic title.
Given thescreen space restrictions, we can only display 15 three-line sentences, hence N=15.
The full-text of each of thedocuments is then split into sentences.
For everysentence that contains one or more query terms, i.e.
anyterm from the title field of the topic, two scores arecalculated: S1 and S2.Sentence selection score 1 (S1) is the sum of idf ofall query terms present in the sentence.Sentence selection score 2 (S2):Where: Wi ?
Weight of the term i, see (3);fs ?
length normalisation factor for sentence s, see (4).The weight of each term in the sentence, exceptstopwords, is calculated as follows:Where: idfi ?
inverse document frequency of term i inthe corpus; tfi ?
frequency of term i in the document;tmax ?
tf of the term with the highest frequency in thedocument.To normalise the length of the sentence weintroduced the sentence length normalisation factor f:Where: smax ?
the length of the longest sentence in thedocument, measured as a number of terms, excludingstopwords; slen ?
the length of the current sentence.All sentences in the document were ranked by S1 asthe primary score and S2 as the secondary score.
Thus,we first select the sentences that contain more queryterms, and therefore are more likely to be related to theuser?s query, and secondarily, from this pool ofsentences select the one which is more content-bearing,i.e.
containing a higher proportion of terms with hightf*idf weights.Because we are restricted by the screen space, wereject sentences that exceed 250 characters, i.e.
threelines.
In addition, to avoid displaying very short, andhence insufficiently informative sentences, we rejectsentences with less than 6 non-stopwords.
If the top-scoring sentence does not satisfy the length criteria, thenext sentence in the ranked list is considered torepresent the document.
Also, since there are a numberof almost identical documents in the corpus, we removethe representations of the duplicate documents from theclarification form using pattern matching, and processthe necessary number of additional documents from thebaseline run sets.By selecting the sentence with the query terms andthe highest proportion of high-weighted terms in thedocument, we are showing query term instances in theirtypical context in this document.
Typically a term isonly used in one sense in the same document.
Also, inmany cases it is sufficient to establish the linguisticsense of a word by looking at its immediate neighboursin the same sentence or a clause.
Based on this, wehypothesise that users will be able to reject thosesentences, where the query terms are used in anunrelated linguistic sense.
However, we recognise that itis more difficult, if not impossible, for users to reliablydetermine the relevance of the document on the basis ofone sentence, especially in cases where the relevance ofthe document to the query is due to more subtle aspectsof the topic.2.2 Selection of query expansion termsThe user?s feedback to the clarification form is used forobtaining query expansion terms for the final run.
Forquery expansion we use collocates of query terms ?words co-occurring within a limited span with queryterms.
Vechtomova et al (2003) have demonstrated thatexpansion with long-span collocates of query termsobtained from 5 known relevant documents showed 72-74% improvement over the use of Title-only queryterms on the Financial Times (TREC volume 4) corpuswith TREC-5 ad hoc topics.We extract collocates from windows surroundingquery term occurrences.
The span of the window is= qidfS1 (1)sifWS =2 (2)))max*5.0(5.0(ttfidfW iii += (3)ssslensf max= (4)measured as the number of sentences to the left andright of the sentence containing the instance of thequery term.
For example, span 0 means that only termsfrom the same sentence as the query term are consideredas collocates, span 1 means that terms from 1 precedingand 1 following sentences are also considered ascollocates.In more detail the collocate extraction and rankingalgorithm is as follows: For each query term we extractall sentences containing its instance, plus s sentences tothe left and right of these sentences, where s is the spansize.
Each sentence is only extracted once.
After allrequired sentences are selected we extract stems fromthem, discarding stopwords.
For each unique stem wecalculate the Z score to measure the significance of itsco-occurrence with the query term as follows:Where: fr(x,y) ?
frequency of x and y occurring in thesame windows in the known relevant document set (see(6); fc(y) ?
frequency of y in the corpus; fr(x) ?frequency of x in the relevant documents; vx(R) ?average size of windows around x in the known relevantdocument set (R); N ?
the total number of non-stopwordoccurrences in the corpus.The frequency of x and y occurring in the samewindows in the relevant set ?
fr(x,y) ?
is calculated asfollows:Where: m ?
number of windows in the relevant set (R);fw(x) ?
frequency of x in the window w; fw(y) ?frequency of y in the window w.All collocates with an insignificant degree ofassociation: Z<1.65 are discarded, see (Church et al1991).
The remaining collocates are sorted by their Zscore.
The above Z score formula is described in moredetail in (Vechtomova et al 2003).After we obtain sorted lists of collocates of eachquery term, we select those collocates for queryexpansion, which co-occur significantly with two ormore query terms.
For each collocate the collocate score(C1) is calculated:Where: ni ?
rank of the collocate in the Z-sortedcollocation list for the query term i;Wi ?
weight of the query term i.The reason why we use the rank of the collocate inthe above formula instead of its Z score is because Zscores of collocates of different terms are notcomparable.Finally, collocates are ranked by two parameters:the primary parameter is the number of query terms theyco-occur with, and the secondary ?
C1 score.We tested the algorithm on past TREC data(Financial Times and Los Angeles Times newswirecorpora, topics 301-450) with blind feedback usingOkapi BM25 search function (Sparck Jones et al 2000).The goal was to determine the optimal values for R - thesize of the pseudo-relevant set, s ?
the span size, and k ?the number of query expansion terms.
The resultsindicate that variations of these parameters have aninsignificant effect on precision.
However, sometendencies were observed, namely: (1) larger R valuestend to lead to poorer performance in both Title-onlyand Title+Desc.
runs; (2) larger span sizes also tend todegrade performance in both Title and Title+Desc runs.Title-only unexpanded run was 10% better thanTitle+Description.
Expansion of Title+Desc.
queriesresulted in relatively poorer performance than expansionof Title-only queries.
For example, AveP of the worstTitle+Desc expansion run (R=50, s=4, k=40) is 23%worse than the baseline, and AveP of the best run (R=5,s=1, k=10) is 8% better than the baseline.
AveP of theworst Title-only run (R=50, s=5, k=20) is 4.5% worsethan the baseline, and AveP of the best Title-only run(R=5, s=1, k=40) is 10.9% better than the baseline.Based on this data we decided to use Title-onlyterms for the official TREC run ?UWAThard2?, and,given that values k=40 and s=1 contributed to asomewhat better performance, we used these values inall of our official expansion runs.
The question of Rvalue is obviously irrelevant here, as we used alldocuments selected by users in the clarification form.We used Okapi BM25 document retrieval functionfor topics with granularity Document, and Okapipassage retrieval function BM250 (Sparck Jones et al2000) for topics with other granularity values.
Fortopics with granularity Sentence the best sentences wereselected from the passages, returned by BM250, usingthe algorithm described in section 2.1 above.3  Query expansion method 2The second user feedback mechanism that we evaluatedconsists of automatically selecting noun phrases fromthe top-ranked documents retrieved in the baseline run,and asking the users to select all phrases that containpossibly useful query expansion terms.The research question explored here is whethernoun phrases provide sufficient context for the user toselect potentially useful terms for query expansion.
)()()()()()(),(RvxfNyfRvxfNyfyxfZxrcxrcr ?= (5)= ii WnC1 (7)1( , ) ( ) ( )mr w wwf x y f x f y== (6)We take top 25 documents from the baseline run,and select 2 sentences per document using the algorithmdescribed above.
We have not experimented withalternative values for these two parameters.
We thenapply Brill?s rule-based tagger (Brill 1995) and BaseNPnoun phrase chunker (Ramshaw and Marcus 1995) toextract noun phrases from these sentences.
The phrasesare then parsed in Okapi to obtain their term weights,removing all stopwords and phrases consisting entirelyof the original query terms.
The remaining phrases areranked by the sum of weights of their constituent terms.Top 78 phrases are then included in the clarificationform for the user to select.
This is the maximum numberof phrases that could fit into the clarification form.All user-selected phrases were split into singleterms, which were then used to expand the original userquery.
The expanded query was then searched againstthe HARD track database in the same way as in thequery expansion method 1 described in the previoussection.4   EvaluationEvery run submitted to the HARD track was evaluatedin three different ways.
The first two evaluations aredone at the document level only, whereas the last onetakes into account the granularity metadata.1.
SOFT-DOC ?
document-level evaluation, whereonly the traditional TREC topic formulations (title,description, narrative) are used as relevance criteria.2.
HARD-DOC ?
the same as the above, plus?purpose?, ?genre?
and ?familiarity?
metadata areused as additional relevance criteria.3.
HARD-PSG ?
passage-level evaluation, which inaddition to all criteria in HARD-DOC also requiresthat retrieved items satisfy the granularity metadata(Allan 2004).Document-level evaluation was done by thetraditional IR metrics of mean average precision andprecision at various document cutoff points.
In thispaper we focus on document-level evaluation.
Passage-level evaluation is discussed elsewhere (Vechtomova  etal.
2004).4.1  Document-level evaluationFor all of our runs we used Okapi BSS (Basic SearchSystem).
For the baseline run we used keywords fromthe title field only, as these proved to be most effectivein our preliminary experiments described in section 2.2.Topic titles were parsed in Okapi, weighted andsearched using BM25 function against the HARD trackcorpus.Document-level results of the three submitted runsare given in table 1.
UWAThard1 is the baseline runusing original query terms from the topic titles.UWAThard2 is a final run using query expansionmethod 1, outlined earlier, plus the granularity andknown relevant documents metadata.
UWAThard3 is afinal run using query expansion method 2 plus theRun Run description SOFT-DOC evaluation HARD-DOC evaluationPrecision@ 10AveragePrecisionPrecision@ 10AveragePrecisionUWAThard1* Original title-only query terms;BM25 used for all topics 0.4875 0.3134 0.3875 0.2638UWAThard2*Query expansion method 1;granularity and related textmetadata0.5479 0.3150 0.4354 0.2978UWAThard3* Query expansion method 2;granularity metadata 0.5958 0.3719 0.4854 0.3335UWAThard4As UWAThard1, but BM250is used for topics requiringpassages0.4729 0.2937 0.3667 0.2450UWAThard5 As UWAThard2, but relatedtext metadata is not used 0.5229 0.3016 0.4062 0.2828Table 1.
Document-level evaluation results (* runs submitted to TREC)granularity metadata.The fact that the query expansion method 1(UWAThard2) produced no improvement over the baseline(UWAThard1) was a surprise, and did not correspond to ourtraining runs with the Financial Times and Los AngelesTimes collections, which showed 21% improvement overthe original title-only query run.
We evaluated the userselection of the sentence using average precision, calculatedas the number of relevant sentences selected by the user outof the total number of sentences selected, and average recall?
the number of relevant sentences selected by the user outof the total number of relevant sentences shown in theclarification form.
Average precision of TREC sentenceselections made by TREC annotators is 0.73, recall ?
0.69,what is slightly better than our selections during trainingruns (precision: 0.70, recall: 0.64).
On average 7.14 relevantsentences were included in the forms.
The annotators onaverage selected 4.9 relevant and 1.8 non-relevantsentences.Figure 1 shows the number of relevant/non-relevantselected sentences by topic.
It is not clear why queryexpansion method 1 performed worse in the officialUWAThard2 run compared to the training run, given verysimilar numbers of relevant sentences selected.
Corpusdifferences could be one reason for that ?
HARD corpuscontains a large proportion of governmental documents, andwe have only evaluated our algorithm on newswire corpora.More experiments need to be done to determine the effectof the governmental documents on our query expansionalgorithm.In addition to clarification forms, we used the ?relatedtext?
metadata for UWAThard2, from which we extractedquery expansion terms using the method described insection 2.2.
To determine the effect of this metadata onperformance, we conducted a run without it (UWAThard5),which showed only a slight drop in performance.
Thissuggests that additional relevant documents from othersources do not affect performance of this query expansionmethod significantly.We thought that one possible reason for the poorperformance of UWAThard2 compared to the baseline runUWAThard1 was the fact that we used document retrievalsearch function BM25 for all topics in the UWAThard1,whereas for UWAThard2 we used BM25 for topicsrequiring document retrieval and BM250 for the topicsrequiring passage retrieval.
The two functions producesomewhat different document rankings.
In UWAThard4 weused BM250 for the topics requiring passages, and got onlya slightly lower average precision of 0.2937 (SOFT-DOCevaluation) and 0.2450 (HARD-DOC evaluation).Our second query expansion method on the contrarydid not perform very well in the training runs, achievingonly 10% improvement over the original title-only queryrun.
The official run UWAThard3, however resulted in 18%increase in average precision (SOFT-DOC evaluation) and26.4% increase in average precision (HARD-DOCevaluation).
Both improvements are statistically significant(using t-test at .05 significance level).TREC annotators selected on average 19 phrases,whereas we selected on average 7 phrases in our tests.
Thissuggests that selecting more phrases leads to a notablybetter performance.
The reason why we selected fewerphrases than the TREC annotators could be due to the factthat on many occasions we were not sufficiently familiarwith the topic, and could not determine how an out-of-context phrase is related or not related to the topic.
TRECannotators are, presumably, more familiar with the topicsthey have formulated.In total 88 runs were submitted by participants to theHARD track.
All our submitted runs are above the medianin all evaluation measures shown in table 1.
The onlyparticipating site, whose expansion runs performed betterthan our UWAThard3 run, was the Queen?s college group(Kwok et al 2004).
Their best baseline system achieved32.7% AveP (HARD-DOC) and their best result afterclarification forms was 36%, which gives 10% increaseover the baseline.
We have achieved 26% improvementover the baseline (HARD-DOC), which is the highestincrease over baseline among the top 50% highest-scoringbaseline runs.Figure 1.
Sentences selected by TREC annotators from the clarification form 1.4.2 The effect of different numbers of relevant andnon-relevant documents on performance followinguser feedbackQuery expansion based on relevance feedback is typicallymore effective than based on blind feedback, however asdiscussed in the previous section, only 73% of the sentencesselected by users from the clarification form 1 were actuallyrelevant.
This has prompted us to explore the followingquestion: How does the presence of different numbers ofrelevant and non-relevant documents in the feedback affectaverage precision?With this goal, we conducted a series of runs onFinancial Times and Los Angeles Times corpora and TRECtopics 301-450.
For each run we composed a set, consistingof the required number of relevant and non-relevantdocuments.
To minimize the difference between relevantand non-relevant documents we selected non-relevantdocuments ranked closely to relevant documents in theranked document set.The process of document selection is as follows: firstall documents in the ranked set are marked as relevant/non-relevant using TREC relevance judgements.
Then, eachtime a relevant document is found, it is recorded togetherwith the nearest non-relevant document, until the necessaryFigure 2: Effect of relevant and non-relevant documents on query expansion from user feedbacknumber of relevant/non-relevant documents is reached.The graph in figure 2 shows that as the number ofrelevant documents increases, average precision (AveP)after feedback increases considerably for each extra relevantdocument used, up to the point when we have 4 relevantdocuments.
The increment in AveP slows down when morerelevant documents are added.Adding few non-relevant documents to relevant onescauses a considerable drop in the AveP.
However, theprecision does not deteriorate further when more non-relevant documents are added (Figure 2).
As long as thereare more than three relevant documents that are used, aplateau is hit at around 4-5 non-relevant documents.We can conclude from this experiment that as a generalrule, the more relevant documents are used for queryexpansion, the better is the average precision.
Even thoughuse of 5 or more relevant documents does not increase theprecision considerably, it still does cause an improvementcompared to 4 and fewer relevant documents.Another finding is that non-relevant documents do notaffect average precision considerably, as long as there are asufficient number of relevant documents.5  Conclusions and future workIn this paper we presented two user-assisted searchrefinement techniques:(1) inviting the user to select from the clarification form anumber of sentences that may represent relevant documents,and then using the documents whose sentences wereselected for query expansion.
(2) showing to the user a list of noun phrases, extracted00.10.20.30.40.50.61 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21Non-relevant documentsAverageprecision1 rel.
document 2 rel.
documents 3 rel.
documents 4 rel.
documents5 rel.
documents 6 rel.
documents 7 rel.
documents 15 rel.
documentsfrom the initial document set, and then expanding the querywith the terms from the user-selected phrases.The evaluation results suggest that the secondexpansion method overall is more promising than the first,demonstrating statistically significant performanceimprovement over the baseline run.
More analysis needs tobe done to determine the key factors influencing theperformance of both methods.The focus of our experiments in the HARD track ofTREC-12 was on developing effective methods of gatheringand utilising the user?s relevance feedback.
Another majorgoal of the HARD track, which we did not address thistime, is to promote research into how contextual and extra-linguistic information about the user and the user?s searchtask could be harnessed to achieve high accuracy retrieval.To effectively use information such as user?s familiaritywith the topic, the purpose of the user?s search or the user?sgenre preferences we need more complex linguistic andstylistic analysis techniques.
We plan to address these issuesin the next year?s entry.AcknowledgementsThis material is based on work supported in part by NaturalSciences and Engineering Research Council of Canada.ReferencesAllan, J.
2004.
HARD Track Overview.
Proceedings of theTwelfth Text Retrieval Conference, November 18-21, 2003,Gaithersburg, MD.Beaulieu, M. 1997.
Experiments with interfaces to supportQuery Expansion.
Journal of Documentation, 53(1), pp.
8-19Brill E. 1995.
Transformation-based error-driven learningand natural language processing: a case study in part ofspeech tagging.
Computational Linguistics, 21(4), pp.
543-565.Church K., Gale W., Hanks P., Hindle D. 1991.
Usingstatistics in lexical analysis.
In Lexical Acquisition: UsingOn-line Resources to Build a Lexicon, ed.
U. Zernik,Englewood Cliffs, NJ: Lawrence Elbraum Associates, pp.115-164.Koenemann J. and Belkin N. J.
1996.
A case forinteraction: a study of interactive information retrievalbehavior and effectiveness.
Proceedings of the HumanFactors in Computing Systems Conference, Zurich, pp.
205-215.Kwok L. et al 2004.
TREC2003 Robust, HARD and QAtrack experiments using PIRCS.
Proceedings of the TwelfthText Retrieval Conference, November 18-21, 2003,Gaithersburg, MD.Ramshaw L. and Marcus M. 1995.
Text Chunking UsingTransformation-Based Learning.
Proceedings of the ThirdACL Workshop on Very Large Corpora, MIT.Robertson S.E., Walker S. and Beaulieu M. 2000.Experimentation as a way of life: Okapi at TREC.Information Processing and Management, 36, pp.
95-108.Ruthven I.
2003.
Re-examining the potential effectivenessof interactive query expansion.
Proceedings of the 26thACM-SIGIR conference, Toronto, Canada, pp.
213-220.Sparck Jones K., Walker S. and Robertson S.E.
2000.
Aprobabilistic model of information retrieval: developmentand comparative experiments.
Information Processing andManagement, 36(6), pp.
779-808 (Part 1); pp.
809-840 (Part2).Tombros A., Sanderson M.  1998.
Advantages of QueryBiased Summaries in Information Retrieval.
Proceedings ofthe 21st ACM SIGIR conference, Melbourne, Australia, pp.2-10.Vechtomova O., Karamuftuoglu M., Lam E. 2004.Interactive Search Refinement Techniques for HARDTasks.
Proceedings of the Twelfth Text RetrievalConference, November 18-21, 2003, Gaithersburg, MD.Vechtomova O., Robertson S.E., Jones S. 2003.
Queryexpansion with long-span collocates.
Information Retrieval,6(2), pp.
251-273.
