Proceedings of the 12th Conference of the European Chapter of the ACL, pages 166?174,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsAn Alignment Algorithm using Belief Propagation and a Structure-BasedDistortion ModelFabien Cromie`resGraduate school of informaticsKyoto UniversityKyoto, Japanfabien@nlp.kuee.kyoto-u.ac.jpSadao KurohashiGraduate school of informaticsKyoto UniversityKyoto, Japankuro@i.kyoto-u.ac.jpAbstractIn this paper, we first demonstrate the in-terest of the Loopy Belief Propagation al-gorithm to train and use a simple align-ment model where the expected marginalvalues needed for an efficient EM-trainingare not easily computable.
We then im-prove this model with a distortion modelbased on structure conservation.1 Introduction and Related WorkAutomatic word alignment of parallel corpora isan important step for data-oriented Machine trans-lation (whether Statistical or Example-Based) aswell as for automatic lexicon acquisition.
Manyalgorithms have been proposed in the last twentyyears to tackle this problem.
One of the most suc-cessfull alignment procedure so far seems to bethe so-called ?IBM model 4?
described in (Brownet al, 1993).
It involves a very complex distor-tion model (here and in subsequent usages ?dis-tortion?
will be a generic term for the reorderingof the words occurring in the translation process)with many parameters that make it very complexto train.By contrast, the first alignment model we aregoing to propose is fairly simple.
But this sim-plicity will allow us to try and experiment differ-ent ideas for making a better use of the sentencestructures in the alignment process.
This model(and even more so its subsequents variations), al-though simple, do not have a computationally ef-ficient procedure for an exact EM-based training.However, we will give some theoretical and empir-ical evidences that Loopy Belief Propagation cangive us a good approximation procedure.Although we do not have the space to review themany alignment systems that have already beenproposed, we will shortly refer to works that sharesome similarities with our approach.
In particu-lar, the first alignment model we will present hasalready been described in (Melamed, 2000).
Wediffer however in the training and decoding pro-cedure we propose.
The problem of making useof syntactic trees for alignment (and translation),which is the object of our second alignment modelhas already received some attention, notably by(Yamada and Knight, 2001) and (Gildea, 2003) .2 Factor Graphs and Belief PropagationIn this paper, we will make several use of Fac-tor Graphs.
A Factor Graph is a graphicalmodel, much like a Bayesian Network.
The threemost common types of graphical models (FactorGraphs, Bayesian Network and Markov Network)share the same purpose: intuitively, they allow torepresent the dependencies among random vari-ables; mathematically, they represent a factoriza-tion of the joint probability of these variables.Formally, a factor graph is a bipartite graph with2 kinds of nodes.
On one side, the Variable Nodes(abbreviated as V-Node from here on), and on theother side, the Factor Nodes (abbreviated as F-Node).
If a Factor Graph represents a given jointdistribution, there will be one V-Node for everyrandom variable in this joint distribution.
Each F-Node is associated with a function of the V-Nodesto which it is connected (more precisely, a func-tion of the values of the random variables associ-ated with the V-Nodes, but for brevity, we will fre-quently mix the notions of V-Node, Random Vari-ables and their values).
The joint distribution isthen the product of these functions (and of a nor-malizing constant).
Therefore, each F-Node actu-ally represent a factor in the factorization of thejoint distribution.As a short example, let us consider a prob-lem classically used to introduce Bayesian Net-work.
We want to model the joint probability ofthe Weather(W) being sunny or rainy, the Sprin-kle(S) being on or off, and the Lawn(L) beingwet or dry.
Figure 1 show the dependencies of166Figure 1: A classical examplethe variables represented with a Factor Graph andwith a Bayesian Network.
Mathematically, theBayesian Network imply that the joint probabil-ity has the following factorization: P (W,L, S) =P (W ) ?
P (S|W ) ?
P (L|W,S).
The Factor Graphimply there exist two functions ?1 and ?2 as wellas a normalization constant C such that we havethe factorization: P (W,L, S) = C ?
?2(W,S) ??1(L,W,S).
If we set C = 1, ?2(W,S) =P (W ) ?
P (S|W ) and ?1(L,W,S) = P (L|W,S),the Factor Graph express exactly the same factor-ization as the Bayesian Network.A reason to use Graphical Models is that we canuse with them an algorithm called Belief Propa-gation (abbreviated as BP from here on) (Pearl,1988).
The BP algorithm comes in two flavors:sum-product BP and max-product BP.
Each onerespectively solve two problems that arise often(and are often intractable) in the use of a proba-bilistic model: ?what are the marginal probabili-ties of each individual variable??
and ?what is theset of values with the highest probability??.
Moreprecisely, the BP algorithm will give the correctanswer to these questions if the graph represent-ing the distribution is a forest.
If it is not the case,the BP algorithm is not even guaranteed to con-verge.
It has been shown, however, that the BP al-gorithm do converge in many practical cases, andthat the results it produces are often surprisinglygood approximations (see, for example, (Murphyet al, 1999) or (Weiss and Freeman, 2001) ).
(Yedidia et al, 2003) gives a very good presen-tation of the sum-product BP algorithm, as well assome theoretical justifications for its success.
Wewill just give an outline of the algorithm.
The BPalgorithm is a message-passing algorithm.
Mes-sages are sent during several iterations until con-vergence.
At each iteration, each V-Node sendsto its neighboring F-Nodes a message represent-ing an estimation of its own marginal values.
Themessage sent by the V-Node Vi to the F-Node Fjestimating the marginal probability of Vi to takethe value x is :mV i?Fj(x) =?Fk?N(V i)\FjmFk?V i(x)(N(Vi) represent the set of the neighbours of Vi)Also, every F-Node send a message to its neigh-boring V-Nodes that represent its estimates of themarginal values of the V-Node:mFj?V i(x) =?v1,...,vn?j(v1, .., x, .., vn)??
?V k?N(Fj)\V imV k?Fj(vk)At any point, the belief of a V-Node V i is givenbybi(x) =?Fk?N(V i)mFk?V i(x), bi being normalized so that?x bi(x) = 1.
Thebelief bi(x) is expected to converge to the marginalprobability (or an approximation of it) of Vi takingthe value x .An interesting point to note is that each messagecan be ?scaled?
(that is, multiplied by a constant)by any factor at any point without changing the re-sult of the algorithm.
This is very useful both forpreventing overflow and underflow during compu-tation, and also sometimes for simplifying the al-gorithm (we will use this in section 3.2).
Also,damping schemes such as the ones proposed in(Murphy et al, 1999) or (Heskes, 2003) are use-ful for decreasing the cases of non-convergence.As for the max-product BP, it is best explainedas ?sum-product BP where each sum is replacedby a maximization?.3 The monolink modelWe are now going to present a simple alignmentmodel that will serve both to illustrate the effi-ciency of the BP algorithm and as basis for fur-ther improvement.
As previously mentioned, thismodel is mostly identical to one already proposedin (Melamed, 2000).
The training and decodingprocedures we propose are however different.3.1 DescriptionFollowing the usual convention, we will designatethe two sides of a sentence pair as French and En-glish.
A sentence pair will be noted (e, f).
ei rep-resents the word at position i in e.167In this first simple model, we will pay little at-tention to the structure of the sentence pair wewant to align.
Actually, each sentence will be re-duced to a bag of words.Intuitively, the two sides of a sentence pair ex-press the same set of meanings.
What we want todo in the alignment process is find the parts of thesentences that originate from the same meaning.We will suppose here that each meaning generateat most one word on each side, and we will nameconcept the pair of words generated by a mean-ing.
It is possible for a meaning to be expressedin only one side of the sentence pair.
In that case,we will have a ?one-sided?
concept consisting ofonly one word.
In this view, a sentence pair ap-pears ?superficially?
as a pair of bag of words, butthe bag of words are themselves the visible part ofan underlying bag of concepts.We propose a simple generative model to de-scribe the generation of a sentence pair (or rather,its underlying bag of concepts):?
First, an integer n, representing the numberof concepts of the sentence is drawn from adistribution Psize?
Then, n concepts are drawn independentlyfrom a distribution PconceptThe probability of a bag of concepts C is then:P (C) = Psize(|C|)?
(w1,w2)?CPconcept((w1, w2))We can alternatively represent a bag of conceptsas a pair of sentence (e, f), plus an alignment a.a is a set of links, a link being represented as apair of positions in each side of the sentence pair(the special position -1 indicating the empty sideof a one-sided concept).
This alternative represen-tation has the advantage of better separating whatis observed (the sentence pair) and what is hidden(the alignment).
It is not a strictly equivalent rep-resentation (it also contains information about theword positions) but this will not be relevant here.The joint distribution of e,f and a is then:P (e, f, a) = Psize(|a|)?
(i,j)?aPconcept(ei, fj)(1)This model only take into consideration one-to-one alignments.
Therefore, from now on, wewill call this model ?monolink?.
Consideringonly one-to-one alignments can be seen as a lim-itation compared to others models that can of-ten produce at least one-to-many alignments, buton the good side, this allow the monolink modelto be nicely symmetric.
Additionally, as alreadyargued in (Melamed, 2000), there are ways todetermine the boundaries of some multi-wordsphrases (Melamed, 2002), allowing to treat sev-eral words as a single token.
Alternatively, a pro-cedure similar to the one described in (Cromieres,2006), where substrings instead of single wordsare aligned (thus considering every segmentationpossible) could be used.With the monolink model, we want to do twothings: first, we want to find out good values forthe distributions Psize and Pconcept.
Then we wantto be able to find the most likely alignment a giventhe sentence pair (e, f).We will consider Psize to be a uniform distribu-tion over the integers up to a sufficiently big value(since it is not possible to have a uniform distri-bution over an infinite discrete set).
We will notneed to determine the exact value of Psize .
Theassumption that it is uniform is actually enough to?remove?
it of the computations that follow.In order to determine the Pconcept distribution,we can use an EM procedure.
It is easy toshow that, at every iteration, the EM procedurewill require to set Pconcept(we, wf ) proportionalto the sum of the expected counts of the concept(we, wf ) over the training corpus.
This, in turn,mean we have to compute the conditional expec-tation:E((i, j) ?
a|e, f) =?a|(i,j)?aP (a|e, f)for every sentence pair (e, f).
This computationrequire a sum over all the possible alignments,whose numbers grow exponentially with the sizeof the sentences.
As noted in (Melamed, 2000),it does not seem possible to compute this expecta-tion efficiently with dynamic programming trickslike the one used in the IBM models 1 and 2 (as apassing remark, these ?tricks?
can actually be seenas instances of the BP algorithm).We propose to solve this problem by applyingthe BP algorithm to a Factor Graph representingthe conditional distribution P (a|e, f).
Given asentence pair (e, f), we build this graph as fol-lows.We create a V-node V ei for every position i inthe English sentence.
This V-Node can take for168Figure 2: A Factor Graph for the monolink modelin the case of a 2-words English sentence and a 3-words french sentence (F recij nodes are noted Fri-j)value any position in the french sentence, or thespecial position ?1 (meaning this position is notaligned, corresponding to a one-sided concept).We create symmetrically a V-node V fj for everyposition in the french sentence.We have to enforce a ?reciprocal love?
condi-tion: if a V-Node at position i choose a position jon the opposite side, the opposite V-Node at po-sition j must choose the position i.
This is doneby adding a F-Node F reci,j between every oppositenode V ei and Vfj , associated with the function:?reci,j (k, l) =????
?1 if (i = l and j = k)or (i 6= l and j 6= k)0 elseWe then connect a ?translation probability?
F-Node F tp.ei to every V-Node Vei associated withthe function:?tp.ei (j) ={?Pconcept(ei, fj) if j 6= ?1Pconcept(ei, ?)
if j = ?1We add symmetrically on the French side F-NodesF tp.fj to the V-Nodes Vfj .It should be fairly easy to see that such a FactorGraph represents P (a|e, f).
See figure 2 for anexample.Using the sum-product BP, the beliefs of ev-ery V-Node V ei to take the value j and of everynode V fj to take the value i should converge to themarginal expectation E((i, j) ?
a|e, f) (or rather,a hopefully good approximation of it).We can also use max-product BP on the samegraph to decode the most likely alignment.
In themonolink case, decoding is actually an instance ofthe ?assignment problem?, for which efficient al-gorithms are known.
However this will not be thecase for the more complex model of the next sec-tion.
Actually, (Bayati et al, 2005) has recentlyproved that max-product BP always give the opti-mal solution to the assignment problem.3.2 Efficient BP iterationsApplying naively the BP algorithm would lead usto a complexity of O(|e|2 ?
|f |2) per BP iteration.While this is not intractable, it could turn out to bea bit slow.
Fortunately, we found it is possible toreduce this complexity to O(|e| ?
|f |) by makingtwo useful observations.Let us note meij the resulting message from Veito V fj (that is the message sent by Freci,j to Vfj af-ter it received its own message from V ei ).
meij(x)has the same value for every x different from i:meij(x 6= i) =?k 6=jbei (k)mfji(k).
We can divide all themessages meij by meij(x 6= i), so that meij(x) = 1except if x = i; and the same can be done for themessages coming from the French side mfij .
It fol-lows that meij(x 6= i) =?k 6=j bei (k) = 1 ?
bei (j)if the bei are kept normalized.
Therefore, at ev-ery step, we only need to compute meij(j), notmeij(x 6= j).Hence the following algorithm (meij(j) will behere abbreviated to meij since it is the only valueof the message we need to compute).
We describethe process for computing the English-side mes-sages and beliefs (meij and bei ) , but the processmust also be done symmetrically for the French-side messages and beliefs (mfij and bfi ) at everyiteration.0- Initialize all messages and beliefs with:me(0)ij = 1 and be(0)i (j) = ?tp.ei (j)Until convergence (or for a set number of itera-tion):1- Compute the messages meij : me(t+1)ij =be(t)i (j)/((1 ?
be(t)i (j)) ?
mf(t)ji )2- Compute the beliefs bei (j):bi(j)e(t+1) =?tp.ei (j) ?
mf(t+1)ji3- And then normalize the bi(j)e(t+1) so that?j bi(j)e(t+1) = 1.A similar algorithm can be found for the max-product BP.3.3 Experimental ResultsWe evaluated the monolink algorithm with twolanguages pairs: French-English and Japanese-English.169For the English-French Pair, we used 200,000sentence pairs extracted from the Hansard cor-pus (Germann, 2001).
Evaluation was done withthe scripts and gold standard provided duringthe workshop HLT-NAACL 20031 (Mihalcea andPedersen, 2003).
Null links are not considered forthe evaluation.For the English-Japanese evaluation, we used100,000 sentence pairs extracted from a corpus ofEnglish/Japanese news.
We used 1000 sentencepairs extracted from pre-aligned data(Utiyama andIsahara, 2003) as a gold standard.
We segmentedall the Japanese data with the automatic segmenterJuman (Kurohashi and Nagao, 1994).
There isa caveat to this evaluation, though.
The reasonis that the segmentation and alignment schemeused in our gold standard is not very fine-grained:mostly, big chunks of the Japanese sentence cover-ing several words are aligned to big chunks of theEnglish sentence.
For the evaluation, we had toconsider that when two chunks are aligned, thereis a link between every pair of words belonging toeach chunk.
A consequence is that our gold stan-dard will contain a lot more links than it should,some of them not relevants.
This means that therecall will be largely underestimated and the pre-cision will be overestimated.For the BP/EM training, we used 10 BP iter-ations for each sentences, and 5 global EM iter-ations.
By using a damping scheme for the BPalgorithm, we never observed a problem of non-convergence (such problems do commonly ap-pears without damping).
With our python/C im-plementation, training time approximated 1 hour.But with a better implementation, it should be pos-sible to reduce this time to something comparableto the model 1 training time with Giza++.For the decoding, although the max-product BPshould be the algorithm of choice, we found wecould obtain slightly better results (by between 1and 2 AER points) by using the sum-product BP,choosing links with high beliefs, and cutting-offlinks with very small beliefs (the cut-off was cho-sen roughly by manually looking at a few alignedsentences not used in the evaluation, so as not tocreate too much bias).Due to space constraints, all of the results of thissection and the next one are summarized in twotables (tables 1 and 2) at the end of this paper.In order to compare the efficiency of the BP1http://www.cs.unt.edu/ rada/wpt/training procedure to a more simple one, we reim-plemented the Competitive Link Algorithm (ab-breviated as CLA from here on) that is used in(Melamed, 2000) to train an identical model.
Thisalgorithm starts with some relatively good esti-mates found by computing correlation score (weused the G-test score) between words based ontheir number of co-occurrences.
A greedy Viterbitraining is then applied to improve this initialguess.
In contrast, our BP/EM training do not needto compute correlation scores and start the trainingwith uniform parameters.We only evaluated the CLA on theFrench/English pair.
The first iteration ofCLA did improve alignment quality, but subse-quent ones decreased it.
The reported score forCLA is therefore the one obtained during the bestiteration.
The BP/EM training demonstrate a clearsuperiority over the CLA here, since it producealmost 7 points of AER improvement over CLA.In order to have a comparison with a well-known and state-of-the-art system, we also usedthe GIZA++ program (Och and Ney, 1999) toalign the same data.
We tried alignments in bothdirection and provide the results for the directionthat gave the best results.
The settings used werethe ones used by the training scripts of the Mosessystem2, which we assumed to be fairly optimal.We tried alignment with the default Moses settings(5 iterations of model 1, 5 of Hmm, 3 of model 3,3 of model 4) and also tried with increased numberof iterations for each model (up to 10 per model).We are aware that the score we obtained formodel 4 in English-French is slightly worse thanwhat is usually reported for a similar size of train-ing data.
At the time of this paper, we did nothave the time to investigate if it is a problem ofnon-optimal settings in GIZA++, or if the train-ing data we used was ?difficult to learn from?
(itis common to extract sentences of moderate lengthfor the training data but we didn?t, and some sen-tences of our training corpus do have more than200 words; also, we did not use any kind of pre-processing).
In any case, Giza++ is compared herewith an algorithm trained on the same data andwith no possibilities for fine-tuning; therefore thecomparison should be fair.The comparison show that performance-wise,the monolink algorithm is between the model 2and the model 3 for English/French.
Considering2http://www.statmt.org/moses/170our model has the same number of parameters asthe model 1 (namely, the word translation prob-abilities, or concept probabilities in our model),these are pretty good results.
Overall, the mono-link model tend to give better precision and worserecall than the Giza++ models, which was to beexpected given the different type of alignmentsproduced (1-to-1 and 1-to-many).For English/Japanese, monolink is at just aboutthe level of model 1, but model 1,2 and 3 have veryclose performances for this language pair (inter-estingly, this is different from the English/Frenchpair).
Incidentally, these performances are verypoor.
Recall was expected to be low, due to thepreviously mentioned problem with the gold stan-dard.
But precision was expected to be better.
Itcould be the algorithms are confused by the veryfine-grained segmentation produced by Juman.4 Adding distortion through structure4.1 DescriptionWhile the simple monolink model gives interest-ing results, it is somehow limited in that it do notuse any model of distortion.
We will now try toadd a distortion model; however, rather than di-rectly modeling the movement of the positions ofthe words, as is the case in the IBM models, wewill try to design a distortion model based on thestructures of the sentences.
In particular, we areinterested in using the trees produced by syntacticparsers.The intuition we want to use is that, much likethere is a kind of ?lexical conservation?
in thetranslation process, meaning that a word on oneside has usually an equivalent on the other side,there should also be a kind of ?structure conserva-tion?, with most structures on one side having anequivalent on the other.Before going further, we should precise the ideaof ?structure?
we are going to use.
As we said, ourprime (but not only) interest will be to make use ofthe syntactic trees of the sentences to be aligned.However these kind of trees come in very differentshapes depending on the language and the type ofparser used (dependency, constituents,.
.
.
).
This iswhy we decided the only information we wouldkeep from a syntactic tree is the set of its sub-nodes.
More specifically, for every sub-node, wewill only consider the set of positions it cover inthe underlying sentence.
We will call such a setof positions a P-set.
This simplification will allowFigure 3: A small syntactic tree and the 3 P-Sets itgeneratesus to process dependency trees, constituents treesand other structures in a uniformized way.
Fig-ure 3 gives an example of a constituents tree andthe P-sets it generates.According to our intuition about the ?conserva-tion of structure?, some (not all) of the P-sets onone side should have an equivalent on the otherside.
We can model this in a way similar to howwe represented equivalence between words withconcepts.
We postulate that, in addition to a bag ofconcepts, sentence pairs are underlaid by a set ofP-concepts.
P-concepts being actually pairs of P-sets (a P-set for each side of the sentence pair).
Wealso allow the existence of one-sided P-concepts.In the previous model, sentence pairs wherejust bag of words underlaid by a or bag of con-cepts, and there was no modeling of the positionof the words.
P-concepts bring a notion of wordposition to the model.
Intuitively, there shouldbe coherency between P-concepts and concepts.This coherence will come from a compatibilityconstraint: if a sentence contains a two-sided P-concept (PSe, PSf ), and if a word we coveredby PSe come from a two-sided concept (we, wf ),then wf must be covered by PSf .Let us describe the model more formally.
Inthe view of this model, a sentence pair is fully de-scribed by: e and f (the sentences themselves), a(the word alignment giving us the underlying bagof concept), se and sf (the sets of P-sets on eachside of the sentence) and as (the P-set algnmentthat give us the underlying set of P-concepts).e,f ,se,sf are considered to be observed (even ifwe will need parsing tools to observe se and sf );a and as are hidden.
The probability of a sentencepair is given by the joint probability of these vari-ables :P (e, f, se, sf , a, as).
By making some sim-ple independence assumptions, we can write:P (a, as, e, f,se, sf ) = Pml(a, e, f)??
P (se, sf |e, f) ?
P (as|a, se, sf )171Pml(a, e, f) is taken to be identical to the mono-link model (see equation (1)).
We are not inter-ested in P (se, sf |e, f) (parsers will deal with it forus).
In our model, P (as|a, se, sf ) will be equal to:P(as|a, se, sf ) = C ??
(i,j)?asPpc(sei , sfj )??
comp(a, as, se, sf )where comp(a, as, se, sf ) is equal to 1 if the com-patibility constraint is verified, and 0 else.
C is anormalizing constant.
Ppc describe the probabilityof each P-concept.Although it would be possible to learn parame-ters for the distribution Ppc depending on the char-acteristics of each P-concepts, we want to keepour model simple.
Therefore, Ppc will have onlytwo different values.
One for the one-sided P-concepts, and one for the two-sided ones.
Con-sidering the constraint of normalization, we thenhave actually one parameter: ?
= Ppc(1?sided)Ppc(2?sided) .Although it would be possible to learn the param-eter ?
during the EM-training, we choose to setit at a preset value.
Intuitively, we should have0 < ?
< 1, because if ?
is greater than 1, thenthe one-sided P-concepts will be favored by themodel, which is not what we want.
Some empiri-cal experiments showed that all values of ?
in therange [0.5,0.9] were giving good results, whichlead to think that ?
can be set mostly indepen-dently from the training corpus.We still need to train the concepts probabilities(used in Pml(a, e, f)), and to be able to decodethe most probable alignments.
This is why we areagain going to represent P (a, as|e, f, se, sf ) as aFactor Graph.This Factor Graph will contain two instances ofthe monolink Factor Graph as subgraph: one fora, the other for as (see figure 4).
More precisely,we create again a V-Node for every position oneach side of the sentence pair.
We will call theseV-Nodes ?Word V-Nodes?, to differentiate themfrom the new ?P-set V-Nodes?.
We will create a?P-set V-Node?
V ps.ei for every P-set in se, and a?P-set V-Node?
V ps.fj for every P-set in sj .
Weinter-connect all of the Word V-Nodes so that wehave a subgraph identical to the Factor Graph usedin the monolink case.
We also create a ?monolinksubgraph?
for the P-set V-Nodes.We now have 2 disconnected subgraphs.
How-ever, we need to add F-Nodes between them to en-force the compatibility constraint between as andFigure 4: A part of a Factor Graph showing theconnections between P-set V-Nodes and Word V-Nodes on the English side.The V-Nodes are con-nected to the French side through the 2 monolinksubgraphsa.
On the English side, for every P-set V-NodeV psek , and for every position i that the correspond-ing P-set cover, we add a F-Node F comp.ek,i betweenV psek and Vei , associated with the function:?comp.ek,i (l, j) =????
?1 if j ?
sfl orj = ?1 or l = ?10 elseWe proceed symmetrically on the French side.Messages inside each monolink subgraph canstill be computed with the efficient procedure de-scribed in section 3.2.
We do not have the space todescribe in details the messages sent between P-setV-Nodes and Word V-Nodes, but they are easilycomputed from the principles of the BP algorithm.Let NE =?ps?se |ps| and NF =?ps?sf |ps|.Then the complexity of one BP iteration will beO(NG ?
ND + |e| ?
|f |).An interesting aspect of this model is that itis flexible towards enforcing the respect of thestructures by the alignment, since not every P-setneed to have an equivalent in the opposite sen-tence.
(Gildea, 2003) has shown that too strict anenforcement can easily degrade alignment qualityand that good balance was difficult to find.Another interesting aspect is the fact thatwe have a somehow ?parameterless?
distortionmodel.
There is only one real-valued parameter tocontrol the distortion: ?.
And even this parameteris actually pre-set before any training on real data.The distortion is therefore totally controlled by thetwo sets of P-sets on each side of the sentence.Finally, although we introduced the P-sets asbeing generated from a syntactic tree, they donot need to.
In particular, we found interest-ing to use P-sets consisting of every pair of adja-172cent positions in a sentence.
For example, witha sentence of length 5, we generate the P-sets{1,2},{2,3},{3,4} and {4,5}.
The underlying in-tuition is that ?adjacency?
is often preserved intranslation (we can see this as another case of?conservation of structure?).
Practically, using P-sets of adjacent positions create a distortion modelwhere permutation of words are not penalized, butgaps are penalized.4.2 Experimental ResultsThe evaluation setting is the same as in the previ-ous section.
We created syntactic trees for everysentences.
For English,we used the Dan Bikel im-plementation of the Collins parser (Collins, 2003).For French, the SYGMART parser (Chauche?,1984) and for Japanese, the KNP parser (Kuro-hashi and Nagao, 1994).The line SDM:Parsing (SDM standing for?Structure-based Distortion Monolink?)
shows theresults obtained by using P-sets from the trees pro-duced by these parsers.
The line SDM:Adjacencyshows results obtained by using adjacent positionsP-sets ,as described at the end of the previous sec-tion (therefore, SDM:Adjacency do not use anyparser).Several interesting observations can be madefrom the results.
First, our structure-based distor-tion model did improve the results of the mono-link model.
There are however some surprisingresults.
In particular, SDM:Adjacency producedsurprisingly good results.
It comes close to theresults of the IBM model 4 in both language pairs,while it actually uses exactly the same parametersas model 1.
The fact that an assumption as simpleas ?allow permutations, penalize gaps?
can pro-duce results almost on par with the complicateddistortion model of model 4 might be an indica-tion that this model is unnecessarily complex forlanguages with similar structure.Another surpris-ing result is the fact that SDM:Adjacency givesbetter results for the English-French language pairthan SDM:Parsing, while we expected that infor-mation provided by parsers would have been morerelevant for the distortion model.
It might be anindication that the structure of English and Frenchis so close that knowing it provide only moder-ate information for word reordering.
The con-trast with the English-Japanese pair is, in this re-spect, very interesting.
For this language pair,SDM:Adjacency did provide a strong improve-Algorithm AER P RMonolink 0.197 0.881 0.731SDM:Parsing 0.166 0.882 0.813SDM:Adjacency 0.135 0.887 0.851CLA 0.26 0.819 0.665GIZA++ /Model 1 0.281 0.667 0.805GIZA++ /Model 2 0.205 0.754 0.863GIZA++ /Model 3 0.162 0.806 0.890GIZA++ /Model 4 0.121 0.849 0.927Table 1: Results for English/FrenchAlgorithm F P RMonolink 0.263 0.594 0.169SDM:Parsing 0.291 0.662 0.186SDM:Adjacency 0.279 0.636 0.179GIZA++ /Model 1 0.263 0.555 0.172GIZA++ /Model 2 0.268 0.566 0.176GIZA++ /Model 3 0.267 0.589 0.173GIZA++ /Model 4 0.299 0.658 0.193Table 2: Results for Japanese/English.ment, but significantly less so than SDM:Parsing.This tend to show that for language pairs that havevery different structures, the information providedby syntactic tree is much more relevant.5 Conclusion and Future WorkWe will summarize what we think are the 4 moreinteresti ng contributions of this paper.
BP al-gorithm has been shown to be useful and flexi-ble for training and decoding complex alignmentmodels.
An original mostly non-parametrical dis-tortion model based on a simplified structure ofthe sentences has been described.
Adjacence con-straints have been shown to produce very efficientdistortion model.
Empirical performances differ-ences in the task of aligning Japanese and Englishto French hint that considering different paradigmsdepending on language pairs could be an improve-ment on the ?one-size-fits-all?
approach generallyused in Statistical alignment and translation.Several interesting improvement could also bemade on the model we presented.
Especially,a more elaborated Ppc, that would take into ac-count the nature of the nodes (NP, VP, head,..) toparametrize the P-set algnment probability, andwould use the EM-algorithm to learn those param-eters.173ReferencesM.
Bayati, D. Shah, and M. Sharma.
2005.
Maxi-mum weight matching via max-product belief prop-agation.
Information Theory, 2005.
ISIT 2005.
Pro-ceedings.
International Symposium on, pages 1763?1767.Peter E Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer, 1993.
Themathematics of statistical machine translation: pa-rameter estimation, volume 19, pages 263?311.J.
Chauche?.
1984.
Un outil multidimensionnel delanalyse du discours.
Coling84.
Stanford Univer-sity, California.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics.Fabien Cromieres.
2006.
Sub-sentential alignment us-ing substring co-occurrence counts.
In Proceedingsof ACL.
The Association for Computer Linguistics.U.
Germann.
2001.
Aligned hansardsof the 36th parliament of canada.http://www.isi.edu/naturallanguage/download/hansard/.D.
Gildea.
2003.
Loosely tree-based alignment formachine translation.
Proceedings of ACL, 3.T.
Heskes.
2003.
Stable fixed points of loopy be-lief propagation are minima of the bethe free energy.Advances in Neural Information Processing Systems15: Proceedings of the 2002 Conference.S.
Kurohashi and M. Nagao.
1994.
A syntactic analy-sis method of long japanese sentences based on thedetection of conjunctive structures.
ComputationalLinguistics, 20(4):507?534.I.
D. Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.I.
Melamed.
2002.
Empirical Methods for ExploitingParallel Texts.
The MIT Press.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Rada Mihalcea andTed Pedersen, editors, HLT-NAACL 2003 Workshop:Building and Using Parallel Texts: Data Driven Ma-chine Translation and Beyond, pages 1?10, Edmon-ton, Alberta, Canada, May 31.
Association for Com-putational Linguistics.Kevin P Murphy, Yair Weiss, and Michael I Jordan.1999.
Loopy belief propagation for approximate in-ference: An empirical study.
In Proceedings of Un-certainty in AI, pages 467?475.Franz Josef Och and Hermann Ney.
1999.
Improvedalignment models for statistical machine translation.University of Maryland, College Park, MD, pages20?28.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann Publishers.M.
Utiyama and H. Isahara.
2003.
Reliable measuresfor aligning japanese-english news articles and sen-tences.
Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics-Volume1, pages 72?79.Y.
Weiss and W. T. Freeman.
2001.
On the optimalityof solutions of the max-product belief propagationalgorithm in arbitrary graphs.
IEEE Trans.
on Infor-mation Theory, 47(2):736?744.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
Proceedings of ACL.Jonathan S. Yedidia, William T. Freeman, and YairWeiss, 2003.
Understanding belief propagation andits generalizations, pages 239?269.
Morgan Kauf-mann Publishers Inc., San Francisco, CA, USA.174
