Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?48,New York, June 2006. c?2006 Association for Computational LinguisticsLearning to recognize features of valid textual entailmentsBill MacCartney, Trond Grenager, Marie-Catherine de Marneffe,Daniel Cer, and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{wcmac, grenager, mcdm, cerd, manning}@cs.stanford.eduAbstractThis paper advocates a new architecture for tex-tual inference in which finding a good alignment isseparated from evaluating entailment.
Current ap-proaches to semantic inference in question answer-ing and textual entailment have approximated theentailment problem as that of computing the bestalignment of the hypothesis to the text, using a lo-cally decomposable matching score.
We argue thatthere are significant weaknesses in this approach,including flawed assumptions of monotonicity andlocality.
Instead we propose a pipelined approachwhere alignment is followed by a classificationstep, in which we extract features representinghigh-level characteristics of the entailment prob-lem, and pass the resulting feature vector to a statis-tical classifier trained on development data.
We re-port results on data from the 2005 Pascal RTE Chal-lenge which surpass previously reported results foralignment-based systems.1 IntroductionDuring the last five years there has been a surge inwork which aims to provide robust textual inferencein arbitrary domains about which the system has noexpertise.
The best-known such work has occurredwithin the field of question answering (Pasca andHarabagiu, 2001; Moldovan et al, 2003); more re-cently, such work has continued with greater focusin addressing the PASCAL Recognizing Textual En-tailment (RTE) Challenge (Dagan et al, 2005) andwithin the U.S. Government AQUAINT program.Substantive progress on this task is key to manytext and natural language applications.
If one couldtell that Protestors chanted slogans opposing a freetrade agreement was a match for people demonstrat-ing against free trade, then one could offer a form ofsemantic search not available with current keyword-based search.
Even greater benefits would flow toricher and more semantically complex NLP tasks.Because full, accurate, open-domain natural lan-guage understanding lies far beyond current capa-bilities, nearly all efforts in this area have soughtto extract the maximum mileage from quite lim-ited semantic representations.
Some have used sim-ple measures of semantic overlap, but the more in-teresting work has largely converged on a graph-alignment approach, operating on semantic graphsderived from syntactic dependency parses, and usinga locally-decomposable alignment score as a proxyfor strength of entailment.
(Below, we argue thateven approaches relying on weighted abduction maybe seen in this light.)
In this paper, we highlight thefundamental semantic limitations of this type of ap-proach, and advocate a multi-stage architecture thataddresses these limitations.
The three key limita-tions are an assumption of monotonicity, an assump-tion of locality, and a confounding of alignment andevaluation of entailment.We focus on the PASCAL RTE data, examplesfrom which are shown in table 1.
This data set con-tains pairs consisting of a short text followed by aone-sentence hypothesis.
The goal is to say whetherthe hypothesis follows from the text and generalbackground knowledge, according to the intuitionsof an intelligent human reader.
That is, the standardis not whether the hypothesis is logically entailed,but whether it can reasonably be inferred.2 Approaching a robust semanticsIn this section we try to give a unifying overviewto current work on robust textual inference, topresent fundamental limitations of current meth-ods, and then to outline our approach to resolvingthem.
Nearly all current textual inference systemsuse a single-stage matching/proof process, and differ41ID Text Hypothesis Entailed59 Two Turkish engineers and an Afghan translator kidnappedin December were freed Friday.translator kidnapped in Iraq no98 Sharon warns Arafat could be targeted for assassination.
prime minister targeted for assassination no152 Twenty-five of the dead were members of the law enforce-ment agencies and the rest of the 67 were civilians.25 of the dead were civilians.
no231 The memorandum noted the United Nations estimated that2.5 million to 3.5 million people died of AIDS last year.Over 2 million people died of AIDS lastyear.yes971 Mitsubishi Motors Corp.?s new vehicle sales in the US fell46 percent in June.Mitsubishi sales rose 46 percent.
no1806 Vanunu, 49, was abducted by Israeli agents and convictedof treason in 1986 after discussing his work as a mid-levelDimona technician with Britain?s Sunday Times newspaper.Vanunu?s disclosures in 1968 led expertsto conclude that Israel has a stockpile ofnuclear warheads.no2081 The main race track in Qatar is located in Shahaniya, on theDukhan Road.Qatar is located in Shahaniya.
noTable 1: Illustrative examples from the PASCAL RTE data set, available at http://www.pascal-network.org/Challenges/RTE.Though most problems shown have answer no, the data set is actually balanced between yes and no.mainly in the sophistication of the matching stage.The simplest approach is to base the entailment pre-diction on the degree of semantic overlap betweenthe text and hypothesis using models based on bagsof words, bags of n-grams, TF-IDF scores, or some-thing similar (Jijkoun and de Rijke, 2005).
Suchmodels have serious limitations: semantic overlap istypically a symmetric relation, whereas entailmentis clearly not, and, because overlap models do notaccount for syntactic or semantic structure, they areeasily fooled by examples like ID 2081.A more structured approach is to formulate theentailment prediction as a graph matching problem(Haghighi et al, 2005; de Salvo Braz et al, 2005).In this formulation, sentences are represented as nor-malized syntactic dependency graphs (like the oneshown in figure 1) and entailment is approximatedwith an alignment between the graph representingthe hypothesis and a portion of the correspondinggraph(s) representing the text.
Each possible align-ment of the graphs has an associated score, and thescore of the best alignment is used as an approxi-mation to the strength of the entailment: a better-aligned hypothesis is assumed to be more likely tobe entailed.
To enable incremental search, align-ment scores are usually factored as a combinationof local terms, corresponding to the nodes and edgesof the two graphs.
Unfortunately, even with factoredscores the problem of finding the best alignment oftwo graphs is NP-complete, so exact computation isintractable.
Authors have proposed a variety of ap-proximate search techniques.
Haghighi et al (2005)divide the search into two steps: in the first step theyconsider node scores only, which relaxes the prob-lem to a weighted bipartite graph matching that canbe solved in polynomial time, and in the second stepthey add the edges scores and hillclimb the align-ment via an approximate local search.A third approach, exemplified by Moldovan et al(2003) and Raina et al (2005), is to translate de-pendency parses into neo-Davidsonian-style quasi-logical forms, and to perform weighted abductivetheorem proving in the tradition of (Hobbs et al,1988).
Unless supplemented with a knowledgebase, this approach is actually isomorphic to thegraph matching approach.
For example, the graphin figure 1 might generate the quasi-LF rose(e1),nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2),dobj(e1, x3), percent(x3), num(x3, x4), 46(x4).There is a term corresponding to each node and arc,and the resolution steps at the core of weighted ab-duction theorem proving consider matching an indi-vidual node of the hypothesis (e.g.
rose(e1)) withsomething from the text (e.g.
fell(e1)), just as inthe graph-matching approach.
The two models be-come distinct when there is a good supply of addi-tional linguistic and world knowledge axioms?as inMoldovan et al (2003) but not Raina et al (2005).Then the theorem prover may generate intermedi-ate forms in the proof, but, nevertheless, individ-ual terms are resolved locally without reference toglobal context.Finally, a few efforts (Akhmatova, 2005; Fowleret al, 2005; Bos and Markert, 2005) have tried to42translate sentences into formulas of first-order logic,in order to test logical entailment with a theoremprover.
While in principle this approach does notsuffer from the limitations we describe below, inpractice it has not borne much fruit.
Because fewproblem sentences can be accurately translated tological form, and because logical entailment is astrict standard, recall tends to be poor.The simple graph matching formulation of theproblem belies three important issues.
First, theabove systems assume a form of upward monotonic-ity: if a good match is found with a part of the text,other material in the text is assumed not to affectthe validity of the match.
But many situations lackthis upward monotone character.
Consider variantson ID 98.
Suppose the hypothesis were Arafat tar-geted for assassination.
This would allow a perfectgraph match or zero-cost weighted abductive proof,because the hypothesis is a subgraph of the text.However, this would be incorrect because it ignoresthe modal operator could.
Information that changesthe validity of a proof can also exist outside a match-ing clause.
Consider the alternate text Sharon deniesArafat is targeted for assassination.1The second issue is the assumption of locality.Locality is needed to allow practical search, butmany entailment decisions rely on global features ofthe alignment, and thus do not naturally factor bynodes and edges.
To take just one example, drop-ping a restrictive modifier preserves entailment in apositive context, but not in a negative one.
For exam-ple, Dogs barked loudly entails Dogs barked, but Nodogs barked loudly does not entail No dogs barked.These more global phenomena cannot be modeledwith a factored alignment score.The last issue arising in the graph matching ap-proaches is the inherent confounding of alignmentand entailment determination.
The way to show thatone graph element does not follow from another isto make the cost of aligning them high.
However,since we are embedded in a search for the lowestcost alignment, this will just cause the system tochoose an alternate alignment rather than recogniz-ing a non-entailment.
In ID 152, we would like thehypothesis to align with the first part of the text, to1This is the same problem labeled and addressed as contextin Tatu and Moldovan (2005).be able to prove that civilians are not members oflaw enforcement agencies and conclude that the hy-pothesis does not follow from the text.
But a graph-matching system will to try to get non-entailmentby making the matching cost between civilians andmembers of law enforcement agencies be very high.However, the likely result of that is that the final partof the hypothesis will align with were civilians atthe end of the text, assuming that we allow an align-ment with ?loose?
arc correspondence.2 Under thiscandidate alignment, the lexical alignments are per-fect, and the only imperfect alignment is the subjectarc of were is mismatched in the two.
A robust in-ference guesser will still likely conclude that there isentailment.We propose that all three problems can be re-solved in a two-stage architecture, where the align-ment phase is followed by a separate phase of en-tailment determination.
Although developed inde-pendently, the same division between alignment andclassification has also been proposed by Marsi andKrahmer (2005), whose textual system is developedand evaluated on parallel translations into Dutch.Their classification phase features an output spaceof five semantic relations, and performs well at dis-tinguishing entailing sentence pairs.Finding aligned content can be done by any searchprocedure.
Compared to previous work, we empha-size structural alignment, and seek to ignore issueslike polarity and quantity, which can be left to asubsequent entailment decision.
For example, thescoring function is designed to encourage antonymmatches, and ignore the negation of verb predicates.The ideas clearly generalize to evaluating severalalignments, but we have so far worked with justthe one-best alignment.
Given a good alignment,the determination of entailment reduces to a simpleclassification decision.
The classifier is built overfeatures designed to recognize patterns of valid andinvalid inference.
Weights for the features can behand-set or chosen to minimize a relevant loss func-tion on training data using standard techniques frommachine learning.
Because we already have a com-plete alignment, the classifier?s decision can be con-2Robust systems need to allow matches with imperfect arccorrespondence.
For instance, given Bill went to Lyons to studyFrench farming practices, we would like to be able to concludethat Bill studied French farming despite the structural mismatch.43ditioned on arbitrary global features of the alignedgraphs, and it can detect failures of monotonicity.3 SystemOur system has three stages: linguistic analysis,alignment, and entailment determination.3.1 Linguistic analysisOur goal in this stage is to compute linguistic rep-resentations of the text and hypothesis that containas much information as possible about their seman-tic content.
We use typed dependency graphs, whichcontain a node for each word and labeled edges rep-resenting the grammatical relations between words.Figure 1 gives the typed dependency graph for ID971.
This representation contains much of the infor-mation about words and relations between them, andis relatively easy to compute from a syntactic parse.However many semantic phenomena are not repre-sented properly; particularly egregious is the inabil-ity to represent quantification and modality.We parse input sentences to phrase structuretrees using the Stanford parser (Klein and Manning,2003), a statistical syntactic parser trained on thePenn TreeBank.
To ensure correct parsing, we pre-process the sentences to collapse named entities intonew dedicated tokens.
Named entities are identi-fied by a CRF-based NER system, similar to thatdescribed in (McCallum and Li, 2003).
After pars-ing, contiguous collocations which appear in Word-Net (Fellbaum, 1998) are identified and grouped.We convert the phrase structure trees to typed de-pendency graphs using a set of deterministic hand-coded rules (de Marneffe et al, 2006).
In these rules,heads of constituents are first identified using a mod-ified version of the Collins head rules that favor se-mantic heads (such as lexical verbs rather than aux-iliaries), and dependents of heads are typed usingtregex patterns (Levy and Andrew, 2006), an exten-sion of the tgrep pattern language.
The nodes in thefinal graph are then annotated with their associatedword, part-of-speech (given by the parser), lemma(given by a finite-state transducer described by Min-nen et al (2001)) and named-entity tag.3.2 AlignmentThe purpose of the second phase is to find a goodpartial alignment between the typed dependencygraphs representing the hypothesis and the text.
Analignment consists of a mapping from each node(word) in the hypothesis graph to a single node inthe text graph, or to null.3 Figure 1 gives the align-ment for ID 971.The space of alignments is large: there areO((m + 1)n) possible alignments for a hypothesisgraph with n nodes and a text graph with m nodes.We define a measure of alignment quality, and aprocedure for identifying high scoring alignments.We choose a locally decomposable scoring function,such that the score of an alignment is the sum ofthe local node and edge alignment scores.
Unfor-tunately, there is no polynomial time algorithm forfinding the exact best alignment.
Instead we use anincremental beam search, combined with a node or-dering heuristic, to do approximate global search inthe space of possible alignments.
We have exper-imented with several alternative search techniques,and found that the solution quality is not very sensi-tive to the specific search procedure used.Our scoring measure is designed to favor align-ments which align semantically similar subgraphs,irrespective of polarity.
For this reason, nodes re-ceive high alignment scores when the words theyrepresent are semantically similar.
Synonyms andantonyms receive the highest score, and unrelatedwords receive the lowest.
Our hand-crafted scor-ing metric takes into account the word, the lemma,and the part of speech, and searches for word relat-edness using a range of external resources, includ-ing WordNet, precomputed latent semantic analysismatrices, and special-purpose gazettes.
Alignmentscores also incorporate local edge scores, which arebased on the shape of the paths between nodes inthe text graph which correspond to adjacent nodesin the hypothesis graph.
Preserved edges receive thehighest score, and longer paths receive lower scores.3.3 Entailment determinationIn the final stage of processing, we make a deci-sion about whether or not the hypothesis is entailedby the text, conditioned on the typed dependencygraphs, as well as the best alignment between them.3The limitations of using one-to-one alignments are miti-gated by the fact that many multiword expressions (e.g.
namedentities, noun compounds, multiword prepositions) have beencollapsed into single nodes during linguistic analysis.44rosesalesMitsubishipercent46nsubj dobjnn numAlignmentrose ?
fellsales ?
salesMitsubishi ?
Mitsubishi Motors Corp.percent ?
percent46 ?
46Alignment score: ?0.8962FeaturesAntonyms aligned in pos/pos context ?Structure: main predicate good match +Number: quantity match +Date: text date deleted in hypothesis ?Alignment: good score +Entailment score: ?5.4262Figure 1: Problem representation for ID 971: typed dependency graph (hypothesis only), alignment, and entailment features.Because we have a data set of examples that are la-beled for entailment, we can use techniques from su-pervised machine learning to learn a classifier.
Weadopt the standard approach of defining a featuralrepresentation of the problem and then learning alinear decision boundary in the feature space.
Wefocus here on the learning methodology; the nextsection covers the definition of the set of features.Defined in this way, one can apply any statisticallearning algorithm to this classification task, suchas support vector machines, logistic regression, ornaive Bayes.
We used a logistic regression classifierwith a Gaussian prior parameter for regularization.We also compare our learning results with thoseachieved by hand-setting the weight parameters forthe classifier, effectively incorporating strong prior(human) knowledge into the choice of weights.An advantage to the use of statistical classifiersis that they can be configured to output a proba-bility distribution over possible answers rather thanjust the most likely answer.
This allows us to getconfidence estimates for computing a confidenceweighted score (see section 5).
A major concern inapplying machine learning techniques to this clas-sification problem is the relatively small size of thetraining set, which can lead to overfitting problems.We address this by keeping the feature dimensional-ity small, and using high regularization penalties intraining.4 Feature representationIn the entailment determination phase, the entail-ment problem is reduced to a representation as avector of 28 features, over which the statisticalclassifier described above operates.
These featurestry to capture salient patterns of entailment andnon-entailment, with particular attention to contextswhich reverse or block monotonicity, such as nega-tions and quantifiers.
This section describes the mostimportant groups of features.Polarity features.
These features capture the pres-ence (or absence) of linguistic markers of negativepolarity contexts in both the text and the hypothesis,such as simple negation (not), downward-monotonequantifiers (no, few), restricting prepositions (with-out, except) and superlatives (tallest).Adjunct features.
These indicate the dropping oradding of syntactic adjuncts when moving from thetext to the hypothesis.
For the common case ofrestrictive adjuncts, dropping an adjunct preservestruth (Dogs barked loudly |= Dogs barked), whileadding an adjunct does not (Dogs barked 6|= Dogsbarked today).
However, in negative-polarity con-texts (such as No dogs barked), this heuristic isreversed: adjuncts can safely be added, but notdropped.
For example, in ID 59, the hypothesisaligns well with the text, but the addition of in Iraqindicates non-entailment.We identify the ?root nodes?
of the problem: theroot node of the hypothesis graph and the corre-sponding aligned node in the text graph.
Using de-pendency information, we identify whether adjunctshave been added or dropped.
We then determinethe polarity (negative context, positive context orrestrictor of a universal quantifier) of the two rootnodes to generate features accordingly.Antonymy features.
Entailment problems mightinvolve antonymy, as in ID 971.
We check whetheran aligned pairs of text/hypothesis words appear tobe antonymous by consulting a pre-computed listof about 40,000 antonymous and other contrastingpairs derived from WordNet.
For each antonymouspair, we generate one of three boolean features, in-dicating whether (i) the words appear in contexts ofmatching polarity, (ii) only the text word appears ina negative-polarity context, or (iii) only the hypoth-esis word does.45Modality features.
Modality features capturesimple patterns of modal reasoning, as in ID 98,which illustrates the heuristic that possibility doesnot entail actuality.
According to the occurrence(or not) of predefined modality markers, such asmust or maybe, we map the text and the hypoth-esis to one of six modalities: possible, not possi-ble, actual, not actual, necessary, and not necessary.The text/hypothesis modality pair is then mappedinto one of the following entailment judgments: yes,weak yes, don?t know, weak no, or no.
For example:(not possible |= not actual)?
?
yes(possible |= necessary)?
?
weak noFactivity features.
The context in which a verbphrase is embedded may carry semantic presuppo-sitions giving rise to (non-)entailments such as Thegangster tried to escape 6|= The gangster escaped.This pattern of entailment, like others, can be re-versed by negative polarity markers (The gangstermanaged to escape |= The gangster escaped whileThe gangster didn?t manage to escape 6|= The gang-ster escaped).
To capture these phenomena, wecompiled small lists of ?factive?
and non-factiveverbs, clustered according to the kinds of entail-ments they create.
We then determine to which classthe parent of the text aligned with the hypothesisroot belongs to.
If the parent is not in the list, weonly check whether the embedding text is an affir-mative context or a negative one.Quantifier features.
These features are designedto capture entailment relations among simple sen-tences involving quantification, such as Every com-pany must report |= A company must report (orThe company, or IBM).
No attempt is made to han-dle multiple quantifiers or scope ambiguities.
Eachquantifier found in an aligned pair of text/hypothesiswords is mapped into one of five quantifier cate-gories: no, some, many, most, and all.
The nocategory is set apart, while an ordering over theother four categories is defined.
The some categoryalso includes definite and indefinite determiners andsmall cardinal numbers.
A crude attempt is made tohandle negation by interchanging no and all in thepresence of negation.
Features are generated giventhe categories of both hypothesis and text.Number, date, and time features.
These are de-signed to recognize (mis-)matches between num-bers, dates, and times, as in IDs 1806 and 231.
Wedo some normalization (e.g.
of date representations)and have a limited ability to do fuzzy matching.
InID 1806, the mismatched years are correctly iden-tified.
Unfortunately, in ID 231 the significance ofover is not grasped and a mismatch is reported.Alignment features.
Our feature representationincludes three real-valued features intended to rep-resent the quality of the alignment: score is theraw score returned from the alignment phase, whilegoodscore and badscore try to capture whether thealignment score is ?good?
or ?bad?
by computingthe sigmoid function of the distance between thealignment score and hard-coded ?good?
and ?bad?reference values.5 EvaluationWe present results based on the First PASCAL RTEChallenge, which used a development set contain-ing 567 pairs and a test set containing 800 pairs.The data sets are balanced to contain equal num-bers of yes and no answers.
The RTE Challengerecommended two evaluation metrics: raw accuracyand confidence weighted score (CWS).
The CWS iscomputed as follows: for each positive integer k upto the size of the test set, we compute accuracy overthe k most confident predictions.
The CWS is thenthe average, over k, of these partial accuracies.
Likeraw accuracy, it lies in the interval [0, 1], but it willexceed raw accuracy to the degree that predictionsare well-calibrated.Several characteristics of the RTE problemsshould be emphasized.
Examples are derived from abroad variety of sources, including newswire; there-fore systems must be domain-independent.
The in-ferences required are, from a human perspective,fairly superficial: no long chains of reasoning areinvolved.
However, there are ?trick?
questions ex-pressly designed to foil simplistic techniques.
Thedefinition of entailment is informal and approx-imate: whether a competent speaker with basicknowledge of the world would typically infer the hy-pothesis from the text.
Entailments will certainly de-pend on linguistic knowledge, and may also dependon world knowledge; however, the scope of required46Algorithm RTE1 Dev Set RTE1 Test SetAcc CWS Acc CWSRandom 50.0% 50.0% 50.0% 50.0%Jijkoun et al 05 61.0% 64.9% 55.3% 55.9%Raina et al 05 57.8% 66.1% 55.5% 63.8%Haghighi et al 05 ?
?
56.8% 61.4%Bos & Markert 05 ?
?
57.7% 63.2%Alignment only 58.7% 59.1% 54.5% 59.7%Hand-tuned 60.3% 65.3% 59.1% 65.0%Learning 61.2% 74.4% 59.1% 63.9%Table 2: Performance on the RTE development and test sets.CWS stands for confidence weighted score (see text).world knowledge is left unspecified.4Despite the informality of the problem definition,human judges exhibit very good agreement on theRTE task, with agreement rate of 91?96% (Daganet al, 2005).
In principle, then, the upper boundfor machine performance is quite high.
In practice,however, the RTE task is exceedingly difficult forcomputers.
Participants in the first PASCAL RTEworkshop reported accuracy from 49% to 59%, andCWS from 50.0% to 69.0% (Dagan et al, 2005).Table 2 shows results for a range of systems andtesting conditions.
We report accuracy and CWS oneach RTE data set.
The baseline for all experimentsis random guessing, which always attains 50% accu-racy.
We show comparable results from recent sys-tems based on lexical similarity (Jijkoun and de Ri-jke, 2005), graph alignment (Haghighi et al, 2005),weighted abduction (Raina et al, 2005), and a mixedsystem including theorem proving (Bos and Mark-ert, 2005).We then show results for our system under severaldifferent training regimes.
The row labeled ?align-ment only?
describes experiments in which all fea-tures except the alignment score are turned off.
Wepredict entailment just in case the alignment scoreexceeds a threshold which is optimized on devel-opment data.
?Hand-tuning?
describes experimentsin which all features are on, but no training oc-curs; rather, weights are set by hand, according tohuman intuition.
Finally, ?learning?
describes ex-periments in which all features are on, and featureweights are trained on the development data.
The4Each RTE problem is also tagged as belonging to one ofseven tasks.
Previous work (Raina et al, 2005) has shown thatconditioning on task can significantly improve accuracy.
In thiswork, however, we ignore the task variable, and none of theresults shown in table 2 reflect optimization by task.figures reported for development data performancetherefore reflect overfitting; while such results arenot a fair measure of overall performance, they canhelp us assess the adequacy of our feature set: ifour features have failed to capture relevant aspectsof the problem, we should expect poor performanceeven when overfitting.
It is therefore encouragingto see CWS above 70%.
Finally, the figures re-ported for test data performance are the fairest ba-sis for comparison.
These are significantly betterthan our results for alignment only (Fisher?s exacttest, p < 0.05), indicating that we gain real valuefrom our features.
However, the gain over compara-ble results from other teams is not significant at thep < 0.05 level.A curious observation is that the results for hand-tuned weights are as good or better than results forlearned weights.
A possible explanation runs as fol-lows.
Most of the features represent high-level pat-terns which arise only occasionally.
Because thetraining data contains only a few hundred exam-ples, many features are active in just a handful ofinstances; their learned weights are therefore quitenoisy.
Indeed, a feature which is expected to fa-vor entailment may even wind up with a negativeweight: the modal feature weak yes is an example.As shown in table 3, the learned weight for this fea-ture was strongly negative ?
but this resulted froma single training example in which the feature wasactive but the hypothesis was not entailed.
In suchcases, we shouldn?t expect good generalization totest data, and human intuition about the ?value?
ofspecific features may be more reliable.Table 3 shows the values learned for selected fea-ture weights.
As expected, the features added ad-junct in all context, modal yes, and text is factivewere all found to be strong indicators of entailment,while date insert, date modifier insert, wideningfrom text to hyp all indicate lack of entailment.
Inter-estingly, text has neg marker and text & hyp diff po-larity were also found to disfavor entailment; whilethis outcome is sensible, it was not anticipated ordesigned.6 ConclusionThe best current approaches to the problem of tex-tual inference work by aligning semantic graphs,47Feature class & condition weightAdjunct added adjunct in all context 1.40Date date mismatch 1.30Alignment good score 1.10Modal yes 0.70Modal no 0.51Factive text is factive 0.46. .
.
.
.
.
.
.
.Polarity text & hyp same polarity ?0.45Modal don?t know ?0.59Quantifier widening from text to hyp ?0.66Polarity text has neg marker ?0.66Polarity text & hyp diff polarity ?0.72Alignment bad score ?1.53Date date modifier insert ?1.57Modal weak yes ?1.92Date date insert ?2.63Table 3: Learned weights for selected features.
Positive weightsfavor entailment.
Weights near 0 are omitted.
Based on trainingon the PASCAL RTE development set.using a locally-decomposable alignment score as aproxy for strength of entailment.
We have arguedthat such models suffer from three crucial limita-tions: an assumption of monotonicity, an assump-tion of locality, and a confounding of alignment andentailment determination.We have described a system which extendsalignment-based systems while attempting to ad-dress these limitations.
After finding the best align-ment between text and hypothesis, we extract high-level semantic features of the entailment problem,and input these features to a statistical classifier tomake an entailment decision.
Using this multi-stagearchitecture, we report results on the PASCAL RTEdata which surpass previously-reported results foralignment-based systems.We see the present work as a first step in a promis-ing direction.
Much work remains in improving theentailment features, many of which may be seen asrough approximations to a formal monotonicity cal-culus.
In future, we aim to combine more precisemodeling of monotonicity effects with better mod-eling of paraphrase equivalence.AcknowledgementsWe thank Anna Rafferty, Josh Ainslie, and partic-ularly Roger Grosse for contributions to the ideasand system reported here.
This work was supportedin part by the Advanced Research and DevelopmentActivity (ARDA)?s Advanced Question Answeringfor Intelligence (AQUAINT) Program.ReferencesE.
Akhmatova.
2005.
Textual entailment resolution via atomicpropositions.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment, 2005.J.
Bos and K. Markert.
2005.
Recognising textual entailmentwith logical inference.
In EMNLP-05.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The PASCALrecognising textual entailment challenge.
In Proceedings ofthe PASCAL Challenges Workshop on Recognising TextualEntailment.Marie-Catherine de Marneffe, Bill MacCartney, and Christo-pher D. Manning.
2006.
Generating typed dependencyparses from phrase structure parses.
In LREC 2006.R.
de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, andM.
Sammons.
2005.
An inference model for semantic entail-ment and question-answering.
In Proceedings of the Twenti-eth National Conference on Artificial Intelligence (AAAI).C.
Fellbaum.
1998.
WordNet: an electronic lexical database.MIT Press.A.
Fowler, B. Hauser, D. Hodges, I. Niles, A. Novischi, andJ.
Stephan.
2005.
Applying COGEX to recognize textualentailment.
In Proceedings of the PASCAL Challenges Work-shop on Recognising Textual Entailment.A.
Haghighi, A. Ng, and C. D. Manning.
2005.
Robust textualinference via graph matching.
In EMNLP-05.J.
R. Hobbs, M. Stickel, P. Martin, and D. D. Edwards.
1988.Interpretation as abduction.
In 26th Annual Meeting of theAssociation for Computational Linguistics: Proceedings ofthe Conference, pages 95?103, Buffalo, New York.V.
Jijkoun and M. de Rijke.
2005.
Recognizing textual entail-ment using lexical similarity.
In Proceedings of the PAS-CAL Challenge Workshop on Recognising Textual Entail-ment, 2005, pages 73?76.D.
Klein and C. D. Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of the 41st Meeting of the Associa-tion of Computational Linguistics.Roger Levy and Galen Andrew.
2006.
Tregex and Tsurgeon:tools for querying and manipulating tree data structures.
InLREC 2006.E.
Marsi and E. Krahmer.
2005.
Classification of semantic re-lations by humans and machines.
In Proceedings of the ACL2005 Workshop on Empirical Modeling of Semantic Equiva-lence and Entailment.A.
McCallum and W. Li.
2003.
Early results for named entityrecognition with conditional random fields, feature inductionand web-enhanced lexicons.
In Proceedings of CoNLL 2003.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Applied morpho-logical processing in English.
In Natural Language Engi-neering, volume 7(3), pages 207?233.D.
Moldovan, C. Clark, S. Harabagiu, and S. Maiorano.
2003.COGEX: A logic prover for question answering.
In NAACL-03.M.
Pasca and S. Harabagiu.
2001.
High performance ques-tion/answering.
In SIGIR-01, pages 366?374.R.
Raina, A .Ng, and C. D. Manning.
2005.
Robust textualinference via learning and abductive reasoning.
In Proceed-ings of the Twentieth National Conference on Artificial Intel-ligence (AAAI).M.
Tatu and D. Moldovan.
2005.
A semantic approach to rec-ognizing textual entailment.
In HLT/EMNLP 2005, pages371?378.48
