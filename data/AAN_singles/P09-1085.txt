Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 755?763,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDiscriminative Lexicon Adaptation for Improved Character Accuracy ?A New Direction in Chinese Language ModelingYi-cheng PanSpeech Processing LabratoryNational Taiwan UniversityTaipei, Taiwan 10617thomashughPan@gmail.comLin-shan LeeSpeech Processing LabratoryNational Taiwan UniversityTaipei, Taiwan 10617lsl@speech.ee.ntu.edu.twSadaoki FuruiFurui LabratoryTokyo Institute of TechnologyTokyo 152-8552 Japanfurui@furui.cs.titech.ac.jpAbstractWhile OOV is always a problem for mostlanguages in ASR, in the Chinese case theproblem can be avoided by utilizing char-acter n-grams and moderate performancescan be obtained.
However, character n-gram has its own limitation and properaddition of new words can increase theASR performance.
Here we propose a dis-criminative lexicon adaptation approach forimproved character accuracy, which notonly adds new words but also deletes somewords from the current lexicon.
Differentfrom other lexicon adaptation approaches,we consider the acoustic features and makeour lexicon adaptation criterion consistentwith that in the decoding process.
The pro-posed approach not only improves the ASRcharacter accuracy but also significantlyenhances the performance of a character-based spoken document retrieval system.1 IntroductionGenerally, an automatic speech recognition (ASR)system requires a lexicon.
The lexicon defines thepossible set of output words and also the buildingunits in the language model (LM).
Lexical wordsoffer local constraints to combine phonemes intoshort chunks while the language model combinesphonemes into longer chunks by more global con-straints.
However, it?s almost impossible to includeall words into a lexicon both due to the technicaldifficulty and also the fact that new words are cre-ated continuously.
The missed out words will neverbe recognized, which is the well-known OOV prob-lem.
Using graphemes for OOV handling is pro-posed in English (Bisani and Ney, 2005).
Althoughthis sacrifices some of the lexical constraints and in-troduces a further difficulty to combine graphemesback into words, it is compensated by its ability for5.8K characters 61.5K full lexiconbigram 63.55% 73.8%trigram 74.27% 79.28%Table 1: Character recognition accuracy under dif-ferent lexicons and the order of language model.open vocabulary ASR.
Morphs are another possi-bility, which are longer than graphemes but shorterthan words, in other western languages (Hirsima?kiet al, 2005).Chinese language, on the other hand, is quitedifferent from western languages.
There are noblanks between words and the definition for wordsis vague.
Since almost all characters in Chinesehave their own meanings and words are composedof the characters, there is an obvious solution forthe OOV problem: simply using all characters asthe lexicon.
In Table 1 we see the differences incharacter recognition accuracy by using only 5.8Kcharacters and a full set of 61.5K lexicon.
The train-ing set and testing set are the same as those thatwill be introduced in Section 4.1.
It is clear thatcharacters alone can provide moderate recognitionaccuracies while augmenting new words signifi-cantly improves the performance.
If the words?semantic functionality can be abandoned, whichdefinitely can not be replaced by characters, we cantreat words as a means to enhance character recog-nition accuracy.
Such arguments stand at least forChinese ASR since they evaluate on character errorrate and do not add explicit blanks between words.Here we formulate a lexicon adaptation problemand try to discriminatively find out not only OOVwords beneficial for ASR but also those existingwords that can be deleted.Unlike previous lexicon adaptation or construc-tion approaches (Chien, 1997; Fung, 1998; Deligneand Sagisaka, 2000; Saon and Padmanabhan, 2001;Gao et al, 2002; Federico and Bertoldi, 2004), we755consider the acoustic signals and also the wholespeech decoding structure.
We propose to usea simple approximation for the character poste-rior probabilities (PPs), which combines acousticmodel and language model scores after decoding.Based on the character PPs, we adapt the currentlexicon.
The language model is then re-trained ac-cording the new lexicon.
Such procedure can beiterated until convergence.Characters, are not only the output units in Chi-nese ASR but also have their roles in spoken docu-ment retrieval (SDR).
It has been shown that char-acters are good indexing units.
Generally, char-acters can at least help OOV query handling; inthe subword-based confusion network (S-CN) pro-posed by Pan et al (2007), characters are evenbetter than words for in-vocabulary (IV) queries.In addition to evaluating the proposed approach onASR performance, we investigate its helpfulnesswhen integrated with an S-CN framework.2 Related WorkPrevious works for lexicon adaptation were focusedon OOV rate reduction.
Given an adaptation cor-pus, the standard way is to first identify OOVwords.These OOV words are selected into the current lex-icon based on the criterion of frequency or recency(Federico and Bertoldi, 2004).
The language modelis also re-estimated according to the new corpusand new derived words.For Chinese, it is more difficult to follow thesame approach since OOV words are not readilyidentifiable.
Several methods have been proposedto extract OOV words from the new corpus basedon different statistics, which include associate normand context dependency (Chien, 1997), mutual in-formation (Gao et al, 2002), morphological andstatistical rules (Chen and Ma, 2002), and strengthand spread measure (Fung, 1998).
The used statis-tics generally help find sequences of charactersthat are consistent to the general concept of words.However, if we focus on ASR performance, theconstraint of the extracted character strings to beword-like is unnecessary.Yang et al (1998) proposed a way to select newcharacter strings based on average character per-plexity reduction.
The word-like constraint is notrequired and they show a significant improvementon character-based perplexity.
Similar ideas werefound to use mutual probability as an effective mea-sure to combine two existing lexicon words into anew word (Saon and Padmanabhan, 2001).
Thoughproposed for English, this method is effective forChinese ASR (Chen et al, 2004).
Gao et al (2002)combined an information gain-like metric and theperplexity reduction criterion for lexicon word se-lection.
The application is on Chinese pinyin-to-character conversion, which has very good correla-tion with the underlying language model perplexity.The above works actually are all focused on thetext level and only consider perplexity effect.
How-ever, as pointed by Rosenfeld (2000), lower per-plexity does not always imply lower ASR error rate.Here we try to face the lexicon adaptation problemfrom another aspect and take the acoustic signalsinvolved in the decoding procedure into account.3 Proposed Approach3.1 Overall PictureordCharacter-basedConfusionAutomaticSpeech Recognition(ASR)Character-basedConfusion Network(CCN) constructionword latticesNetwork (CCN)Adaptation CorpusLexicon Adaptationfor ImprovedCharacter AccuracyAdd/Delete wordsLexicon (Lex i)LanguageModel(LMi)y(LAICA)WordSegmentationLM Training(Lex i)Model (LMi)ManualTranscriptionSegmentationand LM TraininggCorporaFigure 1: The flow chart of the proposed approach.We show the complete flow chart in Figure 1.
Atthe beginning we are given an adaptation spokencorpus and manual transcriptions.
Based on a base-line lexicon (Lex0) and a language model (LM0)we perform ASR on the adaptation corpus and con-struct corresponding word lattices.
We then buildcharacter-based confusion networks (CCNs) (Fuet al, 2006; Qian et al, 2008).
On the CCNs weperform the proposed algorithm to add and deletewords into/from the current lexicon.
The LM train-ing corpora joined with the adaptation corpus isthen segmented using Lex1 and the language modelis in turn re-trained, which gives LM1.
This pro-cedure can be iterated to give Lexi and LMi untilconvergence.3.2 Character Posterior Probability andCharacter-based Confusion Network(CCN)Consider a word W as shown in Figure 2 withcharacters {c1c2c3} corresponding to the edge estarting at time ?
and ending at time t in a wordlattice.
During decoding the boundaries between c1756Figure 2: An edge e of word W composed of char-acters c1c2c3 starting at time ?
and ending at timet.and c2, and c2 and c3 are recorded respectively as t1and t2.
The posterior probability (PP) of the edge egiven the acoustic features A, P (e|A), is (Wesselet al, 2001):P (e|A) =?(?)
?
P (xt?
|W ) ?
PLM (W ) ?
?
(t)?start,(1)where ?(?)
and ?
(t) denote the forward and back-ward probability masses accumulated up to time ?and t obtained by the standard forward-backwardalgorithm, P (xt?
|W ) is the acoustic likelihoodfunction, PLM (W ) the language model score, and?start the sum of all path scores in the lattice.
Equa-tion (1) can be extended to the PP of a character ofW , say c1 with edge e1:P (e1|A) =?(?)
?
P (xt1?
|c1) ?
PLM (c1) ?
?(t1)?start.
(2)Here we need two new probabilities, PLM (c1)and ?(t1).
Since neither is easy to estimate, wemake some approximations.
First, we assumePLM (c1) ?
PLM (W ).
Of course this is not true,the actual relation being PLM (c1) ?
PLM (W ),since the set of events having c1 given its his-tory includes a set of events having W given thesame history.
We used the above approximationfor easier implementation.
Second, we assumethat after c1 there is only one path from t1 tot: through c2 and c3.
This is more reasonablesince we restrain the hypotheses space to be in-side the word lattice, and pruned paths are sim-ply neglected.
With this approximation we have?
(t1) = P (xtt1 |c2c3) ?
?(t).
Substituting thesetwo approximate values for PLM (c1) and ?
(t1) inEquation (2), the result turns out to be very sim-ple: P (e1|A) ?
P (e|A).
With similar assump-tions for the character edges e2 and e3, we haveP (e2|A) ?
P (e3|A) ?
P (e|A).
Similar resultswere obtained by Yao et al (2008) from a differentpoint of view.The result that P (ei|A) ?
P (e|A) seems todiverge from the intuition: approximating ann-segment word by splitting the probability ofthe entire edge over the segments ?
P (ei|A) ?n?P (e|A).
The basic meaning of Equation (1) isto calculate the ratio of the paths going through aspecific edge divided by the total paths while eachpath is weighted properly.
Of course the paths go-ing through a sub-edge ei should be definitely morethan the paths through the corresponding full-edgee.
As a result, P (ei|A) should usually be greaterthan P (e|A), as implied by the intuition.
However,the inter-connectivity between all sub-edges andthe proper weights of them are not easy to be han-dled well.
Here we constrain the inter-connectivityof sub-edges to be only inside its own word edgeand also simplify the calculation of the weightsof paths.
This offers a tractable solution and theperformance is quite acceptable.After we obtain the PPs for each character arcin the lattice, such as P (ei|A) as mentioned above,we can perform the same clustering method pro-posed by Mangu et al (2000) to convert the wordlattice to a strict linear sequence of clusters, eachconsisting of a set of alternatives of character hy-potheses, or a character-based confusion network(CCN) (Fu et al, 2006; Qian et al, 2008).
In CCNwe collect the PPs for all character arc c with begin-ning time ?
and end time t as P ([c; ?, t]|A) (basedon the above mentioned approximation):P ([c; ?, t]|A) =?H = w1 .
.
.
wN ?
lattice :?i ?
{1 .
.
.
N} :wi contains [c; ?, t]P (H)P (A|H)?pathH?
?
latticeP (H ?
)P (A|H ?
),(3)whereH stands for a path in the word lattice.
P (H)is the language model score of H (after proper scal-ing) and P (A|H) is the acoustic model score.
CCNwas known to be very helpful in reducing charactererror rate (CER) since it minimizes the expectedCER (Fu et al, 2006; Qian et al, 2008).
Givena CCN, we simply choose the characters with thehighest PP from each cluster as the recognitionresults.3.3 Lexicon Adaptation with ImprovedCharacter Accuracy (LAICA)In Figure 3 we show a piece of a character-basedconfusion network (CCN) aligned with the corre-sponding manual transcription characters.
Suchalignment can be implemented by an efficient dy-namic programming method.
The CCN is com-posed of several strict linear ordering clusters of757R m-1R mReferenceCharacters ?R m+1R m+2R m+3n ||o ||p ||q ||r ||?Character-basedConfusion Network(CCN)??nstu?????
?.Calign(m)C align(m+2)C align(m+3)oq???..???
?C align(m-1)C align(m+1)align(m+2)pR m:charactervariableat the mth position inthe referencecharactersmpC align(m): a cluster ofCCNalignedwiththemth characterin the referencen~u:symbolsforChinese charactersFigure 3: A character-based confusion network(CCN) and corresponding reference manual tran-scription characters.character alternatives.
In the figure, Calign(m)is a specific cluster aligned with the mth char-acter in the reference, which contains characters{s .
.
.
o .
.
.}
(The alphabets n, o .
.
.
u are symbolsfor specific Chinese characters) .
The characters ineach cluster of CCN are well sorted according tothe PP, and in each cluster a special null character with its PP being equal to 1 minus the summationof PPs for all character hypotheses in that cluster.The clusters with  ranked first are neglected in thealignment.After the alignment, there are only three pos-sibilities corresponding to each reference charac-ter.
(1) The reference character is ranked first inthe corresponding cluster (Rm?1 and the clusterCalign(m?1)).
In this case the reference charac-ter can be correctly recognized.
(2) The refer-ence character is included in the correspondingcluster but not ranked first.
([Rm .
.
.
Rm+2] and{Calign(m), .
.
.
, Calign(m+2)}) (3) The referencecharacter is not included in the corresponding clus-ter (Rm+3 and Calign(m+3)).
For cases (2) and (3),the reference character will be incorrectly recog-nized.The basic idea of the proposed lexicon adapta-tion with an improved character accuracy (LAICA)approach is to enhance the PPs of those incorrectlyrecognized characters by adding new words anddeleting existing words in the lexicon.
Here weonly focus on those characters of case (2) men-tioned above.
This is primarily motivated by theminimum classification error (MCE) discriminativetraining approach proposed by Juang et al (1997),where a sigmoid function was used to suppress theimpacts of those perfectly and very poorly recog-nized training samples.
In our approach, the case(1) is the perfect case and case (3) is the very poorone.
Another motivation is that for characters incase (1), since they are already correctly recognizedwe do not try to enhance their PPs.The procedure of LAICA then becomes simple.Among the aligned reference characters and clus-ters of CCN, case (1) and (3) are anchors.
Thereference characters between two anchors then be-come our focus segment and their PPs should be en-hanced.
By investigating Equation (3), to enhancethe PP of a specific character we can adjust thelanguage model (P (H)), and the acoustic model(P (A|H)), or we can simply modify the lexicon(the constraint under summation).
We should addnew words to cover the characters of case (2) toenlarge the numerator of Equation (3) and at thesame time delete some existing words to suppressthe denominator.In Figure 3, reference characters[RmRm+1Rm+2=opq] and the clusters{Calign(m), .
.
.
, Calign(m+2)} show an exam-ple of our focus segment.
For each such segment,we at most add one new word and delete anexisting word.
From the string [opq] we choosethe longest OOV part from it as a new word.To select a word to be deleted, we choose thelongest in-vocabulary (IV) part from the topranked competitors of [opq], which are then [stu]in clusters {Calign(m), .
.
.
, Calign(m+2)}.
This isalso motivated by MCE that we only suppress thestrongest competitors?
probabilities.
Note that wedo not delete single-characters in the procedure.The ?at most one?
constraint here is motivatedby previous language model adaptation works (Fed-erico, 1999) which usually try to introduce new ev-idences in the adaptation corpus but with the leastmodification of the original model.
Of course themodification of language models led by the addi-tion and deletion of words is hard to quantify andwe choose to add and delete as fewer words as pos-sible, which is just a simple heuristic.
On the otherhand, adding fewer words means that longer wordsare added.
It has been shown that longer words aremore helpful for ASR (Gao et al, 2004; Saon andPadmanabhan, 2001).The proposed LAICA approach can be regardedas a discriminative one since it not only considersthe reference characters but also those wrongly rec-ognized characters.
This can be beneficial since itreduces potential ambiguities existing in the lexi-con.758The Expectation-Maximization algorithm1.
Bootstrap initial word segmentation bymaximum-matching algorithm(Wong and Chan, 1996)2.
Estimate unigram LM3.
Expectation: Re-segment accordingto the unigram LM4.
Maximization: Estimate the n-gram LM5.
Expectation: Re-segment according tothe n-gram LM6.
Go to step 4 until convergenceTable 2: EM algorithm for word segmentation andLM estimation3.4 Word Segmentation and LanguageModel TrainingIf we regard the word segmentation process as ahidden variable, then we can apply EM algorithm(Dempster et al, 1977) to train the underlying n-gram language model.
The procedure is describedin Table 2.
In the algorithm we can see two ex-pectation phases.
This is natural since at the be-ginning the bootstrap segmentation can not givereliable statistics for higher order n-gram and wechoose to only use the unigram marginal probabili-ties.
The procedure was well established by Hwanget al (2006).Actually, the EM algorithm proposed here is sim-ilar to the n-multigram model training procedureproposed by Deligne and Sagisaka (2000).
The roleof multigrams can be regarded as the words here,except that multigrams begin from scratch whilehere we have an initial lexicon and use maximum-matching algorithm to offer an acceptable initialunigram probability distributions.
If the initial lex-icon is not available, the procedure proposed byDeligne and Sagisaka (2000) is preferred.4 Experimental Results4.1 Baseline Lexicon, Corpora and LanguageModelsThe baseline lexicon was automatically constructedfrom a 300 MB Chinese news text corpus rangingfrom 1997 to 1999 using the widely applied PAT-tree-based word extraction method (Chien, 1997).It includes 61521 words in total, of which 5856are single-characters.
The key principles of thePAT-tree-based approach to extract a sequence ofcharacters as a word are: (1) high enough frequencycount; (2) high enough mutual information betweencomponent characters; (3) large enough number ofcontext variations on both sides; (4) not dominatedby the most frequent context among all contextvariations.
In general the words extracted have highfrequencies and clear boundaries, thus very oftenthey have good semantic meanings.
Since all theabove statistics of all possible character sequencesin a raw corpus are combinatorially too many, weneed an efficient data structure such as the PAT-treeto record and access all such information.With the baseline lexicon, we performed the EMalgorithm as in Table 2 to train the trigram LM.Here we used a 313 MB LM training corpus, whichcontains text news articles in 2000 and 2001.
Notethat in the following Sections, the pronunciationsof the added words were automatically labeled byexhaustively generating all possible pronunciationsfrom all component characters?
canonical pronun-ciations.4.2 ASR Character Accuracy ResultsA set of broadcast news corpus collected from aChinese radio station from January to September,2001 was used as the speech corpus.
It contained10K utterances.
We separated these utterances intotwo parts randomly: 5K as the adaptation corpusand 5K as the testing set.
We show the ASR char-acter accuracy results after lexicon adaptation bythe proposed approach in Table 3.LAICA-1 LAICA-2A D A+D A D A+DBaseline +1743 -1679 +1743 +409 -112 +314-1679 -8879.28 80.48 79.31 80.98 80.58 79.33 81.21Table 3: ASR character accuracies for the baselineand the proposed LAICA approach.
Two iterationsare performed, each with three versions.
A: onlyadd new words, D: only delete words and A+D: si-multaneously add and delete words.
+ and - meansthe number of words added and deleted, respec-tively.For the proposed LAICA approach, we showthe results for one (LAICA-1) and two (LAICA-2) iterations respectively, each of which has threedifferent versions: (A) only add new words intothe current lexicon, (D) only delete words, (A+D)simultaneously add and delete words.
The num-ber of added or deleted words are also included inTable 3.There are some interesting observations.
First,we see that deletion of current words brought much759less benefits than adding new words.
We try to givesome explanations.
Deleting existing words in thelexicon actually is a passive assistance for recog-nizing reference characters correctly.
Of coursewe eliminate some strong competitive charactersin this way but we can not guarantee that refer-ence characters will then have high enough PPto be ranked first in its own cluster.
Adding newwords into the lexicon, on the other hand, offersexplicit reinforcement in PP of the reference char-acters.
Such reinforcement offers the main positiveboosting for the PP of reference characters.
Theseboosted characters are under some specific con-texts which normally correspond to OOV wordsand sometimes in-vocabulary (IV) words that arehard to be recognized.From the model training aspect, adding newwords gives the maximum-likelihood flavor whiledeleting existing words provides discriminant abil-ity.
It has been shown that discriminative train-ing does not necessarily outperform maximum-likelihood training until we have enough trainingdata (Ng and Jordan, 2001).
So it is possible thatdiscriminatively trained model performs worse thanthat trained by maximum likelihood.
In our case,adding and deleting words seem to complimenteach other well.
This is an encouraging result.Another good property is that the proposed ap-proach converged quickly.
The number of words tobe added or deleted dropped significantly in the sec-ond iteration, compared to the first one.
Generallythe fewer words to be changed the fewer recogni-tion improvement can be expected.
Actually wehave tried the third iteration and simply obtaineddozens of words to be added and no words to bedeleted, which resulted in negligible changes inASR recognition accuracy.4.3 Comparison with other LexiconAdaptation MethodsIn this section we compare our method with twoother traditionally used approaches: one is the PAT-tree-based as introduced in Section 4.1 and theother is based on mutual probability (Saon and Pad-manabhan, 2001), which is the geometrical averageof the direct and reverse bigram:PM (wi, wj) =?Pf (wj |wi)Pr(wi|wj),where the direct (Pf (?)
and reverse bigram (Pr(?
))can be estimated as:Pf (wj |wi) =P (Wt+1 = wj ,Wt = wi)P (Wt = wi),Pr(wj |wi) =P (Wt+1 = wj ,Wt = wi)P (Wt+1 = wj).PM (wi, wj) is used as a measure about whether tocombine wi and wj as a new word.
By properlysetting a threshold, we may iteratively combineexisting characters and/or words to produce the re-quired number of new words.
For both the PAT-tree-and mutual-information-based approaches, we usethe manual transcriptions of the development 5Kutterances to collect the required statistics and weextract 2159 and 2078 words respectively to matchthe number of added words by the proposed LAICAapproach after 2 iterations (without word deletion).The language model is also re-trained as describedin Section 3.4.
The results are shown in Table 4,where we also include the results of our approachwith 2 iterations and adding words only for refer-ence.PAT-treeMutualProbability LAICA-2(A)CharacterAccuracy 79.33 80.11 80.58Table 4: ASR character accuracies on the lexiconadapted by different approaches.From the results we observe that the PAT-tree-based approach did not give satisfying improve-ments while the mutual probability-based oneworked well.
This may be due to the sparse adap-tation data, which includes only 81K characters.PAT-tree-based approach relies on the frequencycount, and some terms which occur only once inthe adaptation data will not be extracted.
Mutualprobability-based approach, on the other hand, con-siders two simple criterion: the components of anew word occur often together and rarely in con-junction with other words (Saon and Padmanabhan,2001).
Compared with the proposed approach, bothPAT-tree and mutual probability do not consider thedecoding structure.Some new words are clearly good for humansense and definitely convey novel semantic infor-mation, but they can be useless for speech recogni-tion.
That is, character n-gram may handle thesewords equally well due to the low ambiguities withother words.
The proposed LAICA approach triesto focus on those new words which can not be han-dled well by simple character n-grams.
Moreover,the two methods discussed here do not offer pos-sible ways to delete current words, which can beconsidered as a further advantage of the proposedLAICA approach.7604.4 Application: Character-based SpokenDocument Indexing and RetrievalPan et al (2007) recently proposed a new Subword-based Confusion Network (S-CN) indexing struc-ture for SDR, which significantly outperformsword-based methods for IV or OOV queries.
Herewe apply S-CN structure to investigate the effec-tiveness of improved character accuracy for SDR.Here we choose characters as the subword units,and then the S-CN structure is exactly the same asCCN, which was introduced in Section 3.2.For the SDR back-end corpus, the same 5K testutterances as used for the ASR experiment in Sec-tion 4.2 were used.
The previously mentioned lexi-con adaptation approaches and corresponding lan-guage models were used in the same speech recog-nizer for the spoken document indexing.
We auto-matically choose 139 words and terms as queriesaccording to the frequency (at least six times in the5K utterances).
The SDR performance is evaluatedby mean average precision (MAP) calculated bythe trec eval1 package.
The results are shownin Table 5.CharacterAccuracy MAPBaseline 79.28 0.8145PAT-tree 79.33 0.8203MutualProbability 80.11 0.8378LAICA-2(A+D) 81.21 0.8628Table 5: ASR character accuracies and SDR MAPperformances under S-CN structure.From the results, we see that generally theincreasing of character recognition accuracy im-proves the SDR MAP performance.
This seemstrivial but we have to note the relative improve-ments.
Actually the transformation ratios from therelative increased character accuracy to the relativeincreased MAP for the three lexicon adaptation ap-proaches are different.
A key factor making theproposed LAICA approach advantageous is thatwe try to extensively raise the incorrectly recog-nized character posterior probabilities, by meansof adding effective OOV words and deleting am-biguous words.
Actually S-CN is relying on thecharacter posterior probability for indexing, whichis consistent with our criterion and makes our ap-proach beneficial.
The degree of the raise of char-acter posterior probabilities can be visualized moreclearly in the following experiment.1http://trec.nist.gov/4.5 Further Investigation: the ImprovedRank in Character-based ConfusionNetworksIn this experiment, we have the same setup as inSection 4.2.
After decoding, we have character-based confusion networks (CCNs) for each testutterance.
Rather than taking the top ranked char-acters in each cluster as the recognition result, weinvestigate the ranks of the reference characters inthese clusters.
This can be achieved by the samealignment as we did in Section 3.3.
The results areshown in Table 6.# of rankedreferencecharactersAverageRankbaseline 70993 1.92PAT-tree 71038 1.89MutualProbability71054 1.81LAICA-2(A+D) 71083 1.67Table 6: Average ranks of reference characters inthe confusion networks constructed by differentlexicons and corresponding language modelsIn Table 6 we only evaluate ranks on those ref-erence characters that can be found in its corre-sponding confusion network cluster (case (1) and(2) as described in Section 3.3).
The number ofthose evaluated reference characters depends onthe actual CCN and is also included in the results.Generally, over 93% of reference characters are in-cluded (the total number is 75541).
Such ranks arecritical for lattice-based spoken document indexingapproaches such as S-CN since they directly affectretrieval precision.
The advantage of the proposedLAICA approach is clear.
The results here providea more objective point of view since SDR evalua-tion is inevitably effected by the selected queries.5 Conclusion and Future WorkCharacters together is an interesting and distinctlanguage unit for Chinese.
They can be simultane-ously viewed as words and subwords, which offera special means for OOV handling.
While relyingonly on characters gives moderate performances inASR, properly augmenting new words significantlyincreases the accuracy.
An interesting questionwould then be how to choose words to augment.Here we formulate the problem as an adaptationone and try to find the best way to alter the current761lexicon for improved character accuracy.This is a new perspective for lexicon adaptation.Instead of identifying OOV words from adaptationcorpus to reduce OOV rate, we try to pick out wordfragments hidden in the adaptation corpus that helpASR.
Furthermore, we delete some existing wordswhich may result in ambiguities.
Since we directlymatch our criterion with that in decoding, the pro-posed approach is expected to have more consistentimprovements than perplexity based criterions.Characters also play an important role in spokendocument retrieval.
This extends the applicabilityof the proposed approach and we found that theS-CN structure proposed by Pan et al for spokendocument indexing fitted well with the proposedLAICA approach.However, there still remain lots to be improved.For example, considering Equation 3, the languagemodel score and the summation constraint are notindependent.
After we alter the lexicon, the LM isdifferent accordingly and there is no guarantee thatthe obtained posterior probabilities for those incor-rectly recognized characters would be increased.We increased the path alternatives for those refer-ence characters but this can not guarantee to in-crease total path probability mass.
This can beamended by involving the discriminative languagemodel adaptation in the iteration, which results ina unified language model and lexicon adaptationframework.
This can be our future work.
Moreover,the same procedure can be used in the construction.That is, beginning with only characters in the lexi-con and using the training data to alter the currentlexicon in each iteration.
This is also an interestingdirection.ReferencesMaximilian Bisani and Hermann Ney.
2005.
Open vo-cabulary speech recognition with flat hybrid models.In Interspeech, pages 725?728.Keh-Jiann Chen and Wei-Yun Ma.
2002.
Unknownword extraction for chinese documents.
In COLING,pages 169?175.Berlin Chen, Jen-Wei Kuo, and Wen-Hung Tsai.
2004.Lightly supervised and data-driven approaches tomandarin broadcast news transcription.
In ICASSP,pages 777?780.Lee-Feng Chien.
1997.
Pat-tree-based keyword ex-traction for Chinese information retrieval.
In SIGIR,pages 50?58.Sabine Deligne and Yoshinori Sagisaka.
2000.
Sta-tistical language modeling with a class-based n-multigram model.
Comp.
Speech and Lang.,14(3):261?279.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
Journal of the Royal Statistics Soci-ety, 39(1):1?38.Marcello Federico and Nicola Bertoldi.
2004.
Broad-cast news LM adaptation over time.
Comp.
SpeechLang., 18:417?435.Marcello Federico.
1999.
Efficient language modeladaptation through MDI estimation.
In Intersspech,pages 1583?1586.Yi-Sheng Fu, Yi-Cheng Pan, and Lin-Shan Lee.2006.
Improved large vocabulary continuous Chi-nese speech recognition by character-based consen-sus networks.
In ISCSLP, pages 422?434.Pascale Fung.
1998.
Extracting key terms from chi-nese and japanese texts.
Computer Processing ofOriental Languages, 12(1):99?121.Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu Lee.
2002.
Toward a unified approach to statis-tical language modeling for Chinese.
ACM Trans-action on Asian Language Information Processing,1(1):3?33.Jianfeng Gao, Mu Li, Andi Wu, and Chang-NingHuang.
2004.
Chinese word segmentation: A prag-matic approach.
In MSR-TR-2004-123.Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja, and Janne Pylkko?nen.2005.
Unlimited vocabulary speech recognitionwith morph language models applied to Finnish.Comp.
Speech Lang.Mei-Yuh Hwang, Xin Lei, Wen Wang, and TakahiroShinozaki.
2006.
Investigation on mandarinbroadcast news speech recognition.
In Interspeech-ICSLP, pages 1233?1236.Bing-Hwang Juang, Wu Chou, and Chin-Hui Lee.1997.
Minimum classification error rate methods forspeech recognition.
IEEE Trans.
Speech Audio Pro-cess., 5(3):257?265.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: Word er-ror minimization and other applications of confusionnetworks.
Comp.
Speech Lang., 14(2):373?400.Andrew Y. Ng and Michael I. Jordan.
2001.
Ondiscriminative vs. generative classifiers: A compar-ison of logistic regression and naive bayes.
In Ad-vances in Neural Information Processing Systems(14), pages 841?848.762Yi-Cheng Pan, Hung-Lin Chang, and Lin-Shan Lee.2007.
Analytical comparison between position spe-cific posterior lattices and confusion networks basedon words and subword units for spoken documentindexing.
In ASRU.Yao Qian, Frank K. Soong, and Tan Lee.
2008.
Tone-enhanced generalized character posterior probabil-ity (GCPP) for Cantonese LVCSR.
Comp.
SpeechLang., 22(4):360?373.Ronald Rosenfeld.
2000.
Two decades of statisticallanguage modeling: Where do we go from here?Proceeding of IEEE, 88(8):1270?1278.George Saon and Mukund Padmanabhan.
2001.
Data-driven approach to designing compound words forcontinuous speech recognition.
IEEE Trans.
Speechand Audio Process., 9(4):327?332, May.Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and Her-mann Ney.
2001.
Confidence measures for largevocabulary continuous speech recognition.
IEEETrans.
Speech Audio Process., 9(3):288?298, Mar.Pak-kwong Wong and Chorkin Chan.
1996.
Chineseword segmentation based on maximum matchingand word binding force.
In Proceedings of the 16thInternational Conference on Computational Linguis-tic, pages 200?203.Kae-Cherng Yang, Tai-Hsuan Ho, Lee-Feng Chien, andLin-Shan Lee.
1998.
Statistics-based segment pat-tern lexicon: A new direction for chinese languagemodeling.
In ICASSP, pages 169?172.763
