Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2?11,Dublin, Ireland, August 24 2014.Feature Selection for Highly Skewed Sentiment Analysis TasksCan LiuIndiana UniversityBloomington, IN, USAliucan@indiana.eduSandra K?ublerIndiana UniversityBloomington, IN, USAskuebler@indiana.eduNing YuUniversity of KentuckyLexington, KY, USAning.yu@uky.eduAbstractSentiment analysis generally uses large feature sets based on a bag-of-words approach, whichresults in a situation where individual features are not very informative.
In addition, many datasets tend to be heavily skewed.
We approach this combination of challenges by investigatingfeature selection in order to reduce the large number of features to those that are discriminative.We examine the performance of five feature selection methods on two sentiment analysis datasets from different domains, each with different ratios of class imbalance.Our finding shows that feature selection is capable of improving the classification accuracy onlyin balanced or slightly skewed situations.
However, it is difficult to mitigate high skewing ratios.We also conclude that there does not exist a single method that performs best across data sets andskewing ratios.
However we found that TF ?
IDF2can help in identifying the minority classeven in highly imbalanced cases.1 IntroductionIn recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008;Bollen et al., 2011; Liu, 2012).
Sentiment analysis is concerned with extracting opinions or emotionsfrom text, especially user generated web content.
Specific tasks include monitoring mood and emotion;differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinionstrength; and identifying other opinion properties.
At this point, two major approaches exists: lexiconand machine learning based.
The lexicon-based approach uses high quality, often manually generatedfeatures.
The machine learning-based approach uses automatically generated feature sets, which are fromvarious sources of evidence (e.g., part-of-speech, n-grams, emoticons) in order to capture the nuances ofsentiment.
This means that a large set of features is extracted, out of which only a small subset may begood indicators for the sentiment.One major problem associated with sentiment analysis of web content is that for many topics, thesedata sets tend to be highly imbalanced.
There is a general trend that users are willing to submit positivereviews, but they are much more hesitant to submit reviews in the medium to low ranges.
For example,for the YouTube data set that we will use, we collected comments for YouTube videos from the comedycategory, along with their ratings.
In this data set, more than 3/4 of all ratings consist of the highest ratingof 5.
For other types of user generated content, they opposite may be true.Heavy skewing in data sets is challenging for standard classification algorithms.
Therefore, the datasets generally used for research on sentiment analysis are balanced.
Researchers either generate balanceddata sets during data collection, by sampling a certain number of positive and negative reviews, or by se-lecting a balanced subset for certain experiments.
Examples for a balanced data set are the movie reviewdata set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011).
Using a balanced data setallows researchers to focus on finding robust methods and feature sets for the problem.
Particularly, themovie review data set has been used as a benchmark data set that allows for comparisons of various sen-timent analysis models.
For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), KummerThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2and Savoy (2012), O?Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed com-petitive feature selection methods evaluated on the movie review data set.
However, the generalizabilityof such feature selection methods to imbalanced data sets, which better represent real world situations,has not been investigated in much detail.
Forman (2003) provides an extensive study of feature selectionmethods for highly imbalanced data sets, but he uses document classification as task.This current paper investigates the robustness of three feature selection methods that Forman (2003)has shown to be successful, as well as two variants of TF ?
IDF .
The three methods are Odds-Ratio(OR), Information Gain (IG), and Binormal Separation (BNS).
BNS has been found to perform signif-icantly better than other methods in more highly skewed tasks (Forman, 2003).
The two variants ofTF ?
IDF differ in the data set used for calculating document frequency.
We investigate the behaviorof these methods on a subtask of sentiment analysis, namely the prediction of user ratings.
For this, wewill use data sets from two different domains in order to gain insight into whether or not these featureselection methods are robust across domains and across skewing ratios: One set consists of user reviewsfrom Epicurious1, an online community where recipes can be exchanged and reviewed, the other setconsists of user reviews of YouTube comedy videos.The remainder of this paper is organized as follows: In section 2, we explain the rationale for applyingfeature selection and introduce the feature selection methods that are examined in this paper.
Section3 introduces the experimental settings, including a description of the two data sets, data preprocessing,feature representation, and definition of the binary classification tasks.
In section 4, we present anddiscuss the results for the feature selection methods, and in section 5, we conclude.2 Feature Selection and Class SkewingIn a larger picture, feature selection is a method (applicable both in regression and classification prob-lems) to identify a subset of features to achieve various goals: 1) to reduce computational cost, 2) toavoid overfitting, 3) to avoid model failure, and 4) to handle skewed data sets for classification tasks.We concentrate on the last motivation, even though an improvement of efficiency and the reduction ofoverfitting are welcome side effects.
The feature selection methods studied in this paper have been usedin text classification as well, which is a more general but similar task using n-gram features.
However,since all measures are intended for binary classification problems, we reformulate the rating predictioninto a binary classification problem (see section 3.5).Feature selection methods can be divided into wrapper and filter methods.
Wrapper methods usethe classification outcome on a held-out data set to score feature subsets.
Standard wrapper methodsinclude forward selection, backward selection, and genetic algorithms.
Filter methods, in contrast, usean independent measure rather than the error rate on the held-out data.
This means that they can beapplied to larger feature sets, which may be unfeasible with wrapper methods.
Since sentiment analysisoften deals with high dimensional feature representation, we will concentrate on filter methods for ourfeature selection experiments.Previous research (e.g.
(Brank et al., 2002b; Forman, 2003)) has shown that Information Gain andOdds Ratio have been used successfully across different tasks and that Binormal Separation has goodrecall for the minority class under skewed class distributions.
So we will investigate them in this paper.Other filter methods are not investigated in this paper due to two main concerns: We exclude Chi-squared and Z-score, statistical tests because they require a certain sample size.
Our concern is thattheir estimation for rare words may not be accurate.
We also exclude Categorical Proportion Differenceand Probability Proportion Difference since they do not normalize over the sample of size of positiveand negative classes.
Thus, our concern is that they may not provide a fair estimate for features from askewed data sets.2.1 NotationFollowing Zheng et al.
(2004), feature selection methods can be divided into two groups: one-sided andtwo-sided measures.
One-sided measures assign a high score to positively-correlated features and a low1www.epicurious.com3score to negative features while two-sided measures prefer highly distinguishing features, independentof whether they are positively or negatively correlated.
Zheng et al.
(2004) note that the ratio of positiveand negative features affects precision and recall of the classification, especially for the minority class.For one-sided methods, we have control over this ratio by selecting a specified number of features oneeach side; for two-sided methods, however, we do not have this control.
In this paper, we will keep a 1:1ratio for one-sided methods.
For example, if we select 1 000 features, we select the 500 highest rankedfeatures for the positive class, and the 500 highest ranked features for the negative class.
When usingtwo-sided methods, the 1 000 highest ranked features are selected.For the discussion of the feature selection methods, we use the following notations:?
S: target or positive class.?
S: negative class.?
DS: The number of documents in class S.?
DS: The number of documents in class S.?
DSf: The number of documents in class S where feature f occurs.?
DSf: The number of documents in class S where feature f occurs.?
TSf: The number of times feature f occurs in class S.2.2 Feature Selection MethodsIn addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ?
IDF is included forcomparison purposes.
We define these measures for binary classification as shown below.Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unob-served random variable given an observed variable.
It is defined as the entropy of one random variableminus the conditional entropy of the observed variable.
Thus, IG is the reduced uncertainty of class Sgiven a feature f :IG = H(S)?H(S|f) =?f?{0,1}?S?
{0,1}P (f, S)logP (f, S)P (f)P (S)Brank et al.
(2002b) analyzed feature vector sparsity and concluded that IG prefers common featuresover extremely rare ones.
IG can be regarded as the weighted average of Mutual Information, and rarefeatures are penalized in the weighting.
Thus they are unlikely to be chosen (Li et al., 2009).
Forman(2003) observed that IG performs better when only few features (100-500) are used.
Both authors agreedthat IG has a high precision with respect to the minority class.Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the oddsof feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates thata feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated:OR = logP (f, S)(1?
P (f, S))P (f, S)(1?
P (f, S))Brank et al.
(2002b) showed that OR requires a high number of features to achieve a given featurevector sparsity because it prefers rare terms.
Features that occur in very few documents of class S anddo not occur in S have a small denominator, and thus a rather large OR value.4Bi-Normal Separation (BNS): BNS (Forman, 2003) is a two-sided measure that regards the proba-bility of feature f occurring in class S as the area under the normal distribution bell curve.
The wholearea under the bell curve corresponds to 1, and the area for a particular feature has a correspondingthreshold along the x-axis (ranging from negative infinite to positive infinite).
For a feature f , one canfind the threshold that corresponds to the probability of occurring in the positive class, and the thresholdcorresponding to the probability of occurring in S. BNS measures the separation in these two thresholds:BNS = |F?1(DSfDS)?
F?1(DSfDS)|where F?1is the inverse function of the standard normal cumulative probability distribution.
As we cansee, the F?1function exaggerates an input more dramatically when the input is close to 0 or 1 whichmeans that BNS perfers rare words.Term Frequency * Inverse Document Frequency (TF*IDF): TF ?
IDF was originally proposedfor information retrieval tasks, where it measures how representative a term is for the document in whichit occurs.
When TF ?
IDF is adopted for binary classification, we calculate the TF ?
IDF of a featurew.r.t.
the positive class (normalized) and the TF ?
IDF w.r.t.
the negative class (normalized).
We obtainthe absolute value of the difference of these two measures.
If a feature is equally important in bothclasses and thus would not contribute to classification, it receives a small value.
The larger the value,the more discriminative the feature.
We apply two variants of TF ?
IDF , depending on how IDF iscalculated:TF ?
IDF1= (0.5 +0.5?
TSfmaxi(TSfi))?
log(DS+ DSDSf)TF ?
IDF2= (0.5 +0.5?
TSfmaxi(TSfi))?
log(DSDSf)In the first variant, TF ?
IDF1, document frequency is based on the whole set of examples while inthe second variant, TF ?
IDF2, document frequency is based only on the class under consideration, S.3 Experimental Setup3.1 Data SetsEpicurious Data Set: We developed a web crawler to scrape user reviews for 10 146 recipes, publishedon the Epicurious website before and on April 02, 2013.
On the website, each recipe is assigned a ratingof 1 to 4 forks, including the intermediate values of 1.5, 2.5, and 3.5.
This is an accumulated rating overall user reviews.
(Reviews with ratings of 0 were excluded, they usually indicate that recipes have notreceived any ratings.)
We rounded down all the half ratings, e.g., 1.5 forks counts as 1 fork, based onthe observation that users are generous when rating recipes.
Our experiments classify each recipe byaggregating over all its reviews.
While a little more than half of the recipes received 1 to 10 reviews,there are recipes with more than 100 reviews.
To avoid an advantage for highly reviewed recipes, werandomly selected 10 reviews if a recipe has more than 10 reviews.
Recipes with less than 3 reviewswere eliminated since they do not provide enough information.
After these clean-up steps, the data sethas the distribution of ratings shown in table 1.YouTube Data Set: Using the Google YouTube Data API, we collected average user ratings and usercomments for a set of YouTube videos in the category Comedy.
Each video is rated from 1 to 5.
Thedistribution of ratings among all YouTube videos is very skewed, as illustrated in figure 1.
Most videosare rated highly; very few are rated poorly.
The 1% quantile is 1.0; the 6.5% quantile is 3.0; the 40%quantile is 4.75; the 50% quantile is 4.85; and the 77% quantile is 5.0.
We selected a set of 3 000 videos.Videos with less than 5 comments or with non-English comments are discarded.5rating no.1 fork 44 recipes2 forks 304 recipes3 forks 1416 recipes4 forks 1368 recipesTable 1: The distribution of ratings in the Epicurious data set.1 2 3 4 50.00.51.01.52.02.5Video RatingDensityFigure 1: Skewing in the YouTube data set.3.2 Data PreprocessingBefore feature extraction, basic preprocessing is conducted for both data sets individually.
For the Epi-curious data set, we perform stemming using the Porter Stemmer (Porter, 1980) to normalize words, andrare words (?
4 occurrences) are removed.
On the YouTube data set, we perform spelling correctionand normalization because the writing style is rather informal.
Our normalizer collapses repetitions intotheir original forms plus a suffix ?RPT?, thus retaining this potentially helpful clue for reviewer?s strongemotion without increasing the features due to creative spelling.
For example, ?loooooove?
is changedto ?loveRPT?
and?lolololol?
to ?lolRPT?.
The normalizer also replaces all emoticons by either?EMOP?for positive emoticons or ?EMON?
for negative emoticons.
Besides a standard English dictionary, wealso use the Urban Dictionary2since it has a better coverage of online abbreviations.We do not filter stop words for two reasons: 1) Stop words are domain dependent, and some En-glish stop words may be informative for sentiment analysis, and 2) uninformative words that are equallycommon in both classes will be excluded by feature selection if the method is successful.3.3 Feature representationSince our focus is on settings with high numbers of features, we use a bag-of-words approach, in whichevery word represents one feature, and its term frequency serves as its value.
Different feature weightingmethods, including binary weighting, term frequency, and TF?IDF have been adopted in past sentimentanalysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)).
(Pang et al., 2002) found thatsimply using binary feature weighting performed better than using more complicated weightings in atask of classifying positive and negative movie reviews.
However, movie reviews are relatively short, sothere may not be a large difference between binary features and others.
Topic classification usually usesterm frequency as feature weighting.
TF ?
IDF and variants were shown to perform better than binaryweighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010).Since our user rating prediction tasks aggregate all user comments into large documents and predictratings per recipe/YouTube video, term frequency tends to capture richer information than binary fea-2http://www.urbandictionary.com/6Epicurious YouTuberatio no.
NEG no.
POS ratio no.
NEG no.
POS1:8 348 2 784 1:10 56 5591:1.57 348 547 1:1.57 356 5591:1 348 348 1:1 559 559Table 2: Skewing ratios and sizes of positive and negative classes for both data sets.tures.
Thus, we use term frequency weighting for simplicity, not to deviate from the focus of featureselection methods.
Since there is a considerable variance in term frequency in the features, we normalizethe feature values to [0,1] to avoid large feature values from dominating the vector operations in classifieroptimization.For the Epicurious data, the whole feature set consists of 10 677 unigram features.
For YouTube, thefull feature set of features consists of 23 232 unigram features.
We evaluate the performance of featureselection methods starting at 500 features, at a step-size of 500.
For the Epicurious data, we include up to10 500 features.
For the YouTube data, we stop at 15 000 features due to prohibitively long classificationtimes.3.4 ClassifierThe classifier we use in this paper is Support Vector Machines (SVMs) in the implementation ofSVMlight(Joachims, 1999).
Because algorithm optimization is not the focus of this study, we use thedefault linear kernel and other default parameter values.
Classification results are evaluated by accuracyas well as precision and recall for individual classes.3.5 Binary ClassificationSince all feature selection methods we use in our experiments are defined under a binary classificationscenario, we need to redefine the rating prediction task.
For both data sets, this means, we group therecipes and videos into a positive and a negative class.
A baseline classifier predicts every instance as themajority class.
For both data sets, the majority class is positive.For the Epicurious data set, 1-fork and 2-fork recipes are grouped into the negative class (NEG), and3-fork and 4-fork recipes are grouped into the positive class (POS), yielding a data set of 348 NEG and2 784 POS recipes (skewing ratio: 1:8).
The different skewing ratios we use are shown in table 2.
2/3 ofthe data is used for training, and 1/3 for testing, with the split maintaining the class ratio.
Note that forthe less skewed settings, all NEG instances were kept while POS instances were sampled randomly.For the YouTube data set, we sample from all videos with rating 5 for the positive class and fromall videos with ratings between and including 1 and 3 for the negative class.
This yields 559 POS and559 NEG videos.
The different skewing ratios we use are shown in table 2.
7/8 of the data is used fortraining, and 1/8 for testing, with the split maintaining the class ratio.4 Results4.1 Results for the Epicurious Data SetThe results for the Epicurious data set with different skewing ratios are shown in figure 2.
The accuracyof the baseline is 50% for the 1:1 ratio, 61% for 1:1.57, and 88.9% for 1:8.The results show that once we use a high number of features, all the feature selection methods performthe same.
This point where they conflate is reached at around 4 000 features for the ratios of 1:1 and1:1.57.
There, accuracy reaches around 71%.
For the experiment with the highest skewing, this point isreached much later, at around 8 000 features.
For this setting, we also reach a higher accuracy of around89%, which is to be expected since we have a stronger majority class.
Note that once the conflation pointis reached, the accuracy also corresponds to the accuracy when using the full feature set.
This accuracyis always higher than that of the baseline.72000 4000 6000 8000 100000.660.680.700.720.740.76Number of Features ChosenAccuracy llll l ll l l l l l l l l l l l l l lll l llllll lEpicurious 1:1?
AccuracylllIGBNSORTFIDF1TFIDF2baseline = 0.52000 4000 6000 8000 100000.600.650.700.750.80Number of Features ChosenAccuracylllllll l l l l l l l l l l l l l lllllll ll l ll l l lEpicurious 1:1.57 ?
AccuracylllIGBNSORTFIDF1TFIDF2baseline2000 4000 6000 8000 100000.8800.8850.8900.8950.900Number of Features ChosenAccuracyl l l l l l l l l ll l l l l l l l l lll ll lll l llll l ll l lEpicurious 1:8 ?
AccuracylllIGBNSORTFIDF1TFIDF2baselineFigure 2: The results for the Epicurious data set.2000 4000 6000 8000 100000.00.20.40.60.81.0Number of Features ChosenPrecisionl l l l l l l l l l l l l l l l l l l l llllEpicurious 1:8 ?
Precision NEG classlllIGBNSORTFIDF1TFIDF22000 4000 6000 8000 100000.000.020.040.060.080.10Number of Features ChosenRecalll l l l l l l l l l l l l l l l l l l l llllEpicurious 1:8 ?
Recall NEG classlllIGBNSORTFIDF1TFIDF2Figure 3: Precision and recall for the negative cases in the Epicurious set, given a 1:8 skewing.The results also show that the most pronounced differences between feature selection methods occur inthe balanced data set.
In the set with the highest skewing, the differences are minor, and only TF ?IDF2improves over the baseline when using 1 000 features.Another surprising result is that TF ?IDF2, OR, and BNS have a tendency to fluctuate between higherand lower results than the setting using all features.
This means that it is difficult to find a good cut-offpoint for these methods.
TF ?
IDF1and IG show clear performance gains for the balanced setting, butthey also show more fluctuation in settings with higher skewing.From these results, we can conclude that for sentiment analysis tasks, feature selection is useful onlyin a balanced or slightly skewed cases if we are interested in accuracy.
However, a look at the precisionand recall given the highest skewing (see figure 3) shows that TF ?
IDF2in combination with a smallnumber of features is the only method that finds at least a few cases of the minority class.
Thus if agood performance on the minority class examples is more important than overall accuracy, TF ?
IDF2is a good choice.
One explanation is that TF ?
IDF2concentrates on one class and can thus ignore theotherwise overwhelming positive class completely.
TF ?
IDF1and OR have the lowest precision, andBNS fluctuates.
Where recall is concerned, TF ?
IDF1and IG reach the highest recall given a smallfeature set.4.2 Results for the YouTube Data SetThe results for the YouTube data set with different skewing ratios are shown in figure 4.
The accuracy ofthe baseline is 50% for the 1:1 ratio, 61.08% for 1:1.57, and 90.9% for 1:10.The results show that even though the YouTube data set is considerably smaller than the Epicuriousone, is does profit from larger numbers of selected features: For the balanced and the low skewing, there80 5000 10000 150000.500.550.600.650.700.75Number of Features ChosenAccuracyll l lll l ll l l l ll l l l l l l l l l l l l l l l ll ll l l llllll l ll l l l l ll ll lll l lll lll ll l l l l l llYoutube Equal ?
AccuracylllIGBNSORTFIDF1TFIDF2baseline0 5000 10000 150000.600.650.700.75Number of Features ChosenAccuracyll l l ll ll ll l l lll l l lll l ll l ll l l l lll ll l l l l ll l lll ll lll ll lll l l ll ll lll l l l l l l l l l l l l l lYoutube 1:1.57 ?
AccuracylllIGBNSORTFIDF1TFIDF2baseline0 5000 10000 150000.800.850.900.95Number of Features ChosenAccuracyl l l l l l l l l l l l l l l l l l l l l l l l l l l l l lllYoutube 1:10 ?
AccuracylllIGBNSORTFIDF1TFIDF2baselineFigure 4: The results for the YouTube set.0 5000 10000 150000.700.750.800.850.900.951.00Number of Features ChosenPrecisionlllllll ll l l l l l l l l ll l l lll l l l l l ll l l l l l l l ll l l l l l l ll ll llllll l l l l l l l l lYoutube 1:1.57 ?
Precision NEG classlllIGBNSORTFIDF1TFIDF20 5000 10000 150000.00.10.20.30.40.5Number of Features ChosenRecalll ll l ll ll ll l l lll l l lll l l l l l l l l l lll ll l l l l ll l lll ll lll ll lll l l l l l l l l l l l l l l l l l l l l l l lYoutube 1:1.57 ?
Recall NEG classlllIGBNSORTFIDF1TFIDF2Figure 5: Precision and recall for the negative cases in the YouTube set, given a 1:1.57 skewing.is no point at which the methods conflate.
The results for the highly skewed case show that no featureselection method is capable of finding cases of the minority class: all methods consistently identify onlyone instance of the negative class.
However, this may be a consequence of the small data set.
In terms ofaccuracy, we see that a combination of a small number of features with either IG or TF ?
IDF1providesthe best results.
For the YouTube data set, BNS also performs well but requires a larger set of features forreaching its highest accuracy.
We assume that this is the case because BNS has a tendency to prefer rarewords.
Note that we did not test for number of features greater than 15 000 because of the computationcost, but we can see that the performance curve for different feature selection methods tends to conflateto the point that represents the full feature set.If we look at the performance of different feature selection methods on identification of minority classinstances, we find that TF ?
IDF2again manages to increase recall in the highly skewed case, but thistime at the expense of precision.
For the 1:1.57 ratio, all methods reach a perfect precision when a highnumber of features is selected, see figure 5.
TF ?
IDF2is the only method that reaches this precisionwith small numbers of features, too.
However, this is not completely consistent.4.3 DiscussionIf we compare the performance curves across data sets and skewing ratios and aim for high accuracy,we see that there is no single feature selection method that is optimal in all situations.
Thus, we have toconclude that the choice of feature selection method is dependent on the task.
In fact, the performanceof a feature selection method could depend on many factors, such as the difficulty of the classificationtask, the preprocessing step, the feature generation decision, the data representation scheme (Brank etal., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes).9We have also shown on two different data sets, each with three skewing ratios, that it is difficult forfeature selection methods to mitigate the effect of highly skewed class distributions while we can stillimprove performance by using a reduced feature set for slightly skewed cases.
Thus, the higher the skew-ing of the data set is, the more difficult it is to find a feature selection method that has a positive effect,and parameters, such as feature set size, have to be optimized very carefully.
Thus, feature selection ismuch less effective in highly skewed user rating tasks than in document classification.However, if the task requires recall of the minority class, our experiments have shown that TF ?IDF2is able to increase this measure with a small feature set, even for highly imbalanced cases.5 Conclusion and Future WorkIn this paper, we investigated whether feature selection methods reported to be successful for documentclassification perform robustly in sentiment classification problems with a highly skewed class distribu-tion.
Our findings show that feature selection methods are most effective when the data sets are balancedor moderately skewed, while for highly imbalanced cases, we only saw an improvement in recall for theminority class.In the future, we will extend feature selection methods ?
originally defined for binary classificationscenarios ?
to handle multi-class classification problems.
A simple way of implementing this is be tobreak multi-class classification into several 1-vs.-all or 1-vs.-1 tasks, perform feature selection on thesebinary tasks and then aggregate them.
Another direction that we want to take is integrating more complexfeatures, such as parsing or semantic features, into the classification task to investigate how they influencefeature selection.
In addition, we will compare other approaches against feature selection for handlinghighly skewed data sets, including classification by rank, ensemble learning methods, and memory-based learning with a instance-specific weighting.
Finally, a more challenging task is to select featuresfor sentiment analysis problems where no annotation (feature selection under unsupervised learning) orfew annotations (feature selection under semi-supervised learning) are available.ReferencesBasant Agarwal and Namita Mittal.
2012.
Categorical probability proportion difference (CPPD): A feature selec-tion method for sentiment classification.
In Proceedings of the 2nd Workshop on Sentiment Analysis where AImeets Psychology (SAAIP), pages 17?26.Basant Agarwal and Namita Mittal.
2013.
Sentiment classification using rough set based hybrid feature selection.In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment & Social MediaAnalysis (WASSA), pages 115?119, Atlanta, GA.Johan Bollen, Huina Mao, and Xiao-Jun Zeng.
2011.
Twitter mood predicts the stock market.
Journal of Compu-tational Science, 2:1?8.Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic.
2002a.
An extensive empirical studyof feature selection metrics for text classification.
In Proceedings of the Third International Conference on DataMining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy.Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic.
2002b.
Feature selection usingLinear Support Vector Machines.
Technical Report MSR-TR-2002-63, Microsoft Research.George Forman.
2003.
An extensive empirical study of feature selection metrics for text classification.
Journal ofMachine Learning Research, 3:1289?1305.Thorsten Joachims.
1999.
Making large-scale SVM learning practical.
In B. Sch?olkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support Vector Learning.
MIT-Press.Olena Kummer and Jaques Savoy.
2012.
Feature selection in sentiment analysis.
In Proceeding of the Conf?erenceen Recherche d?Infomations et Applications (CORIA), pages 273?284, Bordeaux, France.Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren Huang.
2009.
A framework of feature selection methodsfor text categorization.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Language Processing of the AFNLP, pages 692?700, Suntec,Singapore.10Bing Liu.
2012.
Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technologies.Morgan & Claypool Publishers.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Technologies, pages 142?150, Portland, OR.Frederick Mosteller.
1968.
Association and estimation in contingency tables.
Journal of the American StatisticalAssociation, 63(321):1?28.Tim O?Keefe and Irena Koprinska.
2009.
Feature selection and weighting methods in sentiment analysis.
In Pro-ceedings of the 14th Australasian Document Computing Symposium (ADCS), pages 67?74, Sydney, Australia.Georgios Paltoglou and Mike Thelwall.
2010.
A study of information retrieval weighting schemes for sentimentanalysis.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages1386?1395, Uppsala, Sweden.Bo Pang and Lillian Lee.
2004.
A sentimental education: Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42nd Annual Meeting on Association for Computational Lin-guistics, Barcelona, Spain.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of the 2002 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 79?86, Philadelphia, PA.Martin Porter.
1980.
An algorithm for suffix stripping.
Program, 14(3):130?137.Zhaohui Zheng, Xiayun Wu, and Rohini Srihari.
2004.
Feature selection for text categorization on imbalanceddata.
ACM SIGKDD Explorations Newsletter, 6(1):80?89.11
