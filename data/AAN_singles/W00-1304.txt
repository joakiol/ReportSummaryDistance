Coaxing Confidences from an Old Friend:Probabilistic Classifications from Transformation Rule ListsRadu F lo r ian*  John  C.  Henderson  t Grace  Nga i**Department of Computer  ScienceJohns Hopkins UniversityBaltimore, MD 21218, USA{rf lorian,gyn}@cs.jhu.edutThe MITRE Corporat ion202 Bur l ington RoadBedford,  MA 01730, USAjhndrsn@mitre .orgAbst rac tTransformation-based l arning has been success-fully employed to solve many natural languageprocessing problems.
It has many positive fea-tures, but one drawback is that it does not provideestimates of class membership probabilities.In this paper, we present a novel method forobtaining class membership robabilities from atransformation-based rule list classifier.
Three ex-periments are presented which measure the model-ing accuracy and cross-entropy ofthe probabilisticclassifier on unseen data and the degree to whichthe output probabilities from the classifier can beused to estimate confidences in its classificationdecisions.The results of these experiments show that, forthe task of text chunking 1, the estimates producedby this technique are more informative than thosegenerated by a state-of-the-art decision tree.1 In t roduct ionIn natural language processing, a great amount ofwork has gone into the development of machinelearning algorithms which extract useful linguisticinformation from resources such as dictionaries,newswire feeds, manually annotated corpora andweb pages.
Most of the effective methods canbe roughly divided into rule-based and proba-bilistic algorithms.
In general, the rule-basedmethods have the advantage of capturing thenecessary information in a small and concise setof rules.
In part-of-speech tagging, for exam-ple, rule-based and probabilistic methods achievecomparable accuracies, but rule-based methodscapture the knowledge in a hundred or so simplerules, while the probabilistic methods have avery high--dimensional parameter space (millionsof parameters).One of the main advantages of probabilisticmethods, on the other hand, is that they include ameasure of uncertainty in their output.
This cantake the form of a probability distribution overpotential outputs, or it may be a ranked list ofIA11 the experiments are performed on text chnnklng.The technique presented is general-purpose, however, andcan be applied to many tasks for which transformation-based learning performs well, without changing the inter-rials of the learner.candidate outputs.
These uncertainty measuresare useful in situations where both the classifi-cation of an sample and the system's confidencein that classification are needed.
An example ofthis is a situation in an ensemble system whereensemble members disagree and a decision mustbe made about how to resolve the disagreement.A similar situation arises in pipeline systems, suchas a system which performs parsing on the outputof a probabilistic part-of-speech tagging.Transformation-based learning (TBL) (Brill,1995) is a successful rule-based machine learningalgorithm in natural language processing.
It hasbeen applied to a wide variety of tasks, includingpart of speech tagging (Roche and Schabes, 1995;Brill, 1995), noun phrase chvnklng (Ramshaw andMarcus, 1999), parsing (Brill, 1996; Vilain andDay, 1996), spelling correction (Mangu and Brill,1997), prepositional phrase attachment (Brill andResnik, 1994), dialog act tagging (Samuel etal., 1998), segmentation and message understand-ing (Day et al, 1997), often achieving state-of-the-art performance with a small and easily-understandable list of rules.In this paper, we describe a novel methodwhich enables a transformation-based classifier togenerate a probability distribution on the classlabels.
Application of the method allows thetransformation rule list to retain the robustness ofthe transformation-based algorithms, while bene-fitting from the advantages ofa probabilistic clas-sifter.
The usefulness of the resulting probabilitiesis demonstrated bycomparison with another state-of-the-art classifier, the C4.5 decision tree (Quin-lan, 1993).
The performance of our algorithmcompares favorably across many dimensions: itobtains better perplexity and cross-entropy; anactive learning algorithm using our system outper-forms a similar algorithm using decision trees; andfinally, our algorithm has better rejection curvesthan a similar decision tree.
Section 2 presents thetransformation based learning paradigm; Section3 describes the algorithm for construction of thedecision tree associated with the transformationbased list; Section 4 describes the experimentsin detail and Section 5 concludes the paper andoutlines the future work.262 Trans format ion  ru le  l i s t sThe central idea of transformation-based l arn-ing is to learn an ordered list of rules whichprogressively improve upon the current state ofthe training set.
An initial assignment is madebased on simple statistics, and then rules aregreedily learned to correct he mistakes, until nonet improvement can be made.These definitions and notation will be usedthroughout the paper:?
X denotes the sample space;?
C denotes the set of possible classifications ofthe samples;?
The state space is defined as 8 = X x C.?
7r will usually denote a predicate defined onX;?
A rule r is defined as a predicate - class label- time tuple, (~r,c,t), c E C,t E N, where t isthe learning iteration in which when the rulewas learned, its position in the list.?
A rule r = (~r, c, t) applies to a state (z, y) if7r(z) = true and c # y.Using a TBL framework to solve a problem as-sumes the existence of:?
An initial class assignment (mapping from Xto ,.9).
This can be as simple as the mostcommon class label in the training set, or itcan be the output from another classifier.?
A set of allowable templates for rules.
Thesetemplates determine the predicates the ruleswill test, and they have the biggest influenceover the behavior of the system.?
An objective function for learning.
Unlike inmany other learning algorithms, the objectivefunction for TBL will typically optimize theevaluation function.
An often-used method isthe difference in performance resulting fromapplying the rule.At the beginning of the learning phase, thetraining set is first given an initial class assign-ment.
The system then iteratively executes thefollowing steps:1.
Generate all productive rules.2.
For each rule:(a) Apply to a copy of the most recent stateof the training set.
(b) Score the result using the objective func-tion.3.
Select he rule with the best score.4.
Apply the rule to the current state of thetraining set, updating it to reflect his change.5.
Stop if the score is smaller than some pre-setthreshold T.6.
Repeat from Step 1.The system thus learns a list of rules in a greedyfashion, according to the objective function.
Whenno rule that improves the current state of thetraining set beyond the pre-set threshold canbe found, the training phase ends.
During theevaluation phase, the evaluation set is initializedwith the same initial class assignment.
Each ruleis then applied, in the order it was learned, to theevaluation set.
The final classification is the oneattained when all rules have been applied.3 Probab i l i ty  es t imat ion  w i tht rans format ion  ru le  l i s t sRule lists are infamous for making hard decisions,decisions which adhere entirely to one possibility,excluding all others.
These hard decisions areoften accurate and outperform other types ofclassifiers in terms of exact-match accuracy, butbecause they do not have an associated proba-bility, they give no hint as to when they mightfail.
In contrast, probabilistic systems make softdecisions by assigning a probability distributionover all possible classes.There are many applications where soft deci-sions prove useful.
In situations such as activelearning, where a small number of samples areselected for annotation, the probabilities can beused to determine which examples the classifierwas most unsure of, and hence should provide themost extra information.
A probabilistic systemcan also act as a filter for a more expensivesystem or a human expert when it is permittedto reject samples.
Soft decision-making is alsouseful when the system is one of the componentsin a larger decision-malting process, as is the casein speech recognition systems (Bald et al, 1989),or in an ensemble system like AdaBoost (Freundand Schapire, 1997).
There are many otherapplications in which a probabilistic lassifier isnecessary, and a non-probabHistic classifier cannotbe used instead.3.1 Estimation via conversion to decisiontreeThe method we propose to obtain probabilis-tic classifications from a transformation rule listinvolves dividing the samples into equivalenceclasses and computing distributions over eachequivalence class.
At any given point in time i,each sample z in the training set has an associatedstate si(z) = (z,~l).
Let R (z )  to be the set of rulesr~ that applies to the state el(z),R(z) = {ri ~ 7~Ir~ applies to si(z)}An  equivalence class consists of all the samplesz that have the same R(z).
Class probabilityassignments are then estimated using statisticscomputed on the equivalence classes.27An illustration of the conversion from a rulelist to a decision tree is shown below.
Table 1shows an example transformation rule list.
It isstraightforward to convert this rule list into a de-cision pylon (Bahl et al, 1989)~.
which can be usedto represent all the possible sequences of labelsassigned to a sample during the application of theTBL  algorithm.
The decision pylon associatedwith this particular rule list is displayed on the leftside of Figure 1.
The decision tree shown on theright side of Figure 1 is constructed such that thesamples stored in any leaf have the same class labelsequence as in the displayed decision pylon.
Inthe decision pylon, "no" answers go straight down;in the decision tree, "yes" answers take the rightbranch.
Note that a one rule in the transformationrule list can often correspond to more than onenode in the decision tree.Initial label = AI f  Q1 and label=A then  label+-BI f  Q2 and label=A then  labele-BI f  Q3 and label=B then  label~ATable I: Example of a Transformation Rule List.Figure 1: Converting the transformation rule listfrom Table 1 to a decision tree,The conversion from a transformation rule listto a decision tree is presented as a recursiveprocedure.
The set of samples in the training setis transformed to a set of states by applying theinitial class assignments.
A node n is created foreach of the initial class label assignments c and allstates labeled c are assigned to n.The following recursive procedure is invokedwith an initial "root" node, the complete set ofstates (from the corpus) and the whole sequenceof rules learned uring training:A lgor i thm:  Ru leL is tToDec is ionTree(RLTDT)Input :* A set/3 of N states ((Zl, Yl) --- (ZN, YN)) withlabels Yi E C;?
A set 7~ of M rules (ro,rl .
.
.rM) where ri =Do:1.
If 7~ is empty, the end of the rule list has beenreached.
Create a leaf node, n, and estimatethe probability class distribution based on thetrue classifications of the states in 13.
Returnn.2.
Let rj  = (Ir j ,yj , j )  be the lowest-indexed rulein 7~.
Remove it from 7~.3.
Split the data in/3 using the predicate 7rj andthe current hypothesis uch that samples onwhich 7rj returns true are on the right of thesplit:BL = {x E BlTrj(x ) = false}/3R = {x E/31 j(x) = true}4.
If IBLI > K and IBRI > K,  the split isacceptable:(a) Create a new internal node, n;(b) Set the question: q(n) = 7rj;(c) Create the left child of n using a recursivecall to RLTDT(BL, 7~);(d) Create the right child of n using a recur-sive call to RLTDT(BR, 7~);(e) Return node n.Otherwise, no split is performed using rj.Repeat from Step 1.The parameter K is a constant that determines theminimum weight that a leaf is permitted to have,effectively pruning the tree during construction.In all the experiments, K was set to 5.3.2 Fur ther  growth  o f  the  decis ion t reeWhen a rule list is converted into a decision tree,there are often leaves that are inordinately heavybecause they contain a large number of samples.Examples of such leaves are those containingsamples which were never transformed by anyof the rules in the rule list.
These populationsexist either because they could not be split upduring the rule list learning without incurring anet penalty, or because any rule that acts on themhas an objective function score of less than thethreshold T. This is sub-optimal for estimationbecause when a large portion of the corpus fallsinto the same equivalence class, the distributionassigned to it reflects only the mean of thosesamples.
The undesirable consequence is that allof those samples are given the same probabilitydistribution.To ameliorate this problem, those samples arepartitioned into smaller equivalence classes byfurther growing the decision tree.
Since a decisiontree does not place all the samples with the samecurrent label into a single equivalence class, it doesnot get stuck in the same situation as a rule listm in which no change in the current state ofcorpus can be made without incurring a net lossin performance.28Continuing to grow the decision tree that wasconverted from a rule list can be viewed fromanother angle.
A highly accurate prefix treefor the final decision tree is created by tyingquestions together during the first phase of thegrowth process (TBL).
Unlike traditional decisiontrees which select splitting questions for a nodeby looking only at the samples contained in thelocal node, this decision tree selects questions bylooking at samples contained in all nodes on thefrontier whose paths have a suM< in common.
Anillustration of this phenomenon can be seen inFigure 1, where the choice to split on Question3 was made from samples which tested falseon the predicate of Question 1, together withsamples which tested false on the predicate ofQuestion 2.
The result of this is that questionsare chosen based on a much larger population thanin standard decision tree growth, and thereforehave a much greater chance of being useful andgeneralizable.
This alleviates the problem of over-partitioning of data, which is a widely-recognizedconcern during decision tree growth.The decision tree obtained from this conversioncan be grown further.
When the rule list 7~ isexhausted at Step 1, instead of creating a leafnode, continue splitting the samples contained inthe node with a decision tree induction algorithm.The splitting criterion used in the experiments isthe information gain measure.4 Exper imentsThree experiments that demonstrate the effec-tiveness and appropriateness of our probabilityestimates are presented in this section.
Theexperiments are performed on text chunking, asubproblem ofsyntactic parsing.
Unlike full pars-ing, the sentences are divided into non-overlappingphrases, where each word belongs to the lowestparse constituent that dominates it.The data used in all of these experiments ithe CoNLL-2000 phrase chunking corpus (CoNLL,2000).
The corpus consists of sections 15-18 andsection 20 of the Penn Treebank (Marcus et al,1993), and is pre-divided into a 8936-sentence(211727 tokens) training set and a 2012-sentence(47377 tokens) test set.
The chunk tags arederived from the parse tree constituents, and thepart-of-speech tags were generated by the Brilltagger (Brill, 1995).As was noted by Ramshaw & Marcus (1999),text chunking can be mapped to a tagging task,where each word is tagged with a chunk tagrepresenting the phrase that it belongs to.
Anexample sentence from the corpus is shown inTable 4.
As a contrasting system, our resultsare compared with those produced by a C4.5decision tree system (henceforth C4.5).
Thereason for using C4.5 is twofold: firstly, it is awidely-used algorithm which achieves state-.of-the-art performance on a broad variety of tasks; andWordA.P.Greencurrentlyhas2,664,098sharesoutstandingPOS tagNNPNNPRBVBZCDNNSJJChunk TagB-NPI-NPB-ADVPB-VPB-NPI-NPB-ADJPOTable 2: Example of a sentence with chunk tagssecondly, it belongs to the same class of classifiersas our converted transformation-based rule list(henceforth TBLDT).To perform a fair evaluation, extra care wastaken to ensure that both C4.5 and TBLDTexplore as similar a sample space as possible.
Thesystems were allowed to consult the word, thepart-of-speech, and the chunk tag of all exampleswithin a window of 5 positions (2 words on eitherside) of each target example.
2 Since multiplefeatures covering the entire vocabulary of thetraining set would be too large a space for C4.5to deal with, in all of experiments where TBLDTis directly compared with C4.5, the word typesthat both systems can include in their predicatesare restricted to the most "ambiguous" 100 wordsin the training set, as measured by the number ofchunk tag types that are assigned to them.
Theinitial prediction was made for both systems usinga class assignment based solely on the part-of-speech tag of the word.Considering chunk tags within a contextual win-dow of the target word raises a problem with C4.5.A decision tree generally trains on independentsamples and does not take into account changesof any features in the context.
In our case, thesamples are dependent; the classification ofsamplei is a feature for sample i + 1, which means thatchanging the classification for sample i affectsthe context of sample i + 1.
To address thisproblem, the C4.5 systems are trained with thecorrect chlmk~ in the left context.
When thesystem is used for classification, input is processedin a left-to-right manner;and the output of thesystem is fed forward to be used as featuresin the left context of following samples.
SinceC4.5 generates probabilities for each classificationdecision, they can be redirected into the input forthe next position.
Providing the decision treewiththis confidence information effectively allows it toperform a limited search over the entire sentence.C4.5 does have one advantage over TBLDT,however.
A decision tree can be trained using thesubsetting feature, where questions asked are ofthe form: "does feature f belong to the set FT'.This is not something that a TBL can do readily,2The TBL templates are similar to those used inl~am.~haw and Marcus (1999).29but since the objective is in comparing TBLDT toanother state-of-the-art system, this feature wasenabled.4.1 Evaluation MeasuresThe most commonly used measure for evaluatingtagging tasks is tag accuracy, lit is defined asAccuracy = # of correctly tagged examplesof examplesIn syntactic parsing, though, since the task isto identify the phrasal components, it is moreappropriate o measure the precision and recall:# of correct proposed phrasesPrecision =# of proposed phrases# of correct proposed phrasesRecall = # of correct phrasesTo facilitate the comparison of systems with dif-ferent precision and recall, the F-measure metricis computed as a weighted harmonic mean ofprecision and recall:(82 + 1) ?
Precision x Recall=82 x Precision + RecallThe ~ parameter is used to give more weight toprecision or recall, as the task at hand requires.In all our experiments, ~ is set to 1, giving equalweight o precision and recall.The reported performances are all measuredwith the evaluation tool provided with the CoNLLcorpus (CoNLL, 2000).4.2 Active LearningTo demonstrate the usefulness of obtaining proba-bilities from a transformation rule list, this sectiondescribes an application which utilizes these prob-abilities, and compare the resulting performanceof the system with that achieved by C4.5.Natural language processing has traditionallyrequired large amounts of annotated ata fromwhich to extract linguistic properties.
However,not all data is created equal: a normal distribu-tion of aunotated ata contains much redundantinformation.
Seung et al (1992) and Freund etal.
(1997) proposed a theoretical ctive learningapproach, where samples are intelligently selectedfor annotation.
By eliminating redundant infor-mation, the same performance can be achievedwhile using fewer resources.
Empirically, activelearning has been applied to various NLP taskssuch as text categorization (Lewis and Gale, 1994;Lewis and Catlett, 1994; Liere and Tadepalli,1997), part-of-speech tagging (Dagan and Engel-son, 1995; Engelson and Dagan, 1996), and basenoun phrase chunbiug (Ngai and Yarowsky, 2000),resulting in significantly large reductions in thequantity of data needed to achieve comparableperformance.This section presents two experimental resultswhich show the effectiveness of the probabilitiesgenerated by the TBLDT.
The first experimentcompares the performance achieved by the activelearning algorithm using TBLDT with the perfor-mance obtained by selecting samples equentiallyfrom the training set.
The second experimentcompares the performances achieved by TBLDTand C4.5 training on samples elected by activelearning.The following describes the active learning algo-rithm used in the experiments:1.
Label an initial T1 sentences ofthe corpus;2.
Use the machine learning algorithm (G4.5 orTBLDT) to obtain chunk probabilities on therest of the training data;3.
Choose T2 samples from the rest of the train-ing set, specifically the samples that optimizean evaluation function f ,  based on the classdistribution probability of each sample;4.
Add the samples, including their "true" classi-fication 3 to the training pool and retrain thesystem;5.
If a desired number of samples is reached,stop, otherwise repeat from Step 2.The evaluation function f that was used in ourexperiments is:where H(UIS, i ) is the entropy of the chllnkprobability distribution associated with the wordindex i in sentence S.Figure 2 displays the performance (F-measureand chllnk accuracy) of a TBLDT system trainedon samples elected by active learning and thesame system trained on samples elected sequen-tially from the corpus versus the number of wordsin the annotated tralniug set.
At each step ofthe iteration, the active learning-trained TBLDTsystem achieves a higher accuracy/F-measure, or,conversely, is able to obtain the same performancelevel with less training data.
Overall, our systemcan yield the same performance as the sequentialsystem with 45% less data, a significant reductionin the annotation effort.Figure 3 shows a comparison between two activelearning experiments: one using TBLDT and theother using C4.5.
4 For completeness, a sequentialrun using C4.5 is also presented.
Even thoughC4.5 examines a larger space than TBLDT bySThe true (reference or gold standard) classification isavailable in this experiment.
In an annotation situation,the samples are sent o human annotators for labeling.4As mentioned arlier, both the TBLDT and C4.5 werelimited to the same 100 most ambiguous words in thecorpus to ensure comparability.3O84AL?TBLDT ' ~ ' ' 'i i i I i(a) F-measure vs. number of words in trrdniug setOilAL* 'mI .~ ' - -  .
.
.
.i I(b) Chunk Accuracy vs. number of words in trainingsetFigure 2: Performance of the TBLDT system versus sequential choice.8786| -81...=2- ', .
r .~ .
.
.
r  - - I~ ' ' ' '~ ' '~"\ [~"iI i L i(a) F-measure vs. number of words in tr~inln s set31AL?
'nBL (I0?
~ )  ~ 'J i i ~ i i(b) Accuracy vs. number of words in training setFigure 3: Performance of the TBLDT system versus the DT systemutilizing the feature subset predicates, TBLDTstill performs better.
The difference in accuracy at26200 words (at the end of the active learning runfor TBLDT) is statistically significant at a 0.0003level.As a final remark on this experiment, note thatat an annotation level of 19000 words, the fullylexicalized TBLDT outperformed the C4.5 systemby making 15% fewer errors.4.3 Re jec t ion  curvesIt is often very useful for a classifier to be ableto offer confidence scores associated with its deci-sions.
Confidence scores are associated with theprobability P(C(z) correct\[z) where C(z) is theclassification of sample z.
These scores can beused in real-life problems to reject samples thatthe the classifier is not sure about, in which casea better observation, or a human decision, mightbe requested.
The performance of the classifieris then evaluated on the samples that were notrejected.
This experiment framework is well-established in machine learning and optimizationresearch (Dietterich and Bakiri, 1995; Priebe etal., 1999).Since non-probabilistic classifiers do not offerany insights into how sure they are about aparticular classification, it is not easy to obtainconfidence scores from them.
A probabilisticclassifier, in contrast, offers information about theclass probability distribution of a given sample.Two measures that can be used in generatingconfidence scores are proposed in this section.The first measure, the entropy H of the classprobability distribution of a sample z, C(z) ={p(CllZ),p(c2\[z)...p(cklZ)}, i s  a measure  of theuncertainty in the distribution:kHCCCz)) = - I=) log2 pC Iz)i=IThe higher the entropy of the distribution ofclass probability estimates, the more uncertain the0.990.980.97g~0.~~0.950.940.93///.._.-.--f"/ / C4.5 (hard d=fisions)__ iI / / / .
-  ..... ~ % _ ..
~.
;':" \]I I I I I I I I I0J O2 O3 0.4 0.5 O.6 0,7 O.8 O.9 Zl~c~t of rej~xaed ~(a)  Subcorpus  (batch)  re jec t ion0~ 1 i i 10.985 "0~8O.9750970.9650.96 - TBL-DT0.955095 - C4_5 (soft decisi0.945 ..- ....0.94 .-.:.-.-.r.-.
r.~---.':.'.':.'.
"0.935 \[0 0.2 0.4 0.6 ~8 1Probability of th~ most lflmly tag(b)  Thresho ld  (on l ine)  re jec t ionFigure 4: Rejection curves.classifier is of its classification.
The samples e-lected for rejection are chosen by sorting the datausing the entropies of the estimated probabilities,and then selecting the ones with highest entropies.The resulting curve is a measure of the correlationbetween the true probability distribution and theone given by the classifier.Figure 4(a) shows the rejection curves for theTBLDT system and two C4.5 decision trees - onewhich receives a probability distribution as input("soft" decisions on the left context) , and onewhich receives classifications ("hard" decisions onall fields).
At the left of the curve, no samplesare rejected; at the right side, only the samplesabout which the classifiers were most certain arekept (the samples with minimum entropy).
Notethat the y-values on the right side of the curve arebased on less data, effectively introducing widervariance in the curve as it moves right.As shown in Figure 4(a), the C4.5 classifierthat has access to the left context chunk tagprobability distributions behaves better than theother C4.5 system, because this information aboutthe surrounding context allows it to effectivelyperform a shallow search of the classificationspace.
The TBLDT system, which also receivesa probability distribution on the chunk tags inthe left context, clearly outperforms both C4.5systems at all rejection levels.The second proposed measure is based on theprobability of the most likely tag.
The assumptionhere is that this probability is representative ofhow certain the system is about the classifica-tion.
The samples are put in bins based onthe probability of the most likely chnnk tag, andaccuracies are computed for each bin (these binsare cumulative, meaning that a sample will beincluded in all the bins that have a lower thresholdthan the probability of its most likely chnnl?tag).
At each accuracy level, a sample will berejected if the probability of its most likely chnn~Cross EntropyTBLDT 1.2944 0.2580DT+probs 1.4150 0.3471DT 1.4568 0.3763Table 3: Cross entropy and perplexities for twoC4.5 systems and the TBLDT systemis below the accuracy level.
The resulting curveis a measure of the correlation between the truedistribution probability and the probability of themost likely chunk tag, i.e.
how appropriate thoseprobabilities are as confidence measures.
Unlikethe first measure mentioned before, a thresholdobtained using this measure can be used in anonline manner to identify the samples of whoseclassification the system is confident.Figure 4(b) displays the rejection curve forthe second measure and the same three systems.TBLDT again outperforms both C4.5 systems, atall levels of confidence.In summary, the TBLDT system outperformsboth C4.5 systems presented, resulting in fewer re-jections for the same performance, or, conversely,better performance at the same rejection rate.4.4 Perp lex i ty  and  Cross Ent ropyCross entropy is a goodness measure for probabil-ity estimates that takes into account he accuracyof the estimates as well as the classification accu-racy of the system.
It measures the performanceof a system trained on a set of samples distributedaccording to the probability distribution p whentested on a set following a probability distributionq.
More specifically, we utilize conditional crossentropy, which is defined asn (C lX)  = - q (=) -  q(cl=) ?
log2 pC@:)zEX ?ECwhere X is the set of examples and C is the set ofchnnlr tags, q is the probabil i ty distribution on the32ChunkTypeA c c u r a c y(%)Precisionl Recall(%) I (%)Overall 95.23 92.02 92.50ADJP  - 75.69 68.95ADVP - 80.88 78.64CONJP  - 40.00 44.44INTJ - 50.00 50.00LST - 0.00 0.00NP  - 92.18 92.72PP  95.89 97.90PRT  - 67.80 75.47SBAR 88.71 82.24VP 92.00 92.87Fi92.26!72.1679.7442.1150.000.0092.4596.8871.4385.3592.44Table 4: Performance of TBLDT on the CoNLLTest Settest document and p is the probability distributionon the train corpus.The cross entropy metric fails if any outcome isgiven zero probability by the estimator.
To avoidthis problem, estimators are "smoothed", ensuringthat novel events receive non-zero probabilities.A very simple smoothing technique (interpolationwith a constant) was used for all of these systems.A closely related measure is perplexity, definedasP = 2~(cl x)The cross entropy and perplexity results for thevarious estimation schemes are presented in Table?
3.
The TBLDT outperforms both C4.5 systems,obtaining better cross-entropy and chunk tag per-plexity.
This shows that the overall probabilitydistribution obtained from the TBLDT systembetter matches the true probability distribution.This strongly suggests hat probabilities generatedthis way can be used successfully in system com-bination techniques such as voting or boosting.4.5 Chunk ing  per formanceIt is worth noting that the transformation-basedsystem used in the comparative graphs in Figure3 was not r, uning at full potential.
As describedearlier, the TBLDT system was only allowed toconsider words that C4.5 had access to.
However,a comparison between the corresponding TBLDTcurves in Figures 2 (where the system is givenaccess to all the words) and 3 show that atransformation-based system given access to allthe words performs better than the one with arestricted lexicon, which in turn outperforms thebest C4.5 decision tree system both in terms ofaccuracy and F-measure.Table 4 shows the performance of the TBLDTsystem on the full CoNLL  test set, broken downby chunk type.
Even though the TBLDT resultscould not be compared with other published re-sults on the same task and data (CoNLL  willnot take place until September 2000), our systemsignificantly outperforms a similar system trainedwith a C4.5 decision tree, shown in Table 5, bothin chunk accuracy and F-measure.ChunkTypeAccuracy(%)ADVPCONJPIJrecision(%)Recall(%)Overall 93.80 90.02 90.26ADJP 65.58 64.3874.14 76.7933.33INTJ 50.00 50.00LST 0.00 0.00NP  91.00 90.93PP  92.70 96.36PRT  71.13 65.09SBAR 86.35 61.50VP  90.71 91.22I Fz90.1464.9875.4433.3350.000.0090.9694.5067.9871.8390.97Table 5: Performance of C4.5 on the CoNLL TestSet5 Conclus ionsIn this paper we presented a novel way to converttransformation rule lists, a common paradigm innatural anguage processing, into a form that isequivalent in its classification behavior, but iscapable of providing probability estimates.
Usingthis approach, favorable properties of transfor-mation rule lists that makes them popular forlanguage processing are retained, while the manyadvantages of a probabilistic system axe gained.To demonstrate he efficacy of this approach,the resulting probabilities were tested in threeways: directly measuring the modeling accuracyon the test set via cross entropy, testing thegoodness of the output probabilities in a activelearning algorithm, and observing the rejectioncurves attained from these probability estimates.The experiments clearly demonstrate that theresulting probabilities perform at least as well asthe ones generated by C4.5 decision trees, resultingin better performance in all cases.
This proves thatthe resulting probabilistic lassifier is as least asgood as other state-of-the-art p obabilistic models.The positive results obtained suggest hat theprobabilistic lassifier obtained from transforma-tion rule lists can be successfully used in machinelearning algorithms that require soft-decision clas-sifters, such as boosting or voting.
Future researchwill include testing the behavior of the systemunder AdaBoost (Freund and Schapire, 1997).
Wealso intend to investigate the effects that otherdecision tree growth and smoothing techniquesmay have on continued refinement of the convertedrule list.6 AcknowledgementsWe thank Eric Brill, Fred Jelinek and DavidYaxowsky for their invaluable advice and sugges-tions.
In addition we would like to thank DavidDay, Ben Weliner and the anonymous reviewersfor their useful comments and suggestions on thepaper .
.
.
.The views expressed in this paper are those ofthe authors and do not necessarily reflect he views33of the MITRE Corporation.
It  was performedas a collaborative ffort at \]both MITRE andthe Center for Language and ',Speech Processing,Johns Hopkins University, Baltimore, MD.
It wassupported by NSF grants numbered IRI-9502312and IRI-9618874, as well as the MITRE-SponsoredResearch program.ReferencesL.
Bahl, P. Brown, P. de Souza, and R. Mercer.
1989.A tree-based statistical language model for naturallanguage speech recognition.
IEEE Transactions onAcoustics, Speech and Signal Processing, 37:1001-1008.E.
BriU and P. Resnik.
1994.
A rule-based approachto prepositional phrase attachment disambiguation.In Proceedings of the Fifteenth International Con-ference on Computational Linguistics (COLING-199~), pages 1198--1204, Kyoto.E.
BrllL 1995.
Transformation-based rror-drivenlearning and natural language processing: A casestudy in part of speech tagging.
ComputationalLinguistics, 21(4):543-565.E.
Brill, 1996.
Learning to Parse with Transforma-tions.
In H. Bunt and M. Tomita (eds.)
RecentAdvances in Parsing Technology, Kluwer.CoNLL.
2000.
Shared task for computational natu-ral language learning (CoNLL), 2000. http://lcg-ww w.uia.ac.be/conU2000/chunking.I.
Dagan and S. Engelson.
1995.
Committee-basedsampling for training probabilistic lassifiers.
InProceedings ofInternational Conference on MachineLearning (ICML) 1995, pages 150-157.D.
Day, J. Aberdeen, L. Hirschman, R. Kozierok,P.
Robinson, and M. Vllaln.
1997.
Mixed-initiativedevelopment of language processing systems.
InFifth Conference on Applied Natural Language Pro-cessing, pages 348-355.
Association for Computa-tional Linguistics, March.T.
G. Dietterich and G. Bakiri.
1995.
Solving multi-class learning problems via error-correcting outputcodes.
Journal of Artificial Intelligence Research,2:263-286.S.
Engelson and I. Dagan.
1996.
Minlmi~.ing manualannotation cost in supervised training fxom corpora.In Proceedings of ACL 1996, pages 319-326, SantaCruz, CA.
Association for Computational Linguis-tics.Y.
Freund and R.E.
Schapire.
1997.
A decision-theoretic generalization of on-fine learning and anapplication to boosting.
Journal of Computer andSystem Sciences, 55(1):119--139.Y.
Fremad, H. S. Senng, E. Shamir, and N. Tishby.1997.
Selective sampling using the query by com-mittee algorithm.
Machine Learning, 28:133-168.D.
Lewis and J. Catlett.
1994.
Heterogeneous n-certainty sampling for supervised learning.
In Pro-ceedings of the 11th International Conference onMachine Learning, pages 139---147.D.
Lewis and W. Gale.
1994.
A sequential algorithmfor training text classifiers.
In Proceedings ofA CM-SIGIR 1994, pages 3-12.
ACM-SIGIR.R.
Liere and P. Tadepalli.
1997.
Active learning withcommittees for text categorization.
In Proceedingsof the Fourteenth National Conference on ArtificialIntelligence, pages 591-596.
AAAI.L.
Mangu and E. Brill.
1997.
Automatic rule acquisi-tion for spelling correction.
In Proceedings of theFourteenth International Conference on MachineLearning, pages 734-741, Nashville, Tennessee.M.
P. Marcus, B. Santorini, and M. A. Mareinkiewicz.1993.
Building a large annotated corpus of english:The Penn Treebank.
Computational Linguistics,19(2):313-330.G.
Ngai and D. Yarowsky.
2000.
Rule writing orannotation: Cost-efficient resource usage for basenoun phrase chunking.
In Proceedings ofA CL 2000.Association for Computational Linguistics.C.
E. Priebe, J.-S. Pang, and T. Olson.
1999.
Opt lmiT.ing mine classification performance.
In Proceedingsof the JSM.
American Statistical Association.J.
R. Qnlnlan.
1993.
C~.5: Programs for machinelearning.
Morgan Kanfmann, San Mateo, CA.L.
Ramshaw and M. Marcus, 1999.
Text Chunk-ing Using Transformation-based Learning.
In S.Armstrong, K.W.
Church, P. Isabelle, S. Mauzi,E.
Tzoukermann and D. Yarowsky (eds.)
NaturalLanguage Processing Using Very Large Corpora,Kluwer.E.
Roche and Y. Schabes.
1995.
Computationallinguistics.
Deterministic Part of Speech Taggingwith Finite State Transducers, 21(2):227-253.K.
Samuel, S. Carberry, and K. Vijay-Shanker.
1998.Dialogue act tagging with transformation-basedlearning.
In Proceedings of the 17th InternationalConference on Computational Linguistics and the36th Annual Meeting of the Association for Com-putational Linguistics, pages 1150-1156, Montreal,Quebec, Canada.H.
S. Senng, M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proceedings of the FifthAnnual A CM Workshop on Computational LearningTheory, pages 287-294.
ACM.M.
Vilain and D. Day.
1996.
Finite-state parsingby rule sequences.
In International Conference onComputational Linguistics, pages 274-279, Copen-hagen, Denmark, August.34
