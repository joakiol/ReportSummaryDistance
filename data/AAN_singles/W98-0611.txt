Too ls  fo r  locat ing  noun phrases  w i th  f in i te  s ta te  t ransducers .Jean  Sene l la r tLADL (Laboratoire d'automatique documentaire et linguistique.
)Universitd Paris VII2, place Jussieu75251 PARIS Cedex 05email: senella~ladl.j ussieu.frAbst rac tThe processes of retrieving and describing in-formation in full texts are very close, at leastfor the first user (developer).
We describehere practical tools that allow construction ofan FST database which describes and then toretrieve nominal phrases composed with a se-quence denoting an occupation and a propernoun,1 In t roduct ionTo be able to retrieve information i  a text, onemust be able to describe in details the formsunder which this information is expressed.
Forexample, if we do not have the information thatthe chancellor of the Exchequer is the Britishfinance minister, that the shadow minister forTrade and Industry is not a minister, but refersto the British opposition or that the Christ'sministry has nothing to do with politics, we willbe misled in our search.
We are thus talkingabout linguistic "information".
We have shownin (1998a), how descriptions ofoccupations andproper noun phrases could be processed by finitestate transducer (FST).
In a French daily news-paper, one sentence out of two contains uch anoun phrase, thus, one cannot attempt to parsetexts without recognizing such sequences.
Therecognition at this stage, should be accurate:minister of.finance and the minister, Mr Jones,for Scottish affairs are well-formed nominals,but minister for .finance is not.
The syntacticand semantic description relies on the same ba-sis: we must be able to fully enumerate suchnominals.We can find many formalisms proved to be pow-erful enough to describe such or such naturallanguage phenomenon, but the real problem isthe linguistic description.
Generally, few exam-ples are given, and it is assumed that formalismswill accommodate a completed atabase.
The8ODFigure 1: Starting point of the officer categoryreasons of this situation are several: first, thedescription stage is considered as trivial; how-ever when seriously attempted, many new the-oretical problems appear.
Second, if we areable to handle without problem a dozen ofrules, when their number increase (to severalthousand), processes become rapidly difficult ocheck and to understand, because interactionsbetween rules are not treated.We present here practical methods that allow tocreate dynamically, to maintain and to debug alarge database of finite state transducers.
Wewill develop our example, and show, when onestarts from scratch, how to construct a preciseand large database.
The text we use is one yearof the International Herald Tribune (about 10millions words).2 S tar t ing  f rom scratchLet us suppose we describe (in order to search)the semantic classes corresponding to theoccupation officer (corresponding to the lessambiguous French equivalent word : officier).We start searching the sentences containingonly the word officerwith automaton of figure1.
In our corpus, 85 occurrences are found; theconcordance is then sorted according to rightcontexts:i,enlor cabinet oi~cer after an emb&rr?szingOere in "An Officer &nd & Oent lem&n' ;u ?
pax?troop of Rcer and had heldchief oper&ting of~/~cer and make avai lab leformer Marine of~cer ?
nd  Nat ion?
\ ]  Secur i tythe executive ot~:icer asks from topslde.Afr ican |o~n of 1~cer at one lead ing?
four star of~cer based in Hawai i?
former police ol~cer convicted of murderWe first observe that the meaning army ismixed with the meanings chief executive, loanor chief operating officer.
These meaningsare not relevant at this stage.
The left con-Figure 3: More complex classificationFigure 2: first Officer classificationtext seems more interesting than the rightcontext for desambiguation.
So, we sort theconcordance by left contexts, and extractall different adjectives or nouns that makeexplicit the occupation of army: intelligence,police, Marine... We group these modifiers infour semantic categories intelligence, army,custom and police.
Depending on the need,this grouping could be made more precise.Then we construct transducer incorporatingthese modifiers, and giving as output of eachsequence, the associated class (figure 2).
Noticethat two categories could be combined: armyand intelligence.We find the same 85 occurrences, but aboutone third make more precise the word officer.the h i ,hut  rsnklng army officer killed in morethe French Army off icer who commandsentIineer and lurmy officer who served ua senior intelligence o~cer who defecteds national intelligence off icer .
Oeorl~e Ko l t .~ intelligence officer .
Mr .
|nm&n'sthe fQrmer Marine officer and NationalAS ?
~ov ie t  mi l i ta ry  intellilgence officer in Londonranking Amer ican  mi l i ta ry  off icer on act ive dutyhil~hest rxnkin{~ U.S military officer to visit V ietnama f?
rmer  military officer with the visionIn the same way, looking at the alphabeticallysorted left and right context distinguishesnew modifiers: to the right on active duty,and to the left Soviet, American, U.S, ...former and highest ranking.
Adding thesepossibilities, plus equivalent forms that com~ 1instantly 1 to mind, we obtain the transducerof figure 3.
The number of different path ofthis automaton is now 3,348: whereas thenumber of lines of concordances examined isroughly 20.
We could go on, but the size ofthe automaton becoming larger, it is moreand more difficult to handle it on one screen.Moreover, semantically, we clearly see thatsome categories (e.g.
nationality) will becomefar too important to be represented as onegraph.
It must be clustered.3 Par t i t ion ing  (manua l  c lus ter ing .
)Nationality is a homogeneous semantic cate-gory, composed of simple words, or compounds(South Korean), it can easily be stored in alist.
We begin to construct it, starting with thethree nationalities found: American, Soviet,and French.
To be compatible with the searchengine described in (1998b), an output is alsoassociated with these words: for example thenationality itself, as in:American/American, French/French,Soviet/SovietWe must also construct a list Of countries aswe find productive forms such as U.S militaryofficer, _France's officer...
The list of countriesis thus initiated from the few countries we findinstantly, or we can think of.
To use lists, wedefine special nodes in the automaton (graynodes) containing the name of the dictionary.We show in the automaton of figure 4 anexample of such use.In fact, we can localize other homogeneoussub-category as in automaton of figure 3:senior, high ranking, four star.., indicate therank of the officer.
Here, it is more difficultto gather them into a single list, as their com-binations are not trivial, we thus construct at At this stage, we do not pretend to be complete, weare only building the main structure.Figure 6: AutomatonFigure 4: Example of dictionary usehi,d I~' \ I Ison ,orFigure 5: High rank sub-automatonsub-automaton named HighRank (cf automaton5).
All the paths of this automaton combinedwith officer correspond approximatively tothe senior officer rank.
It will be sufficientfor the search engine.
Finally, we specializeour initial automaton to only military officers,as we dearly see that the rank and othersmodifiers associated with the police categorywill probably not be the same that thoseassociated with the military category.
We thencreate the military ofcer automaton (figure 6).Two main ideas should be kept in mind aboutclustering methodology: - homogeneous seman-tic sub-categories in an automaton should beput in a sub-automata, and - an automatoncontaining two semantic notions that we canseparate, should be transformed into a moresimple automaton containing only one semanticcategory.
This clustering has many advantage:82with different levelfirst, it allows a better readibility of automata,second, it factorizes homogeneous emanticunits that we could use for other purposes.And third, it is very useful for processing, aswe will show later.Now we have established the main structure:we must complete the different automata anddictionaries.
We show in the next section, ageneral method that permits to quickly enrich 2the database.4 Use  o f  var iab lesThe automaton of figure 6 represents thelargest structure we found around the wordofcer.
The initial goal was to constructnominals of the army ofcer semantic ategory.Such categories certainly contain sequencesnot containing at all the base word officer(Synonyms, hyponyms, or any semanticallyrelated nouns).
To find such words, we havenevertheless now a important clue: their rightand left contexts axe close to the context ofthe word ofcer.
We replace the word offiicerby a variable in the automaton Of figure 6.
Avariable is a word or a sequence of words.
Wefind on the same corpus 1,239 occurrences.
Theconcordance we obtain is sorted alphabeticallyon that variable.
This allows us to locate veryquickly (in less than one minute) to locate newterms that should be put in parallel with theword officer.
We give here an extract:the El.st German intelligence chij_~ at the t ime.Mr P&rk an &rmy general who seisedA retired army i~eneral, Mr.  ChungCommunist army General.
sent hisCroatian Army hel icopte~ directed f ighterthe former intelligence ol~cial ~ his\[ormer top intelligence official named toKorea ?
Jepanese intelligence official said.which mil i tary o~cis ls and federalbe determined mil i tary of~cials said.the UN mil i tary Officials said.for them' ,  mil i tary officials said.police and military officials.2 And moretext increases.and more quickly when the size of theen?oZ1cerof f icerFigure 7: Completion of the officer columnFigure 8: Full nominal with both occupation and proper noun partWe also observe new right and left contextsrelevant to our description (but we do nottake them into account here, in order to keepour steps in sequence).
With this new words,we complete the automaton and obtain theautomaton of figure 7.
To maintain clarityin the automaton, we always put in a samecolumn, equivalent units or term.
For thatreason, officer is vertically aligned with general,o~cial... Country is vertically aligned withnational and Nationality, etc.
When we addnew boxes, we add the necessary transitions wecan think of.
Sometimes, choices are difficult:for example, can we find career general ?
Inthose cases, we permit it by default.
The finalautomaton may recognize sequences that arenot stylistically or semantically perfect, butthat are always well-formed, and their presencewill not generate any noise.We skip a few stages to arrive to the propernoun description.
Contrary to the French case,where the full name is always put in an externalapposition of the occupation noun phrase, inEnglish both can be mixed, we find in Ourcorpus for example:General Jean Cot of the French Army said the idea of...... the French commander  of troops there.
General Jean Cot,Genera3 Jean Cot.
the French Army GrAter who commands theAnd as Genenal John Shali&ajh~l~ has said, the Partnership.
.
.83We describe these four different structures(left and right apposition, proper nouns andoccupations mixed, and only proper noun) ina simplified way 3 (to keep small automata forthe presentation) by the automaton of figure8.
This automaton contain FirstName andSurName boxes.
These two dictionaries areinitially empty, they need to be filled, andwe easily imagine that their size will quicklybecome very large.
To that aim, we replacethem by variable.
In that case, we can makemore precise the morphological form of thesevariables.
(As a first approximation, let us saythat they are both single words with a capitalfirst letter).
We apply this automaton to thecorpus, and obtain a list of 582 occurrences.These occurrences are sorted according tothe FirstName and SurName variable, andwe only need to confirm these terms to beactual First and SurName.
A first draft forthe both dictionary is thus constructed: 15entries of FirstName and 67 of SurName.The whole construction is dynamic: oncewe have some proper nouns, we seek theircontexts, and find new structures for the officerautomaton.
These new structures allows us toobtain new proper nouns, new countries, newnationalities...
This is the bootstrap effect.a In particular, this automaton recognizes the BritishGeneral.of France !We have built in one hour, a large coverageautomaton (more than 200, 000 different paths,without counting proper noun combinations)for this mere semantic category.
The finalnumber of recognized items is 863 (to comparewith the 85 that we started ~om, and thatwere in majority used in another context(economic)).
The number of the potentialrecognized sequences, as compared to theeffectively recognized sequences i  a guaranteethat, on a totally new corpus, we will obtaininstantly a good coverage and new potentialSurName and FirstName (and this, with a rateproportional to the size of existing dictionaries).5 So f twareThe needed tools for such a methodology are:1.
Graph ed i to r .
The first tool (included inINTEX (1994)) is a graphic graph editor.This editor allows to create, copy and alignparts of them.
It handles input and outputof transducers.
It allows: by simple click-ing, to open a sub-automaton.2.
Index-based  pars ing  a lgor i thm.
A keyfeature of the text-based automaton con-struction, is in the possibility to have im-mediate concordances.
It is hardly think-able parse sequentially a 10 million wordcorpus.
Moreover, with many levels of au-tomata (sometimes more than 20): the sizeof the developed main automaton becomesquickly huge, that we cannot re-computefor each concordance.
Thus, we have cho-sen to index each sub-automaton i depen-dently, with a dependency graph (cf be-low) like makefile, we only re-compute, themodified graph, and those depending on it.The index parsing algorithm is describedin (Senellart, 1998a).
It allows us, to ob-tain concordances on the whole corpus, ina mean time less than ls on a average per-sonal computer.3.
Concordance  manager .
We have shownduring all along this paper: that the waythe concordance are sorted has a great im-portance.
Under INTEX, we can sort con-cordances according to the recognized se-quence, to the right, or left context, andany combination of the three parameters.Moreover, when we put variables in the au-tomaton, we must are able to validate rec-ognized sequence linked to the variable in84Figure 9: Dependency graph of O~cer automa-tona click.4.
Debugg ing  tools .
Maintaining a largenumber of automata is not simple: someautomata depend on others of which, theexact name and the exact function must berecalled.
Exactly as in compiling programswith a large number of source files, wecan compute and represent (graphically)the dependencies between the different au-tomaton.
For example the graph of figure9, represents the dependent sub-automatonused in the O~icer automaton, and thesame for each of the sub-automaton.
Thedepth in this graph is four.6 Conc lus ionThe simplicity with which we created the O.~-cer automaton is not based on new theoreticalalgorithms or formalism.
It is totally based onuseful tools elaborated at LADL.
I have built forFrench a database of large coverage for propernouns and occupations.
This database includesmore than 200 kinds of different main automata,proper noun dictionaries of towns, surnames,each with several thousand entries.
This is apractical result, and I think that this is the caseof almost all linguistic phenomena.
We can de-scribe them theoretically, but in practice thenumber of the different cases, and the links be-tween them make impossible to list them with-out appropriate tools.
The bootstrap methodwe present is a very general methodology, thatmake use of the text in an efficient way, to con-struct a local grammar.Re ferencesJ.
Senellart.
1998a.
Fast pattern matching in?
indexed texts.
Being published in TCS.J.
Senellart.
1998b.
Locating noun phrases withfinite state transducers.
In Proc.
off A CL-COLING '98.M.
Silbertzein.
1994.
Intex: a corpus process-ing system, in coling proceedings.
In Proc.
ofCOLING '94.
