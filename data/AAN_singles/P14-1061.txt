Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644?654,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLexical Inference over Multi-Word Predicates: A Distributional ApproachOmri Abend Shay B. Cohen Mark SteedmanSchool of Informatics, University of Edinburgh,Edinburgh EH8 9AB, United Kingdom{oabend,scohen,steedman}@inf.ed.ac.ukAbstractRepresenting predicates in terms of theirargument distribution is common practicein NLP.
Multi-word predicates (MWPs) inthis context are often either disregarded orconsidered as fixed expressions.
The lat-ter treatment is unsatisfactory in two ways:(1) identifying MWPs is notoriously diffi-cult, (2) MWPs show varying degrees ofcompositionality and could benefit fromtaking into account the identity of theircomponent parts.
We propose a novelapproach that integrates the distributionalrepresentation of multiple sub-sets of theMWP?s words.
We assume a latent distri-bution over sub-sets of the MWP, and esti-mate it relative to a downstream predictiontask.
Focusing on the supervised identi-fication of lexical inference relations, wecompare against state-of-the-art baselinesthat consider a single sub-set of an MWP,obtaining substantial improvements.
Toour knowledge, this is the first work toaddress lexical relations between MWPsof varying degrees of compositionalitywithin distributional semantics.1 IntroductionMulti-word expressions (MWEs) constitute alarge part of the lexicon and account for muchof its growth (Jackendoff, 2002; Seaton andMacaulay, 2002).
However, despite their impor-tance, MWEs remain difficult to define and model,and consequently pose serious difficulties for NLPapplications (Sag et al, 2001).
Multi-word Predi-cates (MWPs; sometimes termed Complex Predi-cates) form an important and much addressed sub-class of MWEs and are the focus of this paper.MWPs are informally defined as multiple wordsthat constitute a single predicate (Alsina et al,1997).
MWPs encompass a wide range of phe-nomena, including causatives, light verbs, phrasalverbs, serial verb constructions and many others,and pose considerable challenges to both linguistictheory and NLP applications (see Section 2).
Partof the difficulty in treating them stems from theirposition on the borderline between syntax and thelexicon.
It is therefore often unclear whether theyshould be treated as fixed expressions, as compo-sitional phrases that reflect the properties of theircomponent parts or as both.This work addresses the modelling of MWPswithin the context of distributional semantics (Tur-ney and Pantel, 2010), in which predicates arerepresented through the distribution of argumentsthey may take.
In order to collect meaningfulstatistics, the predicate?s lexical unit should be suf-ficiently frequent and semantically unambiguous.MWPs pose a challenge to such models, asna?
?vely collecting statistics over all instances ofhighly ambiguous verbs is likely to result in noisyrepresentations.
For instance, the verb ?take?
mayappear in MWPs as varied as ?take time?, ?takeeffect?
and ?take to the hills?.
This heterogene-ity of ?take?
is likely to have a negative effect ondownstream systems that use its distributional rep-resentation.
For instance, while ?take?
and ?ac-cept?
are often considered lexically similar, thehigh frequency in which ?take?
participates innon-compositional MWPs is likely to push the twoverbs?
distributional representations apart.A straightforward approach to this problem isto represent the predicate as a conjunction of mul-tiple words, thereby trading ambiguity for spar-sity.
For instance, the verb ?take?
could be con-joined with its object (e.g., ?take care?, ?take abus?).
This approach, however, raises the chal-lenge of identifying the sub-set of the predicate?swords that should be taken to represent it (hence-forth, its lexical components or LCs).We propose a novel approach that addresses this644challenge in the context of identifying lexical in-ference relations between predicates (Lin and Pan-tel, 2001; Schoenmackers et al, 2010; Melamud etal., 2013a, inter alia).
A (lexical) inference rela-tion pL?
pRis said to hold if the relation denotedby pRgenerally holds between a set of argumentswhenever the relation pLdoes.
For instance, an in-ference relation holds between ?annex?
and ?con-trol?
since if a country annexes another, it gener-ally controls it.
Most works to this task use dis-tributional similarity, either as their main compo-nent (Szpektor and Dagan, 2008; Melamud et al,2013b), or as part of a more comprehensive system(Berant et al, 2011; Lewis and Steedman, 2013).For example, consider the verb ?take?.
Whilethe inference relation ?have?
take?
does not gen-erally hold, it does hold in the case of some lightverbs, such as ?have a look?
take a look?, under-scoring the importance of taking more inclusiveLCs into account.
On the other hand, the pred-icate ?likely to give a green light?
is unlikely toappear often even within a very large corpus, andcould benefit from taking its lexical sub-units (e.g.,?likely?
or ?give a green light?)
into account.We present a novel approach to the task thatmodels the selection and relative weighting of thepredicate?s LCs using latent variables.
This ap-proach allows the classifier that uses the distri-butional representations to take into account themost relevant LCs in order to make the predic-tion.
By doing so, we avoid the notoriously dif-ficult problem of defining and identifying MWPsand account for predicates of various sizes and de-grees of compositionality.
To our knowledge, thisis the first work to address lexical relations be-tween MWPs of varying degrees of composition-ality within distributional semantics.We conduct experiments on the dataset of Ze-ichner et al (2012) and compare our methods withanalogous ones that select a fixed LC, using state-of-the-art feature sets.
Our method obtains sub-stantial performance gains across all scenarios.Finally, we note that our approach is cognitivelyappealing.
Significant cognitive findings supportthe claim that a speaker?s lexicon consists of par-tially overlapping lexical units of various sizes, ofwhich several can be evoked in the interpretationof an utterance (Jackendoff, 2002; Wray, 2008).2 Background and Related WorkInference Relations.
The detection of inferencerelations between predicates has become a centraltask over the past few years (Sekine, 2005; Zan-zotto et al, 2006; Schoenmackers et al, 2010;Berant et al, 2011; Melamud et al, 2013a, in-ter alia).
Inference rules are used in a wide va-riety of applications including Question Answer-ing (Ravichandran and Hovy, 2002), InformationExtraction (Shinyama and Sekine, 2006), and asa main component in Textual Entailment systems(Dinu and Wang, 2009; Dagan et al, 2013).Most approaches to the task used distributionalsimilarity as a major component within their sys-tem.
Lin and Pantel (2001) introduced DIRT, anunsupervised distributional system for detectinginference relations.
The system is still considereda state-of-the-art baseline (Melamud et al, 2013a),and is often used as a component within larger sys-tems.
Schoenmackers et al (2010) presented anunsupervised system for learning inference rulesdirectly from open-domain web data.
Melamudet al (2013a) used topic models to combine type-level predicate inference rules with token-level in-formation from their arguments in a specific con-text.
Melamud et al (2013b) used lexical expan-sion to improve the representation of infrequentpredicates.
Lewis and Steedman (2013) combineddistributional and symbolic representations, eval-uating on a Question Answering task, as well ason a quantification-focused entailment dataset.Several studies tackled the task using super-vised systems.
Weisman et al (2012) used a setof linguistically motivated features, but evaluatedtheir system on a corpus that consists almost en-tirely of single-word predicates.
Mirkin et al(2006) presented a system for learning inferencerules between nouns, using distributional similar-ity and pattern-based features.
Hagiwara et al(2009) identified synonyms using a supervised ap-proach relying on distributional and syntactic fea-tures.
Berant et al (2011) used distributional simi-larity between predicates to weight the edges of anentailment graph.
By imposing global constraintson the structure of the graph, they obtained a moreaccurate set of inference rules.Previous work used simple methods to selectthe predicate?s LC.
Some filtered out frequenthighly ambiguous verbs (Lewis and Steedman,2013), others selected a single representative word(Melamud et al, 2013a), while yet others usedmulti-word LCs but treated them as fixed expres-sions (Lin and Pantel, 2001; Berant et al, 2011).The goals of the above studies are largely com-645plementary to ours.
While previous work focusedeither on improving the quality of the distribu-tional representations themselves or on their incor-poration into more elaborate systems, we focus onthe integration of the distributional representationof multiple LCs to improve the identification ofinference relations between MWPs.MWP Extraction and Identification.
MWPshave received considerable attention over the yearsin both theoretical and applicative contexts.
Theirposition on the crossroads of syntax and the lexi-con, their varying degrees of compositionality, aswell as the wealth of linguistic phenomena theyexhibit, made them the object of ongoing linguis-tic discussion (Alsina et al, 1997; Butt, 2010).In NLP, the discovery and identification ofMWEs in general and MWPs in particular hasbeen the focus of much work over the years(Lin, 1999; Baldwin et al, 2003; Biemann andGiesbrecht, 2011).
Despite wide interest, thefield has yet to converge to a general and widelyagreed-upon method for identifying MWPs.
See(Ramisch et al, 2013) for an overview.Most work on MWEs emphasized idiosyncraticor non-compositional expressions.
Other lines ofwork focused on specific MWP classes such aslight verbs (Tu and Roth, 2011; Vincze et al,2013) and phrasal verbs (McCarthy et al, 2003;Pichotta and DeNero, 2013).
Our work proposes auniform treatment to MWPs of varying degrees ofcompositionality, and avoids defining MWPs ex-plicitly by modelling their LCs as latent variables.Compositional Distributional Semantics.Much work in recent years has concentrated onthe relation between the distributional representa-tions of composite phrases and the representationsof their component sub-parts (Widdows, 2008;Mitchell and Lapata, 2010; Baroni and Zampar-elli, 2010; Coecke et al, 2010).
Several workshave used compositional distributional semantics(CDS) representations to assess the composition-ality of MWEs, such as noun compounds (Reddyet al, 2011) or verb-noun combinations (Kielaand Clark, 2013).
Despite significant advances,previous work has mostly been concerned withhighly compositional cases and does not addressthe distributional representation of predicates ofvarying degrees of compositionality.3 Our Proposal: A Latent LC ApproachThis section details our approach for distribu-tionally representing MWPs by leveraging theircomponent LCs.
Section 3.1 describes our gen-eral approach, Section 3.2 presents our model andSection 3.3 details the feature set.3.1 General Approach and NotationWe propose a method for addressing MWPs ofvarying degrees of compositionality through theintegration of the distributional representation ofmultiple sub-sets of the predicate?s words (LCs).We use it to tackle a supervised prediction task thatrepresents predicates distributionally.
Our modelassumes a latent distribution over the LCs, and es-timates its parameters so to best conform to thegoals of the target prediction task.Formally, given a predicate p, we denote the setof words comprising it as W (p).
The set of al-lowable LCs for p is denoted with Hp?
2W (p).Hpcontains all sub-sets of p that we consider asapriori possible to represent p. For instance, if p is?likely to give a green light?, Hpmay include LCssuch as ?likely?
or ?give light?.
As our method isaimed at discovering the most relevant LCs, we donot attempt to analyze the MWPs in advance, butrather take an inclusive Hp, allowing the model toestimate the relative weights of the LCs.The task we use as a testbed for our approachis the lexical inference identification task betweenpredicates.
Given a pair of predicates p =(pL, pR), the task is to predict whether an infer-ence relation holds between them.
For instance, ifpLis ?devour?
and pRis ?eat greedily?, the clas-sifier should use the similarity between ?devour?and ?eat?
in order to correctly predict an infer-ence relation in this case.
Selecting the wider LC?eat greedily?
might result in sparser statistics.
Inother examples, however, taking a wider LC is po-tentially beneficial.
For instance, the dissimilar-ity between ?take?
and ?make?
should not preventthe classifier from identifying the inference rela-tion between ?take a step?
and ?make a step?.Our statistical model aims at predicting the cor-rect label by making use of partially overlappingLCs of various sizes, both for the premise left-hand side (LHS) predicate pLand the hypothesisright-hand side (RHS) predicate pR.
More for-mally, we take the space of values for our latentLC variables to be HpL,pR= HpL?HpR.Our evaluation dataset consists of pairs p(i)=(p(i)L, p(i)R) for i ?
{1, .
.
.
,M}, where M is thenumber of examples available, coupled with theirgold-standard labels y(i)?
{1,?1}.
For brevity,we denote H(i)= Hp(i)= Hp(i)L,p(i)R. We also as-646sume the existence of a feature function ?
(p, y, h)which maps a triplet of a predicate pair p, an infer-ence label y, and a latent state h ?
Hpto Rdforsome integer d. We denote the training set by D.3.2 The ModelWe address the task with a latent variable log-linear model, representing the LCs of the predi-cates.
We choose this model for its generality, con-ceptual simplicity, and because it allows to easilyincorporate various feature sets and sets of latentvariables.
We introduce L2regularization to avoidover-fitting.
We use maximum likelihood estima-tion, and arrive at the following objective function:L(w|D) =1MMXi=1logP (y(i)|p(i), w)??2?w?2==1nnXi=10@logXh?H(i)exp?w>?
(p(i), y(i), h)??
logZ(w, i)??
?2?w?2where:Z(w, i) =Xy?{?1,1}Xh?Hiexp(w>?
(pi, y, h)).We maximizeL using the BFGS algorithm (No-cedal and Wright, 1999).
The gradient (with re-spect to w) is the following:?L = Eh[?
(pi, yi, h)]?
Eh,y[?
(pi, y, h)]?
?
?
wHpcan be defined to be any sub-set of 2W (p)given that taking an expectation over H can bedone efficiently.
It is therefore possible to use priorlinguistic knowledge to consider only sub-sets of pthat are likely to be non-compositional (e.g., verb-preposition or verb-noun pairs).In our experiments we attempt to keep the ap-proach maximally general, and defineHpto be theset of all subsets of size 1 or 2 of content words inWp1.
We bound the size of h ?
Hpin order to re-tain computational efficiency and a sufficient fre-quency of the LCs in Hp.
MWPs of length greaterthan 2 are effectively approximated by their set ofsubsets of sizes 1 and 2.Each h can therefore be written as a 4-tuple(hAL, hBL, hAR, hBR), where hAL(hAR) denotes the firstword of the LHS (RHS) predicate?s LC.
hBL(hBR)denotes the (possibly empty) second word of thepredicate.
Inference is carried out by maximizingP (y|p(i)) over y.
As |Hp| = O(k4), where k is the1We use a POS tagger to identify content words.
Preposi-tions are considered content words under this definition.number of content words in p, and as the numberof content words is usually small2, inference canbe carried out by directly summing over H(i).Initialization.
The introduction of latent vari-ables into the log-linear model leads to a non-convex objective function.
Consequently, BFGSis not guaranteed to converge to the global opti-mum, but rather to a stationary point.
The resultmay therefore depend on the parameter initializa-tion.
Indeed, preliminary experiments showed thatboth initializing w to be zero and using a randominitializer results in lower performance.Instead, we initialize our model with a simpli-fied convex model that fixes the LCs to be thepair of left-most content words comprising eachof the predicates.
This is a common method forselecting the predicate?s LC (e.g., Melamud et al,2013a).
Once h has been fixed, the model col-lapses to a convex log-linear model.
The optimalw is then taken as an initialization point for the la-tent variable model.
While this method may stillnot converge to the global maximum, our experi-ments show that this initialization technique yieldshigh quality values for w (see Section 6).3.3 Feature SetThis section lists the features used for our exper-iments.
We intentionally select a feature set thatrelies on either completely unsupervised or shal-low processing tools that are available for a widevariety of languages and domains.Given a predicate pair p(i), a label y ?
{1,?1}and a latent state h ?
H(i), we define their featurevector as ?
(p(i), y, h) = y ?
?
(p(i), h).
The com-putation of ?
(p(i), h) requires a reference corpusR that contains triplets of the type (p, x, y) wherep is a binary predicate and x and y are its argu-ments.
We use the Reverb corpus as R in our ex-periments (Fader et al, 2011; see Section 4).
Werefrain from encoding features that directly reflectthe vocabulary of the training set.
Such featuresare not applicable beyond that set?s vocabulary,and as available datasets contain no more than afew thousand examples, these features are unlikelyto generalize well.Table 1 presents the set of features we use in ourexperiments.
The features can be divided into twomain categories: similarity features between theLHS and the RHS predicates (table?s top), and fea-tures that reflect the individual properties of each2|Hp| is about 15 on average in our dataset, where lessthan 5% of the H(i)are of size greater than 50.647CategoryName DescriptionSimilarityCOSINE DIRT cosine similarity between the vectors of hLand hRCOSINEADIRT cosine similarity between the vectors of hALand hARBInc DIRT BInc similarity between the vectors of hLand hRBIncADIRT BInc similarity between the vectors of hALand hARWordALHSPOSALThe most frequent POS tag for the lemma of hALPOS2ALThe second most frequent POS tag for the word lemma of hALFREQALThe number of occurrences of hALin the reference corpusCOMMONALA binary feature indicating whether hALappears in both predicatesORDINALALThe ordinal number of hALamong the content words of the LHS predicatePairLHSPOSABLThe conjunction of POSALand POSBLFREQABLThe frequency of hALand hBLin the reference corpusPREFABLP (hAL|hAL) as estimated from the reference corpusPREFBALP (hBL|hAL) as estimated from the reference corpusPMIABLThe point-wise mutual information of hALand hBLLDATOPICSLP (topic|hL) for each of the induced topics.TOPICENTLThe entropy of the topic distribution P (topic|hL)Table 1: The feature set used in our experiments.
The top part presents the similarity measures based on the DIRT approach.The rest of the listed features apply to the LHS predicate (hL), and to the first word in it (hAL).
Analogous features areintroduced for the second word, hBL, and for the RHS predicate.
The upper-middle part presents the word features for hAL.
Thelower-middle part presents features that apply where hLis of size 2.
The bottom part lists the LDA-based features.of them.
Within the LHS feature set, we distin-guish between two sub-types of features: wordfeatures that encode the individual properties ofhALand hBL(table?s upper middle part), and pairfeatures that only apply to LCs of size 2 and re-flect the relation between hALand hBL(table?s lowermiddle part).
We further incorporate LDA-basedfeatures that reflect the selectional preferences ofthe predicates (table?s bottom).Distributional Similarity Features.
The distri-butional similarity features are based on the DIRTsystem (Lin and Pantel, 2001).
The score definesfor each predicate p and for each argument slots ?
{L,R} (corresponding to the arguments to theright and left of that predicate) a vector vpswhichrepresents the distribution of arguments appearingin that slot.
We take vps(x) to be the number oftimes that the argument x appeared in the slot s ofthe predicate p. Given these vectors, the similaritybetween the predicates p1and p2is defined as:score(p1, p2) =qsim(vp1L, vp2L) ?
sim(vp1R, vp2R)where sim is some vector similarity measure.We use two common similarity measures: thevector cosine metric, and the BInc (Szpektor andDagan, 2008) similarity measure.
These measuresgive complementary perspectives on the similar-ity between the predicates, as the cosine similar-ity is symmetric between the LHS and RHS predi-cates, while BInc takes into account the direction-ality of the inference relation.
Preliminary exper-iments with other measures, such as those of Lin(1998) and Weeds and Weir (2003) did not yieldadditional improvements.We encode the similarity of all measures for thepair hLand hRas well as the pair hALand hAR.
Thelatter feature is an approximation to the similar-ity between the heads of the predicates, as headsin English tend to be to the left of the predicates.These two features coincide for h values of size 1.Word and Pair Features.
These features en-code the basic properties of the LC.
The motiva-tion behind them is to allow a more accurate lever-aging of the similarity features, as well as to betterdetermine the relative weights of h ?
H(i).The feature set is composed of four analogoussets corresponding to hAL,hBL,hARand hBR, as wellas two sets of features that capture relations be-tween hAL, hBLand hAR, hBR(in cases h is of size 2).The features include the ordinal index of the wordwithin the predicate, the lemma?s frequency ac-cording to R, and a feature that indicates whetherthat word?s lemma also appears in both predicatesof the pair.
For instance, when considering thepredicates ?likely to come?
and ?likely to leave?,?likely?
appears in both predicates, while ?come?and ?leave?
appear only in one of them.In addition, we use POS-based features thatencode the most frequent POS tag for the wordlemma and the second most frequent POS tag (ac-cording toR).
Information about the second mostfrequent POS tag can be important in identifyinglight verb constructions, such as ?take a swim?
or?give a smile?, where the object is derived from averb.
It can thus be interpreted as a generalization648of the feature that indicates whether the object isa deverbal noun, which is used by some light verbidentification algorithms (Tu and Roth, 2011).In cases where hLis of size 2, we additionallyencode features that apply to the conjunction ofhALand hBL.
We encode the conjunction of theirPOS and the number of times the two lemmas oc-curred together in R. We also introduce featuresthat capture the statistical correlation between thewords of hL.
To do so, we use point-wise mu-tual information, and the conditional probabili-ties P (hAL|hBL) and P (hBL|hAL).
Similar measureshave often been used for the unsupervised detec-tion of MWEs (Villavicencio et al, 2007; Fazlyand Stevenson, 2006).
We also include the analo-gous set of features for hR.LDA-based Features.
We further incorporatefeatures based on a Latent Dirichlet Allocation(LDA) topic model (Blei et al, 2003).
Severalrecent works have underscored the usefulness ofusing topic models to model a predicate?s selec-tional preferences (Ritter et al, 2010; Dinu andLapata, 2010; S?eaghdha, 2010; Lewis and Steed-man, 2013; Melamud et al, 2013a).
We adopt theapproach of Lewis and Steedman (2013), and de-fine a pseudo-document for each LC in the evalu-ation corpus.
We populate the pseudo-documentsof an LC with its arguments according to R. Wethen train an LDA model with 25 topics over thesedocuments.
This yields a probability distributionP (topic|h) for each LC h, reflecting the types ofarguments h may take.We further include a feature for the entropy ofthe topic distribution of the predicate, which re-flects its heterogeneity.
This feature is motivatedby the assumption that a heterogeneous predicateis more likely to benefit from selecting a more in-clusive LC than a homogeneous one.Technical Issues.
All features used, except thesimilarity ones and the topic distribution featuresare binary.
Frequency features are binned into 4bins of equal frequency.
We conjoin some of thefeature sets by multiplying their values.
Specifi-cally, we add the cross product of the features ofthe category ?Similarity?
(see Table 1) with therest of the features.
In addition, we conjoin allLHS (RHS) features with an indicator feature thatindicates whether hL(hR) is of size two.
This re-sults in 1605 non-constant features.We further note that some LCs that appear in theevaluation corpus do not appear at all inR.
In ourexperiments they amounted to 0.2% of the LCs inour evaluation dataset.
While previous work of-ten discarded predicates below a certain frequencyfrom the evaluation, we include them in order tofacilitate comparison to future work.
We assignthe similarity features of such examples a 0 value,and assign their other numerical features the meanvalue of those features.4 Experimental SetupCorpora and Preprocessing.
As a referencecorpus R, we use Reverb (Fader et al, 2011), aweb-based corpus consisting of 15M web extrac-tions of binary relations.
Each relation is a tripletof a predicate and two arguments, one preceding itand one following it.
Relations were extracted us-ing regular expressions over the output of a POStagger and an NP chunker.
Each predicate mayconsist of a single verb, a verb and a preposi-tion or a sequence of words starting in a verb andending in a preposition, between which there maynouns, adjectives, adverbs, pronouns, determinersand verbs.
The verb may also be a copula.
Exam-ples of predicates are ?make the most of?, ?couldbe exchanged for?
and ?is happy with?.Reverb is an appealing reference corpus for thistask for several reasons.
First, it uses fairly shal-low preprocessing technology which is availablefor many domains and languages.
Second, Reverbapplies considerable noise filtering, which resultsin extractions of fair quality.
Third, our evaluationdataset is based on Reverb extractions.We evaluate our algorithm on the dataset ofZeichner et al (2012).
This publicly availablecorpus3provides pairs of Reverb binary relationsand an indication of whether an inference rela-tion holds between them within the context ofa specific pair of argument fillers.
The corpuswas compiled using distributional methods to de-tect pairs of relations in Reverb that are likelyto have an inference relation between.
Annota-tors, employed through Amazon Mechanical Turk,were then asked to determine whether each pairis meaningful, and if so, to determine whether aninference relation holds.
Further measures weretaken to monitor the accuracy of the annotation.For example, the pair of predicates ?make themost of?
and ?take advantage of?
appears in thecorpus as a pair between which an inference rela-tion holds.
The arguments in this case are ?stu-dents?
and ?their university experience?.
An ex-3http://tinyurl.com/krx2acd649ample of a pair between which an inference rela-tion does not hold is ?tend to neglect?
and ?under-estimate the importance of?, where the argumentsare ?Robert?
and ?his family?.The dataset contains 6,565 instances in total.We use 5,411 pairs of them, discarding instancesthat were deemed as meaningless by the annota-tors.
We also discard cases where the set of ar-guments is reversed between the LHS and RHSpredicates.
In these examples, pR(x, y) is infer-able from pL(y, x), rather than from pL(x, y).
Asthere are less than 150 reversed instances in thecorpus, experimenting on this sub-set is unlikelyto be informative.The average length of a predicate in the cor-pus is 2.7 words (including function words).
In87.3% of the predicate pairs, there was more thanone LC (i.e., |Hp| > 1), underscoring the im-portance of correctly leveraging the different LCs.We randomly partition the corpus into a trainingset which contains 4,343 instances (?80%), and atest set that contains 1,068 instances, maintainingthe same positive to negative label ratio in bothdatasets4.
Development was carried out usingcross-validation on the training data (see below).We use a Maximum Entropy POS Tagger,trained on the Penn Treebank, and the WordNetlemmatizer, both implemented within the NLTKpackage (Loper and Bird, 2002).
To obtain acoarse-grained set of POS tags, we collapse thetag set to 7 categories: nouns, verbs, adjectives,adverbs, prepositions, the word ?to?
and a cate-gory that includes all other words.
A Reverb argu-ment is represented as the conjunction of its con-tent words that appear more than 10 times in thecorpus.
Function words are defined according totheir POS tags and include determiners, possessivepronouns, existential ?there?, numbers and coordi-nating conjunctions.
Auxiliary verbs and copulasare also considered function words.To compute the LDA features, we use the on-line variational Bayes algorithm of (Hoffman etal., 2010) as implemented in the Gensim softwarepackage (Rehurek and Sojka, 2010).Evaluated Algorithms.
The only two previousworks on this dataset (Melamud et al, 2013a;Melamud et al, 2013b) are not directly compara-ble, as they used unsupervised systems and evalu-4A script that replicates our train-test partition of the cor-pus can be found here: http://homepages.inf.ed.ac.uk/oabend/mwpreds.htmlated on sub-sets of the evaluation dataset.
Instead,we use several baselines to demonstrate the use-fulness of integrating multiple LCs, as well as therelative usefulness of our feature sets.The simplest baseline is ALLNEG, which pre-dicts the most frequent label in the dataset (in ourcase: ?no inference?).
The other evaluated sys-tems are formed by taking various subsets of ourfeature set.
We experiment with 4 feature sets.
Thesmallest set, SIM, includes only the similarity fea-tures.
This feature set is related to the composi-tional distributional model of Mitchell and Lap-ata (2010) (see Section 6).
We note that despiterecent advances in identifying predicate inferencerelations, the DIRT system (Lin and Pantel, 2001)remains a strong baseline, and is often used as acomponent in state-of-the-art systems (Berant etal., 2011), and specifically in the two aforemen-tioned works that used the same evaluation corpus.The next feature set BASIC includes the featuresfound to be most useful during the developmentof the model: the most frequent POS tag, the fre-quency features and the feature Common.
Moreinclusive is the feature set NO-LDA, which in-cludes all features except the LDA features.
Ex-periments with this set were performed in orderto isolate the effect of the LDA features.
Finally,ALL includes our complete set of features.The more direct comparison is against partialimplementations of our system where the LC h isdeterministically selected.
Determining h for eachpredicate yields a regular log-linear binary classi-fication model.
We use two variants of this base-line.
The first, LEFTMOST, selects the left-mostcontent word for each predicate.
Similar selec-tion strategy was carried out by Melamud et al(2013a).
The second, VPREP, selects h to be theverb along with its following preposition.
In casesthe predicate contains multiple verbs, the one pre-ceding the preposition is selected, and where thepredicate does not contain any non-copula verbs,it regresses to LEFTMOST.
This LC selectionmethod approximates a baseline that includes sub-categorized prepositions.
Such cases are highlyfrequent and account for a large portion of theMWPs in English.
Including a verb?s prepositionin its LC was commonly done in previous work(e.g., Lewis and Steedman, 2013).We also attempted to identify verb-prepositionconstructions using a dependency parser.
Unfor-tunately, our evaluation dataset is only available in650a lemmatized version, which posed a difficulty forthe parser.
Due to the low quality of the resultingparses, we implemented VPREP using POS-basedregular expressions as defined above.The full model is denoted with LATENTLC.
Foreach system and feature set, we report results us-ing 10-fold cross-validation on the training set, aswell as results on the test set.
Both cases usethe same set of parameters determined by cross-validation on the training set.
As the task at handis a binary classification problem, we use accuracyscores to rate the performance of our systems.5 ResultsTable 2 presents the results of our experi-ments.
Rows correspond to the evaluated algo-rithms, while columns correspond to the featuresets used and the evaluation scenarios (i.e., train-ing set cross-validation or test set evaluation).
Ourexperiments make first use of this dataset in itsfullest form for the problem of supervised learningof inference relations, and may serve as a startingpoint for further exploration of this dataset.For all feature sets and settings, LATENTLCscored highest, often with a considerable marginof up to 3.0% in the cross-validation and up to4.6% on the test set relative to the LEFTMOSTbaseline, and 5.1% (cross-validation) and 6.8%(test) margins relative to VPREP.The best scoring result of our LATENTLCmodel in the cross-validation scenario is 65.72%,obtained by the feature set All.
The best scoringresult by any of the baseline models in this sce-nario is 62.7%, obtained by the same feature set.For the test set scenario, LATENTLC obtained itshighest accuracy, 65.73%, when using the featureset Basic.
This is a substantial improvement overthe highest scoring baseline model in this scenariothat obtained 61.6% accuracy, using the feature setAll.
This performance gap is substantial when tak-ing into consideration that the improvements ob-tained by the highly competitive DIRT similarityfeatures using the stronger LEFTMOST baseline,result in an improvement of 3.1% and 5.3% overthe trivial ALLNEG baseline in the test set andcross-validation scenarios respectively.Comparing the different feature sets on our pro-posed model, we find that the Basic feature setgives a consistent and substantial increase over theSim feature set.
Improvements are of 2.8% (test)and 2.2% (cross-validation).
Introducing moreelaborate features (i.e., the feature sets NoLDAand All) yields some improvements in the cross-validation, but these improvements are not repli-cated on the test set.
This may be due to idiosyn-crasies in the test set that are averaged out in thecross-validation scenario.For a qualitative analysis, we took the best per-forming model of the data set (i.e., with the Basicfeature set), and extracted the set of instanceswhere it made a correct prediction while bothbaselines made an error.
This set contains manyverb-preposition pairs, such as ?list as ?
reportas?
or ?submit via?
deliver by?, underscoring theutility of leveraging multiple LCs rather than con-sidering only a head word (as with LEFTMOST)or the entire phrase (as with VPREP).
Other ex-amples in this set contain more complex patterns.These include the positive pairs ?talk much about?
have much to say about?
and ?increase with?
go up with?, and the negative ?make predic-tion about ?
meet the challenge of?
and ?enjoywatching?
love to play?.6 DiscussionRelation to CDS.
Much recent work subsumedunder the title Compositional Distributional Se-mantics addressed the distributional representa-tion of multi-word phrases (see Section 2).
Thisline of work focuses on compositional predicates,such as ?kick the ball?
and not on idiosyncraticpredicates such as ?kick the bucket?.A variant of the CDS approach can be framedwithin ours.
Assume we wish to compute thesimilarity of the predicates pL= (w1, ..., wn)and pR= (w?1, ..., w?m).
Let us denote the vec-tor space representations of the individual wordsas v1, ..., vnand v?1, ..., v?mrespectively.
A stan-dard approach in CDS is to compose distributionalrepresentations by taking their vector sum vL=v1+ v2...+ vnand vR= v?1+ ...+ v?m(Mitchelland Lapata, 2010).
One of the most effective sim-ilarity measures is the cosine similarity, which is anormalized dot product.
The distributional sim-ilarity between pLand pRunder this model issim(pL, pR) =?ni=1?mj=1sim(wi, w?j), wheresim(wi, w?j) is the dot product between viand v?j.This similarity score is similar in spirit to asimplified version of our statistical model thatrestricts the set of allowable LCs Hpto be{({wi}, {w?j})|i ?
n, j ?
m}, i.e., only LCs ofsize 1.
Indeed, taking Hpas above, and cosinesimilarity as the only feature (i.e., w ?
R), yieldsthe distribution651Test Set Cross ValidationAlgorithm Sim Basic NoLDA All Sim Basic NoLDA AllLATENTLC 62.9 65.7 64.4 64.6 62.7 ?
1.9 64.9 ?
1.9 65.0 ?
1.7 65.7 ?1.9LEFTMOST 59.0 61.1 60.0 60.4 61.2 ?
2.1 62.5 ?
2.4 62.4 ?2.2 62.7 ?
2.0VPREP 56.1 60.9 60.7 61.6?58.1 ?
1.7 60.8 ?
2.2 60.4 ?
2.6 60.6 ?
2.2ALLNEG 55.9 55.9Table 2: Results for the various evaluated systems.
Accuracy results are presented in percents, followed in the cross vali-dation scenario by the standard deviation over the folds.
The rows correspond to the various systems as defined in Section 4.LATENTLC is our proposed model.
The columns correspond to the various feature sets, from the least to the most inclusive.SIM includes only similarity features.
BASIC additionally includes POS-based and frequency features.
NOLDA includes allfeatures except LDA-based features.
ALL is the full feature set.
ALLNEG is the classifier that invariably predicts the label ?noinference?.
Bold marks best overall accuracy per column, and?marks figures that are not significantly worse (McNemar?s test,p < 0.05).
The same positive to negative label ratio was maintained in both the cross validation and test set scenarios.
In allcases, LATENTLC obtains substantial improvements over the baseline systems.P (y|p) ?X(wi,w?j)?Hpexp`w ?
y ?
sim(wi, w?j)?.This derivation highlights the relation of a sim-plified version of our approach to the additiveCDS model, as both approaches effectively aver-age over the similarities of all pairs of words in pLand pR.
The derivation also highlights a few ad-vantages of our approach.
First, our approach al-lows to straightforwardly introduce additional fea-tures and to weight them in a way most consistentwith the task at hand.
Second, it allows much moreflexibility in defining the set of allowable LCs,Hp.Specifically, Hpmay contain LCs of sizes greaterthan 1.
Third, our approach uses standard proba-bilistic modelling, and therefore has a natural sta-tistical interpretation.In order to appreciate the effect of these advan-tages, we perform an experiment that takes H tobe the set of all LCs of size 1, and uses a sin-gle similarity measure.
We run a 10-fold cross-validation on our training data, obtaining 61.3%accuracy using COSINE and 62.2% accuracy us-ing BInc.
The performance gap between these re-sults and the accuracy obtained by our full model(65.7%) underscores the latter?s effectiveness inintegrating multiple features and LCs.Effectiveness of Optimization Method.
Ourmaximization of the log-likelihood function isnot guaranteed to converge to a global optimum.Therefore, the quality of the learned parametersmay be sensitive to the initialization point.
Wehereby describe an experiment that tests the sen-sitivity of our approach to such variance.Selecting the highest scoring feature set on ourtest set (i.e., BASIC), we ran the model with mul-tiple initializers, by randomly perturbing our stan-dard convex initializer (see Section 3).
Concretely,given a convex initializer w, we select the startingpoint to be w + ?, where ?i?
N (0, ?|wi|).
Weran this experiment 400 times with ?
= 0.8.To combine the resulting weight vectors into asingle classifier, we apply two types of standardapproaches: a Product of Experts (Hinton, 2002),as well as a voting approach that selects the mostfrequently predicted label.
Neither of these exper-iments yielded any significant performance gain.This demonstrates the robustness of our optimiza-tion method to the initialization point.7 ConclusionWe have presented a novel approach to thedistributional representation of multi-word pred-icates.
Since MWPs demonstrate varying levelsof compositionality, a uniform treatment of MWPseither as fixed expressions or through head wordsis lacking.
Instead, our approach integrates mul-tiple lexical units contained in the predicate.
Theapproach takes into account both multi-word LCsthat address low compositionality cases, as well assingle-word LCs that address compositional casesand are more frequent.
It assumes a latent distribu-tion over the LCs of the predicates, and estimatesit relative to a target application task.We addressed the supervised inference identi-fication task, obtaining substantial improvementover state-of-the-art baseline systems.
In futurework we intend to assess the benefit of this ap-proach in MWP classes that are well-known fromthe literature.
We believe that a permissive ap-proach that integrates multiple analyses wouldperform better than standard single-analysis meth-ods in a wide range of applications.Acknowledgements.
We would like to thankMike Lewis, Reshef Meir, Oren Melamud,Michael Roth and Nathan Schneider for their help-ful comments.
This work was supported by ERCAdvanced Fellowship 249520 GRAMPLUS.652ReferencesAlex Alsina, Joan Wanda Bresnan, and Peter Sells.1997.
Complex predicates.
Center for the Study ofLanguage and Information.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL 2003 workshop on Multiwordexpressions: analysis, acquisition and treatment-Volume 18, pages 89?96.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InEMNLP, pages 1183?1193.Jonathan Berant, Jacob Goldberger, and Ido Dagan.2011.
Global learning of typed entailment rules.
InACL, pages 610?619.Chris Biemann and Eugenie Giesbrecht.
2011.
Dis-tributional semantics and compositionality 2011:Shared task description and results.
In Workshopon Distributional Semantics and Compositionality,pages 21?28.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet alocation.
the Journal ofmachine Learning research, 3:993?1022.Miriam Butt.
2010.
The light verb jungle: still hack-ing away.
In Complex predicates: cross-linguisticperspectives on event structure, pages 48?78.
Cam-bridge University Press.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
In J. vanBentham, M. Moortgat, and W. Buszkowski, editors,Linguistic Analysis, volume 36, pages 435?384.Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-simo Zanzotto.
2013.
Recognizing textual entail-ment: Models and applications.
Synthesis Lectureson Human Language Technologies, 6(4):1?220.Georgiana Dinu and Mirella Lapata.
2010.
Topic mod-els for meaning similarity in context.
In COLING:Posters, pages 250?258.Georgiana Dinu and Rui Wang.
2009.
Inference rulesand their application to recognizing textual entail-ment.
In EACL, pages 211?219.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In EMNLP, pages 1535?1545.Afsaneh Fazly and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
In EACL, pages 337?344.Masato Hagiwara, Yasuhiro Ogawa, and KatsuhikoToyama.
2009.
Supervised synonym acquisition us-ing distributional features and syntactic patterns.
In-formation and Media Technologies, 4(2):558?582.Geoffrey E Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural com-putation, 14(8):1771?1800.Matthew Hoffman, Francis R Bach, and David M Blei.2010.
Online learning for latent Dirichlet alocation.In NIPS, pages 856?864.Ray Jackendoff.
2002.
Foundations of language:Brain, meaning, grammar, evolution.
Oxford Uni-versity Press.Douwe Kiela and Stephen Clark.
2013.
Detect-ing compositionality of multi-word expressions us-ing nearest neighbours in vector space models.
InEMNLP, pages 1427?1432.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
TACL, 1:179?192.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
discov-ery of inference rules from text.
In SIGKDD 2001,pages 323?328.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In COLING-ACL, pages 768?774.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In ACL, pages 317?324.Edward Loper and Steven Bird.
2002.
NLTK: Thenatural language toolkit.
In ACL Workshop on Ef-fective tools and methodologies for teaching naturallanguage processing and computational linguistics,pages 63?70.Diana McCarthy, Bill Keller, and John Carroll.2003.
Detecting a continuum of compositionalityin phrasal verbs.
In ACL workshop on Multiwordexpressions: analysis, acquisition and treatment,pages 73?80.Oren Melamud, Jonathan Berant, Ido Dagan, JacobGoldberger, and Idan Szpektor.
2013a.
A two levelmodel for context sensitive inference rules.
In ACL2013, pages 1331?1340.Oren Melamud, Ido Dagan, Jacob Goldberger, and IdanSzpektor.
2013b.
Using lexical expansion to learninference rules from sparse data.
In ACL: Short Pa-pers, pages 283?288.Shachar Mirkin, Ido Dagan, and Maayan Geffet.
2006.Integrating pattern-based and distributional similar-ity methods for lexical entailment acquisition.
InCOLING-ACL: Poster Session, pages 579?586.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Jorge.
Nocedal and Stephen J Wright.
1999.
Numeri-cal optimization, volume 2.
Springer New York.653Karl Pichotta and John DeNero.
2013.
Identify-ing phrasal verbs using many bilingual corpora.
InEMNLP, pages 636?646.Carlos Ramisch, Aline Villavicencio, and Valia Kor-doni.
2013.
Introduction to the special issue onmultiword expressions: From theory to practice anduse.
ACM Transactions on Speech and LanguageProcessing (TSLP), 10(2):3.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answeringsystem.
In ACL, pages 41?47.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality incompound nouns.
In IJCNLP, pages 210?218.Radim Rehurek and Petr Sojka.
2010.
Software frame-work for topic modelling with large corpora.
In Pro-ceedings of LREC 2010 workshop New Challengesfor NLP Frameworks, pages 46?50.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent Dirichlet alocation method for selectional pref-erences.
In ACL, pages 424?434.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2001.
Multiwordexpressions: A pain in the neck for NLP.
In CI-CLing, pages 1?15.Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,and Jesse Davis.
2010.
Learning first-order Hornclauses from web text.
In EMNLP, pages 1088?1098.Diarmuid?O.
S?eaghdha.
2010.
Latent variable modelsof selectional preference.
In ACL 2010, pages 435?444.Maggie Seaton and Alison Macaulay, editors.
2002.Collins COBUILD Idioms Dictionary.
Harper-Collins Publishers, 2nd edition.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between NE pairs.In IWP, pages 4?6.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In HLT-NAACL, pages 304?311.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In COLING, pages849?856.Yuancheng Tu and Dan Roth.
2011.
Learning Englishlight verb constructions: contextual or statistical.
InACL HLT 2011, page 31.Peter D Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Aline Villavicencio, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In EMNLP-CoNLL, pages 1034?1043.Veronika Vincze, Istv?an Nagy T., and Rich?ard Farkas.2013.
Identifying English and Hungarian light verbconstructions: A contrastive approach.
In ACL:Short Papers, pages 255?261.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In EMNLP, pages81?88.Hila Weisman, Jonathan Berant, Idan Szpektor, andIdo Dagan.
2012.
Learning verb inference rulesfrom linguistically-motivated evidence.
In EMNLP-CoNLL, pages 194?204.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Second AAAI Sym-posium on Quantum Interaction, volume 26, pages28?35.Alison Wray.
2008.
Formulaic language: Pushing theboundaries.
Oxford University Press.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using se-lectional preferences.
In ACL-COLING, pages 849?856.Naomi Zeichner, Jonathan Berant, and Ido Dagan.2012.
Crowdsourcing inference-rule evaluation.
InACL: Short Papers, pages 156?160.654
