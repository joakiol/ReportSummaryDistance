Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 665?673,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA model of local coherence effects in human sentence processingas consequences of updates from bottom-up prior to posterior beliefsKlinton Bicknell and Roger LevyDepartment of LinguisticsUniversity of California, San Diego9500 Gilman Dr, La Jolla, CA 92093-0108{kbicknell,rlevy}@ling.ucsd.eduAbstractHuman sentence processing involves integrat-ing probabilistic knowledge from a variety ofsources in order to incrementally determinethe hierarchical structure for the serial inputstream.
While a large number of sentence pro-cessing effects have been explained in terms ofcomprehenders?
rational use of probabilisticinformation, effects of local coherences havenot.
We present here a new model of localcoherences, viewing them as resulting from abelief-update process, and show that the rele-vant probabilities in our model are calculablefrom a probabilistic Earley parser.
Finally, wedemonstrate empirically that an implementedversion of the model makes the correct predic-tions for the materials from the original exper-iment demonstrating local coherence effects.1 IntroductionThe task of human sentence processing, recoveringa hierarchical structure from a serial input fraughtwith local ambiguities, is a complex and difficultproblem.
There is ample evidence that comprehen-ders understand sentences incrementally, construct-ing interpretations of partial structure and expecta-tions for future input (Tanenhaus et al, 1995; Alt-mann and Kamide, 1999).
Many of the main behav-ioral findings in the study of human sentence pro-cessing have now been explained computationally.Using probabilistic models trained on large-scalecorpora, effects such as global and incremental dis-ambiguation preferences have been shown to be aresult of the rational use of syntactic probabilities(Jurafsky, 1996; Hale, 2001; Narayanan and Juraf-sky, 2001; Levy, 2008b; Levy et al, 2009).
Simi-larly, a number of other effects in both comprehen-sion and production have been modeled as resultingfrom rational strategies of languages users that takeinto account all the probabilistic information presentin the linguistic signal (Genzel and Charniak, 2002;Genzel and Charniak, 2003; Keller, 2004; Levy andJaeger, 2007).One class of results from the literature that hasnot yet been explained in terms of a rational com-prehender strategy is that of local coherence effects(Tabor et al, 2004; Gibson, 2006; Konieczny andMu?ller, 2007), cases in which it appears that theparser is systematically ignoring contextual infor-mation about possible syntactic structures and pur-suing analyses that are probable only locally.
Theseeffects are problematic for rational models, becauseof the apparent failure to use all of the available in-formation.
This paper describes a new model of lo-cal coherence effects under rational syntactic com-prehension, which proposes that they arise as a re-sult of updating prior beliefs about the structuresthat a given string of words is likely to have to pos-terior beliefs about the likelihoods of those struc-tures in context.
The critical intuition embodied inthe model is that larger updates in probability distri-butions should be more processing-intensive; hence,the farther the posterior is from the prior, the moreradical the update required and the greater the pro-cessing load.
Section 2 describes the problem of lo-cal coherences in detail and Section 3 describes ex-isting models of the phenomenon.
Following that,Sections 4?5 describe our model and its computa-665SNPDettheNplayerVPVtossedSNPDTheNcoachVPVsmiledPPPatSNPDtheNplayerVPVtossedFigure 1: The difficulty of explaining local-coherence effects as traditional garden-pathing.tion from a probabilistic Earley parser.
Section 6presents the results of an experiment showing thatour model makes the correct predictions for the lo-cal coherence effects seen in the original paper byTabor et al (2004).
Finally, Section 7 concludes anddiscusses the insight our model gives into humanperformance.2 Local coherencesThe first studies to report effects of local coherencesare described in Tabor et al (2004).
In Experiment1, they use a self-paced reading task and materialscontaining relative clauses (RCs) attached to nounsin non-subject position as in (1).
(1) a.
The coach smiled at the player tossed afrisbee by the opposing team.b.
The coach smiled at the player who wastossed a frisbee by the opposing team.c.
The coach smiled at the player thrown afrisbee by the opposing team.d.
The coach smiled at the player who wasthrown a frisbee by the opposing team.Their experimental design crossed RC reductionwith verb ambiguity.
RCs are either reduced (1a,1c)or unreduced (1b,1d), and the RC verb is either lex-ically ambiguous between a past tense active and apast participle (1a?1b), or is unambiguously a pastparticiple (1c?1d).Tabor et al point out that in one of these fourconditions (1a) there is a locally coherent string theplayer tossed a frisbee.
Out of context (e.g., if itwere starting a sentence) this string would have alikely parse in which tossed is a past tense activeverb, the player is its agent, and a frisbee is itstheme (Figure 1, left).
The preceding context withinthe sentence, however, should rule out this interpre-tation because the player appears within a PP andhence should not be able to be the subject of a newsentence (Figure 1, right).
That is, given the preced-ing context, the player tossed a frisbee must begina reduced RC, such that there is no local ambiguity.Thus, if comprehenders are making full use of thelinguistic context, (1a) should be no more difficultthan the other examples, except insofar as ambigu-ous verbs are harder than unambiguous verbs, andreduced RCs are harder than unreduced RCs, pre-dicting there would be only the two main effects ofRC reduction and verb ambiguity on reading timesfor the tossed a frisbee region.Tabor et al, however, predict an interaction suchthat (1a) will have added difficulty above and be-yond these two effects, because of the interferencefrom the locally coherent parse of the player tossed afrisbee.
Concordant with their predictions, they findan interaction in the tossed a frisbee region, suchthat (1a) is super-additively difficult.
Because thisresult requires that an impossible parse influences aword?s difficulty, it is in direct opposition to the pre-dictions of theories of processing difficulty in whichthe probability of a word given context is the pri-mary source of parsing difficulty, and more gener-ally appears to be in opposition to any rational the-ory, in which comprehenders are making use of allthe information in the linguistic context.3 Existing modelsWith the results showing local coherence effectsin mind, we can ask the question of what sortsof theories do predict these effects.
This sectionbriefly describes two recent examples of such the-ories.
The first involves dynamical systems modelsto explain the effects and the second uses a mathe-matical model of the combination of bottom-up andtop-down probabilistic information.In Tabor and Hutchins?s (2004) SOPARSE (self-organized parse) model, reading a word activates aset of lexically anchored tree fragments.
Throughspreading activation between compatible fragmentsand inhibition between incompatible ones, these treefragments then compete in a process which is sen-sitive only to the local environment, i.e., ignoringthe global grammatical context.
Eventually, the sys-666tem stabilizes to the correct parse, and reading timesfor each word are modeled as the time the systemtakes to stabilize after reading a word.
Stabilizationtakes longer for locally coherent regions because thelocally coherent parse will be created and competewith the globally grammatical parse.There are, however, unresolved issues with thismodel.
The model has a number of free parameters,relating to the equations used for the competition,the method by which links between fragments areformed, as well as the question of precisely whattree fragments a given word will activate.
While Ta-bor and Hutchins (2004) work out these questionsin detail for the types of sentences they model, it isunclear how the model could be scaled up to makepredictions for arbitrary types of sentences.
That is,there is no principled system for setting the threetypes of parameters mentioned, and thus no clear in-terpretation of their values.
The model put forwardin this paper is an attempt to remedy this situation.A recent proposal by Gibson (2006) can also ex-plain some of the local coherence results.
Gibson?sproposal is that part-of-speech ambiguities have aspecial status in parsing; in effect, lexical part-of-speech ambiguities can be thought of as one-wordlocal coherences.
In this model, a probability func-tion P?
is calculated over part-of-speech tags givena word.
This probability for tag ti and a word w,P?
(ti|w), is proportional to the context-independentprobability of ti given the word w, P (ti|w) ?
thebottom-up component ?
multiplied by a smoothedprobability Ps of the tag given the context ?
the top-down component:P?
(ti|w) = P (ti|w)Ps(ti|context)?t?TP (t|w)Ps(t|context)(1)Difficulty is predicted to be high when the probabil-ity P?
of the correct tag is low.Because the top-down probabilities are smoothedto allow for all possible parts-of-speech, any wordwhich is lexically ambiguous will be more difficultto process, regardless of whether it is ambiguous ornot in its context.
This can thus explain some of thedifference between the ambiguous and unambiguousverbs in Tabor et al (2004).
It is not clear, however,under such a model why the super-additive interac-tion would obtain?that is, why (1a) should be somuch harder than (1b) starting at the word tossed.In addition, Gibson?s model is a bit underspecified:he does not discuss how the top-down probabilitiesare calculated, nor what the precise linking hypothe-sis is between the final P?
and reading times.
Finally,it is not at all clear why the top-down expectationsshould be smoothed, since the smoothing actuallyhas negative consequences on the processor?s per-formance.4 Parsing as belief updateThe basic intuition behind the model presented hereis that incrementally processing a sentence can beconceptualized as a process of updating one?s be-liefs.
Such an analogy has been used to moti-vate surprisal-based theories of sentence processing(Hale, 2001; Levy, 2008a), where beliefs about thestructure of a sentence after seeing the first i ?
1words in the sentence, which we denote as wi?10 ,are updated upon encountering wi.
In this case, thesurprisal of a word (?
logP (wi|wi?10 )) is equiva-lent to the Kullback-Leibler divergence of the beliefsafter wi0 from the beliefs after wi?10 (Levy, 2008a).Our model focuses on another belief-update processin sentence processing: updating beliefs about thestructures that a string of words is likely to have in-dependent of context to beliefs about what structuresit is likely to have in context.
A bit more formally, itviews the process of integrating a string of wordswji into a sentence as beginning with a ?bottom-up?
prior distribution of syntactic structures likely tospanwji and integrating that with ?top-down?
knowl-edge from the previous words in the sentence wi0 inorder to reach a posterior distribution conditioningon wj0 over which structures actually can span wji .This belief update process can be viewed as a ratio-nal reconstruction of the Tabor and Hutchins (2004)model, where ?
instead of the system dynamics ofcompetition between arbitrary tree fragments ?
dif-ferences between prior and posterior probability dis-tributions over syntactic structures determine pro-cessing difficulty.More formally still, when integrating wji into asentence, for each syntactic category X , we can de-fine the prior probability conditioned only onwji thatwji will form the beginning of that category, i.e., thatanX exists which begins at index i and spans at least667through j:Prior: P (Xk?ji |wji ) (2)It is important to note here that this prior probabilityis conditional only on the value of wji and not thevalues of i or j; that is, in the prior probability, i andj should be interpreted merely as a way to coindexthe start and end points of the string of words beingintegrated with a category X potentially spanningthem, and not as making reference to position in thefull sentence string.For each categoryX , this prior probability will beupdated to the posterior probability of that categoryspanning wji given all the words seen so far:Posterior: P (Xk?ji |wj0) (3)In the equation for the posterior, of course, the in-dices i and j are positions in the sentence string, andnot merely coindices.Given these prior and posterior beliefs, we pre-dict difficulty to arise in cases where the prior re-quires substantial modification to reach the poste-rior, that is, cases in which the prior and poste-rior make substantially different predictions for cat-egories.
A strong local coherence will have sharplydifferent prior and posterior distributions, causingdifficulty.
We represent the prior and posterior be-liefs as vectors of the probabilities of each syntacticcategory spanningwji , and measureMij , the amountof modification required, as the summed K-L diver-gence of the prior from the posterior vector.
That is,if N is the set of nonterminals in the grammar, thesize of the belief update is modeled as1Mij def=?X?ND(P (Xk?ji |wj0) ||P (Xk?ji |wji ))In the remainder of the paper, we show how to com-pute Mij by using Bayesian inference on quanti-ties calculated in ordinary probabilistic incremen-tal Earley parsing with a stochastic context-free1Note that for each syntactic category X ?
N , the proba-bility distribution P (Xk?ji |I) for some information I is over abinary random variable indicating the presence of X .
The dif-ferent syntactic categories X that could span from i to any kare not mutually exclusive, hence we cannot define size of be-lief update as a single K-L divergence defined over multinomialdistributions.grammar (SCFG), and show that our model makesthe correct predictions using an SCFG for Englishon the original local-coherences experiment of Ta-bor et al (2004).5 Computing priors and posteriorsFor SCFGs, a probabilistic Earley parser (Earley,1970; Stolcke, 1995) provides the basic quantitieswe need to compute the prior (2) and posterior(3) for each category X .
Following Stolcke, weuse capital Latin characters to denote non-terminalcategories and use lowercase Greek characters todenote (possibly null) sequences of terminals andnon-terminals.
We write the probability that a non-terminal X can be recursively rewritten by SCFGrules as a certain series of symbols ?
byP (X ??
?
)An edge built from the rule X ?
??
where ?
hasbeen recognized as beginning at position i and end-ing at position j is denotedj : Xi ?
?.
?The forward probability of that edge at position j,?j , is defined to be the joint probability that the rootnode will generate all words recognized so far wj0 aswell as the edge?j(Xi ?
?.?
)With this terminology, we are now in a position todescribe how we calculate the posterior and priorprobability vectors for our model.5.1 Calculating the posteriorTo calculate the posterior, we first use the definitionof conditional probability to rewrite it asP (Xk?ji |wj0) =P (Xk?ji , wj0)P (wj0)In a context-free grammar, given the syntactic cat-egory that dominates a string of words, the words?probability is independent from everything outsidethe category.
Thus, this is equivalent toP (Xk?ji |wj0) =P (wio, Xi)P (wji |Xk?ji )P (wj0)= P (S??
wi0X?
)P (X ??
wji?
)P (S ??
wj0?
)6685.1.1 Posterior: the numerator?s first termThe first term in the numerator P (S ??
wi0X?
)can be computed from a parse of wi0 by summingforward probabilities of the form?i(Xi ?
.?)
(4)5.1.2 Posterior: the denominatorSimilarly, the denominator P (S ??
wj0?)
can becomputed from a parse of wj0 by summing forwardprobabilities of the form?j(Y ?
?wjj?1.?)
(5)for all Y.
This is because the forward probability ofa state is conditioned on generating all the previouswords.5.1.3 Posterior: the numerator?s second termThe second term in the numerator P (X ??
wji?
)for an arbitrary category X cannot necessarily becalculated from a probabilistic Earley parse of thesentence, because the parser does not constructstates that are not potentially useful in forming a sen-tence (i.e., states that would have a forward proba-bility of zero.)
However, to calculate the probabilityof X generating words wji we can parse wji sepa-rately with a goal category of X .
From this parse,we can extract the probability of wji being generatedfrom X in the same way as we extracted the proba-bility of wj0 being generated from S, i.e., as a sum offorward probabilities at j (Eq.
5).25.2 Calculating the priorTo calculate the prior, we first use Bayes rule torewrite it asP (Xk?ji |wji ) =P (wji |Xk?ji )P (Xk?ji )P (wji )(6)Recall that at this point, i and j do not refer to in-dex positions in the actual string but rather serve toidentify the substring wji of interest.
That is, P (wji )denotes the probability that at an arbitrary point in2To calculate the posterior, it is not necessary to parse wjiseparately, since these states are only excluded from the parsewhen their forward probability is zero, in which case the firstterm in the numerator will also be zero.
A separate parse is nec-essary, however, when using this term to calculate the prior.Table 1: Event space for the priorEvent DescriptionE0: There are at least i?
words |w| ?
i?E1: A category X begins at i?
Xi?E2: An Xi?
spans at least through j Xk?ji?E3: There are at least j words |w| ?
jE4: Words wji?
are these specific w?ji?
wji?
= w?ji?an arbitrary sentence, the next j ?
i words will bewji , and P (Xk?ji ) denotes the probability that an ar-bitrary point in an arbitrary sentence will be the leftedge of a category X that spans at least j ?
i words.None of the three terms in Eq.
6 can be directly ob-tained.
However, we can obtain a very good approx-imation of Eq.
6 as follows.
First, we marginalizeover the position within a sentence with which theleft edge i might be identified:P (Xk?ji |wji ) =?i?=0,1,...(P (wji?
|Xk?ji?
)P (Xk?ji?
)P (wji?
))P (i = i?
)(7)In Eq.
7, i?
is identified with the actual string positionwithin the sentence.Second, we rewrite the first term in this sum withevent space notation, using the event space given inTable 1.P (wji?
|Xk?ji?
)P (Xk?ji?
)P (wji?
)= P (E0,3,4|E0...3)P (E0...3)P (E0,3,4)= P (E4|E0...3)P (Eo...3)P (Eo,3,4)Applying the chain rule, we can further simplify.= P (E4|E0...3)P (E1...3|E0)P (E0)P (E3,4|E0)P (E0)= P (E4|E0...3)P (E1...3|E0)P (E3,4|E0)= P (E2...4|E0, E1)P (E1|E0)P (E3,4|E0)669Switching back from event space notation and sub-stituting this term into Eq.
7, we now haveP (Xk?ji |wji ) =?i?=0,1,...(P (wji?
|Xi?
, E0)P (Xi?
|E0)P (wji?
|E0))P (i = i?
)(8)Thus, by conditioning all terms on E0, the presenceof at least i?
words, we have transformed the proba-bilities we need to calculate into these four terms,which are easier to calculate from the parser.
Wenow consider how to calculate each of the terms.5.2.1 Prior: the numerator?s first termThe first term in the numerator can be simplifiedbecause our grammar is context-free:P (wji?
|Xi?
, E0) = P (wji?
|Xi?
)= P (X ??
wji?
)This can be computed as described in Section 5.1.3.5.2.2 Prior: the numerator?s second termThe second term in the numerator can be rewrittenas follows:P (Xi?
|E0) = P (Xi?
, E0)P (E0)= P (S??
w?i?0X?
)P (S ??
w?i?0 ?
)where w?i?0 denotes any sequence of i?
words.
Givena value i?
we can calculate both terms by parsingthe string w?i0X , where each word w?
in w?i0X is aspecial word that can freely act as any preterminal.The denominator can then be calculated by summingthe forward probabilities of the last word w?ii?1 as inEq.
5, and the numerator by summing the forwardprobability of X , as in Eq.
4.5.2.3 Prior: the denominatorThe denominator in the calculation of the priorcan be calculated in a way analogous to the numera-tor?s second term (Section 5.2.2):P (wji?
|E0) =P (wji?
, E0)P (E0)= P (S??
w?i?0wji??
)P (S ??
w?i?0 ?
)5.2.4 Prior: starting position probabilityFinally, we must calculate the second term inEq.
8, the probability of the starting positionP (i = i?).
Given that all our terms are conditionalon the existence of all words in the sentence up toi?
(E0), the probability of a starting position P (i) isthe probability of drawing i?
randomly from the setof positions in sentences generated by the grammarsuch that all words up to that position exist.
For mostlanguage grammars, this distribution can be easilyapproximated by a sample of sentences generatedfrom the SCFG, since most of the probability massis concentrated in small indices.6 ExperimentWe tested the predictions of an implemented ver-sion of our model on the materials from Ta-bor et al (2004).
To generate quantitative predic-tions, we created a small grammar of relevant syn-tactic rules, and estimated the rule probabilities fromsyntactically annotated text.
We calculated summedK-L divergence of the prior from the posterior vectorfor each word in the Tabor et al items, and predictthis sum to be largest at the critical region when thesentence has an effect of local coherence.6.1 Methods6.1.1 GrammarWe defined a small SCFG for the problem, and es-timated its rule probabilities using the parsed Browncorpus.
The resulting SCFG is identical to that usedin Levy (2008b) and is given in Table 2.6.1.2 LexiconLexical rewrite probabilities for part-of-speechtags were also estimated using the entire parsedBrown corpus.6.1.3 MaterialsThe materials were taken from Experiment 1 ofTabor et al (2004).
We removed 8 of their 20 itemsfor which our trained model either did not know thecritical verb or did not know the syntactic structureof some part of the sentence.
For the other 12 items,we replaced unknown nouns (9 instances) and un-known non-critical verbs (2 instances), changed oneplural noun to singular, and dropped one sentence-initial prepositional phrase.670Table 2: The SCFG used in Experiment 3.
Ruleweights given as negative log-probabilities in bits.Rule WeightROOT ?
S 0S ?
S-base CC S-base 7.3S ?
S-base 0.01S-base ?
NP-base VP 0NP ?
NP-base RC 4.1NP ?
NP-base 0.5NP ?
NP-base PP 2.0NP-base ?
DT NN NN 4.7NP-base ?
DT NN 1.9NP-base ?
DT JJ NN 3.8NP-base ?
PRP 1.0NP-base ?
NNP 3.1VP/NP ?
VBD NP 4.0VP/NP ?
VBD 0.1VP ?
VBD PP 2.0VP ?
VBD NP 0.7VP ?
VBD 2.9RC ?WP S/NP 0.5RC ?
VP-pass/NP 2.0RC ?WP FinCop VP-pass/NP 4.9PP ?
IN NP 0S/NP ?
VP 0.7S/NP ?
NP-base VP/NP 1.3VP-pass/NP ?
VBN NP 2.2VP-pass/NP ?
VBN 0.46.2 ProcedureFor these 12 items, we ran our model on the fourconditions in (1).
For each word, we calculatedthe prior and posterior vectors for substrings ofthree lengths at wi.
The summed K-L divergenceis reported for a substring length of 1 word us-ing a prior of P (Xk?ii?1 |wii?1), for a length of 2using P (Xk?ii?2 |wii?2), and for a length of 3 us-ing P (Xk?ii?3 |wii?3).
For all lengths, we predict thesummed divergence to be greater at critical wordsfor the part-of-speech ambiguous conditions (1a,1b)than for unambiguous (1c,1d), because the part-of-speech unambiguous verbs cannot give rise to a priorthat predicts for a sentence to begin.
For a substringlength of 3, we also predict that the divergence issuperadditively greatest in the ambiguous reducedcondition (1a), because of the possibility of startinglll l llll0510152025SummedK?L Divergence(bits)lat the player who was tossed/throwna frisbeelltossedwho was tossedthrownwho was thrownFigure 2: Summed K-L divergence of the prior fromthe posterior vectors at each word: Substring length1l lll l lll0510152025SummedK?L Divergence(bits)lat the player who was tossed/throwna frisbeelltossedwho was tossedthrownwho was thrownFigure 3: Summed K-L divergence of the prior fromthe posterior vectors at each word: Substring length2a sentence with the player tossed.6.3 ResultsThe results of the experiment are shown in Figures2?4.
For all three substring lengths, the model pre-dicts difficulty to be greater in the ambiguous condi-tions at the critical words (tossed/thrown a frisbee).For 1-word substrings, the effect is localized on thecritical verb (tossed/thrown), for 2-word substringsit is localized on the word directly following thecritical verb (tossed/thrown a), and for 3-word sub-strings there are two effects: one on the critical verb(the player tossed/thrown) and one two words later(tossed/thrown a frisbee).
Furthermore, for 3-wordsubstrings, the effect is superadditively greatest forthe player tossed.
These results thus nicely confirm671l l l l l l ll0510152025SummedK?L Divergence(bits)lat the player who was tossed/throwna frisbeelltossedwho was tossedthrownwho was thrownFigure 4: Summed K-L divergence of the prior fromthe posterior vectors at each word: Substring length3both of our predictions and demonstrate that a modelin which large belief updates from a bottom-up priorto a posterior induce difficulty is capable of account-ing for effects of local coherences.7 ConclusionThis paper has described a model of local coherenceeffects in sentence processing, which views the pro-cess of integrating a string of words wji into a sen-tence as a process of updating prior beliefs aboutthe structures spanning those words to posterior be-liefs.
These prior beliefs are simply the probabilitiesof those structures given only the words being inte-grated, and the posterior beliefs are the probabilitiesgiven the entire sentence processed thus far.
Diffi-culty is predicted to result whenever this update islarge ?
which we model in terms of a large summedK-L divergence of the prior from the posterior vec-tor.
We demonstrated a method of normatively cal-culating these probabilities from probabilistic Ear-ley parses and used this implemented model to makepredictions for the materials for the original experi-mental result of effects of local coherences (Tabor etal., 2004).
Our results demonstrated that the modelpredicts difficulty to occur at the correct part of thesentence in the correct condition.We improve on existing models in two ways.First, we make predictions for where local coher-ences should obtain for an arbitrary SCFG, not justone particular class of sentences.
This allows themodel to scale up for use with a broad coveragegrammar and to make predictions for arbitrary sen-tences, which was not possible with a model such asTabor & Hutchins (2004).Second, our model gives a rational basis to an ef-fect which has typically been seen to result from ir-rationality of the human sentence processor.
Specif-ically, the cost that our model describes of updatingbottom-up prior beliefs to in-context posterior be-liefs can be viewed as resulting from a rational pro-cess in the case that the bottom-up prior is availableto the human sentence processor more rapidly thanthe in-context posterior.
Interestingly, the fact thatthe prior is actually more difficult to compute thanthe posterior suggests that the only way it would beavailable more rapidly is if it is precomputed.
Thus,our model provides the insight that, to the extentthat comprehenders are behaving rationally in pro-ducing effects of local coherences, this may indi-cate that they have precomputed the likely syntac-tic structures of short sequences of words.
While itmay be unlikely that they calculate these probabil-ities for sequences directly from their grammar aswe do in this paper, there could be a number of waysto approximate this prior: for example, given a largeenough corpus, these probabilities could be approx-imated for any string of words that appears suffi-ciently often by merely tracking the structures thestring has each time it occurs.
Such a hypothesis forhow comprehenders approximate the prior could betested by manipulating the frequency of the relevantsubstrings in sentences with local coherences.This work can be extended in a number of ways.As already mentioned, one logical step is usinga broad-coverage grammar.
Another possibility re-lates to the problem of correlations between the dif-ferent components of the prior and posterior vec-tors.
For example, in our small grammar, whenever aROOT category begins, so does an S, an S-base, andan NP-base.
Dimensionality reduction techniques onour vectors may be able to remove such correlations.These steps and more exhaustive evaluation of a va-riety of datasets remain for the future.AcknowledgmentsThis research was supported by NIH Training GrantT32-DC000041 from the Center for Research inLanguage at UCSD to the first author.672ReferencesGerry T.M.
Altmann and Yuki Kamide.
1999.
Incremen-tal interpretation at verbs: Restricting the domain ofsubsequent reference.
Cognition, 73:247?264.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Dmitriy Genzel and Eugene Charniak.
2002.
Entropyrate constancy in text.
In Proceedings of the 40thannual meeting of the Association for ComputationalLinguistics, pages 199?206, Philadelphia, July.
Asso-ciation for Computational Linguistics.Dmitriy Genzel and Eugene Charniak.
2003.
Variationof entropy and parse trees of sentences as a function ofthe sentence number.
In Michael Collins and MarkSteedman, editors, Proceedings of the 2003 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 65?72, Sapporo, Japan.
Association forComputational Linguistics.Edward Gibson.
2006.
The interaction of top-down andbottom-up statistics in the resolution of syntactic cat-egory ambiguity.
Journal of Memory and Language,54:363?388.John Hale.
2001.
A probabilistic Earley parser as apsycholinguistic model.
In Proceedings of the SecondMeeting of the North American Chapter of the Associ-ation for Computational Linguistics, volume 2, pages159?166, New Brunswick, NJ.
Associate for Compu-tational Linguistics.Daniel Jurafsky.
1996.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience, 20:137?194.Frank Keller.
2004.
The entropy rate principle as apredictor of processing effort: An evaluation againsteye-tracking data.
In Dekang Lin and Dekai Wu, edi-tors, Proceedings of the 2004 Conference on EmpiricalMethods in Natural Language Processing, pages 317?324, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Lars Konieczny and Daniel Mu?ller.
2007.
Local co-herence interpretation in written and spoken language.Presented at the 20th Annual CUNY Conference onHuman Sentence Processing.
La Jolla, CA.Roger Levy and T. Florian Jaeger.
2007.
Speakers opti-mize information density through syntactic reduction.In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 849?856, Cambridge, MA.
MIT Press.Roger Levy, Florencia Reali, and Thomas L. Griffiths.2009.
Modeling the effects of memory on human on-line sentence processing with particle filters.
In Pro-ceedings of NIPS.Roger Levy.
2008a.
Expectation-based syntactic com-prehension.
Cognition, 106:1126?1177.Roger Levy.
2008b.
A noisy-channel model of rationalhuman sentence comprehension under uncertain input.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 234?243, Honolulu, Hawaii, October.
Association for Com-putational Linguistics.Srini Narayanan and Daniel Jurafsky.
2001.
A Bayesianmodel predicts human parse preference and read-ing time in sentence processing.
In T.G.
Dietterich,S Becker, and Z. Ghahramani, editors, Advances inNeural Information Processing Systems 14, pages 59?65, Cambridge, MA.
MIT Press.Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computes prefixprobabilities.
Computational Linguistics, 21(2):165?201.Whitney Tabor and Sean Hutchins.
2004.
Evidencefor self-organized sentence processing: Digging-in ef-fects.
Journal of Experimental Psychology: Learning,Memory, and Cognition, 30(2):431?450.Whitney Tabor, Bruno Galantucci, and Daniel Richard-son.
2004.
Effects of merely local syntactic coher-ence on sentence processing.
Journal of Memory andLanguage, 50:355?370.Michael K Tanenhaus, Michael J Spivey-Knowlton,Kathleen M Eberhard, and Julie C Sedivy.
1995.
Inte-gration of visual and linguistic information in spokenlanguage comprehension.
Science, 268:1632?1634.673
