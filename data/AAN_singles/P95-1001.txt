Learning Phono log ica l  Ru le  Probabi l i t ies  from Speech Corporaw i th  Exp loratory  Computat iona l  Phono logyGary Tajchman, Daniel Jurafsky, and Eric FoslerInternational Computer Science Institute andUniversity of California at Berkeley{tajchman,jurafsky, fosler}~icsi.berkeley.eduAbstractThis paper presents an algorithm for learn-ing the probabilities of optional phonolog-ical rules from corpora.
The algorithm isbased on using a speech recognition sys-tem to discover the surface pronunciationsof words in spe.ech corpora; using an auto-matic system obviates expensive phoneticlabeling by hand.
We describe the detailsof our algorithm and show the probabili-ties the system has learned for ten commonphonological rules which model reductionsand coarticulation effects.
These probabili-ties were derived from a corpus of 7203 sen-tences of read speech from the Wall StreetJournal, and are shown to be a reason-ably close match to probabilities from pho-netically hand-transcribed data (TIMIT).Finally, we analyze the probability differ-ences between rule use in male versus fe-male speech, and suggest hat the differ-ences are caused by differing average ratesof speech.1 IntroductionPhonological r-ules have formed the basis of phono-logical theory for decades, although their form andtheir coverage of the data has changed over the years.Until recently, however, it was difficult to deter-mine the relationship between hand-written phono-logical rules and actual speech data.
The currentavailability of large speech corpora and pronunci-ation dictionaries has allowed us to connect rulesand speech in much tighter ways.
For example, anumber of algorithms have recently been proposedwhich automatically induce phonological rules fromdictionaries or corpora (Gasser 1993; Ellison 1992;Daelemans c~ al.
1994).While such algorithms have successfully inducedsyllabicity or harmony constraints, or simple oblig-*Currently at Voice Processing Corp, 1 Main St,Cambridge, MA 02142: tajchman@vpro.eomatory phonological rules, there has been much lesswork on non-obligatory (optional) rules.
In part thisis because optional rules like flapping, vowel reduc-tion, and various coarticulation effects are postlexi-cal and often products of fast speech, and hence havebeen considered less central to phonological theory.In part, however, this is because optional rules areinherently probabilistic.
Where obligatory rules ap-ply to every underlying form which meets the en-vironmental conditions, producing a single surfaceform, optional rules may not apply, and hence theunderlying form may appear as the surface form,unmodified by the rule.
This makes the inductionproblem non-deterministic, and not solvable by theabove algorithms.
1While optional rules have received less attentionin linguistics because of their probabilistic nature,in speech recognition, by contrast, optional rules arecommonly used to model pronunciation variation.
Inthis paper, we employ techniques from speech recog-nition research to address the problem of assign-ing probabilities to these optional phonological rules.We introduce acompletely automatic algorithm thatexplores the coverage of a set of phonological ruleson a corpus of lexically transcribed speech using thecomputational resources of a speech recognition sys-tem.
This algorithm belongs to the class of tech-niques we call Exploratory Computational Phonol-ogy, which use statistical pattern recognition toolsto explore phonological spaces.We describe the details of our probability esti-mation algorithm and also present he probabilitiesthe system has learned for ten common phonologicalrules which model reductions and coarticulation ef-fects.
Our probabilities are derived from a corpus of7203 sentences of read speech from the Wall StreetJournal (NIST 1993).
We also benchmark the prob-abilities generated by our system against probabil-ities from phonetically hand-transcribed data, andshow a relatively good fit.
Finally, we analyze theprobability differences between rule use in male ver-1Note that this is true whether phonological theoryconsiders these true phonological rules or rather ules of~phonetic interpretation".sus female speech, and suggest hat the differencesare caused by differing average rates of speech.2 The  A lgor i thmIn this section we describe our algorithm which as-signs probabilities to hand-written, optional phono-logical rules like flapping.
The algorithm takes alexicon of underlying forms and applies phonologi-cal rules to produce a new lexicon of surface forms.Then we use a speech recognition system on a largecorpus of recorded speech to check how many timeseach of these surface forms occurred in the corpus.Finally, by knowing which rules were used to gener-ate each surface form, we can compute a count foreach rule.
By combining this with a count of thetimes a rule did not apply, the algorithm can com-pute a probability for each rule.The rest of this section will discuss each of theaspects of the algorithm in detail.2.1 The  Base  Lex iconOur base lexicon is quite large; it is used to gen-erate the lexicons for all of our speech recognitionwork at ICSI.
It contains 160,000 entries (words)with 300,000 pronunciations.
The lexicon containsunderlying forms which are very shallow; thus theyare post-lexical in the sense that there is no rep-resented relationship between e.g.
'critic' and 'criti-cism' (where critic is pronounced kritik and criticismkritisizrn).
However, the entries do not representflaps, vowel reductions, and other coarticulatory ef-fects.In order to collect our 300,000 pronunciations, wecombined seven different on-line pronunciation dic-tionaries, including the five shown in Table 12 .Source \[ Words \[ Base PronsCMU 95,781 99,279LIMSI 32,873 37,936"PRONLEX 30,353 30,354BRITPRON 77,685 85,450TTS 77,383 83,297All Prons399,26549,59781,936108,834111,028Table 1: Pronunciation sources used to build fullyexpanded lexicon.For further information about these sources pleaserefer to CMU (CMU 1993), LIMSI (Lamel 1993),PRONLEX (COMLEX 1994), BR ITPRON (Robin-son 1994).
A text-to-speech system was used to gen-2Although it was not relevant o the experiments de-scribed here, our lexicon also included two sources whichdirectly supply surface forms.
These were 13,362 hand-transcribed pronunciations of 5871 words from TIMIT(TIMIT 1990), and 230 pronunciations of 36 words de-rived in-house from the OGI Numbers database (Coleet al 1994).erate phone sequences from word orthography as anadditional source of pronunciations.\ [ IPA IARPAI ICS I  I IPA I ARPAI ICS I  Ib b b b ?
bcld d d d ?
dclg g g gO - gclp p p pO pclt t t t ?
- tclk k k k ?
- kcl(1 aa aa s s sae ae  z z zA ah ah J' sh shO ao ao ~ zh zheh eh f f f3 ~ er er v v vih ih IJ th thi iy iy 6 dh dho ow ow t j" ch chc~ uh uh dz jh jhu uw uw h hh hhct w aw aw l'i - hva ~ ay ay y y ye ey ey r r r3 y oy oy w w wel 1 1 1em m m men n n na ax rj ng ngi x  r dxaxr silence h# h#Table 2: Baseform phone set used was the ARPA-BET.
This was expanded to include syllabics, stopclosures, and reduced vowels, alveolar flap, andvoiced h.We represent pronunciations with the set of 54ARPAbet-like phones detailed in Table 2.
All thelexicon sources except LIMSI use ARPABET-l ikephone sets 3.
CMU, BRITPRON,  and PRONLEXphone sets include three levels of vowel stress.
Thepronunciations from all these sources were mappedinto our phone set using a set of obligatory rulesfor stop closures \[bcl, dcl, gcl, pcl, tcl, kcl\], and op-tional rules to introduce the syllabic consonants \[el,em, en\], reduced vowels \[ax, ix, axr\], voiced h \[hv\],and alveolar flap \[dx\].2.2 App ly ing  Phono log ica l  Ru les  to  Bu i ld  aSur face  Lex iconWe next apply phonological rules to our base lexi-con to produce the surface lexicon.
Since the rules3The LIMSI pronunciations already included the syl-labic consonants and reduced vowels.
For this reason,the words found only in the LIMSI source lexicon didnot participate in the probability estimates for the syl-labic and reduced vowel rules.2Name Code RuleReductionsMid vowels RV1High vowels RV2R-vowel RV3Syllabic n SL1Syllabic m SL2Syllabic 1 SL3Syllabic r SL4Flapping FL1Flapping-r FL2H-voicing VH1Table-stress \[aa ae ah ao eh er ey ow uh\]---~ ax-stress \[iy ih uw\] --* ix-stress er --* axr\[ax ix\] n --* en\[ax ix\] m ~ em\[ax ix\] 1 ---* el\ [ax ix\] r ~ ~xr\[tcl dcl\] \[t d\]--~ dx /V  \[ax ix axr\]?
\[tcl dcl\] \[t d\]--* dx /V  r __  \[ax ix axr\] .hh ~ hv / \[+voice\] \[+voice\]3: Phonological Rulesare optional, the surface lexicon must contain eachunderlying pronunciation unmodified, as well as thepronunciation resulting from the application of eachrelevant phonological rule.
Table 3 gives the 10phonological rules used in these experiments.One goal of our rule-application procedure wasto build a tagged lexicon to avoid having to imple-ment a phonological-rule parser to p~rse the surfacepronunciations.
In a tagged lexicon, each surfacepronunciation is annotated with the names of thephonological rules that applied to produce it.
Thuswhen the speech recognizer finds a particular pro-nunciation in the speech input, the list of rules whichapplied to produce it can simply be looked up in thetagged lexicon.The algorithm applies rules to pronunciations re-cursively; when a context matches the left hand sideof a phonological rule "RULE," two pronunciationsare produced: one unchanged by the rule (marked-RULE), and one with the rule applied (marked+RULE).
The procedure places the +RULE pro-nunciation on the queue for later recursive rule ap-plication, and continues trying to apply phonologicalrules to the -RULE pronunciation.
See Figure 1 fordetails of the algorithm.
While our procedure is notguaranteed to terminate, in practice the phonologi-cal rules we apply have a finite recursive depth.The nondeterministic mapping produces a taggedequiprobable multiple pronunciation lexicon of510,000 pronunciations for 160,000 words.
For ex-ample, Table 4 gives our base forms for the word"butter" :SourceTTSBPUBPUCMULIMPLXPronunciationbah  t axrb ah taxb ah t axrbah  t erbah  t axrbah  t erTable 4: Base forms for "butter"For each lexical item, L, do:Place all base prons of L onto queue qWhile Q is not empty do:Dequeue pronunciation P from qFor each phonological rule R, do:If context of R could apply to PApply R to P, giving P'Tag P' with +R, put on queueTag P with -ROutput P with tagsFigure 1: Applying Rules to the Base LexiconThe resulting tagged surface lexicon would havethe entries in Table 5.2.3 F i l te r ing  w i th  fo rced-V i te rb iGiven a lexicon with tagged surface pronunciations,the next required step is to count how many timeseach of these pronunciations occurs in a speechcorpus.
The algorithm we use has two steps;PHONETIC LIKELIHOOD ESTIMATION and FORCED-VITERBI ALIGNMENT.In the first step, PHONETIC LIKELIHOOD ESTI-MATION, we examine each 20ms frame of speechdata, and probabilistically abel each frame with thephones that were likely to produce the data.
Thatis, for each of the 54 phones in our phone-set, wecompute the probability that the slice of acousticdata was produced by that phone.
The result ofthis labeling is a vector of phone-likelihoods for eachacoustic frame.Our algorithm is based on a multi-layer percep-tron (MLP) which is trained to compute the condi-tional probability of a phone given an acoustic fea-ture vector for one frame, together with 80 ms ofsurrounding context.
Bourlard ~ Morgan (1991)3bcl b ah dx ax:+BPU +FL1; +CWtl +FL1 +RVl; +PLX +FL1 +RVlbcl bah dx axr: +TTS +FL1; +BPU +FL1; +CI~J +FL1 -RVl +RV3; +LIM +FL1; +PLX +FL1 -RV1 +RV3bcl b ah tel t ax:+BPU -FL1; +C~d -FL1 +RV1; +PLX -FL1 +RV1bcl bah tel t axr:?TT$ -FL1; +BPU -FL1; +C/fiLl -FL1 -RVl +RV3; +LIM -FL1; +PLX -FL1 -RVl +KV3bcl bah tcl t er:+CMrd -RVl -RV3; +PLX -RVl -RV3Table 5: Resulting tagged entriesand Renals et al (1991) show that with a few as-sumptions, an MLP may be viewed as estimatingthe probability P(ql x) where q is a phone and xis the input acoustic speech data.
The estimatorconsists of a simple three-layer feed forward MLPtrained with the back-propagation algorithm (seeFigure 2).
The input layer consists of 9 frames of in-put speech data.
Each frame, representing 10 msecof speech, is typically encoded by 9 PLP (Hermansky1990) coefficients, 9 delta-PLP coefficients, 9 delta-delta PLP coefficients, delta-energy and delta-delta-energy terms.
Typically, we use 500-4000 hiddenunits.
The output layer has one unit for each phone.The MLP is trained on phonetically hand-labeledspeech (TIMIT), and then further trained by an it-erative Viterbi procedure (forced-Viterbi providingthe labels) with Wall Street Journal corpora.v b m r z Output: ~ 54 Phones~r .
.
.
- .
, - '~f - ,~- - .
, - - - , f ' x r - .~- .~-" '~"~ Hidden Layer:500-4000 FullyConnected UnitsInput Layer: 9 Frames~_ 0, - - - - ,., " of 20RASTA features,'- .
.
.
.
total 180 unitsL e f t ~  Current l F r a m e - ~ ~ ,  ,.
r_- - "  .
.
.
.
.
.
.
.
.
", , , , t .
.
.
.
R ight  ContextI I I I I I I I I-~,,:-Y~a: -Zor., - tam tats 2ores Saw ~t~asFigure 2: Phonetic Likelihood EstimatorThe probability P(qlx) produced by the MLP foreach frame is first converted to the likelihood P(xlq )by dividing by the prior P(q), according to Bayes'rule; we ignore P(z) since it is constant here:P(x l q) - P(q l x)P(z)P(q)The second step of the algorithm, FORCED-VITERBI  ALIGNMENT, takes this vector of likelihoodsfor each frame and produces the most likely phoneticstring for the Sentence.
If each word had only a sin-gle pronunciation and if each phone had some fixedduration, the phonetic string would be completelydetermined by the word string.
However, phonesvary in length as a function of idiolect and rate ofspeech, and of course the very fact of optional phono-logical rules implies multiple possible pronunciationsfor each word.
These pronunciations are encoded ina hidden Markov model (HMM) for each word.The Viterbi algorithm is a dynamic programmingsearch, which works by computing for each phone ateach frame the most likely string of phones endingin that phone.
Consider a sentence whose first twowords are "of the", and assume the simplified lexiconin Figure 3.P( ax I start }-......,, ~ .
0~'~ 66~-0~9 ~ ~1.0~the ~Figure 3: Pronunciation models for "of" and "the"Each pronunciation of the words 'of' and 'the'is represented by a path through the probabilisticautomaton for the word.
For expository simplic-ity, we have made the (incorrect) assumption thatconsonants have a duration of i frame, and vowel aduration of 2 or 3 frames.
The algorithm analyzesthe input frame by frame, keeping track of the bestpath of phones.
Each path is ranked by its proba-bility, which is computed by multiplying each of thetransition probabilities and the phone probabilitiesfor each frame.
Figure 4 shows a schematic of thepath computation.
The size of each dot indicates themagnitude of the local phone likelihood.
The max-imum path at each point is extended; non-maximalpaths are pruned.The result of the forced-Viterbi alignment on asingle sentence is a phonetic labeling for the sen-tence (see Figure 5 for an example), from which we4ah -ah-v-dh-ax-ax-axEND six .~ .~~P(ax I dh)= .7lydh P(v J acoustlcs) = .9 ~ 0 )~ax'ax'ax'v'dx-iy-iyv x ''~" / ""~, ,  \P(v I oh)= .4START P(ah I START)= .5Figure 4: Computing most-likely phone paths in aForced-Viterbi alignment of 'of the'new york city's freshnyuw yaorkc lk  s ih tc l t i yz  f rehshkills landfill onkc lk ih l z  laendc l f ih l  aanstaten island for ones te l taetc l ten  ay laxndc l  faor  wahndumps four milliondc ldahmpc lps  faor  mih ly ixngallons of toxicgc lgae laxnz  ax f  tc l taakc lks ixkc lliquid into nearbyl i hkc lkwihdc l  entc l t  uw n ihrbc lbayfreshwater streams everyf rehshwaodxaxr  s tc l t  r i ymz eh v r iydaydcl d eyFigure 5: A forced-Viterbi phonetic labelling for aWall Street Journal sentencecan produce aphonetic pronunciation for each word.By running this algorithm on a large corpus of sen-tences, we produce a list of "bottom-up" pronunci-ations for each word in the corpus.2.4 Rule probabi l i ty est imat ionThe rule-tagged surface lexicon described in ?2.1 andthe counts derived from the forced-Viterbi describedin ?2.3 can be combined to form a tagged lexiconthat also has counts for each pronunciation of eachword.
Following is a sample ntry from this lexiconfor the word Adams which shows the five derivationsfor its single pronunciation:Adams: ae dz az m z: count=2derivation 1: +ATS +FL1 -SL2derivation 2: +BPU +FL1 -$L2derivation 3: +?MU +FL1 +RV1 -SL2derivation 4: +LIH +FL1 -SL2derivation 5: +PLX +FL1 -SL2Each pronunciation ofeach word in this lexicon isannotated with rule tags.
Since each pronunciationmay be derived from different source dictionaries orvia different rules, each pronunciation fa word maycontain multiple derivations, each consisting of thelist of rules which applied to give the pronunciationfrom the base form.
These tags are either positive,indicating that a rule applied, or negative, indicatingthat it did not.To produce the initial rule probabilities, we needto count the number of times each rule applies, outof the number-of times it had the potential to apply.If each pronunciation only had a single derivation,this would be computed simply as follows:P(R) = Zv~PRONCt (Rule R applied in p)Ct (Rule R could have applied in p)This could be computed from the tags as :Ct(+R tags in p)--P-(-R) = Z Ct(-I-R tags in p) -I- Ct(-R tags in p)v~PRONHowever, since each pronunciation can have mul-tiple derivations, the counts for each rule from eachderivation eed to be weighted by the probabilityof the derivation.
The derivation probability is com-puted simply by multiplying together the probabilityof each of the applications or non-applications of therule.
Let?
DERIVS(p} be the set of all derivations of apronunciation p,?
POSR ULES(p, r, d) be 1.0 if derivation dof pro-nunciation p uses rule r, else 0.?
ALLRULES(p,r) be the count of all derivationsof p in which rule r could have applied (i.e.
inwhich d has either a +R or -R tag).?
P(d\]p) be the probability of the derivation d ofpronunciation p.?
PRON be the set of pronunciations derived fromthe forced-Viterbi output.Now a single iteration of the rule-probability al-gorithm must perform the following computation:POSRULES(p,r,d)P(r) = ~_~ ~ P(dlP) ALLRULES(p,r)pePRON aeDERIVS(p)Since we have no prior knowledge, we make thezero-knowledge initial assumption that P(d\[p) =1 The algorithm can the be run as a \[DERIVS(p)I"successive stimation-maximization o provide suc-cessive approximations to P(dlp ).
For efficiency rea-sons, we actually compute the probabilities of allrules in parallel, as shown in Figure 6.For each word/pron pair P E PRON from-- - forced-Viterbi alignmentLet DERIVS(P) be the set of rulederivations of PFor  every  d q DERIVS(P)For every  rule R 6 di f  (R = +RULE)then1 ruleapp{RULE} += \[DERIVS(P)\[e lserulenoapp{RULE} += 1 \[DERIVS(P)IFor every rule RULEP( RU L E) = r,te,pp( RU L~) ruleapp( RU L E )Truleapp( RU L E )Figure 6: Parallel computation of rule probabilities3 Resu l t sWe ran the estimation algorithm on 7203 sea ,  noes(129,864 words) read from the Wall Street Journal.The corpus (!993 WSJ Hub 2 (WSJ 0) training data)-consisted of 12 hours of speech, and had 8916 uniquewords.
Table 6 shows the probabilities for the tenphonological rules described in ?2.2.Note that all of the rules are indeed quite op-tional; even the most commonly-employed rules, likeflapping and h-voicing, only apply on average about90% of the time.
Many of the other rules, such asthe reduced-vowel or reduced-liquid rules, only ap-ply about 50% of the time.We next attempted to judge the reliability ofour automatic rule-probability estimation algorithmby comparing it with hand transcribed pronuncia-tions.
We took the hand-transcribed pronunciationsof each word in TIMIT, and computed rule probabil-ities by the same rule-tag counting procedure usedfor our forced-Viterbi output.
Figure 7 shows the fitbetween the automatic and hand-transcribed proba-bilities.
Since the TIMIT pronunciations were froma completely different data collection effort with avery different corpus and speakers, the closeness ofthe probabilities i quite encouraging.Figure 8 breaks down our automatically generatedrule probabilities for the Wall Street Journal corpusPercent of Phonological Rule Use, WSJO vs. TIMITPercent' 00 I" I 90.0080.00 i70.00i 50.0020.0010.000.00VHIRuleFigure 7: Automatic vs Hand-transcribed Probabil-ities for Phonological Rulesinto male and female speakers.
Notice that many ofthe rules seem to be employed more often by menthan by women.
For example, men are about 5%more likely to flap, more likely to reduce vowels ih._."
1 and er, and slightly more likely to reduce Lqums andnasals.
- -~Since ~'- ,~ese are coarticulation or fast-speech ef-fects, our initial hypothesis was that the differ-ence between male and female speakers was due toa faster speech-rate by males.
By computing theweighted average seconds per phone for male andfemale speakers, we found that females had an av-erage of 71 ms/phone, while males had an averageof 68 ms/phone, a difference of about 4%, quite cor-related with the similar differences in reduction andflapping.4 Re la ted  WorkOur algorithm for phonological rule probability esti-mation synthesizes and extends earlier work by (Co-hen 1989) and (Wooters 1993).
The idea of usingoptional phonological rules to construct a speech-recognition lexicon derives from Cohen (1989), whoapplied optional phonological rules to a baseformdictionary to produce a surface lexicon and thenused TIMIT to assign probabilities for each pronun-ciation.
The use of a forced-Viterbi speech decoderto discover pronunciations from a corpus was pro-posed by Wooters (1993).
Weseniek & Sehiel (1994)independently propose avery similar forced-Viterbi-decoder-based technique which they use for measur-ing the accuracy of hand-written phonology.6Name CodeReductionsMid vowels RV1High vowels RV2R-vowel RV3Syllabic n SL1Syllabic m SL2Syllabic 1 SL3Syllabic r SL4Flapping FL1Flapping-r FL2H-voicing VH 1Rule-stress \[aa ae ah ao eh er ey ow uh\]--~ ax-stress \[iy ih uw\] ---* ix-stress er ---* axr\[ax ix\] n -+ en\[ax ix\] in ---.
em\[ax ix\] 1 ~ el\[ax ix\] r ~ axr\[tcl dcll It d\]---* dx /V  __  \[ax ix axr\] i\[tcl dcl\] It d\]-~ dx/Vr  ~ lax ix axr\] ,?
hh --* hv / \[+voice\] __ \[+voice\]Table 6: Results of the Rule-Probability-Estimation AlgorithmPr.60.57.74.35.35.72.77.87.92.92Percent of Phonological Rule UsePercent90.00 m ....80.00 ,i 70.00 .... 160.00m50.0040.0020.00I0.000.00 1 1 2 3 1RulemfemalellllllFigure 8: Male vs Female Probabilities for Phono-logical RulesChen (1990) and Riley (1991) model the relation-ship between phonemes and their Mlophonic realiza-tions by training decision trees on TIMIT data.
Adecision tree is learned for each underlying phonemespecifying its .surface realization in different con-texts.
These completely automatic techniques, re-quiring no hand-written rules, can allow a morefine-grained analysis than our rule-based algorithm.However, as a consequence, it is more difficult toextract generalizations across classes of phonemesto which rules can apply.
We think that a hybridbetween a rule-based and a decision-tree approachcould prove quite powerful.5 Conc lus ion  and  Future  WorkAlthough the paradigm of exploratory computa-tional phonology is only in its infancy, we believeour rule-probability estimation algorithm to be anew and useful instance of the use of probabilistictechniques and spoken-language corpora in compu-tational inguistics.
In Tajchman et al (1995) wereport on the results of our algorithm on speechrecognition performance.
We plan in future workto address a number of shortcomings of these ex-periments, for example including some spontaneousspeech corpora, and looking at a wider variety ofrules.In addition, we have extended our algorithm to in-duce new pronunciations which generalize over pro-nunciations een in the corpus (Wooters & Stolcke1994).
We now plan to augment our probability es-timation to use the pronunciations from this newHMM-induction-based generalization step.
This willrequire extending our tag-based probability estima-tion step to parse the phone strings from the forced-Viterbi.In other current work we have also been usingthis algorithm to model the phonological componentof the accent of non-native speakers.
Finally, wehope in future work to be able to combine our rule-based approach with more bottom-up methods likethe decision-tree or phonological parsing algorithmsto induce rules as well as merely training their prob-abilities?AcknowledgmentsThanks to Mike Hochberg, Nelson Morgan, Steve Re-nals, Tony Robinson, Florian Schiel, Andreas Stolcke,and Chuck Woofers.
This work was partially fundedby ICSI and an SRI subcontract from ARPA contractMDA904-90-C-5253.
Partial funding also came from ES-PRIT project 6487 (The Wernicke project).Re ferencesBOURLARD, H., & N. MORGAN.
1991.
Merging mul-tilayer perceptrons & Hidden Markov Models:7Some experiments in continuous peech recog-nition.
In Artificial Neural Networks: Advancesand Applications, ed.
by E. Gelenbe.
North Hol-land Press.CHEN, F. 1990.
Identification ofcontextual factorsfor pronounciation networks.
In IEEE ICASSP-90,753-756.CMU, 1993.
The Carnegie Mellon Pronouncing Dic-tionary v0.1.
Carnegie Mellon University.COHEN, M. H., 1989.
Phonological Structures forSpeech Recognition.
University of California,Berkeley dissertation.COLE, R. A., K. ROGINSKI, ~5 M.
FANTY., 1994.The OGI Numbers Database.
Oregon GraduateInstitute.COMLEX, 1994.
The COMLEX English Pronounc-ing Dictionary.
copyright Trustees of the Uni-versity of.Pennsylvania.DAELEMANS, WALTER, STEVEN GILLIS, ~ GERTDURmUX.
1994.
The acquisition of stress: Adata-oriented approach.
Computational Lin-guistics 208.421-451.ELLISON, T. MARK, 1992.
The Machine Learning ofPhonological Structure.
University of WesternAustralia dissertation.GASSER, MICHAEL, 1993.
Learning words in time:Towards a modular connectionist account of theacquisition of receptive morphology.
Draft.HERMANSKY, H. 1990.
Perceptual linear predictive(pip) analysis of speech.
J. Acoustical Societyof America 87.LAMEL, LORI, 1993.
The Limsi Dictionary.NIST, 1993.
Continuous Speech Recognition Corpus(WSJ 0).
National Institute of Standards andTechnology Speech Disc 11-1.1 to 11-3.1.RENALS, S., N. MORGAN, H. BOURLARD, M. CO-HEN, H. FRANCO, C. WOOTERS, ~ P. KOHN.1991.
Connectionist peech recognition: Sta-tus and prospects.
Technical Report TR-91-070,ICSI, Berkeley, CA.RILEY, MICHAEL D. 1991.
A statistical model forgenerating pronunciation networks.
In IEEEICASSP-91, 737-740.ROBINSON, ANTHONY, 1994.
The British EnglishExample Pronunciation Dictionary, v0.1.
Cam-bridge University.TAJCHMAN, GARY, ERIC FOSLER, ~ DANIEL JU-RAFSKY.
1995.
Building multiple pronunciationmodels for novel words using exploratory com-putational phonology.
To appear in Eurospeech-95.TIMIT, 1990.
TIMIT Acoustic-Phonetic ContinuousSpeech Corpus.
National Institute of Standardsand Technology Speech Disc 1-1.1.
NTIS OrderNo.
PB91-505065.WESENICK, MARIA-BARBARA, ~ FLORIAN SCHIEL.1994.
Applying speech verification to a largedata base of German to obtain a statistical sur-vey about rules of pronunciation.
In ICSLP-9~,279-282.WOOTERS, CHARLES C., 1993.
Lexical Modelingin a Speaker Independent Speech Understand-ing System.
Berkeley: University of Californiadissertation.
Available as ICSI TR-92-062.WOOTERS, CHUCK, ~5 ANDREAS STOLCKE.
1994.Multiple-pronunciation lexical modeling in aspeaker-independent speech understanding sys-tem.
In ICSLP-94.8
