Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 193?200,Sydney, July 2006. c?2006 Association for Computational LinguisticsDependencies between Student State and Speech RecognitionProblems in Spoken Tutoring DialoguesMihai RotaruUniversity of PittsburghPittsburgh, USAmrotaru@cs.pitt.eduDiane J. LitmanUniversity of PittsburghPittsburgh, USAlitman@cs.pitt.eduAbstractSpeech recognition problems are a realityin current spoken dialogue systems.
Inorder to better understand these phenom-ena, we study dependencies betweenspeech recognition problems and severalhigher level dialogue factors that defineour notion of student state: frustra-tion/anger, certainty and correctness.
Weapply Chi Square (?2) analysis to a cor-pus of speech-based computer tutoringdialogues to discover these dependenciesboth within and across turns.
Significantdependencies are combined to produceinteresting insights regarding speech rec-ognition problems and to propose newstrategies for handling these problems.We also find that tutoring, as a new do-main for speech applications, exhibits in-teresting tradeoffs and new factors toconsider for spoken dialogue design.1 IntroductionDesigning a spoken dialogue system involvesmany non-trivial decisions.
One factor that thedesigner has to take into account is the presenceof speech recognition problems (SRP).
Previouswork (Walker et al, 2000) has shown that thenumber of SRP is negatively correlated withoverall user satisfaction.
Given the negative im-pact of SRP, there has been a lot of work in try-ing to understand this phenomenon and its impli-cations for building dialogue systems.
Most ofthe previous work has focused on lower leveldetails of SRP: identifying components responsi-ble for SRP (acoustic model, language model,search algorithm (Chase, 1997)) or prosodiccharacterization of SRP (Hirschberg et al, 2004).We extend previous work by analyzing the re-lationship between SRP and higher level dia-logue factors.
Recent work has shown that dia-logue design can benefit from several higherlevel dialogue factors: dialogue acts (Framptonand Lemon, 2005; Walker et al, 2001), prag-matic plausibility (Gabsdil and Lemon, 2004).Also, it is widely believed that user emotions, asanother example of higher level factor, interactwith SRP but, currently, there is little hard evi-dence to support this intuition.
We perform ouranalysis on three high level dialogue factors:frustration/anger, certainty and correctness.
Frus-tration and anger have been observed as the mostfrequent emotional class in many dialogue sys-tems (Ang et al, 2002) and are associated with ahigher word error rate (Bulyko et al, 2005).
Forthis reason, we use the presence of emotions likefrustration and anger as our first dialogue factor.Our other two factors are inspired by anothercontribution of our study: looking at speech-based computer tutoring dialogues instead ofmore commonly used information retrieval dia-logues.
Implementing spoken dialogue systemsin a new domain has shown that many practicesdo not port well to the new domain (e.g.
confir-mation of long prompts (Kearns et al, 2002)).Tutoring, as a new domain for speech applica-tions (Litman and Forbes-Riley, 2004; Pon-Barryet al, 2004), brings forward new factors that canbe important for spoken dialogue design.
Herewe focus on certainty and correctness.
Both fac-tors have been shown to play an important role inthe tutoring process (Forbes-Riley and Litman,2005; Liscombe et al, 2005).A common practice in previous work on emo-tion prediction (Ang et al, 2002; Litman andForbes-Riley, 2004) is to transform an initialfiner level emotion annotation (five or more la-bels) into a coarser level annotation (2-3 labels).We wanted to understand if this practice can im-193pact the dependencies we observe from the data.To test this, we combine our two emotion1 fac-tors (frustration/anger and certainty) into a binaryemotional/non-emotional annotation.To understand the relationship between SRPand our three factors, we take a three-step ap-proach.
In the first step, dependencies betweenSRP and our three factors are discovered usingthe Chi Square (?2) test.
Similar analyses on hu-man-human dialogues have yielded interestinginsights about human-human conversations(Forbes-Riley and Litman, 2005; Skantze, 2005).In the second step, significant dependencies arecombined to produce interesting insights regard-ing SRP and to propose strategies for handlingSRP.
Validating these strategies is the purpose ofthe third step.
In this paper, we focus on the firsttwo steps; the third step is left as future work.Our analysis produces several interesting in-sights and strategies which confirm the utility ofthe proposed approach.
With respect to insights,we show that user emotions interact with SRP.We also find that incorrect/uncertain studentturns have more SRP than expected.
In addition,we find that the emotion annotation level affectsthe interactions we observe from the data, withfiner-level emotions yielding more interactionsand insights.In terms of strategies, our data suggests thatfavoring misrecognitions over rejections (bylowering the rejection threshold) might be morebeneficial for our tutoring task ?
at least in termsof reducing the number of emotional studentturns.
Also, as a general design practice in thespoken tutoring applications, we find an interest-ing tradeoff between the pedagogical value ofasking difficult questions and the system?s abilityto recognize the student answer.2 CorpusThe corpus analyzed in this paper consists of 95experimentally obtained spoken tutoring dia-logues between 20 students and our systemITSPOKE (Litman and Forbes-Riley, 2004), aspeech-enabled version of the text-based WHY2conceptual physics tutoring system (VanLehn etal., 2002).
When interacting with ITSPOKE, stu-dents first type an essay answering a qualitativephysics problem using a graphical user interface.ITSPOKE then engages the student in spoken dia-logue (using speech-based input and output) tocorrect misconceptions and elicit more complete1 We use the term ?emotion?
loosely to cover both affectsand attitudes that can impact student learning.explanations, after which the student revises theessay, thereby ending the tutoring or causing an-other round of tutoring/essay revision.
For rec-ognition, we use the Sphinx2 speech recognizerwith stochastic language models.
Because speechrecognition is imperfect, after the data was col-lected, each student utterance in our corpus wasmanually transcribed by a project staff member.An annotated excerpt from our corpus is shownin Figure 1 (punctuation added for clarity).
Theexcerpts show both what the student said (theSTD labels) and what ITSPOKE recognized (theASR labels).
The excerpt is also annotated withconcepts that will be described next.2.1 Speech Recognition Problems (SRP)One form of SRP is the Rejection.
Rejectionsoccur when ITSPOKE is not confident enough inthe recognition hypothesis and asks the studentto repeat (Figure 1, STD3,4).
For our ?2 analysis,we define the REJ variable with two values: Rej(a rejection occurred in the turn) and noRej (norejection occurred in the turn).
Not surprisingly,ITSPOKE also misrecognized some student turns.When ITSPOKE heard something different thanwhat the student actually said but was confidentin its hypothesis, we call this an ASR Misrecog-nition (a binary version of the commonly usedWord Error Rate) (Figure 1, STD1,2).
Similarly,we define the ASR MIS variable with two val-ues: AsrMis and noAsrMis.Semantic accuracy is more relevant for dia-logue evaluation, as it does not penalize for worderrors that are unimportant to overall utteranceinterpretation.
In the case of form-based informa-tion access spoken dialogue systems, computingsemantic accuracy is straightforward (i.e.
con-cept accuracy = percentage of correctly recog-nized concepts).
In contrast, in the tutoring do-main there are no clear forms with slots to befilled.
We base our semantic accuracy on the?correctness?
measure of the student turn.
Foreach student turn, ITSPOKE interprets it and la-bels its correctness with regard to whether thestudent correctly answered the tutor question (seethe labels between square brackets in Figure 1).We define Semantic Misrecognition as caseswhere ITSPOKE was confident in its recognitionhypothesis and the correctness interpretation ofthe recognition hypothesis is different from thecorrectness interpretation of the manual tran-script (Figure 1, STD1).
Similarly, we define theSEM MIS variable with two values: SemMisand noSemMis.
The top part of Table 1 lists thedistribution for our three SRP variables.1943 Student State AnnotationOur definition of student state includes annota-tion on three dimensions (our three dialogue fac-tors): Frustration/Anger/Hyperarticulation, Cer-tainty and Correctness (see Table 1).
The firsttwo dimensions capture the student emotions andthe last one the student correctness.ITSPOKE: Good.
Apply the same force to an object ofmass M3, which is three times the mass of M1.What is the magnitude M3's acceleration in terms ofA1 [acceleration of the object M1]?STD1: third of a one (CERT=Uncertain, CRCT=C)[correct]ASR1: they're it but a one (AsrMis, SemMis) [incorrect]ITSPOKE: The acceleration of M3 [...] would be one-third the acceleration of M1.
[...] In general for thesame force what remains constant?STD2: weight (CRCT=I) [incorrect]ASR2: great (AsrMis) [incorrect]ITSPOKE: Well...
The product of the mass of a bodyand its acceleration remains constant for a givenforce.
What is the relationship between a force act-ing on the box, the mass of the box, and its accel-eration?STD3: newton's second law (CERT=Uncertain,CRCT=PC)ASR3: the cancel the fire (Rej)ITSPOKE: Could you please repeat that?STD4: newton's second law (FAH=FrAng, CRCT=PC)ASR4: newton second long (Rej)Figure 1: Human-Computer Dialogue ExcerptThe Frustration/Anger/Hyperarticulationdimension captures the perceived negative stu-dent emotional response to the interaction withthe system.
Three labels were used to annotatethis dimension: frustration-anger, hyperarticula-tion and neutral.
Similar to (Ang et al, 2002),because frustration and anger can be difficult todistinguish reliably, they were collapsed into asingle label: frustration-anger (Figure 1, STD4).Often, frustration and anger is prosodicallymarked and in many cases the prosody used isconsistent with hyperarticulation (Ang et al,2002).
For this reason we included in this dimen-sion the hyperarticulation label (even though hy-perarticulation is not an emotion but a state).
Weused the hyperarticulation label for turns whereno frustration or anger was perceived but never-theless were hyperarticulated.
For our interactionexperiments we define the FAH variable withthree values: FrAng (frustration-anger), Hyp(hyperarticulation) and Neutral.The Certainty dimension captures the per-ceived student reaction to the questions asked byour computer tutor and her overall reaction to thetutoring domain (Liscombe et al, 2005).
(Forbes-Riley and Litman, 2005) show that stu-dent certainty interacts with a human tutor?s dia-logue decision process (i.e.
the choice of feed-back).
Four labels were used for this dimension:certain, uncertain (e.g.
Figure 1, STD1), mixedand neutral.
In a small number of turns, both cer-tainty and uncertainty were expressed and theseturns were labeled as mixed (e.g.
the student wascertain about a concept, but uncertain about an-other concept needed to answer the tutor?s ques-tion).
For our interaction experiments we definethe CERT variable with four values: Certain,Uncertain, Mixed and Neutral.Vari-able ValuesStudent turns(2334)Speech recognition problemsASR MISAsrMisnoAsrMis25.4%74.6%SEM MISSemMisnoSemMis5.7%94.3%REJ Rej noRej7.0%93.0%Student stateFAHFrAngHypNeutral9.9%2.1%88.0%CERTCertainUncertainMixedNeutral41.3%19.1%2.4%37.3%CRCTCIPCUA63.3%23.3%6.2%7.1%EnE Emotional Neutral64.8%35.2%Table 1: Variable distributions in our corpus.To test the impact of the emotion annotationlevel, we define the Emotional/Non-Emotionalannotation based on our two emotional dimen-sions: neutral turns on both the FAH and theCERT dimension are labeled as neutral2; all otherturns were labeled as emotional.
Consequently,we define the EnE variable with two values:Emotional and Neutral.Correctness is also an important factor of thestudent state.
In addition to the correctness labelsassigned by ITSPOKE (recall the definition ofSEM MIS), each student turn was manually an-notated by a project staff member in terms oftheir physics-related correctness.
Our annotatorused the human transcripts and his physicsknowledge to label each student turn for various2 To be consistent with our previous work, we label hyperar-ticulated turns as emotional even though hyperarticulation isnot an emotion.195degrees of correctness: correct, partially correct,incorrect and unable to answer.
Our system canask the student to provide multiple pieces of in-formation in her answer (e.g.
the question ?Tryto name the forces acting on the packet.
Please,specify their directions.?
asks for both the namesof the forces and their direction).
If the studentanswer is correct and contains all pieces of in-formation, it was labeled as correct (e.g.
?grav-ity, down?).
The partially correct label was usedfor turns where part of the answer was correctbut the rest was either incorrect (e.g.
?gravity,up?)
or omitted some information from the idealcorrect answer (e.g.
?gravity?).
Turns that werecompletely incorrect (e.g.
?no forces?)
were la-beled as incorrect.
Turns where the students didnot answer the computer tutor?s question werelabeled as ?unable to answer?.
In these turns thestudent used either variants of ?I don?t know?
orsimply did not say anything.
For our interactionexperiments we defined the CRCT variable withfour values: C (correct), I (incorrect), PC (par-tially correct) and UA (unable to answer).Please note that our definition of student stateis from the tutor?s perspective.
As we mentionedbefore, our emotion annotation is for perceivedemotions.
Similarly, the notion of correctness isfrom the tutor?s perspective.
For example, thestudent might think she is correct but, in reality,her answer is incorrect.
This correctness shouldbe contrasted with the correctness used to defineSEM MIS.
The SEM MIS correctness usesITSPOKE?s language understanding module ap-plied to recognition hypothesis or the manualtranscript, while the student state?s correctnessuses our annotator?s language understanding.All our student state annotations are at the turnlevel and were performed manually by the sameannotator.
While an inter-annotator agreementstudy is the best way to test the reliability of ourtwo emotional annotations (FAH and CERT),our experience with annotating student emotions(Litman and Forbes-Riley, 2004) has shown thatthis type of annotation can be performed reliably.Given the general importance of the student?suncertainty for tutoring, a second annotator hasbeen commissioned to annotate our corpus forthe presence or absence of uncertainty.
This an-notation can be directly compared with a binaryversion of CERT: Uncertain+Mixed versus Cer-tain+Neutral.
The comparison yields an agree-ment of 90% with a Kappa of 0.68.
Moreover, ifwe rerun our study on the second annotation, wefind similar dependencies.
We are currentlyplanning to perform a second annotation of theFAH dimension to validate its reliability.We believe that our correctness annotation(CRCT) is reliable due to the simplicity of thetask: the annotator uses his language understand-ing to match the human transcript to a list of cor-rect/incorrect answers.
When we compared thisannotation with the correctness assigned byITSPOKE on the human transcript, we found anagreement of 90% with a Kappa of 0.79.4 Identifying dependencies using ?2To discover the dependencies between our vari-ables, we apply the ?2 test.
We illustrate ouranalysis method on the interaction between cer-tainty (CERT) and rejection (REJ).
The ?2 valueassesses whether the differences between ob-served and expected counts are large enough toconclude a statistically significant dependencybetween the two variables (Table 2, last column).For Table 2, which has 3 degrees of freedom ((4-1)*(2-1)), the critical ?2 value at a p<0.05 is 7.81.We thus conclude that there is a statistically sig-nificant dependency between the student cer-tainty in a turn and the rejection of that turn.Combination  Obs.
Exp.
?2CERT ?
REJ    11.45Certain ?
Rej - 49 67 9.13Uncertain ?
Rej + 43 31 6.15Table 2: CERT ?
REJ interaction.If any of the two variables involved in a sig-nificant dependency has more than 2 possiblevalues, we can look more deeply into this overallinteraction by investigating how particular valuesinteract with each other.
To do that, we computea binary variable for each variable?s value in partand study dependencies between these variables.For example, for the value ?Certain?
of variableCERT we create a binary variable with two val-ues: ?Certain?
and ?Anything Else?
(in this caseUncertain, Mixed and Neutral).
By studying thedependency between binary variables we canunderstand how the interaction works.Table 2 reports in rows 3 and 4 all significantinteractions between the values of variablesCERT and REJ.
Each row shows: 1) the valuefor each original variable, 2) the sign of the de-pendency, 3) the observed counts, 4) the ex-pected counts and 5) the ?2 value.
For example,in our data there are 49 rejected turns in whichthe student was certain.
This value is smallerthan the expected counts (67); the dependencybetween Certain and Rej is significant with a ?2value of 9.13.
A comparison of the observedcounts and expected counts reveals the direction196(sign) of the dependency.
In our case we see thatcertain turns are rejected less than expected (row3), while uncertain turns are rejected more thanexpected (row 4).
On the other hand, there is nointeraction between neutral turns and rejectionsor between mixed turns and rejections.
Thus, theCERT ?
REJ interaction is explained only by theinteraction between Certain and Rej and the in-teraction between Uncertain and Rej.5 Results - dependenciesIn this section we present all significant depend-encies between SRP and student state bothwithin and across turns.
Within turn interactionsanalyze the contribution of the student state tothe recognition of the turn.
They were motivatedby the widely believed intuition that emotioninteracts with SRP.
Across turn interactions lookat the contribution of previous SRP to the currentstudent state.
Our previous work (Rotaru andLitman, 2005) had shown that certain SRP willcorrelate with emotional responses from the user.We also study the impact of the emotion annota-tion level (EnE versus FAH/CERT) on the inter-actions we observe.
The implications of thesedependencies will be discussed in Section 6.5.1 Within turn interactionsFor the FAH dimension, we find only one sig-nificant interaction: the interaction between theFAH student state and the rejection of the currentturn (Table 3).
By studying values?
interactions,we find that turns where the student is frustratedor angry are rejected more than expected (34 in-stead of 16; Figure 1, STD4 is one of them).Similarly, turns where the student response ishyperarticulated are also rejected more than ex-pected (similar to observations in (Soltau andWaibel, 2000)).
In contrast, neutral turns in theFAH dimension are rejected less than expected.Surprisingly, FrAng does not interact withAsrMis as observed in (Bulyko et al, 2005) butthey use the full word error rate measure insteadof the binary version used in this paper.Combination  Obs.
Exp.
?2FAH ?
REJ    77.92FrAng ?
Rej + 34 16 23.61Hyp ?
Rej + 16 3 50.76Neutral ?
Rej - 113 143 57.90Table 3: FAH ?
REJ interaction.Next we investigate how our second emotionannotation, CERT, interacts with SRP.
All sig-nificant dependencies are reported in Tables 2and 4.
In contrast with the FAH dimension, herewe see that the interaction direction depends onthe valence.
We find that ?Certain?
turns haveless SRP than expected (in terms of AsrMis andRej).
In contrast, ?Uncertain?
turns have moreSRP both in terms of AsrMis and Rej.
?Mixed?turns interact only with AsrMis, allowing us toconclude that the presence of uncertainty in thestudent turn (partial or overall) will result in ASRproblems more than expected.
Interestingly, onthis dimension, neutral turns do not interact withany of our three SRP.Combination  Obs.
Exp.
?2CERT ?
ASRMIS    38.41Certain ?
AsrMis - 204 244 15.32Uncertain ?
AsrMis + 138 112 9.46Mixed ?
AsrMis + 29 13 22.27Table 4: CERT ?
ASRMIS interaction.Finally, we look at interactions between stu-dent correctness and SRP.
Here we find signifi-cant dependencies with all types of SRP (see Ta-ble 5).
In general, correct student turns havefewer SRP while incorrect, partially correct orUA turns have more SRP than expected.
Partiallycorrect turns have more AsrMis and SemMisproblems than expected, but are rejected lessthan expected.
Interestingly, UA turns interactonly with rejections: these turns are rejectedmore than expected.
An analysis of our corpusreveals that in most rejected UA turns the studentdoes not say anything; in these cases, the sys-tem?s recognition module thought the studentsaid something but the system correctly rejectsthe recognition hypothesis.Combination  Obs.
Exp.
?2CRCT ?
ASRMIS    65.17C ?
AsrMis - 295 374 62.03I ?
AsrMis + 198 137 45.95PC ?
AsrMis + 50 37 5.9CRCT ?
SEMMIS    20.44C ?
SemMis + 100 84 7.83I ?
SemMis - 14 31 13.09PC ?
SemMis + 15 8 5.62CRCT ?
REJ    99.48C ?
Rej - 53 102 70.14I ?
Rej + 84 37 79.61PC ?
Rej - 4 10 4.39UA ?
Rej + 21 11 9.19Table 5: Interactions between Correctness and SRP.The only exception to the rule is SEM MIS.We believe that SEM MIS behavior is explainedby the ?catch-all?
implementation in our system.In ITSPOKE, for each tutor question there is a listof anticipated answers.
All other answers are197treated as incorrect.
Thus, it is less likely that arecognition problem in an incorrect turn will af-fect the correctness interpretation (e.g.
Figure 1,STD2: very unlikely to misrecognize the incor-rect ?weight?
with the anticipated ?the product ofmass and acceleration?).
In contrast, in correctturns recognition problems are more likely toscrew up the correctness interpretation (e.g.
mis-recognizing ?gravity down?
as ?gravity sound?
).5.2 Across turn interactionsNext we look at the contribution of previous SRP?
variable name or value followed by (-1) ?
to thecurrent student state.
Please note that there aretwo factors involved here: the presence of theSRP and the SRP handling strategy.
InITSPOKE, whenever a student turn is rejected,unless this is the third rejection in a row, the stu-dent is asked to repeat using variations of ?Couldyou please repeat that??.
In all other cases,ITSPOKE makes use of the available informa-tion ignoring any potential ASR errors.Combination  Obs.
Exp.
?2ASRMIS(-1) ?
FAH    7.64AsrMis(-1) ?
FrAng -t 46 58 3.73AsrMis(-1) ?
Hyp -t 7 12 3.52AsrMis(-1) ?
Neutral + 527 509 6.82REJ(-1) ?
FAH    409.31Rej(-1) ?
FrAng + 36 16 28.95Rej(-1) ?
Hyp + 38 3 369.03Rej(-1) ?
Neutral - 88 142 182.9REJ(-1) ?
CRCT    57.68Rej(-1) ?
C - 68 101 31.94Rej(-1) ?
I + 74 37 49.71Rej(-1) ?
PC - 3 10 6.25Table 6: Interactions across turns (t ?
trend, p<0.1).Here we find only 3 interactions (Table 6).
Wefind that after a non-harmful SRP (AsrMis) thestudent is less frustrated and hyperarticulatedthan expected.
This result is not surprising sincean AsrMis does not have any effect on the nor-mal dialogue flow.In contrast, after rejections we observe severalnegative events.
We find a highly significant in-teraction between a previous rejection and thestudent FAH state, with student being more frus-trated and more hyperarticulated than expected(e.g.
Figure 1, STD4).
Not only does the systemelicit an emotional reaction from the student aftera rejection, but her subsequent response to therepetition request suffers in terms of the correct-ness.
We find that after rejections student an-swers are correct or partially correct less thanexpected and incorrect more than expected.
TheREJ(-1) ?
CRCT interaction might be explainedby the CRCT ?
REJ interaction (Table 5) if, ingeneral, after a rejection the student repeats herprevious turn.
An annotation of responses to re-jections as in (Swerts et al, 2000) (repeat, re-phrase etc.)
should  provide additional insights.We were surprised to see that a previousSemMis (more harmful than an AsrMis but lessdisruptive than a Rej) does not interact with thestudent state; also the student certainty does notinteract with previous SRP.5.3 Emotion annotation levelWe also study the impact of the emotion annota-tion level on the interactions we can observefrom our corpus.
In this section, we look at inter-actions between SRP and our coarse-level emo-tion annotation (EnE) both within and acrossturns.
Our results are similar with the results ofour previous work (Rotaru and Litman, 2005) ona smaller corpus and a similar annotationscheme.
We find again only one significant in-teraction: rejections are followed by more emo-tional turns than expected (Table 7).
The strengthof the interaction is smaller than in previouswork, though the results can not be compareddirectly.
No other dependencies are present.Combination  Obs.
Exp.
?2REJ(-1) ?
EnE    6.19Rej(-1) ?
Emotional + 119 104 6.19Table 7: REJ(-1) ?
EnE interaction.We believe that the REJ(-1) ?
EnE interaction isexplained mainly by the FAH dimension.
Notonly is there no interaction between REJ(-1) andCERT, but the inclusion of the CERT dimensionin the EnE annotation decreases the strength ofthe interaction between REJ and FAH (the ?2value decreases from 409.31 for FAH to a mere6.19 for EnE).
Collapsing emotional classes alsoprevents us from seeing any within turn interac-tions.
These observations suggest that what isbeing counted as an emotion for a binary emo-tion annotation is critical its success.
In our case,if we look at affect (FAH) or attitude (CERT) inisolation we find many interactions; in contrast,combining them offers little insight.6 Results ?
insights & strategiesOur results put a spotlight on several interestingobservations which we discuss below.Emotions interact with SRPThe dependencies between FAH/CERT andvarious SRP (Tables 2-4) provide evidence thatuser?s emotions interact with the system?s ability198to recognize the current turn.
This is a widelybelieved intuition with little empirical support sofar.
Thus, our notion of student state can be auseful higher level information source for SRPpredictors.
Similar to (Hirschberg et al, 2004),we believe that peculiarities in the acous-tic/prosodic profile of specific student states areresponsible for their SRP.
Indeed, previous workhas shown that the acoustic/prosodic informationplays an important role in characterizing andpredicting both FAH (Ang et al, 2002; Soltauand Waibel, 2000) and CERT (Liscombe et al,2005; Swerts and Krahmer, 2005).The impact of the emotion annotation levelA comparison of the interactions yielded byvarious levels of emotion annotation shows theimportance of the annotation level.
When using acoarser level annotation (EnE) we find only oneinteraction.
By using a finer level annotation, notonly we can understand this interaction better butwe also discover new interactions (five interac-tions with FAH and CERT).
Moreover, variousstate annotations interact differently with SRP.For example, non-neutral turns in the FAH di-mension (FrAng and Hyp) will be always re-jected more than expected (Table 3); in contrast,interactions between non-neutral turns in theCERT dimension and rejections depend on thevalence (?certain?
turns will be rejected less thanexpected while ?uncertain?
will be rejected morethan expected; recall Table 2).
We also see thatthe neutral turns interact with SRP depending onthe dimension that defines them: FAH neutralturns interact with SRP (Table 3) while CERTneutral turns do not (Tables 2 and 4).This insight suggests an interesting tradeoffbetween the practicality of collapsing emotionalclasses (Ang et al, 2002; Litman and Forbes-Riley, 2004) and the ability to observe meaning-ful interactions via finer level annotations.Rejections: impact and a handling strategyOur results indicate that rejections andITSPOKE?s current rejection-handling strategyare problematic.
We find that rejections are fol-lowed by more emotional turns (Table 7).
Asimilar effect was observed in our previous work(Rotaru and Litman, 2005).
The fact that it gen-eralizes across annotation scheme and corpus,emphasizes its importance.
When a finer levelannotation is used, we find that rejections arefollowed more than expected by a frustrated, an-gry and hyperarticulated user (Table 6).
More-over, these subsequent turns can result in addi-tional rejections (Table 3).
Asking to repeat aftera rejection does not also help in terms of correct-ness: the subsequent student answer is actuallyincorrect more than expected (Table 6).These interactions suggest an interesting strat-egy for our tutoring task: favoring misrecogni-tions over rejections (by lowering the rejectionthreshold).
First, since rejected turns are morethan expected incorrect (Table 5), the actual rec-ognized hypothesis for such turns turn is verylikely to be interpreted as incorrect.
Thus, ac-cepting a rejected turn instead of rejecting it willhave the same outcome in terms of correctness:an incorrect answer.
In this way, instead of at-tempting to acquire the actual student answer byasking to repeat, the system can skip these extraturn(s) and use the current hypothesis.
Second,the other two SRP are less taxing in terms ofeliciting FAH emotions (recall Table 6; note thata SemMis might activate an unwarranted andlengthy knowledge remediation subdialogue).This suggests that continuing the conversationwill be more beneficial even if the system mis-understood the student.
A similar behavior wasobserved in human-human conversations througha noisy speech channel (Skantze, 2005).Correctness/certainty?SRP interactionsWe also find an interesting interaction betweencorrectness/certainty and system?s ability to rec-ognize that turn.
In general correct/certain turnshave less SRP while incorrect/uncertain turnshave more SRP than expected.
This observationsuggests that the computer tutor should ask theright question (in terms of its difficulty) at theright time.
Intuitively, asking a more complicatedquestion when the student is not prepared to an-swer it will increase the likelihood of an incor-rect or uncertain answer.
But our observationsshow that the computer tutor has more troublerecognizing correctly these types of answers.This suggests an interesting tradeoff between thetutor?s question difficulty and the system?s abil-ity to recognize the student answer.
This tradeoffis similar in spirit to the initiative-SRP tradeoffthat is well known when designing information-seeking systems (e.g.
system initiative is oftenused instead of a more natural mixed initiativestrategy, in order to minimize SRP).7 ConclusionsIn this paper we analyze the interactions betweenSRP and three higher level dialogue factors thatdefine our notion of student state: frustra-tion/anger/hyperarticulation, certainty and cor-rectness.
Our analysis produces several interest-ing insights and strategies which confirm the199utility of the proposed approach.
We show thatuser emotions interact with SRP and that theemotion annotation level affects the interactionswe observe from the data, with finer-level emo-tions yielding more interactions and insights.We also find that tutoring, as a new domainfor speech applications, brings forward new im-portant factors for spoken dialogue design: cer-tainty and correctness.
Both factors interact withSRP and these interactions highlight an interest-ing design practice in the spoken tutoring appli-cations: the tradeoff between the pedagogicalvalue of asking difficult questions and the sys-tem?s ability to recognize the student answer (atleast in our system).
The particularities of thetutoring domain also suggest favoring misrecog-nitions over rejections to reduce the negative im-pact of asking to repeat after rejections.In our future work, we plan to move to thethird step of our approach: testing the strategiessuggested by our results.
For example, we willimplement a new version of ITSPOKE that neverrejects the student turn.
Next, the current versionand the new version will be compared with re-spect to users?
emotional response.
Similarly, totest the tradeoff hypothesis, we will implement aversion of ITSPOKE that asks difficult questionsfirst and then falls back to simpler questions.
Acomparison of the two versions in terms of thenumber of SRP can be used for validation.While our results might be dependent on thetutoring system used in this experiment, we be-lieve that our findings can be of interest to practi-tioners building similar voice-based applications.Moreover, our approach can be applied easily tostudying other systems.AcknowledgementsThis work is supported by NSF Grant No.0328431.
We thank Dan Bohus, Kate Forbes-Riley, Joel Tetreault and our anonymous review-ers for their helpful comments.ReferencesJ.
Ang, R. Dhillon, A. Krupski, A. Shriberg and A.Stolcke.
2002.
Prosody-based automatic detectionof annoyance and frustration in human-computerdialog.
In Proc.
of ICSLP.I.
Bulyko, K. Kirchhoff, M. Ostendorf and J. Gold-berg.
2005.
Error-correction detection and responsegeneration in a spoken dialogue system.
SpeechCommunication, 45(3).L.
Chase.
1997.
Blame Assignment for Errors Madeby Large Vocabulary Speech Recognizers.
In Proc.of Eurospeech.K.
Forbes-Riley and D. J. Litman.
2005.
Using Bi-grams to Identify Relationships Between StudentCertainness States and Tutor Responses in a Spo-ken Dialogue Corpus.
In Proc.
of SIGdial.M.
Frampton and O.
Lemon.
2005.
ReinforcementLearning of Dialogue Strategies using the User'sLast Dialogue Act.
In Proc.
of IJCAI Workshop onKnow.&Reasoning in Practical Dialogue Systems.M.
Gabsdil and O.
Lemon.
2004.
Combining Acousticand Pragmatic Features to Predict RecognitionPerformance in Spoken Dialogue Systems.
In Proc.of ACL.J.
Hirschberg, D. Litman and M. Swerts.
2004.
Pro-sodic and Other Cues to Speech Recognition Fail-ures.
Speech Communication, 43(1-2).M.
Kearns, C. Isbell, S. Singh, D. Litman and J.Howe.
2002.
CobotDS: A Spoken Dialogue Systemfor Chat.
In Proc.
of National Conference on Arti-ficial Intelligence (AAAI).J.
Liscombe, J. Hirschberg and J. J. Venditti.
2005.Detecting Certainness in Spoken Tutorial Dia-logues.
In Proc.
of Interspeech.D.
Litman and K. Forbes-Riley.
2004.
AnnotatingStudent Emotional States in Spoken Tutoring Dia-logues.
In Proc.
of SIGdial Workshop on Discourseand Dialogue (SIGdial).H.
Pon-Barry, B. Clark, E. O. Bratt, K. Schultz and S.Peters.
2004.
Evaluating the effectiveness of Scot:aspoken conversational tutor.
In Proc.
of ITS Work-shop on Dialogue-based Intellig.
Tutoring Systems.M.
Rotaru and D. Litman.
2005.
Interactions betweenSpeech Recognition Problems and User Emotions.In Proc.
of Eurospeech.G.
Skantze.
2005.
Exploring human error recoverystrategies: Implications for spoken dialogue sys-tems.
Speech Communication, 45(3).H.
Soltau and A. Waibel.
2000.
Specialized acousticmodels for hyperarticulated speech.
In Proc.
ofICASSP.M.
Swerts and E. Krahmer.
2005.
Audiovisual Pros-ody and Feeling of Knowing.
Journal of Memoryand Language, 53.M.
Swerts, D. Litman and J. Hirschberg.
2000.
Cor-rections in Spoken Dialogue Systems.
In Proc.
ofICSLP.K.
VanLehn, P. W. Jordan, C. P.
Ros?, et al 2002.The Architecture of Why2-Atlas: A Coach forQualitative Physics Essay Writing.
In Proc.
of In-telligent Tutoring Systems (ITS).M.
Walker, D. Litman, C. Kamm and A. Abella.2000.
Towards Developing General Models of Us-ability with PARADISE.
Natural Language Engi-neering.M.
Walker, R. Passonneau and J. Boland.
2001.Quantitative and Qualitative Evaluation of DarpaCommunicator Spoken Dialogue Systems.
In Proc.of ACL.200
