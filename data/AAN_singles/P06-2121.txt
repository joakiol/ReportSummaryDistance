Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 945?952,Sydney, July 2006. c?2006 Association for Computational LinguisticsHAL-based Cascaded Model for Variable-LengthSemantic Pattern Induction from Psychiatry Web ResourcesLiang-Chih Yu and Chung-Hsien WuDepartment of Computer Science and Information EngineeringNational Cheng Kung UniversityTainan, Taiwan, R.O.C.
{lcyu, chwu}@csie.ncku.edu.twFong-Lin JangDepartment of PsychiatryChi-Mei Medical CenterTainan, Taiwan, R.O.C.jcj0429@seed.net.twAbstractNegative life events play an importantrole in triggering depressive episodes.Developing psychiatric services that canautomatically identify such events isbeneficial for mental health care and pre-vention.
Before these services can beprovided, some meaningful semantic pat-terns, such as <lost, parents>, have to beextracted.
In this work, we present a textmining framework capable of inducingvariable-length semantic patterns fromunannotated psychiatry web resources.This framework integrates a cognitivemotivated model, Hyperspace Analog toLanguage (HAL), to represent words aswell as combinations of words.
Then, acascaded induction process (CIP) boot-straps with a small set of seed patternsand incorporates relevance feedback toiteratively induce more relevant patterns.The experimental results show that bycombining the HAL model and relevancefeedback, the CIP can induce semanticpatterns from the unannotated web cor-pora so as to reduce the reliance on anno-tated corpora.1 IntroductionDepressive disorders have become a major threatto mental health.
People in their daily life maysuffer from some negative or stressful life events,such as death of a family member, argumentswith a spouse, loss of a job, and so forth.
Suchlife events play an important role in triggeringdepressive symptoms, such as depressed mood,suicide attempts, and anxiety.
Therefore, it isdesired to develop a system capable of identify-ing negative life events to provide more effectivepsychiatric services.
For example, through thenegative life events, the health professionals canknow the background information about subjectsso as to make more correct decisions and sugges-tions.
Negative life events are often expressed innatural language segments (e.g., sentences).
Toidentify them, the critical step is to transform thesegments into machine-interpretable semanticrepresentation.
This involves the extraction ofkey semantic patterns from the segments.
Con-sider the following example.Two years ago, I lost my parents.
(Event)Since that, I have attempted to kill myselfseveral times.
(Suicide)In this example, the semantic pattern <lost, par-ents> is constituted by two words, which indi-cates that the subject suffered from a negativelife event that triggered the symptom ?Suicide?.A semantic pattern can be considered as a se-mantically plausible combination of k words,where k is the length of the pattern.
Accordingly,a semantic pattern may have variable length.
InWu et al?s study (2005), they have presented amethodology to identify depressive symptoms.
Inthis work, we go a further step to devise a textmining framework for variable-length semanticpattern induction from psychiatry web resources.Traditional approaches to semantic pattern in-duction can be generally divided into twostreams: knowledge-based approaches and cor-pus-based approaches (Lehnert et al, 1992;Muslea, 1999).
Knowledge-based approachesrely on exploiting expert knowledge to designhandcrafted semantic patterns.
The major limita-tions of such approaches include the requirementof significant time and effort on designing thehandcrafted patterns.
Besides, when applying toa new domain, these patterns have to be redes-igned.
Such limitations form a knowledge acqui-sition bottleneck.
A possible solution to reducingthe problem is to use a general-purpose ontology945such as WordNet (Fellbaum, 1998), or a domain-specific ontology constructed using automaticapproaches (Yeh et al, 2004).
These ontologiescontain rich concepts and inter-concept relationssuch as hypernymy-hyponymy relations.
How-ever, an ontology is a static knowledge resource,which may not reflect the dynamic characteris-tics of language.
For this consideration, we in-stead refer to the web resources, or more restrict-edly, the psychiatry web resources as our knowl-edge resource.Corpus-based approaches can automaticallylearn semantic patterns from domain corpora byapplying statistical methods.
The corpora have tobe annotated with domain-specific knowledge(e.g., events).
Then, various statistical methodscan be applied to induce variable-length semanticpatterns from all possible combinations of wordsin the corpora.
However, statistical methods maysuffer from data sparseness problem, thus theyrequire large corpora with annotated informationto obtain more reliable parameters.
For some ap-plication domains, such annotated corpora maybe unavailable.
Therefore, we propose the use ofweb resources as the corpora.
When facing withthe web corpora, traditional corpus-based ap-proaches may be infeasible.
For example, it isimpractical for health professionals to annotatethe whole web corpora.
Besides, it is also im-practical to enumerate all possible combinationsof words from the web corpora, and then searchfor the semantic patterns.To address the problems, we take the notion ofweakly supervised (Stevenson and Greenwood,2005) or unsupervised learning (Hasegawa, 2004;Grenager et al, 2005) to develop a frameworkable to bootstrap with a small set of seed patterns,and then induce more relevant patterns form theunannotated psychiatry web corpora.
By thisway, the reliance on annotated corpora can besignificantly reduced.
The proposed frameworkis divided into two parts: Hyperspace Analog toLanguage (HAL) model (Burgess et al, 1998;Bai et al, 2005), and a cascaded induction proc-ess (CIP).
The HAL model, which is a cognitivemotivated model, provides an informative infra-structure to make the CIP capable of learningfrom unannotated corpora.
The CIP treats thevariable-length induction task as a cascadedprocess.
That is, it first induces the semantic pat-terns of length two, then length three, and so on.In each stage, the CIP initializes the set of se-mantic patterns to be induced based on the betterresults of the previous stage, rather than enumer-ating all possible combinations of words.
Thiswould be helpful to avoid noisy patterns propa-gating to the next stage, and the search space canalso be reduced.A crucial step for semantic pattern induction isthe representation of words as well as combina-tions of words.
The HAL model constructs ahigh-dimensional context space for the psychia-try web corpora.
Each word in the HAL space isrepresented as a vector of its context words,which means that the sense of a word can be in-ferred through its contexts.
Such notion is de-rived from the observation of human behavior.That is, when an unknown word occurs, humanbeings may determine its sense by referring tothe words appearing in the contexts.
Based onthe cognitive behavior, if two words share morecommon contexts, they are more semanticallysimilar.
To further represent a semantic pattern,the HAL model provides a mechanism to com-bine its constituent words over the HAL space.Once the HAL space is constructed, the CIPtakes as input a seed pattern per run, and in turninduces the semantic patterns of different lengths.For each length, the CIP first creates the initialset based on the results of the previous stage.Then, the induction process is iteratively per-formed to induce more patterns relevant to thegiven seed pattern by comparing their contextdistributions.
In addition, we also incorporateexpert knowledge to guide the induction processby using relevance feedback (Baeza-Yates andRibeiro-Neto, 1999), the most popular query re-formulation strategy in the information retrieval(IR) community.
The induction process is termi-nated until the termination criteria are satisfied.In the remainder of this paper, Section 2 pre-sents the overall framework for variable-lengthsemantic pattern induction.
Section 3 describesthe process of constructing the HAL space.
Sec-tion 4 details the cascaded induction process.Section 5 summarizes the experiment results.Finally, Section 6 draws some conclusions andsuggests directions for future work.2 Framework for Variable-Length Se-mantic Pattern InductionThe overall framework, as illustrated in Figure 1,is divided into two parts: the HAL model and thecascaded induction process.
First of all, the HALspace is constructed for the psychiatry webcorpora after word segmentation.
Then, eachword in HAL space is evaluated by computing itsdistance to a given seed pattern.
A smallerdistance   represents   that    the   word   is   more946DistanceEvaluationStop InducedPatternsPsychiatryWeb CorporaHAL SpaceConstructionSeedPatternsWordSegmentationHAL modelIteration +1QualityConceptslength 2 length 3 length k...NoRelevanceFeedbackIteration=0Initial Set(length k)Yesk +1Cascaded Induction ProcessInducedPatternsRelevantPatternsFigure 1.
Framework for variable-length seman-tic pattern induction.semantically related to the seed pattern.According to the distance measure, the CIPgenerates quality concepts, i.e., a set ofsemantically related words to the seed pattern.The quality concepts and the better semanticpatterns induced in the previous stage arecombined to generate the initial set for eachlength.
For example, in the beginning stage, i.e.,length two, the initial set is the all possiblecombinations of two quality concepts.
In the laterstages, each initial set is generated by adding aquality concept to each of the better semanticpatterns.
After the initial set for a particularlength is created, each semantic pattern and theseed pattern are represented in the HAL space forfurther computing their distance.
The moresimilar the context distributions between twopatterns, the closer they are.
Once all thesemantic patterns are evaluated, the relevancefeedback is applied to provide a set of relevantpatterns judged by the health professionals.According to the relevant information, the seedpattern can be refined to be more similar to therelevant set.
The refined seed pattern will betaken as the reference basis in the next iteration.The induction process for each stage isperformed iteratively until no more patterns arejudged as relevant or a maximum number ofiteration is reached.
The relevant set produced atthe last iteration is considered as the result of thesemantic patterns.3 HAL Space ConstructionThe HAL model represents each word in the vo-cabulary  using   a   vector  representation.
Eachw1 w2 wl-2 wl-1 wlAObservation window of lengthweight =12Figure 2.
Weighting scheme of the HAL model.two years ago I lost my parentstwo 0 0 0 0 0 0 0years 5 0 0 0 0 0 0ago 4 5 0 0 0 0 0I 3 4 5 0 0 0 0lost 2 3 4 5 0 0 0my 1 2 3 4 5 0 0parents 0 1 2 3 4 5 0Table 1.
Example of HAL Space (window size=5)dimension of the vector is a weight representingthe strength of association between the targetword and its context word.
The weights are com-puted by applying an observation window oflength l over the corpus.
All words within thewindow are considered as co-occurring with eachother.
Thus, for any two words of distance dwithin the window, the weight between them iscomputed as 1l d?
+ .
Figure 2 shows an exam-ple.
The HAL space views the corpus as a se-quence of words.
Thus, after moving the windowby one word increment over the whole corpus,the HAL space is constructed.
The resultant HALspace is an N N?
matrix, where N is the vo-cabulary size.
In addition, each word in the HALspace is called a concept.
Table 1 presents theHAL space for the example text ?Two years ago,I lost my parents.
?3.1 Representation of a Single ConceptFor each concept in Table 1, the correspond-ing row vector represents its left context infor-mation, i.e., the weights of the words preceding it.Similarly, the corresponding column vectorrepresents its right context information.
Accord-ingly, each concept can be represented by a pairof vectors.
That is,( )1 2 1 2( , ),  ,  .
.
.
,  , ,  ,  .
.
.
,  ,i ii i i N i i i Nleft righti c cleft left left right right rightc t c t c t c t c t c tc v vw w w w w w==(1)whereileftcv and irightcv represent the vectors of theleft context information and right context infor-mation of a concept ic , respectively, i jc tw denotes9471 1 1...NLeft Contextleft leftc t c tw w1cNc...1 1 1...NRight Contextright rightc t c tw wFigure 3.
Conceptual representation of the HALspace.the weight of the j-th dimension ( jt ) of a vector,and N is the dimensionality of a vector, i.e., vo-cabulary size.
The conceptual representation isdepicted in Figure 3.The weighting scheme of the HAL model isfrequency-based.
For some extremely infrequentwords, we consider them as noises and removethem from the vocabulary.
On the other hand, ahigh frequent word tends to get a higher weight,but this does not mean the word is informative,because it may also appear in many other vectors.Thus, to measure the informativeness of a word,the number of the vectors the word appears inshould be taken into account.
In principle, themore vectors the word appears in, the less infor-mation it carries to discriminate the vectors.
Herewe use a weighting scheme analogous to TF-IDF(Baeza-Yates and Ribeiro-Neto, 1999) to re-weight the dimensions of each vector, as de-scribed in Equation (2).
* log ,( )i j i jvectorc t c tjNw wvf t=            (2)where vectorN  denotes the total number of vectors,and ( )jvf t  denotes the number of vectors with jtas the dimension.
After each dimension is re-weighted, the HAL space is transformed into aprobabilistic framework.
Accordingly, eachweight can be redefined as( | ) ,i ji ji jc tc t j ic tjww P t cw?
= ?
(3)where ( | )j iP t c  is the probability that jt  appearsin the vector of ic .3.2 Concept CombinationA semantic pattern is constituted by a set of con-cepts, thus it can be represented through conceptcombination over the HAL space.
This forms anew concept in the HAL space.
Let1( ,..., )Ssp c c=  be a semantic pattern with S con-stituent concepts, i.e., length S. The conceptcombination is defined as1 2 3((...( ) ) ... ),s Sc c c c c?
?
?
?
?
?
(4)where ?
denotes the symbol representing thecombination operator over the HAL space, sc?denotes a new concept generated by the conceptcombination.
The new concept is the representa-tion of a semantic pattern, also a vector represen-tation.
That is,( )1 1( ) ( ) ( ) ( )( , ),  .
.
.
,  , ,  .
.
.
,  ,s ss s N s s Nleft rights c cleft left right rightc t c t c t c tc v vw w w w?
??
?
?
??
==(5)The combination operator, ?
, is implementedby the product of the weights of the constituentconcepts, described as follows.
( )11( | ),s j s jSc t c tsSj ssw wP t c?====??
(6)where ( )s jc tw ?
denotes the weight of the j-th di-mension of the new concept sc?
.4 Cascaded Induction ProcessGiven a seed pattern, the CIP is to induce a set ofrelevant semantic patterns with variable lengths(from 2 to k).
Let 1( ,..., )seed Rsp c c=  be a seedpattern of length R, and 1( ,..., )Ssp c c=  be asemantic pattern of length S. The formaldescription of the CIP is presented as{ }{ } ( )1 1|( ,..., )  |  ( ,..., )     iff   , ,seedR S r ssp spc c c c Dist c c ???
?
?
?
?
?
(7)where |?
denotes the symbol representing thecascaded induction, rc?
and sc?
are the twonew concepts representing seedsp  and sp , respec-tively, and (  ,   )Dist i i  represents the distancebetween two semantic patterns.
The main stepsin the CIP include the initial set generation, dis-tance measure, and relevance feedback.4.1 Initial Set GenerationThe initial set for a particular length contains aset of semantic patterns to be induced, i.e., thesearch space.
Reducing the search space wouldbe helpful for speeding up the induction process,948especially for inducing those patterns with a lar-ger length.
For this purpose, we consider that thewords and the semantic patterns similar to agiven seed pattern are the better candidates forcreating the initial sets.
Therefore, we generatequality concepts, a set of semantically relatedwords to a seed pattern, as the basis to create theinitial set for each length.
Thus, each seed patternwill be associated with a set of quality concepts.In addition, the better semantic patterns inducedin the previous stage are also considered.
Thegoodness of words and semantic patterns ismeasured by their distance to a seed pattern.Here, a word is considered as a quality concept ifits distance is smaller than the average distanceof the vocabulary.
Similarly, only the semanticpatterns with a distance smaller than the averagedistance of all semantic patterns in the previousstage are preserved to the next stage.
By the way,the semantically unrelated patterns, possiblynoisy patterns, will not be propagated to the nextstage, and the search space can also be reduced.The principles of creating the initial sets of se-mantic patterns are summarized as follows.?
In the beginning stage, the aim is to cre-ate the initial set for the semantic pat-terns with length two.
Thus, the initialset is the all possible combinations oftwo quality concepts.?
In the latter stages, each initial set is cre-ated by adding a quality concept to eachof the better semantic patterns induced inthe previous stage.4.2 Distance MeasureThe distance measure is to measure the distancebetween the seed patterns and semantic patternsto be induced.
Let 1( ,..., )Ssp c c=  be a semanticpattern and 1( ,..., )seed Rsp c c=  be a given seedpattern, their distance is defined as( ), ( , ),seed s rDist sp sp Dist c c= ?
?
(8)where ( , )s rDist c c?
?
denotes the distance be-tween two semantic patterns in the HAL space.As mentioned earlier, after concept combination,a semantic pattern becomes a new concept in theHAL space, which means the semantic patterncan be represented by its left and right contexts.Thus, the distance between two semantic patternscan be computed through their context distance.Equation (8) thereby can be written as( ), ( , ) ( , ).s r s rleft left Right Rightseed c c c cDist sp sp Dist v v Dist v v?
?
?
?= +  (9)Because the weights of the vectors are repre-sented using a probabilistic framework, eachvector of a concept can be considered as a prob-abilistic distribution of the context words.
Ac-cordingly, we use the Kullback-Liebler (KL) dis-tance (Manning and Sch?tze, 1999) to computethe distance between two probabilistic distribu-tions, as shown in the following.1( )( ) ( ) log ,( )s rNj sc c j sj j rP t cD v v P t cP t c?
?
=?= ?
??
(10)where (    )D i i  denotes the KL distance be-tween two probabilistic distributions.
WhenEquation (10) is ill-conditioned, i.e., zero de-nominator, the denominator will be set to a smallvalue (10-6).
For the consideration of a symmet-ric distance, we use the divergence measure,shown as follows.
( , ) ( ) ( ).s r s r r sc c c c c cDiv v v D v v D v v?
?
?
?
?
?= +        (11)By this way, the distance between two probabil-istic distributions can be computed by their KLdivergence.
Thus, Equation (9) becomes( , ) ( , ) ( , ).s r s r s rleft left Right Rightc c c c c cDist v v Div v v Div v v?
?
?
?
?
?= + (12)After each semantic pattern is evaluated, aranked list is produced for relevance judgment.4.3 Relevance FeedbackIn the induction process, some non-relevant se-mantic patterns may have smaller distance to aseed pattern, which may decrease the precisionof the final results.
To overcome the problem,one possible solution is to incorporate expertknowledge to guide the induction process.
Forthis purpose, we use the technique of relevancefeedback.
In the IR community, the relevancefeedback is to enhance the original query fromthe users by indicating which retrieved docu-ments are relevant.
For our task, the relevancefeedback is applied after each semantic pattern isevaluated.
Then, the health professionals judgewhich semantic patterns are relevant to the seedpattern.
In practice, only the top n semantic pat-terns are presented for relevance judgment.
Fi-nally, the semantic patterns judged as relevantare considered to form the relevant set, and theothers form the non-relevant set.
According tothe relevant and non-relevant information, theseed pattern can be refined to be more similar tothe relevant set, such that the induction processcan induce more relevant patterns and moveaway from noisy patterns in the future iterations.949The refinement of the seed pattern is to adjustits context distributions (left and right).
Such ad-justment is based on re-weighting the dimensionsof the context vectors of the seed pattern.
Thedimensions more frequently regarded as relevantpatterns are more significant for identifying rele-vant patterns.
Hence, such dimensions of theseed pattern should be emphasized.
The signifi-cance of a dimension is measured as follows.
( )( )( ) ,i kij kjc tc Rkc tc RwSig tw??
???
?=??
(13)where ( )kSig t  denotes the significance of the di-mension kt , ic?
and jc?
denote the semanticpatterns of the relevant set and non-relevant set,respectively, and ( )i kc tw ?
and ( )j kc tw ?
denote theweights of kt  of ic?
and jc?
, respectively.
Thehigher the ratio, the more significant the dimen-sion is.
In order to smooth ( )kSig t  to the rangefrom zero to one, the following formula is used:1( ) ( )1( ) .1i k j ki jkc t c tc R c RSig tw w??
??
?
?
?=?
??
?+ ?
??
??
?
(14)The corresponding dimension of the seed patternseed rsp c= ?
is then re-weighted by( ) ( ) ( ).r k r kc t c t kw w Sig t?
?= +           (15)Once the context vectors of the seed patternare re-weighted, they are also transformed into aprobabilistic form using Equation (3).
The re-fined seed pattern will be taken as the referencebasis in the next iteration.
The relevance feed-back is performed iteratively until no more se-mantic patterns are judged as relevant or amaximum number of iteration is reached.
At thesame time, the induction process for a particularlength is also stopped.
The whole CIP process isstopped until the seed patterns are exhausted5 Experimental ResultsTo evaluate the performance of the CIP, we builta prototype system and provided a set of seedpatterns.
The seed patterns were collected by re-ferring to the well-defined instruments for as-sessing negative life events (Brostedt and Peder-sen, 2003; Pagano et al, 2004).
A total of 20seed patterns were selected by the health profes-sionals.
Then, the CIP randomly selects one seedpattern per run without replacement from theseed set, and iteratively induces relevant patternsfrom the psychiatry web corpora.
The psychiatryweb corpora used here include some professionalmental health web sites, such as PsychPark(http://www.psychpark.org) (Bai, 2001) and JohnTung Foundation (http://www.jtf.org.tw).In the following sections, we describe someexperiments to in turn examine the effect of us-ing relevance feedback or not, and the coverageon real data using the semantic patterns inducedby different approaches.
Because the semanticpatterns with a length larger than 4 are very rareto express a negative life event, we limit thelength k to the range of 2 to 4.5.1 Evaluation on Relevance FeedbackThe relevance feedback employed in this studyprovides the relevant and non-relevant informa-tion for the CIP so that it can refine the seed pat-tern to induce more relevant patterns.
The rele-vance judgment is carried out by three experi-enced psychiatric physicians.
For practical con-sideration, only the top 30 semantic patterns arepresented to the physicians.
During relevancejudgment, a majority vote mechanism is used tohandle the disagreements among the physicians.That is, a semantic pattern is considered as rele-vant if any two or more physicians judged it asrelevant.
Finally, the semantic patterns with ma-jority votes are obtained to form the relevant set.To evaluate the effectiveness of the relevancefeedback, we construct three variants of the CIP,RF(5), RF(10), and RF(20), implemented by ap-plying the relevance feedback for 5, 10, and 20iterations, respectively.
These three CIP variantsare then compared to the one without using therelevance feedback, denoted as RF(?).
We usethe evaluation metric, precision at 30 (prec@30),over all seed patterns to examine if the relevancefeedback can help the CIP induce more relevantpatterns.
For a particular seed pattern, prec@n iscomputed as the number of relevant semanticpatterns ranked in the top n of the ranked list,divided by n. Table 2 presents the results for k=2.The results reveal that the relevance feedbackcan help the CIP induce more relevant semanticpatterns.
Another observation indicates that ap-plying the relevance feedback for more iterationsRF(?)
RF(5) RF(10) RF(20)prec@30 0.203 0.263 0.318 0.387Table 2.
Effect of applying relevance feedbackfor different number of iterations or not.9500.150.20.250.30.350.40.450 5 10 15 20 25 30 35 40 45 50Num.
of Iterationsprec@30RF(10)+pseudoRF(20)RF(?
)Figure 4.
Effect of using the combination of rele-vance feedback and pseudo-relevance feedback.can further improve the precision.
However, it isusually impractical for experts to involve in theguiding process for too many iterations.
Conse-quently, we further consider pseudo-relevancefeedback to automate the guiding process.
Thepseudo-relevance feedback carries out the rele-vance judgment based on the assumption that thetop ranked semantic patterns are more likely tobe the relevant ones.
Thus, this approach usuallyrelies on setting a threshold or selecting only thetop n semantic patterns to form the relevant set.However, determining the threshold is not trivial,and the threshold may be different with differentseed patterns.
Therefore, we apply the pseudo-relevance feedback only after certain expert-guided iterations, rather than applying itthroughout the induction process.
The notion isthat we can get a more reliable threshold valueby observing the behavior of the relevant seman-tic patterns in the ranked list for a few iterations.To further examine the effectiveness of thecombined approach, we additionally construct aCIP variant, RF(10)+pseudo, by applying thepseudo-relevance feedback after 10 expert-guided iterations.
The threshold is determined bythe physicians during their judgments in the 10-th iteration.
The results are presented in Figure 4.The precision of RF(10)+pseudo is inferior tothat of RF(20) before the 25-th iteration.
Mean-while, after the 30-th iteration, RF(10)+pseudoachieves higher precision than the other methods.This indicates that the pseudo-relevance feed-back can also contribute to semantic pattern in-duction in the stage without expert intervention.5.2 Coverage on Real DataThe final results of the semantic patterns are therelevant sets of the last iteration produced byRF(10)+pseudo, denoted as CIPSP .
Parts of themare shown in Table 3.SeedPattern < boyfriend, argue >InducedPatterns <girlfriend, break up>; <friend, fight>Table 3.
Parts of induced semantic patterns.We compare CIPSP    to    those    created    by   acorpus-based approach.
The corpus-based ap-proach relies on an annotated domain corpus anda learning mechanism to induce the semanticpatterns.
Thus, we collected 300 consultationrecords from the PsychPark as the domain corpus,and each sentence in the corpus is annotated witha negative life event or not by the three physi-cians.
After the annotation process, the sentenceswith negative life events are together to form thetraining set.
Then, we adopt Mutual Information(Manning and Sch?tze, 1999) to learn variable-length semantic patterns.
The mutual informationbetween k words is defined as11 11( ,..., )( ,..., ) ( ,..., ) log( )kk k kiiP w wMI w w P w wP w==?
(16)where 1( ,... )kP w w  is the probability of the kwords co-occurring in a sentence in the trainingset, and ( )iP w  is the probability of a single wordoccurring in the training set.
Higher mutual in-formation indicates that the k words are morelikely to form a semantic pattern of length k.Here the length k also ranges from 2 to 4.
Foreach k, we compute the mutual information forall possible combinations of words in the trainingset, and those with their mutual informationabove a threshold are selected to be the final re-sults of the semantic patterns, denoted as MISP .In order to obtain reliable mutual informationvalues, only words with at least the minimumnumber of occurrences (>5) are considered.To examine the coverage of CIPSP  and MISP  onreal data, 15 human subjects are involved in cre-ating a test set.
The subjects provide their experi-enced negative life events in the form of naturallanguage sentences.
A total of 69 sentences arecollected to be the test set, of which 39 sentencescontain a semantic pattern of length two, 21 sen-tences contain a semantic pattern of length three,and 9 sentences contain a semantic pattern oflength four.
The evaluation metric used is out-of-pattern (OOP) rate, a ratio of unseen patternsoccurring in the test set.
Thus, the OOP can bedefined as the number of test sentences contain-ing the semantic patterns not occurring in thetraining set, divided by the total number of sen-tences in the test set.
Table 4 presents the results.951k=2 k=3 k=4CIPSP  0.36 (14/39) 0.48 (10/21) 0.44 (4/9)MISP  0.51 (20/39) 0.62 (13/21) 0.67 (6/9)Table 4.
OOP rate of the CIP and a corpus-basedapproach.The results show that the OOP of MISP  ishigher than that of CIPSP .
The main reason is thelack of a large enough domain corpus with anno-tated life events.
In this circumstance, many se-mantic patterns, especially for those with a largerlength, could not be learned, because the numberof their occurrences would be very rare in thetraining set.
With no doubt, one could collect alarge amount of domain corpus to reduce theOOP rate.
However, increasing the amount ofdomain corpus also increases the amount of an-notation and computation complexity.
Our ap-proach, instead, exploits the quality concepts toreduce the search space, also applies the rele-vance feedback to guide the induction process,thus it can achieve better results with time-limited constraints.6 ConclusionThis study has presented an HAL-based cascadedmodel for variable-length semantic pattern in-duction.
The HAL model provides an informa-tive infrastructure for the CIP to induce semanticpatterns from the unannotated psychiatry webcorpora.
Using the quality concepts and preserv-ing the better results from the previous stage, thesearch space can be reduced to speed up the in-duction process.
In addition, combining the rele-vance feedback and pseudo-relevance feedback,the induction process can be guided to inducemore relevant semantic patterns.
The experimen-tal results demonstrated that our approach cannot only reduce the reliance on annotated corporabut also obtain acceptable results with time-limited constraints.
Future work will be devotedto investigating the detection of negative lifeevents using the induced patterns so as to makethe psychiatric services more effective.ReferencesR.
Baeza-Yates and B. Ribeiro-Neto.
1999.
ModernInformation Retrieval.
Addison-Wesley, Reading,MA.Y.
M. Bai, C. C. Lin, J. Y. Chen, and W. C. Liu.
2001.Virtual Psychiatric Clinics.
American Journal ofPsychiatry, 158(7):1160-1161.J.
Bai, D. Song, P. Bruza, J. Y. Nie, and G. Cao.
2005.Query Expansion Using Term Relationships inLanguage Models for Information Retrieval.
InProc.
of the 14th ACM International Conferenceon Information and Knowledge Management,pages 688-695.E.
M. Brostedt and N. L. Pedersen.
2003.
StressfulLife Events and Affective Illness.
Acta Psychiat-rica Scandinavica, 107:208-215.C.
Burgess, K. Livesay, and K. Lund.
1998.
Explora-tions in Context Space: Words, Sentences, Dis-course.
Discourse Processes.
25(2&3):211-257.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT Press.T.
Grenager, D. Klein, and C. D. Manning.
2005.
Un-supervised Learning of Field Segmentation Modelsfor Information Extraction.
In Proc.
of the 43thAnnual Meeting of the ACL, pages 371-378.T.
Hasegawa, S. Sekine, R. Grishman.
2004.
Discov-ering Relations among Named Entities from LargeCorpora.
In Proc.
of the 42th Annual Meeting ofthe ACL,  pages 415-422.W.Lehnert, C. Cardie, D. Fisher, J. McCarthy, E.Riloff, and S. Soderland.
1992.
University of Mas-sachusetts: Description of the CIRCUS Systemused for MUC-4.
In Proc.
of the Fourth MessageUnderstanding Conference, pages 282-288.C.
Manning and H. Sch?tze.
1999.
Foundations ofStatistical Natural Language Processing.
MITPress.
Cambridge, MA.I.
Muslea.
1999.
Extraction Patterns for InformationExtraction Tasks: A Survey.
In Proc.
of the AAAI-99 Workshop on Machine Learning for InformationExtraction, pages 1-6.M.
E. Pagano, A. E. Skodol, R. L. Stout, M. T. Shea,S.
Yen, C. M. Grilo, C.A.
Sanislow, D. S. Bender,T.
H. McGlashan, M. C. Zanarini, and J. G. Gun-derson.
2004.
Stressful Life Events as Predictors ofFunctioning: Findings from the Collaborative Lon-gitudinal Personality Disorders Study.
Acta Psy-chiatrica Scandinavica, 110:421-429.M.
Stevenson and M. A. Greenwood.
2005.
A Seman-tic Approach to IE Pattern Induction.
In Proc.
ofthe 43th Annual Meeting of the ACL, pages 379-386.C.
H. Wu, L. C. Yu, and F. L. Jang.
2005.
Using Se-mantic Dependencies to Mine Depressive Symp-toms from Consultation Records.
IEEE IntelligentSystem, 20(6):50-58.J.
F. Yeh, C. H. Wu, M. J. Chen, and L. C. Yu.
2004.Automated Alignment and Extraction of BilingualDomain Ontology for Cross-Language Domain-Specific Applications.
In Proc.
of the 20th COL-ING, pages 1140-1146.952
