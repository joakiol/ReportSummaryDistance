Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 305?308,Dublin, Ireland, August 23-24, 2014.HulTech: A General Purpose System for Cross-Level Semantic Similaritybased on Anchor Web CountsJose G. Moreno Rumen Moraliyski Asma Berrezoug Ga?el DiasNormandie UniversityUNICAEN, GREYC CNRSF-14032 Caen, Francefirstname.lastname@unicaen.frAbstractThis paper describes the HULTECH team par-ticipation in Task 3 of SemEval-2014.
Fourdifferent subtasks are provided to the partici-pants, who are asked to determine the semanticsimilarity of cross-level test pairs: paragraph-to-sentence, sentence-to-phrase, phrase-to-word and word-to-sense.
Our system adoptsa unified strategy (general purpose system) tocalculate similarity across all subtasks basedon word Web frequencies.
For that purpose,we define ClueWeb InfoSimba, a cross-levelsimilarity corpus-based metric.
Results showthat our strategy overcomes the proposed base-lines and achieves adequate to moderate re-sults when compared to other systems.1 IntroductionSimilarity between text documents is considered achallenging task.
Recently, many works concentrate onthe study of semantic similarity for multi-level text doc-uments (Pilehvar et al., 2013), but skipping the cross-level similarity task.
In the later, the underlying idea isthat text similarity can be considered between pairs oftext documents at different granularities levels: para-graph, sentence, phrase or word.
One obvious partic-ularity of this task is that text pairs may not share thesame characteristics of size, context or structure, i.e.,the granularity level.In task 3 of SemEval-2014, two different strategieshave been proposed to solve this issue.
On the onehand, participants may propose a combination of indi-vidual systems, each one solving a particular subtask.On the other hand, a general purpose system may beproposed, which deals with all the subtasks followingthe exact same strategy.In this paper, we describe a language-independentcorpus-based general purpose system, which relies ona huge freely available Web collection called Anchor-ClueWeb12 (Hiemstra and Hauff, 2010).
In particular,we calculate ClueWeb InfoSimba1a cross-level seman-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1It is a Web version of InfoSimba (Dias et al., 2007).tic similarity based on word-word frequencies.
Indeed,these frequencies are captured by the use of a colloca-tion metric called SCP2(Silva et al., 1999), which hassimilar properties as the well studied PMI-IR (Turney,2001) but does not over-evaluate rare events.Our system outputs a normalized (between 0 and 1)similarity value between two pieces of texts.
However,the subtasks proposed in task 3 of SemEval-2014 in-clude a different scoring scale between 0 and 4.
Tosolve this issue, we applied linear, polynomial and ex-ponential regressions as three different runs.
Resultsshow that our strategy overcomes the proposed base-lines and achieves adequate to moderate results whencompared to other systems.2 System DescriptionOur system is based on a reduced version of theClueWeb12 dataset called Anchor ClueWeb12 and aninformative attributional similarity measure called In-foSimba (Dias et al., 2007) adapted to this dataset.2.1 Anchor ClueWeb12 DatasetThe Anchor ClueWeb12 dataset contains 0.5 billionWeb pages, which cover about 64% of the total num-ber of Web pages in ClueWeb12.
The particularity ofAnchor ClueWeb12 is that each Web page is repre-sented by the anchor texts of the links pointing to itin ClueWeb12.
Web pages are indexed not on theircontent but on their references.
As such, the size ofthe index is drastically reduced and the overall resultsare consistent with full text indexing as discussed in(Hiemstra and Hauff, 2010).For development purposes, this dataset was indexedin Solr 4.4 on a desktop computer using a batch in-dexing script.
Particularly, each compressed part fileof the Anchor ClueWeb12 was uncompressed, prepro-cessed and indexed in a sequential way using the fea-tures of incremental indexing offered by Solr (Smileyand Pugh, 2009).2.2 InfoSimbaIn (Dias et al., 2007), the authors proposed the hypothe-sis that two texts are similar if they share related (even-tually different) constituents.
So, their concept of simi-2Symmetric Conditional Probability.305larity is not any more based on the exact match of con-stituents but relies on related constituents (e.g.
words).For example, it is clear that the following text piecesextracted from the sentence-to-phrase subtask are re-lated3although they do not share any word.1.
he is a nose-picker2.
an uncouth young manThe InfoSimba similarity measure models this phe-nomenon evaluating individual similarities between allpossible words pairs.
Indeed, each piece of text is rep-resented by the vector of its words.
So, given twopieces of texts Xiand Xj, their similarity is definedin Equation 1 where SCP (., .)
is the Symmetric Con-ditional Probability association measure proposed in(Silva et al., 1999) and defined in Equation 2.IS(Xi, Xj) =1pqp?k=1q?l=1SCP (wik, wjl).
(1)SCP (wik, wjl) =P (wik, wjl)2P (wik)?
P (wjl).
(2)Following the previous example, the In-foSimba value between the two vectorsX1= {?he?, ?is?, ?a?, ?nose-picker?}
andX2= {?an?, ?uncouth?, ?young?, ?man?}
isan average weight formed by all possible words pairsassociations as illustrated in Figure 1.
Note that eachvertex is a word of a Xlvector and each edge isweighted by the SCP (., .)
value of the connectedwords.
In particular, each wijcorresponds to theword at the jthposition in vector Xi, P (., .)
is thejoint probability of two words appearing in the samedocument, P (.)
is the marginal probability of anyword appearing in a document and p (resp.
q) is thesize of the vector Xi(resp.
Xj).Figure 1: Pairs of words evaluated when InfoSimba iscalculated.In the case of task 3 of SemEval-2014, each textpair is represented by two word vectors for which amodified version of InfoSimba, ClueWeb InfoSimba,is computed.3The score of this pair (#85) in the training set is the max-imum value 4.2.3 ClueWeb InfoSimbaThe final similarity metric, called ClueWeb InfoSimba(CWIS), between two pieces of texts is defined inEquation 3, where hits(w) returns the number of doc-uments retrieved by Solr over Anchor ClueWeb12 forthe query w and hits(wa?
wb) is the number of doc-uments retrieved when both words are present simul-taneously.
In this case, SCP is modified into SCP-IRsimilarly as PMI is to PMI-IR, i.e., using hits countsinstead of probability values (see Equation 4).CWIS(Xi, Xj) =1pqp?k=1q?l=1SCP ?
IR(wik, wjl).
(3)SCP ?
IR(wik, wjl) =hits(wik?
wjl)2hits(wik).hits(wjl).
(4)2.4 System InputThe task 3 of SemEval-2014 consists of (1) paragraph-to-sentence, (2) sentence-to-phrase, (3) phrase-to-wordand (4) word-to-sense subtasks.
Before submitting thepieces of texts to our system, we first performed simplestop-words removal with the NLTK toolkit (Bird et al.,2009).
Note that in the case of the word-to-sense sub-task, the similarity is performed over the word itselfand the gloss of the corresponding sense4.2.5 Output Values TransformationsThe CWIS(., .)
similarity metric returns a value be-tween 0 and 1.
However, the subtasks suppose thateach pair must be attributed a score between 0 and 4.As such, an adequate scale transformation must be per-formed.
For that purpose, we proposed linear, polyno-mial and exponential regressions and submitted threedifferent runs, one for each regression5.
Note that theregressions have been tuned on the training dataset us-ing the respective R regression functions with defaultparameters:?
lm(y ?
x),?
lm(y ?
x + I(x2) + I(x3)),?
lm(log(y + ) ?
x),where 6is a small value included to avoid undefinedlog values.
The regression results on the test datasetsare presented in Figure 2.4Glosses are obtained from WordNet using the sense idprovided for the task by the organizers.5In the case of linear and exponential, these are mono-thetic functions therefore ranking-based evaluation metricsgive the same score before and after this step.6In our experiments, this value was set to 0.001.306Figure 2: Linear, polynomial and exponential predic-tions for the test dataset of the paragraph-to-sentencesubtask (colored dots).
Black dots correspond to theobtained ClueWeb InfoSimba value versus the manu-ally assigned score in the training dataset.3 Evaluation and ResultsFor evaluation purposes, two metrics have been se-lected by the organizers: Pearson correlation (Pearson,1895) and Spearman?s rank correlation (Hollander andWolfe, 1973).
Detailed information about the evalu-ation setup can be found in the task discussion paper(Jurgens et al., 2014).All results are given in Tables 1 and 2 for eachrun.
Note that the baseline metric is calculated for thelongest common string (LCS) and that each regressionhas been tuned on the training dataset for each one ofthe four tasks.First, in almost all cases, the results outperform thebaseline.
Second, performances show that with a cer-tain amount of information (longer pieces of texts), in-teresting results can be obtained.
However, when thesize decreases, the performance diminishes and extrainformation is certainly needed to better capture the se-mantics between two pieces of text.
Third, the poly-nomial regression provides better results for the Pear-son correlation evaluation, while for the Rho test, linearand polynomial regressions get the lead.
Note that thissituation depends on the data distribution and cannotbe seen as a conclusive remark.
However, it is cer-tainly an important subject of study for our unsuper-vised methodology.Another key point is that training examples wereused only for evaluation purposes7.
In the case ofSpearman?s rank correlation, the linear and exponen-7For Pearson correlation, valid interval was fixed to [0,4].tial transformations obviously show exact same values(See Table 2).4 ConclusionsIn this paper, we proposed a general purpose systemto deal with cross-level text similarity.
The aim ofour research was to push as far as possible the lim-its of language-independent corpus-based solutions ina general context of text similarity.
We were also con-cerned with reproducibility and as such we exclusivelyused publicly available datasets and tools8.
The resultsclearly show the limits of a simple solution based onword statistics.
Nevertheless, the framework can easilybe empowered with the straightforward introduction ofmore competitive resources.AcknowledgementThe authors would like to thank the University ofMostaganem (Algeria) for providing an internship toAsma Berrezoug at the Normandie University.ReferencesSteven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media, Inc., 1st edition.Ga?el Dias, Elsa Alves, and Jos?e Gabriel Pereira Lopes.2007.
Topic segmentation algorithms for text sum-marization and passage retrieval: An exhaustiveevaluation.
In Proceedings of AAAI, pages 1334?1339.Djoerd Hiemstra and Claudia Hauff.
2010.
Mirex:Mapreduce information retrieval experiments.
InCTIT Technical Report TR-CTIT-10-15, Centre forTelematics and Information Technology, Universityof Twente, pages 1?8.Myles Hollander and Douglas A. Wolfe.
1973.
Non-parametric Statistical Methods.
John Wiley andSons, New York.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
Task 3: Cross-level seman-tic similarity.
In Proceedings of SemEval-2014.Karl Pearson.
1895.
Note on regression and inheri-tance in the case of two parents.
Proceedings of theRoyal Society of London, 58(347-352):240?242.Mohammad Taher Pilehvar, David Jurgens, andRoberto Navigli.
2013.
Align, disambiguate andwalk: A unified approach for measuring semanticsimilarity.
In Proceedings of ACL, pages 1341?1351.Joaquim Ferreira da Silva, Ga?el Dias, Sylvie Guillor?e,and Jos?e Gabriel Pereira Lopes.
1999.
Using local-maxs algorithm for the extraction of contiguous and8Scripts to Index the Anchor ClueWeb12 Dataset areavailable under request.307Method Paragraph2Sentence Sentence2Phrase Phrase2Word Word2SenseLinear (run 3) 0.669 0.671 0.232 0.137Polynomial (run 1) 0.693 0.665 0.254 0.150Exponential (run 2) 0.667 0.633 0.180 0.169Baseline (LCS) 0.527 0.562 0.165 0.109Table 1: Overall results for the Pearson correlation.Method Paragraph2Sentence Sentence2Phrase Phrase2Word Word2SenseLinear (run 3) 0.688 0.633 0.260 0.124Polynomial (run 1) 0.666 0.633 0.260 0.126Exponential (run 2) 0.688 0.633 0.260 0.124Baseline (LCS) 0.613 0.626 0.162 0.130Table 2: Overall results for the Spearman?s rank correlation.non-contiguous multiword lexical units.
In Proceed-ings of EPIA, pages 113?132.David Smiley and Eric Pugh.
2009.
Solr 1.4 Enter-prise Search Server.
Packt Publishing.Peter Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In Proceedings of ECML,pages 491?502.308
