Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1121?1128,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Effective Two-Stage Model for Exploiting Non-Local Dependencies inNamed Entity RecognitionVijay KrishnanComputer Science DepartmentStanford UniversityStanford, CA 94305vijayk@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305manning@cs.stanford.eduAbstractThis paper shows that a simple two-stageapproach to handle non-local dependen-cies in Named Entity Recognition (NER)can outperform existing approaches thathandle non-local dependencies, while be-ing much more computationally efficient.NER systems typically use sequence mod-els for tractable inference, but this makesthem unable to capture the long distancestructure present in text.
We use a Con-ditional Random Field (CRF) based NERsystem using local features to make pre-dictions and then train another CRF whichuses both local information and featuresextracted from the output of the first CRF.Using features capturing non-local depen-dencies from the same document, our ap-proach yields a 12.6% relative error re-duction on the F1 score, over state-of-the-art NER systems using local-informationalone, when compared to the 9.3% relativeerror reduction offered by the best systemsthat exploit non-local information.
Ourapproach also makes it easy to incorpo-rate non-local information from other doc-uments in the test corpus, and this givesus a 13.3% error reduction over NER sys-tems using local-information alone.
Ad-ditionally, our running time for inferenceis just the inference time of two sequen-tial CRFs, which is much less than thatof other more complicated approaches thatdirectly model the dependencies and doapproximate inference.1 IntroductionNamed entity recognition (NER) seeks to lo-cate and classify atomic elements in unstructuredtext into predefined entities such as the namesof persons, organizations, locations, expressionsof times, quantities, monetary values, percent-ages, etc.
A particular problem for Named En-tity Recognition(NER) systems is to exploit thepresence of useful information regarding labels as-signed at a long distance from a given entity.
Anexample is the label-consistency constraint that ifour text has two occurrences of New York sepa-rated by other tokens, we would want our learnerto encourage both these entities to get the same la-bel.Most statistical models currently used forNamed Entity Recognition, use sequence mod-els and thereby capture local structure.
HiddenMarkov Models (HMMs) (Leek, 1997; Freitagand McCallum, 1999), Conditional Markov Mod-els (CMMs) (Borthwick, 1999; McCallum et al,2000), and Conditional Random Fields (CRFs)(Lafferty et al, 2001) have been successfully em-ployed in NER and other information extractiontasks.
All these models encode the Markov prop-erty i.e.
labels directly depend only on the labelsassigned to a small window around them.
Thesemodels exploit this property for tractable com-putation as this allows the Forward-Backward,Viterbi and Clique Calibration algorithms to be-come tractable.
Although this constraint is essen-tial to make exact inference tractable, it makes usunable to exploit the non-local structure present innatural language.Label consistency is an example of a non-localdependency important in NER.
Apart from labelconsistency between the same token sequences,we would also like to exploit richer sources of de-pendencies between similar token sequences.
Forexample, as shown in Figure 1, we would wantit to encourage Einstein to be labeled ?Person?
ifthere is strong evidence that Albert Einstein shouldbe labeled ?Person?.
Sequence models unfortu-1121told that Albert Einstein proved .
.
.
on seeing Einstein at theFigure 1: An example of the label consistency problem.
Here we would like our model to encourage entities Albert Einsteinand Einstein to get the same label, so as to improve the chance that both are labeled PERSON.nately cannot model this due to their Markovianassumption.Recent approaches attempting to capture non-local dependencies model the non-local dependen-cies directly, and use approximate inference al-gorithms, since exact inference is in general, nottractable for graphs with non-local structure.Bunescu and Mooney (2004) define a Rela-tional Markov Network (RMN) which explicitlymodels long-distance dependencies, and use it torepresent relations between entities.
Sutton andMcCallum (2004) augment a sequential CRF withskip-edges i.e.
edges between different occur-rences of a token, in a document.
Both theseapproaches use loopy belief propagation (Pearl,1988; Yedidia et al, 2000) for approximate infer-ence.Finkel et al (2005) hand-set penalties for incon-sistency in entity labeling at different occurrencesin the text, based on some statistics from trainingdata.
They then employ Gibbs sampling (Gemanand Geman, 1984) for dealing with their local fea-ture weights and their non-local penalties to do ap-proximate inference.We present a simple two-stage approach whereour second CRF uses features derived from theoutput of the first CRF.
This gives us the advan-tage of defining a rich set of features to modelnon-local dependencies, and also eliminates theneed to do approximate inference, since we do notexplicitly capture the non-local dependencies in asingle model, like the more complex existing ap-proaches.
This also enables us to do inference ef-ficiently since our inference time is merely the in-ference time of two sequential CRF?s; in contrastFinkel et al (2005) reported an increase in runningtime by a factor of 30 over the sequential CRF,with their Gibbs sampling approximate inference.In all, our approach is simpler, yields higherF1 scores, and is also much more computationallyefficient than existing approaches modeling non-local dependencies.2 Conditional Random FieldsWe use a Conditional Random Field (Lafferty etal., 2001; Sha and Pereira, 2003) since it rep-resents the state of the art in sequence model-ing and has also been very effective at NamedEntity Recognition.
It allows us both discrim-inative training that CMMs offer as well andthe bi-directional flow of probabilistic informationacross the sequence that HMMs allow, therebygiving us the best of both worlds.
Due to thebi-directional flow of information, CRFs guardagainst the myopic locally attractive decisions thatCMMs make.
It is customary to use the Viterbi al-gorithm, to find the most probably state sequenceduring inference.
A large number of possibly re-dundant and correlated features can be suppliedwithout fear of further reducing the accuracy ofa high-dimensional distribution.
These are well-documented benefits (Lafferty et al, 2001).2.1 Our Baseline CRF for Named EntityRecognitionOur baseline CRF is a sequence model in which la-bels for tokens directly depend only on the labelscorresponding to the previous and next tokens.
Weuse features that have been shown to be effectivein NER, namely the current, previous and nextwords, character n-grams of the current word, Partof Speech tag of the current word and surround-ing words, the shallow parse chunk of the currentword, shape of the current word, the surroundingword shape sequence, the presence of a word in aleft window of size 5 around the current word andthe presence of a word in a left window of size 5around the current word.
This gives us a compet-itive baseline CRF using local information alone,whose performance is close to the best publishedlocal CRF models, for Named Entity Recognition3 Label ConsistencyThe intuition for modeling label consistency isthat within a particular document, different occur-1122Document Level Statistics Corpus Level StatisticsPER LOC ORG MISC PER LOC ORG MISCPER 3141 4 5 0 33830 113 153 0LOC 6436 188 3 346966 6749 60ORG 2975 0 43892 223MISC 2030 66286Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is givena certain label and the other occurrence is given a certain label.
We show these counts both within documents, as well as overthe whole corpus.
As we would expect, most pairs of the same entity sequence are labeled the same(i.e.
the diagonal has mostof the density) at both the document and corpus levels.
These statistics are from the CoNLL 2003 English training set.Document Level Statistics Corpus Level StatisticsPER LOC ORG MISC PER LOC ORG MISCPER 1941 5 2 3 9111 401 261 38LOC 0 167 6 63 68 4560 580 1543ORG 22 328 819 191 221 19683 5131 4752MISC 14 224 7 365 50 12713 329 8768Table 2: Table showing the number of (token sequence, token subsequence) pairs where the token sequence is assigned a certainentity label, and the token subsequence is assigned a certain entity label.
We show these counts both within documents, as wellas over the whole corpus.
Rows correspond to sequences, and columns to subsequences.
These statistics are from the CoNLL2003 English training set.rences of a particular token sequence (or similartoken sequences) are unlikely to have different en-tity labels.
While this constraint holds stronglyat the level of a document, there exists additionalvalue to be derived by enforcing this constraintless strongly across different documents.
We wantto model label consistency as a soft and not a hardconstraint; while we want to encourage differentoccurrences of similar token sequences to get la-beled as the same entity, we do not want to forcethis to always hold, since there do exist exceptions,as can be seen from the off-diagonal entries of ta-bles 1 and 2.A named entity recognition system modelingthis structure would encourage all the occurrencesof the token sequence to the same entity type,thereby sharing evidence among them.
Thus, ifthe system has strong evidence about the label ofa given token sequence, but is relatively unsureabout the label to be assigned to another occur-rence of a similar token sequence, the system cangain significantly by using the information aboutthe label assigned to the former occurrence, to la-bel the relatively ambiguous token sequence, lead-ing to accuracy improvements.The strength of the label consistency constraint,can be seen from statistics extracted from theCoNLL 2003 English training data.
Table 1 showsthe counts of entity labels pairs assigned for eachpair of identical token sequences both within adocument and across the whole corpus.
As wewould expect, inconsistent labelings are relativelyrare and most pairs of the same entity sequenceare labeled the same(i.e.
the diagonal has mostof the density) at both the document and corpuslevels.
A notable exception to this is the labelingof the same text as both organization and locationwithin the same document and across documents.This is a due to the large amount of sports news inthe CoNLL dataset due to which city and countrynames are often also team names.
We will see thatour approach is capable of exploiting this as well,i.e.
we can learn a model which would not pe-nalize an Organization-Location inconsistency asstrongly as it penalizes other inconsistencies.In addition, we also want to model subsequenceconstraints: having seen Albert Einstein earlier ina document as a person is a good indicator that asubsequent occurrence of Einstein should also belabeled as a person.
Here, we would expect that asubsequence would gain much more by knowingthe label of a supersequence, than the other wayaround.However, as can be seen from table 2, wefind that the consistency constraint does not holdnearly so strictly in this case.
A very common caseof this in the CoNLL dataset is that of documentscontaining references to both The China Daily, anewspaper, and China, the country (Finkel et al,2005).
The first should be labeled as an organiza-tion, and second as a location.
The counts of sub-sequence labelings within a document and acrossdocuments listed in Table 2, show that there aremany off-diagonal entries: the China Daily case isamong the most common, occurring 328 times inthe dataset.
Just as we can model off-diagonal pat-1123terns with exact token sequence matches, we canalso model off-diagonal patterns for the token sub-sequence case.In addition, we could also derive some value byenforcing some label consistency at the level ofan individual token.
Obviously, our model wouldlearn much lower weights for these constraints,when compared to label consistency at the levelof token sequences.4 Our Approach to Handling non-localDependenciesTo handle the non-local dependencies betweensame and similar token sequences, we define threesets of feature pairs where one member of the fea-ture pair corresponds to a function of aggregatestatistics of the output of the first CRF at the doc-ument level, and the other member correspondsto a function of aggregate statistics of the out-put of the first CRF over the whole test corpus.Thus this gives us six additional feature types forthe second round CRF, namely Document-levelToken-majority features, Document-level Entity-majority features, Document-level Superentity-majority features, Corpus-level Token-majorityfeatures, Corpus-level Entity-majority featuresand Corpus-level Superentity-majority features.These feature types are described in detail below.All these features are a function of the outputlabels of the first CRF, where predictions on thetest set are obtained by training on all the data, andpredictions on the train data are obtained by 10fold cross-validation (details in the next section).Our features fired based on document and corpuslevel statistics are:?
Token-majority features: These refer to themajority label assigned to the particular to-ken in the document/corpus.
Eg: Supposewe have three occurrences of the token Aus-tralia, such that two are labeled Locationand one is labeled Organization, our token-majority feature would take value Locationfor all three occurrences of the token.
Thisfeature can enable us to capture some depen-dence between token sequences correspond-ing to a single entity and having common to-kens.?
Entity-majority features: These refer to themajority label assigned to the particular en-tity in the document/corpus.
Eg: Suppose wehave three occurrences of the entity sequence(we define it as a token sequence labeled as asingle entity by the first stage CRF) Bank ofAustralia, such that two are labeled Organi-zation and one is labeled Location, our entity-majority feature would take value Organiza-tion for all tokens in all three occurrences ofthe entity sequence.
This feature enables usto capture the dependence between identicalentity sequences.
For token labeled as not aNamed Entity by the first CRF, this featurereturns the majority label assigned to that to-ken when it occurs as a single token namedentity.?
Superentity-majority features: These re-fer to the majority label assigned to superse-quences of the particular entity in the docu-ment/corpus.
By entity supersequences, werefer to entity sequences, that strictly containwithin their span, another entity sequence.For example, if we have two occurrences ofBank of Australia labeled Organization andone occurrence of Australia Cup labeled Mis-cellaneous, then for all occurrences of the en-tity Australia, the superentity-majority fea-ture would take value Organization.
This fea-ture enables us to take into account labels as-signed to supersequences of a particular en-tity, while labeling it.
For token labeled as nota Named Entity by the first CRF, this featurereturns the majority label assigned to all enti-ties containing the token within their span.The last feature enables entity sequences tobenefit from labels assigned to entities whichare entity supersequences of it.
We attemptedto add subentity-majority features, analogous tothe superentity-majority features to model depen-dence on entity subsequences, but got no bene-fit from it.
This is intuitive, since the basic se-quence model would usually be much more cer-tain about labels assigned to the entity superse-quences, since they are longer and have more con-textual information.
As a result of this, whilethere would be several cases in which the basicsequence model would be uncertain about labelsof entity subsequences but relatively certain aboutlabels of token supersequences, the converse isvery unlikely.
Thus, it is difficult to profit fromlabels of entity subsequences while labeling en-tity sequences.
We also attempted using more fine1124grained features corresponding to the majority la-bel of supersequences that takes into account theposition of the entity sequence in the entity su-persequence(whether the entity sequence occurs inthe start, middle or end of the supersequence), butcould obtain no additional gains from this.It is to be noted that while deciding if to-ken sequences are equal or hold a subsequence-supersequence relation, we ignore case, whichclearly performs better than being sensitive tocase.
This is because our dataset contains sev-eral entities in allCaps such as AUSTRALIA, es-pecially in news headlines.
Ignoring case enablesus to model dependences with other occurrenceswith a different case such as Australia.It may appear at first glance, that our frame-work can only learn to encourage entities to switchto the most popular label assigned to other occur-rences of the entity sequence and similar entity se-quences.
However this framework is capable oflearning interesting off-diagonal patterns as well.To understand this, let us consider the example ofdifferent occurrences of token sequences being la-beled Location and Organization.
Suppose, themajority label of the token sequence is Location.While this majority label would encourage the sec-ond CRF to switch the labels of all occurrencesof the token sequence to Location, it would notstrongly discourage the CRF from labeling theseas Organization, since there would be several oc-currences of token sequences in the training datalabeled Organization, with the majority label ofthe token sequence being Location.
However itwould discourage the other labels strongly.
Thereasoning is analogous when the majority label isOrganization.In case of a tie (when computing the majoritylabel), if the label assigned to a particular tokensequence is one of the majority labels, we fire thefeature corresponding to that particular label beingthe majority label, instead of breaking ties arbi-trarily.
This is done to encourage the second stageCRF to make its decision based on local informa-tion, in the absence of compelling non-local infor-mation to choose a different label.5 Advantages of our approachWith our two-stage approach, we manage to getimprovements on the F1 measure over existing ap-proaches that model non-local dependencies.
Atthe same time, the simplicity of our two-stage ap-proach keeps inference time down to just the in-ference time of two sequential CRFs, when com-pared to approaches such as those of Finkel et al(2005) who report that their inference time withGibbs sampling goes up by a factor of about 30,compared to the Viterbi algorithm for the sequen-tial CRF.Below, we give some intuition about areas forimprovement in existing work and explain howour approach incorporates the improvements.?
Most existing work to capture label-consistency, has attempted to create all(n2)pairwise dependencies between the differentoccurrences of an entity, (Finkel et al, 2005;Sutton and McCallum, 2004), where n isthe number of occurrences of the givenentity.
This complicates the dependencygraph making inference harder.
It also leadsto the penalty for deviation in labeling togrow linearly with n, since each entity wouldbe connected to ?
(n) entities.
When anentity occurs several times, these modelswould force all occurrences to take the samevalue.
This is not what we want, sincethere exist several instances in real-life datawhere different entities like persons andorganizations share the same name.
Thus,our approach makes a certain entity?s labeldepend on certain aggregate information ofother labels assigned to the same entity, anddoes not enforce pairwise dependencies.?
We also exploit the fact that the predictionsof a learner that takes non-local dependen-cies into account would have a good amountof overlap with a sequential CRF, since thesequence model is already quite competitive.We use this intuition to approximate the ag-gregate information about labels assigned toother occurrences of the entity by the non-local model, with the aggregate informationabout labels assigned to other occurrences ofthe entity by the sequence model.
This intu-ition enables us to learn weights for non-localdependencies in two stages; we first get pre-dictions from a regular sequential CRF andin turn use aggregate information about pre-dictions made by the CRF as extra features totrain a second CRF.?
Most work has looked to model non-local de-pendencies only within a document (Finkel1125et al, 2005; Chieu and Ng, 2002; Suttonand McCallum, 2004; Bunescu and Mooney,2004).
Our model can capture the weaker butstill important consistency constraints acrossthe whole document collection, whereas pre-vious work has not, for reasons of tractabil-ity.
Capturing label-consistency at the levelof the whole test corpus is particularly helpfulfor token sequences that appear only once intheir documents, but occur a few times overthe corpus, since they do not have strong non-local information from within the document.?
For training our second-stage CRF, we needto get predictions on our train data as well astest data.
Suppose we were to use the sametrain data to train the first CRF, we would getunrealistically good predictions on our traindata, which would not be reflective of its per-formance on the test data.
One option is topartition the train data.
This however, canlead to a drop in performance, since the sec-ond CRF would be trained on less data.
Toovercome this problem, we make predictionson our train data by doing a 10-fold cross val-idation on the train data.
For predictions onthe test data, we use all the training data totrain the CRF.
Intuitively, we would expectthat the quality of predictions with 90% ofthe train data would be similar to the qual-ity of predictions with all the training data.
Itturns out that this is indeed the case, as canbe seen from our improved performance.6 Experiments6.1 Dataset and EvaluationWe test the effectiveness of our techniqueon the CoNLL 2003 English named en-tity recognition dataset downloadable fromhttp://cnts.uia.ac.be/conll2003/ner/.
The datacomprises Reuters newswire articles annotatedwith four entity types: person (PER), location(LOC), organization (ORG), and miscellaneous(MISC).
The data is separated into a training set,a development set (testa), and a test set (testb).The training set contains 945 documents, andapproximately 203,000 tokens and the test sethas 231 documents and approximately 46,000tokens.
Performance on this task is evaluated bymeasuring the precision and recall of annotatedentities (and not tokens), combined into an F1score.
There is no partial credit for labeling partof an entity sequence correctly; an incorrect entityboundary is penalized as both a false positive andas a false negative.6.2 Results and DiscussionIt can be seen from table 3, that we achieve a12.6% relative error reduction, by restricting our-selves to features approximating non-local depen-dency within a document, which is higher thanother approaches modeling non-local dependen-cies within a document.
Additionally, by incorpo-rating non-local dependencies across documentsin the test corpus, we manage a 13.3% relative er-ror reduction, over an already competitive base-line.
We can see that all three features approxi-mating non-local dependencies within a documentyield reasonable gains.
As we would expect theadditional gains from features approximating non-local dependencies across the whole test corpusare relatively small.We use the approximate randomization test(Yeh, 2000) for statistical significance of the dif-ference between the basic sequential CRF and oursecond round CRF, which has additional featuresderived from the output of the first CRF.
With a1000 iterations, our improvements were statisti-cally significant with a p-value of 0.001.
Sincethis value is less than the cutoff threshold of 0.05,we reject the null hypothesis.The simplicity of our approach makes it easy toincorporate dependencies across the whole corpus,which would be relatively much harder to incor-porate in approaches like (Bunescu and Mooney,2004) and (Finkel et al, 2005).
Additionally,our approach makes it possible to do inferencein just about twice the inference time with a sin-gle sequential CRF; in contrast, approaches likeGibbs Sampling that model the dependencies di-rectly can increase inference time by a factor of30 (Finkel et al, 2005).An analysis of errors by the first stage CRF re-vealed that most errors are that of single token en-tities being mislabeled or missed altogether fol-lowed by a much smaller percentage of multi-ple token entities mislabelled completely.
All ourfeatures directly encode information that is use-ful to reducing these errors.
The widely preva-lent boundary detection error is that of miss-ing a single-token entity (i.e.
labeling it asOther(O)).
Our approach helps correct many sucherrors based on occurrences of the token in other1126F1 scores on the CoNLL DatasetApproach LOC ORG MISC PER ALL Relative Error reductionBunescu and Mooney (2004) (Relational Markov Networks)Only Local Templates - - - - 80.09Global and Local Templates - - - - 82.30 11.1%Finkel et al (2005)(Gibbs Sampling)Local+Viterbi 88.16 80.83 78.51 90.36 85.51Non Local+Gibbs 88.51 81.72 80.43 92.29 86.86 9.3%Our Approach with the 2-stage CRFBaseline CRF 88.09 80.88 78.26 89.76 85.29+ Document token-majority features 89.17 80.15 78.73 91.60 86.50+ Document entity-majority features 89.50 81.98 79.38 91.74 86.75+ Document superentity-majority features 89.52 82.27 79.76 92.71 87.15 12.6%+ Corpus token-majority features 89.48 82.36 79.59 92.65 87.13+ Corpus entity-majority features 89.72 82.40 79.71 92.65 87.23+ Corpus superentity-majority features(All features) 89.80 82.39 79.76 92.57 87.24 13.3%Table 3: Table showing improvements obtained with our additional features, over the baseline CRF.
We also compare ourperformance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvementthan existing work despite starting from a very competitive baseline CRF.named entities.
Other kinds of boundary detec-tion errors involving multiple tokens are very rare.Our approach can also handle these errors by en-couraging certain tokens to take different labels.This together with the clique features encodingthe markovian dependency among neighbours cancorrect some multiple-token boundary detectionerrors.7 Related WorkRecent work looking to directly model non-localdependencies and do approximate inference arethat of Bunescu and Mooney (2004), who usea Relational Markov Network (RMN) (Taskar etal., 2002) to explicitly model long-distance de-pendencies, Sutton and McCallum (2004), whointroduce skip-chain CRFs, which add additionalnon-local edges to the underlying CRF sequencemodel (which Bunescu and Mooney (2004) lack)and Finkel et al (2005) who hand-set penaltiesfor inconsistency in labels based on the trainingdata and then use Gibbs Sampling for doing ap-proximate inference where the goal is to obtainthe label sequence that maximizes the product ofthe CRF objective function and their penalty.
Un-fortunately, in the RMN model, the dependenciesmust be defined in the model structure before do-ing any inference, and so the authors use heuristicpart-of-speech patterns, and then add dependen-cies between these text spans using clique tem-plates.
This generates an extremely large num-ber of overlapping candidate entities, which ren-ders necessary additional templates to enforce theconstraint that text subsequences cannot both bedifferent entities, something that is more naturallymodeled by a CRF.
Another disadvantage of thisapproach is that it uses loopy belief propagationand a voted perceptron for approximate learningand inference, which are inherently unstable algo-rithms leading to convergence problems, as notedby the authors.
In the skip-chain CRFs model,the decision of which nodes to connect is alsomade heuristically, and because the authors focuson named entity recognition, they chose to connectall pairs of identical capitalized words.
They alsoutilize loopy belief propagation for approximatelearning and inference.
It is hard to directly ex-tend their approach to model dependencies richerthan those at the token level.The approach of Finkel et al (2005) makesit possible a to model a broader class of long-distance dependencies than Sutton and McCallum(2004), because they do not need to make any ini-tial assumptions about which nodes should be con-nected and they too model dependencies betweenwhole token sequences representing entities andbetween entity token sequences and their token su-persequences that are entities.
The disadvantageof their approach is the relatively ad-hoc selec-tion of penalties and the high computational costof running Gibbs sampling.Early work in discriminative NER employedtwo stage approaches that are broadly similar toours, but the effectiveness of this approach appearsto have been overlooked in more recent work.Mikheev et al (1999) exploit label consistencyinformation within a document using relativelyad hoc multi-stage labeling procedures.
Borth-1127wick (1999) used a two-stage approach similar toours with CMM?s where Reference Resolution fea-tures which encoded the frequency of occurrencesof other entities similar to the current token se-quence, were derived from the output of the firststage.
Malouf (2002) and Curran and Clark (2003)condition the label of a token at a particular posi-tion on the label of the most recent previous in-stance of that same token in a previous sentenceof the same document.
This violates the Markovproperty and therefore instead of finding the max-imum likelihood sequence over the entire docu-ment (exact inference), they label one sentence at atime, which allows them to condition on the max-imum likelihood sequence of previous sentences.While this approach is quite effective for enforc-ing label consistency in many NLP tasks, it per-mits a forward flow of information only, which canresult in loss of valuable information.
Chieu andNg (2002) propose a solution to this problem: foreach token, they define additional features basedon known information, taken from other occur-rences of the same token in the document.
This ap-proach has the advantage of allowing the trainingprocedure to automatically learn good weights forthese ?global?
features relative to the local ones.However, it is hard to extend this to incorporateother types of non-local structure.8 ConclusionWe presented a two stage approach to model non-local dependencies and saw that it outperformedexisting approaches to modeling non-local depen-dencies.
Our approach also made it easy to ex-ploit various dependencies across documents inthe test corpus, whereas incorporating this infor-mation in most existing approaches would makethem intractable due to the complexity of the resul-tant graphical model.
Our simple approach is alsovery computationally efficient since the inferencetime is just twice the inference time of the basic se-quential CRF, while for approaches doing approx-imate inference, the inference time is often wellover an order of magnitude over the basic sequen-tial CRF.
The simplicity of our approach makes iteasy to understand, implement, and adapt to newapplications.AcknowledgmentsWe wish to Jenny R. Finkel for discussions onNER and her CRF code.
Also, thanks to TrondGrenager for NER discussions and to WilliamMorgan for help with statistical significance tests.Also, thanks to Vignesh Ganapathy for helpful dis-cussions and Rohini Rajaraman for comments onthe writeup.This work was supported in part by a Scot-tish Enterprise Edinburgh-Stanford Link grant(R37588), as part of the EASIE project.ReferencesA.
Borthwick.
1999.
A Maximum Entropy Approach toNamed Entity Recognition.
Ph.D. thesis, New York Uni-versity.R.
Bunescu and R. J. Mooney.
2004.
Collective informationextraction with relational Markov networks.
In Proceed-ings of the 42nd ACL, pages 439?446.H.
L. Chieu and H. T. Ng.
2002.
Named entity recognition: amaximum entropy approach using global information.
InProceedings of the 19th Coling, pages 190?196.J.
R. Curran and S. Clark.
2003.
Language independent NERusing a maximum entropy tagger.
In Proceedings of the7th CoNLL, pages 164?167.J.
Finkel, T. Grenager, and C. D. Manning.
2005.
Incorporat-ing non-local information into information extraction sys-tems by gibbs sampling.
In Proceedings of the 42nd ACL.D.
Freitag and A. McCallum.
1999.
Information extractionwith HMMs and shrinkage.
In Proceedings of the AAAI-99 Workshop on Machine Learning for Information Ex-traction.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration of im-ages.
IEEE Transitions on Pattern Analysis and MachineIntelligence, 6:721?741.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
ConditionalRandom Fields: Probabilistic models for segmenting andlabeling sequence data.
In Proceedings of the 18th ICML,pages 282?289.
Morgan Kaufmann, San Francisco, CA.T.
R. Leek.
1997.
Information extraction using hiddenMarkov models.
Master?s thesis, U.C.
San Diego.R.
Malouf.
2002.
Markov models for language-independentnamed entity recognition.
In Proceedings of the 6thCoNLL, pages 187?190.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maximum en-tropy Markov models for information extraction and seg-mentation.
In Proceedings of the 17th ICML, pages 591?598.
Morgan Kaufmann, San Francisco, CA.A.
Mikheev, M. Moens, and C. Grover.
1999.
Named entityrecognition without gazetteers.
In Proceedings of the 9thEACL, pages 1?8.J.
Pearl.
1988.
Probabilistic reasoning in intelligent systems:Networks of plausible inference.
In Morgan Kauffmann.F.
Sha and F. Pereira.
2003.
Shallow parsing with con-ditional random fields.
In Proceedings of NAACL-2003,pages 134?141.C.
Sutton and A. McCallum.
2004.
Collective segmentationand labeling of distant entities in information extraction.In ICML Workshop on Statistical Relational Learning andIts connections to Other Fields.B.
Taskar, P. Abbeel, and D. Koller.
2002.
Discriminativeprobabilistic models for relational data.
In Proceedings ofUAI-02.J.
S. Yedidia, W. T. Freeman, and Y. Weiss.
2000.
Gener-alized belief propagation.
In Proceedings of NIPS-2000,pages 689?695.Alexander Yeh.
2000.
More accurate tests for the statisti-cal significance of result differences.
In Proceedings ofCOLING 2000.1128
