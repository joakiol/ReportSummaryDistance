CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 268?272Manchester, August 2008Applying Sentence Simplification to the CoNLL-2008 Shared TaskDavid Vickrey and Daphne KollerStanford UniversityStanford, CA 94305-9010{dvickrey,koller}@cs.stanford.eduAbstractOur submission to the CoNLL-2008shared task (Surdeanu et al, 2008) focusedon applying a novel method for semanticrole labeling to the shared task.
Our systemfirst simplifies each sentence to be labeledusing a set of hand-constructed rules; theweights of the system are trained on se-mantic role labeling data to generate sim-plifications which are as useful as possiblefor semantic role labeling.
Our system isonly a semantic role labeling system, andthus did not receive a score for SyntacticDependencies (or, by extension, a score forthe complete problem).
Unlike most sys-tems in the shared task, our system tookconstituency parses as input.
On the sub-task of semantic dependencies, our systemobtained an F1 score of 76.17, the high-est in the open task.
In this paper we givea high-level overview of the sentence sim-plification system, and discuss and analyzethe modifications to this system requiredfor the CoNLL-2008 shared task.1 Sentence SimplificationThe main technical interest of our method is a sen-tence simplification system.
This system is de-scribed in depth in (Vickrey and Koller, 2008); forlack of space, we omit many details, including adiscussion of related work, from this paper.Current semantic role labeling systems rely pri-marily on syntactic features in order to identifyand classify roles.
Features derived from a syntac-tic parse of the sentence have proven particularlyuseful (Gildea and Jurafsky, 2002).
For example,the syntactic subject of ?eat?
is nearly always thec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.SNP VPVPNP PPTom wants SatoeatVPNP NPsaladcroutonswithTom: NP S(NP) VPVP VPS TNP1croutons: VPPP(with)Tsalad: NP1 VP TFigure 1: Constituency parse with path features for verb?eat?.
I was not given a chance to eat.Someone gave me a chance to eat.I had a chance to eat.I ate.depassivizegive -> havechance to XI was given a chance to eat.remove notFigure 2: Example simplificationARG0.
An example sentence with extracted pathfeatures is shown in Figure 1.A major problem with this approach is that thepath from a phrase to the verb can be quite com-plicated.
In the sentence ?He expected to receive a prizefor winning,?
the path from ?win?
to its ARG0, ?he?,involves the verbs ?expect?
and ?receive?
and thepreposition ?for.?
The corresponding path throughthe parse tree likely occurs a small number of times(or not at all) in the training corpus.
If the test setcontained exactly the same sentence but with ?ex-pected?
replaced by ?did not expect?
we would ex-tract a different parse path feature; therefore, as faras the classifier is concerned, the syntax of the twosentences is totally unrelated.The idea of our method is to learn a mappingfrom full, complicated sentences to simplified sen-268NP-7[Someone] VB-5 NPVP-4give chanceNP-2IS-1S-1NP-2 VP-3VB*-6VBN-5be VP-4TransformedRuleReplace 3 with 4Create new node 7 ?
[Someone]Substitute 7 for 2Add 2 after 5Set category of 5 to VBSNP VPVBDVBN NPwasVPgiven chanceIOriginalFigure 3: Rule for depassivizing a sentencetences.
Figure 2 shows an example of a seriesof simplifications applied to the sentence ?I wasnot given a chance to eat.?
Our method com-bines hand-written syntactic simplification ruleswith machine learning, which determines whichrules to prefer.
We then use the output of the sim-plification system as input to an SRL system thatis trained to label simplified sentences.There are several reasons to simplify sentencesbefore doing semantic role labeling.
First, label-ing simplified sentences is much easier than label-ing raw sentences.
Second, by mapping all sen-tences to a common, canonical form, we can gener-alize more effectively across sentences with differ-ing syntax.
Third, our model is effective at sharinginformation across verbs, since most of our simpli-fication rules apply equally well regardless of thetarget verb.
These latter two benefits are particu-larly important for verbs with few labeled traininginstances; using training examples efficiently canlead to considerable gains in performance.Note that unlike most participants in theCoNLL-2008 Shared Task (Surdeanu et al, 2008),our model took as input constituency parses asgenerated by the Charniak parser (specifically, weused the parses provided with the CoNLL-2005shared task distribution).
This was the only labeleddata used that was not available in the closed task.1.1 Transformation RulesAt the center of our sentence simplification systemis a hand-written set of transformation rules.
Atransformation rule takes as input a parse tree andproduces as output a different, changed parse tree.Since our goal is to produce a simplified versionof the sentence, the rules are designed to bring allsentences toward the same common format.A rule (see left side of Figure 3) consists of twoparts.
The first is a ?tree regular expression?, a treefragment with optional constraints at each node.The rule assigns numbers to each node which arereferred to in the second part of the rule.
Formally,a rule node X matches a parse-tree node A if: (1)SimplifiedOriginal#Rule CategoryI ate the food.Float(The food) Iate.5Floating nodesHe slept.I said he slept.4Sentence extractionFood is tasty.Salt makes food tasty.8?Make?
rewritesThe total includes tax.Including tax, the total?7Verb acting as PP/NPJohn has achance to eat.John?s chance to eat?7PossessiveI will eat.Will I eat?7Questions I will eat.Nor will I eat.7Inverted sentencesFloat(The food) Iate.The food I ate ?8Modified nounsI eat.I have a chance toeat.7Verb RC (Noun)I eat.I am likely to eat.6Verb RC (ADJP/ADVP) I eat.I want to eat.17Verb Raising/Control (basic)I eat.I must eat.14Verb Collapsing/RewritingI ate.I ate and slept.8Conjunctions John is a lawyer.John, a lawyer, ?20Misc Collapsing/RewritingA car hit me.I was hit by a car.5PassiveI slept Thursday.Thursday, I slept.24Sentence normalizationTable 1: Rule categories with sample simplifica-tions.
Target verbs are underlined.All constraints of node X (e.g., constituent cate-gory, head word, etc.)
are satisfied by node A.
(2) For each child node Y of X, there is a childB of A that matches Y; two children of X cannotbe matched to the same child B.
There are no otherrequirements.
A can have other children besidesthose matched, and leaves of the rule pattern canmatch to internal nodes of the parse (correspond-ing to entire phrases in the original sentence).
Forexample, the same rule is used to simplify both ?Ihad a chance to eat,?
and ?I had a chance to eat asandwich,?
(into ?I ate,?
and ?I ate a sandwich,?
).The second part of the rule is a series of simplesteps that are applied to the matched nodes.
For ex-ample, one type of simple step applied to the pairof nodes (X,Y) removes X from its current parentand adds it as the final child of Y.
Figure 3 showsthe depassivizing rule and the result of applying itto the sentence ?I was given a chance.?
The trans-formation steps are applied sequentially from topto bottom.
Any nodes not matched are unaffectedby the transformation; they remain where they arerelative to their parents.
For example, ?chance?is not matched by the rule, and thus remains as achild of the VP headed by ?give.
?1.2 Rule SetAltogether, we currently have 154 (mostly unlex-icalized) rules.
Table 1 shows a summary of ourrule-set, grouped by type.
Note that each row lists269only one possible sentence and simplification rulefrom that category; many of the categories handle avariety of syntax patterns.
Our rule set was devel-oped by analyzing performance and coverage onthe PropBank WSJ training set; neither the devel-opment set nor (of course) the test set were usedduring rule creation.
Please refer to (Vickrey andKoller, 2008) for more details about the rule set.In the context of the CoNLL-2008 Shared Task,the rule set might be viewed as consisting of out-side information.
Since we only submitted a sys-tem to the open task, this was not an issue.1.3 Generating Simple SentencesWe now describe how to produce all possible sim-plified sentences for a given input sentence.
Wemaintain a set of derived parses S which is initial-ized to contain only the original, untransformedparse.
One iteration of the algorithm consists ofapplying every possible matching transformationrule to every parse in S, and adding all resultingparses to S. With carefully designed rules, re-peated iterations are guaranteed to converge; thatis, we eventually arrive at a set?S such that if weapply an iteration of rule application to?S, no newparses are added.
Note that we simplify the wholesentence without respect to a particular verb.We then find all parses in?S that have ?eat?
asthe main verb.
We call such a parse a valid simplesentence; this is exactly the canonicalized versionof the sentence which our simplification rules aredesigned to produce.1.4 Labeling Simple SentencesFor a particular sentence/target verb pair s, v, theoutput from the previous section is a set Ssv={tsvi}iof valid simple sentences.
From the train-ing set, we now extract a set of role patternsGv= {gvj}jfor each verb v. For example, acommon role pattern for ?give?
is that of ?I gavehim a sandwich?.
We represent this pattern asggive1= {ARG0 = Subject NP, ARG1 =Postverb NP2, ARG2 = Postverb NP1}.For each simple sentence tsvi?
Ssv, we ap-ply all extracted role patterns gvjto tsvi, obtaininga set of possible role labelings.
We call a sim-ple sentence/role labeling pair a simple labelingand denote the set of candidate simple labelingsCsv= {csvk}k.1.5 Probabilistic ModelGiven a (possibly large) set of candidate simple la-belings Csv, we need to select a correct one.
WeRule = DepassivizePattern = {ARG0 = Subj NP, ARG1 = PV NP2, ARG2 = PV NP1}Role = ARG0, Head Word = JohnRole = ARG1, Head Word = sandwichRole = ARG2, Head Word = IRole = ARG0, Category = NPRole = ARG1, Category = NPRole = ARG2, Category = NPRole = ARG0, Position = Subject NPRole = ARG1, Position = Postverb NP2Role = ARG2, Position = Postverb NP1Figure 4: Features for ?John gave me a sandwich.
?assign a score to each candidate based on its fea-tures: which rules were used to obtain the simplesentence, which role pattern was used, and fea-tures about the assignment of constituents to roles.The set of extracted features for the sentence ?Iwas given a sandwich by John?
with simplification?John gave me a sandwich?
is shown in Figure 4.We now define a log-linear model which as-signs a probability to each candidate simple label-ing based on its score.
Specifically, the probabilityof a simple labeling csvkwith respect to a weightvector w is P (csvk) =ewTfsvk?k?ewTfsvk?.Unfortunately, we do not have labeled examplesof correct simplifications.
To get around this, wetreat the correct simplification as a hidden variable.Thus, we say that the probability of a particularsemantic role labeling is?csvk?KsvP (csvk).
Thisleads to our final objective,?s,v??log?csvk?KsvewTfsvk?csvk??CsvewTfsvk???
?wTw2?2.We train our model by optimizing the objectiveusing standard methods, specifically BFGS.
Dueto the summation over the hidden variable repre-senting the choice of simplification (not observedin the training data), our objective is not convex.Thus, we are not guaranteed to find a global opti-mum; in practice we have gotten good results usingthe initialization of setting all weights to 0.2 Baseline ModelIn addition to our simplification system, we alsobuilt a high-performing logistic regression modelfor semantic role labeling, which we refer to asBaseline.
This model uses a slightly modified ver-sion of the features used in (Pradhan et al, 2005).This model was also trained on the PropBank train-ing set, using Charniak constituency parses.270Both our simplification model andBaseline pro-duce labeled constituencies.
Since we were re-quired to produce semantic dependency relations,we simply labeled the head word of each con-stituent with the role chosen by the model.3 Labeling NounsThe 2008 shared task requires systems to label thearguments of both nouns and verbs.
However, oursentence simplification system was built to handleonly verbs.
While in principle the model can nat-urally be extended to label nouns by the additionof further syntactic simplification rules, we werenot able to complete this extension in time for thecontest deadline.
Instead, we trained our Baselinemodel to label the arguments of nouns as well asverbs.
The features of this model are the same asthose used to label verbs, and were not extended tohandle special features of nouns.4 Identifying PredicatesAnother important subtask was to identify thepredicates to be labeled.
In the labeled trainingcorpus, nouns with no labeled arguments are gen-erally skipped (i.e., not labeled as predicates at all).Thus, we made a strong simplifying assumption: ifa predicate (either noun or verb) is labeled by oursystem as having no arguments, we should not la-bel it as being a predicate.
On the developmentset, out of a total of 6390 labeled predicates, only23 had no labeled arguments; thus, this assumptionappears to be quite reasonable.Our system architecture was as follows.
First,we modified the training (and test) set by mark-ing as a potential predicate every word that was ei-ther: a) a verb that wasn?t ?do?, ?be?, or ?have?
orb) a noun found in the nombank index.
Then, wetrained our system on all potential predicates (notjust predicates that were actually labeled).
Finally,after applying our classifier to the test data, we re-moved any predicate with no labeled arguments.5 Sense-Tagging PredicatesWe tried three simple heuristics for sense-labelingthe predicates.
All of them were applied at the endof our pipeline, and thus did not interact with theargument labeling decisions.The simplest heuristic labeled every predicate assense 1.
A slightly more intelligent heuristic la-beled every predicate with its most common sensein the training set.
Finally, we extended this heuris-tic to label each verb with its most common sensefor the subcategorization (i.e., set of roles) actu-ally produced by the labeling system.
Thus, if onesense was intransitive while the other was transi-tive, we would be able to distinguish between them(assuming that our labeling system produced thecorrect set of arguments).
For this third heuris-tic, we ignored all but the core arguments (ARG0 -ARG5).
The final heuristic was the most effective,as discussed in the results section.6 ResultsThe first stage of Baseline, which tries to filter outconstituents which are obviously not arguments,took about three hours and approximately 4Gb ofmemory to train1.
The second stage, which per-forms the final classification of arguments, tookabout four hours and 3Gb of memory to train.The sentence simplification system, which wewill refer to as Simplification, works in two steps.First, it generates the set of all possible simplifi-cations for each sentence.
This step took a rela-tively small amount of memory, under 1Gb, buttook around 24 hours to complete.
The set of sim-plifications is stored in a compact form; the totalstorage required for all simplifications of all sen-tences was roughly 4 times the (compressed) sizeof the Charniak input parses.
The second step,which trains the model using the possible simplifi-cations, took around 12 hours and 3Gb of memory.We only submitted results for the semantic de-pendencies portion of the competition.
The sys-tem we used was the Combined system describedin (Vickrey and Koller, 2008), which combinesthe simplification procedure with the Baselinemodel.
The Combined model was augmentedwith the modifications described above.
Our sys-tem achieved an official F1 score on the SRLsubtask of 76.17, the highest in the open task.Our results are not strictly comparable to thosein the closed task, due to the use of the Char-niak parser trained on Penn Treebank constituencyparses.
However, a comparison still provides in-sight into the relative strength of our system; ourscore would place us tied for fourth in the closedchallenge for semantic dependencies.We will now discuss the relative contributionsof various components of our system.
All resultsin this section are for TestWSJ + TestBrown.Our Combined model provides the same ben-efit over Baseline as described in (Vickrey and1All runs were done on a dual core 2.66Mhz Xeon ma-chine with 4Gb of RAM271Koller, 2008) for labeling the arguments of verbs2.When applied to just verb predicates, the Com-binedmodel provides a statistically significant im-provement of 1.2 points of F1 score over Base-line.
However, since the CoNLL-2008 shared taskadds both labeling of noun dependencies and pred-icate identification and sense tagging, the gain dueto better labeling arguments of verbs is reduced.The Baseline model achieves an F1 score of 75.31on the semantic dependencies task, .86 F1 pointslower than the Combined system.Note that while most of this gain is directly dueto better verb argument labeling, better verb ar-gument labeling also indirectly slightly improvespredicate identification and sense-tagging since weuse the predicted arguments for both of of thesesubtasks.
We do in fact see a small increase forlabeling and sense-tagging predicates, from 80.72F1 for the Baseline to 80.81 F1 for Combined.As mentioned, we use Baseline to label the ar-guments of nouns.
Noun argument labeling ap-pears to be more difficult than verb argument la-beling, or at least requires some modification ofthe features.
Baseline obtains an F1 score of 75.64for verbs, but only 68.19 F1 for nouns.On the subtask of predicate identification, Com-bined achieved an F1 of 90.65.
It performed bet-ter on verbs than nouns.
For predicates with partof speech VB*3it scored 95.43 F1; for predicateswith part of speech NN*, it scored 85.97 F1.
Verbswithout arguments are often labeled in the golddata, so the verb score could perhaps be improvedby retaining verb predicates without arguments.As described above, we tried three heuristics forsense-labeling predicates.
Our final system usedthe third heuristic, which chose the most com-mon sense for the set of labeled arguments pro-duced by the system.
Combined obtained an F1score of 80.81 on the combined predicate identifi-cation/classification task, with a score of 82.58 forverbs and 79.28 for nouns.
The decrease in per-formance by adding classification is much largerfor verbs than nouns; verb sense classification isapparently significantly more difficult than nounsense classification (at least for verbal nouns).Table 2 compares the results of the Combinedsystem using each of the three heuristics.
Going2Note that the scoring metrics are different between theCoNLL-2005 and CoNLL-2008 shared tasks.
The CoNLL-2005 required the constituent boundaries to be labeled cor-rectly, while the CoNLL-2008 only requires identifying thehead word of each argument.3This category includes some nouns, e.g.
gerunds.Overall Predicate ID/ClassHeuristic Score All Verbs NounsAlways 1 75.69 79.29 81.26 77.58Most common 76.02 80.33 81.73 79.21Best for subcat 76.17 80.81 82.58 79.28Table 2: Relative performance of sense-labeling heuristicsfrom the simplest heuristic to the third heuristicgained 1.52 points of F1 score on the subtask ofpredicate identification/classification, and an im-provement of .48 F1 score for the overall seman-tic dependency score.
Another interesting thing tonote is that all of improvement for noun predicatescame from choosing the most common sense in-stead of always choosing sense 1.
On the otherhand, using subcategorization information is quiteimportant for sense-tagging verbs.7 Discussion and Future WorkThe CoNLL-08 task introduces two new sub-tasks for labeling semantic dependencies: predi-cate identification and predicate classification.
Ourexperimental results show that both are non-trivialand suggest that there is room for additional im-provement on these subtasks.We are particularly interested in two extensionsto our simplification model related to the 2008shared task.
The first is extending our simplifica-tion model to handle the arguments of nouns.
Asdiscussed above, there is a large amount of roomfor improvement for argument labeling of nouns.The second is incorporating uncertainty from theparser into our model.
Specifically, we would liketo extract a complete parse forest from the Char-niak parser and use it as input to our model.
Thiswould allow our simplification model to jointlyreason about the correct parse, possible simplifica-tions of those parses, and semantic role labelingsof the resulting simplified sentences.ReferencesGildea, D. and D. Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics.Pradhan, S., K. Hacioglu, V. Krugler, W. Ward, J. H.Martin, and D. Jurafsky.
2005.
Support vector learn-ing for semantic argument classification.
MachineLearning, 60(1-3):11?39.Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez,and J. Nivre.
2008.
The CoNLL-2008 shared task onjoint parsing of syntactic and semantic dependencies.In Proceedings of CoNLL.Vickrey, D. and D. Koller.
2008.
Sentence simplifica-tion for semantic role labeling.
Proceedings of ACL.272
