Mining New Word Translations from Comparable CorporaLi Shao and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{shaoli, nght}@comp.nus.edu.sgAbstractNew words such as names, technical terms,etc appear frequently.
As such, the bilinguallexicon of a machine translation system hasto be constantly updated with these new wordtranslations.
Comparable corpora such asnews documents of the same period from dif-ferent news agencies are readily available.
Inthis paper, we present a new approach to min-ing new word translations from comparablecorpora, by using context information tocomplement transliteration information.
Weevaluated our approach on six months of Chi-nese and English Gigaword corpora, with en-couraging results.1.
IntroductionNew words such as person names, organizationnames, technical terms, etc.
appear frequently.
Inorder for a machine translation system to trans-late these new words correctly, its bilingual lexi-con needs to be constantly updated with newword translations.Much research has been done on using parallelcorpora to learn bilingual lexicons (Melamed,1997; Moore, 2003).
But parallel corpora arescarce resources, especially for uncommon lan-guage pairs.
Comparable corpora refer to textsthat are not direct translation but are about thesame topic.
For example, various news agenciesreport major world events in different languages,and such news documents form a readily avail-able source of comparable corpora.
Being morereadily available, comparable corpora are thusmore suitable than parallel corpora for the task ofacquiring new word translations, although rela-tively less research has been done in the past oncomparable corpora.
Previous research efforts onacquiring translations from comparable corporainclude (Fung and Yee, 1998; Rapp, 1995; Rapp,1999).When translating a word w, two sources of in-formation can be used to determine its transla-tion: the word w itself and the surrounding wordsin the neighborhood (i.e., the context) of w. Mostprevious research only considers one of the twosources of information, but not both.
For exam-ple, the work of (Al-Onaizan and Knight, 2002a;Al-Onaizan and Knight, 2002b; Knight andGraehl, 1998) used the pronunciation of w intranslation.
On the other hand, the work of (Caoand Li, 2002; Fung and Yee, 1998; Koehn andKnight, 2002; Rapp, 1995; Rapp, 1999) used thecontext of w to locate its translation in a secondlanguage.In this paper, we propose a new approach forthe task of mining new word translations fromcomparable corpora, by combining both contextand transliteration information.
Since bothsources of information are complementary, theaccuracy of our combined approach is better thanthe accuracy of using just context or translitera-tion information alone.
We fully implementedour method and tested it on Chinese-Englishcomparable corpora.
We translated Chinesewords into English.
That is, Chinese is the sourcelanguage and English is the target language.
Weachieved encouraging results.While we have only tested our method on Chi-nese-English comparable corpora, our method isgeneral and applicable to other language pairs.2.
Our approachThe work of (Fung and Yee, 1998; Rapp, 1995;Rapp, 1999) noted that if an English word e  isthe translation of a Chinese word c , then thecontexts of the two words are similar.
We couldview this as a document retrieval problem.
Thecontext (i.e., the surrounding words) of c  isviewed as a query.
The context of each candidatetranslation 'e  is viewed as a document.
Since thecontext of the correct translation e  is similar tothe context of c , we are likely to retrieve thecontext of e  when we use the context of c  asthe query and try to retrieve the most similardocument.
We employ the language modelingapproach (Ng, 2000; Ponte and Croft, 1998) forthis retrieval problem.
More details are given inSection 3.On the other hand, when we only look at theword w itself, we can rely on the pronunciationof w to locate its translation.
We use a variant ofthe machine transliteration method proposed by(Knight and Graehl, 1998).
More details aregiven in Section 4.Each of the two individual methods provides aranked list of candidate words, associating witheach candidate a score estimated by the particularmethod.
If a word e  in English is indeed thetranslation of a word c  in Chinese, then wewould expect e  to be ranked very high in bothlists in general.
Specifically, our combinationmethod is as follows: we examine the top Mwords in both lists and find keee ,...,, 21 that ap-pear in top M positions in both lists.
We thenrank these words keee ,...,, 21  according to theaverage of their rank positions in the two lists.The candidate ei that is ranked the highest ac-cording to the average rank is taken to be the cor-rect translation and is output.
If no words appearwithin the top M positions in both lists, then notranslation is output.Since we are using comparable corpora, it ispossible that the translation of a new word doesnot exist in the target corpus.
In particular, ourexperiment was conducted on comparable cor-pora that are not very closely related and as such,most of the Chinese words have no translationsin the English target corpus.3.
Translation by contextIn a typical information retrieval (IR) problem, aquery is given and a ranked list of documentsmost relevant to the query is returned from adocument collection.For our task, the query is )(cC , the context(i.e., the surrounding words) of a Chinese wordc .
Each )(eC , the context of an English worde , is considered as a document in IR.
If an Eng-lish word e  is the translation of a Chinese wordc , they will have similar contexts.
So we use thequery )(cC  to retrieve a document )( *eC  thatbest matches the query.
The English word *ecorresponding to that document )( *eC  is thetranslation of c .Within IR, there is a new approach to docu-ment retrieval called the language modeling ap-proach (Ponte & Croft, 98).
In this approach, alanguage model is derived from each documentD .
Then the probability of generating the queryQ  according to that language model, )|( DQP ,is estimated.
The document with the highest)|( DQP  is the one that best matches the query.The language modeling approach to IR has beenshown to give superior retrieval performance(Ponte & Croft, 98; Ng, 2000), compared withtraditional vector space model, and we adopt thisapproach in our current work.To estimate )|( DQP , we use the approach of(Ng, 2000).
We view the document D as a mul-tinomial distribution of terms and assume thatquery Q  is generated by this model:?
?= tct ttDtPnDQPc)|(!!
)|(where t  is a term in the corpus, tc  is the numberof times term t  occurs in the query Q ,?= t tcn is the total number of terms in queryQ .For ranking purpose, the first fraction?t tcn !/!
can be omitted as this part depends onthe query only and thus is the same for all thedocuments.In our translation problem, )(cC  is viewed asthe query and )(eC  is viewed as a document.
Soour task is to compute ))(|)(( eCcCP for eachEnglish word e  and find the e  that gives thehighest ))(|)(( eCcCP , estimated as:??
)()()))((|(cCttqcccceCTtPTerm ct is a Chinese word.
)( ctq  is the numberof occurrences of ct  in )(cC .
))(( eCTc  is thebag of Chinese words obtained by translating theEnglish words in )(eC , as determined by a bi-lingual dictionary.
If an English word is ambigu-ous and has K translated Chinese words listed inthe bilingual dictionary, then each of the K trans-lated Chinese words is counted as occurring 1/Ktimes in ))(( eCTc  for the purpose of probabilityestimation.We use backoff and linear interpolation forprobability estimation:)()1()))((|()))((|(cmlccmlcctPeCTtPeCTtP??+?=???
?=))(())(())(()()()))((|(eCTteCTceCTccmlccctdtdeCTtPwhere )(?mlP  are the maximum likelihood esti-mates, )())(( ceCT td c  is the number of occurrencesof the term ct  in ))(( eCTc , and )( cml tP is esti-mated similarly by counting the occurrences ofct  in the Chinese translation of the whole Eng-lish corpus.
?
is set to 0.6 in our experiments.4.
Translation by transliterationFor the transliteration model, we use a modifiedmodel of (Knight and Graehl, 1998) and (Al-Onaizan and Knight, 2002b).Knight and Graehl (1998) proposed a prob-abilistic model for machine transliteration.
In thismodel, a word in the target language (i.e., Eng-lish in our task) is written and pronounced.
Thispronunciation is converted to source languagepronunciation and then to source language word(i.e., Chinese in our task).
Al-Onaizan andKnight (2002b) suggested that pronunciation canbe skipped and the target language letters can bemapped directly to source language letters.Pinyin is the standard Romanization system ofChinese characters.
It is phonetic-based.
Fortransliteration, we estimate )|( ceP as follows:??
?===a iiaiaplPpinyinaePpinyinePceP)|()|,()|()|(First, each Chinese character in a Chineseword c is converted to pinyin form.
Then we sumover all the alignments that this pinyin form of ccan map to an English word e. For each possiblealignment, we calculate the probability by takingthe product of each mapping.
ip  is the ith sylla-ble of pinyin, ail  is the English letter sequencethat the ith pinyin syllable maps to in the particu-lar alignment a.Since most Chinese characters have only onepronunciation and hence one pinyin form, weassume that Chinese character-to-pinyin mappingis one-to-one to simplify the problem.
We use theexpectation maximization (EM) algorithm togenerate mapping probabilities from pinyin syl-lables to English letter sequences.
To reduce thesearch space, we limit the number of Englishletters that each pinyin syllable can map to as 0,1, or 2.
Also we do not allow cross mappings.That is, if an English letter sequence 1e  precedesanother English letter sequence 2e in an Englishword, then the pinyin syllable mapped to 1emust precede the pinyin syllable mapped to 2e .Our method differs from (Knight and Graehl,1998) and (Al-Onaizan and Knight, 2002b) inthat our method does not generate candidates butonly estimates )|( ceP for candidates e appear-ing in the English corpus.
Another difference isthat our method estimates )|( ceP  directly, in-stead of )|( ecP  and )(eP .5.
Experiment5.1  ResourcesFor the Chinese corpus, we used the LinguisticData Consortium (LDC) Chinese Gigaword Cor-pus from Jan 1995 to Dec 1995.
The corpus ofthe period Jul to Dec 1995 was used to come upwith new Chinese words c for translation intoEnglish.
The corpus of the period Jan to Jun1995 was just used to determine if a Chineseword c from Jul to Dec 1995 was new, i.e., notoccurring from Jan to Jun 1995.
Chinese Giga-word corpus consists of news from two agencies:Xinhua News Agency and Central News Agency.As for English corpus, we used the LDC Eng-lish Gigaword Corpus from Jul to Dec 1995.
TheEnglish Gigaword corpus consists of news fromfour newswire services: Agence France PressEnglish Service, Associated Press WorldstreamEnglish Service, New York Times NewswireService, and Xinhua News Agency English Ser-vice.
To avoid accidentally using parallel texts,we did not use the texts of Xinhua News AgencyEnglish Service.The size of the English corpus from Jul to Dec1995 was about 730M bytes, and the size of theChinese corpus from Jul to Dec 1995 was about120M bytes.We used a Chinese-English dictionary whichcontained about 10,000 entries for translating thewords in the context.
For the training of translit-eration probability, we required a Chinese-English name list.
We used a list of 1,580 Chi-nese-English name pairs as training data for theEM algorithm.5.2 PreprocessingUnlike English, Chinese text is composed ofChinese characters with no demarcation forwords.
So we first segmented Chinese text with aChinese word segmenter that was based onmaximum entropy modeling (Ng and Low,2004).We then divided the Chinese corpus from Julto Dec 1995 into 12 periods, each containing textfrom a half-month period.
Then we determinedthe new Chinese words in each half-month pe-riod p. By new Chinese words, we refer to thosewords that appeared in this period p but not fromJan to Jun 1995 or any other periods that pre-ceded p. Among all these new words, we se-lected those occurring at least 5 times.
Thesewords made up our test set.
We call these wordsChinese source words.
They were the words thatwe were supposed to find translations from theEnglish corpus.For the English corpus, we performed sen-tence segmentation and converted each word toits morphological root form and to lower case.We also divided the English corpus into 12 pe-riods, each containing text from a half-monthperiod.
For each period, we selected those Eng-lish words occurring at least 10 times and werenot present in the 10,000-word Chinese-Englishdictionary we used and were not stop words.
Weconsidered these English words as potentialtranslations of the Chinese source words.
We callthem English translation candidate words.
For aChinese source word occurring within a half-month period p, we looked for its English trans-lation candidate words occurring in news docu-ments in the same period p.5.3 Translation candidatesThe context )(cC  of a Chinese word c was col-lected as follows: For each occurrence of c, weset a window of size 50 characters centered at c.We discarded all the Chinese words in the con-text that were not in the dictionary we used.
Thecontexts of all occurrences of a word c were thenconcatenated together to form )(cC .
The contextof an English translation candidate word e,)(eC , was similarly collected.
The window sizeof English context was 100 words.After all the counts were collected, we esti-mated ))(|)(( eCcCP  as described in Section 3,for each pair of Chinese source word and Englishtranslation candidate word.
For each Chinesesource word, we ranked all its English translationcandidate words according to the esti-mated ))(|)(( eCcCP .For each Chinese source word c  and an Eng-lish translation candidate word e , we also calcu-lated the probability )|( ceP  (as described inSection 4), which was used to rank the Englishcandidate words based on transliteration.Finally, the English candidate word with thesmallest average rank position and that appearswithin the top M positions of both ranked lists isthe chosen English translation (as described inSection 2).
If no words appear within the top Mpositions in both ranked lists, then no translationis output.Note that for many Chinese words, only oneEnglish word e  appeared within the top M posi-tions for both lists.
And among those cases wheremore than one English words appeared within thetop M positions for both lists, many were multi-ple translations of a Chinese word.
This hap-pened for example when a Chinese word was anon-English person name.
The name could havemultiple translations in English.
For example,????
was a Russian name.
Mirochina andMiroshina both appeared in top 10 positions ofboth lists.
Both were correct.5.4 EvaluationWe evaluated our method on each of the 12 half-month periods.
The results when we set M = 10are shown in Table 1.Period #c #e #o #Cor Prec.
(%)1 420 15505 7 5 71.42 419 15863 15 9 60.03 417 16434 25 21 84.04 382 17237 11 8 72.75 301 16106 8 5 62.56 295 15905 10 9 90.07 513 15315 13 8 61.58 465 17121 17 14 82.49 392 16075 13 11 84.610 361 15970 10 9 90.011 329 15924 9 8 88.912 205 15066 9 8 88.9Total 4499 192521 147 115 78.2Table 1.
Accuracy of our system in each period (M =10)In Table 1, period 1 is Jul 01 ?
Jul 15, period 2is Jul 16 ?
Jul 31, ?, period 12 is Dec 16 ?
Dec31.
#c is the total number of new Chinese sourcewords in the period.
#e is the total number ofEnglish translation candidates in the period.
#o isthe total number of output English translations.#Cor is the number of correct English transla-tions output.
Prec.
is the precision.
The correct-ness of the English translations was manuallychecked.Recall is somewhat difficult to estimate be-cause we do not know whether the English trans-lation of a Chinese word appears in the Englishpart of the corpus.
We attempted to estimate re-call by manually finding the English translationsfor all the Chinese source words for the two peri-ods Dec 01 ?
Dec 15 and Dec 16 ?
Dec 31 in theEnglish part of the corpus.
During the whole De-cember period, we only managed to find Englishtranslations which were present in the Englishside of the comparable corpora for 43 Chinesewords.
So we estimate that English translationsare present in the English part of the corpus for3624499)205329(43 =?+  words in all 12 pe-riods.
And our program finds correct translationsfor 115 words.
So we estimate that recall (for M= 10) is approximately %8.31362/115 = .We also investigated the effect of varying M .The results are shown in Table 2.M Number ofoutputPrecision(%)Recall(%)30 378 38.1 39.820 246 53.3 36.210 147 78.2 31.85 93 93.5 24.03 77 93.5 19.91 35 94.3 9.1Table 2.
Precision and recall for different values ofMThe past research of (Fung and Yee, 1998;Rapp, 1995; Rapp, 1999) utilized context infor-mation alone and was evaluated on different cor-pora from ours, so it is difficult to directlycompare our current results with theirs.
Simi-larly, Al-Onaizan and Knight (2002a; 2002b)only made use of transliteration informationalone and so was not directly comparable.To investigate the effect of the two individualsources of information (context and translitera-tion), we checked how many translations couldbe found using only one source of information(i.e., context alone or transliteration alone), onthose Chinese words that have translations in theEnglish part of the comparable corpus.
As men-tioned earlier, for the month of Dec 1995, thereare altogether 43 Chinese words that have theirtranslations in the English part of the corpus.This list of 43 words is shown in Table 3.
8 ofthe 43 words are translated to English multi-wordphrases (denoted as ?phrase?
in Table 3).
Sinceour method currently only considers unigramEnglish words, we are not able to find transla-tions for these words.
But it is not difficult toextend our method to handle this problem.
Wecan first use a named entity recognizer and nounphrase chunker to extract English names andnoun phrases.The translations of 6 of the 43 words arewords in the dictionary (denoted as ?comm.?
inTable 3) and 4 of the 43 words appear less than10 times in the English part of the corpus (de-noted as ?insuff?).
Our method is not able to findthese translations.
But this is due to search spacepruning.
If we are willing to spend more time onsearching, then in principle we can find thesetranslations.Chinese English Cont.rankTrans.rank??
Bork 1 1?????
Dabwali 1 1?????
?Khasbulatov 1 1???
Nazal 1 1????
Ousland 1 1???
Douala 1 2???
Erbakan 1 2???
Yilmaz 1 120???
Bazelya 1 NA??
crucible 1 NA???
Fatah 2 1????
Kardanov 2 1????
Mirochina 3 2????
Matteoli 4 2???
Tulkarm 8 7???
Preval 8 NA??
Soho 9 1????
Lamassoure 9 3????
Kaminski 10 1??
Muallem 19 52????
Cherkassky 46 2???
Erbakan 49 2???
Laitinen 317 2???
Courier 328 21??
leopard 1157 NA????
Naumov insuff???
Shangzhou insuff???
Voeller insuff????
Wassenaar insuff??
bald comm??
base comm???
Christmas comm??
decrease comm??
pension comm????
Saudi comm????-????Bosnia-Hercegovinaphrase???
ChristmasCardphrase???
exhibition hall phrase??
hatch egg phrase????
KawasakiSteel Co.phrase????
Mount San phraseJose???
Our Home BeRussiaphrase??
UnionElectionphraseTable 3.
Rank of correct translation for period Dec 01?
Dec 15 and Dec 16 ?
Dec 31.
?Cont.
rank?
is thecontext rank, ?Trans.
Rank?
is the transliteration rank.?NA?
means the word cannot be transliterated.
?insuff?means the correct translation appears less than 10times in the English part of the comparable corpus.?comm?
means the correct translation is a word ap-pearing in the dictionary we used or is a stop word.?phrase?
means the correct translation contains multi-ple English words.As shown in Table 3, using just context infor-mation alone, 10 Chinese words (the first 10)have their correct English translations at rank oneposition.
And using just transliteration informa-tion alone, 9 Chinese words have their correctEnglish translations at rank one position.On the other hand, using our method of com-bining both sources of information and setting M= ?, 19 Chinese words (i.e., the first 22 Chinesewords in Table 3 except ???,??,???
)have their correct English translations at rank oneposition.
If M = 10, 15 Chinese words (i.e., thefirst 19 Chinese words in Table 3 except???,???,??,???)
have their correctEnglish translations at rank one position.
Hence,our method of using both sources of informationoutperforms using either information sourcealone.6.
Related workAs pointed out earlier, most previous researchonly considers either transliteration or contextinformation in determining the translation of asource language word w, but not both sources ofinformation.
For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan andKnight, 2002b; Knight and Graehl, 1998) usedonly the pronunciation or spelling of w in transla-tion.
On the other hand, the work of (Cao and Li,2002; Fung and Yee, 1998; Rapp, 1995; Rapp,1999) used only the context of w to locate itstranslation in a second language.
In contrast, ourcurrent work attempts to combine both comple-mentary sources of information, yielding higheraccuracy than using either source of informationalone.Koehn and Knight (2002) attempted to com-bine multiple clues, including similar context andspelling.
But their similar spelling clue uses thelongest common subsequence ratio and worksonly for cognates (words with a very similarspelling).The work that is most similar to ours is the re-cent research of (Huang et al, 2004).
They at-tempted to improve named entity translation bycombining phonetic and semantic information.Their contextual semantic similarity model isdifferent from our language modeling approachto measuring context similarity.
It also made useof part-of-speech tag information, whereas ourmethod is simpler and does not require part-of-speech tagging.
They combined the two sourcesof information by weighting the two individualscores, whereas we made use of the average rankfor combination.7.
ConclusionIn this paper, we proposed a new method to minenew word translations from comparable corpora,by combining context and transliteration infor-mation, which are complementary sources ofinformation.
We evaluated our approach on sixmonths of Chinese and English Gigaword cor-pora, with encouraging results.AcknowledgementsWe thank Jia Li for implementing the EM algo-rithm to train transliteration probabilities.
Thisresearch is partially supported by a research grantR252-000-125-112 from National University ofSingapore Academic Research Fund.ReferencesY.
Al-Onaizan and K. Knight.
2002a.
Translatingnamed entities using monolingual and bilingual re-sources.
In Proc.
of ACL.Y.
Al-Onaizan and K. Knight.
2002b.
Machine trans-literation of names in Arabic text.
In Proc.
of theACL Workshop on Computational Approaches toSemitic Languages.Y.
Cao and H. Li.
2002.
Base noun phrase translationusing web data and the EM algorithm.
In Proc.
ofCOLING.P.
Fung and L. Y. Yee.
1998.
An IR approach fortranslating new words from nonparallel, comparabletexts.
In Proc.
of COLING-ACL.F.
Huang, S. Vogel and A. Waibel.
2004.
Improvingnamed entity translation combining phonetic andsemantic similarities.
In Proc.
of HLT-NAACL.K.
Knight and J. Graehl.
1998.
Machine translitera-tion.
Computational Linguistics, 24(4): 599-612.P.
Koehn and K. Knight.
2002.
Learning a translationlexicon from monolingual corpora.
In Proc.
of theACL Workshop on Unsupervised Lexical Acquisi-tion.I.
D. Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Proc.of EMNLP.R.
C. Moore.
2003.
Learning translations of named-entity phrases from parallel corpora.
In Proc.
ofEACL.H.
T. Ng and J. K. Low.
2004.
Chinese part-of-speechtagging: one-at-a-time or all-at-once?
word-basedor character-based?
To appear in Proc of EMNLP.K.
Ng.
2000.
A maximum likelihood ratio informationretrieval model.
In Proc.
of TREC-8.J.
M. Ponte and W. B. Croft.
1998.
A language mod-eling approach to information retrieval.
In Proc.
ofSIGIR.R.
Rapp.
1995.
Identifying word translations in non-parallel texts.
In Proc.
of ACL (student session).R.
Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and Germancorpora.
In Proc.
of ACL.
