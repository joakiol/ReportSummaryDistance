93Combining Utterance-Boundary and Predictability Approaches toSpeech SegmentationAris XANTHOSLinguistics Department, University of LausanneUNIL - BFSH21015 LausanneSwitzerlandaris.xanthos@ling.unil.chAbstractThis paper investigates two approaches tospeech segmentation based on different heuris-tics: the utterance-boundary strategy, and thepredictability strategy.
On the basis of for-mer empirical results as well as theoretical con-siderations, it is suggested that the utterance-boundary approach could be used as a prepro-cessing step in order to lighten the task of thepredictability approach, without damaging theresulting segmentation.
This intuition leads tothe formulation of an explicit model, which isempirically evaluated for a task of word segmen-tation on a child-oriented phonemically tran-scribed French corpus.
The results show thatthe hybrid algorithm outperforms its compo-nent parts while reducing the total memory loadinvolved.1 IntroductionThe design of speech segmentation1 methodshas been much studied ever since Harris?
sem-inal propositions (1955).
Research conductedsince the mid 1990?s by cognitive scientists(Brent and Cartwright, 1996; Saffran et al,1996) has established it as a paradigm of its ownin the field of computational models of languageacquisition.In this paper, we investigate two boundary-based approaches to speech segmentation.
Suchmethods ?attempt to identify individual word-boundaries in the input, without reference towords per se?
(Brent and Cartwright, 1996).The first approach we discuss relies on theutterance-boundary strategy, which consists inreusing the information provided by the occur-rence of specific phoneme sequences at utter-ance beginnings or endings in order to hypoth-1To avoid a latent ambiguity, it should be stated thatspeech segmentation refers here to a process taking as in-put a sequence of symbols (usually phonemes) and pro-ducing as output a sequence of higher-level units (usuallywords).esize boundaries inside utterances (Aslin et al,1996; Christiansen et al, 1998; Xanthos, 2004).The second approach is based on the predictabil-ity strategy, which assumes that speech shouldbe segmented at locations where some mea-sure of the uncertainty about the next symbol(phoneme or syllable for instance) is high (Har-ris, 1955; Gammon, 1969; Saffran et al, 1996;Hutchens and Adler, 1998; Xanthos, 2003).Our implementation of the utterance-boundary strategy is based on n-gramsstatistics.
It was previously found to performa ?safe?
word segmentation, that is with arather high precision, but also too conser-vative as witnessed by a not so high recall(Xanthos, 2004).
As regards the predictabilitystrategy, we have implemented an incrementalinterpretation of the classical successor count(Harris, 1955).
This approach also relies on theobservation of phoneme sequences, the lengthof which is however not restricted to a fixedvalue.
Consequently, the memory load involvedby the successor count algorithm is expectedto be higher than for the utterance-boundaryapproach, and its performance substantiallybetter.The experiments presented in this paper wereinspired by the intuition that both algorithmscould be combined in order to make the mostof their respective strengths.
The utterance-boundary typicality could be used as a compu-tationally inexpensive preprocessing step, find-ing some true boundaries without inducing toomany false alarms; then, the heavier machineryof the successor count would be used to accu-rately detect more boundaries, its burden be-ing lessened as it would process the chunks pro-duced by the first algorithm rather than wholeutterances.
We will show the results obtainedfor a word segmentation task on a phoneticallytranscribed and child-oriented French corpus,focusing on the effect of the preprocessing stepon precision and recall, as well as its impact on94memory load and processing time.The next section is devoted to the formal def-inition of both algorithms.
Section 3 discussessome issues related to the space and time com-plexity they involve.
The experimental setupas well as the results of the simulations are de-scribed in section 4, and in conclusion we willsummarize our findings and suggest directionsfor further research.2 Description of the algorithms2.1 Segmentation by thresholdingMany distributional segmentation algorithmsdescribed in the literature can be seen as in-stances of the following abstract procedure(Harris, 1955; Gammon, 1969; Saffran et al,1996; Hutchens and Adler, 1998; Bavaud andXanthos, 2002).
Let S be the set of phonemes(or segments) in a given language.
In the mostgeneral case, the input of the algorithm is anutterance of length l, that is a sequence of lphonemes u := s1 .
.
.
sl (where si denotes thei-th phoneme of u).
Then, for 1 ?
i ?
l ?
1, weinsert a boundary after si iff D(u, i) > T (u, i),where the values of the decision variable D(u, i)and of the threshold T (u, i) may depend on boththe whole sequence and the actual position ex-amined (Xanthos, 2003).The output of such algorithms can be evalu-ated in reference to the segmentation performedby a human expert, using traditional measuresfrom the signal detection framework.
It is usualto give evaluations both for word and boundarydetection (Batchelder, 2002).
The word preci-sion is the probability for a word isolated bythe segmentation procedure to be present inthe reference segmentation, and the word recallis the probability for a word occurring in thetrue segmentation to be correctly isolated.
Sim-ilarly, the segmentation precision is the proba-bility that an inferred boundary actually occursin the true segmentation, and the segmentationrecall is the probability for a true boundary tobe detected.In the remaining of this section, we will usethis framework to show how the two algorithmswe investigate rely on different definitions ofD(u, i) and T (u, i).2.2 Frequency estimatesLet U ?
S?
be the set of possible utterances inthe language under examination.
Suppose weare given a corpus C ?
UT made of T successiveutterances.The absolute frequency of an n-gram w ?
Snin the corpus is given by n(w) := ?Tt=1 nt(w)where nt(w) denotes the absolute frequency of win the t-th utterance of C. In the same way, wedefine the absolute frequency of w in utterance-initial position as n(w|I) := ?Tt=1 nt(w|I) wherent(w|I) denotes the absolute frequency of w inutterance-initial position in the t-th utteranceof C (which is 1 iff the utterance begins withw and 0 otherwise).
Similarly, the absolute fre-quency of w in utterance-final position is givenby n(w|F) := ?Tt=1 nt(w|F).Accordingly, the relative frequency of w ob-tains as f(w) := n(w)/ ?w?
?Sn n(w?).
Itsrelative frequencies in utterance-initial and-final position respectively are given byf(w|I) := n(w|I)/ ?w?
?Sn n(w?|I) and f(w|F) :=n(w|F)/ ?w?
?Sn n(w?|F) 2.Both algorithms described below process theinput incrementally, one utterance after an-other.
This implies that the frequency measuresdefined in this section are in fact evolving allalong the processing of the corpus.
In general,for a given input utterance, we chose to updaten-gram frequencies first (over the whole utter-ance) before performing the segmentation.2.3 Utterance-boundary typicalityWe use the same implementation of theutterance-boundary strategy that is describedin more details by Xanthos (2004).
Intuitively,the idea is to segment utterances where se-quences occur, which are typical of utteranceboundaries.
Of course, this implies that the cor-pus is segmented in utterances, which seems areasonable assumption as far as language acqui-sition is concerned.
In this sense, the utterance-boundary strategy may be viewed as a kind oflearning by generalization.Probability theory provides us with astraightforward way of evaluating how much ann-gram w ?
Sn is typical of utterance end-ings.
Namely, we know that events ?occur-rence of n-gram w?
and ?occurrence of an n-gram in utterance-final position?
are indepen-dent iff p(w ?
F) = p(w)p(F) or equivalentlyiff p(w|F) = p(w).
Thus, using maximum-likelihood estimates, we may define the typical-2Note that in general,?w?
?Sn n(w?|F) =?w?
?Sn n(w?|I) = T?
, where T?
?
T is the numberof utterances in C that have a length greater than orequal to n.95ity of w in utterance-final position as:t(w|F) := f(w|F)f(w) (1)This measure is higher than 1 iff w is more likelyto occur in utterance-final position (than in anyposition), lower iff it is less likely to occur there,and equal to 1 iff its probability is independentof its position.In the context of a segmentation procedure,this suggest a ?natural?
constant thresholdT (u, i) := 1 (which can optionally be fine-tunedin order to obtain a more or less conservativeresult).
Regarding the decision variable, if wewere dealing with an utterance u of infinitelength, we could simply set the order r ?
1of the typicality computation and define d(u, i)as t(si?
(r?1) .
.
.
si|F) (where si denotes the i-th phoneme of u).
Since the algorithm is morelikely to process an utterance of finite lengthl, there is a problem when considering a po-tential boundary close to the beginning of theutterance, in particular when r > i.
In thiscase, we can compute the typicality of smallersequences, thus defining the decision variable ast(si?(r?
?1) .
.
.
si|F), where r?
:= min(r, i).As was already suggested by Harris (1955),our implementation actually combines the typ-icality in utterance-final position with its ana-logue in utterance-initial position.
This is doneby taking the average of both statistics, andwe have found empirically efficient to weight itby the relative lengths of the conditioning se-quences:D(u, i) := r?r?
+ r??
t(w|F ) +r??r?
+ r??
t(w?|I) (2)where w := si?(r?
?1) .
.
.
si ?
S r?, w?
:=si+1 .
.
.
si+r??
?
S r??
, r?
:= min(r, i) and r??
:=min(r, l ?
i).
This definition helps compensatefor the asymmetry of arguments when i is eitherclose to 1 or close to l.Finally, in the simulations below, we ap-ply a mechanism that consists in incrementingn(w|F) and n(w?|I) (by one) whenever D(u, i) >T (u, i).
The aim of this is to enable the dis-covery of new utterance-boundary typical se-quences.
It was found to considerably raise therecall as more utterances are processed, at thecost of a slight reduction in precision (Xanthos,2004).2.4 Successor countThe second algorithm we investigate in this pa-per is an implementation of Harris?
successorcount (Harris, 1955), the historical source ofall predictability-based approaches to segmen-tation.
It relies on the assumption that in gen-eral, the diversity of possible phonemes tran-sitions is high after a word boundary and de-creases as we consider transitions occurring fur-ther inside a word.The diversity of transitions following an n-gram w ?
Sn is evaluated by the successorcount (or successor variety), simply defined asthe number of different phonemes that can oc-cur after it:succ(w) := |{s ?
S|n(ws) > 0}| (3)Transposing the indications of Harris in theterms of section 2.1, for an utterance u :=s1 .
.
.
sl, we define D(u, i) as succ(w) wherew := s1 .
.
.
si, and T (u, i) as max[D(u, i ?1), D(u, i + 1)].
Here again a ?backward?
mea-sure can be defined, the predecessor count:predec(w) := |{s ?
S|n(sw) > 0}| (4)Accordingly, we have D?
(u, i) = predec(w?
)where w?
:= si+1 .
.
.
sl, and T ?
(u, i) :=max[D?
(u, i?
1), D?
(u, i + 1)].
In order to com-bine both statistics, we have found efficient touse a composite decision rule, where a boundaryis inserted after phoneme si iff D(u, i) > T (u, i)or D?
(u, i) > T ?
(u, i).These decision variables differ from thoseused in the utterance-boundary approach inthat there is no fixed bound on the length oftheir arguments.
As will be discussed in sec-tion 3, this has important consequences for thecomplexity of the algorithm.
Also, the thresh-old used for the successor count depends ex-plicitely on both u and i: rather than seek-ing values higher than a given threshold, thismethod looks for peaks of the decision variablemonitored over the input, whether the actualvalue is high or not.
This is a more or less ar-bitrary feature of this class of algorithms, andmuch work remains to be done in order to pro-vide theoretical justifications rather than mereempirical evaluations.3 Complexity issuesIt is not easy to evaluate the complexity of thealgorithms discussed in this paper, which con-sist mainly in the space and time needed to store96and retrieve the necessary information for thecomputation of n-grams frequencies.
Of course,this depends much on the actual implementa-tion.
For instance, in a rather naive approach,utterances can be stored as such and the mem-ory load is then roughly equivalent to the size ofthe corpus, but computing the frequency of ann-gram requires scanning the whole memory.A first optimization is to count utterancesrather than merely store them.
Some program-ming languages have a very convenient and effi-cient built-in data structure for storing elementsindexed by a string3, such as the frequency as-sociated with an utterance.
However, the actualgain depends on the redundancy of the corpus atutterances level, and even in an acquisition cor-pus, many utterances occur only once.
The timeneeded to compute the frequency of an n-gramis reduced accordingly, and due to the averageefficiency of hash coding, the time involved bythe storage of an utterance is approximately aslow as in the naive case above.It is possible to store not only the frequencyof utterances, but also that of their subparts.
Inthis approach, storing an n-gram and retrievingits frequency need comparable time resources,expected to be low if hashing is performed.
Ofcourse, from the point of view of memory load,this is much more expensive than the two pre-vious implementations discussed.
However, wecan take advantage of the fact that in an utter-ance of length l, every n-gram w with 1 ?
n < lis the prefix and/or suffix of at least an n + 1-gram w?.
Thus, it is much more compact tostore them in a directed tree, the root of whichis the empty string, and where each node corre-sponds to a phoneme in a given context4, andeach child of a node to a possible successor ofthat phoneme in its context.
The frequency ofan n-gram can be stored in a special child of thenode representing the terminal phoneme of then-gram.This implementation (tree storage) will beused in the simulations described below.
It isnot claimed to be more psychologically plausiblethan another, but we believe the size in nodesof the trees built for a given corpus providesan intuitive and accurate way of comparing thememory requirements of the algorithms we dis-cuss.
From the point of view of time complexity,however, the tree structure is less optimal thana flat hash table since the time needed for the3This type of storage is known as hash coding.4defined by the whole sequence of its parent nodesstorage or retrieval of an n-gram grows linearlywith n.4 Empirical evaluation4.1 Experimental setupBoth algorithms described above were imple-mented implemented in Perl5 and evaluatedusing a phonemically transcribed and child-oriented French corpus (Kilani-Schoch corpus6).We have extracted from the original corpusall the utterances of Sophie?s parents (mainlyher mother) between ages 1;6.14 and 2;6.25(year;month.day).
These were transcribedphonemically in a semi-automatic fashion, usingthe BRULEX database (Content et al, 1990)and making the result closer to oral Frenchwith a few hand-crafted rules.
Eventually thefirst 10?000 utterances were used for simula-tions.
This corresponds to 37?663 words (992types) and 103?325 phonemes (39 types).In general, we will compare the results ob-served for the successor count used on its own(?SC alone?, on the figures) with those obtainedwhen the utterance-boundary typicality is usedfor preprocessing7.
The latter were recorded for1 ?
r ?
5, where r is the order for the com-putation of typicalities.
The threshold value fortypicality was set to 1 (see section 2.3).
Theresults of the algorithms for word segmenta-tion were evaluated by comparing their outputto the segmentation given in the original tran-scription using precision and recall for word andboundary detection (computed over the wholecorpus).
The memory load is measured by thenumber of nodes in the trees built by each al-gorithm, and the processing time is the numberof seconds needed to process the whole corpus.4.2 Segmentation performanceWhen used in isolation, our implementation ofthe successor count has a segmentation preci-sion as high as 82.5%, with a recall of 50.5%;the word precision and recall are 57% and 40.8%5Perl was chosen here because of the ease it provideswhen it comes to textual statistics; however, execution isnotoriously slower than with C or C++, and this shouldbe kept in mind when interpreting the large differencesin processing time reported in section 4.4.6Sophie, a French speaking Swiss child, was recordedat home by her mother every ten days in situations ofplay (Kilani-Schoch and Dressler, 2001).
The transcrip-tion and coding were done according to CHILDES con-ventions (MacWhinney, 2000).7Results of the utterance-boundary approach aloneare given in (Xanthos, 2004)97Figure 1: Segmentation precision and recall ob-tained with the successor count alone and withutterance-boundary preprocessing on n-grams.respectively.
For comparison, the highest seg-mentation precision obtained with utterance-boundary typicality alone is 80.8% (for r = 5),but the corresponding recall does not exceed37.6%, and the highest word precision is 44.4%(r = 4) with a word recall of 31.4%.
As ex-pected, the successor count performs much bet-ter than the utterance boundary typicality inisolation.Using utterance-boundary typicality as a pre-processing step has a remarkable impact on theperformance of the resulting algorithm.
Figure1 shows the segmentation performance obtainedfor boundary detection with the successor countalone or in combination with preprocessing (for1 ?
r ?
5).
The segmentation precision is al-ways lower with preprocessing, but the differ-ence dwindles as r grows: for r = 5, it reaches79.9%, so only 2.1% are lost.
On the contrary,the segmentation recall is always higher withpreprocessing.
It reaches a peak of 79.3% forr = 3, and stays as high as 71.2% for r = 5 ,meaning a 20.7% difference with the successorcount alone.Concerning the detection of whole words, (fig-ure 2), the word precision is strictly increasingwith r and ranges between 15.2% and 60.2%,the latter being a 3.2% increase with regardto the successor count alone.
The word recallis lower when preprocessing is performed withn = 1 (-18.2%), but higher in all other cases,with a peak of 56% for n = 4 (+15.2%).Overall, we can say the segmentation perfor-Figure 2: Word precision and recall ob-tained with the successor count alone and withutterance-boundary preprocessing on n-grams.mance exhibited by our hybrid algorithm con-firms our expectations regarding the comple-mentarity of the two strategies examined: theircombination is clearly superior to each of themtaken independently.
There may be a slight lossin precision, but it is massively counterbalancedby the gain in recall.4.3 Memory loadThe second hypothesis we made was that thepreprocessing step would reduce the memoryload of the successor count algorithm.
In ourimplementation, the space used by each algo-rithm can be measured by the number of nodesof the trees storing the distributions.
Five dis-tinct trees are involved: three for the utterance-boundary approach (one for the distribution ofn-grams in general and two for their distribu-tions in utterance-initial and -final position),and two for the predictability approach (onefor successors and one for predecessors).
Thememory load of each algorithm is obtained bysummation of these values.As can be seen on figure 3, the size of the treesbuilt by the successor count is drastically re-duced by preprocessing.
Successor count aloneuses as many as 99?405 nodes; after preprocess-ing, the figures range between 7?965 for n = 1and 38?786 for n = 5 (SC, on the figure)8.
How-ever, the additional space used by the n-grams8These values are highly negatively correlated withthe number of boundaries?true or false?inserted by pre-processing (r = ?0.96).98Figure 3: Memory load (in thousands of nodes)measured with the successor count alone andwith utterance-boundary preprocessing on n-grams (see text).distributions needed to compute the utterance-boundary typicality (UBT) grows quickly withn, and the total number of nodes even exceedsthat of the successor count alone when n = 5.Still, for lower values of n, preprocessing leadsto a substantial reduction in total memory load.4.4 Processing timeIt seems unlikely that the combination of thetwo algorithms does not exhibit any drawback.We have said in section 3 that storing distribu-tions in a tree was not optimal from the pointof view of time complexity, so we did not havehigh expectations on this topic.
Nevertheless,we recorded the time used by the algorithmsfor the sake of completeness.
CPU time9 wasmeasured in seconds, using built-in functions ofPerl, and the durations we report were averagedover 10 runs of the simulation10.What can be seen on figure 4 is that althoughthe time used by the successor count computa-tion is slightly reduced by preprocessing, thisdoes not compensate for the additional time re-quired by the preprocessing itself.
On average,the total time is multiplied by 1.6 when pre-processing is performed.
Again, this is really aconsequence of the chosen implementation, asthis factor could be reduced to 1.15 by storing9on a pentium III 700MHz10This does not give a very accurate evaluation of pro-cessing time, and we plan to express it in terms of num-ber of computational steps.Figure 4: Processing time (in seconds) mea-sured with the successor count alone and withutterance-boundary preprocessing on n-grams.distributions in flat hash tables rather than treestructures.5 Conclusions and discussionIn this paper, we have investigated two ap-proaches to speech segmentation based on dif-ferent heuristics: the utterance-boundary strat-egy, and the predictability strategy.
On the ba-sis of former empirical results as well as theoret-ical considerations regarding their performanceand complexity, we have suggested that theutterance-boundary approach could be used asa preprocessing step in order to lighten the taskof the predictability approach without damag-ing the segmentation.This intuition was translated into an explicitmodel, then implemented and evaluated for atask of word segmentation on a child-orientedphonetically transcribed french corpus.
Our re-sults show that:?
the combined algorithm outperforms itscomponent parts considered separately;?
the total memory load of the combined al-gorithm can be substantially reduced bythe preprocessing step;?
however, the processing time of the com-bined algorithm is generally longer andpossibly much longer depending on the im-plementation.These findings are in line with recent researchadvocating the integration of various strate-gies for speech segmentation.
In his work on99            Figure 5: Average successor count for n-grams(based on the corpus described in section 4.1).computational morphology, Goldsmith (2001)uses Harris?
successor count as a means to re-duce the search space of a more powerful al-gorithm based on minimum description length(Marcken, 1996).
We go one step further andshow that an utterance-boundary heuristic canbe used in order to reduce the complexity of thesuccessor count algorithm11.Besides complexity issues, there is a prob-lem of data sparseness with the successor count,as it decreases very quickly while the size n ofthe context grows.
In the case of our quite re-dundant child-oriented corpus, the (weighted)average of the successor count12 for a randomn-gram ?w?Sn f(w) succ(w) gets lower than 1for n ?
9 (see figure 5).
This means that inmost utterances, no more boundary can be in-serted after the first 9 phonemes (respectivelybefore the last 9 phonemes) unless we get closeenough to the other extremity of the utter-ance for the predecessor (respectively successor)count to operate.
As regards the utterance-boundary typicality, on the other hand, the po-sition in the utterance makes no difference.
Asa consequence, many examples can be found inour corpus, where the middle part of a long ut-terance would be undersegmented by the succes-sor count alone, whereas preprocessing providesit with more tractable chunks.
This is illus-trated by the following segmentations of the ut-terance   	 (Daddy doesn?t11at least as regards memory load, which could morerestrictive in a developmental perspective12The predecessor count behaves much the same.like carrots), where vertical bars denote bound-aries predicted by the utterance-boundary typ-icality (for r = 5), and dashes represent bound-aries inferred by the successor count:SC    fffiflffi !ff"UBT (r = 5)   #fl%$  !&$ UBT + SC    fffi'ff"ffi$ (ff) !&$ This suggests that the utterance-boundarystrategy could be more than an additional de-vice that safely predicts some boundaries thatthe successor count alone might have found ornot: it could actually have a functional rela-tionship with it.
If the predictability strategyhas some relevance for speech segmentation inearly infancy (Saffran et al, 1996), then it maybe necessary to counterbalance the data sparse-ness; this is what these authors implicitely doby using first-order transition probabilities, andit would be easy to define an n-th order succes-sor count in the same way.
Yet another possi-bility would be to ?reset?
the successor countafter each boundary inserted.
Further researchshould bring computational and psychologicalevidence for or against such ways to address rep-resentativity issues.We conclude this paper by raising an issuethat was already discussed by Gammon (1969),and might well be tackled with our methodol-ogy.
It seems that various segmentation strate-gies correlate more or less with different segmen-tation levels.
We wonder if these different kindsof sensitivity could be used to make inferencesabout the hierarchical structure of utterances.6 AcknowledgementsThe author is grateful to Marianne Kilani-Schoch and the mother of Sophie for providingthe acquisition corpus (see p.4), as well as toFranc?ois Bavaud, Marianne Kilani-Schoch andtwo anonymous reviewers for useful commentson earlier versions of this paper.ReferencesR.N.
Aslin, J.Z.
Woodward, N.P.
Lamendola,and T.G.
Bever.
1996.
Models of word seg-mentation in fluent maternal speech to in-fants.
In J.L Morgan and Demuth K., ed-itors, Signal to Syntax: Bootstrapping fromSpeech to Grammar in Early Language Ac-quisition, pages 117?134.
Lawrence ErlbaumAssociates, Mahwah (NJ).E.
Batchelder.
2002.
Bootstrapping the lexi-con: A computational model of infant speechsegmentation.
Cognition, 83:167?206.100F.
Bavaud and A. Xanthos.
2002.
Thermody-namique et statistique textuelle: concepts etillustrations.
In Actes des 6e` Journe?es Inter-nationales d?Analyse Statistique des Donne?esTextuelles (JADT 2002), pages 101?111.M.R.
Brent and T.A.
Cartwright.
1996.
Distri-butional regularity and phonotactics are use-ful for segmentation.
Cognition, 61:93?125.M.H.
Christiansen, J. Allen, and M. Seidenberg.1998.
Learning to segment speech using mul-tiple cues.
Language and Cognitive Processes,13:221?268.A.
Content, P. Mousty, and M. Radeau.
1990.Brulex: Une base de donne?es lexicales in-formatise?e pour le franc?ais e?crit et parle?.L?Anne?e Psychologique, 90:551?566.E.
Gammon.
1969.
Quantitative approxima-tions to the word.
In Papers presented to theInternational Conference on ComputationalLinguistics COLING-69.J.
Goldsmith.
2001.
Unsupervised learning ofthe morphology of a natural language.
Com-putational Linguistics, 27 (2):153?198.Z.S.
Harris.
1955.
From phoneme to morpheme.Language, 31:190?222.J.L.
Hutchens and M.D.
Adler.
1998.
Find-ing structure via compression.
In Proceedingsof the International Conference on Computa-tional Natural Language Learning, pages 79?82.M.
Kilani-Schoch and W.U.
Dressler.
2001.Filler + infinitive and pre- and protomorphol-ogy demarcation in a french acquisition cor-pus.
Journal of Psycholinguistic Research, 30(6):653?685.B.
MacWhinney.
2000.
The CHILDES Project:Tools for Analyzing Talk.
Third Edition.Lawrence Erlbaum Associates, Mahwah (NJ).C.G.
de Marcken.
1996.
Unsupervised LanguageAcquisition.
Phd dissertation, MassachusettsInstitute of Technology.J.R.
Saffran, E.L. Newport, and R.N.
Aslin.1996.
Word segmentation: The role of distri-butional cues.
Journal of Memory and Lan-guage, 35:606?621.A.
Xanthos.
2003.
Du k-gramme au mot: vari-ation sur un the`me distributionnaliste.
Bul-letin de linguistique et des sciences du langage(BIL), 21.A.
Xanthos.
2004.
An incremental implemen-tation of the utterance-boundary approach tospeech segmentation.
To appear in the Pro-ceedings of Computational Linguistics in theNetherlands 2003 (CLIN 2003).
