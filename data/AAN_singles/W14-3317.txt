Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 157?162,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsThe RWTH Aachen German-English Machine Translation System forWMT 2014Stephan Peitz, Joern Wuebker, Markus Freitag and Hermann NeyHuman Language Technology and Pattern Recognition GroupComputer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany<surname>@cs.rwth-aachen.deAbstractThis paper describes the statistical ma-chine translation (SMT) systems devel-oped at RWTH Aachen University for theGerman?English translation task of theACL 2014 Eighth Workshop on Statisti-cal Machine Translation (WMT 2014).Both hierarchical and phrase-based SMTsystems are applied employing hierarchi-cal phrase reordering and word class lan-guage models.
For the phrase-based sys-tem, we run discriminative phrase training.In addition, we describe our preprocessingpipeline for German?English.1 IntroductionFor the WMT 2014 shared translation task1RWTH utilized state-of-the-art phrase-based andhierarchical translation systems.
First, we describeour preprocessing pipeline for the language pairGerman?English in Section 2.
Furthermore, weutilize morpho-syntactic analysis to preprocess thedata (Section 2.3).
In Section 3, we give a surveyof the employed systems and the basic methodsthey implement.
More details are given about thediscriminative phrase training (Section 3.4) andthe hierarchical reordering model for hierarchicalmachine translation (Section 3.5).
Experimentalresults are discussed in Section 4.2 PreprocessingIn this section we will describe the modification ofour preprocessing pipeline compared to our 2013WMT German?English setup.2.1 CategorizationWe put some effort in building better categories fordigits and written numbers.
All written numbers1http://www.statmt.org/wmt14/translation-task.htmlwere categorized.
In 2013 they were just handledas normal words which leads to a higher number ofout-of-vocabulary words.
For German?English,in most cases for numbers like ?3,000?
or ?2.34?the decimal mark ?,?
and the thousands separator?.?
has to be inverted.
As the training data and alsothe test sets contain several errors for numbers inthe source as well as in the target part, we put moreeffort into producing correct English numbers.2.2 Remove Foreign LanguagesThe WMT German?English corpus containssome bilingual sentence pairs with non-Germansource or/and non-English target sentences.
Forthis WMT translation task, we filtered all non-matching language pairs (in terms of source lan-guage German and target language English) fromour bilingual training set.First, we filtered languages which contain non-ascii characters.
For example Chinese, Arabic orRussian can be easily filtered when deleting sen-tences which contain more than 70 percent non-ascii words.
The first examples of Table 1 wasfiltered due to the fact, that the source sentencecontains too many non-ascii characters.In a second step, we filtered European lan-guages containing ascii characters.
We used theWMT monolingual corpora in Czech, French,Spanish, English and German to filter these lan-guages from our bilingual data.
We could bothdelete a sentence pair if it contains a wrong sourcelanguage or a wrong target language.
That is thereason why we even search for English sentencesin the source part and for German sentences inthe target part.
For each language, we built aword count of all words in the monolingual datafor each language separately.
We removed punc-tuation which are no indicator of a language.
Inour experiments, we only considered words withfrequency higher than 20 (e.g.
to ignore names).Given the word frequency, we removed a bilingual157Table 1: Examples of sentences removed in preprocessing.Exampleremove non-ascii symbols ?????????
.zum Bericht A?noveros Tr?
?as de Besremove wrong languages from target Honni soit qui mal y pense !as you yourself have said : travailler plus pour gagner plusremove wrong languages from source je d?eclare interrompue la session du Parlement europ?een .Quelle der Tabelle : ?
what Does the European Union do ?
?sentence pair from our training data if more than70 percent of the words had a higher count in adifferent language then the one we expected.
InTable 1 some example sentences, which were re-moved, are illustrated.In Table 2 the amount of sentences and the cor-responding vocabulary sizes of partial and totallycleaned data sets are given.
Further we provide thenumber of out-of-vocabulary words (OOVs) fornewstest2012.
The vocabulary size could be re-duced by ?130k words for both source and targetside of our bilingual training data while the OOVrate kept the same.
Our experiments showed, thatthe translation quality is the same with or with-out removing wrong sentences.
Nevertheless, wereduced the training data size and also the vocabu-lary size without any degradation in terms of trans-lation quality.2.3 Morpho-syntactic AnalysisIn order to reduce the source vocabulary size forthe German?English translation further, the Ger-man text is preprocessed by splitting German com-pound words with the frequency-based method de-scribed in (Koehn and Knight, 2003).
To reducetranslation complexity, we employ the long-rangepart-of-speech based reordering rules proposed byPopovi?c and Ney (2006).3 Translation SystemsIn this evaluation, we employ phrase-based trans-lation and hierarchical phrase-based translation.Both approaches are implemented in Jane (Vilar etal., 2012; Wuebker et al., 2012), a statistical ma-chine translation toolkit which has been developedat RWTH Aachen University and is freely avail-able for non-commercial use.2In the newest inter-nal version, we use the KenLM Language ModelInterface provided by (Heafield, 2011) for both de-coders.2http://www.hltpr.rwth-aachen.de/jane/3.1 Phrase-based SystemIn the phrase-based decoder (source cardinalitysynchronous search, SCSS, Wuebker et al.
(2012)),we use the standard set of models with phrasetranslation probabilities and lexical smoothing inboth directions, word and phrase penalty, distance-based distortion model, an n-gram target languagemodel and three binary count features.
Additionalmodels used in this evaluation are the hierarchicalreordering model (HRM) (Galley and Manning,2008) and a word class language model (wcLM)(Wuebker et al., 2013).
The parameter weightsare optimized with minimum error rate training(MERT) (Och, 2003).
The optimization criterionis BLEU (Papineni et al., 2002).3.2 Hierarchical Phrase-based SystemIn hierarchical phrase-based translation (Chiang,2007), a weighted synchronous context-free gram-mar is induced from parallel text.
In addition tocontiguous lexical phrases, hierarchical phraseswith up to two gaps are extracted.
The search iscarried out with a parsing-based procedure.
Thestandard models integrated into our Jane hierar-chical systems (Vilar et al., 2010; Huck et al.,2012) are: Phrase translation probabilities and lex-ical smoothing probabilities in both translation di-rections, word and phrase penalty, binary featuresmarking hierarchical phrases, glue rule, and ruleswith non-terminals at the boundaries, three binarycount features, and an n-gram language model.We utilize the cube pruning algorithm for decod-ing (Huck et al., 2013a) and optimize the modelweights with MERT.
The optimization criterion isBLEU.3.3 Other Tools and TechniquesWe employ GIZA++(Och and Ney, 2003) to trainword alignments.
The two trained alignmentsare heuristically merged to obtain a symmetrizedword alignment for phrase extraction.
All lan-158Table 2: Corpus statistics after each filtering step and compound splitting.Vocabulary OOVsSentences German English newstest2012Preprocessing 2013 4.19M 1.43M 784K 1019Preprocessing 2014 4.19M 1.42M 773K 1018+ remove non-ascii symbols 4.17M 1.36M 713K 1021+ remove wrong languages from target 4.15M 1.34M 675K 1027+ remove wrong languages from source 4.08M 1.30M 655K 1039+ compound splitting 4.08M 652K 655K 441guage models (LMs) are created with the SRILMtoolkit (Stolcke, 2002) or with the KenLM lan-guage model toolkit (Heafield et al., 2013) and arestandard 4-gram LMs with interpolated modifiedKneser-Ney smoothing (Kneser and Ney, 1995;Chen and Goodman, 1998).
We evaluate in true-case with BLEU and TER (Snover et al., 2006).3.4 Discriminative Phrase TrainingIn our baseline translation systems the phrase ta-bles are created by a heuristic extraction fromword alignments and the probabilities are esti-mated as relative frequencies, which is still thestate-of-the-art for many standard SMT systems.Here, we applied a more sophisticated discrimi-native phrase training method for the WMT 2014German?English task.
Similar to (He and Deng,2012), a gradient-based method is used to opti-mize a maximum expected BLEU objective, forwhich we define BLEU on the sentence level withsmoothed 3-gram and 4-gram precisions.
To thatend, the training data is decoded to generate 100-best lists.
We apply a leave-one-out heuristic(Wuebker et al., 2010) to make better use of thetraining data.
Using these n-best lists, we itera-tively perform updates on the phrasal translationscores of the phrase table.
After each iteration,we run MERT, evaluate on the development setand select the best performing iteration.
In thiswork, we perform two rounds of discriminativetraining on two separate data sets.
In the firstround, training is performed on the concatenationof newstest2008 through newstest2010 and an au-tomatic selection from the News-commentary, Eu-roparl and Common Crawl corpora.
The selec-tion is based on cross-entropy difference of lan-guage models and IBM-1 models as described byMansour et al.
(2011) and contains 258K sentencepairs.
The training took 4.5 hours for 30 iterations.On top of the final phrase-based systems, a secondround of discriminative training is run on the fullnews-commentary corpus concatenated with new-stest2008 through newstest2010.3.5 A Phrase Orientation Model forHierarchical Machine TranslationIn Huck et al.
(2013b) a lexicalized reorder-ing model for hierarchical phrase-based machinetranslation was introduced.
The model scoresmonotone, swap, and discontinuous phrase ori-entations in the manner of the one presented by(Tillmann, 2004).
Since improvements were re-ported on a Chinese?English translation task, weinvestigate the impact of this model on a Europeanlanguage pair.
As in German the word order ismore flexible compared with the target languageEnglish, we expect that an additional reorderingmodel could improve the translation quality.
Inour experiments we use the same settings whichworked best in (Huck et al., 2013b).4 SetupWe trained the phrase-based and the hierarchicaltranslation system on all available bilingual train-ing data.
Corpus statistics can be found in thelast row of Table 2.
The language model are4-grams trained on the respective target side ofthe bilingual data,12of the Shuffled News Crawlcorpus,14of the 109French-English corpus and12of the LDC Gigaword Fifth Edition corpus.The monolingual data selection is based on cross-entropy difference as described in (Moore andLewis, 2010).
For the baseline language model,we trained separate models for each corpus, whichwere then interpolated.
For our final experiments,we also trained a single unpruned language modelon the concatenation of all monolingual data withKenLM.159Table 3: Results (truecase) for the German?English translation task.
BLEU and TER are given inpercentage.
All HPBT setups are tuned on the concatenation of newstest2012 and newstest2013.
Thevery first SCSS setups are optimized on newstest2012 only.newstest2011 newstest2012 newstest2013BLEU TER BLEU TER BLEU TERSCSS +HRM 22.4 60.1 23.7 59.0 25.9 55.7+wcLM 22.8 59.6 24.0 58.6 26.3 55.4+1st round discr.
23.0 59.5 24.2 58.2 26.8 55.1+tune11+12.
23.4 59.5 24.2 58.6 26.8 55.2+unprunedLM 23.6 59.5 24.2 58.6 27.1 55.0+2nd round discr.
23.7 59.5 24.4 58.5 27.2 55.0HPBT baseline 23.3 59.9 24.2 58.9 26.7 55.6+wcLM 23.4 59.8 24.1 58.9 26.8 55.6+HRM 23.3 60.0 24.2 58.9 26.9 55.5+HRM +wcLM 23.3 59.9 24.1 59.1 26.7 55.94.1 Experimental ResultsThe results of the phrase-based system (SCSS)as well as the hierarchical phrase-based system(HPBT) are summarized in Table 3.The phrase-based baseline system, which in-cludes the hierarchical reordering model by (Gal-ley and Manning, 2008) and is tuned on new-stest2012, reaches a performance of 25.9% BLEUon newstest2013.
Adding the word class languagemodel improves performance by 0.4% BLEU ab-solute and the first round of discriminative phrasetraining by 0.5% BLEU absolute.
Next, weswitched to tuning on a concatenation of new-stest2011 and newstest2012, which we expect tobe more reliable with respect to unseen data.
Al-though the BLEU score does not improve and TERgoes up slightly, we kept this tuning set in the sub-sequent setups, as it yielded longer translations,which in our experience will usually be preferredby human evaluators.
Switching from the inter-polated language model to the unpruned languagemodel trained with KenLM on the full concate-nated monolingual training data in a single passgained us another 0.3% BLEU.
For the final sys-tem, we ran a second round of discriminative train-ing on different training data (cf.
Section 3.4),which increased performance by 0.1% BLEU tothe final score 27.2.For the phrase-based system, we also exper-imented with weighted phrase extraction (Man-sour and Ney, 2012), but did not observe improve-ments.The hierarchical phrase-based baseline withoutany additional model is on the same level as thephrase-based system including the word class lan-guage model, hierarchical reordering model anddiscriminative phrase training in terms of BLEU.However, extending the system with a word classlanguage model or the additional reordering mod-els does not seem to help.
Even the combinationof both models does not improve the translationquality.
Note, that the hierarchical system wastuned on the concatenation newstest2011 and new-stest2012.
The final system employs both wordclass language model and hierarchical reorderingmodel.Both phrase-based and hierarchical phrase-based final systems are used in the EU-Bridge sys-tem combination (Freitag et al., 2014).5 ConclusionFor the participation in the WMT 2014 sharedtranslation task, RWTH experimented with bothphrase-based and hierarchical translation systems.For both approaches, we applied a hierarchicalphrase reordering model and a word class lan-guage model.
For the phrase-based system we em-ployed discriminative phrase training.
Addition-ally, improvements of our preprocessing pipelinecompared to our WMT 2013 setup were described.New introduced categories lead to a lower amountof out-of-vocabulary words.
Filtering the corpusfor wrong languages gives us lower vocabularysizes for source and target without loosing any per-formance.160AcknowledgmentsThe research leading to these results has partiallyreceived funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement no287658.Furthermore, this material is partially basedupon work supported by the DARPA BOLTproject under Contract No.
HR0011- 12-C-0015.Any opinions, findings and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof DARPA.ReferencesStanley F. Chen and Joshua Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University, Cam-bridge, Massachusetts, USA, August.David Chiang.
2007.
Hierarchical Phrase-BasedTranslation.
Computational Linguistics, 33(2):201?228.Markus Freitag, Stephan Peitz, Joern Wuebker, Her-mann Ney, Matthias Huck, Rico Sennrich, NadirDurrani, Maria Nadejde, Philip Williams, PhilippKoehn, Teresa Herrmann, Eunah Cho, and AlexWaibel.
2014.
EU-BRIDGE MT: Combined Ma-chine Translation.
In Proceedings of the ACL 2014Ninth Workshop on Statistical Machine Translation,Baltimore, MD, USA, June.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 847?855, Honolulu, Hawaii, USA,October.Xiaodong He and Li Deng.
2012.
Maximum ExpectedBLEU Training of Phrase and Lexicon TranslationModels.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 292?301, Jeju, Republic of Korea, Jul.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 690?696,Sofia, Bulgaria, August.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation, pages 187?197, Edinburgh, Scot-land, United Kingdom, July.Matthias Huck, Jan-Thorsten Peter, Markus Freitag,Stephan Peitz, and Hermann Ney.
2012.
Hierar-chical Phrase-Based Translation with Jane 2.
ThePrague Bulletin of Mathematical Linguistics, 98:37?50, October.Matthias Huck, David Vilar, Markus Freitag, and Her-mann Ney.
2013a.
A Performance Study ofCube Pruning for Large-Scale Hierarchical MachineTranslation.
In Proceedings of the NAACL 7thWork-shop on Syntax, Semantics and Structure in Statis-tical Translation, pages 29?38, Atlanta, Georgia,USA, June.Matthias Huck, Joern Wuebker, Felix Rietig, and Her-mann Ney.
2013b.
A phrase orientation modelfor hierarchical machine translation.
In ACL 2013Eighth Workshop on Statistical Machine Transla-tion, pages 452?463, Sofia, Bulgaria, August.Reinhard Kneser and Hermann Ney.
1995.
Im-proved Backing-Off for M-gram Language Model-ing.
In Proceedings of the International Conferenceon Acoustics, Speech, and Signal Processing, vol-ume 1, pages 181?184, May.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In Proceedings ofEuropean Chapter of the ACL (EACL 2009), pages187?194.Saab Mansour and Hermann Ney.
2012.
A Simple andEffective Weighted Phrase Extraction for MachineTranslation Adaptation.
In Proceedings of the Inter-national Workshop on Spoken Language Translation(IWSLT), pages 193?200, Hong Kong, December.Saab Mansour, Joern Wuebker, and Hermann Ney.2011.
Combining Translation and Language ModelScoring for Domain-Specific Data Filtering.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 222?229, SanFrancisco, California, USA, December.Robert C. Moore and William Lewis.
2010.
IntelligentSelection of Language Model Training Data.
In ACL(Short Papers), pages 220?224, Uppsala, Sweden,July.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318,Philadelphia, Pennsylvania, USA, July.161Maja Popovi?c and Hermann Ney.
2006.
POS-basedWord Reorderings for Statistical Machine Transla-tion.
In International Conference on Language Re-sources and Evaluation, pages 1278?1283, Genoa,Italy, May.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA,August.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.on Speech and Language Processing (ICSLP), vol-ume 2, pages 901?904, Denver, Colorado, USA,September.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
In Pro-ceedings of HLT-NAACL 2004: Short Papers, HLT-NAACL-Short ?04, pages 101?104, Boston, MA,USA.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2010.
Jane: Open Source Hierarchi-cal Translation, Extended with Reordering and Lex-icon Models.
In ACL 2010 Joint Fifth Workshop onStatistical Machine Translation and Metrics MATR,pages 262?270, Uppsala, Sweden, July.David Vilar, Daniel Stein, Matthias Huck, and Her-mann Ney.
2012.
Jane: an advanced freely avail-able hierarchical machine translation toolkit.
Ma-chine Translation, 26(3):197?216, September.Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training phrase translation models withleaving-one-out.
In Proceedings of the 48th AnnualMeeting of the Assoc.
for Computational Linguistics,pages 475?484, Uppsala, Sweden, July.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012.
Jane 2:Open Source Phrase-based and Hierarchical Statis-tical Machine Translation.
In International Confer-ence on Computational Linguistics, pages 483?491,Mumbai, India, December.Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-mann Ney.
2013.
Improving statistical machinetranslation with word class models.
In Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1377?1381, Seattle, USA, October.162
