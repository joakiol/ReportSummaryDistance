Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 425?432,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Fast, Accurate Deterministic Parser for ChineseMengqiu Wang Kenji Sagae Teruko MitamuraLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University{mengqiu,sagae,teruko}@cs.cmu.eduAbstractWe present a novel classifier-based deter-ministic parser for Chinese constituencyparsing.
Our parser computes parse treesfrom bottom up in one pass, and usesclassifiers to make shift-reduce decisions.Trained and evaluated on the standardtraining and test sets, our best model (us-ing stacked classifiers) runs in linear timeand has labeled precision and recall above88% using gold-standard part-of-speechtags, surpassing the best published re-sults.
Our SVM parser is 2-13 times fasterthan state-of-the-art parsers, while produc-ing more accurate results.
Our Maxentand DTree parsers run at speeds 40-270times faster than state-of-the-art parsers,but with 5-6% losses in accuracy.1 Introduction and BackgroundSyntactic parsing is one of the most fundamentaltasks in Natural Language Processing (NLP).
Inrecent years, Chinese syntactic parsing has alsoreceived a lot of attention in the NLP commu-nity, especially since the release of large collec-tions of annotated data such as the Penn Chi-nese Treebank (Xue et al, 2005).
Corpus-basedparsing techniques that are successful for Englishhave been applied extensively to Chinese.
Tradi-tional statistical approaches build models whichassign probabilities to every possible parse treefor a sentence.
Techniques such as dynamic pro-gramming, beam-search, and best-first-search arethen employed to find the parse tree with the high-est probability.
The massively ambiguous natureof wide-coverage statistical parsing,coupled withcubic-time (or worse) algorithms makes this ap-proach too slow for many practical applications.Deterministic parsing has emerged as an attrac-tive alternative to probabilistic parsing, offeringaccuracy just below the state-of-the-art in syn-tactic analysis of English, but running in lineartime (Sagae and Lavie, 2005; Yamada and Mat-sumoto, 2003; Nivre and Scholz, 2004).
Encour-aging results have also been shown recently byCheng et al (2004; 2005) in applying determin-istic models to Chinese dependency parsing.We present a novel classifier-based determin-istic parser for Chinese constituency parsing.
Inour approach, which is based on the shift-reduceparser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a suc-cession of classification tasks.
The parser makesone pass through the input sentence.
At each parsestate, it consults a classifier to make shift/reducedecisions.
The parser then commits to a decisionand enters the next parse state.
Shift/reduce deci-sions are made deterministically based on the lo-cal context of each parse state, and no backtrack-ing is involved.
This process can be viewed as agreedy search where only one path in the wholesearch space is considered.
Our parser producesboth dependency and constituent structures, but inthis paper we will focus on constituent parsing.By separating the classification task from theparsing process, we can take advantage of manymachine learning techniques such as classifier en-semble.
We conducted experiments with fourdifferent classifiers: support vector machines(SVM), Maximum-Entropy (Maxent), DecisionTree (DTree) and memory-based learning (MBL).We also compared the performance of three differ-ent classifier ensemble approaches (simple voting,classifier stacking and meta-classifier).Our best model (using stacked classifiers) runsin linear time and has labeled precision andrecall above 88% using gold-standard part-of-speech tags, surpassing the best published results(see Section 5).
Our SVM parser is 2-13 timesfaster than state-of-the-art parsers, while produc-425ing more accurate results.
Our Maxent and DTreeparsers are 40-270 times faster than state-of-the-art parsers, but with 5-6% losses in accuracy.2 Deterministic parsing modelLike other deterministic parsers, our parser as-sumes input has already been segmented andtagged with part-of-speech (POS) informationduring a preprocessing step1.
The main data struc-tures used in the parsing algorithm are a queue anda stack.
The input word-POS pairs to be processedare stored in the queue.
The stack holds the partialparse trees that are built during parsing.
A parsestate is represented by the content of the stack andqueue.The classifier makes shift/reduce decisionsbased on contextual features that represent theparse state.
A shift action removes the first itemon the queue and puts it onto the stack.
A reduceaction is in the form of Reduce-{Binary|Unary}-X, where {Binary|Unary} denotes whether one ortwo items are to be removed from the stack, and Xis the label of a new tree node that will be domi-nating the removed items.
Because a reduction iseither unary or binary, the resulting parse tree willonly have binary and/or unary branching nodes.Parse trees are also lexicalized to produce de-pendency structures.
For lexicalization, we usedthe same head-finding rules reported in (Bikel,2004).
With this additional information, reduceactions are now in the form of Reduce-{Binary|Unary}-X-Direction.
The ?Direction?
tag givesinformation about whether to take the head-nodeof the left subtree or the right subtree to be thehead of the new tree, in the case of binary reduc-tion.
A simple transformation process as describedin (Sagae and Lavie, 2005) is employed to con-vert between arbitrary branching trees and binarytrees.
This transformation breaks multi-branchingnodes down into binary-branching nodes by in-serting temporary nodes; temporary nodes are col-lapsed and removed when we transform a binarytree back into a multi-branching tree.The parsing process succeeds when all the itemsin the queue have been processed and there is onlyone item (the final parse tree) left on the stack.If the classifier returns a shift action when thereare no items left on the queue, or a reduce ac-tion when there are no items on the stack, the1We constructed our own POS tagger based on SVM; seeSection 3.3.parser fails.
In this case, the parser simply com-bines all the items on the stack into one IP node,and outputs this as a partial parse.
Sagae andLavie (2005) have shown that this algorithm haslinear time complexity, assuming that classifica-tion takes constant time.
The next example il-lustrates the process for the input ?Y?
(Brown)6?
(visits)?0 (Shanghai)?
that is tagged withthe POS sequence ?NR (Proper Noun) VV (Verb)NR (Proper Noun)?.1.
In the initial parsing state, the stack (S) isempty, and the queue (Q) holds word andPOS tag pairs for the input sentence.
(S): Empty(Q): NRY?VV6?NR?02.
The first action item that the classifier givesis a shift action.
(S): NRY?
(Q): VV6?NR?03.
The next action is a reduce-Unary-NP, whichmeans reducing the first item on the stack to aNP node.
Node (NRY?)
becomes the headof the new NP node and this information ismarked by brackets.
The new parse state is:(S): NP (NRY?)NRY?
(Q): VV6?NR?04.
The next action is shift.
(S): NP (NRY?)NRY?VV6?
(Q): NR?05.
The next action is again shift.
(S): NP (NRY?
)NRY?VV6?NR?0(Q): Empty6.
The next action is reduce-Unary-NP.
(S): NP (NRY?
)NRY?VV6?NP (NR?0)NR?0(Q): Empty7.
The next action is reduce-Binary-VP-Left.The node (VV6?)
will be the head of the426new VP node.
(S): NP (NRY?
)NRY?VP (VV6?
)VV6?NP (NR?0)NR?0(Q): Empty8.
The next action is reduce-Binary-IP-Right.Since after the action is performed, there willbe only one tree node(IP) left on the stack andno items on the queue, this is the final action.The final state is:(S): IP (VV6?
)NP (NRY?
)NRY?VP (VV6?
)VV6?NP (NR?0)NR?0(Q): Empty3 Classifiers and Feature SelectionClassification is the key component of our parsingmodel.
We conducted experiments with four dif-ferent types of classifiers.3.1 ClassifiersSupport Vector Machine: Support Vector Ma-chine is a discriminative classification techniquewhich solves the binary classification problem byfinding a hyperplane in a high dimensional spacethat gives the maximum soft margin, based onthe Structural Risk Minimization Principle.
Weused the TinySVM toolkit (Kudo and Matsumoto,2000), with a degree 2 polynomial kernel.
To traina multi-class classifier, we used the one-against-allscheme.Maximum-Entropy Classifier: In aMaximum-entropy model, the goal is to esti-mate a set of parameters that would maximizethe entropy over distributions that satisfy certainconstraints.
These constraints will force the modelto best account for the training data (Ratnaparkhi,1999).
Maximum-entropy models have been usedfor Chinese character-based parsing (Fung et al,2004; Luo, 2003) and POS tagging (Ng and Low,2004).
In our experiments, we used Le?s Maxenttoolkit (Zhang, 2004).
This implementation usesthe Limited-Memory Variable Metric method forparameter estimation.
We trained all our modelsusing 300 iterations with no event cut-off, anda Gaussian prior smoothing value of 2.
Maxentclassifiers output not only a single class label, butalso a number of possible class labels and theirassociated probability estimate.Decision Tree Classifier: Statistical decisiontree is a classic machine learning technique thathas been extensively applied to NLP.
For exam-ple, decision trees were used in the SPATTER sys-tem (Magerman, 1994) to assign probability dis-tribution over the space of possible parse trees.In our experiment, we used the C4.5 decisiontree classifier, and ignored lexical features whosecounts were less than 7.Memory-Based Learning: Memory-BasedLearning approaches the classification problemby storing training examples explicitly in mem-ory, and classifying the current case by findingthe most similar stored cases (using k-nearest-neighbors).
We used the TiMBL toolkit (Daele-mans et al, 2004) in our experiment, with k = 5.3.2 Feature selectionFor each parse state, a set of features areextracted and fed to each classifier.
Fea-tures are distributionally-derived or linguistically-based, and carry the context of a particular parsestate.
When input to the classifier, each feature istreated as a contextual predicate which maps anoutcome and a context to true, false value.The specific features used with the classifiersare listed in Table 1.Sun and Jurafsky (2003) studied the distribu-tional property of rhythm in Chinese, and used therhythmic feature to augment a PCFG model fora practical shallow parsing task.
This feature hasthe value 1, 2 or 3 for monosyllabic, bi-syllabic ormulti-syllabic nouns or verbs.
For noun and verbphrases, the feature is defined as the number ofwords in the phrase.
Sun and Jurafsky found thatin NP and VP constructions there are strong con-straints on the word length for verbs and nouns(a kind of rhythm), and on the number of wordsin a constituent.
We employed these same rhyth-mic features to see whether this property holds forthe Penn Chinese Treebank data, and if it helps inthe disambiguation of phrase types.
Experimentsshow that this feature does increase classificationaccuracy of the SVM model by about 1%.In both Chinese and English, there are punctu-ation characters that come in pairs (e.g., parenthe-ses).
In Chinese, such pairs are more frequent(quotes, single quotes, and book-name marks).During parsing, we note how many opening punc-4271 A Boolean feature indicates if a closing punctuation is expected or not.2 A Boolean value indicates if the queue is empty or not.3 A Boolean feature indicates whether there is a comma separating S(1) and S(2) or not.4 Last action given by the classifier, and number of words in S(1) and S(2).5 Headword and its POS of S(1), S(2), S(3) and S(4), and word and POS of Q(1), Q(2), Q(3) and Q(4).6 Nonterminal label of the root of S(1) and S(2), and number of punctuations in S(1) and S(2).7 Rhythmic features and the linear distance between the head-words of the S(1) and S(2).8 Number of words found so far to be dependents of the head-words of S(1) and S(2).9 Nonterminal label, POS and headword of the immediate left and right child of the root of S(1) and S(2).10 Most recently found word and POS pair that is to the left of the head-word of S(1) and S(2).11 Most recently found word and POS pair that is to the right of the head-word of S(1) and S(2).Table 1: Features for classificationtuations we have seen on the stack.
If the numberis odd, then feature 2 will have value 1, otherwise0.
A boolean feature is used to indicate whether ornot an odd number of opening punctuations havebeen seen and a closing punctuation is expected;in this case the feature gives a strong hint to theparser that all the items in the queue before theclosing punctuation, and the items on the stackafter the opening punctuation should be under acommon constituent node which begins and endswith the two punctuations.3.3 POS taggingIn our parsing model, POS tagging is treated asa separate problem and it is assumed that the in-put has already been tagged with POS.
To com-pare with previously published work, we evaluatedthe parser performance on automatically taggeddata.
We constructed a simple POS tagger usingan SVM classifier.
The tagger makes two passesover the input sentence.
The first pass extracts fea-tures from the two words and POS tags that camebefore the current word, the two words follow-ing the current word, and the current word itself(the length of the word, whether the word con-tains numbers, special symbols that separates for-eign first and last names, common Chinese familynames, western alphabets or dates).
Then the tagis assigned to the word according to SVM classi-fier?s output.
In the second pass, additional fea-tures such as the POS tags of the two words fol-lowing the current word, and the POS tag of thecurrent word (assigned in the first pass) are used.This tagger had a measured precision of 92.5% forsentences ?
40 words.4 ExperimentsWe performed experiments using the Penn Chi-nese Treebank.
Sections 001-270 (3484 sentences,84,873 words) were used for training, 271-300(348 sentences, 7980 words) for development, and271-300 (348 sentences, 7980 words) for testing.The whole dataset contains 99629 words, which isabout 1/10 of the size of the English Penn Tree-bank.
Standard corpus preparation steps weredone prior to parsing, so that empty nodes wereremoved, and the resulting A over A unary rewritenodes are collapsed.
Functional labels of the non-terminal nodes are also removed, but we did notrelabel the punctuations, unlike in (Jiang, 2004).Bracket scoring was done by the EVALB pro-gram2, and preterminals were not counted as con-stituents.
In all our experiments, we used labeledrecall (LR), labeled precision (LP) and F1 score(harmonic mean of LR and LP) as our evaluationmetrics.4.1 Results of different classifiersTable 2 shows the classification accuracy and pars-ing accuracy of the four different classifiers on thedevelopment set for sentences ?
40 words, withgold-standard POS tagging.
The runtime (Time)of each model and number of failed parses (Fail)are also shown.Classification Parsing AccuracyModel Accuracy LR LP F1 Fail TimeSVM 94.3% 86.9% 87.9% 87.4% 0 3m 19sMaxent 92.6% 84.1% 85.2% 84.6% 5 0m 21sDTree1 92.0% 78.8% 80.3% 79.5% 42 0m 12sDTree2 N/A 81.6% 83.6% 82.6% 30 0m 18sMBL 90.6% 74.3% 75.2% 74.7% 2 16m 11sTable 2: Comparison of different classifier mod-els?
parsing accuracies on development set for sen-tences ?
40 words, with gold-standard POSFor the DTree learner, we experimented withtwo different classification strategies.
In our firstapproach, the classification is done in a singlestage (DTree1).
The learner is trained for a multi-2http://nlp.cs.nyu.edu/evalb/428class classification problem where the class labelsinclude shift and all possible reduce actions.
Butthis approach yielded a lot of parse failures (42 outof 350 sentences failed during parsing, and par-tial parse tree was returned).
These failures weremostly due to false shift actions in cases wherethe queue is empty.
To alleviate this problem, webroke the classification process down to two stages(DTree2).
A first stage classifier makes a binarydecision on whether the action is shift or reduce.If the output is reduce, a second-stage classifier de-cides which reduce action to take.
Results showedthat breaking down the classification task into twostages increased overall accuracy, and the numberof failures was reduced to 30.The SVM model achieved the highest classifi-cation accuracy and the best parsing results.
Italso successfully parsed all sentences.
The Max-ent model?s classification error rate (7.4%) was30% higher than the error rate of the SVM model(5.7%), and its F1 (84.6%) was 3.2% lower thanSVM model?s F1 (87.4%).
But Maxent model wasabout 9.5 times faster than the SVM model.
TheDTree classifier achieved 81.6% LR and 83.6%LP.
The MBL model did not perform well; al-though MBL and SVM differed in accuracy byonly about 3 percent, the parsing results showeda difference of more than 10 percent.
One pos-sible explanation for the poor performance ofthe MBL model is that all the features we usedwere binary features, and memory-based learneris known to work better with multivalue featuresthan binary features in natural language learningtasks (van den Bosch and Zavrel, 2000).In terms of speed and accuracy trade-off, thereis a 5.5% trade-off in F1 (relative to SVM?s F1)for a roughly 14 times speed-up between SVMand two-stage DTree.
Maxent is more balancedin the sense that its accuracy was slightly lower(3.2%) than SVM, and was just about as fast as thetwo-stage DTree on the development set.
The highspeed of the DTree and Maxent models make themvery attractive in applications where speed is morecritical than accuracy.
While the SVM modeltakes more CPU time, we show in Section 5 thatwhen compared to existing parsers, SVM achievesabout the same or higher accuracy but is at leasttwice as fast.Using gold-standard POS tagging, the best clas-sifier model (SVM) achieved LR of 87.2% and LPof 88.3%, as shown in Table 4.
Both measures sur-pass the previously known best results on parsingusing gold-standard tagging.
We also tested theSVM model using data automatically tagged byour POS tagger, and it achieved LR of 78.1% andLP of 81.1% for sentences ?
40 words, as shownin Table 3.4.2 Classifier Ensemble ExperimentsClassifier ensemble by itself has been a fruitfulresearch direction in machine learning in recentyears.
The basic idea in classifier ensemble isthat combining multiple classifiers can often givesignificantly better results than any single classi-fier alone.
We experimented with three differentclassifier ensemble strategies: classifier stacking,meta-classifier, and simple voting.Using the SVM classifier?s results as a baseline,we tested these approaches on the developmentset.
In classifier stacking, we collect the outputsfrom Maxent, DTree and TiMBL, which are alltrained on a separate dataset from the training set(section 400-650 of the Penn Chinese Treebank,smaller than the original training set).
We use theirclassification output as features, in addition to theoriginal feature set, to train a new SVM modelon the original training set.
We achieved LR of90.3% and LP of 90.5% on the development set,a 3.4% and 2.6% improvement in LR and LP, re-spectively.
When tested on the test set, we gained1% improvement in F1 when gold-standard POStagging is used.
When tested with automatic tag-ging, we achieved a 0.5% improvement in F1.
Us-ing Bikel?s significant tester with 10000 times ran-dom shuffle, the p-value for LR and LP are 0.008and 0.457, respectively.
The increase in recallis statistically significant, and it shows classifierstacking can improve performance.On the other hand, we did not find meta-classification and simple voting very effective.
Insimple voting, we make the classifiers to vote ineach step for every parse action.
The F1 of sim-ple voting method is downgraded by 5.9% rela-tive to SVM model?s F1.
By analyzing the inter-agreement among classifiers, we found that therewere no cases where Maxent?s top output andDTree?s output were both correct and SVM?s out-put was wrong.
Using the top output from Maxentand DTree directly does not seem to be comple-mentary to SVM.In the meta-classifier approach, we first col-lect the output from each classifier trained on sec-429MODEL ?
40 words ?
100 words UnlimitedLR LP F1 POS LR LP F1 POS LR LP F1 POSBikel & Chiang 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -Levy & Manning 2003 79.2% 78.4% 78.8% - - - - - - - - -Xiong et al 2005 78.7% 80.1% 79.4% - - - - - - - - -Bikel?s Thesis 2004 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -Chiang & Bikel 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -Jiang?s Thesis 2004 80.1% 82.0% 81.1% 92.4% - - - - - - - -Sun & Jurafsky 2004 85.5% 86.4% 85.9% - - - - - 83.3% 82.2% 82.7% -DTree model 71.8% 76.9% 74.4% 92.5% 69.2% 74.5% 71.9% 92.2% 68.7% 74.2% 71.5% 92.1%SVM model 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%Stacked classifier model 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%Table 3: Comparison with related work on the test set using automatically generated POStion 1-210 (roughly 3/4 of the entire training set).Then specifically for Maxent, we collected the topoutput as well as its associated probability esti-mate.
Then we used the outputs and probabil-ity estimate as features to train an SVM classifierthat makes a decision on which classifier to pick.Meta-classifier results did not change at all fromour baseline.
In fact, the meta-classifier alwayspicked SVM as its output.
This agrees with ourobservation for the simple voting case.5 Comparison with Related WorkBikel and Chiang (2000) constructed two parsersusing a lexicalized PCFG model that is based onCollins?
model 2 (Collins, 1999), and a statisti-cal Tree-adjoining Grammar(TAG) model.
Theyused the same train/development/test split, andachieved LR/LP of 76.8%/77.8%.
In Bikel?s the-sis (2004), the same Collins emulation modelwas used, but with tweaked head-finding rules.Also a POS tagger was used for assigning tagsfor unseen words.
The refined model achievedLR/LP of 78.0%/81.2%.
Chiang and Bikel (2002)used inside-outside unsupervised learning algo-rithm to augment the rules for finding heads, andachieved an improved LR/LP of 78.8%/81.1%.Levy and Manning (2003) used a factored modelthat combines an unlexicalized PCFG model witha dependency model.
They achieved LR/LPof 79.2%/78.4% on a different test/developmentsplit.
Xiong et al (2005) used a similar model tothe BBN?s model in (Bikel and Chiang, 2000),and augmented the model by semantic categori-cal information and heuristic rules.
They achievedLR/LP of 78.7%/80.1%.
Hearne and Way (2004)used a Data-Oriented Parsing (DOP) approachthat was optimized for top-down computation.They achieved F1 of 71.3 on a different test andtraining set.
Jiang (2004) reported LR/LP of80.1%/82.0% on sentences ?
40 words (resultsnot available for sentences ?
100 words) by ap-plying Collins?
parser to Chinese.
In Sun andJurafsky (2004)?s work on Chinese shallow se-mantic parsing, they also applied Collin?s parserto Chinese.
They reported up-to-date the bestparsing performance on Chinese Treebank.
Theyachieved LR/LP of 85.5%/86.4% on sentences ?40 words, and LR/LP of 83.3%/82.2% on sen-tences ?
100 words, far surpassing all other pre-viously reported results.
Luo (2003) and Fung etal.
(2004) addressed the issue of Chinese text seg-mentation in their work by constructing character-based parsers.
Luo integrated segmentation, POStagging and parsing into one maximum-entropyframework.
He achieved a F1 score of 81.4% inparsing.
But the score was achieved using 90% ofthe 250K-CTB (roughly 2.5 times bigger than ourtraining set) for training and 10% for testing.
Funget al(2004) also took the maximum-entropy mod-eling approach, but augmented by transformation-based learning.
They used the standard trainingand testing split.
When tested with gold-standardsegmentation, they achieved a F1 score of 79.56%,but POS-tagged words were treated as constituentsin their evaluation.In comparison with previous work, our parser?saccuracy is very competitive.
Compared to Jiang?swork and Sun and Jurafsky?s work, the classifierensemble model of our parser is lagging behind by1% and 5.8% in F1, respectively.
But comparedto all other works, our classifier stacking modelgave better or equal results for all three measures.In particular, the classifier ensemble model andSVM model of our parser achieved second andthird highest LP, LR and F1 for sentences ?
100words as shown in Table 3.
(Sun and Jurafsky didnot report results on sentences ?
100 words, butit is worth noting that out of all the test sentences,430only 2 sentences have length > 100).Jiang (2004) and Bikel (2004)3 also evaluatedtheir parsers on the test set for sentences ?
40words, using gold-standard POS tagged input.
Ourparser gives significantly better results as shownin Table 4.
The implication of this result is two-fold.
On one hand, it shows that if POS taggingaccuracy can be increased, our parser is likely tobenefit more than the other two models; on theother hand, it also indicates that our deterministicmodel is less resilient to POS errors.
Further de-tailed analysis is called for, to study the extent towhich POS tagging errors affects the deterministicparsing model.Model LR LP F1Bikel?s Thesis 2004 80.9% 84.5% 82.7%Jiang?s Thesis 2004 84.5% 88.0% 86.2%DTree model 80.5% 83.9% 82.2%Maxent model 81.4% 82.8% 82.1%SVM model 87.2% 88.3% 87.8%Stacked classifier model 88.3% 88.1% 88.2%Table 4: Comparison with related work on the testset for sentence ?
40 words, using gold-standardPOSTo measure efficiency, we ran two publiclyavailable parsers (Levy and Manning?s PCFGparser (2003) and Bikel?s parser (2004)) onthe standard test set and compared the run-time4.
The runtime of these parsers are shownin minute:second format in Table 5.
Our SVMmodel is more than 2 times faster than Levy andManning?s parser, and more than 13 times fasterthan Bikel?s parser.
Our DTree model is 40 timesfaster than Levy and Manning?s parser, and 270times faster than Bikel?s parser.
Another advan-tage of our parser is that it does not take as muchmemory as these other parsers do.
In fact, noneof the models except MBL takes more than 60megabytes of memory at runtime.
In compari-son, Levy and Manning?s PCFG parser requiresmore than 400 mega-bytes of memory when pars-ing long sentences (70 words or longer).6 Discussion and future workOne unique attraction of this deterministic pars-ing framework is that advances in machine learn-ing field can be directly applied to parsing, which3Bikel?s parser used gold-standard POS tags for unseenwords only.
Also, the results are obtained from a parsertrained on 250K-CTB, about 2.5 times bigger than CTB 1.0.4All the experiments were conducted on a Pentium IV2.4GHz machine with 2GB of RAM.Model runtimeBikel 54m 6sLevy & Manning 8m 12sOur DTree model 0m 14sOur Maxent model 0m 24sOur SVM model 3m 50sTable 5: Comparison of parsing speedopens up lots of possibilities for continuous im-provements, both in terms of accuracy and effi-ciency.
For example, in this paper we experi-mented with one method of simple voting.
An al-ternative way of doing simple voting is to let theparsers vote on membership of constituents aftereach parser has produced its own parse tree (Hen-derson and Brill, 1999), instead of voting at eachstep during parsing.Our initial attempt to increase the accuracy ofthe DTree model by applying boosting techniquesdid not yield satisfactory results.
In our exper-iment, we implemented the AdaBoost.M1 (Fre-und and Schapire, 1996) algorithm using re-sampling to vary the training set distribution.Results showed AdaBoost suffered severe over-fitting problems and hurts accuracy greatly, evenwith a small number of samples.
One possiblereason for this is that our sample space is veryunbalanced across the different classes.
A fewclasses have lots of training examples while a largenumber of classes are rare, which could raise thechance of overfitting.In our experiments, SVM model gave better re-sults than the Maxent model.
But it is importantto note that although the same set of features wereused in both models, a degree 2 polynomial ker-nel was used in the SVM classifier while Maxentonly has degree 1 features.
In our future work, wewill experiment with degree 2 features and L1 reg-ularization in the Maxent model, which may giveus closer performance to the SVM model with amuch faster speed.7 ConclusionIn this paper, we presented a novel determinis-tic parser for Chinese constituent parsing.
Us-ing gold-standard POS tags, our best model (us-ing stacked classifiers) runs in linear time and haslabeled recall and precision of 88.3% and 88.1%,respectively, surpassing the best published results.And with a trade-off of 5-6% in accuracy, ourDTree and Maxent parsers run at speeds 40-270times faster than state-of-the-art parsers.
Our re-431sults have shown that the deterministic parsingframework is a viable and effective approach toChinese parsing.
For future work, we will fur-ther improve the speed and accuracy of our mod-els, and apply them to more Chinese and multi-lingual natural language applications that requirehigh speed and accurate parsing.AcknowledgmentThis work was supported in part by ARDA?sAQUAINT Program.
We thank Eric Nyberg forhis help during the final preparation of this paper.ReferencesDaniel M. Bikel and David Chiang.
2000.
Two sta-tistical parsing models applied to the Chinese Tree-bank.
In Proceedings of the Second Chinese Lan-guage Processing Workshop, ACL ?00.Daniel M. Bikel.
2004.
On the Parameter Space ofGenerative Lexicalized Statistical Parsing Models.Ph.D.
thesis, University of Pennsylvania.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2004.
Deterministic dependency structureanalyzer for Chinese.
In Proceedings of IJCNLP?04.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2005.
Machine learning-based dependencyanalyzer for Chinese.
In Proceedings of ICCC ?05.David Chiang and Daniel M. Bikel.
2002.
Recoveringlatent information in treebanks.
In Proceedings ofCOLING ?02.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
Timbl version 5.1 ref-erence guide.
Technical report, Tilburg University.Yoav Freund and Robert E. Schapire.
1996.
Experi-ments with a new boosting algorithm.
In Proceed-ings of ICML ?96.Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-feng Chen.
2004.
A maximum-entropy Chineseparser augmented by transformation-based learning.ACM Transactions on Asian Language InformationProcessing, 3(2):159?168.Mary Hearne and Andy Way.
2004.
Data-orientedparsing and the Penn Chinese Treebank.
In Proceed-ings of IJCNLP ?04.John Henderson and Eric Brill.
1999.
Exploiting di-versity in natural language processing: Combiningparsers.
In Proceedings of EMNLP ?99.Zhengping Jiang.
2004.
Statistical Chinese parsing.Honours thesis, National University of Singapore.Taku Kudo and Yuji Matsumoto.
2000.
Use of supportvector learning for chunk identification.
In Proceed-ings of CoNLL and LLL ?00.Roger Levy and Christopher D. Manning.
2003.
Is itharder to parse Chinese, or the Chinese Treebank?In Proceedings of ACL ?03.Xiaoqiang Luo.
2003.
A maximum entropy Chinesecharacter-based parser.
In Proceedings of EMNLP?03.David M. Magerman.
1994.
Natural Language Pars-ing as Statistical Pattern Recognition.
Ph.D. thesis,Stanford University.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofEMNLP ?04.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedingsof COLING ?04.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1-3):151?175.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the IWPT ?05.Honglin Sun and Daniel Jurafsky.
2003.
The effect ofrhythm on structural disambiguation in Chinese.
InProceedings of SIGHAN Workshop ?03.Honglin Sun and Daniel Jurafsky.
2004.
Shallow se-mantic parsing of Chinese.
In Proceedings of theHLT/NAACL ?04.Antal van den Bosch and Jakub Zavrel.
2000.
Un-packing multi-valued symbolic features and classesin memory-based language learning.
In Proceedingsof ICML ?00.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,and Yueliang Qian.
2005.
Parsing the Penn ChineseTreebank with semantic knowledge.
In Proceedingsof IJCNLP ?05.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT ?03.Le Zhang, 2004.
Maximum Entropy Modeling Toolkitfor Python and C++.
Reference Manual.432
