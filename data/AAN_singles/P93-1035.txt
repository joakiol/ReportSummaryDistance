Automat ic  Grammar Induction and Parsing Free Text:A Transformation-Based ApproachEr ic  Bri l l*Depar tment  of Computer  and In format ion  Sc ienceUn ivers i ty  of Pennsy lvan iabr i l l@unag i .c i s .upenn.eduAbst ractIn this paper we describe a new technique forparsing free text: a transformational grammar Iis automatically learned that is capable of accu-rately parsing text into binary-branching syntac-tic trees with nonterminals unlabelled.
The algo-rithm works by beginning in a very naive state ofknowledge about phrase structure.
By repeatedlycomparing the results of bracketing in the currentstate to proper bracketing provided in the trainingcorpus, the system learns a set of simple structuraltransformations that can be applied to reduce er-ror.
After describing the algorithm, we presentresults and compare these results to other recentresults in automatic grammar induction.INTRODUCTIONThere has been a great deal of interest of late inthe automatic induction of natural anguage gram-mar.
Given the difficulty inherent in manuallybuilding a robust parser, along with the availabil-ity of large amounts of training material, auto-matic grammar induction seems like a path worthpursuing.
A number of systems have been builtthat can be trained automatically to bracket extinto syntactic onstituents.
In (MM90) mutual in-formation statistics are extracted from a corpus oftext and this information is then used to parsenew text.
(Sam86) defines a function to score thequality of parse trees, and then uses simulated an-nealing to heuristically explore the entire space ofpossible parses for a given sentence.
In (BM92a),distributional analysis techniques are applied to alarge corpus to learn a context-free grammar.The most promising results to date have been*The author would like to thank Mark Liberman,Melting Lu, David Magerman, Mitch Marcus, RichPito, Giorgio Satta, Yves Schabes and Tom Veatch.This work was supported by DARPA and AFOSRjointly under grant No.
AFOSR-90-0066, and by AROgrant No.
DAAL 03-89-C0031 PRI.1 Not in the traditional sense of the term.based on the inside-outside algorithm, which canbe used to train stochastic ontext-free grammars.The inside-outside algorithm is an extension ofthe finite-state based Hidden Markov Model (by(Bak79)), which has been applied successfully inmany areas, including speech recognition and partof speech tagging.
A number of recent papershave explored the potential of using the inside-outside algorithm to automatically learn a gram-mar (LY90, SJM90, PS92, BW92, CC92, SRO93).Below, we describe a new technique for gram-mar induction.
The algorithm works by beginningin a very naive state of knowledge about phrasestructure.
By repeatedly comparing the results ofparsing in the current state to the proper phrasestructure for each sentence in the training corpus,the system learns a set of ordered transformationswhich can be applied to reduce parsing error.
Webelieve this technique has advantages over othermethods of phrase structure induction.
Some ofthe advantages include: the system is very simple,it requires only a very small set of transforma-tions, a high degree of accuracy is achieved, andonly a very small training corpus is necessary.
Thetrained transformational parser is completely sym-bolic and can bracket ext in linear time with re-spect to sentence length.
In addition, since sometokens in a sentence are not even considered inparsing, the method could prove to be consid-erably more robust than a CFG-based approachwhen faced with noise or unfamiliar input.
Afterdescribing the algorithm, we present results andcompare these results to other recent results inautomatic phrase structure induction.TRANSFORMATION-BASEDERROR-DRIVEN LEARNINGThe phrase structure learning algorithm is atransformation-based error-driven learner.
Thislearning paradigm, illustrated in figure 1, hasproven to be successful in a number of differ-ent natural language applications, including partof speech tagging (Bri92, BM92b), prepositional259UNANNOTATEDTEXTSTATEANNOTATED TRUTHRULESFigure 1: Transformation-Based Error-DrivenLearning.phrase attachment (BR93), and word classifica-tion (Bri93).
In its initial state, the learner iscapable of annotating text but is not very goodat doing so.
The initial state is usually very easyto create.
In part of speech tagging, the initialstate annotator assigns every word its most likelytag.
In prepositional phrase attachment, he ini-tial state annotator always attaches prepositionalphrases low.
In word classification, all words areinitially classified as nouns.
The naively annotatedtext is compared to the true annotation as indi-cated by a small manually annotated corpus, andtransformations are learned that can be applied tothe output of the initial state annotator to makeit better resemble the truth.LEARNING PHRASESTRUCTUREThe phrase structure learning algorithm is trainedon a small corpus of partially bracketed text whichis also annotated with part of speech informa-tion.
All of the experiments presented belowwere done using the Penn Treebank annotatedcorpus(MSM93).
The learner begins in a naiveinitial state, knowing very little about the phrasestructure of the target corpus.
In particular, allthat is initially known is that English tends tobe right branching and that final punctuationis final punctuation.
Transformations are thenlearned automatically which transform the out-put of the naive parser into output which bet-ter resembles the phrase structure found in thetraining corpus.
Once a set of transformationshas been learned, the system is capable of takingsentences tagged with parts of speech and return-ing a binary-branching structure with nontermi-nals unlabelled.
2The  Initial State Of  The  ParserInitially, the parser operates by assigning a right-linear structure to all sentences.
The only excep-tion is that final punctuation is attached high.
So,the sentence "The dog and old cat ate ."
would beincorrectly bracketed as:( (The(dog(and(o ld  (cat  a te ) ) ) ) ) .
)The parser in its initial state will obviouslynot bracket sentences with great accuracy.
Insome experiments below, we begin with an evenmore naive initial state of knowledge: sentencesare parsed by assigning them a random binary-branching structure with final punctuation alwaysattached high.Structural  Transformat ionsThe next stage involves learning a set of trans-formations that can be applied to the output ofthe naive parser to make these sentences betterconform to the proper structure specified in thetraining corpus.
The list of possible transforma-tion types is prespecified.
Transformations involvemaking a simple change triggered by a simple en-vironment.
In the current implementation, thereare twelve allowable transformation types:?
(1-8) (AddHelete) a (leftlr ight) parenthesis tothe (leftlright) of part of speech tag X.?
(9-12) (Add\]delete) a (left\]right) parenthesisbetween tags X and Y.To carry out a transformation by adding ordeleting a parenthesis, a number of additional sim-ple changes must take place to preserve balancedparentheses and binary branching.
To give an ex-ample, to delete a left paren in a particular envi-ronment, the following operations take place (as-suming, of course, that there is a left paren todelete):1.
Delete the left paren.2.
Delete the right paren that matches the justdeleted paren.3.
Add a left paren to the left of the constituentimmediately to the left of the deleted left paren.2This is the same output given by systems de-scribed in (MM90, Bri92, PS92, SRO93).2604.
Add a right paren to the right of the con-stituent immediately to the right of the deletedleft paren.5.
If there is no constituent immediately to theright, or none immediately to the left, then thetransformation fails to apply.Structurally, the transformation can be seenas follows.
If we wish to delete a left paten tothe right of constituent X 3, where X appears in asubtree of the form:XAYY Zcarrying out these operations will transform thissubtree into: 4ZAX YYGiven the sentence: 5The dog barked .this would initially be bracketed by the naiveparser as:( (The(dogbarked) ) .
)If the transformation delete a left parch tothe right of a determiner is applied, the structurewould be transformed to the correct bracketing:( ( (Thedog)  barked) ,  )To add a right parenthesis to the right of YY,YY must once again be in a subtree of the form:X3To the right of the rightmost erminal dominatedby X if X is a nonterminal.4The twelve transformations can be decomposedinto two structural transformations, that shownhere and its converse, along with six triggeringenvironments.5Input sentences are also labelled with parts ofspeech.If it is, the following steps are carried out toadd the right paren:1.
Add the right paren.2.
Delete the left paten that now matches thenewly added paren.3.
Find the right paren that used to match the justdeleted paren and delete it.4.
Add a left paren to match the added right paren.This results in the same structural change asdeleting a left paren to the right of X in this par-ticular structure.Applying the transformation add a right patento the right of a noun to the bracketing:( (The(dogbarked) ) .
)will once again result in the correct bracketing:( ( (Thedog)barked) .
)Learning TransformationsLearning proceeds as follows.
Sentences in thetraining set are first parsed using the naive parserwhich assigns right linear structure to all sen-tences, attaching final punctuation high.
Next, foreach possible instantiation of the twelve transfor-mation templates, that particular transformationis applied to the naively parsed sentences.
The re-suiting structures are then scored using some mea-sure of success that compares these parses to thecorrect structural descriptions for the sentencesprovided in the training corpus.
The transforma-tion resulting in the best scoring structures thenbecomes the first transformation of the ordered setof transformations that are to be learned.
Thattransformation is applied to the right-linear struc-tures, and then learning proceeds on the corpusof improved sentence bracketings.
The followingprocedure is carried out repeatedly on the train-ing corpus until no more transformations can befound whose application reduces the error in pars-ing the training corpus:1.
The best transformation is found for the struc-tures output by the parser in its current state.
62.
The transformation is applied to the output re-sulting from bracketing the corpus using theparser in its current state.3.
This transformation is added to the end of theordered list of transformations.SThe state of the parser is defined as naive initial-state knowledge plus all transformations that cur-rently have been learned.2614.
Go to 1.After a set of transformations has beenlearned, it can be used to effectively parse freshtext.
To parse fresh text, the text is first naivelyparsed and then every transformation is applied,in order, to the naively parsed text.One nice feature of this method is that dif-ferent measures of bracketing success can be used:learning can proceed in such a way as to try tooptimize any specified measure of success.
Themeasure we have chosen for our experiments i  thesame measure described in (PS92), which is one ofthe measures that arose out of a parser evaluationworkshop (ea91).
The measure is the percentageof constituents (strings of words between matchingparentheses) from sentences output by our systemwhich do not cross any constituents in the PennTreebank structural description of the sentence.For example, if our system outputs:( ( (Theb ig )  (dogate) ) .
)and the Penn Treebank bracketing for this sen-tence was:( ( (Theb igdog)  a te ) .
)then the constituent he big would be judged cor-rect whereas the constituent dog ate would not.Below are the first seven transformationsfound from one run of training on the Wall StreetJournal corpus, which was initially bracketed us-ing the right-linear initial-state parser.1.
Delete a left paren to the left of a singular noun.2.
Delete a left paren to the left of a plural noun.3.
Delete a left paren between two proper nouns.4.
Delet a left paten to the right of a determiner.5.
Add a right paten to the left of a comma.6.
Add a right paren to the left of a period.7.
Delete a right paren to the left of a plural noun.The first four transformations all extract nounphrases from the right linear initial structure.
Thesentence "The cat meowed ."
would initially bebracketed as: 7( (The  (cat  meowed) )  .
)Applying the first transformation to thisbracketing would result in:7These examples are not actual sentences in thecorpus.
We have chosen simple sentences for clarity.
( ( (Thecat )meowed) .
)Applying the fifth transformation to thebracketing:( ( We ( ran (would result in( ( ( We ran )(and( theywalked) ) ) ) ) .
), (and( they  wa lked) ) ) ) .
)RESULTSIn the first experiment we ran, training and test-ing were done on the Texas Instruments Air TravelInformation System (ATIS) corpus(HGD90).
8 Intable 1, we compare results we obtained to re-sults cited in (PS92) using the inside-outside al-gorithm on the same corpus.
Accuracy is mea-sured in terms of the percentage of noncrossingconstituents in the test corpus, as described above.Our system was tested by using the training setto learn a set of transformations, and then ap-plying these transformations to the test set andscoring the resulting output.
In this experiment,64 transformations were learned (compared with4096 context-free rules and probabilities used inthe inside-outside algorithm experiment).
It is sig-nificant that we obtained comparable performanceusing a training corpus only 21% as large as thatused to train the inside-outside algorithm.Method # of Training AccuracyCorpus SentencesInside-Outside 700 90.36%TransformationLearner 150 91.12%Table 1: Comparing two learning methods on theATIS corpus.After applying all learned transformations tothe test corpus, 60% of the sentences had no cross-ing constituents, 74% had fewer than two crossingconstituents, and 85% had fewer than three.
Themean sentence length of the test corpus was 11.3.In figure 2, we have graphed percentage correctas a function of the number of transformationsthat have been applied to the test corpus.
Asthe transformation number increases, overtrainingsometimes occurs.
In the current implementationof the learner, a transformation is added to thelist if it results in any positive net change in theSin all experiments described in this paper, resultsare calculated on a test corpus which was not used inany way in either training the learning algorithm or indeveloping the system.262training set.
Toward the end of the learning proce-dure, transformations are found that only affect avery small percentage of training sentences.
Sincesmall counts are less reliable than large counts, wecannot reliably assume that these transformationswill also improve performance in the test corpus.One way around this overtraining would be to seta threshold: specify a minimum level of improve-ment that must result for a transformation to belearned.
Another possibility is to use additionaltraining material to prune the set of learned trans-formations.tO0O~?1?.-0U 00?10_00 10 20 30 40 50 60RuleNumberFigure 2: Results From the ATIS Corpus, StartingWith Right-Linear Structure.We next ran an experiment to determine whatperformance could be achieved if we dropped theinitial right-linear assumption.
Using the sametraining and test sets as above, sentences were ini-tially assigned a random binary-branching struc-ture, with final punctuation always attached high.Since there was less regular structure in this casethan in the right-linear case, many more transfor-mations were found, 147 transformations in total.When these transformations were applied to thetest set, a bracketing accuracy of 87.13% resulted.The ATIS corpus is structurally fairly regular.To determine how well our algorithm performs ona more complex corpus, we ran experiments onthe Wall Street Journal.
Results from this exper-iment can be found in table 2.
9 Accuracy is again9For sentences of length 2-15, the initial right-linearparser achieves 69% accuracy.
For sentences of lengthmeasured as the percentage of constituents in thetest set which do not cross any Penn Treebankconstituents.l?As a point of comparison, in (SRO93) an ex-periment was done using the inside-outside algo-r ithm on a corpus of WSJ sentences of length 1-15.Training was carried out on a corpus of 1,095 sen-tences, and an accuracy of 90.2% was obtained inbracketing a test set.# Training # ofSent.
Corpus Trans- %Length Sents formations Accuracy2-15 250 83 88.12-15 500 163 89.32-15 1000 221 91.62-20 250 145 86.22-25 250 160 83.8Table 2: WSJ SentencesIn the corpus we used for the experiments ofsentence length 2-15, the mean sentence lengthwas 10.80.
In the corpus used for the experi-ment of sentence length 2-25, the mean lengthwas 16.82.
As would be expected, performancedegrades omewhat as sentence length increases.In table 3, we show the percentage of sentences inthe test corpus that have no crossing constituents,and the percentage that have only a very smallnumber of crossing constituents.11SentLength2-152-152-25#TrainingCorpusSents5001000250% ofO-errorSents53.762.429.2% of<_l-errorSents72.377.244.9% of<2-errorSents84.687.859.9Table 3: WSJ Sentences.In table 4, we show the standard deviationmeasured from three different randomly chosentraining sets of each sample size and randomlychosen test sets of 500 sentences each, as well as2-20, 63% accuracy is achieved and for sentences oflength 2-25, accuracy is 59%.a?In all of our experiments carried out on the WallStreet Journal, the test set was a randomly selectedset of 500 sentences.nFor sentences of length 2-15, the initial right linearparser parses 17% of sentences with no crossing errors,35% with one or fewer errors and 50% with two orfewer.
For sentences of length 2-25, 7% of sentencesare parsed with no crossing errors, 16% with one orfewer, and 24% with two or fewer.263the accuracy as a function of training corpus sizefor sentences of length 2 to 20.# TrainingCorpus Sents%Correct0 63.010 75.850 82.1100 84.7250 86.2750 87.3Std.Dev.0.692.951.940.560.460.61Table 4: WSJ Sentences of Length 2 to 20.We also ran an experiment on WSJ sen-tences of length 2-15 starting with random binary-branching structures with final punctuation at-tached high.
In this experiment, 325 transfor-mations were found using a 250-sentence trainingcorpus, and the accuracy resulting from applyingthese transformations to a test set was 84.72%.Finally, in figure 3 we show the sentencelength distribution in the Wall Street Journal cor-pus.0800CO:3 o ?o.>-~ o rr0O04020 40 60 80 1 O0Sentence LengthFigure 3: The Distribution of Sentence Lengths inthe WSJ Corpus.While the numbers presented above allowus to compare the transformation learner withsystems trained and tested on comparable cor-pora, these results are all based upon the as-sumption that the test data is tagged fairly re-liably (manually tagged text was used in all ofthese experiments, as well in the experiments of(PS92, SRO93).)
When parsing free text, we can-not assume that the text will be tagged with theaccuracy of a human annotator.
Instead, an au-tomatic tagger would have to be used to first tagthe text before parsing.
To address this issue, weran one experiment where we randomly induced a5% tagging error rate beyond the error rate of thehuman annotator.
Errors were induced in such away as to preserve the unigram part of speech tagprobability distribution in the corpus.
The exper-iment was run for sentences of length 2-15, with atraining set of 1000 sentences and a test set of 500sentences.
The resulting bracketing accuracy was90.1%, compared to 91.6% accuracy when usingan unadulterated training corpus.
Accuracy onlydegraded by a small amount when training on thecorpus with adulterated part of speech tags, sug-gesting that high parsing accuracy rates could beachieved if tagging of the input were done auto-matically by a part of speech tagger.CONCLUSIONSIn this paper, we have described a new approachfor learning a grammar to automatical ly parsetext.
The method can be used to obtain highparsing accuracy with a very small training set.Instead of learning a traditional grammar,  an or-dered set of structural transformations is learnedthat can be applied to the output of a very naiveparser to obtain binary-branching trees with un-labelled nonterminals.
Experiments have shownthat these parses conform with high accuracy tothe structural descriptions pecified in a manuallyannotated corpus.
Unlike other recent attemptsat automatic grammar induction that rely heav-ily on statistics both in training and in the re-sulting grammar, our learner is only very weaklystatistical.
For training, only integers are neededand the only mathematical  operations carried outare integer addition and integer comparison.
Theresulting grammar is completely symbolic.
Un-like learners based on the inside-outside algorithmwhich attempt to find a grammar to maximizethe probability of the training corpus in hope thatthis grammar will match the grammar that pro-vides the most accurate structural descriptions,the transformation-based l arner can readily useany desired success measure in learning.We have already begun the next step in thisproject: automatically labelling the nonterminalnodes.
The parser will first use the ~ransforma-~ioual grammar to output a parse tree withoutnonterminal labels, and then a separate algorithmwill be applied to that tree to label the nontermi-nals.
The nonterminal-node labelling algorithmmakes use of ideas suggested in (Bri92), wherenonterminals are labelled as a function of the la-264bels of their daughters.
In addition, we plan toexperiment with other types of transformations.Currently, each transformation in the learned listis only applied once in each appropriate nviron-ment.
For a transformation to be applied morethan once in one environment, i  must appear inthe transformation list more than once.
One pos-sible extension to the set of transformation typeswould be to allow for transformations of the form:add/delete a paren as many times as is possiblein a particular environment.
We also plan to ex-periment with other scoring functions and controlstrategies for finding transformations and to usethis system as a postprocessor to other grammarinduction systems, learning transformations to im-prove their performance.
We hope these futurepaths will lead to a trainable and very accurateparser for free text.\[Bak79\]\[BM92a\]\[BM92b\]\[BR93\]\[Bri92\]\[Bri93\]\[BW92\]Re ferencesJ.
Baker.
Trainable grammars forspeech recognition.
In Speech commu-nication papers presented at the 97thMeeting of the Acoustical Society ofAmerica, 1979.E.
Brill and M. Marcus.
Automaticallyacquiring phrase structure using distri-butional analysis.
In Darpa Workshopon Speech and Natural Language, Har-riman, N.Y., 1992.E.
Brill and M. Marcus.
Tagging an un-familiar text with minimal human su-pervision.
In Proceedings of the FallSymposium on Probabilistic Approachesto Natural Language - AAAI Technical-Report.
American Association for Arti-ficial Intelligence, 1992.E.
Brill and P. Resnik.
A transformationbased approach to prepositional phraseattachment.
Technical report, Depart-ment of Computer and Information Sci-ence, University of Pennsylvania, 1993.E.
Brill.
A simple rule-based partof speech tagger.
In Proceedings ofthe Third Conference on Applied Natu-ral Language Processing, A CL, Trento,Italy, 1992.E.
Brill.
A Corpus-Based Approach toLanguage Learning.
PhD thesis, De-partment of Computer and Informa-tion Science, University of Pennsylva-nia, 1993.
Forthcoming.T.
Briscoe and N. Waegner.
Ro-bust stochastic parsing using the inside-outside algorithm.
In Workshop notes\[CC92\]\[ca91\]\[HGDg0\]\[LY90\]\[MMg0\]\[MSM93\]\[PS92\]\[Sam86\]\[SJM90\]\[SR093\]from the AAAI Statistically-Based NLPTechniques Workshop, 1992.G.
Carroll and E. Charniak.
Learn-ing probabilistic dependency grammarsfrom labelled text - aaai technical re-port.
In Proceedings of the Fall Sym-posium on Probabilisiic Approaches toNatural Language.
American Associa-tion for Artificial Intelligence, 1992.E.
Black et al A procedure for quan-titatively comparing the syntactic ov-erage of English grammars.
In Proceed-ings of Fourth DARPA Speech and Nat-ural Language Workshop, pages 306-311, 1991.C.
Hemphill, J. Godfrey, and G. Dod-dington.
The ATIS spoken languagesystems pilot corpus.
In Proceedings ofthe DARPA Speech and Natural Lan-guage Workshop, 1990.K.
Lari and S. Young.
The estimation ofstochastic ontext-free grammars usingthe inside-outside algorithm.
ComputerSpeech and Language, 4, 1990.D.
Magerman and M. Marcus.
Parsinga natural anguage using mutual infor-mation statistics.
In Proceedings, EighthNational Conference on Artificial Intel-ligence (AAAI 90), 1990.M.
Marcus, B. Santorini,and M. Marcinkiewiez.
Building a largeannotated corpus of English: the PennTreebank.
To appear in ComputationalLinguistics, 1993.F.
Pereira and Y. Schabes.
Inside-outside reestimation from partiallybracketed corpora.
In Proceedings ofthe30th Annual Meeting of the Associationfor Computational Linguistics, Newark,De., 1992.G.
Sampson.
A stochastic approachto parsing.
In Proceedings of COLING1986, Bonn, 1986.R.
Sharman, F. Jelinek, and R. Mer-cer.
Generating a grammar for sta-tistical training.
In Proceedings of the1990 Darpa Speech and Natural Lan-guage Workshop, 1990.Y.
Schabes, M. Roth, and R. Osborne.Parsing the Wall Street Journal withthe inside-outside algorithm.
In Pro-ceedings of the 1993 European ACL,Uterich, The Netherlands, 1993.265
