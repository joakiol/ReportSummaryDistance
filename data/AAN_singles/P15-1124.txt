Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1282?1291,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsModel-based Word Embeddings from Decompositions of Count MatricesKarl Stratos Michael Collins Daniel HsuColumbia University, New York, NY 10027, USA{stratos, mcollins, djhsu}@cs.columbia.eduAbstractThis work develops a new statistical un-derstanding of word embeddings inducedfrom transformed count data.
Using theclass of hidden Markov models (HMMs)underlying Brown clustering as a genera-tive model, we demonstrate how canoni-cal correlation analysis (CCA) and certaincount transformations permit efficient andeffective recovery of model parameterswith lexical semantics.
We further show inexperiments that these techniques empir-ically outperform existing spectral meth-ods on word similarity and analogy tasks,and are also competitive with other pop-ular methods such as WORD2VEC andGLOVE.1 IntroductionThe recent spike of interest in dense, low-dimensional lexical representations?i.e., wordembeddings?is largely due to their ability to cap-ture subtle syntactic and semantic patterns thatare useful in a variety of natural language tasks.A successful method for deriving such embed-dings is the negative sampling training of theskip-gram model suggested by Mikolov et al(2013b) and implemented in the popular softwareWORD2VEC.
The form of its training objectivewas motivated by efficiency considerations, buthas subsequently been interpreted by Levy andGoldberg (2014b) as seeking a low-rank factor-ization of a matrix whose entries are word-contextco-occurrence counts, scaled and transformed ina certain way.
This observation sheds new lighton WORD2VEC, yet alo raises several new ques-tions about word embeddings based on decompos-ing count data.
What is the right matrix to de-compose?
Are there rigorous justifications for thechoice of matrix and count transformations?In this paper, we answer some of these ques-tions by investigating the decomposition specifiedby CCA (Hotelling, 1936), a powerful techniquefor inducing generic representations whose com-putation is efficiently and exactly reduced to thatof a matrix singular value decomposition (SVD).We build on and strengthen the work of Stratos etal.
(2014) which uses CCA for learning the classof HMMs underlying Brown clustering.
We showthat certain count transformations enhance the ac-curacy of the estimation method and significantlyimprove the empirical performance of word rep-resentations derived from these model parameters(Table 1).In addition to providing a rigorous justifica-tion for CCA-based word embeddings, we alsosupply a general template that encompasses arange of spectral methods (algorithms employingSVD) for inducing word embeddings in the lit-erature, including the method of Levy and Gold-berg (2014b).
In experiments, we demonstrate thatCCA combined with the square-root transforma-tion achieves the best result among spectral meth-ods and performs competitively with other popu-lar methods such as WORD2VEC and GLOVE onword similarity and analogy tasks.
We addition-ally demonstrate that CCA embeddings providethe most competitive improvement when used asfeatures in named-entity recognition (NER).2 NotationWe use [n] to denote the set of integers {1, .
.
.
, n}.We denote the m?m diagonal matrix with valuesv1.
.
.
vmalong the diagonal by diag(v1.
.
.
vm).We write [a1.
.
.
am] to denote a matrix whose i-th column is ai.
The expected value of a randomvariable X is denoted by E[X].
Given a matrix ?and an exponent a, we distinguish the entrywisepower operation ??a?
(i.e., ?
?a?i,j= (?i,j)a) fromthe matrix power operation ?a(defined only forsquare ?
).12823 Background in CCAIn this section, we review the variational charac-terization of CCA.
This provides a flexible frame-work for a wide variety of tasks.
CCA seeks tomaximize a statistical quantity known as the Pear-son correlation coefficient between random vari-ables L,R ?
R:Cor(L,R) :=E[LR]?
E[L]E[R]?E[L2]?
E[L]2?E[R2]?
E[R]2This is a value in [?1, 1] indicating the degree oflinear dependence between L and R.3.1 CCA objectiveLet X ?
Rnand Y ?
Rn?be two random vectors.Without loss of generality, we will assume that Xand Y have zero mean.1Let m ?
min(n, n?
).CCA can be cast as finding a set of projection vec-tors (called canonical directions) a1.
.
.
am?
Rnand b1.
.
.
bm?
Rn?such that for i = 1 .
.
.m:(ai, bi) = arg maxa?Rn, b?Rn?Cor(a>X, b>Y ) (1)Cor(a>X, a>jX) = 0 ?j < iCor(b>Y, b>jY ) = 0 ?j < iThat is, at each i we simultaneously optimize vec-tors a, b so that the projected random variablesa>X, b>Y ?
R are maximally correlated, subjectto the constraint that the projections are uncorre-lated to all previous projections.Let A := [a1.
.
.
am] and B := [b1.
.
.
bm].Then we can think of the joint projectionsX = A>X Y = B>Y (2)as new m-dimensional representations of the orig-inal variables that are transformed to be as corre-lated as possible with each other.
Furthermore, of-ten m min(n, n?
), leading to a dramatic reduc-tion in dimensionality.3.2 Exact solution via SVDEq.
(1) is non-convex due to the terms a and b thatinteract with each other, so it cannot be solvedexactly using a standard optimization technique.However, a method based on SVD provides anefficient and exact solution.
See Hardoon et al(2004) for a detailed discussion.1This can be always achieved through data preprocessing(?centering?
).Lemma 3.1 (Hotelling (1936)).
Assume X andY have zero mean.
The solution (A,B) to (1)is given by A = E[XX>]?1/2U and B =E[Y Y>]?1/2V where the i-th column of U ?Rn?m(V ?
Rn?
?m) is the left (right) singularvector of?
:= E[XX>]?1/2E[XY>]E[Y Y>]?1/2(3)corresponding to the i-th largest singular value ?i.Furthermore, ?i= Cor(a>iX, b>iY ).3.3 Using CCA for word representationsAs presented in Section 3.1, CCA is a generalframework that operates on a pair of random vari-ables.
Adapting CCA specifically to inducingword representations results in a simple recipe forcalculating (3).A natural approach is to set X to represent aword and Y to represent the relevant ?context?information about a word.
We can use CCA toproject X and Y to a low-dimensional space inwhich they are maximally correlated: see Eq.
(2).The projected X can be considered as a new wordrepresentation.Denote the set of distinct word types by [n].
Weset X,Y ?
Rnto be one-hot encodings of wordsand their associated context words.
We define acontext word to be a word occurring within ?
po-sitions to the left and right (excluding the currentword).
For example, with ?
= 1, the followingsnippet of text where the current word is ?souls?
:Whatever our souls are made ofwill generate two samples of X ?
Y : a pair ofindicator vectors for ?souls?
and ?our?, and a pairof indicator vectors for ?souls?
and ?are?.CCA requires performing SVD on the followingmatrix ?
?
Rn?n:?
=(E[XX>]?
E[X]E[X]>)?1/2(E[XY>]?
E[X]E[Y ]>)(E[Y Y>]?
E[Y ]E[Y ]>)?1/2At a quick glance, this expression looks daunting:we need to perform matrix inversion and multipli-cation on potentially large dense matrices.
How-ever, ?
is easily computable with the followingobservations:Observation 1.
We can ignore the centering oper-ation when the sample size is large (Dhillon et al,12832011).
To see why, let {(x(i), y(i))}Ni=1be N sam-ples of X and Y .
Consider the sample estimate ofthe term E[XY>]?
E[X]E[Y ]>:1NN?i=1x(i)(y(i))>?1N2(N?i=1x(i))(N?i=1y(i))>The first term dominates the expression whenN islarge.
This is indeed the setting in this task wherethe number of samples (word-context pairs in acorpus) easily tends to billions.Observation 2.
The (uncentered) covariancematrices E[XX>] and E[Y Y>] are diagonal.This follows from our definition of the wordand context variables as one-hot encodings sinceE[XwXw?]
= 0 for w 6= w?and E[YcYc?]
= 0 forc 6= c?.With these observations and the binary definitionof (X,Y ), each entry in ?
now has a simpleclosed-form solution:?w,c=P (Xw= 1, Yc= 1)?P (Xw= 1)P (Yc= 1)(4)which can be readily estimated from a corpus.4 Using CCA for parameter estimationIn a less well-known interpretation of Eq.
(4),CCA is seen as a parameter estimation algorithmfor a language model (Stratos et al, 2014).
Thismodel is a restricted class of HMMs introduced byBrown et al (1992), henceforth called the Brownmodel.
In this section, we extend the result ofStratos et al (2014) and show that its correctnessis preserved under certain element-wise data trans-formations.4.1 Clustering under a Brown modelA Brown model is a 5-tuple (n,m, pi, t, o) forn,m ?
N and functions pi, t, o where?
[n] is a set of word types.?
[m] is a set of hidden states.?
pi(h) is the probability of generating h ?
[m]in the first position of a sequence.?
t(h?|h) is the probability of generating h??
[m] given h ?
[m].?
o(w|h) is the probability of generating w ?
[n] given h ?
[m].Importantly, the model makes the following addi-tional assumption:Assumption 4.1 (Brown assumption).
For eachword type w ?
[n], there is a unique hidden stateH(w) ?
[m] such that o(w|H(w)) > 0 ando(w|h) = 0 for all h 6= H(w).In other words, this model is an HMM in whichobservation states are partitioned by hidden states.Thus a sequence of N words w1.
.
.
wN?
[n]Nhas probability pi(H(w1))??Ni=1o(wi|H(wi))?
?N?1i=1t(H(wi+1)|H(wi)).An equivalent definition of a Brown model isgiven by organizing the parameters in matrix form.Under this definition, a Brown model has param-eters (pi, T,O) where pi ?
Rmis a vector andT ?
Rm?m, O ?
Rn?mare matrices whose en-tries are set to:pih= pi(h) h ?
[m]Th?,h= t(h?|h) h, h??
[m]Ow,h= o(w|h) h ?
[m], w ?
[n]Our main interest is in obtaining some represen-tations of word types that allow us to identify theirassociated hidden states under the model.
For thispurpose, representing a word by the correspond-ing row of O is sufficient.
To see this, note thateach row of O must have a single nonzero entryby Assumption 4.1.
Let v(w) ?
Rmbe the w-th row of O normalized to have unit 2-norm: thenv(w) = v(w?)
iffH(w) = H(w?).
See Figure 1(a)for illustration.A crucial aspect of this representational schemeis that its correctness is invariant to scaling androtation.
In particular, clustering the normalizedrows of diag(s)O?a?diag(s2)Q>where O?a?isany element-wise power of O with any a 6= 0,Q ?
Rm?mis any orthogonal transformation, ands1?
Rnand s2?
Rmare any positive vectorsyields the correct clusters under the model.
SeeFigure 1(b) for illustration.4.2 Spectral estimationThus we would like to estimate O and use its rowsfor representing word types.
But the likelihoodfunction under the Brown model is non-convex,making an MLE estimation of the model param-eters difficult.
However, the hard-clustering as-sumption (Assumption 4.1) allows for a simple128411smile grinfrowncringe11smile grinfrowncringe smilegrinfrowncringesmilegrinfrowncringe(a) (b)Figure 1: Visualization of the representational scheme under a Brown model with 2 hidden states.
(a)Normalizing the original rows of O.
(b) Normalizing the scaled and rotated rows of O.spectral method for consistent parameter estima-tion of O.To state the theorem, we define an additionalquantity.
Let ?
be the number of left/right contextwords to consider in CCA.
Let (H1, .
.
.
,HN) ?
[m]Nbe a random sequence of hidden statesdrawn from the Brown model where N ?
2?+ 1.Independently, pick a position I ?
[?+ 1, N ?
?
]uniformly at random.
Define p?i ?
Rmwherep?ih:= P (HI= h) for each h ?
[m].Theorem 4.1.
Assume p?i > 0 and rank(O) =rank(T ) = m. Assume that a Brown model(pi, T,O) generates a sequence of words.
LetX,Y ?
Rnbe one-hot encodings of words andtheir associated context words.
Let U ?
Rn?mbe the matrix of m left singular vectors of ??a?
?Rn?ncorresponding to nonzero singular valueswhere ?
is defined in Eq.
(4) and a 6= 0:?
?a?w,c=P (Xw= 1, Yc= 1)a?P (Xw= 1)aP (Yc= 1)aThen there exists an orthogonal matrix Q ?Rm?mand a positive s ?
Rmsuch that U =O?a/2?diag(s)Q>.This theorem states that the CCA projection ofwords in Section 3.3 is the rows of O up to scalingand rotation even if we raise each element of ?
inEq.
(4) to an arbitrary (nonzero) power.
The proofis a variant of the proof in Stratos et al (2014) andis given in Appendix A.4.3 Choice of data transformationGiven a corpus, the sample estimate of ?
?a?isgiven by:??
?a?w,c=#(w, c)a?#(w)a#(c)a(5)where #(w, c) denotes the co-occurrence count ofword w and context c in the corpus, #(w) :=?c#(w, c), and #(c) :=?w#(w, c).
Whatchoice of a is beneficial and why?
We use a = 1/2for the following reason: it stabilizes the varianceof the term and thereby gives a more statisticallystable solution.4.3.1 Variance stabilization for word countsThe square-root transformation is a variance-stabilizing transformation for Poisson randomvariables (Bartlett, 1936; Anscombe, 1948).
Inparticular, the square-root of a Poisson variablehas variance close to 1/4, independent of its mean.Lemma 4.1 (Bartlett (1936)).
Let X be a randomvariable with distribution Poisson(n ?
p) for anyp ?
(0, 1) and positive integer n. Define Y :=?X .
Then the variance of Y approaches 1/4 asn?
?.This transformation is relevant for word countsbecause they can be naturally modeled as Pois-son variables.
Indeed, if word counts in a corpusof length N are drawn from a multinomial distri-bution over [n] with N observations, then thesecounts have the same distribution as n indepen-dent Poisson variables (whose rate parameters arerelated to the multinomial probabilities), condi-tioned on their sum equalingN (Steel, 1953).
Em-pirically, the peaky concentration of a Poisson dis-tribution is well-suited for modeling word occur-rences.4.3.2 Variance-weighted squared-errorminimizationAt the heart of CCA is computing the SVD of the?
?a?matrix: this can be interpreted as solving thefollowing (non-convex) squared-error minimiza-tion problem:minuw,vc?Rm?w,c(??a?w,c?
u>wvc)21285But we note that minimizing unweighted squared-error objectives is generally suboptimal when thetarget values are heteroscedastic.
For instance, inlinear regression, it is well-known that a weightedleast squares estimator dominates ordinary leastsquares in terms of statistical efficiency (Aitken,1936; Lehmann and Casella, 1998).
For our set-ting, the analogous weighted least squares opti-mization is:minuw,vc?Rm?w,c1Var(??a?w,c)(??a?w,c?
u>wvc)2(6)where Var(X) := E[X2]?E[X]2.
This optimiza-tion is, unfortunately, generally intractable (Sre-bro et al, 2003).
The square-root transformation,nevertheless, obviates the variance-based weight-ing since the target values have approximately thesame variance of 1/4.5 A template for spectral methodsFigure 2 gives a generic template that encom-passes a range of spectral methods for derivingword embeddings.
All of them operate on co-occurrence counts #(w, c) and share the low-rankSVD step, but they can differ in the data transfor-mation method (t) and the definition of the matrixof scaled counts for SVD (s).We introduce two additional parameters ?, ?
?1 to account for the following details.
Mikolov etal.
(2013b) proposed smoothing the empirical con-text distribution as p??
(c) := #(c)?/?c#(c)?and found ?
= 0.75 to work well in practice.
Wealso found that setting ?
= 0.75 gave a small butconsistent improvement over setting ?
= 1.
Notethat the choice of ?
only affects methods that makeuse of the context distribution (s ?
{ppmi, cca}).The parameter ?
controls the role of singularvalues in word embeddings.
This is always 0for CCA as it does not require singular values.But for other methods, one can consider setting?
> 0 since the best-fit subspace for the rowsof ?
is given by U?.
For example, Deerwesteret al (1990) use ?
= 1 and Levy and Goldberg(2014b) use ?
= 0.5.
However, it has been foundby many (including ourselves) that setting ?
= 1yields substantially worse representations than set-ting ?
?
{0, 0.5} (Levy et al, 2015).Different combinations of these aspects repro-duce various spectral embeddings explored in theliterature.
We enumerate some meaningful combi-nations:SPECTRAL-TEMPLATEInput: word-context co-occurrence counts #(w, c), dimen-sion m, transformation method t, scaling method s, contextsmoothing exponent ?
?
1, singular value exponent ?
?
1Output: vector v(w) ?
Rmfor each word w ?
[n]Definitions: #(w) :=?c#(w, c), #(c) :=?w#(w, c),N(?)
:=?c#(c)?1.
Transform all #(w, c), #(w), and #(c):#(?)??????#(?)
if t = ?log(1 + #(?))
if t = log#(?
)2/3if t = two-thirds?#(?)
if t = sqrt2.
Scale statistics to construct a matrix ?
?
Rn?n:?w,c???????
?#(w, c) if s = ?#(w,c)#(w)if s = regmax(log#(w,c)N(?
)#(w)#(c)?, 0)if s = ppmi#(w,c)?#(w)#(c)??N(?
)N(1)if s = cca3.
Perform rank-m SVD on ?
?
U?V>where ?
=diag(?1, .
.
.
, ?m) is a diagonal matrix of ordered sin-gular values ?1?
?
?
?
?
?m?
0.4.
Define v(w) ?
Rmto be thew-th row of U?
?normal-ized to have unit 2-norm.Figure 2: A template for spectral word embeddingmethods.No scaling[t ?
{?, log, sqrt}, s = ?].
This isa commonly considered setting (e.g., in Penning-ton et al (2014)) where no scaling is applied to theco-occurrence counts.
It is however typically ac-companied with some kind of data transformation.Positive point-wise mutual information (PPMI)[t = ?, s = ppmi].
Mutual information is a pop-ular metric in many natural language tasks (Brownet al, 1992; Pantel and Lin, 2002).
In this setting,each term in the matrix for SVD is set as the point-wise mutual information between wordw and con-text c:logp?
(w, c)p?(w)p??
(c)= log#(w, c)?c#(c)?#(w)#(c)?Typically negative values are thresholded to 0 tokeep ?
sparse.
Levy and Goldberg (2014b) ob-served that the negative sampling objective of theskip-gram model of Mikolov et al (2013b) is im-plicitly factorizing a shifted version of this ma-trix.22This is not equivalent to applying SVD on this matrix,however, since the loss function is different.1286Regression[t ?
{?, sqrt}, s = reg].
An-other novelty of our work is considering a low-rank approximation of a linear regressor that pre-dicts the context from words.
Denoting the wordsample matrix by X ?
RN?nand the contextsample matrix by Y ?
RN?n, we seek U?=arg minU?Rn?n||Y ?
XU ||2whose closed-fromsolution is given by:U?= (X>X )?1X>Y (7)Thus we aim to compute a low-rank approxima-tion ofU?with SVD.
This is inspired by other pre-dictive models in the representation learning lit-erature (Ando and Zhang, 2005; Mikolov et al,2013a).
We consider applying the square-roottransformation for the same variance stabilizingeffect discussed in Section 4.3.CCA[t ?
{?, two-thirds, sqrt}, s = cca].This is the focus of our work.
As shown in The-orem 4.1, we can take the element-wise powertransformation on counts (such as the power of1, 2/3, 1/2 in this template) while preserving therepresentational meaning of word embeddings un-der the Brown model interpretation.
If there is nodata transformation (t = ?
), then we recover theoriginal spectral algorithm of Stratos et al (2014).6 Related workWe make a few remarks on related works not al-ready discussed earlier.
Dhillon et al (2011) and(2012) propose novel modifications of CCA (LR-MVL and two-step CCA) to derive word embed-dings, but do not establish any explicit connectionto learning HMM parameters or justify the square-root transformation.
Pennington et al (2014) pro-pose a weighted factorization of log-transformedco-occurrence counts, which is generally an in-tractable problem (Srebro et al, 2003).
In contrast,our method requires only efficiently computablematrix decompositions.
Finally, word embeddingshave also been used as features to improve per-formance in a variety of supervised tasks such assequence labeling (Dhillon et al, 2011; Collobertet al, 2011) and dependency parsing (Lei et al,2014; Chen and Manning, 2014).
Here, we focuson understanding word embeddings in the contextof a generative word class model, as well as in em-pirical tasks that directly evaluate the word embed-dings themselves.7 Experiments7.1 Word similarity and analogyWe first consider word similarity and analogytasks for evaluating the quality of word embed-dings.
Word similarity measures the Spearman?scorrelation coefficient between the human scoresand the embeddings?
cosine similarities for wordpairs.
Word analogy measures the accuracy onsyntactic and semantic analogy questions.
We re-fer to Levy and Goldberg (2014a) for a detaileddescription of these tasks.
We use the multiplica-tive technique of Levy and Goldberg (2014a) foranswering analogy questions.For the choice of corpus, we use a pre-processed English Wikipedia dump (http://dumps.wikimedia.org/).
The corpus con-tains around 1.4 billion words.
We only preserveword types that appear more than 100 times andreplace all others with a special symbol, resultingin a vocabulary of size around 188k.
We definecontext words to be 5 words to the left/right for allconsidered methods.We use three word similarity datasets each con-taining 353, 3000, and 2034 word pairs.3Wereport the average similarity score across thesedatasets under the label AVG-SIM.
We use twoword analogy datasets that we call SYN (8000syntactic analogy questions) and MIXED (19544syntactic and semantic analogy questions).4We implemented the template in Figure 2 inC++.5We compared against the public implemen-tation of WORD2VEC by Mikolov et al (2013b)and GLOVE by Pennington et al (2014).
Theseexternal implementations have numerous hyperpa-rameters that are not part of the core algorithm,such as random subsampling in WORD2VEC andthe word-context averaging in GLOVE.
We referto Levy et al (2015) for a discussion of the effectof these features.
In our experiments, we enableall these features with the recommended defaultsettings.We reserve a half of each dataset (by category)3WordSim-353: http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/; MEN:http://clic.cimec.unitn.it/?elia.bruni/MEN.html; Stanford Rare Word: http://www-nlp.stanford.edu/?lmthang/morphoNLM/.4http://research.microsoft.com/en-us/um/people/gzweig/Pubs/myz_naacl13_test_set.tgz; http://www.fit.vutbr.cz/?imikolov/rnnlm/word-test.v1.txt5The code is available at https://github.com/karlstratos/singular.1287Configuration 500 dimensions 1000 dimensionsTransform (t) Scale (s) AVG-SIM SYN MIXED AVG-SIM SYN MIXED?
?
0.514 31.58 28.39 0.522 29.84 32.15sqrt ?
0.656 60.77 65.84 0.646 57.46 64.97log ?
0.669 59.28 66.86 0.672 55.66 68.62?
reg 0.530 29.61 36.90 0.562 32.78 37.65sqrt reg 0.625 63.97 67.30 0.638 65.98 70.04?
ppmi 0.638 41.62 58.80 0.665 47.11 65.34sqrt cca 0.678 66.40 74.73 0.690 65.14 77.70Table 2: Performance of various spectral methods on the development portion of data.Transform (t) AVG-SIM SYN MIXED?
0.572 39.68 57.64log 0.675 55.61 69.26two-thirds 0.650 60.52 74.00sqrt 0.690 65.14 77.70Table 1: Performance of CCA (1000 dimensions)on the development portion of data with differentdata transformation methods (?
= 0.75, ?
= 0).as a held-out portion for development and use theother half for final evaluation.7.1.1 Effect of data transformation for CCAWe first look at the effect of different data trans-formations on the performance of CCA.
Table 1shows the result on the development portion with1000-dimensional embeddings.
We see that with-out any transformation, the performance can bequite bad?especially in word analogy.
But thereis a marked improvement upon transforming thedata.
Moreover, the square-root transformationgives the best result, improving the accuracy onthe two analogy datasets by 25.46% and 20.06%in absolute magnitude.
This aligns with the dis-cussion in Section 4.3.7.1.2 Comparison among different spectralembeddingsNext, we look at the performance of various com-binations in the template in Figure 2.
We smooththe context distribution with ?
= 0.75 for PPMIand CCA.
We use ?
= 0.5 for PPMI (which hasa minor improvement over ?
= 0) and ?
= 0 forall other methods.
We generally find that using?
= 0 is critical to obtaining good performancefor s ?
{?, reg}.Table 2 shows the result on the developmentportion for both 500 and 1000 dimensions.
Evenwithout any scaling, SVD performs reasonablywell with the square-root and log transformations.The regression scaling performs very poorly with-out data transformation, but once the square-roottransformation is applied it performs quite well(especially in analogy questions).
The PPMI scal-ing achieves good performance in word similaritybut not in word analogy.
The CCA scaling, com-bined with the square-root transformation, givesthe best overall performance.
In particular, it per-forms better than all other methods in mixed anal-ogy questions by a significant margin.7.1.3 Comparison with other embeddingmethodsWe compare spectral embedding methods againstWORD2VEC and GLOVE on the test portion.
Weuse the following combinations based on their per-formance on the development portion:?
LOG: log transform, ?
scaling?
REG: sqrt transform, reg scaling?
PPMI: ?
transform, ppmi scaling?
CCA: sqrt transform, cca scalingFor WORD2VEC, there are two model options:continuous bag-of-words (CBOW) and skip-gram(SKIP).
Table 3 shows the result for both 500 and1000 dimensions.In word similarity, spectral methods generallyexcel, with CCA consistently performing the best.SKIP is the only external package that performscomparably, with GLOVE and CBOW falling be-hind.
In word analogy, REG and CCA are signifi-cantly better than other spectral methods.
They arealso competitive to GLOVE and CBOW, but SKIPdoes perform the best among all compared meth-ods on (especially syntactic) analogy questions.1288Method 500 dimensions 1000 dimensionsAVG-SIM SYN MIXED AVG-SIM SYN MIXEDSpectral LOG 0.652 59.52 67.27 0.635 56.53 68.67REG 0.602 65.51 67.88 0.609 66.47 70.48PPMI 0.628 43.81 58.38 0.637 48.99 63.82CCA 0.655 68.38 74.17 0.650 66.08 76.38Others GLOVE 0.576 68.30 78.08 0.586 67.40 78.73CBOW 0.597 75.79 73.60 0.509 70.97 60.12SKIP 0.642 81.08 78.73 0.641 79.98 83.35Table 3: Performance of different word embedding methods on the test portion of data.
See the main textfor the configuration details of spectral methods.7.2 As features in a supervised taskFinally, we use word embeddings as features inNER and compare the subsequent improvementsbetween various embedding methods.
The ex-perimental setting is identical to that of Stratoset al (2014).
We use the Reuters RCV1 cor-pus which contains 205 million words.
With fre-quency thresholding, we end up with a vocabu-lary of size around 301k.
We derive LOG, REG,PPMI, and CCA embeddings as described in Sec-tion 7.1.3, and GLOVE, CBOW, and SKIP em-beddings again with the recommended default set-tings.
The number of left/right contexts is 2 for allmethods.
For comparison, we also derived 1000Brown clusters (BROWN) on the same vocabu-lary and used the resulting bit strings as features(Brown et al, 1992).Table 4 shows the result for both 30 and 50 di-mensions.
In general, using any of these lexicalfeatures provides substantial improvements overthe baseline.6In particular, the 30-dimensionalCCA embeddings improve the F1 score by 2.84on the development portion and by 4.88 on thetest portion.
All spectral methods perform com-petitively with external packages, with CCA andSKIP consistently delivering the biggest improve-ments on the development portion.8 ConclusionIn this work, we revisited SVD-based methodsfor inducing word embeddings.
We examineda framework provided by CCA and showed thatthe resulting word embeddings can be viewed ascluster-revealing parameters of a certain modeland that this result is robust to data transformation.6We mention that the well-known dev/test discrepancy inthe CoNLL 2003 dataset makes the results on the test portionless reliable.Features 30 dimensions 50 dimensionsDev Test Dev Test?
90.04 84.40 90.04 84.40BROWN 92.49 88.75 92.49 88.75LOG 92.27 88.87 92.91 89.67REG 92.51 88.08 92.73 88.88PPMI 92.25 89.27 92.53 89.37CCA 92.88 89.28 92.94 89.01GLOVE 91.49 87.16 91.58 86.80CBOW 92.44 88.34 92.83 89.21SKIP 92.63 88.78 93.11 89.32Table 4: NER F1 scores when word embeddingsare added as features to the baseline (?
).Our proposed method gives the best result amongspectral methods and is competitive to other pop-ular word embedding techniques.This work suggests many directions for fu-ture work.
Past spectral methods that involvedCCA without data transformation (e.g., Cohen etal.
(2013)) may be revisited with the square-roottransformation.
Using CCA to induce representa-tions other than word embeddings is another im-portant future work.
It would also be interestingto formally investigate the theoretical merits andalgorithmic possibility of solving the variance-weighted objective in Eq.
(6).
Even though theobjective is hard to optimize in the worst case, itmay be tractable under natural conditions.AcknowledgmentsWe thank Omer Levy, Yoav Goldberg, and DavidBelanger for helpful discussions.
This workwas made possible by a research grant fromBloomberg?s Knowledge Engineering team.1289A Proof of Theorem 4.1We first define some random variables.
Let ?
bethe number of left/right context words to considerin CCA.
Let (W1, .
.
.
,WN) ?
[n]Nbe a randomsequence of words drawn from the Brown modelwhere N ?
2?
+ 1, along with the correspond-ing sequence of hidden states (H1, .
.
.
,HN) ?[m]N.
Independently, pick a position I ?
[?
+1, N ?
?]
uniformly at random; pick an integerJ ?
[?
?, ?
]\{0} uniformly at random.
DefineB ?
Rn?n, u, v ?
Rn, p?i ?
Rm, and?T ?
Rm?mas follows:Bw,c:= P (WI= w,WI+J= c) ?w, c ?
[n]uw:= P (WI= w) ?w ?
[n]vc:= P (WI+J= c) ?c ?
[n]p?ih:= P (HI= h) ?h ?
[m]?Th?,h:= P (HI+J= h?|HI= h) ?h, h??
[m]First, we show that ?
?a?has a particular structureunder the Brown assumption.
For the choice ofpositive vector s ?
Rmin the theorem, we definesh:= (?wo(w|h)a)?1/2for all h ?
[m].Lemma A.1.
?
?a?= A?>where ?
?
Rn?mhasrank m and A ?
Rn?mis defined as:A := diag(Op?i)?a/2O?a?diag(p?i)a/2diag(s)Proof.
Let?O := O?T .
It can be algebraicallyverified that B = Odiag(p?i)?O>, u = Op?i, andv =?Op?i.
By Assumption 4.1, each entry of B?a?has the formB?a?w,c=???h?[m]Ow,h?
p?ih??Oc,h??a=(Ow,H(w)?
p?iH(w)?
?Oc,H(w))a= Oaw,H(w)?
p?iaH(w)??Oac,H(w)=?h?[m]Oaw,h?
p?iah?
?Oac,hThus B?a?= O?a?diag(p?i)a(?O?a?)>.
Therefore,?
?a?=(diag(u)?1/2Bdiag(v)?1/2)?a?= diag(u)?a/2B?a?diag(v)?a/2= diag(Op?i)?a/2O?a?diag(p?i)a/2diag(s)diag(s)?1diag(p?i)a/2(?O?a?
)>diag(?Op?i)?a/2This gives the desired result.Next, we show that the left component of ?
?a?is in fact the emission matrix O up to (nonzero)scaling and is furthermore orthonormal.Lemma A.2.
The matrix A in Lemma A.1 has theexpression A = O?a/2?diag(s) and has orthonor-mal columns.Proof.
By Assumption 4.1, each entry ofA is sim-plified as follows:Aw,h=o(w|h)a?
p?ia/2h?
sho(w|H(w))a/2?
p?ia/2H(w)= o(w|h)a/2?
shThis proves the first part of the lemma.
Note that:[A>A]h,h?={s2h?
?wo(w|h)aif h = h?0 otherwiseThus our choice of s gives A>A = Im?m.Proof of Theorem 4.1.
With Lemma A.1 and A.2,the proof is similar to the proof of Theorem 5.1 inStratos et al (2014).ReferencesAlexander C Aitken.
1936.
On least squares and lin-ear combination of observations.
Proceedings of theRoyal Society of Edinburgh, 55:42?48.Rie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multipletasks and unlabeled data.
The Journal of MachineLearning Research, 6:1817?1853.Francis J Anscombe.
1948.
The transformationof poisson, binomial and negative-binomial data.Biometrika, pages 246?254.MSo Bartlett.
1936.
The square root transformation inanalysis of variance.
Supplement to the Journal ofthe Royal Statistical Society, pages 68?78.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?479.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the Empirical Methods inNatural Language Processing, pages 740?750.Shay B Cohen, Karl Stratos, Michael Collins, Dean PFoster, and Lyle H Ungar.
2013.
Experiments withspectral learning of latent-variable pcfgs.
In Pro-ceedings of the North American Chapter of the As-sociation of Computational Linguistics, pages 148?157.1290Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viacca.
In Proceedings of the Advances in Neural In-formation Processing Systems, pages 199?207.Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,and Lyle H. Ungar.
2012.
Two step cca: Anew spectral method for estimating vector modelsof words.
In Proceedings of the International Con-ference on Machine learning.David Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis:An overview with application to learning methods.Neural Computation, 16(12):2639?2664.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Erich Leo Lehmann and George Casella.
1998.
Theoryof point estimation, volume 31.
Springer Science &Business Media.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the As-sociation for Computational Linguistics, volume 1,pages 1381?1391.Omer Levy and Yoav Goldberg.
2014a.
Linguistic reg-ularities in sparse and explicit word representations.In Proceedings of the Computational Natural Lan-guage Learning, page 171.Omer Levy and Yoav Goldberg.
2014b.
Neural wordembedding as implicit matrix factorization.
In Pro-ceedings of the Advances in Neural Information Pro-cessing Systems, pages 2177?2185.Omer Levy, Yoav Goldberg, Ido Dagan, and IsraelRamat-Gan.
2015.
Improving distributional simi-larity with lessons learned from word embeddings.Transactions of the Association for ComputationalLinguistics, 3.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of the Advances in Neural Infor-mation Processing Systems, pages 3111?3119.Patrick Pantel and Dekang Lin.
2002.
Discover-ing word senses from text.
In Proceedings of theACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 613?619.ACM.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors forword representation.
In Proceedings of the Empiri-cial Methods in Natural Language Processing, vol-ume 12.Nathan Srebro, Tommi Jaakkola, et al 2003.
Weightedlow-rank approximations.
In Proceedings of the In-ternational Conference on Machine learning, vol-ume 3, pages 720?727.Robert G. D. Steel.
1953.
Relation between pois-son and multinomial distributions.
Technical ReportBU-39-M, Cornell University.Karl Stratos, Do-kyum Kim, Michael Collins, andDaniel Hsu.
2014.
A spectral algorithm for learn-ing class-based n-gram models of natural language.In Proceedings of the Association for Uncertainty inArtificial Intelligence.1291
