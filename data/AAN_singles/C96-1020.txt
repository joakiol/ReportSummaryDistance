Beyond Skeleton Parsing: Producing a ComprehensiveLarge-Scale General-English Treebank With FullGrammatical AnalysisEzra Black, Stephen EubankHideki Kashioka, David Magerman*AT\]R, Interpret ing Telecommunicat ionsLaboratories2-2 Hikaridai, Seika-choSoraku-gun, Kyoto,  Japan 619-02Roger Gars ide ,  Geof f rey  LeechI)epts of Comput ingand l, inguisticsUniversity of Lancaster,Bailrigg, Lancaster LA1 4YT, UKrgg?comp, lanes ,  ac .
uk{black,eubank,kashioka}@atr.itl.co.jp G.LeechOcentl.lancs.ac.uk1 In t roduct ionA treebank is a body of natural language textwhich has been grammatically annotated by hand,in terms of some previously-established scheme ofgrammatical nalysis.
Treebanks have been usedwithin the field of natural anguage processing asa source of training data for statistical part ogspeech taggers (Black et al, 1992; Brill, 1994;Merialdo, 1994; Weischedel et al, 1993) and forstatistical parsers (Black et al, 1993; Brill, 1993;aelinek et al, 1994; Magerman, 1995; Magermanand Marcus, 1991).In this article, we present he AT'R/Lancaster7'reebauk of American English, a new resourcetbr natural-language-, processing research, whichhas been prepared by Lancaster University (UK)'sUnit for Computer Research on the English Lan-guage, according to specifications provided byATR (Japan)'s Statistical Parsing Group.
Firstwe provide a "static" description, with (a) a dis-cussion of the mode of selection and initial pro-cessing of text for inclusion in the treebank, and(b) an explanation of the scheme of grammaticalannotation we then apply to the text.
Sec.ond, wesupply a "process" description of the treebank, inwhich we detail the physical and computationalmechanisms by which we have created it.
Finally,we lay out plans for the further development ofthis new treebank.All of the features of the ATR/Lancaster Tree-bank that are described below represent a radi-cal departure from extant large-scale (Eyes andLeech, 1993; Garside and McEnery, 1993; Marcuset al, 1993) treebanks.
We have chosen in this ar-ticle to present our treebank in some detail, ratherthan to compare and contrast it with other tree-banks.
But the major differences between this andearlier treebanks can easily be grasped via a corn-*Current alfiliation: Renaissance TechnologiesCorp., 25 East Loop Road, Suite 211, StonyBrook, NY 11776 USA; Consultant, ATR Interpret-ing Telecommunications Laboratories, 3-12/94parison of the descriptions below with those of thesources just, cited.2 Genera l  Descr ip t ion  o f  theTreebank2.1 Document Selection andPreprocessingThe ATR/Lancaster Treebank consists of approx-imately 730,000 words of grammatically-analyzedtext divided into roughly 950 documents rangingin length ffmn about 30 to about 3600 words.The idea informing the selection of documentsfor inclusion in this new treebank was to packinto it the maximum degree of document variationalong many different scales---document length,subject area, style, point of view, etc.
-but with-out establishing a single, predetermined classifica-tion of the included documentsJ Differing pur-poses for which the treebank might be utilizedmay favor differing groupings or classifications ofits component documents.
Overall., the rationalefor seeking to take as broad as possible asample ofcurrent standard American English, is to supportthe parsing and tagging of unconstrained Amer-ican English text by providing a training corpuswhich includes documents fairly similar to almostany input which might arise.Documents were obtained from three sources:the Internet; optically-scanned hardcopy "occa-sional" documents (restaurant take out menus;flmdraising letters; utility bills); and purchasefrom commercial or academic vendors.
To illus-trate the diverse nature of the documents includedin this treebank, we list, in Table 1, titles of ninetypical documents.In general, and as one might expect, the doc-uments we have used were written in the earlyto mid 1990s, in the United States, in "Standard"American English.
However, there are fairly many1as was done, by contrast, in the Brown Corpus(Kucer~t and Francis, 1967).107Empire Szechuan Flier (Chinese take out food)Catalog of Guitar DealerUN Chart, er: Chapters 1 5Airplane Exit-Row Seating: Passenger Information SheetBicycles: How 31'<) TrackstandGow~'rnment: US Goals at G7Shoe Store Sale FlierHair Loss t{,ernedy BrochureCancer: Ewing's Sarcoma Patient Infbrmation'Fable 1: Nine Typical Docnments From A'ft{/I,an<:aster T eebankexceptions: documents written by Captain JohnSmith of Plymouth Plantation (1600s), by Ben-jamin Franklin (1700s), by Americans writing inperiods throughout he 1800s and 1900s; docu-ments written in Australian, British, Canadian,and Indian English; and docnments featuring a.range of dialects and regional wtrieties of cur=rent American English.
A smattering of suchdocuments is included because within standardEnglish, these linguistic varieties are sometimesquoted or otherwise utilized, and so they shouldbe represented.As noted abow=', each document within the trek-bank is classified along many different axes, in or-der to support a large variety of different taskspecific groupings of the documents.
Each docu-ment is classifed according to tone, style, linguisticlevel, point of view, physical description of doc-ument, geographical background of author, etc.Sample values for these attributes are: "friendly","dense", "literary", %echnical", "how-to guide",and "American South", respectively.
To conveydomain information, one or more Dewey DecimalSystem three digit classifiers are associated witheach document.
For instance, for the cv o\[' a f>hys -iologist, Dewey 612 and 616 (Medical Sciences:\] lumen Physiology; Diseases) were chosen.
Ona more mundane, "bookkeeping" level, values fortext title, author, publication date, text source,etc.
are recorded as well.An SGML like markup language is used to cap-lure a variety of organizational level facts abouteach document, such as LIST structure; T ITLEsand CAPTIONs; and even more recondite ventssuch as POEM and IMAGE.
HIGltLl(?,II'\]'ing ofwords and phrases is recorded, along with the w~-riety of highlighting: italics, boldface, large font,e~c.
Spelling errors and, where essential, other ty-pographical lapses, are scrupulously recorded andthen corrected.Tokenization (i.e.
word splitting: Edward's- -+ Edward 's )  and sentence spli~ting (e.g.
tiesaid, "Hi there.
Long time no see."
~ (Sen-tence.l:) Be said, (Sentence.2:) "Hi there.
(Sen-tence.3:) Long time no see.")
are performed byhand according to predetermined policies.
Hencethe treebank provides the resource of multifariouscorrect instances of word and sentence sI>litting.2.2 Scheme of Grammat ica l  Annotat iontlcretofore, all existing large =scale treebanks haveemployed the gra.nmnatical nalysis technique ofskeleton parsin(\] (Eyes and Leech, 1993; Garsideand McEnery, 1993; Marcus et el., 1993), 2 inwhich only a partial, relatively sket<'hy, grammat-ical analysis of each sentence in the treebank isprovided, a In contrast, the AT\[g/Lancaster T ee-bank assigns to each of its sentences a full and(:omplete grammatical analysis with respect o avery detailed, very comprehensive broad coveragegrammar of English.
Moreover, a very large,highly del;ailed part of speech tagset is used tolabel each word of each sentence with its syntac-tic a~,d semantic ategories.
The result is an ex-tremely specific and informative syntactic and se-mantic diagram of every sentence in the treebank.This shift fi'om skeleton parsing based tree-banks to a treebank providing flfll, detailed gram-matical analysis resolves a set of problems, de-tailed in (Black, 1994), involved in using skeletonparsing based treebanks as a means of initializ-ing training statistics for probabilistic grammars(Black et el., 1993).
Briefly, the tirst of these prob-lems, which applies even where the grammar be-ing trained has been induced from the trainingtreebank (Sherman el; al., 1990), is thai; the syn-tactic sketchiness of a skeleton ~parsed treebankleads a statistical training algorithm to overcount,in some circumstances, and in other cases to un-~The 1995 release Penn Treebank adds flmctionMintormation to some nonterminals (Marcus et al,1994), but with its rudimentary (roughly 45 tag)tagset, its non detailed internal analysis of noun con>pounds and NPs more generally, its lack of seman-tic categorization of words and phrases, etc., it ar-guably remains a skeleton parsed treebank, albeit anenriched one.aA ditfercnt kind of partial parse- crucially,one generated automatically and not by hand-characterizes the "treebank" produced by processingthe 200 million word Birmingham \[?
'niversity (UK)Bank of-English text corpus with the dependencygrammar-based ENGCG lfelsinki Parser (Karlsson etel., 11995).108dercotlnl, instances of rule firings it+ trainil,g data(treel)a, nk) pars(s, and thus 1,o incorrecl,ly esti-niatc rtth> probal)ilitics.
The second I>rol>leut isthat where the gramniar  being l, raino(l is moredetailed syntact ical ly (,hail I,\[le sl,:ehq.Oli parsingbased trainiilg I, reelm.nk, the training corptts radi-(:ally tltl(1Cq'l)el'\['orlllS ill il,s crucial job of speci\['yiilgcorrect parses For training i>url)osrs (l+lack+ 1!
)gd).It) addit ion to resolvhig gramtna,r t, rahlingpro l ) len: ls ,  our Trocl)atik l-ir<-ivides a tneatis o\['training non grmmnar  based parsi.,g t.Iroc(>dures(Brill, 1993; Jel inek cta l .
,  t994; Mag;erlnalt, 1995)at, new, higher l('v<'ls of gI'~Lttltll;/t,i('al detail and(~Oll i) l+C h('qD-iiv(~l icss.
'l'r('eIH/,llk S(?lltCll('(~s ;a.r(~ \[-iarso(I in {CI'IIIS O\[' i,\]1("/1 7'/~ I?
?Lqlish (Trammar, whose charn('lerist,ics w<'will bri<qly d(>scribe.
'Fhe ( ;Nmmlar 's  t)a.rt of SF,<>(;c\]l <.aSset r('s(qJ>I)les llie 17!)
Lag (',laws I, agsot (l<w<.l(.Il)(~<l I>ylY(:I{I,;1, (Eyes and I,cc<'\[i, 19!
);l): bul with tiullpr-'.
)us maj<)r a.,l<l nlinor di\[f'(!r<mc<!s.
()no ntajor d i ft'er<~nc(~, for inst,a.c<:, is <,hal, I,he ATI I .
lags('< (:al>-lures the (lifft~r<',c(' t>etwc'(m e.g.
<<wall <:ov<wi,f~wh('r(> +'('ov(~ril~+g" is a l(~xica\]izc<t ,,(>ul/ cry<lifts in-inS, and %1,<" cov('ring o\[' all l-i('l.s", wh<'r<' +'('ovcr-i .g" is a verbal llOtlll.
In (: laws pl+;ICl, iCe~ I>ot.h arcNNI  (s i l lgular  conmlon n,:)u.).
The A'I ' I{ i .gselinnovatx~s the lag 1.y\[>c NV\ ' ( ;  for verl)a\] nouns.Anoth<:r n : t jor  difl'<'x<'tw<> is t.h<: (ling\[(,.l) us(.
<)\["sul)('atcgorizati(>n', (".g.
VI)t+~I,()B,I for (Io,ibl<.ol)jecl, v(>rt)s (t<mch Bill l,atin, el(+),Each v<+rl>, tl(-illt\], a(l j(~ctiv<, a .< l  adverb it,the A' I 'R tagsct includes a se.
ianl ic  I:~b<'\],('hos('.. from 42 n<)uli/a<lj<:ctiv<ja<lv<.
'l) <';at.<>gorics an<l 2+) ver l ) /vert )a l  (t;l+l.
(+gjI'(-il'i<>S, S<>tll('ov(+,r\]ap ('xist, ing b(~lwe('n t, hes(' cat<'gory se/,s.
'| 'hese s<>tu.~l.nl.i(: (:al;(>go|+i('s are in\[.(~.
(i('({ fbr (*,'+9"%'la~+dard American lCngli.sD" Icml, i .
a~\]  do-m, ai~.
Sani\])l(~ <;al,(~gories in(hid(': +'physical.a.l, tt: ib,m?
+ (t,o,,t,s/adjectiv<~s/adv(~rbs), "ai=ter" (verbs/vcrl~als), and "intcrp(+rsonal.;,ct.
"( ,,out,s / a<lj<+cl:iv<~s / adv<,rl>s / v(+rhs /v<..I;als ) .
'lh<>ywcrc d<weh-ip<'d by the A' I 'R grau lmar ia ,  a,,d the .prov<'.n and r(~li.(.
(I via <lay in <lay ottl laggingfor six mouths at ATI{, by two huu,u l  "+lrccbankers",  l\[len \['or \['our ttlonl, hs at, I,nticast,er I)yfive l,r(>el)auk<~rs, with  dai ly  int,eract,ions ; l l l lO l igI,r<~el)aukcrs, and I-iei:~w~en i h<" l.r('el>ank(q's and i;h(,ATR g~ra li i I i tar ia l i .\[1' we ignore 1,he seltiaall.ic F, or l ion  o\[" A'I'I~ lags,l l i e  t/-tgsel, cont.ains 165 (tifl'(q'ent l.ags.
In('lu(Ibig the S('liia.iil, ic cai,egories iii the tag.
',, i.\[lere arerougl l ly  2700 i,ags.
As is l, li<> ('as(~ in I\]le ( l l ; twst.at~sct.
, so (:ailed "ditl.<)Lags" (:;Ill I)C ('r(>nl.<'(l I)ase(lOil a\[ni<)sl.
; i l iy  l,;I..~ of  t.hc lagset, 17)r t h<' l)urll(-is~'o1' lal>ellhig tiiul\[.iwor<l (,Xl) l '(>ssioi ls.
\["or i l/Sl;il lCO,"wi l l  o' the wisp" is lal>clle<i ;is a ,4 word s i \ ] lgt l larCOililtiOil liC-illD. '
l ' l i i s  p rocess  <:;tl\] ad<l <'olisidcral>lcIiIlll-il-iers 0\[' Lags (,o I,}l(!
ahoy(" tot, als.,<"Jelil,ell(;c~s ill I, tie Tre<>lm.nk ar(~ l>ars(xl  wi l ; l iresl-iecl, t,o the A7'I7 lgnglish (Tran+mar.
The( ' , rammar,  a feature based context fl:cc phrasest.ructtJre gratmnar,  is related to the IBM t:,nglish(h 'ammar  as l>ul)lished in (Blac.k c ta l .
,  1993), butdiffers tuorc: l'rolN, the IBM (h'at imlar than ourl, agset does t'roln the (',laws tagsel,.
For instance,the notion of "numntol\]iC" has no al>plicatioi~ tothe ATI{ (~rallll l lar; l, ho ATR Gratl\]l\[lal" has 67features and I \ ]O0 rules, whereas the IBM (~ram+mar had 40 \[>al, tn'es aud '750 rules, c:t+c.'
l 'he i>reciscly corrcc.t parse (;~s pre st+ecificd bya human "trecbanker")  figures among the parsesI-iroduced for any given sentence by tile A' I 'R(~ralnlnar, roughly !
)0% of the time, \['or l;ext, o1'the unconsl,rained, wide open SOl'l, that, tim Tree-DaHk ix <'onil>OSCd of.
The job of the treebattkers il.o local.<' this exact; l>arse, \['or each s(mt.<m(:e, andadd it to t lp  'l'recl>ar~k.\["i~tlre l shows |,wo Salll\[:.\[(+ parsc({ SOlil,ellC+c+s/'rolll l.he A'l l 't Treebank (aud originally l'rom a(thitwse I,ake oul, fOOd \[li(>r).
l~ecatlse il, is inf'or-nial,ive t,o know which of the 1 lO0 rtlles is used nta givou lr(,o no(h'+ and sitice the part icular "tlon-lernlinal <'at, egory" associated wilh any .lode ofllw l, ree is alwa.ys rccoveralfle, 4 nodos are labelledwith ATIL  (~ratnnlar ule nantes rat l>t + t.ha,, as islll()l+e tlSllal+ with llOltl.
(~rlttillal iHUllCS.3 Producing tile TreebankIll ibis I>art of t.\]le article, we Liirll t'rolll "what," t,o" l , )w ' ,  a ,d  discuss the nlcchaiiistns by which theA' 1' I{ / I,ail('asl er ' Fr<'etm.ul~ was produced+3.1 The  So f tware  I lackbone  GWBToo I :A Trecbanker ' s  Works ta t ion(a\?B'l 'ool is n Mol,if based X-Windows appli(:a-ti(>,t which allows the treel>anker to int.era{:t withthe A'I'I{ l%glish Gra.mmar in order to produce\[,he lltOsl accllrate t,reel:.alik ill {,he shorl.csL amotuH,o\[' t.illl<'.The i,reebauking process begins ill the Treel>ankI:,dh,or sol'ten oF the treel>anker's worksta, tion witha list o\[ + scnl,enccs lagged wii,h pat'l, ol'-st>eech cal.egorics.
'l 'he 1,1'~>N)a.nkc'r s h;cl,s a SCtll;eltcc.
\['rOtll thelist, for proccsshig.
Next, with tit<+ cli<'k o1" a bttl:.lot<, t ho Tl 'eNmnk l'Rlitor graphical ly displays tit<>l>arse \[bt'<>st+ \[br 1.he s<:lll,ellC(' ill ~1 IlIC,/ISO-SCI\]SiI, iVCI 'arse Tr<+r w i ,dow (Figure 2).
Each node disi)\]ay<'(l t'el+r(+s(>nts a const, ituel+lt, in l,h<+ parse \['()rest.A shaded cons<it<tent o<h: itidicates that there ar<:all ernatiw+ alialyses of' thai, constii,uent, only <)it<!ol  + which is displayed, lay clicking t, lic right+ niottscl)ul3on Oll ;I shaded node, t.hc: l.r<~cbanl,:cr an dis-play a pOl)Ul> nicnu listhlg tho all,el'nat\]w+ analy-s('s, atly of which <:atl b(> disl>laycd l>y s<tecl,hlg t,heal)l>ropriat,e ill(Hill il.etU.
(',lickhig i, he h>f't ntouseI)tll,l,Oll (.Ill ~-t COIISl,iI, IICII{, l\]OdC pOpS tip a Willdowlisl,htg tho fea,t, llt'e wthles for l, ha, l, const, it+llCnL.411, is contained in the rule t/;i,tne it, stir.109<S id="39" count=8><HIGH r endit ion="it alic">\[start \[quo ( _ (  \[sprpd23 \[sprime2 \[ibbar2 Jr2 Please_RRCONCESSIVE r2\] ibbar2\]\[sc3 \[v4 Mention_VVIVERBAL-ACT \[nbar4 \[dl this_DDl dl\]\[nla coupon_NNIDOCUMENT nla\] nbar4\] \[fal when_CSWHEN\[vl ordering_VVGINTER-ACT vl\] fall v4\] sc3\] sprime2\] sprpd23\] )_) quo\] start\]</HIGH></S><S id="48" count=S><HIGH rendition ="large">\[start \[sprpd22 \[coord3 \[cc3 \[ccl OI~_CCOR ccl\] cc3\]\[nbarl3 \[d3 ONE_MCIWORD d3\] \[jl FREE_JJSTATUS j l\] \[n4 \[nla FANTAIL_NNIANIMAL nla\]\[nla SHRIMPS_NNIFOOD nla\] n4\] nbarl3\] coord3\] sprpd22\] start\]</HIGH></S>Figure 1: Two ATR/Lancaster Treebank Sentences (8 words, italicized; 5 words, large font) from ChineseTake-Out Food FlierV~: i vbar2  ~prl.olJ .IAA,,lsuBsT,, EIApos  = vbarnum = twon_sem = substancenumber = V5tense_aspect  = presv sem = sendv type = main_verbvp_type = auxFigure 2: The GWBTool Treebanker's Workstation Parse Window display, showing the parse forest foran example sentence.
On the far right, the feature values of the VBAR2 constituent, indicating thatthe constituent is an auxiliary verb phrase (bar level 2) containing a present-tense v rb phrase withnoun semantics SUBSTANCE and verb semantics SEND.
The fact that the number feature is variable(NUMBER=V5) indicates that the number of the verb phrase is not specified by the sentence.
Theshaded nodes indicate where there are alternative parses.i i0The Treebank Editor also displays the numberof parses in the parse forest.
If the parse forestis unmanageably arge, the treebanker can par-tially bracket the sentence and, again with theclick of a button, see the parse forest containingonly those parses which are consistent with thepartial bracketing (i.e.
which do not have anyconstituents which violate the constituent bound-aries in the partial bracketing).
Note that thetreebanker need not specify any labels in the par-tial bracketing, only constituent boundaries.
Theprocess described above is repeated until the tree-banker can narrow the parse forest down to a sin-gle correct parse.
Crucially, for experienced Lan-caster treebankers, the number of such iterationsis, by now, normally none or one.3.2 Two-Stage Par t -Of -Speech TaggingPart-of-speech tags are assigned in a two-stageprocess: (a) one or more potential tags are as-signed automatically using the Claws HMM tag-ger (?
); (b) the tags are corrected by a treebankerusing a special-purpose X-windows-based editor,Xanthippe.
This displays a text segment and, tbreach word contained therein, a ranked list of sug-gested tags.
The analyst can choose among thesetags or, by clicking on a panel of all possible tags,insert a tag not in the ranked list.The automatic tagger inserts only the syntacticpart of the tag.
To insert the semantic part of thetag, Xanthippe presents a panel representing allpossible semantic continuations of the syntacticpart of the tag selected.
'lbkenization, sentence-splitting, and speltchecking are carried out according to rule by thetreebankers themselves ( ee 2.1 above).
However,the Claws tagger performs basic and preliminarytokenization and sentence-splitting, for optionalcorrection using the Xanthippe ditor.
Xanthipperetains control at all times during the tag correc-tion process, for instance allowing the insertiononly of tags valid according to the ATR.
Gram-mar .3.3 The Annotat ion ProcessInitially a file consists of a header detailing thefile name, text title, author, etc., and the textitself, which may be in a variety of formats; it; may('ontain HTML mark-up, and files vary in the wayin which, for example, emphasis is represented.The first stage of processing is a scan of the text toestablish its format and, for large files, to delimita sample to be annotated.The second stage is the insertion of SGMLlike mark-up.
As with the tagging I)rocess, thisis done by an automatic procedure with manualcorrection, using microemacs with a special set ofn lacros .Third, the tagging process described in section3.2 is carried out.
The tagged text is then ex-tracted into a file for parsing via GWBTool (See3.1.1).The final stage is merging the parsed and taggedtext with all the annotation (SGML-like mark-up, header information) for return to ATR.3.4 Staff Training; Output AccuracyEven though all Treebank parses are guaranteedto be acceptable to the ATR Grammar, insuringconsistency and accuracy of output has requiredconsiderable planning and effort.
Of all the parsesoutput for a sentence being treebanked, only asmall subset are appropriate choices, given thesentence's meaning in the document in which itoccurs.
The five Lancaster treebankers had to un-dergo extensive training over a long period, to un-derstand the manifold devices of the ATR Gram-mar expertly enough to make the requisite choices.This training was affected in three ways: a weekof classroom training was followed by four monthsof daily email interaction between the treebankersand the creator of tile ATR Grammar; and oncethis training period ended, daily Lancaster/ATRemail interaction continued, as well as constantconsultation among the treebankers themselves.A body of documentation a d lore was developedand frequently referred to, concerning how all se-mantic and certain syntactic aspects of the tagset,as well as various grammar ules, are to be appliedand interpreted.
(This material is organized viaa menu system, and updated at least weekly.)
Asearchable version of files annotated to date, anda list of past tagging decisions, ordered by wordand by tag, are at the treebankers' disposal.In addition to tile constant dialogue betweenthe treebankers and the ATR grammarian, Lan-caster output was sampled periodically at ATR,hand-corrected, and sent back to the treebankers.In this way, quality control, determination of out-put accuracy, and consistency control were han-dled conjointly via the twin methods of samplecorrection and constant reebanker/grammariandialogue.With regard both to accuracy and consistencyof output analyses, individual treebanker abilitiesclustered in a fortunate manner.
Scoring of thou-sands of words of sample data over time revealedthat three of the five treebankers had parsing errorrates (percentage of sentences parsed incorrectly)of 7%, 10%, and 14% respectively, while the othertwo treebankers' error rates were 30% and 36%respectively.
Tagging error rates (percentage ofall tags that were incorrect), similarly, were 2.3%,1.7%, 4.0%, 7.3% and 3.6%.
Expected parsing er-ror rate worked out to 11.9% for the first three,but 32.0% for the other two treebankers; whileexpected tagging error rates were 2.9% and 6.1%respectively.
55Almost all t~gging errors were semantic.iiiWhat is fortnnate about this clustering of abil-it, ies is that the less able treebankers were alsomuch less prolific than the others, producing only25% of the total treel)ank.
Therefore, we areprovisionally excluding this 25% of the treebank(about 180,000 words) fi'om use fbr parser train-ing, though we are experimeating with the use ofthe entire treebank (expected tagging error rate:3.9%) for tagger training.
Finally, parsing andtagging consistency among the first, three tree-bankers appears high.4 Conc lus ionWithin the next two years, we intend to produceVersion 2 o\[' our Treebank, in which the 25% ofthe treebank that is currently suitable for t, rainiugtaggers but not parsers, is fully corrected/~Over the next several years, the A'\['R/LancasterTreebank of American English will form the ba-sis for the research of A'l'l{'s Statistical ParsingGroup in statistical parsing, part of speech tag-ging, and related fields.Re ferencesE.
Black, F. Jelinek, J. Lai\['e, rty, 1{,.
Mercer, S.l{oukos.
1992. l)ecision tree models applied tothe labelling of text with parts of speech.
InProceedings, DARPA Speech a~d Natural Lan-.quagc Workshop, Arden llouse, Morgan Kauf-man Publishers.E.
Black, IL Ga.rside, and G. l,eech, Editors.1993.
Statistically Dr'iron Computer Gram-mars Of l';nglish: The IBM/Lanca,sler Ap-proach, l{.odopi Editions.
Amsterdam.E.
I~lack.
199d.
An experiment in customizing theLancaster Treebank.
In Oostdijk and de Ilaan,1 !
)9/1, pages 159-168.E.
Brill.
1993.
A utomatic grammar induclion andparsing fl'ee text: A fransf'ormation I)ased ap-proach.
In Proceedi'ngs, 31sl Annual Mceli'og oJlk<- Associalio~ for (/o'mpulalional Linguislics,Association for Computational Linguistics.E.
Brill.
1994.
Some Advances inTransR)rmation Based Part of Speech Tagging.In Proceedings of lkc Twelfll~ National ConJ'cr-encc o~ Arlificial h~telligencc, pages 722-727,Seattle, Washington.
American Association forArtificial Intelligence.E.
Eyes and G. Leech.
1993.
Syntactic Annol.a-lion: I,inguistic Aspects of (h:ammatical Tag-ging and Skeleton I)arsing.
('hapter 3 of Itlacket.
al.
1993.c'Scv(m tenths of ~his 25% is already correct, sothat lhe task involved is re l)axsing 30% of' 25% ( -7.5%) of the trceba.nk.IL Garside and A. McEnery.
1993.
Treebank-ing: The Compilation of'a Corpus of SkeletonParsed Sentences.
C'hapter 2 of Black el;.
al.1993.F.
,lelinek, J. l,afferty, 1).
Magennan, R. Metcer, A. Ratnaparkhi, S. R.oukos.
1994.
Deci-sion Tree Parsing using a Hidden DerivationModel.
In Proceedings, ARPA Workshop onHuman Language Technology, pages 260-2(55,Plainsboro, New Jersey, AI{PA.F.
Karlsson, A. Voutilainen, J. Heikkila, amlA.
Anttila.
1995.
Constraint Grammar: ALanguage Independent System for Parsing Un-restricted Text.
Mouton de Gruyter: Berlin andNew York.11.
Kucera and W. N. Francis.
1967.
Compltla-lional Analysis of Presenl- Day American l~%-qlish.
13rown University Press.
Providence, RI.I).
M. Magerman and M. P. Marcus.
1991.
Pearl:A Probabilistic Chart Parser.
In Proceedinqs,l'Juropean A CL Conference, March 1991, Berlin,Germany.l).
M. Magerman.
1995.
Statistical Decision TreeModels for Parsing.
In Proceedings, 33rd A~-nual Mecling of lhe Association for' Compula-tional Linguistics, pages 276 283, (,%mhridge,Massachusetts, Association for (k)mputationalLinguistics.M.
1 ).
Marcus, B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a Large Anno-tated (:orpus of English: The Pe\]m Treebank.Computalional Ling,6slics, 19.2:313-330.M.
Mar(ms, G. Kim, M. A. Marcinkiewicz, R.Maclntyre, A. Bies, M. Ferguson, K. Katz, andB.
Schasberger.
1994.
The Penn Treel)ank: An-notating Predicate Argument Structure.
Pro-ceedinqs, ARPA lluman Language "l>chnologyWorkshop, Morgan Kaufinann Publishers Inc.,San Francisco.1~.
Merialdo.
1!)94.
Tagging English Text witha Probabilistic Model.
Compulalional Li'ngu~s-ties, 20.2:155- 171.N.
()ostdi.jl~ and P. de lla.an, l'3ditors.
199d.Cor7ms Ilased l~cscarch Into Language: h~ kou-our@ Yau Aarls.
Rodol)i l~;ditions.
Amsterdal//.I{.
A. Sharmau, F. ,Icliuek, and 1{.
Mercer.
1990.Generating a (~rammar for Statistical Train-ing.
In Proceedin/is, DA RPA ?
'peeck aud /v%t-ural Lan.quagc Workshop, Hidden Valley, Penn-sylvania.l{.
Weischedel, M. Meteer, 1{.
Schwartz, I,.l{amshaw, and J. I)almuc('i.
1993.
Copingwith Ambiguity and iJnknown Words throughProbabilistic Models.
(7ompalational Liuguis-lies, 19.2:359-382.112
