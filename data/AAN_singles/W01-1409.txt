Building a Statistical Machine Translation System from Scratch:How Much Bang for the Buck Can We Expect?Ulrich GermannUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292germann@isi.eduAbstractWe report on our experience with building astatistical MT system from scratch, includ-ing the creation of a small parallel Tamil-English corpus, and the results of a task-based pilot evaluation of statistical MT sys-tems trained on sets of ca.
1300 and ca.5000 parallel sentences of Tamil and Englishdata.
Our results show that even with appar-ently incomprehensible system output, hu-mans without any knowledge of Tamil canachieve performance rates as high as 86%accuracy for topic identification, 93% recallfor document retrieval, and 64% recall onquestion answering (plus an additional 14%partially correct answers).1 IntroductionCrises and disasters frequently attract international at-tention to regions of the world that have previouslybeen largely ignored by the international community.While it is possible to stock up on emergency reliefsupplies and, for the worst case, weapons, regardlessof where exactly they are eventually going to be used,this cannot be done with multilingual information pro-cessing technology.
This technology will often have tobe developed after the fact in a quick response to thegiven situation.
Multilingual data resources for sta-tistical approaches, such as parallel corpora, may notalways be available.In the fall of 2000, we decided to put the currentstate of the art to the test with respect to the rapid con-struction of a machine translation system from scratch.Within one month, we would hire translators; translate as much text as possible; and train a statistical MT system on the data thus cre-ated.The language of choice was Tamil, which is spokenin Sri Lanka and in the southern part of India.
Tamil isa head-last language with a very rich morphology andtherefore quite different from English.2 Data Collection and Preparation2.1 Obtaining Tamil DataTamil data is not very difficult to find on the web.There are several Tamil newspapers and magazineswith online editions, and the large international Tamilcommunity fosters the use of the Internet for the dis-semination of information.
After initial investigationof several web sites we decided to download our exper-imental corpus from www.tamilnet.com, a newssite that provides local news on Sri Lanka in bothTamil and English.
The Tamil and English news textson this site do not seem to be translations of each other.The availability of a fairly large in-domain corpus oflocal news on Sri Lanka in English (over 2 millionwords) allowed us to train an in-domain English lan-guage model of Sri Lankan news.2.2 Encoding and TokenizationTamil is written in a phonematic, non-Latin script.Several encoding schemes exist in parallel.
Eventhough the Unicode standard includes a set of glyphsfor Tamil, it is not widely used in practice.
Most websites that offer Tamil language material assume Latin-1encoding and rely on special true type fonts, which of-ten are also offered for free download at those sites.Tamil text is therefore fairly easy to identify on websites via the face attribute of the HTML font tag.
Allthat is necessary is a list of Tamil font names used bythe different sites, and knowledge about which encod-ings these fonts implement.
While we could restrictourselves to one data source and encoding for our ex-periment, any large-scale system would have to takethis into account.
In order to make the source textrecognizable to humans who have no knowledge ofTamil, we decided to work with transliterated text1.2.3 Translating the CorpusOriginally we hoped to be able to create a parallel cor-pus of about 100,000 words on the Tamil side withinone month, using several translators.
Professional1Translations, however, were produced from the originalTamil.translation services in the US currently charge rates ofabout 30 cents per English word for translations fromTamil into English.
Given that the English transla-tion of a Tamil text usually contains about 1.2 timesas many words as the Tamil original, the translation ofa corpus of 100,000 Tamil words would cost approxi-mately USD 36,000.
This was far beyond our budget.In India, by comparison, raw translations may costas little as one cent per Tamil word2.
However, out-sourcing the translation work abroad was not feasiblefor us, since we had neither the administrative infras-tructure nor the time to manage such an effort.
Also,working with partners so remote would have made itvery difficult to communicate our exact needs and toimplement proper quality control.We finally decided to hire as translators four enter-ing and second-year graduate students in the depart-ment of engineering whose native language is Tamiland who had responded to an ad posted in the localmailing list for students from India.In order to manage the corpus translation process,we set up a web interface through which the transla-tors could retrieve source texts and upload their trans-lations, post-editors could post-edit text online, theproject progress could be monitored, and all incomingtext was available to other project members as soon asit was submitted.We originally assumed that translators would beable to translate about 500 words per hour if we werecontent with raw translations and hardly any format-ting, and if we allowed them to skip difficult wordsor sentences.
This estimate was based on an inter-nal evaluation, in which multilingual members of ourgroup translated sample documents from their nativelanguage (Arabic, German, Romanian) into Englishand kept track of the time they spent on this.It turned out that our expectations were very muchexaggerated with both respect to translation speed andthe quality of translation.
The actual translation speedfor Tamil varied between 156 and 247 words per hourwith an average of 170 words per hour.
In 139 hours ofreported translation time (over a period of eventually 6weeks), about 24,000 words / 1,300 sentences of Tamiltext were translated, at an effective cost of ca.
10.8cents per Tamil word (translators?
compensation plusadministrative overhead).
This figure does not includethe effort for manually post-editing the translations bya native speaker of English (12-16 person hours).The overall organization of the project (source dataretrieval, hiring and management of the translators,design and implementation of the web interface formanaging the project via the Internet, development oftransliterator and stemmer, etc.)
required an additional2Personal communications with Thomas Malten, Universityof Cologne.estimated 2.5 person months.
However, a good part ofthis effort led to resources that can also be used forother purposes.2.4 Lessons Learned for Future ProjectsIf we were to give advice for future, similar projects,we would emphasize and recommend the following:2.4.1 Good translators are not easy to findIt is difficult to find good translators for a short-term commitment.
Unless one is willing to pay a pre-mium price, it is unlikely that one will find profes-sional translators who are willing to commit much oftheir time for a limited period of time and on short no-tice.2.4.2 Make the translation job attractiveAs foreign students, our translators would each havebeen allowed to work up to twenty hours per week.None of them did, because the work was frustratingand boring, and because they found more attractive,long term employment on campus.
Our translators?frustration may have been fostered by several factors: the differences between Sri Lankan Tamil (the va-riety used in our corpus) and the Tamil spoken inSouthern India (the native language of our trans-lators), which made translating, according to ourtranslators, very difficult; the lack of translation experience of our transla-tors; and our high expectations.
We originally told ourtranslators that since they were not working onsite, we would expect the translation of 500words per hour reported.
When we later switchedto hourly pay regardless of translation volume,the translation volume picked up slightly.2.4.3 Be prepared to post-editIn professional translating, translators typicallytranslate into their native language only.
One may notbe able to find translators with English as their na-tive language for low density or ?small?
languages,so it may be necessary to have the translations post-edited by people with greater language proficiency inEnglish.2.4.4 Have translators and post-editors work onsiteIt is better to have translators and post-editors workon site and ideally as teams, so that they can resolveambiguities and misunderstandings immediately with-out the delays of communicating indirectly, be it byemail or other means.
A post-editor who does notknow the source language may misinterpret the trans-lator, as the following case from our corpus illustrates:0 20 40 60 80 100 120020406080100corpus size (in thousand tokens)textcoverage(in%)English0 20 40 60 80 100 120020406080100corpus size (in thousand tokens)textcoverage(in%)TamilFigure 1: Text coverage on previously unseen text for English (left) and Tamil (right).
The upper line in eachgraph shows the coverage by tokens that have been seen at least once, the lower line shows the coverage by tokensthat have been seen at least 5 times.
The error bars indicate standard deviation.Raw translation: Information about the schools inwhich people who migrated to Kudaanadu are stayingis being gathered.Post-edited version: Information about the schools in(sic!)
which immigrants to Kudaanadu are attendingis being gathered.In this case, the post-editor clearly misinterpreted thetranslator.
What the translator meant to and actuallydid say is that information was being gathered aboutthe schools in which migrants/war refugees who hadarrived in Kudaanadu had found shelter.
However,the post-editor interpreted the phrase people who mi-grated to Kudaana as describing immigrants and as-sumed that information was being gathered about theireducation rather than their housing.3 Evaluation Experiments3.1 A Priori ConsiderationsThe richer the morphology of a language is, the greateris the total number of distinct word forms that a givencorpus consists of, and the smaller is the probabilitythat a certain word form actually occurs in any giventext segment.
Figure 1 shows the percentage of wordforms in unseen text that have occurred in previouslyseen text as a function of the amount of previouslyseen text.
The graph on the left shows the curves forEnglish, the one on the right the curves for Sri LankanTamil.
The graphs show the averages of 100 runs ondifferent text fragments; the error bars indicate stan-dard deviation.The numbers were computed in the following man-ner: A corpus of 120,000 tokens was split into seg-ments of 1000 tokens each.
For each segment nk,we computed how many of the tokens had been pre-viously seen in the segments n1: : : nk 1.
The upperline in the graphs shows the percentage of tokens in nkthat had occurred at least once before in the segmentsn1: : : nk 1, the lower line shows the percentage of to-kens that had been seen at least five times before.For the purpose of statistical NLP, it seems reason-able to assume that the lower curve gives a better indi-cation of how many percent of previously unseen textwe can expect to be ?known?
to a statistical modeltrained on a corpus of m tokens.At a corpus size of 24,000 tokens, which is approx-imately the size of the parallel corpus we were able tocreate during our experiment, about 28% of all wordforms in previously unseen Sri Lankan Tamil text can-not be found in the corpus, and 50% have been seenless than 5 times.
In other words, if we train a systemon this data, we can expect it to stumble over everyother word!
At a corpus size of 100,000 tokens, thenumbers are 17% and 33%.For English, the numbers are 9%/23% for a corpusof 24K tokens and 0%/8% for a corpus of 100K to-kens.In order to boost the text coverage we built a simpletext stemmer for Tamil, based on the Tamil inflectiontables in Steever (1990) and some additional inspec-tion of our parallel corpus.
The stemmer uses regularexpression matching to cut off inflectional endings andintroduce some extra tokens for negation and certaincase markings (such as locative and genitive), whichare all marked morphologically in Tamil.
It should benoted that the stemmer is far from perfect and was onlyintended to be an interim solution.
The performanceincreases are displayed in Figure 2.
For a corpus size0 20 40 60 80 100 120020406080100corpus size (in thousand tokens)textcoverage(in%)Figure 2: Text coverage increase by stemming forTamil.
The solid lines indicate text coverage for un-stemmed data (seen at least once and at least five times,respectively), the dashed lines the text coverage forstemmed data.of 24K tokens, the percentages of unknown items dropto 19% (from 28%; never seen before) and 36% (from50%; seen less than 5 times).
For a training corpusof 100K tokens, the numbers are 12% and 23% (from17%/33%).3.2 Task-Based Pilot EvaluationGiven these numbers, it is obvious that one cannot ex-pect much performance from a system that relies onmodels trained on only 24K tokens of data.
As a mat-ter of fact, it is close to impossible to make any sensewhatsoever of the output of such a system (cf.
Fig.
3).To get an estimate of the performance with moretraining data, we augmented our corpus with a paral-lel corpus of international news texts in Southern In-dian Tamil which was made available to us by FredGey of the University of California at Berkeley (hence-forth: Berkeley corpus).
This corpus contains ca.3,800 sentence pairs with 75,800 Tamil tokens afterstemming (before stemming: 60,000; the differenceis due to the introduction of additional markers dur-ing stemming).
Some of the parallel data was with-held for system evaluation; the augmented trainingcorpus (Berkeley and TamilNet corpus; short B+TN)had a size of 85K tokens on the Tamil side.
The aug-mented training corpus had a text coverage of 81%(seen at least once; 75% without augmentation), and67% (seen at least 5 times; 60% without augmenta-tion), respectively, for Sri Lankan Tamil.
We trainedIBM Translation Model 4 (Brown et al, 1993) both onour corpus alone and on the augmented corpus, usingthe EGYPT toolkit (Knight et al, 1999; Al-Onaizan etal., 1999), and then translated a number of texts us-ing different translation models and different transfermethods, namely glossing (replacing each Tamil wordby the most likely candidate from the translation tablescreated with the EGYPT toolkit) and Model 4 decoding(Brown et al, 1995; Germann et al, 2001).Figure 3 shows the output of the different systemsin comparison with the human translation.We then conducted the following experiments.3.2.1 Document Classification TaskSeven human subjects without any knowledge ofTamil were given translations of a set of 15 texts (allfrom the Berkeley corpus) and asked to categorizethem according to the following topic hierarchy: News about Sri Lanka Reports about clashes between the SriLankan army and the Liberation Tigers Sri Lankan security-related news (arrests,arms deals, etc.
) Sri Lankan political news (strikes, transport,telecom) concerns Sri Lanka but doesn?t fit any of theabove News about Pakistan/India Nuclear tests in Pakistan and India, includ-ing their aftermath (international reactions,etc.
) Corruption investigation against BenazirButo News about Pakistan/India but none of theabove International news Disasters, accidents Nelson Mandela?s birthday Other international news Impossible to tellExcept for one duplicate set, each subject received adifferent set of translations.
The sets differed in train-ing parameters and the translation method used.
Ta-ble 1 shows the results of this evaluation.
The dif-ference between the subjects 5a and 5b, who receivedthe same set of translations, suggests that the individ-ual classifiers?
accuracy influences the results so muchas to blur the effect of the other parameters.
Thereseems to be a tendency for glossing to work better thanModel 4 decoding.
Glossing, in our system, is a simplebase line algorithm that provides the most likely wordtranslation for each word of input.
Translation can-didates and their probabilities are retrieved from thetranslation table, which is part of the translation modeltrained on the parallel corpus.The document classification test is foremost andabove all a measure of the quality of the translation ta-ble for frequently occurring words.
In practice, actualSmall Corpus24K tokens (Tamil) of training dataAugmented Corpus85K tokens (Tamil) of training dataHuman translation Gloss Model 4 decodinga Gloss Model 4 decodingGovernment - UnitedNational Partymeeting will not takeplace tomorrow.vanni united nationalparty kalloyatomorrownaTaipeRamaaTTachildren unitednational partytomorrow .government unitednational party meetdaynaTaipeRamaaTTathe government withunited national partymeets day .The proposedmeeting tomorrow,Thursday, betweenthe Peoples FrontGovernment and theUnited NationalParty regarding theNew PoliticalDocument wasannounced aspostponed to a laterdate.friday progressive itand united nationalsame kaTcikkumiTaitomorrow thursdaynaTaipeRaviru newpolitical vaappuinformation kalloyapiRitorutin2attiRkupapin2pooTappaTT-uLLataaka foundison friday healthmothers unitednational tomorrow onthursday newpolitical found oninformation .throughoutprogressivegovernment theunited national inkaTcikkumiTai daythursday leno newpolitical vaappufound meet piRitorutin2attiRkupapin2pooTappaTT-uLLataaka movies.throughout thegovernment and theunited national party.
day thursday lenonew political foundmeet movies .PresidentialSecretariat sourcessay that this meetingwas postponed as theSri Lankan PresidentChandrikaBandaranayakka hasgone to a foreigncountry.of lankan to andfreedom tamiliannowveLinaaTTukkuccen2RiruppaticcantippupiRpooTappaTT-uLLataaka tosecretariat in * is?s lanka to advisorfreedom of thetamilian team tosecretariat is calledminister .of lankan presidentchandrika sankarifreedom nowveLinaaTTukkuccen2RiruppaticcantippupiRpooTappaTT-uLLataaka presidentsecretariat circlesreported .the sri lankan forcesof presidentchandrika sankarifreedom presentlysecretariat circlesreported .At the same time it isto be noted that themeeting of the subcommitteeexamining thepolitical documentbetween the UnitedNational Party andthe Government washeld yesterdayTuesday eveningthat about returningand iccantippunaTaipeRumen2aand ivvaTTaarammore and * isher about the militaryand they wereannounced .he countriesreturning theiccantippunaTaipeRumen2a theivvaTTaaram morethe reported .his country and thereturning to increasethe radio .aBrown et al (1993); Brown et al (1995)Figure 3: Sample output of various systemsTable 1: Results of the Document Classification Task.
Test subjects were asked to classify the translations of 15documents into 4 major and 11 minor categories.input pegginga?
transfer correct partiallycorrectb incorrect1 raw no M4 decodingc 7 4 42 stemmed yes M4 decoding 8 3 43 stemmed no M4 decoding 13 2 04 raw no gloss 13 1 15a stemmed yes gloss 8 3 45b stemmed yes gloss 12 2 16 stemmed no gloss 11 2 2apegging causes the training algorithm to consider a larger search spacebcorrect top level category but incorrect sub-categoryctranslation by maximizing the IBM Model 4 probability of the source/translation pair (Brownet al, 1993; Brown et al, 1995)classification might be performed by automatic pro-cedures rather than humans.
If we dare to accept thetop performances of our human subjects as the ten-tative upper bound of what can be achieved with thecurrent system using a translation model trained on85K tokens of Tamil text and the corresponding En-glish translations, we can conclude that the classifica-tion accuracy can exceed 86% (13/15) for fine-grainedclassification and reach 100% for coarse-grained clas-sification.
However, given the extremely small samplesize in this evaluation, the evidence should not be con-sidered conclusive.3.2.2 Document Retrieval TaskThe document retrieval task and the question an-swering task (see below) were combined into one task.The subjects received 14 texts (from the TamilNet cor-pus) and 15 lead questions plus 13 additional follow-up questions.
Their task was to identify the docu-ment(s) that contain(s) the answer to the question andto answer the questions asked.
Typical lead questionswere questions such as What is the security situationin Trinconmalee?, or Who is S.
Thivakarasa?
; typi-cal follow-up questions were Who is in control of thesituation?, What happened to him/her?, or How did(other) people react to what happened?.
As in the pre-vious experiment, each subject received the output ofa different system.Table 2 shows the result of the document retrievaltask.
Again, the sample size was too small to drawany final conclusions, but our results seem to suggestthe following.
Firstly, the test subject in the groupdealing with output of systems trained on the biggertraining set tend to perform better than the ones deal-ing with the results of training on less data.
This sug-gests that the jump from 24K to 85K tokens of train-ing data might improve system performance in a sig-nificant manner.
We were surprised that even withthe poor translation performance of our system, re-calls as high as 93% at a precision of 88% could beachieved.
Secondly, the data shows that gaps are notrandomly distributed over the data, but that some ques-tions clearly seem to have been more difficult than oth-ers.
One of the particular difficult aspects of the taskwas the spelling of names.
Question 11, for exam-ple, asked What happened to Chandra Kumar Abayas-ingh?.
In the translations, however, it was rendered insimple transliteration: cantirakumaara apayacingka.It requires a considerable degree of tenacity and imag-ination to find this connection.3.2.3 Question Answering TaskIn order to measure the performance in the ques-tion answering part of this evaluation, we consideredonly questions relevant to the documents that the testsubjects had identified correctly.
Because of the diffi-culty of the task, we were lenient to some degree in theevaluation.
For example, if the correct answer was theformer president of the teacher?s union and the answergiven was an official of the teacher?s union, we stillcounted this as ?close enough?
and therefore correct.In addition, we also allowed partially correct answers,that is, answers that went into the right direction butwere not quite correct.
For example, if the correct an-swer was The army imposed a curfew on fishing, wecounted the answer the army is stopping fishing boatsas partially correct.
All in all, it was very difficult toevaluate this section of the task, because it was oftenclose to impossible to determine whether the answerwas just an educated guess or actually based on thetext.
There were some cases where answers were par-tially or even fully correct even though the correct doc-ument had not been identified.
In retrospect we con-clude that it would have been better to have the testTable 2: Recall and precision on the document retrieval task.
Test subjects were asked to identify the document(s)containing the answers to 15 lead questions.
Black dots indicate successful identification of at least one documentcontaining the answer.trainingcorpustransl.method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 recall precision1 TNa glossing           67% 79%2 TN M4dec.b         53% 80%3 TN bothc          60% 48%4 TNd both           67% 79%5 B+TN.1 glossing             80% 87%6 B+TN.1 M4dec.
          67% 86%7 B+TN.1 both             80% 86%8 B+TN.1f both         60% 73%9 B+TN.1g both               93% 88%10 B+TN.2h both              87% 75%Human Translations                100% 100%aTamilNet corpus only; stemmed; 1291 aligned text chunks; 23,359 tokens on Tamil side; 1000 training iterations.bIBM Model 4 decoding.cBoth glossing and IBM Model 4 decoding were available to the test subject.dsame as above, but trained with pegging option (more thorough search during training); 10 training iterations.eBerkeley and TamilNet corpora; 5069 aligned text chunks; 85421 tokens on Tamil side; 100 training iterations.fsame as above; 10 training iterations.gsame as above, trained with pegging option; 10 training iterations.hBerkeley and TamilNet Corpora, raw (unstemmed); 64439 tokens on Tamil side, 50 training iterations.subjects mark up those text passages in the text thatjustify their answers.Again, the data suggests that the difference in train-ing corpus size does affect the amount of informationthat is available from the system output.
Subjects us-ing output of a system based on a translation modelthat was trained on only the TamilNet data tend to per-form worse than subjects using output from a systembased on a translation model trained on the larger cor-pus.
The poor performance on test set No.
6 maysuggest that for this task and at this level of transla-tion quality, glossing provides more informative out-put than Model 4 decoding.
This result is not partic-ularly surprising, since we noticed that Model 4 de-coding tends to leave out more words than acceptable.Clearly, this is one area where the translation modelhas to be improved.Test set 10 is the only set produced by a systemusing a translation model trained on raw, unstemmeddata.
It is unclear whether the poor performance onquestion answering for this test set is due to a princi-pally worse translation quality or the (lack of) tenacityand willingness of the test subject to work her waythrough the system output.All in all, we were astonished by the amount of in-formation that our test subjects were able to retrievefrom the material they received (the top recall for thequestion answering task is 64%, plus an additionalTable 3: Accuracy on question answering.
The testsets are the same as in Table 2.
Only questions con-cerning documents that were identified correctly wereconsidered in this evaluation.testsettrainingcorpusNo.
of rele-vant questions correctpartiallycorrect1 TN 17 2 = 12% 0 = 0%2 TN 16 3 = 19% 4 = 25%3 TN 18 2 = 11% 0 = 0%4 TN 17 7 = 41% 5 = 19%4 B+TN.1 23 14 = 61% 6 = 26%6 B+TN.1 19 5 = 26% 3 = 16%7 B+TN.1 22 14 = 64% 3 = 14%8 B+TN.1 19 12 = 63% 1 = 5%9 B+TN.1 26 14 = 54% 5 = 19%10 B+TN.2 24 8 = 33% 9 = 38%human 28 24 = 86% 2 = 7%14% partially correct answers!).
However, using asystem such as the one discussed in the paper is notan option for actual information processing.
Espe-cially those subjects that had to deal with the outputof systems trained on the smaller corpus experiencedthe task as utterly frustrating and would not want to doit again.4 ConclusionsWe have reported on our experience with rapidlybuilding a statistical MT system from scratch.
Withinca.
140 translator hours, we were able to create a par-allel corpus of about 1300 sentence pairs with 24,000tokens on the Tamil side, at an average translation rateof approximately 170 Tamil words per hour.Very clearly, the effort needed to create parallel datais one of the biggest obstacles to the rapid developmentof statistical MT systems for new languages.With the output of a system which uses a translationmodel trained on the small amount of parallel data thatwe created during the course of our experiment, hu-man test subjects achieved a recall of over 50% onthe document retrieval task but generally performedpoorly on question answering (less than 20%).The addition of an additional corpus of 3,800 sen-tence pairs allowed us to estimate the benefits of in-creasing the overall corpus size by roughly 300%.Based on our experience with translating the TamilNetcorpus, this additional effort would require an addi-tional 450 translator and 36 to 48 post-editor hours.With the additional training data, we were able toproduce output that increased the performance on ourevaluation tasks (document retrieval and question an-swering) to up to 93% for document retrieval and 64%for question answering.With respect to the scenario of ?MT in a month?,we can now make the following calculation: If we as-sume that the average translator translates at a rate of170 words/hour and is able to spend 6-7 hours per dayon actual translations, then a translator can translateabout 1000-1200 words per day.
In order to translate acorpus of 100,000 words within one month (assuminga five-day work week), we therefore need four to fivefull time translators.
For this effort, we can expecta translation system whose performance resembles theone shown in our evaluation.This, of course, raises the following questions,which we are only able to ask but not to answer at thispoint. Can the translation model and the algorithms forstatistical training be improved so that they re-quire less data to produce acceptable results? Are there more efficient uses of scarce resources(such as language experts and translators) forbuilding a statistical (or any other) MT systemquickly, for example the creation of less but moreinformative data, e.g.
a parallel corpus withalignments on the word level, or the compilationof a glossary/dictionary of the most frequentlyused terms? How do the various approaches compare with re-spect to the ratio of construction effort versusperformance improvement when the MT systemsare scaled up?
One approach may show rapidimprovements initially but also reach a plateauquickly, whereas another may show slow butsteady improvements. Is there any potential for bootstrapping the re-source creation process by using knowledge thatcan be extracted from little and poor data to speedup the creation of more and better data?These are some of the the questions that will needto be addressed in future research on Quick MT.5 AcknowledgmentsThis research has been funded by the DARPA TIDESprogram under grant No.
N66001-00-1-8914.
Wewould like to thank our translators as well as FredGey of the University of California at Berkeley andThomas Malten of the University of Cologne for theirkind support of our investigations into Tamil, and ourtest subjects for their tenacity and patience during thisexperiment.ReferencesYaser Al-Onaizan, David Purdy, Jan Curin, MichaelJahr, Kevin Knight, John Lafferty, Dan Melamed,Noah A. Smith, Franz Josef Och, and DavidYarowsky.
1999.
Statistical machine translation.Final report, Center for Language and Speech Pro-cessing, John Hopkins University.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Peter Brown, John Cocke, Stephen Della Pietra, Vin-cent Della Pietra, Frederick Jelinek, Jennifer Lai,and Robert Mercer.
1995.
Method and system fornatural language translation.
U.S. Patent 5,477,451,Dec 19.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decodingand optimal decoding for MT.
In 39th Annual Meet-ing of the Association for Computational Linguis-tics (ACL-2001).Kevin Knight, Yaser Al-Onaizan, David Purdy, JanCurin, Michael Jahr, John Lafferty, Dan Melamed,Noah Smith, Franz Josef Och, and David Yarowsky.1999.
EGYPT: a statistical machine translationtoolkit.
http://www.clsp.jhu.edu/ws99/projects/mt/.Sanford B. Steever.
1990.
Tamil and the Dravidianlanguages.
In Bernard Comrie, editor, The World?sMajor Languages, pages 725?746.
Oxford Univer-sity Press, New York, Oxford.
