PART-OF-SPEECH TAGGING WITH NEURAL NETWORKSHehnut SchmidInstitute for Computational Linguistics, Azenbergstr.12, 70174 Stuttgart, Germany,schmid@ims.uni-stuttgart.deTopic area: large text corpora, part-of-speech tag-ging, neural networks1 ABSTRACTText corpora which are tagged with part-of-speech in-formation are useful in many areas of linguistic re-search.
In this paper, a new part-of-speech taggingmethod hased on neural networks (Net-Tagger) is pre-sented and its performance is compared to that of allMM-tagger (Cutting et al, 1992) and a trigram-based tagger (Kempe, 1993).
It is shown that theNet-Tagger performs as well as the trigram-based tag-ger and better than the iIMM-tagger.2 INTRODUCTIONWords are often ambiguous in their part of speech.The English word store for example can be either anoun, a finite verb or an infinitive.
In an utterance,this ambiguity is normally resolved by the context of aword: e.g.
in the seutence "The 1977 P6's could storetwo pages of data.
", store can only be an intluitive.A part-of-speech tagger is a system which automat-ically assigns the part of speech to words using con-textual information.
Potential applications for part-of-speech taggers exist in many areas inclnding speechrecognition, speech synthesis, machine translation andinformation retrieval.l)ifi'ereut methods have been used for the im plemen-ration of part-of-speech taggers.
TAGGIT (Greene,Rnbin, 1971), an early system, which was used for theinitial tagging of the Brown corpus was rule-based.
Itwas able to assign the correct part-of-speech to about77 % of the words in the Brown corpus.In another approach contextual dependencies aremodelled statistically.
Churcb (1988) and Kempe(1993) use second order Markov Models and traintheir systems on large handtagged corpora.
Using thismetbod, they are able to tag more than 96 % of theirtest words with the correct part-of-speech.
The needfor reliably tagged training data, however, is a prob-lem for languages, where such data is not availablein sufficient quantities.
Jelinek (1985) and Cutting etal.
(1992) circumvent his problem by training theirtaggers on untagged ata using tile Itaum-Welch algo-rithm (also know as the forward-backward algorithm).They report rates of correctly tagged words which arecomparable to that presented by Church (1988) andKempe (1993).A third and rather new approach is tagging withartificial neural networks.
In the area of speech recog-nition neural networks have been used for a decader, ow.
They have shown performances comparable tothat of IIidden Ivlarkov model systems or even better(Lippmann, 1989).
Part-of-speech prediction is an-other area, closer to POS tagging, where neural net-works have been applied successfidly.
Nakamura el;al.
(1990) trained a d-layer feed-forward network withup to three preceding part-of-speech tags ,as input topredict the word category of the next word.
The pre-diction accuracy was similar to that of a trigram-b,~sedpredictor.
Using tile predictor, Nakamura et al wereable to improve the recognition rate of their speechrecognition system from 81.0 % to 86.9 %.Federici and Pirrelli (199a) developed a part-of-speech tagger which is based on a special type ofneural network.
It disambiguates between alternativemorphosyntactic tags which are generated by a roofphological analyzer.
The tagger is trained with ananalogy-driven learning procedure.
Only preliminaryresults are presented, so that a comparison with othermethods is difficult.Ill this paper, a part-of-speech tagger based on amultilayer perceptrou network is presented.
It is simi-lar to tile network of Nakamura et al (1990) in so faras the same training procedure (Backpropagation) isused; but it differs in the structure of tile network andalso in its purpose (disambignation vs. prediction).The performance of tl,e presented tagger is measuredand compared to that of two other taggers (Cuttinget al, 1992; Kempe, 1993).3 NEURAL NETWORKSArtificial neural networks consist of a large number ofsimple processing units.
These units are highly inter-connected by directed weighted links.
Associated witheach unit is an activation value.
Through tile connec-tions, this activation is propagated to other units.In mnltilayer perceptron etworks (MLP-networks),tile most popular network type, the processing unitsare arranged vertically in several ayers (fig.
I).
Con-nections exist only between units in adjacent layers.The bottom layer is called input layer', because theactivations of the units in this layer represent the in-put of tile network.
Correspondingly, the top layer iscalled output layer.
Any layers between input layer772Figure 1: A 3-layer perceptron etwork( output unitshidden unitsb input unitsand outlmt layer are called hidden layers.
Their acti-wttions are not visible externally.During the processing in a MLP-network, actiwt-tions are propagated from inlmt units through hiddenunits to output units.
At each unit j ,  the weightedinlmt activations aiwij are summed and a bias pa-rameter Oj is added.net i = ~ aiwlj + Oj (1)tThe resulting network input ,telj is then l)~uqsedthrough a sigmoid fimction (the logistic funclion) inorder to restrict the value range of the resulting acti-vation aj to the interval \[0,i\].1a~ - t + e .
.
.
.
~, (:~)The network learns by adapting the weights of theconnections between units, tmtil the correct output ist~rocluced.
One widely used method is the backl.
'o p-~gation algorithm which performs a gradient descentsearch on the error surface, The weight update ~XlOij ,i.e.
the difference between the old and the new valueof weight wij, is here defined ,~s:AWij - -  rlapi6pj, where{ ,,pj(1 --,,,)(t,,j - "p J ) ,if j is an output unita,,~ = ,,vj(l _avs)~vk,oik, (a)kif j is a hidden unitIlere, Zp is the target output vector which the networklnnst learn t .
"Daining the MLP-network with the backpropagaotion rule guarantees that a local minimum of the er-ror surface is found, thougl, this is not necessarily theglobal one.
In order to speed up the trahfiug process,a momentum term is often introduced into the updaterormula:?kWij(t -~" 1) "~ Oapi~pj "+ ( :~ l t ) i j ( l )  (4)1We assume here that the hia.s parameter Oj is realized msa weight o an additional unit whidt has always the activationva}.ue 1 (cp.
(B.umelhart, McChdland, t98,1)).For a de.tailed introduction to MLP networks see e.g.
(l{unaelhart, McClellan(l, 1984).r4 Tt IG  I~_AGGER NI i ' ,TWO1{I (The Net-Tagger consists of a Ml, P-network and a lex-icon (see tlg.
2).l;'igu,'e 2: Structure.
of I.he Net-Tagger without hiddenlayer; tile arrow symbolizes the connections betweenthe layers.11f \ @ @ ?...?@...?
?
?
?
?.-.?@...?
?
@ @ @...@@...?
@ ?
?
?...??...
@ ?
@ ?
@...@p fIn the output layer of the MLP network, each unitcorresponds to one of the tags in the tagset.
The net-work learns during the training to activate that outputunit which represents the correct ag and to deactivateall other output units, llence, in the trained network,the output unit with the higlu.
'st activation indicates,which tag shouhl be attached to the word that is cur-rently processed.The input of the network comprises all the informa-tion whicii the systeni ti;Ls about the parts of speech ofthe current word, the p precedhig words al,d the f fol-lowing words.
More precisely, for each part-of-speechtag posj and each of the p-t- 1-kf words in the context,there is an input unit whose activation in U representsthe probability that wordl h~Ls part of speech posj.For the word which is being tagged and the fol-lowing words, the lezical part-of-speech probabilityl'(posj\]wordi) is all we know about the part ofspeech ~, This probability does not take into accountarty contextual influences.
So, we get the following in-put representation for the currently tagged word andthe following words:i , , , j  : v(vo.,v I,,,o,.d,), ir i > o (s)2 Lexical probabilities are estimated hy dividing, the numberof times a word occurs with a giw:n tag by the own'all numher oftimes the word occurs.
This method is known as the Ma.vimumLikelihood Principle.IZ '~For tile preceding words, there is more informationavailable, because they have already bccn tagged.
Theactivation values of the output units at the time ofprocessing are here used instead of the lexieal part-of-speech probabilitiesa:i , , ; /t)  = o, , t / t  + O, if ; < 0 (6)Copying output activations of tile network into theinput units introduces recurrence into the network.This complicates the training process, because the out-put of the network is not correct, when the trainingstarts and therefore, it cannot be fed back directly,when the training starts.
Instead a weighted averageof the actual output and the target output is used.It resembles more the output of the trained networkwhich is similar (or at least shouhl be similar) to thetarget output.
At tile beginning of the training, theweighting of the target output is high.
It fails to zeroduring the training.The network is trained on a tagged corpus.
Targetactivations are 0 for all output units, excepted for theunit which corresponds to the correct tag, for which itis 1.
A slightly modified version of the backpropaga-tion algorithm with momentum term which has beenpresented in the last section is used: if the differencebetween the activation of an output unit j and the cor-responding target output is below a predefined thresh-old (we used 0.1), the error signal ~pJ is set to zero.
Inthis way the network is forced to pay more attention tolarger error signals.
This resulted in an improvementof the tagging accuracy by more than 1 percent.Network architectures with and without hidden lay-ers have been trained and tested.
In general, MLP-networks with hidden layers are more powerful thannetworks without one, but they also need more train-ing and there is a higher risk of overlearning 4.
As willbe shown in the next section, the Net-Tagger did notprofit from a hidden layer.In both network types, the tagging of a single wordis performed by copying the tag probabilities of thecurrent word and its neighbours into the input units,propagating the activations through the network tothe output units and determining the output unitwhich has the highest activation.
The tag correspond-ing to this unit is then attached to the current word.If the second strongest activation in the output layeris close to the strongest one, tile tag correspondingto the second strongest activation may be given asan alternative output.
No additional computation isrequired for this.
Further, it is possible to give a scoredlist of all tags as output.aThe output  act ivat ions  of the network do not necessar-ily sum to 1.
Therefore,  they should not he interpreted asprobabi l i t ies.40verlearning means  that  irrelevant features of the t ra in ingset are learned.
As a result ,  the uetwork is unable to generalize.5 TIIE LEX ICONThe lexicon which contains the a priori tag probabili-ties for each word is similar to the lexicon which wasused by Cutting et al (1992).
it has three parts: afullform lexicon, a suffix lexicon and a default enlry.No documentation f tile construction algorithm of thesu\[lix lexicon in (Cutting et al, 1992) was available.Thus, a new method based on information theoreticprinciples was developed.During the lookup of a word in the lexicon of theNet-Tagger, the fifllform lexicon is searched first.
Ifthe word is found there, the corresponding tag prob-ability vector is returned.
Otherwise, the uppercaseletters of the word are turned to lowercase, and thesearch in the fullform lexicon is repeated.
If it failsagain, the suIfix lexicon is searched next.
If none ofthe previous teps has been snccessfull, tile default en-try of the lexicon is returned.The fullform lexicon was created from a taggedtraining corpus (some 2 million words of the PennTreebank Corpus).
First, the number of occurrencesof each word/tag pair was counted.
Afterwards, thosetags of each word with an estimated probability of lessthan 1 percent were removed, because they were inmost eases the result of tagging errors in the originalcorpus.Figure 3: A sample suffix tree of length 3i esOilsousscdoldblelieneeiveingionSOIltonmanityThe second part of the lexicon, the suflix lexicon,forms a tree.
Each node of tile tree (excepted tile rootnode) is labeled with a character.
At tile leaves, tagprobability vectors are attached.
During a lookup, tilesuffix tree is searched from the root.
In each step, tilebranch which is labeled with the next character fromtile end of the word suffix, is followed.Assume e.g., wc want to look for tile word taggiu 9in the suflqx lexicon which is shown in fig.
3.
We startat the root (labeled #) and follow the branch whichleads to the node labeled g. From there, we move tothe node labeled n, and finally we end up in tile node174Table 1: Sample frequencies at a tree node and its twochild nodes.suffix ess10I gp /  <iSl_5 __1 2t~a~ 143sufllx ness suffix less1 852 845 00 248 95labeled i.
This node is a leaf and the attached tagprobability vector (which is not shown in lib.
3) isreturned.The suffix lexicon was automatically built from thetraining corpus.
First, a sujJiz tree wits constructedfrom the suffices of length 5 of sill words wliich wereannotated with an open class l)art-of-speecli s. Thentag frequencies were cotlnted for all suffices and storedat the corresponding tree nodes.In the next step, an information measure I(S) wascalculated for each node of the tree:I(S) = - ~ P(posiS ) tomd'(p,>,qS) (7)po*IIere, S is the suffix which corresponds to the currentnode and P(poslS ) is the probability of tag pos givena word with suff ix S.Using this information measure, the suffix tree hasbeen pruned.
For each leaf, the weighted informationgain G(aS) was calculated:a(aS) = V(aS) (S(S) - S(<,S)), (8)where S is the suffix of the parent node, aS is thesuffix of the current node and F(aS) is the frequencyof suffix nS.If the information gain at some leaf of the suffix treeis below a given threshoht ~, it is removed.
The tagfrequencies of all deleted subnodes of a parent nodeare collected at the defi, ult node of the parent node.If the default node is the only renlaining subnodc, itis deleted too.
In this case, the parent node becomesa leaf and is also checked, whether it is deletable.To illustrate this process consider the following ex-ample, where ess is the suffix of the parent node, lessis tim suffix of one child node and hess is the suffixof the other child node.
The tag frequencies of thesenodes are given in table 1.Tim information measure for the parent node is:86 86 10 10S(ess) .
.
.
.
.
Iog~ .
.
.
.
.
.
l o ,a~- -  ... ~ 1.32 (9)143 143 143 14'3'\].
'lie corresponding values for the chihl nodes are 0.39for hess and 0.56 for less.
Now, we can determine thewelghted information gain at each of the ehihl nodes.We get:G(ness) = 48(1.32 - 0.39) = 44.64 (10)5Opell class parts-of-speech are those, width allow for theproduction of new words {e.g.
noun, verb, adjective).6We used a gain threshohl of 10.Table 2: Comparison of recognition ratesmethod accuracy tNet-Tagger 96.22 %trigrarn tagger 96.06 %IIMM tagger 94.24 %G(less) = 95(1.32-  0.56) = 72,20 (11)Both wdues are well above a threshohl of 10, and there-fore none of them should be deleted.As explained before, the suflix tree is walked duringa lookup along the l)ath, where the nodes are anno-tated with the letters of the word snflix in reversed or-der.
If at some node on the path, no matching subnodecan be found, and there is a default subitode, then thedefault node is followed.
If a leaf is reached at the endof the path, the corresponding tag probability vectoris returned.
Otherwise, the search fails and the defaultentry is returned.The defaull entry is constructed by subtracting thetag frequencies at all leaves of the pruned suffix treefrom the tag frequencies of the root node and nor-malizing the resulting frequencies.
Thereby, relativefrequencies are obtained which sum to one.6 Rl,~suursThe 2-layer version of the Net-Tagger w,~s trained on a2 million word subpart of the Pe.nn-Treebank corpus.Its performance was tested on a 100,000 word subpartwhich was not part of the training corlms.
The set-tings of the network parameters were as follows: thenumber of preceding words in the context p w,~s 3, thenumber of following words f was 2 and the numberof training cycles was 4 millions.
The training of thetagger took one day on a Sparcl0 workstation and thetagging of 100,000 words took 12 minutes on the samemachine.In tabh; 2, the accuracy rate of the Net-Tagger iscOrolLated to that of a trigram l)msed tagger (Kempe,1993) and a lIidden Markov Model tagger (Cutting etal., 1992) which were.
trained and tested on the samedata.
In order to determine the influence of tim sizeof the training sample, the taggers were also trainedon corpora of different sizes and tested again r. Theresulting percentages of correctly tagged words areshown in figure 4.These experiments demonstrate that the perfor-mance of the Net-Tagger is comparable to that of thetrigram tagger and better than that of the IIMM tag-ger.
They further show tl,at the performance of theNet-Tagger is less affected by a small amount of train-ing data than that of tim trigram tagger.
This maybe due to a much smaller number of paraineters in theNet-Tagger: while the trigram tagger must accurately~l:or this test, a slightly simpler netwm'k structure with twopreceding and one following word in the input context was used.775Figure 4: Recognition rates for varying sizes of thetraining corpus.100 "', ' '  ?
?
? '
.... , '  ?
?
.
.
.
.
.
.
,95.=_90 ~ ?.jr ?
'":~ 85 Net-Tagger - " -~o= Xer0x-Tagger ....o J 80 -~ Trigram Tagger -='-75 ..,I .
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
I10000 100000 1 e+06size of training corpusestimate 110,592 trigrams, the Net-Tagger only has totrain 13,824 network parameters.It was fitrther tested, whether an additional hid-den layer in the network with 50 units would improvethe accuracy of the tagging.
It turned out that theaccuracy actually deteriorated slightly, although thenumber of training cycles had been increased to 50millions .Also, tire influence of the size of the input contextwas determined.
Shrinking the context from threepreceding and two following words to two precedingand one following word reduced the accuracy only by0.1%.
Enlarging the context gave no improvement.A context of three preceding and two following wordsseems to he optimal.As mentioned previously, the tagger can producean alternative tag, if the decision between two tags isdifficult.
In that way, the accuracy can be raised to97.79 % at the expense of 4.6 % ambiguously taggedwords.An analysis of tire errors of the Net-Tagger and thetrigram tagger shows that both have problems withthe same words, althot, gh the individual errors are of-ten different 9 .7 CONCLUSIONSIn this paper, the Net-Tagger was presented, a part-of-speech tagger which is based on a MLP-network.A comparison of the tagging results with those of atrigram tagger and a IIMM tagger showed that theaccuracy is as high as that of the trigram tagger andthe robustness on small training corpora is as goodas that of the HMM tagger.
Thus, the Net-Taggercombines advantages of both of these methods.The Net-Tagger has the additional advantage thatproblematic decisions between tags are easy to detect,aDue to the large training times needed to train the 3-layer-network, no further tests have been conducted.o Less than 60 % of the tagging errors were made in commonby both taggers.so that in these cases an additional tag can be givenin the output.
In this way, the final decision can bedelayed to a later processing stage, e.g.
a parser.A disadvantage of the presented method may be itslower processing speed compared to statistical meth-ods.
In the light of the high speed of present computerhardware, however, this does not seem to be a seriousdrawback.8 REFERENCESChurch, K. W. (1985).
A stochastic parts programand noun phrase parser for unrestricted text.
Pro-ceedings of the Second Conference on Applied NatvralLanguage Processing, p. 136-143.Cutting, D., a. Kupiec, a. Pedersen and P. Sibun(1992).
A practical part-of-speech tagger.
Proceedingsof the Third Conference on Applied Nalural LaguageProcessing, r1?ento, Italy (ACL), pages 133-140, 1992.Also awtilable as Xerox technical report SSL-92-01.Federici, S. and V. Pirrelli (1993).
Analogical mod-elling of text tagging, unpublished report, Istituto diLinguistica Computazionale, Pisa, Italy.Greene, 1t.
B and G. M. R.ubin (1971).
Auto-matic grammatical tagging of English.
technical re-port, Department of Linguistics, Brown University,Providence, Rhode Island.aelinek, F. (1985).
Markov Source modeling of textgeneration".
In J.K. Skwirzinski Ed., Impact of Pro-cessing Techniques on Communication, Nijhoff, Dor-drecht.Kempe, A.
(1993).
A stochastic Tagger and anAnalysis of Tagging Errors.
Internal paper.
In-stitute for Computational Linguistics, University ofStuttgart.Lippmann, R.. P. (1989).
Review of Neural Networksfor Speech Recognition.
Neural Computation, Vol.
i ,p. 1-38.Nakamura, M., I(.
Marnyama, T. Kawabata and K.Shikano (1990).
Neural network approach to word cat-egory prediction for Englis}i texts.
In iI.
l(arlgren Ed.,COLING-90, lIelslnki University, p. 213-218.Rumelhart, D. E. and J. L. McClelland (1984).
Par-allel Distributed Processing.
MIT-Press, Cambridge,MA.176
