Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,pages 81?85, Dublin, Ireland, August 23-29 2014.Nerdle: Topic-Specific Question Answering Using Wikia SeedsUmar Maqsud, Sebastian Arnold, Michael H?ulfenhaus and Alan AkbikDatabase Systems and Information Management GroupTechnische Universit?at BerlinEinsteinufer 17, 10587 Berlin, GermanyAbstractThe WIKIA project maintains wikis across a diverse range of subjects from areas of popularculture.
Each wiki consists of collaboratively authored content and focuses on a particular topic,including franchises such as ?Star Trek?, ?Star Wars?
and ?The Simpsons?.
In this paper, weinvestigate the use of such wikis to create Question-Answering (QA) systems for a given topic.Our key idea is to use a wiki as seed to gather large amounts of relevant text and to use semanticrole labeling (SRL) methods to extract N-ary facts from this data.
By applying our method to verylarge amounts of topically focused text, we propose to address the coverage issues that have beennoted for QA systems built using such techniques.
To illustrate the strengths and weaknessesof the proposed approach, we make a Web demonstrator of our system publicly available; itprovides a QA view that enables users to pose natural language questions to the system and thatvisualizes how questions are interpreted and matched to answers.
In addition, the demonstratorprovides a graph exploration view in which users can directly browse the fact base in order toinspect the scope of the extracted information.1 IntroductionThe WIKIA project operates the largest network of collaboratively authored wikis, consisting of over390.000 wikis on subjects such as games, entertainment and lifestyle, and is available online atwww.wikia.com.
Each wiki is focused on one particular topic and may consist of tens of thousandsof pages of text content.
Such data, we argue, provides a unique opportunity for extracting structuredrelational data confined to one domain of interest.In this paper, we investigate such an approach; our overall goal is to automatically create QuestionAnswering (QA) systems that are ?experts?
in one field of interest.
Our key idea is to use wikis asseeds to a focused crawler to gather as much text as possible for a given topic.
The more text we cangather, the greater the chance that we can address coverage issues that related works in QA have noted:Phenomena such as coreferences, synonyms and paraphrasing may negatively affect a QA systems abilityto find answers to questions that are phrased differently from occurrences in text (Fader et al., 2013;Ravichandran and Hovy, 2002).
By gathering large amounts of topically relevant text and by employingsemantic role labeling (SRL) as a means for fact extraction and representation (Shen and Lapata, 2007),our hypothesis is that we can sidestep many of these issues.The main purpose of this demonstration is therefore to illustrate and discuss in how far a direct ap-plication of SRL to domain-specific text can be employed to create a QA system.
To this end, we makepublicly available a Web demonstrator of a QA system on three topics of popular culture, namely the?Star Trek?, ?Star Wars?
and ?The Simpsons?
franchises.
Our system, named NERDLE, supports eighttypes of questions, examples of which are given in Table 1.
The fact base on these topics is gatheredwith our proposed approach.
The demonstrator offers two views that highlight different aspects of thesystem: The QA view allows users to pose natural language questions and visualizes how questions areThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/81STAR TREK THE SIMPSONS STAR WARSWHO Who attacked the Enterprise?
Who is Homer?s father?
Who destroyed the Death Star?WHERE Where was Picard born?
Where was Homer born?
Where was the Death Star destroyed?WHEN When did James Kirk die?
When was Homer born?
When was Vader killed?HOW How did James Kirk die?
How does Homer work?
How did Luke destroy the Death Star?WHOM Whom did the Klingons attack?
Whom does Homer know?
Whom did Luke Skywalker attack?WHAT What are Klingons?
What does Bart think?
What is the Rebel Alliance?WHY Why was Voyager destroyed?
Why was Homer sad?
Why does Luke Skywalker die?WHICH Which captain Which food critic Which Jediwas born on Earth?
was born in Springfield?
attacked the Death Star?Table 1: The eight question types currently supported by our system and example questions for each ofthe three topics.interpreted and how they are matched to answers.
The graph exploration view enables users to directlybrowse the fact base in order to inspect the scope of the extracted information.In the following sections, we briefly describe our method for using WIKIA to gather text data for agiven topic.
We discuss how we employ SRL for fact extraction and representation and give details onour method for aligning eight types of natural language questions to these facts.
Finally, we discuss thevisualization and results of the created QA system.2 MethodIn this section, we describe our method for gathering relevant text using a wiki seed and extracting factsusing SRL.
As a running example, we use the ?Star Trek?-universe as the topic of interest.2.1 Constructing Topic-Specific CorporaAs a first step, we select the appropriate wiki for the given topic and retrieve all its text as well as thenames of the page titles.
For the ?Star Trek?
example, the WIKIA project offers two wikis, namelyMemory Alpha and Memory Beta1, both of which we select as relevant.
Using these wikis, we downloadover 95.000 pages or 250 MB of text content for the domain of interest.We then determine combinations of search keywords using the page titles and use these as queries insearch engines like BING2or FAROO3.
Examples of such queries are ?Kirk Spock Star Trek?
or ?PicardData Star Trek?.
While FAROO (unlike BING) does not limit the number of allowed queries per month,its index is much smaller.
We therefore developed the following strategy to use the FAROO index in ourtext gathering effort: For each combination of search keywords, we retrieve all matching pages and thenfollow their outgoing links to find more possibly related Web pages.
We check each Web page to containat least one mention of the domain keyword (?Star Trek?)
as a simple sanity check to ensure that thecrawler has not left the domain of interest.
We download all pages we reach with this method.Using this method, we find over 500 MB of text for the ?Star Trek?-domain.
As this is an ongoingeffort, the corpus size is expected to expand further.
The generated corpus is then passed to the factextraction step of the pipeline.2.2 Fact ExtractionWe detect English language sentences in the gathered corpus and apply SRL to detect predicate-argumentstructures for each verb.
We use the ClearNLP toolkit (Choi and Adviser-Palmer, 2012) for this task;it links each predicate to a PROPBANK (Martha and Palmer, 2002) verb sense and its arguments toPROPBANK semantic roles.
We choose PROPBANK over FRAMENET (Baker et al., 1998) as it modelssemantics more broadly and has a more complete coverage of verb frames.
For our purpose, we areespecially interested in the argument roles: PROPBANK gives us verb-specific argument roles as wellas universal roles such as temporals (TMP), locatives (LOC), causal adverbials (CAU) and adverbials1Available at http://en.memory-alpha.org/ and http://memory-beta.wikia.com/ respectively.2http://www.bing.com3http://www.faroo.com/82Figure 1: The QA-view of the Web demonstrator.
The user types in a question and receives a list ofarguments as answers below.
For the question ?Who attacked the Enterprise??
the user receives a totalof 38 answers, three of which are shown here, namely ?Taryn?s ship?, ?an Orion scout ship?
and ?theNgultor?.
By clicking on one of the answers, the user inspects the predicate-argument structures of bothquestion (lower right graph) and the answer (lower left graph), as well as the sentence in which theanswer is found.of manner (MNR) and purpose (PRP).
In this work, we treat the predicate-argument structures as N-aryfacts and store these in a graph database.To illustrate the fact extraction process, consider the sentence ?In 2254, the Enterprise was attackedby the Ngultor?.
Using SRL, we determine a predicate-argument structure in which ?attack?
is the pred-icate and there are three arguments: Two arguments with verb-specific roles, namely ?by the Ngultor?
(attacker) and ?the Enterprise?
(that which is attacked), as well as the argument ?in 2254?, which isrecognized to be of type AM-TMP, meaning that it confers additional temporal information to the ternaryfact.
This predicate-argument structure is illustrated in Figure 1.2.3 Question ParsingThe question parsing process is similar to fact extraction; we apply SRL to a question to determine itspredicate-argument structure.
We then try to find matching predicate-argument structures in the fact baseby searching for facts that share the same predicate and as many arguments as possible.
The greater thenumber of matching arguments, the higher the score of the matching fact.
In addition, we require match-ing facts to also contain an answer argument labeled with a specific semantic role which is determinedthrough the question type.We allow seven basic types of questions and one type of composite question.
The basic question typeswe support are factual questions beginning with the question words ?who?, ?where?, ?when?, ?whom?,?what?, ?how?
and ?why?.
Depending on the question type, we require answer arguments to be labeledwith a different semantic role.
For ?where?-questions, for example, we require answer arguments to belabeled as an AM-LOC argument.
For ?when?-questions, the answer argument must be labeled as anAM-TMP argument.
For ?who?-questions, we require an argument that shares the semantic role of the83question word, which typically will be either A0 or A1.
These answer arguments are returned answers tothe question.We illustrate this in Figure 1.
The question ?Who attacked the Enterprise??
is parsed into a predicate-argument structure.
It is aligned with the predicate-argument structure of the sentence ?In 2254 theEnterprise was attacked by the Ngultor?
because they share the same predicate as well as the argument?the Enterprise?
with the role A1.
Of the two remaining arguments, namely ?in 2254?
and ?the Ngultor?the latter is selected as the answer argument because it carries the same semantic role as was assigned tothe token ?who?
in the question, namely A0.
In case of a ?when?-question, ?in 2254?
would instead beselected as answer argument, as it is labeled with the required AM-TMP role.In addition, we support one type of composite question, namely questions beginning with ?which?,as in ?Which captain was born on Earth??.
Such questions are decomposed into two separate ?who?-questions, namely ?Who is a captain??
and ?Who was born on Earth??.
We then determine answerarguments that match both questions and return these.3 DemonstrationWe present a Web demonstrator4in which users can query the fact base in one of two views:QA view.
In this view, users pose natural language questions and are presented with matching answersif they exist.
For each answer, both the source sentences as well as the URLs to the original Web pagesare displayed.
Answers are ranked by a score which is determined through the number of matchingarguments between the predicate-argument structures of the question and the answer.
As illustrated inFigure 1, users inspect a visualization of these predicate-argument structures.
This view is primarilydesigned to aid with understanding issues with precision, i.e.
to understand how answers to questionscome to be and how fact extraction and question parsing function.Graph exploration view.
In this view, users can browse the graph database directly for facts.
Togetherwith the QA view, this view is designed to examine issues of recall, i.e.
to help understand the scope ofthe extracted information and why some questions are not answered.4 DiscussionWith our method, we find a total of 7 million facts for ?Star Trek?, 6.5 million for ?Star Wars?
and, dueto its smaller wiki size, 3.5 million for ?The Simpsons?.
Next to the availability of large amounts of Webtext, our focus on topics of popular culture has the advantage that there are a large number of resourcesavailable online that can be used to analyze the QA capabilities of our system.
In our analysis, we makeuse of ABSURDTRIVIA5, a community powered Web site where users write and rate trivia quizzes onitems of popular culture.
The trivia quizzes consist of a set of multiple-choice questions.
We crawl 50of these questions on the NERDLE topics that conform to our question types and pose them to NERDLE.We find that NERDLE chooses the correct answer for 16 questions, a wrong answer for 5 questions andno answer at all for 29 questions.Our demonstrator allows us to inspect wrong and unanswered questions.
We find that the system ofteneither lacks the correct facts in the fact base or cannot align questions to answers due to problems ofsynonymy, entailment and coreferences.
An example of this is the question ?Who played Phlox??
towhich no answer is found, while the correct answer is found for ?Who portrayed Phlox??.
This suggeststhat the coverage of the system might be improved by adding knowledge on synonymous argumentsas well as synonymous or entailing verbs.
Future work will accordingly examine how synonyms andentailment could be added to improve the coverage of the system.
One idea is to leverage wiki pagelinks to identify synonymous entities similar to the work presented in (Spitkovsky and Chang, 2012).
Inaddition, we will expand our crawling efforts to gather larger text corpora and add more question typesto the question parser.Future work will continue to emphasize the visualization of question parsing and answer alignment inorder to aid discussion with the research community about the strengths and limitations of SRL for QA.4The demonstrator is available online at http://www.textmining.tu-berlin.de/nerdle/5http://www.absurdtrivia.com84AcknowledgementsWe would like to thank the anonymous reviewers for their helpful comments.
Umar Maqsud, Sebastian Arnold, MichaelH?ulfenhaus and Alan Akbik received funding from the European Union?s Seventh Framework Programme (FP7/2007-2013)under grant agreement no ICT-2009-4-1 270137 ?Scalable Preservation Environments?
(SCAPE).ReferencesCollin F Baker, Charles J Fillmore, and John B Lowe.
1998.
The berkeley framenet project.
In Proceedings of the 36thAnnual Meeting of the Association for Computational Linguistics and 17th International Conference on ComputationalLinguistics-Volume 1, pages 86?90.
Association for Computational Linguistics.Jinho D Choi and Martha Adviser-Palmer.
2012.
Optimization of natural language processing components for robustness andscalability.Anthony Fader, Luke S Zettlemoyer, and Oren Etzioni.
2013.
Paraphrase-driven learning for open question answering.
In ACL(1), pages 1608?1618.Paul Kingsbury Martha and Martha Palmer.
2002.
From treebank to propbank.Deepak Ravichandran and Eduard Hovy.
2002.
Learning surface text patterns for a question answering system.
In Proceedingsof the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47.
Association for ComputationalLinguistics.Dan Shen and Mirella Lapata.
2007.
Using semantic roles to improve question answering.
In EMNLP-CoNLL, pages 12?21.Citeseer.Valentin I Spitkovsky and Angel X Chang.
2012.
A cross-lingual dictionary for english wikipedia concepts.
In LREC, pages3168?3175.85
