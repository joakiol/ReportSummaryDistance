In: Proceedings of CoNLL-2000 and LLL-2000, pages 127-132, Lisbon, Portugal, 2000.Introduction to the CoNLL-2000 Shared Task: ChunkingErik F. Tjong Kim SangCNTS - Language Technology GroupUniversity of Antwerper ik tOu ia ,  ua.
ac.
beSabine Buchho lzILK, Computat iona l  LinguisticsTi lburg Universitys.
buchholz@kub, nlAbst rac tWe describe the CoNLL-2000 shared task:dividing text into syntactically related non-overlapping groups of words, so-called textchunking.
We give background information onthe data sets, present a general overview of thesystems that have taken part in the shared taskand briefly discuss their performance.1 In t roduct ionText chunking is a useful preprocessing stepfor parsing.
There has been a large inter-est in recognizing non-overlapping oun phrases(Ramshaw and Marcus (1995) and follow-up pa-pers) but relatively little has been written aboutidentifying phrases of other syntactic ategories.The CoNLL-2000 shared task attempts to fillthis gap.2 Task  descr ip t ionText chunking consists of dividing a text intophrases in such a way that syntactically re-lated words become member of the same phrase.These phrases are non-overlapping which meansthat one word can only be a member of onechunk.
Here is an example sentence:\[NP He \] \[vP reckons \] \[NP the currentaccount deficit \] \[vP will narrow \]\[pp to \] \[NP only ?
1.8 billion \]\[pp in \]\[NP September \] .Chunks have been represented as groups ofwords between square brackets.
A tag next tothe open bracket denotes the type of the chunk.As far as we know, there are no annotated cor-pora available which contain specific informa-tion about dividing sentences into chunks ofwords of arbitrary types.
We have chosen towork with a corpus with parse information, theWall  Street Journal WSJ  part of the Penn Tree-bank  II corpus (Marcus et al, 1993), and to ex-tract chunk information from the parse trees inthis corpus.
We will give a global description ofthe various chunk types in the next section.3 Chunk  TypesThe  chunk types are based on the syntactic cat-egory part (i.e.
without function tag) of thebracket label in the Treebank (cf.
Bies (1995)p.35).
Roughly, a chunk contains everything tothe left of and including the syntactic head ofthe constituent of the same name.
Some Tree-bank  constituents do not have related chunks.The  head of S (simple declarative clause) for ex-ample is normally thought to be the verb, butas the verb is already part of the VP  chunk, noS chunk exists in our example  sentence.Besides the head, a chunk also contains pre-modifiers (like determiners and adjectives inNPs), but no postmodifiers or arguments.
Thisis why  the PP  chunk only contains the preposi-tion, and not the argument  NP,  and the SBARchunk consists of only the complementizer.There are several difficulties when convertingtrees into chunks.
In the most  simple case, achunk is just a syntactic constituent withoutany further embedded constituents, like the NPsin our examples.
In some cases, the chunk con-tains only what  is left after other chunks havebeen removed from the constituent, cf. "
(VPloves (NP  Mary))"  above, or ADJPs  and PPsbelow.
We will discuss some special cases dur-ing the following description of the individualchunk types.3.1 NPOur  NP  chunks are very similar to the ones ofRamshaw and Marcus  (1995).
Specifically, pos-sessive NP  constructions are split in front ofthe possessive marker  (e.g.
\[NP Eastern Air-lines \] \ [NP ' creditors \]) and the handling of co-127ordinated NPs follows the Treebank annotators.However, as Ramshaw and Marcus do not de-scribe the details of their conversion algorithm,results may differ in difficult cases, e.g.
involv-ing NAC and NX.
1An ADJP constituent inside an NP con-stituent becomes part of the NP chunk:(NP The (ADJP most volatile) form)\[NP the most volatile form \]3.2 VPIn the Treebank, verb phrases are highly embed-ded; see e.g.
the following sentence which con-tains four VP constituents.
Following Ramshawand Marcus' V-type chunks, this sentence willonly contain one VP chunk:((S (NP-SBJ-3 Mr. Icahn) (VP maynot (VP want (S (NP-SBJ *-3) (VP to(VP sell ...))))) .
))\[NP Mr. Icahn \] \[vP may not wantto sell \] ...It is still possible however to have one VP chunkdirectly follow another: \[NP The impression \]\[NP I \ ]  \[VP have got  \] \[vP is \] \[NP they\]  \[vP 'dlove to do \] \[PRT away \] \[pp with \] \[NP it \].
In thiscase the two VP constituents did not overlap inthe Treebank.Adverbs/adverbial phrases becorae part ofthe VP chunk (as long as they are in front ofthe main verb):(VP could (ADVP very well) (VPshow .
.
.  )
)"-+ \[ve could very well show \] ...In contrast to Ramshaw and Marcus (1995),predicative adjectives of the verb are not partof the VP chunk, e.g.
in "\[NP they \] \[vP are \]\[ADJP unhappy \]'.In inverted sentences, the auxiliary verb is notpart of any verb phrase in the Treebank.
Con-sequently it does not belong to any VP chunk:((S (SINV (CONJP Not only) does(NP-SBJ-1 your product) (VP have (SIE.g.
(NP-SBJ (NP Robin Leigh-Pemberton) , (NP(NAC Bank (PP of (NP England))) governor) ,) whichwe convert to \[NP Robin Leigh-Pemberton \] , Bank\[pp of \] \[NP England \] \[NP governor \] whereas Ramshawand Marcus state that ' "governor" is not included inany baseNP chunk'.
(NP-SBJ *-1) (VP to (VP be (ADJP-PRD excellent)))))) , but ...\[CONJP Not only \] does \[NP yourproduct \] \[vP have to be \] \[ADJP ex-cellent \] , but ...3.3 ADVP and  ADJPADVP chunks mostly correspond to ADVP con-stituents in the Treebank.
However, ADVPs in-side ADJPs or inside VPs if in front of the mainverb are assimilated into the ADJP respectivelyVP chunk.
On the other hand, ADVPs thatcontain an NP make two chunks:(ADVP-TMP (NP a year) earlier)-+ \[NP a year \] \[ADVP earlier \]ADJPs inside NPs are assimilated into the NP.And parallel to ADVPs, ADJPs that contain anNP make two chunks:(ADJP-PRD (NP 68 years) old)\[NP 68 years \] \[ADJP old \]It would be interesting to see how chang-ing these decisions (as can be done in theTreebank-to-chunk conversion script 2) infiu-ences the chunking task.3.4 PP  and  SBARMost PP  chunks just consist of one word (thepreposition) with the part-of-speech tag IN.This does not mean, though, that finding PPchunks is completely trivial.
INs can also con-stitute an SBAR chunk (see below) and somePP  chunks contain more than one word.
Thisis the case with fixed multi-word prepositionssuch as such as, because of, due to, with prepo-sitions preceded by a modifier: well above, justafter, even in, particularly among or with coor-dinated prepositions: inside and outside.
Wethink that PPs behave sufficiently differentlyfrom NPs in a sentence for not wanting to groupthem into one class (as Ramshaw and Marcusdid in their N-type chunks), and that on theother hand tagging all NP chunks inside a PPas I -PP would only confuse the chunker.
Wetherefore chose not to handle the recognition oftrue PPs (prep.+NP) during this first chunkingstep.~The Treebank-to-chunk conversion script is availablefrom http://ilk.kub.nl/-sabine/chunklink/128SBAR Chunks mostly consist of one word (thecomplementizer) with the part-of-speech tag IN,but like multi-word prepositions, there are alsomulti-word complementizers: even though, sothat, just as, even if, as if, only if.3.5 CONJP ,  PRT,  INT J ,  LST, UCPConjunctions can consist of more than one wordas well: as well as, instead of, rather than, notonly, but also.
One-word conjunctions (like and,or) are not annotated as CONJP in the Tree-bank, and are consequently no CONJP chunksin our data.The Treebank uses the PRT constituent toannotate verb particles, and our PRT chunkdoes the same.
The only multi-word particleis on and off.
This chunk type should be easyto recognize as it should coincide with the part-of-speech tag RP, but through tagging errors itis sometimes also assigned IN (preposition) orRB (adverb).INTJ is an interjection phrase/chunk li e no,oh, hello, alas, good grief!.
It is quite rare.The list marker LST is even rarer.
Examplesare 1., 2 ,  3., .first, second, a, b, c. It might con-sist of two words: the number and the period.The UCP chunk is reminiscent of the UCP(unlike coordinated phrase) constituent in theTreebank.
Arguably, the conjunction is thehead of the UCP, so most UCP chunks consistof conjunctions like and and or.
UCPs are therarest chunks and are probably not very usefulfor other NLP tasks.3.6 Tokens outs ideTokens outside any chunk are mostly punctua-tion signs and the conjunctions inordinary coor-dinated phrases.
The word not may also be out-side of any chunk.
This happens in two cases:Either not is not inside the VP constituent inthe Treebank annotation e.g.
in... (VP have (VP told (NP-1 clients)(S (NP-SBJ *-1) not (VP to (VP ship(NP anything))))))or not is not followed by another verb (becausethe main verb is a form of to be).
As the rightchunk boundary is defined by the chunk's head,i.e.
the main verb in this case, not is thenin facta postmodifier and as such not included in thechunk: "... \[SBAR that \] \[NP there \] \[vP were \]n't \[NP any major problems \].
"3.7 P rob lemsAll chunks were automatically extracted fromthe parsed version of the Treebank, guided bythe tree structure, the syntactic onstituent la-bels, the part-of-speech tags and by knowledgeabout which tags can be heads of which con-stituents.
However, some trees are very complexand some annotations are inconsistent.
Whatto think about a VP in which the main verb istagged as NN (common oun)?
Either we al-low NNs as heads of VPs (not very elegant butwhich is what we did) or we have a VP withouta head.
The first solution might also introduceerrors elsewhere... As Ramshaw and Marcus(1995) already noted: "While this automaticderivation process introduced a small percent-age of errors on its own, it was the only practi-cal way both to provide the amount of trainingdata required and to allow for fully-automatictesting.
"4 Data  and Eva luat ionFor the CoNLL shared task, we have chosento work with the same sections of the PennTreebank as the widely used data set for basenoun phrase recognition (Ramshaw and Mar-cus, 1995): WSJ sections 15-18 of the PennTreebank as training material and section 20as test material 3.
The chunks in the datawere selected to match the descriptions in theprevious section.
An overview of the chunktypes in the training data can be found in ta-ble 1.
De data sets contain tokens (words andpunctuation marks), information about the lo-cation of sentence boundaries and informationabout chunk boundaries.
Additionally, a part-of-speech (POS) tag was assigned to each tokenby a standard POS tagger (Brill (1994) trainedon the Penn Treebank).
We used these POStags rather than the Treebank ones in order tomake sure that the performance rates obtainedfor this data are realistic estimates for data forwhich no treebank POS tags are available.In our example sentence in section 2, we haveused brackets for encoding text chunks.
In thedata sets we have represented chunks with threetypes of tags:3The text chunking data set is available at http://lcg-www.uia.ac.be/conll2000/chunking/129count %55081 51%21467 20%21281 20%4227 4%2207 2%2060 2%556 1%56 O%31 0%10 O%2 0%typeNP (noun phrase)VP (verb phrase)PP (prepositional phrase)ADVP (adverb phrase)SBAR (subordinated clause)ADJP (adjective phrase)PRT (particles)CONJP (conjunction phraseINTJ (interjection)LST (list marker)UCP (unlike coordinated phrase)Table 1: Number of chunks per phrase typein the training data (211727 tokens, 106978chunks).B-XI-X0first word of a chunk of type Xnon-initial word in an X chunkword outside of any chunkThis representation type is based on a repre-sentation proposed by Ramshaw and Marcus(1995) for noun phrase chunks.
The three taggroups are sufficient for encoding the chunks inthe data since these are non-overlapping.
Usingthese chunk tags makes it possible to approachthe chunking task as a word classification task.We can use chunk tags for representing our ex-ample sentence in the following way:He/B-NP reckons/B-VP the/B-NPcurrent/I-NP account/I-NPdeficit/I-NP will/B-VP narrow/I-VPto/B-PP only/B-NP #/ I -NP1.8/I-NP billion/B-NP in/B-PPSeptember/B-NP ./OThe output of a chunk recognizer may containinconsistencies in the chunk tags in case a wordtagged I-X follows a word tagged O or I-Y, withX and Y being different.
These inconsistenciescan be resolved by assuming that such I-X tagsstart a new chunk.The performance on this task is measuredwith three rates.
First, the percentage ofdetected phrases that are correct (precision).Second, the percentage of phrases in thedata that were found by the chunker (recall).And third, the FZ=i rate which is equal to(f12 + 1)*precision*recall / (~2,precision+recall)with ~=1 (van Rijsbergen, 1975).
The lat-ter rate has been used as the target foroptimization 4.5 Resul tsThe eleven systems that have been applied tothe CoNLL-2000 shared task can be divided infour groups:1.
Rule-based systems: Villain and Day; Jo-hansson; D6jean.2.
Memory-based systems: Veenstra nd Vanden Bosch.3.
Statistical systems: Pla, Molina and Pri-eto; Osborne; Koeling; Zhou, Tey and Su.4.
Combined systems: Tjong Kim Sang; VanHalteren; Kudoh and Matsumoto.Vilain and Day (2000) approached the sharedtask in three different ways.
The most success-ful was an application of the Alembic parserwhich uses transformation-based rules.
Johans-son (2000) uses context-sensitive and context-free rules for transforming part-of-speech (POS)tag sequences to chunk tag sequences.
D6jean(2000) has applied the theory refinement sys-tem ALLiS to the shared task.
In order to ob-tain a system which could process XML format-ted data while using context information, hehas used three extra tools.
Veenstra and Vanden Bosch (2000) examined ifferent parame-ter settings of a memory-based learning algo-rithm.
They found that modified value differ-ence metric applied to POS information onlyworked best.A large number of the systems applied tothe CoNLL-2000 shared task uses statisticalmethods.
Pla, Molina and Prieto (2000) usea finite-state version of Markov Models.
Theystarted with using POS information only andobtained a better performance when lexicalinformation was used.
Zhou, Tey and Su(2000) implemented a chunk tagger based onHMMs.
The initial performance of the tag-ger was improved by a post-process correctionmethod based on error driven learning and by4In the literature about related tasks sometimes thetagging accuracy is mentioned as well.
However, sincethe relation between tag accuracy and chunk precisionand recall is not very strict, tagging accuracy is not agood evaluation measure for this task.130test dataKudoh and MatsumotoVan HalterenTjong Kim SangZhou, Tey and SuD@jeanKoelingOsborneVeenstra nd Van den BoschPla, Molina and PrietoJohanssonVilain and Daybaselineprecision93.45%93.13%94.04%91.99%91.87%92.08%91.65%91.05%90.63%86.24%88.82%72.58%recall93.51%93.51%91.00%92.25%91.31%91.86%92.23%92.03%89.65%88.25%82.91%82.14%F~=i93.4893.3292.5092.1292.0991.9791.9491.5490.1487.2385.7677.07Table 2: Performance of the eleven systems on the test data.
The baseline results have beenobtained by selecting the most frequent chunk tag for each part-of-speech tag.incorporating chunk probabilities generated bya memory-based learning process.
The twoother statistical systems use maximum-entropybased methods.
Osborne (2000) trained Ratna-parkhi's maximum-entropy POS tagger to out-put chunk tags.
Koeling (2000) used a stan-dard maximum-entropy learner for generatingchunk tags from words and POS tags.
Bothhave tested different feature combinations be-fore finding an optimal one and their final re-sults are close to each other.Three systems use system combination.Tjong Kim Sang (2000) trained and tested fivememory-based learning systems to produce dif-ferent representations of the chunk tags.
Acombination of the five by majority voting per-formed better than the individual parts.
VanHalteren (2000) used Weighted Probability Dis-tribution Voting (WPDV) for combining theresults of four WPDV chunk taggers and amemory-based chunk tagger.
Again the com-bination outperformed the individual systems.Kudoh and Matsumoto (2000) created 231 sup-port vector machine classifiers to predict theunique pairs of chunk tags.
The results of theclassifiers were combined by a dynamic pro-gramming algorithm.The performance of the systems can be foundin Table 2.
A baseline performance was ob-tained by selecting the chunk tag most fre-quently associated with a POS tag.
All systemsoutperform the baseline.
The majority of thesystems reached an F~=i score between 91.50and 92.50.
Two approaches performed a lotbetter: the combination system WPDV used byVan Halteren and the Support Vector Machinesused by Kudoh and Matsumoto.6 Re la ted  WorkIn the early nineties, Abney (1991) proposedto approach parsing by starting with findingrelated chunks of words.
By then, Church(1988) had already reported on recognitionof base noun phrases with statistical meth-ods.
Ramshaw and Marcus (1995) approachedchunking by using a machine learning method.Their work has inspired many others to studythe application of learning methods to nounphrase chunking 5.
Other chunk types have notreceived the same attention as NP chunks.
Themost complete work is Buchholz et al (1999),which presents results for NP, VP, PP, ADJPand ADVP chunks.
Veenstra (1999) works withNP, VP and PP chunks.
Both he and Buchholzet al use data generated by the script that pro-duced the CoNLL-2000 shared task data sets.Ratnaparkhi (1998) has recognized arbitrarychunks as part of a parsing task but did not re-port on the chunking performance.
Part of theSparkle project has concentrated on finding var-ious sorts of chunks for the different languages~An elaborate overview of the work done on nounphrase chunking can be found on http://lcg-www.uia.ac.be/- erikt/reseaxch/np-chunking.html131(Carroll et al, 1997).
'7 Conc lud ing  RemarksWe have presented an introduction to theCoNLL-2000 shared task: dividing text intosyntactically related non-overlapping groups ofwords, so-called text chunking.
For this task wehave generated training and test data from thePenn Treebank.
This data has been processedby eleven systems.
The best performing systemwas a combination of Support Vector Machinessubmitted by Taku Kudoh and Yuji Matsumoto.It obtained an FZ=i score of 93.48 on this task.AcknowledgementsWe would like to thank the members ofthe CNTS - Language Technology Group inAntwerp, Belgium and the members of the ILKgroup in Tilburg, The Netherlands for valuablediscussions and comments.
Tjong Kim Sang isfunded by the European TMR network Learn-ing Computational Grammars.
Buchholz is sup-ported by the Netherlands Organization for Sci-entific Research (NWO).Re ferencesSteven Abney.
1991.
Parsing by chunks.
InPrinciple-Based Parsing.
Kluwer Academic Pub-lishers.Ann Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre.
1995.
Bracket Guidelines /or Tree-bank H Style Penn Treebank Project.
Penn Tree-bank II cdrom.Eric Brill.
1994.
Some advances in rule-basedpart of speech tagging.
In Proceedings o\] theTwelfth National Con/erence on Artificial Intel-ligence (AAAI-9~).
Seattle, Washington.Sabine Buchholz, Jorn Veenstra, and Walter Daele-mans.
1999.
Cascaded grammatical :relation as-signment.
In Proceedings o\] EMNLP/VLC-99.Association for Computational Linguistics.John Carroll, Ted Briscoe, Glenn Carroll, MarcLight, Dethleff Prescher, Mats Rooth, StefanoFederici, Simonetta Montemagni, Vito Pirrelli,Irina Prodanof, and Massimo Vanocchi.
1997.Phrasal Parsing Software.
Sparkle Work Package3, Deliverable D3.2.Kenneth Ward Church.
1988.
A stochastic partsprogram and noun phrase parser for unrestrictedtext.
In Second Con\]erence on Applied NaturalLanguage Processing.
Austin, Texas.H@rve D@jean.
2000.
Learning syntactic structureswith xml.
In Proceedings o/ CoN~LL-2000 andLLL-2000.
Lisbon, Portugal.Christer Johansson.
2000.
A context sensitive max-imum likelihood approach to chunking.
In Pro-ceedings o\] CoNLL-2000 and LLL-2000.
Lisbon,Portugal.Rob Koeling.
2000.
Chunking with maximum en-tropy models.
In Proceedings o/ CoNLL-2000 andLLL-2000.
Lisbon, Portugal.Taku Kudoh and Yuji Matsumoto.
2000.
Use of sup-port vector learning for chunk identification.
InProceedings o~ CoNLL-2000 and LLL-2000.
Lis-bon, Portugal.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building a largeannotated corpus of english: the penn treebank.Computational Linguistics, 19(2).Miles Osborne.
2000.
Shallow parsing as part-of-speech tagging.
In Proceedings o\] CoNLL-2000and LLL-2000.
Lisbon, Portugal.Ferran Pla, Antonio Molina, and Natividad Pri-eto.
2000.
Improving chunking by means oflexical-contextual information in statistical lan-guage models.
In Proceedings o\] CoNLL-2000 andLLL-2000.
Lisbon, Portugal.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learn-ing.
In Proceedings o\] the Third A CL Workshopon Very Large Corpora.
Association for Compu-tational Linguistics.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels \]or Natural Language Ambiguity Resolu-tion.
PhD thesis Computer and Information Sci-ence, University of Pennsylvania.Erik F. Tjong Kim Sang.
2000.
Text chunking bysystem combination.
In Proceedings of CoNLL-2000 and LLL-2000.
Lisbon, Portugal.Hans van Halteren.
2000.
Chunking with wpdvmodels.
In Proceedings o/ CoNLL-2000 and LLL-2000.
Lisbon, Portugal.C.J.
van Rijsbergen.
1975.
In/ormation Retrieval.Buttersworth.Jorn Veenstra and Antal van den Bosch.
2000.Single-classifier memory-based phrase chunking.In Proceedings o\] CoNLL-2000 and LLL-2000.Lisbon, Portugal.Jorn Veenstra.
1999.
Memory-based text chunking.In Nikos Fakotakis, editor, Machine learning inhuman language technology, workshop at ACAI99.Marc Vilain and David Day.
2000.
Phrase parsingwith rule sequence processors: an application tothe shared conll task.
In Proceedings o/CoNLL-2000 and LLL-2000.
Lisbon, Portugal.GuoDong Zhou, Jian Su, and TongGuan Tey.
2000.Hybrid text chunking.
In Proceedings o/CoNLL-2000 and LLL-2000.
Lisbon, Portugal.132
