Inducing Information Extraction Systems for New Languagesvia Cross-Language ProjectionEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.eduCharles Schafer and David YarowskyDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218{cschafer,yarowsky}@cs.jhu.eduAbstractInformation extraction (IE) systems are costly tobuild because they require development texts, pars-ing tools, and specialized dictionaries for each ap-plication domain and each natural language thatneeds to be processed.
We present a novelmethod for rapidly creating IE systems for new lan-guages by exploiting existing IE systems via cross-language projection.
Given an IE system for asource language (e.g., English), we can transfer itsannotations to corresponding texts in a target lan-guage (e.g., French) and learn information extrac-tion rules for the new language automatically.
Inthis paper, we explore several ways of realizing boththe transfer and learning processes using off-the-shelf machine translation systems, induced wordalignment, attribute projection, and transformation-based learning.
We present a variety of experimentsthat show how an English IE system for a planecrash domain can be leveraged to automatically cre-ate a French IE system for the same domain.1 IntroductionInformation extraction (IE) is an important appli-cation for natural language processing, and recentresearch has made great strides toward making IEsystems easily portable across domains.
However,IE systems depend on parsing tools and specializeddictionaries that are language specific, so they arenot easily portable across languages.
In this re-search, we explore the idea of using an informationextraction system designed for one language to au-tomatically create a comparable information extrac-tion system for a different language.To achieve this goal, we rely on the idea of cross-language projection.
The basic approach is the fol-lowing.
First, we create an artificial parallel cor-pus by applying an off-the-shelf machine translation(MT) system to source language text (here, English)to produce target language text (here, French).
Orconversely, in some experiments we generate a par-allel corpus by applying MT to a French corpusto produce artificial English.
We then run a wordalignment algorithm over the parallel corpus.
Next,we apply an English IE system to the English textsand project the IE annotations over to the corre-sponding French words via the induced word align-ments.
In effect, this produces an automatically an-notated French corpus.
We explore several strate-gies for transferring the English IE annotations tothe target language, including evaluation of theFrench annotations produced by the direct projec-tion alone, as well as the use of transformation-based learning to create French extraction rulesfrom the French annotations.2 Information ExtractionThe goal of information extraction systems is toidentify and extract facts from natural language text.IE systems are usually designed for a specific do-main, and the types of facts to be extracted are de-fined in advance.
In this paper, we will focus on thedomain of plane crashes and will try to extract de-scriptions of the vehicle involved in the crash, vic-tims of the crash, and the location of the crash.Most IE systems use some form of extractionpatterns to recognize and extract relevant informa-tion.
Many techniques have been developed to gen-erate extraction patterns for a new domain automat-ically, including PALKA (Kim & Moldovan, 1993),AutoSlog (Riloff, 1993), CRYSTAL (Soderland etal., 1995), RAPIER (Califf, 1998), SRV (Freitag,1998), meta-bootstrapping (Riloff & Jones, 1999),and ExDisco (Yangarber et al, 2000).
For thiswork, we will use AutoSlog-TS (Riloff, 1996b) togenerate IE patterns for the plane crash domain.AutoSlog-TS is a derivative of AutoSlog that auto-matically generates extraction patterns by gatheringstatistics from a corpus of relevant texts (within thedomain) and irrelevant texts (outside the domain).Each extraction pattern represents a linguistic ex-pression that can extract noun phrases from one ofthree syntactic positions: subject, direct object, orobject of a prepositional phrase.
For example, thefollowing patterns could extract vehicles involvedin a plane crash: ?<subject> crashed?, ?hijacked<direct-object>?, and ?wreckage of <np>?.We trained AutoSlog-TS using AP news storiesabout plane crashes as the relevant text, and APnews stories that do not mention plane crashes asthe irrelevant texts.
AutoSlog-TS generates a listof extraction patterns, ranked according to their as-sociation with the domain.
A human must reviewthis list to decide which patterns are useful for theIE task and which ones are not.
We manually re-viewed the top patterns and used the accepted pat-terns for the experiments described in this paper.
Toapply the extraction patterns to new text, we used ashallow parser called Sundance that also performsinformation extraction.3 Cross-Language Projection3.1 Motivation and Previous Projection WorkNot all languages have received equal investmentin linguistic resources and tool development.
Fora select few, resource-rich languages such as En-glish, annotated corpora and text analysis tools arereadily available.
However, for the large majorityof the world?s languages, resources such as tree-banks, part-of-speech taggers, and parsers do notexist.
And even for many of the better-supportedlanguages, cutting edge analysis tools in areas suchas information extraction are not readily available.One solution to this NLP-resource disparity isto transfer linguistic resources, tools, and do-main knowledge from resource-rich languages toresource-impoverished ones.
In recent years, therehas been a burst of projects based on this paradigm.Yarowsky et al (2001) developed cross-languageprojection models for part-of-speech tags, basenoun phrases, named-entity tags, and morpholog-ical analysis (lemmatization) for four languages.Resnik et al (2001) developed related models forprojecting dependency parsers from English to Chi-nese.
There has also been extensive work on thecross-language transfer and development of ontolo-gies and WordNets (e.g., (Atserias et al, 1997)).3.2 Mechanics of ProjectionThe cross-language projection methodology em-ployed in this paper is based on Yarowsky et al(2001), with one important exception.
Given theabsence of available naturally occurring bilingual    LOCATIONVICTIMtuant ses  20 occupantswas crushed Thursday evening in the south?east of Haiti  ,A two?motor aircraft Beechcraft of the Air?Saint?Martin companyUn avion bi?moteur Beechcraft de la compagnie Air?Saint?MartinVEHICLEkilling its 20 occupantss?
est ?cras?
jeudi soir dans le sud?est   d?
Haiti    ,..Figure 1: French text word aligned with its Englishmachine translation (extractions highlighted)corpora in our target domain, we employ commer-cial, off-the-shelf machine translation to generatean artificial parallel corpus.
While machine transla-tion errors present substantial problems, MT offersgreat opportunities because it frees cross-languageprojection research from the relatively few largeexisting bilingual corpora (such as the CanadianHansards).
MT allows projection to be performedon any corpus, such as the domain-specific plane-crash news stories employed here.
Section 5 givesthe details of the MT system and corpora that weused.Once the artificial parallel corpus has been cre-ated, we apply an English IE system to the Englishtexts and transfer the IE annotations to the targetlanguage as follows:1.
Sentence align the parallel corpus.12.
Word-align the parallel corpus using theGiza++ system (Och and Ney, 2000).3.
Transfer English IE annotations and noun-phrase boundaries to French via the mecha-nism described in Yarowsky et al (2001),yielding annotated sentence pairs as illustratedin Figure 1.4.
Train a stand-alone IE tagger on these pro-jected annotations (described in Section 4).4 Transformation-Based LearningWe used transformation-based learning (TBL)(Brill, 1995) to learn information extraction rulesfor French.
TBL is well-suited for this task becauseit uses rule templates as the basis for learning, whichcan be easily modeled after English extraction pat-terns.
However, information extraction systems typ-ically rely on a shallow parser to identify syntacticelements (e.g., subjects and direct objects) and verb1This is trivial because each sentence has a numbered an-chor preserved by the MT system.constructions (e.g., passive vs. active voice).
Ourhope was that the rules learned by TBL would be ap-plicable to new French texts without the need for aFrench parser.
One of our challenges was to designrule templates that could approximate the recogni-tion of syntactic structures well enough to duplicatemost of the functionality of a French shallow parser.When our TBL training begins, the initial state isthat no words are annotated.
We experimented withtwo sets of ?truth?
values: Sundance?s annotationsand human annotations.
We defined 56 language-independent rule templates, which can be brokendown into four sets designed to produce differenttypes of behavior.
Lexical N-gram rule templateschange the annotation of a word if the word(s) im-mediately surrounding it exactly match the rule.
Wedefined rule templates for 1, 2, and 3-grams.
InTable 1, Rules 1-3 are examples of learned Lexi-cal N-gram rules.
Lexical+POS N-gram rule tem-plates can match exact words or part-of-speech tags.Rules 4-5 are Lexical+POS N-gram rules.
Rule 5will match verb phrases such as ?went down in?,?shot down in?, and ?came down in?.One of the most important functions of a parser isto identify the subject of a sentence, which may beseveral words away from the main verb phrase.
Thisis one of the trickest behaviors to duplicate withoutthe benefit of syntactic parsing.
We designed Sub-ject Capture rule templates to identify words thatare likely to be a syntactic subject.
As an example,Rule 6 looks for an article at the beginning of a sen-tence and the word ?crashed?
a few words ahead2,and infers that the article belongs to a vehicle nounphrase.
(The NP Chaining rules described next willextend the annotation to include the rest of the nounphrase.)
Rule 7 attempts relative pronoun disam-biguation when it finds the three tokens ?COMMAwhich crashed?
and infers that the word precedingthe comma is a vehicle.Without the benefit of a parser, another challengeis identifying noun phrase boundaries.
We designedNP Chaining rule templates to look at words thathave already been labelled and extend the bound-aries of the annotation to cover a complete nounphrase.
As examples, Rules 8 and 9 extend loca-tion and victim annotations to the right, and Rule 10extends a vehicle annotation to the left.2?
is a start-of-sentence token.
w4?7means that the itemoccurs in the range of word4through word7.Rule Condition Rule Effect1.
w1=crashed w2=in w3is LOC.2.
w1=wreckage w2=of w3is VEH.3.
w1=injuring w2is VIC.4.
w1=NOUN w2=crashed w1is VEH.5.
w1=VERB w2=down w3=in w4is LOC.6.
w1=?
w2=ART w4?7=crashed w2is VEH.7.
w2=COMMA w3=which w4=crashed w1is VEH.8.
w1=in w2=LOCATION w3=NOUN w3is LOC.9.
w1=VERB w2=VICTIM w3=NOUN w3is VIC.10.
w1=ART w2=VEHICLE w1is VEH.Table 1: Examples of Learned TBL Rules(LOC.=location, VEH.=vehicle, VIC.=victim)5 ResourcesThe corpora used in these experiments were ex-tracted from English and French AP news stories.We created the corpora automatically by searchingfor articles that contain plane crash keywords.
Thenews streams for the two languages came from dif-ferent years, so the specific plane crash events de-scribed in the two corpora are disjoint.
The En-glish corpus contains roughly 420,000 words, andthe French corpus contains about 150,000 words.For each language, we hired 3 fluent universitystudents to do annotation.
We instructed the anno-tators to read each story and mark relevant entitieswith SGML-style tags.
Possible labels were loca-tion of a plane crash, vehicle involved in a crash,and victim (any persons killed, injured, or surviv-ing a crash).
We asked the annotators to align theirannotations with noun phrase boundaries.
The an-notators marked up 1/3 of the English corpus andabout 1/2 of the French corpus.We used a high-quality commercial machinetranslation (MT) program (Systran ProfessionalEdition) to generate a translated parallel corpus foreach of our English and French corpora.
These willhenceforth be referred to as MT-French (the Systrantranslation of the English text) and MT-English (theSystran translation of our French text).6 Experiments and Evaluation6.1 Scoring and Annotator AgreementWe explored two ways of measuring annotatoragreement and system performance.
(1) Theexact-word-match measure considers annotations tomatch if their start and end positions are exactly thesame.
(2) The exact-NP-match measure is more for-giving and considers annotations to match if theyboth include the head noun of the same noun phrase.The exact-word-match criterion is very conservativebecause annotators may disagree about equally ac-ceptable alternatives (e.g., ?Boeing 727?
vs. ?newBoeing 727?).
Using the exact-NP-match measure,?Boeing 727?
and ?new Boeing 727?
would con-stitute a match.
We used different tools to identifynoun phrases in English and French.
For English,we applied the base noun phrase chunker suppliedwith the fnTBL toolkit (Ngai & Florian, 2001).
InFrench, we ran a part-of-speech tagger (Cucerzan& Yarowsky, 2000) and applied regular-expressionheuristics to detect the heads of noun phrases.We measured agreement rates among our humanannotators to assess the difficulty of the IE task.
Wecomputed pairwise agreement scores among our 3English annotators and among our 3 French anno-tators.
The exact-word-match scores ranged from16-31% for French and 24-27% for English.
Theserelatively low numbers suggest that the exact-word-match criterion is too strict.
The exact-NP-matchagreement scores were much higher, ranging from43-54% for French and 51-59% for English3.These agreement numbers are still relatively low,however, which partly reflects the fact that IE is asubjective and difficult task.
Inspection of the datarevealed some systematic differences of approachamong annotators.
For example, one of the Frenchannotators marked 4.5 times as many locations asanother.
On the English side, the largest disparitywas a factor of 1.4 in the tagging of victims.6.2 Monolingual English & French EvaluationAs a key baseline for our cross-language projec-tion studies, we first evaluated the AutoSlog-TSand TBL training approaches on monolingual En-glish and French data.
Figure 2 shows (1) Englishtraining by running AutoSlog-TS on unannotatedtexts and then applying its patterns to the human-annotated English test data, (2) English training andtesting by applying TBL to the human-annotatedEnglish data with 5-fold cross-validation, (3) En-glish training by applying TBL to annotations pro-duced by Sundance (using AutoSlog-TS patterns)and then testing the TBL rules on the human-annotated English data, and (4) French training andtesting by applying TBL to human annotated Frenchdata with 5-fold cross-validation.Table 2 shows the performance in terms of Pre-cision (P), Recall (R) and F-measure (F).
Through-3Agreement rates were computed on a subset of the dataannotated by multiple people; systems were scored against thefull corpus, of which each annotator provided the standard forone third.out our experiments, AutoSlog-TS training achieveshigher precision but lower recall than TBL training.This may be due to the exhaustive coverage pro-vided by the human annotations used by TBL, com-pared to the more labor-efficient but less-completeAutoSlog-TS training that used only unannotateddata.English TEST140K words(English)SUNDANCEEnglish (plain)English (plain)(English)SUNDANCE4/5Eng TEST Eng TEST1/5French TEST1/5French TEST4/5TBLESS1ES1TS1Train TBL(1)(3)Test TBL(4) (French)  TBL TF0Train TBLTest TBLEnglish TESTS0140K words280K words280K wordsAutoslog?TSAutoslog?TS[+ 280K words irrel.
text](2) (English)  TBL T0112K words 28K wordsTrain TBLTest TBL[+ 280K words irrel.
text][cross validation][cross validation]16K words64K wordsFigure 2: Monolingual IE Evaluation pathways4Monolingual Training Route P R FEnglish(1) Train AutoSlog-TS on English-plain (ASE)S0: Apply ASE to English Test .44 .42 .43(2) Train TBL on 4/5 of English-Test (TBLE)T0: Apply TBLE to 1/5 of English Test .35 .62 .45(perform in 5-fold cross-validation)(3) Train AutoSlog-TS on English-plain (ASE)S1: Apply ASE to English-plain .31 .40 .35TS1 Train TBL on Sundance annotationsES1: Apply TBLES to English TestFrench(4) Train TBL on 4/5 of French-Test (TBLF)TF0: Apply TBLF to 1/5 of French Test .47 .66 .54(perform in 5-fold cross-validation)Table 2: Monolingual IE Baseline Performance6.3 TBL-based IE Projection and InductionAs noted in Section 5, both the English and Frenchcorpora were divided into unannotated (?plain?
)and annotated (?antd?
or ?Tst?)
sections.
Figure3 illustrates these native-language data subsets inwhite.
Each native-language data subset alo hasa machine-translated mirror in French/English re-spectively (shown in black), with an identical num-ber of sentences to the original.
By word-aligningthese 4 native/MT pairs, each becomes a potentialvehicle for cross-language information projection.Consider the pathway TE1?P1 ?
TF1 as a rep-resentative example pathway for projection.
Herean English TBL classifier is trained on the 140K-word human annotated data and the learned TBLrules are applied to the unannotated English sub-corpus.
The annotations are then projected acrossthe Giza++ word alignments to their MT-Frenchmirror.
Next, a French TBL classifier (TBL1) istrained on the projected MT-French annotations andthe learned French TBL rules are subsequently ap-plied to the native-French test data.An alternative path (TE4 ?
P4 ?
French-Test)is more direct, in that the English TBL classifieris applied immediately to the word-aligned MT-English translation of the French test data.
The MT-English annotations can then be directly projectedto the French test data, so no additional trainingis necessary.
Another short direct projection path(PHA2 ?
THA2 ?
French-test) skips the need totrain an English TBL model by projecting the En-glish human annotations directly onto MT-Frenchtexts, which can then be used to train a French TBLsystem which can be applied to the French test data.HUMANAnnotatorsTBL (English)French TBLTraining andTransfer toTest DataEnglishAnnotationP1English (plain)P    2HATBL1T    2HATBL2hEnglish (antd)TBLTrainingT  1ET  1FCross?LanguageProjectionP3TBL3French (plain)T  3FT   3EP4MT?English TstT   4E(plain)(plain)MT?EnglishMT?FrenchMT?French(annotated) French TestFigure 3: TBL-based IE projection pathwaysTable 3 shows the results of our TBL-based ex-periments.
The top performing pathway is theTE4 ?
P4 two-step projection pathway shown inFigure 3.
Note the F-measure of the best pathwayis .45, which is equal to the highest F-measure formonolingual English and only 9% lower than the F-measure for monolingual French.4The irrelevant texts are needed to train AutoSlog-TS, butnot TBL.Projection and Training Route P R FTE1: Apply TBLE to English-plainP1: Project to MT-French(English-Plain) .69 .24 .36TF1: Train TBL & Apply to FrTest?
Use human Annos from Eng AntdPha2: Project to MT-French(English Antd) .56 .29 .39Tha2: Train TBL & Apply to FrTestTE3: Apply TBLE to MT-Eng(FrenchPlain)P3: Project to French-Plain .49 .34 .40TF3: Train TBL & Apply to FrTestTE4: Apply TBLE to MT-Eng(FrenchTest)P4: Direct Project to French-Test .49 .41 .45Table 3: TBL-based IE projection performance6.4 Sundance-based IE Projection andInductionFigure 4 shows the projection and induction modelusing Sundance for English IE annotation, which isalmost isomorphic to that using TBL.
One notabledifference is that Sundance was trained by apply-ing AutoSlog-TS to the unannotated English textrather than the human-annotated data.
Figure 4 alsoshows an additional set of experiments (SMT 3 andSMT 4) in which AutoSlog-TS was trained on theEnglish MT translations of the unannotated Frenchdata.
The motivation was that native-English extrac-tion patterns tend to achieve low recall when appliedto MT-English text (given frequent mistranslationssuch as ?to crush?
a plane rather than ?to crash?
aplane).
By training AutoSlog-TS on the sentencesgenerated by an MT system (seen in the SMT 3 andSMT 4 pathways), the F-measure increases.5French TBLTraining andTransfer toTest DataEnglishAnnotationP2P1T1T2S1S2SUNDANCE(English)English (plain)TBL1TBL2English (antd)ProjectionLanguageCross?P4(MT?English)SUNDANCEMT?English TstP     3S     4P     4MTMT MTFrench (plain)S4S     3MTS3P3TBL3T3 TBL3mT     3MTAutoslog?TSAutoslog?TS(plain)MT?FrenchMT?English(plain)MT?French(annotated) French TestFigure 4: Sundance-based projection pathways5This is a ?fair?
gain, in that the MT-trained AutoSlog-TSpatterns didn?t use translations of any of the French test data.Projection and Training Route P R FAutoSlog-TS trained on native English (AS E)S2: Apply ASE to English-AntdP2: Project to MT-French(English-Antd) .39 .24 .29T2: Train TBLFP2 & Apply to FrTestS(1+2): Apply ASE to English Antd+PlainP (1+2): Project to MT-French(Eng-Ant+Pl) .43 .23 .30T (1+2): Train TBLFP1+2 & Apply to FrTestS3: Apply ASE to MT-Eng(FrenchPlain)P3: Project to French-Plain .45 .04 .07T3: Train TBLFP3 & Apply to FrTestS4: Apply ASE to MT-Eng(FrenchTest)P4: Direct Project to French-Test .48 .07 .13AutoSlog-TS trained on MT English (AS MTE)SMT 3: Apply ASMTE to MT-Eng(FrPlain)PMT 3: Project to French-Plain .46 .25 .32TMT 3: Train TBLFMT3 & Apply to FrTestSMT 4: Apply ASMTE to MT-Eng(FrTest)PMT 4: Direct Project to French-Test .55 .28 .37Table 4: Sundance-based IE projection performance 6Table 4 shows that the best Sundance pathwayachieved an F-measure of .37.
Overall, Sundanceaveraged 7% lower F-measures than TBL on com-parable projection pathways.
However, AutoSlog-TS training required only 3-4 person hours to reviewthe learned extraction patterns while TBL trainingrequired about 150 person-hours of manual IE an-notations, so this may be a viable cost-reward trade-off.
However, the investment in manual English IEannotations can be reused for projection to new for-eign languages, so the larger time investment is afixed cost per-domain rather than per-language.6.5 Analysis and Implications?
For both TBL and Sundance, the P1, P2 andP3-family of projection paths all yield stand-alonemonolingual French IE taggers not specialized forany particular test set.
In contrast, the P4 series ofpathways (e.g.
PMT 4 for Sundance), were trainedspecifically on the MT output of the target test data.Running an MT system on test data can be done au-tomatically and requires no additional human lan-guage knowledge, but it requires additional time(which can be substantial for MT).
Thus, the higherperformance of the P4 pathways has some cost.?
The significant performance gains shown bySundance when AutoSlog-TS is trained on MT-English rather than native-English are not free be-cause the MT data must be generated for each newlanguage and/or MT system to optimally tune to6S(1+2) combines the training data in S1 (280K) and S2(140K), yielding a 420K-word sample.its peculiar language variants.
No target-languageknowledge is needed in this process, however, andreviewing AutoSlog-TS?
patterns can be done suc-cessfully by imaginative English-only speakers.?
In general, recall and F-measure drop as thenumber of experimental steps increases.
Averagedover TBL and Sundance pathways, when compar-ing 2 and 3-step projections, mean recall decreasesfrom 26.8 to 21.8 (5 points), and mean F-measuredrops from 32.6 to 28.8 (3.8 points).
Viable extrac-tion patterns may simply be lost or corrupted via toomany projection and retraining phases.?
One advantage of the projection path familiesof P1 and P2 is that no domain-specific documentsin the foreign language are required (as they are inthe P3 family).
A collection of domain-specific En-glish texts can be used to project and induce new IEsystems even when no domain-specific documentsexist in the foreign language.6.6 Multipath ProjectionFinally, we explored the use of classifier combina-tion to produce a premium system.
We considered asimple voting scheme over sets of individual IE sys-tems.
Every annotation of a head noun was consid-ered a vote.
We tried 4 voting combinations: (1) thesystems that used Sundance with English extractionpatterns, (2) the systems that used Sundance withMT-English extraction patterns, (3) the systems thatused TBL trained on English human annotations,(4) all systems.
For each combination of n sys-tems, n answer sets were produced using the votingthresholds Tv = 1..n. For example, for Tv = 2 ev-ery annotation receiving >= 2 votes (picked by atleast 2 individual systems) was output in the answerset.
This allowed us to explore a precision/recalltradeoff based on varying levels of consensus.Figure 5 shows the precision/recall curves.
Vot-ing yields some improvement in F-measure and pro-vides a way to tune the system for higher preci-sion or higher recall by choosing the Tv threshold.When using all English knowledge sources, the F-measure at Tv=1 (.48) is nearly 3% higher than thestrongest individual system.
Figure 5 also showsthe performance of a 5th system (5), which is aTBL system trained directly from the French anno-tations under 5-fold cross-validation.
It is remark-able that the most effective voting-based projectionsystem from English to French comes within 6% F-measure of the monolingually trained system, giventhat this cross-validated French monolingual systemwas trained directly on data in the same languageand source as the test data.
This suggests that cross-language projection of IE analysis capabilities cansuccessfully approach the performance of dedicatedsystems in the target language.PrecisionRecall(5)(2)(1) (3) (4)(5) TBL Trained from French Annotations(4) English TBL + Sundance pathways(3) English TBL pathways(2) Sundance?MT pathways(1) Sundance pathways[under 5?fold cross?validation]0.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7Figure 5: Precision/Recall curves for voting systems.
Eachpoint represents performance for a particular voting threshold.In all cases, precision increases and recall decreases as thethreshold is raised.French Test-Set Performance P R FMultipath projection from all English resources .43 .54 .48Table 5: Best multipath English-French Projection Per-formance (from English TBL and Sundance pathways)7 ConclusionsWe have used IE systems for English to automati-cally derive IE systems for a second language.
Evenwith the quality of MT available today, our resultsdemonstrate that we can exploit translation tools totransfer information extraction expertise from onelanguage to another.
Given an IE system for asource language, an MT system that can translatebetween the source and target languages, and a wordalignment algorithm, our approach allows a user tocreate a functionally comparable IE system for thetarget language with very little human effort.
Ourexperiments demonstrated that the new IE systemcan achieve roughly the same level of performanceas the source-language IE system.
French and En-glish are relatively close languages, however, sohow well these techniques will work for more dis-tant language pairs is still an open question.Additional performance benefits could beachieved in two ways: (1) put more effort intoobtaining better resources for English, or (2)implement (minor) specializations per language.While it is expensive to advance the state of the artin English IE or to buy annotated data for a newdomain, these additions will improve performancenot only in English but for other languages aswell.
On the other hand, with minimal effort(hours) it is possible to custom-train a systemsuch as Autoslog/Sundance to work relativelywell on noisy MT-English, providing a substantialperformance boost for the IE system learned for thetarget language, and further gains are achieved viavoting-based classifier combination.ReferencesJ.
Atserias, S. Climent, X. Farreres, G. Rigau and H. Rodriguez.1997.
Combining multiple methods for the automatic con-struction of multilingual WordNets.
In Proceedings of theInternational Conference on Recent Advances in NaturalLanguage Processing.E.
Brill.
1995.
Transformation-based error-driven learning andnatural language processing: A case study in part of speechtagging.
Computational Linguistics, 21(4):543?565.M.
E. Califf.
1998.
Relational learning techniques for naturallanguage information extraction.
Ph.D. thesis, Tech.
Rept.AI98-276, Artificial Intelligence Laboratory, The Universityof Texas at Austin.S.
Cucerzan and D. Yarowsky.
2000.
Language independentminimally supervised induction of lexical probabilities.
InProceedings of ACL-2000, pages 270-277.D.
Freitag.
1998.
Toward general-purpose learning for in-formation extraction.
In Proceedings of COLING-ACL?98,pages 404-408.J.
Kim and D. Moldovan.
1993.
Acquisition of semantic pat-terns for information extraction from corpora.
In Proceed-ings of the Ninth IEEE Conference on Artificial Intelligencefor Applications, pages 171?176.G.
Ngai and R. Florian.
2001.
Transformation-based learningin the fast lane.
In Proceedings of NAACL, pages 40-47.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics, pages 440?447.E.
Riloff.
1993.
Automatically Constructing a dictionary forinformation extraction tasks.
In Proceedings of the EleventhNational Conference on Artificial Intelligence, pages 811?816.E.
Riloff.
1996b.
Automatically generating extraction patternsfrom untagged text.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence, pages 1044?1049.
AAAI Press/MIT Press.E.
Riloff and R. Jones.
1999.
Learning dictionaries for infor-mation extraction by multi-level bootstrapping.
In Proceed-ings of the Sixteenth National Conference on Artificial Intel-ligence, pages 474?479.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995.CRYSTAL: Inducing a conceptual dictionary.
In Proceed-ings of the Fourteenth International Joint Conference on Ar-tificial Intelligence, pages 1314?1319.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.2000.
Automatic acquisiton of domain knowledge for infor-mation extraction.
In Proceedings of COLING-2000, pages940-946.Yarowsky, D., G. Ngai and R. Wicentowski.
2001.
Inducingmultilingual text analysis tools via robust projection acrossaligned corpora.
In Proceedings of HLT-01, pages 161?168.
