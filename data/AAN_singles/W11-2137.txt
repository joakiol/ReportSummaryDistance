Proceedings of the 6th Workshop on Statistical Machine Translation, pages 323?329,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsRegMT System for Machine Translation, System Combination, andEvaluationErgun Bic?iciKoc?
University34450 Sariyer, Istanbul, Turkeyebicici@ku.edu.trDeniz YuretKoc?
University34450 Sariyer, Istanbul, Turkeydyuret@ku.edu.trAbstractWe present the results we obtain using ourRegMT system, which uses transductive re-gression techniques to learn mappings be-tween source and target features of given par-allel corpora and use these mappings to gen-erate machine translation outputs.
Our train-ing instance selection methods perform fea-ture decay for proper selection of training in-stances, which plays an important role to learncorrect feature mappings.
RegMT uses L2regularized regression as well as L1 regular-ized regression for sparse regression estima-tion of target features.
We present transla-tion results using our training instance selec-tion methods, translation results using graphdecoding, system combination results withRegMT, and performance evaluation with theF1 measure over target features as a metric forevaluating translation quality.1 IntroductionRegression can be used to find mappings betweenthe source and target feature sets derived from givenparallel corpora.
Transduction learning uses a sub-set of the training examples that are closely relatedto the test set without using the model induced bythe full training set.
In the context of statistical ma-chine translation, translations are performed at thesentence level and this enables us to select a smallnumber of training instances for each test instanceto guide the translation process.
This also gives us acomputational advantage when considering the highdimensionality of the problem as each sentence canbe mapped to many features.The goal in transductive regression based ma-chine translation (RegMT) is both reducing the com-putational burden of the regression approach by re-ducing the dimensionality of the training set and thefeature set and also improving the translation qualityby using transduction.We present translation results using our traininginstance selection methods, translation results us-ing graph decoding, system combination results withRegMT, and performance evaluation with the F1measure over target features as a metric for eval-uating translation quality.
RegMT work builds onour previous regression-based machine translationresults (Bicici and Yuret, 2010) especially with in-stance selection and additional graph decoding ca-pability.
We present our results to this year?s chal-lenges.Outline: Section 2 gives an overview of theRegMT model.
In section 3, we present our train-ing instance selection techniques and WMT?11 re-sults.
In section 4, we present the graph decoding re-sults on the Haitian Creole-English translation task.Section 5 presents our system combination resultsusing reranking with the RegMT score.
Section 6evaluates the F1 measure that we use for the auto-matic evaluation metrics challenge.
The last sectionpresent our contributions.2 Machine Translation Using RegressionLet X and Y correspond to the sets of tokensthat can be used in the source and target strings,then, m training instances are represented as(x1, y1), .
.
.
, (xm, ym) ?
X?
?
Y ?, where (xi, yi)corresponds to a pair of source and target language323token sequences for 1 ?
i ?
m. Our goal is to finda mapping f : X?
?
Y ?
that can convert a sourcesentence to a target sentence sharing the same mean-ing in the target language (Figure 1).X?
Y ?-?
R ?-FX FYg?X ?Y6?
?1YfhFigure 1: String-to-string mapping.We define feature mappers ?X : X?
?
FX =RNX and ?Y : Y ?
?
FY = RNY that map eachstring sequence to a point in high dimensional realnumber space.
Let MX ?
RNX?m and MY ?RNY ?m such that MX = [?X(x1), .
.
.
,?X(xm)]and MY = [?Y (y1), .
.
.
,?Y (ym)].
The ridge re-gression solution usingL2 regularization is found byminimizing the following cost:WL2 = arg minW?RNY ?NX?MY ?WMX ?2F +?
?W?2F .
(1)Two main challenges of the regression based ma-chine translation (RegMT) approach are learningthe regression function, h : FX ?
FY , andsolving the pre-image problem, which, given thefeatures of the estimated target string sequence,h(?X(x)) = ?Y (y?
), attempts to find y ?
Y ?
:y = arg miny?Y ?
||h(?X(x)) ?
?Y (y)||2.
Pre-image calculation involves a search over possibletranslations minimizing the cost function:f(x) = arg miny?Y ??
?Y (y)?W?X(x)?2 .
(2)2.1 L1 Regularized RegressionString kernels lead to sparse feature representationsand L1 regularized regression is effective to find themappings between sparsely observed features.Wewould like to observe only a few nonzero target co-efficients corresponding to a source feature in the co-efficient matrix.
L1 regularization helps us achievesolutions close to permutation matrices by increas-ing sparsity (Bishop, 2006) (page 145).
In contrast,L2 regularized solutions give us dense matrices.WL2 is not a sparse solution and most of the coef-ficients remain non-zero.
We are interested in pe-nalizing the coefficients better; zeroing the irrele-vant ones leading to sparsification to obtain a solu-tion that is closer to a permutation matrix.
L1 normbehaves both as a feature selection technique and amethod for reducing coefficient values.WL1 = arg minW?RNY ?NX?MY ?WMX ?2F +?
?W?1 .
(3)Equation 3 presents the lasso (Tibshirani, 1996) so-lution where the regularization term is now the L1matrix norm defined as ?W?1=?i,j |Wi,j |.
WL2can be found by taking the derivative but sinceL1 regularization cost is not differentiable, WL1 isfound by optimization or approximation techniques.We use forward stagewise regression (FSR) (Hastieet al, 2006), which approximates lasso for L1 regu-larized regression.2.2 Related Work:Regression techniques can be used to model therelationship between strings (Cortes et al, 2007).Wang et al (2007) applies a string-to-string map-ping approach to machine translation by using ordi-nary least squares regression and n-gram string ker-nels to a small dataset.
Later they use L2 regularizedleast squares regression (Wang and Shawe-Taylor,2008).
Although the translation quality they achieveis not better than Moses (Koehn et al, 2007), whichis accepted to be the state-of-the-art, they show thefeasibility of the approach.
Serrano et al (2009)use kernel regression to find translation mappingsfrom source to target feature vectors and experimentwith translating hotel front desk requests.
Locallyweighted regression solves separate weighted leastsquares problems for each instance (Hastie et al,2009), weighted by a kernel similarity function.3 Instance Selection for MachineTranslationProper selection of training instances plays an im-portant role for accurately learning feature mappingswith limited computational resources.
Coverage ofthe features is important since if we do not have thecorrect features in the training matrices, we will notbe able to translate them.
Coverage is measured bythe percentage of target features of the test set foundin the training set.
For each test sentence, we picka limited number of training instances designed to324improve the coverage of correct features to build aregression model.We use two techniques for this purpose: (1)Feature Decay Algorithm (FDA), which optimizessource languge bigram coverage to maximize thetarget coverage, (2) dice.
Feature decay algorithms(FDA) aim to maximize the coverage of the tar-get language features (such as words, bigrams, andphrases) for the test sentences.
FDA selects traininginstances one by one updating the coverage of thefeatures already added to the training set in contrastto the features found in the test sentence.We also use a technique that we call dice, whichoptimizes source language bigram coverage suchthat the difficulty of aligning source and target fea-tures is minimized.
We define Dice?s coefficientscore as:dice(x, y) =2C(x, y)C(x)C(y), (4)where C(x, y) is the number of times x and y co-occurr and C(x) is the count of observing x inthe selected training set.
Given a test source sen-tence, SU , we can estimate the goodness of a train-ing sentence pair, (S, T ), by the sum of the align-ment scores:?dice(SU , S, T ) =?x?X(SU )|T |?j=1?y?Y (x)dice(y, Tj)|T | log |S|,(5)where X(SU ) stores the features of SU and Y (x)lists the tokens in feature x.
The difficulty of wordaligning a pair of training sentences, (S, T ), can beapproximated by |S||T |.
We use a normalization fac-tor proportional to |T | log |S|.The details of both of these techniques and furtherresults can be found in (Bicici and Yuret, 2011).3.1 Moses Experiments on the TranslationTaskWe have used FDA and dice algorithms to selecttraining sets for the out-of-domain challenge testsets used in (Callison-Burch et al, 2011).
The par-allel corpus contains about 1.9 million training sen-tences and the test set contain 3003 sentences.
Webuilt separate Moses systems using all of the paral-lel corpus for the language pairs en-de, de-en, en-es, and es-en.
We created training sets using allen-de de-en en-es es-enBLEUALL .1376 .2074 .2829 .2919FDA .1363 .2055 .2824 .2892dice .1374 .2061 .2834 .2857wordsALL 47.4 49.6 52.8 50.4FDA 7.9 8.0 8.7 8.2dice 6.9 7.0 3.9 3.6% ALLFDA 17 16 16 16dice 14 14 7.4 7.1Table 1: Performance for the out-of-domain taskof (Callison-Burch et al, 2011).
ALL corresponds to thebaseline system using all of the parallel corpus.
wordslist the size of the target words used in millions.of the features of the test set to select training in-stances.
The results given in Table 1 show that wecan achieve similar BLEU performance using about7% of the parallel corpus target words (200,000 in-stances) using dice and about 16% using FDA.
In theout-of-domain translation task, we are able to reducethe training set size to achieve a performance closeto the baseline.
We may be able to achieve betterperformance in this out-of-domain task as well asexplained in (Bicici and Yuret, 2011).4 Graph Decoding for RegMTWe perform graph-based decoding by first generat-ing a De Bruijn graph from the estimated y?
(Cortes etal., 2007) and then finding Eulerian paths with max-imum path weight.
We use four features when scor-ing paths: (1) estimation weight from regression, (2)language model score, (3) brevity penalty as foundby e?
(lR?|s|/|path|) for lR representing the length ra-tio from the parallel corpus and |path| representingthe length of the current path, (4) future cost as inMoses (Koehn et al, 2007) and weights are tunedusing MERT (Och, 2003) on the de-en dev set.We demonstrate that sparse L1 regularized regres-sion performs better than L2 regularized regression.Graph based decoding can provide an alternative tostate of the art phrase-based decoding system Mosesin translation domains with small vocabulary andtraining set size.4.1 Haitian Creole to English Translation Taskwith RegMTWe have trained a Moses system for the Haitian Cre-ole to English translation task, cleaned corpus, us-325ing the options as described in section 3.1.
Mosesachieves 0.3186 BLEU on this task.
We observedthat graph decoding performs better where targetcoverage is high such that the bigrams used leadto a connected graph.
To increase the connec-tivity, we have included Moses translations in thetraining set and performed graph decoding withRegMT.
RegMT with L2 regularized regressionachieves 0.2708 BLEU with graph decoding andlasso achieves 0.26 BLEU.Moses makes use of a number of distortion pa-rameters and lexical weights, which are estimatedusing all of the parallel corpus.
Thus, our Mosestranslation achieves a better performance than graphdecoding with RegMT using 100 training instancesfor translating each source test sentence.5 System Combination with RegMTWe perform experiments on the system com-bination task for the English-German, German-English, English-Spanish, and Spanish-English lan-guage pairs using the training corpus provided inWMT?11 (Callison-Burch et al, 2011).
We havetokenized and lowercased each of the system out-puts and combined these in a single N -best file perlanguage pair.
We use these N -best lists for rerank-ing by RegMT to select the best translation model.Feature mappers used are 2-spectrum counting wordkernels (Taylor and Cristianini, 2004).We rerank N -best lists by a linear combination ofthe following scoring functions:1.
RegMT: Regression based machine translationscores as found by Equation 2.2.
CBLEU: Comparative BLEU scores we obtainby measuring the average BLEU performanceof each translation relative to the other systems?translations in the N -best list.3.
LM: We calculate 5-gram language modelscores for each translation using the languagemodel trained over the target corpus providedin the translation task.Since we do not have access to the reference trans-lations nor to the translation model scores each sys-tem obtained for each sentence, we estimate trans-lation model performance (CBLEU) by measuringthe average BLEU performance of each translationrelative to the other translations in the N -best list.Thus, each possible translation in the N -best list isBLEU scored against other translations and the av-erage of these scores is selected as the CBLEU scorefor the sentence.
Sentence level BLEU score calcu-lation avoids singularities in n-gram precisions bytaking the maximum of the match count and 12|si| for|si| denoting the length of the source sentence si asused in (Macherey and Och, 2007).Table 2 presents reranking results on all of the lan-guage pairs we considered, using RegMT, CBLEU,and LM scores with the same combination weightsas above.
We also list the performance of the bestmodel (Max) as well as the worst (Min).
We areable to achieve close or better BLEU scores in allof the listed systems when compared with the per-formance of the best translation system except forthe ht-en language pair.
The lower performance inthe ht-en language pair may be due to having a sin-gle best translation system that outperforms otherssignificantly.
This happens for instance when an un-constrained model use external resources to achievea significantly better performance than the secondbest model.
2nd best in Table 2 lists the second bestmodel?s performance to estimate how much the bestmodel?s performance is better than the rest.BLEU en-de de-en en-es es-en ht-enMin .1064 .1572 .2174 .1976 .2281Max .1727 .2413 .3375 .3009 .37082nd best .1572 .2302 .3301 .2973 .3288Average .1416 .1997 .292 .2579 .2993Oracle .2529 .3305 .4265 .4233 .4336RegMT .1631 .2322 .3311 .3052 .3234Table 2: System combination results.RegMT model may prefer sentences with lowerBLEU, which can sometimes cause it to achieve alower BLEU performance than the best model.
Thisis clearly the case for en-de with 1.6 BLEU pointsdifference with the second best model performanceand for de-en task with 1.11 BLEU points differ-ence.
Also this observation holds for en-es with0.74 BLEU points difference and for ht-en with 4.2BLEU points difference.
For es-en task, there is 0.36BLEU points difference with the second best modeland these models likely to complement each other.326The existence of complementing SMT models isimportant for the reranking approach to achieve aperformance better than the best model, as there isa need for the existence of a model performing bet-ter than the best model on some test sentences.
Wecan use the competitive SMT model to achieve theperformance of the best with a guarantee even whena single model is dominating the rest (Bicici andKozat, 2010).
For competing translation systemsin an on-line machine translation setting adaptivelylearning of model weights can be performed basedon the previous transaltion performance (Bicici andKozat, 2010).6 Target F1 as a Performance EvaluationMetricWe use target sentence F1 measure over the tar-get features as a translation performance evaluationmetric.
We optimize the parameters of the RegMTmodel with the F1 measure comparing the targetvector with the estimate we get from the RegMTmodel.
F1 measure uses the 0/1-class predictionsover the target feature with the estimate vector,?Y (y?).
Let TP be the true positive, TN the true neg-ative, FP the false positive, and FN the false negativerates, we use the following measures for evaluation:prec =TPTP + FP, BER = ( FPTN+FP +FNTP+FN )/2 (6)rec =TPTP + FN, F1 =2?prec?recprec+rec (7)where BER is the balanced error rate, prec is pre-cision, and rec is recall.
The evaluation techniquesmeasure the effectiveness of the learning models inidentifying the features of the target sentence mak-ing minimal error to increase the performance of thedecoder and its translation quality.We use gapped word sequence kernels (Taylorand Cristianini, 2004) when using F1 for evaluatingtranslations since a given translation system may notbe able to translate a given word but can correctlyidentify the surrounding phrase.
For instance, let thereference translation be the following sentence:a sound compromise has been reachedSome possible translations for the reference aregiven in Table 3 together with their BLEU (Papineniet al, 2001) and F1 scores for comparison.
F1 scoredoes not have a brevity penalty but a brief transla-tion is penalized by a low recall value.
We use upto 3 tokens as gaps.
F1 measure is able to increasethe ranking of Trans4 by using a gapped sequencekernel, which can be preferrable to Trans3.We note that a missing token corresponds to vary-ing decreases in the n-gram precision used in theBLEU score.
A sentence containing m tokens hasm 1-grams, m?1 2-grams, andm?n+1 n-grams.A missing token degrades the performance more inhigher order n-gram precision values.
A missing to-ken decreases n-gram precision by 1m for 1-gramsand by nm?n+1 for n-grams.
Based on this obser-vation, we use F1 measure with gapped word se-quence kernels to evaluate translations.
Gapped fea-tures allows us to consider the surrounding phrasefor a missing token as present in the translation.Let the reference sentence be represented witha b c d e f where a-f, x, y, z correspond to to-kens in the sentence.
Then, Trans3 has the forma b x y f, and Trans4 has the form a c y f.Then, F1 ranks Trans4 higher than Trans3 for ordersgreater than 3 as there are two consecutive word er-rors in Trans3.
F1 can also prefer a missing tokenrather than a word error as we see by comparingTrans4 and Trans5 and it can still prefer contigu-ity over a gapped sequence as we see by comparingTrans5 and Trans6 in Table 3.We calculate the correlation of F1 with BLEU onthe en-de development set.
We use 5-grams with theF1 measure as this increases the correlation with 4-gram BLEU.
Table 4 gives the correlation results us-ing both Pearson?s correlation score and Spearman?scorrelation score.
Spearman?s correlation score is abetter metric for comparing the relative orderings.Metric No gaps GapsPearson .8793 .7879Spearman .9068 .8144Table 4: F1 correlation with 4-gram BLEU using blended5-gram gapped word sequence features on the develop-ment set.7 ContributionsWe present the results we obtain using our RegMTsystem, which uses transductive regression tech-niques to learn mappings between source and tar-327Format BLEU F1Ref: a sound compromise has been reached a b c d e f 4-grams 3-grams 4-grams 5-gramsTrans1: a sound agreement has been reached a b x d e f .2427 .6111 .5417 .5Trans2: a compromise has reached a c d f .137 .44 .3492 .3188Trans3: a sound agreement is reached a b x y f .1029 .2 .1558 .1429Trans4: a compromise is reached a c y f .0758 .2 .1587 .1449Trans5: a good compromise is reached a z c y f .0579 .1667 .1299 .119Trans6: a good compromise is been a z c y e .0579 .2 .1558 .1429Table 3: BLEU vs. F1 on sample sentence translation task.get features of given parallel corpora and use thesemappings to generate machine translation outputs.We also present translation results using our train-ing instance selection methods, translation resultsusing graph decoding, system combination resultswith RegMT, and performance evaluation with F1measure over target features.
RegMT work buildson our previous regression-based machine transla-tion results (Bicici and Yuret, 2010) especially withinstance selection and additional graph decoding ca-pability.ReferencesErgun Bicici and S. Serdar Kozat.
2010.
Adaptivemodel weighting and transductive regression for pre-dicting best system combinations.
In Proceedings ofthe ACL 2010 Joint Fifth Workshop on Statistical Ma-chine Translation and Metrics MATR, Uppsala, Swe-den, July.
Association for Computational Linguistics.Ergun Bicici and Deniz Yuret.
2010.
L1 regularized re-gression for reranking and system combination in ma-chine translation.
In Proceedings of the ACL 2010Joint Fifth Workshop on Statistical Machine Transla-tion and Metrics MATR, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Ergun Bicici and Deniz Yuret.
2011.
Instance selec-tion for machine translation using feature decay al-gorithms.
In Proceedings of the EMNLP 2011 SixthWorkshop on Statistical Machine Translation, Edin-burgh, England, July.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, and Omar Zaidan, editors.
2011.
Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.
Edinburgh, England, July.Corinna Cortes, Mehryar Mohri, and Jason Weston.2007.
A general regression framework for learn-ing string-to-string mappings.
In Gokhan H. Bakir,Thomas Hofmann, and Bernhard Sch editors, Predict-ing Structured Data, pages 143?168.
The MIT Press,September.Trevor Hastie, Jonathan Taylor, Robert Tibshirani, andGuenther Walther.
2006.
Forward stagewise regres-sion and the monotone lasso.
Electronic Journal ofStatistics, 1.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.2009.
The Elements of Statistical Learning: DataMining, Inference and Prediction.
Springer-Verlag,2nd edition.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In AnnualMeeting of the Assoc.
for Computational Linguistics,pages 177?180, Prague, Czech Republic, June.Wolfgang Macherey and Franz Josef Och.
2007.
Anempirical study on computing consensus translationsfrom multiple machine translation systems.
In Pro-ceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 986?995, Prague, Czech Republic,June.
Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
Association for Com-putational Linguistics, 1:160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic evalu-ation of machine translation.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Nicolas Serrano, Jesus Andres-Ferrer, and FranciscoCasacuberta.
2009.
On a kernel regression approachto machine translation.
In Iberian Conference on Pat-tern Recognition and Image Analysis, pages 394?401.J.
Shawe Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.328Robert J. Tibshirani.
1996.
Regression shrinkage andselection via the lasso.
Journal of the Royal StatisticalSociety, Series B, 58(1):267?288.Zhuoran Wang and John Shawe-Taylor.
2008.
Kernelregression framework for machine translation: UCLsystem description for WMT 2008 shared translationtask.
In Proceedings of the Third Workshop on Sta-tistical Machine Translation, pages 155?158, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-mak.
2007.
Kernel regression based machine trans-lation.
In Human Language Technologies 2007: TheConference of the North American Chapter of the As-sociation for Computational Linguistics; CompanionVolume, Short Papers, pages 185?188, Rochester, NewYork, April.
Association for Computational Linguis-tics.329
