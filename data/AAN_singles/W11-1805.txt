Proceedings of BioNLP Shared Task 2011 Workshop, pages 36?40,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsBiomedical Event Extraction from Abstracts and Full Papers usingSearch-based Structured PredictionAndreas Vlachos and Mark CravenDepartment of Biostatistics and Medical InformaticsUniversity of Wisconsin-Madison{vlachos,craven}@biostat.wisc.eduAbstractIn this paper we describe our approach tothe BioNLP 2011 shared task on biomedicalevent extraction from abstracts and full pa-pers.
We employ a joint inference system de-veloped using the search-based structured pre-diction framework and show that it improveson a pipeline using the same features and it isbetter able to handle the domain shift from ab-stracts to full papers.
In addition, we report onexperiments using a simple domain adaptationmethod.1 IntroductionThe term biomedical event extraction is used to re-fer to the task of extracting descriptions of actionsand relations among one or more entities from thebiomedical literature.
The BioNLP 2011 sharedtask GENIA Task1 (BioNLP11ST-GE1) (Kim et al,2011) focuses on extracting events from abstractsand full papers.
The inclusion of full papers in thedatasets is the only difference from Task1 of theBioNLP 2009 shared task (BioNLP09ST1) (Kim etal., 2009), which used the same task definition andabstracts dataset.
Each event consists of a triggerand one or more arguments, the latter being proteinsor other events.
The protein names are annotated inadvance and any token in a sentence can be a trig-ger for one of the nine event types.
In an exam-ple demonstrating the complexity of the task, giventhe passage ?.
.
.
SQ 22536 suppressed gp41-inducedIL-10 production in monocytes?, systems should ex-tract the three nested events shown in Fig.
1d.In our submission, we use the event extractionsystem of Vlachos and Craven (2011) which em-ploys the search-based structured prediction frame-work (SEARN) (Daume?
III et al, 2009).
SEARNconverts the problem of learning a model for struc-tured prediction into learning a set of models forcost-sensitive classification (CSC).
In CSC, eachtraining instance has a vector of misclassificationcosts associated with it, thus rendering some mis-takes in some instances to be more expensive thanothers.
Compared to other structured predictionframeworks such as Markov Logic Networks (Poonand Vanderwende, 2010), SEARN provides highmodeling flexibility but it does not requiring task-dependent approximate inference.In this work, we show that SEARN is more accu-rate than a pipeline using the same features and it isbetter able to handle the domain shift from abstractsto full papers.
Furthermore, we report on exper-iments with the simple domain adaptation methodproposed by Daume?
III (2007), which creates a ver-sion of each feature for each domain.
While the re-sults were mixed, this method improves our perfor-mance on full papers of the test set, for which littletraining data is available.2 Event extraction decompositionFigure 1 describes the event extraction decomposi-tion that is used throughout the paper.
Each stage hasits own module to perform the classification needed.In trigger recognition the system decides whethera token acts as a trigger for one of the nine eventtypes or not.
We only consider tokens that are taggedas nouns, verbs or adjectives by the parser, as they36SQ 22536 suppressedNeg reggp41 -inducedPos regIL-10 productionGene exp(a) Trigger recognitionSQ 22536 suppressedNeg reggp41 -inducedPos regIL-10 productionGene expThemeThemeTheme(b) Theme assignmentSQ 22536 suppressedNeg reggp41 -inducedPos regIL-10 productionGene expThemeThemeCauseTheme(c) Cause assignmentID type Trigger Theme CauseE1 Neg reg suppressed E2E2 Pos reg induced E3 gp41E3 Gene exp production IL-10(d) Event constructionFigure 1: The stages of our biomedical event extraction system.cover the majority of the triggers in the data.
Themain features used in the classifier represent thelemma of the token which is sufficient to predictthe event type correctly in most cases.
In addition,we include features that conjoin each lemma withits part-of-speech tag and its immediate lexical andsyntactic context, which allows us to handle wordsthat can represent different event types, e.g.
?activ-ity?
often denotes a Regulation event but in ?bindingactivity?
it denotes a Binding event instead.In Theme assignment, we form an agenda of can-didate trigger-argument pairs for all trigger-proteincombinations in the sentence and classify them asThemes or not.
Whenever a trigger is predicted to beassociated with a Theme, we form candidate pairsbetween all the Regulation triggers in the sentenceand that trigger as the argument, thus allowing theprediction of nested events.
Also, we remove candi-date pairs that could result in directed cycles, as theyare not allowed by the task.
In Cause assignment,we form an agenda of candidate trigger-argumentpairs and classify them as Causes or not.
We formpairs between Regulation class triggers that were as-signed at least one Theme, and protein names andother triggers that were assigned at least one Theme.The features used in these two stages are extractedfrom the syntactic dependency path and the textualstring between the trigger and the argument.
Weextract the shortest unlexicalized dependency pathconnecting each trigger-argument pair using Dijk-stra?s algorithm, allowing the paths to follow eitherdependency direction.
One set of features representsthe shortest unlexicalized path between the pair andin addition we have sets of features representingeach path conjoined with the lemma, the PoS tag andthe event type of the trigger, the type of the argumentand the first and last lemmas in the dependency path.In the event construction stage, we convert thepredictions of the previous stages into events.
Ifa Binding trigger is assigned multiple Themes, wechoose to form either one event per Theme or oneevent with multiple Themes.
For this purpose, wegroup the arguments of each nominal Binding trig-ger according to the first label in their dependencypath and generate events using the cross-product ofthese groups.
For example, assuming the parse wascorrect and all the Themes recognized, ?interactionsof A and B with C?
results in two Binding eventswith two Themes each, A with C, and B with C re-spectively.
We add the exceptions that if two Themesare part of the same token (e.g.
?A/B interactions?
),or the trigger and one of the Themes are part of thesame token, or the lemma of the trigger is ?bind?then they form one Binding event with two Themes.3 Structured prediction with SEARNSEARN (Daume?
III et al, 2009) forms the struc-tured output prediction of an instance s as a se-quence of T multiclass predictions y?1:T made by ahypothesis h. The latter is a weighted ensemble ofclassifiers that are learned jointly.
Each prediction y?tcan use features from s as well as from all the pre-vious predictions y?1:t?1, thus taking structure into37account.
These predictions are referred to as actionsand we adopt this term in order to distinguish themfrom the structured output predictions.The SEARN algorithm is presented in Alg.
1.
Ineach iteration, SEARN uses the current hypothesish to generate a CSC example for each action y?t cho-sen to form the prediction for each labeled instances (steps 6-12).
The cost associated with each actionis estimated using the gold standard according to aloss function l which corresponds to the task eval-uation metric (step 11).
Using a CSC learning al-gorithm, a new hypothesis hnew is learned (step 13)which is combined with the current one according tothe interpolation parameter ?
(step 14).
h is initial-ized to the optimal policy (step 2) which is derivedfrom the gold standard.
In each iteration SEARN?corrupts?
the optimal policy with the learned hy-potheses.
Thus, each hnew is adapted to the actionschosen by h instead of the optimal policy.
The algo-rithm terminates when the dependence on the latterbecomes insignificant.Algorithm 1 SEARN1: Input: labeled instances S , optimal policy pi, CSClearning algorithm CSCL, loss function `2: current policy h = pi3: while h depends significantly on pi do4: Examples E = ?5: for s in S do6: Predict h(s) = y?1 .
.
.
y?T7: for y?t in h(s) do8: Extract features ?t = f(s, y?1:t?1)9: for each possible action yit do10: Predict y?t+1:T = h(s|y?1:t?1, yit)11: Estimate cit = `(y?1:t?1, yit, y?t+1:T )12: Add (?t, ct) to E13: Learn a classifier hnew = CSCL(E)14: h = ?hnew + (1?
?
)h15: Output: hypothesis h without pi4 Biomedical event extraction withSEARNIn this section we describe how we learn the eventextraction decomposition described in Sec.
2 underSEARN.
Each instance is a sentence and the hypoth-esis learned in each iteration consists of a classifierfor each stage of the pipeline, excluding event con-struction which is rule-based.SEARN allows us to extract structural features foreach action from the previous ones.
During trig-ger recognition, we add as features the combinationof the lemma of the current token combined withthe event type (if any) assigned to the previous andthe next token, as well as to the tokens that havesyntactic dependencies with it.
During Theme as-signment, when considering a trigger-argument pair,we add features based on whether the pair forms anundirected cycle with previously predicted Themes,whether the trigger has been assigned a protein as aTheme and the candidate Theme is an event trigger(and the reverse), and whether the argument is theTheme of a trigger with the same event type.
Wealso add a feature indicating whether the trigger hasthree Themes predicted already.
During Cause as-signment, we add features representing whether thetrigger has been assigned a protein as a Cause andthe candidate Cause is an event trigger.Since the features extracted for an action dependon previous ones, we need to define a prediction or-der for the actions.
In trigger recognition, we pro-cess the tokens from left to right since modifiersappearing before nouns tend to affect the meaningof the latter, e.g.
?binding activity?.
In Themeand Cause assignment, we predict trigger-argumentpairs in order of increasing dependency path length,assuming that, since they are the main source of fea-tures in these stages and shorter paths are less sparse,pairs containing shorter ones should be predictedmore reliably.
The loss function sums the number offalse positive and false negative events, which is theevaluation measure of the shared task.
The optimalpolicy is derived from the gold standard and returnsthe action that minimizes the loss over the sentencegiven the previous actions and assuming that all fu-ture actions are optimal.In step 11 of Alg.
1, the cost of each action is esti-mated over the whole sentence.
While this allows usto take structure into account, it can result in costsbeing affected by a part of the output that is not re-lated to that action.
This is likely to occur in eventextraction, as sentences can often be long and con-tain disconnected event components in their outputgraphs.
For this reason we use focused costing (Vla-chos and Craven, 2011), in which the cost estimationfor an action takes into account only the part of theoutput graph connected with that action.38pipeline (R/P/F) SEARN (R/P/F)trigger 49.1 64.0 55.6 83.2 28.6 42.6Theme 43.7 78.6 56.2 63.8 72.0 67.6Cause 13.9 61.0 22.6 33.9 53.8 41.6Event 31.7 70.1 43.6 45.8 60.51 52.1Table 1: Results on the development dataset.5 ExperimentsIn our experiments, we perform multiclass CSClearning using our implementation of the on-line passive-aggressive (PA) algorithm proposed byCrammer et al (2006).
The aggressiveness param-eter and the number of rounds in parameter learn-ing are set by tuning on 10% of the training dataand we use the variant named PA-II with prediction-based updates.
For SEARN, we set the interpolationparameter ?
to 0.3.
For syntactic parsing, we usethe output of the parser of Charniak and Johnson(2005) adapted to the biomedical domain by Mc-Closky (2010), as provided by the shared task orga-nizers in the Stanford collapsed dependencies withconjunct dependency propagation (Stenetorp et al,2011).
Lemmatization is performed using morpha(Minnen et al, 2001).
No other knowledge sourcesor tools are used.In order to assess the benefits of joint learning un-der SEARN, we compare it against a pipeline of in-dependently learned classifiers using the same fea-tures and task decomposition.
Table 1 reports theRecall/Precision/F-score achieved in each stage, aswell as the overall performance.
SEARN obtainsbetter performance on the development set by 8.5F-score points.
This increase is larger than the 7.3points reported in Vlachos and Craven (2011) onthe BioNLP09ST1 datasets which contain only ab-stracts.
This result suggests that the gains of jointinference under SEARN are greater when learningfrom the additional data from full papers.
Notethat while the classifier learned with SEARN over-predicts triggers, the Theme and Cause classifiersmaintain relatively high precision with substantiallyhigher recall as they are learned jointly with it.As triggers that do not form events are ignored bythe evaluation, trigger overprediction without eventoverprediction does not result in performance loss.The results of our submission on the testdataset using SEARN were 42.6/61.2/50.2(Recall/Precision/F-score) which ranked sixthin the shared task.
In the Regulation events whichare considered harder due to nesting, our submis-sion was ranked fourth.
This demonstrates thepotential of SEARN for structured prediction, as theperformance on regulation events depends partly onthe performance on the simple ones on which oursubmission was ranked eighth.After the end of the shared task, we experimentedwith the domain adaptation method proposed byDaume?
III (2007), which creates multiple versionsfor each feature by conjoining it with the domain la-bel of the instance it is extracted from (abstracts orfull papers).
While this improved the performanceof the pipeline baseline by 0.3 F-score points, theperformance under SEARN dropped by 0.4 pointson the development data.
Using the online serviceprovided by the organizers, we evaluated the perfor-mance of the domain adapted SEARN-based systemon the test set and the overall performance improvedto 50.72 in F-score (would have ranked 5th).
Inparticular, domain adaptation improved the perfor-mance on full papers by 1.22 points, thus reaching51.22 in F-score.
This version of the system wouldhave ranked 3rd overall and 1st in the Regulationevents in this part of the corpus.
We hypothesizethat these mixed results are due to the sparse fea-tures used in the stages of the event extraction de-composition, which become even sparser using thisdomain adaptation method, thus rendering the learn-ing of appropriate weights for them harder.6 ConclusionsWe presented a joint inference approach to theBioNLP11ST-GE1 task using SEARN which con-verts a structured prediction task into a set of CSCtasks whose models are learned jointly.
Our resultsdemonstrate that SEARN achieves substantial per-formance gains over a standard pipeline using thesame features.AcknowledgmentsThe authors were funded by NIH/NLM grant R01LM07050.39ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Hal Daume?
III, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, 75:297?325.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages256?263.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 shared task on event extraction.
In Pro-ceedings of the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 1?9.Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-nori Yonezawa.
2011.
Overview of the Genia Eventtask in BioNLP Shared Task 2011.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task.David McClosky.
2010.
Any domain parsing: Auto-matic domain adaptation for natural language pars-ing.
Ph.D. thesis, Department of Computer Science,Brown University.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Hoifung Poon and Lucy Vanderwende.
2010.
Joint in-ference for knowledge extraction from biomedical lit-erature.
In Proceedings of the Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 813?821.Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, TomokoOhta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011.BioNLP Shared Task 2011: Supporting Resources.
InProceedings of the BioNLP 2011 Workshop Compan-ion Volume for Shared Task.Andreas Vlachos and Mark Craven.
2011.
Search-basedstructured prediction applied to biomedical event ex-traction.
In Proceedings of the Fifteenth Conferenceon Computational Natural Language Learning.40
