Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253?261,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDiscovering the Discriminative Views: Measuring Term Weights forSentiment AnalysisJungi Kim, Jin-Ji Li and Jong-Hyeok LeeDivision of Electrical and Computer EngineeringPohang University of Science and Technology, Pohang, Republic of Korea{yangpa,ljj,jhlee}@postech.ac.krAbstractThis paper describes an approach to uti-lizing term weights for sentiment analysistasks and shows how various term weight-ing schemes improve the performance ofsentiment analysis systems.
Previously,sentiment analysis was mostly studied un-der data-driven and lexicon-based frame-works.
Such work generally exploits tex-tual features for fact-based analysis tasksor lexical indicators from a sentiment lexi-con.
We propose to model term weightinginto a sentiment analysis system utilizingcollection statistics, contextual and topic-related characteristics as well as opinion-related properties.
Experiments carriedout on various datasets show that ourapproach effectively improves previousmethods.1 IntroductionWith the explosion in the amount of commentarieson current issues and personal views expressed inweblogs on the Internet, the field of studying howto analyze such remarks and sentiments has beenincreasing as well.
The field of opinion miningand sentiment analysis involves extracting opin-ionated pieces of text, determining the polaritiesand strengths, and extracting holders and targetsof the opinions.Much research has focused on creating testbedsfor sentiment analysis tasks.
Most notableand widely used are Multi-Perspective QuestionAnswering (MPQA) and Movie-review datasets.MPQA is a collection of newspaper articles anno-tated with opinions and private states at the sub-sentence level (Wiebe et al, 2003).
Movie-reviewdataset consists of positive and negative reviewsfrom the Internet Movie Database (IMDb) archive(Pang et al, 2002).Evaluation workshops such as TREC and NT-CIR have recently joined in this new trend of re-search and organized a number of successful meet-ings.
At the TREC Blog Track meetings, re-searchers have dealt with the problem of retriev-ing topically-relevant blog posts and identifyingdocuments with opinionated contents (Ounis etal., 2008).
NTCIR Multilingual Opinion Analy-sis Task (MOAT) shared a similar mission, whereparticipants are provided with a number of topicsand a set of relevant newspaper articles for eachtopic, and asked to extract opinion-related proper-ties from enclosed sentences (Seki et al, 2008).Previous studies for sentiment analysis belongto either the data-driven approach where an anno-tated corpus is used to train a machine learning(ML) classifier, or to the lexicon-based approachwhere a pre-compiled list of sentiment terms is uti-lized to build a sentiment score function.This paper introduces an approach to the senti-ment analysis tasks with an emphasis on how torepresent and evaluate the weights of sentimentterms.
We propose a number of characteristics ofgood sentiment terms from the perspectives of in-formativeness, prominence, topic?relevance, andsemantic aspects using collection statistics, con-textual information, semantic associations as wellas opinion?related properties of terms.
These termweighting features constitute the sentiment analy-sis model in our opinion retrieval system.
We testour opinion retrieval system with TREC and NT-CIR datasets to validate the effectiveness of ourterm weighting features.
We also verify the ef-fectiveness of the statistical features used in data-driven approaches by evaluating an ML classifierwith labeled corpora.2 Related WorkRepresenting text with salient features is an im-portant part of a text processing task, and there ex-ists many works that explore various features for253text analysis systems (Sebastiani, 2002; Forman,2003).
Sentiment analysis task have also been us-ing various lexical, syntactic, and statistical fea-tures (Pang and Lee, 2008).
Pang et al (2002)employed n-gram and POS features for ML meth-ods to classify movie-review data.
Also, syntac-tic features such as the dependency relationship ofwords and subtrees have been shown to effectivelyimprove the performances of sentiment analysis(Kudo and Matsumoto, 2004; Gamon, 2004; Mat-sumoto et al, 2005; Ng et al, 2006).While these features are usually employed bydata-driven approaches, there are unsupervised ap-proaches for sentiment analysis that make use of aset of terms that are semantically oriented towardexpressing subjective statements (Yu and Hatzi-vassiloglou, 2003).
Accordingly, much researchhas focused on recognizing terms?
semantic ori-entations and strength, and compiling sentimentlexicons (Hatzivassiloglou and Mckeown, 1997;Turney and Littman, 2003; Kamps et al, 2004;Whitelaw et al, 2005; Esuli and Sebastiani, 2006).Interestingly, there are conflicting conclusionsabout the usefulness of the statistical features insentiment analysis tasks (Pang and Lee, 2008).Pang et al (2002) presents empirical results in-dicating that using term presence over term fre-quency is more effective in a data-driven sentimentclassification task.
Such a finding suggests thatsentiment analysis may exploit different types ofcharacteristics from the topical tasks, that, unlikefact-based text analysis tasks, repetition of termsdoes not imply a significance on the overall senti-ment.
On the other hand, Wiebe et al (2004) havenoted that hapax legomena (terms that only appearonce in a collection of texts) are good signs fordetecting subjectivity.
Other works have also ex-ploited rarely occurring terms for sentiment anal-ysis tasks (Dave et al, 2003; Yang et al, 2006).The opinion retrieval task is a relatively recentissue that draws both the attention of IR and NLPcommunities.
Its task is to find relevant documentsthat also contain sentiments about a given topic.Generally, the opinion retrieval task has been ap-proached as a two?stage task: first, retrieving top-ically relevant documents, then reranking the doc-uments by the opinion scores (Ounis et al, 2006).This approach is also appropriate for evaluationsystems such as NTCIR MOAT that assumes thatthe set of topically relevant documents are alreadyknown in advance.
On the other hand, there arealso some interesting works on modeling the topicand sentiment of documents in a unified way (Meiet al, 2007; Zhang and Ye, 2008).3 Term Weighting and SentimentAnalysisIn this section, we describe the characteristics ofterms that are useful in sentiment analysis, andpresent our sentiment analysis model as part ofan opinion retrieval system and an ML sentimentclassifier.3.1 Characteristics of Good Sentiment TermsThis section examines the qualities of useful termsfor sentiment analysis tasks and correspondingfeatures.
For the sake of organization, we cate-gorize the sources of features into either global orlocal knowledge, and either topic-independent ortopic-dependent knowledge.Topic-independently speaking, a good senti-ment term is discriminative and prominent, suchthat the appearance of the term imposes greaterinfluence on the judgment of the analysis system.The rare occurrence of terms in document collec-tions has been regarded as a very important featurein IR methods, and effective IR models of today,either explicitly or implicitly, accommodate thisfeature as an Inverse Document Frequency (IDF)heuristic (Fang et al, 2004).
Similarly, promi-nence of a term is recognized by the frequency ofthe term in its local context, formulated as TermFrequency (TF) in IR.If a topic of the text is known, terms that are rel-evant and descriptive of the subject should be re-garded to be more useful than topically-irrelevantand extraneous terms.
One way of measuring thisis using associations between the query and terms.Statistical measures of associations between termsinclude estimations by the co-occurrence in thewhole collection, such as Point-wise Mutual In-formation (PMI) and Latent Semantic Analysis(LSA).
Another method is to use proximal infor-mation of the query and the word, using syntacticstructure such as dependency relations of wordsthat provide the graphical representation of thetext (Mullen and Collier, 2004).
The minimumspans of words in such graph may represent theirassociations in the text.
Also, the distance betweenwords in the local context or in the thesaurus-like dictionaries such as WordNet may be approx-imated as such measure.2543.2 Opinion Retrieval ModelThe goal of an opinion retrieval system is to find aset of opinionated documents that are relevant to agiven topic.
We decompose the opinion retrievalsystem into two tasks: the topical retrieval taskand the sentiment analysis task.
This two-stageapproach for opinion retrieval has been taken bymany systems and has been shown to perform well(Ounis et al, 2006).
The topic and the sentimentaspects of the opinion retrieval task are modeledseparately, and linearly combined together to pro-duce a list of topically-relevant and opinionateddocuments as below.ScoreOpRet(D,Q) = ??Scorerel(D,Q)+(1??
)?Scoreop(D,Q)The topic-relevance model Scorerel may be sub-stituted by any IR system that retrieves relevantdocuments for the query Q.
For tasks such asNTCIR MOAT, relevant documents are alreadyknown in advance and it becomes unnecessary toestimate the relevance degree of the documents.We focus on modeling the sentiment aspect ofthe opinion retrieval task, assuming that the topic-relevance of documents is provided in some way.To assign documents with sentiment degrees,we estimate the probability of a document D togenerate a query Q and to possess opinions as in-dicated by a random variable Op.1 Assuming uni-form prior probabilities of documentsD, queryQ,and Op, and conditional independence between Qand Op, the opinion score function reduces to es-timating the generative probability of Q and Opgiven D.Scoreop(D,Q) ?
p(D | Op,Q) ?
p(Op,Q | D)If we regard that the document D is representedas a bag of words and that the words are uniformlydistributed, thenp(Op,Q | D) =Xw?Dp(Op,Q | w) ?
p(w | D)=Xw?Dp(Op | w) ?
p(Q | w) ?
p(w | D) (1)Equation 1 consists of three factors: the proba-bility of a word to be opinionated (P (Op|w)), thelikelihood of a query given a word (P (Q|w)), andthe probability of a document generating a word(P (w|D)).
Intuitively speaking, the probability ofa document embodying topically related opinion isestimated by accumulating the probabilities of all1Throughout this paper, Op indicates Op = 1.words from the document to have sentiment mean-ings and associations with the given query.In the following sections, we assess the threefactors of the sentiment models from the perspec-tives of term weighting.3.2.1 Word Sentiment ModelModeling the sentiment of a word has been a pop-ular approach in sentiment analysis.
There aremany publicly available lexicon resources.
Thesize, format, specificity, and reliability differ in allthese lexicons.
For example, lexicon sizes rangefrom a few hundred to several hundred thousand.Some lexicons assign real number scores to in-dicate sentiment orientations and strengths (i.e.probabilities of having positive and negative sen-timents) (Esuli and Sebastiani, 2006) while otherlexicons assign discrete classes (weak/strong, pos-itive/negative) (Wilson et al, 2005).
There aremanually compiled lexicons (Stone et al, 1966)while some are created semi-automatically by ex-panding a set of seed terms (Esuli and Sebastiani,2006).The goal of this paper is not to create or choosean appropriate sentiment lexicon, but rather it isto discover useful term features other than thesentiment properties.
For this reason, one sen-timent lexicon, namely SentiWordNet, is utilizedthroughout the whole experiment.SentiWordNet is an automatically generatedsentiment lexicon using a semi-supervised method(Esuli and Sebastiani, 2006).
It consists of Word-Net synsets, where each synset is assigned threeprobability scores that add up to 1: positive, nega-tive, and objective.These scores are assigned at sense level (synsetsin WordNet), and we use the following equationsto assess the sentiment scores at the word level.p(Pos | w) = maxs?synset(w)SWNPos(s)p(Neg | w) = maxs?synset(w)SWNNeg(s)p(Op | w) = max (p(Pos | w), p(Neg | w))where synset(w) is the set of synsets of w andSWNPos(s), SWNNeg(s) are positive and neg-ative scores of a synset in SentiWordNet.
We as-sess the subjective score of a word as the maxi-mum value of the positive and the negative scores,because a word has either a positive or a negativesentiment in a given context.The word sentiment model can also make useof other types of sentiment lexicons.
The sub-255jectivity lexicon used in OpinionFinder2 is com-piled from several manually and automaticallybuilt resources.
Each word in the lexicon is taggedwith the strength (strong/weak) and polarity (Pos-itive/Negative/Neutral).
The word sentiment canbe modeled as below.P (Pos|w) =8><>:1.0 if w is Positive and Strong0.5 if w is Positive and Weak0.0 otherwiseP (Op | w) = max (p(Pos | w), p(Neg | w))3.2.2 Topic Association ModelIf a topic is given in the sentiment analysis, termsthat are closely associated with the topic shouldbe assigned heavy weighting.
For example, sen-timent words such as scary and funny are morelikely to be associated with topic words such asbook and movie than grocery or refrigerator.In the topic association model, p(Q | w) is es-timated from the associations between the word wand a set of query terms Q.p(Q | w) =Pq?Q Asc-Score(q, w)| Q |?Xq?QAsc-Score(q, w)Asc-Score(q, w) is the association score betweenq and w, and | Q | is the number of query words.To measure associations between words, weemploy statistical approaches using document col-lections such as LSA and PMI, and local proximityfeatures using the distance in dependency trees ortexts.Latent Semantic Analysis (LSA) (Landauer andDumais, 1997) creates a semantic space from acollection of documents to measure the semanticrelatedness of words.
Point-wise Mutual Informa-tion (PMI) is a measure of associations used in in-formation theory, where the association betweentwo words is evaluated with the joint and individ-ual distributions of the two words.
PMI-IR (Tur-ney, 2001) uses an IR system and its search op-erators to estimate the probabilities of two termsand their conditional probabilities.
Equations forassociation scores using LSA and PMI are givenbelow.Asc-ScoreLSA(w1, w2) =1 + LSA(w1, w2)2Asc-ScorePMI(w1, w2) =1 + PMI-IR(w1, w2)22http://www.cs.pitt.edu/mpqa/For the experimental purpose, we used publiclyavailable online demonstrations for LSA and PMI.For LSA, we used the online demonstration modefrom the Latent Semantic Analysis page from theUniversity of Colorado at Boulder.3 For PMI, weused the online API provided by the CogWorksLab at the Rensselaer Polytechnic Institute.4Word associations between two terms may alsobe evaluated in the local context where the termsappear together.
One way of measuring the prox-imity of terms is using the syntactic structures.Given the dependency tree of the text, we modelthe association between two terms as below.Asc-ScoreDTP (w1, w2) =(1.0 min.
span in dep.
tree ?
Dsyn0.5 otherwisewhere, Dsyn is arbitrarily set to 3.Another way is to use co-occurrence statisticsas below.Asc-ScoreWP (w1, w2) =(1.0 if distance betweenw1andw2 ?
K0.5 otherwisewhere K is the maximum window size for theco-occurrence and is arbitrarily set to 3 in our ex-periments.The statistical approaches may suffer from datasparseness problems especially for named entityterms used in the query, and the proximal cluescannot sufficiently cover all term?query associa-tions.
To avoid assigning zero probabilities, ourtopic association models assign 0.5 to word pairswith no association and 1.0 to words with perfectassociation.Note that proximal features using co-occurrenceand dependency relationships were used in pre-vious work.
For opinion retrieval tasks, Yang etal.
(2006) and Zhang and Ye (2008) used the co-occurrence of a query word and a sentiment wordwithin a certain window size.
Mullen and Collier(2004) manually annotated named entities in theirdataset (i.e.
title of the record and name of theartist for music record reviews), and utilized pres-ence and position features in their ML approach.3.2.3 Word Generation ModelOur word generation model p(w | d) evaluates theprominence and the discriminativeness of a word3http://lsa.colorado.edu/, default parameter settings forthe semantic space (TASA, 1st year college level) and num-ber of factors (300).4http://cwl-projects.cogsci.rpi.edu/msr/, PMI-IR with theGoogle Search Engine.256w in a document d. These issues correspond to thecore issues of traditional IR tasks.
IR models, suchas Vector Space (VS), probabilistic models suchas BM25, and Language Modeling (LM), albeit indifferent forms of approach and measure, employheuristics and formal modeling approaches to ef-fectively evaluate the relevance of a term to a doc-ument (Fang et al, 2004).
Therefore, we estimatethe word generation model with popular IR mod-els?
the relevance scores of a document d given was a query.5p(w | d) ?
IR-SCORE(w, d)In our experiments, we use the Vector Spacemodel with Pivoted Normalization (VS), Proba-bilistic model (BM25), and Language modelingwith Dirichlet Smoothing (LM).V SPN(w, d) =1 + ln(1 + ln(c(w, d)))(1?
s) + s ?| d |avgdl?
lnN + 1df(w)BM25(w, d) = lnN ?
df(w) + 0.5df(w) + 0.5?
(k1 + 1) ?
c(w, d)k1?(1?
b) + b |d|avgdl?+ c(w, d)LMDI(w, d) = ln1 +c(w, d)?
?
c(w,C)!+ ln?| d | +?c(w, d) is the frequency of w in d, | d | is thenumber of unique terms in d, avgdl is the average| d | of all documents, N is the number of doc-uments in the collection, df(w) is the number ofdocuments with w, C is the entire collection, andk1 and b are constants 2.0 and 0.75.3.3 Data-driven ApproachTo verify the effectiveness of our term weight-ing schemes in experimental settings of the data-driven approach, we carry out a set of simple ex-periments with ML classifiers.
Specifically, weexplore the statistical term weighting features ofthe word generation model with Support Vectormachine (SVM), faithfully reproducing previouswork as closely as possible (Pang et al, 2002).Each instance of train and test data is repre-sented as a vector of features.
We test variouscombinations of the term weighting schemes listedbelow.?
PRESENCE: binary indicator for the pres-ence of a term?
TF: term frequency5With proper assumptions and derivations, p(w | d) canbe derived to language modeling approaches.
Refer to (Zhaiand Lafferty, 2004).?
VS.TF: normalized tf as in VS?
BM25.TF: normalized tf as in BM25?
IDF: inverse document frequency?
VS.IDF: normalized idf as in VS?
BM25.IDF: normalized idf as in BM254 ExperimentOur experiments consist of an opinion retrievaltask and a sentiment classification task.
We useMPQA and movie-review corpora in our experi-ments with an ML classifier.
For the opinion re-trieval task, we use the two datasets used by TRECblog track and NTCIR MOAT evaluation work-shops.The opinion retrieval task at TREC Blog Trackconsists of three subtasks: topic retrieval, opinionretrieval, and polarity retrieval.
Opinion and polar-ity retrieval subtasks use the relevant documentsretrieved at the topic retrieval stage.
On the otherhand, the NTCIR MOAT task aims to find opin-ionated sentences given a set of documents that arealready hand-assessed to be relevant to the topic.4.1 Opinion Retieval Task ?
TREC BlogTrack4.1.1 Experimental SettingTREC Blog Track uses the TREC Blog06 corpus(Macdonald and Ounis, 2006).
It is a collectionof RSS feeds (38.6 GB), permalink documents(88.8GB), and homepages (28.8GB) crawled onthe Internet over an eleven week period from De-cember 2005 to February 2006.Non-relevant content of blog posts such asHTML tags, advertisement, site description, andmenu are removed with an effective internal spamremoval algorithm (Nam et al, 2009).
While oursentiment analysis model uses the entire relevantportion of the blog posts, further stopword re-moval and stemming is done for the blog retrievalsystem.For the relevance retrieval model, we faithfullyreproduce the passage-based language model withpseudo-relevance feedback (Lee et al, 2008).We use in total 100 topics from TREC 2007 and2008 blog opinion retrieval tasks (07:901-950 and08:1001-1050).
We use the topics from Blog 07to optimize the parameter for linearly combiningthe retrieval and opinion models, and use Blog 08topics as our test data.
Topics are extracted onlyfrom the Title field, using the Porter stemmer anda stopword list.257Table 1: Performance of opinion retrieval modelsusing Blog 08 topics.
The linear combination pa-rameter ?
is optimized on Blog 07 topics.
?
indi-cates statistical significance at the 1% level overthe baseline.Model MAP R-prec P@10TOPIC REL.
0.4052 0.4366 0.6440BASELINE 0.4141 0.4534 0.6440VS 0.4196 0.4542 0.6600BM25 0.4235?
0.4579 0.6600LM 0.4158 0.4520 0.6560PMI 0.4177 0.4538 0.6620LSA 0.4155 0.4526 0.6480WP 0.4165 0.4533 0.6640BM25?PMI 0.4238?
0.4575 0.6600BM25?LSA 0.4237?
0.4578 0.6600BM25?WP 0.4237?
0.4579 0.6600BM25?PMI?WP 0.4242?
0.4574 0.6620BM25?LSA?WP 0.4238?
0.4576 0.65804.1.2 Experimental ResultRetrieval performances using different combina-tions of term weighting features are presented inTable 1.
Using only the word sentiment model isset as our baseline.First, each feature of the word generation andtopic association models are tested; all features ofthe models improve over the baseline.
We observethat the features of our word generation model ismore effective than those of the topic associationmodel.
Among the features of the word generationmodel, the most improvement was achieved withBM25, improving the MAP by 2.27%.Features of the topic association model showonly moderate improvements over the baseline.We observe that these features generally improveP@10 performance, indicating that they increasethe accuracy of the sentiment analysis system.PMI out-performed LSA for all evaluation mea-sures.
Among the topic association models, PMIperforms the best in MAP and R-prec, while WPachieved the biggest improvement in P@10.Since BM25 performs the best among the wordgeneration models, its combination with other fea-tures was investigated.
Combinations of BM25with the topic association models all improve theperformance of the baseline and BM25.
Thisdemonstrates that the word generation model andthe topic association model are complementary toeach other.The best MAP was achieved with BM25, PMI,and WP (+2.44% over the baseline).
We observethat PMI and WP also complement each other.4.2 Sentiment Analysis Task ?
NTCIRMOAT4.2.1 Experimental SettingAnother set of experiments for our opinion analy-sis model was carried out on the NTCIR-7 MOATEnglish corpus.
The English opinion corpusfor NTCIR MOAT consists of newspaper articlesfrom the Mainichi Daily News, Korea Times, Xin-hua News, Hong Kong Standard, and the StraitsTimes.
It is a collection of documents manu-ally assessed for relevance to a set of queriesfrom NTCIR-7 Advanced Cross-lingual Informa-tion Access (ACLIA) task.
The corpus consists of167 documents, or 4,711 sentences for 14 test top-ics.
Each sentence is manually tagged with opin-ionatedness, polarity, and relevance to the topic bythree annotators from a pool of six annotators.For preprocessing, no removal or stemming isperformed on the data.
Each sentence was pro-cessed with the Stanford English parser6 to pro-duce a dependency parse tree.
Only the Title fieldsof the topics were used.For performance evaluations of opinion and po-larity detection, we use precision, recall, and F-measure, the same measure used to report the offi-cial results at the NTCIR MOAT workshop.
Thereare lenient and strict evaluations depending on theagreement of the annotators; if two out of three an-notators agreed upon an opinion or polarity anno-tation then it is used during the lenient evaluation,similarly three out of three agreements are usedduring the strict evaluation.
We present the perfor-mances using the lenient evaluation only, for thetwo evaluations generally do not show much dif-ference in relative performance changes.Since MOAT is a classification task, we use athreshold parameter to draw a boundary betweenopinionated and non-opinionated sentences.
Wereport the performance of our system using theNTCIR-7 dataset, where the threshold parameteris optimized using the NTCIR-6 dataset.4.2.2 Experimental ResultWe present the performance of our sentiment anal-ysis system in Table 2.
As in the experiments with6http://nlp.stanford.edu/software/lex-parser.shtml258Table 2: Performance of the Sentiment Analy-sis System on NTCIR7 dataset.
System parame-ters are optimized for F-measure using NTCIR6dataset with lenient evaluations.OpinionatedModel Precision Recall F-MeasureBASELINE 0.305 0.866 0.451VS 0.331 0.807 0.470BM25 0.327 0.795 0.464LM 0.325 0.794 0.461LSA 0.315 0.806 0.453PMI 0.342 0.603 0.436DTP 0.322 0.778 0.455VS?LSA 0.335 0.769 0.466VS?PMI 0.311 0.833 0.453VS?DTP 0.342 0.745 0.469VS?LSA?DTP 0.349 0.719 0.470VS?PMI?DTP 0.328 0.773 0.461the TREC dataset, using only the word sentimentmodel is used as our baseline.Similarly to the TREC experiments, the featuresof the word generation model perform exception-ally better than that of the topic association model.The best performing feature of the word genera-tion model is VS, achieving a 4.21% improvementover the baseline?s f-measure.
Interestingly, this isthe tied top performing f-measure over all combi-nations of our features.While LSA and DTP show mild improvements,PMI performed worse than baseline, with higherprecision but a drop in recall.
DTP was the bestperforming topic association model.When combining the best performing featureof the word generation model (VS) with the fea-tures of the topic association model, LSA, PMIand DTP all performed worse than or as well asthe VS in f-measure evaluation.
LSA and DTP im-proves precision slightly, but with a drop in recall.PMI shows the opposite tendency.The best performing system was achieved usingVS, LSA and DTP at both precision and f-measureevaluations.4.3 Classification task ?
SVM4.3.1 Experimental SettingTo test our SVM classifier, we perform the classi-fication task.
Movie Review polarity dataset7 was7http://www.cs.cornell.edu/people/pabo/movie-review-data/Table 3: Average ten-fold cross-validation accura-cies of polarity classification task with SVM.AccuracyFeatures Movie-review MPQAPRESENCE 82.6 76.8TF 71.1 76.5VS.TF 81.3 76.7BM25.TF 81.4 77.9IDF 61.6 61.8VS.IDF 83.6 77.9BM25.IDF 83.6 77.8VS.TF?VS.IDF 83.8 77.9BM25.TF?BM25.IDF 84.1 77.7BM25.TF?VS.IDF 85.1 77.7first introduced by Pang et al (2002) to test variousML-based methods for sentiment classification.
Itis a balanced dataset of 700 positive and 700 neg-ative reviews, collected from the Internet MovieDatabase (IMDb) archive.
MPQA Corpus8 con-tains 535 newspaper articles manually annotatedat sentence and subsentence level for opinions andother private states (Wiebe et al, 2005).To closely reproduce the experiment with thebest performance carried out in (Pang et al, 2002)using SVM, we use unigram with the presencefeature.
We test various combinations of our fea-tures applicable to the task.
For evaluation, we useten-fold cross-validation accuracy.4.3.2 Experimental ResultWe present the sentiment classification perfor-mances in Table 3.As observed by Pang et al (2002), using the rawtf drops the accuracy of the sentiment classifica-tion (-13.92%) of movie-review data.
Using theraw idf feature worsens the accuracy even more(-25.42%).
Normalized tf-variants show improve-ments over tf but are worse than presence.
Nor-malized idf features produce slightly better accu-racy results than the baseline.
Finally, combiningany normalized tf and idf features improved thebaseline (high 83% ?
low 85%).
The best combi-nation was BM25.TF?VS.IDF.MPQA corpus reveals similar but somewhat un-certain tendency.8http://www.cs.pitt.edu/mpqa/databaserelease/2594.4 DiscussionOverall, the opinion retrieval and the sentimentanalysis models achieve improvements using ourproposed features.
Especially, the features of theword generation model improve the overall per-formances drastically.
Its effectiveness is also ver-ified with a data-driven approach; the accuracy ofa sentiment classifier trained on a polarity datasetwas improved by various combinations of normal-ized tf and idf statistics.Differences in effectiveness of VS, BM25, andLM come from parameter tuning and corpus dif-ferences.
For the TREC dataset, BM25 performedbetter than the other models, and for the NTCIRdataset, VS performed better.Our features of the topic association modelshow mild improvement over the baseline perfor-mance in general.
PMI and LSA, both modelingthe semantic associations between words, showdifferent behaviors on the datasets.
For the NT-CIR dataset, LSA performed better, while PMIis more effective for the TREC dataset.
We be-lieve that the explanation lies in the differencesbetween the topics for each dataset.
In general,the NTCIR topics are general descriptive wordssuch as ?regenerative medicine?, ?American econ-omy after the 911 terrorist attacks?, and ?law-suit brought against Microsoft for monopolisticpractices.?
The TREC topics are more named-entity-like terms such as ?Carmax?, ?Wikipediaprimary source?, ?Jiffy Lube?, ?Starbucks?, and?Windows Vista.?
We have experimentally shownthat LSA is more suited to finding associationsbetween general terms because its training docu-ments are from a general domain.9 Our PMI mea-sure utilizes a web search engine, which covers avariety of named entity terms.Though the features of our topic associationmodel, WP and DTP, were evaluated on differentdatasets, we try our best to conjecture the differ-ences.
WP on TREC dataset shows a small im-provement of MAP compared to other topic asso-ciation features, while the precision is improvedthe most when this feature is used alone.
The DTPfeature displays similar behavior with precision.
Italso achieves the best f-measure over other topicassociation features.
DTP achieves higher rela-tive improvement (3.99% F-measure verse 2.32%MAP), and is more effective for improving the per-formance in combination with LSA and PMI.9TASA Corpus, http://lsa.colorado.edu/spaces.html5 ConclusionIn this paper, we proposed various term weightingschemes and how such features are modeled in thesentiment analysis task.
Our proposed features in-clude corpus statistics, association measures usingsemantic and local-context proximities.
We haveempirically shown the effectiveness of the featureswith our proposed opinion retrieval and sentimentanalysis models.There exists much room for improvement withfurther experiments with various term weightingmethods and datasets.
Such methods include,but by no means limited to, semantic similaritiesbetween word pairs using lexical resources suchas WordNet (Miller, 1995) and data-driven meth-ods with various topic-dependent term weightingschemes on labeled corpus with topics such asMPQA.AcknowledgmentsThis work was supported in part by MKE & IITAthrough IT Leading R&D Support Project and inpart by the BK 21 Project in 2009.ReferencesKushal Dave, Steve Lawrence, and David M. Pennock.
2003.Mining the peanut gallery: Opinion extraction and seman-tic classification of product reviews.
In Proceedings ofWWW, pages 519?528.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinion min-ing.
In Proceedings of the 5th Conference on LanguageResources and Evaluation (LREC?06), pages 417?422,Geneva, IT.Hui Fang, Tao Tao, and ChengXiang Zhai.
2004.
A formalstudy of information retrieval heuristics.
In SIGIR ?04:Proceedings of the 27th annual international ACM SIGIRconference on Research and development in informationretrieval, pages 49?56, New York, NY, USA.
ACM.George Forman.
2003.
An extensive empirical study of fea-ture selection metrics for text classification.
Journal ofMachine Learning Research, 3:1289?1305.Michael Gamon.
2004.
Sentiment classification on customerfeedback data: noisy data, large feature vectors, and therole of linguistic analysis.
In Proceedings of the Inter-national Conference on Computational Linguistics (COL-ING).Vasileios Hatzivassiloglou and Kathleen R. Mckeown.
1997.Predicting the semantic orientation of adjectives.
In Pro-ceedings of the 35th Annual Meeting of the Associationfor Computational Linguistics (ACL?97), pages 174?181,madrid, ES.Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten De Rijke.
2004.
Using wordnet to measure se-mantic orientation of adjectives.
In Proceedings of the4th International Conference on Language Resources andEvaluation (LREC?04), pages 1115?1118, Lisbon, PT.260Taku Kudo and Yuji Matsumoto.
2004.
A boosting algorithmfor classification of semi-structured text.
In Proceedingsof the Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Thomas K. Landauer and Susan T. Dumais.
1997.
A solutionto plato?s problem: The latent semantic analysis theory ofacquisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240, April.Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob Nam,Hun young Jung, and Jong-Hyeok Lee.
2008.
Kle at trec2008 blog track: Blog post and feed retrieval.
In Proceed-ings of TREC-08.Craig Macdonald and Iadh Ounis.
2006.
The TREC Blogs06collection: creating and analysing a blog test collection.Technical Report TR-2006-224, Department of ComputerScience, University of Glasgow.Shotaro Matsumoto, Hiroya Takamura, and Manabu Oku-mura.
2005.
Sentiment classification using word sub-sequences and dependency sub-trees.
In Proceedings ofPAKDD?05, the 9th Pacific-Asia Conference on Advancesin Knowledge Discovery and Data Mining.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture: Mod-eling facets and opinions in weblogs.
In Proceedings ofWWW, pages 171?180, New York, NY, USA.
ACM Press.George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Commun.
ACM, 38(11):39?41.Tony Mullen and Nigel Collier.
2004.
Sentiment analysisusing support vector machines with diverse informationsources.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing (EMNLP),pages 412?418, July.
Poster paper.Sang-Hyob Nam, Seung-Hoon Na, Yeha Lee, and Jong-Hyeok Lee.
2009.
Diffpost: Filtering non-relevant con-tent based on content difference between two consecutiveblog posts.
In ECIR.Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006.Examining the role of linguistic knowledge sources in theautomatic identification and classification of reviews.
InProceedings of the COLING/ACL Main Conference PosterSessions, pages 611?618, Sydney, Australia, July.
Associ-ation for Computational Linguistics.I.
Ounis, M. de Rijke, C. Macdonald, G. A. Mishne, andI.
Soboroff.
2006.
Overview of the trec-2006 blog track.In Proceedings of TREC-06, pages 15?27, November.I.
Ounis, C. Macdonald, and I. Soboroff.
2008.
Overviewof the trec-2008 blog track.
In Proceedings of TREC-08,pages 15?27, November.Bo Pang and Lillian Lee.
2008.
Opinion mining and sen-timent analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.Thumbs up?
Sentiment classification using machinelearning techniques.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing(EMNLP), pages 79?86.Fabrizio Sebastiani.
2002.
Machine learning in automatedtext categorization.
ACM Computing Surveys, 34(1):1?47.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando.
2008.
Overview of mul-tilingual opinion analysis task at ntcir-7.
In Proceedingsof The 7th NTCIR Workshop (2007/2008) - Evaluation ofInformation Access Technologies: Information Retrieval,Question Answering and Cross-Lingual Information Ac-cess.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith, andDaniel M. Ogilvie.
1966.
The General Inquirer: A Com-puter Approach to Content Analysis.
MIT Press, Cam-bridge, USA.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on Information Sys-tems, 21(4):315?346.Peter D. Turney.
2001.
Mining the web for synonyms: Pmi-ir versus lsa on toefl.
In EMCL ?01: Proceedings of the12th European Conference on Machine Learning, pages491?502, London, UK.
Springer-Verlag.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.
InProceedings of the 14th ACM international conferenceon Information and knowledge management (CIKM?05),pages 625?631, Bremen, DE.Janyce Wiebe, E. Breck, Christopher Buckley, Claire Cardie,P.
Davis, B. Fraser, Diane Litman, D. Pierce, Ellen Riloff,Theresa Wilson, D. Day, and Mark Maybury.
2003.
Rec-ognizing and organizing opinions expressed in the worldpress.
In Proceedings of the 2003 AAAI Spring Sympo-sium on New Directions in Question Answering.Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjec-tive language.
Computational Linguistics, 30(3):277?308,September.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation,39(2/3):164?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level sentimentanalysis.
In Proceedings of the Conference on HumanLanguage Technology and Empirical Methods in NaturalLanguage Processing (HLT-EMNLP?05), pages 347?354,Vancouver, CA.Kiduk Yang, Ning Yu, Alejandro Valerio, and Hui Zhang.2006.
WIDIT in TREC-2006 Blog track.
In Proceedingsof TREC.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towards an-swering opinion questions: Separating facts from opinionsand identifying the polarity of opinion sentences.
In Pro-ceedings of 2003 Conference on the Empirical Methods inNatural Language Processing (EMNLP?03), pages 129?136, Sapporo, JP.Chengxiang Zhai and John Lafferty.
2004.
A study ofsmoothing methods for language models applied to infor-mation retrieval.
ACM Trans.
Inf.
Syst., 22(2):179?214.Min Zhang and Xingyao Ye.
2008.
A generation modelto unify topic relevance and lexicon-based sentiment foropinion retrieval.
In SIGIR ?08: Proceedings of the 31stannual international ACM SIGIR conference on Researchand development in information retrieval, pages 411?418,New York, NY, USA.
ACM.261
