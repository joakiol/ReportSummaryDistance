Proceedings of ACL-08: HLT, pages 371?379,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Single Generative Modelfor Joint Morphological Segmentation and Syntactic ParsingYoav GoldbergBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israelyoavg@cs.bgu.ac.ilReut TsarfatyInstitute for Logic Language and ComputationUniversity of AmsterdamPlantage Muidergracht 24, Amsterdam, NLrtsarfat@science.uva.nlAbstractMorphological processes in Semitic languagesdeliver space-delimited words which intro-duce multiple, distinct, syntactic units into thestructure of the input sentence.
These wordsare in turn highly ambiguous, breaking theassumption underlying most parsers that theyield of a tree for a given sentence is known inadvance.
Here we propose a single joint modelfor performing both morphological segmenta-tion and syntactic disambiguation which by-passes the associated circularity.
Using a tree-bank grammar, a data-driven lexicon, and alinguistically motivated unknown-tokens han-dling technique our model outperforms previ-ous pipelined, integrated or factorized systemsfor Hebrew morphological and syntactic pro-cessing, yielding an error reduction of 12%over the best published results so far.1 IntroductionCurrent state-of-the-art broad-coverage parsers as-sume a direct correspondence between the lexicalitems ingrained in the proposed syntactic analyses(the yields of syntactic parse-trees) and the space-delimited tokens (henceforth, ?tokens?)
that consti-tute the unanalyzed surface forms (utterances).
InSemitic languages the situation is very different.In Modern Hebrew (Hebrew), a Semitic languagewith very rich morphology, particles marking con-junctions, prepositions, complementizers and rela-tivizers are bound elements prefixed to the word(Glinert, 1989).
The Hebrew token ?bcl?1, for ex-ample, stands for the complete prepositional phrase1We adopt here the transliteration of (Sima?an et al, 2001).
?in the shadow?.
This token may further embedinto a larger utterance, e.g., ?bcl hneim?
(literally?in-the-shadow the-pleasant?, meaning roughly ?inthe pleasant shadow?)
in which the dominated Nounis modified by a proceeding space-delimited adjec-tive.
It should be clear from the onset that the parti-cle b (?in?)
in ?bcl?
may then attach higher than thebare noun cl (?shadow?).
This leads to word- andconstituent-boundaries discrepancy, which breaksthe assumptions underlying current state-of-the-artstatistical parsers.One way to approach this discrepancy is to as-sume a preceding phase of morphological segmen-tation for extracting the different lexical items thatexist at the token level (as is done, to the best ofour knowledge, in all parsing related work on Arabicand its dialects (Chiang et al, 2006)).
The input forthe segmentation task is however highly ambiguousfor Semitic languages, and surface forms (tokens)may admit multiple possible analyses as in (Bar-Haim et al, 2007; Adler and Elhadad, 2006).
Theaforementioned surface form bcl, for example, mayalso stand for the lexical item ?onion?, a Noun.
Theimplication of this ambiguity for a parser is that theyield of syntactic trees no longer consists of space-delimited tokens, and the expected number of leavesin the syntactic analysis in not known in advance.Tsarfaty (2006) argues that for Semitic languagesdetermining the correct morphological segmentationis dependent on syntactic context and shows that in-creasing information sharing between the morpho-logical and the syntactic components leads to im-proved performance on the joint task.
Cohen andSmith (2007) followed up on these results and pro-371posed a system for joint inference of morphologicaland syntactic structures using factored models eachdesigned and trained on its own.Here we push the single-framework conjectureacross the board and present a single model thatperforms morphological segmentation and syntac-tic disambiguation in a fully generative framework.We claim that no particular morphological segmen-tation is a-priory more likely for surface forms be-fore exploring the compositional nature of syntac-tic structures, including manifestations of variouslong-distance dependencies.
Morphological seg-mentation decisions in our model are delegated to alexeme-based PCFG and we show that using a sim-ple treebank grammar, a data-driven lexicon, anda linguistically motivated unknown-tokens handlingour model outperforms (Tsarfaty, 2006) and (Co-hen and Smith, 2007) on the joint task and achievesstate-of-the-art results on a par with current respec-tive standalone models.22 Modern Hebrew StructureSegmental morphology Hebrew consists ofseven particles m(?from?)
f (?when?/?who?/?that?)h(?the?)
w(?and?)
k(?like?)
l(?to?)
and b(?in?
).which may never appear in isolation and mustalways attach as prefixes to the following open-classcategory item we refer to as stem.
Several suchparticles may be prefixed onto a single stem, inwhich case the affixation is subject to strict linearprecedence constraints.
Co-occurrences among theparticles themselves are subject to further syntacticand lexical constraints relative to the stem.While the linear precedence of segmental mor-phemes within a token is subject to constraints, thedominance relations among their mother and sisterconstituents is rather free.
The relativizer f(?that?
)for example, may attach to an arbitrarily long rela-tive clause that goes beyond token boundaries.
Theattachment in such cases encompasses a long dis-tance dependency that cannot be captured by Marko-vian processes that are typically used for morpho-logical disambiguation.
The same argument holdsfor resolving PP attachment of a prefixed prepositionor marking conjunction of elements of any kind.A less canonical representation of segmental mor-2Standalone parsing models assume a segmentation Oracle.phology is triggered by a morpho-phonological pro-cess of omitting the definite article h when occur-ring after the particles b or l. This process triggersambiguity as for the definiteness status of Nounsfollowing these particles.We refer to such casesin which the concatenation of elements does notstrictly correspond to the original surface form assuper-segmental morphology.
An additional case ofsuper-segmental morphology is the case of Pronom-inal Clitics.
Inflectional features marking pronom-inal elements may be attached to different kinds ofcategories marking their pronominal complements.The additional morphological material in such casesappears after the stem and realizes the extendedmeaning.
The current work treats both segmentaland super-segmental phenomena, yet we note thatthere may be more adequate ways to treat super-segmental phenomena assuming Word-Based mor-phology as we explore in (Tsarfaty and Goldberg,2008).Lexical and Morphological Ambiguity The richmorphological processes for deriving Hebrew stemsgive rise to a high degree of ambiguity for Hebrewspace-delimited tokens.
The form fmnh, for exam-ple, can be understood as the verb ?lubricated?, thepossessed noun ?her oil?, the adjective ?fat?
or theverb ?got fat?.
Furthermore, the systematic way inwhich particles are prefixed to one another and ontoan open-class category gives rise to a distinct sortof morphological ambiguity: space-delimited tokensmay be ambiguous between several different seg-mentation possibilities.
The same form fmnh can besegmented as f-mnh, f (?that?)
functioning as a rele-tivizer with the form mnh.
The form mnh itself canbe read as at least three different verbs (?counted?,?appointed?, ?was appointed?
), a noun (?a portion?
),and a possessed noun (?her kind?
).Such ambiguities cause discrepancies betweentoken boundaries (indexed as white spaces) andconstituent boundaries (imposed by syntactic cate-gories) with respect to a surface form.
Such discrep-ancies can be aligned via an intermediate level ofPoS tags.
PoS tags impose a unique morphologicalsegmentation on surface tokens and present a uniquevalid yield for syntactic trees.
The correct ambigu-ity resolution of the syntactic level therefore helps toresolve the morphological one, and vice versa.3723 Previous Work on Hebrew ProcessingMorphological analyzers for Hebrew that analyze asurface form in isolation have been proposed by Se-gal (2000), Yona and Wintner (2005), and recentlyby the knowledge center for processing Hebrew (Itaiet al, 2006).
Such analyzers propose multiple seg-mentation possibilities and their corresponding anal-yses for a token in isolation but have no means todetermine the most likely ones.
Morphological dis-ambiguators that consider a token in context (an ut-terance) and propose the most likely morphologi-cal analysis of an utterance (including segmentation)were presented by Bar-Haim et al (2005), Adlerand Elhadad (2006), Shacham and Wintner (2007),and achieved good results (the best segmentation re-sult so far is around 98%).The development of the very first Hebrew Tree-bank (Sima?an et al, 2001) called for the explorationof general statistical parsing methods, but the appli-cation was at first limited.
Sima?an et al (2001) pre-sented parsing results for a DOP tree-gram modelusing a small data set (500 sentences) and semi-automatic morphological disambiguation.
Tsarfaty(2006) was the first to demonstrate that fully auto-matic Hebrew parsing is feasible using the newlyavailable 5000 sentences treebank.
Tsarfaty andSima?an (2007) have reported state-of-the-art resultson Hebrew unlexicalized parsing (74.41%) albeit as-suming oracle morphological segmentation.The joint morphological and syntactic hypothesiswas first discussed in (Tsarfaty, 2006; Tsarfaty andSima?an, 2004) and empirically explored in (Tsar-faty, 2006).
Tsarfaty (2006) used a morphologicalanalyzer (Segal, 2000), a PoS tagger (Bar-Haim etal., 2005), and a general purpose parser (Schmid,2000) in an integrated framework in which morpho-logical and syntactic components interact to shareinformation, leading to improved performance onthe joint task.
Cohen and Smith (2007) later onbased a system for joint inference on factored, inde-pendent, morphological and syntactic componentsof which scores are combined to cater for the jointinference task.
Both (Tsarfaty, 2006; Cohen andSmith, 2007) have shown that a single integratedframework outperforms a completely streamlinedimplementation, yet neither has shown a single gen-erative model which handles both tasks.4 Model Preliminaries4.1 The Status Space-Delimited TokensA Hebrew surface token may have several readings,each of which corresponding to a sequence of seg-ments and their corresponding PoS tags.
We referto different readings as different analyses wherebythe segments are deterministic given the sequence ofPoS tags.
We refer to a segment and its assigned PoStag as a lexeme, and so analyses are in fact sequencesof lexemes.
For brevity we omit the segments fromthe analysis, and so analysis of the form ?fmnh?
asf/REL mnh/VB is represented simply as REL VB.Such tag sequences are often treated as ?complextags?
(e.g.
REL+VB) (cf.
(Bar-Haim et al, 2007;Habash and Rambow, 2005)) and probabilities areassigned to different analyses in accordance withthe likelihood of their tags (e.g., ?fmnh is 30%likely to be tagged NN and 70% likely to be taggedREL+VB?).
Here we do not submit to this view.When a token fmnh is to be interpreted as the lex-eme sequence f /REL mnh/VB, the analysis intro-duces two distinct entities, the relativizer f (?that?
)and the verb mnh (?counted?
), and not as the com-plex entity ?that counted?.
When the same tokenis to be interpreted as a single lexeme fmnh, it mayfunction as a single adjective ?fat?.
There is no re-lation between these two interpretations other thenthe fact that their surface forms coincide, and we ar-gue that the only reason to prefer one analysis overthe other is compositional.
A possible probabilisticmodel for assigning probabilities to complex analy-ses of a surface form may beP (REL,VB|fmnh, context) =P (REL|f)P (VB|mnh,REL)P (REL,VB| context)and indeed recent sequential disambiguation modelsfor Hebrew (Adler and Elhadad, 2006) and Arabic(Smith et al, 2005) present similar models.We suggest that in unlexicalized PCFGs the syn-tactic context may be explicitly modeled in thederivation probabilities.
Hence, we take the prob-ability of the event fmnh analyzed as REL VB to beP (REL?
f|REL) ?
P (VB?
mnh|VB)This means that we generate f and mnh indepen-dently depending on their corresponding PoS tags,373and the context (as well as the syntactic relation be-tween the two) is modeled via the derivation result-ing in a sequence REL VB spanning the form fmnh.4.2 Lattice RepresentationWe represent all morphological analyses of a givenutterance using a lattice structure.
Each lattice arccorresponds to a segment and its corresponding PoStag, and a path through the lattice corresponds toa specific morphological segmentation of the utter-ance.
This is by now a fairly standard representa-tion for multiple morphological segmentation of He-brew utterances (Adler, 2001; Bar-Haim et al, 2005;Smith et al, 2005; Cohen and Smith, 2007; Adler,2007).
Figure 1 depicts the lattice for a 2-wordssentence bclm hneim.
We use double-circles to in-dicate the space-delimited token boundaries.
Notethat in our construction arcs can never cross tokenboundaries.
Every token is independent of the oth-ers, and the sentence lattice is in fact a concatena-tion of smaller lattices, one for each token.
Fur-thermore, some of the arcs represent lexemes notpresent in the input tokens (e.g.
h/DT, fl/POS), how-ever these are parts of valid analyses of the token (cf.super-segmental morphology section 2).
Segmentswith the same surface form but different PoS tagsare treated as different lexemes, and are representedas separate arcs (e.g.
the two arcs labeled neim fromnode 6 to 7).05bclm/NNP1b/IN2bcl/NN7hneim/VB6h/DTclm/NNclm/VBcl/NN3h/DT4fl/POSclm/NNhm/PRPneim/VBneim/JJFigure 1: The Lattice for the Hebrew Phrase bclm hneimA similar structure is used in speech recognition.There, a lattice is used to represent the possible sen-tences resulting from an interpretation of an acousticmodel.
In speech recognition the arcs of the latticeare typically weighted in order to indicate the prob-ability of specific transitions.
Given that weights onall outgoing arcs sum up to one, weights induce aprobability distribution on the lattice paths.
In se-quential tagging models such as (Adler and Elhadad,2006; Bar-Haim et al, 2007; Smith et al, 2005)weights are assigned according to a language modelbased on linear context.
In our model, however, alllattice paths are taken to be a-priori equally likely.5 A Generative PCFG ModelThe input for the joint task is a sequence W =w1, .
.
.
, wn of space-delimited tokens.
Each tokenmay admit multiple analyses, each of which a se-quence of one or more lexemes (we use li to denotea lexeme) belonging a presupposed Hebrew lexiconLEX .
The entries in such a lexicon may be thoughtof as meaningful surface segments paired up withtheir PoS tags li = ?si, pi?, but note that a surfacesegment s need not be a space-delimited token.The Input The set of analyses for a token is thusrepresented as a lattice in which every arc corre-sponds to a specific lexeme l, as shown in Figure1.
A morphological analyzer M : W ?
L is afunction mapping sentences in Hebrew (W ?
W)to their corresponding lattices (M(W ) = L ?
L).We define the lattice L to be the concatenation of thelattices Li corresponding to the input words wi (s.t.M(wi) = Li).
Each connected path ?l1 .
.
.
lk?
?L corresponds to one morphological segmentationpossibility of W .The Parser Given a sequence of input tokensW = w1 .
.
.
wn and a morphological analyzer, welook for the most probable parse tree pi s.t.p?i = arg maxpiP (pi|W,M)Since the lattice L for a given sentence W is deter-mined by the morphological analyzer M we havep?i = arg maxpiP (pi|W,M,L)Hence, our parser searches for a parse tree pi overlexemes ?l1 .
.
.
lk?
s.t.
li = ?si, pi?
?
LEX ,?l1 .
.
.
lk?
?
L and M(W ) = L. So we remain withp?i = arg maxpiP (pi|L)which is precisely the formula corresponding to theso-called lattice parsing familiar from speech recog-nition.
Every parse pi selects a specific morphologi-cal segmentation ?l1...lk?
(a path through the lattice).This is akin to PoS tags sequences induced by dif-ferent parses in the setup familiar from English andexplored in e.g.
(Charniak et al, 1996).374Our use of an unweighted lattice reflects our be-lief that all the segmentations of the given input sen-tence are a-priori equally likely; the only reason toprefer one segmentation over the another is due tothe overall syntactic context which is modeled viathe PCFG derivations.
A compatible view is pre-sented by Charniak et al (1996) who consider thekind of probabilities a generative parser should getfrom a PoS tagger, and concludes that these shouldbe P (w|t) ?and nothing fancier?.3 In our setting,therefore, the Lattice is not used to induce a proba-bility distribution on a linear context, but rather, it isused as a common-denominator of state-indexationof all segmentations possibilities of a surface form.This is a unique object for which we are able to de-fine a proper probability model.
Thus our proposedmodel is a proper model assigning probability massto all ?pi,L?
pairs, where pi is a parse tree and L isthe one and only lattice that a sequence of characters(and spaces) W over our alpha-beth gives rise to.
?pi,LP (pi,L) = 1; L uniquely index WThe Grammar Our parser looks for the mostlikely tree spanning a single path through the lat-tice of which the yield is a sequence of lexemes.This is done using a simple PCFG which is lexeme-based.
This means that the rules in our grammarare of two kinds: (a) syntactic rules relating non-terminals to a sequence of non-terminals and/or PoStags, and (b) lexical rules relating PoS tags to latticearcs (lexemes).
The possible analyses of a surfacetoken pose constraints on the analyses of specificsegments.
In order to pass these constraints onto theparser, the lexical rules in the grammar are of theform pi ?
?si, pi?Parameter Estimation The grammar probabili-ties are estimated from the corpus using simple rela-tive frequency estimates.
Lexical rules are estimatedin a similar manner.
We smooth Prf (p ?
?s, p?)
forrare and OOV segments (s ?
l, l ?
L, s unseen) us-ing a ?per-tag?
probability distribution over rare seg-ments which we estimate using relative frequencyestimates for once-occurring segments.3An English sentence with ambiguous PoS assignment canbe trivially represented as a lattice similar to our own, whereevery pair of consecutive nodes correspond to a word, and everypossible PoS assignment for this word is a connecting arc.Handling Unknown tokens When handling un-known tokens in a language such as Hebrew variousimportant aspects have to be borne in mind.
Firstly,Hebrew unknown tokens are doubly unknown: eachunknown token may correspond to several segmen-tation possibilities, and each segment in such se-quences may be able to admit multiple PoS tags.Secondly, some segments in a proposed segment se-quence may in fact be seen lexical events, i.e., forsome p tag Prf (p ?
?s, p?)
> 0, while other seg-ments have never been observed as a lexical eventbefore.
The latter arcs correspond to OOV wordsin English.
Finally, the assignments of PoS tags toOOV segments is subject to language specific con-straints relative to the token it was originated from.Our smoothing procedure takes into account allthe aforementioned aspects and works as follows.We first make use of our morphological analyzer tofind all segmentation possibilities by chopping offall prefix sequence possibilities (including the emptyprefix) and construct a lattice off of them.
The re-maining arcs are marked OOV.
At this stage the lat-tice path corresponds to segments only, with no PoSassigned to them.
In turn we use two sorts of heuris-tics, orthogonal to one another, to prune segmenta-tion possibilities based on lexical and grammaticalconstraints.
We simulate lexical constraints by usingan external lexical resource against which we verifywhether OOV segments are in fact valid Hebrew lex-emes.
This heuristics is used to prune all segmenta-tion possibilities involving ?lexically improper?
seg-ments.
For the remaining arcs, if the segment is infact a known lexeme it is tagged as usual, but for theOOV arcs which are valid Hebrew entries lackingtags assignment, we assign all possible tags and thensimulate a grammatical constraint.
Here, all token-internal collocations of tags unseen in our trainingdata are pruned away.
From now on all lattice arcsare tagged segments and the assignment of probabil-ity P (p ?
?s, p?)
to lattice arcs proceeds as usual.4A rather pathological case is when our lexicalheuristics prune away all segmentation possibilitiesand we remain with an empty lattice.
In such caseswe use the non-pruned lattice including all (possiblyungrammatical) segmentation, and let the statistics(including OOV) decide.
We empirically control for4Our heuristics may slightly alter Ppi,L P (pi, L) ?
1375the effect of our heuristics to make sure our pruningdoes not undermine the objectives of our joint task.6 Experimental SetupPrevious work on morphological and syntactic dis-ambiguation in Hebrew used different sets of data,different splits, differing annotation schemes, anddifferent evaluation measures.
Our experimentalsetup therefore is designed to serve two goals.
Ourprimary goal is to exploit the resources that are mostappropriate for the task at hand, and our secondarygoal is to allow for comparison of our models?
per-formance against previously reported results.
Whena comparison against previous results requires addi-tional pre-processing, we state it explicitly to allowfor the reader to replicate the reported results.Data We use the Hebrew Treebank, (Sima?anet al, 2001), provided by the knowledge centerfor processing Hebrew, in which sentences fromthe daily newspaper ?Ha?aretz?
are morphologicallysegmented and syntactically annotated.
The tree-bank has two versions, v1.0 and v2.0, containing5001 and 6501 sentences respectively.
We use v1.0mainly because previous studies on joint inferencereported results w.r.t.
v1.0 only.5 We expect thatusing the same setup on v2.0 will allow a cross-treebank comparison.6 We used the first 500 sen-tences as our dev set and the rest 4500 for trainingand report our main results on this split.
To facili-tate the comparison of our results to those reportedby (Cohen and Smith, 2007) we use their data set inwhich 177 empty and ?malformed?7 were removed.The first 3770 trees of the resulting set then wereused for training, and the last 418 are used testing.
(we ignored the 419 trees in their development set.
)Morphological Analyzer Ideally, we would usean of-the-shelf morphological analyzer for mappingeach input token to its possible analyses.
Such re-sources exist for Hebrew (Itai et al, 2006), but un-fortunately use a tagging scheme which is incom-5The comparison to performance on version 2.0 is meaning-less not only because of the change in size, but also conceptualchanges in the annotation scheme6Unfortunatley running our setup on the v2.0 data set is cur-rently not possible due to missing tokens-morphemes alignmentin the v2.0 treebank.7We thank Shay Cohen for providing us with their data setand evaluation Software.patible with the one of the Hebrew Treebank.8 Forthis reason, we use a data-driven morphological an-alyzer derived from the training data similar to (Co-hen and Smith, 2007).
We construct a mapping fromall the space-delimited tokens seen in the trainingsentences to their corresponding analyses.Lexicon and OOV Handling Our data-drivenmorphological-analyzer proposes analyses for un-known tokens as described in Section 5.
We use theHSPELL9 (Har?el and Kenigsberg, 2004) wordlistas a lexeme-based lexicon for pruning segmenta-tions involving invalid segments.
Models that em-ploy this strategy are denoted hsp.
To control forthe effect of the HSPELL-based pruning, we also ex-perimented with a morphological analyzer that doesnot perform this pruning.
For these models we limitthe options provided for OOV words by not consid-ering the entire token as a valid segmentation in caseat least some prefix segmentation exists.
This ana-lyzer setting is similar to that of (Cohen and Smith,2007), and models using it are denoted nohsp,Parser and Grammar We used BitPar (Schmid,2004), an efficient general purpose parser,10 togetherwith various treebank grammars to parse the in-put sentences and propose compatible morpholog-ical segmentation and syntactic analysis.We experimented with increasingly rich gram-mars read off of the treebank.
Our first model isGTplain, a PCFG learned from the treebank afterremoving all functional features from the syntacticcategories.
In our second model GTvpi we alsodistinguished finite and non-finite verbs and VPs as8Mapping between the two schemes involves non-deterministic many-to-many mappings, and in some cases re-quire a change in the syntactic trees.9An open-source Hebrew spell-checker.10Lattice parsing can be performed by special initializationof the chart in a CKY parser (Chappelier et al, 1999).
Wecurrently simulate this by crafting a WCFG and feeding it toBitPar.
Given a PCFG grammar G and a lattice L with nodesn1 .
.
.
nk , we construct the weighted grammar GL as follows:for every arc (lexeme) l ?
L from node ni to node nj , we addto GL the rule [l ?
tni , tni+1 , .
.
.
, tnj?1 ] with a probability of1 (this indicates the lexeme l spans from node ni to node nj).GL is then used to parse the string tn1 .
.
.
tnk?1 , where tni isa terminal corresponding to the lattice span between node niand ni+1.
Removing the leaves from the resulting tree yields aparse for L under G, with the desired probabilities.
We use apatched version of BitPar allowing for direct input of probabili-ties instead of counts.
We thank Felix Hageloh (Hageloh, 2006)for providing us with this version.376proposed in (Tsarfaty, 2006).
In our third modelGTppp we also add the distinction between gen-eral PPs and possessive PPs following Goldberg andElhadad (2007).
In our forth model GTnph weadd the definiteness status of constituents follow-ing Tsarfaty and Sima?an (2007).
Finally, modelGTv = 2 includes parent annotation on top of thevarious state-splits, as is done also in (Tsarfaty andSima?an, 2007; Cohen and Smith, 2007).
For allgrammars, we use fine-grained PoS tags indicatingvarious morphological features annotated therein.Evaluation We use 8 different measures to eval-uate the performance of our system on the joint dis-ambiguation task.
To evaluate the performance onthe segmentation task, we report SEG, the stan-dard harmonic means for segmentation Precisionand Recall F1 (as defined in Bar-Haim et al (2005);Tsarfaty (2006)) as well as the segmentation ac-curacy SEGTok measure indicating the percentageof input tokens assigned the correct exact segmen-tation (as reported by Cohen and Smith (2007)).SEGTok(noH) is the segmentation accuracy ignor-ing mistakes involving the implicit definite articleh.11 To evaluate our performance on the taggingtask we report CPOS and FPOS correspondingto coarse- and fine-grained PoS tagging results (F1)measure.
Evaluating parsing results in our jointframework, as argued by Tsarfaty (2006), is not triv-ial under the joint disambiguation task, as the hy-pothesized yield need not coincide with the correctone.
Our parsing performance measures (SY N )thus report the PARSEVAL extension proposed inTsarfaty (2006).
We further report SY NCS , theparsing metric of Cohen and Smith (2007), to fa-cilitate the comparison.
We report the F1 value ofboth measures.
Finally, our U (unparsed) measureis used to report the number of sentences to whichour system could not propose a joint analysis.7 Results and AnalysisThe accuracy results for segmentation, tagging andparsing using our different models and our standarddata split are summarized in Table 1.
In additionwe report for each model its performance on gold-segmented input (GS) to indicate the upper bound11Overt definiteness errors may be seen as a wrong featurerather than as wrong constituent and it is by now an acceptedstandard to report accuracy with and without such errors.for the grammars?
performance on the parsing task.The table makes clear that enriching our grammarimproves the syntactic performance as well as mor-phological disambiguation (segmentation and POStagging) accuracy.
This supports our main thesis thatdecisions taken by single, improved, grammar arebeneficial for both tasks.
When using the segmen-tation pruning (using HSPELL) for unseen tokens,performance improves for all tasks as well.
Yet wenote that the better grammars without pruning out-perform the poorer grammars using this technique,indicating that the syntactic context aids, to someextent, the disambiguation of unknown tokens.Table 2 compares the performance of our systemon the setup of Cohen and Smith (2007) to the bestresults reported by them for the same tasks.Model SEGTok CPOS FPOS SY NCSGTnohsp/pln 89.50 81.00 77.65 62.22GTnohsp/??
?+nph 89.58 81.26 77.82 64.30CSpln 91.10 80.40 75.60 64.00CSv=2 90.90 80.50 75.40 64.40GThsp/pln 93.13 83.12 79.12 64.46GTnohsp/??
?+v=2 89.66 82.85 78.92 66.31Oracle CSpln 91.80 83.20 79.10 66.50Oracle CSv=2 91.70 83.00 78.70 67.40GThsp/??
?+v=2 93.38 85.08 80.11 69.11Table 2: Segmentation, Parsing and Tagging Results us-ing the Setup of (Cohen and Smith, 2007) (sentencelength ?
40).
The Models?
are Ordered by Performance.We first note that the accuracy results of oursystem are overall higher on their setup, on allmeasures, indicating that theirs may be an easierdataset.
Secondly, for all our models we providebetter fine- and coarse-grained POS-tagging accu-racy, and all pruned models outperform the Ora-cle results reported by them.12 In terms of syn-tactic disambiguation, even the simplest grammarpruned with HSPELL outperforms their non-Oracleresults.
Without HSPELL-pruning, our simplergrammars are somewhat lagging behind, but as thegrammars improve the gap is bridged.
The addi-tion of vertical markovization enables non-prunedmodels to outperform all previously reported re-12Cohen and Smith (2007) make use of a parameter (?
)which is tuned separately for each of the tasks.
This essentiallymeans that their model does not result in a true joint inference,as executions for different tasks involve tuning a parameter sep-arately.
In our model there are no such hyper-parameters, andthe performance is the result of truly joint disambiguation.377Model U SEGTok / no H SEGF CPOS FPOS SY N / SY NCS GS SY NGTnohsp/pln 7 89.77 / 93.18 91.80 80.36 76.77 60.41 / 61.66 65.00??
?+vpi 7 89.80 / 93.18 91.84 80.37 76.74 61.16 / 62.41 66.70??
?+ppp 7 89.79 / 93.20 91.86 80.43 76.79 61.47 / 62.86 67.22??
?+nph 7 89.78 / 93.20 91.86 80.43 76.87 61.85 / 63.06 68.23??
?+v=2 9 89.12 / 92.45 91.77 82.02 77.86 64.53 / 66.02 70.82GThsp/pln 11 92.00 / 94.81 94.52 82.35 78.11 62.10 / 64.17 65.00??
?+vpi 11 92.03 / 94.82 94.58 82.39 78.23 63.00 / 65.06 66.70??
?+ppp 11 92.02 / 94.85 94.58 82.48 78.33 63.26 / 65.42 67.22??
?+nph 11 92.14 / 94.91 94.73 82.58 78.47 63.98 / 65.98 68.23??
?+v=2 13 91.42 / 94.10 94.67 84.23 79.25 66.60 / 68.79 70.82Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentencessults.
Furthermore, the combination of pruning andvertical markovization of the grammar outperformsthe Oracle results reported by Cohen and Smith.This essentially means that a better grammar tunesthe joint model for optimized syntactic disambigua-tion at least in as much as their hyper parametersdo.
An interesting observation is that while verticalmarkovization benefits all our models, its effect isless evident in Cohen and Smith.On the surface, our model may seem as a specialcase of Cohen and Smith in which ?
= 0.
How-ever, there is a crucial difference: the morphologicalprobabilities in their model come from discrimina-tive models based on linear context.
Many morpho-logical decisions are based on long distance depen-dencies, and when the global syntactic evidence dis-agrees with evidence based on local linear context,the two models compete with one another, despitethe fact that the PCFG takes also local context intoaccount.
In addition, as the CRF and PCFG look atsimilar sorts of information from within two inher-ently different models, they are far from independentand optimizing their product is meaningless.
Cohenand Smith approach this by introducing the ?
hy-perparameter, which performs best when optimizedindependently for each sentence (cf.
Oracle results).In contrast, our morphological probabilities arebased on a unigram, lexeme-based model, and allother (local and non-local) contextual considerationsare delegated to the PCFG.
This fully generativemodel caters for real interaction between the syn-tactic and morphological levels as a part of a singlecoherent process.8 Discussion and ConclusionEmploying a PCFG-based generative framework tomake both syntactic and morphological disambigua-tion decisions is not only theoretically clean andlinguistically justified and but also probabilisticallyapropriate and empirically sound.
The overall per-formance of our joint framework demonstrates thata probability distribution obtained over mere syn-tactic contexts using a Treebank grammar and adata-driven lexicon outperforms upper bounds pro-posed by previous joint disambiguation systems andachieves segmentation and parsing results on a parwith state-of-the-art standalone applications results.Better grammars are shown here to improve per-formance on both morphological and syntactic tasks,providing support for the advantage of a joint frame-work over pipelined or factorized ones.
We conjec-ture that this trend may continue by incorporatingadditional information, e.g., three-dimensional mod-els as proposed by Tsarfaty and Sima?an (2007).
Inthe current work morphological analyses and lexi-cal probabilities are derived from a small Treebank,which is by no means the best way to go.
Usinga wide-coverage morphological analyzer based on(Itai et al, 2006) should cater for a better cover-age, and incorporating lexical probabilities learnedfrom a big (unannotated) corpus (cf.
(Levinger etal., 1995; Goldberg et al, ; Adler et al, 2008)) willmake the parser more robust and suitable for use inmore realistic scenarios.Acknowledgments We thank Meni Adler andMichael Elhadad (BGU) for helpful comments anddiscussion.
We further thank Khalil Simaan (ILLC-UvA) for his careful advise concerning the formaldetails of the proposal.
The work of the first au-thor was supported by the Lynn and William FrankelCenter for Computer Sciences.
The work of the sec-ond author as well as collaboration visits to Israelwas financed by NWO, grant number 017.001.271.378ReferencesMeni Adler and Michael Elhadad.
2006.
An Unsuper-vised Morpheme-Based HMM for Hebrew Morpho-logical Disambiguation.
In Proceeding of COLING-ACL-06, Sydney, Australia.Meni Adler, Yoav Goldberg, David Gabay, and MichaelElhadad.
2008.
Unsupervised Lexicon-Based Reso-lution of Unknown Words for Full MorpholologicalAnalysis.
In Proceedings of ACL-08.Meni Adler.
2001.
Hidden Markov Model for HebrewPart-of-Speech Tagging.
Master?s thesis, Ben-GurionUniversity of the Negev.Meni Adler.
2007.
Hebrew Morphological Disambigua-tion: An Unsupervised Stochastic Word-based Ap-proach.
Ph.D. thesis, Ben-Gurion University of theNegev, Beer-Sheva, Israel.Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.
2005.Choosing an optimal architecture for segmentation andpos- tagging of modern Hebrew.
In Proceedings ofACL-05 Workshop on Computational Approaches toSemitic Languages.Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.
2007.Part-of-speech tagging of Modern Hebrew text.
Natu-ral Language Engineering, 14(02):223?251.J.
Chappelier, M. Rajman, R. Aragues, and A. Rozen-knop.
1999.
Lattice Parsing for Speech Recognition.Eugene Charniak, Glenn Carroll, John Adcock, An-thony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz,Michael L. Littman, and John McCann.
1996.
Tag-gers for Parsers.
AI, 85(1-2):45?57.David Chiang, Mona Diab, Nizar Habash, Owen Ram-bow, and Safiullah Shareef.
2006.
Parsing Arabic Di-alects.
In Proceedings of EACL-06.Shay B. Cohen and Noah A. Smith.
2007.
Joint morpho-logical and syntactic disambiguation.
In Proceedingsof EMNLP-CoNLL-07, pages 208?217.Lewis Glinert.
1989.
The Grammar of Modern Hebrew.Cambridge University Press.Yoav Goldberg and Michael Elhadad.
2007.
SVM ModelTampering and Anchored Learning: A Case Studyin Hebrew NP Chunking.
In Proceeding of ACL-07,Prague, Czech Republic.Yoav Goldberg, Meni Adler, and Michael Elhadad.
EMCan Find Pretty G]ood HMM POS-Taggers (WhenGiven a Good Start), booktitle = Proceedings of ACL-08, year = 2008,.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceeding ofACL-05.Felix Hageloh.
2006.
Parsing Using Transforms overTreebanks.
Master?s thesis, University of Amsterdam.Nadav Har?el and Dan Kenigsberg.
2004.
HSpell - thefree Hebrew Spell Checker and Morphological Ana-lyzer.
Israeli Seminar on Computational Linguistics.Alon Itai, Shuly Wintner, and Shlomo Yona.
2006.
AComputational Lexicon of Contemporary Hebrew.
InProceedings of LREC-06.Moshe Levinger, Uzi Ornan, and Alon Itai.
1995.
Learn-ing Morpholexical Probabilities from an UntaggedCorpus with an Application to Hebrew.
Computa-tional Linguistics, 21:383?404.Helmut Schmid, 2000.
LoPar: Design and Implementa-tion.
Institute for Computational Linguistics, Univer-sity of Stuttgart.Helmut Schmid.
2004.
Efficient Parsing of Highly Am-biguous Context-Free Grammars with Bit Vector.
InProceedings of COLING-04.Erel Segal.
2000.
Hebrew Morphological Analyzer forHebrew Undotted Texts.
Master?s thesis, Technion,Haifa, Israel.Danny Shacham and Shuly Wintner.
2007.
Morpho-logical Disambiguation of Hebrew: A Case Study inClassifier Combination.
In Proceedings of EMNLP-CoNLL-07, pages 439?447.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank forModern Hebrew Text.
In Traitement Automatique desLangues, volume 42.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proceedings of HLT-05, pages475?482, Morristown, NJ, USA.
Association for Com-putational Linguistics.Reut Tsarfaty and Yoav Goldberg.
2008.
Word-Based orMorpheme-Based?
Annotation Strategies for ModernHebrew Clitics.
In Proceedings of LREC-08.Reut Tsarfaty and Khalil Sima?an.
2004.
An IntegratedModel for Morphological and Syntactic Disambigua-tion in Modern Hebrew.
MOZAIEK detailed proposal,NWO Mozaiek scheme.Reut Tsarfaty and Khalil Sima?an.
2007.
Three-Dimensional Parametrization for Parsing Morphologi-cally Rich Languages.
In Proceedings of IWPT-07.Reut Tsarfaty.
2006.
Integrated Morphological and Syn-tactic Disambiguation for Modern Hebrew.
In Pro-ceedings of ACL-SRW-06.Shlomo Yona and Shuly Wintner.
2005.
A Finite-state Morphological Grammar of Hebrew.
In Proceed-ings of the ACL-05 Workshop on Computational Ap-proaches to Semitic Languages.379
