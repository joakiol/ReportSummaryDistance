A CONTEXT VECTOR-BASED SELF ORGANIZING MAPFOR INFORMATION VISUALIZATIONDavid  A. Rushall, Marc  R. I lgenHNC Software, Inc., 5930 Cornerstone Court West, San Diego, CA 92121 USAemail: dar@hnc.com, mri@hne.eomABSTRACTHNC Software, Inc. has developed a systemcalled DOCUVERSE for visualizing the informationcontent of large textual corpora.
The system is builtaround two separate neural network methodologies:context vectors and self organizing maps.
Contextvectors (CVs) are high dimensional informationrepresentations that encode the semantic ontent ofthe textual entities they represent.
Self organizingmaps (SOMs) are capable of transforming an input,high dimensional signal space into a much lower(usually two or three) dimensional output space usefulfor visualization.
Related information themescontained in the corpus, depicted graphically, arepresented inspatial proximity to one another.
Neitherprocess requires human intervention, nor an externalknowledge base.
Together, these neural networktechniques can be utilized to automatically identi~ therelevant information themes present in a corpus, andpresent hose themes to the user in a intuitive visualform.1.
INTRODUCTIONIn recent years there has been an explosion inthe amount of information available on-line.
Much ofthis explosion has been fueled by the spectaculargrowth of the Internet and especially the World WideWeb.
Along with this spectacular growth has comenew challenges for effectively locating on-lineinformation, especially when browsing rather thanperforming a directed search for a specific piece ofinformation.
Key word and linguistically baseddirected search engines offer some ability to presentrelevant information to the user, and have resulted inuseful Interact products uch as Yahoo \[1\], Lycos \[2\],and Alta Vista\[3\].
However, these ngines uffer fi'omthe fact that hey require the user to specify a query oflimited length and they offer no visual interface forbrowsing.
To solve the problem of browsing theinformation space in order to fred information ofinterest, new techniques for data retrieval andpresentation must be developed.HNC has developed an underlyinginformation representation technology and a conceptfor information visualization that can solve theproblem of effectively browsing large textual corpora.As part of HNC's involvement inthe ARPA sponsoredTIPSTER program, HNC has developed a neuralnetwork technique that can learn word levelrelationships from free text.
This capability is basedupon an approach called context vectors whichencodes the meaning and context of words anddocuments in the form of unit vectors in a highdimensional vector space.
Furthermore, as part ofHNC's involvement inthe US intelligence community-sponsored P1000 visualization effort, HNC hasapplied a secondary neural network process, the SelfOrganizing Map (SOM) \[4\], which uses the documentcontext vectors to build a visual representation f theinformation content of the corpus.
The combination ofthese technologies allows users to effectively browsethe information space, to locate related documents,and to discover relationships between different themesin the information space.The remainder of this paper is organized asfollows.
Section 2 presents an overview of bothcontext vectors and the Self Organizing Map.
Section3 presents the DOCUVERSE system and presents theuser interface, automatic region fmding and regionlabeling, information retrieval and documenthighlighting, and temporal analysis of the informationspace.
Finally, Section 4 presents ome concludingremarks and directions for future research.2.
TECHNICAL BACKGROUNDThe DOCUVERSE system is based on twotechnologies: context vectors and the SOM.
Contextvector technology was developed at HNC and hasbeen demonstrated to be highly effective for such tasksas text retrieval (using a system called MatchPlus) \[5\],159text routing \[6\], and image retrieval \[7\].
SOMtechnology was originally developed by T. Kohonenand has been used throughout the neural networkcommunity as a method for representing informationin a manner suitable for visualization \[8\].
Thefollowing subsections present an overview of each ofthese technologies.2.1 Context VectorsThe key technical feature of context vectortechnology is the representation f terms, documents,and queries by high dimensional vectors consisting ofreal-valued numbers or components.
These vectorsare constrained to be unit vectors in the highdimensional vector space.
Since both terms (words orstems) and documents are represented in the sameflame of reference, this allows several uniqueoperations.
All operations in MatchPlus are based ongeometry of these high dimensional spaces \[9\].Specifically, closeness in the space is equivalent tocloseness in subject content.
A neural network-basedlearning algorithm is designed to adjust word vectorssuch that terms that are used in a similar context willhave vectors that point in similar directions.
Thedetermination f similar context is based upon the useof word stem co-occurrence statistics.
Once trained,these word stem context vectors are used as buildingblocks for creating document and query contextvectors.
Specifically, context vectors for documentsand queries are formed as the normalized weightedsum of the word stem context vectors for the wordstems found in the document or query.
This processresults in the fact that context vectors for documentswith similar subject content will point in similardirections.
This vector representation f informationcontent can thus be used for document retrieval,routing, document clustering (self organizing subjectindex) and other text processing.2.2 Kohonen's Self Organizing MapThe concept of self organizing maps was firstdeveloped by Tuevo Kohonen in 1981 at theUniversity of Helsinki.
Kohonen demonstrated that asystem could be taught to organize the data it wasgiven, without the need for supervision or externalintervention, through the use of competitive l arning.Typically, the SOM consists of a collection of nodesarranged in a regular two dimensional grid.
Each nodecorresponds to a cluster centroid vector for the highdimensional input vector space.
A self-organizingtraining process is used to adjust the node vectorcomponents in an iterative fashion.
Upon completionof training, the SOM node vectors have the propertythat node vectors that are close in the high dimensionalvector space will be close in the two dimensional gridspace, or "map space."
This training process isdescribed in more detail below.2.2.1 Training the SOMAssume the input space is comprised of Nvectors, each of dimension .
Furthermore, assume atwo dimensional rray of "nodes" that will be trainedto represent the N input vectors.
Each of these nodesis also a vector of dimension .
Figure 1 depicts thisarbitrary array of nodes, in this case, a 5-by-5regularly spaced array of 25 nodes.
Each node isuniquely identified by it's (i,j) position.?@@@@@@@@@@@@@@@@@@?
@@@@@Figure 1.
An array of map nodes for selforganization.Before training, the node vectors are assignedrandom values.
That is, a pseudo random numbergenerator is used to assign each node in the map arandom unit vector of dimension .
In the case wheren is very large (-300), these initial conditionsrepresent a quasi-orthogonal state, i.e., each unitvector is approximately orthogonal to each other unitvector.The SOM training algorithm is based on asimple, iterative comparison process, where the Ninput vectors are compared to each of the node vectorsin the map.
In essence, the node vectors arecompeting to have their values adjusted.
The idea isto find the node vector that is "nearest" (in vectorspace) to the input vector.
The node vector that isnearest is deemed the "winner" of this competition,160and is rewarded by having its vector adjusted.
Theadjustment comes in the form of moving the winningnode vector in the direction of the input vector.
TheSOM extends this simple competitive l arning processto include updates for the "neighbor" nodes to thewinning node.
These neighbor nodes are nodes that areclose (in the map space) to the winning node.
Forexample, in Figure 1, the neighborhood fnode We.3 isdepicted, and is defined as those nodes that are withinone row or one column of node W2,3.
Neighborhoodscan be larger or smaller; this is just one example.
Theupdates for these neighbor nodes are smaller than theupdates for the winning node, and the size of theneighbor node update is smaller for neighbors that arefarther away (in map space) ffi'om the winning node.The inclusion of neighbor updates results in theorganization ofthe information i to a form suitable forvisualization.
The algorithm can be represented inpseudo-code as:For each input vector VFind node vector Wid that is closest to V( IW~j -V l< lW, ,~-V l  Vk, l)Update Widaccording to W 0 = Wi, i + of V- WO)Find nodes that are close to node i j  in map spaceFor each of these "neighbor" nodes It~,tUpdate W t, t according to Wk, t= W ~t + s of V- W t, t )where (0.0 ~s ~ LO)End loop over neighbor nodesEnd loop over input vectorsThe size of the adjustment, a will determinehow quickly the map space node vectors will convergeto an accurate representation of the input spacevectors.
One loop through the input vector set is notsufficient to train the node vectors.
It is necessary toperform this loop hundreds, and possibly thousands, oftimes before training is completed.
The value of theparameter s for updating neighbor nodes is determinedby a Gaussian function based on the nearness of theneighbor node to the winning node.
Therefore, closeneighbors will be updated, or adjusted, more thanneighbors that are further away.2.2.2 Win Frequency and ConscienceAn ideal characteristic of this training is tohave the map node vectors win the competition withequal probabilities.
Unfortunately, this is not the casefor the standard SOM algorithm.
A consequence ofthis type of training is that some map nodes may neverwin the competition.
This will result in a less usefulrepresentation f the input space.
To eliminate thisundesirable ffect, DeSieno \[10\] has developed animproved competitive learning algorithm that makesuse of the idea of "conscience".
The consciencemechanism allows nodes that are observed to rarelywin the competition to subsequently win more often,and it prevents nodes that ffi'equently win thecompetition ffi'om subsequently winning too oRen.The conscience mechanism is employed as asecond competition based on the outcome of the firstcompetition described above in the self organizingalgorithm.
Before conducting the second competition,the conscience mechanism creates a bias factor foreach node.
The value of the bias is determined by therunning statistics kept on the first competition.
Nodesthat normally lose the first competition are givenfavorable biases, and those that normally win are givenunfavorable biases.
The winner of this secondcompetition is determined upon the basis of biaseddistance to the input vector.
This biasing enforces anequiprobable winning distribution and results in amore useful clustering of the input information space.2.2.3 Computation IssuesEarlier we alluded to the fact that thesealgorithms are computationally intensive.
Thisintensity depends upon three factors.
One is thedimensionality of the vectors.
Using higherdimensioned vectors (-1000) is possible, but adds tothe timely computation problem.
Likewise, thenumber of input vectors, as well as the number of mapnode vectors, will determine the scale of the problem.Fortunately, the nature of these algorithms is wellsuited for parallel processing architectures.
Therefore,scalability of the algorithm depends on the number ofprocessors that can be used to compute a solution.HNC has developed a hardware architecturethat is designed to handle neural networks, and inparticular, the compute intensive processes theymodel.
The hardware is a SIMD numerical arrayprocessor (SNAP), in essence a floating-point parallelarray processor.
The SNAP is ideally suited todetermine the computationally intensive solutionrequired by the SOM algorithms.The SNAP comes in a variety ofconfigurations.
The fastest SNAP available, theSNAP-64, has 64 processors, and delivers anunmatched price-to-performance ratio of around $20per megaflop.
At its peak, the SNAP computes at arate of 2.56 gigaflops.
This type of performanceenables HNC to develop and deliver the computeintensive solutions to a wide variety of problems,including information visualization.1613.
DOCUVERSEThe DOCUVERSE system, developed as partof the IC P1000 research effort at HNC, is an ongoingresearch and development effort with a goal ofproviding users with a tool to quickly and easily assessthe information content of large textual corpora.DOCUVERSE is based on the context vectortechnology foundation developed at HNC over thepast few years, and additionally provides a visualinterface that allows the user to browse theinformation space in a visually appealing fashion.Figure 2 presents the process by which a textcorpus is transformed into some intuitive visualparadigm that users can easily relate to andunderstand.
The initial neural network process isshown along the top part of Figure 2.
The processflow indicates that some set of textual data, thetraining text, is used to obtain context vectors for thevocabulary set contained in the training text.
Thisprocess involves a preliminary step of word"stemming" and stop list removal.
Stemming is theprocess of representing similar word forms as the baseform of the words (i.e.
words like driver, driving,drives, driven, and drove are all stemmed to the worddrive).
Stop list removal refers to the removal ofwords with high frequency occurrence and littlemeaning in the training text (i.e.
words like the, of,and, etc.).
Afler preprocessing, context vectors for theremaining word stems are learned and stored into adatabase.~ ~  Preprocecsing?
Stems?
Stop List~ ?
Stems ?
Stop ListLesm Stem \] 5~m \]ContextVectorsContext VectorsVisual PmdigmOvrnlTln ~~ MapTrainingFigure 2.
The process of transforming textual datainto an intuitive, graphical visual.At this point, the system is ready to processthe user's desired corpus: Note that the corpus to bevisualized' oes not need to be the same corpus that thesystem was trained with.
It is help~l, however, if thetraining corpus is statistically representative of thecorpora that will be visualized.As shown in Figure 2, the control flow nowswitches to the middle part of the diagram, where theuser identifies the corpus to be visualized.
This text ispreprocessed in the same exact manner as the trainingtext was preprocessed.
Afler this step, one passthrough each document is all that is required tocalculate a context vector for each document.
Thestem vectors learned from the training text are used tocompute the context vectors for the user's text.
Theseare stored in a document context vector database.
It isthese context vectors that are given to the selforganizing map for visualization.2.3 The InterfaceThe DOCUVERSE interface presents theuser with an array of nodes not unlike the array ofnodes depicted in Figure 1.
The size of the array isconfigurable by the user, but the default is a 20-by-20array of nodes.
Recall that each of these 400 nodeshas a context vector associated with it, and that thecontext vectors have been adjusted to represent theprevalent themes in the corpus.
Therefore ach noderepresents an information theme contained in thecorpus.
It is important to note that it is not necessarythat each node have a different heme.
Nodes canhave similar themes, and in fact, the same theme ifthere is a relatively large amount of informationpertaining to that particular theme within the corpus.This discussion raises the question as to howthe nodes reflect he amount of information present forthe theme they represent.
Afler experimenting withvarious paradigms such as color or icons, we'veconcluded that the size, or radius, of the nodes bestconveys this information.
Large nodes imply arelatively large number of documents for the giveninformation theme.
Small nodes imply a relativelysmall number of documents for the given informationtheme.The way in which the system measures notonly the amount of information for a theme, but alsothe similarity of themes, documents, words, or any fleetext, is through the use of the vector dot productoperation.
Recall that each of the aforementionedtextual entities is associated with a context vector.Similar entities have context vectors that point insimilar directions.
The dot product for similardirection vectors will be close to 1.0, while dissimilarvectors will have dot products that are near zero.Figure 3 depicts what we call the "corpusintegral".
This is the broad view of information162content for the entire corpus.
It is an attempt ographically illustrate, through various sizedinformation odes, the entire set of prevalent themescontained in the corpus.
Again, nodes that are largerepresent he themes that occur with the highestfi'equency and volume in the corpus.
Nodes that aresmall, or not even visible (such as those in the upperleft comer of the map), represent themes that occurwith a much lower ffi'equency and volume, relativelyspeaking.?
?
?
- ?
?
= ?
o g O O - w  ??
= u ?
g ?
?
?
O m O O O O O g. .
.
.
O O O O 0 0 0 0 W O  w ?
?
?.
.
.
.
.
uoooooOoOOOOO="e=OOOOOOOOOO=-?
?
- eoOOOeoOO000o,?
t ~ t = m u ?
?
~ m O Q O  = 0 0 9 9  ??
?
o m w ?
= l e  e = O O O *  ?
= O g  ?- - O 0 0 0 0 -  w w O O O O !
= ?
?
?o O g O O 0  u g ?
w g I ?
!
g * = - ?
-?
o e e e e o .
; : : -  ?
-  ?
?
.
.
.
.OOOOOOOe .
.
?
?
?
.000000OOO-*oO0000ooooOOO000oo-*oOOOoOOOOThe corpus integral is very useful in that theuser knows, at a glance, which themes are present inthe corpus.
By "mousing" on a node (i.e.
clicking themouse button once on a node), a pop-up menu reveals,among other choices, the information theme the noderepresents.The corpus that was used to generate theintegral depicted in Figure 3 is a set of over 17,700documents.
The documents are news reports takendirectly off the AP News Wire during a four monthspan in 1990.Other system capabilities, discussed in thesections below, allow the user to do a variety ofinformation assimilation and information gatheringtasks.
For example, an undirected information search,commonly referred to as browsing, is made even easierwhen using the automatic region finding and labelingmechanism.
Searching for specific information is alsosupported so that the user can request, in flee textform, any desired information.
A tool for visualizinginformation i  the time domain is also provided.Figure 3.
The corpus integral.The corpus integral is computed by summingthe dot products of each document context vector witheach node context vector.
The summed ot productsfor each node are used to determine the size of thenode.
In pseudo-code,For each node vector It~jNode_sizeij = 0For each document vector VNode_size U += Wsj ?
VEnd loop over document vectorsNode size U/= number o f  document vectorsEnd loop over node vectors1632.4 Automatic Region Finding andLabelingAs we stated earlier, nodes that are near oneanother on the map have the property that theyrepresent similar themes of information.
We canexploit this fact to have the system automaticallygroup the nodes into regions of similar themes.
Thiscan be thought of as a clustering of the clusters.Furthermore, the system will automatically generate anappropriate name, or label for the region.
ConsiderFigures 4a and 4b.Figure 4b.
The automatically generated labels.Figure 4a.
The automatically generated regionsFigures 4a and 4b show one of the manyways that a user can make use of the automatic regiongeneration.
They depict the option of selecting allregions found on the map.
Figure 4a shows all theregions that were found by the system.
The regionsare outlines drawn around sets of nodes.
Figure 4bshows a second dialog window that is used to displaythe labels for the regions.
Although all the labels arenot visible in the dialog, the region algorithm found atotal of 84 distinct regions for the self organized mapof the 17,700 AP News Wire documents.Instead of showing all regions at once, a usercan select regions of interest from either the map orthe label dialog.
If a user is interested in a particulararea on the map, the region outlines for the nodes ofinterest can be toggled on or off by the pop-up menuprovided on each node.Alternatively, the user can peruse the list ofregion labels and select them directly from the list.This will draw the region outline on the map.The algorithm for region finding and labelingis a two step process.
The first step involves findingthe regions.
Initially, each node is given its ownregion.
An iterative algorithm compares a region toevery other region on the map.
The comparison is avector dot product operation.
If the regions are similarenough (i.e., if the dot product between the tworegions exceeds some threshold), the two regions aremerged into one region.
This process is repeated untilno region combining occurs.The next step involves finding an appropriatelabel for each region.
First, a context vector iscomputed for the region.
This is done by taking theweighted average of the context vectors belonging tothe nodes in the region.
This centroid region context164vector is compared to all of the stem word contextvectors in the vocabulary.
The stem word vector thatresults in the highest dot product with the centroidregion vector becomes the label for the region.Because of the stemming process, some of the stemwords are truncated.
Therefore, the user is given theability to edit the labels to put them in correctgrammatical form.2.5 Information Retrieval andDocument HighlightingDOCUVERSE makes use of a rich set ofinformation retrieval functionality.
This functionalitywas inherited from another system developed at HNCcalled MatchPlus \[5\].
MatchPlus focuses oninformation retrieval from large textual corpora.When a user desires a more focused searchfor specific information, it is easily accomplished in avariety of ways.
If a user has identified a map noderepresenting an interesting theme of information, theuser can select, from the node pop-up menu, an optionto retrieve documents pertaining to the theme.
Thesystem uses the node's context vector to perform dotproducts with every document context vector in thecorpus.
The user is presented with a ranked list of themost relevant documents.
The list is presented in awindow with the document ID, the value of the dotproduct, and the first line of text in the document.Figure 5 shows an example of the ranked list.DooumentSooroSu~eot~s2395 0.442 Christian Mi l i t ia  Re8ro, ~16507 0.442 Rival Christian Forces i l l16424 0.430 Battles RaBe 8etueen Ch\[I2755 0.429 Sniper Fire Persists In I I539 0.427 Roun Cells Up Reservist:\[\[11059 0.427 53 Bead.
133 Nounded tnll1893 0.427 MedIatln 8 Committee Stri\[I16721 0.425 F18htin 8 6etseen Chr l s t \ [ I827 0.424 Rlval Christian Forces I l l17302 0.424 Flghtin 8 in East 6e?rut \ [ \ [2412 0.424 Rlvsl Forces ExchanBe Si l l16259 0.423 Christian N l l l t l a  Clamp ill13665 0.423 Rival Christian Gunners ill4350 0.423 Roun's Forces Penetratei\[I10735 0.421 fit Least Two Nllled 8s ,!\[\[1940 0.421 Roun Gives Rival 72 Hou\[ll13598 0.420 Christian Forces Duel u i l l7069 0.417 Christians Rak Hraui to u1790 0.417 Helicopter Base Falls t , l~17409 0.417 Truce Holds In Mountaln,lD3401 0.417 Reneued Flghtln8 Thwart: I397 0.416 8oun's Forces Head ?ntoiB110 0.416 Aoun Forces Selze Strat.
1..................................................................................................................... i__ BFigure 5.
A ranked list of documents.Alternatively, rather than using a node for aquery, the user can type free text into a window andsubmit he free text as a query.
The system convertsthe free text into a context vector, and the sameretrieval process is performed.
Yet another possibilityis for the user to use an entire document as a query.Regardless of the method, the retrieval is done viacontext vector comparisons.Once a ranked list of documents has beenretrieved, the user can select any document from thelist to view.
The document selected will appear in aseparate window, along with a highlighting tool tofurther examine the relevant parts of the document.The highlight tool segments the documentinto 5-line paragraphs.
The user is presented withanother window containing a histogram, where eachinterval of the histogram corresponds to each of theparagraphs in the document, and the height of theinterval corresponds to the dot product of theparagraph with the query that was issued.
This toolprovides the user with a tool to quickly and easilylocate the most relevant portions of any document.2.6 Temporal AnalysisWith corpora comprised of periodicallyreleased information, it might be useful to visualize theinformation in the time domain.
The DOCUVERSEtemporal analysis tool makes this possible.Documents are segmented into user-defined timeintervals, typically one hour, one day, or one week,depending upon the nature of the data.
By performinga cumulative dot product operation for each documentin the interval with the corpus integral, we can obtain avisual summary of the information content of thedocuments received during that time interval.
Theresulting time series of information themes can beviewed in rapid succession.
The user is provided amedia-player type interface with buttons for "play',"stop", and stepping forward and backward.
Using thestep buttons, the user can manually step through eachtime increment, or alternatively, the play button willrapidly step through the time increments in succession.When using the temporal tool on the 17,700AP News Wire documents, weekly cycles are easilyidentified.
By stepping through the data in one dayincrements, the weekdays (Monday through Friday)are identified by three predominate r gions pertainingto themes like banks, stocks, world news, and taxation.The next two days, the weekend, show maps with twopredominate r gions, pertaining to themes like music,TV, movies, and various other entertainment themes.1654.
SUMMARYAs we continue through the information age,tools such as DOCUVERSE will no longer beconsidered luxuries, but rather necessities.
HNC hasdeveloped DOCUVERSE as merely a proof ofconcept system.
We feel that this technology is farfrom realizing its full potential.
As informationvisualization technology evolves and matures, so toowill tools like DOCUVERSE.HNC is in the process of exploring new waysto visualize this powerful information representationtechnology.
One area of interest is developing threedimensional SOMs.
The user will be presented with aspherical array of nodes, representing the "world" ofinformation.
Used in conjunction with flat maps, theuser would have a hierarchical SOM capablevisualizing information at various levels of resolution.It is clear that in terms of visualization, "onesize fits all" does not apply.
What is intuitivelyobvious to one user is unclear and convoluted toanother.
Realizing this, we have identified numerousbrowsing paradigms to appeal to a broader audience.Tools like the Virtual Reality Modeling Language(VRML) are well suited for use with this technologyfor visualizing information on the World Wide Web,and will aid us as we strive to improve informationvisualization.\[9\] Watson, G.S., "Statistics on Spheres", John Wileyand Sons, 1983.\[10\] DeSieno, D., "Adding a Conscience toCompetitive Learning", in Proceedings of theInternational Conference on Neural Networks, I IEEEPress, NY, 1988.REFERENCES\[1\] Yahoo!, http://www.yahoo.com/\[2\] Lycos, http://www.lycos.com/\[3\] Alta Vista, http://altavista.digital.corrg\[4\] Kohonen, T., Self-Organizing Maps, Springer-Verlag, Berlin, 1995.\[5\] Gallant, S.I., W. R. Caid, et al "Feedback andMixing Experiments with MatchPlus", ProceedingsTREC-2 Conference, D. Harman, Ed, Gaithersburg,MD.
Aug. 1993.\[6\] Sasseen, R. V., J. L. Carleton, W. R. Caid,"CONVECTIS: A Context Vector-Based On-LineIndexing System", in Proceedings IEEE Dual-UseConference, 1995.\[7\] Pu, K. Q., C. Z. Ren, "Image/Text AutomaticIndexing and Retrieval System Using Context VectorApproach," SPIE Volume 2606, 1995.\[8\] Kohonen, et.
al., http://websom.hut.fffwebsom/166
