BioNLP 2007: Biological, translational, and clinical language processing, pages 9?16,Prague, June 2007. c?2007 Association for Computational LinguisticsDetermining the Syntactic Structure of Medical Terms in Clinical NotesBridget T. McInnesDept.
of Computer Scienceand EngineeringUniversity of MinnesotaMinneapolis, MN, 55455bthomson@cs.umn.eduTed PedersenDept.
of Computer ScienceUniversity of Minnesota DuluthDuluth, MN, 55812tpederse@d.umn.eduSerguei V. PakhomovDept.
of Pharmaceutical Careand Health Systems Centerfor Health InformaticsUniversity of MinnesotaMinneapolis, MN, 55455pakh0002@umn.eduAbstractThis paper demonstrates a method for de-termining the syntactic structure of medi-cal terms.
We use a model-fitting methodbased on the Log Likelihood Ratio to clas-sify three-word medical terms as right orleft-branching.
We validate this method bycomputing the agreement between the clas-sification produced by the method and man-ually annotated classifications.
The resultsshow an agreement of 75% - 83%.
Thismethod may be used effectively to enablea wide range of applications that dependon the semantic interpretation of medicalterms including automatic mapping of termsto standardized vocabularies and inductionof terminologies from unstructured medicaltext.1 IntroductionMost medical concepts are expressed via a domainspecific terminology that can either be explicitlyagreed upon or extracted empirically from domainspecific text.
Regardless of how it is constructed,a terminology serves as a foundation for informa-tion encoding, processing and exchange in a special-ized sub-language such as medicine.
Concepts in themedical domain are encoded through a variety of lin-guistic forms, the most typical and widely acceptedis the noun phrase (NP).
In some even further spe-cialized subdomains within medicine, such as nurs-ing and surgery, an argument can be made that someconcepts are represented by an entire predicationrather than encapsulated within a single nominal-ized expression.
For example, in order to describesomeone?s ability to lift objects 5 pounds or heav-ier above their head, it may be necessary to use aterm consisting of a predicate such as [LIFT] and aset of arguments corresponding to various thematicroles such as <PATIENT> and <PATH> (Ruggieriet al, 2004).
In this paper, we address typical med-ical terms encoded as noun phrases (NPs) that areoften structurally ambiguous, as in Example 1, anddiscuss a case for extending the proposed method tonon-nominalized terms as well.small1 bowel2 obstruction3 (1)The NP in Example 1 can have at least two interpre-tations depending on the syntactic analysis:[[small1 bowel2] obstruction3] (2)[small1 [bowel2 obstruction3]] (3)The term in Example 2 denotes an obstruction inthe small bowel, which is a diagnosable disorder;whereas, the term in Example 3 refers to a small un-specified obstruction in the bowel.Unlike the truly ambiguous general English casessuch as the classical ?American History Professor?where the appropriate interpretation depends on thecontext, medical terms, such as in Example 1, tendto have only one appropriate interpretation.
Thecontext, in this case, is the discourse domain ofmedicine.
From the standpoint of the English lan-guage, the interpretation that follows from Example3 is certainly plausible, but unlikely in the contextof a medical term.
The syntax of a term only shows9what interpretations are possible without restrictingthem to any particular one.
From the syntactic anal-ysis, we know that the term in Example 1 has the po-tential for being ambiguous; however, we also knowthat it does have an intended interpretation by virtueof being an entry term in a standardized terminologywith a unique identifier anchoring its meaning.
Whatwe do not know is which syntactic structure gen-erated that interpretation.
Being able to determinethe structure consistent with the intended interpreta-tion of a clinical term can improve the analysis ofunrestricted medical text and subsequently improvethe accuracy of Natural Language Processing (NLP)tasks that depend on semantic interpretation.To address this problem, we propose to use amodel-fitting method which utilizes an existing sta-tistical measure, the Log Likelihood Ratio.
We val-idate the application of this method on a corpusof manually annotated noun-phrase-based medicalterms.
First, we present previous work on structuralambiguity resolution.
Second, we describe the LogLikelihood Ratio and then its application to deter-mining the structure of medical terms.
Third, wedescribe the training corpus and discuss the compi-lation of a test set of medical terms and human ex-pert annotation of those terms.
Last, we present theresults of a preliminary validation of the method anddiscuss several possible future directions.2 Previous WorkThe problem of resolving structural ambiguity hasbeen previously addressed in the computational lin-guistics literature.
There are multiple approachesranging from purely statistical (Ratnaparkhi, 1998),to hybrid approaches that take into account the lexi-cal semantics of the verb (Hindle and Rooth, 1993),to corpus-based, which is the approach discussedin this paper.
(Marcus, 1980) presents an early ex-ample of a corpus-based approach to syntactic am-biguity resolution.
One type of structural ambigu-ity that has received much attention has to do withnominal compounds as seen in the work of (Resnik,1993), (Resnik and Hearst, 1993), (Pustejovsky etal., 1993), and (Lauer, 1995).
(Lauer, 1995) points out that the existing ap-proaches to resolving the ambiguity of noun phrasesfall roughly into two camps: adjacency and de-pendency.
The proponents of the adjacency model((Liberman and Sproat, 1992), (Resnik, 1993) and(Pustejovsky et al, 1993)) argue that, given a threeword noun phrase XYZ, there are two possible an-alyzes [[XY]Z] and [X[YZ]].
The correct analysisis chosen based on the ?acceptability?
of the adja-cent bigrams A[XY] and A[YZ].
If A[XY] is moreacceptable than A[YZ], then the left-branching anal-ysis [[XY]Z] is preferred.
(Lauer and Dras, 1994) and (Lauer, 1995) addressthe issue of structural ambiguity by developing a de-pendency model where instead of computing the ac-ceptability of A[YZ] one would compute the accept-ability of A[XZ].
(Lauer, 1995) argues that the de-pendency model is not only more intuitive than theadjacency model, but also yields better results.
(La-pata and Keller, 2004) results also support this as-sertion.The difference between the approaches within thetwo models is the computation of acceptability.
Pro-posals for computing acceptability (or preference)include raw frequency counts ((Evans and Zhai,1996) and (Lapata and Keller, 2004)), Latent Se-mantic Indexing ((Buckeridge and Sutcliffe, 2002))and statistical measures of association ((Lapata etal., 1999) and (Nakov and Hearst, 2005)).One of the main problems with using frequencycounts or statistical methods for structural ambigu-ity resolution is the sparseness of data; however,(Resnik and Hearst, 1993) used conceptual associa-tions (associations between groups of terms deemedto form conceptual units) in order to alleviate thisproblem.
(Lapata and Keller, 2004) use the doc-ument counts returned by WWW search engines.
(Nakov and Hearst, 2005) use the ?2 measure basedon statistics obtained from WWW search engines tocompute values to determine acceptability of a syn-tactic analysis for nominal compounds.
This methodis tested using a set of general English nominal com-pounds developed by (Lauer, 1995) as well as a setof nominal compounds extracted from MEDLINEabstracts.The novel contribution of our study is in demon-strating and validating a corpus-based method fordetermining the syntactic structure of medical termsthat relies on using the statistical measure of asso-ciation, the Log Likelihood Ratio, described in thefollowing section.103 Log Likelihood RatioThe Log Likelihood Ratio (G2) is a ?goodness offit?
statistic first proposed by (Wilks, 1938) to test ifa given piece of data is a sample from a set of datawith a specific distribution described by a hypothe-sized model.
It was later applied by (Dunning, 1993)as a way to determine if a sequence of N words (N-gram) came from an independently distributed sam-ple.
(Pedersen et al, 1996) pointed out that there ex-ists theoretical assumptions underlying the G2 mea-sure that were being violated therefore making themunreliable for significance testing.
(Moore, 2004)provided additional evidence that although G2 maynot be useful for determining the significance of anevent, its near equivalence to mutual informationmakes it an appropriate measure of word associa-tion.
(McInnes, 2004) applied G2 to the task of ex-tracting three and four word collocations from rawtext.G2, formally defined for trigrams in Equation 4,compares the observed frequency counts with thecounts that would be expected if the words in thetrigram (3-gram; a sequence of three words) corre-sponded to the hypothesized model.G2 = 2 ?
?x,y,znxyz ?
log(nxyzmxyz) (4)The parameter nxyz is the observed frequency ofthe trigram where x, y, and z respectively representthe occurrence of the first, second and third wordsin the trigram.
The variable mxyz is the expectedfrequency of the trigram which is calculated basedon the hypothesized model.
This calculation variesdepending on the model used.
Often the hypothe-sized model used is the independence model whichassumes that the words in the trigram occur togetherby chance.
The calculation of the expected valuesbased on this model is as follows:mxyz = nx++ ?
n+y+ ?
n++z/n+++ (5)The parameter, n+++, is the total number of tri-grams that exist in the training data, and nx++,n+y+, and n++z are the individual marginal countsof seeing words x, y, and z in their respective posi-tions in a trigram.
A G2 score reflects the degree towhich the observed and expected values diverge.
AG2 score of zero implies that the observed values areequal to the expected and the trigram is representedperfectly by the hypothesized model.
Hence, wewould say that the data ?fits?
the model.
Therefore,the higher the G2 score, the less likely the wordsin the trigram are represented by the hypothesizedmodel.4 Methods4.1 Applying Log Likelihood to StructuralDisambiguationThe independence model is the only hypothesizedmodel used for bigrams (2-gram; a sequence oftwo words).
As the number of words in an N-gram grows, the number of hypothesized modelsalso grows.
The expected values for a trigram canbe based on four models.
The first model is theindependence model discussed above.
The secondis the model based on the probability that the firstword and the second word in the trigram are depen-dent and independent of the third word.
The thirdmodel is based on the probability that the secondand third words are dependent and independent ofthe first word.
The last model is based on the prob-ability that the first and third words are dependentand independent of the second word.
Table 1 showsthe different models for the trigram XYZ.Table 1: Models for the trigram XYZModel 1 P(XYZ) / P(X) P(Y) P(Z)Model 2 P(XYZ) / P(XY) P(Z)Model 3 P(XYZ) / P(X) / P(YZ)Model 4 P(XYZ) / P(XZ) P(Y)Slightly different formulas are used to calculatethe expected values for the different hypothesizedmodels.
The expected values for Model 1 (the in-dependence model) are given above in Equation 5.The calculation of expected values for Model 2, 3, 4are seen in Equations 6, 7, 8 respectively.mxyz = nxy+ ?
n++z/n+++ (6)mxyz = nx++ ?
n+yz/n+++ (7)mxyz = nx+z ?
n+y+/n+++ (8)The parameter nxy+ is the number of times wordsx and y occur in their respective positions, n+yz is11the number of times words y and z occur in theirrespective positions and nx+z is the number of timesthat words x and z occur in their respective positionsin the trigram.The hypothesized models result in different ex-pected values which results in a different G2 score.A G2 score of zero implies that the data are perfectlyrepresented by the hypothesized model and the ob-served values are equal to the expected.
Therefore,the model that returns the lowest score for a giventrigram is the model that best represents the struc-ture of that trigram, and hence, best ?fits?
the trigram.For example, Table 2 shows the scores returned foreach of the four hypothesized models for the trigram?small bowel obstruction?.Table 2: Example for the term ?small bowel obstruc-tion?Model G2 score Model G2 scoreModel 1 11,635.45 Model 2 5,169.81Model 3 8,532.90 Model 4 7,249.90The smallest G2 score is returned by Model 2which is based on the first and second words be-ing dependent and independent of the third.
Basedon the data, Model 2 best represents or ?fits?
the tri-gram, ?small bowel obstruction?.
In this particularcase that happens to be the correct analysis.The frequency counts and G2 scores for eachmodel were obtained using the N-gram StatisticsPackage 1 (Banerjee and Pedersen, 2003).4.2 DataThe data for this study was collected from twosources: the Mayo Clinic clinical notes andSNOMED-CT terminology (Stearns et al, 2001).4.2.1 Clinical NotesThe corpus used in this study consists of over100,000 clinical notes covering a variety of ma-jor medical specialties at the Mayo Clinic.
Thesenotes document each patient-physician contact andare typically dictated over the telephone.
They rangein length from a few lines to several pages of textand represent a quasi-spontaneous discourse wherethe dictations are made partly from notes and partly1http://www.d.umn.edu/ tpederse/nsp.htmlfrom memory.
At the Mayo Clinic, the dictationsare transcribed by trained personnel and are storedin the patient?s chart electronically.4.2.2 SNOMED-CTSNOMED-CT (Systematized Nomenclature ofMedicine, Clinical Terminology) is an ontologi-cal resource produced by the College of AmericanPathologists and distributed as part of the UnifiedMedical Language System2 (UMLS) Metathesaurusmaintained by the National Library of Medicine.SNOMED-CT is the single largest source of clini-cal terms in the UMLS and as such lends itself wellto the analysis of terms found in clinical reports.SNOMED-CT is used for many applications in-cluding indexing electronic medical records, ICUmonitoring, clinical decision support, clinical trials,computerized physician order entry, disease surveil-lance, image indexing and consumer health informa-tion services.
The version of SNOMED-CT used inthis study consists of more than 361,800 unique con-cepts with over 975,000 descriptions (entry terms)(SNOMED-CT Fact Sheet, 2004).4.3 Testset of Three Word TermsWe used SNOMED-CT to compile a list of termsin order to develop a test set to validate the G2method.
The test set was created by extracting alltrigrams from the corpus of clinical notes and allthree word terms found in SNOMED-CT.
The inter-section of the SNOMED-CT terms and the trigramsfound in the clinical notes was further restricted toinclude only simple noun phrases that consist of ahead noun modified with a set of other nominal oradjectival elements including adjectives and presentand past participles.
Adverbial modification of ad-jectives was also permitted (e.g.
?partially edentu-lous maxilla?).
Noun phrases with nested prepo-sitional phrases such as ?fear of flying?
as well asthree word terms that are not noun phrases such as?does not eat?
or ?unable to walk?
were excludedfrom the test set.
The resulting test set contains 710items.The intended interpretation of each three wordterm (trigram) was determined by arriving at a2Unified Medical Language System is a compendium ofover 130 controlled medical vocabularies encompassing overone million concepts.12consensus between two medical index experts(kappa=0.704).
These experts have over ten years ofexperience with classifying medical diagnoses andare highly qualified to carry out the task of deter-mining the intended syntactic structure of a clinicalterm.Table 3: Four Types of Syntactic Structures of Tri-gram Termsleft-branching ((XY)Z):[[urinary tract] infection][[right sided] weakness]right-branching (X(YZ)):[chronic [back pain]][low [blood pressure]]non-branching ((X)(Y)(Z)):[[follicular][thyroid][carcinoma]][[serum][dioxin][level]]monolithic (XYZ):[difficulty finding words][serous otitis media]In the process of annotating the test set of tri-grams, four types of terms emerged (Table 3).
Thefirst two types are left and right-branching where theleft-branching phrases contain a left-adjoining groupthat modifies the head of the noun phrase.
The right-branching phrases contain a right-adjoining groupthat forms the kernel or the head of the noun phraseand is modified by the remaining word on the left.The non-branching type is where the phrase containsa head noun that is independently modified by theother two words.
For example, in ?follicular thyroidcarcinoma?, the experts felt that ?carcinoma?
wasmodified by both ?follicular?
and ?thyroid?
indepen-dently, where the former denotes the type of cancerand the latter denotes its location.
This intuition isreflected in some formal medical classification sys-tems such as the Hospital International Classifica-tion of Disease Adaptation (HICDA) where cancersare typically classified with at least two categories -one for location and one for the type of malignancy.This type of pattern is rare.
We were able to iden-tify only six examples out of the 710 terms.
Themonolithic type captures the intuition that the termsfunction as a collocation and are not decomposableinto subunits.
For example, ?leg length discrepancy?denotes a specific disorder where one leg is of a dif-ferent length from the other.
Various combinationsof subunits within this term result in nonsensical ex-pressions.Table 4: Distribution of term types in the test setType Count %totalLeft-branching 251 35.5Right-branching 378 53.4Non-branching 6 0.8Monolithic 73 10.3Total 708 100Finally, there were two terms for which no con-sensus could be reached: ?heart irregularly irregu-lar?
and ?subacute combined degeneration?.
Thesecases were excluded from the final set.
Table 4shows the distribution of the four types of terms inthe test set.5 EvaluationWe hypothesize that general English typically hasa specific syntactic structure in the medical domain,which provides a single semantic interpretation.
Thepatterns observed in the set of 710 medical termsdescribed in the previous section suggest that theG2 method offers an intuitive way to determine thestructure of a term that underlies its syntactic struc-ture.Table 5: G2 Model Descriptionsleft-branching Model 2 [ [XY] Z ]right-branching Model 3 [ X [YZ] ]The left and right-branching patterns roughly cor-respond to Models 2 and 3 in Table 5.
Models 1and 4 do not really correspond to any of the pat-terns we were able to identify in the set of terms.Model 1 would represent a term where words arecompletely independent of each other, which is anunlikely scenario given that we are working withterms whose composition is dependent by definition.This is not to say that in other applications (e.g.,syntactic parsing) this model would not be relevant.Model 4 suggests dependence between the outeredges of a term and their independence from the13Figure 1: Comparison of the results with two base-lines: L-branching and R-branching assumptionsmiddle word, which is not motivated from the stand-point of a traditional context free grammar whichprohibits branch crossing.
However, this model maybe welcome in a dependency grammar paradigm.One of the goals of this study is to test an ap-plication of the G2 method trained on a corpus ofmedical data to distinguish between left and right-branching patterns.
The method ought to suggestthe most likely analysis for an NP-based medicalterm based on the empirical distribution of the termand its components.
As part of the evaluation, wecompute the G2 scores for each of the terms in thetest set, and picked the model with the lowest scoreto represent the structural pattern of the term.
Wecompared these results with manually identified pat-terns.
At this preliminary stage, we cast the problemof identifying the structure of a three word medicalterm as a binary classification task where a term isconsidered to be either left or right-branching, ef-fectively forcing all terms to either be representedby either Model 2 or Model 3.6 Results and DiscussionIn order to validate the G2 method for determin-ing the structure of medical terms, we calculatedthe agreement between human experts?
interpreta-tion of the syntactic structure of the terms and theinterpretation suggested by the G2 method.
Theagreement was computed as the ratio of match-ing interpretations to the total number of terms be-ing interpreted.
We used two baselines, one estab-lished by assuming that each term is left-branchingand the other by assuming that each term is right-branching.
As is clear from Table 4, the left-branching baseline is 35.5% and the right-branchingbaseline is 53.4% meaning that if we simply as-sign left-branching pattern to each three word term,we would agree with human experts 35.5% of thetime.
The G2 method correctly identifies 185 tri-grams as being left-branching (Model 2) and 345 tri-grams as being right-branching (Model 3).
There are116 right-branching trigrams incorrectly identifiedas left-branching, and 62 left-branching trigrams in-correctly identified as right- branching.
Thus themethod and the human experts agreed on 530 (75%)terms out of 708 (kappa=0.473), which is better thanboth baselines (Figure 1).
We did not find any over-lap between the terms that human experts annotatedas non-branching and the terms whose corpus dis-tribution can be represented by Model 4 ([[XZ]Y]).This is not surprising as this pattern is very rare.Most of the terms are represented by either Model 2(left-branching) or Model 3 (right-branching).
Themonolithic terms that the human experts felt werenot decomposable constitute 10% of all terms andmay be handled through some other mechanismsuch as collocation extraction or dictionary lookup.Excluding monolithic terms from testing results in83.5% overall agreement (kappa=0.664).We observed that 53% of the terms in our testset are right-branching while only 35% are left-branching.
(Resnik, 1993) found between 64% and67% of nominal compounds to be left-branching andused that finding to establish a baseline for his exper-iments with structural ambiguity resolution.
(Nakovand Hearst, 2005) also report a similar percentage(66.8%) of left-branching noun compounds.
Ourtest set is not limited to nominal compounds, whichmay account for the fact that a slight majority of theterms are found to be right-branching as adjectivalmodification in English is typically located to theleft of the head noun.
This may also help explainthe fact that the method tends to have higher agree-ment within the set of right-branching terms (85%)vs. left-branching (62%).We also observed that many of the terms markedas monolithic by the experts are of Latin origin suchas the term in Example 9 or describe the functional14status of a patient such as the term in Example 10.erythema1 ab2 igne3 (9)difficulty1 swallowing2 solids3 (10)Example 10 merits further discussion as it illus-trates another potential application of the methodin the domain of functional status terminology.
Aswas mentioned in the introduction, functional statusterms may be be represented as a predication witha set of arguments.
Such view of functional statusterminology lends itself well to a frame-based repre-sentation of functional status terms in the context ofa database such as FrameNet 3 or PropBank4.
One ofthe challenging issues in representing functional sta-tus terminology in terms of frames is the distinctionbetween the core predicate and the frame elements(Ruggieri et al, 2004).
It is not always clear whatlexical material should be part of the core predicateand what lexical material should be part of one ormore arguments.
Consider the term in Example 10which represents a nominalized form of a predica-tion.
Conceivably, we could analyze this term as aframe shown in Example 11 where the predicationconsists of a predicate [DIFFICULTY] and two ar-guments.
Alternatively, Example 12 presents a dif-ferent analysis where the predicate is a specific kindof difficulty with a single argument.
[P:DIFFICULTY][ARG1:SWALLOWING<ACTIVITY>][ARG2:SOLIDS<PATIENT>](11)[P:SWALLOWING DIFFICULTY][ARG1: SOLIDS<PATIENT>](12)The analysis dictates the shape of the framesand how the frames would fit into a network offrames.
The G2 method identifies Example 10 asleft-branching (Model 2), which suggests that itwould be possible to have a parent DIFFICULTYframe and a child CLIMBING DIFFICULTY thatwould inherit form its parent.
An example wherethis is not possible is the term ?difficulty stayingasleep?
where it would probably be nonsensical or atleast impractical to have a predicate such as [STAY-ING DIFFICULTY].
It would be more intuitive to3http://www.icsi.berkeley.edu/framenet/4http://www.cis.upenn.edu/ ace/assign this term to the DIFFICULTY frame witha frame element whose lexical content is ?stayingasleep?.
The method appropriately identifies theterm ?difficulty staying asleep?
as right-branching(Model 3) where the words ?staying asleep?
aregrouped together.
This is an example based on in-formal observations; however, it does suggest a util-ity in constructing frame-based representation of atleast some clinical terms.7 LimitationsThe main limitation of the G2 method is the expo-nential growth in the number of models to be evalu-ated with the growth in the length of the term.
Thislimitation can be partly alleviated by either only con-sidering adjacent models and limiting the length to5-6 words, or using a forward or backward sequen-tial search proposed by (Pedersen et al, 1997) forthe problem of selecting models for the Word SenseDisambiguation task.8 Conclusions and Future WorkThis paper presented a simple but effective methodbased on G2 to determine the internal structure ofthree-word noun phrase medical terms.
The abil-ity to determine the syntactic structure that givesrise to a particular semantic interpretation of a med-ical term may enable accurate mapping of unstruc-tured medical text to standardized terminologies andnomenclatures.
Future directions to improve the ac-curacy of our method include determining how othermeasures of association, such as dice coefficient and?2, perform on this task.
We feel that there is a pos-sibility that no single measure performs best over alltypes of terms.
In that case, we plan to investigate in-corporating the different measures into an ensemble-based algorithm.We believe the model-fitting method is not lim-ited to structural ambiguity resolution.
This methodcould be applied to automatic term extraction andautomatic text indexing of terms from a standard-ized vocabulary.
More broadly, the principles of us-ing distributional characteristics of word sequencesderived from large corpora may be applied to unsu-pervised syntactic parsing.15AcknowledgmentsWe thank Barbara Abbott, Debra Albrecht andPauline Funk for their contribution to annotating thetest set and discussing aspects of medical terms.This research was supported in part by theNLM Training Grant in Medical Informatics (T15LM07041-19).
Ted Pedersen?s participation in thisproject was supported by the NSF Faculty Early Ca-reer Development Award (#0092784).ReferencesS.
Banerjee and T. Pedersen.
2003.
The design, imple-mentation, and use of the Ngram Statistic Package.
InProc.
of the Fourth International Conference on Intel-ligent Text Processing and Computational Linguistics,Mexico City, February.A.M.
Buckeridge and R.F.E.
Sutcliffe.
2002.
Disam-biguating noun compounds with latent semantic index-ing.
International Conference On Computational Lin-guistics, pages 1?7.T.
Dunning.
1993.
Accurate methods for the statistics ofsurprise and coincidence.
Computational Linguistics,19(1):61?74.D.A.
Evans and C. Zhai.
1996.
Noun-phrase analysis inunrestricted text for information retrieval.
Proc.
of the34th conference of ACL, pages 17?24.D.
Hindle and M. Rooth.
1993.
Structural Ambigu-ity and Lexical Relations.
Computational Linguistics,19(1):103?120.M.
Lapata and F. Keller.
2004.
The Web as a Base-line: Evaluaing the Performance of UnsupervisedWeb-based Models for a Range of NLP Tasks.
Proc.of HLT-NAACL, pages 121?128.M.
Lapata, S. McDonald, and F. Keller.
1999.
Determi-nants of Adjective-Noun Plausibility.
Proc.
of the 9thConference of the European Chapter of ACL, 30:36.M.
Lauer and M. Dras.
1994.
A Probabilistic Model ofCompound Nouns.
Proc.
of the 7th Australian JointConference on AI.M.
Lauer.
1995.
Corpus Statistics Meet the Noun Com-pound: Some Empirical Results.
Proc.
of the 33rd An-nual Meeting of ACL, pages 47?55.M.
Liberman and R. Sproat.
1992.
The stress and struc-ture of modified noun phrases in English.
Lexical Mat-ters, CSLI Lecture Notes, 24:131?181.M.P.
Marcus.
1980.
Theory of Syntactic Recognitionfor Natural Languages.
MIT Press Cambridge, MA,USA.B.T.
McInnes.
2004.
Extending the log-likelihood ratioto improve collocation identification.
Master?s thesis,University of Minnesota.R.
Moore.
2004.
On log-likelihood-ratios and the sig-nificance of rare events.
In Dekang Lin and DekaiWu, editors, Proc.
of EMNLP 2004, pages 333?340,Barcelona, Spain, July.
Association for ComputationalLinguistics.P.
Nakov and M. Hearst.
2005.
Search engine statisticsbeyond the n-gram: Application to noun compoundbracketing.
In Proceedings of the Ninth Conference onComputational Natural Language Learning (CoNLL-2005), pages 17?24, Ann Arbor, Michigan, June.
As-sociation for Computational Linguistics.T.
Pedersen, M. Kayaalp, and R. Bruce.
1996.
Signifi-cant lexical relationships.
In Howard Shrobe and TedSenator, editors, Proc.
of the Thirteenth National Con-ference on Artificial Intelligence and the Eighth Inno-vative Applications of Artificial Intelligence Confer-ence, Vol.
2, pages 455?460, Menlo Park, California.AAAI Press.T.
Pedersen, R. Bruce, and J. Wiebe.
1997.
Sequen-tial model selection for word sense disambiguation.
InProc.
of the Fifth Conference on Applied Natural Lan-guage Processing, pages 388?395, Washington, DC,April.J.
Pustejovsky, P. Anick, and S. Bergler.
1993.
Lexi-cal semantic techniques for corpus analysis.
Compu-tational Linguistics, 19(2):331?358.A.
Ratnaparkhi.
1998.
Maximum Entropy Models forNatural Lnaguage Ambiguity Resolution.
Ph.D. thesis,University of Pennsylvania.P.
Resnik and M. Hearst.
1993.
Structural Ambiguityand Conceptual Relations.
Proc.
of the Workshop onVery Large Corpora: Academic and Industrial Per-spectives, June, 22(1993):58?64.P.S.
Resnik.
1993.
Selection and Information: A Class-Based Approach to Lexical Relationships.
Ph.D. the-sis, University of Pennsylvania.A.P.
Ruggieri, S. Pakhomov, and C.G.
Chute.
2004.
ACorpus Driven Approach Applying the ?Frame Se-mantic?
Method for Modeling Functional Status Ter-minology.
Proc.
of MedInfo, 11(Pt 1):434?438.M.Q.
Stearns, C. Price, KA Spackman, and AY Wang.2001.
SNOMED clinical terms: overview of the de-velopment process and project status.
Proc AMIASymp, pages 662?6.S.
S. Wilks.
1938.
The large-sample distribution of thelikelihood ratio for testing composite hypotheses.
TheAnnals of Mathematical Statistics, 9(1):60?62, March.16
