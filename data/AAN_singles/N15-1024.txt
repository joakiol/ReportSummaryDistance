Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 225?230,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEntity Linking for Spoken LanguageAdrian BentonHuman Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MD, USAadrian@jhu.eduMark DredzeHuman Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MD, USAmdredze@jhu.eduAbstractResearch on entity linking has considered abroad range of text, including newswire, blogsand web documents in multiple languages.However, the problem of entity linking forspoken language remains unexplored.
Spo-ken language obtained from automatic speechrecognition systems poses different types ofchallenges for entity linking; transcription er-rors can distort the context, and named entitiestend to have high error rates.
We propose fea-tures to mitigate these errors and evaluate theimpact of ASR errors on entity linking usinga new corpus of entity linked broadcast newstranscripts.1 IntroductionEntity linking identifies for each textual mentionof an entity a corresponding entry contained in aknowledge base, or indicates when no such entryexits (NIL).
Numerous studies have explored en-tity linking in a wide range of domains, includingnewswire (Milne and Witten, 2008; Mcnamee et al,2009; McNamee and Dang, 2009; Dredze et al,2010), blog posts (Ji et al, 2010), web pages (De-martini et al, 2012; Lin et al, 2012), social media(Cassidy et al, 2012; Guo et al, 2013a; Shen et al,2013; Liu et al, 2013), email (Gao et al, 2014) andmulti-lingual documents (Mayfield et al, 2011; Mc-Namee et al, 2011; Wang et al, 2012).
A commontheme across all these settings requires addressingtwo difficulties in linking decisions: matching thetextual name mention to the form contained in theknowledge base, and using contextual clues to dis-ambiguate similar entities.
However, all of thesestudies have focused on written language, whilelinking of spoken language remains untested.
Yetmany intended applications of entity linking, suchas supporting search (Hachey et al, 2013) and iden-tifying relevant sources for reports (He et al, 2010;He et al, 2011), linking of spoken language is crit-ical.
Search results regularly include audio content(e.g.
YouTube) and numerous information sourcesare audio recordings (e.g.
media reports.)
An evalu-ation of entity linking for spoken language can helpclarify issues and challenges in this domain.In addition to the two main challenges discussedabove, audio entity linking presents two paralleldifficulties that arise from automatic transcription(ASR) of speech.
First, the context can be bothshorter (than newswire formats) and contain ASRerrors, which can make the context of the mentionless like supporting material in the knowledge base.Second, named entities are often more difficult torecognize (Huang, 2005; Horlock and King, 2003);they are often out-of-vocabulary and less commonoverall in training data.
This can mislead the namematching techniques on which most entity linkingsystems depend.In this paper we consider the task of entity link-ing for spoken language by evaluating linking ontranscripts of broadcast news.
We select broadcastnews as a comparable domain of spoken language tonewswire documents, which have been the focus ofconsiderable research for entity linking (Mcnameeet al, 2009; Ji et al, 2010).
We chose this compara-ble domain to focus on issues introduced because ofa transition to spoken language from written, as op-posed to issues that arise from a general domain shiftassociated with conversational speech, an issue thathas been previously studied in the shift from writ-ten news to written conversations (weblogs, social225media, etc.)
(Baldwin et al, 2013; Han et al, 2013).We proceed as follows.
We first introduce a newbroadcast news dataset annotated for entity linking.We then propose new features based on ASR outputto address the two sources of error specific to spo-ken language: 1) context errors and shortening, 2)name mention transcription errors.
We then test ourfeatures on the automated output of an ASR systemto validate our findings.2 Entity Linked Spoken Language DataWe created entity linking annotations for HUB4(Fiscus et al, 1997), a manually-transcribed broad-cast news corpus.
We used gold named entity anno-tations from Parada et al (2011), who manually an-notated 9971 utterances with CONLL style (TjongKim Sang and De Meulder, 2003) named entities.We selected 2140 person entities and obtained en-tity linking decisions with regards to the TAC KBPknowledge base (Mcnamee et al, 2009; Ji et al,2010), which contains 818,741 entries derived fromWikipedia.Annotations were initially obtained using Ama-zon Mechanical Turk (Callison-Burch and Dredze,2010) using the same entity linking annotationscheme as Mayfield et al (2011).
Turkers wereasked which of five provided Wikipedia articlesmatched a person mention highlighted in a displayedutterance.
The provided Wikipedia articles were se-lected based on token overlap between the mentionand article title, weighted by TFIDF.
In addition toselecting one of the five articles, turkers could select?None of the above?, ?Not enough information?
or?Not a person?.
We collected one Turker annotationper query and manually verified and corrected eachprovided label.
For mentions that were not matchedto an article, we verified that the article was not inthe KB, or manually assigned a KB entry otherwise.Because we manually corrected each annotation,mistakes and biases resulting from crowdsourcedannotations are not present in this corpus.
Of the2140 annotated mentions, 41% (n=887) were NIL,compared to 54.6% in TAC-KBP 2010 (Ji et al,2010) and 57.1% in TAC-KBP 2009 (Mcnamee etal., 2009).
The named entity and linking annota-tions can be found at https://github.com/mdredze/speech_ner_entity_linking_data/releases/tag/1.0.We divided the 2140 mentions into 60% train(n = 1283) and 40% test (n = 857.)
We ensuredthat the 1218 unique mention strings were disjointbetween the two sets, i.e.
no mention in the test datawas observed during training.Each instance is represented by the query (men-tion string) and document context.
Unlike writtenarticles, broadcast news does not indicate where onetopic starts and another ends.
Therefore, we ex-perimented with different size contexts by includingthe utterance containing the name mention and upto eight utterances before and after so long as theyoccurred within 150 seconds of the start of the utter-ance containing the name mention.
We found thatfive utterances before and after gave the highest av-erage accuracy when using a standard set of featureson the reference transcript to perform entity linking;we use this setting in the experiments below.3 Entity Linking SystemFor entity linking, we use Slinky (Benton et al,2014), an entity linking system that uses parallelprocessing of queries and candidates in a learnedcascade to achieve fast and accurate entity linkingperformance.
We use standard features from Mc-Namee et al (2012) as described in Benton et al(2014).
For a query q composed of a context (mul-tiple utterances) and a named entity string, Slinkyfirst triages (Dredze et al, 2010; McNamee et al,2012; Guo et al, 2013b) the query to identify a set ofcandidates Cqin the knowledge base that may cor-respond to the mention string.
Up to 1000 candi-dates are considered, though its usually much fewer.The system then extracts features based on queryand candidate pairs and ranks them using a seriesof classifiers.
The candidates are then ordered bytheir final scores; the highest ranking candidate isselected as the system?s prediction.
NIL is includedas a candidate for every query and is then rankedby the system.
For all training settings, we sweepover hyper-parameters using 5-fold cross validationon the training data to find the most accurate system.Reported results are based on the test data.2263.1 Spoken Language FeaturesASR transcription errors pose a challenge for stan-dard text processing features, which rely on textualsimilarity to measure relatedness of both context andentity mention strings.
However, ASR errors are notrandom; incorrectly decoded words may be phoneti-cally similar to the original spoken words.
This sug-gests that similarity can still be captured by consid-ering phonetic similarity.We experiment with four feature types.?
Text: Our baseline system uses features basedon the text of the mention string and document.We used the feature set presented by McNameeet al (2012) and used in Benton et al (2014),which was the best performing submission in theTAC-KBP 2009 entity linking task.
These fea-tures include, among others, features that com-pare the candidate entity name to the mentionstring as well as the document?s terms to thosestored in the candidate?s description in the KB.These include the dice coefficient, cosine simi-larity (boolean and weighted), and proportion ofcandidate tokens in the query document.?
Phone: Words in the document, mention stringas well as the knowledge base are representedas phone sequences instead of text.
We convertall words to phones using a grapheme string tophone (G2P) system.?
Metaphones: Two distinct phones can soundsimilar, yet still appear different when match-ing phones.
Metaphones (Philips, 1990), a morerecent version of Soundex, map similar sound-ing phones to the same representation.
We con-vert the phones used in the previous paragraphto metaphones.?
Lattice: Expected word counts of the query doc-ument from the ASR lattice.
Extracted unigramsare treated as a weighted bag-of-words for thequery document.
We compute all the featuresthat use the query document?s content and weighthem by the term?s expectation.The features in each of the above sets depend ontheir representation of the text (e.g.
text, phone,metaphone, lattice).
Additionally, we include thefollowing features in all experiments: Bias featuresthat fire for all candidates, only non-NIL candidates,and only NIL candidates; NIL features indicative ofbeing linked to no article in the knowledge base suchas the mention string is only 1 or 2 characters/tokens,the number of candidates emitted by the triager; andthe Popularity (number of Wikipedia in-links) of thecandidate.Triage The above feature sets change the rankingof candidates.
We also modified the triage methodsthat produce Cqbased on these new features.
ForText we used the triage methods of Mcnamee et al(2009): string similarity based on character/tokenn-gram overlap, same acronym, exact match, etc.Phone triage used the same heuristics but based onphone representations of the mention strings andcandidate names.
Metaphone triage worked as inphone, but used metaphones.
When two representa-tions are used we take the union of their candidates.G2P System For phone features we use a G2P sys-tem based on the Sequitur G2P grapheme-phone-converter (Bisani and Ney, 2008).
We trainedthe system on version 0.7a of CMUdict1(stressomitted from phone sequences, case-insensitive forgraphemes), by predicting the phoneme sequence ofan English token given its string (G2P).
The lan-guage model was a 9-gram model with modifiedKneser-Ney smoothing applied.
For Phone featureswe converted each token to its best phone repre-sentation, where each phone, as well as diphthong,is represented by a single character for similaritymatching.4 ExperimentsWe evaluate reference transcripts and output fromtwo ASR systems run on our dataset (HUB4).
Weuse Kaldi (Povey et al, 2011) trained on the spokenversion of the Wall Street Journal corpus (Paul andBaker, 1992).2The first system (mono) relies on anHMM whose hidden states are context-independentphones.
The second system (tri4b) uses an HMMthat outputs phones dependent on their immediateleft and right contexts.
These systems respectivelyachieve 70.6% and 50.7% WER over our trainingset, where high error rates are likely due to a shift indomain from primarily financial news to the wider1http://www.speech.cs.cmu.edu/cgi-bin/cmudict2Linguistic Data Consortium number LDC94S13A.227Reference mono tri4bWER - 71% 51%Mention WER - 90% 63%Mention Exact - 5% 22%Text 0.77/0.77 0.44/0.48 0.56/0.55Phone 0.77/0.79 0.42/0.45 0.47/0.46Text+Phn 0.81/0.81 0.45/0.49 0.58/0.61*Text+Phn+Latt - 0.45/0.49 0.59/0.60*Metaphone 0.52/0.61 0.43/48 0.52/0.56Text+Metaphn 0.78/0.78 0.45/0.50 0.59/0.61*Text+Metaphn+Latt - 0.45/0.51 0.59/0.63**Table 1: Performance of different feature sets for refer-ence transcripts and ASR output (mono, tri4b).
Resultsare cross-fold validation/test accuracy.
Significance ofsystem test accuracy compared to the Text baseline com-puted with two-sample proportion test.
(*p < 0.05, **p < 0.01).Text Phone Text+PhnReference 0.92/0.87 0.95/0.91 0.98/0.97mono 0.48/0.13 0.50/0.17 0.51/0.19tri4b 0.68/0.47 0.71/0.52 0.73/0.56Table 2: Overall/non-NIL triager recall.news variety in HUB4.
These higher error settingstest the limits of entity linking in noisy ASR.To find the ASR-corrupted mention string usedfor the query q we align the ASR transcript by token-level edit distance ?
additions and deletions cost1, while substitutions cost the Jaccard distance be-tween two tokens.
This is done at both training andtest time, and allows us to evaluate performance ofentity linking features without worrying about errorsintroduced by a named entity recognizer on the tran-scripts.Entity Linking System Training The entity link-ing system relies on a linear Ranking SVM objective(Joachims, 2006), and the optimal slack parameterC was chosen using 5-fold cross validation over thetraining set (C varied from 1 and 5 ?10?5...3).
Dur-ing cross-validation, mention string types were keptdisjoint between the train and development folds.Ranking was performed over the (up to) 1000 can-didates produced by triage selected from the TAC-KBP 2009/10 KB (Mcnamee et al, 2009; Ji et al,2010).
Using the selected C we trained over the en-tire training set and evaluated on the test set.Text Phone Text+Phn T+P+LattReference 0.82 0.82 0.85 0.85tri4b 0.76 0.62 0.77 0.78Table 3: Cross validation accuracy evaluated over onlythose queries whose correct candidate was output by thetriager for both tri4b and reference.tri4b Referenceon joseph Don Josephira magazine Ira Magazinerbob defiance Bob Mathiasgave deforest Dave Deforestgeorgia the books George W. Bush?sgeorge w. porsche George W. Bushlouis freer Louis Freehnorman monetta Norman Minetaedward and Edward Egankeith clarke Nikki ClarkTable 4: Examples of improved linking accuracy.5 ResultsTable 1 reports both the average accuracy for 5-foldcross validation (CV) on train and for the best tunedsystem from CV on test data.
The reference testaccuracy is relatively high, but lags behind personentity linking for written language.
When accuratetranscripts are available, entity linking for spokenlanguage, while harder, achieves just a little behindwritten language.
However, on ASR transcripts, ac-curacy drops considerably: 0.77 reference to 0.48(mono, 71% WER) or 0.55 (tri4b, 51% WER).
Ourfeatures improve accuracy for both ASR systems.Metaphone features do better than Phone features.Lattice do not show significant improvements, likelybecause they help with context but not mentions (seebelow.)
When combined with text, both metaphoneand phone features do similarly.The majority of our improvement comes from im-provements to recall.
Table 3 shows the accuracyof queries for which the triager found the correctcandidate in both the reference transcript and tri4b,providing a consistent set for comparison.
For thesequeries, tri4b is much closer to the results obtainedon reference and much higher than the best resultsin Table 1.
This is encouraging, especially given the50% WER of tri4b; entity linking accuracy is not se-riously impacted by noisy transcripts, provided that228the correct candidate is recalled for ranking.Table 2 shows the recall of the triager on the ref-erence, tri4b and mono transcripts.
Reference re-call is quite high, while recall for the ASR systemsis much lower.
Here, our features dramatically im-prove the recall, giving the ranker an opportunity tocorrectly score these queries.
The challenge of re-call is that many of the mention strings are incor-rectly recognized.
Table 1 shows the WER of themention string and the number of mention strings forwhich the recognition is completely correct.
Unsur-prisingly, error rates for mentions are higher than theoverall WER.
In short, success on ASR transcriptsis primarily dictated by the effectiveness of findingcandidates in triage, which is much harder given thelow recognition rate.
Our features most benefit over-all accuracy by improving recall.Finally, Table 4 provides example of improved re-call: mention strings that are incorrectly recognizedby the tri4b ASR system leading to linking failures,but are then correctly linked by our improved fea-tures.
These examples demonstrate the effectivenessof phonetic matching, retrieving the correct ?GeorgeW.
Bush?
when the recognizer output ?Georgia theBooks.
?6 ConclusionWe have conducted the first analysis of entity linkingfor spoken language.
Our new features, which relyon phonetic representations of words and expectedcounts of the lattice for context, improve the accu-racy of an entity linker on ASR output.
Our anal-ysis reveals that while the linker is not sensitive tolarge drops in error rates in the context, it is highlysensitive to error rates in mention strings, due to adrop in triage recall.
Our features improve the over-all accuracy by improving the recall of the triager.Future work should focus on additional methods foridentifying relevant KB candidates given inaccuratetranscriptions of mention strings.ReferencesTimothy Baldwin, Paul Cook, Marco Lui, AndrewMacKinlay, and Li Wang.
2013.
How noisy socialmedia text, how diffrnt social media sources.
In Pro-ceedings of the 6th International Joint Conference onNatural Language Processing (IJCNLP 2013), pages356?364.Adrian Benton, Jay Deyoung, Adam Teichert, MarkDredze, Benjamin Van Durme, Stephen Mayhew, andMax Thomas.
2014.
Faster (and better) entity link-ing with cascades.
In NIPS Workshop on AutomatedKnowledge Base Construction.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
In Speech Communication, volume 50, pages434?451.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with amazon?s mechanicalturk.
In Workshop on Creating Speech and LanguageData With Mechanical Turk at NAACL-HLT.Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-biaga, and Hongzhao Huang.
2012.
Analysis and en-hancement of wikification for microblogs with contextexpansion.
In COLING, pages 441?456.Gianluca Demartini, Djellel Eddine Difallah, andPhilippe Cudr?e-Mauroux.
2012.
Zencrowd: Lever-aging probabilistic reasoning and crowdsourcing tech-niques for large-scale entity linking.
In Proceedings ofthe 21st International Conference on World Wide Web,WWW ?12, pages 469?478, New York, NY, USA.ACM.Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambiguation forknowledge base population.
In COLING.Jonathan Fiscus, John Garofolo, Mark Przybocki,William Fisher, and David Pallett.
1997.
1997 englishbroadcast news speech (hub4).
In LDC98S71.Ning Gao, Douglas Oard, and Mark Dredze.
2014.
A testcollection for email entity linking.
In NIPS Workshopon Automated Knowledge Base Construction.Stephen Guo, Ming-Wei Chang, and Emre Kiciman.2013a.
To link or not to link?
a study on end-to-endtweet entity linking.
In NAACL.Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and ShengLi.
2013b.
Improving candidate generation for entitylinking.
In Natural Language Processing and Infor-mation Systems, pages 225?236.
Springer.Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-nibal, and James R. Curran.
2013.
Evaluating en-tity linking with wikipedia.
Artificial Intelligence,194(0):130 ?
150.
Artificial Intelligence, Wikipediaand Semi-Structured Resources.Bo Han, Paul Cook, and Timothy Baldwin.
2013.
Lex-ical normalization for social media text.
ACM Trans.Intell.
Syst.
Technol., 4(1):5:1?5:27, February.Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and LeeGiles.
2010.
Context-aware citation recommendation.In Proceedings of the 19th International Conference229on World Wide Web, WWW ?10, pages 421?430, NewYork, NY, USA.
ACM.Qi He, Daniel Kifer, Jian Pei, Prasenjit Mitra, and C. LeeGiles.
2011.
Citation recommendation without authorsupervision.
In Proceedings of the Fourth ACM Inter-national Conference on Web Search and Data Mining,WSDM ?11, pages 755?764, New York, NY, USA.ACM.James Horlock and Simon King.
2003.
Named entityextraction from word lattices.
Eurospeech.Fei Huang.
2005.
Multilingual Named Entity Extractionand Translation from Text and Speech.
Ph.D. thesis,Carnegie Mellon University.Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2010.
Overview of the tac 2010knowledge base population track.
Third Text AnalysisConference (TAC 2010).Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In Proceedings of the ACM Conference onKnowledge Discovery and Data Mining (KDD).Thomas Lin, Oren Etzioni, et al 2012.
Entity link-ing at web scale.
In Proceedings of the Joint Work-shop on Automatic Knowledge Base Construction andWeb-scale Knowledge Extraction, pages 84?88.
Asso-ciation for Computational Linguistics.Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, FuruWei, and Yi Lu.
2013.
Entity linking for tweets.
InACL.James Mayfield, Dawn Lawrie, Paul McNamee, andDouglasW.
Oard.
2011.
Building a cross-languageentity linking collection in twenty-one languages.In Pamela Forner, Julio Gonzalo, Jaana Kek?al?ainen,Mounia Lalmas, and Marteen de Rijke, editors, Mul-tilingual and Multimodal Information Access Evalua-tion, volume 6941 of Lecture Notes in Computer Sci-ence, pages 3?13.
Springer Berlin Heidelberg.Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the TAC 2009 knowledge base population track.
InTAC.Paul Mcnamee, Mark Dredze, Adam Gerber, NikeshGarera, Tim Finin, James Mayfield, Christine Pi-atko, Delip Rao, David Yarowsky, and Markus Dreyer.2009.
Hltcoe approaches to knowledge base popula-tion at tac 2009.
In Text Analysis Conference (TAC.Paul McNamee, James Mayfield, Douglas W. Oard, TanXu, Ke Wu, Veselin Stoyanov, and David Doerman.2011.
Cross-language entity linking in maryland dur-ing a hurricane.
In TAC.Paul McNamee, Veselin Stoyanov, James Mayfield, TimFinin, Tim Oates, Tan Xu, Douglas Oard, and DawnLawrie.
2012.
HLTCOE participation at TAC 2012:Entity linking and cold start knowledge base construc-tion.
In TAC.David N. Milne and Ian H. Witten.
2008.
Learning tolink with wikipedia.
In CIKM, pages 509?518.Carolina Parada, Mark Dredze, and Frederick Jelinek.2011.
Oov sensitive named-entity recognition inspeech.
In International Speech Communication As-sociation (INTERSPEECH).Douglas Paul and Janet Baker.
1992.
The design forthe wall street journal-based csr corpus.
In DARPASpeech and Language Workshop.
Morgan KaufmannPublishers.Lawrence Philips.
1990.
Hanging on the metaphone.Computer Language, 7(12 (December)).Daniel Povey, Arnab Ghoshal, Gilles Boulianne, LukasBurget, Ondrej Glembek, Nagendra Goel, Mirko Han-nemann, Petr Motlicek, Yanmin Qian, Petr Schwarz,Jan Silovsky, Georg Stemmer, and Karel Vesely.
2011.The kaldi speech recognition toolkit.
In IEEE 2011Workshop on Automatic Speech Recognition and Un-derstanding.
IEEE Signal Processing Society, Decem-ber.
IEEE Catalog No.
: CFP11SRW-USB.Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.2013.
Linking named entities in tweets with knowl-edge base via user interest modeling.
In KDD.Erik F Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003-Volume 4, pages 142?147.Association for Computational Linguistics.Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie Tang.2012.
Cross-lingual knowledge linking across wikiknowledge bases.
In Proceedings of the 21st Inter-national Conference on World Wide Web, WWW ?12,pages 459?468, New York, NY, USA.
ACM.230
