Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448?453,Dublin, Ireland, August 23-24, 2014.NTNU: Measuring Semantic Similarity withSublexical Feature Representations and Soft CardinalityAndr?e Lynum, Partha Pakray, Bj?orn Gamb?ack Sergio Jimenez{andrely,parthap,gamback}@idi.ntnu.no sgjimenezv@unal.edu.coNorwegian University of Science and Technology Universidad Nacional de ColombiaTrondheim, Norway Bogot?a, ColombiaAbstractThe paper describes the approaches takenby the NTNU team to the SemEval 2014Semantic Textual Similarity shared task.The solutions combine measures basedon lexical soft cardinality and charactern-gram feature representations with lexi-cal distance metrics from TakeLab?s base-line system.
The final NTNU system isbased on bagged support vector machineregression over the datasets from previousshared tasks and shows highly competi-tive performance, being the best system onthree of the datasets and third best overall(on weighted mean over all six datasets).1 IntroductionThe Semantic Textual Similarity (STS) shared taskaims at providing a unified framework for evaluat-ing textual semantic similarity, ranging from ex-act semantic equivalence to completely unrelatedtexts.
This is represented by the prediction ofa similarity score between two sentences, drawnfrom a particular category of text, which rangesfrom 0 (different topics) to 5 (exactly equivalent)through six grades of semantic similarity (Agirreet al., 2013).
This paper describes the NTNUsubmission to the SemEval 2014 STS shared task(Task 10).
The approach is based on the lexicaland distributional features of the baseline Take-Lab system from the 2012 shared task (?Sari?c et al.,2012), but improves on it in three ways: by addingtwo new categories of features and by using a bag-ging regression model to predict similarity scores.The new feature categories added are based onsoft cardinality and character n-grams, describedThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence de-tails:http://creativecommons.org/licenses/by/4.0/in Section 2.
The parameters of the two cate-gories are optimised over several corpora and thefeatures are combined through support vector re-gression (Section 3) to create the actual systems(Section 4).
As Section 5 shows, the new mea-sures give the baseline system a substantial boost,leading to very competitive results in the sharedtask evaluation.2 Feature Generation MethodsThe methods used for creating new features utilisesoft cardinality and character n-grams.
Soft cardi-nality (Jimenez et al., 2010) was used successfullyfor the STS task in previous SemEval editions(Jimenez et al., 2012a; Jimenez et al., 2013a).The NTNU systems utilise an ensemble of such 18measures, based only on surface text information,which were extracted using soft cardinality withdifferent similarity functions, as further describedin Section 2.1.Section 2.2 then introduces the similarity mea-sures based on character n-gram feature represen-tations, which proved themselves as the strongestfeatures in the STS 2013 task (Marsi et al., 2013).The measures used here replace character n-gramfeatures with cluster frequencies or vector val-ues based on the n-gram collocational structurelearned in an unsupervised manner from text data.A variety of n-gram feature representations weretrained on subsets of Wikipedia and the best per-forming ones were used for the new measures,which are based on cosine similarity between thedocument vectors derived from each sentence in agiven pair.2.1 Soft Cardinality MeasuresSoft cardinality resembles classical set cardinalityas it is a method for counting the number of ele-ments in a set, but differs from it in that similaritiesamong elements are being considered for the ?softcounting?.
The soft cardinality of a set of words448A = {a1, a2, .., a|A|} (a sentence) is defined by:|A|sim=|A|?i=1wai?|A|j=1sim(ai, aj)p(1)Where p is a parameter that controls the cardinal-ity?s softness (p?s default value is 1) and waiareweights for each word, obtained through inversedocument frequency (idf ) weighting.
sim(ai, aj)is a similarity function that compares two wordsaiand ajusing the symmetrized Tversky?s index(Tversky, 1977; Jimenez et al., 2013a) represent-ing them as sets of 3-grams of characters.
Thatis, ai= {ai,1, ai,2, ..., ai,|ai|} where ai,nis the nthcharacter trigram in the word aiin A.
Thus, theproposed word-to-word similarity is given by:sim(ai, aj)=|c|?(?|amin|+(1??)|amax|)+|c|(2)????
?|c| = |ai?
aj|+ biassim|amin| = min {|ai\ aj|, |aj\ ai|}|amax| = max {|ai\ aj|, |aj\ ai|}The sim function is equivalent to the Dice?s co-efficient if the three parameters are given their de-fault values, namely ?
= 0.5, ?
= 1 and bias = 0.The soft cardinalities of any pair of sentencesA,B andA?B can be obtained using Eq.
1.
The softcardinality of the intersection is approximated by|A?B|sim= |A|sim+|B|sim?|A?B|sim.
Thesefour basic soft cardinalities are algebraically re-combined to produce an extended set of 18 fea-tures as shown in Table 1.
The featureSTSsimis aparameterized similarity function built by reusingat word level the symmetrized Tversky?s index(Eq.
2), whose parameters are tuned from trainingdata (as further described in Subsection 3.2).Although this method is based purely on stringmatching, the soft cardinality has been shown tobe a very strong baseline for semantic textual com-parison.
The word-to-word similarity sim in Eq.
1could be replaced by other similarity functionsbased on semantic networks or any distributionalrepresentation making this method able to capturemore complex semantic relations among words.2.2 Sublexical Feature RepresentationsWe have created a set of similarity measures basedon induced representations of character n-grams.The measures are based on similarity betweenSTSsim(|A|?|A?B|)/|A||A|(|A|?|A?B|)/|A?B||B||B|/|A?B||A ?B|(|B|?|A?B|)/|B||A ?B|(|B|?|A?B|)/|A?B||A| ?
|A ?B||A?B|/|A||B| ?
|A ?B||A?B|/|B||A ?B| ?
|A ?B||A?B|/|A?B||A|/|A?B|(|A?B|?|A?B|)/|A?B|NB: in this table only, | ?
| is short for | ?
|simTable 1: Soft cardinality features.document vectors, here the centroid of the individ-ual term vector representations, which are trainedon character n-grams rather than full words.
Thevector representations are induced in an unsuper-vised manner from large unannotated corpora us-ing word clustering, topic learning and word rep-resentation learning methods.In this paper, three different methods havebeen used for creating the character n-gram fea-ture representations: Brown Clusters (Brown etal., 1992), Latent Semantic Indexing (LSI) topics(Deerwester et al., 1990), and log linear skip-grammodels (Mikolov et al., 2013).
The Brown clusterswere trained using the implementation by Liang(2005), while the LSI topic vectors and log linearskip-gram representations were trained using theGensim topic modelling framework (?Reh?u?rek andSojka, 2010).
In addition, tf-idf (Term-FrequencyInverse Document Frequency) weighting was usedwhen training LSI topic models.
We used a cosinedistance measure between document vectors con-sisting of the centroid of the term representationvectors.
For Brown clusters, the normalized termfrequency vectors were used with the cluster IDsinstead of the terms themselves.
For LSI topic rep-resentations, the tf-idf weighted topic mixture foreach term was used as the term representation.
Forthe log linear skip-grams, the word representationswere extracted from the model weight matrix.3 Feature and Parameter OptimisationThe extracted features and the parameters for thetwo methods described in the previous sectionwere optimised over several sets of training data.As no training data was explicitly provided for theSTS evaluation campaign this year, we used dif-ferent training sets from past campaigns and fromWikipedia for the new test sets.449Test set Training setdeft-forumMSRvid 2012 train and test +OnWN 2012 and 2013 testdeft-news MSRvid 2012 train + testheadlines headlines 2013 testimages MSRvid 2012 train + testOnWN OnWN 2012 and 2013 testtweet-newsSMTeuroparl 2012 test +SMTnews 2012 testTable 2: Training-test set pairs.3.1 Training Data and Pre-processingThe training-test sets pairs used for optimising theparameters of the soft cardinality methods wereselected from the STS 2012 and STS 2013 task,as shown in Table 2.
The character n-gram repre-sentation vectors were trained in an unsupervisedmanner on two subsets of Wikipedia consisting,respectively, of the first 12 million words (108characters, hence referred to as Wiki8) and of 125million words (109characters; Wiki9).First, however, the training data had to be pre-processed.
Thus, before extracting the idf weightsand the soft cardinality features, all the textsshown in Table 2 were passed through the follow-ing four pre-processing steps:(i) tokenization and stop-word removal (pro-vided by NLTK, Bird et al.
(2009)),1(ii) conversion to lowercase characters,(iii) punctuation and special character removal(e.g., ?.
?, ?
;?, ?$?, ?&?
), and(iv) Porter stemming.Character n-grams including whitespace weregenerated from the Wikipedia texts, which in con-trast only were pre-processed in a 3-step chain:(i) removal of punctuation and extra whites-pace,(ii) replacing numbers with their single digitword (?one?, ?two?, etc.
), and(iii) lowercasing all text.1http://www.nltk.org/Data ?
?
bias p ???
?bias?deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45Table 3: Optimal parameters used for each dataset.3.2 Soft Cardinality Parameter OptimisationThe first feature in Table 1, STSsim, was used tooptimise the four parameters ?, ?, bias, and p inthe following way.
First, we built a text similarityfunction reusing Eq.
2 for comparing two sets ofwords (instead of two sets of character 3-grams)and replacing the classic cardinality |?| by the softcardinality | ?
|simfrom Eq.
1.
This text similarityfunction adds three parameters (?
?, ?
?, and bias?
)to the initial four parameter set (?, ?, bias, and p).Second, these seven parameters were set to theirdefault values and the scores obtained from thisfunction for each pair of sentences were comparedto the gold standards in the training data usingPearson?s correlation.
The parameter search spacewas then explored iteratively using hill-climbinguntil reaching optimal Pearson?s correlation.
Thecriterion for assignment of training-test set pairswas by closeness of average character length.
Theoptimal training parameters are shown in Table 3.3.3 Parameters for N-gram Feature TrainingThe character n-gram feature representation vec-tors were trained while varying the parameters ofn-gram size, cluster size, and term frequency cut-offs for all models.
For the log linear skip-grammodels, our intuition is that a larger skip-gramcontext is needed than the 5 or 10 wide skip-gramsused to train word-based representations due to thesmaller term vocabulary and dependency betweenadjacent n-grams, so instead we trained models us-ing skip-gram widths of 25 or 50 terms.
Term fre-quency cut-offs were set to limit the model size,but also potentially serve as a regularization onthe resulting measure.
In detail, the following sub-lexical representation measures are used:?
Log linear skip-gram representations of char-acter 3- and 4-grams of size 1000 and 2000,respectively.
Trained on the Wiki8 corpus us-ing a skip gram window of size 25 and 50,and frequency cut-off of 5.450?
Brown clusters with size 1024 of character 4-grams using a frequency cut-off of 20.?
Brown clusters of character 3-, 4- and 5-grams with cluster sizes of resp.
1024, 2048and 1024.
The representations are trained onthe Wiki9 corpus with successively increas-ing frequency cut-offs of 20, 320 and 1200.?
LSI topic vectors based on character 4-gramsof size 2000.
Trained on the Wiki8 corpususing a frequency cut-off of 5.?
LSI topic vectors based on character 4-gramsof size 1000.
Trained on the Wiki9 corpususing a frequency cut-off of 80.3.4 Similarity Score RegressionThe final sentence pair similarity score is predictedby a Support Vector Regression (SVR) model witha Radial Basis (RBF) kernel (Vapnik et al., 1997).The model is trained on all the test data for the2013 STS shared task combined with all the trialand test data of the 2012 STS shared task.The combined dataset hence consists of about7,500 sentence pairs from nine different text cat-egories: five sets from the annotated data sup-plied to STS 2012, based on Microsoft ResearchParaphrase and Video description corpora (MSR-par and MSvid), statistical machine translationsystem output (SMTeuroparl and SMTnews), andsense mappings between OntoNotes and WordNet(OnWN); and four sets from the STS 2013 testdata: headlines (news headlines), SMT, OnWN,and FNWM (mappings of sense definitions fromFrameNet and WordNet).The SVR model was trained as a bagged classi-fier, that is, for each run, 100 regression modelswere trained with 80% of the samples and fea-tures of the original training set drawn with re-placement.
The outputs of all models were thenaveraged into a final prediction.
This bagged train-ing procedure adds extra regularization, which canreduce the instability of prediction accuracy be-tween different test data categories.The prediction pipeline was implemented withthe Scikit-learn software framework (Pedregosa etal., 2011), and the SVR models were trained withthe implementation?s default parameters: costpenalty (C) 1.0, margin () 0.1, and RBF precision(?)
1/|featurecount|.We were unable to improve the performanceover these defaults by cross validation parametersearch unless the models were trained for specifictext categories.
Consequently no parameter opti-mization was performed during training of the fi-nal systems.4 Submitted SystemsThe three submitted systems consist of one us-ing only the soft cardinality features described inSection 3.2 (NTNU-run1), one system using abaseline set of lexical measures and WordNet aug-mented similarity in addition to the new sublexicalrepresentation measures (NTNU-run2), and one(NTNU-run3) which combines the output fromthe other two systems by taking the mean of thetwo sets of predictions.
NTNU-run3 thus repre-sents a combination of the measures and methodsintroduced by NTNU-run1 and NTNU-run2.In addition to the sublexical feature measuresdescribed in Section 3.3, NTNU-run2 uses the fol-lowing baseline features adapted from the Take-Lab 2012 system submission (?Sari?c et al., 2012).?
Simple lexical features: Relative documentlength differences, number overlap, caseoverlap, and stock symbol named entityrecognition.?
Lemma and word n-gram overlap of orders 1-3, frequency weighted lemma and word over-lap, and WordNet augmented overlap.?
Cosine similarity between the summed wordrepresentation vectors from each sentence us-ing LSI models based on large corpora withor without frequency weighting.The specific measures used in the submittedsystems were found by training the regressionmodel on the STS 2012 shared task data and eval-uating on the STS 2013 test data.
We used a step-wise forward feature selection method by compar-ing mean (but unweighted) correlation on the fourtest categories in order to identify the subset ofmeasures to include in the final system.The system composes a feature set of similar-ity scores from these 20 baseline measures and thenine sublexical representation measures, and usesthese to train a bagged SVM regressor as describedin Section 3.4 in order to predict the final semanticsimilarity score for new sentence pairs.451NTNU-run1 NTNU-run2 NTNU-run3 BestDataset r rank r rank r rank rdeft-forum 0.4369 16 0.5084 2 0.5305 1 0.5305deft-news 0.7138 14 0.7656 6 0.7813 2 0.7850headlines 0.7219 17 0.7525 13 0.7837 1 0.7837images 0.8000 9 0.8129 4 0.8343 1 0.8343OnWN 0.8348 7 0.7767 20 0.8502 4 0.8745tweet-news 0.4109 33 0.7921 1 0.6755 13 0.7921mean 0.6531 20 0.7347 4 0.7426 2 0.7429weighted mean 0.6631 21 0.7491 4 0.7549 3 0.7610Table 4: Final evaluation results for the submitted systems.5 Results and DiscussionThe final evaluation results for the three submit-ted systems are shown in Table 4, where the right-most column (?Best?)
for comparison displays theperformance figures obtained by any of the 38 sys-tems on each dataset.The systems using sublexical representationbased measures show competitive performance,ranking third and fourth among the submitted sys-tems with a weighted mean correlation of ?0.75.They also produced the best result in four out ofthe six text categories in the evaluation dataset,with NTNU-run3 being the #1 system on deft-forum, headlines and images, #2 on deft-news, and#4 on OnWN.
It would thus have been the clearwinner if it had not been for its sub-par perfor-mance on the tweet-news dataset, which on theother hand is the category NTNU-run2 was thebest of all systems on.The system based solely on soft cardinality fea-tures, NTNU-run1, displays more modest perfor-mance ranking at 21stplace (of the in total 38 sub-mitted systems) with ?0.66 correlation.
This is abit surprising, since this method for obtaining fea-tures from pairs of texts was used successfully inother SemEval tasks such as cross-lingual textualentailment (Jimenez et al., 2012b) and student re-sponse analysis (Jimenez et al., 2013b).
Similarly,Croce et al.
(2012) used soft cardinality represent-ing text as a bag of dependencies (syntactic softcardinality) obtaining the best results in the typed-similarity task (Croce et al., 2013).From our results it can be noted that for mostcategories the sublexical representation measuresshow strong performance in NTNU-run2, with asignificantly better result for the combined sys-tem NTNU-run3.
This indicates that while the softcardinality features are weaker predictors overall,they are complimentary to the sublexical and lex-ical features of NTNU-run2.
It is also indicativethat this is not the case for the tweet-news cate-gory, where the text is more ?free form?
and lessnormative, so it would be expected that sublexicalapproaches should have stronger performance.AcknowledgementsThis work was made possible with the supportfrom Department of Computer and InformationScience, Norwegian University of Science andTechnology.Partha Pakray was 2013?2014 supported by anERCIM Alain Bensoussan Fellowship.The NTNU systems are partly based on codemade available by the Text Analysis and Knowl-edge Engineering Laboratory, Department ofElectronics, Microelectronics, Computer and In-telligent Systems, Faculty of Electrical Engineer-ing and Computing, University of Zagreb.452ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), Volume 1: Proceedings of the MainConference and the Shared Task: Semantic TextualSimilarity, pages 32?43, Atlanta, Georgia, USA,June.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media, Inc.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Danilo Croce, Valerio Storch, P. Annesi, and RobertoBasili.
2012.
Distributional compositional seman-tics and text similarity.
In 2012 IEEE Sixth Interna-tional Conference on Semantic Computing (ICSC),pages 242?249, September.Danilo Croce, Valerio Storch, and Roberto Basili.2013.
UNITOR-CORE TYPED: Combining textsimilarity and semantic filters through SV regres-sion.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 1: Pro-ceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 59?65, At-lanta, Georgia, USA, June.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JASIS,41(6):391?407.Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-bukh.
2010.
Text comparison using soft cardi-nality.
In Edgar Chavez and Stefano Lonardi, ed-itors, String Processing and Information Retrieval,volume 6393 of LNCS, pages 297?302.
Springer,Berlin, Heidelberg.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012a.
Soft cardinality: A parameterizedsimilarity function for text comparison.
In Proceed-ings of the Sixth International Workshop on Seman-tic Evaluation (SemEval 2012), Montr?eal, Canada,7-8 June.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012b.
Soft cardinality+ ML: Learning adap-tive similarity functions for cross-lingual textual en-tailment.
In Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation (SemEval 2012),Montr?eal, Canada, 7-8 June.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013a.
SOFTCARDINALITY-CORE: Im-proving text overlap with distributional measures forsemantic textual similarity.
In Second Joint Con-ference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Con-ference and the Shared Task: Semantic Textual Sim-ilarity, Atlanta, Georgia, USA, June.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013b.
SOFTCARDINALITY: Hierarchicaltext overlap for student response analysis.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 1: Proceedings of the MainConference and the Shared Task: Semantic TextualSimilarity, Atlanta, Georgia, USA, June.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Ph.D. thesis, Massachusetts Instituteof Technology.Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,Bj?orn Gamb?ack, and Andr?e Lynum.
2013.
NTNU-CORE: Combining strong features for semantic sim-ilarity.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 1: Pro-ceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 66?73, At-lanta, Georgia, USA, June.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA.Amos Tversky.
1977.
Features of similarity.
Psycho-logical Review, 84(4):327?352, July.Vladimir Vapnik, Steven E. Golowich, and AlexSmola.
1997.
Support vector method for functionapproximation, regression estimation, and signalprocessing.
In Michael C. Mozer, Michael I. Jordan,and Thomas Petsche, editors, Advances in NeuralInformation Processing Systems, volume 9, pages281?287.
MIT Press, Cambridge, Massachusetts.Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
TakeLab: Sys-tems for measuring semantic text similarity.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 441?448,Montr?eal, Canada, 7-8 June.453
