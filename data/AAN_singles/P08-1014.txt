Proceedings of ACL-08: HLT, pages 114?120,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAutomatic Editing in a Back-End Speech-to-Text SystemMaximilian Bisani Paul Vozila Olivier Divay Jeff AdamsNuance CommunicationsOne Wayside RoadBurlington, MA 01803, U.S.A.{maximilian.bisani,paul.vozila,olivier.divay,jeff.adams}@nuance.comAbstractWritten documents created through dictationdiffer significantly from a true verbatim tran-script of the recorded speech.
This posesan obstacle in automatic dictation systems asspeech recognition output needs to undergoa fair amount of editing in order to turn itinto a document that complies with the cus-tomary standards.
We present an approachthat attempts to perform this edit from recog-nized words to final document automaticallyby learning the appropriate transformationsfrom example documents.
This addresses anumber of problems in an integrated way,which have so far been studied independently,in particular automatic punctuation, text seg-mentation, error correction and disfluency re-pair.
We study two different learning methods,one based on rule induction and one based ona probabilistic sequence model.
Quantitativeevaluation shows that the probabilistic methodperforms more accurately.1 IntroductionLarge vocabulary speech recognition today achievesa level of accuracy that makes it useful in the produc-tion of written documents.
Especially in the medicaland legal domains large volumes of text are tradi-tionally produced by means of dictation.
Here docu-ment creation is typically a ?back-end?
process.
Theauthor dictates all necessary information into a tele-phone handset or a portable recording device andis not concerned with the actual production of thedocument any further.
A transcriptionist will thenlisten to the recorded dictation and produce a well-formed document using a word processor.
The goalof introducing speech recognition in this process isto create a draft document automatically, so that thetranscriptionist only has to verify the accuracy of thedocument and to fix occasional recognition errors.We observe that users try to spend as little time aspossible dictating.
They usually focus only on thecontent and rely on the transcriptionist to composea readable, syntactically correct, stylistically accept-able and formally compliant document.
For this rea-son there is a considerable discrepancy between thefinal document and what the speaker has said liter-ally.
In particular in medical reports we see differ-ences of the following kinds:?
Punctuation marks are typically not verbalized.?
No instructions on the formatting of the reportare dictated.
Section headings are not identifiedas such.?
Frequently section headings are only implied.
(?vitals are?
?
?PHYSICAL EXAMINATION:VITAL SIGNS:?)?
Enumerated lists.
Typically speakers usephrases like ?number one .
.
.
next number .
.
.
?,which need to be turned into ?1.
.
.
.
2. .
.
.
??
The dictation usually begins with a preamble(e.g.
?This is doctor Xyz ...?)
which does notappear in the report.
Similarly there are typ-ical phrases at the end of the dictation whichshould not be transcribed (e.g.
?End of dicta-tion.
Thank you.?)114?
There are specific standards regarding the useof medical terminology.
Transcriptionists fre-quently expand dictated abbreviations (e.g.?CVA?
?
?cerebrovascular accident?)
or oth-erwise use equivalent terms (e.g.
?nonictericsclerae??
?no scleral icterus?).?
The dictation typically has a more narrativestyle (e.g.
?She has no allergies.
?, ?I examinedhim?).
In contrast, the report is normally moreimpersonal and structured (e.g.
?ALLERGIES:None.
?, ?he was examined?).?
For the sake of brevity, speakers frequentlyomit function words.
(?patient?
?
?the pa-tient?, ?denies fever pain?
?
?he denies anyfever or pain?)?
As the dictation is spontaneous, disfluencies arequite frequent, in particular false starts, correc-tions and repetitions.
(e.g.
?22-year-old fe-male, sorry, male 22-year-old male?
?
?22-year-old male?)?
Instruction to the transcriptionist and so-callednormal reports, pre-defined text templates in-voked by a short phrase like ?This is a normalchest x-ray.??
In addition to the above, speech recognitionoutput has the usual share of recognition errorssome of which may occur systematically.These phenomena pose a problem that goes beyondthe speech recognition task which has traditionallyfocused on correctly identifying speech utterances.Even with a perfectly accurate verbatim transcript ofthe user?s utterances, the transcriptionist would needto perform a significant amount of editing to obtaina document conforming to the customary standards.We need to look for what the user wants rather thanwhat he says.Natural language processing research has ad-dressed a number of these issues as individual prob-lems: automatic punctuation (Liu et al, 2005),text segmentation (Beeferman et al, 1999; Matusovet al, 2003) disfluency repair (Heeman et al, 1996)and error correction (Ringger and Allen, 1996;Strzalkowski and Brandow, 1997; Peters and Drexel,2004).
The method we present in the following at-tempts to address all this by a unified transforma-tion model.
The goal is simply stated as transform-ing the recognition output into a text document.
Wewill first describe the general framework of learn-ing transformations from example documents.
Inthe following two sections we will discuss a rule-induction-based and a probabilistic transformationmethod respectively.
Finally we present experimen-tal results in the context of medical transcription andconclude with an assessment of both methods.2 Text transformationIn dictation and transcription management systemscorresponding pairs of recognition output and editedand corrected documents are readily available.
Theidea of transformation modeling, outlined in fig-ure 1, is to learn to emulate the transcriptionist.
Tothis end we first process archived dictations with thespeech recognizer to create approximate verbatimtranscriptions.
For each document this yields thespoken or source word sequence S = s1 .
.
.
sM ,which is supposed to be a word-by-word transcrip-tion of the user?s utterances, but which may actu-ally contain recognition errors.
The correspondingfinal reports are cleaned (removal of page headersetc.
), tagged (identification of section headings andenumerated lists) and tokenized, yielding the text ortarget token sequence T = t1...tN for each docu-ment.
Generally, the token sequence correspondsto the spoken form.
(E.g.
?25mg?
is tokenized as?twenty five milligrams?.)
Tokens can be ordinarywords or special symbols representing line breaks,section headings, etc.
Specifically, we representeach section heading by a single indivisible token,even if the section name consists of multiple words.Enumerations are represented by special tokens, too.Different techniques can be applied to learn and ex-ecute the actual transformation from S to T .
Twooptions are discussed in the following.With the transformation model at hand, a draftfor a new document is created in three steps.
Firstthe speech recognizer processes the audio recordingand produces the source word sequence S. Next,the transformation step converts S into the target se-quence T .
Finally the transformation output T isformatted into a text document.
Formatting is the115archiveddictationsrecognizenewdictationrecognizestoreootranscripts@Atrain//transcripttransformtransformationmodel//targetsGF //tokensformatarchiveddocumentstokenizeOOdraftdocumentmanualcorrectionfinaldocument@AstoreOOFigure 1: Illustration of how text transformation is inte-grated into a speech-to-text system.inverse of tokenization and includes conversion ofnumber words to digits, rendition of paragraphs andsection headings, etc.Before we turn to concrete transformation tech-niques, we can make two general statements aboutthis problem.
Firstly, in the absence of observa-tions to the contrary, it is reasonable to leave wordsunchanged.
So, a priori the mapping should bethe identity.
Secondly, the transformation is mostlymonotonous.
Out-of-order sections do occur but arethe exception rather than the rule.3 Transformation based learningFollowing Strzalkowski and Brandow (1997) andPeters and Drexel (2004) we have implementeda transformation-based learning (TBL) algorithm(Brill, 1995).
This method iteratively improves thematch (as measured by token error rate) of a col-lection of corresponding source and target token se-quences by positing and applying a sequence of sub-stitution rules.
In each iteration the source and tar-get tokens are aligned using a minimum edit dis-tance criterion.
We refer to maximal contiguoussubsequences of non-matching tokens as error re-gions.
These consist of paired sequences of sourceand target tokens, where either sequence may beempty.
Each error region serves as a candidate sub-stitution rule.
Additionally we consider refinementsof these rules with varying amounts of contiguouscontext tokens on either side.
Deviating from Petersand Drexel (2004), in the special case of an emptytarget sequence, i.e.
a deletion rule, we considerdeleting all (non-empty) contiguous subsequencesof the source sequence as well.
For each candi-date rule we accumulate two counts: the number ofexactly matching error regions and the number offalse alarms, i.e.
when its left-hand-side matchesa sequence of already correct tokens.
Rules areranked by the difference in these counts scaled bythe number of errors corrected by a single rule ap-plication, which is the length of the correspondingerror region.
This is an approximation to the to-tal number of errors corrected by a rule, ignoringrule interactions and non-local changes in the mini-mum edit distance alignment.
A subset of the top-ranked non-overlapping rules satisfying frequencyand minimum impact constraints are selected andthe source sequences are updated by applying the se-lected rules.
Again deviating from Peters and Drexel(2004), we consider two rules as overlapping if theleft-hand-side of one is a contiguous subsequenceof the other.
This procedure is iterated until no ad-ditional rules can be selected.
The initial rule setis populated by a small sequence of hand-craftedrules (e.g.
?impression colon??
?IMPRESSION:?
).A user-independent baseline rule set is generatedby applying the algorithm to data from a collec-tion of users.
We construct speaker-dependent mod-els by initializing the algorithm with the speaker-independent rule set and applying it to data from thegiven user.4 Probabilistic modelThe canonical approach to text transformation fol-lowing statistical decision theory is to maximize thetext document posterior probability given the spokendocument.T ?
= argmaxTp(T |S) (1)Obviously, the global model p(T |S) must be con-structed from smaller scale observations on the cor-116respondence between source and target words.
Weuse a 1-to-n alignment scheme.
This means eachsource word is assigned to a sequence of zero, oneor more target words.
We denote the target wordsassigned to source word si as ?i.
Each replacement?i is a possibly empty sequence of target words.
Asource word together with its replacement sequencewill be called a segment.
We constrain the set of pos-sible transformations by selecting a relatively smallset of allowable replacements A(s) to each sourceword.
This means we require ?i ?
A(si).
We usethe usual m-gram approximation to model the jointprobability of a transformation:p(S, T ) =M?i=1p(si, ?i|si?m+1, ?i?m+1, .
.
.
si?1, ?i?1)(2)The work of Ringger and Allen (1996) is similarin spirit to this method, but uses a factored source-channel model.
Note that the decision rule (1) isover whole documents.
Therefore we processescomplete documents at a time without prior segmen-tation into sentences.To estimate this model we first align all trainingdocuments.
That is, for each document, the tar-get word sequence is segmented into M segmentsT = ?1^ .
.
.^?M .
The criterion for this alignmentis to maximize the likelihood of a segment unigrammodel.
The alignment is performed by an expec-tation maximization algorithm.
Subsequent to thealignment step, m-gram probabilities are estimatedby standard language modeling techniques.
We cre-ate speaker-specific models by linearly interpolatingan m-gram model based on data from the user witha speaker-independent background m-gram modeltrained on data pooled from a collection of users.To select the allowable replacements for eachsource word we count how often each particular tar-get sequence is aligned to it in the training data.
Asource target pair is selected if it occurs twice ormore times.
Source words that were not observedin training are immutable, i.e.
the word itself is itsonly allowable replacement A(s) = {(s)}.
As anexample suppose ?patient?
was deleted 10 times, leftunchanged 105 times, replaced by ?the patient?
113times and once replaced by ?she?.
The word patientwould then have three allowables: A(patient) ={(), (patient), (the, patient)}.
)The decision rule (1) minimizes the document er-ror rate.
A more appropriate loss function is thenumber of source words that are replaced incor-rectly.
Therefore we use the following minimumword risk (MWR) decision strategy, which mini-mizes source word loss.T ?
= (argmax?1?A(si)p(?1|S))^ .
.
.^( argmax?M?A(sM )p(?M |S))(3)This means for each source sequence position wechoose the replacement that has the highest poste-rior probability p(?i|S) given the entire source se-quence.
To compute the posterior probabilities, firsta graph is created representing alternatives ?around?the most probable transform using beam search.Then the forward-backward algorithm is applied tocompute edge posterior probabilities.
Finally edgeposterior probabilities for each source position areaccumulated.5 Experimental evaluationThe methods presented were evaluated on a set ofreal-life medical reports dictated by 51 doctors.
Foreach doctor we use 30 reports as a test set.
Trans-formation models are trained on a disjoint set of re-ports that predated the evaluation reports.
The typ-ical document length is between one hundred andone thousand words.
All dictations were recordedvia telephone.
The speech recognizer works withacoustic models that are specifically adapted foreach user, not using the test data, of course.
Itis hard to quote the verbatim word error rate ofthe recognizer, because this would require a care-ful and time-consuming manual transcription of thetest set.
The recognition output is auto-punctuatedby a method similar in spirit to the one proposed byLiu et al (2005) before being passed to the transfor-mation model.
This was done because we consid-ered the auto-punctuation output as the status quoante which transformation modeling was to be com-pared to.
Neither of both transformation methodsactually relies on having auto-punctuated input.
Theauto-punctuation step only inserts periods and com-mas and the document is not explicitly segmentedinto sentences.
(The transformation step always ap-plies to entire documents and the interpretation of aperiod as a sentence boundary is left to the human117Table 1: Experimental evaluation of different text transformation techniques with different amounts of user-specificdata.
Precision, recall, deletion, insertion and error rate values are given in percent and represent the average of 51users, where the results for each user are the ratios of sums over 30 reports.user sections punctuation all tokensmethod docs precision recall precision recall deletions insertions errorsnone (only auto-punct) 0.00 0.00 66.68 71.21 11.32 27.48 45.32TBL SI 69.18 44.43 73.90 67.22 11.41 17.73 34.993-gram SI 65.19 44.41 73.79 62.26 18.15 12.27 36.09TBL 25 75.38 53.39 75.59 69.11 10.97 15.97 32.623-gram 25 80.90 59.37 78.88 69.81 11.50 12.09 28.87TBL 50 76.67 56.18 76.11 69.81 10.81 15.53 31.923-gram 50 81.10 62.69 79.39 70.94 11.31 11.46 27.76TBL 100 77.92 58.03 76.41 70.52 10.67 15.19 31.293-gram 100 81.69 64.36 79.35 71.38 11.48 10.82 27.123-gram without MWR 100 81.39 64.23 79.01 71.52 11.55 10.92 27.29reader of the document.)
For each doctor a back-ground transformation model was constructed using100 reports from each of the other users.
This is re-ferred to as the speaker-independent (SI) model.
Inthe case of the probabilistic model, all models were3-gram models.
User-specific models were createdby augmenting the SI model with 25, 50 or 100 re-ports.
One report from the test set is shown as anexample in the appendix.5.1 Evaluation metricThe output of the text transformation is aligned withthe corresponding tokenized report using a mini-mum edit cost criterion.
Alignments between sec-tion headings and non-section headings are not per-mitted.
Likewise no alignment of punctuation andnon-punctuation tokens is allowed.
Using the align-ment we compute precision and recall for sectionsheadings and punctuation marks as well as the over-all token error rate.
It should be noted that the so de-rived error rate is not comparable to word error ratesusually reported in speech recognition research.
Allmissing or erroneous section headings, punctuationmarks and line breaks are counted as errors.
Aspointed out in the introduction the reference texts donot represent a literal transcript of the dictation.
Fur-thermore the data were not cleaned manually.
Thereare, for example, instances of letter heads or pagenumbers that were not correctly removed when thetext was extracted from the word processor?s file for-mat.
The example report shown in the appendixfeatures some of the typical differences between theproduced draft and the final report that may or maynot be judged as errors.
(For example, the date ofthe report was not given in the dictation, the sec-tion names ?laboratory data?
and ?laboratory evalu-ation?
are presumably equivalent and whether ?sta-ble?
is preceded by a hyphen or a period in the lastsection might not be important.)
Nevertheless, thenumbers reported do permit a quantitative compari-son between different methods.5.2 ResultsResults are stated in table 1.
In the baseline setupno transformation is applied to the auto-punctuatedrecognition output.
Since many parts of the sourcedata do not need to be altered, this constitutes thereference point for assessing the benefit of transfor-mation modeling.
For obvious reasons precision andrecall of section headings are zero.
A high rate ofinsertion errors is observed which can largely be at-tributed to preambles.
Both transformation methodsreduce the discrepancy between the draft documentand the final corrected document significantly.
With100 training documents per user the mean token er-ror rate is reduced by up to 40% relative by the prob-abilistic model.
When user specific data is used, theprobabilistic approach performs consistently betterthan TBL on all accounts.
In particular it alwayshas much lower insertion rates reflecting its supe-118rior ability to remove utterances that are not typi-cally part of the report.
On the other hand the prob-abilistic model suffers from a slightly higher dele-tion rate due to being overzealous in this regard.In speaker independent mode, however, the deletionrate is excessively high and leads to inferior overallperformance.
Interestingly the precision of the au-tomatic punctuation is increased by the transforma-tion step, without compromising on recall, at leastwhen enough user specific training data is available.The minimum word risk criterion (3) yields slightlybetter results than the simpler document risk crite-rion (1).6 ConclusionsAutomatic text transformation brings speech recog-nition output much closer to the end result desiredby the user of a back-end dictation system.
It au-tomatically punctuates, sections and rephrases thedocument and thereby greatly enhances transcrip-tionist productivity.
The holistic approach followedhere is simpler and more comprehensive than a cas-cade of more specialized methods.
Whether or notthe holistic approach is also more accurate is not aneasy question to answer.
Clearly the outcome woulddepend on the specifics of the specialized methodsone would compare to, as well as the complexityof the integrated transformation model one applies.The simple models studied in this work admittedlyhave little provisions for targeting specific transfor-mation problems.
For example the typical length ofa section is not taken into account.
However, this isnot a limitation of the general approach.
We haveobserved that a simple probabilistic sequence modelperforms consistently better than the transformation-based learning approach.
Even though neither ofboth methods is novel, we deem this an importantfinding since none of the previous publications weknow of in this domain allow this conclusion.
Whilethe present experiments have used a separate auto-punctuation step, future work will aim to eliminateit by integrating the punctuation features into thetransformation step.
In the future we plan to inte-grate additional knowledge sources into our statis-tical method in order to more specifically addresseach of the various phenomena encountered in spon-taneous dictation.ReferencesBeeferman, Doug, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.Machine Learning, 34(1-3):177 ?
210.Brill, Eric.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21(4):543 ?
565.Heeman, Peter A., Kyung-ho Loken-Kim, andJames F. Allen.
1996.
Combining the detec-tion and correction of speech repairs.
In Proc.Int.
Conf.
Spoken Language Processing (ICSLP),pages 362 ?
365.
Philadelphia, PA, USA.Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, andMary Harper.
2005.
Using conditional randomfields for sentence boundary detection in speech.In Proc.
Annual Meeting of the ACL, pages 451 ?458.
Ann Arbor, MI, USA.Matusov, Evgeny, Jochen Peters, Carsten Meyer,and Hermann Ney.
2003.
Topic segmentationusing markov models on section level.
In Proc.IEEE Workshop on Automatic Speech Recogni-tion and Understanding (ASRU), pages 471 ?
476.IEEE, St. Thomas, U.S. Virgin Islands.Peters, Jochen and Christina Drexel.
2004.Transformation-based error correction for speech-to-text systems.
In Proc.
Int.
Conf.
Spoken Lan-guage Processing (ICSLP), pages 1449 ?
1452.Jeju Island, Korea.Ringger, Eric K. and James F. Allen.
1996.
A fertil-ity channel model for post-correction of continu-ous speech recognition.
In Proc.
Int.
Conf.
SpokenLanguage Processing (ICSLP), pages 897 ?
900.Philadelphia, PA, USA.Strzalkowski, Tomek and Ronald Brandow.
1997.A natural language correction model for contin-uous speech recognition.
In Proc.
5th Workshopon Very Large Corpora (WVVLC-5):, pages 168 ?177.
Beijing-Hong Kong.119AppendixA.ExampleofamedicalreportRecognitionoutput.Verticalspacewasaddedtofacilitatevisualcomparison.doctorsnamedictatingaprogressnoteonfirstnamelastnamepatientwithoutcomplaintshasbeenambulatingwithoutproblemsnochestpainchestpressurestillhassomeshortnessofbreathbutoverallhasimprovedsignificantlyvitalsignsarestablesheisafebrilelungsshowdecreasedbreathsoundsatthebaseswithbilateralralesandrhonchiheartisregularrateandrhythmtwooversixcrescendodecrescendomurmurattherightsternalborderabdomensoftnontendernondistendedextremitiesshowonepluspedaledemabilaterallyneurologicalexamisnonfocalwhitecountoffivepointsevenH.andH.elevenpointsixandthirtyfivepointfiveplateletcountofonefiftyfivesodiumonethirtysevenpotassiumthreepointninechlorideonehundredcarbondioxidethirtyninecalciumeightpointsevenglucoseninetyoneBUNandcreatininethirtysevenandonepointoneimpressionnumberoneCOPDexacerbationcontinuebreathingtreatmentsnumbertwoasthmaexacerbationcontinueoralprednisonenumberthreebronchitiscontinueLevaquinnumberfourhypertensionstablenumberfiveuncontrolleddiabetesmellitusimprovednumbersixgastroesophagealrefluxdiseasestablenumbersevencongestiveheartfailurestablenewparagraphpatientisinstableconditionandwillbedischargedtonamenursinghomeandwillbemonitoredcloselyonanoutpatientbasisprogressnoteAutomaticallygenerateddraft(speechrecognitionoutputaftertransformationandformatting)ProgressnoteSUBJECTIVE:Thepatientiswithoutcomplaints.Hasbeenambulatingwithoutproblems.Nochestpain,chestpressure,stillhassomeshortnessofbreath,butoverallhasimprovedsignificantly.PHYSICALEXAMINATION:VITALSIGNS:Stable.Sheisafebrile.LUNGS:Showdecreasedbreathsoundsatthebaseswithbilateralralesandrhonchi.HEART:Regularrateandrhythm2/6crescendodecrescendomurmurattherightsternalborder.ABDOMEN:Soft,nontender,nondistended.EXTREMITIES:Show1+pedaledemabilaterally.NEUROLOGICAL:Nonfocal.LABORATORYDATA:Whitecountof5.7,hemoglobinandhematocrit11.6and35.5,plateletcountof155,sodium137,potassium3.9,chloride100,CO239,calcium8.7,glucose91,BUNandcreatinine37and1.1.IMPRESSION:1.Chronicobstructivepulmonarydiseaseexacerbation.Continuebreathingtreatments.2.Asthmaexacerbation.Continueoralprednisone.3.Bronchitis.ContinueLevaquin.4.Hypertension.Stable.5.Uncontrolleddiabetesmellitus.Improved.6.Gastroesophagealrefluxdisease,stable.7.Congestiveheartfailure.Stable.PLAN:Thepatientisinstableconditionandwillbedischargedtonamenursinghomeandwillbemonitoredcloselyonanoutpatientbasis.Finalreportproducedbyahumantranscriptionistwithoutreferencetotheautomaticdraft.ProgressNoteDATE:July26,2005.HISTORYOFPRESENTILLNESS:Thepatienthasnocomplaints.Sheisambulatingwithoutproblems.Nochestpainorchestpressure.Shestillhassomeshortnessofbreath,butoverallhasimprovedsignificantly.PHYSICALEXAMINATION:VITALSIGNS:Stable.She?safebrile.LUNGS:Decreasedbreathsoundsatthebaseswithbilateralralesandrhonchi.HEART:Regularrateandrhythm.2/6crescendo,decrescendomurmurattherightsternalborder.ABDOMEN:Soft,nontenderandnondistended.EXTREMITIES:1+pedaledemabilaterally.NEUROLOGICALEXAMINATION:Nonfocal.LABORATORYEVALUATION:Whitecount5.7,H&H11.6and35.5,plateletcountof155,sodium137,potassium3.9,chloride100,co239,calcium8.7,glucose91,BUNandcreatinine37and1.1.IMPRESSION:1.Chronicobstructivepulmonarydiseaseexacerbation.Continuebreathingtreatments.2.Asthmaexacerbation.Continueoralprednisone.3.Bronchitis.ContinueLevaquin.4.Hypertension-stable.5.Uncontrolleddiabetesmellitus-improved.6.Gastroesophagealrefluxdisease-stable.7.Congestiveheartfailure-stable.ThepatientisinstableconditionandwillbedischargedtonameNursingHome,andwillbemonitoredonanoutpatientbasis.120
