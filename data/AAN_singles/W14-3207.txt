Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 51?60,Baltimore, Maryland USA, June 27, 2014.c?2014 Association for Computational LinguisticsQuantifying Mental Health Signals in TwitterGlen Coppersmith Mark Dredze Craig HarmanHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBalitmore, MD, USAAbstractThe ubiquity of social media provides arich opportunity to enhance the data avail-able to mental health clinicians and re-searchers, enabling a better-informed andbetter-equipped mental health field.
Wepresent analysis of mental health phe-nomena in publicly available Twitter data,demonstrating how rigorous application ofsimple natural language processing meth-ods can yield insight into specific disor-ders as well as mental health writ large,along with evidence that as-of-yet undis-covered linguistic signals relevant to men-tal health exist in social media.
We presenta novel method for gathering data fora range of mental illnesses quickly andcheaply, then focus on analysis of four inparticular: post-traumatic stress disorder(PTSD), depression, bipolar disorder, andseasonal affective disorder (SAD).
We in-tend for these proof-of-concept results toinform the necessary ethical discussion re-garding the balance between the utility ofsuch data and the privacy of mental healthrelated information.1 IntroductionWhile mental health issues pose a significanthealth burden on the general public, mental healthresearch lacks the quantifiable data available tomany physical health disciplines.
This is partlydue to the complexity of the underlying causesof mental illness and partly due to longstandingsocietal stigma making the subject all but taboo.Lack of data has hampered mental health researchin terms of developing reliable diagnoses and ef-fective treatment for many disorders.
Moreover,population-level analysis via traditional methodsis time consuming, expensive, and often comeswith a significant delay.In contrast, social media is plentiful and hasenabled diverse research on a wide range of top-ics, including political science (Boydstun et al.,2013), social science (Al Zamal et al., 2012), andhealth at an individual and population level (Pauland Dredze, 2011; Dredze, 2012; Aramaki et al.,2011; Hawn, 2009).
Of the numerous health top-ics for which social media has been considered,mental health may actually be the most appropri-ate.
A major component of mental health researchrequires the study of behavior, which may be man-ifest in how an individual acts, how they com-municate, what activities they engage in and howthey interact with the world around them includ-ing friends and family.
Additionally, capturingpopulation level behavioral trends from Web datahas previously provided revolutionary capabilitiesto health researchers (Ayers et al., 2014).
Thus,social media seems like a perfect fit for study-ing mental health in both individual and overalltrends in the population.
Such topics have alreadybeen the focus of several studies (Coppersmith etal., 2014; De Choudhury et al., 2014; De Choud-hury et al., 2013d; De Choudhury et al., 2013b;De Choudhury et al., 2013c; Ayers et al., 2013).What can we expect to learn about mental healthby studying social media?
How does a service likeTwitter inform our knowledge in this area?
Nu-merous studies indicate that language use, socialexpression and interaction are telling indicators ofmental health.
The well-known Linguistic InquiryWord Count (LIWC), a validated tool for the psy-chometric analysis of language data (Pennebakeret al., 2007), has been repeatedly used to studylanguage associated with all types of disorders(Resnik et al., 2013; Alvarez-Conrad et al., 2001;Tausczik and Pennebaker, 2010).
Furthermore, so-cial media is by nature social, which means thatsocial patterns, a critical part of mental health andillness, may be readily observable in raw Twitterdata.
Thus, Twitter and other social media provide51a unique quantifiable perspective on human behav-ior that may otherwise go unobserved, suggestingit as a powerful tool for mental health researchers.The main vehicle for studying mental health insocial media has been the use of surveys, e.g.,depression battery (De Choudhury, 2013) or per-sonality test (Schwartz et al., 2013), to deter-mine characteristics of a user coupled with analyz-ing their corresponding social media data.
Workin this area has mostly focused on depression(De Choudhury et al., 2013d; De Choudhury et al.,2013b; De Choudhury et al., 2013c), and the num-ber of users is limited by those that can completethe appropriate survey.
For example, De Choud-hury et al.
(2013d) solicited Twitter users to takethe CES-D and to share their public Twitter pro-file, analyzing linguistic and behavioral patterns.While this type of study has produced high qual-ity data, it is limited in size (by survey respon-dents) and scope (to diagnoses which have a bat-tery amenable to administration over the internet).In this paper we examine a range of mentalhealth disorders using automatically derived sam-ples from large amounts of Twitter data.
Ratherthan rely on surveys, we automatically identifyself-expressions of mental illness diagnoses andleverage these messages to construct a labeled dataset for analysis.
Using this dataset, we make thefollowing contributions:?
We demonstrate the effectiveness of our au-tomatically derived data by showing that sta-tistical classifiers can differentiate users withfour different mental health disorders: de-pression, bipolar, post traumatic stress disor-der and seasonal affective disorder.?
We conduct a LIWC analysis of each dis-order to measure deviations in each illnessgroup from a control group, replicating pre-vious findings for depression and providingnew findings for bipolar, PTSD and SAD.?
We conduct an open-vocabulary analysis thatcaptures language use relevant to mentalhealth beyond what is captured with LIWC.Our results open the door to a range of large scaleanalysis of mental health issues using Twitter.2 Related WorkFor a good retrospective and prospective sum-mary of the role of social media in mental healthresearch, we refer the reader to De Choudhury(2013).
De Choudhury identifies ways in whichNLP has and can be used on social media data toproduce what the relevant mental health literaturewould predict, both at an individual level and apopulation level.
She proceeds to identify waysin which these types of analyses can be used inthe near and far term to influence mental healthresearch and interventions alike.Differences in language use have been observedin the personal writing of students who scorehighly on depression scales (Rude et al., 2004),forum posts for depression (Ramirez-Esparza etal., 2008), self narratives for PTSD (He et al.,2012; D?Andrea et al., 2011; Alvarez-Conrad etal., 2001), and chat rooms for bipolar (Krameret al., 2004).
Specifically in social media, dif-ferences have previously been observed betweendepressed and control groups (as assessed byinternet-administered batteries) via LIWC: de-pressed users more frequently use first person pro-nouns (Chung and Pennebaker, 2007) and morefrequently use negative emotion words and angerwords on Twitter, but show no differences in posi-tive emotion word usage (Park et al., 2012).
Simi-larly, an increase in negative emotion and first per-son pronouns, and a decrease in third person pro-nouns, (via LIWC) is observed, as well as manymanifestations of literature findings in the patternof life of depressed users (e.g., social engagement,demographics) (De Choudhury et al., 2013d).
Dif-ferences in language use in social media via LIWChave also been observed between PTSD and con-trol groups (Coppersmith et al., 2014).For population-level analysis, surveys such asthe Behavioral Risk Factor Surveillance System(BRFSS) are conducted via telephone (Centersfor Disease Control and Prevention (CDC), 2010).Some of these surveys cover relatively few par-ticipants (often in the thousands), have significantcost, and have long delays between data collec-tion and dissemination of the findings.
However,De Choudhury et al.
(2013c) presents a promisingpopulation-level analysis of depression that high-lights the role of NLP and social media.3 DataAll data we obtain is public, posted between2008 and 2013, and made available from Twittervia their application programming interface (API).Specifically, this does not include any data that has52Genuine Statements of DiagnosisIn loving memory my mom, she was only 42, I was 17 & taken away from me.
I was diagnosed with having P.T.S.D LINKSo today I started therapy, she diagnosed me with anorexia, depression, anxiety disorder, post traumatic stress disorder andwants me to@USER The VA diagnosed me with PTSD, so I can?t go in that direction anymoreI wanted to share some things that have been helping me heal lately.
I was diagnosed with severe complex PTSD and... LINKDisingenuous Statements of Diagnosis?I think I?m I?m diagnosed with SAD.
Sexually active disorder?
-anonymousLOL omg my bro the ?psychologist?
just diagnosed me with seasonal ADHD AHAHAHAAAAAAAAAAA IM DYING.The winter blues: Yesterday I was diagnosed with seasonal affective disorder.
Now, this sounds a lot more dramat... LINKTable 1: Examples found via regular expression keyword search for diagnosis tweets.been marked as ?private?
by the author or any di-rect messages.Diagnosed Group We seek users who publiclystate that they have been diagnosed with variousmental illnesses.
Users may make such a state-ment to seek support from others in their socialnetwork, to fight the taboo of mental illness, orperhaps as an explanation of some of their behav-ior.
Tweets were obtained using regular expres-sions on a large multi-year health related collec-tion, e.g.
?I was diagnosed with X.?
We searchedfor four conditions: depression, bipolar disorder,post traumatic stress disorder (PTSD) and sea-sonal affective disorder (SAD).
The matched diag-nosis tweets were manually labeled as to whetherthe tweet contained a genuine statement of a men-tal health diagnosis.
Table 1 shows examples ofboth genuine statements of diagnosis and disin-genuous statements (often jokes or quotes).Next, we retrieved the most recent tweets (upto 3200) for each user with a genuine diagnosistweet.
We then filtered the users to remove thosewith fewer than 25 tweets and those whose tweetswere not at least 75% in English (measured usingthe Compact Language Detector1).
These filter-ing steps left us with users that were consideredpositive examples.
Table 2 indicates the numberof users and tweets found for each of the mentalhealth categories examined.
We manually exam-ined and annotated only half the diagnosis state-ments for depression ?
indicating there are likely800-900 depression users available via these auto-matic methods from our collection, compared tothe 117 obtained via the methods of De Choud-hury et al.
(2013d).
Additionally, we emphasizethe low cost and effort of our automated effortas compared to their crowdsourced survey meth-1https://code.google.com/p/cld2/ods.
The difference in collection methods alsosuggests that the two have a reasonable chance ofbeing complementary.
This is especially signif-icant when considering disorders with lower in-cidence rates than depression (arguably the high-est), where respondents to crowdsourced surveysor self-stated diagnoses alike are rare.This method is similar in spirit to that of DeChoudhury et al.
(2013c), where they inferreda tweet-level classifier for depression from user-level labels (specifically, tweets from the past threemonths from users scoring highly on CES-D forthe positive class and conversely for the negative).Control Group To build models for analysisand to validate the data, we also need a sample ofthe general population to use as an approximationof community controls.
We follow a similar pro-cess: randomly select 10k usernames from a listof Twitter users who posted to a separate randomhistorical collection within a selected two weekwindow, downloaded the 3200 most recent tweetsfrom these users, and apply our two filters: at least25 tweets and 75% English.
This yields a controlgroup of 5728 random users, whose 13.7 milliontweets were used as negative examples.Caveats Our method for finding users withmental health diagnoses has significant caveats: 1)the method may only capture a subpopulation ofeach disorder (i.e., those who are speaking pub-licly about what is usually a very private mat-ter), which may not truly represent all aspects ofthe population as a whole.
2) This method inno way verifies whether this diagnosis is genuine(i.e., people are not always truthful in self-reports).However, given the stigma often associated withmental illness, it seems unlikely users would tweetthat they are diagnosed with a condition they donot have.
3) The control group is likely contami-53Match Users TweetsBipolar 6k 394 992kDepression 5k 441 1.0mPTSD 477 244 573kSAD 389 159 421kControl 10k 5728 13.7mTable 2: Number of users matching the diagnosis regularexpression, users labeled with genuine diagnoses and tweetsretrieved from diagnosed users for each mental health condi-tion.nated by the presence of users that are diagnosedwith the various conditions investigated.
We makeno attempt to remove these users, and if we as-sume that the prevalence of each disorder in thegeneral population is similar in our control groups,we likely have hundreds of such diagnosed userscontaminating our control training data.
4) Twitterusers are not an entirely representative sample ofthe population as a whole.
Despite these caveats,we find that this method yielded promising resultsas discussed in the next sections.Comorbidity Since some of these disordershave high comorbidity, there are some users inmore than one class (e.g., those that state a diagno-sis for PTSD and depression): Bipolar and depres-sion have 19 users in common (4.8% of the bipo-lar users, 4.3% of the depression users), PTSD anddepression share 10 (4.0% of PTSD, 2.2% of de-pression), and bipolar and PTSD share 9 (2.2% ofbipolar, 3.6% of PTSD).
Two users state diagnosisof bipolar, PTSD and depression (less than 1% ofeach set).
No users stated diagnoses of both SADand any other condition investigated.4 MethodsWe quantify various aspects of each user?s lan-guage usage and pattern of life via automatedmethods, extracting features for subsequent ma-chine learning.
We use these to (1) replicate pre-vious findings, (2) build classifiers to separate di-agnosed from control users, and (3) introspect onthose classifiers.
Introspection here shows us whatquantified signals in the content the classifiers basetheir decision on, and thus we can gain intuitionabout what signals are present in the content rele-vant to mental health.4.1 Linguistic Inquiry Word Count (LIWC)LIWC provides clinicians with a tool for gather-ing quantitative data regarding the state of a pa-tient from the patient?s writing (Pennebaker et al.,2007).
Previous work has found signal in the ?pos-itive affect?
and ?negative affect?
categories of theLIWC when applied to social media (includingTwitter), so we examine their correlations sepa-rately, as well as in the context of other LIWCcategories (De Choudhury et al., 2013a).
In all,we examine some of the LIWC categories directly(Swear, Anger, PosEmo, NegEmo, Anx) and com-bine pronoun classes by linguistic form: I and Weclasses are combined to form Pro1, You becomesPro2 and SheHe and They become Pro3.
Each ofthese classes provides one feature used by subse-quent machine learning and our other analyses.4.2 Language Models (LMs)Language models are commonly used to estimatehow likely a given sequence of words is.
Gener-ally, an n-gram language model refers to a modelthat examines strings of up to n words long.
Thisis less than ideal for applications in social me-dia: spelling errors, shortenings, space removal,and other aspects of social media data (especiallyTwitter) confounds many traditional word-basedapproaches.
Thus, we employ two LMs, first atraditional 1-gram LM (ULM) that examines theprobability of each whole word.
Second, a char-acter 5-gram LM (CLM) to examine sequences ofup to 5 characters.LMs model the likelihood of sequences fromtraining data.
In our case, we build one of eachmodel from the positive class (tweets from oneclass of diagnosed users ?
e.g., PTSD), yield-ing ULM+and CLM+.
We also build one ofeach model from the negative class (control users),yielding ULM?and CLM?.
We score each tweetby computing these probabilities and classifying itaccording to which model has a higher probability(e.g., for a given tweet, is ULM+> ULM??
).4.3 Pattern of Life AnalyticsFor brevity, we only briefly discuss the pattern oflife analytics, since they do not depend on sig-nificant NLP.
They examine how correlates foundto be significant in the mental health literaturemay manifest and be measured in social mediadata.
These are all imperfect proxies for the find-ings from the literature, but our experiments willdemonstrate that they do collectively provide in-formation relevant to mental health.For each of the following analytics we extractone feature to use in subsequent machine learn-ing.
Social engagement has been correlated with54??????????????????????????????????
??????????????????????????????????????
????
??????
???????????????????????
?????
???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
?0.000.050.100.15 Pro1* * Pro2* Pro3* Swear* Anger* PosEmo NegEmo* Anxiety****00.0050.010.015Figure 1: Box and whiskers plot of proportion of tweets each user has (y-axis) matching various LIWC categories.
Eachbar represents one LIWC category for one condition ?
PTSD in purple, depression in blue, SAD in orange, bipolar in red andcontrol in gray.
Anxiety occurs an order of magnitude less often than the others, so its proportion is on the right y-axis (and thusnot comparable to the others).
Statistically significant deviations from control users are denoted by asterisks.positive mental health outcomes (Greetham et al.,2011; Berkman et al., 2000; Organization, 2001;De Choudhury et al., 2013d), which is difficultto measure directly so we examine various waysin which this may be manifest in a user?s tweetstream: Tweet rate measures how often a twit-ter user posts (a measure of overall engagementwith this social media platform) and Proportionof tweets with @mentions measures how oftena user posts ?in conversation?
(for lack of betterterms) with other users.
Number of @mentions isa measure of how often the user in question en-gages other users, while Number of self @men-tions is a measure of how often the user respondsto mentions of themselves (since users rarely in-clude their own username in a tweet).
To estimatethe size of a user?s social network, we calculateNumber of unique users @mentioned and Numberof users @mentioned at least 3 times, respectively.For each of the following analytics, we calcu-late the proportion of a user?s tweets that the ana-lytic finds evidence in: Insomnia and sleep distur-bance is often a symptom of mental health disor-ders (Weissman et al., 1996; De Choudhury et al.,2013d), so we calculate the proportion of tweetsthat a user makes between midnight and 4am ac-cording to their local timezone.
Exercise hasalso been correlated with positive mental healthoutcomes (Penedo and Dahn, 2005; Callaghan,2004), so we examine tweets mentioning one of asmall set of exercise-related terms.
We also use anEnglish sentiment analysis lexicon from Mitchellet al.
(2013) to score individual tweets accordingto the presence and valence of sentiment words.We apply no thresholds, so any tweet with a senti-ment score above 0 was considered positive, below0 was considered negative, and those with score 0were considered to have no sentiment.
Thus weuse the proportion of Insomnia, Exercise, PositiveSentiment and Negative Sentiment tweets as fea-tures in subsequent machine learning and analysis.5 ResultsWe present three types of experiments to evalu-ate the quality and character of these data, and todemonstrate some quantifiable mental health sig-nals in Twitter.
First, we validate our method forobtaining data by replicating previous findings us-ing LIWC.
Next, we build classifiers to distinguisheach group from the control group, demonstratingthat there is useful signal in the language of eachgroup, and compare these classifiers.
Finally, weanalyze the correlations between our analytics andclassifiers to uncover relationships between themand derive insight into quantifiable and relevantmental health signals in Twitter.Validation First, we provide some validationfor our novel method for gathering samples.
Wedemonstrate that language use, as measured byLIWC, is statistically significantly different be-tween control and diagnosed users.
Figure 1shows the proportion of tweets from each userthat scores positively on various LIWC categories(i.e., have at least one word from that category).Box-and-whiskers plots (Tukey, 1977)2summa-rize a distribution of observations and ease com-2For a modern implementation see Wickham (2009).55False Alarm: 0.1 0.2Bipolar 0.64 0.82Depression 0.48 0.68PTSD 0.67 0.81SAD 0.42 0.65Figure 2: ROC curves for separating diagnosed from con-trol users, compared across disorders: bipolar in red, depres-sion in blue, PTSD in purple, SAD in orange.
The preci-sion (diagnosed, correctly labeled) for each disorder at falsealarm (control, labeled as diagnosed) rates of 10% and 20%are shown to the right of the ROC curve.
Chance performanceis indicated by the dotted black line.parison between them (here, each observation isthe proportion of a user?s tweets that score posi-tively on LIWC).
The median of the distributionis the black horizontal line in the middle of thebar, the bar covers the inter quartile range (where50% of the observations lie), the whiskers are arobust estimate of the extent of the data, with out-liers plotted as circles beyond the whiskers.
Anapproximation of statistical significance is indi-cated by the pinched in notches on each bar.
Ifthe notches on the bars do not overlap, the dif-ferences between those distributions is different(?<0.05, 95% confidence interval).
Each bar iscolored according to diagnosis, and each groupof 5 bars notes the scores for one LIWC cat-egory.
Differences that reach statistical signifi-cance from the control group are noted with as-terisks (e.g., Pro1, Swear, Anger, NegEmo andAnxiety are statistically significantly different forthe depression group).
Importantly, this repli-cates previous findings of significant differencesbetween depressed users (according to an internet-administered diagnostic battery): significant in-creases are expected in NegEmo, Anger, Pro1 andPro3 and no change in PosEmo, given all previouswork (Park et al., 2012; Chung and Pennebaker,2007; De Choudhury et al., 2013d).
We repli-cate all these findings except the increase in Pro3(which only De Choudhury et al.
(2013d) found),which validates our data collection methods.Classification We next explore the ability ofthe various analytics to separate diagnosed fromcontrol users and assess performance on a leave-one-out cross-validation task.
We train a log lin-ear classifier on the features described in ?4 usingscikit-learn (Pedregosa et al., 2011).Bipolar DepressionPTSD SADFigure 3: ROC curves of performance of individual analyt-ics for each disorder: LIWC in blue, pattern of life in yellow,CLM in red, ULM in green, all in black.
Chance performanceis indicated by the dotted black line.The receiver operating characteristic (ROC)curves in Figures 2 and 3 demonstrate perfor-mance of the various classifiers at the task of sepa-rating diagnosed from control groups.
In all cases,the correct detections (or hits) are on the y-axisand the false detections (or false alarms) are onthe x-axis.
Figure 2 compares performance acrossdiagnoses, one line per disorder.Figure 3 shows one plot per mental health con-dition, with the performance of the various an-alytics, individually and in concert as individualROC curves.
A few trends emerge ?
1) All an-alytics show some ability to separate the classes,indicating they are finding useful signals.
2) TheLMs provide superior performance to the other an-alytics, indicating there are more signals presentin the language than are captured by LIWC andpattern-of-life analytics.
For readability we do notshow the performance of all combinations of an-alytics, but they perform as expected: any set ofthem perform equal to or better than their indi-vidual components.
Taken together, this indicatesthat there is information relevant to separating di-agnosed users from controls in all the analyticsdiscussed here.
Furthermore, this highlights thatthere remains significant signals to be uncoveredand understood in the language of social media.These trends also allow us to compare the dis-orders as manifest in language usage, though this56tends to raise more questions than it answers.
Gen-erally, the pattern-of-life analytics and LIWC areon par, but this is decidedly not true for depres-sion, where pattern-of-life seems to perform espe-cially poorly, and for SAD, where pattern-of-lifeseems to perform especially well.
This indicatesthat the depression users have patterns-of-life thatlook more similar to the controls than is the casefor the other disorders (perhaps especially surpris-ing given the inclusion of the sentiment lexicon)and that there may be significant correlation be-tween pattern-of-life factors and SAD.5.1 Analytic IntrospectionTo examine correlations between the analytics andthe linguistic content they depend on, we scoreda random subset of 1 million tweets from controlusers with each of the linguistic analytics, and plottheir Pearson?s correlation coefficients (r) in Fig-ure 4.
A simple overlap of wordlists is not suf-ficient to assess the true utility of these methodssince it does not take into account the frequencyof occurrence of each word, nor the correlation be-tween these words in real data (e.g., does a classi-fier based on the LIWC category Swear provideredundant information to the sentiment analysis).Each row and column in Figure 4 represents one ofthe 17 analytics, in the same order.
Colors denoteBonferroni-corrected Pearson?s r for statisticallysignificant correlations between the analytic on therow and column.
Correlations that do not reachstatistical significance are in aquamarine (corre-sponding to r=0).
Excluded for brevity is a sanitycheck of a ?2test between the analytics to assertthey were scoring significantly differently.The strong correlations between the variousLIWC analytics, notably Swear, Anger andNegEmo, likely indicates that the analytics aretriggered by the same word(s) ?
in this case pro-fanity.
Similarly for LIWC?s PosEmo and the sen-timent lexicon ?
?happy?
for example.
The corre-lation between CLM for various diagnoses is par-ticularly intriguingly, as it is in line with knownpatterns of comorbidity: major depressive disor-der, PTSD, and bipolar all have observed comor-bidity (Brady et al., 2000; Campbell et al., 2007;McElroy et al., 2001) while SAD is currently con-sidered a specifier of major depressive disorder orbipolar disorder (American Psychiatric Associa-tion, 2013; Lurie et al., 2006), without publishedfindings indicating comorbidity.
Indeed our smallFigure 4: Pearson?s r correlations between various analyt-ics, color indicates the strength of statistically significant cor-relations, or 0 (aquamarine) otherwise.
Bonferroni corrected,each comparison is significant only if ?<0.0002).
Rows andcolumns represent the analytics in the same order, so the di-agonal is self-correlation.sample dataset follows the same trends, wherewe observed users with multiple diagnoses existwithin depression, PTSD, and bipolar, but noneexist with SAD.
The correlation observed is toolarge to be solely attributed to those users sharedbetween the groups, though (correlations at mostr = 0.05 would be attributable to that alone).
Fur-thermore, when taken in combination with the dif-ferent patterns exhibited by the groups as seen inFigure 1, this correlation is not solely attributableto LIWC categories either.
At its core, these cor-relations seem to suggest that similar languageis employed by users diagnosed with these occa-sionally comorbid disorders, and dissimilar lan-guage by users with SAD.
This should be taken asmerely suggestive of the type of analysis one coulddo, though, since the literature does not present astrong and clear prediction for the comorbidity andexhibited symptoms (to include language use).Interestingly, the lack of (or negative) correla-tion between most of the analytics again highlightsthe complexity of the mental illnesses and the di-vergent signals it presents.
Additionally, the lackof correlation between ULM and the other modelsis to be expected, since they are basing their scoreson significantly more words (or different signals asis the case for CLM).
Each one of these analytics ishighly imperfect, and often give contradictory ev-idence, but when combined, the machine learningalgorithms are able to sort through the conflictingsignals with some success.57Analytic Example Tweet TextBipolar LM I?m insecure because being around your ex of 4 years little sister, makes me feel a slight bit uncom-fortable.
Ok.Depression LM Pain has a weird way of working.
You?re still the same person from before the pain, but that person isunderneath & doesn?t come out.PTSD LM Don?t wanna get out my bed but I really need to get up & prepare myself for workSentiment(+) NAME is absolutely unbelievable, he just gets better and better every time I see him.
The best play inthe world, no doubt about it.Sentiment(-) I hate losing people in my life.
I try so hard to not let it happenPosEmo Wowee...that was a hectic day... Got more done than expected but so glad to be in bed now.
Gratefulfor my supportive husband & loving poochFunctioning if i had a dollar for all the grammatical errors ive ever typed, my college tuition, book cost, and dormrent would be paid in fullNegEmo My tooth hurts, my neck hurts, my mouth hurts, my toungue hurts, my head hurts...kill me now.Anx don?t stress over someone who is going to stress over you..Anger Ugly n arrogant sums everytin up.shdnt hv ffd her seffTable 3: Example high scoring tweets from each analytic.6 ConclusionWe demonstrate quantifiable signals in Twitterdata relevant to bipolar disorder, major depres-sive disorder, post-traumatic-stress disorder andseasonal affective disorder.
We introduce a novelmethod for automatic data collection and validateits veracity by 1) replicating observations of sig-nificant differences between depressed and controluser groups and 2) constructing classifiers capa-ble of separating diagnosed from control users foreach disorder.
This data allows us to demonstrateequivalent differences in language use (accordingto LIWC) for bipolar, PTSD, and SAD.
Further-more, we provide evidence that more informationrelevant to mental health is encoded in languageuse in social media (above and beyond that cap-tured by methods based on the mental health lit-erature).
By examining correlations between thevarious analytics investigated, we provide someinsight into what quantifiable linguistic informa-tion is captured by our classifiers.
We finallydemonstrate the utility of examining multiple dis-orders simultaneously and other larger analyses,difficult or impossible with other methods.Crucially, we expect that these novel data col-lection methods can provide complementary infor-mation to existing survey-based methods, ratherthan supplant them.
For many disorders rarerthan depression (which has comparatively high in-cidence rates), we suspect that finding any datawill be a challenge, in which case combiningthese methods with the existing survey collectionmethods may be the best way to obtain sufficientamounts of data for statistical analyses.Since the LMs take more information into ac-count when modeling the language usage of di-agnosed and control users, it is unsurprising thatthey outperform LIWC and pattern-of-life analy-ses alone, but this is evidence of as-of-yet undis-covered linguistic differences between diagnosedand control users for all disorders investigated.Uncovering and interpreting these signals can bebest accomplished through collaboration betweenNLP and mental health researchers.Naturally, some caveats come with these re-sults: while identifying genuine self-statements ofdiagnosis in Twitter works well for some condi-tions, others exist for which there were few orno diagnoses stated.
For Alzheimer?s, the demo-graphic with the majority of diagnoses does notfrequently use Twitter (or likely any social me-dia).
Eating disorders are also elusive via thismethod, though related automatic methods (e.g.,using disorder-related hashtags) may address this.Finally, those willing to publicly reveal a mentalhealth diagnosis may not be representative of thepopulation suffering from that mental illness.All these experiments, taken together, indicatethat there are a diverse set of quantifiable signalsrelevant to mental health observable in Twitter.They indicate that individual- and population-levelanalyses can be made cheaper and more timelythan current methods, yet there remains as-of-yetuntapped information encoded in language use ?promising a rich collaboration between the fieldsof natural language processing and mental health.Acknowledgments: The authors would like tothank Kristy Hollingshead for thoughtful com-ments and contributions throughout this research.58ReferencesFaiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of Twitter users from neighbors.
InProceedings of the International AAAI Conferenceon Weblogs and Social Media (ICWSM).Jennifer Alvarez-Conrad, Lori A. Zoellner, andEdna B. Foa.
2001.
Linguistic predictors of traumapathology and physical health.
Applied CognitivePsychology, 15(7):S159?S170.American Psychiatric Association.
2013.
DiagnosticStatistical Manual 5.
American Psychiatric Associ-ation.Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.2011.
Twitter catches the flu: Detecting influenzaepidemics using twitter.
In Empirical Natural Lan-guage Processing Conference (EMNLP).John W. Ayers, Benjamin M. Althouse, Jon-PatrickAllem, J. Niels Rosenquist, and Daniel E. Ford.2013.
Seasonality in seeking mental health infor-mation on google.
American journal of preventivemedicine, 44(5):520?525.John W. Ayers, Benjamin M. Althouse, and MarkDredze.
2014.
Could behavioral medicine lead theweb data revolution?
Journal of the American Med-ical Association (JAMA), February 27.Lisa F. Berkman, Thomas Glass, Ian Brissette, andTeresa E. Seeman.
2000.
From social integrationto health: Durkheim in the new millennium?
SocialScience & Medicine, 51(6):843?857, September.Amber Boydstun, Rebecca Glazier, Timothy Jurka, andMatthew Pietryka.
2013.
Examining debate effectsin real time: A report of the 2012 React Labs: Ed-ucate study.
The Political Communication Report,23(1), February.
[Online; accessed 25-February-2014].Kathleen T. Brady, Therese K. Killeen, Tim Brewerton,and Sylvia Lucerini.
2000.
Comorbidity of psy-chiatric disorders and posttraumatic stress disorder.Journal of Clinical Psychiatry.Patrick Callaghan.
2004.
Exercise: a neglected inter-vention in mental health care?
Journal of Psychi-atric and Mental Health Nursing, 11:476?483.Duncan G. Campbell, Bradford L. Felker, Chuan-FenLiu, Elizabeth M. Yano, JoAnn E. Kirchner, DominChan, Lisa V. Rubenstein, and Edmund F. Chaney.2007.
Prevalence of depression-PTSD comorbidity:Implications for clinical practice guidelines and pri-mary care-based interventions.
Journal of GeneralInternal Medicine, 22(6):711?718.Centers for Disease Control and Prevention (CDC).2010.
Behavioral risk factor surveillance systemsurvey data.Cindy Chung and James Pennebaker.
2007.
The psy-chological functions of function words.
Social com-munication, pages 343?359.Glen A. Coppersmith, Craig T. Harman, and MarkDredze.
2014.
Measuring post traumatic stressdisorder in Twitter.
In Proceedings of the Interna-tional AAAI Conference on Weblogs and Social Me-dia (ICWSM).Wendy D?Andrea, Pearl H. Chiu, Brooks R. Casas,and Patricia Deldin.
2011.
Linguistic predictors ofpost-traumatic stress disorder symptoms following11 September 2001.
Applied Cognitive Psychology,26(2):316?323, October.Munmun De Choudhury, Scott Counts, and EricHorvitz.
2013a.
Major life changes and behav-ioral markers in social media: Case of childbirth.
InProceedings of the ACM Conference on ComputerSupported Cooperative Work and Social Computing(CSCW).Munmun De Choudhury, Scott Counts, and EricHorvitz.
2013b.
Predicting postpartum changes inemotion and behavior via social media.
In Proceed-ings of the ACM Annual Conference on Human Fac-tors in Computing Systems (CHI), pages 3267?3276.ACM.Munmun De Choudhury, Scott Counts, and EricHorvitz.
2013c.
Social media as a measurementtool of depression in populations.
In Proceedings ofthe Annual ACM Web Science Conference.Munmun De Choudhury, Michael Gamon, ScottCounts, and Eric Horvitz.
2013d.
Predicting de-pression via social media.
In Proceedings of the In-ternational AAAI Conference on Weblogs and SocialMedia (ICWSM).Munmun De Choudhury, Andres Monroy-Hernandez,and Gloria Mark.
2014. ?
narco?
emotions: Affectand desensitization in social media during the mexi-can drug war.Munmun De Choudhury.
2013.
Role of social mediain tackling challenges in mental health.
In Proceed-ings of the 2nd International Workshop on Socially-Aware Multimedia, pages 49?52.Mark Dredze.
2012.
How social media will changepublic health.
IEEE Intelligent Systems, 27(4):81?84.Danica Vukadinovic Greetham, Robert Hurling,Gabrielle Osborne, and Alex Linley.
2011.
Socialnetworks and positive and negative affect.
Procedia- Social and Behavioral Sciences, 22:4?13, January.Carleen Hawn.
2009.
Take Two Aspirin And TweetMe In The Morning: How Twitter, Facebook, AndOther Social Media Are Reshaping Health Care.Health Affairs, 28(2):361?368.59Qiwei He, Bernard P. Veldkamp, and Theo de Vries.2012.
Screening for posttraumatic stress disorderusing verbal features in self narratives: A text min-ing approach.
Psychiatry Research.Adam D. I. Kramer, Susan R. Fussell, and Leslie D.Setlock.
2004.
Text analysis as a tool for analyz-ing conversation in online support groups.
In Pro-ceedings of the ACM Annual Conference on HumanFactors in Computing Systems (CHI).Stephen J. Lurie, Barbara Gawinski, Deborah Pierce,and Sally J. Rousseau.
2006.
Seasonal affective dis-order.
American family physician, 74(9).Susan L. McElroy, Lori L. Altshuler, Trisha Suppes,Paul E. Keck, Mark A. Frye, Kirk D. Denicoff,Willem A. Nolen, Ralph W. Kupka, Gabriele S. Lev-erich, Jennifer R. Rochussen, A. John Rush Rush,and Robert M. Post Post.
2001.
Axis I psychi-atric comorbidity and its relationship to historical ill-ness variables in 288 patients with bipolar disorder.American Journal of Psychiatry, 158(3):420?426.Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-son, and Benjamin Van Durme.
2013.
Open domaintargeted sentiment.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1643?1654.World Health Organization.
2001.
The world healthreport 2001 - Mental health: New understanding,new hope.
Technical report, Genf, Schweiz.Minsu Park, Chiyoung Cha, and Meeyoung Cha.
2012.Depressive moods of users portrayed in Twitter.
InProceedings of the ACM SIGKDD Workshop onHealthcare Informatics (HI-KDD).Michael J. Paul and Mark Dredze.
2011.
You are whatyou tweet: Analyzing Twitter for public health.
InProceedings of the International AAAI Conferenceon Weblogs and Social Media (ICWSM).Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,and Matthieu Perrot?Edouard Duchesnay.
2011.scikit-learn: Machine learning in Python.
The Jour-nal of Machine Learning Research, 12:2825?2830.Frank J. Penedo and Jason R. Dahn.
2005.
Exer-cise and well-being: a review of mental and phys-ical health benefits associated with physical activ-ity.
Current Opinion in Psychiatry, 18(2):189?193,March.James W. Pennebaker, Cindy K. Chung, Molly Ire-land, Amy Gonzales, and Roger J. Booth.
2007.The development and psychometric properties ofLIWC2007.Nairan Ramirez-Esparza, Cindy K. Chung, EwaKacewicz, and James W. Pennebaker.
2008.
Thepsychology of word use in depression forums in En-glish and in Spanish: Testing two text analytic ap-proaches.
In Proceedings of the International AAAIConference on Weblogs and Social Media (ICWSM).Philip Resnik, Anderson Garron, and Rebecca Resnik.2013.
Using topic modeling to improve predictionof neuroticism and depression.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural, pages 1348?1353.Stephanie S. Rude, Eva-Maria Gortner, and James W.Pennebaker.
2004.
Language use of depressed anddepression-vulnerable college students.
Cognition& Emotion, 18(8):1121?1133, December.H.
Andrew Schwartz, Johannes C. Eichstaedt, Mar-garet L. Kern, Lukasz Dziurzynski, Stephanie M.Ramones, Megha Agrawal, Achal Shah, MichalKosinski, David Stillwell, Martin E. P. Seligman,and Lyle H. Ungar.
2013.
Personality, gender,and age in the language of social media: The open-vocabulary approach.
PLOS One, 8(9).Yla R. Tausczik and James W. Pennebaker.
2010.
Thepsychological meaning of words: LIWC and com-puterized text analysis methods.
Journal of Lan-guage and Social Psychology, 29(1):24?54.John W. Tukey.
1977.
Box-and-whisker plots.
Ex-ploratory Data Analysis, pages 39?43.Myrna M. Weissman, Roger C. Bland, Glorisa J.Canino, Carlo Faravelli, Steven Greenwald, Hai-Gwo Hwu, Peter R. Joyce, Eile G. Karam, Chung-Kyoon Lee, Joseph Lellouch, Jean-Pierre L?epine,Stephen C. Newman, Maritza Rubio-Stipec, J. Elis-abeth Wells, Priya J. Wickramaratne, Hans-UlrichWittchen, and Eng-Kung Yeh.
1996.
Cross-nationalepidemiology of major depression and bipolar dis-order.
Journal of the American Medical Association(JAMA), 276(4):293?299.Hadley Wickham.
2009. ggplot2: elegant graphics fordata analysis.
Springer.60
