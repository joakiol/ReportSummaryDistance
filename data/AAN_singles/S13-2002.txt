Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 10?14, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsClearTK-TimeML: A minimalist approach to TempEval 2013Steven BethardCenter for Computational Language and Education ResearchUniversity of Colorado BoulderBoulder, Colorado 80309-0594, USAsteven.bethard@colorado.eduAbstractThe ClearTK-TimeML submission to Temp-Eval 2013 competed in all English tasks: identi-fying events, identifying times, and identifyingtemporal relations.
The system is a pipeline ofmachine-learning models, each with a small setof features from a simple morpho-syntactic an-notation pipeline, and where temporal relationsare only predicted for a small set of syntac-tic constructions and relation types.
ClearTK-TimeML ranked 1st for temporal relation F1,time extent strict F1 and event tense accuracy.1 IntroductionThe TempEval shared tasks (Verhagen et al 2007;Verhagen et al 2010; UzZaman et al 2013) havebeen one of the key venues for researchers to com-pare methods for temporal information extraction.
InTempEval 2013, systems are asked to identify events,times and temporal relations in unstructured text.This paper describes the ClearTK-TimeML systemsubmitted to TempEval 2013.
This system is basedoff of the ClearTK framework for machine learning(Ogren et al 2008)1, and decomposes TempEval2013 into a series of sub-tasks, each of which is for-mulated as a machine-learning classification problem.The goals of the ClearTK-TimeML approach were:?
To use a small set of simple features that can bederived from either tokens, part-of-speech tags orsyntactic constituency parses.?
To restrict temporal relation classification to a sub-set of constructions and relation types for whichthe models are most confident.1http://cleartk.googlecode.com/Thus, each classifier in the ClearTK-TimeMLpipeline uses only the features shared by success-ful models in previous work (Bethard and Martin,2006; Bethard and Martin, 2007; Llorens et al 2010;UzZaman and Allen, 2010) that can be derived froma simple morpho-syntactic annotation pipeline2.
Andeach of the temporal relation classifiers is restrictedto a particular syntactic construction and to a partic-ular set of temporal relation labels.
The followingsections describe the models, classifiers and datasetsbehind the ClearTK-TimeML approach.2 Time modelsTime extent identification was modeled as a BIOtoken-chunking task, where each token in the textis classified as being at the B(eginning) of, I(nside)of, or O(utside) of a time expression.
The followingfeatures were used to characterize tokens:?
The token?s text?
The token?s stem?
The token?s part-of-speech?
The unicode character categories for each characterof the token, with repeats merged (e.g.
Dec28would be ?LuLlNd?)?
The temporal type of each alphanumeric sub-token,derived from a 58-word gazetteer of time words?
All of the above features for the preceding 3 andfollowing 3 tokensTime type identification was modeled as a multi-class classification task, where each time is classified2 OpenNLP sentence segmenter, ClearTK PennTreebank-Tokenizer, Apache Lucene Snowball stemmer, OpenNLP part-of-speech tagger, and OpenNLP constituency parser10as DATE, TIME, DURATION or SET.
The followingfeatures were used to characterize times:?
The text of all tokens in the time expression?
The text of the last token in the time expression?
The unicode character categories for each characterof the token, with repeats merged?
The temporal type of each alphanumeric sub-token,derived from a 58-word gazetteer of time wordsTime value identification was not modeled by thesystem.
Instead, the TimeN time normalization sys-tem (Llorens et al 2012) was used.3 Event modelsEvent extent identification, like time extent identi-fication, was modeled as BIO token chunking.
Thefollowing features were used to characterize tokens:?
The token?s text?
The token?s stem?
The token?s part-of-speech?
The syntactic category of the token?s parent in theconstituency tree?
The text of the first sibling of the token in theconstituency tree?
The text of the preceding 3 and following 3 tokensEvent aspect identification was modeled as a multi-class classification task, where each event is classi-fied as PROGRESSIVE, PERFECTIVE, PERFECTIVE-PROGRESSIVE or NONE.
The following featureswere used to characterize events:?
The part-of-speech tags of all tokens in the event?
The text of any verbs in the preceding 3 tokensEvent class identification was modeled as a multi-class classification task, where each event is classi-fied as OCCURRENCE, PERCEPTION, REPORTING,ASPECTUAL, STATE, I-STATE or I-ACTION.
Thefollowing features were used to characterize events:?
The stems of all tokens in the event?
The part-of-speech tags of all tokens in the eventEvent modality identification was modeled as amulti-class classification task, where each event isclassified as one of WOULD, COULD, CAN, etc.
Thefollowing features were used to characterize events:?
The text of any prepositions, adverbs or modalverbs in the preceding 3 tokensEvent polarity identification was modeled as a bi-nary classification task, where each event is classifiedas POS or NEG.
The following features were used tocharacterize events:?
The text of any adverbs in the preceding 3 tokensEvent tense identification was modeled as a multi-class classification task, where each event is clas-sified as FUTURE, INFINITIVE, PAST, PASTPART,PRESENT, PRESPART or NONE.
The following fea-tures were used to characterize events:?
The last two characters of the event?
The part-of-speech tags of all tokens in the event?
The text of any prepositions, verbs or modal verbsin the preceding 3 tokens4 Temporal relation modelsThree different models, described below, were trainedfor temporal relation identification.
All models fol-lowed a multi-class classification approach, pairingan event and a time or an event and an event, andtrying to predict a temporal relation type (BEFORE,AFTER, INCLUDES, etc.)
or NORELATION if therewas no temporal relation between the pair.While the training and evaluation data allowedfor 14 possible relation types, each of the temporalrelation models was restricted to a subset of relations,with all other relations mapped to the NORELATIONtype.
The subset of relations for each model wasselected by inspecting the confusion matrix of themodel?s errors on the training data, and removingrelations that were frequently confused and whoseremoval improved performance on the training data.Event to document creation time relations wereclassified by considering (event, time) pairs whereeach event in the text was paired with the documentcreation time.
The classifier was restricted to the rela-tions BEFORE, AFTER and INCLUDES.
The follow-ing features were used to characterize such relations:?
The event?s aspect (as classified above)?
The event?s class (as classified above)?
The event?s modality (as classified above)?
The event?s polarity (as classified above)?
The event?s tense (as classified above)?
The text of the event, only if the event was identi-fied as having class ASPECTUAL11Event to same sentence time relations were clas-sified by considering (event, time) pairs where thesyntactic path from event to time matched a regu-lar expression of syntactic categories and up/downmovements through the tree: ?((NP|PP|ADVP)?)*((VP|SBAR|S)?
)* (S|SBAR|VP|NP) (?(VP|SBAR|S))*(?(NP|PP|ADVP))*$.
The classifier relations were re-stricted to INCLUDES and IS-INCLUDED.
The follow-ing features were used to characterize such relations:?
The event?s class (as classified above)?
The event?s tense (as classified above)?
The text of any prepositions or verbs in the 5 tokensfollowing the event?
The time?s type (as classified above)?
The text of all tokens in the time expression?
The text of any prepositions or verbs in the 5 tokenspreceding the time expressionEvent to same sentence event relations were clas-sified by considering (event, event) pairs wherethe syntactic path from one event to the othermatched ?((VP?|ADJP?|NP?)?
(VP|ADJP|S|SBAR)(?
(S|SBAR|PP))* ((?VP|?ADJP)*|(?NP)*)$.
The classi-fier relations were restricted to BEFORE and AFTER.The following features were used to characterize suchrelations:?
The aspect (as classified above) for each event?
The class (as classified above) for each event?
The tense (as classified above) for each event?
The text of the first child of the grandparent of theevent in the constituency tree, for each event?
The path through the syntactic constituency treefrom one event to the other?
The tokens appearing between the two events5 ClassifiersThe above models described the translation fromTempEval tasks to classification problems and clas-sifier features.
For BIO token-chunking problems,Mallet3 conditional random fields and LIBLINEAR4support vector machines and logistic regression wereapplied.
For the other problems, LIBLINEAR, Mal-let MaxEnt and OpenNLP MaxEnt5 were applied.All classifiers have hyper-parameters that must be3http://mallet.cs.umass.edu/4http://www.csie.ntu.edu.tw/?cjlin/liblinear/5http://opennlp.apache.org/tuned during training ?
LIBLINEAR has the classi-fier type and the cost parameter, Mallet CRF has theiteration count and the Gaussian prior variance, etc.6The best classifier for each training data set wasselected via a grid search over classifiers and param-eter settings.
The grid of parameters was manuallyselected to provide several reasonable values for eachclassifier parameter.
Each (classifier, parameters)point on the grid was evaluated with a 2-fold crossvalidation on the training data, and the best perform-ing (classifier, parameters) was selected as the finalmodel to run on the TempEval 2013 test set.6 Data setsThe classifiers were trained using the followingsources of training data:TB The TimeBank event, time and relation annota-tions, as provided by the TempEval organizers.AQ The AQUAINT event, time and relation annota-tions, as provided by the TempEval organizers.SLV The ?Silver?
event, time and relation annota-tions, from the TempEval organizers?
system.BMK The verb-clause temporal relation annotationsof (Bethard et al 2007).
These relations areadded on top of the original relations.PM The temporal relations inferred via closure onthe TimeBank and AQUAINT data by PhilippeMuller7.
These relations replace the originalones, except in files where no relations wereinferred (because of temporal inconsistencies).7 ResultsTable 1 shows the performance of the ClearTK-TimeML models across the different tasks whentrained on different sets of training data.
The ?Data?column of each row indicates both the training datasources (as in Section 6), and whether the events andtimes were predicted by the models (?system?)
ortaken from the annotators (?human?).
Performanceis reported in terms of strict precision (P), Recall (R)and F1 for event extents, time extents and temporalrelations, and in terms of Accuracy (A) on the cor-rectly identified extents for event and time attributes.6For BIO token-chunking tasks, LIBLINEAR also had a pa-rameter for how many previous classifications to use as features.7https://groups.google.com/d/topic/tempeval/LJNQKwYHgL812Data Event Time Relationannotation events extent class tense aspect extent value type typesources & times F1 P R A A A F1 P R A A F1 P RTB+BMK system 77.3 81.9 73.3 84.6 80.4 91.0 82.7 85.9 79.7 71.7 93.3 31.0 34.1 28.4TB system 77.3 81.9 73.3 84.6 80.4 91.0 82.7 85.9 79.7 71.7 93.3 29.8 34.5 26.2TB+AQ system 78.8 81.4 76.4 86.1 78.2 90.9 77.0 83.2 71.7 69.9 92.9 28.6 30.9 26.6TB+AQ+PM system 78.8 81.4 76.4 86.1 78.2 90.9 77.0 83.2 71.7 69.9 92.9 28.5 29.7 27.3*TB+AQ+SLV system 80.5 82.1 78.9 88.4 71.6 91.2 80.0 91.6 71.0 73.6 91.5 27.8 26.5 29.3Highest in TempEval 81.1 82.0 80.8 89.2 80.4 91.8 82.7 91.4 80.4 86.0 93.7 31.0 34.5 34.4TB+BMK human - - - - - - - - - - - 36.3 37.3 35.2TB human - - - - - - - - - - - 35.2 37.6 33.0TB+AQ human - - - - - - - - - - - 34.1 33.3 35.0TB+AQ+PM human - - - - - - - - - - - 35.9 35.2 36.6*TB+AQ+SLV human - - - - - - - - - - - 37.7 34.9 41.0Highest in TempEval - - - - - - - - - - - 36.3 37.6 65.6Table 1: Performance across different training data.
Systems marked with * were tested after the official evaluation.Scores in bold are at least as high as the highest in TempEval.Training on the AQUAINT (AQ) data in addition tothe TimeBank (TB) hurt times and relations.
Addingthe AQUAINT data caused a -2.7 drop in extent preci-sion, a -8.0 drop in extent recall, a -1.8 drop in valueaccuracy and a -0.4 drop in type accuracy, and a -3.6to -4.3 drop in relation recall.Training on the ?Silver?
(SLV) data in additionto TB+AQ data gave mixed results.
There were biggains for time extent precision (+8.4), time value ac-curacy (+3.7), event extent recall (+2.5) and eventclass accuracy (+2.3), but a big drop for event tenseaccuracy (-6.6).
Relation recall improved (+2.7 withsystem events and times, +6.0 with manual) but pre-cision varied (-4.4 with system, +1.6 with manual).Adding verb-clause relations (BMK) and closure-inferred relations (PM) increased recall but low-ered precision.
With system-annotated events andtimes, the change was +2.2/-0.4 (recall/precision)for verb-clause relations, and +0.7/-1.2 for closure-inferred relations.
With manually-annotated eventsand times, the change was +2.2/-0.3 for verb-clauserelations, and (the one exception where recall im-proved) +1.5/+1.9 for closure-inferred relations.8 DiscussionOverall, the ClearTK-TimeML ranked 1st in relationF1, time extent strict F1 and event tense accuracy.Analysis across the different ClearTK-TimeMLruns showed that including annotations from theAQUAINT corpus hurt model performance acrossa variety of tasks.
A manual inspection of theAQUAINT corpus revealed many annotation errors,suggesting that the drop may be the result of attempt-ing to learn from inconsistent training data.
TheAQUAINT corpus may thus have to be partially re-annotated to be useful as a training corpus.Analysis also showed that adding more relationannotations increased recall, typically at the cost ofprecision, even though the added annotations werehighly accurate: (Bethard et al 2007) reported agree-ment of 90%, and temporal closure relations were100% deterministic from the already-annotated re-lations.
One would expect that adding such high-quality relations would only improve performance.But not all temporal relations were annotated by theTempEval 2013 annotators, so the system could bemarked wrong for a finding a true temporal relationthat was not noticed by the annotators.
Further analy-sis is necessary to investigate this hypothesis.AcknowledgementsThanks to Philippe Muller for providing the closure-inferred relations.
The project described was supported inpart by Grant Number R01LM010090 from the NationalLibrary Of Medicine.
The content is solely the responsi-bility of the authors and does not necessarily represent theofficial views of the National Library Of Medicine or theNational Institutes of Health.13References[Bethard and Martin2006] Steven Bethard and James H.Martin.
2006.
Identification of event mentions andtheir semantic class.
In Empirical Methods in NaturalLanguage Processing (EMNLP), page 146154.
(Accep-tance rate 31%).
[Bethard and Martin2007] Steven Bethard and James H.Martin.
2007.
CU-TMP: temporal relation classifica-tion using syntactic and semantic features.
In Proceed-ings of the 4th International Workshop on SemanticEvaluations, pages 129?132, Prague, Czech Republic.Association for Computational Linguistics.
[Bethard et al007] Steven Bethard, James H. Martin, andSara Klingenstein.
2007.
Finding temporal structurein text: Machine learning of syntactic temporal rela-tions.
International Journal of Semantic Computing,01(04):441.
[Llorens et al010] Hector Llorens, Estela Saquete, andBorja Navarro.
2010.
TIPSem (English and Spanish):Evaluating CRFs and semantic roles in TempEval-2.In Proceedings of the 5th International Workshop onSemantic Evaluation, page 284291, Uppsala, Sweden,July.
Association for Computational Linguistics.
[Llorens et al012] Hector Llorens, Leon Derczynski,Robert Gaizauskas, and Estela Saquete.
2012.
TIMEN:an open temporal expression normalisation resource.In Proceedings of the Eight International Conferenceon Language Resources and Evaluation (LREC?12),Istanbul, Turkey, May.
European Language ResourcesAssociation (ELRA).
[Ogren et al008] Philip V. Ogren, Philipp G. Wetzler,and Steven Bethard.
2008.
ClearTK: A UIMA toolkitfor statistical natural language processing.
In TowardsEnhanced Interoperability for Large HLT Systems:UIMA for NLP workshop at Language Resources andEvaluation Conference (LREC), 5.
[UzZaman and Allen2010] Naushad UzZaman and JamesAllen.
2010.
TRIPS and TRIOS system for TempEval-2: extracting temporal information from text.
In Pro-ceedings of the 5th International Workshop on SemanticEvaluation, page 276283, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.
[UzZaman et al013] Naushad UzZaman, Hector Llorens,James F. Allen, Leon Derczynski, Marc Verhagen,and James Pustejovsky.
2013.
SemEval-2013 task1: TempEval-3 evaluating time expressions, events, andtemporal relations.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013), in conjunction with the Second Joint Conferenceon Lexical and Computational Semantcis (*SEM 2013).Association for Computational Linguistics, June.
[Verhagen et al007] Marc Verhagen, Robert Gaizauskas,Frank Schilder, Mark Hepple, Graham Katz, and JamesPustejovsky.
2007.
SemEval-2007 task 15: TempEvaltemporal relation identification.
In Proceedings of the4th International Workshop on Semantic Evaluations,pages 75?80, Prague, Czech Republic.
Association forComputational Linguistics.
[Verhagen et al010] Marc Verhagen, Roser Sauri, Tom-maso Caselli, and James Pustejovsky.
2010.
SemEval-2010 task 13: TempEval-2.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation, page5762, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.14
