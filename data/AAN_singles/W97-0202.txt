Experience in WordNet Sense Tagging in the Wall Street JournalJanyce Wiebet, Julie Maplest, Lei Duant, and Rebecca Bruce~ttDept, of Computer Science and the Computing Research LaboratoryNew Mexico State UniversityLas Cruces, NM 88003:~Dept.
of Computer Science and EngineeringSouthern Methodist UniversityDallas, TX  75275-0112wiebe, jmaples, lduan@cr l  .nmsu.
edu, rbruce@seas,  smu.
eduAbstractThis paper eports on our experience handtagging the senses of 25 of the most fre-quent verbs in 12,925 sentences ofthe WallStreet Journal Treebank corpus (Marcus etal.
1993).
The verbs are tagged with re-spect to senses in WordNet (Miller 1990).Some of the annotated verbs can functionas both main and auxiliary verbs, and someare often used in idioms.
This paper sug-gests consistently representing these as sep-arate subclasses.
Strategies described inthe coding instruction for recognizing id-ioms are described, as well as some chal-lenging ambiguities found in the data.1 Introduct ionThis paper reports on our experience hand taggingthe senses of 25 of the most frequent verbs in 12,925sentences of the Wall Street Journal Treebank cor-pus (Marcus et al 1993).
The purpose of this workis to support related work in automatic word-sensedisambiguation.The verbs are tagged with respect o senses inWordNet (Miller 1990), which has become widelyused, for example in corpus-annotation projects(Miller et al 1994, Ng & Hian 1996, and Grish-man et al 1994) and for performing disambiguation(Resnik 1995 and Leacock et ai.
1993).The verbs to tag were chosen on the basis of howfrequently they occur in the text, how wide theirrange of senses, and how distinguishable the sensesare from one another.In related work, we have begun to tag nouns andadjectives as well.
These are being chosen addition-ally on the basis of co-occurrence with the verbs al-ready tagged, to support approaches such as (Hirst1987), in which word-sense ambiguities are resolvedwith respect to one another.Some of the chosen verbs can function as bothmain and auxiliary verbs, and some are often usedin idioms.
In this paper, we suggest consistentlyrepresenting these as separate subclasses.We apply a preprocessor to the data, which auto-matically identifies some classes of verb occurrencewith good accuracy.
This facilitates manual annota-tion, because it is easier to fix a moderate number oferrors than to tag the verbs completely from scratch.The preprocessor performs other miscellaneous tasksto aide in the tagging task, such as separating outpunctuation marks and contractions.At the end of the paper, we share some strategiesfrom our coding instructions for recognizing idioms,and show some challenging ambiguities we found inthe data.2 The Verbs and the Basic TagFormatThe following are the verbs that were tagged.
Thetotal number of occurrences is 6,197.VERB NUMBER VERB NUMBERhave 2740 make 473take 316 get 231add 118 pay 189see 159 call 151decline 84 hold 127come 191 give 168keep I01 know 87find 130 lose 82believe 103 raise 124drop 61 lead 105work 101 leave 81run 105 look 95meet 75The basic tags have the following form.
Exten-sions will be given below.word_<lemma, WordNet POS, WordNet sense number>8For example:The Sacramento-based S&L  had_(haveverb 4) assets of $2.4 billion at the end ofSeptember.That is, 'had' is a form of the main verb 'have' oc-curring as WordNet sense number 4.3 Ref inementsWe consistently break out certain uses of verbs to agreater extent than WordNet does, in particular, id-ioms and verbs of intermediate (and auxiliary) func-tion.
There are several reasons for doing so.The primary reason is to perform more accuratetagging.
Not all such uses are covered by WordNetentries.A second reason is to support identifying bet-ter features for automatic tagging.
Some of thesespecial-case uses can be identified with good accu-racy with simple grammars, while the more semanti-cally weighty uses of the same verb generally cannotbe.
Thus, different features will be appropriate forthe special-case versus other uses.
By tagging themas separate categories, one can search for separatefeatures characterizing each class.Finally, it is helpful to the human tagger for thepreprocessor to target these distinguished classes, forwhich relatively high-accuracy automatic solutionsare possible.3.1 Auxil iary VerbsWordNet does not provide sense information for aux-iliary uses of verbs.
SEMCOR (Miller et al 1994)leaves these uses untagged.
Among the verbs taggedin our corpus, only 'have' has an auxiliary use, whichwe tag as follows, with the string "aux" replacing thesense number:South Korea has_(have verb_aux) recordeda trade surplus of 71 million dollars so farthis year.As they can be recognized automatically with highaccuracy, auxiliaries are automatically annotated bythe preprocessor.3.2 Intermediate Verbs"Intermediate verb" is a term used in Quirk etal.
(1985; pp.
138-148), defined as an occurrence"whose status is in some degree intermediate be-tween auxiliaries and main verbs."
Quirk et al ar-range verbs on a scale ranging from modal auxiliariesto main verbs, and "many of the intermediate v rbs,particularly those at the higher end of the scale, havemeanings associated with aspect, tense, and modal-ity: meanings which are primarily expressed throughauxiliary verb constructions.
"Among the verbs tagged in our corpus, 'had', 'get',and 'keep' are used with intermediate function in thefollowing constructions: 'had better' (or 'had best')and 'have got to' (called "modal idioms" by Quirket al), 'have to' (called a "semi-auxiliary"), 'get' +-ed participle, and 'keep' + -ing participle (whichare given the title "catenatives').Some but not all of these are represented by sensesin WordNet (and none are identified as having thisspecial function).
Since WordNet senses cannot beconsistently assigned to these occurrences, we use anew tag, "int", in place of a sense number (or in ad-dition to one, when there is an appropriate sense),creating a new category, as we did with the auxil-iaries.An  example of an intermediate verb occurrenceis the following.
Note that sense 5 of 'have' is anappropriate WordNet sense for this occurrence:Apple II owners, for example, had_(have_toverbJnt 5) to use their television sets asscreens and stored data on audiocassettes.These intermediate occurrences can also be recog-nized with good accuracy, and so are also added tothe corpus by the preprocessor.The auxiliary and intermediateuses of 'have' to-gether represent well over half of the occurrences,so breaking these out as separate categories enablesthe preprocessor to assist the tagger greatly.
In ad-dition, it would allow for separate evaluation of anautomatic classifier tagging 'have'.4 Verb  Id ioms4.1 Manual AnnotationThe occurrence of a variety of verb idioms---semanticunits consisting of a verb followed by a particle orother modifying word--accounted for a recognizablesegment--about 6%-  of the tagged data.
For exam-ple:The borrowing to raise these funds wouldbe paid_(pay_off verb I) off as assets of sickthrifts are sold.WordNet does not provide entries for all idioms,and the entries it does provide do not always includea sense for the occurrences observed in the corpus.It is important o recognize idioms, because in-terpreting their constituent words separately wouldoften change the meaning of the sentence (cf., e.g.,9Wilks 1977 and Wilensky & Arens 1980).
Our cod-ing instructions specify that the tagger should at-tempt to identify idioms even if WordNet does notprovide an entry for it.
The preprocessor assists inthis task, by identifying potential idioms.The following axe strategies we found useful indealing with the difficult problem of manually iden-tifying idioms.1.
Does the word following the verb cease to haveany of its usual or literal meanings as suppliedby WordNet when used with that verb?If America can keep_(keep.up verb1) up the present situation ... theeconomies of these countries would betotally restructured to be able to al-most sustain growth by themselves.The 'situation' here does not need to be keptin a lofty position, but rather maintained.
Theuse of 'up' as a particle takes away its literal,physical meaning, and attaches it semanticallyto 'keep', making an idiom definition necessary.2.
Could the idiom be replaced with a single verbwhich has the same meaning?For example:But the New York Stock Exchangechairman said he doesn't supportreinstating a "collar" on programtrading, arguing that firms couldget_(get_around verb 2) around such alimit.WordNet's entry for this sense of "get around"includes as synonyms "avoid" and "bypass",which, if used in place of the idiom, do notchange the meaning of the sentence.3.
Would the particle be mistaken for a prepositionbeginning a prepositional phrase-and therebychanging the meaning of the sentence--if viewedas separate from the main verb?Consider this example:Coleco failed to come_(come.up_withverb 1) up with another winner andfiled for bankruptcy-law protection ...This example actually meets all three criteria.
'Come up with' must be considered a single id-iom partly to avoid a literal interpretation thatwould change the meaning of the sentence, asdescribed in criterion (1), and it also has themeaning "locate", which further qualifies thissentence as an idiom according to criterion (2).10If this sentence were given a literal reading, per-haps by an automatic tagger, 'with another win-ner' might be identified as an acceptable prepo-sitional phrase.4.2 A Flexible Tag FormatFor the purposes of the larger project of which thisannotation project is a part, the words are annotatedwith information in addition to the WordNet sensetags.
A simple example is the richer part-of-speechtags produced by Brill's tagger (1992).
We note herea problem that we encountered using SEMCOR'stag format for idioms: SEMCOR merges the compo-nent words of the idiom into one annotation, therebymaking it impossible to unambiguously represent in-formation about he individual words.
Representingsplit idioms is also a problem with this scheme.To maintain separate ~nnotations and also tie theconstituents of an idiom together, we suggest heformat below (or an analogous one), which is gener-ated by the preprocessor.
The annotations for theindividual words are delimited by "(wf" and"(/wf )".
The human annotator's tags are includedin the individual word annotations.
For example, be-low the annotator tagged "take" with the first Word-Net entry for 'take place'.
When there is an appro-priate WordNet entry for the idiom as a whole, westore that entry with the first word of the idiom (butthe entry could be stored with both).
AppropriateWordNet entries for the individual words can alsobe stored in the individual word annotations.
TheBrill part-of-speech tags illustrate other informationwe would like to retain for the individual words.<wf BrilIPOSffiVBD idiomffitake_place-iwnentry=_<take_place verb l>>took</wf><wf pos=NN idiomffitake_place-2>place</wf>The first two lines contain the annotation for thefirst word in the idiom.
It contains a Brill POS tagfor 'take' and a WordNet entry for 'take place'.
Thestring 'take-place-l' encodes the fact that this is thefirst word of a 'take place' idiom.The third line represents he second word in theidiom ('take-place-T), which is a noun ('NN').An intervening adverb, for example, would sim-ply be represented with its own annotation placedbetween the annotations for the words in the idiom.5 Challenging AmbiguitiesThere are some instances in the corpus that we foundto be truly ambiguous.
These instances support wocompletely different interpretations even with thehelp of the context.
For example:mmmmm\[\]mmmm\[\]mmmThe group has_(have verb l?aux) forecast1989 revenue of 56.9 billion francs.In this sentence, two interpretations of the verb'have' are equally possible, even when the sentenceis viewed in context: 'Have' can be seen as an auxil-iary, meaning that the group have themselves clonethe forecasting, oras WordNet sense I (in which case'forecast' is an adjective), implying that someoneelse has given them an amount, 56.9 billion francs,that represents heir expected revenue.A problem found several times in the corpus oc-curred when a single verb is used in a sentence thathas two objects, and each object suggests a differ-ent sense of the verb.
In the sentence below, forexample, two senses of the main verb 'have' are rep-resented simultaneously in the sentence.
Sense 4 car-ties the idea of ownership, which should be appliedto the object 'papers', while sense 3 has the meaning'~o experience or receive", which should be appliedto the object 'sales'.PAPERS: Backe Group Inc. agreed toacquire Atlantic Publications Inc., whichhas_(have verb 4114) 30 community papersand annual sales of $7 million.Such cases are borderline, hovering in between twodistinct meanings.6 Conc lus ionData manually annotated with lexical semanticsclearly has many applications in NLP.
This pa-per shared our experience in manual annotation ofWordNet senses in the Wall Street Journal Treebankcorpus.
WordNet proved to be a valuable and usefultool.
Its wide range of senses made possible a highlyspecific level of tagging.
WordNet's structure, withthe alignment of hierarchical information and theaddition of synsets and sample sentences, was espe-cially helpful.
We have made some suggestions forconsistently identifying certain uses of verbs and forrepresenting tags, and have shared some guidelinesfrom our annotation instructions for identifying id-ioms in the corpus.7 AcknowledgementsReferencesBrill, E. 1992.
A simple rule-based part of speechtagger.
In Proc.
of the Third Conference on Ap-plied Natural Language Processing, pp.
152-155.Grishman, R., Macleod, C., & Meyers, A.
(1994).COMPLEX syntax: building a computational lex-icon.
In Proc.
15th International Conference ofComputational Linguistics (COLING 94).Hirst, G. 1987.
Semantic Interpretation a d the Res-olution of Ambiguity.
Cambridge: Cambridge Uni-versity Press.Leacock, C., Towell, G., Voorhees, E. 1993.
In Proc.of the ARPA Human Language Technology Work-shop.Marcus, M., Santorini, B., & Marcinkiewicz, M.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Lin-guistics 19 (2): 313-330.Miller, G. 1990.
Special Issue, WordNet: An on-linelexical database.
International Journal of Lexicog-raphy 3 (4).Miller, G., Chodorow, M., Landes, S., Leacock, C.,& Thomas, R. 1994.
Using a semantic concordancefor sense identification.
In Proc.
ARPA HumanLanguage Technology Workshop.Ng, H.T.
& Hian, B.L.
1996.
Integrating multipleknowledge sources to disambiguate word senses:an exemplar-based approach.
In Proc.
34th An-nual Meeting of the Association for Computa-tional Linguistics (ACL-96), pp.
40--47.Quirk, R., Greenbaum, S., Leech, G., & Svartvik, J.1985.
A Comprehensive Grammar of the EnglishLanguage.
(New York: Long'man).Resnik, P. 1995.
Disambiguating noun groupingswith respect o wordnet senses.
In Proc.
of theThird Workshop on Very Large Corpora, pp.
54-68.Wilensky, R. & Arens, Y.
1980.
PHRAN - a knowl-edge based natural language understander.
InPrac.
18th Annual Meeting of the Association forComputational Linguistics (ACL-80), pp.
117-121.Wilks, Y.
1977.
Making preferences more active.
Ar-tificial Intelligence 8, pp.
75-97.This research was supported in part by the Officeof Naval Research under grant number N00014-95-1-0776.11
