Minimizing Manual Annotation CostIn Supervised Training From CorporaSean P. Engelson and Ido  DaganDepar tment  of Mathemat ics  and Computer  ScienceBar- I lan University52900 Ramat  Gan, Israel{engelson, dagan}@bimacs, cs.
biu.
ac.
ilAbst ractCorpus-based methods for natural lan-guage processing often use supervisedtraining, requiring expensive manual an-notation of training corpora.
This paperinvestigates methods for reducing annota-tion cost by sample selection.
In this ap-proach, during training the learning pro-gram examines many unlabeled examplesand selects for labeling (annotation) onlythose that are most informative at eachstage.
This avoids redundantly annotatingexamples that contribute little new infor-mation.
This paper extends our previouswork on committee-based sample selectionfor probabilistic lassifiers.
We describea family of methods for committee-basedsample selection, and report experimentalresults for the task of stochastic part-of-speech tagging.
We find that all variantsachieve a significant reduction in annota-tion cost, though their computational effi-ciency differs.
In particular, the simplestmethod, which has no parameters to tune,gives excellent results.
We also show thatsample selection yields a significant reduc-tion in the size of the model used by thetagger.1 In t roduct ionMany corpus-based methods for natural languageprocessing (NLP) are based on supervised training--acquiring information from a manually annotatedcorpus.
Therefore, reducing annotation cost is animportant research goal for statistical NLP.
The ul-timate reduction in annotation cost is achieved byunsupervised training methods, which do not requirean annotated corpus at all (Kupiec, 1992; Merialdo,1994; Elworthy, 1994).
It has been shown, how-ever, that some supervised training prior to the un-supervised phase is often beneficial.
Indeed, fullyunsupervised training may not be feasible for cer-tain tasks.
This paper investigates an approachfor optimizing the supervised training (learning)phase, which reduces the annotation effort requiredto achieve a desired level of accuracy of the trainedmodel.In this paper, we investigate and extend thecommittee-based sample selection approach to min-imizing training cost (Dagan and Engelson, 1995).When using sample selection, a learning program ex-amines many unlabeled (not annotated) examples,selecting for labeling only those that are most in-formative for the learner at each stage of training(Seung, Opper, and Sompolinsky, 1992; Freund etal., 1993; Lewis and Gale, 1994; Cohn, Atlas, andLadner, 1994).
This avoids redundantly annotatingmany examples that contribute roughly the same in-formation to the learner.Our work focuses on sample selection for trainingprobabilistic lassifiers.
In statistical NLP, prob-abilistic classifiers are often used to select a pre-ferred analysis of the linguistic structure of a text(for example, its syntactic structure (Black et al,1993), word categories (Church, 1988), or wordsenses (Gale, Church, and Yarowsky, 1993)).
As arepresentative task for probabilistic lassification iNLP, we experiment in this paper with sample se-lection for the popular and well-understood methodof stochastic part-of-speech tagging using HiddenMarkov Models.We first review the basic approach of committee-based sample selection and its application to part-of-speech tagging.
This basic approach gives riseto a family of algorithms (including the original al-gorithm described in (Dagan and Engelson, 1995))which we then describe.
First, we describe the 'sim-plest' committee-based selection algorithm, whichhas no parameters to tune.
We then generalize theselection scheme, allowing more options to adaptand tune the approach for specific tasks.
The papercompares the performance of several instantiationsof the general scheme, including a batch selectionmethod similar to that of Lewis and Gale (1994).In particular, we found that the simplest version ofthe method achieves a significant reduction in an-notation cost, comparable to that of other versions.319We also evaluate the computational efficiency of thedifferent variants, and the number of unlabeled ex-amples they consume.
Finally, we study the effectof sample selection on the size of the model acquiredby the learner.2 P robab i l i s t i c  C lass i f i ca t ionThis section presents the framework and terminol-ogy assumed for probabilistic lassification, as wellas its instantiation for stochastic bigram part-of-speech tagging.A probabilistic lassifier classifies input examplese by classes c E C, where C is a known set of pos-sible classes.
Classification is based on a score func-tion, FM(C, e), which assigns a score to each possibleclass of an example.
The classifier then assigns theexample to the class with the highest score.
FM isdetermined by a probabilistic model M. In manyapplications, FM is the conditional probability func-tion, PM (cle), specifying the probability of each classgiven the example, but other score functions thatcorrelate with the likelihood of the class are oftenused.In stochastic part-of-speech tagging, the model as-sumed is a Hidden Markov Model (HMM), and inputexamples are sentences.
The class c, to which a sen-tence is assigned is a sequence of the parts of speech(tags) for the words in the sentence.
The score func-tion is typically the joint (or conditional) probabilityof the sentence and the tag sequence 1 .
The taggerthen assigns the sentence to the tag sequence whichis most probable according to the HMM.The probabilistic model M, and thus the scorefunction FM, are defined by a set of parameters,{hi}.
During training, the values of the parametersare estimated from a set of statistics, S, extractedfrom a training set of annotated examples.
We de-note a particular model by M = {hi}, where each aiis a specific value for the corresponding cq.In bigram part-of-speech tagging the HMM modelM contains three types of parameters: transitionprobabilities P(ti---*tj) giving the probability of tagtj occuring after tag ti, lexical probabilities P(t\[w)giving the probability of tag t labeling word w, andtag probabilities P(t) giving the marginal probability2 of a tag occurring.
The values of these parametersare estimated from a tagged corpus which providesa training set of labeled examples (see Section 4.1).3 Eva luat ing  Example  Uncer ta in tyA sample selection method needs to evaluate theexpected usefulness, or information gain, of learn-ing from a given example.
The methods we investi-1This gives the Viterbi model (Merialdo, 1994), whichwe use here.2This version of the method uses Bayes' theorem~ (Church, 1988).
(P(wdt,) o?
P(t,) Jgate approach this evaluation implicitly, measuringan example's informativeness a the uncertainty inits classification given the current raining data (Se-ung, Opper, and Sompolinsky, 1992; Lewis and Gale,1994; MacKay, 1992).
The reasoning is that if anexample's classification is uncertain given currenttraining data then the example is likely to containunknown information useful for classifying similarexamples in the future.We investigate the committee-based method,where the learning algorithm evaluates an exampleby giving it to a committee containing several vari-ant models, all 'consistent' with the training dataseen so far.
The more the committee members agreeon the classification of the example, the greater ourcertainty in its classification.
This is because whenthe training data entails a specific classification withhigh certainty, most (in a probabilistic sense) classi-tiers consistent with the data will produce that clas-sification.The committee-based approach was first proposedin a theoretical context for learning binary non-probabilistic lassifiers (Seung, Opper, and Som-polinsky, 1992; Freund et al, 1993).
In this pa-per, we extend our previous work (Dagan and En-gelson, 1995) where we applied the basic idea of thecommittee-based approach to probabilistic lassifi-cation.
Taking a Bayesian perspective, the posteriorprobability of a model, P(M\[S), is determined givenstatistics S from the training set (and some prior dis-tribution for the models).
Committee members arethen generated by drawing models randomly fromP(MIS ).
An example is selected for labeling if thecommittee members largely disagree on its classifi-cation.
This procedure assumes that one can samplefrom the models' posterior distribution, at least ap-proximately.To illustrate the generation of committee-members, consider a model containing a single bi-nomial parameter a (the probability of a success),with estimated value a.
The statistics S for such amodel are given by N, the number of trials, and x,the number of successes in those trials.Given N and x, the 'best' parameter value maybe estimated by one of several estimation methods.For example, the maximum likelihood estimate for aX is a = ~,  giving the model M = {a} = {~}.
Whengenerating a committee of models, however, we arenot interested in the 'best' model, but rather in sam-pling the distribution of models given the statistics.For our example, we need to sample the posteriordensity of estimates for a, namely P(a = a\]S).
Sam-pling this distribution yields a set of estimates cat-tered around ~ (assuming a uniform prior), whosevariance decreases as N increases.
In other words,the more statistics there are for estimating the pa-rameter, the more similar are the parameter valuesused by different committee members.For models with multiple parameters, parame-320ter estimates for different committee members differmore when they are based on low training counts,and they agree more when based on high counts.
Se-lecting examples on which the committee membersdisagree contributes statistics to currently uncertainparameters whose uncertainty also affects classifica-tion.It may sometimes be difficult to sample P(M\[S)due to parameter interdependence.
Fortunately,models used in natural language processing oftenassume independence between most model parame-ters.
In such cases it is possible to generate commit-tee members by sampling the posterior distributionfor each independent group of parameters separately.4 B igram Par t -Of -Speech  Tagg ing4.1 Sampl ing  mode l  parametersIn order to generate committee members for bigramtagging, we sample the posterior distributions fortransition probabilities, P(ti---~tj), and for lexicalprobabilities, P(t\[w) (as described in Section 2).Both types of the parameters we sample have theform ofmultinomialdistributions.
Each multinomialrandom variable corresponds to a conditioning eventand its values are given by the corresponding set ofconditioned events.
For example, a transition prob-ability parameter P(ti--*tj) has conditioning eventti and conditioned event tj.Let {ui} denote the set of possible values of agiven multinomial variable, and let S = {hi} de-note a set of statistics extracted from the trainingset for that variable, where ni is the number of timesthat the value ui appears in the training set forthe variable, defining N = ~-~i hi.
The parameterswhose posterior distributions we wish to estimateare oil = P(ui).The maximum likelihood estimate for each of themultinomial's distribution parameters, ai, is &i =In practice, this estimator is usually smoothed in N'some way to compensate for data sparseness.
Suchsmoothing typically reduces slightly the estimatesfor values with positive counts and gives small pos-itive estimates for values with a zero count.
Forsimplicity, we describe here the approximation ofP(~i = ailS) for the unsmoothed estimator 3.We approximate the posterior P(ai = ai\[S) byfirst assuming that the multinomial is a collection ofindependent binomials, each of which corresponds toa single value ui of the multinomial; we then normal-ize the values so that they sum to 1.
For each suchbinomial, we approximate P(a i  = ai\[S) as a trun-3In the implementation we smooth the MLE by in-terpolation with a uniform probability distribution, fol-lowing Merialdo (1994).
Approximate adaptation ofP(c~i = ai\[S) to the smoothed version of the estimatoris simple.cated normal distribution (restricted to \[0,1\]), withand variance ~2 = #(1--#) 4 estimated mean#---- N N "To generate a particular multinomial distribution,we randomly choose values for the binomial param-eters ai from their approximated posterior distribu-tions (using the simple sampling method given in(Press et al, 1988, p. 214)), and renormalize themso that they sum to 1.
Finally, to generate a randomHMM given statistics S, we choose values indepen-dently for the parameters of each multinomial, sinceall the different multinomials in an HMM are inde-pendent.4.2 Examples  in b ig ram t ra in ingTypically, concept learning problems are formulatedsuch that there is a set of training examples that areindependent of each other.
When training a bigrammodel (indeed, any HMM), this is not true, as eachword is dependent on that before it.
This problemis solved by considering each sentence as an individ-ual example.
More generally, it is possible to breakthe text at any point where tagging is unambiguous.We thus use unambiguous words (those with onlyone possible part of speech) as example boundariesin bigram tagging.
This allows us to train on smallerexamples, focusing training more on the truly infor-mative parts of the corpus.5 Se lec t ion  A lgor i thmsWithin the committee-based paradigm there existdifferent methods for selecting informative examples.Previous research in sample selection has used eithersequential selection (Seung, Opper, and Sompolin-sky, 1992; Freund et al, 1993; Dagan and Engelson,1995), or batch selection (Lewis and Catlett, 1994;Lewis and Gale, 1994).
We describe here generalalgorithms for both sequential and batch selection.Sequential selection examines unlabeled examplesas they are supplied, one by one, and measures thedisagreement in their classification by the commit-tee.
Those examples determined to be sufficientlyinformative are selected for training.
Most simply,we can use a committee of size two and select anexample when the two models disagree on its clas-sification.
This gives the following, parameter-free,two  member  sequent ia l  se lect ion a lgor i thm,executed for each unlabeled input example :1.
Draw 2 models randomly from P(MIS), whereS are statistics acquired from previously labeledexamples;4The normal approximation, while easy to imple-ment, can be avoided.
The posterior probability P(c~i --ai\[S) for the multinomial is given exactly by the Dirich-let distribution (Johnson, 1972) (which reduces to theBeta distribution in the binomial case).
In this work weassumed a uniform prior distribution for each model pa-rameter; we have not addressed the question of how tobest choose a prior for this problem.3212.
Classify e by each model, giving classificationscl and c~;3.
If cl ~ c~, select e for annotation;4.
If e is selected, get its correct label and updateS accordingly.This basic algorithm needs no parameters.
If de-sired, it is possible to tune the frequency of selection,by changing the variance of P(MIS ) (or the varianceof P(~i = ailS) for each parameter), where largervariances increase the rate of disagreement amongthe committee members.
We implemented this ef-fect by employing a temperature parameter t, usedas a multiplier of the variance of the posterior pa-rameter distribution.A more general algorithm results from allowing(i) a larger number of committee members, k, in or-der to sample P(MIS ) more precisely, and (it) morerefined example selection criteria.
This gives the fol-lowing genera l  sequent ia l  se lect ion a lgor i thm,executed for each unlabeled input example :1.
Draw k models {Mi) randomly from P(MIS )(possibly using a temperature t);2.
Classify e by each model Mi giving classifica-tions {ci);3.
Measure the disagreement D over {ci);4.
Decide whether to select e for annotation, basedon the value of D;5.
If e is selected, get its correct label and updateS accordingly.It is easy to see that two member sequential selec-tion is a special case of general sequential selection,where any disagreement is considered sufficient forselection.
In order to instantiate the general algo-rithm for larger committees, we need to define (i) ameasure for disagreement (Step 3), and (it) a selec-tion criterion (Step 4).Our approach to measuring disagreement is to usethe vote entropy, the entropy of the distribution ofclassifications assigned to an example ('voted for')by the committee members.
Denoting the numberof committee members assigning c to e by V(c, e),the vote entropy is:1 V(e, e) log V(e, e)D-logk ke(Dividing by log k normalizes the scale for the num-ber of committee members.)
Vote entropy is maxi-mized when all committee members disagree, and iszero when they all agree.In bigram tagging, each example consists of a se-quence of several words.
In our system, we measureD separately for each word, and use the average n-tropy over the word sequence as a measurement ofdisagreement for the example.
We use the averageentropy rather than the entropy over the entire se-quence, because the number of committee membersis small with respect o the total number of possibletag sequences.
Note that we do not look at the en-tropy of the distribution given by each single modelto the possible tags (classes), since we are only in-terested in the uncertainty of the final classification(see the discussion in Section 7).We consider two alternative selection criteria (forStep 4).
The simplest is thresholded seleclion, inwhich an example is selected for annotation if itsvote entropy exceeds ome threshold 0.
The otheralternative is randomized selection, in which an ex-ample is selected for annotation based on the flipof a coin biased according to the vote entropy--ahigher vote entropy entailing a higher probability ofselection.
We define the selection probability as alinear function of vote entropy: p = gD, where g isan entropy gain parameter.
The selection methodwe used in our earlier work (Dagan and Engelson,1995) is randomized sequential selection using thislinear selection probability model, with parametersk, t and g.An alternative to sequential selection is batch se-lection.
Rather than evaluating examples individ-ually for their informativeness a large batch of ex-amples is examined, and the m best are selected forannotation.
The batch  se lect ion  a lgor i thm,  exe-cuted for each batch B of N examples, is as follows:1.
For each example in B:(a) Draw k models randomly from P(MIS);(b) Classify e by each model, giving classifica-tions {ci};(c) Measure the disagreement De for e over{ei};2.
Select for annotation the m examples from Bwith the highest De;3.
Update S by the statistics of the selected exam-ples.This procedure is repeated sequentially for succes-sive batches of N examples, returning to the start ofthe corpus at the end.
If N is equal to the size of thecorpus, batch selection selects the m globally bestexamples in the corpus at each stage (as in (Lewisand Catlett, 1994)).
On the other hand, as N de-creases, batch selection becomes closer to sequentialselection.6 Exper imenta l  Resu l tsThis section presents results of applying committee-based sample selection to bigram part-of-speech tag-ging, as compared with complete training on all ex-amples in the corpus.
Evaluation was performedusing the University of Pennsylvania tagged corpusfrom the ACL/DCI CD-ROM I.
For ease of im-plementation, we used a complete (closed) lexiconwhich contains all the words in the corpus.The committee-based sampling algorithm was ini-tialized using the first 1,000 words from the corpus,32235000 t250002OO0O15OOOI00005OOOI I I I I i I IBatch selection ira=5; N=I00)Thresholded sel&-lion (fi,~0.2) ......Randomized selection (.g=0.5) ......Two metnber s lection ....... ,Co~l~ training/!
// / :!
"i /i /  : , "  / n , .
y .
.
,~ 1  I I I0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93Accuracy(a)/ I I I I I0.96 ~- Batch selection (m=5; N=IO0) - -| Th~sholded s lection (th=0.3) .....\[ Randomized s lection (g=0.5) ......0.94 1- Two member s lection ---Complete training ...... .......................0.92 I ..... " ~'--'--"~--':":=" ............ "l 0.9\[/:::.
::yi / ,' fi '.. : I  0.88 ~ /../0.860 50000 I00000 150000 200000 250000 300000Examined training(b)Figure 1: Training versus accuracy.
In batch, random,and thresholded runs, k = 5 and t = 50.
(a) Numberof ambiguous words selected for labeling versus classifi-cation accuracy achieved.
(b) Accuracy versus numberof words examined from the corpus (both labeled andunlabeled).and then sequential ly examined the following exam-ples in the corpus for possible labeling.
The trainingset consisted of the first mi l l ion words in the cor-pus, with sentence ordering randomized to compen-sate for inhomogeneity  in corpus composit ion.
Thetest set was a separate port ion of the corpus, con-sist ing of 20,000 words.
We compare the amountof t ra in ing required by different selection methodsto achieve a given tagging accuracy on the test set,where both the amount  of training and tagging ac-curacy are measured over ambiguous words.
5The effectiveness of randomized committee-based5Note that most other work on tagging has measuredaccuracy over all words, not just ambiguous ones.
Com-plete training of our system on 1,000,000 words gave usan accuracy of 93.5% over ambiguous words, which cor-responds to an accuracy of 95.9% over all words in the0.925 I I I I I I I I I3640 words selected - -0.92 6640 words selected .
.
.
.
.......... ~ 9660 words seleaed ......12660 words seleaed .......0.915 /~o 0.91 / :'f ......: , j  - - .
.8 :.
:: < 0.905 ..............................................0.90.895 i0.890 100 200 300 400 500 600 700 800 900 1000Batch size(a)0.98 , , , , , , ,Two member s lection0.96 Batch selection (m=5; N=50) .....Batch selection (m=5; N=I00) ......Batch selection (m=5; N=-500) ....0.94 Batch selection (m=5; N=IO00) ......co~!~.~.
: : : : : .
.0.92<0.9 t~ J  t0.88 y/.1/./..~'/.
.
...........086I I I I I I I0 50000 100000 150000 200000 250000 300000 350000 400000Examined training(b)Figure 2: Evaluating batch selection, for m = 5.
(a) Ac-curacy achieved versus batch size at different numbers ofselected training words.
(b) Accuracy versus number ofwords examined from the corpus for different batch sizes.selection for part-of-speech tagging, with 5 and 10committee members,  was demonstrated in (Daganand Engelson, 1995).
Here we present and compareresults for batch, randomized,  thresholded, and twomember  committee-based selection.Figure 1 presents the results of compar ing the sev-eral selection methods against  each other.
The plotsshown are for the best parameter  sett ings that  wefound through manual  tuning for each method.
Fig-ure l (a)  shows the advantage that  sample  selectiongives with regard to annotat ion cost.
For example,complete training requires annotated examples con-taining 98,000 ambiguous words to achieve a 92.6%accuracy (beyond the scale of the graph),  while theselective methods require only 18,000-25,000 am-biguous words to achieve this accuracy.
We also findtest set, comparable to other published results on bigramtagging.32320000180001600014000120001000080OO6OO0400O20O0I0.85 0.86I I I ;Two member selectionComplete training" .... /, ///// // (iJI I I0.9 0.91 0.72I I I0.87 0.88 0.89 0.93Accuracy(a)i i 1600 i i i i i iTwo m~mbersel~on -:-140o Complete training },.Z'_./// 12011 /~ 1000 l1/ 'i Nil // /200 i i i i I i I I0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.72 0.93 0.94Accuracy(b)Figure 3: The size of the trained model, measured bythe  number of frequency counts > 0, plotted (y-axis) ver-sus classification accuracy achieved (x-axis).
(a) Lexicaicounts (freq(t, w)) (b) Bigram counts (freq(tl--+t2)).that, to a first approximation, all selection methodsconsidered give similar results.
Thus, it seems that arefined choice of the selection method is not crucialfor achieving large reductions in annotation cost.This equivalence of the different methods alsolargely holds with respect to computational effi-ciency.
Figure l(b) plots classification accuracy ver-sus number of words examined, instead of thoseselected.
We see that while all selective methodsare less efficient in terms of examples examinedthan complete training, they are comparable to eachother.
Two member selection seems to have a clear,though small, advantage.In Figure 2 we investigate further the propertiesof batch selection.
Figure 2(a) shows that accuracyincreases with batch size only up to a point, andthen starts to decrease.
This result is in line withtheoretical difficulties with batch selection (Freundet al, 1993) in that batch selection does not accountfor the distribution of input examples.
Hence, oncebatch size increases past a point, the input distribu-tion has too little influence on which examples areselected, and hence classification accuracy decreases.Furthermore, as batch size increases, computationalefficiency, in terms of the number of examples exam-ined to attain a given accuracy, decreases tremen-dously (Figure 2(5)).The ability of committee-based selection to fo-cus on the more informative parts of the trainingcorpus is analyzed in Figure 3.
Here we examinedthe number of lexical and bigram counts that werestored (i.e, were non-zero) during training, usingthe two member selection algorithm and completetraining.
As the graphs show, the sample selec-tion method achieves the same accuracy as completetraining with fewer lexical and bigram counts.
Thismeans that many counts in the data  are  less usefulfor correct agging, as replacing them with smoothedestimates works just as well.
6 Committee-based se-lection ignores such counts, focusing on parameterswhich improve the model.
This behavior has thepractical advantage of reducing the size of the modelsignificantly (by a factor of three here).
Also, theaverage count is lower in a model constructed byselective training than in a fully trained model, sug-gesting that the selection method avoids using ex-amples which increase the counts for already knownparameters.7 DiscussionWhy does committee-based sample selection work?Consider the properties of those examples that areselected for training.
In general, a selected train-ing example will contribute data to several statistics,which in turn will improve the estimates of severalparameter vMues.
An informative xample is there-fore one whose contribution to the statistics leads toa significantly useful improvement of model parame-ter estimates.
Model parameters for which acquiringadditional statistics is most beneficial can be char-acterized by the following three properties:1.
The current estimate of the parameter is uncer-tain due to insufficient statistics in the trainingset.
Additional statistics would bring the esti-mate closer to the true value.2.
Classification of examples i sensitive to changesin the current estimate of the parameter.
Oth-erwise, even if the current value of the pa-rameter is very uncertain, acquiring additionalstatistics will not change the resulting classifi-cations.3.
The parameter affects classification for a largeproportion of examples in the input.
Parame-6As noted above, we smooth the MLE estimatesby interpolation with a uniform probability distribution(Merialdo, 1994).324ters that affect only few examples have low over-all utility.The committee-based selection algorithms workbecause they tend to select examples that affect pa-rameters with the above three properties.
Prop-erty 1 is addressed by randomly drawing the parame-ter values for committee members from the posteriordistribution given the current statistics.
When thestatistics for a parameter are insufficient, the vari-ance of the posterior distribution of the estimates ilarge, and hence there will be large differences in thevalues of the parameter chosen for different commit-tee members.
Note that property 1 is not addressedwhen uncertainty in classification is only judged rel-ative to a single model 7 (as in, eg, (Lewis and Gale,1994)).Property 2 is addressed by selecting examples forwhich committee members highly disagree in clas-sification (rather than measuring disagreement inparameter estimates).
Committee-based selectionthus addresses properties 1 and 2 simultaneously:it acquires statistics just when uncertainty in cur-rent parameter estimates entails uncertainty regard-ing the appropriate classification of the example.Our results show that this effect is achieved evenwhen using only two committee members to samplethe space of likely classifications.
By appropriateclassification we mean the classification given by aperfectly-trained model, that is, one with accurateparameter values.Note that this type of uncertainty regarding theidentity of the appropriate classification, is differ-ent than uncertainty regarding the correctness of theclassification itself.
For example, sufficient statisticsmay yield an accurate 0.51 probability estimate fora class c in a given example, making it certain thatc is the appropriate classification.
However, the cer-tainty that c is the correct classification is low, sincethere is a 0.49 chance that c is the wrong class forthe example.
A single model can be used to estimateonly the second type of uncertainty, which does notcorrelate directly with the utility of additional train-ing.Finally, property 3 is addressed by independentlyexamining input examples which are drawn from theinput distribution.
In this way, we implicitly modelthe distribution of model parameters used for clas-sifying input examples.
Such modeling is absent inbatch selection, and we hypothesize that this is thereason for its lower effectiveness.8 Conc lus ionsAnnotating large textual corpora for training natu-ral language models is a costly process.
We proposereducing this cost significantly using committee-rThe use of a single model is also criticized in (Cohn,Atlas, and Ladner, 1994).based sample selection, which reduces redundant an-notation of examples that contribute little new in-formation.
The method can be applied in a semi-interactive process, in which the system selects ev-eral new examples for annotation at a time and up-dates its statistics after receiving their labels fromthe user.
The implicit modeling of uncertaintymakes the selection system generally applicable andquite simple to implement.Our experimental study of variants of the selec-tion method suggests everal practical conclusions.First, it was found that the simplest version of thecommittee-based method, using a two-member com-mittee, yields reduction in annotation cost compa-rable to that of the multi-member committee.
Thetwo-member version is simpler to implement, has noparameters to tune and is computationally more ef-ficient.
Second, we generalized the selection schemegiving several alternatives for optimizing the methodfor a specific task.
For bigram tagging, comparativeevaluation of the different variants of the methodshowed similar large reductions in annotation cost,suggesting the robustness of the committee-basedapproach.
Third, sequential selection, which im-plicitly models the expected utility of an examplerelative to the example distribution, worked in gen-eral better than batch selection.
The latter wasfound to work well only for small batch sizes, wherethe method mimics sequential selection.
Increas-ing batch size (approaching 'pure' batch selection)reduces both accuracy and efficiency.
Finally, westudied the effect of sample selection on the size ofthe trained model, showing a significant reductionin model size.8.1 Fur ther  researchOur results suggest applying committee-based sam-ple selection to other statistical NLP tasks whichrely on estimating probabilistic parameters from anannotated corpus.
Statistical methods for thesetasks typically assign a probability estimate, or someother statistical score, to each alternative analysis(a word sense, a category label, a parse tree, etc.
),and then select the analysis with the highest score.The score is usually computed as a function of theestimates of several 'atomic' parameters, often bino-mials or multinomials, uch as:?
In word sense disambiguation (Hearst, 1991;Gale, Church, and Varowsky, 1993): P(sl f  ),where s is a specific sense of the ambiguous wordin question w, and f is a feature of occurrencesof w. Common features are words in the contextof w or morphological ttributes of it.?
In prepositional-phrase (PP) attachment (Hin-dle and Rooth, 1993): P(alf), where a is a pos-sible attachment, such as an attachment to ahead verb or noun, and f is a feature, or a com-bination of features, of the attachment.
Corn-325mon features are the words involved in the at-tachment, such as the head verb or noun, thepreposition, and the head word of the PP.?
In statistical parsing (Black et al, 1993):P(rlh), the probability of applying the rule rat a certain stage of the top down derivation ofthe parse tree given the history h of the deriva-tion process.?
In text categorization (Lewis and GMe, 1994;Iwayama nd Tokunaga, 1994): P(tlC), wheret is a term in the document to be categorized,and C is a candidate category label.Applying committee-based lection to supervisedtraining for such tasks can be done analogously toits application in the current paper s. ~rthermore,committee-based selection may be attempted alsofor training non-probabilistic classifiers, where ex-plicit modeling of information gain is typically im-possible.
In such contexts, committee membersmight be generated by randomly varying some ofthe decisions made in the learning algorithm.Another important area for future work is in de-veloping sample selection methods which are inde-pendent of the eventual learning method to be ap-plied.
This would be of considerable advantage indeveloping selectively annotated corpora for generalresearch use.
Recent work on heterogeneous ncer-tainty sampling (Lewis and Catlett, 1994) supportsthis idea, using one type of model for example selec-tion and a different type for classification.Acknowledgments.
We thank Yoav Freund andYishay Mansour for helpful discussions.
The firstauthor gratefully acknowledges the support of theFulbright Foundation.Re ferencesBlack, Ezra, Fred Jelinek, John Lafferty, DavidMagerman, Robert Mercer, and Salim Roukos.1993.
Towards history-based grammars: usingricher models for probabilistic parsing.
In Proc.of the Annual Meeting of the ACL, pages 31-37.Church, Kenneth W. 1988.
A stochastic parts pro-gram and noun phrase parser for unrestricted text.In Proc.
of ACL Conference on Applied NaturalLanguage Processing.Cohn, David, Les Atlas, and Richard Ladner.
1994.Improving generalization with active learning.Machine Learning, 15:201-221.SMeasuring disagreement i  full syntactic parsing iscomplicated.
It may be approached by similar methodsto those used for parsing evaluation, which measure thedisagreement between the parser's output and the cor-rect parse.Dagan, Ido and Sean Engelson.
1995.
Committee-based sampling for training probabilistic lassi-tiers.
In Proc.
Int'l Conference on Machine Learn-ing, July.Elworthy, David.
1994.
Does Baum-Welch re-estimation improve taggers?
In Proc.
of A CLConference on Applied Natural Language Process-ing, pages 53-58.Freund, Y., H. S. Seung, E. Shamir, and N. Tishby.1993.
Information, prediction, and query by com-mittee.
In Advances in Neural Information Pro-cessing, volume 5.
Morgan Kaufmann.Gale, William, Kenneth Church, and DavidYarowsky.
1993.
A method for disambiguatingword senses in a large corpus.
Computers and theHumanities, 26:415-439.Hearst, Marti.
1991.
Noun homograph disambigua-tion using local context in large text corpora.
InProc.
of the Annual Conference of the UW Centerfor the New OED and Text Research, pages 1-22.Hindle, Donald and Mats Rooth.
1993.
Structuralambiguity and lexical relations.
ComputationalLinguistics, 19(1):103-120.Iwayama, M. and T. Tokunaga.
1994.
A probabilis-tic model for text categorization based on a sin-gle random variable with multiple values.
In Pro-ceedings of the .4th Conference on Applied NaturalLanguage Processing.Johnson, Norman L. 1972.
Continuous MultivariateDistributions.
John Wiley & Sons, New York.Kupiec, Julian.
1992.
Robust part-of-speech taggingusing a hidden makov model.
Computer Speechand Language, 6:225-242.Lewis, David D. and Jason Catlett.
1994.
Heteroge-neous uncertainty sampling for supervised learn-ing.
In Proc.
lnt'l Conference on Machine Learn-ing.Lewis, David D. and William A. Gale.
1994.
Asequential algorithm for training text classifiers.In Proc.
of the ACM SIGIR Conference.MacKay, David J. C. 1992.
Information-based ob-jective functions for active data selection.
NeuralComputation, 4.Merialdo, Bernard.
1994.
Tagging text with aprobabilistic model.
Computational Linguistics,20(2):155-172.Press, William H., Brian P. Flannery, Saul A.Teukolsky, and William T. Vetterling.
1988.Numerical Recipes in C. Cambridge UniversityPress.Seung, H. S., M. Opper, and H. Sompolinsky.
1992.Query by committee.
In Proc.
A CM Workshop onComputational Learning Theory.326
