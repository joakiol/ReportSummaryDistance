Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 524?533,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPMining Search Engine Clickthrough Log forMatching N-gram FeaturesHuihsin Tseng, Longbin Chen, Fan Li, Ziming Zhuang,Lei Duan, Belle TsengYahoo!
Inc., Santa Clara, CA 95054{huihui,longbin,fanli,ziming,leiduan,belle}@yahoo-inc.comAbstractUser clicks on a URL in response to a query areextremely useful predictors of the URL?s rele-vance to that query.
Exact match click featurestend to suffer from severe data sparsity issues inweb ranking.
Such sparsity is particularly pro-nounced for new URLs or long queries whereeach distinct query-url pair will rarely occur.
Toremedy this, we present a set of straightforwardyet informative query-url n-gram features that al-lows for generalization of limited user click datato large amounts of unseen query-url pairs.
Themethod is motivated by techniques leveraged inthe NLP community for dealing with unseenwords.
We find that there are interesting regulari-ties across queries and their preferred destinationURLs; for example, queries containing ?form?tend to lead to clicks on URLs containing ?pdf?.We evaluate our set of new query-url features ona web search ranking task and obtain improve-ments that are statistically significant at a p-value< 0.0001 level over a strong baseline with exactmatch clickthrough features.1 IntroductionClickthrough logs record user click behaviors,which are a critical source for improving searchrelevance (Bilenko and White, 2008; Radlinski etal., 2007; Agichtein and Zheng, 2006; Lu et al2006).
Previous work (Agichtein et al, 2006)demonstrated that clickthrough features (e.g.,IsNextClicked and IsPreviousClicked) can leadto substantial improvements in relevance.
Suchfeatures summarize query-specific user interac-tions on a search engine.
One commonly usedclickthrough feature is generated based on thefollowing observation: if a URL receives a largenumber of first and last clicks across many usersessions, then it indicates that this URL might bea strongly preferred destination of a query.
Forexample, when a user searches for ?yahoo?, theytend to only click on the URL www.yahoo.comrather than other alternatives.
This results inwww.yahoo.com being the first and last clickedURL for the query.
We refer to such behavior asbeing navigational clicks (NavClicks).
Featuresthat use exact query and URL string matches(e.g., NavClick, IsNextClicked and IsPrevious-Clicked)  are referred to as exact match features(ExactM) for the remainder of this paper.The coverage of ExactM features is sparse, espe-cially for long queries and new URLs.
Manylong queries are either unique or very low fre-quency.
Hence, the improvements from ExactMfeatures are limited to the more popular queries.In addition, ExactM features tend to be weightedheavily in the ranking of results when they areavailable.
This introduces a bias where the rank-ing models tend to strongly favor older URLsover new URLs even when the latter otherwiseappear to be more relevant.By inspecting the clickthrough logs, we observedthat unseen query-url pairs are often composed ofinformative previously observed subsequences.Specifically, we saw that query n-grams can becorrelated with sequences of URL n-grams.
Forexample, we find that there are interesting regu-larities across queries and URLs, such as queriescontaining ?form?
tending to lead to clicks onURLs containing ?pdf?.
This strongly motivatesthe adoption of an approach similar to the Natu-ral Language Processing (NLP) technique of us-ing n-grams to deal with unseen words.
For ex-ample, part-of-speech tagging (Brants, 2000) andparsing (Klein and Manning, 2003) both requiredealing with unknown words.
By using n-gramsubstrings, novel items can be dealt with usingany informative substrings they contain that wereactually observed in the training data.524The remainder of the paper is organized as fol-lows.
In Section 2, we introduce our overall me-thodology.
Section 2.1 presents a data miningmethod for building a query-url n-gram diction-ary, Section 2.2 describes the new ranking fea-tures in detail.
In section 3, we present our ex-perimental results.
Section 4 discusses relatedwork, and Section 5 summarizes the contributionof this work.2 MethodologyThis section describes the detailed methodologyused in generating the query-url n-gram features.Our features require association scores to be pre-viously calculated, and, hence, we first introducea data mining approach that is used to build anassociation dictionary in Section 2.1.
Then, wepresent the procedure used to generate the query-url n-gram features that use the dictionary in Sec-tion 2.2.Figure 1: Steps to build a query-url n-gram dic-tionary2.1 Data Mining on a Query-URLN-gram DictionaryThe steps involved in building the dictionary areshown in Figure 1.
We first collect seed query-url pairs from clickthrough data based on Nav-Clicks.
The queries and URLs from the collectedpairs are tokenized and converted into a collec-tion of paired query-url n-grams.
For each pair,we calculate the mutual information of the queryn-gram and its corresponding URL n-gram.
Forour experiment, we collect a total of more than15M seed pairs and 0.5B query-url n-gram pairsusing six months of query log data.
The detailsare described in the following sections.2.1.1 Seed ListWe identify the seed list based on characteristicuser click behavior.
Given a query, we select theURL with the most NavClicks as compared toother URLs returned.
During data collection, therank positions of the top 5 URLs were shuffled toavoid the position bias.
We aggregate NavClicksfor a URL occurring in these positions in order toboth obtain more click data and to avoid the posi-tion bias issue discussed in Dupret and Piwowar-ski (2008) and Craswell et al (2008).For example, in Figure 1, the numbers of Nav-Clicks for the top three URLs are shown.
TheURL www.irs.gov/pub/irs-pdf/f1040.pdf receivesthe largest number of NavClicks, and, therefore,it is used to create the query-url pair:[irs 1040 form, www.irs.gov/pub/irs-pdf/f1040.pdf]2.1.2 Query and URL SegmentationWe segment the seed pairs to n-gram pairs inorder to increase the coverage beyond that ofExactM click features.
Within NLP, n-grams aretypically extracted such that words that are adja-cent in the original sequence are also adjacent inthe extracted n-grams.
Furthermore, we attemptto achieve additional generalization by using skipn-grams (Lin and Och, 2004).
This means we notonly extract n-grams for adjacent terms but alsofor sequences that leave out intermediate terms.This is motivated by the observation that the se-mantics of user queries is often preserved evenwhen some intermediate terms are removed.
Thedetails of the segmentation methods are de-scribed below.2.1.2.1 Query SegmentationPrior to query segmentation, we normalize rawqueries by replacing punctuations with spaces.Queries are then segmented into a sequence of525space delimited tokens.
From these, we extractall possible query n-grams and skip n-grams forn smaller than or equal to three (i.e., all unigrams,bigrams, and trigrams).
For example, given thesequence ?irs 1040 form?
the adjacent bigramswould be ?irs 1040?
and ?1040 form?.
With skipn-grams we also extract ?irs form?
as shown inTable 1.
We do not use n-grams longer than 3 inorder to avoid problems with overfitting.
We willrefer to this segmentation method as Affix Seg-mentation.Table 1: An Example of Affix SegmentationN-gram Affix SegmentationUnigram irs, 1040, formBigram irs 1040, 1040 form, irs formTrigram irs 1040 form2.1.2.2 URL SegmentationAs shown in Table 2, after the queries are seg-mented, URLs are categorized into four groups:domain, URL language, URL region and URLpath.
In general, a URL is delimited by punctua-tion characters such as ??
?, ?.?,?
?/?, and ?=?.Table 2: An Example of URL SegmentationURL Groups ExampleDomain irs.govURL language enURL region usURL path pub, irs, pdf, f1040, pdfThe domain group includes one domain token,for example, irs.gov.
Although domains could bedivided into multiple n-grams, we treat them as asingle unit, with the exception of encoded lan-guage and region information.The language and region groups are based on thelanguage or region part of the URL n-grams suchas the suffixes ?.en?
and ?.de?.
The language andregion of a URL n-gram are identified by a tablelook-up method.
The table is created based onthe information available at en.wikipedia.org/wiki/List_of_ISO_639-1_codes and en.wikipedia.org/wiki/ISO_3166.
When there is no clear lan-guage or region URL n-gram, we use English (en)as the default language and United States (us) asthe default region.2.1.3 Calculation of Mutual InformationAfter query and URL n-grams are extracted, wecalculate mutual information (Gale and Church,1991) to determine the degree of association be-tween the n-grams.
The definition of query-url n-gram mutual information (MI) is given in Equa-tion 1.
)Freq( )Freq(),Freq(log2),MI( uququq =   (1)Here q corresponds to a query n-gram and u cor-responds to a URL n-gram.
Freq (q) is the countof q in the seed list normalized by the total num-ber of q. Freq (u) is the count of u normalized bythe total number of u.  Freq (q, u) is the count ofq and u that co-occurred in a full query-url pairnormalized by the total number of q and u.
A pairwill be assigned to a MI score of zero if the itemsoccur together no more than expected by chance,under the assumption that the two items are sta-tistically independent.
When a pair occurs morethan is expected by chance, the MI score is posi-tive.
On the other hand, if a pair occurs togetherless than is expected by chance, the mutual in-formation score is negative.
In order to increasethe confidence of the MI scores, we remove alln-grams with less than 3 occurrences in the seedlist, and assign a zero MI score for any pairs in-volving these n-grams.
No smoothing is applied.This scoring scheme fits well with the associa-tion properties we would like to have for ourquery-url n-gram click features.
If a query n-gram cues for a certain URL through one of its n-grams, the feature will take on a positive value.Similarly, if a query n-gram cues against a cer-tain URL, the feature will take on a negative val-ue.2.1.4 Analysis of Query-URL N-gramAssociationBy examining our dictionary, we observed anumber of pairs that are interesting from a rele-vance ranking perspective.
To illustrate, we pre-sent four examples of n-gram pairs and intui-tively explore the nature of the n-gram associa-tions in the dictionary.Table 3: Examples of MI ScoresQuery n-gram URL n-gram MI score?iphone?
apple.com 8.7713?iphone?
amazon.com -0.1555?iphone plan?
att.com 11.5388?iphone plan?
apple.com 8.9676First, let?s examine the association betweenquery n-grams and URL n-grams for the queries526?iphone?
and ?iphone plan?.
Notice that thequery unigram ?iphone?
is strongly associatedwith apple.com, but negatively associated withamazon.com.
This can be explained by the factthat ?iphone?
as a product is not only developedby Apple but also strongly associated with theApple brand.
In contrast, while Amazon.comsells iphones, it also sells a large variety of otherproducts, thus is not regarded as a very authorita-tive source of information about the ?iphone?.However, by adding additional context, the mostpreferred URL according to MI can change.
Thetwo examples in the bottom of Table 3 illustratethe URL preferences for the query bigram?iphone plan?.
While apple.com is still a stronglypreferred destination, there is a much strongerpreference for att.com.
This preference followssince apple.com has more product information onthe ?iphone?
while the information provided byatt.com will be more targeted at visitors whowant to explore what rate plans are available.Second, Table 4 shows the association between?kimo?, ?.tw?
and ?.us?.
?Kimo?
was a Taiwan-ese start-up acquired by Yahoo!.
The mutual in-formation scores accurately reflect the associa-tion between the query n-gram and region ids.Table 4: Example of MI ScoresQuery n-gram URL n-gram MI score?kimo?
tw (taiwan) 12.8303?kimo?
us (united states) 0.7209Third, Table 5 shows the association between?kanji?, and URLs with Language identificationof ?Japanese?, ?Chinese?
and ?English?.
?Kanji?means ?Chinese?
in Japanese.
Since queries con-taining ?Kanji?
are typically from users inter-ested in Japanese sites, the mutual informationshows higher correlation with Japanese than withEnglish or Chinese.Table 5: Example of MI ScoresQuery n-gram URL n-gram MI score?kanji?
ja (japanese) 11.3862?kanji?
zh (chinese) 6.2567?kanji?
en (english) 4.2110Table 6: Example of MI ScoreQuery n-gram URL n-gram MI score?form?
pdf 4.9067?form?
htm 1.0916?video?
watch 5.7192?video?
htm -1.9079Fourth, Table 6 shows the association betweentwo query n-grams, ?form?
and ?video?, that atfirst glance may not actually look very informa-tive for URL path selection.
However, notice thatthe unigram ?form?
has a strong preference forpdf documents over more standard web pageswith an html extension.
Similarly, queries thatinclude ?video?
convey a preference for URLscontaining ?watch?, a characteristic URL n-gramfor many video sharing websites.It is reasonable to anticipate that incorporatingsuch associations into a search engine?s rankingfunction should help improve both search qualityand user experience.
Take the example where,there are two high ranking competing URLs forthe query ?irs 1040 form?.
Let?s also assumeboth documents contain the same query relevantkeywords, but one is an introduction of the ?irs1040 form?
as an htm webpage and the other oneis the real filing form given as a pdf document.Since in our dictionary, ?form?
is more associ-ated with pdf than htm, we predict that most us-ers would prefer the real pdf form directly, so itshould be placed first in the list of query results.While click data for the exact query-url pairsconfirms this preference, it is reassuring that wecould identify it without needing to rely on see-ing the specific query string before.
As describedin detail below, and motivated by this analysis,we designed our query-url click features basedon the contents of the n-gram MI dictionary.2.2 Query-URL N-gram FeaturesFor our feature set, we explored the use of differ-ent query segmentation approaches (concept andaffix segmentation) in order to increase the di-versity of n-grams.
In the following section, weuse an unseen query ?irs 1040 forms?
and con-trast it with the known query ?irs 1040 form?from the last section.2.2.1 Concept Segmentation FeaturesQuery concept segmentation is a weighted querysegmentation approach.
Each query is analyti-cally interpreted as being a main concept and asub concept.
We search for the unique segmenta-tion of the query that maximizes its cumulativemutual information score with the URL n-grams.Main concepts and sub concepts are n-gramsfrom the query that have the strongest associationwith URL n-grams and thus assist in identifyingrelevant landing URL n-grams when the wholequery or the whole URL has not been seen.527Algorithm 1: Concept Segmentationfor U = domain, URL language, URL region,URL path dofor j = 0... n-1 doM  ?
W0...jS   ?
Wj+1...nfor k = 0... m docurr_mi_M ?
arg maxk=1...m  MI (M, Uk)curr_mi_S ?
arg maxk=1...m  MI (S, Uk)if curr_mi_M + curr_mi_S > curr_bestthencurr_best = curr_mi_M + curr_mi_Smi_M ?
curr_mi_Mmi_S ?
curr_mi_Send ifend foradding mi_M as a featureadding mi_S as a featureend forend forPseudo-code for generating query-url n-gramfeatures based on the concept segmentation isgiven in Algorithm 1.
Each query (Q) is com-posed of a number of words, w1, w2, w3?,wn.Each URL is segmented and categorized to fourgroups: domain, URL language, URL region andURL path.
Each URL group has m number ofURL n-grams.
M is the main concept of Q and Sis the sub concept of Q.One potential drawback of such concept segmen-tation is data sparsity.
When we look for themaximum of cumulative mutual information, wemay obtain main concepts with very high mutualinformation and sub concepts which do not existin the dictionary.
In order to address this problem,we implement a second query segmentation me-thod, affix segmentation, that is discussed in sec-tion 2.2.2.Table 7 shows eight concept segmented features.?Coverage?
is the percentage of query-url pairsthat have valid feature values.
Some of the sam-ples do not have values because no clicks for thepairs were seen in the sample of data used tobuild the dictionary.
When a pair does not have avalue, the default value of zero is assigned.
Thisdefault value is based on the assumption thatunless we have evidence otherwise, we assumeall query-url n-grams are statistically independ-ent and thus provide no preference signal.Table 7: Eight Features Generated based onConcept Segmentation.Feature Query N-gramURL N-gramCoverage(%)MainDS M domain 54.09SubDS S domain 30.46MainLang M lang.
94.41SubLang S lang.
72.40MainReg M reg.
90.34SubReg S reg.
68.19MainPath M path 64.96SubPath S path 58.76Query-URL Domain Features are defined asthe mutual information of a query n-gram and thedomain level URL.
There are two features in thiscategory, one for the query main concept and onefor the sub concept.
They help to identify theuser preferred host given a query.Table 8: Example of Selecting Query Segmenta-tionMI(q,u) irs.gov?irs?
11.2174?1040?11.6175?forms?
7.504911.5550Cumulative MI 19.1224 22.7724Seg.
1 Seg.2To illustrate the concept segmentation features,let?s examine the query, ?irs 1040 forms?
in thecontext of the domain irs.gov.
The query ?irs1040 forms?
can be segmented either as ?irs1040?
and ?forms?
or as ?irs?
and ?1040 forms?.As shown in Table 8, taking the cumulative max-imum, the second segmentation scores higherthan the first one.
Therefore, the ?irs?
and ?1040forms?
segmentation is preferred.
The featurevalue for the main concept is 11.5550, and thesub concept is then assigned to be 11.2174.Query-URL Language and Region Featuresare the mutual information of a query n-gram andURL language/region.
They are used for provid-ing language and region information.Query-URL Path Features are the mutualinformation of a query n-gram and a URL path n-gram.
While there are typically many URL pathn-grams, only one URL path n-gram is selectedto be paired with each query n-gram.
The se-lected n-gram is the one that achieves the highest528cumulative maximum MI score.
They are usedfor providing association between query n-gramsand url n-grams such as ?forms?
and ?pdf?.2.2.2 Affix Segmentation FeaturesAs previously mentioned, affix segmentationaddresses sparsity issues associated with conceptsegmentation.
Here, we introduce the featuresgenerated based on affix segmentation.
Pseudo-code for generating the features is given in Algo-rithm 2.
Two query unigrams (w0 and wn) andone bigram (w0wn) is used.
Each URL is seg-mented and categorized to four groups: domain,URL language, URL region and URL path.
EachURL group has m number of URL n-grams.This approach is complementary to the conceptsegmentation for long queries.
The affix n-gramsare in smaller unit, and therefore, are less sparse.In addition, the skip bigrams allow for generali-zations using non-adjacent terms.
Table 9 showsthe coverage of the twelve affix features.Algorithm 2:  Affix Segmentationfor U = domain, URL language, URL region,URL path dofor q = w0, wn, w0wn dofor k = 0... m docurr_mi_q ?
arg maxk=1...m  MI (q, Uk)if curr_mi_q > curr_best thencurr_best = curr_mi_qend ifend foradding curr_mi_q as a featureend forend forTable 9: Twelve Features Generated based onAffix SegmentationFeature Query N-gramURL N-gramCoverage(%)PreDS w0 domain 48.09SufDS wn domain 47.72PresufDS w0wn domain 23.57PreLang w0 lang.
55.58SufLang wn lang.
58.22PresufLang w0wn lang.
24.91PreReg w0 reg.
93.82SufReg wn reg.
93.59PresufReg w0wn reg.
69.29PrePath w0 path 98.15SufPath wn path 97.80PresufPath w0wn path 75.81Query-url domain affix features has three fea-tures: MI(w0, domain), MI(wn, domain), andMI(w0wn, domain).
In the example of ?irs 1040forms?
and ?irs.gov?, the features are MI(irs,irs.gov), MI(forms, irs.gov), and MI(irs forms,irs.gov).Query-url language and region affix featureshas three features respectively: MI(w0, language),MI(wn, language), MI(w0wn, language) MI(w0,region), MI(wn, region), and MI(w0wn, region).In the example of ?irs 1040 forms?, ?en?
and?us?, the features are MI (irs, en), MI (forms, en),MI (irs forms, en), MI (irs, us), MI (forms, us),and MI (irs forms,us).Query-url path affix features has three fea-tures: MI(w0, path), MI(wn, path), and MI(w0wn,path).
In the example of ?irs 1040 forms?
and?www.irs.gov/pub/irs-pdf/f1040.pdf?, there arefour URL path n-grams, ?pub?, ?irs?, ?pdf?, and?f1040?.
The URL path n-gram, irs, gets maxi-mum MI score.
Therefore, the query-url path af-fix features are MI (irs, irs), MI (forms, irs), andMI (irs forms, irs).We demonstrated the procedure to generate 20query-url n-gram features, and in Section 3, wewill present their effectiveness in relevance rank-ing.3 ExperimentWe evaluate the performance of query-url n-grams features (8 concept and 12 affix features)on a ranking application and analyze the resultsfrom several different perspectives.3.1 DatasetsFor all experiments, our training and test data arequery-url pairs annotated with human judgments.In our data, we use five grades to evaluate rele-vance of a query and URL pair.The data includes 94K queries for training and3.4K queries for evaluation, and each query isassociated with the top ranked URLs returnedfrom a search engine.
Totally, there are 916Kquery-url pairs for training and 42K pairs fortesting.
The queries are general and uniformlyand randomly sampled with replacement, result-ing in more frequent queries also appearing morefrequently in our training and test sets.5293.2 Ranking AlgorithmGBRank is a supervised learning algorithm thatuses boosted decision trees and incorporates thepair-wise information from the training data(Zheng et al 2007).
It is able to deal with a largeamount of training data with hundreds of features.We use an internal C++ implementation ofGBRank.3.3 Evaluation MetricWe use Discounted Cumulative Gain (J?rvelinand Kek?l?inen, 2002) to evaluate our rankingaccuracy.
Discounted Cumulative Gain (DCG)has been widely used in evaluating the quality ofsearch engine rankings and is defined as:?=+=kiik iGDCG1 2 )1(log(2)Gi represents the editorial judgment of the i-thdocument.
In this paper, we only report normal-ized DCG5, which is an absolute DCG5 normal-ized by a baseline, and relative DCG5 im-provement, which is an improvement normal-ized by the baseline.
Note normalized DCG5 isdifferent than NDCG (Normalized DiscountedCumulative Gain defined in J?rvelin andKek?l?inen, 2002).
We use Wilcoxon signed test(Wilcoxon, 1945) to evaluate the significance formodel comparison.3.4 Feature SetsFive feature sets are used in our experiments.Details are listed in Table 10.Table 10: Five Feature SetsTag DescriptionBase FeatureSetCore Feature Set and ExactMclick featuresQ-U N-gramFeature Set (I)Base Feature Set and Q-U N-gram featuresCore FeatureSetquery-based, document-based,query-document based fea-turesNavClick Fea-ture SetCore Feature Set and Nav-ClickQ-U N-gramFeature Set (II)Core Feature Set and Q-U N-gram featuresBase Feature Set is a strong baseline feature setfrom a state-of-the-art commercial search engine.This set includes NavClick features, and otherinternal ExactM click features.
It is used forevaluating Query-URL N-gram Feature Set (I) inorder to know whether query-url n-gram featurescan achieve gains when stacked on top of Ex-actM features.Core Feature Set is a weaker variant of thebaseline system that excludes ExactM click fea-tures.
This system is used for evaluatingNavClick Feature Set and Query-URL N-gramFeature Set (II) independently in order to studyand contrast the effected queries.3.5 Experimental ResultsWe compare the query-URL N-gram feature set(I) with the base feature set in Section 3.5.1, andcontrast the NavClick features and the query-URL N-gram features (II) using the Core FeatureSet in Section 3.5.2.3.5.1 Query-URL N-gram Feature Set (I)versus Base Feature SetAs shown in Figure 2, Query-URL N-gram Fea-ture Set (I) outperforms Base Feature Set.
Theadditional 20 query-url n-gram features achievestatistically significant gains at a p-value <0.0001 level, suggesting that they are compli-mentary to ExactM click features.
Even thoughthe query-url n-gram features are generated fromthe same data as the ExactM features, the gain isadditive and stackable.
The DCG5 impact is0.53% relative improvement when runningGBRank using 2500 trees.
Every data point isnormalized by the DCG5 of the baseline featureset using 2500 trees.
This is represented in thegraph as the rightmost point of Base Feature Setcurve.0.960.970.980.9911.01500 1000 1500 2000Q-U N-gram Features (I)Base FeaturesNavClickQ-U N-gram Features (II)Core FeaturesFigure 2: Comparison of the five feature sets onthe normalized DCG5 (Y-axis) against number oftrees (X-axis).5303.5.2 NavClick and Query-URL N-gramFeature Set (II) versus Core Fea-ture SetWe compare NavClick Feature Set and Query-URL N-gram Feature Set (II) in the context ofCore Feature Set, in order to evaluate the twoindependently.
As shown in Figure 2, bothNavClick and Query-URL N-gram Feature Set(II) outperform Core Feature Set.
It is not sur-prising that NavClick also outperforms Query-URL N-gram Feature Set (II) since the n-gramfeatures are backoff of NavClick.
However, theirgains are competitive suggesting the query-url n-gram features are very good relevance indicators.The impact of NavClick and Query-URL N-gramFeature Set (II) is 0.72% and 0.62% relativeDCG5 improvement at Tree 2500 respectively.3.5.3 Feature ImportanceUsing the GBRank model, features are evaluatedand sequentially selected to build the boosteddecision trees.
The split of each node increasesthe DCG during training.
We evaluate a feature?simportance by aggregating the DCG impact ofthe feature over all trees (Zheng et al, 2007).Here, the feature importance is rescaled so thatthe feature with largest DCG impact is assigned anormalized score of 1.
Figure 3 illustrates therelative influence of each of query-url n-gramfeature.
Of these, n-gram features associated witha domain name (i.e., MainDS) rank highest.Figure 3: Feature importance of query-url n-gram features.
The importance (Y axis) is nor-malized so that the most important feature(MainDS)?s importance is 1.3.6 AnalysisWe access system performance with respect toboth query length and frequency using the twoclick features sets in combination with the CoreFeature Set in order to gain insight into the ef-fected queries.3.6.1 Query LengthAs shown in Table 11, NavClick (NavClick Fea-ture Set) best improves relevance for two wordqueries.
In contrast, Query-url n-gram features inisolation (Query-URL N-gram Feature II) areable to show sizable improvements on longerqueries, while slightly degrading performance onshort 1-word queries.
Using both feature sets to-gether (Query-URL N-gram Feature I) results inimprovement for queries of all lengths.These results suggest that the strong signal beingprovided by NavClick for short queries helps tocompensate for any additional noisy introducedby the n-gram features, while allowing the n-gram features to handle  longer queries that areless well covered by NavClick.
These longerqueries are exactly the type of queries our query-url n-gram features were designed to help with.Table 11: Relative DCG5 Improvement ofNavClick, Query-URL N-gram (II), and Query-URL N-gram Features  (I) vs Core Feature SetLength NavClickvs Core(%)QU N-gram (II)vs Core(%)QU N-gram (I)vs Core(%)1 word 0.03 -0.04 0.622 words 1.04 1.06 1.583 words 1.00 1.44 2.124+ words 0.4 0.68 1.013.6.2 Query FrequencyWe found that query-url n-gram features improvetail queries.
Head queries are considered as toptwo million frequent queries in our traffic andtail queries include anything outside of that range.Table 12: Relative DCG5 Improvement ofNavClick, Query-URL N-gram Features (II) andQuery-URL N-gram Features (I) vs Core FeatureSetNavClick vsCore (%)QU N-gram(II) vs Core(%)QU N-gram (I) vsCore (%)Head 0.91 -0.15 1.11Tail 0.59 1.11 1.40As shown in Table 12, query-url n-gram features(Query-URL Feature Set II) differ fromNavClick (NavClick Feature Set) in that they get531more gain from tail queries.
Together, they(Query-URL Feature Set I) improve both headand tail queries.3.7 Case StudyBelow we examine queries from the test set andanalyze the effects of Query-URL N-gram Fea-ture Set (II) versus Core Feature Set.3.7.1 Positive Cases1) Animal shelter in va: this query targets a spe-cific geographic location.
Using the baseline fea-ture set, the root url wvanimalshelter.org is incor-rectly ranked higher than www.netpets.com/cats/catresc/virginia.htm.
Without any addition-ally ranking information, general URLs (root)tend to be ranked more highly than more specificURLs (path), as the root pages tend to be morepopular.
However, our new features express apreference between ?va?
and ?virginia?, and thiscorrectly flips the ranking order.2) Myspace profile generator: www.
myspacgens.com/handler.php?gen=profile was incorrectlyranked higher than www.profilemods.com/myspace-generators.
Our new features convey ahigh user preference association between ?profilegenerator?
and the domain profilemods.com,which helps to correctly swap the order.3.7.2 Negative CasesWe determined that negative cases where thebaseline feature set outperforms the new featuresare typically one word navigational queries suchas ?craigslist?.
However, after we combine thequery-url n-gram features with NavClick, oneword navigational queries are ranked correctly.4 Related WorkOur work is mainly related to Gao et al (2009)and Bilenko and White (2008).
Gao et al (2009)addressed the sparsity issue by propagating clickinformation among similar queries in the samecluster.
Their idea is based on an observation thatsimilar queries go to similar pages.
When twoqueries have similar clicked URLs, it is likelythat they share clicked URLs.
In contrast, ouridea is to utilize NLP techniques to break downlong, infrequent queries into shorter, frequentqueries.
The two approaches can be mutuallybeneficial.
Bilenko and White (2008) expandedclick data with a search engine by using post-search user experience collected from toolbars.Toolbars keep track of users?
click behavior bothwhen they are using the search engine directlyand beyond.
Their relevance features are builtbased on whole session clicks extracted from thetoolbar.
In contrast, our n-gram features are builton search engine clicks directly.
We should beable to expand our method to integrate the post-search clicks with toolbar data.Other related work can be found in the domain ofquery rewriting.
Our n-gram dictionary was orig-inally designed for query rewriting.
Query re-writing (Xu and Croft, 1996; Salton and Voor-hees, 1984) reformulates a query to its synonymsor related terms automatically.
However, thecoverage of query rewriting is normally small,because an inappropriate rewrite can cause sig-nificant decrease in precision.
In contrast, ourapproach can cover a larger number of querieswithout decreasing precision, because it does notneed to make a binary decision whether a queryshould be reformulated.
The association scoresbetween queries and rewrites are used as rankingfeatures which are trained discriminatively to-ward search quality.5 ConclusionIn this paper, we presented a set of straightfor-ward yet informative query-url n-gram features.They allow for generalization of limited userclick data to large amounts of unseen query-urlpairs.
Our experiments showed such featuresgave significant improvement over models with-out using the features.
In addition, we mined aninteresting dictionary which contains informa-tive, but not necessarily obvious, query-url syno-nym pairs such as ?form?
and ?pdf?.
We are cur-rently extending our work to a variety of exactmatch features and different sources of click-through logs.AcknowledgementThanks to the anonymous reviewers for detailedsuggestion and our colleagues: Jon Degenhardtand Narayanan Sadagopan for assistance on gen-erating clickthrough data, Jiang Chen for devel-oping the decision tree package, Xiangyu Jin fora discussion on map/reduce, Beno?t Dumoulin,Fuchun Peng, Yumao Lu, and Xing Wei for pro-ductizing the work, and Rosie Jones, Su-lin Wu,Bo Long, Xin Li and Ruiqiang Zhang for com-ments on an earlier draft.532ReferencesAgichtein, E., E. Brill, and S. Dumais.
2006.
Im-proving web search ranking by incorporatinguser behavior information.
In Proceedings ofthe ACM SIGIR 29.Agichtein, Eugene, Zijian Zheng.
2006.
Identify-ing "best bet" web search results by miningpast user behavior.
In Proceedings of KDD.Bilenko, Mikhail and Ryen W. White.
2008.Mining the search trails of surfing crowds:identifying relevant websites from user activ-ity.
In Proceedings of WWW.Brants, T. 2000.
Tnt: a statistical part-ofspeechtagger.
In Proceedings of ANLP 6.Craswell, Nick and Martin Szummer.
2007.
Ran-dom walks on the click graph.
In Proceedingsof SIGIR.Craswell, Nick, Onno Zoeter, Michael Taylor,Bill Ramsey.
2008.
An experimental compari-son of click position-bias models in WSDM.Dupret, Georges, Benjamin Piwowarski.
2008.
Auser browsing model to predict search engineclick data from past observations.
In Proceed-ings of SIGIR 31.Gale, William A. and Kenneth W. Church.
1991.Identifying word correspondence in paralleltexts.
In Proceedings of HLT 91.Gao, Jianfeng, Wei Yuan, Xiao Li, Kefeng Deng,and Jian-Yun Nie.
2009.
Smoothing Click-through Data for Web Search Ranking.
In Pro-ceedings of SIGIR 32.J?rvelin, K. and J. Kek?l?inen.
2002.
Cumulatedgain-based evaluation of IR techniques, Jour-nal ACM Transactions on Information Sys-tems, 20: 422-446.Klein, D. and C. Manning.
2003.
Accurate unlex-icalized parsing.
In Proceedings of ACL 41.Lin, Chin-Yew and Franz Josef Och.
2004.
Au-tomatic evaluation of machine translationquality using longest common subsequenceand skip-bigram.
In In Proceedings of ACL 42.Lu, Yumao, Fuchun Peng, Xin Li and NawaazAhmed, 2006, Coupling Feature Selection andMachine Learning Methods for NavigationalQuery Identification, In Proceeding of CIKM.Radlinski, F., Kurup, M. and Joachims, T. 2007.Active exploration for learning rankings fromclickthrough data.
In SIGKDD.Salton G. and E. Voorhees.
1984.
Comparison oftwo methods for Boolean query relevancyfeedback.
Information Processing & Manage-ment, 20(5).Wilcoxon, F. 1945.
Individual Comparisons byRanking Methods.
Biometrics, 1:80?83.Xu Q. and W. Croft.
1996.
Query expansion us-ing local and global document analysis.
InProceed of the 19th annual international ACMSIGIR.Zheng, Z., H. Zha, K. Chen, and G. Sun.
2007.
Aregression framework for learning rankingfunctions using relative relevance judgments.In Proceedings of SIGIR 30.533
