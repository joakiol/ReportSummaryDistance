Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284?1295,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsMorpho-syntactic Lexical Generalizationfor CCG Semantic ParsingAdrienne WangComputer Science & EngineeringUniversity of WashingtonSeattle, WAaxwang@cs.washington.eduTom KwiatkowskiAllen Institute for AISeattle, WAtomk@allenai.orgLuke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WAlsz@cs.washington.eduAbstractIn this paper, we demonstrate thatsignificant performance gains can beachieved in CCG semantic parsingby introducing a linguistically moti-vated grammar induction scheme.
Wepresent a new morpho-syntactic fac-tored lexicon that models systematicvariations in morphology, syntax, andsemantics across word classes.
Thegrammar uses domain-independentfacts about the English language torestrict the number of incorrect parsesthat must be considered, therebyenabling effective learning from lessdata.
Experiments in benchmarkdomains match previous models withone quarter of the data and providenew state-of-the-art results with allavailable data, including up to 45%relative test-error reduction.1 IntroductionSemantic parsers map sentences to formalrepresentations of their meaning (Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Liang et al., 2011).
One common approach isto induce a probabilistic CCG grammar, whichdefines the meanings of individual words andphrases and how to best combine them to an-alyze complete sentences.
There has been re-cent work developing learning algorithms forCCG semantic parsers (Kwiatkowski et al.,2010; Artzi and Zettlemoyer, 2011) and usingthem for applications ranging from questionanswering (Cai and Yates, 2013b; Kwiatkowskiet al., 2013) to robot control (Matuszek et al.,2012; Krishnamurthy and Kollar, 2013).One key learning challenge for this styleof learning is to induce the CCG lexicon,which lists possible meanings for each phraseand defines a set of possible parses foreach sentence.
Previous approaches haveeither hand-engineered a small set of lexi-cal templates (Zettlemoyer and Collins, 2005,2007) or automatically learned such tem-plates (Kwiatkowski et al., 2010, 2011).
Thesemethods are designed to learn grammars thatovergenerate; they produce spurious parsesthat can complicate parameter estimation.In this paper, we demonstrate that signif-icant gains can instead be achieved by usinga more constrained, linguistically motivatedgrammar induction scheme.
The grammaris restricted by introducing detailed syntac-tic modeling of a wider range of constructionsthan considered in previous work, for exampleintroducing explicit treatments of relationalnouns, Davidsonian events, and verb tense.We also introduce a new lexical generalizationmodel that abstracts over systematic morpho-logical, syntactic, and semantic alternationswithin word classes.
This includes, for exam-ple, the facts that verbs in relative clauses andnominal constructions (e.g., ?flights departingNYC?
and ?departing flights?)
should be in-finitival while verbs in phrases (e.g., ?Whatflights depart at noon??)
should have tense.These grammar modeling techniques use uni-versal, domain-independent facts about theEnglish language to restrict word usage to ap-propriate syntactic contexts, and as such arepotentially applicable to any semantic parsingapplication.More specifically, we introduce a newmorpho-syntactic, factored CCG lexicon thatimposes our grammar restrictions duringlearning.
Each lexical entry has (1) a wordstem, automatically constructed from Wik-tionary, with part-of-speech and morpholog-ical attributes, (2) a lexeme that is learned1284and pairs the stem with semantic content thatis invariant to syntactic usage, and (3) a lexi-cal template that specifies the remaining syn-tactic and semantic content.
The full set oftemplates is defined in terms of a small set ofbase templates and template transformationsthat model morphological variants such as pas-sivization and nominalization of verbs.
Thisapproach allows us to efficiently encode a gen-eral grammar for semantic parsing while alsoeliminating large classes of incorrect analysesconsidered by previous work.We perform experiments in two benchmarksemantic parsing datasets: GeoQuery (Zelleand Mooney, 1996) and ATIS (Dahl et al.,1994).
In both cases, our approachachieves state-of-the-art performance, includ-ing a nearly 45% relative error reduction onthe ATIS test set.
We also show that the gainsincrease with less data, including matchingprevious model?s performance with less than25% of the training data.
Such gains are par-ticularly practical for semantic parsers; theycan greatly reduce the amount of data that isneeded for each new application domain.2 Related WorkGrammar induction methods for CCG seman-tic parsers have either used hand-engineeredlexical templates, e.g.
(Zettlemoyer andCollins, 2005, 2007; Artzi and Zettlemoyer,2011), or algorithms to learn such templatesdirectly from data, e.g.
(Kwiatkowski et al.,2010, 2011).
Here, we extend the first ap-proach, and show that better lexical general-ization provides significant performance gains.Although CCG is a common choicefor semantic parsers, many other for-malisms have been studied, including DCStrees (Liang et al., 2011), integer linear pro-grams (Clarke et al., 2010), and synchronousgrammars (Wong and Mooney, 2007; Joneset al., 2012; Andreas et al., 2013).
All of theseapproaches build complete meaning represen-tations for individual sentences, but the datawe use has also been studied in related work oncross-sentence reasoning (Miller et al., 1996;Zettlemoyer and Collins, 2009) and model-ing semantic interpretation as a tagging prob-lem (Tur et al., 2013; Heck et al., 2013).
Al-though we focus on full analysis with CCG,the general idea of using linguistic constraintsto improve learning is broadly applicable.Semantic parsers are also commonly learnedfrom a variety of different types of supervision,including logical forms (Kate and Mooney,2006; Wong and Mooney, 2007; Muresan,2011; Kwiatkowski et al., 2012), question-answer pairs (Clarke et al., 2010; Liang et al.,2011), conversational logs (Artzi and Zettle-moyer, 2011), distant supervision (Krishna-murthy and Mitchell, 2012; Cai and Yates,2013b), sentences paired with system behav-ior (Goldwasser and Roth, 2011; Chen andMooney, 2011; Artzi and Zettlemoyer, 2013b),and even from database constraints with noexplicit semantic supervision (Poon, 2013).We learn from logical forms, but CCG learn-ing algorithms have been developed for eachcase above, making our techniques applicable.There has been significant related work thatinfluenced the design of our morpho-syntacticgrammars.
This includes linguistics stud-ies of relational nouns (Partee and Borschev,1998; de Bruin and Scha, 1988), Davidsonianevents (Davidson, 1967), parsing as abduc-tion (Hobbs et al., 1988), and other more gen-eral theories for lexicons (Pustejovsky, 1991)and CCG (Steedman, 2011).
It also includeswork on using morphology in CCG syntac-tic parsing (Honnibal et al., 2010) and morebroad-coverage semantics in CCG (Bos, 2008;Lewis and Steedman, 2013).
However, ourwork is unique in studying the use of relatedideas for semantic parsing.Finally, there has also been recent progresson semantic parsing against large, open do-main databases such as Freebase (Cai andYates, 2013a; Kwiatkowski et al., 2013; Berantet al., 2013).
Unfortuantely, existing Freebasedatasets are not a good fit to test our approachbecause the sentences they include have rela-tively simple structure and can be interepretedaccurately using only factoid lookups with nodatabase joins (Yao and Van Durme, 2014).Our work focuses on learning more syntacti-cally rich models that support compositionalreasoning.3 BackgroundLambda Calculus We represent the mean-ings of sentences, words and phrases with1285list one way flights from various citiesS/N N/N N PP/NP NP/N N?f.f ?f?x.oneway(x) ?
f(x) ?x.flight(x) ?x?y.from(y, x) ?fAx.f(x) ?x.city(x)>NPAx.city(x)>PP?x.from(x,Ay.city(y))>TN\N?x.from(x,Ay.city(y))<N?x.flight(x) ?
from(x,Ay.city(y))>N?x.flight(x) ?
from(x,Ay.city(y)) ?
oneway(x)>S?x.flight(x) ?
from(x,Ay.city(y)) ?
oneway(x)Figure 1: An example CCG parse.lambda calculus logical expressions.
We use aversion of the typed lambda calculus (Carpen-ter, 1997), in which the basic types include en-tities, events, truth values and numbers.
Func-tion types are assigned to lambda expressions.The expression ?x.flight(x) with type ?e, t?takes an entity and returns a truth value, andrepresents a set of flights.Combinatory Categorial GrammarCCG (Steedman, 1996, 2000) is a formalismthat tightly couples syntax and semantics,and can be used to model a wide range oflinguistic phenomena.
A traditional CCGgrammar includes a lexicon ?
with lexicalentries like the following:flights ` N :?x.flight(x)from ` PP/NP :?y.
?x.from(x, y)cities ` N :?x.city(x)where a lexical item w `X : h has words w,syntactic category X, and logical expression h.CCG uses a small set of combinatory rulesto jointly build syntactic parses and semanticrepresentations.
Two common combinatoryrules are forward (>) and backward (<)application:X/Y : f Y : g ?
X : f(g) (>)Y : g X\Y : f ?
X : f(g) (<)CCG also includes combinatory rules of for-ward (> B) and backward (< B) composition:X/Y : f Y/Z : g ?
X/Z : ?x.f(g(x)) (> B)Y \Z : g X\Y : f ?
X\Z : ?x.f(g(x)) (< B)These rules apply to build syntactic and se-mantic derivations concurrently.In this paper, we also implement typeraising rules for compact representation ofPP (prepositional phrase) and AP (adverbialphrase).PP : g ?
N\N : ?f?x.f(x) ?
g(x) (T)AP : g ?
S\S : ?f?e.f(e) ?
g(e) (T)AP : g ?
S/S : ?f?e.f(e) ?
g(e) (T)Figure 1 shows an example CCGparse (Steedman, 1996, 2000) where thelexical entries are listed across the top andthe output lambda-calculus meaning repre-sentation is at the bottom.
This meaning is afunction (denoted by ?x...) that defines a setof flights with certain properties and includesa generalized Skolem constant (Steedman,2011) (Ay...) that performs existential quan-tification.
Following recent work (Artzi andZettlemoyer, 2013b), we use meaning repre-sentations that model a variety of linguisticconstructions, for example including Skolemconstants for plurals and Davidson quantifiersfor events, which we will introduce brieflythroughout this paper as they appear.Weighted CCGs A weighted CCG gram-mar is defined as G = (?,?
), where ?
is aCCG lexicon and ?
?
Rdis a d-dimensionalparameter vector, which will be used to rankthe parses allowed under ?.For a sentence x, G produces a set of candi-1286date parse trees Y = Y(x;G).
Given a featurevector ?
?
Rd, each parse tree y for sentencex is scored by S(y; ?)
= ?
??
(x, y).
The outputlogical form z?
is then defined to be at the rootof the highest-scoring parse y?:y?
= arg maxy?Y(x;G)S(y; ?)
(1)We use existing CKY-style parsing algo-rithms for this computation, implementedwith UW SPF (Artzi and Zettlemoyer, 2013a).Section 7 describes the set of features we usein the learned models.Learning with GENLEX We will alsomake use of an existing learning algo-rithm (Zettlemoyer and Collins, 2007) (ZC07).We first briefly review the ZC07 algorithm,and describe our modifications in Section 7.Given a set of training examples D ={(xi, zi) : i = 1...n}, xibeing the ith sentenceand zibeing its annotated logical form, the al-gorithm learns a set of parameters ?
for thegrammar, while also inducing the lexicon ?.The ZC07 learning algorithm uses a functionGENLEX(x, z) to define a set of lexical entriesthat could be used to parse the sentence x toconstruct the logical form z.
For each trainingexample (x, z), GENLEX(x, z) maps all sub-strings x to a set of potential lexical entries,generated by exhaustively pairing the logicalconstants in z using a set of hand-engineeredtemplates.
The example is then parsed withthis much bigger lexicon and lexical entriesfrom the highest scoring parses are added to ?.The parameters ?
used to score parses are up-dated using a perceptron learning algorithm.4 Morpho-Syntactic LexiconThis section defines our morpho-syntactic lex-ical formalism.
Table 1 shows examples of howlexemes, templates, and morphological trans-formations are used to build lexical entries forexample verbs.
In this section, we formally de-fine each of these components and show howthey are used to specify the space of possiblelexical entries that can be built for each inputword.
In the following two sections, we willprovide more discussion of the complete setsof templates (Section 5) and transformations(Section 6).Verb, Noun, Preposition, Pronoun, Adjective,Adverb, Conjunction, Numeral, Symbol,Proper Noun, Interjection, ExpressionTable 2: Part-of-Speech typesPOS Attribute ValuesNoun Number singular, pluralVerb Person first, second, thirdVerb Voice active, passiveVerb Tense present, pastVerb Aspect simple, progressive, perfectVerb Participle present participle,past participleAdj, Degree of comparative, superlativeAdv, comparisonDetTable 3: Morphological attributes and values.We build on the factored CCG lexicon in-troduced by Kwiatkowski et al.
(2011) but (a)further generalize lexemes to represent wordstems, (b) constrain the use of templates withwidely available syntactic information, and (c)efficiently model common morphological vari-ations between related words.The first step, given an input word w, isto do morphological and part-of-speech analy-sis with the morpho-syntactic function F .F maps a word to a set of possible morpho-syntactic representations, each containing atriple (s, p,m) of word stem s, part-of-speechp and morphological category m. For exam-ple, F maps the word flies to two possiblerepresentations:F (flies) = {(fly,Noun, (plural)),(fly,Verb, (third, singular, simple, present))}for the plural noun and present-tense verbsenses of the word.
F is defined based on thestems, part-of-speech types, and morpholog-ical attributes marked for each definition inWiktionary.1The full sets of possible part-of-speech and morphological types required forour domains are shown in Table 2 and Table 3.Each morpho-syntactic analysis a ?
F (w)is then paired with lexemes based on stemmatch.
A lexeme (s,~c) pairs a word stems with a list of logical constants ~c = [c1.
.
.
ck].Table 1 shows the words ?depart?, ?departing?,?departure?, which are all assigned the lex-eme (depart, [depart]).
In general, there can1www.wiktionary.com1287Word Lexeme : Base Template Trans Lexical entrydepart(depart, [depart]) :I depart `S\NP :?x?e.depart(e, x)departing I departing `S\NP :?x?e.depart(e, x)departing?
`S\NP :?x?e.v1(e, x)fpresdeparting `PP :?x?e.depart(e, x)departure fnomdeparture `N :?x?e.depart(e, x)use(use, [airline]) :I use `S\NP/NP :?x?y?e.airline(e, y, x)using I using `S\NP/NP :?x?y?e.airline(e, y, x)using?
`S\NP/NP :?x?y?e.v1(e, y, x)fpresusing `PP/NP :?x?e.airline(e, y, x)use fnomuse `N/NP :?x?y?e.airline(e, y, x)Trans Template Transformationfpres?
`S\NP/T :?x1..xn?e.v(e, xn..x1) ?
?
`PP/T : ?x1..xn?e.v(e, xn..x1)fnom?
`S\NP/T :?x1..xn?e.v(e, xn..x1) ?
?
`N/T : ?x1..xn?e.v(e, xn..x1)Table 1: Lexical entries constructed by combining a lexeme, base template, and transformationfor the intransitive verb ?depart?
and the transitive verb ?use?.be many different lexemes for each stem, thatvary in the selection of which logical constantsare included.Given analysis (s, p,m) and lexeme (s,~c), wecan use a lexical template to construct alexical entry.
Each template has the form:?(?,~v).[?
`X : h~v]where ?
and ~v are variables that abstract overthe words and logical constants that will beused to define a lexical entry with syntax Xand templated logical form h~v.To instantiate a template, ?
is filled with theoriginal word w and the constants in ~c replacethe variables ~v.
For example, the template?(?,~v).[?
` S\NP : ?x?e.v1(e, x)] could beused with the word ?departing?
and the lexeme(depart, [depart]) to produce the lexical entrydeparting ` S\NP : ?x?e.depart(e, x).
Whenclear from context, we will omit the functionsignature ?p(?,~v).
for all templates, as seen inTable 1.In general, there can be many applicabletemplates, which we organize as follows.
Eachfinal template is defined by applying a mor-phological transformation to one of a smallset of possible base templates.
The pairingis found based on the morphological analysis(s, p,m), where each base template is associ-ated with part-of-speech p and each transfor-mation is indexed by the morphology m. Atransformation fmis a function:fm(?p(?,~v).[?
`X : h~v]) = ?p(?,~v).[?
`X?
: h?~v]that takes the base template as input and pro-duces a new template to model the inflectedform specified by m.For example, both base templates in Ta-ble 1 are for verbs.
The template ?
`S\NP : ?x?e.v1(e, x) can be translated intothree other templates based on the transfor-mations I, fpres, and fnom, depending on theanalysis of the original words.
These transfor-mations generalize across word type; they canbe used for the transitive verb ?use?
as well asthe intransitive ?depart.?
Each resulting tem-plate, potentially including the original inputif the identity transformation I is available,can then be used to make an output lexicalentry, as we described above.5 Lexical TemplatesThe templates in our lexicon, as introducedin Section 4, model the syntactic and seman-tic aspects of lexical entries that are sharedwithin each word class.
Previous approacheshave also used hand-engineered lexical tem-plates, as described in Section 2, but we dif-fer by (1) using more templates allowing formore fine grained analysis and (2) using wordclass information to restrict template use, forexample ensuring that words which cannot beverbs are never paired with templates designedfor verbs.
This section describes the templatesused during learning, first presenting those de-signed to model grammatical sentences andthen a small second set designed for more el-liptical spoken utterances.Base Forms Table 4 lists the primary tem-plate set, where each row shows an examplewith a sentence illustrating its use.
Templatesare also grouped by the word classes, includingadjectives, adverbs, prepositions, and severaltypes of nouns and verbs.
While there is notenough space to discuss each row, it is worth1288word class example usage base templateNoun phrase Boston ?
`NP : vNoun (regular) What flight is provided by delta?
?
`N : ?x.v(x)Noun (relation) I need fares of flights ?
`N/PP : ?x?y.v(x, y)delta schedule ?
`N\(N/N) : ?f?x.v(Ay.f(?z.true, y), x)Noun (function) size of California ?
`NP/NP : ?x.v(x)VintransWhat flights depart from New York?
?
`S\NP : ?x?e.v(e, x)VtransWhich airlines serve Seattle (active verb) ?
`S\NP/NP :?x?y?e.v(e, y, x)What airlines have flights (passive verb) ?
`S\NP/NP :?x?y?e.v(e, x, y)VditransThey give him a book ?
`S\NP/NP/NP : ?x?y?z?e.v(e, z, y, x)VimpersonIt costs $500 to fly to Boston ?
`S\NP/NP/NP :?x?y?z?e.v(e, y, x)VauxThe flights have arrived at Boston ?
`S\NP/(S\NP ) :?f.f?
`S/NP/(S/NP ) :?f.fDoes delta provide flights from Seattle?
?
`S/S :?f.fVcopulaThe flights are from Boston ?
`S\NP/PP :?f?x.f(x)What flight is cheap?
?
`S\NP/(N/N) :?f?x.f(?y.true, x)Alaska is the state with the most rivers ?
`S\NP/NP :?x?y.equals(y, x)Adjective I need a one way flight ?
`N/N :?f?x.f(x) ?
v(x)Boston flights round trip ?
`PP :?x.v(x)How long is mississippi?
?
`DEG :?x.v(x)Preposition List flights from Boston ?
`PP/NP :?x?y.v(y, x)List flights that go to Dallas ?
`AP/NP :?x?e.v(e, x)List flights between Dallas and Boston ?
`PP/NP/NP :?x?y?z.v1(z, x) ?
v2(z, y)What flights leave between 8am and 9am?
?
`AP/NP/NP :?x?y?e.v1(e, x) ?
v2(e, y)Adverb Which flight departs daily?
?
`AP :?e.v(e)How early does the flight arrive?
?
`DEG :?x.v(x)Determiner Which airline has a flight from Boston?
?
`NP/N :?fAx.f(x)Table 4: Base templates that define different syntactic roles.type example usage base templatetellipticalflights Newark to Cleveland ?
`PP :?x.P (x, v)flights arriving 2pm ?
`AP :?e.P (e, v)american airline from Denver ?
`N :?x.P (x, v)tmetonymyList airlines from Seattle ?
`N/PP :?f?x.v(x) ?
P (Ay.f(y), x))Shat airlines depart from Seattle?
?
`N/(S\NP ) :?f?x.v(x) ?
P (Ay.f(y), x)fares from miami to New York ?
`N/PP :?f?x.v(Ay.f(y), x)Table 5: Base templates for ungrammatical linguistic phenomenaconsidering nouns as an illustrative example.We model nouns as denoting a set of entitiesthat satisfy a given property.
Regular nounsare represented using unary predicates.
Rela-tional nouns syntactically function as regularnouns but semantically describe sets of enti-ties that have some relationship with a comple-ment (Partee and Borschev, 1998).
For exam-ple, the relational noun fare describes a binaryrelationship between flights and their price in-formation, as we see in this parse:fares of flightsN/PP PP/NP N?x?y.fare(x, y) ?x.x ?x.flight(x)>TNPAx.flight(x)>PPAx.flight(x)>N?x.fare(Ay.flight(y), x)This analysis differs from previous ap-proaches (Zettlemoyer and Collins, 2007),where relational nouns were treated as regu-lar nouns and prepositions introduced the bi-nary relationship.
The relational noun modelreduces lexical ambiguity for the prepositions,which are otherwise highly polysemous.Adjectives are nominal modifiers that takea noun or a noun phrase as an argument andadd properties through conjunction.
Preposi-tions take nominal objects and function as ad-jectival modifiers for nouns or adverbial modi-fiers for verbs.
Verbs can be subcategorizedby their grammatical structures into transi-tive (Vtrans), intransitive (Vintrans), imper-sonal (Vimperson), auxiliary (Vaux) and copula(Vcopula).
Adverbs are verb modifiers defin-ing aspects like time, rate and duration.
Theadoption of event semantics allows adverbialmodifiers to be represented by predicates and1289linked by the shared events.
Determiners pre-cede nouns or noun phrases and distinguisha reference of the noun.
Following the gen-eralized Skolem terms, we model determiners,including indefinite and definite articles, as a?
?e, t?, e?
function that selects a unique indi-vidual from a ?e, t?-typed function defining asingleton set.Missing Words The templates presented sofar model grammatically correct input.
How-ever, in dialogue domains such as ATIS, speak-ers often omit words.
For example, speak-ers can drop the preposition ?from?
in ?flightsfrom Newark to Cleveland?
to create the ellip-tical utterance ?flights Newark to Cleveland?.We address this issue with the templatestellipticalillustrated in Table 5.
Each of theseadds a binary relation P to a lexeme with asingle entity typed constant.
For our example,the word ?Newark?
could be assigned the lexi-cal item Newark `PP : ?x.from(x, newark)by selecting the first template and P = from.Another common problem is the use ofmetonymy.
In the utterance ?What airlinesdepart from New York?
?, the word ?airlines?is used to reference flight services operated bya specific airline.
This is problematic becausethe word ?depart?
needs to modify an event oftype flight.
We solve this with the tmetonymytemplates in Table 5.
These introduce a binarypredicate P that would, in the case of our ex-ample, map airlines on to the flights that theyoperate.The templates in Table 5 handle the ma-jor cases of missing words seen in our dataand are more efficient than the approach takenby (Zettlemoyer and Collins, 2007) who intro-duced complex type shifting rules and relaxedthe grammar to allow every word order.6 Morphological TransformationsFinally, the morpho-syntactic lexicon intro-duces morphological transformations, whichare functions from base lexical templates tolexical templates that model the syntactic andsemantic variation as the word is inflected.These transformations allow us to compactlymodel, for example, the facts that argumentorder is reversed when moving from active topassive forms of the same verb, and that thesubject can be omitted.
To the best of ourknowledge, we are the first to study such trans-formations for semantic parsing.Table 6 shows the transformations.
Eachrow groups a set of transformations by linguis-tic category, including singular vs. plural num-ber, active vs. passive voice, and so on, andalso includes example sentences where the out-put templates could be used.
Again, for space,we do not detail the motivation for every class,but it is worth looking at some of the alterna-tions for verbs and nouns as our prototypicalexample.Some verbs can act as noun modifiers.
Forexample, the present participle ?using?
mod-ifies ?flights?
in ?flights using twa?.
Tomodel this variation, we use the transforma-tion fpresent part, a mapping that changes theroot of the verb signature S\NP to PP :fpresent part: ?
`S\NP/T :?x1..xn?e.v(e, xn..x1)?
?
`PP/T : ?x1..xn?e.v(e, xn..x1)where T = [,NP,NP/NP ] instantiates thisrule for verbs that take different sets of argu-ments, effectively allowing any verb that is inits finite or -ing form to behave syntacticallylike a prepositional phrase.Intransitive present participles can also actas prenominal adjectival modifiers as in ?thedeparting flight?.
We add a second mappingthat maps the intransitive category S\NP tothe noun modifier N/N .fpresent part: ?
`S\NP :?x?e.v(e, x)?
?
`N/N : ?f?x?e.f(x) ?
v(e, x)Finally, verbal nouns have meanings derivedfrom actions typically described by verbs butsyntactically function as nouns.
For example,landing in the phrase ?landing from jfk?
is thegerundive use of the verb land.
We add thefollowing mapping to fpresent partand fnominal:?
`S\NP/T :?x1..xn?e.v(e, xn..x1) ??
`N/T : ?x1..xn?e.v(e, xn..x1)with T from above.
This allows for reuse of thesame meaning across quite different syntac-tic constructs, including for example ?flightsthat depart from Boston?
and ?departure fromBoston.
?1290Template transformations fmExample usagePlural Number (fplural)I flight ?
early flights?
`N : ?x.v(x) ?
?
`NP :Ax.v(x) city ?
flights to citiesSingular Number (fsingular)I flight ?
flightPossessive (fpossess)?
`NP : v ?
?
`N/N :?f?x.f(x) ?
P (x, v) delta ?
delta?s flights?
`N : ?x.v(x) ?
?
`N/N :?f?x.f(x) ?
P (x,Ay.v(y)) airline ?
airline?s flightsPassive Voice (fpassive)?
`Y/NP :?x1..xn?e.v(e, x1..xn) ?
?
`Y/PP : ?x1..xn?e.v(e, xn..x1) serves ?is served by?
`Y/NP :?x1..xn?e.v(e, x1, .., xn) ?
?
`Y : ?x1..xn?1?e.v(e, xn?1..x1) name ?city named AustinPresent Participle (fpresent)?
`S\NP/T :?x1..xn?e.v(e, xn..x1) ?
?
`PP/T : ?x1..xn?e.v(e, xn..x1) use ?flights using twa?
`S\NP :?x?e.v(e, x) ?
?
`N/N : ?f?x?e.f(x) ?
v(e, x) arrive ?arriving flights?
`S\NP/T :?x1..xn?e.v(e, xn..x1) ?
?
`N/T : ?x1..xn?e.v(e, xn..x1) land ?
landings at jfkPast Participle (fpast)?
`S\NP/NP :?x1..xn?e.v(e, xn..x1) ?
?
`PP/PP : ?x1..xn?e.v(e, x1..xn) use ?
plane used byNominalization (fnominal)?
`S\NP/T :?x1..xn?e.v(e, xn..x1) ?
?
`N/T : ?x1..xn?e.v(e, xn..x1) depart ?
departureComparative (fcomp)?
`DEG :?x.v(x) ?
?
`PP/PP :?x?y.v(y) < v(x) short ?
shorter?
`DEG :?x.v(x) ?
?
`PP/PP :?x?y.v(y) > v(x) long ?
longerSuperlative (fsuper)?
`DEG :?x.v(x) ?
?
`NP/N :?f.argmin(?x.f(x), ?x.v(x)) short ?
shortest?
`DEG :?x.v(x) ?
?
`NP/N :?f.argmax(?x.f(x), ?x.v(x)) long ?
longestTable 6: Morphological transformations with examples.
T = [,NP,NP/NP ] and Y =[S\NP,S\NP/NP ] allow a single transformation to generalize across word type.Nouns can be inflected by number to de-note singular and plural forms or by addingan apostrophe to mark a possessive case.
Thetransformation function fsingularis an identitytransformation.
Plurals may have different in-terpretations: one is the generic ?e, t?
set rep-resentation, which requires no transformationon the base, or plurals can occur without overtdeterminers (bare plurals), but semanticallyimply quantification.
We create a plural tosingular type shifting rule which implementsthe ?
?e, t?, e?
skolem function to select a uniqueindividual from the set.
The possessive trans-formation fpossesstransfers the base templateto a noun modifier, and adds a binary predi-cate P that encodes the relation.There are also a number of instances of theidentity transformation function I, which doesnot change the base template.
Because the se-mantics we are constructing was designed toanswer questions against a static database, itdoes not need to represent certain phenomenato return the correct answer.
This includesmore advanced variants of person, tense, as-pect, and potentially many others.
Ideally,these morphological attributes should add se-mantic modifiers to the base meaning, for ex-ample, tense can constrain the time at whichan event occurs.
However, none of our do-mains support such reasoning, so we assign theidentity transformation, and leave the explo-ration of these issues to future work.7 LearningOne advantage of our morpho-syntactic, fac-tored lexicon is that it can be easily learnedwith small modifications to existing algo-rithms (Zettlemoyer and Collins, 2007).
Weonly need to modify the GENLEX proce-dure that defines the space of possible lexi-cal entries.
For each training example (x, z),GENLEX(x, z, F ) first maps each substring inthe sentence x into the morphological repre-sentation (s, p, c) using F introduced in Sec-tion 4.
A candidate lexeme set L?is then gen-erated by exhaustively pairing the word stemswith all subsets of the logical constants fromz.
Lexical templates are applied to the lexemesin L?to generate candidate lexical entries forx.
Finally, the lexemes that participate in thetop scoring correct parse of x are added to thepermanent lexicon.Initialization Following standard practice,we compile an initial lexicon ?0, which con-sists of a list of domain independent lexical1291items for function words, such as interrogativewords and conjunctions.
These lexical itemsare mostly semantically vacuous and serve par-ticular syntactic functions that are not gener-alizable to other word classes.
We also initial-ize the lexemes with a list of NP entities com-plied from the database, e.g., (Boston, [bos]).Features We use two types of features inthe model for discriminating parses.
Four lex-ical features are fired on each lexical item:?
(s,~c)for the lexeme, ?tpfor the base tem-plate, ?tmfor the morphologically modifiedtemplate, and ?lfor the complete lexicalitem.
We also compute the standard logicalexpression features (Zettlemoyer and Collins,2007) on the root semantics to track the pair-wise predicate-argument relations and the co-occuring predicate-predicate relations in con-junctions and disjunctions.8 Experimental SetupData and Metrics We evaluate perfor-mance on two benchmark semantic pars-ing datasets, Geo880 and ATIS.
We usethe standard data splits, including 600/280train/test for Geo880 and 4460/480/450train/develop/test for ATIS.
To support thenew representations in Section 5, we sys-tematically convert annotations with existen-tial quantifiers, temporal events and relationalnouns to new logical forms with equivalentmeanings.
All systems are evaluated with ex-act match accuracy, the percentage of fullycorrect logical forms.Initialization We assign positive initialweights to the indicator features for entries inthe initial lexicon, as defined in Section 7, toencourage their use.
The elliptical templateand metonymy template features are initial-ized with negative weights to initially discour-age word skipping.Comparison Systems We compare perfor-mance with all recent CCG grammar induc-tion algorithms that work with our datasets.This includes methods that used a limitedset of hand-engineered templates for inducingthe lexicon, ZC05 (Zettlemoyer and Collins,2005) and ZC07 (Zettlemoyer and Collins,2007), and those that learned grammar struc-ture by automatically splitting the labeled log-System TestZC05 79.3ZC07 86.1UBL 87.9FUBL 88.6DCS 87.9FULL 90.4DCS+91.1Table 7: Exact-match Geo880 test accuracy.System Dev TestZC07 74.4 84.6UBL 65.6 71.4FUBL 81.9 82.8GUSP - 83.5TEMP-ONLY 85.5 87.2FULL 87.5 91.3Table 8: Exact-match accuracy on the ATISdevelopment and test sets.ical forms, UBL (Kwiatkowski et al., 2010)and FUBL (Kwiatkowski et al., 2011).
Wealso compare the state-of-the-art for Geo880(DCS (Liang et al., 2011) and DCS+ which in-cludes an engineered seed lexicon) and ATIS(which is ZC07).
Finally, we include resultsfor GUSP (Poon, 2013), a recent unsupervisedapproach for ATIS.System Variants We report results for acomplete approach (Full), and variants whichuse different aspects of the morpho-syntacticlexicon.
The TEMP-ONLY variant learnedwith the templates from Section 5 but, likeZC07, does not use any word class informationto restrict their use.
The TEMP-POS removesmorphology from the lexemes, but includes theword class information from Wiktionary.
Fi-nally, we also include DCS+, which initialize aset of words with POS tag JJ, NN, and NNS.9 ResultsFull Models Tables 7 and 8 report themain learning results.
Our approach achievesstate-of-the-art accuracies on both datasets,demonstrating that our new grammar induc-tion scheme provides a type of linguisticallymotivated regularization; restricting the algo-rithm to consider a much smaller hypothesisspace allows to learn better models.12920.40.50.60.70.80.9100  500  1000  2000  4460RecallTraining samplesTEMP_ONLYTEMP_POSFULLFUBLFigure 2: ATIS Learning CurveOn Geo880 the full method edges out thebest systems by 2% absolute on the test set,as compared to other systems with no domain-specific lexical initialization.
Although DCSrequires less supervision, it also uses externalsignals including a POS tagger.We see similarly strong results for ATIS,outperforming FUBL on the ATIS develop-ment set by 6.8%, and improving the accu-racy on the test set by 7.9% over the previousbest system ZC07.
Unlike FUBL, which excelsat the development set but trails ZC07?s tem-plated grammar by almost 2 points on the testset, our approach demonstrates consistent im-provements on both.
Additionally, althoughthe unsupervised model (GUSP) rivals previ-ous approaches, we are able to show that morecareful use of supervision open a much widerperformance gap.Learning Curve with Ablations Figure 2presents a learning curve for the ATIS domain,demonstrating that the learning improvementsbecome even more dramatic for smaller train-ing set sizes.
Our model outperforms FUBL bywide margins, matching its final accuracy withonly 22% of the total training examples.
Ourfull model also consistently beats the variantswith fewer word class restrictions, althoughby smaller margins.
Again, these results fur-ther highlight the benefit of importing externalsyntactic resources and enforcing linguisticallymotivated constraints during learning.Learned Lexicon The learned lexicon isalso more compact.
Table 9 summarizesstatistics on unique lexical entries requiredto parse the ATIS development set.
TheSystem Lexical Entries LexemesFUBL 1019 721Our Approach 818 495Table 9: Lexicon size comparison on the ATISdev set (460 unique tokens).morpho-syntactic model uses 80.3% of the lex-ical entries and 63.7% of the lexemes thatFUBL needs, while increase performance bynearly 7 points.
Upon inspection, our modelachieves better lexical decomposition by learn-ing shorter lexical units, for example, theadoption of Davidsonian events allows us tolearn unambiguous adverbial modifiers, andthe formal modeling of nominalized nouns andrelational nouns treats prepositions as syntac-tic modifiers, instead of being encoded in thesemantics.
Such restrictions generalize to amuch wider variety of syntactic contexts.10 Summary and Future WorkWe demonstrated that significant performancegains can be achieved in CCG semantic pars-ing by introducing a more constrained, linguis-tically motivated grammar induction scheme.We introduced a morpho-syntactic factoredlexicon that uses domain-independent factsabout the English language to restrict thenumber of incorrect parses that must be con-sidered and demonstrated empirically that itenables effective learning of complete parsers,achieving state-of-the-art performance.Because our methods are domain indepen-dent they should also benefit other semanticparsing applications and other learning algo-rithms that use different types of supervision,as we hope to verify in future work.
We wouldalso like to study how to generalize these gainsto languages other than English, by inducingmore of the syntactic structure.AcknowledgementsThe research was supported by the NSF (IIS-1115966, IIS-1252835) and the Intel Centerfor Pervasive Computing at the Univeristyof Washington.
The authors thank RobertGens, Xiao Ling, Xu Miao, Mark Yatskar andthe UW NLP group for helpful discussions,and the anonymous reviewers for helpful com-ments.1293ReferencesAndreas, J., Vlachos, A., and Clark, S. (2013).Semantic parsing as machine translation.Artzi, Y. and Zettlemoyer, L. (2011).
Boot-strapping semantic parsers from conversa-tions.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing.Artzi, Y. and Zettlemoyer, L. (2013a).
UWSPF: The University of Washington Seman-tic Parsing Framework.Artzi, Y. and Zettlemoyer, L. (2013b).
Weaklysupervised learning of semantic parsers formapping instructions to actions.
Transac-tions of the Association for ComputationalLinguistics, 1(1):49?62.Berant, J., Chou, A., Frostig, R., and Liang, P.(2013).
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of theConference on Empirical Methods in Natu-ral Language Processing.Bos, J.
(2008).
Wide-coverage semantic anal-ysis with boxer.
In Proceedings of the Con-ference on Semantics in Text Processing.Cai, Q. and Yates, A.
(2013a).
Large-scalesemantic parsing via schema matching andlexicon extension.
In Proceedings of the An-nual Meeting of the Association for Compu-tational Linguistics.Cai, Q. and Yates, A.
(2013b).
Semantic pars-ing freebase: Towards open-domain seman-tic parsing.
In Proceedings of the Joint Con-ference on Lexical and Computational Se-mantics.Carpenter, B.
(1997).
Type-Logical Semantics.The MITPress.Chen, D. and Mooney, R. (2011).
Learningto interpret natural language navigation in-structions from observations.
In Proceedingsof the National Conference on Artificial In-telligence.Clarke, J., Goldwasser, D., Chang, M., andRoth, D. (2010).
Driving semantic parsingfrom the world?s response.
In Proceedingsof the Conference on Computational NaturalLanguage Learning.Dahl, D. A., Bates, M., Brown, M., Fisher,W., Hunicke-Smith, K., Pallett, D., Pao, C.,Rudnicky, A., and Shriberg, E. (1994).
Ex-panding the scope of the atis task: The atis-3 corpus.
In Proceedings of the workshop onHuman Language Technology.Davidson, D. (1967).
The logical form ofaction sentences.
Essays on actions andevents, pages 105?148.de Bruin, J. and Scha, R. (1988).
The interpre-tation of relational nouns.
In Proceedings ofthe Conference of the Association of Com-putational Linguistics, pages 25?32.
ACL.Goldwasser, D. and Roth, D. (2011).
Learningfrom natural instructions.
In Proceedings ofthe International Joint Conference on Arti-ficial Intelligence.Heck, L., Hakkani-Tu?r, D., and Tur, G.(2013).
Leveraging knowledge graphs forweb-scale unsupervised semantic parsing.
InProc.
of the INTERSPEECH.Hobbs, J. R., Stickel, M., Martin, P., and Ed-wards, D. (1988).
Interpretation as abduc-tion.
In Proceedings of the Association forComputational Linguistics.Honnibal, M., Kummerfeld, J. K., and Cur-ran, J. R. (2010).
Morphological analysiscan improve a ccg parser for english.
In Pro-ceedings of the International Conference onComputational Linguistics.Jones, B. K., Johnson, M., and Goldwater, S.(2012).
Semantic parsing with bayesian treetransducers.
In Proceedings of Associationof Computational Linguistics.Kate, R. and Mooney, R. (2006).
Using string-kernels for learning semantic parsers.
InProceedings of the Conference of the Asso-ciation for Computational Linguistics.Krishnamurthy, J. and Kollar, T. (2013).Jointly learning to parse and perceive: Con-necting natural language to the physicalworld.
Transactions of the Association forComputational Linguistics, 1(2).Krishnamurthy, J. and Mitchell, T. (2012).Weakly supervised training of semanticparsers.
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural Lan-guage Processing and Computational Natu-ral Language Learning.Kwiatkowski, T., Choi, E., Artzi, Y., and1294Zettlemoyer, L. (2013).
Scaling semanticparsers with on-the-fly ontology matching.Kwiatkowski, T., Goldwater, S., Zettlemoyer,L., and Steedman, M. (2012).
A probabilis-tic model of syntactic and semantic acquisi-tion from child-directed utterances and theirmeanings.
Proceedings of the Conference ofthe European Chapter of the Association ofComputational Linguistics.Kwiatkowski, T., Zettlemoyer, L., Goldwa-ter, S., and Steedman, M. (2010).
Induc-ing probabilistic CCG grammars from log-ical form with higher-order unification.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing.Kwiatkowski, T., Zettlemoyer, L., Goldwa-ter, S., and Steedman, M. (2011).
Lexicalgeneralization in CCG grammar inductionfor semantic parsing.
In Proceedings of theConference on Empirical Methods in Natu-ral Language Processing.Lewis, M. and Steedman, M. (2013).
Com-bined distributional and logical semantics.Transactions of the Association for Compu-tational Linguistics, 1:179?192.Liang, P., Jordan, M., and Klein, D. (2011).Learning dependency-based compositionalsemantics.
In Proceedings of the Conferenceof the Association for Computational Lin-guistics.Matuszek, C., FitzGerald, N., Zettlemoyer, L.,Bo, L., and Fox, D. (2012).
A joint modelof language and perception for grounded at-tribute learning.
In Proceedings of the Inter-national Conference on Machine Learning.Miller, S., Stallard, D., Bobrow, R., andSchwartz, R. (1996).
A fully statistical ap-proach to natural language interfaces.
InProceedings Association for ComputationalLinguistics.Muresan, S. (2011).
Learning for deep lan-guage understanding.
In Proceedings of theInternational Joint Conference on ArtificialIntelligence.Partee, B. H. and Borschev, V. (1998).
Inte-grating lexical and formal sematics: Gen-itives, relational nouns, and type-shifting.In Proceedings of the Second Tbilisi Sympo-sium on Language, Logic, and Computation.Poon, H. (2013).
Grounded unsupervised se-mantic parsing.
In Association for Compu-tational Linguistics (ACL).Pustejovsky, J.
(1991).
The generative lexicon.volume 17.Steedman, M. (1996).
Surface Structure andInterpretation.
The MIT Press.Steedman, M. (2000).
The Syntactic Process.The MIT Press.Steedman, M. (2011).
Taking Scope.
The MITPress.Tur, G., Deoras, A., and Hakkani-Tur, D.(2013).
Semantic parsing using word con-fusion networks with conditional randomfields.
In Proc.
of the INTERSPEECH.Wong, Y. and Mooney, R. (2007).
Learningsynchronous grammars for semantic parsingwith lambda calculus.
In Proceedings of theConference of the Association for Computa-tional Linguistics.Yao, X. and Van Durme, B.
(2014).
Informa-tion extraction over structured data: Ques-tion answering with freebase.
In Associationfor Computational Linguistics (ACL).Zelle, J. and Mooney, R. (1996).
Learning toparse database queries using inductive logicprogramming.
In Proceedings of the Na-tional Conference on Artificial Intelligence.Zettlemoyer, L. and Collins, M. (2005).
Learn-ing to map sentences to logical form: Struc-tured classification with probabilistic cate-gorial grammars.
In Proceedings of the Con-ference on Uncertainty in Artificial Intelli-gence.Zettlemoyer, L. and Collins, M. (2007).
On-line learning of relaxed CCG grammars forparsing to logical form.
In Proceedings ofthe Joint Conference on Empirical Methodsin Natural Language Processing and Com-putational Natural Language Learning.Zettlemoyer, L. and Collins, M. (2009).
Learn-ing context-dependent mappings from sen-tences to logical form.
In Proceedings ofthe Joint Conference of the Associationfor Computational Linguistics and Interna-tional Joint Conference on Natural Lan-guage Processing.1295
