Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 126?135,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLearning High-Level Planning from TextS.R.K.
Branavan, Nate Kushman, Tao Lei, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{branavan, nkushman, taolei, regina}@csail.mit.eduAbstractComprehending action preconditions and ef-fects is an essential step in modeling the dy-namics of the world.
In this paper, we ex-press the semantics of precondition relationsextracted from text in terms of planning oper-ations.
The challenge of modeling this con-nection is to ground language at the level ofrelations.
This type of grounding enables us tocreate high-level plans based on language ab-stractions.
Our model jointly learns to predictprecondition relations from text and to per-form high-level planning guided by those rela-tions.
We implement this idea in the reinforce-ment learning framework using feedback au-tomatically obtained from plan execution at-tempts.
When applied to a complex virtualworld and text describing that world, our rela-tion extraction technique performs on par witha supervised baseline, yielding an F-measureof 66% compared to the baseline?s 65%.
Ad-ditionally, we show that a high-level plannerutilizing these extracted relations significantlyoutperforms a strong, text unaware baseline?
successfully completing 80% of planningtasks as compared to 69% for the baseline.11 IntroductionUnderstanding action preconditions and effects is abasic step in modeling the dynamics of the world.For example, having seeds is a precondition forgrowing wheat.
Not surprisingly, preconditions havebeen extensively explored in various sub-fields ofAI.
However, existing work on action models haslargely focused on tasks and techniques specific toindividual sub-fields with little or no interconnectionbetween them.
In NLP, precondition relations havebeen studied in terms of the linguistic mechanisms1The code, data and experimental setup for this work areavailable at http://groups.csail.mit.edu/rbg/code/planningA pickaxe, which is used to harvest stone, can bemade from wood.
(a)Low Level Actions for: wood?
pickaxe?
stonestep 1: move from (0,0) to (2,0)step 2: chop tree at: (2,0)step 3: get wood at: (2,0)step 4: craft plank from woodstep 5: craft stick from plankstep 6: craft pickaxe from plank and stick?
?
?step N-1: pickup tool: pickaxestep N: harvest stone with pickaxe at: (5,5)(b)Figure 1: Text description of preconditions and effects(a), and the low-level actions connecting them (b).that realize them, while in classical planning, theserelations are viewed as a part of world dynamics.In this paper, we bring these two parallel views to-gether, grounding the linguistic realization of theserelations in the semantics of planning operations.The challenge and opportunity of this fusioncomes from the mismatch between the abstractionsof human language and the granularity of planningprimitives.
Consider, for example, text describing avirtual world such as Minecraft2 and a formal de-scription of that world using planning primitives.Due to the mismatch in granularity, even the simplerelations between wood, pickaxe and stone describedin the sentence in Figure 1a results in dozens of low-level planning actions in the world, as can be seenin Figure 1b.
While the text provides a high-leveldescription of world dynamics, it does not providesufficient details for successful plan execution.
Onthe other hand, planning with low-level actions doesnot suffer from this limitation, but is computation-ally intractable for even moderately complex tasks.As a consequence, in many practical domains, plan-ning algorithms rely on manually-crafted high-level2http://www.minecraft.net/126abstractions to make search tractable (Ghallab et al,2004; Lekavy?
and Na?vrat, 2007).The central idea of our work is to express the se-mantics of precondition relations extracted from textin terms of planning operations.
For instance, theprecondition relation between pickaxe and stone de-scribed in the sentence in Figure 1a indicates thatplans which involve obtaining stone will likely needto first obtain a pickaxe.
The novel challenge of thisview is to model grounding at the level of relations,in contrast to prior work which focused on object-level grounding.
We build on the intuition that thevalidity of precondition relations extracted from textcan be informed by the execution of a low-levelplanner.3 This feedback can enable us to learn theserelations without annotations.
Moreover, we can usethe learned relations to guide a high level plannerand ultimately improve planning performance.We implement these ideas in the reinforcementlearning framework, wherein our model jointlylearns to predict precondition relations from text andto perform high-level planning guided by those rela-tions.
For a given planning task and a set of can-didate relations, our model repeatedly predicts a se-quence of subgoals where each subgoal specifies anattribute of the world that must be made true.
Itthen asks the low-level planner to find a plan be-tween each consecutive pair of subgoals in the se-quence.
The observed feedback ?
whether the low-level planner succeeded or failed at each step ?
isutilized to update the policy for both text analysisand high-level planning.We evaluate our algorithm in the Minecraft virtualworld, using a large collection of user-generated on-line documents as our source of textual information.Our results demonstrate the strength of our relationextraction technique ?
while using planning feed-back as its only source of supervision, it achievesa precondition relation extraction accuracy on parwith that of a supervised SVM baseline.
Specifi-cally, it yields an F-score of 66% compared to the65% of the baseline.
In addition, we show thatthese extracted relations can be used to improve theperformance of a high-level planner.
As baselines3If a planner can find a plan to successfully obtain stoneafter obtaining a pickaxe, then a pickaxe is likely a preconditionfor stone.
Conversely, if a planner obtains stone without firstobtaining a pickaxe, then it is likely not a precondition.for this evaluation, we employ the Metric-FF plan-ner (Hoffmann and Nebel, 2001),4 as well as a text-unaware variant of our model.
Our results show thatour text-driven high-level planner significantly out-performs all baselines in terms of completed plan-ning tasks ?
it successfully solves 80% as comparedto 41% for the Metric-FF planner and 69% for thetext unaware variant of our model.
In fact, the per-formance of our method approaches that of an ora-cle planner which uses manually-annotated precon-ditions.2 Related WorkExtracting Event Semantics from Text The taskof extracting preconditions and effects has previ-ously been addressed in the context of lexical se-mantics (Sil et al, 2010; Sil and Yates, 2011).These approaches combine large-scale distributionaltechniques with supervised learning to identify de-sired semantic relations in text.
Such combined ap-proaches have also been shown to be effective foridentifying other relationships between events, suchas causality (Girju and Moldovan, 2002; Chang andChoi, 2006; Blanco et al, 2008; Beamer and Girju,2009; Do et al, 2011).Similar to these methods, our algorithm capital-izes on surface linguistic cues to learn preconditionsfrom text.
However, our only source of supervisionis the feedback provided by the planning task whichutilizes the predictions.
Additionally, we not onlyidentify these relations in text, but also show theyare valuable in performing an external task.Learning Semantics via Language GroundingOur work fits into the broad area of grounded lan-guage acquisition, where the goal is to learn linguis-tic analysis from a situated context (Oates, 2001;Siskind, 2001; Yu and Ballard, 2004; Fleischmanand Roy, 2005; Mooney, 2008a; Mooney, 2008b;Branavan et al, 2009; Liang et al, 2009; Vogeland Jurafsky, 2010).
Within this line of work, weare most closely related to the reinforcement learn-ing approaches that learn language by interactingwith an external environment (Branavan et al, 2009;Branavan et al, 2010; Vogel and Jurafsky, 2010;Branavan et al, 2011).4The state-of-the-art baseline used in the 2008 InternationalPlanning Competition.
http://ipc.informatik.uni-freiburg.de/127Text (input):A pickaxe, which is used to harvest stone,can be made from wood.Precondition Relations:pickaxe stonewood pickaxePlan Subgoal Sequence:initialstatestone(goal)wood(subgoal 1)pickaxe(subgoal 2)Figure 2: A high-level plan showing two subgoals ina precondition relation.
The corresponding sentence isshown above.The key distinction of our work is the use ofgrounding to learn abstract pragmatic relations, i.e.to learn linguistic patterns that describe relationshipsbetween objects in the world.
This supplements pre-vious work which grounds words to objects in theworld (Branavan et al, 2009; Vogel and Jurafsky,2010).
Another important difference of our setupis the way the textual information is utilized in thesituated context.
Instead of getting step-by-step in-structions from the text, our model uses text that de-scribes general knowledge about the domain struc-ture.
From this text, it extracts relations betweenobjects in the world which hold independently ofany given task.
Task-specific solutions are then con-structed by a planner that relies on these relations toperform effective high-level planning.Hierarchical Planning It is widely accepted thathigh-level plans that factorize a planning prob-lem can greatly reduce the corresponding searchspace (Newell et al, 1959; Bacchus and Yang,1994).
Previous work in planning has studiedthe theoretical properties of valid abstractions andproposed a number of techniques for generatingthem (Jonsson and Barto, 2005; Wolfe and Barto,2005; Mehta et al, 2008; Barry et al, 2011).
In gen-eral, these techniques use static analysis of the low-level domain to induce effective high-level abstrac-tions.
In contrast, our focus is on learning the ab-straction from natural language.
Thus our techniqueis complementary to past work, and can benefit fromhuman knowledge about the domain structure.3 Problem FormulationOur task is two-fold.
First, given a text documentdescribing an environment, we wish to extract a setof precondition/effect relations implied by the text.Second, we wish to use these induced relations todetermine an action sequence for completing a giventask in the environment.We formalize our task as illustrated in Figure 2.As input, we are given a world defined by the tuple?S,A, T ?, where S is the set of possible world states,A is the set of possible actions and T is a determin-istic state transition function.
Executing action a instate s causes a transition to a new state s?
accordingto T (s?
| s, a).
States are represented using proposi-tional logic predicates xi ?
X , where each state issimply a set of such predicates, i.e.
s ?
X .The objective of the text analysis part of our taskis to automatically extract a set of valid precondi-tion/effect relationships from a given document d.Given our definition of the world state, precondi-tions and effects are merely single term predicates,xi, in this world state.
We assume that we are givena seed mapping between a predicate xi, and theword types in the document that reference it (seeTable 3 for examples).
Thus, for each predicatepair ?xk, xl?, we want to utilize the text to predictwhether xk is a precondition for xl; i.e., xk ?
xl.For example, from the text in Figure 2, we want topredict that possessing a pickaxe is a preconditionfor possessing stone.
Note that this relation impliesthe reverse as well, i.e.
xl can be interpreted as theeffect of an action sequence performed on state xk.Each planning goal g ?
G is defined by a startingstate sg0, and a final goal state sgf .
This goal state isrepresented by a set of predicates which need to bemade true.
In the planning part of our task our objec-tive is to find a sequence of actions ~a that connect sg0to sgf .
Finally, we assume document d does not con-tain step-by-step instructions for any individual task,but instead describes general facts about the givenworld that are useful for a wide variety of tasks.4 ModelThe key idea behind our model is to leverage textualdescriptions of preconditions and effects to guide theconstruction of high level plans.
We define a high-level plan as a sequence of subgoals, where each128subgoal is represented by a single-term predicate,xi, that needs to be set in the corresponding worldstate ?
e.g.
have(wheat)=true.
Thus the set ofpossible subgoals is defined by the set of all possi-ble single-term predicates in the domain.
In contrastto low-level plans, the transition between these sub-goals can involve multiple low-level actions.
Our al-gorithm for textually informed high-level planningoperates in four steps:1.
Use text to predict the preconditions of eachsubgoal.
These predictions are for the entiredomain and are not goal specific.2.
Given a planning goal and the induced pre-conditions, predict a subgoal sequence thatachieves the given goal.3.
Execute the predicted sequence by giving eachpair of consecutive subgoals to a low-levelplanner.
This planner, treated as a black-box,computes the low-level plan actions necessaryto transition from one subgoal to the next.4.
Update the model parameters, using the low-level planner?s success or failure as the sourceof supervision.We formally define these steps below.Modeling Precondition Relations Given a docu-ment d, and a set of subgoal pairs ?xi, xj?, we wantto predict whether subgoal xi is a precondition forxj .
We assume that precondition relations are gener-ally described within single sentences.
We first useour seed grounding in a preprocessing step wherewe extract all predicate pairs where both predicatesare mentioned in the same sentence.
We call this setthe Candidate Relations.
Note that this set will con-tain many invalid relations since co-occurrence in asentence does not necessarily imply a valid precon-dition relation.5 Thus for each sentence, ~wk, asso-ciated with a given Candidate Relation, xi ?
xj ,our task is to predict whether the sentence indicatesthe relation.
We model this decision via a log lineardistribution as follows:p(xi ?
xj | ~wk, qk; ?c) ?
e?c?
?c(xi,xj , ~wk,qk), (1)where ?c is the vector of model parameters.
Wecompute the feature function ?c using the seed5In our dataset only 11% of Candidate Relations are valid.Input: A document d, Set of planning tasks G,Set of candidate precondition relations Call,Reward function r(), Number of iterations TInitialization:Model parameters ?x = 0 and ?c = 0.for i = 1 ?
?
?T doSample valid preconditions:C ?
?foreach ?xi, xj?
?
Call doforeach Sentence ~wk containing xi and xj dov ?
p(xi ?
xj | ~wk, qk; ?c)if v = 1 then C = C ?
?xi, xj?endendPredict subgoal sequences for each task g.foreach g ?
G doSample subgoal sequence ~x as follows:for t = 1 ?
?
?n doSample next subgoal:xt ?
p(x | xt?1, sg0, sgf , C; ?x)Construct low-level subtask from xt?1 to xtExecute low-level planner on subtaskendUpdate subgoal prediction model using Eqn.
2endUpdate text precondition model using Eqn.
3endAlgorithm 1: A policy gradient algorithm for pa-rameter estimation in our model.grounding, the sentence ~wk, and a given dependencyparse qk of the sentence.
Given these per-sentencedecisions, we predict the set of all valid precondi-tion relations, C, in a deterministic fashion.
We dothis by considering a precondition xi ?
xj as validif it is predicted to be valid by at least one sentence.Modeling Subgoal Sequences Given a planninggoal g, defined by initial and final goal states sg0 andsgf , our task is to predict a sequence of subgoals ~xwhich will achieve the goal.
We condition this de-cision on our predicted set of valid preconditions C,by modeling the distribution over sequences ~x as:p(~x | sg0, sgf , C; ?x) =n?t=1p(xt | xt?1, sg0, sgf , C; ?x),p(xt | xt?1, sg0, sgf , C; ?x) ?
e?x?
?x(xt,xt?1,sg0,sgf ,C).Here we assume that subgoal sequences are Marko-vian in nature and model individual subgoal predic-tions using a log-linear model.
Note that in con-129trast to Equation 1 where the predictions are goal-agnostic, these predictions are goal-specific.
As be-fore, ?x is the vector of model parameters, and ?x isthe feature function.
Additionally, we assume a spe-cial stop symbol, x?, which indicates the end of thesubgoal sequence.Parameter Update Parameter updates in our modelare done via reinforcement learning.
Specifically,once the model has predicted a subgoal sequence fora given goal, the sequence is given to the low-levelplanner for execution.
The success or failure of thisexecution is used to compute the reward signal r forparameter estimation.
This predict-execute-updatecycle is repeated until convergence.
We assume thatour reward signal r strongly correlates with the cor-rectness of model predictions.
Therefore, duringlearning, we need to find the model parameters thatmaximize expected future reward (Sutton and Barto,1998).
We perform this maximization via stochasticgradient ascent, using the standard policy gradientalgorithm (Williams, 1992; Sutton et al, 2000).We perform two separate policy gradient updates,one for each model component.
The objective of thetext component of our model is purely to predict thevalidity of preconditions.
Therefore, subgoal pairs?xk, xl?, where xl is reachable from xk, are givenpositive reward.
The corresponding parameter up-date, with learning rate ?c, takes the following form:?
?c ?
?c r[?c(xi, xj , ~wk, qk) ?Ep(xi??xj?
|?)[?c(xi?
, xj?
, ~wk, qk)]].
(2)The objective of the planning component of ourmodel is to predict subgoal sequences that success-fully achieve the given planning goals.
Thus we di-rectly use plan-success as a binary reward signal,which is applied to each subgoal decision in a se-quence.
This results in the following update:?
?x ?
?x r?t[?x(xt, xt?1, sg0, sgf , C) ?Ep(x?t|?
)[?x(x?t, xt?1, sg0, sgf , C)] ], (3)where t indexes into the subgoal sequence and ?x isthe learning rate.fishironshears bucketmilkstringseeds wooliron doorbone mealfishing rodwoodplankstickfenceFigure 3: Example of the precondition dependenciespresent in the Minecraft domain.Domain #Objects #Pred Types #ActionsParking 49 5 4Floortile 61 10 7Barman 40 15 12Minecraft 108 16 68Table 1: A comparison of complexity between Minecraftand some domains used in the IPC-2011 sequential satis-ficing track.
In the Minecraft domain, the number of ob-jects, predicate types, and actions is significantly larger.5 Applying the ModelWe apply our method to Minecraft, a grid-based vir-tual world.
Each grid location represents a tile of ei-ther land or water and may also contain resources.Users can freely move around the world, harvestresources and craft various tools and objects fromthese resources.
The dynamics of the world requirecertain resources or tools as prerequisites for per-forming a given action, as can be seen in Figure 3.For example, a user must first craft a bucket beforethey can collect milk.Defining the Domain In order to execute a tradi-tional planner on the Minecraft domain, we definethe domain using the Planning Domain DefinitionLanguage (PDDL) (Fox and Long, 2003).
This is thestandard task definition language used in the Inter-national Planning Competitions (IPC).6 We defineas predicates all aspects of the game state ?
for ex-ample, the location of resources in the world, the re-sources and objects possessed by the player, and theplayer?s location.
Our subgoals xi and our task goalssgf map directly to these predicates.
This results ina domain with significantly greater complexity thanthose solvable by traditional low-level planners.
Ta-ble 1 compares the complexity of our domain withsome typical planning domains used in the IPC.6http://ipc.icaps-conference.org/130Low-level Planner As our low-level planner weemploy Metric-FF (Hoffmann and Nebel, 2001),the state-of-the-art baseline used in the 2008 In-ternational Planning Competition.
Metric-FF is aforward-chaining heuristic state space planner.
Itsmain heuristic is to simplify the task by ignoring op-erator delete lists.
The number of actions in the so-lution for this simplified task is then used as the goaldistance estimate for various search strategies.Features The two components of our model lever-age different types of information, and as a result,they each use distinct sets of features.
The text com-ponent features ?c are computed over sentences andtheir dependency parses.
The Stanford parser (deMarneffe et al, 2006) was used to generate the de-pendency parse information for each sentence.
Ex-amples of these features appear in Table 2.
The se-quence prediction component takes as input both thepreconditions induced by the text component as wellas the planning state and the previous subgoal.
Thus?x contains features which check whether two sub-goals are connected via an induced precondition re-lation, in addition to features which are simply theCartesian product of domain predicates.6 Experimental SetupDatasets As the text description of our virtual world,we use documents from the Minecraft Wiki,7 themost popular information source about the game.Our manually constructed seed grounding of pred-icates contains 74 entries, examples of which can beseen in Table 3.
We use this seed grounding to iden-tify a set of 242 sentences that reference predicatesin the Minecraft domain.
This results in a set of694 Candidate Relations.
We also manually anno-tated the relations expressed in the text, identifying94 of the Candidate Relations as valid.
Our corpuscontains 979 unique word types and is composed ofsentences with an average length of 20 words.We test our system on a set of 98 problems thatinvolve collecting resources and constructing ob-jects in the Minecraft domain ?
for example, fish-ing, cooking and making furniture.
To assess thecomplexity of these tasks, we manually constructedhigh-level plans for these goals and solved them us-ing the Metric-FF planner.
On average, the execu-7http://www.minecraftwiki.net/wiki/Minecraft Wiki/WordsDependency TypesDependency Type ?
DirectionWord ?
Dependency TypeWord ?
Dependency Type ?
DirectionTable 2: Example text features.
A subgoal pair ?xi, xj?is first mapped to word tokens using a small groundingtable.
Words and dependencies are extracted along pathsbetween mapped target words.
These are combined withpath directions to generate the text features.Domain Predicate Noun Phraseshave(plank) wooden plank, wood plankhave(stone) stone, cobblestonehave(iron) iron ingotTable 3: Examples in our seed grounding table.
Eachpredicate is mapped to one or more noun phrases that de-scribe it in the text.tion of the sequence of low-level plans takes 35 ac-tions, with 3 actions for the shortest plan and 123actions for the longest.
The average branching fac-tor is 9.7, leading to an average search space of morethan 1034 possible action sequences.
For evaluationpurposes we manually identify a set of Gold Rela-tions consisting of all precondition relations that arevalid in this domain, including those not discussedin the text.Evaluation Metrics We use our manual annotationsto evaluate the type-level accuracy of relation extrac-tion.
To evaluate our high-level planner, we use thestandard measure adopted by the IPC.
This evalu-ation measure simply assesses whether the plannercompletes a task within a predefined time.Baselines To evaluate the performance of our rela-tion extraction, we compare against an SVM classi-fier8 trained on the Gold Relations.
We test the SVMbaseline in a leave-one-out fashion.To evaluate the performance of our text-awarehigh-level planner, we compare against five base-lines.
The first two baselines ?
FF and No Text ?do not use any textual information.
The FF base-line directly runs the Metric-FF planner on the giventask, while the No Text baseline is a variant of ourmodel that learns to plan in the reinforcement learn-ing framework.
It uses the same state-level features8SVMlight (Joachims, 1999) with default parameters.131?Seeds  for growing  wheat  can be obtained by breaking  tall grass (false negative)Sticks  are the only building material required to craft a  fence  or  ladder.Figure 4: Examples of precondition relations predicted by our model from text.
Check marks (3) indicate correctpredictions, while a cross (8) marks the incorrect one ?
in this case, a valid relation that was predicted as invalid byour model.
Note that each pair of highlighted noun phrases in a sentence is a Candidate Relation, and pairs that arenot connected by an arrow were correctly predicted to be invalid by our model.200100 15050Figure 5: The performance of our model and a supervisedSVM baseline on the precondition prediction task.
Alsoshown is the F-Score of the full set of Candidate Rela-tions which is used unmodified by All Text, and is given asinput to our model.
Our model?s F-score, averaged over200 trials, is shown with respect to learning iterations.as our model, but does not have access to text.The All Text baseline has access to the full set of694 Candidate Relations.
During learning, our fullmodel refines this set of relations, while in contrastthe All Text baseline always uses the full set.The two remaining baselines constitute the upperbound on the performance of our model.
The first,Manual Text, is a variant of our model which directlyuses the links derived from manual annotations ofpreconditions in text.
The second, Gold, has accessto the Gold Relations.
Note that the connectionsavailable to Manual Text are a subset of the Goldlinks, because the text does not specify all relations.Experimental Details All experimental results areaveraged over 200 independent runs for both ourmodel as well as the baselines.
Each of these tri-als is run for 200 learning iterations with a max-imum subgoal sequence length of 10.
To find alow-level plan between each consecutive pair of sub-goals, our high-level planner internally uses Metric-FF.
We give Metric-FF a one-minute timeout to findsuch a low-level plan.
To ensure that the comparisonMethod %PlansFF 40.8No text 69.4All text 75.5Full model 80.2Manual text 84.7Gold connection 87.1Table 4: Percentage of tasks solved successfully by ourmodel and the baselines.
All performance differences be-tween methods are statistically significant at p ?
.01.between the high-level planners and the FF baselineis fair, the FF baseline is allowed a runtime of 2,000minutes.
This is an upper bound on the time that ourhigh-level planner can take over the 200 learning it-erations, with subgoal sequences of length at most10 and a one minute timeout.
Lastly, during learningwe initialize all parameters to zero, use a fixed learn-ing rate of 0.0001, and encourage our model to ex-plore the state space by using the standard -greedyexploration strategy (Sutton and Barto, 1998).7 ResultsRelation Extraction Figure 5 shows the perfor-mance of our method on identifying preconditionsin text.
We also show the performance of the super-vised SVM baseline.
As can be seen, after 200 learn-ing iterations, our model achieves an F-Measure of66%, equal to the supervised baseline.
These resultssupport our hypothesis that planning feedback is apowerful source of supervision for analyzing a giventext corpus.
Figure 4 shows some examples of sen-tences and the corresponding extracted relations.Planning Performance As shown in Table 4 ourtext-enriched planning model outperforms the text-free baselines by more than 10%.
Moreover, theperformance improvement of our model over the AllText baseline demonstrates that the accuracy of the1320% 20% 40% 60% 80% 100%No textAll textFull modelManual textGoldEasyHard71%64%59%48%31% 88%89%91%94%95%Figure 6: Percentage of problems solved by various mod-els on Easy and Hard problem sets.extracted text relations does indeed impact planningperformance.
A similar conclusion can be reachedby comparing the performance of our model and theManual Text baseline.The difference in performance of 2.35% betweenManual Text and Gold shows the importance of theprecondition information that is missing from thetext.
Note that Gold itself does not complete alltasks ?
this is largely because the Markov assump-tion made by our model does not hold for all tasks.9Figure 6 breaks down the results based on the dif-ficulty of the corresponding planning task.
We mea-sure problem complexity in terms of the low-levelsteps needed to implement a manually constructedhigh-level plan.
Based on this measure, we dividethe problems into two sets.
As can be seen, all ofthe high-level planners solve almost all of the easyproblems.
However, performance varies greatly onthe more challenging tasks, directly correlating withplanner sophistication.
On these tasks our modeloutperforms the No Text baseline by 28% and theAll Text baseline by 11%.Feature Analysis Figure 7 shows the top five pos-itive features for our model and the SVM baseline.Both models picked up on the words that indicateprecondition relations in this domain.
For instance,the word use often occurs in sentences that describethe resources required to make an object, such as?bricks are items used to craft brick blocks?.
In ad-dition to lexical features, dependency information isalso given high weight by both learners.
An example9When a given task has two non-trivial preconditions, ourmodel will choose to satisfy one of the two first, and the Markovassumption blinds it to the remaining precondition, preventingit from determining that it must still satisfy the other.path has word "craft"path has dependency type "partmod"path has word "equals"path has word "use"path has dependency type "xsubj"path has word "use"path has word "fill"path has dependency type "dobj"path has dependency type "xsubj"path has word "craft"Figure 7: The top five positive features on words anddependency types learned by our model (above) and bySVM (below) for precondition prediction.of this is a feature that checks for the direct objectdependency type.
This analysis is consistent withprior work on event semantics which shows lexico-syntactic features are effective cues for learning textrelations (Blanco et al, 2008; Beamer and Girju,2009; Do et al, 2011).8 ConclusionsIn this paper, we presented a novel technique for in-ducing precondition relations from text by ground-ing them in the semantics of planning operations.While using planning feedback as its only sourceof supervision, our method for relation extractionachieves a performance on par with that of a su-pervised baseline.
Furthermore, relation groundingprovides a new view on classical planning problemswhich enables us to create high-level plans based onlanguage abstractions.
We show that building high-level plans in this manner significantly outperformstraditional techniques in terms of task completion.AcknowledgmentsThe authors acknowledge the support of theNSF (CAREER grant IIS-0448168, grant IIS-0835652), the DARPA Machine Reading Program(FA8750-09-C-0172, PO#4910018860), and Batelle(PO#300662).
Thanks to Amir Globerson, TommiJaakkola, Leslie Kaelbling, George Konidaris, Dy-lan Hadfield-Menell, Stefanie Tellex, the MIT NLPgroup, and the ACL reviewers for their suggestionsand comments.
Any opinions, findings, conclu-sions, or recommendations expressed in this paperare those of the authors, and do not necessarily re-flect the views of the funding organizations.133ReferencesFahiem Bacchus and Qiang Yang.
1994.
Downwardrefinement and the efficiency of hierarchical problemsolving.
Artificial Intell., 71(1):43?100.Jennifer L. Barry, Leslie Pack Kaelbling, and TomsLozano-Prez.
2011.
DetH*: Approximate hierarchi-cal solution of large markov decision processes.
InIJCAI?11, pages 1928?1935.Brandon Beamer and Roxana Girju.
2009.
Using a bi-gram event model to predict causal potential.
In Pro-ceedings of CICLing, pages 430?441.Eduardo Blanco, Nuria Castell, and Dan Moldovan.2008.
Causal relation extraction.
In Proceedings ofthe LREC?08.S.R.K Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofACL, pages 82?90.S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay.2010.
Reading between the lines: Learning to maphigh-level instructions to commands.
In Proceedingsof ACL, pages 1268?1277.S.
R. K. Branavan, David Silver, and Regina Barzilay.2011.
Learning to win by reading manuals in a monte-carlo framework.
In Proceedings of ACL, pages 268?277.Du-Seong Chang and Key-Sun Choi.
2006.
Incremen-tal cue phrase learning and bootstrapping method forcausality extraction using cue phrase and word pairprobabilities.
Inf.
Process.
Manage., 42(3):662?678.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC 2006.Q.
Do, Y. Chan, and D. Roth.
2011.
Minimally super-vised event causality identification.
In EMNLP, 7.Michael Fleischman and Deb Roy.
2005.
Intentionalcontext in situated natural language learning.
In Pro-ceedings of CoNLL, pages 104?111.Maria Fox and Derek Long.
2003.
Pddl2.1: An ex-tension to pddl for expressing temporal planning do-mains.
Journal of Artificial Intelligence Research,20:2003.Malik Ghallab, Dana S. Nau, and Paolo Traverso.
2004.Automated Planning: theory and practice.
MorganKaufmann.Roxana Girju and Dan I. Moldovan.
2002.
Text miningfor causal relations.
In Proceedigns of FLAIRS, pages360?364.Jo?rg Hoffmann and Bernhard Nebel.
2001.
The FF plan-ning system: Fast plan generation through heuristicsearch.
JAIR, 14:253?302.Thorsten Joachims.
1999.
Advances in kernel meth-ods.
chapter Making large-scale support vector ma-chine learning practical, pages 169?184.
MIT Press.Anders Jonsson and Andrew Barto.
2005.
A causalapproach to hierarchical decomposition of factoredmdps.
In Advances in Neural Information ProcessingSystems, 13:10541060, page 22.
Press.Maria?n Lekavy?
and Pavol Na?vrat.
2007.
Expressivityof strips-like and htn-like planning.
Lecture Notes inArtificial Intelligence, 4496:121?130.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proceedings of ACL, pages 91?99.Neville Mehta, Soumya Ray, Prasad Tadepalli, andThomas Dietterich.
2008.
Automatic discovery andtransfer of maxq hierarchies.
In Proceedings of the25th international conference on Machine learning,ICML ?08, pages 648?655.Raymond J. Mooney.
2008a.
Learning language from itsperceptual context.
In Proceedings of ECML/PKDD.Raymond J. Mooney.
2008b.
Learning to connect lan-guage and perception.
In Proceedings of AAAI, pages1598?1601.A.
Newell, J.C. Shaw, and H.A.
Simon.
1959.
The pro-cesses of creative thinking.
Paper P-1320.
Rand Cor-poration.James Timothy Oates.
2001.
Grounding knowledgein sensors: Unsupervised learning for language andplanning.
Ph.D. thesis, University of MassachusettsAmherst.Avirup Sil and Alexander Yates.
2011.
Extract-ing STRIPS representations of actions and events.In Recent Advances in Natural Language Learning(RANLP).Avirup Sil, Fei Huang, and Alexander Yates.
2010.
Ex-tracting action and event semantics from web text.
InAAAI 2010 Fall Symposium on Commonsense Knowl-edge (CSK).Jeffrey Mark Siskind.
2001.
Grounding the lexical se-mantics of verbs in visual perception using force dy-namics and event logic.
Journal of Artificial Intelli-gence Research, 15:31?90.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning: An Introduction.
The MIT Press.Richard S. Sutton, David McAllester, Satinder Singh, andYishay Mansour.
2000.
Policy gradient methods forreinforcement learning with function approximation.In Advances in NIPS, pages 1057?1063.Adam Vogel and Daniel Jurafsky.
2010.
Learning tofollow navigational directions.
In Proceedings of theACL, pages 806?814.Ronald J Williams.
1992.
Simple statistical gradient-following algorithms for connectionist reinforcementlearning.
Machine Learning, 8.134Alicia P. Wolfe and Andrew G. Barto.
2005.
Identify-ing useful subgoals in reinforcement learning by localgraph partitioning.
In In Proceedings of the Twenty-Second International Conference on Machine Learn-ing, pages 816?823.Chen Yu and Dana H. Ballard.
2004.
On the integrationof grounding language and learning objects.
In Pro-ceedings of AAAI, pages 488?493.135
