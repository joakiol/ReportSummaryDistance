Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1556?1566,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsImproved Semantic Representations FromTree-Structured Long Short-Term Memory NetworksKai Sheng Tai, Richard Socher*, Christopher D. ManningComputer Science Department, Stanford University, *MetaMind Inc.kst@cs.stanford.edu, richard@metamind.io, manning@stanford.eduAbstractBecause of their superior ability to pre-serve sequence information over time,Long Short-Term Memory (LSTM) net-works, a type of recurrent neural net-work with a more complex computationalunit, have obtained strong results on a va-riety of sequence modeling tasks.
Theonly underlying LSTM structure that hasbeen explored so far is a linear chain.However, natural language exhibits syn-tactic properties that would naturally com-bine words to phrases.
We introduce theTree-LSTM, a generalization of LSTMs totree-structured network topologies.
Tree-LSTMs outperform all existing systemsand strong LSTM baselines on two tasks:predicting the semantic relatedness of twosentences (SemEval 2014, Task 1) andsentiment classification (Stanford Senti-ment Treebank).1 IntroductionMost models for distributed representations ofphrases and sentences?that is, models where real-valued vectors are used to represent meaning?fallinto one of three classes: bag-of-words models,sequence models, and tree-structured models.
Inbag-of-words models, phrase and sentence repre-sentations are independent of word order; for ex-ample, they can be generated by averaging con-stituent word representations (Landauer and Du-mais, 1997; Foltz et al, 1998).
In contrast, se-quence models construct sentence representationsas an order-sensitive function of the sequence oftokens (Elman, 1990; Mikolov, 2012).
Lastly,tree-structured models compose each phrase andsentence representation from its constituent sub-phrases according to a given syntactic structureover the sentence (Goller and Kuchler, 1996;Socher et al, 2011).x1x2x3x4y1y2y3y4x1x2x4x5x6y1y2y3y4y6Figure 1: Top: A chain-structured LSTM net-work.
Bottom: A tree-structured LSTM networkwith arbitrary branching factor.Order-insensitive models are insufficient tofully capture the semantics of natural languagedue to their inability to account for differences inmeaning as a result of differences in word orderor syntactic structure (e.g., ?cats climb trees?
vs.?trees climb cats?).
We therefore turn to order-sensitive sequential or tree-structured models.
Inparticular, tree-structured models are a linguisti-cally attractive option due to their relation to syn-tactic interpretations of sentence structure.
A nat-ural question, then, is the following: to what ex-tent (if at all) can we do better with tree-structuredmodels as opposed to sequential models for sen-tence representation?
In this paper, we work to-wards addressing this question by directly com-paring a type of sequential model that has recentlybeen used to achieve state-of-the-art results in sev-eral NLP tasks against its tree-structured general-ization.Due to their capability for processing arbitrary-length sequences, recurrent neural networks1556(RNNs) are a natural choice for sequence model-ing tasks.
Recently, RNNs with Long Short-TermMemory (LSTM) units (Hochreiter and Schmid-huber, 1997) have re-emerged as a popular archi-tecture due to their representational power and ef-fectiveness at capturing long-term dependencies.LSTM networks, which we review in Sec.
2, havebeen successfully applied to a variety of sequencemodeling and prediction tasks, notably machinetranslation (Bahdanau et al, 2015; Sutskever et al,2014), speech recognition (Graves et al, 2013),image caption generation (Vinyals et al, 2014),and program execution (Zaremba and Sutskever,2014).In this paper, we introduce a generalization ofthe standard LSTM architecture to tree-structurednetwork topologies and show its superiority forrepresenting sentence meaning over a sequentialLSTM.
While the standard LSTM composes itshidden state from the input at the current timestep and the hidden state of the LSTM unit in theprevious time step, the tree-structured LSTM, orTree-LSTM, composes its state from an input vec-tor and the hidden states of arbitrarily many childunits.
The standard LSTM can then be considereda special case of the Tree-LSTM where each inter-nal node has exactly one child.In our evaluations, we demonstrate the empiri-cal strength of Tree-LSTMs as models for repre-senting sentences.
We evaluate the Tree-LSTMarchitecture on two tasks: semantic relatednessprediction on sentence pairs and sentiment clas-sification of sentences drawn from movie reviews.Our experiments show that Tree-LSTMs outper-form existing systems and sequential LSTM base-lines on both tasks.
Implementations of our mod-els and experiments are available at https://github.com/stanfordnlp/treelstm.2 Long Short-Term Memory Networks2.1 OverviewRecurrent neural networks (RNNs) are able to pro-cess input sequences of arbitrary length via the re-cursive application of a transition function on ahidden state vector ht.
At each time step t, thehidden state htis a function of the input vector xtthat the network receives at time t and its previoushidden state ht?1.
For example, the input vector xtcould be a vector representation of the t-th word inbody of text (Elman, 1990; Mikolov, 2012).
Thehidden state ht?
Rdcan be interpreted as a d-dimensional distributed representation of the se-quence of tokens observed up to time t.Commonly, the RNN transition function is anaffine transformation followed by a pointwise non-linearity such as the hyperbolic tangent function:ht= tanh (Wxt+ Uht?1+ b) .Unfortunately, a problem with RNNs with transi-tion functions of this form is that during training,components of the gradient vector can grow or de-cay exponentially over long sequences (Hochre-iter, 1998; Bengio et al, 1994).
This problem withexploding or vanishing gradients makes it difficultfor the RNN model to learn long-distance correla-tions in a sequence.The LSTM architecture (Hochreiter andSchmidhuber, 1997) addresses this problem oflearning long-term dependencies by introducing amemory cell that is able to preserve state over longperiods of time.
While numerous LSTM variantshave been described, here we describe the versionused by Zaremba and Sutskever (2014).We define the LSTM unit at each time step t tobe a collection of vectors in Rd: an input gate it, aforget gate ft, an output gate ot, a memory cell ctand a hidden state ht.
The entries of the gatingvectors it, ftand otare in [0, 1].
We refer to d asthe memory dimension of the LSTM.The LSTM transition equations are the follow-ing:it= ?
(W(i)xt+ U(i)ht?1+ b(i)), (1)ft= ?
(W(f)xt+ U(f)ht?1+ b(f)),ot= ?
(W(o)xt+ U(o)ht?1+ b(o)),ut= tanh(W(u)xt+ U(u)ht?1+ b(u)),ct= itut+ ftct?1,ht= ottanh(ct),where xtis the input at the current time step, ?
de-notes the logistic sigmoid function and  denoteselementwise multiplication.
Intuitively, the for-get gate controls the extent to which the previousmemory cell is forgotten, the input gate controlshow much each unit is updated, and the output gatecontrols the exposure of the internal memory state.The hidden state vector in an LSTM unit is there-fore a gated, partial view of the state of the unit?sinternal memory cell.
Since the value of the gatingvariables vary for each vector element, the model1557can learn to represent information over multipletime scales.2.2 VariantsTwo commonly-used variants of the basic LSTMarchitecture are the Bidirectional LSTM and theMultilayer LSTM (also known as the stacked ordeep LSTM).Bidirectional LSTM.
A Bidirectional LSTM(Graves et al, 2013) consists of two LSTMs thatare run in parallel: one on the input sequence andthe other on the reverse of the input sequence.
Ateach time step, the hidden state of the Bidirec-tional LSTM is the concatenation of the forwardand backward hidden states.
This setup allows thehidden state to capture both past and future infor-mation.Multilayer LSTM.
In Multilayer LSTM archi-tectures, the hidden state of an LSTM unit in layer` is used as input to the LSTM unit in layer `+1 inthe same time step (Graves et al, 2013; Sutskeveret al, 2014; Zaremba and Sutskever, 2014).
Here,the idea is to let the higher layers capture longer-term dependencies of the input sequence.These two variants can be combined as a Multi-layer Bidirectional LSTM (Graves et al, 2013).3 Tree-Structured LSTMsA limitation of the LSTM architectures describedin the previous section is that they only allow forstrictly sequential information propagation.
Here,we propose two natural extensions to the basicLSTM architecture: the Child-Sum Tree-LSTMand the N-ary Tree-LSTM.
Both variants allow forricher network topologies where each LSTM unitis able to incorporate information from multiplechild units.As in standard LSTM units, each Tree-LSTMunit (indexed by j) contains input and outputgates ijand oj, a memory cell cjand hiddenstate hj.
The difference between the standardLSTM unit and Tree-LSTM units is that gatingvectors and memory cell updates are dependenton the states of possibly many child units.
Ad-ditionally, instead of a single forget gate, the Tree-LSTM unit contains one forget gate fjkfor eachchild k. This allows the Tree-LSTM unit to se-lectively incorporate information from each child.For example, a Tree-LSTM model can learn to em-phasize semantic heads in a semantic relatednessh1c1u1x1c3c2h3h2f2f3i1o1Figure 2: Composing the memory cell c1and hid-den state h1of a Tree-LSTM unit with two chil-dren (subscripts 2 and 3).
Labeled edges cor-respond to gating by the indicated gating vector,with dependencies omitted for compactness.task, or it can learn to preserve the representationof sentiment-rich children for sentiment classifica-tion.As with the standard LSTM, each Tree-LSTMunit takes an input vector xj.
In our applications,each xjis a vector representation of a word in asentence.
The input word at each node dependson the tree structure used for the network.
For in-stance, in a Tree-LSTM over a dependency tree,each node in the tree takes the vector correspond-ing to the head word as input, whereas in a Tree-LSTM over a constituency tree, the leaf nodes takethe corresponding word vectors as input.3.1 Child-Sum Tree-LSTMsGiven a tree, let C(j) denote the set of childrenof node j.
The Child-Sum Tree-LSTM transitionequations are the following:?hj=?k?C(j)hk, (2)ij= ?
(W(i)xj+ U(i)?hj+ b(i)), (3)fjk= ?
(W(f)xj+ U(f)hk+ b(f)), (4)oj= ?
(W(o)xj+ U(o)?hj+ b(o)), (5)uj= tanh(W(u)xj+ U(u)?hj+ b(u)), (6)cj= ijuj+?k?C(j)fjkck, (7)hj= ojtanh(cj), (8)where in Eq.
4, k ?
C(j).Intuitively, we can interpret each parameter ma-trix in these equations as encoding correlations be-tween the component vectors of the Tree-LSTM1558unit, the input xj, and the hidden states hkof theunit?s children.
For example, in a dependency treeapplication, the model can learn parameters W(i)such that the components of the input gate ijhavevalues close to 1 (i.e., ?open?)
when a semanti-cally important content word (such as a verb) isgiven as input, and values close to 0 (i.e., ?closed?
)when the input is a relatively unimportant word(such as a determiner).Dependency Tree-LSTMs.
Since the Child-Sum Tree-LSTM unit conditions its componentson the sum of child hidden states hk, it is well-suited for trees with high branching factor orwhose children are unordered.
For example, it is agood choice for dependency trees, where the num-ber of dependents of a head can be highly variable.We refer to a Child-Sum Tree-LSTM applied to adependency tree as a Dependency Tree-LSTM.3.2 N -ary Tree-LSTMsThe N -ary Tree-LSTM can be used on tree struc-tures where the branching factor is at most N andwhere children are ordered, i.e., they can be in-dexed from 1 to N .
For any node j, write the hid-den state and memory cell of its kth child as hjkand cjkrespectively.
The N -ary Tree-LSTM tran-sition equations are the following:ij= ?
(W(i)xj+N?`=1U(i)`hj`+ b(i)), (9)fjk= ?
(W(f)xj+N?`=1U(f)k`hj`+ b(f)),(10)oj= ?
(W(o)xj+N?`=1U(o)`hj`+ b(o)), (11)uj= tanh(W(u)xj+N?`=1U(u)`hj`+ b(u)),(12)cj= ijuj+N?`=1fj`cj`, (13)hj= ojtanh(cj), (14)where in Eq.
10, k = 1, 2, .
.
.
, N .
Note thatwhen the tree is simply a chain, both Eqs.
2?8and Eqs.
9?14 reduce to the standard LSTM tran-sitions, Eqs.
1.The introduction of separate parameter matri-ces for each child k allows the N -ary Tree-LSTMmodel to learn more fine-grained conditioning onthe states of a unit?s children than the Child-Sum Tree-LSTM.
Consider, for example, a con-stituency tree application where the left child of anode corresponds to a noun phrase, and the rightchild to a verb phrase.
Suppose that in this caseit is advantageous to emphasize the verb phrasein the representation.
Then the U(f)k`parameterscan be trained such that the components of fj1areclose to 0 (i.e., ?forget?
), while the components offj2are close to 1 (i.e., ?preserve?
).Forget gate parameterization.
In Eq.
10, wedefine a parameterization of the kth child?s for-get gate fjkthat contains ?off-diagonal?
param-eter matrices U(f)k`, k 6= `.
This parameteriza-tion allows for more flexible control of informa-tion propagation from child to parent.
For exam-ple, this allows the left hidden state in a binary treeto have either an excitatory or inhibitory effect onthe forget gate of the right child.
However, forlarge values of N , these additional parameters areimpractical and may be tied or fixed to zero.Constituency Tree-LSTMs.
We can naturallyapply Binary Tree-LSTM units to binarized con-stituency trees since left and right child nodes aredistinguished.
We refer to this application of Bi-nary Tree-LSTMs as a Constituency Tree-LSTM.Note that in Constituency Tree-LSTMs, a node jreceives an input vector xjonly if it is a leaf node.In the remainder of this paper, we focus onthe special cases of Dependency Tree-LSTMs andConstituency Tree-LSTMs.
These architecturesare in fact closely related; since we consider onlybinarized constituency trees, the parameterizationsof the two models are very similar.
The key dif-ference is in the application of the compositionalparameters: dependent vs. head for DependencyTree-LSTMs, and left child vs. right child for Con-stituency Tree-LSTMs.4 ModelsWe now describe two specific models that applythe Tree-LSTM architectures described in the pre-vious section.4.1 Tree-LSTM ClassificationIn this setting, we wish to predict labels y?
from adiscrete set of classes Y for some subset of nodesin a tree.
For example, the label for a node in a1559parse tree could correspond to some property ofthe phrase spanned by that node.At each node j, we use a softmax classifier topredict the label y?jgiven the inputs {x}jobservedat nodes in the subtree rooted at j.
The classifiertakes the hidden state hjat the node as input:p??
(y | {x}j) = softmax(W(s)hj+ b(s)),y?j= argmaxyp??
(y | {x}j) .The cost function is the negative log-likelihoodof the true class labels y(k)at each labeled node:J(?)
= ?1mm?k=1log p??(y(k)???{x}(k))+?2??
?22,where m is the number of labeled nodes in thetraining set, the superscript k indicates the kth la-beled node, and ?
is an L2 regularization hyperpa-rameter.4.2 Semantic Relatedness of Sentence PairsGiven a sentence pair, we wish to predict areal-valued similarity score in some range [1,K],where K > 1 is an integer.
The sequence{1, 2, .
.
.
,K} is some ordinal scale of similarity,where higher scores indicate greater degrees ofsimilarity, and we allow real-valued scores to ac-count for ground-truth ratings that are an averageover the evaluations of several human annotators.We first produce sentence representations hLand hRfor each sentence in the pair using aTree-LSTM model over each sentence?s parse tree.Given these sentence representations, we predictthe similarity score y?
using a neural network thatconsiders both the distance and angle between thepair (hL, hR):h?= hLhR, (15)h+= |hL?
hR|,hs= ?(W(?
)h?+W(+)h++ b(h)),p?
?= softmax(W(p)hs+ b(p)),y?
= rTp?
?,where rT= [1 2 .
.
.
K] and the absolute valuefunction is applied elementwise.
The use of bothdistance measures h?and h+is empirically mo-tivated: we find that the combination outperformsthe use of either measure alone.
The multiplicativemeasure h?can be interpreted as an elementwisecomparison of the signs of the input representa-tions.We want the expected rating under the predicteddistribution p?
?given model parameters ?
to beclose to the gold rating y ?
[1,K]: y?
= rTp???
y.We therefore define a sparse target distribution1pthat satisfies y = rTp:pi=????
?y ?
byc, i = byc+ 1byc ?
y + 1, i = byc0 otherwisefor 1 ?
i ?
K. The cost function is the regular-ized KL-divergence between p and p??:J(?)
=1mm?k=1KL(p(k)???p?(k)?)+?2??
?22,where m is the number of training pairs and thesuperscript k indicates the kth sentence pair.5 ExperimentsWe evaluate our Tree-LSTM architectures on twotasks: (1) sentiment classification of sentencessampled from movie reviews and (2) predictingthe semantic relatedness of sentence pairs.In comparing our Tree-LSTMs against sequen-tial LSTMs, we control for the number of LSTMparameters by varying the dimensionality of thehidden states2.
Details for each model variant aresummarized in Table 1.5.1 Sentiment ClassificationIn this task, we predict the sentiment of sen-tences sampled from movie reviews.
We usethe Stanford Sentiment Treebank (Socher et al,2013).
There are two subtasks: binary classifica-tion of sentences, and fine-grained classificationover five classes: very negative, negative, neu-tral, positive, and very positive.
We use the stan-dard train/dev/test splits of 6920/872/1821 for thebinary classification subtask and 8544/1101/2210for the fine-grained classification subtask (thereare fewer examples for the binary subtask since1In the subsequent experiments, we found that optimizingthis objective yielded better performance than a mean squarederror objective.2For our Bidirectional LSTMs, the parameters of the for-ward and backward transition functions are shared.
In ourexperiments, this achieved superior performance to Bidirec-tional LSTMs with untied weights and the same number ofparameters (and therefore smaller hidden vector dimension-ality).1560Relatedness SentimentLSTM Variant d |?| d |?|Standard 150 203,400 168 315,840Bidirectional 150 203,400 168 315,8402-layer 108 203,472 120 318,720Bidirectional 2-layer 108 203,472 120 318,720Constituency Tree 142 205,190 150 316,800Dependency Tree 150 203,400 168 315,840Table 1: Memory dimensions d and compositionfunction parameter counts |?| for each LSTM vari-ant that we evaluate.neutral sentences are excluded).
Standard bina-rized constituency parse trees are provided foreach sentence in the dataset, and each node inthese trees is annotated with a sentiment label.For the sequential LSTM baselines, we predictthe sentiment of a phrase using the representationgiven by the final LSTM hidden state.
The sequen-tial LSTM models are trained on the spans corre-sponding to labeled nodes in the training set.We use the classification model described inSec.
4.1 with both Dependency Tree-LSTMs(Sec.
3.1) and Constituency Tree-LSTMs(Sec.
3.2).
The Constituency Tree-LSTMs arestructured according to the provided parse trees.For the Dependency Tree-LSTMs, we producedependency parses3of each sentence; each nodein a tree is given a sentiment label if its spanmatches a labeled span in the training set.5.2 Semantic RelatednessFor a given pair of sentences, the semantic relat-edness task is to predict a human-generated ratingof the similarity of the two sentences in meaning.We use the Sentences Involving Composi-tional Knowledge (SICK) dataset (Marelli et al,2014), consisting of 9927 sentence pairs in a4500/500/4927 train/dev/test split.
The sentencesare derived from existing image and video descrip-tion datasets.
Each sentence pair is annotated witha relatedness score y ?
[1, 5], with 1 indicatingthat the two sentences are completely unrelated,and 5 indicating that the two sentences are veryrelated.
Each label is the average of 10 ratings as-signed by different human annotators.Here, we use the similarity model described inSec.
4.2.
For the similarity prediction network(Eqs.
15) we use a hidden layer of size 50.
We3Dependency parses produced by the Stanford NeuralNetwork Dependency Parser (Chen and Manning, 2014).Method Fine-grained BinaryRAE (Socher et al, 2013) 43.2 82.4MV-RNN (Socher et al, 2013) 44.4 82.9RNTN (Socher et al, 2013) 45.7 85.4DCNN (Blunsom et al, 2014) 48.5 86.8Paragraph-Vec (Le and Mikolov, 2014) 48.7 87.8CNN-non-static (Kim, 2014) 48.0 87.2CNN-multichannel (Kim, 2014) 47.4 88.1DRNN (Irsoy and Cardie, 2014) 49.8 86.6LSTM 46.4 (1.1) 84.9 (0.6)Bidirectional LSTM 49.1 (1.0) 87.5 (0.5)2-layer LSTM 46.0 (1.3) 86.3 (0.6)2-layer Bidirectional LSTM 48.5 (1.0) 87.2 (1.0)Dependency Tree-LSTM 48.4 (0.4) 85.7 (0.4)Constituency Tree-LSTM?
randomly initialized vectors 43.9 (0.6) 82.0 (0.5)?
Glove vectors, fixed 49.7 (0.4) 87.5 (0.8)?
Glove vectors, tuned 51.0 (0.5) 88.0 (0.3)Table 2: Test set accuracies on the Stanford Sen-timent Treebank.
For our experiments, we reportmean accuracies over 5 runs (standard deviationsin parentheses).
Fine-grained: 5-class sentimentclassification.
Binary: positive/negative senti-ment classification.produce binarized constituency parses4and depen-dency parses of the sentences in the dataset for ourConstituency Tree-LSTM and Dependency Tree-LSTM models.5.3 Hyperparameters and Training DetailsThe hyperparameters for our models were tunedon the development set for each task.We initialized our word representations usingpublicly available 300-dimensional Glove vec-tors5(Pennington et al, 2014).
For the sentimentclassification task, word representations were up-dated during training with a learning rate of 0.1.For the semantic relatedness task, word represen-tations were held fixed as we did not observe anysignificant improvement when the representationswere tuned.Our models were trained using AdaGrad (Duchiet al, 2011) with a learning rate of 0.05 and aminibatch size of 25.
The model parameters wereregularized with a per-minibatch L2 regularizationstrength of 10?4.
The sentiment classifier wasadditionally regularized using dropout (Srivastavaet al, 2014) with a dropout rate of 0.5.
We did notobserve performance gains using dropout on thesemantic relatedness task.4Constituency parses produced by the Stanford PCFGParser (Klein and Manning, 2003).5Trained on 840 billion tokens of Common Crawl data,http://nlp.stanford.edu/projects/glove/.1561Method Pearson?s r Spearman?s ?
MSEIllinois-LH (Lai and Hockenmaier, 2014) 0.7993 0.7538 0.3692UNAL-NLP (Jimenez et al, 2014) 0.8070 0.7489 0.3550Meaning Factory (Bjerva et al, 2014) 0.8268 0.7721 0.3224ECNU (Zhao et al, 2014) 0.8414 ?
?Mean vectors 0.7577 (0.0013) 0.6738 (0.0027) 0.4557 (0.0090)DT-RNN (Socher et al, 2014) 0.7923 (0.0070) 0.7319 (0.0071) 0.3822 (0.0137)SDT-RNN (Socher et al, 2014) 0.7900 (0.0042) 0.7304 (0.0076) 0.3848 (0.0074)LSTM 0.8528 (0.0031) 0.7911 (0.0059) 0.2831 (0.0092)Bidirectional LSTM 0.8567 (0.0028) 0.7966 (0.0053) 0.2736 (0.0063)2-layer LSTM 0.8515 (0.0066) 0.7896 (0.0088) 0.2838 (0.0150)2-layer Bidirectional LSTM 0.8558 (0.0014) 0.7965 (0.0018) 0.2762 (0.0020)Constituency Tree-LSTM 0.8582 (0.0038) 0.7966 (0.0053) 0.2734 (0.0108)Dependency Tree-LSTM 0.8676 (0.0030) 0.8083 (0.0042) 0.2532 (0.0052)Table 3: Test set results on the SICK semantic relatedness subtask.
For our experiments, we report meanscores over 5 runs (standard deviations in parentheses).
Results are grouped as follows: (1) SemEval2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs.6 Results6.1 Sentiment ClassificationOur results are summarized in Table 2.
The Con-stituency Tree-LSTM outperforms existing sys-tems on the fine-grained classification subtask andachieves accuracy comparable to the state-of-the-art on the binary subtask.
In particular, we find thatit outperforms the Dependency Tree-LSTM.
Thisperformance gap is at least partially attributable tothe fact that the Dependency Tree-LSTM is trainedon less data: about 150K labeled nodes vs. 319Kfor the Constituency Tree-LSTM.
This differenceis due to (1) the dependency representations con-taining fewer nodes than the corresponding con-stituency representations, and (2) the inability tomatch about 9% of the dependency nodes to a cor-responding span in the training data.We found that updating the word representa-tions during training (?fine-tuning?
the word em-bedding) yields a significant boost in performanceon the fine-grained classification subtask and givesa minor gain on the binary classification subtask(this finding is consistent with previous work onthis task by Kim (2014)).
These gains are to beexpected since the Glove vectors used to initial-ize our word representations were not originallytrained to capture sentiment.6.2 Semantic RelatednessOur results are summarized in Table 3.
FollowingMarelli et al (2014), we use Pearson?s r, Spear-man?s ?
and mean squared error (MSE) as evalua-tion metrics.
The first two metrics are measures ofcorrelation against human evaluations of semanticrelatedness.We compare our models against a number ofnon-LSTM baselines.
The mean vector baselinecomputes sentence representations as a mean ofthe representations of the constituent words.
TheDT-RNN and SDT-RNN models (Socher et al,2014) both compose vector representations for thenodes in a dependency tree as a sum over affine-transformed child vectors, followed by a nonlin-earity.
The SDT-RNN is an extension of the DT-RNN that uses a separate transformation for eachdependency relation.
For each of our baselines,including the LSTM models, we use the similaritymodel described in Sec.
4.2.We also compare against four of the top-performing systems6submitted to the SemEval2014 semantic relatedness shared task: ECNU(Zhao et al, 2014), The Meaning Factory (Bjervaet al, 2014), UNAL-NLP (Jimenez et al, 2014),and Illinois-LH (Lai and Hockenmaier, 2014).These systems are heavily feature engineered,generally using a combination of surface formoverlap features and lexical distance features de-rived from WordNet or the Paraphrase Database(Ganitkevitch et al, 2013).Our LSTM models outperform all these sys-6We list the strongest results we were able to find for thistask; in some cases, these results are stronger than the officialperformance by the team on the shared task.
For example,the listed result by Zhao et al (2014) is stronger than theirsubmitted system?s Pearson correlation score of 0.8280.15620 5 10 15 20 25 30 35 40 45sentence length0.300.350.400.450.500.550.600.650.70accuracyDT-LSTMCT-LSTMLSTMBi-LSTMFigure 3: Fine-grained sentiment classification ac-curacy vs. sentence length.
For each `, we plotaccuracy for the test set sentences with length inthe window [` ?
2, ` + 2].
Examples in the tailof the length distribution are batched in the finalwindow (` = 45).tems without any additional feature engineering,with the best results achieved by the DependencyTree-LSTM.
Recall that in this task, both Tree-LSTM models only receive supervision at the rootof the tree, in contrast to the sentiment classifi-cation task where supervision was also providedat the intermediate nodes.
We conjecture that inthis setting, the Dependency Tree-LSTM benefitsfrom its more compact structure relative to theConstituency Tree-LSTM, in the sense that pathsfrom input word vectors to the root of the treeare shorter on aggregate for the Dependency Tree-LSTM.7 Discussion and Qualitative Analysis7.1 Modeling Semantic RelatednessIn Table 4, we list nearest-neighbor sentences re-trieved from a 1000-sentence sample of the SICKtest set.
We compare the neighbors ranked by theDependency Tree-LSTM model against a baselineranking by cosine similarity of the mean word vec-tors for each sentence.The Dependency Tree-LSTM model exhibitsseveral desirable properties.
Note that in the de-pendency parse of the second query sentence, theword ?ocean?
is the second-furthest word from theroot (?waving?
), with a depth of 4.
Regardless, theretrieved sentences are all semantically related tothe word ?ocean?, which indicates that the Tree-LSTM is able to both preserve and emphasize in-formation from relatively distant nodes.
Addi-tionally, the Tree-LSTM model shows greater ro-4 6 8 10 12 14 16 18 20mean sentence length0.780.800.820.840.860.880.90rDT-LSTMCT-LSTMLSTMBi-LSTMFigure 4: Pearson correlations r between pre-dicted similarities and gold ratings vs. sentencelength.
For each `, we plot r for the pairs withmean length in the window [`?2, `+2].
Examplesin the tail of the length distribution are batched inthe final window (` = 18.5).bustness to differences in sentence length.
Giventhe query ?two men are playing guitar?, the Tree-LSTM associates the phrase ?playing guitar?
withthe longer, related phrase ?dancing and singing infront of a crowd?
(note as well that there is zerotoken overlap between the two phrases).7.2 Effect of Sentence LengthOne hypothesis to explain the empirical strengthof Tree-LSTMs is that tree structures help miti-gate the problem of preserving state over long se-quences of words.
If this were true, we would ex-pect to see the greatest improvement over sequen-tial LSTMs on longer sentences.
In Figs.
3 and 4,we show the relationship between sentence lengthand performance as measured by the relevant task-specific metric.
Each data point is a mean scoreover 5 runs, and error bars have been omitted forclarity.We observe that while the Dependency Tree-LSTM does significantly outperform its sequen-tial counterparts on the relatedness task forlonger sentences of length 13 to 15 (Fig.
4), italso achieves consistently strong performance onshorter sentences.
This suggests that unlike se-quential LSTMs, Tree-LSTMs are able to encodesemantically-useful structural information in thesentence representations that they compose.8 Related WorkDistributed representations of words (Rumelhartet al, 1988; Collobert et al, 2011; Turian et al,2010; Huang et al, 2012; Mikolov et al, 2013;1563Ranking by mean word vector cosine similarity Scorea woman is slicing potatoesa woman is cutting potatoes 0.96a woman is slicing herbs 0.92a woman is slicing tofu 0.92a boy is waving at some young runners from the oceana man and a boy are standing at the bottom of some stairs , 0.92which are outdoorsa group of children in uniforms is standing at a gate and 0.90one is kissing the mothera group of children in uniforms is standing at a gate and 0.90there is no one kissing the mothertwo men are playing guitarsome men are playing rugby 0.88two men are talking 0.87two dogs are playing with each other 0.87Ranking by Dependency Tree-LSTM model Scorea woman is slicing potatoesa woman is cutting potatoes 4.82potatoes are being sliced by a woman 4.70tofu is being sliced by a woman 4.39a boy is waving at some young runners from the oceana group of men is playing with a ball on the beach 3.79a young boy wearing a red swimsuit is jumping out of a 3.37blue kiddies poolthe man is tossing a kid into the swimming pool that is 3.19near the oceantwo men are playing guitarthe man is singing and playing the guitar 4.08the man is opening the guitar for donations and plays 4.01with the casetwo men are dancing and singing in front of a crowd 4.00Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set.
The Tree-LSTM model is able to pick up on more subtle relationships, such as that between ?beach?
and ?ocean?in the second example.Pennington et al, 2014) have found wide appli-cability in a variety of NLP tasks.
Followingthis success, there has been substantial interest inthe area of learning distributed phrase and sen-tence representations (Mitchell and Lapata, 2010;Yessenalina and Cardie, 2011; Grefenstette et al,2013; Mikolov et al, 2013), as well as distributedrepresentations of longer bodies of text such asparagraphs and documents (Srivastava et al, 2013;Le and Mikolov, 2014).Our approach builds on recursive neural net-works (Goller and Kuchler, 1996; Socher et al,2011), which we abbreviate as Tree-RNNs in or-der to avoid confusion with recurrent neural net-works.
Under the Tree-RNN framework, the vec-tor representation associated with each node ofa tree is composed as a function of the vectorscorresponding to the children of the node.
Thechoice of composition function gives rise to nu-merous variants of this basic framework.
Tree-RNNs have been used to parse images of natu-ral scenes (Socher et al, 2011), compose phraserepresentations from word vectors (Socher et al,2012), and classify the sentiment polarity of sen-tences (Socher et al, 2013).9 ConclusionIn this paper, we introduced a generalization ofLSTMs to tree-structured network topologies.
TheTree-LSTM architecture can be applied to treeswith arbitrary branching factor.
We demonstratedthe effectiveness of the Tree-LSTM by applyingthe architecture in two tasks: semantic relatednessand sentiment classification, outperforming exist-ing systems on both.
Controlling for model di-mensionality, we demonstrated that Tree-LSTMmodels are able to outperform their sequentialcounterparts.
Our results suggest further lines ofwork in characterizing the role of structure in pro-ducing distributed representations of sentences.AcknowledgementsWe thank our anonymous reviewers for their valu-able feedback.
Stanford University gratefully ac-knowledges the support of a Natural LanguageUnderstanding-focused gift from Google Inc. andthe Defense Advanced Research Projects Agency(DARPA) Deep Exploration and Filtering of Text(DEFT) Program under Air Force Research Lab-oratory (AFRL) contract no.
FA8750-13-2-0040.Any opinions, findings, and conclusion or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewof the DARPA, AFRL, or the US government.ReferencesBahdanau, Dzmitry, Kyunghyun Cho, and YoshuaBengio.
2015.
Neural machine translation byjointly learning to align and translate.
In Pro-ceedings of the 3rd International Conference onLearning Representations (ICLR 2015).Bengio, Yoshua, Patrice Simard, and Paolo Fras-coni.
1994.
Learning long-term dependencieswith gradient descent is difficult.
IEEE Trans-actions on Neural Networks 5(2).1564Bjerva, Johannes, Johan Bos, Rob van der Goot,and Malvina Nissim.
2014.
The Meaning Fac-tory: Formal semantics for recognizing textualentailment and determining semantic similarity.In Proceedings of the 8th International Work-shop on Semantic Evaluation (SemEval 2014).Blunsom, Phil, Edward Grefenstette, Nal Kalch-brenner, et al 2014.
A convolutional neural net-work for modelling sentences.
In Proceedingsof the 52nd Annual Meeting of the Associationfor Computational Linguistics.Chen, Danqi and Christopher D Manning.
2014.
Afast and accurate dependency parser using neu-ral networks.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Collobert, Ronan, Jason Weston, L?eon Bottou,Michael Karlen, Koray Kavukcuoglu, and PavelKuksa.
2011.
Natural language processing (al-most) from scratch.
The Journal of MachineLearning Research 12:2493?2537.Duchi, John, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learn-ing and stochastic optimization.
The Journal ofMachine Learning Research 12:2121?2159.Elman, Jeffrey L. 1990.
Finding structure in time.Cognitive science 14(2):179?211.Foltz, Peter W, Walter Kintsch, and Thomas KLandauer.
1998.
The measurement of textualcoherence with latent semantic analysis.
Dis-course processes 25(2-3):285?307.Ganitkevitch, Juri, Benjamin Van Durme, andChris Callison-Burch.
2013.
PPDB: The Para-phrase Database.
In Proceedings of HLT-NAACL 2013.Goller, Christoph and Andreas Kuchler.
1996.Learning task-dependent distributed representa-tions by backpropagation through structure.
InIEEE International Conference on Neural Net-works.Graves, Alex, Navdeep Jaitly, and A-R Mohamed.2013.
Hybrid speech recognition with deepbidirectional LSTM.
In IEEE Workshop on Au-tomatic Speech Recognition and Understanding(ASRU).Grefenstette, Edward, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, andMarco Baroni.
2013.
Multi-step regressionlearning for compositional distributional se-mantics.
In Proceedings of the 10th Interna-tional Conference on Computational Semantics.Hochreiter, Sepp.
1998.
The vanishing gradientproblem during learning recurrent neural netsand problem solutions.
International Journal ofUncertainty, Fuzziness and Knowledge-BasedSystems 6(02):107?116.Hochreiter, Sepp and J?urgen Schmidhuber.
1997.Long Short-Term Memory.
Neural Computa-tion 9(8).Huang, Eric H., Richard Socher, Christopher D.Manning, and Andrew Y. Ng.
2012.
Improv-ing word representations via global context andmultiple word prototypes.
In Annual Meetingof the Association for Computational Linguis-tics (ACL).Irsoy, Ozan and Claire Cardie.
2014.
Deep re-cursive neural networks for compositionality inlanguage.
In Advances in Neural InformationProcessing Systems.Jimenez, Sergio, George Duenas, Julia Baquero,Alexander Gelbukh, Av Juan Dios B?atiz, andAv Mendiz?abal.
2014.
UNAL-NLP: Combin-ing soft cardinality features for semantic textualsimilarity, relatedness and entailment.
In Pro-ceedings of the 8th International Workshop onSemantic Evaluation (SemEval 2014).Kim, Yoon.
2014.
Convolutional neural networksfor sentence classification.
In Proceedings ofthe 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP).Klein, Dan and Christopher D Manning.
2003.Accurate unlexicalized parsing.
In Proceedingsof the 41st Annual Meeting on Association forComputational Linguistics.Lai, Alice and Julia Hockenmaier.
2014.
Illinois-LH: A denotational and distributional approachto semantics.
In Proceedings of the 8th Inter-national Workshop on Semantic Evaluation (Se-mEval 2014).Landauer, Thomas K and Susan T Dumais.
1997.A solution to Plato?s problem: The latent se-mantic analysis theory of acquisition, induction,and representation of knowledge.
Psychologicalreview 104(2):211.Le, Quoc and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In1565Proceedings of the 31st International Confer-ence on Machine Learning (ICML-14).Marelli, Marco, Luisa Bentivogli, Marco Ba-roni, Raffaella Bernardi, Stefano Menini, andRoberto Zamparelli.
2014.
SemEval-2014 Task1: Evaluation of compositional distributionalsemantic models on full sentences through se-mantic relatedness and textual entailment.
InProceedings of the 8th International Workshopon Semantic Evaluation (SemEval 2014).Mikolov, Tom?a?s.
2012.
Statistical Language Mod-els Based on Neural Networks.
Ph.D. thesis,Brno University of Technology.Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg SCorrado, and Jeff Dean.
2013.
Distributedrepresentations of words and phrases and theircompositionality.
In Advances in Neural Infor-mation Processing Systems.Mitchell, Jeff and Mirella Lapata.
2010.
Composi-tion in distributional models of semantics.
Cog-nitive science 34(8):1388?1429.Pennington, Jeffrey, Richard Socher, and Christo-pher D Manning.
2014.
Glove: Global vectorsfor word representation.
In Proceedings of the2014 Conference on Empiricial Methods in Nat-ural Language Processing (EMNLP).Rumelhart, David E, Geoffrey E Hinton, andRonald J Williams.
1988.
Learning represen-tations by back-propagating errors.
Cognitivemodeling 5.Socher, Richard, Brody Huval, Christopher DManning, and Andrew Y Ng.
2012.
Seman-tic compositionality through recursive matrix-vector spaces.
In Proceedings of the 2012 JointConference on Empirical Methods in NaturalLanguage Processing and Computational Nat-ural Language Learning (EMNLP).Socher, Richard, Andrej Karpathy, Quoc V Le,Christopher D Manning, and Andrew Y Ng.2014.
Grounded compositional semantics forfinding and describing images with sentences.Transactions of the Association for Computa-tional Linguistics 2.Socher, Richard, Cliff C Lin, Chris Manning, andAndrew Y Ng.
2011.
Parsing natural scenesand natural language with recursive neural net-works.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11).Socher, Richard, Alex Perelygin, Jean Y Wu,Jason Chuang, Christopher D Manning, An-drew Y Ng, and Christopher Potts.
2013.
Re-cursive deep models for semantic composition-ality over a sentiment treebank.
In Proceedingsof the 2013 Conference on Empirical Methodsin Natural Language Processing (EMNLP).Srivastava, Nitish, Geoffrey Hinton, AlexKrizhevsky, Ilya Sutskever, and RuslanSalakhutdinov.
2014.
Dropout: A simpleway to prevent neural networks from overfit-ting.
Journal of Machine Learning Research15:1929?1958.Srivastava, Nitish, Ruslan Salakhutdinov, and Ge-offrey Hinton.
2013.
Modeling documents witha Deep Boltzmann Machine.
In Uncertainty inArtificial Intelligence.Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le.2014.
Sequence to sequence learning with neu-ral networks.
In Advances in Neural Informa-tion Processing Systems.Turian, Joseph, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and gen-eral method for semi-supervised learning.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics.Vinyals, Oriol, Alexander Toshev, Samy Bengio,and Dumitru Erhan.
2014.
Show and tell: Aneural image caption generator.
arXiv preprintarXiv:1411.4555 .Yessenalina, Ainur and Claire Cardie.
2011.
Com-positional matrix-space models for sentimentanalysis.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Lan-guage Processing (EMNLP).Zaremba, Wojciech and Ilya Sutskever.2014.
Learning to execute.
arXiv preprintarXiv:1410.4615 .Zhao, Jiang, Tian Tian Zhu, and Man Lan.
2014.ECNU: One stone two birds: Ensemble of het-erogenous measures for semantic relatednessand textual entailment.
In Proceedings of the8th International Workshop on Semantic Evalu-ation (SemEval 2014).1566
