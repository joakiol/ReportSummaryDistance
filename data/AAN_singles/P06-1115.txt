Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913?920,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing String-Kernels for Learning Semantic ParsersRohit J. KateDepartment of Computer SciencesThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233, USArjkate@cs.utexas.eduRaymond J. MooneyDepartment of Computer SciencesThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233, USAmooney@cs.utexas.eduAbstractWe present a new approach for mappingnatural language sentences to their for-mal meaning representations using string-kernel-based classifiers.
Our system learnsthese classifiers for every production in theformal language grammar.
Meaning repre-sentations for novel natural language sen-tences are obtained by finding the mostprobable semantic parse using these stringclassifiers.
Our experiments on two real-world data sets show that this approachcompares favorably to other existing sys-tems and is particularly robust to noise.1 IntroductionComputational systems that learn to transform nat-ural language sentences into formal meaning rep-resentations have important practical applicationsin enabling user-friendly natural language com-munication with computers.
However, most of theresearch in natural language processing (NLP) hasbeen focused on lower-level tasks like syntacticparsing, word-sense disambiguation, informationextraction etc.
In this paper, we have consideredthe important task of doing deep semantic parsingto map sentences into their computer-executablemeaning representations.Previous work on learning semantic parserseither employ rule-based algorithms (Tang andMooney, 2001; Kate et al, 2005), or use sta-tistical feature-based methods (Ge and Mooney,2005; Zettlemoyer and Collins, 2005; Wong andMooney, 2006).
In this paper, we present anovel kernel-based statistical method for learn-ing semantic parsers.
Kernel methods (Cristianiniand Shawe-Taylor, 2000) are particularly suitablefor semantic parsing because it involves mappingphrases of natural language (NL) sentences to se-mantic concepts in a meaning representation lan-guage (MRL).
Given that natural languages are soflexible, there are various ways in which one canexpress the same semantic concept.
It is difficultfor rule-based methods or even statistical feature-based methods to capture the full range of NL con-texts which map to a semantic concept becausethey tend to enumerate these contexts.
In contrast,kernel methods allow a convenient mechanism toimplicitly work with a potentially infinite numberof features which can robustly capture these rangeof contexts even when the data is noisy.Our system, KRISP (Kernel-based Robust In-terpretation for Semantic Parsing), takes NL sen-tences paired with their formal meaning represen-tations as training data.
The productions of the for-mal MRL grammar are treated like semantic con-cepts.
For each of these productions, a Support-Vector Machine (SVM) (Cristianini and Shawe-Taylor, 2000) classifier is trained using string sim-ilarity as the kernel (Lodhi et al, 2002).
Eachclassifier then estimates the probability of the pro-duction covering different substrings of the sen-tence.
This information is used to compositionallybuild a complete meaning representation (MR) ofthe sentence.Some of the previous work on semantic pars-ing has focused on fairly simple domains, primar-ily, ATIS (Air Travel Information Service) (Price,1990) whose semantic analysis is equivalent to fill-ing a single semantic frame (Miller et al, 1996;Popescu et al, 2004).
In this paper, we havetested KRISP on two real-world domains in whichmeaning representations are more complex withricher predicates and nested structures.
Our exper-iments demonstrate that KRISP compares favor-913NL: ?If the ball is in our goal area then our player 1 shouldintercept it.
?CLANG: ((bpos (goal-area our))(do our {1} intercept))Figure 1: An example of an NL advice and itsCLANG MR.ably to other existing systems and is particularlyrobust to noise.2 Semantic ParsingWe call the process of mapping natural language(NL) utterances into their computer-executablemeaning representations (MRs) as semantic pars-ing.
These MRs are expressed in formal languageswhich we call meaning representation languages(MRLs).
We assume that all MRLs have deter-ministic context free grammars, which is true foralmost all computer languages.
This ensures thatevery MR will have a unique parse tree.
A learn-ing system for semantic parsing is given a trainingcorpus of NL sentences paired with their respec-tive MRs from which it has to induce a semanticparser which can map novel NL sentences to theircorrect MRs.Figure 1 shows an example of an NL sentenceand its MR from the CLANG domain.
CLANG(Chen et al, 2003) is the standard formal coachlanguage in which coaching advice is given to soc-cer agents which compete on a simulated soccerfield in the RoboCup 1 Coach Competition.
In theMR of the example, bpos stands for ?ball posi-tion?.The second domain we have considered is theGEOQUERY domain (Zelle and Mooney, 1996)which is a query language for a small database ofabout 800 U.S. geographical facts.
Figure 2 showsan NL query and its MR form in a functional querylanguage.
The parse of the functional query lan-guage is also shown along with the involved pro-ductions.
This example is also used later to illus-trate how our system does semantic parsing.
TheMR in the functional query language can be readas if processing a list which gets modified by vari-ous functions.
From the innermost expression go-ing outwards it means: the state of Texas, the listcontaining all the states next to the state of Texasand the list of all the rivers which flow throughthese states.
This list is finally returned as the an-swer.1http://www.robocup.org/NL: ?Which rivers run through the states bordering Texas?
?Functional query language:answer(traverse(next to(stateid(?texas?
))))Parse tree of the MR in functional query language:ANSWERanswer RIVERTRAVERSEtraverseSTATENEXT TOnext toSTATESTATEIDstateid ?texas?Productions:ANSWER ?
answer(RIVER) RIVER ?
TRAVERSE(STATE)STATE ?
NEXT TO(STATE) STATE ?
STATEIDTRAVERSE ?
traverse NEXT TO ?
next toSTATEID ?
stateid(?texas?
)Figure 2: An example of an NL query and its MRin a functional query language with its parse tree.KRISP does semantic parsing using the notionof a semantic derivation of an NL sentence.
Inthe following subsections, we define the seman-tic derivation of an NL sentence and its probabil-ity.
The task of semantic parsing then is to findthe most probable semantic derivation of an NLsentence.
In section 3, we describe how KRISPlearns the string classifiers that are used to obtainthe probabilities needed in finding the most prob-able semantic derivation.2.1 Semantic DerivationWe define a semantic derivation, D, of an NL sen-tence, s, as a parse tree of an MR (not necessarilythe correct MR) such that each node of the parsetree also contains a substring of the sentence inaddition to a production.
We denote nodes of thederivation tree by tuples (pi, [i..j]), where pi is itsproduction and [i..j] stands for the substring s[i..j]of s (i.e.
the substring from the ith word to the jthword), and we say that the node or its productioncovers the substring s[i..j].
The substrings cov-ered by the children of a node are not allowed tooverlap, and the substring covered by the parentmust be the concatenation of the substrings cov-ered by its children.
Figure 3 shows a semanticderivation of the NL sentence and the MR parsewhich were shown in figure 2.
The words are num-bered according to their position in the sentence.Instead of non-terminals, productions are shownin the nodes to emphasize the role of productionsin semantic derivations.Sometimes, the children of an MR parse tree914(ANSWER?
answer(RIVER), [1..9])(RIVER?
TRAVERSE(STATE), [1..9])(TRAVERSE?traverse, [1..4])which1 rivers2 run3 through4(STATE?
NEXT TO(STATE), [5..9])(NEXT TO?
next to, [5..7])the5 states6 bordering7(STATE?
STATEID, [8..9])(STATEID?
stateid ?texas?, [8..9])Texas8 ?9Figure 3: Semantic derivation of the NL sentence ?Which rivers run through the states bordering Texas?
?which gives MR as answer(traverse(next to(stateid(texas)))).node may not be in the same order as are the sub-strings of the sentence they should cover in a se-mantic derivation.
For example, if the sentencewas ?Through the states that border Texas whichrivers run?
?, which has the same MR as the sen-tence in figure 3, then the order of the children ofthe node ?RIVER ?
TRAVERSE(STATE)?
wouldneed to be reversed.
To accommodate this, a se-mantic derivation tree is allowed to contain MRparse tree nodes in which the children have beenpermuted.Note that given a semantic derivation of an NLsentence, it is trivial to obtain the correspondingMR simply as the string generated by the parse.Since children nodes may be permuted, this stepalso needs to permute them back to the way theyshould be according to the MRL productions.
If asemantic derivation gives the correct MR of theNL sentence, then we call it a correct semanticderivation otherwise it is an incorrect semanticderivation.2.2 Most Probable Semantic DerivationLet Ppi(u) denote the probability that a productionpi of the MRL grammar covers the NL substringu.
In other words, the NL substring u expressesthe semantic concept of a production pi with prob-ability Ppi(u).
In the next subsection we will de-scribe how KRISP obtains these probabilities usingstring-kernel based SVM classifiers.
Assumingthese probabilities are independent of each other,the probability of a semantic derivationD of a sen-tence s is then:P (D) =?
(pi,[i..j])?DPpi(s[i..j])The task of the semantic parser is to find themost probable derivation of a sentence s. Thistask can be recursively performed using the no-tion of a partial derivation En,s[i..j], which standsfor a subtree of a semantic derivation tree with nas the left-hand-side (LHS) non-terminal of theroot production and which covers s from indexi to j.
For example, the subtree rooted at thenode ?
(STATE ?
NEXT TO(STATE),[5..9])?
inthe derivation shown in figure 3 is a partial deriva-tion which would be denoted as ESTATE,s[5..9].Note that the derivation D of sentence s is thensimply Estart,s[1..|s|], where start is the start sym-bol of the MRL?s context free grammar, G.Our procedure to find the most probable par-tial derivation E?n,s[i..j] considers all possible sub-trees whose root production has n as its LHS non-terminal and which cover s from index i to j.Mathematically, the most probable partial deriva-tion E?n,s[i..j] is recursively defined as:E?n,s[i..j] =makeTree( argmaxpi = n ?
n1..nt ?
G,(p1, .., pt) ?partition(s[i..j], t)(Ppi(s[i..j])?k=1..tP (E?nk,pk )))where partition(s[i..j], t) is a function which re-turns the set of all partitions of s[i..j] with t el-ements including their permutations.
A parti-tion of a substring s[i..j] with t elements is at?tuple containing t non-overlapping substringsof s[i..j] which give s[i..j] when concatenated.For example, (?the states bordering?, ?Texas ??
)is a partition of the substring ?the states bor-dering Texas ??
with 2 elements.
The proce-duremakeTree(pi, (p1, .., pt)) constructs a partialderivation tree by making pi as its root productionand making the most probable partial derivationtrees found through the recursion as children sub-trees which cover the substrings according to thepartition (p1, .., pt).The most probable partial derivation E?n,s[i..j]is found using the above equation by trying allproductions pi = n ?
n1..nt in G which have915n as the LHS, and all partitions with t elementsof the substring s[i..j] (n1 to nt are right-hand-side (RHS) non-terminals of pi, terminals do notplay any role in this process and are not shownfor simplicity).
The most probable partial deriva-tion E?STATE,s[5..9] for the sentence shown in fig-ure 3 will be found by trying all the productionsin the grammar with STATE as the LHS, for ex-ample, one of them being ?STATE ?
NEXT TOSTATE?.
Then for this sample production, all parti-tions, (p1, p2), of the substring s[5..9] with two el-ements will be considered, and the most probablederivations E?NEXT TO,p1 and E?STATE,p2 will befound recursively.
The recursion reaches basecases when the productions which have n on theLHS do not have any non-terminal on the RHS orwhen the substring s[i..j] becomes smaller thanthe length t.According to the equation, a production pi ?
Gand a partition (p1, .., pt) ?
partition(s[i..j], t)will be selected in constructing the most probablepartial derivation.
These will be the ones whichmaximize the product of the probability of pi cov-ering the substring s[i..j] with the product of prob-abilities of all the recursively found most proba-ble partial derivations consistent with the partition(p1, .., pt).A naive implementation of the above recursionis computationally expensive, but by suitably ex-tending the well known Earley?s context-free pars-ing algorithm (Earley, 1970), it can be imple-mented efficiently.
The above task has some re-semblance to probabilistic context-free grammar(PCFG) parsing for which efficient algorithms areavailable (Stolcke, 1995), but we note that our taskof finding the most probable semantic derivationdiffers from PCFG parsing in two important ways.First, the probability of a production is not inde-pendent of the sentence but depends on which sub-string of the sentence it covers, and second, theleaves of the tree are not individual terminals ofthe grammar but are substrings of words of the NLsentence.
The extensions needed for Earley?s al-gorithm are straightforward and are described indetail in (Kate, 2005) but due to space limitationwe do not describe them here.
Our extended Ear-ley?s algorithm does a beam search and attemptsto find the ?
(a parameter) most probable semanticderivations of an NL sentence s using the probabil-ities Ppi(s[i..j]).
To make this search faster, it usesa threshold, ?, to prune low probability derivationtrees.3 KRISP?s Training AlgorithmIn this section, we describe how KRISP learnsthe classifiers which give the probabilities Ppi(u)needed for semantic parsing as described in theprevious section.
Given the training corpus ofNL sentences paired with their MRs {(si,mi)|i =1..N}, KRISP first parses the MRs using the MRLgrammar, G. We represent the parse of MR, mi,by parse(mi).Figure 4 shows pseudo-code for KRISP?s train-ing algorithm.
KRISP learns a semantic parser it-eratively, each iteration improving upon the parserlearned in the previous iteration.
In each itera-tion, for every production pi of G, KRISP collectspositive and negative example sets.
In the firstiteration, the set P(pi) of positive examples forproduction pi contains all sentences, si, such thatparse(mi) uses the production pi.
The set of nega-tive examples,N (pi), for production pi includes allof the remaining training sentences.
Using thesepositive and negative examples, an SVM classi-fier 2, Cpi, is trained for each production pi usinga normalized string subsequence kernel.
Follow-ing the framework of Lodhi et al (2002), we de-fine a kernel between two strings as the number ofcommon subsequences they share.
One difference,however, is that their strings are over characterswhile our strings are over words.
The more thetwo strings share, the greater the similarity scorewill be.Normally, SVM classifiers only predict the classof the test example but one can obtain class proba-bility estimates by mapping the distance of the ex-ample from the SVM?s separating hyperplane tothe range [0,1] using a learned sigmoid function(Platt, 1999).
The classifier Cpi then gives us theprobabilities Ppi(u).
We represent the set of theseclassifiers by C = {Cpi|pi ?
G}.Next, using these classifiers, the extendedEarley?s algorithm, which we call EX-TENDED EARLEY in the pseudo-code, is invokedto obtain the ?
best semantic derivations for eachof the training sentences.
The procedure getMRreturns the MR for a semantic derivation.
At thispoint, for many training sentences, the resultingmost-probable semantic derivation may not givethe correct MR.
Hence, next, the system collectsmore refined positive and negative examplesto improve the result in the next iteration.
It2We use the LIBSVM package available at: http://www.csie.ntu.edu.tw/?cjlin/libsvm/916function TRAIN KRISP(training corpus {(si,mi)|i = 1..N}, MRL grammar G)for each pi ?G // collect positive and negative examples for the first iterationfor i = 1 to N doif pi is used in parse(mi) theninclude si in P(pi)else include si in N (pi)for iteration = 1 to MAX ITR dofor each pi ?G doCpi = trainSVM(P(pi),N (pi)) // SVM trainingfor each pi ?G P(pi) = ?
// empty the positive examples, accumulate negatives thoughfor i = 1 to N doD =EXTENDED EARLEY(si, G, P ) // obtain best derivationsif 6 ?
d ?
D such that parse(mi) = getMR(d) thenD = D ?
EXTENDED EARLEY CORRECT(si, G, P,mi) // if no correct derivation then force to find oned?
= argmaxd?D&getMR(d)=parse(mi) P (d)COLLECT POSITIVES(d?)
// collect positives from maximum probability correct derivationfor each d ?
D doif P (d) > P (d?)
and getMR(d) 6= parse(mi) then// collect negatives from incorrect derivation with larger probability than the correct oneCOLLECT NEGATIVES(d, d?
)return classifiers C = {Cpi|pi ?
G}Figure 4: KRISP?s training algorithmis also possible that for some sentences, noneof the obtained ?
derivations give the correctMR.
But as will be described shortly, the mostprobable derivation which gives the correct MR isneeded to collect positive and negative examplesfor the next iteration.
Hence in these cases, aversion of the extended Earley?s algorithm, EX-TENDED EARLEY CORRECT, is invoked whichalso takes the correct MR as an argument andreturns the best ?
derivations it finds, all ofwhich give the correct MR.
This is easily done bymaking sure all subtrees derived in the process arepresent in the parse of the correct MR.From these derivations, positive and negativeexamples are collected for the next iteration.
Pos-itive examples are collected from the most prob-able derivation which gives the correct MR, fig-ure 3 showed an example of a derivation whichgives the correct MR. At each node in such aderivation, the substring covered is taken as a pos-itive example for its production.
Negative exam-ples are collected from those derivations whoseprobability is higher than the most probable cor-rect derivation but which do not give the cor-rect MR.
Figure 5 shows an example of an in-correct derivation.
Here the function ?next to?is missing from the MR it produces.
The fol-lowing procedure is used to collect negative ex-amples from incorrect derivations.
The incorrectderivation and the most probable correct deriva-tion are traversed simultaneously starting from theroot using breadth-first traversal.
The first nodeswhere their productions differ is detected, and allof the words covered by the these nodes (in bothderivations) are marked.
In the correct and incor-rect derivations shown in figures 3 and 5 respec-tively, the first nodes where the productions differare ?
(STATE ?
NEXT TO(STATE), [5..9])?
and?
(STATE ?
STATEID, [8..9])?.
Hence, the unionof words covered by them: 5 to 9 (?the statesbordering Texas??
), will be marked.
For eachof these marked words, the procedure considersall of the productions which cover it in the twoderivations.
The nodes of the productions whichcover a marked word in the incorrect derivationbut not in the correct derivation are used to col-lect negative examples.
In the example, the node?(TRAVERSE?traverse,[1..7])?
will be usedto collect a negative example (i.e.
the words 1to 7 ?
?which rivers run through the states border-ing?
will be a negative example for the produc-tion TRAVERSE?traverse) because the pro-duction covers the marked words ?the?, ?states?and ?bordering?
in the incorrect derivation butnot in the correct derivation.
With this as a neg-ative example, hopefully in the next iteration, theprobability of this derivation will decrease signif-icantly and drop below the probability of the cor-rect derivation.In each iteration, the positive examples fromthe previous iteration are first removed so thatnew positive examples which lead to better cor-rect derivations can take their place.
However,negative examples are accumulated across iter-ations for better accuracy because negative ex-amples from each iteration only lead to incor-rect derivations and it is always good to includethem.
To further increase the number of nega-tive examples, every positive example for a pro-duction is also included as a negative example forall the other productions having the same LHS.After a specified number of MAX ITR iterations,917(ANSWER?
answer(RIVER), [1..9])(RIVER?
TRAVERSE(STATE), [1..9])(TRAVERSE?traverse, [1..7])Which1 rivers2 run3 through4 the5 states6 bordering7(STATE?
STATEID, [8..9])(STATEID?
stateid texas, [8..9])Texas8 ?9Figure 5: An incorrect semantic derivation of the NL sentence ?Which rivers run through the statesbordering Texas??
which gives the incorrect MR answer(traverse(stateid(texas))).the trained classifiers from the last iteration arereturned.
Testing involves using these classifiersto generate the most probable derivation of a testsentence as described in the subsection 2.2, andreturning its MR.The MRL grammar may contain productionscorresponding to constants of the domain, for e.g.,state names like ?STATEID ?
?texas?
?, or rivernames like ?RIVERID ?
?colorado??
etc.
Oursystem allows the user to specify such produc-tions as constant productions giving the NL sub-strings, called constant substrings, which directlyrelate to them.
For example, the user may give?Texas?
as the constant substring for the produc-tion ?STATEID ?
?texas?.
Then KRISP doesnot learn classifiers for these constant productionsand instead decides if they cover a substring of thesentence or not by matching it with the providedconstant substrings.4 Experiments4.1 MethodologyKRISP was evaluated on CLANG and GEOQUERYdomains as described in section 2.
The CLANGcorpus was built by randomly selecting 300 piecesof coaching advice from the log files of the 2003RoboCup Coach Competition.
These formal ad-vice instructions were manually translated intoEnglish (Kate et al, 2005).
The GEOQUERY cor-pus contains 880 English queries collected fromundergraduates and from real users of a web-basedinterface (Tang and Mooney, 2001).
These weremanually translated into their MRs.
The averagelength of an NL sentence in the CLANG corpusis 22.52 words while in the GEOQUERY corpus itis 7.48 words, which indicates that CLANG is theharder corpus.
The average length of the MRs is13.42 tokens in the CLANG corpus while it is 6.46tokens in the GEOQUERY corpus.KRISP was evaluated using standard 10-foldcross validation.
For every test sentence, only thebest MR corresponding to the most probable se-mantic derivation is considered for evaluation, andits probability is taken as the system?s confidencein that MR.
Since KRISP uses a threshold, ?, toprune low probability derivation trees, it some-times may fail to return any MR for a test sen-tence.
We computed the number of test sentencesfor which KRISP produced MRs, and the numberof these MRs that were correct.
For CLANG, anoutput MR is considered correct if and only if itexactly matches the correct MR. For GEOQUERY,an output MR is considered correct if and only ifthe resulting query retrieves the same answer asthe correct MR when submitted to the database.Performance was measured in terms of precision(the percentage of generated MRs that were cor-rect) and recall (the percentage of all sentences forwhich correct MRs were obtained).In our experiments, the threshold ?
was fixedto 0.05 and the beam size ?
was 20.
These pa-rameters were found through pilot experiments.The maximum number of iterations (MAX ITR) re-quired was only 3, beyond this we found that thesystem only overfits the training corpus.We compared our system?s performance withthe following existing systems: the string and treeversions of SILT (Kate et al, 2005), a system thatlearns transformation rules relating NL phrasesto MRL expressions; WASP (Wong and Mooney,2006), a system that learns transformation rulesusing statistical machine translation techniques;SCISSOR (Ge and Mooney, 2005), a system thatlearns an integrated syntactic-semantic parser; andCHILL (Tang and Mooney, 2001) an ILP-basedsemantic parser.
We also compared with theCCG-based semantic parser by Zettlemoyer et al(2005), but their results are available only for theGEO880 corpus and their experimental set-up isalso different from ours.
Like KRISP, WASP andSCISSOR also give confidences to the MRs theygenerate which are used to plot precision-recallcurves by measuring precisions and recalls at vari-91850607080901000  10  20  30  40  50  60  70  80  90  100PrecisionRecallKRISPWASPSCISSORSILT-treeSILT-stringFigure 6: Results on the CLANG corpus.50607080901000  10  20  30  40  50  60  70  80  90  100PrecisionRecallKRISPWASPSCISSORSILT-treeSILT-stringCHILLZettlemoyer et al (2005)Figure 7: Results on the GEOQUERY corpus.ous confidence levels.
The results of the other sys-tems are shown as points on the precision-recallgraph.4.2 ResultsFigure 6 shows the results on the CLANG cor-pus.
KRISP performs better than either versionof SILT and performs comparable to WASP.
Al-though SCISSOR gives less precision at lower re-call values, it gives much higher maximum recall.However, we note that SCISSOR requires more su-pervision for the training corpus in the form of se-mantically annotated syntactic parse trees for thetraining sentences.
CHILL could not be run be-yond 160 training examples because its Prolog im-plementation runs out of memory.
For 160 trainingexamples it gave 49.2% precision with 12.67% re-call.Figure 7 shows the results on the GEOQUERYcorpus.
KRISP achieves higher precisions thanWASP on this corpus.
Overall, the results showthat KRISP performs better than deterministicrule-based semantic parsers like CHILL and SILTand performs comparable to other statistical se-mantic parsers like WASP and SCISSOR.4.3 Experiments with Other NaturalLanguagesWe have translations of a subset of the GEOQUERYcorpus with 250 examples (GEO250 corpus) in50607080901000  10  20  30  40  50  60  70  80  90  100PrecisionRecallEnglishJapaneseSpanishTurkishFigure 8: Results of KRISP on the GEO250 corpusfor different natural languages.three other natural languages: Spanish, Turkishand Japanese.
Since KRISP?s learning algorithmdoes not use any natural language specific knowl-edge, it is directly applicable to other natural lan-guages.
Figure 8 shows that KRISP performs com-petently on other languages as well.4.4 Experiments with Noisy NL SentencesAny real world application in which semanticparsers would be used to interpret natural languageof a user is likely to face noise in the input.
If theuser is interacting through spontaneous speech andthe input to the semantic parser is coming formthe output of a speech recognition system thenthere are many ways in which noise could creepin the NL sentences: interjections (like um?s andah?s), environment noise (like door slams, phonerings etc.
), out-of-domain words, grammaticallyill-formed utterances etc.
(Zue and Glass, 2000).As opposed to the other systems, KRISP?s string-kernel-based semantic parsing does not use hard-matching rules and should be thus more flexibleand robust to noise.
We tested this hypothesis byrunning experiments on data which was artificiallycorrupted with simulated speech recognition er-rors.The interjections, environment noise etc.
arelikely to be recognized as real words by a speechrecognizer.
To simulate this, after every word ina sentence, with some probability Padd, an ex-tra word is added which is chosen with proba-bility proportional to its word frequency found inthe British National Corpus (BNC), a good rep-resentative sample of English.
A speech recog-nizer may sometimes completely fail to detectwords, so with a probability of Pdrop a word issometimes dropped.
A speech recognizer couldalso introduce noise by confusing a word with ahigh frequency phonetically close word.
We sim-9190204060801000  1  2  3  4  5F-measureNoise levelKRISPWASPSCISSORFigure 9: Results on the CLANG corpus with in-creasing amounts of noise in the test sentences.ulate this type of noise by substituting a word inthe corpus by another word, w, with probabilityped(w)?P (w), where p is a parameter, ed(w) isw?sedit distance (Levenshtein, 1966) from the originalword and P (w) is w?s probability proportional toits word frequency.
The edit distance which calcu-lates closeness between words is character-basedrather than based on phonetics, but this should notmake a significant difference in the experimentalresults.Figure 9 shows the results on the CLANG cor-pus with increasing amounts of noise, from level0 to level 4.
The noise level 0 corresponds to nonoise.
The noise parameters, Padd and Pdrop, werevaried uniformly from being 0 at level 0 and 0.1 atlevel 4, and the parameter p was varied uniformlyfrom being 0 at level 0 and 0.01 at level 4.
Weare showing the best F-measure (harmonic meanof precision and recall) for each system at differ-ent noise levels.
As can be seen, KRISP?s perfor-mance degrades gracefully in the presence of noisewhile other systems?
performance degrade muchfaster, thus verifying our hypothesis.
In this exper-iment, only the test sentences were corrupted, weget qualitatively similar results when both trainingand test sentences are corrupted.
The results arealso similar on the GEOQUERY corpus.5 ConclusionsWe presented a new kernel-based approach tolearn semantic parsers.
SVM classifiers based onstring subsequence kernels are trained for each ofthe productions in the meaning representation lan-guage.
These classifiers are then used to com-positionally build complete meaning representa-tions of natural language sentences.
We evaluatedour system on two real-world corpora.
The re-sults showed that our system compares favorablyto other existing systems and is particularly robustto noise.AcknowledgmentsThis research was supported by Defense Ad-vanced Research Projects Agency under grantHR0011-04-1-0007.ReferencesMao Chen et al 2003.
Users manual: RoboCup soccer server manual for soc-cer server version 7.07 and later.
Available at http://sourceforge.net/projects/sserver/.Nello Cristianini and John Shawe-Taylor.
2000.
An Introduction to SupportVector Machines and Other Kernel-based Learning Methods.
CambridgeUniversity Press.Jay Earley.
1970.
An efficient context-free parsing algorithm.
Communica-tions of the Association for Computing Machinery, 6(8):451?455.R.
Ge and R. J. Mooney.
2005.
A statistical semantic parser that integratessyntax and semantics.
In Proc.
of 9th Conf.
on Computational NaturalLanguage Learning (CoNLL-2005), pages 9?16, Ann Arbor, MI, July.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.
Learning to transform naturalto formal languages.
In Proc.
of 20th Natl.
Conf.
on Artificial Intelligence(AAAI-2005), pages 1062?1068, Pittsburgh, PA, July.Rohit J. Kate.
2005.
A kernel-based approach to learning semantic parsers.Technical Report UT-AI-05-326, Artificial Intelligence Lab, University ofTexas at Austin, Austin, TX, November.V.
I. Levenshtein.
1966.
Binary codes capable of correcting insertions andreversals.
Soviet Physics Doklady, 10(8):707?710, February.Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and ChrisWatkins.
2002.
Text classification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz.
1996.
Afully statistical approach to natural language interfaces.
In Proc.
of the 34thAnnual Meeting of the Association for Computational Linguistics (ACL-96), pages 55?61, Santa Cruz, CA.John C. Platt.
1999.
Probabilistic outputs for support vector machines andcomparisons to regularized likelihood methods.
In Alexander J. Smola, Pe-ter Bartlett, Bernhard Scho?lkopf, and Dale Schuurmans, editors, Advancesin Large Margin Classifiers, pages 185?208.
MIT Press.Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and AlexanderYates.
2004.
Modern natural language interfaces to databases: Composingstatistical parsing with semantic tractability.
In Proc.
of 20th Intl.
Conf.
onComputational Linguistics (COLING-04), Geneva, Switzerland, August.Patti J.
Price.
1990.
Evaluation of spoken language systems: The ATIS do-main.
In Proc.
of 3rd DARPA Speech and Natural Language Workshop,pages 91?95, June.Andreas Stolcke.
1995.
An efficient probabilistic context-free parsing al-gorithm that computes prefix probabilities.
Computational Linguistics,21(2):165?201.L.
R. Tang and R. J. Mooney.
2001.
Using multiple clause constructors ininductive logic programming for semantic parsing.
In Proc.
of the 12thEuropean Conf.
on Machine Learning, pages 466?477, Freiburg, Germany.Yuk Wah Wong and Raymond J. Mooney.
2006.
Learning for semantic pars-ing with statistical machine translation.
In Proc.
of Human Language Tech-nology Conf.
/ North American Association for Computational LinguisticsAnnual Meeting (HLT/NAACL-2006), New York City, NY.
To appear.John M. Zelle and Raymond J. Mooney.
1996.
Learning to parse databasequeries using inductive logic programming.
In Proc.
of 13th Natl.
Conf.
onArtificial Intelligence (AAAI-96), pages 1050?1055, Portland, OR, August.Luke S. Zettlemoyer and Michael Collins.
2005.
Learning to map sentences tological form: Structured classification with probabilistic categorial gram-mars.
In Proc.
of 21th Conf.
on Uncertainty in Artificial Intelligence (UAI-2005), Edinburgh, Scotland, July.Victor W. Zue and James R. Glass.
2000.
Conversational interfaces: Advancesand challenges.
In Proc.
of the IEEE, volume 88(8), pages 1166?1180.920
