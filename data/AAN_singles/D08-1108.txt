Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1031?1040,Honolulu, October 2008. c?2008 Association for Computational LinguisticsMining and Modeling Relations betweenFormal and Informal Chinese Phrases from Web CorporaZhifei Li and David YarowskyDepartment of Computer Science and Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218, USAzhifei.work@gmail.com and yarowsky@cs.jhu.eduAbstractWe present a novel method for discoveringand modeling the relationship between in-formal Chinese expressions (including collo-quialisms and instant-messaging slang) andtheir formal equivalents.
Specifically, we pro-posed a bootstrapping procedure to identifya list of candidate informal phrases in webcorpora.
Given an informal phrase, we re-trieve contextual instances from the web us-ing a search engine, generate hypotheses offormal equivalents via this data, and rankthe hypotheses using a conditional log-linearmodel.
In the log-linear model, we incorpo-rate as feature functions both rule-based intu-itions and data co-occurrence phenomena (ei-ther as an explicit or indirect definition, orthrough formal/informal usages occurring infree variation in a discourse).
We test oursystem on manually collected test examples,and find that the (formal-informal) relation-ship discovery and extraction process usingour method achieves an average 1-best preci-sion of 62%.
Given the ubiquity of informalconversational style on the internet, this workhas clear applications for text normalizationin text-processing systems including machinetranslation aspiring to broad coverage.1 IntroductionInformal text (e.g., newsgroups, online chat, blogs,etc.)
is the majority of all text appearing on the Inter-net.
Informal text tends to have very different stylefrom formal text (e.g., newswire, magazine, etc.
).In particular, they are different in vocabulary, syn-tactic structure, semantic interpretation, discourseFormal Informal??
(BaiBai)[bye-bye] 88 (BaBa)??
(XiHuan)[like] ?, (XiFan)[gruel]??
(GeGe)[elder brother] GG??
(GeMi)[fans] N?
(FenSi)[a food]Table 1: Example Chinese Formal-informal Relations.The PinYin pronunciation is in parentheses and an op-tional literal gloss is in brackets.structure, and so on.
On the other hand, certain re-lations exist between the informal and formal text,and informal text often has a viable formal equiva-lent.
Table 1 shows several naturally occurring ex-amples of informal expressions in Chinese, and Ta-ble 2 provides a more detailed inventory and charac-terization of this phenomena1.
The first example ofinformal phrase ?88?
is used very often in Chineseon-line chat when a person wants to say ?bye-bye?to the other person.
This can be explained as fol-lows.
In Chinese, the standard equivalent to ?bye-bye?
is ????
whose PinYin is ?BaiBai?.
Coin-cidentally, the PinYin of ?88?
is ?BaBa?.
Because?BaBa?
and ?BaiBai?
are near homophones, peopleoften use ?88?
to represent ???
?, either for inputconvenience or just for fun.
The other relations inTable 1 are formed due to similar processes as willbe described later.Due to the often substantial divergence between1For clarity, we represent Chinese words in the format: Chi-nese characters (optional PinYin equivalent in parentheses andoptional English gloss in brackets).1031informal and formal text, a text-processing systemtrained on formal text does not typically work wellon informal genres.
For example, in a machinetranslation system (Koehn et al, 2007), if the bilin-gual training data does not contain the word ??,?
(the second example in Table 1), it leaves theword untranslated.
On the other hand, if the word??,?
does appear in the training data but it hasonly a translation ?gruel?
as that is the meaning inthe formal text, the translation system may wronglytranslate ??,?
into ?gruel?
for the informal textwhere the word ??,?
is more likely to mean?like?.
Therefore, as a text-normalization step, itis desirable to transform the informal text into itsstandard formal equivalent before feeding it into ageneral-purpose text-processing system.
Unfortu-nately, there are many processes for generating in-formal expressions in common use today.
Suchtransformations are highly flexible/diverse, and newphrases are invented on the Internet every day due tomajor news events, popular movies, TV shows, ra-dio talks, political activities, and so on.
Therefore,it is of great interest to have a data-driven methodthat can automatically find the relations between in-formal and formal expressions.In this paper, we present a novel method for dis-covering and modeling the relationship between in-formal Chinese expressions found in web corporaand their formal equivalents.
Specifically, we im-plement a bootstrapping procedure to identify alist of candidate informal phrases.
Given an indi-vidual informal phrase, we retrieve contextual in-stances from the web using a search engine (in thiscase, www.baidu.com), generate hypotheses of for-mal equivalents via this data, and rank the hypothe-ses using a conditional log-linear model.
In the log-linear model, we incorporate as feature functionsboth rule-based intuitions and data co-occurrencephenomena (either as an explicit or indirect defini-tion, or through formal/informal usages occurring infree variation in a discourse).
We test our system onmanually collected test examples2, and find that the(formal-informal) relationship discovery and extrac-tion process using our method achieves an averageprecision of more than 60%.
This work has applica-2The training and test examples are freely available athttp://www.cs.jhu.edu/?zfli.tions for text normalization in many general-purposetext-processing tasks, e.g., machine translation.To the best of our knowledge, our work is thefirst published machine-learning approach to pro-ductively model the broad types of relationships be-tween informal and formal expressions in Chineseusing web corpora.2 Formal to Informal: Phenomena andExamplesIn this section, we describe the phenomena and pro-vide examples of the relations between formal andinformal expressions in Chinese (we refer to therelation as formal-informal phrases hereafter, evenin the case of single-word expressions).
We man-ually collected 908 formal-informal relations, andclassified these relations into four categories.
Wecollected these pairs by investigating multiple web-pages where the formal-informal relations are man-ually compiled, and then merged these seed relationsand removed duplicates.
In this way, the 908 exam-ples should give good coverage on the typical cat-egories in the formal-informal relations.
Also, thedistribution of the categories found in the 908 exam-ples should be representative of the actual distribu-tion of the formal-informal relations occurring in thereal text.
Table 2 presents these categories and ex-amples in each category.
In the last column, the tablealso shows the relative frequency of each category,computed based on the 908 examples.
Recall thatwe represent Chinese words in the format: Chinesecharacters (optional PinYin equivalent in parenthe-ses and optional English gloss in brackets).2.1 HomophoneIn general, a homophone is a word that is pro-nounced the same as another word but differs inmeaning and/or written-form.
Here, we use the word?homophone?
in a loose way.
In particular, we re-fer an informal phrase as a homophone of a formalphrase if its pronunciation is the same or similar tothe formal phrase.
In the three examples belongingto the homophone category in Table 2, the first ex-ample is a true homophone, while the other two areloose homophones.
The third example represents amajor sub-class where the informal phrase is a num-ber (e.g., 88).1032Category Formal Informal %Homophone ??
(BanZhu) [system administrator] ??
(BanZhu) [bamboo] 4.2??
(XiHuan)[like] ?, (XiFan)[gruel] 4.4??
(BaiBai)[bye-bye] 88 (BaBa) 21Abbreviation ?)?
(MeiGuoJunDui)[american army] ? (MeiJun)[american army] 3.8Acronym ??
(GeGe)[elder brother] GG 12.3E??
(Nu?PengYou)[girl friend] GF 7.2Transliteration ??
(GeMi)[fans] N?
(FenSi)[a Chinese food]2.3\\ (XieXie)[thank you] 3Q (SanQiu)Others ?n?N?
(XiLaLiFenSi)[fans of Hilary] ?, (XiFan)[gruel]44.8??jN?
(AoBaMaFenSi)[fans of Obama] QN (OuFen)[a food]? (ChaoQiang)[super strong] P??
(ZouZhaoGongXu)Table 2: Chinese Formal-informal Relations: Categories and Examples.
Literal glosses in brackets.For illustrative purposes, we can present thetransformation path showing how the informalphrase is obtained from the formal phrase.
In par-ticular, the transformation path for this category is?Formal ?
PinYin ?
Informal (similar or samePinYin as the formal phrase)?.2.2 Abbreviation and AcronymA Chinese abbreviation of a formal phrase is ob-tained by selecting one or more characters from thisformal phrase, and the selected characters can be atany position in the formal phrase (Li and Yarowsky,2008; Lee, 2005; Yin, 1999).
In comparison, anacronym is a special form of abbreviation, whereonly the first character of each word in the formalphrase is selected to form the informal phrase.
Table2 presents three examples belonging to this category.While the first example is an abbreviation, and theother two examples are acronyms.The transformation path for the second exam-ple is ?Formal ?
PinYin ?
Acronym?, and thetransformation path for the third example is ?For-mal?
English?
Acronym?.
Clearly, they differ inwhether PinYin or English is used as a bridge.2.3 TransliterationA transliteration is transcribing a word or text writ-ten in one writing system into another writing sys-tem.
Table 2 presents examples belonging to thiscategory.
In the first example, the Chinese infor-mal phrase ?N?
(FenSi)[a Chinese food]?
can bethought as a transliteration of the English phase?fans?
as the pronunciation of ?fans?
is quite sim-ilar to the PinYin ?FenSi?.The transformation path for this category is ?For-mal?
English?
Chinese Transliteration?.2.4 OthersDue to the inherently informal and flexible nature ofexpressions in informal genre, the formation of aninformal phrase can be very complex or ad-hoc.
Forexample, an informal phrase can be generated by ap-plying the above transformation rules jointly.
Moreimportantly, many relations cannot be described us-ing a simple set of rules.
Table 2 presents three suchexamples, where the first two examples are gener-ated by applying rules jointly and the third exampleis created by decomposing the Chinese characters inthe formal form.
The statistics collected from the904 examples tells us that about 45% of the relationsbelonging to this category.
This motivates us to usea data-driven method to automatically discover therelations between informal and formal phrases.3 Data Co-occurrenceIn natural language, related words tend to appear to-gether (i.e., co-occurrence).
For example, Bill Gates1033tends to appear together with Microsoft more of-ten than expected by chance.
Such co-occurrencemay imply the existence of a relationship, and is ex-ploited in formal-informal relation discovery underdifferent conditions.3.1 Data Co-occurrence in DefinitionsIn general, for many informal phrases in popular use,there is likely to be an explicit definition somewherethat provides or paraphrases its meaning for an unfa-miliar audience.
People have created dedicated def-inition web-pages to explain the relations betweenformal and informal phrases.
For example, the firstexample in Table 3 is commonly explained in manydedicated definition web-pages on the Internet.
Onthe other hand, in some formal text (e.g., researchpapers), people tend to define the informal phrasebefore it is used frequently in the later part of thetext.
The second example of Table 3 illustrates thisphenomena.
Clearly, the definition text normallycontains salient patterns.
For example, the first ex-ample follows the ?informal4formal{???
defi-nition pattern, while the second example follows thepattern ?formal (informal)?.
This gives us a reliableway to seed and bootstrap a list of informal phrasesas will be discussed in Section 4.1.Relation Definition Text(E?
?, GF) GF4E??{??(-??
?,-?)&?
{ yf~-???
(-?)X~yy?
?ZTable 3: Data Co-occurrence in Definitions3.2 Data Co-occurrence in Online ChatInformal phrases appear in online chat very often forinput convenience or just for fun.
Since differentpeople may have different ways or traditions to ex-press semantically-equivalent phrases, one may findmany nearby data co-occurrence examples in chattext.
For example, in Table 4, after a series of mes-sage exchanges, person A wants to end the conver-sation and types ????
(meaning ?bye-bye?
), per-son B later includes the same semantic content, butin a different (more or less formal) expression (e.g.?88?
)....Person A: ?X??
?"?Person A: ?
?Person B: 88Table 4: Data Co-occurrence in Online Chat for Relation(?
?, 88) meaning ?bye-bye?3.3 Data Co-occurrence in News ArticlesFor some formal-informal relations, since both ofthe informal and formal phrases have been used inpublic very often and people are normally awareof these relations, an author may use the informaland formal phrases interchangeably without bother-ing to explain the relations.
This is particularly truein news articles for some well-known relations.
Ta-ble 5 shows an example, where the abbreviation ?????
(meaning ?winter olympics?)
appears in thetitle and its full-form ??????
appears in thetext of the same document.
In general, the relativedistance between an informal phrase and its formalphrase varies.
For example, they may appear in thesame sentence, or in neighboring sentences.Title ?????*R?<?
?Text c???2?9??(V?c??)?20?????{?*R?h?
-10?t8??????.??t*y ?{?
?Table 5: Data Co-occurrence in News Article for Relation(????,???)
meaning ?winter olympics?4 Mining Relations between Informal andFormal Phrases from WebIn this section, we describe an approach that auto-matically discovers the relation between a formalphrase and an informal phrase from web corpora.Specifically, we propose a bootstrapping procedureto identify a list of candidate informal phrases.Given a target informal phrase, we retrieve a largeset of of instances in context from the Web, generatecandidate hypotheses (i.e, candidate formal phrases)from the data, and rank the hypotheses by using aconditional log-linear model.
The log-linear modelis very flexible to incorporate both the rule- and data-1034driven intuitions (described in Sections 2 and 3, re-spectively) into the model as feature functions.4.1 Identifying Informal PhrasesBefore finding the formal phrase corresponding toan informal phrase, we first need to identify infor-mal phrases of interest.
For example, one can collectinformal phrases manually.
However, this is too ex-pensive as new relations between informal and for-mal phrases emerge every day on the Internet.
Alter-natively, one can employ a large amount of formaltext (e.g., newswire) and informal text (e.g., Inter-net blogs) to derive such a list as follows.
Specifi-cally, from the informal corpus we can extract thosephrases whose frequency in the informal corpus issignificantly different from that in the formal cor-pus.
However, such a list may be quite noisy, i.e.,many of them are not informal phrases at all.An alternative approach to extracting the infor-mal phrases is to use a bootstrapping algorithm (e.g.,Yarowsky (1995)).
Specifically, we first manuallycollect a small set of example relations.
Then, usingthese relations as a seed set, we extract the text pat-terns (e.g., the definition pattern showing how theinformal and formal phrases co-occur in the data asdiscussed in Section 3.1).
With these patterns, weidentify many more new relations from the data andaugment them into the seed set.
The procedure it-erates.
Using such an approach, we should be ableto extract a large list of formal-informal relations.Clearly, the list extracted in this way may be quitenoisy, and thus it is important to exploit both thedata- and rule-driven intuitions to rank these rela-tions properly.4.2 Retrieving Data from WebGiven an informal phrase, we retrieve training datafrom the web on the fly.
Specifically, we first usea search engine to identify a set of hyper-links thatpoint to web pages containing contexts relevant tothe informal phrase, and then follow the hyper-linksto download the web pages.
The input to the searchengine is a text query.
One can simply use the infor-mal phrase as a query.
However, this may lead to aset of pages that have nothing to do with the infor-mal phrase.
For example, if we search the informalphrase ?88?
(the third example in Table 2) using thewell-known Chinese search engine www.baidu.com,none of the top-10 pages are related to the infor-mal phrase ?88?.
To avoid this situation, one canuse a search engine that is dedicated to informal textsearch (e.g., blogsearch.baidu.com).
Alternatively,one can use the general-purpose search engine butexpanding the query with domain information.
Forexample, for the informal phrase ?88?, we can usea query ?88 d??
?, where ?d???
meansinternet language.4.3 Generating Candidate HypothesesGiven an informal phrase, we generate a set of hy-potheses which are candidate formal phrases corre-sponding to the informal phrase.
We considered twogeneral approaches to the generation of hypotheses.Rule-driven Hypothesis Generation: One canuse the rules described in Section 2 to generate aset of hypotheses.
However, with this approach, onemay generate an exponential number of hypotheses.For example, assuming the number of English wordsstarting with a given letter is O(|V |), we can generateO(|V |n) hypotheses given an acronym containing nletters.
Another problem with this approach is thata relation between an informal phrase and a formalphrase may not be explained by a specific rule.
Infact, as shown in the last row of Table 2, such rela-tions consist of 44.8% of all corpus instances.Data-driven Hypothesis Generation: With dataretrieved from the Web, we can generate hypothesesby enumerating the frequent n-grams co-occurringwith the informal phrase within certain distance.This exploits the data co-occurrence phenomena de-scribed in Section 3, that is, the formal phrase tendsto co-occur with the informal phrase nearby in thedata, for the multiple reasons described above.
Thiscan deal with the cases where the relation betweenan informal phrase and a formal phrase cannot beexplained by a rule.
However, it also suffers fromthe over-generation problem as in the rule-driven ap-proach.In this paper, we use the data-driven method togenerate hypotheses, and rank the hypotheses usinga conditional log-linear model that incorporates boththe rule and data intuitions as feature functions.10354.4 Ranking Hypotheses: ConditionalLog-linear ModelLog-linear models are known for flexible incorpora-tion of features into the model.
Each feature func-tion reflects a hint/intuition that can be used to rankthe hypotheses.
In this subsection, we develop aconditional log-linear model that incorporates boththe rule and data intuitions as feature functions.4.4.1 Conditional Log-linear ModelGiven an informal phrase (say x) and a candidateformal phrase (say y), the model assigns the pair ascore (say s(x, y)), which will be used to rank thehypothesis y.
The score s(x, y) is a linear combina-tion of the feature scores (say ?i(x, y)) over a set offeature functions indexed by i. Formally,s(x, y) =K?i=1?i(x, y)?
?i (1)where K is the number of feature functions definedand ?i is the weight assigned to the i-th feature func-tion (i.e., ?i).
To learn the weight vector ~?, we firstdefine a probability measure,P~?
(y|x) =1Z(x, ~?
)es(x,y) (2)where Z(x, ~?)
is a normalization constant.
Now, wedefine the regularized log-likelihood (LLR) of thetraining data (i.e, a set of pairs of (x, y)), as follows,LLR(~?)
=N?j=1log P~?
(yj |xj)?||~?||22?2(3)whereN is the number of training examples, and theregularization term ||~?||22?2 is a Gaussian prior with avariance ?2 (Roark et al, 2007).
The optimal weightvector ~??
is obtained by maximizing the regularizedlog-likelihood (LLR), that is,~??
= arg max~?LLR(~?)
(4)To maximize the above function, we use a limited-memory variable method (Benson and More, 2002)that is implemented in the TAO package (Benson etal., 2002) and has been shown to be very effective invarious natural language processing tasks (Malouf,2002).During test time, the following decision rule isnormally used to predict the optimal formal phrasey?
for a given informal phrase x,y?
= arg maxys(x, y).
(5)4.4.2 Feature FunctionsAs mentioned before, we incorporate both therule- and data-driven intuitions as feature functionsin the log-linear model.Rule-driven feature functions: Clearly, if a pair(x, y) matches the rule patterns described in Table 2,the pair has a high possibility to be a true formal-informal relation.
To reflect this intuition, we de-velop several feature functions as follows.?
LD-PinYin(x, y): the Levenshtein distance onPinYin of x and y.
The distance betweentwo PinYin characters is weighted based onthe similarity of pronunciation, for example,the weight w(l, n) is smaller than the weightw(a, z).?
LEN-PinYin(x, y): the difference in the num-ber of PinYin characters between x and y.?
Is-PinYin-Acronym(x, y): is x a PinYinacronym of y?
For example,Is-PinYin-Acronym(GG,??)=1,Is-PinYin-Acronym(GG,w?)=0.?
Is-CN-Abbreviation(x, y): is x a Chinese ab-breviation of y?
For example,Is-CN-Abbreviation(?,?)?)=1,Is-CN-Abbreviation(?,?)?
)=0.Data-driven feature functions: As described inSection 3, the informal and formal phrases tends toco-occur in the data.
Here, we develop several fea-ture functions to reflect this intuition.?
n-gram co-occurrence relative frequency: wecollect the n-grams that occur in the data withina window of the occurrence of the informalphrase, and compute their relative frequencyas feature values.
Since different orders ofgrams will have quite different statistics, wedefine 7 features in this category: 1-gram, 2-gram, 3-gram, 4-gram, 5-gram, 6to10-gram,and 11to15-gram.
Note that the order n of an-gram is in terms of number of Chinese char-acters instead of words.1036?
Features on a definition pattern: we have dis-cussed definition patterns in Section 3.1.
Foreach definition pattern, we can define a featurefunction saying that if the co-occurrence of xand y satisfies the definition pattern, the featurevalue is one, otherwise is zero.?
Features on the number of relevant web-pages:another interesting feature function can be de-fined as follows.
For each candidate relation(x, y), we use the pair as a query to search theweb, and treat the number of pages returned bythe search engine as a feature value.3 However,these features are quite expensive as millions ofqueries may need to be served.5 Experimental ResultsRecall that in Section 2 we categorize the formal-informal relations based on the manually collectedrelations.
In this section, we use a subset of them fortraining and testing.
In particular, we use 252 exam-ples to train the log-linear model that is describedin Section 4, and use 249 examples as test data tocompute the precision.4Table 6 shows the weights5 learned for the var-ious feature functions described in Section 4.4.Clearly, different feature functions get quite differ-ent weights.
This is intuitive as the feature functionsmay differ in the scale of the feature values or intheir importance in ranking the hypotheses.
In fact,this shows the importance of using the log-linearmodel to learn the optimal weights in a principledand automatic manner, instead of manually tuningthe weights in an ad-hoc way.Tables 7-9 show the precision results for differentcategories as described in Section 2, using the rule-driven, data-driven, or both rule and data-driven fea-tures, respectively.
In the tables, the precision corre-sponding to the ?top-N?
is computed in the follow-ing way: if the true hypothesis is among the top-Nhypotheses ranked by the model, we tag the classi-fication as correct, otherwise as wrong.
Clearly, the3Note that the number of pages relevant to a query can beeasily obtained as most search engines return this number.4Again, the training and test examples are freely available athttp://www.cs.jhu.edu/?zfli.5Note that we do not use the features on definition patternsand on the number of relevant web pages, for efficiency.Category Feature WeightRule-drivenLD-PinYin 0.800Len-PinYin 0.781Is-PinYin-Acronym 7.594Is-CN-Abbreviation 7.464Data-driven1-gram 14.5062-gram 108.1933-gram 82.9754-gram 66.8725-gram 42.2586to10-gram 21.22911to15-gram 0.985Table 6: Optimal Weights in the Log-linear Modellarger the N is, the higher the precision is.
Comput-ing the top-N precision (instead of just computingthe usual top-1 precision) is meaningful especiallywhen we consider our relation extractor as an inter-mediate step in an end-to-end text-processing sys-tem (e.g., machine translation) since the final deci-sion can be delayed to later stage based on more ev-idence.
In general, our model gets quite respectablyhigh precision for such a task (e.g., more than 60%for top-1 and more than 85% for top-100) when us-ing both data and rule-driven features, as shown inTable 9.
Moreover, the data-driven features are morehelpful than the rule-driven features (e.g, 25.3% ab-solute improvement in 1-best precision), while thecombination of these features does boost the perfor-mance of any individual feature set (e.g., 10.4% ab-solute improvement in 1-best precision over the caseusing data-driven features only).We also carried out experiments (see Table 10)in the bootstrapping procedure described in Section4.1.
In particular, we start from a seed set having130 relations.
We identify the frequent patterns fromthe data retrieved from the web for these seed exam-ples.
Then, we use these patterns to identify manymore new possible formal-informal relations.
Afterthe first iteration, we select the top 3000 pairs of re-lations matched by the patterns.
The recall of a man-ually collected test set (having 750 pairs) on these3000 pairs is around 30%, which is quite promisinggiven the highly noisy data.1037CategoryPrecision (%)Top-1 Top-10 Top-50 Top-100Homophone Same PinYin 31.6 47.4 68.4 73.7Similar PinYin 15.0 35.0 45.0 50.0Number 31.6 64.2 84.2 90.5Abbreviation Chinese abbreviation 11.8 35.3 41.2 41.2Acronym PinYin Acronym 39.3 82.1 91.1 92.9English Acronym 3.1 6.3 9.4 28.1Transliteration 10.0 20.0 20.0 20.0Average 26.1 53.4 66.3 72.3Table 7: Rule-driven Features only: Precision on Chinese Formal-informal Relation ExtractionCategoryPrecision (%)Top-1 Top-10 Top-50 Top-100Homophone Same PinYin 52.6 73.7 73.7 78.9Similar PinYin 45.0 65.0 75.0 75.0Number 66.3 86.3 94.7 96.8Abbreviation Chinese abbreviation 0.0 23.5 47.1 47.1Acronym PinYin Acronym 58.9 78.6 85.7 87.5English Acronym 25.0 46.9 68.6 68.8Transliteration 50.0 50.0 50.0 50.0Average 51.4 71.1 81.1 82.7Table 8: Data-driven Features only: Precision on Chinese Formal-informal Relation ExtractionCategoryPrecision (%)Top-1 Top-10 Top-50 Top-100Homophone Same PinYin 63.2 73.7 84.2 84.2Similar PinYin 40.0 60.0 70.0 80.0Number 81.1 91.6 95.8 96.8Abbreviation Chinese abbreviation 11.8 41.2 52.9 52.9Acronym PinYin Acronym 82.1 94.6 96.4 96.4English Acronym 21.9 46.9 56.3 59.4Transliteration 20.0 40.0 50.0 50.0Average 61.8 77.1 83.1 84.7Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction1038Size of seed set 130Size of candidate set 3000Size of test set 750Recall 30%Table 10: Recall of Test Set on a Candidate Set Extractedby a Bootstrapping Procedure6 Related WorkAutomatically extracting the relations between full-form Chinese phrases and their abbreviations is aninteresting and important task for many NLP appli-cations (e.g., machine translation, information re-trieval, etc.).
Recently, Chang and Lai (2004), Lee(2005), Chang and Teng (2006), Li and Yarowsky(2008) have investigated this task.
Specifically,Chang and Lai (2004) describes a hidden markovmodel (HMM) to model the relationship betweena full-form phrase and its abbreviation, by treat-ing the abbreviation as the observation and the full-form words as states in the model.
Using a setof manually-created full-abbreviation relations astraining data, they report experimental results ona recognition task (i.e., given an abbreviation, thetask is to obtain its full-form, or the vice versa).Chang and Teng (2006) extends the work in Changand Lai (2004) to automatically extract the relationsbetween full-form phrases and their abbreviations,where both the full-form phrase and its abbrevia-tion are not given.
Clearly, the method in (Changand Lai, 2004; Chang and Teng, 2006) is super-vised because it requires the full-abbreviation rela-tions as training data.
Li and Yarowsky (2008) pro-pose an unsupervised method to extract the relationsbetween full-form phrases and their abbreviations.They exploit the data co-occurrence phenomena inthe newswire text, as we have done in this paper.Moreover, they augment and improve a statisticalmachine translation by incorporating the extractedrelations into the baseline translation system.Other interesting work that addresses a similartask as ours includes the work on homophones (e.g.,Lee and Chen (1997)), abbreviations with their defi-nitions (e.g., Park and Byrd (2001)), abbreviationsand acronyms in the medical domain (Pakhomov,2002), and transliteration (e.g., (Knight and Graehl,1998; Virga and Khudanpur, 2003; Li et al, 2004;Wu and Chang, 2007)).While all the above work deals with the rela-tions occurring within the formal text, we considerthe formal-informal relations that occur across bothformal and informal text, and we extract the rela-tions from the web corpora, instead from just formaltext.
Moreover, our method is semi-supervised inthe sense that the weights of the feature functionsare tuned in a supervised log-linear model using asmall number of seed relations while the generationand ranking of the hypotheses are unsupervised byexploiting the data co-occurrence phenomena.7 ConclusionsIn this paper, we have first presented a taxonomy ofthe formal-informal relations occurring in Chinesetext.
We have then proposed a novel method fordiscovering and modeling the relationship betweeninformal Chinese expressions (including colloqui-alisms and instant-messaging slang) and their formalequivalents.
Specifically, we have proposed a boot-strapping procedure to identify a list of candidateinformal phrases in web corpora.
Given an infor-mal phrase, we retrieved contextual instances fromthe web using a search engine, generated hypothe-ses of formal equivalents via this data, and rankedthe hypotheses using a conditional log-linear model.In the log-linear model, we incorporated as featurefunctions both rule-based intuitions and data co-occurrence phenomena (either as an explicit or in-direct definition, or through formal/informal usagesoccurring in free variation in a discourse).
We testedour system on manually collected test examples,and found that the (formal-informal) relationshipdiscovery and extraction process using our methodachieves an average 1-best precision of 62%.
Giventhe ubiquity of informal conversational style on theinternet, this work has clear applications for text nor-malization in text-processing systems including ma-chine translation aspiring to broad coverage.AcknowledgmentsWe would like to thank Yi Su, Sanjeev Khudanpur,and the anonymous reviewers for their helpful com-ments.
This work was partially supported by the De-fense Advanced Research Projects Agency?s GALEprogram via Contract No?HR0011-06-2-0001.1039ReferencesS.
J. Benson, L. C. McInnes, J. J.
More, and J. Sarich.2002.
Tao users manual, Technical Report ANL/MCS-TM-242-Revision 1.4, Argonne National Laboratory.S.
J. Benson and J. J.
More.
2002.
A limited memory vari-able metric method for bound constrained minimiza-tion.
preprint ANL/ACSP909-0901, Argonne NationalLaboratory.Jing-Shin Chang and Yu-Tso Lai.
2004.
A preliminarystudy on probabilistic models for Chinese abbrevia-tions.
In Proceedings of the 3rd SIGHAN Workshopon Chinese Language Processing, Barcelona, Spain(2004),pages 9-16.Jing-Shin Chang and Wei-Lun Teng.
2006.
MiningAtomic Chinese Abbreviation Pairs: A ProbabilisticModel for Single Character Word Recovery.
In Pro-ceedings of the 5rd SIGHAN Workshop on ChineseLanguage Processing, Sydney, Australia (2006), pages17-24.Kevin Knight and Jonathan Graehl.
1998.
MachineTransliteration.
Computational Linguistics, 24(4):599-612.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan,Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-strantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL, Demonstration Session, pages 177-180.H.W.D Lee.
2005.
A study of automatic expansion ofChinese abbreviations.
MA Thesis, The University ofHong Kong.Yue-Shi Lee and Hsin-Hsi Chen.
1997.
Applying RepairProcessing in Chinese Homophone Disambiguation.In Proceedings of the Fifth Conference on Applied Nat-ural Language Processing, pages 57-63.Haizhou Li, Min Zhang, and Jian Su.
2004.
A joint sourcechannel model for machine transliteration.
In Proceed-ings of ACL 2004, pages 159-166.Zhifei Li and David Yarowsky.
2008.
UnsupervisedTranslation Induction for Chinese Abbreviations usingMonolingual Corpora.
In Proceedings of ACL 2008,pages 425-433.R.
Malouf.
2002.
A comparison of algorithms for maxi-mum entropy parameter estimation.
In Proceedings ofCoNLL 2002, pages 49-55.Serguei Pakhomov.
2002.
Semi-Supervised MaximumEntropy Based Approach to Acronym and Abbrevia-tion Normalization in Medical Texts.
In Proceedingsof ACL 2002, pages 160-167.Youngja Park and Roy J. Byrd.
2001.
Hybrid text min-ing for finding abbreviations and their definitions.
InProceedings of EMNLP 2001, pages 126-133.Brian Roark, Murat Saraclar, and Michael Collins.
2007.Discriminative n-gram language modeling.
ComputerSpeech and Language, 21(2):373-392.Paola Virga and Sanjeev Khudanpur.
2003.
Transliter-ation of Proper Names in Cross lingual InformationRetrieval.
In Proceedings of the ACL 2003 Workshopon Multilingual and Mixed-language Named EntityRecognition.Jian-Cheng Wu and Jason S. Chang.
2007.
Learning toFind English to Chinese Transliterations on the Web.In Proceedings of EMNLP-CoNLL 2007, pages 996-1004.David Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceedingsof ACL 1995, pages 189-196.Z.P.
Yin.
1999.
Methodologies and principles of Chi-nese abbreviation formation.
In Language Teachingand Study, No.2 (1999) 73-82.1040
