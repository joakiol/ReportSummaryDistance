Tutorials, NAACL-HLT 2013, pages 1?3,Atlanta, Georgia, June 9 2013. c?2013 Association for Computational LinguisticsDeep Learning for NLP (without Magic)Richard Socher, Chris ManningStanford Universityrichard@socher.orgmanning@stanford.edu1 OverviewMachine learning is everywhere in today?s NLP, but by and large machine learningamounts to numerical optimization of weights for human designed representationsand features.
The goal of deep learning is to explore how computers can take ad-vantage of data to develop features and representations appropriate for complexinterpretation tasks.
This tutorial aims to cover the basic motivation, ideas, modelsand learning algorithms in deep learning for natural language processing.
Recently,these methods have been shown to perform very well on various NLP tasks suchas language modeling, POS tagging, named entity recognition, sentiment analysisand paraphrase detection, among others.
The most attractive quality of these tech-niques is that they can perform well without any external hand-designed resourcesor time-intensive feature engineering.
Despite these advantages, many researchersin NLP are not familiar with these methods.
Our focus is on insight and understand-ing, using graphical illustrations and simple, intuitive derivations.
The goal of thetutorial is to make the inner workings of these techniques transparent, intuitive andtheir results interpretable, rather than black boxes labeled ?magic here?.
The firstpart of the tutorial presents the basics of neural networks, neural word vectors, sev-eral simple models based on local windows and the math and algorithms of trainingvia backpropagation.
In this section applications include language modeling andPOS tagging.
In the second section we present recursive neural networks whichcan learn structured tree outputs as well as vector representations for phrases andsentences.
We cover both equations as well as applications.
We show how trainingcan be achieved by a modified version of the backpropagation algorithm intro-duced before.
These modifications allow the algorithm to work on tree structures.Applications include sentiment analysis and paraphrase detection.
We also drawconnections to recent work in semantic compositionality in vector spaces.
Theprinciple goal, again, is to make these methods appear intuitive and interpretable1rather than mathematically confusing.
By this point in the tutorial, the audiencemembers should have a clear understanding of how to build a deep learning systemfor word-, sentence- and document-level tasks.
The last part of the tutorial givesa general overview of the different applications of deep learning in NLP, includ-ing bag of words models.
We will provide a discussion of NLP-oriented issues inmodeling, interpretation, representational power, and optimization.2 OutlinePart I: The Basics?
Motivation?
From logistic regression to neural networks?
Theory: Backpropagation training?
Applications: Word vector learning, POS, NER?
Unsupervised pre-training, multi-task learning, and learning relationsPART II: Recursive Neural Networks?
Motivation?
Definition of RNNs?
Theory: Backpropagation through structure?
Applications: Sentiment Analysis, Paraphrase detection, Relation Classifi-cationPART III: Applications and Discussion?
Overview of various NLP applications?
Efficient reconstruction or prediction of high-dimensional sparse vectors?
Discussion of future directions, advantages and limitations23 Speaker BiosRichard Socher1 is a PhD student at Stanford working with Chris Manning andAndrew Ng.
His research interests are machine learning for NLP and vision.
Heis interested in developing new models that learn useful features, capture composi-tional and hierarchical structure in multiple modalities and perform well across dif-ferent tasks.
He was awarded the 2011 Yahoo!
Key Scientific Challenges Award,the Distinguished Application Paper Award at ICML 2011 and a Microsoft Re-search PhD Fellowship in 2012.Christopher Manning2 is an Associate Professor of Computer Science andLinguistics at Stanford University (PhD, Stanford, 1995).
Manning has coauthoredleading textbooks on statistical approaches to NLP (Manning and Schuetze 1999)and information retrieval (Manning et al2008).
His recent work concentrates onmachine learning and natural language processing, including applications such asstatistical parsing and text understanding, joint probabilistic inference, clustering,and deep learning over text and images.1http://www.socher.org/2http://nlp.stanford.edu/?manning/3
