Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 216?223,New York, June 2006. c?2006 Association for Computational LinguisticsTowards Automatic Scoring of Non-Native Spontaneous SpeechKlaus Zechner and Isaac I. BejarEducational Testing ServicePrinceton, NJ, USA(kzechner,ibejar)@ets.orgAbstractThis paper investigates the feasibility ofautomated scoring of spoken Englishproficiency of non-native speakers.Unlike existing automated assessmentsof spoken English, our data consists ofspontaneous spoken responses tocomplex test items.
We perform both aquantitative and a qualitative analysis ofthese features using two differentmachine learning approaches.
(1)  Weuse support vector machines to producea score and evaluate it with respect to amode  baseline and to human rateragreement.
We find that scoring basedon support vector machines yieldsaccuracies approaching inter-rateragreement in some cases.
(2) We useclassification and regression trees  tounderstand the role of different featuresand feature classes in thecharacterization of speaking proficiencyby human scorers.
Our analysis showsthat across all the test items most or allthe feature classes are used in the nodesof the trees suggesting that the scoresare, appropriately, a combination ofmultiple components of speakingproficiency.
Future research willconcentrate on extending the set offeatures and introducing new featureclasses to arrive at a scoring model thatcomprises additional relevant aspects ofspeaking proficiency.1 IntroductionWhile automated scoring of open-ended writtendiscourse has been approached by severalgroups recently (Rudner & Gagne, 2001; Sher-mis & Burstein, 2003), automated scoring ofspontaneous spoken language  has proven to bemore challenging and complex.
Spoken lan-guage tests are still mostly scored by human rat-ers.
However, several systems exist that scoredifferent aspects of spoken language; (Bernstein,1999; C. Cucchiarini, H. Strik, & L. Boves,1997a; Franco et al, 2000).
Our work departsfrom previous research in that our goal is tostudy the feasibility of automating scoring forspontaneous speech, that is, when the spokentext is not known in advance.We approach scoring here as the characteri-zation of a speaker?s oral proficiency based onfeatures that can be extracted from a spoken re-sponse to a well defined test question by meansof automatic speech recognition (ASR).
Wefurther approach scoring as the construction of amapping from a set of features to a score scale,in our case five discrete scores from 1 (least pro-ficient) to 5 (most proficient).
The set of fea-tures and the specific mapping are motivated bythe concept of communicative competence(Bachman, 1990; Canale & Swain, 1980;Hymes, 1972).
This means that the features inthe scoring system we are developing are meantto characterize specific components of commu-nicative competence, such as mastery of pronun-ciation, fluency, prosodic, lexical, grammaticaland pragmatical subskills.
The selection of fea-tures is guided by an understanding of the natureof speaking proficiency.
We rely on the scoringbehavior of judges to evaluate the features (sec-tion 8) as well as a convenient criterion forevaluating the feasibility of automated scoringbased on those features (section 7).
That is, therole of human scorers in this context is to pro-vide a standard for system evaluations (see sec-tion 7), as well as to validate specific featuresand feature classes chosen by the authors (sec-tion 8).
We use support vector machines (SVMs)1216to determine how well the features recover hu-man scores.
We collect performance data underthree different conditions, where features areeither based on actual recognizer output or onforced alignment.
(Forced alignment describes aprocedure in speech recognition where the rec-ognizer is looking for the most likely paththrough the Hidden Markov Models given atranscription of the speech file by an experi-enced transcriber.
This helps, e.g., in findingstart and end times of words or phonemes.)
Wethen use classification and regression trees(CART) as a means to evaluate the relative im-portance and salience of our features.
When theclassification criterion is a human score, as is thecase in this study, an inspection of the CARTtree can give us insights into the feature prefer-ences a human judge might have in deciding ona score.The organization of this paper is as follows:first, we discuss related work in spoken lan-guage scoring.
Next, we introduce the data ofour study and the speech recognizer used.
Insection 5 we describe features we used for thisstudy.
Section 6 describes the agreement amongraters for this data.
Section 7 describes the SVManalysis, section 8 the CART analysis.
This isfollowed by a discussion and then finally byconclusions and an outlook on future work.2 Related workThere has been previous work to characterizeaspects of communicative competence such asfluency, pronunciation, and prosody.
(Franco etal., 2000) present a system for automatic evalua-tion of pronunciation performance on a phonelevel and a sentence level of native and non-native speakers of English and other languages(EduSpeak).
Candidates read English text and aforced alignment between the speech signal andthe ideal path through the Hidden MarkovModel (HMM) was computed.
Next, the logposterior probabilities for pronouncing a certainphone at a certain position in the signal werecomputed to achieve a local pronunciation score.These scores are then combined with otherautomatically derived measures such as the rateof speech (number of words per second) or theduration of phonemes to yield global scores.(C.
Cucchiarini, S. Strik, & L. Boves,1997b)) and (Cucchiarini et al, 1997a)) describea system for Dutch pronunciation scoring alongsimilar lines.
Their feature set, however, is moreextensive and contains, in addition to log likeli-hood Hidden Markov Model scores, various du-ration scores, and information on pauses, wordstress, syllable structure, and intonation.
In anevaluation, they find good agreement betweenhuman scores and machine scores.
(Bernstein, 1999)) presents a test for spo-ken English (SET-10) that has the followingtypes of items: reading, repetition, fill-in-the-blank, opposites and open-ended answers.
Alltypes except for the last are scored automaticallyand a score is reported that can be interpreted asan indicator of how native-like a speaker?sspeech is.
In (Bernstein, DeJong, Pisoni, &Townshend, 2000), an experiment is performedto establish the generalizability of the SET-10test.
It is shown that this test?s output can suc-cessfully be mapped to the Council of Europe?sFramework for describing second language pro-ficiency (North, 2000).
This paper further re-ports on studies done to correlate the SET-10with two other tests of English proficiency,which are scored by humans and where commu-nicative competence is tested for.
Correlationswere found to be between 0.73 and 0.88.3 DataThe data we are using for the experimentsin this paper comes from a 2002 trial administra-tion of TOEFLiBT?
(Test Of English as a For-eign Language?internet-Based Test) for non-native speakers (LanguEdge ?).
Item responseswere transcribed from the digital recording ofeach response.
In all there are 927 responsesfrom 171 speakers.
Of these, 798 recordingswere from one of five main test items, identifiedas P-A, P-C, P-T, P-E and P-W.
The remaining129 responses were from other questions.
Asreported below, we use all 927 responses in theadaptation of the speech recognizer but the SVMand CART analyses are based on the 798 re-sponses to the five test items.
Of the five testitems, three are independent tasks (P-A, P-C, P-T) where candidates have to talk freely about acertain topic for 60 seconds.
An example mightbe ?Tell me about your favorite teacher.?
Two of2217the test items are integrated tasks (P-E, P-W)where candidates first read or listen to some ma-terial to which they then have to relate in theirresponses (90 seconds speaking time).
An ex-ample might be that the candidates listen to aconversational argument about studying at homevs.
studying abroad and then are asked to sum-marize the advantages and disadvantages of bothpoints of view.The textual transcription of our data set con-tains about 123,000 words and the audio files arein WAV format and recorded with a samplingrate of 11025Hz and a resolution of 8 bit.For the purpose of adaptation of the speechrecognizer, we split the full data (927 re-cordings) into a training (596) and a test set (331recordings).
For the CART and SVM analyseswe have 511 files in the train and 287 files inthe eval set, summing up to 798.
(Both data setsare subsets from the ASR adaptation trainingand test sets, respectively.)
The transcriptions ofthe audio files were done according to a tran-scription manual derived from the GermanVerbMobil project (Burger, 1995).
A wide vari-ety of disfluencies are accounted for, such as,e.g., false starts, repetitions, fillers, or incom-plete words.
One single annotator transcribedthe complete corpus; for the purpose of testinginter-coder agreement, a second annotator tran-scribed about 100 audio files, which were ran-domly selected from the complete set of 927files.
The disagreement between annotators,measured as word error rate (WER = (substitu-tions + deletions + insertions) / (substitutions +deletions + correct)) was slightly above 20%(only lexical entries were measured here).
Thisis markedly more disagreement than in othercorpora, e.g., in SwitchBoard (Meteer & al.,1995) where disagreements in the order of 5%are reported, but we have non-native speechfrom speakers at different levels of proficiencywhich is more challenging to transcribe.4 Speech recognition systemOur speech recognizer is a gender-independentHidden Markov Model system that was trainedon 200 hours of dictation data by native speakersof English.
32 cepstral coefficients are used; thedictionary has about 30,000 entries.
The sam-pling rate of the recognizer is 16000Hz as op-posed to 11025Hz for the LanguEdge?
corpus.The recognizer can accommodate this differenceinternally by up-sampling the input data stream.As our speech recognition system wastrained on data quite different from our applica-tion (dictation vs. spontaneous speech and nativevs.
non-native speakers) we adapted the systemto the LanguEdge ?
corpus.
We were able toincrease word accuracy on the unseen test setfrom 15% before adaptation to 33% in the fullyadapted model (both acoustic and languagemodel adaptation).5 FeaturesOur feature set, partly inspired by (Cucchiariniet al, 1997a), focuses on low-level fluency fea-tures, but also includes some features related tolexical sophistication and to content.
The featureset alo stems, in part, from the written guide-lines used by human raters for scoring this data.The features can be categorized as follows: (1)Length measures, (2) lexical sophisticationmeasures, (3) fluency measures, (4) rate meas-ures, and (5) content measures.
Table 1 renders acomplete list of the features we computed, alongwith a brief explanation.
We do not claim thesefeatures to provide a full characterization ofcommunicative competence; they should be seenas a first step in this direction.
The goal of theresearch is to gradually build such a set of fea-tures to eventually achieve as large a coverageof communicative competence as possible.
Thefeatures are computed based on the output of therecognition engine based on either forced align-ment or on actual recognition.
The output con-sists of (a) start and end time of every token andhence potential silence in between (used formost features); (b) identity of filler words (fordisfluency-related features); and (c) word iden-tity (for content features).3218Lexical counts and length measuresSegdur Total duration in seconds of all the utterancesNumutt Number of utterances in the responseNumwds Total number of word forms in the speech sampleNumdff Number of disfluencies (fillers)Numtok Number of tokens = Numwds+NumdffLexical sophisticationTypes Number of unique word forms in the speech sampleTtratio Ratio Types/Numtok (type-token ratio, TTR)Fluency measures(based on pause information)Numsil Number of silences, excluding silences between utterancesSilpwd Ratio Numsil/NumwdsSilmean  Mean duration in seconds of all silences in a response to a test itemSilstddv Standard deviation of silence durationRate measuresWpsec Number of words per secondDpsec.
Number of disfluencies per secondTpsec Number of types per secondSilpsec.
Number of silences per secondContent measures We first compute test-item-specific word vectors with the frequencycounts of all words occurring in the train set for each test item(wvec_testitem).
Then we generate for every item response a wordvector in kind (wvec_response) and finally compute the inner prod-uct to yield a similarity score:sim = wvec_testitem*wvec_responseCvfull  wvec_testitem*wvec_response6 other Cv*-features As Cvfull but measure similarity to a subset of wvec_testitem, basedon the scores in the train set (e.g., ?all responses with score 1?
)Cvlennorm Length-normalized Cvfull: Cvfull/NumwdsTable 1: List of features with definitions.6 Inter-rater agreementThe training and scoring procedures followedstandard practices in large scale testing.
Scorersare trained to apply the scoring standards thathave been previously agreed upon by the devel-opers of the test.
The training takes the form ofdiscussing multiple instances of responses ateach score level.
The scoring of the responsesused for training other raters is done by moreexperienced scorers working closely with thedesigners of the test.All the 927 speaking samples (see section 3)were rated once by one of several expert raters,which we call Rater1.
A second rating was ob-tained for approximately one half (454) of thespeaking samples, which we call Rater2.
Wecomputed the exact agreement for all Rater1-Rater2 pairs for all five test items and report theresults in the last column of Table 2.
Overall, theexact agreement was about 49% and the kappacoefficient 0.34.
These are rather low numbersand certainly demonstrate the difficulty of therating task for humans.
Inter-rater agreement forintegrated tasks is lower than for independenttasks.
We conjecture that this is related to thedual nature of scoring integrated tasks: for one,the communicative competence per se needs tobe assessed, but on the other hand so does thecorrect interpretation of the written or auditorystimulus material.
The low agreement in generalis also understandable since the number of fea-ture dimensions that have to be mentally inte-4219grated pose a significant cognitive load forjudges.17 SVM modelsAs we have mentioned earlier, the rationale be-hind using support vector machines for scoreprediction is to yield a quantitative analysis ofhow well our features would work in an actualscoring system, measured against human expertraters.
The choice of the particular classifier be-ing SVMs was due to their superior performancein many machine learning tasks.7.1 Support vector machinesSupport vector machines (SVMs) were in-troduced by (Vapnik, 1995) as an instantiationof his approach to model regularization.
Theyattempt to solve a multivariate discrete classifi-cation problem where an n-dimensional hyper-plane separates the input vectors into, in thesimplest case, two distinct classes.
The optimalhyperplane is selected to minimize the classifi-cation error on the training data, while maintain-ing a maximally large margin (the distance ofany point from the separating hyperplane).1 Inter-human agreement rates for written language, such asessays, are significantly higher, around 70-80% with a 5-point scale (Y.Attali, personal communication).
More re-cently we observed agreement rates of about 60% for spo-ken test items, but here a 4-point scale was used.7.2 ExperimentsWe built five SVM models based on thetrain data, one for each of the five test items.Each model has two versions: (a) based onforced alignment with the true reference, repre-senting the case with 100% word accuracy(align), and (b) based on the actual recognitionoutput hypotheses (hypo).
The SVM modelswere tested on the eval data set and there werethree test conditions: (1) both training and testconditions derived from forced alignment (align-align); (2) models trained on forced alignmentand evaluated based on actual recognition hy-potheses (align-hypo; this represented the realis-tic situation that while human transcriptions aremade for the training set, they would turn out tobe too costly when the system is running con-tinuously); and (3) both training and evaluationare based on ASR output in recognition mode(hypo-hypo).We identified the best models by running aset of SVMs with varying cost factors, rangingfrom 0.01 to 15, and three different kernels: ra-dial basis function, and polynomial, of seconddegree and of third degree.
We selected the bestperforming models measured on the train setand report results with these models on the evalset.
The cost factor for all three configurationsvaried between 5 and 15 among the five testitems, and as best kernel we found the radialbasis function in almost all cases, except forsome polynomial kernels in the hypo-hypo con-figurationMode(% ofevalset)Train : alignEval : alignTrain : alignEval : hypoTrain : hypoEval : hypoHuman RaterAgreement (%of all pairs)P-A (ind) 34 40.7 33.9 35.9 53P-C (ind) 53 50.0 55.0 56.7 57P-T (ind) 38 43.4 18.9 37.7 54P-E (int) 25 42.1 26.3 47.4 43P-W (int) 29 34.5 20.7 39.7 42Table 2: Speech scoring:  Mode baseline, SVM performance on forced alignment and standard recogni-tion data, and human agreement for all five test items (ind=independent task; int=integrated task).52207.3 ResultsTable 2 shows the results for the SVM analysisas well as a baseline measure of agreement andthe inter rater agreement.
The baseline refersto the expected level of agreement with Rater1by simply assigning the mode of the distributionof scores for a given question, i.e., to alwaysassign the most frequently occurring score onthe train set.
Table 2 also reports the agreementbetween trained raters.
As can be seen the hu-man agreement is consistently higher than themode agreement but the difference is less for theintegrated questions suggesting that humansscorers found those questions more challengingto score consistently.The other 3 columns of Table 2 report theresults for the perfect agreement between a scoreassigned by the SVM developed for that testquestion and Rater1 on the eval corpus, whichwas not used in the development of the SVM.We observe that for the align-align configura-tion, accuracies are all clearly better than themode baseline, except for P-C, which has anunusually skewed score distribution and there-fore a rather high mode baseline.
In the align-hypo case, where SVM models were built basedon features derived from ASR forced alignmentand where these models were tested using ASRoutput in recognition mode, we see a generaldrop in performance ?
again except for P-C ?which is to be expected as the training and testdata were derived in different ways.
Finally, inthe hypo-hypo configuration, using ASR recog-nition output for both training and testing, SVMmodels are, in comparison to the align-alignmodels, improved for the two integrated tasksbut not for the independent tasks, again exceptfor P-C.
The SVM classification accuracies forthe integrated tasks are in the range of humanscorer agreement, which indicates that a per-formance ceiling may have been reached al-ready.
These results suggest that the recovery ofscores is more feasible for integrated rather thanindependent tasks.
However, it is also the casethat human scorers had more difficulty with theintegrated tasks, as discussed in the previoussection.The fact that the classification performance ofthe hypo-hypo models is not greatly lower thanthat of the align-align models, and in some caseseven higher ---and that with the relatively lowword accuracy of 33%---, leads to our conjecturethat this could be due to the majority of featuresbeing based on measures which do not require acorrect word identity such as measures of rate orpauses.In a recent study (Xi, Zechner, & Bejar, 2006)with a similar speech corpus we found that whilethe hypo-hypo models are better than the align-align models when using features related to flu-ency, the converse is true when using word-based vocabulary features.8 CART models8.1 Classification and regression treesClassification and regression trees (CART trees)were introduced by (Breiman, Friedman, Ol-shen, & Stone, 1984).
The goal of a classifica-tion tree is to classify the data such that the datain the terminal or classification nodes is as pureas possible meaning all the cases have the sametrue classification, in the present case a scoreprovided by a human rater, the variable Rater1above.
At the top of the tree all the data is avail-able and is split into two groups based on a splitof one of the features available.
Each split istreated in the same manner until no further splitsare possible, in which case a terminal node hasbeen reached.8.2 Tree analysisFor each of the five test items described abovewe estimated a classification tree using as inde-pendent variables the features described in Table1 and as the dependent variable a human score.The trees were built on the train set.
Table 3shows the distribution of features in the CARTtree nodes of the five test items (rows) based onfeature classes (columns).
For P-A, for exam-ple, it can be seen that three of the featureclasses have a count greater than 0.
The lastcolumn shows the number of classes appearingin the tree and the number of total features, inparentheses.
The P-A tree, for example has sixfeatures from three classes.
The last row sum-marizes the number of test items that relied on afeature class and the number of features from6221that class across all five test items, in parenthe-sis.
For example, Rate and Length were presentin every test item and lexical sophistication waspresent in all but one test item.
The table sug-gests that across all test items there was goodcoverage of feature classes but length was espe-cially well represented.
This is to be expectedwith a group heterogeneous in speaking profi-ciency.
The length features often were used toclassify students in the lower scores, that is, stu-dents who could not manage to speak suffi-ciently to be responsive to the test item.9 Discussion9.1 Speech recognitionWe successfully adapted an off-the-shelf speechrecognition engine for the purpose of assessingspontaneous speaking proficiency.
By acousticand language model adaptation, we were able tomarkedly increase our speech recognition en-gine?s word accuracy, from initially 15% toeventually 33%.
Although a 33% recognitionrate is not high by current standards, the hurdlesto higher recognition are significant, includingthe fact that the recognizer?s acoustic model wasoriginally trained on quite different data, and thefact that our data is based on highly accentedspeech from non-native speakers of English of arange of proficiencies, which are harder to rec-ognize than native speakers.9.2 SVM and CART modelsOur goal in this research has been to developmodels for automatically scoring communicativecompetence in non-native speakers of English.The approach we took is to compute featuresfrom ASR output that may eventually serve asindicators of communicative competence.
Weevaluated those features (a) in quantitative re-spect by using SVM models for score predictionand (b) in qualitative respect in terms of theirroles in assigning scores based on a human crite-rion by means of CART analyses.We found in the analysis of the SVM mod-els that despite low word accuracy, with ASRrecognition as a basis for training and testing,scores near inter-rater agreement levels can bereached for those items that include a listeningor reading passage.
When simulating perfectword accuracy (in the align-align configuration),4 of 5 test items achieve scoring accuraciesabove the mode baseline.
These results are veryencouraging in the light that we are continuingto add features to the models on various levels ofspeech proficiency.Test item Length LexicalsophisticationFluency Rate Content Total:# classes(# features)P-A 4 1 0 1 0 3 (6)P-C 4 0 1 1 1 4 (7)P-T 2 1 0 1 1 4 (5)P-E 1 1 2 1 1 5 (6)P-W 1 2 0 1 0 3 (4)Total #classes (#features)5 (12) 4 (5) 2 (3) 5 (5) 3 (3) 19 (28)Table 3: Distribution of features from the nodes of five CART trees (rows) into feature classes (columns).
The ?to-tals?
in the last colunmn and row count first the number of classes with at least one feature and then sums the fea-tures (in parentheses).7222CART trees have the advantage of being in-spectable and interpretable (unlike, e.g., neuralnets or support vector machines with non-linearkernels).
It is easy to trace a path from the rootof the tree to any leaf node and record the finaldecisions made along the way.
We looked at thedistribution of features in these CART treenodes (Table 3) andfound that all the different categories of featureswere used by the set of trees.
For all 5 test items,most classes occurred in the nodes of the respec-tive CART trees (with a minimum of 3 out of 5classes).10 Conclusions and future workThis paper is concerned with explorations intoscoring spoken language test items of non-nativespeakers of English.
We demonstrated that an ex-tended feature set comprising features related tolength, lexical sophistication, fluency, rate andcontent could be used to predict human scores inSVM models and to illuminate their distributioninto five different classes by means of a CARTanalysis.An important step for future work will be totrain the acoustic and language models of thespeech recognizer directly from our corpus; we areadditionally planning to use automatic speaker ad-aptation and to evaluate its benefits.
Furthermorewe are aware that, maybe with the exception of theclasses related to fluency, rate and length, our fea-ture set is as of yet quite rudimentary and will needsignificant expansion in order to obtain a broadercoverage of communicative competence.In summary, future work will focus on im-proving speech recognition, and on significantlyextending the feature sets in different categories.The eventual goal is to have a well-balanced multi-component scoring system which can both ratenon-native speech as closely as possible accordingto communicative criteria, as well as provide use-ful feedback for the language learner.ReferencesBachman, L. F. (1990).
Fundamental considerations inlanguage testing.
Oxford: Oxford University Press.Bernstein, J.
(1999).
PhonePass Testing: Structure andConstruct.
Menlo Park, CA: Ordinate Corporation.Bernstein, J., DeJong, J., Pisoni, D., & Townshend, B.(2000).
Two experiments in automatic scoring ofspoken language proficiency.
Paper presented at theInSTIL2000, Dundee, Scotland.Breiman, L., Friedman, J., Olshen, R., & Stone, C.(1984).
Classification and Regression Trees.
Bel-mont, California: Wadsworth Int.
Group.Burger, S. (1995).
Konventionslexikon zur Translitera-tion von Spontansprache.
Munich, Germany.Canale, M., & Swain, M. (1980).
Theoretical bases ofcommunicative approaches to second languageteaching and testing.
Applied Linguistics, 1(1), 1-47.Cucchiarini, C., Strik, H., & Boves, L. (1997a, Septem-ber).
Using speech recognition technology to assessforeign speakers' pronunciation of Dutch.
Paper pre-sented at the Third international symposium on theacquisition of second language speech: NEWSOUNDS 97, Klagenfurt, Austria.Cucchiarini, C., Strik, S., & Boves, L. (1997b).
Auto-matic evaluation of Dutch pronunciation by usingspeech recognition technology.
Paper presented atthe IEEE Automatic Speech Recognition and Under-standing Workshop, Santa Barbara, CA.Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R.,& Butzberger, J.
(2000).
The SRI EduSpeak system:Recognition and pronunciation scoring for languagelearning.
Paper presented at the InSTiLL-2000 (In-telligent Speech Technology in Language Learning),Dundee, Scotland.Hymes, D. H. (1972).
On communicative competence.In J.
B.
Pride & J. Holmes (Eds.
), Sociolinguistics:selected readings (pp.
269-293).
Harmondsworth,Middlesex: Penguin.Meteer, M., & al., e. (1995).
Dysfluency AnnotationStylebook for the Switchboard Corpus.Unpublishedmanuscript.North, B.
(2000).
The Development of a CommonFramework Scale of Language Proficiency.
NewYork, NY: Peter Lang.Rudner, L., & Gagne, P. (2001).
An overview of threeapproaches to scoring written essays by computer.Practical Assessment, Research & Development,7(26).Shermis, M. D., & Burstein, J.
(2003).
Automated essayscoring: A cross-disciplinary perspective.
Hillsdale,NJ: Lawrence Erlbaum Associates, Inc.Vapnik, V. N. (1995).
The Nature of Statistical Learn-ing Theory: Springer.Xi, X., Zechner, K., & Bejar, I.
(2006, April).
Extract-ing meaningful speech features to support diagnosticfeedback: an ECD approach to automated scoring.Paper presented at the NCME, San Francisco, CA.8223
