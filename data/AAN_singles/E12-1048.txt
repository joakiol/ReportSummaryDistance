Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 471?481,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsEvaluating language understanding accuracy with respect to objectiveoutcomes in a dialogue systemMyroslava O. Dzikovska and Peter Bell and Amy Isard and Johanna D. MooreInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh, United Kingdom{m.dzikovska,peter.bell,amy.isard,j.moore}@ed.ac.ukAbstractIt is not always clear how the differencesin intrinsic evaluation metrics for a parseror classifier will affect the performance ofthe system that uses it.
We investigate therelationship between the intrinsic evalua-tion scores of an interpretation componentin a tutorial dialogue system and the learn-ing outcomes in an experiment with humanusers.
Following the PARADISE method-ology, we use multiple linear regression tobuild predictive models of learning gain,an important objective outcome metric intutorial dialogue.
We show that standardintrinsic metrics such as F-score alone donot predict the outcomes well.
However,we can build predictive performance func-tions that account for up to 50% of the vari-ance in learning gain by combining fea-tures based on standard evaluation scoresand on the confusion matrix entries.
Weargue that building such predictive mod-els can help us better evaluate performanceof NLP components that cannot be distin-guished based on F-score alone, and illus-trate our approach by comparing the cur-rent interpretation component in the systemto a new classifier trained on the evaluationdata.1 IntroductionMuch of the work in natural language processingrelies on intrinsic evaluation: computing standardevaluation metrics such as precision, recall and F-score on the same data set to compare the perfor-mance of different approaches to the same NLPproblem.
However, once a component, such asa parser, is included in a larger system, it is notalways clear that improvements in intrinsic eval-uation scores will translate into improved over-all system performance.
Therefore, extrinsic ortask-based evaluation can be used to complementintrinsic evaluations.
For example, NLP com-ponents such as parsers and co-reference resolu-tion algorithms could be compared in terms ofhow much they contribute to the performance ofa textual entailment (RTE) system (Sammons etal., 2010; Yuret et al 2010); parser performancecould be evaluated by how well it contributes toan information retrieval task (Miyao et al 2008).However, task-based evaluation can be difficultand expensive for interactive applications.
Specif-ically, task-based evaluation for dialogue systemstypically involves collecting data from a numberof people interacting with the system, which istime-consuming and labor-intensive.
Thus, it isdesirable to develop an off-line evaluation pro-cedure that relates intrinsic evaluation metrics topredicted interaction outcomes, reducing the needto conduct experiments with human participants.This problem can be addressed via the use ofthe PARADISE evaluation methodology for spo-ken dialogue systems (Walker et al 2000).
In aPARADISE study, after an initial data collectionwith users, a performance function is created topredict an outcome metric (e.g., user satisfaction)which can normally only be measured throughuser surveys.
Typically, a multiple linear regres-sion is used to fit a predictive model of the desiredmetric based on the values of interaction param-eters that can be derived from system logs with-out additional user studies (e.g., dialogue length,word error rate, number of misunderstandings).PARADISE models have been used extensivelyin task-oriented spoken dialogue systems to estab-lish which components of the system most needimprovement, with user satisfaction as the out-come metric (Mo?ller et al 2007; Mo?ller et al2008; Walker et al 2000; Larsen, 2003).
In tu-torial dialogue, PARADISE studies investigated471which manually annotated features predict learn-ing outcomes, to justify new features needed inthe system (Forbes-Riley et al 2007; Rotaru andLitman, 2006; Forbes-Riley and Litman, 2006).We adapt the PARADISE methodology to eval-uating individual NLP components, linking com-monly used intrinsic evaluation scores with ex-trinsic outcome metrics.
We describe an evalua-tion of an interpretation component of a tutorialdialogue system, with student learning gain as thetarget outcome measure.
We first describe theevaluation setup, which uses standard classifica-tion accuracy metrics for system evaluation (Sec-tion 2).
We discuss the results of the intrinsic sys-tem evaluation in Section 3.
We then show thatstandard evaluation metrics do not serve as goodpredictors of system performance for the systemwe evaluated.
However, adding confusion matrixfeatures improves the predictive model (Section4).
We argue that in practical applications suchpredictive metrics should be used alongside stan-dard metrics for component evaluations, to bet-ter predict how different components will performin the context of a specific task.
We demonstratehow this technique can help differentiate the out-put quality between a majority class baseline, thesystem?s output, and the output of a new classifierwe trained on our data (Section 5).
Finally, wediscuss some limitations and possible extensionsto this approach (Section 6).2 Evaluation Procedure2.1 Data CollectionWe collected transcripts of students interactingwith BEETLE II (Dzikovska et al 2010b), a tu-torial dialogue system for teaching conceptualknowledge in the basic electricity and electron-ics domain.
The system is a learning environmentwith a self-contained curriculum targeted at stu-dents with no knowledge of high school physics.When interacting with the system, students spend3-5 hours going through pre-prepared reading ma-terial, building and observing circuits in a simula-tor, and talking with a dialogue-based computertutor via a text-based chat interface.During the interaction, students can be askedtwo types of questions.
Factual questions requirethem to name a set of objects or a simple prop-erty, e.g., ?Which components in circuit 1 are ina closed path??
or ?Are bulbs A and B wiredin series or in parallel?.
Explanation and defi-nition questions require longer answers that con-sist of 1-2 sentences, e.g., ?Why was bulb A onwhen switch Z was open??
(expected answer ?Be-cause it was still in a closed path with the bat-tery?)
or ?What is voltage??
(expected answer?Voltage is the difference in states between twoterminals?).
We focus on the performance of thesystem on these long-answer questions, since re-acting to them appropriately requires processingmore complex input than factual questions.We collected a corpus of 35 dialogues frompaid undergraduate volunteers interacting with thesystem as part of a formative system evaluation.Each student completed a multiple-choice test as-sessing their knowledge of the material before andafter the session.
In addition, system logs con-tained information about how each student?s utter-ance was interpreted.
The resulting data set con-tains 3426 student answers grouped into 35 sub-sets, paired with test results.
The answers werethen manually annotated to create a gold standardevaluation corpus.2.2 BEETLE II Interpretation OutputThe interpretation component of BEETLE II usesa syntactic parser and a set of hand-authored rulesto extract the domain-specific semantic represen-tations of student utterances from the text.
Thestudent answer is first classified with respect to itsdomain-specific speech act, as follows:?
Answer: a contentful expression to whichthe system responds with a tutoring action,either accepting it as correct or remediatingthe problems as discussed in (Dzikovska etal., 2010a).?
Help request: any expression indicating thatthe student does not know the answer andwithout domain content.?
Social: any expression such as ?sorry?
whichappears to relate to social interaction and hasno recognizable domain content.?
Uninterpretable: the system could not arriveat any interpretation of the utterance.
It willrespond by identifying the likely source oferror, if possible (e.g., a word it does not un-derstand) and asking the student to rephrasetheir utterance (Dzikovska et al 2009).472If the student utterance was determined to be ananswer, it is further diagnosed for correctness asdiscussed in (Dzikovska et al 2010b), using a do-main reasoner together with semantic representa-tions of expected correct answers supplied by hu-man tutors.
The resulting diagnosis contains thefollowing information:?
Consistency: whether the student statementcorrectly describes the facts mentioned inthe question and the simulation environment:e.g., student saying ?Switch X is closed?
islabeled inconsistent if the question stipulatedthat this switch is open.?
Diagnosis: an analysis of how well the stu-dent?s explanation matches the expected an-swer.
It consists of 4 parts?
Matched: parts of the student utterancethat matched the expected answer?
Contradictory: parts of the student ut-terance that contradict the expected an-swer?
Extra: parts of the student utterance thatdo not appear in the expected answer?
Not-mentioned: parts of the expectedanswer missing from the student utter-ance.The speech act and the diagnosis are passed tothe tutorial planner which makes decisions aboutfeedback.
They constitute the output of the inter-pretation component, and its quality is likely toaffect the learning outcomes, therefore we needan effective way to evaluate it.
In future work,performance of individual pipeline componentscould also be evaluated in a similar fashion.2.3 Data AnnotationThe general idea of breaking down the student an-swer into correct, incorrect and missing parts iscommon in tutorial dialogue systems (Nielsen etal., 2008; Dzikovska et al 2010b; Jordan et al2006).
However, representation details are highlysystem specific, and difficult and time-consumingto annotate.
Therefore we implemented a simpli-fied annotation scheme which classifies whole an-swers as correct, partially correct but incomplete,or contradictory, without explicitly identifying thecorrect and incorrect parts.
This makes it easier tocreate the gold standard and still retains useful in-formation, because tutoring systems often choosethe tutoring strategy based on the general answerclass (correct, incomplete, or contradictory).
Inaddition, this allows us to cast the problem interms of classifier evaluation, and to use standardclassifier evaluation metrics.
If more detailed an-notations were available, this approach could eas-ily be extended, as discussed in Section 6.We employed a hierarchical annotation schemeshown in Figure 1, which is a simplification ofthe DeMAND coding scheme (Campbell et al2009).
Student utterances were first annotatedas either related to domain content, or not con-taining any domain content, but expressing thestudent?s metacognitive state or attitudes.
Utter-ances expressing domain content were then codedwith respect to their correctness, as being fullycorrect, partially correct but incomplete, contain-ing some errors (rather than just omissions) orirrelevant1.
The ?irrelevant?
category was usedfor utterances which were correct in general butwhich did not directly answer the question.
Inter-annotator agreement for this annotation schemeon the corpus was ?
= 0.69.The speech acts and diagnoses logged by thesystem can be automatically mapped into our an-notation labels.
Help requests and social actsare assigned the ?non-content?
label; answersare assigned a label based on which diagnosisfields were filled: ?contradictory?
for those an-swers labeled as either inconsistent, or contain-ing something in the contradictory field; ?incom-plete?
if there is something not mentioned, butsomething matched as well, and ?irrelevant?
ifnothing matched (i.e., the entire expected answeris in not-mentioned).
Finally, uninterpretable ut-terances are treated as unclassified, analogous to asituation where a statistical classifier does not out-put a label for an input because the classificationprobability is below its confidence threshold.This mapping was then compared against themanually annotated labels to compute the intrin-sic evaluation scores for the BEETLE II interpreterdescribed in Section 3.3 Intrinsic Evaluation ResultsThe interpretation component of BEETLE II wasdeveloped based on the transcripts of 8 sessions1Several different subcategories of non-content utter-ances, and of contradictory utterances, were recorded.
How-ever, they resulting classes were too small and so were col-lapsed into a single category for purposes of this study.473Category Subcategory DescriptionNon-content Metacognitive and social expressions without domain content, e.g., ?Idon?t know?, ?I need help?, ?you are stupid?Content The utterance includes domain content.correct The student answer is fully correctpc incomplete The student said something correct, but incomplete, with some parts ofthe expected answer missingcontradictory The student?s answer contains something incorrect or contradicting theexpected answer, rather than just an omissionirrelevant The student?s statement is correct in general, but it does not answer thequestion.Figure 1: Annotation scheme used in creating the gold standardLabel Count Frequencycorrect 1438 0.43pc incomplete 796 0.24contradictory 808 0.24irrelevant 105 0.03non content 232 0.07Table 1: Distribution of annotated labels in the evalu-ation corpusof students interacting with earlier versions of thesystem.
These sessions were completed prior tothe beginning of the experiment during which ourevaluation corpus was collected, and are not in-cluded in the corpus.
Thus, the corpus constitutesunseen testing data for the BEETLE II interpreter.Table 1 shows the distribution of codes inthe annotated data.
The distribution is unbal-anced, and therefore in our evaluation results weuse two different ways to average over per-classevaluation scores.
Macro-average combines per-class scores disregarding the class sizes; micro-average weighs the per-class scores by class size.The overall classification accuracy (defined as thenumber of correctly classified instances out of allinstances) is mathematically equivalent to micro-averaged recall; however, macro-averaging betterreflects performance on small classes, and is com-monly used for unbalanced classification prob-lems (see, e.g., (Lewis, 1991)).The detailed evaluation results are presentedin Table 2.
We will focus on two metrics: theoverall classification accuracy (listed as ?micro-averaged recall?
as discussed above), and themacro-averaged F score.The majority class baseline is to assign ?cor-rect?
to every instance.
Its overall accuracy is43%, the same as BEETLE II.
However, this isobviously not a good choice for a tutoring sys-tem, since students who make mistakes will neverget tutoring feedback.
This is reflected in a muchlower value of the F score (0.12 macroaverage Fscore for baseline vs. 0.44 for BEETLE II).
Notealso that there is a large difference in the micro-and macro- averaged scores.
It is not immediatelyclear which of these metrics is the most important,and how they relate to actual system performance.We discuss machine learning models to help an-swer this question in the next section.4 Linking Evaluation Measures toOutcome MeasuresAlthough the intrinsic evaluation shows that theBEETLE II interpreter performs better than thebaseline on the F score, ultimately system devel-opers are not interested in improving interpreta-tion for its own sake: they want to know whetherthe time spent on improvements, and the compli-cations in system design which may accompanythem, are worth the effort.
Specifically, do suchchanges translate into improvement in overall sys-tem performance?To answer this question without running expen-sive user studies we can build a model which pre-dicts likely outcomes based on the data observedso far, and then use the model?s predictions as anadditional evaluation metric.
We chose a multiplelinear regression model for this task, linking theclassification scores with learning gain as mea-sured during the data collection.
This approachfollows the general PARADISE approach (Walkeret al 2000), but while PARADISE is typicallyused to determine which system components need474Label baseline BEETLE IIprec.
recall F1 prec.
recall F1correct 0.43 1.00 0.60 0.93 0.52 0.67pc incomplete 0.00 0.00 0.00 0.42 0.53 0.47contradictory 0.00 0.00 0.00 0.57 0.22 0.31irrelevant 0.00 0.00 0.00 0.17 0.15 0.16non-content 0.00 0.00 0.00 0.91 0.41 0.57macroaverage 0.09 0.20 0.12 0.60 0.37 0.44microaverage 0.18 0.43 0.25 0.70 0.43 0.51Table 2: Intrinsic Evaluation Results for the BEETLE II and a majority class baselinethe most improvement, we focus on finding a bet-ter performance metric for a single component(interpretation), using standard evaluation scoresas features.Recall from Section 2.1 that each participantin our data collection was given a pre-test anda post-test, measuring their knowledge of coursematerial.
The test score was equal to the propor-tion of correctly answered questions.
The normal-ized learning gain, post?pre1?pre is a metric typicallyused to assess system quality in intelligent tutor-ing, and this is the metric we are trying to model.Thus, the training data for our model consists of35 instances, each corresponding to a single dia-logue and the learning gain associated with it.
Wecan compute intrinsic evaluation scores for eachdialogue, in order to build a model that predictsthat student?s learning gain based on these scores.If the model?s predictions are sufficiently reliable,we can also use them for predicting the learninggain that a student could achieve when interactingwith a new version of the interpretation compo-nent for the system, not yet tested with users.
Wecan then use the predicted score to compare dif-ferent implementations and choose the one withthe highest predicted learning gain.4.1 FeaturesTable 4 lists the feature sets we used.
We tried twobasic types of features.
First, we used the eval-uation scores reported in the previous section asfeatures.
Second, we hypothesized that some er-rors that the system makes are likely to be worsethan others from a tutoring perspective.
For ex-ample, if the student gives a contradictory answer,accepting it as correct may lead to student miscon-ceptions; on the other hand, calling an irrelevantanswer ?partially correct but incomplete?
may beless of a problem.
Therefore, we computed sepa-rate confusion matrices for each student.
We nor-malized each confusion matrix cell by the totalnumber of incorrect classifications for that stu-dent.
We then added features based on confusionfrequencies to our feature set.2Ideally, we should add 20 different features toour model, corresponding to every possible con-fusion.
However, we are facing a sparse dataproblem, illustrated by the overall confusion ma-trix for the corpus in Table 3.
For example,we only observed 25 instances where a contra-dictory utterance was miscategorized as correct(compared to 200 ?contradictory?pc incomplete?confusions), and so for many students this mis-classification was never observed, and predictionsbased on this feature are not likely to be reliable.Therefore, we limited our features to those mis-classifications that occurred at least twice for eachstudent (i.e., at least 70 times in the entire cor-pus).
The list of resulting features is shown in the?conf?
row of Table 4.
Since only a small num-ber of features was included, this limits the appli-cability of the model we derived from this dataset to the systems which make similar types ofconfusions.
However, it is still interesting to in-vestigate whether confusion probabilities provideadditional information compared to standard eval-uation metrics.
We discuss how better coveragecould be obtained in Section 6.4.2 Regression ModelsTable 5 shows the regression models we obtainedusing different feature sets.
All models were ob-tained using stepwise linear regression, using theAkaike information criterion (AIC) for variable2We also experimented with using % unclassified as anadditional feature, since % of rejections is known to be aproblem for spoken dialogue systems.
However, it did notimprove the models, and we do not report it here for brevity.475ActualPredicted contradictory correct irrelevant non-content pc incompletecontradictory 175 86 3 0 43correct 25 752 1 4 26irrelevant 31 12 16 4 29non-content 1 3 2 95 3pc incomplete 200 317 40 28 419Table 3: Confusion matrix for BEETLE II.
System predicted values are in rows; actual values in columns.selection implemented in the R stepwise regres-sion library.
As measures of model quality, we re-port R2, the percentage of variance accounted forby the models (a typical measure of fit in regres-sion modeling), and mean squared error (MSE).These were estimated using leave-one-out cross-validation, since our data set is small.We used feature ablation to evaluate the contri-bution of different features.
First, we investigatedmodels using precision, recall or F-score alone.As can be seen from the table, precision is not pre-dictive of learning gain, while F-score and recallperform similarly to one another, withR2 = 0.12.In comparison, the model using only confusionfrequencies has substantially higher estimated R2and a lower MSE.3 In addition, out of the 3 con-fusion features, only one is selected as predictive.This supports our hypothesis that different typesof errors may have different importance within apractical system.The confusion frequency feature chosen bythe stepwise model (?predicted-pc incomplete-actual-contradictory?)
has a reasonable theoret-ical justification.
Previous research shows thatstudents who give more correct or partially cor-rect answers, either in human-human or human-computer dialogue, exhibit higher learning gains,and this has been established for different sys-tems and tutoring domains (Litman et al 2009).Consequently, % of contradictory answers is neg-atively predictive of learning gain.
It is reasonableto suppose, as predicted by our model, that sys-tems that do not identify such answers well, andtherefore do not remediate them correctly, will doworse in terms of learning outcomes.Based on this initial finding, we investigatedthe models that combined either F scores or the3The decrease in MSE is not statistically significant, pos-sibly because of the small data set.
However, since we ob-serve the same pattern of results across our models, it is stilluseful to examine.full set of intrinsic evaluation scores with confu-sion frequencies.
Note that if the full set of met-rics (precision, recall, F score) is used, the modelderives a more complex formula which coversabout 33% of the variance.
Our best models,however, combine the averaged scores with con-fusion frequencies, resulting in a higher R2 anda lower MSE (22% relative decrease between the?scores.f?
and ?conf+scores.f?
models in the ta-ble).
This shows that these features have comple-mentary information, and that combining them inan application-specific way may help to predicthow the components will behave in practice.5 Using prediction models in evaluationThe models from Table 5 can be used to comparedifferent possible implementations of the inter-pretation component, under the assumption thatthe component with a higher predicted learninggain score is more appropriate to use in an ITS.To show how our predictive models can be usedin making implementation decisions, we comparethree possible choices for an interpretation com-ponent: the original BEETLE II interpreter, thebaseline classifier described earlier, and a new de-cision tree classifier trained on our data.We built a decision tree classifier using theWeka implementation of C4.5 pruned decisiontrees, with default parameters.
As features, weused lexical similarity scores computed by theText::Similarity package4.
We computed8 features: the similarity between student answerand either the expected answer text or the questiontext, using 4 different scores: raw number of over-lapping words, F1 score, lesk score and cosinescore.
Its intrinsic evaluation scores are shown inTable 6, estimated using 10-fold cross-validation.We can compare BEETLE II and baseline clas-sifier using the ?scores.all?
model.
The predicted4http://search.cpan.org/dist/Text-Similarity/476Name Variablesscores.fm fmeasure.microaverage, fmeasure.macroaverage, fmeasure.correct,fmeasure.contradictory, fmeasure.pc incomplete,fmeasure.non-content,fmeasure.irrelevantscores.precision precision.microaverage, precision.macroaverage, precision.correct,precision.contradictory, precision.pc incomplete,precision.non-content,precision.irrelevantscores.recall recall.microaverage, recall.macroaverage, recall.correct, recall.contradictory,recall.pc incomplete,recall.non-content, recall.irrelevantscores.all scores.fm + scores.precision + scores.recallconf Freq.predicted.contradictory.actual.correct,Freq.predicted.pc incomplete.actual.correct,Freq.predicted.pc incomplete.actual.contradictoryTable 4: Feature sets for regression modelsVariables Cross-validationR2Cross-validationMSEFormulascores.f 0.12(0.02)0.0232(0.0302)0.32+ 0.56 ?
fmeasure.microaveragescores.precision 0.00(0.00)0.0242(0.0370)0.61scores.recall 0.12(0.02)0.0232(0.0310)0.37+ 0.56 ?
recall.microaverageconf 0.25(0.03)0.0197(0.0262)0.74?
0.56 ?Freq.predicted.pc incomplete.actual.contradictoryscores.all 0.33(0.03)0.0218(0.0264)0.63+ 4.20 ?
fmeasure.microaverage?
1.30 ?
precision.microaverage?
2.79 ?
recall.microaverage?
0.07 ?
recall.non?
contentconf+scores.f 0.36(0.03)0.0179(0.0281)0.52?
0.66 ?Freq.predicted.pc incomplete.actual.contradictory+ 0.42 ?
fmeasure.correct?
0.07 ?
fmeasure.non?
contentfull(conf+scores.all)0.49(0.02)0.0189(0.0248)0.88?
0.68 ?Freq.predicted.pc incomplete.actual.contradictory?
0.06 ?
precision.non domain+ 0.28 ?
recall.correct?
0.79 ?
precision.microaverage+ 0.65 ?
fmeasure.microaverageTable 5: Regression models for learning gain.
R2 and MSE estimated with leave-one-out cross-validation.Standard deviation in parentheses.477score for BEETLE II is 0.66.
The predictedscore for the baseline is 0.28.
We cannot usethe models based on confusion scores (?conf?,?conf+scores.f?
or ?full?)
for evaluating the base-line, because the confusions it makes are alwaysto predict that the answer is correct when theactual label is ?incomplete?
or ?contradictory?.Such situations were too rare in our training data,and therefore were not included in the models (asdiscussed in Section 4.1).
Additional data willneed to be collected before this model can rea-sonably predict baseline behavior.Compared to our new classifier, BEETLE II haslower overall accuracy (0.43 vs. 0.53), but per-forms micro- and macro- averaged scores.
BEE-TLE II precision is higher than that of the classi-fier.
This is not unexpected given how the systemwas designed: since misunderstandings causeddialogue breakdown in pilot tests, the interpreterwas built to prefer rejecting utterances as uninter-pretable rather than assigning them to an incorrectclass, leading to high precision but lower recall.However, we can use all our predictive modelsto evaluate the classifier.
We checked the the con-fusion matrix (not shown here due to space lim-itations), and saw that the classifier made someof the same types of confusions that BEETLE IIinterpreter made.
On the ?scores.all?
model, thepredicted learning gain score for the classifier is0.63, also very close to BEETLE II.
But with the?conf+scores.all?
model, the predicted score is0.89, compared to 0.59 for BEETLE II, indicatingthat we should prefer the newly built classifier.Looking at individual class performance, theclassifier performs better than the BEETLE II in-terpreter on identifying ?correct?
and ?contradic-tory?
answers, but does not do as well for par-tially correct but incomplete, and for irrelevant an-swers.
Using our predictive performance metrichighlights the differences between the classifiersand effectively helps determine which confusiontypes are the most important.One limitation of this prediction, however, isthat the original system?s output is considerablymore complex: the BEETLE II interpreter explic-itly identifies correct, incorrect and missing partsof the student answer which are then used by thesystem to formulate adaptive feedback.
This isan important feature of the system because it al-lows for implementation of strategies such as ac-knowledging and restating correct parts of the an-Label prec.
recall F1correct 0.66 0.76 0.71pc incomplete 0.38 0.34 0.36contradictory 0.40 0.35 0.37irrelevant 0.07 0.04 0.05non-content 0.62 0.76 0.68macroaverage 0.43 0.45 0.43microaverage 0.51 0.53 0.52Table 6: Intrinsic evaluation scores for our newly builtclassifier.swer.
However, we could still use a classifier to?double-check?
the interpreter?s output.
If thepredictions made by the original interpreter andthe classifier differ, and in particular when theclassifier assigns the ?contradictory?
label to ananswer, BEETLE II may choose to use a genericstrategy for contradictory utterances, e.g.
tellingthe student that their answer is incorrect withoutspecifying the exact problem, or asking them tore-read portions of the material.6 Discussion and Future WorkIn this paper, we proposed an approach for cost-sensitive evaluation of language interpretationwithin practical applications.
Our approach isbased on the PARADISE methodology for dia-logue system evaluation (Walker et al 2000).We followed the typical pattern of a PARADISEstudy, but instead of relying on a variety of fea-tures that characterize the interaction, we usedscores that reflect only the performance of theinterpretation component.
For BEETLE II wecould build regression models that account fornearly 50% variance in the desired outcomes, onpar with models reported in earlier PARADISEstudies (Mo?ller et al 2007; Mo?ller et al 2008;Walker et al 2000; Larsen, 2003).
More impor-tantly, we demonstrated that combining averagedscores with features based on confusion frequen-cies improves prediction quality and allows us tosee differences between systems which are not ob-vious from the scores alone.Previous work on task-based evaluation of NLPcomponents used RTE or information extractionas target tasks (Sammons et al 2010; Yuret et al2010; Miyao et al 2008), based on standard cor-pora.
We specifically targeted applications whichinvolve human-computer interaction, where run-ning task-based evaluations is particularly expen-478sive, and building a predictive model of systemperformance can simplify system development.Our evaluation data limited the set of featuresthat we could use in our models.
For most con-fusion features, there were not enough instancesin the data to build a model that would reliablypredict learning gain for those cases.
One wayto solve this problem would be to conduct a userstudy in which the system simulates random er-rors appearing some of the time.
This could pro-vide the data needed for more accurate models.The general pattern we observed in our datais that a model based on F-scores alone predictsonly a small proportion of the variance.
If a fullset of metrics (including F-score, precision andrecall) is used, linear regression derives a morecomplex equation, with different weights for pre-cision and recall.
Instead of the linear model, wemay consider using a model based on F?
score,F?
= (1 + ?2) PR?2P+R , and fitting it to the data toderive the ?
weight rather than using the standardF1 score.
We plan to investigate this in the future.Our method would apply to a wide range ofsystems.
It can be used straightforwardly withmany current spoken dialogue systems which relyon classifiers to support language understandingin domains such as call routing and technical sup-port (Gupta et al 2006; Acomb et al 2007).We applied it to a system that outputs more com-plex logical forms, but we showed that we couldsimplify its output to a set of labels which stillallowed us to make informed decisions.
Simi-lar simplifications could be derived for other sys-tems based on domain-specific dialogue acts typ-ically used in dialogue management.
For slot-based systems, it may be useful to consider con-cept accuracy for recognizing individual slot val-ues.
Finally, for tutoring systems it is possibleto annotate the answers on a more fine-grainedlevel.
Nielsen et al(2008) proposed an annota-tion scheme based on the output of a dependencyparser, and trained a classifier to identify individ-ual dependencies as ?expressed?, ?contradicted?or ?unaddressed?.
Their system could be evalu-ated using the same approach.The specific formulas we derived are not likelyto be highly generalizable.
It is a well-knownlimitation of PARADISE evaluations that modelsbuilt based on one system often do not performwell when applied to different systems (Mo?ller etal., 2008).
But using them to compare implemen-tation variants during the system development,without re-running user evaluations, can provideimportant information, as we illustrated with anexample of evaluating a new classifier we built forour interpretation task.
Moreover, the confusionfrequency feature that our models picked is con-sistent with earlier results from a different tutor-ing domain (see Section 4.2).
Thus, these modelscould provide a starting point when making sys-tem development choices, which can then be con-firmed by user evaluations in new domains.The models we built do not fully account forthe variance in the training data.
This is expected,since interpretation performance is not the onlyfactor influencing the objective outcome: otherfactors, such choosing the the appropriate tutor-ing strategy, are also important.
Similar modelscould be built for other system components to ac-count for their contribution to the variance.
Fi-nally, we could consider using different learningalgorithms.
Mo?ller et al(2008) examined deci-sion trees and neural networks in addition to mul-tiple linear regression for predicting user satisfac-tion in spoken dialogue.
They found that neuralnetworks had the best prediction performance fortheir task.
We plan to explore other learning algo-rithms for this task as part of our future work.7 ConclusionIn this paper, we described an evaluation of aninterpretation component of a tutorial dialoguesystem using predictive models that link intrin-sic evaluation scores with learning outcomes.
Weshowed that adding features based on confusionfrequencies for individual classes significantlyimproves the prediction.
This approach can beused to compare different implementations of lan-guage interpretation components, and to decidewhich option to use, based on the predicted im-provement in a task-specific target outcome met-ric trained on previous evaluation data.AcknowledgmentsWe thank Natalie Steinhauser, Gwendolyn Camp-bell, Charlie Scott, Simon Caine, Leanne Taylor,Katherine Harrison and Jonathan Kilgour for helpwith data collection and preparation; and Christo-pher Brew for helpful comments and discussion.This work has been supported in part by the USONR award N000141010085.479ReferencesKate Acomb, Jonathan Bloom, Krishna Dayanidhi,Phillip Hunter, Peter Krogh, Esther Levin, andRoberto Pieraccini.
2007.
Technical support dia-log systems: Issues, problems, and solutions.
InProceedings of the Workshop on Bridging the Gap:Academic and Industrial Research in Dialog Tech-nologies, pages 25?31, Rochester, NY, April.Gwendolyn C. Campbell, Natalie B. Steinhauser,Myroslava O. Dzikovska, Johanna D. Moore,Charles B. Callaway, and Elaine Farrow.
2009.
TheDeMAND coding scheme: A ?common language?for representing and analyzing student discourse.
InProceedings of 14th International Conference onArtificial Intelligence in Education (AIED), postersession, Brighton, UK, July.Myroslava O. Dzikovska, Charles B. Callaway, ElaineFarrow, Johanna D. Moore, Natalie B. Steinhauser,and Gwendolyn E. Campbell.
2009.
Dealing withinterpretation errors in tutorial dialogue.
In Pro-ceedings of the SIGDIAL 2009 Conference, pages38?45, London, UK, September.Myroslava Dzikovska, Diana Bental, Johanna D.Moore, Natalie B. Steinhauser, Gwendolyn E.Campbell, Elaine Farrow, and Charles B. Callaway.2010a.
Intelligent tutoring with natural languagesupport in the Beetle II system.
In Sustaining TEL:From Innovation to Learning and Practice - 5th Eu-ropean Conference on Technology Enhanced Learn-ing, (EC-TEL 2010), Barcelona, Spain, October.Myroslava O. Dzikovska, Johanna D. Moore, NatalieSteinhauser, Gwendolyn Campbell, Elaine Farrow,and Charles B. Callaway.
2010b.
Beetle II: a sys-tem for tutoring and computational linguistics ex-perimentation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL-2010) demo session, Uppsala, Swe-den, July.Kate Forbes-Riley and Diane J. Litman.
2006.
Mod-elling user satisfaction and student learning in aspoken dialogue tutoring system with generic, tu-toring, and user affect parameters.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Asso-ciation of Computational Linguistics (HLT-NAACL?06), pages 264?271, Stroudsburg, PA, USA.Kate Forbes-Riley, Diane Litman, Amruta Purandare,Mihai Rotaru, and Joel Tetreault.
2007.
Compar-ing linguistic features for modeling learning in com-puter tutoring.
In Proceedings of the 2007 confer-ence on Artificial Intelligence in Education: Build-ing Technology Rich Learning Contexts That Work,pages 270?277, Amsterdam, The Netherlands.
IOSPress.Narendra K. Gupta, Go?khan Tu?r, Dilek Hakkani-Tu?r,Srinivas Bangalore, Giuseppe Riccardi, and MazinGilbert.
2006.
The AT&T spoken language un-derstanding system.
IEEE Transactions on Audio,Speech & Language Processing, 14(1):213?222.Pamela W. Jordan, Maxim Makatchev, and UmaraniPappuswamy.
2006.
Understanding complex nat-ural language explanations in tutorial applications.In Proceedings of the Third Workshop on ScalableNatural Language Understanding, ScaNaLU ?06,pages 17?24.Lars Bo Larsen.
2003.
Issues in the evaluation of spo-ken dialogue systems using objective and subjectivemeasures.
In Proceedings of the 2003 IEEE Work-shop on Automatic Speech Recognition and Under-standing, pages 209?214.David D. Lewis.
1991.
Evaluating text categorization.In Proceedings of the workshop on Speech and Nat-ural Language, HLT ?91, pages 312?318, Strouds-burg, PA, USA.Diane Litman, Johanna Moore, Myroslava Dzikovska,and Elaine Farrow.
2009.
Using natural lan-guage processing to analyze tutorial dialogue cor-pora across domains and modalities.
In Proceed-ings of 14th International Conference on ArtificialIntelligence in Education (AIED), Brighton, UK,July.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-orientedevaluation of syntactic parsers and their representa-tions.
In Proceedings of ACL-08: HLT, pages 46?54, Columbus, Ohio, June.Sebastian Mo?ller, Paula Smeele, Heleen Boland, andJan Krebber.
2007.
Evaluating spoken dialoguesystems according to de-facto standards: A casestudy.
Computer Speech & Language, 21(1):26 ?53.Sebastian Mo?ller, Klaus-Peter Engelbrecht, andRobert Schleicher.
2008.
Predicting the quality andusability of spoken dialogue services.
Speech Com-munication, pages 730?744.Rodney D. Nielsen, Wayne Ward, and James H. Mar-tin.
2008.
Learning to assess low-level conceptualunderstanding.
In Proceedings 21st InternationalFLAIRS Conference, Coconut Grove, Florida, May.Mihai Rotaru and Diane J. Litman.
2006.
Exploit-ing discourse structure for spoken dialogue perfor-mance analysis.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?06, pages 85?93, Strouds-burg, PA, USA.Mark Sammons, V.G.Vinod Vydiswaran, and DanRoth.
2010.
?Ask not what textual entailment cando for you...?.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 1199?1208, Uppsala, Sweden, July.Marilyn A. Walker, Candace A. Kamm, and Diane J.Litman.
2000.
Towards Developing General Mod-els of Usability with PARADISE.
Natural Lan-guage Engineering, 6(3).480Deniz Yuret, Aydin Han, and Zehra Turgut.
2010.SemEval-2010 task 12: Parser evaluation using tex-tual entailments.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, pages51?56, Uppsala, Sweden, July.481
