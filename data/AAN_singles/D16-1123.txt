Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1153?1162,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsConvolutional Neural Network Language ModelsNgoc-Quan Pham and German Kruszewski and Gemma BoledaCenter for Mind/Brain SciencesUniversity of Trento{firstname.lastname}@unitn.itAbstractConvolutional Neural Networks (CNNs) haveshown to yield very strong results in severalComputer Vision tasks.
Their application tolanguage has received much less attention,and it has mainly focused on static classifica-tion tasks, such as sentence classification forSentiment Analysis or relation extraction.
Inthis work, we study the application of CNNsto language modeling, a dynamic, sequentialprediction task that needs models to capturelocal as well as long-range dependency infor-mation.
Our contribution is twofold.
First,we show that CNNs achieve 11-26% betterabsolute performance than feed-forward neu-ral language models, demonstrating their po-tential for language representation even in se-quential tasks.
As for recurrent models, ourmodel outperforms RNNs but is below state ofthe art LSTM models.
Second, we gain someunderstanding of the behavior of the model,showing that CNNs in language act as featuredetectors at a high level of abstraction, like inComputer Vision, and that the model can prof-itably use information from as far as 16 wordsbefore the target.1 IntroductionConvolutional Neural Networks (CNNs) are thefamily of neural network models that feature a typeof layer known as the convolutional layer.
This layercan extract features by convolving a learnable filter(or kernel) along different positions of a vectorial in-put.CNNs have been successfully applied in Com-puter Vision in many different tasks, including ob-ject recognition, scene parsing, and action recogni-tion (Gu et al, 2015), but they have received lessattention in NLP.
They have been somewhat ex-plored in static classification tasks where the modelis provided with a full linguistic unit as input (e.g.
asentence) and classes are treated as independent ofeach other.
Examples of this are sentence or docu-ment classification for tasks such as Sentiment Anal-ysis or Topic Categorization (Kalchbrenner et al,2014; Kim, 2014), sentence matching (Hu et al,2014), and relation extraction (Nguyen and Grish-man, 2015).
However, their application to sequen-tial prediction tasks, where the input is construed tobe part of a sequence (for example, language model-ing or POS tagging), has been rather limited (withexceptions, such as Collobert et al (2011)).
Themain contribution of this paper is a systematic evalu-ation of CNNs in the context of a prominent sequen-tial prediction task, namely, language modeling.Statistical language models are a crucial compo-nent in many NLP applications, such as AutomaticSpeech Recognition, Machine Translation, and In-formation Retrieval.
Here, we study the problemunder the standard formulation of learning to predictthe upcoming token given its previous context.
Onesuccessful approach to this problem relies on count-ing the number of occurrences of n-grams whileusing smoothing and back-off techniques to esti-mate the probability of an upcoming word (Kneserand Ney, 1995).
However, since each individualword is treated independently of the others, n-grammodels fail to capture semantic relations betweenwords.
In contrast, neural network language mod-els (Bengio et al, 2006) learn to predict the up-1153coming word given the previous context while em-bedding the vocabulary in a continuous space thatcan represent the similarity structure between words.Both feed-forward (Schwenk, 2007) and recurrentneural networks (Mikolov et al, 2010) have beenshown to outperform n-gram models in various se-tups (Mikolov et al, 2010; Hai Son et al, 2011).These two types of neural networks make differentarchitectural decisions.
Recurrent networks take onetoken at a time together with a hidden ?memory?vector as input and produce a prediction and an up-dated hidden vector for the next time step.
In con-trast, feed-forward language models take as input thelast n tokens, where n is a fixed window size, anduse them jointly to predict the upcoming word.In this paper we define and explore CNN-basedlanguage models and compare them with both feed-forward and recurrent neural networks.
Our resultsshow a 11-26% perplexity reduction of the CNNwith respect to the feed-forward language model,comparable or higher performance compared tosimilarly-sized recurrent models, and lower perfor-mance with respect to larger, state-of-the-art recur-rent language models (LSTMs as trained in Zarembaet al (2014)).Our second contribution is an analysis of the kindof information learned by the CNN, showing that thenetwork learns to extract a combination of grammat-ical, semantic, and topical information from tokensof all across the input window, even those that arethe farthest from the target.2 Related WorkConvolutional Neural Networks (CNNs) were orig-inally designed to deal with hierarchical representa-tion in Computer Vision (LeCun and Bengio, 1995).Deep convolutional networks have been success-fully applied in image classification and understand-ing (Simonyan and Zisserman, 2014; He et al,2015).
In such systems the convolutional kernelslearn to detect visual features at both local and moreabstract levels.In NLP, CNNs have been mainly applied to staticclassification tasks for discovering latent structuresin text.
Kim (2014) uses a CNN to tackle sentenceclassification, with competitive results.
The samework also introduces kernels with varying windowsizes to learn complementary features at differentaggregation levels.
Kalchbrenner et al (2014) pro-pose a convolutional architecture for sentence repre-sentation that vertically stacks multiple convolutionlayers, each of which can learn independent convo-lution kernels.
CNNs with similar structures havealso been applied to other classification tasks, suchas semantic matching (Hu et al, 2014), relation ex-traction (Nguyen and Grishman, 2015), and infor-mation retrieval (Shen et al, 2014).
In contrast, Col-lobert et al (2011) explore a CNN architecture tosolve various sequential and non-sequential NLPtasks such as part-of-speech tagging, named entityrecognition and also language modeling.
This is per-haps the work that is closest to ours in the existingliterature.
However, their model differs from ours inthat it uses a max-pooling layer that picks the mostactivated feature across time, thus ignoring tempo-ral information, whereas we explicitly avoid doingso.
More importantly, the language models trainedin that work are only evaluated through downstreamtasks and through the quality of the learned wordembeddings, but not on the sequence prediction taskitself, as we do here.Besides being applied to word-based sequences,the convolutional layers have also been used tomodel sequences at the character level.
Kim et al(2015) propose a recurrent language model that re-places the word-indexed projection matrix with aconvolution layer fed with the character sequencethat constitutes each word to find morphological pat-terns.
The main difference between that work andours is that we consider words as the smallest lin-guistic unit, and thus apply the convolutional layerat the word level.Statistical language modeling, the task we tackle,differs from most of the tasks where CNNs havebeen applied before in multiple ways.
First, the inputtypically consists of incomplete sequences of wordsrather than complete sentences.
Second, as a classi-fication problem, it features an extremely large num-ber of classes (the words in a large vocabulary).
Fi-nally, temporal information, which can be safely dis-carded in many settings with little impact in perfor-mance, is critical here: An n-gram appearing closeto the predicted word may be more informative, oryield different information, than the same n-gramappearing several tokens earlier.11543 ModelsOur model is constructed by extending a feed-forward language model (FFLM) with convolutionallayers.
In what follows, we first explain the imple-mentation of the base FFLM and then describe theCNN model that we study.3.1 Baseline FFLMOur baseline feed-forward language model (FFLM)is almost identical to the original model proposedby Bengio et al (2006), with only slight changes topush its performance as high as we can, producinga very strong baseline.
In particular, we extend itwith highway layers and use Dropout as regulariza-tion.
The model is illustrated in Figure 1 and worksas follows.
First, each word in the input n-gram ismapped to a low-dimensional vector (viz.
embed-ding) though a shared lookup table.
Next, theseword vectors are concatenated and fed to a highwaylayer (Srivastava et al, 2015).
Highway layers im-prove the gradient flow of the network by computingas output a convex combination between its input(called the carry) and a traditional non-linear trans-formation of it (called the transform).
As a result, ifthere is a neuron whose gradient cannot flow throughthe transform component (e.g., because the activa-tion is zero), it can still receive the back-propagationupdate signal through the carry gate.
We empiri-cally observed the usage of a single highway layer tosignificantly improve the performance of the model.Even though a systematic evaluation of this aspect isbeyond the scope of the current paper, our empiricalresults demonstrate that the resulting model is a verycompetitive one (see Section 4).Finally, a softmax layer computes the model pre-diction for the upcoming word.
We use ReLU for allnon-linear activations, and Dropout (Hinton et al,2012) is applied between each hidden layer.3.2 CNN and variantsThe proposed CNN network is produced by inject-ing a convolutional layer right after the words in theinput are projected to their embeddings (Figure 2).Rather than being concatenated into a long vector,the embeddings xi ?
Rk are concatenated transver-sally producing a matrix x1:n ?
Rn?k, where n is1.........sharedwordspace SoftmaxP(wj = i|hj)wj-1wj-n+1wj-2wj-3Highwaylayerdropout dropoutHI HOtransformcarryFigure 1: Overview of baseline FFLM.the size of the input and k is the embedding size.This matrix is fed to a time-delayed layer, whichconvolves a sliding window of w input vectors cen-tered on each word vector using a parameter matrixW ?
Rw?k.
Convolution is performed by takingthe dot-product between the kernel matrix W andeach sub-matrix xi?w/2:i+w/2 resulting in a scalarvalue for each position i in input context.
This valuerepresents how much the words encompassed by thewindow match the feature represented by the filterW .
A ReLU activation function is applied subse-quently so negative activations are discarded.
Thisoperation is repeated multiple times using variouskernel matrices W , learning different features in-dependently.
We tie the number of learned kernelsto be the same as the embedding dimensionality k,such that the output of this stage will be another ma-trix of dimensions n ?
k containing the activationsfor each kernel at each time step.
The number ofkernels was tied to the embedding size for two rea-sons, one practical, namely, to limit the hyper pa-rameter search, one methodological, namely, to keepthe network structure identical to that of the baselinefeed-forward model.Next, we add a batch normalization stage imme-diately after the convolutional output, which facil-itates learning by addressing the internal covariate1155shift problem and regularizing the learned represen-tations (Ioffe and Szegedy, 2015).Finally, this feature matrix is directly fed intoa fully connected layer that can project the ex-tracted features into a lower-dimensional represen-tation.
This is different from previous work, wherea max-over-time pooling operation was used to findthe most activated feature in the time series.
Ourchoice is motivated by the fact that the max poolingoperator loses the specific position where the featurewas detected, which is important for word predic-tion.After this initial convolutional layer, the networkproceeds identically to the FFNN by feeding the pro-duced features into a highway layer, and then, to asoftmax output.This is our basic CNN architecture.
We also ex-periment with three expansions to the basic model,as follows.
First, we generalize the CNN by ex-tending the shallow linear kernels with deeper multi-layer perceptrons, in what is called a MLP Convolu-tion (MLPConv) structure (Lin et al, 2013).
Thisallows the network to produce non-linear filters, andit has achieved state-of-the-art performance in objectrecognition while reducing the number of total lay-ers compared to other mainstream networks.
Con-cretely, we implement MLPConv networks by usinganother convolutional layer with a 1 ?
1 kernel ontop of the convolutional layer output.
This results inan architecture that is exactly equivalent to sliding aone-hidden-layer MLP over the input.
Notably, wedo not include the global pooling layer in the origi-nal Network-in-Network structure (Lin et al, 2013).Second, we explore stacking convolutional lay-ers on top of each other (Multi-layer CNN or ML-CNN) to connect the local features into broader re-gional representations, as commonly done in com-puter vision.
While this proved to be useful forsentence representation (Kalchbrenner et al, 2014),here we have found it to be rather harmful for lan-guage modeling, as shown in Section 4.
It is impor-tant to note that, in ML-CNN experiments, we stackconvolutions with the same kernel size and numberof kernels on top of each other, which is to be distin-guished from the MLPConv that refers to the deeperstructure in each CNN layer mentioned above.Finally, we consider combining features learnedthrough different kernel sizes (COM), as depicted in1.shharedwoapcSftmxPw(jtrxa(xePwxps(tr=tdixstreSe|a)-1wnnsr+Figure 2: Convolutional layer on top of the context matrix..shareadwoapceSshfstmacshdxdPr(j.shfd tsS=i|.shfd tsS=i)-o11chni|-o11chni)+c22rhdto3rpHxgsyawoeFigure 3: Combining kernels with different sizes.
We concate-nate the outputs of 2 convolutional blocks with kernel size of 5and 3 respectively.Figure 3.
For example, we can have a combinationof kernels that learn filters over 3-grams with oth-ers that learn over 5-grams.
This is achieved simplyby applying in parallel two or more sets of kernelsto the input and concatenating their respective out-puts (Kim, 2014).4 ExperimentsWe evaluate our model on three English corpora ofdifferent sizes and genres, the first two of whichhave been used for language modeling evaluationbefore.
The Penn Treebank contains one mil-lion words of newspaper text with 10K words inthe vocabulary.
We reuse the preprocessing andtraining/test/validation division from Mikolov et1156al.
(2014).
Europarl-NC is a 64-million word cor-pus that was developed for a Machine Translationshared task (Bojar et al, 2015), combining Europarldata (from parliamentary debates in the EuropeanUnion) and News Commentary data.
We prepro-cessed the corpus with tokenization and true-casingtools from the Moses toolkit (Koehn et al, 2007).The vocabulary is composed of words that occur atleast 3 times in the training set and contains approx-imately 60K words.
We use the validation and testset of the MT shared task.
Finally, we took a sub-set of the ukWaC corpus, which was constructedby crawling UK websites (Baroni et al, 2009).
Thetraining subset contains 200 million words and thevocabulary consists of the 200K words that appearmore than 5 times in the training subset.
The val-idation and test sets are different subsets of theukWaC corpus, both containing 120K words.
Wepreprocessed the data similarly to what we did forEuroparl-NC.We train our models using Stochastic GradientDescent (SGD), which is relatively simple to tunecompared to other optimization methods that involveadditional hyper parameters (such as alpha in RM-Sprop) while being still fast and effective.
SGD iscommonly used in similar work (Devlin et al, 2014;Zaremba et al, 2014; Sukhbaatar et al, 2015).
Thelearning rate is kept fixed during a single epoch, butwe reduce it by a fixed proportion every time the val-idation perplexity increases by the end of the epoch.The values for learning rate, learning rate shrinkingand mini-batch sizes as well as context size are fixedonce and for all based on insights drawn from pre-vious work (Hai Son et al, 2011; Sukhbaatar et al,2015; Devlin et al, 2014) as well as experimentationwith the Penn Treebank validation set.Specifically, the learning rate is set to 0.05, withmini-batch size of 128 (we do not take the average ofloss over the batch, and the training set is shuffled).We multiply the learning rate by 0.5 every time weshrink it and clip the gradients if their norm is largerthan 12.
The network parameters are initialized ran-domly on a range from -0.01 to 0.01 and the contextsize is set to 16.
In Section 6 we show that this largecontext window is fully exploited.For the base FFNN and CNN we varied em-bedding sizes (and thus, number of kernels) k =128, 256.
For k = 128 we explore the simple CNN,incrementally adding MLPConv and COM varia-tions (in that order) and, alternatively, using a ML-CNN.
For k = 256, we only explore the formerthree alternatives (i.e.
all but the ML-CNN).
For thekernel size, we set it to w = 3 words for the sim-ple CNN (out of options 3, 5, 7, 9), whereas for theCOM variant we use w = 3 and 5, based on experi-mentation on PTB.
However, we observed the mod-els to be generally robust to this parameter.
Dropoutrates are tuned specifically for each combination ofmodel and dataset based on the validation perplex-ity.
We also add small dropout (p = 0.05?0.15)when we train the networks on the smaller corpus(Penn Treebank).The experimental results for recurrent neural net-work language models, such as Recurrent NeuralNetworks (RNN) and Long-Short Term Memorymodels (LSTM), on the Penn Treebank are quotedfrom previous work; for Europarl-NC, we train ourown models (we also report the performance of thesein-house trained RNN and LSTM models on thePenn Treebank for reference).
Specifically, we trainLSTMs with embedding size k = 256 and numberof layers L = 2 as well as k = 512 with L = 1, 2.We train one RNN with k = 512 and L = 2.
To trainthese models, we use the published source code fromZaremba et al (2014).
Our own models are alsoimplemented in Torch7 for easier comparison.1 Fi-nally, we selected the best performing convolutionaland recurrent language models on Europarl-NC andthe Baseline FFLM to be evaluated on the ukWaCcorpus.For all models trained on Europarl-NC andukWaC, we speed up training by approximatingthe softmax with Noise Contrastive Estimation(NCE) (Gutmann and Hyva?rinen, 2010), with theparameters being set following previous work (Chenet al, 2015).
Concretely, for each predicted word,we sample 10 words from the unigram distribution,and the normalization factor is such that lnZ = 9.
2For comparison, we also implemented a simplerversion of the FFNN without dropout and highwaylayers (Bengio et al, 2006).
These networks havetwo hidden layers (Arisoy et al, 2012) with the size1Available at https://github.com/quanpn90/NCE CNNLM.2We also experimented with Hierarchical Softmax (Mikolovet al, 2011) and found out that the NCE gave better perfor-mance in terms of speed and perplexity.1157of 2 times the embedding size (k), thus having thesame number of parameters as our baseline.5 ResultsOur experimental results are summarized in Table 1.First of all, we can see that, even though theFFNN gives a very competitive performance,3 theaddition of convolutional layers clearly improvesit even further.
Concretely, we observe a solid11-26% reduction of perplexity compared to thefeed-forward network after using MLP Convolution,depending on the setup and corpus.
CNN aloneyields a sizable improvement (5-24%), while MLP-Conv, in line with our expectations, adds anotherapproximately 2-5% reduction in perplexity.
A fi-nal (smaller) improvement comes from combiningkernels of size 3 and 5, which can be attributed toa more expressive model that can learn patterns ofn-grams of different sizes.
In contrast to the suc-cessful two variants above, the multi-layer CNN didnot help in better capturing the regularities of text,but rather the opposite: the more convolutional lay-ers were stacked, the worse the performance.
Thisalso stands in contrast to the tradition of convolu-tional networks in Computer Vision, where usingvery deep convolutional neural networks is key tohaving better models.
Deep convolution for textrepresentation is in contrast rather rare, and to ourknowledge it has only been successfuly applied tosentence representation (Kalchbrenner et al, 2014).We conjecture that the reason why deep CNNs maynot be so effective for language could be the effect ofthe convolution on the data: The convolution outputfor an image is akin to a new, more abstract image,which yet again can be subject to new convolutionoperations, whereas the textual counterpart may nolonger have the same properties, in the relevant as-pects, as the original linguistic input.Regarding the comparison with a stronger LSTM,our models can perform competitively under thesame embedding dimension (e.g.
see k = 256 ofk = 512) on the first two datasets.
However, theLSTM can be easily scaled using larger models, asshown in Zaremba et al (2014), which gives the3In our experiments, increasing the number of fully con-nected layers of the FFNN is harmful.
Two hidden layers withhighway connections is the best setting we could find.best known results to date.
This is not an option forour model, which heavily overfits with large hiddenlayers (around 1000) even with very large dropoutvalues.
Furthermore, the experiments on the largerukWaC corpus show an even clearer advantage forthe LSTM, which seems to be more efficient at har-nessing this volume of data, than in the case of thetwo smaller corpora.To sum up, we have established that the resultsof our CNN model are well above those of sim-ple feed forward networks and recurrent neural net-works.
While they are below state of the art LSTMs,they are able to perform competitively with them forsmall and moderate-size models.
Scaling to largersizes may be today the main roadblock for CNNsto reach the same performances as large LSTMs inlanguage modeling.6 Model AnalysisIn what follows, we obtain insights into the innerworkings of the CNN by looking into the linguis-tic patterns that the kernels learn to extract and alsostudying the temporal information extracted by thenetwork in relation to its prediction capacity.Learned patterns To get some insight into thekind of patterns that each kernel is learning to de-tect, we fed trigrams from the validation set of thePenn Treebank to each of the kernels, and extractedthe ones that most highly activated the kernel, simi-larly to what was done in Kalchbrenner et al (2014).Some examples are shown in Figure 4.
Since theword windows are made of embeddings, we can ex-pect patterns with similar embeddings to have closeactivation outputs.
This is borne out in the analysis:The kernels specialize in distinct features of the data,including more syntactic-semantic constructions (cf.the ?comparative kernel?
including as .
.
.
as pat-terns, but also of more than) and more lexical or top-ical features (cf.
the ?ending-in-month-name?
ker-nel).
Even in the more lexicalized features, how-ever, we see linguistic regularities at different lev-els being condensed in a single kernel: For instance,the ?spokesman?
kernel detects phrases consistingof an indefinite determiner, a company name (or theword company itself) and the word ?spokesman?.We hypothesize that the convolutional layer adds an?I identify one specific feature, but at a high level of1158Model k w Penn Treebank Europarl-NC ukWaCval test #p val test #p val test #pFFNN (Bengio et al, 2006) 128 - 156 147 4.5 - - - - - -Baseline FFNN 128 - 114 109 4.5 - - - - - -+CNN 128 3 108 102 4.5 - - - - - -+MLPConv 128 3 102 97 4.5 - - - - - -+MLPConv+COM 128 3+5 96 92 8 - - - - - -+ML-CNN (2 layers) 128 3 113 108 8 - - - - - -+ML-CNN (4 layers) 128 3 130 124 8 - - - - - -FFNN (Bengio et al, 2006) 256 - 161 152 8.2 - - - - - -Baseline FFNN 256 - 110 105 8.2 133 174 48 136 147 156+CNN 256 3 104 98 8.3 112 133 48 - - -+MLPConv 256 3 97 93 8.3 107 128 48 108 116 156+MLPConv+COM 256 3+5 95 91 18 108 128 83 - - -+MLPConv+COM 512 3+5 96 92 52 - - - - - -Model k L Penn Treebank Europarl-NC ukWaCval test #p val test #p val test #pRNN (Mikolov et al, 2014) 300 1 133 129 6 - - - - - -LSTM (Mikolov et al, 2014) 300 1 120 115 6.3 - - - - - -LSTM (Zaremba et al, 2014) 1500 2 82 78 48 - - - - - -LSTM (trained in-house) 256 2 108 103 5.1 137 155 31 - - -LSTM (trained in-house) 512 1 123 118 12 133 149 62 - - -LSTM (trained in-house) 512 2 94 90 11 114 124 63 79 83 205RNN (trained in-house) 512 2 129 121 10 152 173 61 - - -Table 1: Results on Penn Treebank and Europarl-NC.
Figure of merit is perplexity (lower is better).
Legend: k: embedding size(also number of kernels for the convolutional models and hidden layer size for the recurrent models); w: kernel size; val: results onvalidation data; test: results on test data; #p: number of parameters in millions; L: number of layers..shareedwhosprwdhrcwrSfhosptmdxeSs.hSxhospwdarS.S.Phrwdhospeshxr(hosprxhjSeejdhrxschaswdheor.rxhoSPohrxrxham ohrxrxhjsphrxrhadw hx=sidxar.rh sa=r.
(hx=sidxar.rh|sdS.Phx=sidxar.rhcSfdjSe(hx=sidxar.rhtmsews.hx=sidxpsar.rawh orSwar.hws|dwe oSdchd s.saSxeh)so.
oS rPshS.-dxeswhpSjjSrahd1 or.Pdh orSwar.h)so.ed1rxh|SjjSs.rSwdhws|dwepsmjfhrjjspheodfsdxhrjjspheodxeSjjhd1=d ehcswfprwwr.ehrjjspxheodcm.fxhrjjsphS.-dxeswxaswdhd-Sfd.ehras.PrhfSx=medhras.P|rwPrS.nom.eS.Phras.PPwspS.Phcdrwhras.P=rS.eS.PxhjSxedfhras.Pcr SjSeSdxhpSjjhxm|xer.eSrjj(poS ohpsmjfhxm|xer.eSrjj(fdr.hpSeedwhr emrjj(pdh+jjh=ws|r|j((smhxosmjfhwdrjj(or-dhm.eSjh.s-2s=dwreSs.hxS.
dhrmP2tmrwedwhd.fdfhxd=e2edwwS|jdhemdxfr(hs e2d-d.h|dcswdh)m.dFigure 4: Some example phrases that have highest activations for 8 example kernels (each box), extracted from the validation setof the Penn Treebank.
Model trained with 256 kernels for 256-dimensional word vectors.abstraction?
dimension to a feed-forward neural net-work, similarly to what has been observed in imageclassification (Krizhevsky et al, 2012).Temporal information To the best of our knowl-edge, the longest context used in feed-forward lan-guage models is 10 tokens (Hai Son et al, 2012),11591101201301401501601701801902002100  2  4  6  8  10  12  14  16Sum of positiveweightsPositionsFigure 5: The distribution of positive weights over context po-sitions, where 1 is the position closest to the predicted word.where no significant change in terms of perplexitywas observed for bigger context sizes, even thoughin that work only same-sentence contexts were con-sidered.
In our experiments, we use a larger contextsize of 16 while removing the sentence boundarylimit (as commonly done in n-gram language mod-els) such that the network can take into account thewords in the previous sentences.To analyze whether all this information waseffectively used, we took our best model, theCNN+MLPConv+COM model with embedding sizeof 256 (fifth line of second block in Table 1), andwe identified the weights in the model that map theconvolutional output (of size n ?
k) to a lower di-mensional vector (the ?mapping?
layer in Figure 2).Recall that the output of the convolutional layer is amatrix indexed by time step and kernel index con-taining the activation of the kernel when convolvedwith a window of text centered around the wordat the given time step.
Thus, output units of theabove mentioned mapping predicate over an ensem-ble of kernel activations for each time step.
Wecan identify the patterns that they learn to detect byextracting the time-kernel combinations for whichthey have positive weights (since we have ReLU ac-tivations, negative weights are equivalent to ignor-ing a feature).
First, we asked ourselves whetherthese units tend to be more focused on the time stepscloser to the target or not.
To test this, we calculatedthe sum of the positive weights for each position intime using an average of the mappings that corre-spond to each output unit.
The results are shown in4.555.566.570  2  4  6  8  10  12  14  16CrossEntropyNumber of positions revealedFigure 6: Perplexity change over position, by incrementally re-vealing the Mapping?s weights corresponding to each position.Figure 5.
As could be expected, positions that areclose to the token to be predicted have many activeunits (local context is very informative; see positions2-4).
However, surprisingly, positions that are actu-ally far from the target are also quite active.
It seemslike the CNN is putting quite a lot of effort on char-acterizing long-range dependencies.Next, we checked that the information extractedfrom the positions that are far in the past are actu-ally used for prediction.
To measure this, we arti-ficially lesioned the network so it would only readthe features from a given range of time steps (wordsin the context).
To lesion the network we manuallymasked the weights of the mapping that focus ontimes outside of the target range by setting them tozero.
We started using only the word closest to thefinal position and sequentially unmasked earlier po-sitions until the full context was used again.
The re-sult of this experiment is presented in Figure 6, andit confirms our previous observation that positionsthat are the farthest away contribute to the predic-tions of the model.
The perplexity drops dramati-cally as the first positions are unmasked, and thendecreases more slowly, approximately in the formof a power law (f(x) ?
x?0.9).
Even though the ef-fect is smaller, the last few positions still contributeto the final perplexity.7 ConclusionIn this work, we have investigated the potential ofConvolutional Neural Networks for one prominentNLP task, language modeling, a sequential predic-1160tion task.
We incorporate a CNN layer on top ofa strong feed-forward model enhanced with moderntechniques like Highway Layers and Dropout.
Ourresults show a solid 11-26% reduction in perplexitywith respect to the feed-forward model across threecorpora of different sizes and genres when the modeluses MLP Convolution and combines kernels of dif-ferent window sizes.
However, even without theseadditions we show CNNs to effectively learn lan-guage patterns that allow it to significantly decreasethe model perplexity.In our view, this improvement responds to twokey properties of CNNs, highlighted in the analysis.First, as we have shown, they are able to integrateinformation from larger context windows, using in-formation from words that are as far as 16 positionsaway from the predicted word.
Second, as we havequalitatively shown, the kernels learn to detect spe-cific patterns at a high level of abstraction.
This isanalogous to the role of convolutions in ComputerVision.
The analogy, however, has limits; for in-stance, a deeper model stacking convolution layersharms performance in language modeling, while itgreatly helps in Computer Vision.
We conjecturethat this is due to the differences in the nature of vi-sual vs. linguistic data.
The convolution creates sortof abstract images that still retain significant proper-ties of images.
When applied to language, it detectsimportant textual features but distorts the input, suchthat it is not text anymore.As for recurrent models, even if our model out-performs RNNs, it is well below state-of-the-artLSTMs.
Since CNNs are quite different in nature,we believe that a fruitful line of future research couldfocus on integrating the convolutional layer into arecurrent structure for language modeling, as wellas other sequential problems, perhaps capturing thebest of both worlds.AcknowledgmentsWe thank Marco Baroni and three anonymous re-viewers for fruitful feedback.
This project has re-ceived funding from the European Union?s Hori-zon 2020 research and innovation programme un-der the Marie Sklodowska-Curie grant agreementNo 655577 (LOVe); ERC 2011 Starting IndependentResearch Grant n. 283554 (COMPOSES) and theErasmus Mundus Scholarship for Joint Master Pro-grams.
We gratefully acknowledge the support ofNVIDIA Corporation with the donation of the GPUsused in our research.ReferencesEbru Arisoy, Tara N Sainath, Brian Kingsbury, and Bhu-vana Ramabhadran.
2012.
Deep neural network lan-guage models.
In Proceedings of the NAACL-HLT2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modelingfor HLT, pages 20?28.
Association for ComputationalLinguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Yoshua Bengio, Holger Schwenk, Jean-Se?bastienSene?cal, Fre?deric Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Ondr?ej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp, PhilippKoehn, Varvara Logacheva, Christof Monz, MatteoNegri, Matt Post, Carolina Scarton, Lucia Specia, andMarco Turchi.
2015.
Findings of the 2015 workshopon statistical machine translation.
In Proceedings ofthe Tenth Workshop on Statistical Machine Transla-tion, pages 1?46, Lisbon, Portugal, September.
Asso-ciation for Computational Linguistics.Xie Chen, Xunying Liu, Mark JF Gales, and Philip CWoodland.
2015.
Recurrent neural network languagemodel training with noise contrastive estimation forspeech recognition.
In 2015 IEEE International Con-ference on Acoustics, Speech and Signal Processing(ICASSP), pages 5411?5415.
IEEE.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard M Schwartz, and John Makhoul.2014.
Fast and robust neural network joint modelsfor statistical machine translation.
In ACL (1), pages1370?1380.
Citeseer.Jiuxiang Gu, Zhenhua Wang, Jason Kuen, LianyangMa, Amir Shahroudy, Bing Shuai, Ting Liu, Xingx-ing Wang, and Gang Wang.
2015.
Recent ad-1161vances in convolutional neural networks.
CoRR,abs/1512.07108.Michael Gutmann and Aapo Hyva?rinen.
2010.
Noise-contrastive estimation: A new estimation principle forunnormalized statistical models.
In AISTATS, vol-ume 1, page 6.Le Hai Son, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011.
Structured outputlayer neural network language model.
In Acoustics,Speech and Signal Processing (ICASSP), 2011 IEEEInternational Conference on, pages 5524?5527.
IEEE.Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.2012.
Measuring the influence of long range depen-dencies with neural network language models.
In Pro-ceedings of the NAACL-HLT 2012 Workshop: Will WeEver Really Replace the N-gram Model?
On the Fu-ture of Language Modeling for HLT, pages 1?10.
As-sociation for Computational Linguistics.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun.
2015.
Deep residual learning for image recogni-tion.
arXiv preprint arXiv:1512.03385.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2012.
Im-proving neural networks by preventing co-adaptationof feature detectors.
CoRR, abs/1207.0580.Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.2014.
Convolutional neural network architectures formatching natural language sentences.
In Advances inNeural Information Processing Systems, pages 2042?2050.Sergey Ioffe and Christian Szegedy.
2015.
Batchnormalization: Accelerating deep network trainingby reducing internal covariate shift.
arXiv preprintarXiv:1502.03167.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics, ACL 2014, June 22-27, 2014, Baltimore, MD,USA, Volume 1: Long Papers, pages 655?665.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M. Rush.
2015.
Character-aware neural languagemodels.
CoRR.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
arXiv preprint arXiv:1408.5882.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Acous-tics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, volume 1,pages 181?184.
IEEE.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of the45th annual meeting of the ACL on interactive posterand demonstration sessions, pages 177?180.
Associa-tion for Computational Linguistics.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.2012.
Imagenet classification with deep convolutionalneural networks.
In Advances in neural informationprocessing systems, pages 1097?1105.Yann LeCun and Yoshua Bengio.
1995.
Convolu-tional networks for images, speech, and time series.The handbook of brain theory and neural networks,3361(10):1995.Min Lin, Qiang Chen, and Shuicheng Yan.
2013.
Net-work in network.
arXiv preprint arXiv:1312.4400.Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky`, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In INTER-SPEECH, volume 2, page 3.Toma?s?
Mikolov, Stefan Kombrink, Luka?s?
Burget,Jan Honza C?ernocky`, and Sanjeev Khudanpur.
2011.Extensions of recurrent neural network languagemodel.
In Acoustics, Speech and Signal Processing(ICASSP), 2011 IEEE International Conference on,pages 5528?5531.
IEEE.Tomas Mikolov, Armand Joulin, Sumit Chopra, MichaelMathieu, and Marc?Aurelio Ranzato.
2014.
Learninglonger memory in recurrent neural networks.
arXivpreprint arXiv:1412.7753.Thien Huu Nguyen and Ralph Grishman.
2015.
Relationextraction: Perspective from convolutional neural net-works.
In Proceedings of NAACL-HLT, pages 39?48.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech & Language, 21(3):492?518.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, andGre?goire Mesnil.
2014.
A latent semantic modelwith convolutional-pooling structure for informationretrieval.
In Proceedings of the 23rd ACM Interna-tional Conference on Conference on Information andKnowledge Management, pages 101?110.
ACM.Karen Simonyan and Andrew Zisserman.
2014.
Verydeep convolutional networks for large-scale imagerecognition.
arXiv preprint arXiv:1409.1556.Rupesh Kumar Srivastava, Klaus Greff, and Ju?rgenSchmidhuber.
2015.
Highway networks.
CoRR,abs/1505.00387.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advances inNeural Information Processing Systems, pages 2431?2439.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.
arXivpreprint arXiv:1409.2329.1162
