/////////CHOOSING A DISTANCE METRIC FOR AUTOMATICWORD CATEGORIZATIONEmin  Erkan Korkmaz G6ktiirk U~;olukDepartment of Computer EngineeringMiddle East Technical UniversityAnkara-TurkeyEmails: korkmaz~ceng.metu.edu.trucolu k@ceng.metu.ed u.trAbst ractThis paper analyzes the functionality of dif-ferent distance metrics that can be used ina bottom-up unsupervised algorithm for au-tomatic word categorization.
The proposedmethod uses a modified greedy-type algorithm.The formulations of fuzzy theory are also usedto calculate the degree of membership for theelements in the linguistic clusters formed.
Theunigram and the bigram statistics of a corpusof about two million words are used.
Empiri-cal comparisons are made in order to supportthe discussions proposed for the type of dis-tance metric that would be most suitable formeasuring the similarity between linguistic el-ements.1 IntroductionStatistical natural language processing is a challeng-ing area in the field of computational natural an-guage learning.
Researchers of this field have anapproach to language acquisition in which learningis visualized as developing a generative, stochasticmodel of language and putting this model into prac-tice (Marcken, 1996).Automatic word categorization is an importantfield of application in statistical natural languageprocessing where the process is unsupervised and iscarried out by working on n-gram statistics to findout the categories of words.
Research in this areapoints out that it is possible to determine the struc-ture of a natural anguage by examining the regu-laxities of the statistics of language (Finch, 1993).It is possible to construct a bottom-up unsuper-vised algorithm for the categorization process.
Inour paper named "A Method for Improving Au-tomatic Word Categorization"(Korkmaz&Uqoluk,1997) such a method, using a modified greedy-typealgorithm supported by the notions of fuzzy logic,has been proposed.
The distance metric used tomeasure the similarities of linguistic elements in thisresearch is the Manhattan Metric.
This metric isbased on the absolute difference between the corre-sponding values of vector components.
The compo-nents of the vectors correspond to bigrarn statisticsof words for our case.
However words from the samelinguistic category in natural language may have to-tally different frequencies.
So using a distance met-ric based on only the absolute differences may not beso suitable for the linguistic categorization process.In this paper various distance metrics are analyzedwith the same algorithm in order to find out themost suitable one that could be used for linguisticelements.
Comparisons are made for the results ob-tained using different metrics.The organization of this paper is as follows.
Firstthe related work in the area of word categorizationis presented in section 2.
Then a general descrip-tion of the categorization process and our proposedalgorithm is given in 3 section, which is followed bypresentation of different distance metrics that canbe used with the algorithm.
In section 5 the resultsof the experiments and the comparisons between themetrics are given.
We discuss the relevance of theresults and conclude in the last section.2 Related WorkUsually unigram and the bigram statistics are usedfor automatic word categorization.
There exists re-search where bigram statistics are used for the deter-ruination of the weight matrix of a neural network(Finch, 1992).
Also bigrams are used with greedyalgorithm to form the hierarchical clusters of words(Brown, 1992).Genetic algorithms have also been success-fully used for the categorization process(Lankhorst,1994).
Lankhorst uses genetic algorithms to deter-mine the members of predetermined classes.
Thedrawback of his work is that the number of classesis determined previous to run-time and the geneticalgorithm only searches for the membership of thoseKorkmaz and G6ktark (lqoluk 111 Choosing A Distance Metric for Word CategorizationEmin Erkan Korkmaz and G6ktOrk l)~oluk (1998) Choosing A Distance Metric for Automatic Word Categorization.
InD.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural LanguageLearning, ACL, pp 111-120.classes.McMahon and Smith also use the bigram statisticsof a corpus to find the hierarchical clusters (McMa-hon, 1996).
However instead of using a greedy al-gorithm they use a top-down approach to form theclusters.
Firstly the system divides the initial setcontaining all the words to be clustered into twoparts and then the process continues on these newclusters iteratively.Statistical NLP methods have been used also to-gether with other methods of NLP.
Wilms (Wilms,1995) uses corpus based techniques together withknowledge-based techniques in order to induce a lex-ical sublanguage grammar.
Machine Translation isan other area where knowledge bases and statisticsare integrated.
Knight et al, (Knight, 1994) aim toscale-up grammar-based, knowledge-based MT tech-niques by means of statistical methods.3 Word CategorizationZipf, (Zipf, 1935), who is a linguist, was one of theearly researchers in statistical language models.
Hiswork states that 66% of large English corpus will fallwithin the first 2,000 most frequent words.
There-fore, the number of distinct structures needed to findan approximation to a large proportion of naturallanguage would be small compared to the size of cor-pus that could be used.
It can be claimed that byworking on a small set consisting of frequent words,it is possible to build a framework for the whole nat-ural language.N-gram models of language are commonly usedto build up such a framework.
An N-gram modelcan be formed by collecting the probabilities of wordstreams (wiIi = 1..n) where wi is followed by wi+l.These probabilities will be used to form the modelwhere we can predict he behavior of the language upto n words.
There exists current research that usesbigram statistics for word categorization i whichprobabilities of word pairs in the text are collectedand processed.These n-gram models can be used together withthe concept of mutual information to form the clus-ters.
Mutua//nformation is based on the conceptof entropy which can be defined informally as theunpredictability of a stochastic experiment.
Forlinguistic categorization, mutual information calcu-lated would denote the amount of knowledge pre-served in the bigram statistics.
The detailed expla-nation of mutual information and adapting the for-mulations for automatic word categorization processcould be found in (Lankhorst, 1994).3.1 Clustering ApproachWhen the mutual information is used for cluster-ing, the process is carried out somewhat at a macro-level.
Usually search techniques and tools are usedtogether with the mutual information in order toform some combinations of different sets, each ofwhich is then subject to some validity test.
Theidea used for the validity testing process is as follows.Since the mutual information denotes the amount ofprobabilistic knowledge that a word provides on theproceeding word, if similar behaving words wouldbe collected together into the same cluster, then theloss of mutual information would be minimal.
So,the search is among possible alternatives for sets orclusters with the aim to obtain a minimal loss inmutual information.Though this top-to-bottom method seems the-oretically possible, in the presented work (Kork-maz~Uqoluk, 1997) a different approach, which isbottom-up, is used.
In this incremental approach,set prototypes axe built and then combined withother sets or single words to form larger ones.
Themethod is based on the similarities or differences be-tween single words rather than the mutual informa-tion of a whole corpus.
In combining words into setsa fuzzy set approach is used.Using this constructive approach, it is possible tovisualize the word clustering problem as the problemof clustering points in an n-dimensional space if thelexicon space to be clustered consists of n words.The points which are the words of the corpus arepositioned on this n-dimensional space according totheir behavior relative to other words in the lexiconspace.
Each word is placed on the i th dimensionaccording to its bigram statistic with the word rep-resenting the dimension amely wi.
So the degree ofsimilarity between two words can be defined as hav-ing close bigram statistics in the corpus.
Words aredistributed in the n-dimensional space according tothose bigram statistics.
The idea is quite simple: Letwl and w2 be two words from the corpus.
Let Z bethe stochastic variable ranging over the words to beclustered.
Then if Px(wl, Z) is close to Px(w~, Z)and if Px(Z, wl) is close to Px(Z, w2) for Z rang-Lug over all the words to be clustered in the corpus,then we can state a closeness between the words Wland w2.
Here Px is the probability of occurrencesof word pairs.
Px (wl, Z) is the probability where ~wl appears as the first element in a word pair andPx(Z, wl) is the reverse probability where wl is thesecond element of the word pair.
This is the samefor w2 respectively.In order to start the clustering process, a distancefunction has to be defined between the elements inKorkmaz and G6ktark (J?oluk 112 Choosing A Distance Metric for Word CategorizationIIIIlIIIII|IIIIIIB/I/II////lIIIthe space.
Assume that the bigram statistics forword couples are placed in a matrix N, where N~jdenotes the number of times word-couple (w~, wj) isobserved in the corpus.
So formulating the similar-ity between two linguistic elements would be findingout the distance between two vectors that can beobtained from this matrix.
Different distance met-rics are proposed for the distance between vectors.The usage of a distance metric forms the main dis-cussion point of this paper.
In next section first thealgorithm used for categorization will be presentedand in section 4 these metrics and their usage forlinguistic categorization will be discussed.3.2 The  A lgor i thm for Categorizat ionHaving a distance function, it is possible to startthe clustering process.
The first idea that can beused is to form a greedy algorithm to start form-ing the hierarchy of word clusters.
If the lexiconspace to be clustered consists of {wl,w2,...,wn},then the first element from the lexicon space w~ istaken and a cluster with this word and its near-est neighbor or neighbors is formed.
Then thelexicon space is {(wl, ws~, ..., w~), wi, ..., w,} where(wl, ws~, ..., ws~) is the first cluster formed.
The pro-cess is repeated with the first element in the listwhich does not belong to any set yet (wi for ourcase) and the process iterates until no such word isleft.
The sets formed will be the clusters at the bot-tom of the cluster hierarchy.
Then to determine thebehavior of a set, the frequencies of its elements axeadded and the previous process this time is carriedon the sets rather than on single words until the clus-ter hierarchy is formed, so the algorithm stops whena single set is formed that contains all the words in.the lexicon space.In the early stages of this research such a greedymethod was used to form the clusters.
However,though some clusters at the low levels of the treeseemed to be correctly formed, as the number ofelements in a cluster increased towards the higherlevels, the clustering results became unsatisfactory.Two main factors were observed as the reasons forthe unsatisfactory esults.These were:?
Shortcomings of the greedy type algorithm.?
inadequacy of the method used to obtain the setbehavior from the properties of its elements.The greedy method results in a non optimal clus-tering in the initial level.
To make this point clearerconsider the following example: Let us assume thatfour words wl,w2, w3 and w4 axe forming the lexiconLEXICON SPACE 2Figure 1: Example for the clustering problem of greedy al-gorithm in a lexicon space with four different words.
Note thatd~2.~ s is the smallest distance in the distribution.
However sincewl is taken into consideration, it forms setl with its nearest neigh-bor w2 and w3 combines with w4 and form set2, although w2 isnearer.
And the expected third set is not formed.space.
Furthermore, let the distances between thesewords be defined as dw~,wj.
Then consider the distri-bution in Figure 1.
If the greedy method first tries tocluster Wl, then it will be clustered with w2, sincethe smallest dwl,w, value is d~l,~ 2.
So the secondword will be captured in the set and the algorithmwill continue the clustering process with w3.
At thispoint, though w3 is closest o w2, it is captured ina set and since w3 is closer to w4 than the center ofthis set is, a new cluster will be formed with mem-bers w3 and w4.
However, as it can be obviouslyseen visually from Figure 1 the first optimal clus-ter to be formed between these four words is the setThe second problem causing unsatisfactory clus-tering occurs after the initial sets axe formed.
Ac-cording to the algorithm, the clusters behave xactlylike other single words and participate in the cluster-ing just as single words do.
However to continue theprocess, the bigram statistics of the clusters houldbe determined.
This means that the distance be-tween the cluster and all the other elements in thesearch space have to be calculated.
One easy wayto determine this behavior is to find the average ofthe statistics of all the elements in a cluster.
Thismethod has its drawbacks.
If the corpus used for theprocess is not large, the proximity problem becomessevere.
On the other hand the linguistic role of aword may vary in contexts in different sentences.Many words axe used as noun , adjective or fallingintosome other linguistic category depending on thecontext.
It can be claimed that each word initiallyshall be placed in a cluster according to its dominantrole.
However to determine the behavior of a set thedominant roles of its elements hould also be used.Somehow the common properties (bigrams) of theelements hould be always used and the deviationsof each element should be eliminated in the process.Korkmaz and G6kt~rk O~oluk 113 Choosing,4 Distance Metn'c for Word Categorization3.2.1 Improving the Greedy MethodThe clustering process is improved to overcomethe above mentioned rawbacks.
To overcome thefirst problem the idea used is to allow words to bemembers of more than one cluster.
So after the firstpass over the lexicon space, intersecting clusters areformed.
For the lexicon space presented in Figure1 with four words, the expected third set will bealso .formed.
As the second step these intersectingsets are combined into a single set.
Then the closesttwo words in each combined set (according to thedistance function) are found and these two closestwords are taken into consideration as the centroidfor that set.
After finding the centroids of all sets,the distances between a member and all the cen-troids are calculated for all the words in the lexiconspace.
Following this, each word is moved to the setwhere the distance between this member and the setcenter is minimal.
This procedure is necessary sincethe initial sets are formed by combining the inter-secting sets.
When these intersecting sets are com-bined the set center of the resulting set might be faraway from some elements and there may be othercloser set centers formed by other combinations, oa reorganization of membership s appropriate.3.2.2 Fuzzy Membersh ipAs presented in the previous ection the cluster-ing process builds up a cluster hierarchy.
In the firststep, words are combined to form the initial clusters,then those clusters become members of the processthemselves.
To combine dusters into new ones theirstatistical behavior should be determined.
The sta-tistical behavior of a cluster is related to the bigramsof its members.
In order to find out the dominantstatistical role of each cluster the notion of fuzzymembership s used.The problem that each word can belong to morethan one linguistic category brings up the idea thatthe sets of word clusters cannot have crisp borderlines and even if a word seems to be in a set dueto its dominant linguistic role in the corpus, it canhave a degree of membership to the other clustersin the search space.
Therefore the concept of fuzzymembership can be used for determining the bigramstatistics of a cluster.Researchers working on fuzzy clustering presenta framework for defining fuzzy membership of ele-ments.
Gath and Geva (Gath, 1989) describe suchan unsupervised optimal fuzzy clustering.
Theypresent he K-means algorithm based on minimiza-tion of an objective function.
For the purpose ofthis research only the membership function of thealgorithm presented is used.
The membership func-tion uij that is the degree of membership of the i thelement o the j th cluster is defined as:I (q-i)Uij  -~ K Ek-~l I 1 I (q-~l } (1)Here Xi denotes an element in the search space,Vj is the centroid of the jth cluster.
K denotes thenumber of clusters.
And d2(Xi, Vj) is the distanceof Xith element to the centroid Vj of the jth cluster.The parameter q is the weighting exponent for uijand controls the fuzziness of the resulting cluster.After the degrees of membership of all the ele-ments of all classes in the search space are calcu-lated, the bigram statistics of the classes are de-rived.
To find those statistics the following methodis used: For each subject cluster, the bigram statis-tics of each element is multiplied with its mem-bership value.
This forms the amount of statisti-cal knowledge passed from the element to that set.So the elements chosen as set centroids will be theones that affect a set's statistical behavior tile most.Hepce an element away from a centroid will have alesser statistical contribution.4 Distance MetricsVarious distance metrics have been proposed bymathematicians that can be used to formulate thesimilarity between vectors.
Four of them are ex-mined  and used for this study.
The first one is theManhattan Metric which just calculates the absolutedifference between the values of two vector elements.It is defined by:D(x ,y )= E Ix i -y i \ ]  (2)l <_i<_nHere x = {Xl,Xa,...,zn} and y = {Yl,Y2,...,Yn}are two vectors defined over 7~ n.Having such a metric it is possible to define thedistance function between two linguistic elements.The distance function D between two words wl andw2 could be defined as follows:D(wl, w2) = D1 (wx, w2) + D2 (Wl, W2) (3)Here the distance function consists of two differ-ent parts D1 and D2.
This is because we want thedistance function to be based on both proceedingand preceding words.
So the first part denotes thedistance on proceeding words and the second one de-notes the distance obviously on the preceding words.If we use the Manhattan metric, the distance func-tion would be :Korkmaz and G61aiirk (\]9oluk 114 Choosing,4 Distance Metric for Word CategorizationIImII|IIIIIIIImIImmIimIIII////lI//II/D(wl,w2)= ~ I Nw, i-Nw=i l+  Ni~-Niw= Il< i<n(4)Here n is the total number of words to be clus-tered, gwai is the number of times word couple(wt, wi) is observed in the corpus and Niwa is thenumber of times word couple (wi, wl) is observed.Obviously it is the same for word w2.
This dis-tahoe metric just calculates the total difference ontwo vector-couples obtained from the frequency ma-trix N, where the first couple denotes the vectors ob-tained by the frequencies of the word-couples formedby wl, w2 and their proceeding words.
The secondcouple denotes the vectors formed by the frequencieswith the preceding words correspondingly.The above formulation explains the structure ofthe distance metric used for the study.
For theresearched presented in our previous paper (Kork-maz&0~oluk, 1997) Manhattan Metric was the onlymetric used for the distance function.
However oth-ers axe proposed for the similarity between vectors.Another metric is the Euclidean Metric:/D(x,y)  = .\[  E (xi - y,)2 (5)V l<i<nHere x and y axe again two vectors defined over7~ n. Also the formulation of the angle between twovectors is also used for this study as a distance met-ric.
If 0 is the angle between the two vectors x andy, then cos 0 is calculated by:x 'y  El  <i<n xiyiCOS 0 = ~ = Ixllyl z.2?
2?
,_<.
, \] \](6)Here, x 'y  denote the scalar product of the twovectors x and y and I x \] denote the magnitude ofthe vector x.
Since the components of the vectorsin our case are corresponding to the frequencies ofwords, they will be non-negative.
So the angle be-tween the two vectors will be between 0 ?
and 90 ?.Since cos 0 ?
is unity and cos 90 ?
is zero, a distancemetric between the two vectors can be defined as:D(x ,y )  = 1 - cos0 (7)This distance metric will give us a number fromthe closed interval \[0, 1\], 0 denoting that the twovectors are overlapping and 1 denoting that thereis an angle of 90 ?
which is the highest differencebetween the vectors.The last distance metric used for the similarityfunction is the Spearman Rank Correlation Coeffi-cient.
This metric is based on the difference betweenthe ranks of two vectors rather than the differencebetween their elements.
The metric is defined as:D(x,y)  = Z (Rix - Ri~)2 (8)l< i<nHere x and y axe again two vectors as definedabove.
Ri z nd R/~ are the ranks of the correspond-ing vectors.
The rank is calculated for our case bynormalizing the vectors in the interval \[0,1\].
Thecomponent with the highest value among the com-ponents of the vector takes the value 1 and if thereaxe n elements in the vector, the one with the secondhighest value will correspond to the number 1 - ( l /n )and so on.
The smallest value will correspond tozero.For the process of formulating the distance be-tween linguistic elements, the main problem appearsdue to the difference between the frequencies ofwords from the same linguistic category.
For in-stance the word go has a very high frequency innatural anguage corpora compared to many otherverbs, but still we have to cluster go with low fre-quency verbs.
However if we use a distance met-ric based on only the absolute differences of vectorslike the Euclidean Metric or Manhattan Metric, thedistance calculated between high frequency and lowfrequency words would be high, which is undesired.Therefore when comparing a high frequency wordwith a low frequency one, we should be able to de-termine if the difference is caused by some regularmagnitude difference.
A similarity can exist betweenth e corresponding values when this magnitude differ-ence is discarded.
Without having a distance func-tion that compensates for this, it is not possible toovercome the errors introduced by having differentfrequencies for words from the same linguistic cate-gory.
This acts as a considerable factor disturbingthe quality of formed clusters.Having this in mind the Spearman Rank Corre-lation Coefficient Metric and the Angle Metric areused as distance function.
These two seem to dis-card the magnitude difference between the compo-nents of the vectors.
Such a comparison seems tobe more suitable for evaluating the similarity of lin-guistic elements.In the Spearman Rank Correlation Coefficient thevectors are normalized into the closed interval \[0,1\].So the vectors are similar if the change from onecomponent to the next is similar, regardless of thedifference in the absolute values.
We have a similarcomparison for the Angle Metric.
When this metricKorkmaz and GOlaiirk @oluk 115 Choosing A Distance Metric for Word CategorizationiIIiiiiIISpearmanTest  Cr i ter ia  Manhat tan  Ang le  Euc l idean Rank  CombinedMetr ic  Metr ic  Met r i c  Corre lat ion Metr icCoe/~e ientof initial clusters 60 169 132 185 171# of elm.
in theinitial clusters 16.6 5.9 7.56 5.4 5.8Depth of the tree 85Zaand6th level$97tnand8thlevel5i19tnandlOShlevels Location ofleaves 3thlevel#of  nodes on the 18 39 35 41 37second levelI19ta andlOthlevelsTable 2: Comparison of cluster hierarchies obtained with different metrics.disappeared.
We were able to get an initial successrate of about 90% with the Manhattan Metric whenwe discarded this large faulty cluster.
However withthe other metrics this success rate has been obtainedfor all the lexicon space.The second problem encountered for the catego-rization process appears while combining the initialclusters into larger ones?
Although it is possible toobtain some local successful combinations with thefirst metric, the overall performance in combiningthese initial clusters is not so satisfactory.
So differ-ent metrics presented in section 4 have been testedon the algorithm.
Unfortunately, although the pro-posed metrics were able to overcome the first prob-lem of having a large faulty cluster, the progress ob-tained in combining initial clusters into larger oneswas not so significant.
This has been the factor trig-gering the idea that a metric taking into considera-tion both of the approaches for linguistic similaritywould be more suitable for our case.
So the fifthmetric, the Combined Metric, has been constructed.The main progress obtained with this fifth metric ison the second problem described.In table 2 the hierarchies obtained using differ-ent metrics are presented.
When the propertiespresented in this table are examined, the hierarchyformed by the Manhattan Metric has the minimumnumber of initial clusters.
This is due to the largefaulty cluster formed with this metric.
The proper-ties of the hierarchies presented in table 2 seem tobe similar to each other.
Only the depth of the treeformed with the Angle Metric differs from the otherones.
This is because more initial clusters are com-bined on the second level in the hierarchy obtainedwith this metric.
This brings in an increase in thenumber of ill-structured clusters on the second levelover-combining distinct linguistic categories.5.1 Empir ica l  Compar i sonThe main progress for the clustering hierarchy is ob-tained by the Combined Metric.
It seems uitable toexamine this metric in detail and compare the re-sults with the initial organization obtained by theManhattan Metric.Some linguistic categories inferred by the algo-rithm using the Combined Metric are listed below:?
professor oppos i te  church hall l eas t  p resent  once  las tbaby prisoner doctor wind gate village sun country?
earth forest garden truth river?
picture case glass?
captain servant book horse meeting situation circumstancessummer  afternoon evening night morning day future early?
large new small great very strange certain good fine fewlittle?
slight man 's  sudden thousand hundred different?
rich fair secret blue soft cold bright quick frightened sur-prised plain clear true greater worse better tall dead livingwrong?
notice cry hold touch influence act account form effect care?
meant ought  wanted used enough back  began tried turnedcame?
enter pass follow carry call give bring tell do let forgive?
impossible possible necessary?
calm pale warm simple sweet quiet busy hot angry ill?
aunt uncle sister husband's?
duty attention desire turning coming  close?
listening ready trying going?
died fallen drawn learned written gone?
known taken brought given?
shoulders neck pocket hat chair shoulder arm mouth?
person girl lady woman gentleman man fellow else thing?
affairs age speech act ion marr iage  questions ideas lookssi lence society love exper ience?
between towards upon  aga ins t  a f ter  before like about  roundoff away up?
under  into through on at  over?
dur ing  near toward bes ide wi th in  around behind gave  to ldtook?
shall should may will must  would might  i?
won ' t  cannot  can can ' t  are d idn ' t  don ' tKorkmaz and G6ktark Ugoluk 117 Choosing,4 Distance Metric for Word CategorizationJ Combined  Manhat tanMetr ic  Metr icNounsLargest  ~ of  words  94co!
!~eedS, ,~qs  Rate  91.5%# of  in i t ia l  15clusters connectedVerbs (present perfect~11194.6%6- - Largest  ~ of  words 67collectedSuccess  Rate  100%of  in i t ia l  12c lusters  connectedVerl~s (past perfect)Largest ~ o f  words 16co l lectedSuccess Rate  100%~# of  in i t ia l  5clusters connected4573.3%2100%Larlgest ~ o f  wordscollectedof  in i t ia lc lusters  connected68 1792.6% 100%7Largest ~ o f  wordsco l lectedS , ,~x~ Rateo f  in i t ia lc lus ters  connectedAdverbs9 4100% 100%1 1LargeSt  ~ of  wordscol lectedSuccess  Rateof  in i t ia lclusters connectedI, ar~est ~ o f  wordsen l l *c tedS~MS Rat?of  in i t ia lc lus ters  connectedAuxiliaries7 9100% 100%1 1~)etecmlners16 10100% 100%1 1Table 3: Comparison made between Combined Met-ric and Manhattan Metric based on the largest num-ber of elements combined in a cluster.?
anybody everyone nobody everybody everything?
exactly finding hearing watching all leaving seeing givingkeeping knowing?
those these our an a this his their the your my her any nosome not such itsThe ill-placed members in the clusters above areshown using bold font.
The  above initial clustersrepresent the linguistic categories with a success rateof 90.2%.
Also the plural nouns in singular nounclusters are shown in italics.
If we consider thoseplacements as faulty ones also, the calculated suc-cess rate would fall to 88.1%.
This success rate seemsto be similar to the results obtained with other dis-tance metrics.
However as explained above the mainprogress obtained with this Combined Metric is onthe process of combining these initial clusters intolarger ones in the upper levels of the duster hierar-chy.Two examples from the cluster hierarchy obtainedwith this metric are given in tables 4 and 5.
In ta-ble 4 94 nouns coming from different initial clustersare combined in the same part of the cluster hier-archy.
Only one cluster seems to be misplaced inthis region.
This is an adjective cluster.
In table 567 different verbs are collected.
They are all presenttense verbs and no misplaced word exists in tiffs partof the hierarchy.
This is another well-formed part ofthe cluster organization.
It is believed that this is animportant improvement compared to earlier esults,since there is an increase in the number of success-fully connected initial clusters.Table 3 exhibits the improvement obtained usingthe Combined Metric.
Maximum number of wordscorrectly classified for some linguistic categories areshown in this table.
Obviously there are other clus-ters having dements from the same linguistic cate-gories in different parts of the hierarchy.
This tablemakes a comparison of the maximum numbers ofwords successfully collected in order to analyze theimprovement obtained.
Gathering nouns and auzil-iaries seems to be carried out better with the Man-hattan Metric.
However if we consider the numberof initial clusters forming these largest ones, a sig-nilicant progress seems to exist for the CombinedMetric.
There is a big difference for these num-bers between the two.
For instance 12 present per-fect verb classes are combined successfully when theCombined Metric is used, but only 8 of them werecombined with the Manhattan Metric.
For adjec-tives this is 7 to 2, for past perfect verbs 5 to 1 andalthough number of nouns collected by the Manhat-tan Metric is larger, number of initial clusters ue-cessfully combined by the Combined Metric is stilllarger.It can be claimed that there is a significantprogress in the process of successfully combining theinitial clusters when the new metric is used.
Thiswas the main problem encountered with the Man-hat'tan Metric and the other ones.
This is denotedas the progress obtained by using the Combined Met-ric trying to represent both of the two approachesthat can be taken into account for the similarity oflinguistic dements.6 Discussion And ConclusionThis research has focussed on the usage of distancefunction for an unsupervised, bottom-up algorithmfor automatic word categorization.
The results ob-tained seem to show that natural language preservesthe necessary information implicitly for the acquisi-tion of the linguistic categories it has.
A convergenceof linguistic categories could be obtained by usingthe algorithm we have presented.
This result is amotivating one for further studies on acquisition ofKorkmaz and G6ktark ~19oluk 118 Choosing A Distance Metric for Word CategorizationIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIbbbbbbb bbbcwater book, horsefamily meeting, situationbbbbb bbbbc children early, summer~ money afternoon, evening~ ~ night, morning~ ~  .day, futuresubject sight middle line direction~ .
~ ~ s tmatter end point bbbbcb bbbbcc ?
rest cause question sound story ~ / \/ / / ~.nf...2. h=l\[ // / bank, steps/ wal~, ladiesworld best least present e earth / streets, fire \' forest city floor,horses ?
pictur same room most once, last garden l ight darkness, path casefirst house whole baby, prisoner truth crowd court, watch glassdoor old doctor, wind river drawing, factother gate, village scene, newssun, country windows, sickTable 4: Part of the cluster hierarchy holding nounsbcbcbbcbcbbtake send carrymake call keep marry savebcbcbcbcbcbcb bcbcbcc getbe paygive help meet ~?b~ng leave 7/  J /  tell do find /#  / / /let see / / / /  / /  / ,o- y .
// 2~app en / / // / stop / / I wait / / =: / /  /write .
.
/ / =~; / dAk l;e ./ / begin / die change conmdercome talk answer stay play fall forgetgo speak return live fight stand understandtry sleep walk sayorder bed sit ?
imagineregardrunturndriveseemTable 5: Part of the cluster hierarchy holding present tense verbsKorkmaz and G6ktfirk ~/?oluk 119 Choosing A Distance Metn'c for Word Categorizationstructures preserved in natural anguage at variousabstraction levels.Different distance metrics are used for the algo-rithm.
The results obtained by the Combined Met-ric show that special distance metrics trying to com-bine different properties of linguistic elements couldbe developed for linguistic categorization.Considering the results obtained by the experi-ments carried out, the following remarks could bemade on the linguistic clusters formed in the study.In the initial clusters formed the success rate ob-tained is satisfactory.
Though it was not possible toto combine these initial clusters into exact linguisticcategories, the cluster hierarchy obtained with Com-bined metric is encouraging.
The faulty placementsaxe mainly due to the the very complex structureof natural anguage.
The fact that many words canbe used with different linguistic roles in natural an-guage sentences produces deviations in the informa-tion given by the bigrams.
Using fuzzy logic and asuitable distance metric is a way to decrease thesedeviations, however it was not possible to removethem totally.Korkmaz, E. E. and G. U~oluk A Method ForImproving Automatic Word Categorization.
Pro-ceedings of the Workshop on ComputationalNatural Language Learning.
(Conl197).
Madrid,Spain.
pp.
43-49, 1997.Lankhorst, M.M.
A Genetic Algorithm for Auto-matic Word Categorization.
In: E. Backer (ed.
),Proceedings of Computing Science in the Nether-lands CSN'94, SION, 1994, pp.
171-182.McMahon, John G. and Francis J. Smith.
Improv-ing Statistical Language Model Performance withAutomatically Generated Word Hierarchies.
Com-putational Linguistics, 22(2):217-247,1996.Wilms, G. J.
Automated Induction of a Lexical Sub-language Grammar Using a Hybrid System of Cor-pus and Knowledge-Based Techniques.
MississippiState University.
PhD Thesis, 1995.Zipf, G.K.
The psycho-biology of Language.
Boston:Houghton Mifflin.
1935ReferencesBrown P.F., V.J.
Della Pietra, P.V.
deSouza, J.C.Lai, and R.L.
Mercer.
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4):467-477, 1992de Marcken, Carl G. Unsupervised Language Acqui-sition.
Phi) Thesis, Department of Electrical En-gineering and Computer Science, MassachusettaInstitute of Technology, 1996.Finch, S. Finding Structure in language.
PhD The-sis.
Centre for Cognitive Science, University of Ed-inburgh, 1993.Finch, S. and N. Chater, Automatic methods forfinding linguistic ategories.
In Igor Alexander andJohn Taylor, editors, Artificial Neural Networks,volume 2.
Elsevier Science Publishers, 1992.Gath, I and A.B.
Geva Unsupervised Optimal FuzzyClustering.
IEEE Transactions on pattern analy-sis and machine intelligence, Vol.
11, No.
7, July1989.Knight, Kevin, Ishwar Chander, Haines Matthew,Hatzivassiloglou Vasieios, Hovy Eduard, IidaMasayo, Luk Steve, Okumura Akitoshi, WhitneyRichard, Yamada Kenji.
Intagrating KnowledgeBases and Statistics in MT.
Proceedings of the1st AMTA Conference.
Columbia, MD.
1994.Korlcmaz and G61aiirk (l?oluk 120 Choosing.4 Distance Metric for Word CategorizationIIIIIIIIIIIll
