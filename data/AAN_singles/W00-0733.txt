In: Proceedings of CoNLL-2000 and LLL-2000, pages 151-153, Lisbon, Portugal, 2000.Text Chunking by System CombinationErik F. Tjong Kim SangCNTS - Language Technology GroupUniversity of Antwerper ik t@uia ,  ua.
ac.
be1 In t roduct ionWe will apply a system-internal combinationof memory-based learning classifiers to theCoNLL-2000 shared task: finding base chunks.Apart from testing different combination meth-ods, we will also examine if dividing the chunk-ing process in a boundary recognition phase anda type identification phase would aid perfor-mance.2 ApproachTjong Kim Sang (2000) describes how a system-internal combination of memory-based learnerscan be used for base noun phrase (baseNP)recognition.
The idea is to generate differentchunking models by using different chunk rep-resentations.
Chunks can be represented withbracket structures but alternatively one can usea tagging representation which classifies wordsas being inside a chunk (I), outside a chunk(O) or at a chunk boundary (B) (Ramshaw andMarcus, 1995).
There are four variants of thisrepresentation.
The B tags can be used for thefirst word of chunks that immediately follow an-other chunk (the IOB1 representation) or theycan be used for every chunk-initial word (IOB2).Alternatively an E tag can be used for labelingthe final word of a chunk immediately preced-ing another chunk (IOE1) or it can be used forevery chunk-final word (IOE2).
Bracket struc-tures can also be represented as tagging struc-tures by using two streams of tags which de-fine whether words start a chunk or not (O)or whether words are at the end of a chunk ornot (C).
We need both for encoding the phrasestructure and hence we will treat the two tagstreams as a single representation (O+C).
Acombination of baseNP classifiers that use thefive representation performs better than any ofthe included systems (Tjong Kim Sang, 2000).We will apply such a classifier combination tothe CoNLL-2000 shared task.The individual classifiers will use thememory-based learning algorithm IBi-IG(Daelemans et al, 1999) for determiningthe most probable tag for each word.
Inmemory-based learning the training data isstored and a new item is classified by the mostfrequent classification among training itemswhich are closest o this new item.
Data itemsare represented as sets of feature-value pairs.Features receive weights which are based onthe amount of information they provide forclassifying the training data (Daelemans et al,1999).We will evaluate nine different methods forcombining the output of our five chunkers (VanHalteren et al, 1998).
Five are so-called votingmethods.
They assign weights to the output ofthe individual systems and use these weights todetermine the most probable output tag.
Sincethe classifiers generate different output formats,all classifier output has been converted to theO and the C representations.
The most sim-ple voting method assigns uniform weights andpicks the tag that occurs most often (Majority).A more advanced method is to use as a weightthe accuracy of the classifier on some held-outpart of the training data, the tuning data (Tot-Precision).
One can also use the precision ob-tained by a classifier for a specific output valueas a weight (TagPrecision).
Alternatively, weuse as a weight a combination of the precisionscore for the output tag in combination withthe recall score for competing tags (Precision-Recall).
The most advanced voting method ex-amines output values of pairs of classifiers andassigns weights to tags based on how often theyappear with this pair in the tuning data (Tag-Pair, Van Halteren et al, (1998)).151Apart from these voting methods we have alsoapplied two memory-based learners t;o the out-put of the five chunkers: IBI-IG and IGTREE, adecision tree variant of IBI-IG (Daelemans etal., 1999).
This approach is called classifierstacking.
Like with the voting algorithms, wehave tested these meta-classifiers with the out-put of the first classification stage.
Unlike thevoting algorithms, the classifiers do not requirea uniform input.
Therefore we have tested iftheir performance can be improved by supply-ing them with information about the input ofthe first classification stage.
For this purposewe have used the part-of-speech tag of the cur-rent word as compressed representation f thefirst stage input (Van Halteren et al, 1998).The combination methods will generate a listof open brackets and a list of close brackets.
Wehave converted these to phrases by only usingbrackets which could be matched with the clos-est matching candidate and ignoring the others.For example, in the structure \[NP a \[NP b \]gg\[VP c \]pg d \]vg, we would accept \[NP b \]NPas a noun phrase and ignore all other bracketssince they cannot be matched with their clos-est candidate for a pair, either because of typeinconsistencies or because there was some otherbracket in between them.We will examine three processing strategiesin order to test our hypothesis that chunkingperformance can be increased by making a dis-tinction between finding chunk boundaries andidentifying chunk types.
The first is the single-pass method.
Here each individual classifier at-tempts to find the correct chunk tag for eachword in one step.
A variant of this is the double-pass method.
It processes the data twice: firstit searches for chunks boundaries and then itattempts to identify the types of the chunksfound.
The third processing method is the n-pass method.
It contains as many passes asthere are different chunk types.
In each pass,it attempts to find chunks of a single type.
Incase a word is classified as belonging to morethan one chunk type, preference will be givento the chunk type that occurs most often in thetraining data.
We expect he n-pass method tooutperform the other two methods.
However,we are not sure if the performance differencewill be large enough to compensate for the extracomputation that is required for this processingmethod.3 Resu l tsIn order to find out which of the three process-ing methods and which of the nine combinationmethods performs best, we have applied themto the training data of the CoNLL-2000 sharedtask (Tjong Kim Sang and Buchholz, 2000) in a10-fold cross-validation experiment (Weiss andKulikowski, 1991).
For the single-pass method,we trained IBI-IG classifiers to produce the mostlikely output tags for the five data representa-tions.
In the input of the classifiers a word wasrepresented as itself, its part-of-speech tag anda context of four left and four right word/part-of-speech tag pairs.
For the four IO represen-tations we used a second phase with a lim-ited input context (3) but with additionally thetwo previous and the two next chunk tags pre-dicted by the first phase.
The classifier out-put was converted to the O representation (openbrackets) and the C representation (close brack-ets) and the results were combined with thenine combination methods.
In the double-passmethod finding the most likely tag for each wordwas split in finding chunk boundaries and as-signing types to the chunks.
The n-pass methoddivided this process into eleven passes each ofwhich recognized one chunk type.For each processing strategy, all combinationresults were better than those obtained with thefive individual classifiers.
The differences be-tween combination results within each process-ing strategy were small and between the threestrategies the best results were not far apart:the best FZ=i rates were 92.40 (single-pass),92.35 (double-pass) and 92.75 (n-pass).Since the three processing methods reach asimilar performances, we can choose any ofthem for our remaining experiments.
The n-pass method performed best but it has thedisadvantage of needing as many passes asthere are chunk types.
This will require alot of computation.
The single-pass methodwas second-best but in order to obtain goodresults with this method, we would need touse a stacked classifier because those performedbetter (F~=1=92.40) than the voting methods(Fz=1=91.98).
This stacked classifier equirespreprocessed combinator training data whichcan be obtained by processing the original train-152ing data with 10-fold cross-validation.
Againthis will require a lot of work for new data sets.We have chosen for the double-pass methodbecause in this processing strategy it is possi-ble to obtain good results with majority vot-ing.
The advantage of using majority voting isthat it does not require extra preprocessed com-binator training data so by using it we avoidthe extra computation required for generatingthis data.
We have applied the double-passmethod with majority voting to the CoNLL-2000 test data while using the complete train-ing data.
The results can be found in table 1.The recognition method performs well for themost frequently occurring chunk types (NP, VPand PP) and worse for the other seven (the testdata did not contain UCP chunks).
The recog-nition rate for NP chunks (F~=1=93.23) is closeto the result for a related standard baseNP dataset obtained by Tjong Kim Sang (2000) (93.26).Our method outperforms the results mentionedin Buchholz et al (1999) in four of the fivecases (ADJP, NP, PP and VP); only for ADVPchunks it performs lightly worse.
This is sur-prising given that Buchholz et al (1999) used956696 tokens of training data and we have usedonly 211727 (78% less).4 Conc lud ing  RemarksWe have evaluated three methods for recogniz-ing non-recursive non-overlapping text chunksof arbitrary syntactical categories.
In eachmethod a memory-based learner was trainedto recognize chunks represented in five differ-ent ways.
We have examined nine differentmethods for combining the five results.
A 10-fold cross-validation experiment on the train-ing data of the CoNLL-2000 shared task re-vealed that (1) the combined results were betterthan the individual results, (2) the combinationmethods perform equally well and (3) the bestperformances of the three processing methodswere similar.
We have selected the double-passmethod with majority voting for processing theCoNLL-2000 shared task data.
This methodoutperformed an earlier text chunking study formost chunk types, despite the fact that it usedabout 80% less training data.Re ferencesSabine Buchholz, Jorn Veenstra, and Walter Daele-mans.
1999.
Cascaded grammatical relation as-test dataADJPADVPCONJPINTJLSTNPPPPRTSBARVPallprecision85.25%85.03%42.86%100.00%0.00%94.14%96.45%79.49%89.81%93.97%94.04%recall59.36%71.48%33.33%50.00%0.00%92.34%96.59%58.49%72.52%91.35%91.00%Ffl=l69.9977.6737.5066.670.0093.2396.5267.3980.2592.6492.50Table h The results per chunk type of process-ing the test data with the double-pass methodand majority voting.
Our method outper-forms most chunk type results mention in Buch-holz et al (1999) (FAD jR=66.7, FADVp=77.9FNp=92.3, Fpp=96.8, FNp=91.8) despite thefact that we have used about 80% less trainingdata.signment.
In Proceedings o\] EMNLP/VLC-99.Association for Computational Linguistics.Walter Daelemans, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
1999.
TiMBL:Tilburg Memory Based Learner, version 2.0,Reference Guide.
ILK Technical Report 99-01.http://ilk.kub.nl/.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learn-ing.
In Proceedings of the Third A CL Workshopon Very Large Corpora.
Association for Compu-tational Linguistics.Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 sharedtask: Chunking.
In Proceedings of the CoNLL-2000.
Association for Computational Linguistics.Erik F. Tjong Kim Sang.
2000.
Noun phrase recog-nition by system combination.
In Proceedings ofthe ANLP-NAACL 2000.
Seattle, Washington,USA.
Morgan Kaufman Publishers.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
1998.
Improving data driven wordclasstagging by system combination.
In Proceedingsof COLING-ACL '98.
Association for Computa-tional Linguistics.Sholom M. Weiss and Casimir A. Kulikowski.
1991.Computer Systems That Learn: Classification andPrediction Methods ~rom Statistics, Neural Nets,Machine Learning and Expert Systems.
MorganKaufmann.153
