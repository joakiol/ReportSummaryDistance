Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 700?706,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMachine Comprehension with Syntax, Frames, and SemanticsHai Wang Mohit Bansal Kevin Gimpel David McAllesterToyota Technological Institute at Chicago, Chicago, IL, 60637, USA{haiwang,mbansal,kgimpel,mcallester}@ttic.eduAbstractWe demonstrate significant improvementon the MCTest question answering task(Richardson et al., 2013) by augmentingbaseline features with features based onsyntax, frame semantics, coreference, andword embeddings, and combining them ina max-margin learning framework.
Weachieve the best results we are aware of onthis dataset, outperforming concurrently-published results.
These results demon-strate a significant performance gradientfor the use of linguistic structure in ma-chine comprehension.1 IntroductionRecent question answering (QA) systems (Fer-rucci et al., 2010; Berant et al., 2013; Bordes etal., 2014) have focused on open-domain factoidquestions, relying on knowledge bases like Free-base (Bollacker et al., 2008) or large corpora ofunstructured text.
While clearly useful, this typeof QA may not be the best way to evaluate natu-ral language understanding capability.
Due to theredundancy of facts expressed on the web, manyquestions are answerable with shallow techniquesfrom information extraction (Yao et al., 2014).There is also recent work on QA based on syn-thetic text describing events inadventure games (Weston et al., 2015;Sukhbaatar et al., 2015).
Synthetic text providesa cleanroom environment for evaluating QAsystems, and has spurred development of power-ful neural architectures for complex reasoning.However, the formulaic semantics underlyingthese synthetic texts allows for the constructionof perfect rule-based question answering sys-tems, and may not reflect the patterns of naturallinguistic expression.In this paper, we focus on machine compre-hension, which is QA in which the answer is con-tained within a provided passage.
Several compre-hension tasks have been developed, including Re-media (Hirschman et al., 1999), CBC4kids (Brecket al., 2001), and the QA4MRE textual questionanswering tasks in the CLEF evaluations (Pe?nas etal., 2011; Pe?nas et al., 2013; Clark et al., 2012;Bhaskar et al., 2012).We consider the Machine Comprehension ofText dataset (MCTest; Richardson et al., 2013),a set of human-authored fictional stories with as-sociated multiple-choice questions.
Knowledgebases and web corpora are not useful for this task,and answers are typically expressed just once ineach story.
While simple baselines presented byRichardson et al.
answer over 60% of questionscorrectly, many of the remaining questions requiredeeper analysis.In this paper, we explore the use of depen-dency syntax, frame semantics, word embeddings,and coreference for improving performance onMCTest.
Syntax, frame semantics, and coref-erence are essential for understanding who didwhat to whom.
Word embeddings address varia-tion in word choice between the stories and ques-tions.
Our added features achieve the best resultswe are aware of on this dataset, outperformingconcurrently-published results (Narasimhan andBarzilay, 2015; Sachan et al., 2015).2 ModelWe use a simple latent-variable classifier trainedwith a max-margin criterion.
Let P denote thepassage, q denote the question of interest, and Adenote the set of candidate answers for q, whereeach a ?
A denotes one candidate answer.
Wewant to learn a function h : (P, q)?
A that, givena passage and a question, outputs a legal a ?
A.We use a linear model for h that uses a latent vari-able w to identify the sentence in the passage inwhich the answer can be found.Let W denote the set of sentences within the700passage, where a particular w ?
W denotes onesentence.Given a feature vector f(P,w, q, a) and aweight vector ?
with an entry for each feature, theprediction a?
for a new P and q is given by:a?
= arg maxa?Amaxw?W?>f(P,w, q, a)Given triples {?Pi, qi, ai?
}ni=1, we minimize an`2-regularized max-margin loss function:min?
?||?||2+n?i=1{?maxw?W?>f(Pi, w, qi, ai)+ maxa?A{maxw?
?W?>f(Pi, w?, qi, a) + ?
(a, ai)}}where ?
is the weight of the `2term and?
(a, ai) = 1 if a 6= aiand 0 otherwise.
The latentvariable w makes the loss function non-convex.3 FeaturesWe start with two features from Richardson et al.(2013).
Our first feature corresponds to their slid-ing window similarity baseline, which measuresweighted word overlap between the bag of wordsconstructed from the question/answer and the bagof words in the window.
We call this feature B.The second feature corresponds to their word dis-tance baseline, and is the minimal distance be-tween two word occurrences in the passage thatare also contained in the question/answer pair.
Wecall this feature D. Space does not permit a de-tailed description.3.1 Frame Semantic FeaturesFrame semantic parsing (Das et al., 2014)is the problem of extracting frame-specificpredicate-argument structures from sentences,where the frames come from an inventory such asFrameNet (Baker et al., 1998).
This task can bedecomposed into three subproblems: target iden-tification, in which frame-evoking predicates aremarked; frame label identification, in which theevoked frame is selected for each predicate; andargument identification, in which arguments toeach frame are identified and labeled with a rolefrom the frame.
An example output of the SE-MAFOR frame semantic parser (Das et al., 2014)is given in Figure 1.Three frames are identified.
The target wordspulled, all, and shelves have respective frame la-bels CAUSE MOTION, QUANTITY, and NATU-Figure 1: Example output from SEMAFOR.RAL FEATURES.
Each frame has its own set of ar-guments; e.g., the CAUSE MOTION frame has thelabeled Agent, Theme, and Goal arguments.
Fea-tures from these parses have been shown to be use-ful for NLP tasks such as slot filling in spoken dia-logue systems (Chen et al., 2013).
We expect thatthe passage sentence containing the answer willoverlap with the question and correct answer interms of predicates, frames evoked, and predictedargument labels, and we design features to capturethis intuition.
Given the frame semantic parse for asentence, let T be the bag of frame-evoking targetwords/phrases.1We define the bag of frame labelsin the parse as F .
For each target t ?
T , there is anassociated frame label denoted Ft?
F .
Let R bethe bag of phrases assigned with an argument labelin the parse.
We denote the bag of argument labelsin the parse by L. For each phrase r ?
R, there isan argument label denoted Lr?
L. We define aframe semantic parse as a tuple ?T, F,R, L?.
Wedefine six features based on two parsed sentences?T1, F1, R1, L1?
and ?T2, F2, R2, L2?:?
f1: # frame label matches: |{?s, t?
: s ?F1, t ?
F2, s = t}|?
f2: # argument label matches: |{?s, t?
: s ?L1, t ?
L2, s = t}|.?
f3: # target matches, ignoring frame labels:|{?s, t?
: s ?
T1, t ?
T2, s = t}|.?
f4: # argument matches, ignoring arg.
labels:|{?s, t?
: s ?
R1, t ?
R2, s = t}|.?
f5: # target matches, using frame labels:|{?s, t?
: s ?
T1, t ?
T2, s = t, F1s= F2t}|.?
f6: # argument matches, using arg.
labels:|{?s, t?
: s ?
R1, t ?
R2, s = t, L1s= L2t}|.We use two versions of each of these six features:one version for the passage sentence w and thequestion q, and an additional version for w and thecandidate answer a.3.2 Syntactic FeaturesIf two sentences refer to the same event, then it islikely that they have some overlapping dependen-1By bag, we mean here a set with possible replicates.701Figure 2: Transforming the question to a statement.cies.
To compare a Q/A pair to a sentence in thepassage, we first use rules to transform the ques-tion into a statement and insert the candidate an-swer into the trace position.
Our simple rule setis inspired by the rich history of QA research intomodeling syntactic transformations between ques-tions and answers (Moschitti et al., 2007; Wang etal., 2007; Heilman and Smith, 2010).
Given Stan-ford dependency tree and part-of-speech (POS)tags for the question, let arc(u, v) be the label ofthe dependency between child word u and headword v, let POS (u) be the POS tag of u, let c bethe wh-word in the question, let r be the root wordin the question?s dependency tree, and let a be thecandidate answer.
We use the following rules:2?
c = what, POS (r) = VB, and arc(c, r) = dobj.Insert a after word u where arc(u, r) = nsubj.Delete c and the word after c.?
c = what, POS (r) = NN, and arc(c, r) =nsubj.
Replace c by a.?
c = where, POS (r) = VB, and arc(c, r) = ad-vmod.
Delete c and the word after c. If r has achild u such that arc(u, r) = dobj, insert a afteru; else, insert a after r and delete r.?
c = where, r = is, POS(r) = VBZ, and arc(c,r) = advmod.
Delete c. Find r?s child u suchthat arc(u, r) = nsubj, move r to be right afteru.
Insert a after r.?
c = who, POS(r) = NN, and arc(c, r) =nsubj.
Replace c by a.?
c = who, POS(r) ?
{VB, VBD}, and arc(c, r)= nsubj.
Replace c by a.We use other rules in addition to those above:change ?why x??
to ?the reason x is a?, andchange ?how many x?, ?how much x?, or ?whenx?
to ?x a?.Given each candidate answer, we attempt totransform the question to a statement using the2There are existing rule-based approaches to transformingstatements to questions (Heilman, 2011); our rules reversethis process.rules above.3An example of the transformationis given in Figure 2.
In the parse, pull is the rootword and What is attached as a dobj.
This matchesthe first rule, so we delete did and insert the can-didate answer pudding after pull, making the finaltransformed sentence: James pull pudding off.After this transformation of the question (anda candidate answer) to a statement, we mea-sure its similarity to the sentence in the windowusing simple dependency-based similarity fea-tures.
Denoting a dependency as (u, v, arc(u, v)),then two dependencies (u1, v1, arc(u1, v1)) and(u2, v2, arc(u2, v2)) match if and only if u1= u2,v1= v2, and arc(u1, v1) = arc(u2, v2).
Onefeature simply counts the number of dependencymatches between the transformed question and thepassage sentence.
We include three additionalcount features that each consider a subset of de-pendencies from the following three categories:(1) v = r and u = a; (2) v = r but u 6= a; and(3) v 6= r. In Figure 2, the triples(James, pull,nsubj) and (off, pull,prt) belong tothe second category while (pudding, pull,dobj)belongs to the first.3.3 Word EmbeddingsWord embeddings (Mikolov et al., 2013) repre-sent each word as a low-dimensional vector wherethe similarity of vectors captures some aspect ofsemantic similarity of words.
They have beenused for many tasks, including semantic role label-ing (Collobert et al., 2011), named entity recogni-tion (Turian et al., 2010), parsing (Bansal et al.,2014), and for the Facebook QA tasks (Weston etal., 2015; Sukhbaatar et al., 2015).
We first de-fine the vector f+was the vector summation of allwords inside sentence w and f?was the element-wise multiplication of the vectors in w. To definevectors for answer a for question q, we concate-nate q and a, then calculate f+qaand f?qa.
For thebag-of-words feature B, instead of merely count-ing matches of the two bags of words, we also usecos(f+qa, f+w) and cos(f?qa, f?w) as features, wherecos is cosine similarity.
For syntactic features,where ?wis the bag of dependencies of w and?qais the bag of dependencies for the transformedquestion for candidate answer a, we use a featurefunction that returns the following:?(u,v,`)??w?(u?,v?,`?)?
?qa1`=`?cos(u, u?)
cos(v, v?
)3If no rule applies, we return 0 for all syntactic features.702where ` is short for arc(u, v).43.4 Coreference ResolutionCoreference resolution systems aim to identifychains of mentions (within and across sentences)that refer to the same entity.
We integrate coref-erence information into the bag-of-words, framesemantic, and syntactic features.
We run a coref-erence resolution system on each passage, then forthese three sets of features, we replace exact stringmatch with a check for membership in the samecoreference chain.When using features augmented by word em-beddings or coreference, we create new versionsof the features that use the new information, con-catenating them with the original features.4 ExperimentsMCTest splits its stories into train, development,and test sets.
The original MCtest DEV is toosmall, to choose the best feature set, we mergedthe train and development sets in MC160 andMC500 and split them randomly into a 250-storytraining set (TRAIN) and a 200-story developmentset (DEV).
We optimize the max-margin trainingcriteria on TRAIN and use DEV to tune the regular-izer ?
and choose the best feature set.
We reportfinal performance on the original two test sets (forcomparability) from MCTest, named MC160 andMC500.We use SEMAFOR (Das et al., 2010; Das etal., 2014) for frame semantic parsing and the lat-est Stanford dependency parser (Chen and Man-ning, 2014) as our dependency parser.
We usethe Stanford rule-based system for coreferenceresolution (Lee et al., 2013).
We use the pre-trained 300-dimensional word embeddings down-loadable from the word2vec site.5We denotethe frame semantic features by F and the syntac-tic features by S. We use superscriptswandctoindicate the use of embeddings and coreferencefor a particular feature set.
To minimize the loss,we use the miniFunc package in MATLAB withLBFGS (Nocedal, 1980; Liu and Nocedal, 1989).The accuracy of different feature sets on DEV isgiven in Table 1.6The boldface results correspond4Similar to the original syntactic features (see end of Sec-tion 3.2), we also have 3 additional features for the three sub-set categories.5https://code.google.com/p/word2vec/6All accuracies are computed with tie-breaking partialcredit (similar to previous work), i.e., if we have the sameto the best feature set combination chosen by eval-uating on DEV.
In this case, the feature dimen-sionality is 29, which includes 4 bag-of-words fea-tures, 1 distance feature, 12 frame semantic fea-tures, and with the remaining being syntactic fea-tures.
After choosing the best feature set on DEV,we then evaluate our system on TEST.Negations: in preliminary experiments, wefound that our system suffered with negation ques-tions, so we developed a simple heuristic to dealwith them.
We identify a question as negation if itcontains ?not?
or ?n?t?
and does not begin with?how?
or ?why?.
If a question is identified asnegation, we then negate the final score for eachcandidate answer.Features DEV Accuracy (%)B + D + F 64.18B + D + F + S 66.24Bwc+ D + Fc+ Swc69.87Table 1: Accuracy on DEV.The final test results are shown in Table 2.
Wefirst compare to results from prior work (Richard-son et al., 2013).
Their first result uses a slid-ing window with the bag-of-words feature B de-scribed in Sec.
3; this system is called ?Base-line 1?
(B1).
They then add the distance featureD, also described in Sec.
3.
The combined sys-tem, which uses B and D, is called ?Baseline 2?(B2).
Their third result adds a rich textual entail-ment system to B2; it is referred to as B2+RTE.7We also compare to concurrently-published re-sults (Narasimhan and Barzilay, 2015; Sachan etal., 2015).We report accuracies for all questions as wellas separately for the two types: those that areanswerable with a single sentence from the pas-sage (?Single?)
and those that require multiplesentences (?Multiple?).
We see gains in accuracyof 6% absolute compared to the B2+RTE base-line and also outperform concurrently-publishedresults (Narasimhan and Barzilay, 2015; Sachanet al., 2015).
Even though our system only ex-plicitly uses a single sentence from the passagewhen choosing an answer, we improve baselineaccuracy for both single-sentence and multiple-sentence questions.8score for all four candidate answers, then we get partial creditof 0.25 for this question.7These three results are obtained from files athttp://research.microsoft.com/en-us/um/redmond/projects/mctest/results.html.8However, we inspected these question annotations and703SystemMC160 MC500Single (112) Multiple (128) All Single (272) Multiple (328) AllB1 64.73 56.64 60.41 58.21 56.17 57.09Richardson et al.
(2013) B2 75.89 60.15 67.50 64.00 57.46 60.43B2+RTE 76.78 62.50 69.16 68.01 59.45 63.33Narasimhan and Barzilay (2015) 82.36 65.23 73.23 68.38 59.90 63.75Sachan et al.
(2015) - - - 67.65 67.99 67.83our system 84.22 67.85 75.27 72.05 67.94 69.94Table 2: Accuracy comparison of published results on test sets.Features DEV Accuracy (%)full (Bwc+D+Fc+Swc) 69.87?
Bwc(D + Fc+Swc) 58.46?
D (Bwc+Fc+Swc) 65.89?
Bwc, ?
D (Fc+Swc) 54.19?
embeddings (Bc+D+Fc+Sc) 68.28?
coreference (Bw+D+F+Sw) 68.43?
frame semantics (Bwc+D+Swc) 67.89?
syntax (Bwc+D+Fc) 67.64?
negation (Bwc+D+Fc+Swc) 68.72Table 3: Ablation study of feature types on the dev set.We also measure the contribution of each fea-ture set by deleting it from the full feature set.These ablation results are shown in Table 3.
Wefind that frame semantic and syntax features con-tribute almost equally, and using word embed-dings contributes slightly more than coreferenceinformation.
If we delete the bag-of-words anddistance features, then accuracy drops signifi-cantly, which suggests that in MCTest, simplesurface-level similarity features suffice to answera large portion of questions.5 AnalysisSuccesses To show the effects of different fea-tures, we show cases where the full system givesthe correct prediction (marked with ?)
but ablat-ing the named features causes the incorrect answer(marked with ?)
to be predicted:Ex.
1: effect of embeddings: we find the soft similaritybetween ?noodle?
and ?spaghetti?.clue: Marsha?s favorite dinner was spaghetti.q: What is Marsha?s noodle made out of?
?A) Spaghetti;?C) mom;Ex.
2: coreference resolves She to Hannah Harvey.Hannah Harvey was a ten year old.
She lived in New York.q: Where does Hannah Harvey live?
?A) New York; ?C)Kenya;Ex.
4: effect of syntax: by inserting answer C, the trans-formed statement is: Todd say there?s no place like homewhen he got home from the city.occasionally found them to be noisy, which may cloud thesecomparisons.When his mom asked him about his trip to the city Toddsaid, ?There?s no place like home.
?q: What did Todd say when he got home from the city?
?B)There were so many people in cars; ?C) There?s no placelike home;Errors To give insight into our system?s perfor-mance and reveal future research directions, wealso analyzed the errors made by our system.
Wefound that many required inferential reasoning,counting, set enumeration, multiple sentences,time manipulation, and comparisons.
Some ran-domly sampled examples are given below, with thecorrect answer starred (?):Ex.
1: requires inference across multiple sentences:One day Fritz got a splinter in his foot.
Stephen did notbelieve him.
Fritz showed him the picture.
Then Stephenbelieved him.
q: What made Stephen believe Fritz?
?A)the picture of the splinter in his foot; ?C) the picture of thecereal with milk;Ex.
2: requires temporal reasoning and world knowledge:Ashley woke up bright and early on Friday morning.
Herbirthday was only a day away.
q: What day of the week wasAshley?s birthday?
?A) Saturday; ?C) Friday;Ex.
3: requires comparative reasoning:Tommy has an old bicycle now.
He is getting too big forit.
q: What?s wrong with Tommy?s old bicycle?
?B) it?s toosmall; ?C) it?s old;6 ConclusionWe proposed several novel features for machinecomprehension, including those based on framesemantics, dependency syntax, word embeddings,and coreference resolution.
Empirical resultsdemonstrate substantial improvements over sev-eral strong baselines, achieving new state-of-the-art results on MCTest.
Our error analysis sug-gests that deeper linguistic analysis and inferentialreasoning can yield further improvements on thistask.AcknowledgmentsWe thank Dekang Lin, Nathan Schneider and theanonymous reviewers for helpful comments.704ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 17th International Conference on Com-putational Linguistics-Volume 1, pages 86?90.
As-sociation for Computational Linguistics.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers), pages809?815, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of EMNLP.Pinaki Bhaskar, Partha Pakray, Somnath Banerjee,Samadrita Banerjee, Sivaji Bandyopadhyay, andAlexander F Gelbukh.
2012.
Question answeringsystem for QA4MRE@CLEF 2012.
In CLEF (On-line Working Notes/Labs/Workshop).Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, pages 1247?1250.
ACM.Antoine Bordes, Sumit Chopra, and Jason Weston.2014.
Question answering with subgraph embed-dings.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), pages 615?620, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Eric Breck, Marc Light, Gideon S Mann, Ellen Riloff,Brianne Brown, Pranav Anand, Mats Rooth, andMichael Thelen.
2001.
Looking under the hood:Tools for diagnosing your question answering en-gine.
In Proceedings of the workshop on Open-domain question answering-Volume 12, pages 1?8.Association for Computational Linguistics.Danqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), pages 740?750, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Yun-Nung Chen, William Yang Wang, and Alexander IRudnicky.
2013.
Unsupervised induction and fill-ing of semantic slots for spoken dialogue systemsusing frame-semantic parsing.
In Automatic SpeechRecognition and Understanding (ASRU), 2013 IEEEWorkshop on, pages 120?125.
IEEE.Peter Clark, Philip Harrison, and Xuchen Yao.2012.
An entailment-based approach to theQA4MRE challenge.
In CLEF (Online WorkingNotes/Labs/Workshop).Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 948?956, Los Angeles, California,June.
Association for Computational Linguistics.Dipanjan Das, Desai Chen, Andr?e F. T. Martins,Nathan Schneider, and Noah A. Smith.
2014.Frame-semantic parsing.
Computational Linguis-tics, 40(1):9?56.David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya A Kalyanpur,Adam Lally, J William Murdock, Eric Nyberg, JohnPrager, et al.
2010.
Building Watson: An overviewof the DeepQA project.
AI magazine, 31(3):59?79.Michael Heilman and Noah A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 1011?1019,Los Angeles, California, June.
Association for Com-putational Linguistics.M.
Heilman.
2011.
Automatic factual question gener-ation from text.
Ph.D. thesis, Carnegie Mellon Uni-versity.Lynette Hirschman, Marc Light, Eric Breck, andJohn D Burger.
1999.
Deep read: A read-ing comprehension system.
In Proceedings of the37th annual meeting of the Association for Compu-tational Linguistics on Computational Linguistics,pages 325?332.
Association for Computational Lin-guistics.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Comput.
Linguist., 39(4):885?916, December.D.
C. Liu and J. Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Math.
Programming, 45:503?528.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their composition-ality.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 3111?3119.
Curran Associates, Inc.705Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for questionanswer classification.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 776?783, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.Karthik Narasimhan and Regina Barzilay.
2015.
Ma-chine comprehension with discourse relations.
In53rd Annual Meeting of the Association for Com-putational Linguistics.Jorge Nocedal.
1980.
Updating quasi-newton matriceswith limited storage.
Mathematics of Computation,35:773?782.Anselmo Pe?nas, Eduard H Hovy, Pamela Forner,?Alvaro Rodrigo, Richard Sutcliffe, Corina Forascu,and Caroline Sporleder.
2011.
Overview ofQA4MRE at CLEF 2011: Question answering formachine reading evaluation.
In CLEF (NotebookPapers/Labs/Workshop), pages 1?20.Anselmo Pe?nas, Eduard Hovy, Pamela Forner,?AlvaroRodrigo, Richard Sutcliffe, and Roser Morante.2013.
QA4MRE 2011-2013: Overview of ques-tion answering for machine reading evaluation.In Information Access Evaluation.
Multilinguality,Multimodality, and Visualization, pages 303?320.Springer.Matthew Richardson, Christopher J.C. Burges, andErin Renshaw.
2013.
MCTest: A challenge datasetfor the open-domain machine comprehension oftext.
In Proceedings of the 2013 Conference on Em-pirical Methods in Natural Language Processing,pages 193?203, Seattle, Washington, USA, October.Association for Computational Linguistics.Mrinmaya Sachan, Avinava Dubey, Eric P. Xing, andMatthew Richardson.
2015.
Learning answer-entailing structures for machine comprehension.
In53rd Annual Meeting of the Association for Compu-tational Linguistics.S.
Sukhbaatar, A. Szlam, J. Weston, and R. Fer-gus.
2015.
Weakly supervised memory networks.March.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 384?394,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
a quasi-synchronous gram-mar for QA.
In Proc.
of EMNLP-CoNLL.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards ai-complete ques-tion answering: A set of prerequisite toy tasks.April.X.
Yao, J. Berant, and B.
Van Durme.
2014.
FreebaseQA: Information extraction or semantic parsing?
InWorkshop on Semantic Parsing.706
