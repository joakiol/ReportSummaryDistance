Rapid Parser Development:A Machine Learning Approach for KoreanUl f  HermjakobUSC In format ion  Sc iences Ins t i tu te4676 Admira l ty  Way #1000 ?
Mar ina  del Rey,  CA  90292 ?
USAu l f@cs .utexas .eduAbst ractThis paper demonstrates that machine learning isa suitable approach for rapid parser development.From 1000 newly treebanked Korean sentences wegenerate a deterministic shift-reduce parser.
Thequality of the treebank, particularly crucial given itssmall size, is supported by a consistency checker.1 In t roduct ionGiven the enormous complexity of natural anguage,parsing is hard enough as it is, but often unforeseenevents like the crises in Bosnia or East-Timor createa sudden demand for parsers and machine transla-tion systems for languages that have not benefitedfrom major attention of the computational linguis-tics community up to that point.Good machine translation relies strongly on thecontext of the words to be translated, a context hatoften goes well beyond neighboring surface words.Often basic relationships, like that between a verband its direct object, provide crucial support fortranslation.
Such relationships are usually providedby parsers.The NLP resources for a language of sudden inter-national interest are typically quite limited.
There isprobably a dictionary, but most likely no treebank.Maybe basic tools for morphological analysis, butprobably no semantic ontology.This paper reports on the rapid development ofa parser based on very limited resources.
We showthat by building a small treebank of only a thousandsentences, we could develop a good basic parser us-ing machine learning within only three months.
Forthe language we chose, Korean, a number of researchgroups have been working on parsing and/or ma-chine translation in recent years (Yoon, 1997; Seo,1998; Lee, 1997), but advanced resources have notbeen made publicly available, and we have not usedany, thereby so-to-speak at least simulating a lowdensity language scenario.2 KoreanLike Japanese, Korean is a head-final agglutinativelanguage.
It is written in a phonetic alphabet calledhangul, in which each two-byte character representsone syllable.
While our parser operates on the orig-inal Korean hangul, this paper presents examplesin a romanized transcription.
In sentence (1) forexample, the verb is preceded by a number of so-called eojeols (equivalent to bunsetsus in Japanese)like "chaeg-eul", which are typically composed of acontent part ("chaeg" = book) and a postposition,which often corresponds to a preposition in English,but is also used as a marker of topic, subject or ob-ject ("eul").,_ ,_ _ I-~ ?\];gl 7Na-neun eo-je geu chaeg-eul sass-da.ITOPIC yesterday this bookoBJ bought.
(1)I bought this book yesterday.Our parser produces a tree describing the structureof a given sentence, including syntactic and semanticroles, as well as additional information such as tense.For example, the parse tree for sentence (1) is shownbelow:\[1\] na-netm eo-je geu chaeg-eul sass-da.
\[S\](SUB J) \[2\] na-neun \[NP\](HEAD) \[3\] na  \[KEG-NOUN\](PARTICLE) \[4\] neun \[DUPLICATE-PRT\](TIME) \[5\] eo-je \[REG-ADVERB\](HEAD) \[6\] eo-je \[REG-ADVERB\](OBJ) \[7\] geu chaeg-eul \[NP\](MOD) \[8\] geu \[DEMONSTR-ADNOMINAL\](HEAD) \[9\] geu \[DEMONSTR-ADNOMINAL\](HEAD) \[I0\] chaeg-eul \[NP\](HEAD) \[II\] chae E \[KEG-NOUN\](PARTICLE) \[12\] eul \[OBJ-CASE-PRT\](HEAD) \[13\] sass-da.
\[VERB; PAST-TENSE\](HEAD) \[14\] sa \[VERB-STEM\](SUFFIX) \[15\] eoss \[INTEEMED-SUF-VERB\](SUFFIX) \[16\] da \[CONNECTIVE-SUF-VERB\](DUMMY) \[17\] .
\[PERIOD\]Figure 1: Parse tree for sentence 1 (simplified)For preprocessing, we use a segmenter and mor-phological analyzer, KMA, and a tagger, KTAG,both provided by the research group of Prof. Rim of118Korea University.
KMA, which comes with a built-in Korean lexicon, segments Korean text into eojeolsand provides a set of possible sub-segmentations andmorphological analyses.
KTAG then tries to selectthe most likely such interpretation.
Our parser isinitialized with the result of KMA, preserving allinterpretations, but marking KTAG's  choice as thetop alternative.3 T reebank ing  E f fo r tThe additional resources used to train and test aparser for Korean, which we will describe in moredetail in the next section, were (1) a 1187 sentencetreebank, (2) a set of 133 context features, and (3)background knowledge in form of an 'is-a' ontologywith about 1000 entries.
These resources were builtby a team consisting of the principal researcher andtwo graduate students, each contributing about 3months.3.1 T reebankThe treebank sentences are taken from the Koreannewspaper Chosun, two-thirds from 1994 and the re-mainder from 1999.
Sentences represent continuousarticles with no sentences kipped for length or anyother reason.
The average sentence length is 21.0words.3.2  Feature  SetThe feature set describes the context of a partiallyparsed state, including syntactic features like thepart of speech of the constituent at the front/topof the input list (as sketched in figure 2) or whetherthe second constituent on the parse stack ends in acomma, as well as semantic features like whether ornot a constituent is a time expression or containsa location particle.
The feature set can accommo-date any type of feature as long as it is computable,and can thus easily integrate different ypes of back-ground knowledge.3.3 Background KnowledgeThe features are supported by background knowl-edge in the form of an ontology, which for examplehas a time-particle concept with nine sub-concepts(accounting for 9 of the 1000 entries mentionedabove).
Most of the background knowledge groupsconcepts like particles, suffixes, units (e.g.
for lengthsor currencies), temporal adverbs - semantic lassesthat are not covered by part of speech informationof the lexicon, yet provide valuable clues for parsing.3.4 T ime Ef fortThe first graduate student, a native Korean andlinguistics major, hired for 11 weeks, spent about2 weeks getting trained, 6 weeks on building two-thirds of the treebank, 2 weeks providing most back-ground knowledge entries and 1 week helping to< parse stack-3 -2~ "bought"synt: verbtop ofstack-1(R 2 TO S-VP AS PRED OBJ)front/top oflist<:input list>1i, "today" Isynt: adv I"reduce the 2 top elements of the parse stackto a frame with syntax 'vp'and roles 'pred' and 'obj'""bought"synt: verb"bought abook"synt: vpsub: (pred) (obj)"today"synt: advFigure 2: A typical parse action (simplified).Boxes represent frames.
The asterisk (*) represents hecurrent parse position.
Optionally, parse actions canhave additional arguments, like target syntactic or se-mantic classes to overwrite any default.
Elements on theinput list are identified by positive integers, elements onthe parse stack by negative integers.
The feature 'Synt of-1' for example refers to the (main) syntactic ategory ofthe top stack element.
Before the reduce operation, thefeature 'Synt of-1' would evaluate to np (for "a book"),after the operation to vp (for "bought a book").
The in-put list is initialized with the morphologically analyzedwords, possibly still ambiguous.
After a sequence of shift(from input list to parse stack) and reduce (on the parsestack) operations, the parser eventually ends up with asingle element on the parse stack, which is then returnedas the parse tree.identify useful features.
The other graduate student,a native Korean and computer science major, in-stalled Korean tools including a terminal for hanguland the above mentioned KMA and KTAG, wrote anumber of scripts tying all tools together, made sometool improvements, built one-third of the treebank119and also contributed to the feature set.
The prin-cipal researcher, who does not speak Korean, con-tributed about 3 person months, coordinating theproject, training the graduate students, writing tree-bank consistency checking rules (see section 6), mak-ing extensions to the tree-to-parse-action-sequencemodule (see section 4.1) and contributing to thebackground knowledge and feature set.4 Learn ing  to  ParseWe base our training on the machine learning basedapproach of (Hermjakob k: Mooney, 1997), allow-ing however unrestricted text and deriving the parseaction sequences required for training from a tree-bank.
The basic mechanism for parsing text intoa shallow semantic representation is a shift-reducetype parser (Marcus, 1980) that breaks parsing intoan ordered sequence of small and manageable parseactions.
Figure 2 shows a typical reduce action.
Thekey task of machine learning then is to learn to pre-dict which parse action to perform next.Two key advantages of this type of deterministicparsing are that its linear run-time complexity withrespect to sentence length makes the parser veryfast, and that the parser is very robust in that itproduces a parse tree for every input sentence.Figure 3 shows the overall architecture of parsertraining.
From the treebank, we first automaticallygenerate a parse action sequence.
Then, for everystep in the parse action sequence, typically severaldozens per sentence, we automatically compute thevalue for every feature in the feature set, add on theparse action as the proper classification of the parseaction example, and then feed these examples into amachine learning program, for which we use an ex-tension of decision trees (Quinlan, 1986; Hermjakob& Mooney, 1997).We built our parser incrementally.
Starting with asmall set of syntactic features that are useful acrossall languages, early training and testing runs revealmachine learning conflict sets and parsing errors thatpoint to additionally required features and possiblyalso additional background knowledge.
A conflictset is a set of training examples that have identicalvalues for all features, yet differ in their classification(= parse action).
Machine learning can therefore notpossibly learn how to handle all examples correctly.This is typically resolved by adding an additionalfeature that differentiates between the examples ina linguistically relevant way.Even treebanking benefits from an incremental p-proach.
Trained on more and more sentences, andat the same time with also more and more features,parser quality improves, so that the parser as a tree-banking tool has to be corrected less and less fre-quently, thereby accelerating the treebanking pro-cess.Knowledge Base ("ontology")?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.temporal-concept~- the-year :i day-of-the-week 'Monday ... Sundaysyntactic-elementverb noun adverbcount-noun mass-noun.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.IiFeature set: ', l Svnt Svnt of-2 of- 1 S~n~Treebankcomputer science~ parse action sequence generator (automatic)Parse action sequence:Shift nounShift nounReduce 2 as mod headDone~ parse example generator (automatic)Parse action examples:\[Unavail Unavail Noun \[ Shift noun \[\[ Unavaii Noun Noun \[ Shift noun I\[Noun Noun Unavail I Reduce 2 as mod head I\[Unavaii Noun Unavail \ [Done Idecision structure builder (automatic)Parse decision structure:Synt of 1 N~ai lShi~t noun / /~nt  of-2Done Reduce 2 as rood headFigure 3: Derivation of the parser from a treebankand a feature set.
The resulting parser has the formof a decision structure, an extension of decision trees.Given a seen or unseen sentence in form of a listof words, the decision structure keeps selecting thenext parse action until a single parse tree coveringthe entire sentence has been built.120word level constituentlabeled precision+ i /SUFFIX-NOUN + I /OBJ-CASE-PRT+ i /NUMERAL + I /OBJ-CASE-PRT+ i l /UNIT-NOUN+ i l /REGULAR-NOUN86.0%-The analyzer divides '31i1' into groups with varyingnumber of sub-components with different parts ofspeech.
When shifting in an element, the parser hasto decide which one to pick, the third one in thiscase, using context of course.The module generating parse action sequencesfrom a tree needs special split and merge operationsfor cases where the correct segmentation is not of-fered as a choice at all.
To make things a little ugly,these splits can not only occur in the middle of a leafconstituent, but even in the middle of a characterthat might have been contracted from two charac-ters, each with its own meaning.5 Chosun Newspaper  Exper imentsTable 1 presents evaluation results with the numberof training sentences varying from 32 to 1024 andwith the remaining 163 sentences of the treebankused for testing.Precision:number of correct constituents in system parsenumber of constituents in system parseRecal l :number of correct constituents in system parsenumber of constituents in logged parseCross ing  brackets :  number of constituentswhich violate constituent boundaries with a con-stituent in the logged parse.
Labe led  preci-sion/recall measures not only structural correctness,but also the correctness of the syntactic label.
Cor -rect  operat ions  measures the number of correctoperations during a parse that is continuously cor-rected based on the logged sequence; it measuresthe core machine learning algorithm performance inisolation.
A sentence has a correct operat ing  se-quence,  if the system fully predicts the logged parseaction sequence, and a correct s t ruc ture  and  la-be l ing,  if the structure and syntactic labeling of thefinal system parse of a sentence is 100% correct, re-gardless of the operations leading to it.Figures 4 and 5 plot the learning curves for twokey metrics.
While both curves are clearly headingz KMA actually produces 10 different alternatives in thiscase, of which only four are shown here.87.0%-85.0%-84.0%-I t I I I i32 64 128 256 512 1024number of training sentences2.12.01.91.81.71.61.531/NUMERAL31/NUMERAL31/NUMERAL31/NUMERALFigure 4: Learning curve for labeled precision corre-sponding to table 1crossings brackets per sentence4.1 Spec ia l  Adaptat ion  for  KoreanThe segmenter and morphological nalyzer KMA re-turns a list of alternatives for each eojeol.
However,the alternatives are not atomic but rather two-levelconstituents, or mini-trees.
Consider for examplethe following four  1 alternatives for the eojeol '31il'(the 31st day of a month):32 64 128 256 512 1024number of training sentencesFigure 5: Learning curve for crossing brackets persentence corresponding to table 1in the right direction, up for precision, and downfor crossing brackets, their appearance is somewhatjagged.
For smaller data sets like in our case, thiscan often be avoided by running an n-fold cross val-idation test.
However, we decided not to do so,because many training sentences were also used forfeature set and background knowledge development121Training sentences 32 64 128 256 512 1024PrecisionRecallLabeled precisionLabeled recallTagging accuracyCrossings/sentence0 crossings< 1 crossing< 2 crossings< 3 crossings< 4 crossingsCorrect operationsOperation SequenceStructure&Label88.6%87.3%84.1%81.2%94.3%1.9727.6%56.4%70.6%81.0%88.3%63.0%2.5%5.5%88.1%87.4%83.9%81.9%92.9%2.0035.0%58.9%72.4%81.6%84.0%68.3%6.1%12.9%90.0%89.2%85.8%83.6%93.9%1.7238.7%63.2%73.0%82.2%91.4%71.5%8.O%11.7%89.6%89.1%85.6%83.6%93.4%1.7940.5%59.5%71.8%81.6%89.0%73.4%8.6%16.o%90.7%89.6%86.7%84.7%94.0%1.6943.6%64.4%73.0%82.2%90.8%75.0%11.0%19.0%91 .O%89.8%86.9%85.O%94.2%1.6342.9%62.6%74.2%83.4%89.6%76.3%7.4%16.0%Table 1: Evaluation results with varying number of training sentencesas well as for intermediate inspection, and thereforemight have unduly influenced the evaluation.5.1 Tagging accuracyA particularly striking number is the tagging accu-racy, 94.2%, which is dramatically below the equiv-alent 98% to 99% range for a good English orJapanese parser.
In a Korean sentence, only largerconstituents hat typically span several words areseparated by spaces, and even then not consistently,so that segmentation errors are a major source fortagging problems (as it is to some degree howeveralso for Japanese2).
We found that the segmen-tation part of KMA sometimes still struggles withrelatively simple issues like punctuation, proposingfor example words that contain a parenthesis in themiddle of standard alphabetic haracters.
We havecorrected some of these problems by pre- and post-processing the results of KMA, but believe that thereis still a significant potential for further improve-ment.In order to assess the impact of the relatively lowtagging accuracy, we conducted experiments thatsimulated a perfect agger by initializing the parserwith the correctly segmented, morphologically ana-lyzed and tagged sentence according to the treebank.By construction, the tagging accuracy in table 2rises to 100%.
Since the segmenter/tagger r turnsnot just atomic but rather two-level constituents,the precision and recall values benefit particularlystrongly, possibly inflating the improvements forthese metrics, but other metrics like crossing brack-ets per sentence show substantial gains as well.
Thuswe believe that refined pre-parsing tools, as they are2Whi le  Japanese does not  use spaces at all, script changesbetween kanji, hiragana, and katakana provide a lot of seg-mentat ion  guidance.
Modern Korean,  however, a lmost  exclu-sively uses only a single phonet ic  script.Segmentation/ Regular SimulatingTagging seg/tag as perfect( "seg/tag" ) implemented seg/tagLabeled precisionLabeled recallTagging accuracyCrossings/sentence0 crossings< 2 crossingsStructure&Label86.9%85 .O%94.2%1.6342.9%74.2%16.0%93.4%92.9%100.0%1.1348.5%85.3%28.8%Table 2: Impact of segmentation/tagging errorsin the process of becoming available for Korean, willgreatly improve parsing accuracy.However, for true low density languages, uch highquality preprocessors are probably not available sothat our experimental scenario might be more re-alistic for those conditions.
On the other hand,some low density languages like for example Tetun,the principal indigenous language of East Timor,are based on the Latin alphabet, separate words byspaces and have relatively little inflection, and there-fore make morphological nalysis and segmentationrelatively simple.6 T reebank  Cons is tency  Check ingIt is difficult to maintain a high treebank quality.When training on a small treebank, this is particu-larly important, because there is not enough data toallow generous pruning.Treebanking is done by humans and humans err.Even with annotation guidelines there are often ad-ditional inconsistencies when there are several an-notators.
In the Penn Treebank (Marcus, 1993) forexample, the word ago as in 'two years ago', is tagged122414 times as an adverb and 150 times as a preposi-tion.In many treebanking efforts, basic taggers andparsers suggest parts of speech and tree structuresthat can be accepted or corrected, typically speed-ing up the treebanking effort considerably.
How-ever, incorrect defaults can easily slip through, leav-ing blatant inconsistencies like the one where theconstituent ' hat' as in 'the dog that bit her' is tree-banked as a noun phrase containing a conjunction(as opposed to a pronoun).From the very beginning of treebanking, we havetherefore passed all trees to be added to the tree-bank through a consistency checker that looks forany suspicious patterns in the new tree.
For everytype of phrase, the consistency checker draws on alist of acceptable patterns in a BNF style notation.While this consistency checking certainly does notguarantee to find all errors, and can produce falsealarms when encountering rare but legitimate con-structions, we have found it a very useful tool tomaintain treebank quality from the very beginning,easily offsetting the about three man days that ittook to adapt the consistency checker to Korean.For a number of typical errors, we extended thechecker to automatically correct errors for which thiscould be done safely, or, alternatively, suggest alikely correction for errors and prompt for confir-mation/correction by the treebanker.7 Conc lus ionsComparisons with related work are unfortunatelyvery problematic, because the corpora are differ-ent and are sometimes not even described in otherwork.
In most cases Korean research groups also useother evaluation metrics, particularly dependencyaccuracy, which is often used in dependency struc-ture approaches.
Training on about 40,000 sentences(Collins, 1997) achieves a crossing brackets rate of1.07, a better value than our 1.63 value for regularparsing or the 1.13 value assuming perfect segmen-tation/tagging, but even for similar text types, com-parisons across languages are of course problematic.It is clear to us that with more training sentences,and with more features and background knowledgeto better leverage the increased number of train-ing sentences, accuracy rates can still be improvedsignificantly.
But we believe that the reduction ofparser development time from two years or moredown to three months is in many cases already veryvaluable, even if the accuracy has not 'maxed out'yet.
And given the experience we have gained fromthis project, we hope this research to be only a firststep to an even steeper development time reduction.A particularly promising research direction for thisis to harness knowledge and training resources acrosslanguages.AcknowledgmentsI would like to thank Kyoosung Lee for installing,improving and conncecting Korean pre-processingtools like segmenter and tagger as well as startingthe treebanking, and Mina Lee, who did most of thetreebanking.Re ferencesM.
J. Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
In 35th Proceedingsof the ACL, pages 16-23.U.
Hermjakob and R. J. Mooney.
1997.
LearningParse and Translation Decisions From ExamplesWith Rich Context.
In 35th Proceedings of theACL, pages 482-489.URL: fi le://ftp.cs.utexas.edu/pub/mooney/papers/contex-acl-97.ps.ZU.
Hermjakob.
1997.
Learning Parse and Transla-tion Decisions From Examples With Rich Context.Ph.D.
thesis, University of Texas at Austin, Dept.of Computer Sciences TR 97-12.URL: file://ftp.cs.utexas.edu/pub/mooney/papers/hermjakob-dissertation-97.ps.ZGeunbae Lee, Jong-Hyeok Lee, and Hyuncheol Rho.1997.
Natural Language Processing for Session-Based Information Retrieval Interface on the Web.In Proceedings of IJCAI-97 workshop on AI in dig-ital libraries, pages 43-48.M.
P. Marcus.
1980.
A Theory of Syntactic Recog-nition for Natural Language.
MIT Press.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Lin-guistics 19(2), pages 313-330.J.
R. Quinlan.
1993.
C4.5 Programs for MachineLearning.
Morgan Kaufmann Publishers, San Ma-teo, California.K.
J. Seo, K. C. Nam, and K. S. Choi.
1998.
A Prob-abilistic Model for Dependency Parsing Consider-ing Ascending Dependencies.
Journal of Literaryand Linguistic Computing, Vol 13(2).Juntae Yoon, Seonho Kim, and Mansuk Song.
1997.New Parsing Method Using Global AssociationTable.
In Proc.
of the International Workshop onParsing Technology.123
