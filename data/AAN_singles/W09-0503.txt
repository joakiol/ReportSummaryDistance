Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 19?25,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsIdentifying Segment Topics in Medical DictationsJohannes Matiasek, Jeremy JancsaryAlexandra KleinAustrian Research Institute forArtificial IntelligenceFreyung 6, Wien, Austriafirstname.lastname@ofai.atHarald TrostDepartment of Medical Cyberneticsand Artificial Intelligenceof the Center for Brain Research,Medical University Vienna, Austriaharald.trost@meduniwien.ac.atAbstractIn this paper, we describe the use of lexi-cal and semantic features for topic classi-fication in dictated medical reports.
First,we employ SVM classification to assignwhole reports to coarse work-type cate-gories.
Afterwards, text segments andtheir topic are identified in the outputof automatic speech recognition.
Thisis done by assigning work-type-specifictopic labels to each word based on fea-tures extracted from a sliding context win-dow, again using SVM classification uti-lizing semantic features.
Classifier stack-ing is then used for a posteriori error cor-rection, yielding a further improvement inclassification accuracy.1 IntroductionThe use of automatic speech recognition (ASR) isquite common in the medical domain, where forevery consultation or medical treatment a writtenreport has to be produced.
Usually, these reportsare dictated and transcribed afterwards.
The use ofASR can, thereby, significantly reduce the typingefforts, but, as can be seen in figure 1, quite somework is left.complaint dehydration weakness and diarrhea fullstop Mr. Will Shawn is a 81-year-old cold Asiangentleman who came in with fever and Persiandiaper was sent to the emergency department by hisprimary care physician due him being dehydratedperiod .
.
.
neck physical exam general alert andoriented times three known acute distress vitalsigns are stable .
.
.
diagnosis is one chronicdiarrhea with hydration he also has hypokalemianeck number thromboctopenia probably duty livercirrhosis .
.
.
a plan was discussed with patient indetail will transfer him to a nurse and facilityfor further care .
.
.
end of dictationFigure 1: Raw output of speech recognitionWhen properly edited and formatted, the samedictation appears significantly more comprehensi-ble, as can be seen in figure 2.CHIEF COMPLAINTDehydration, weakness and diarrhea.HISTORY OF PRESENT ILLNESSMr.
Wilson is a 81-year-old Caucasian gentlemanwho came in here with fever and persistentdiarrhea.
He was sent to the emergency departmentby his primary care physician due to him beingdehydrated.. .
.PHYSICAL EXAMINATIONGENERAL: He is alert and oriented times three,not in acute distress.VITAL SIGNS: Stable.. .
.DIAGNOSIS1.
Chronic diarrhea with dehydration.
He alsohas hypokalemia.2.
Thromboctopenia, probably due to livercirrhosis.. .
.PLAN AND DISCUSSIONThe plan was discussed with the patient in detail.Will transfer him to a nursing facility forfurther care.. .
.Figure 2: A typical medical reportBesides the usual problem with recognition er-rors, section headers are often not dictated or hardto recognize as such.
One task that has to be per-formed in order to arrive at the structured reportshown in figure 2 is therefore to identify topicalsections in the text and to classify them accord-ingly.In the following, we first describe the problemsetup, the steps needed for data preparation, andthe division of the classification task into subprob-lems.
We then describe the experiments performedand their results.In the outlook we hint at ways to integrate thisapproach with another, multilevel, segmentationframework.2 Data Description and Problem SetupAvailable corpus data consists of raw recognitionresults and manually formatted and corrected re-ports of medical dictations.
11462 reports were19available in both forms, 51382 reports only as cor-rected transcripts.
When analysing the data, itbecame clear that the structure of segment topicsvaried strongly across different work-types.
Thuswe decided to pursue a two-step approach: firstlyclassify reports according to their work-type and,secondly, train and apply work-type specific clas-sification models for segment topic classification.2.1 Classification frameworkFor all classification tasks discussed here, we em-ployed support-vector machines (SVM, Vapnik(1995)) as the statistical framework, though in dif-ferent incarnations and setups.
SVMs have provento be an effective means for text categorization(Joachims, 1998) as they are capable to robustlydeal with high-dimensional, sparse feature spaces.Depending on the task, we experimented with dif-ferent feature weighting schemes and SVM kernelfunctions as will be described in section 3.2.2 Features used for classificationThe usual approach in text categorization is to usebag-of-word features, i.e.
the words occuring in adocument are collected disregarding the order oftheir appearance.
In the domain of medical dic-tation, however, often abbreviations or differentmedical terms may be used to refer to the same se-mantic concept.
In addition, medical terms oftenare multi-word expressions, e.g., ?coronary heartdisease?.
Therefore, a better approach for featuremapping is needed to arrive at features at an ap-propriate generalization level:?
Tokenization is performed using a largefinite-state lexicon including multi-wordmedical concepts extracted from the UMLSmedical metathesaurus (Lindberg et al,1993).
Thus, multi-word terms remain intact.In addition, numeric quantities in special(spoken or written) formats or together with adimension are mapped to semantic types (e.g.
?blood pressure?
or ?physical quantity?
), alsousing a finite-state transducer.?
The tokens are lemmatized and, if possi-ble, replaced by the UMLS semantic con-cept identifier(s) they map to.
Thus,?CHD?, ?coronary disease?
and ?coronaryheart disease?
all map to the same concept?C0010068?.?
In addition, also the UMLS semantic type, ifavailable, is used as a feature, so, in the ex-ample above, ?B2.2.1.2.1?
(Disease or Syn-drome) is added.?
Since topics in a medical report roughly fol-low an order, for the segment topic identifica-tion task also the relative position of a wordin the report (ranging from -1 to +1) is used.We also explored different weighting schemes:?
binary: only the presence of a feature is in-dicated.?
term frequency: the number of occurencesof a feature in the segment to be classified isused as weight.?
TFIDF: a measure popular from informationretrieval, where tfidfi,j of term ti in docu-ment dj ?
D is usually defined ascti,j?i cti,j.
log|D||{dj : ti ?
dj}|An example of how this feature extraction pro-cess works is given below:token(s) feature(s) comment...an stop word78 year old QH OLD pattern-based typefemale C0085287 UMLS conceptA2.9.2 UMLS semtypeintubated intubate lemmatized (no concept)with stop wordlung cancer C0242379 UMLS conceptC0684249 UMLS conceptB2.2.1.2.1.2 UMLS semtype...2.3 Data AnnotationFor the first classification task, i.e.
work-type clas-sification, no further annotation is necessary, ev-ery report in our data corpus had a label indicatingthe work-type.
For the segment topic classificationtask, however, every token of the report had to beassigned a topic label.2.3.1 Analysis of Corrected TranscriptsFor the experiments described here, we con-centrated on the ?Consultations?
work-type, forwhich clear structuring recommendations, suchas E2184-02 (ASTM International, 2002), exist.However, in practice the structure of medical re-ports shows high variation and deviations fromthe guidelines, making it harder to come up with20an appropriate set of class labels.
Therefore, us-ing the aforementioned standard, we assigned theheadings that actually appeared in the data to theclosest type, introducing new types only when ab-solutely necessary.
Thus we arrived at 23 headingclasses.
Every (possibly multi-word) token wasthen labeled with the heading class of the last sec-tion heading occurring before it in the text using asimple parser.2.3.2 Aligment and Label TransferWhen inspecting manually corrected reports (cf.fig.
2), one can easily identify a heading and clas-sify the topic of the text below it accordingly.However, our goal is to develop a model for iden-tifying and classifying segments in the dictation,thus we have to map the annotation of correctedreports onto the corresponding ASR output.
Thebasic idea here is to align the tokens of the cor-rected report with the tokens in ASR output and tocopy the annotations (cf.
figure 3).
There are someproblems we have to take care of during align-ment:1. non-dictated items in the corrected test (e.g.punctuation, headings)2. dictated words that do not occur in the cor-rected text (meta instructions, repetitions)3. non-identical but corresponding items(recognition errors, reformulations)For this alignment task, a standard string-editdistance based method is not sufficient.
There-fore, we augment it with a more sophisticated costfunction.
It assigns tokens that are similar (ei-ther from a semantic or from a phonetic point ofview) a low cost for substitution, whereas dissimi-lar tokens receive a prohibitively expensive score.Costs for deletion and insertion are assigned in-versely.
Semantic similarity is computed usingWordnet (Fellbaum, 1998) and UMLS.
For pho-netic matching, the Metaphone algorithm (Philips,1990) was used (for details see Huber et al (2006)and Jancsary et al (2007)).3 Experiments3.1 Work-Type CategorizationIn total we had 62844 written medical reportswith assigned work-type information from differ-ent hospitals, 7 work-types are distinguished.
Werandomly selected approximately a quarter of thecorrected report OP ASR output.
.
.
.
.
.
.
.
.
.
.
.
.
.
.ChiefCompl CHIEF delChiefCompl COMPLAINT sub complaint ChiefComplChiefCompl Dehydration sub dehydration ChiefComplChiefCompl , delChiefCompl weakness sub weakness ChiefComplChiefCompl and sub and ChiefComplChiefCompl diarrhea sub diarrhea ChiefComplChiefCompl .
sub fullstop ChiefComplHistoryOfP Mr. sub Mr. HistoryOfPHistoryOfP Wilson sub Will HistoryOfPins Shawn HistoryOfPHistoryOfP is sub is HistoryOfPHistoryOfP a sub a HistoryOfPHistoryOfP 81-year-old sub 81-year-old HistoryOfPHistoryOfP Caucasian sub cold HistoryOfPHistoryOfP ins Asian HistoryOfPHistoryOfP gentleman sub gentleman HistoryOfPHistoryOfP who sub who HistoryOfPHistoryOfP came sub came HistoryOfPHistoryOfP in delHistoryOfP here sub here HistoryOfPHistoryOfP with sub with HistoryOfPHistoryOfP fever sub fever HistoryOfPHistoryOfP and sub and HistoryOfPHistoryOfP persistent sub Persian HistoryOfPHistoryOfP diarrhea sub diaper HistoryOfPHistoryOfP .
del.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 3: Mapping labels via alignmentreports as the training set, the rest was used fortesting.
The distribution of the data can be seen intable 1.Trainingset Testset Work-Type649 4.1 1966 4.2 CACardiology7965 51.0 24151 51.1 CL ClinicalReports1867 11.9 5590 11.8 CNConsultations1120 7.2 3319 7.0 DS DischargeSummaries335 2.1 878 1.8 ER EmergencyMedicine2185 14.0 6789 14.4 HP HistoryAndPhysicals1496 9.6 4534 9.6 OROperativeReports15617 47227 TotalTable 1: Distribution of Work-typesAs features for categorization, we used a bag-of-words approach, but instead of the surface formof every token of a report, we used its semanticfeatures as described in section 2.2.
As a catego-rization engine, we used LIBSVM (Chang&Lin,2001) with an RBF kernel.
The features whereweighted with TFIDF.
In order to compensate fordifferent document length, each feature vector wasnormalized to unit length.
After some param-eter tuning iterations, the SVM model performsreally well with a microaveraged F11 value of0.9437.
This indicates high overall accuracy, andthe macroaveraged F1 value of 0.9341 shows, thatalso lower frequency categories are predicted quitereliably.
The detailed results are shown in table 2.Thus the first step in the cascaded model, i.e.the selection of the work-type specific segment1F1 = 2?precision?recallprecision+recall21predicted rec.
prec.
F1true CA CL CN DS ER HP ORCA 1966 1882 53 5 6 0 9 11 0.9573 0.9787 0.9679CL 24151 25 23675 217 13 18 155 48 0.9803 0.9529 0.9664CN 5590 1 447 4695 7 17 413 10 0.8399 0.8814 0.8601DS 3319 1 37 8 3241 2 27 3 0.9765 0.9818 0.9792ER 878 0 90 7 10 754 13 4 0.8588 0.9425 0.8987HP 6789 4 512 393 22 7 5838 13 0.8599 0.9040 0.8814OR 4534 10 31 2 2 2 3 4484 0.9890 0.9805 0.9847microaveraged 0.9437macroaveraged 0.9341Table 2: Work-Type categorization resultstopic model, yields reliable performance.3.2 Segment Topic ClassificationIn contrast to work-type categorization, wherewhole reports need to be categorized, the identifi-cation of segment topics requires a different setup.Since not only the topic labels are to be deter-mined, but also segment boundaries are unknownin the classification task, each token constitutesan example under this setting.
Segments are thencontiguous text regions with the same topic label.It is clearly not enough to consider only featuresof the token to be classified, thus we include alsocontextual and positional features.3.2.1 Feature and Kernel SelectionIn particular, we employ a sliding window ap-proach, i.e.
for each data set not only the token tobe classified, but also the 10 preceding and the 10following tokens are considered (at the beginningor towards the end of a report, context is reducedappropriately).
This window defines the text frag-ment to be used for classifying the center token,and features are collected from this window againas described in section 2.2.
Additionaly, the rela-tive position (ranging from -1 to +1) of the centertoken is used as a feature.The rationale behind this setup is that1.
usually topics in medical reports follow an or-dering, thus relative position may help.2.
holding features also from adjacent segmentsmight also be helpful since topic successionalso follows typical patterns.3.
a sufficiently sized context might also smoothlabel assignment and prevent label oscilla-tion, since the classification features for ad-jacent words overlap to a great deal.A second choice to be made was the selectionof the kernel best suited for this particular classifi-cation problem.
In order to get an impression, wemade a preliminary mini-experiment with just 5reports each for training (4341 datasets) and test-ing (3382 datasets), the results of which are re-ported in table 3.AccuracyFeature Weight linear RBFTFIDF 0.4977 0.3131TFIDF normalized 0.5544 0.6199Binary 0.6417 0.6562Table 3: Preliminary Kernel ComparisonWhile these results are of course not significant,two things could be learned from the preliminaryexperiment:1. linear kernels may have similar or even betterperformance,2.
training times with LIBSVM with a largenumber of examples may soon get infeasible(we were not able to repeat this experimentwith 50 reports due to excessive runtime).Since LibSVM solves linear and nonlinearSVMs in the same way, LibSVM is not particu-larly efficient for linear SVMs.
Therefore we de-cided to switch to Liblinear (Fan et al, 2008), alinear classifier optimized for handling data withmillions of instances and features2.2Indeed, training a model from 669 reports (463994 ex-amples) could be done in less then 5 minutes!22predicted class label (#)# True Label Total F1 .
.
.
3 4 .
.
.
14 .
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.3 Diagnosis 40871 0.603 .
.
.
24391 2864 .
.
.
8691 .
.
.4 DiagAndPlan 21762 0.365 .
.
.
5479 6477 .
.
.
7950 .
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.14 Plan 31729 0.598 .
.
.
5714 3419 .
.
.
21034 .
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Table 4: Confusion matrix (part of)3.2.2 Segment Topic Classification ResultsExperiments were performed on a randomly se-lected subset of reports from the ?Consultations?work-type (1338) that were available both in cor-rected form and in raw ASR output form.
An-notations were constructed for the corrected tran-scripts, as described in section 2.3, transfer of la-bels to the ASR output was performed as shown insection 2.3.2.Both data sets were split into training and testsets of equal size (669 reports each), experimentswith different feature weighting schemes havebeen performed on both corrected data and ASRoutput.
The overall results are shown in table 5.corrected reports ASR outputmicro- macro- micro- macro-Feature weights avg.F1 avg.F1 avg.F1 avg.F1TFIDF 0.7553 0.5178 0.7136 0.4440TFIDF norm.
0.7632 0.3470 0.7268 0.3131Binary 0.7693 0.4636 0.7413 0.3953Table 5: Segment topic classification resultsConsistently, macroaveraged F1 values aremuch lower than their microaveraged counterpartsindicating that low-frequency topic labels are pre-dicted with less accuracy.Also, segment classification works better withcorrected reports than with raw ASR output.
Thereason for that behaviour is1.
ASR data are more noisy due to recognitionerrors, and2.
while in corrected reports appropriate sectionheaders are available (not as header, but thewords) this is not necessarily the case in ASRoutput (also the wording of dictated headersand written headers may be different).A general note on the used topic labels mustalso be made: Due to the nature of our data itwas inevitable to use topic labels that overlap insome cases.
The most prominent example here is?Diagnosis?, ?Plan?, and ?Diagnosis and Plan?.The third label clearly subsumes the other two, butin the data available the physicians often decidedto dictate diagnoses and the respective treatmentin an alternating way, associating each diagnosiswith the appropriate plan.
This made it necessaryto include all three labels, with obvious effects thatcould easily seen when inspecting the confusionmatrix, a part of which is shown in table 4.When looking at the misclassifications in these3 categories it can easily be seen, that they are pre-dominantly due to overlapping categories.Another source of problems in the data is theskewed distribution of segment types in the re-ports.
Sections labelled with one of the four la-bel categories that weren?t predicted at all (Chief-Complaints, Course, Procedure, and Time, cf.
ta-ble 6) occur in less than 2% of the reports or areinfrequent and extremely short.
This fact had, ofcourse, undesirable effects on the macroavered F1scores.
Additional difficulties that are similar tothe overlap problem discussed above are strongthematic similarities between some section types(e.g., Findings and Diagnosis, or ReasonForEn-counter andHistoryOfPresentIllness) that result ina very similar vocabulary used.Given these difficulties due to the data, the re-sults are encouraging.
There is, however, stillplenty of room left for improvement.3.3 Improving Topic ClassificationLiblinear does not only provide class label pre-dictions, it is also possible to obtain class proba-bilities.
The usual way then to predict the labelis to choose the one with the highest probability.When analysing the errors made by the segmenttopic classification task described above, it turnedout that often the correct label was ranked secondor third (cf.
table 6).
Thus, the idea of just taking23correct prediction inLabel count best best 2 best 3Allergies 3456 29.72 71.64 85.21ChiefComplai 697Course 30Diagnosis 43565 64.69 83.29 91.37DiagAndPlan 19409 35.24 70.45 86.81DiagnosticSt 35554 82.47 91.34 93.05Findings 791 0.38 1.26Habits 2735 7.31 32.69 41.76HistoryOfPre 122735 92.26 97.55 98.20Medication 14553 85.87 93.38 95.22Neurologic 5226 54.08 86.93 89.19PastHistory 43775 71.13 86.26 88.82PastSurgical 5752 49.32 78.88 84.47PhysicalExam 86031 93.56 97.01 97.57Plan 36476 62.57 84.63 94.65Practitioner 1262 55.07 76.78 82.73Procedures 109ReasonForEnc 15819 25.42 42.35 43.47ReviewOfSyst 29316 79.81 89.90 91.87Time 58Total 467349 76.93 88.65 92.00Table 6: Ranked predictionsthe highest ranked class label could be possiblyimproved by a more informed choice.While the segment topic classifier already takescontextual features into account, it has still no in-formation on the classification results of the neigh-boring text segments.
However, there are con-straints on the length of text segments, thus, e.g.a text segment of length 1 with a different topic la-bel than the surrounding text is highly implausible.Furthermore, there are also regularities in the suc-cession of topic labels, which can be captured bythe monostratal local classification only indirectly?
if at all.A look at figure 4 exemplifies how a bet-ter informed choice of the label could result inhigher prediction accuracy.
The segment labelled?PastHistory?
correctly ends 4 tokens earlier thanpredicted, and, additionally, this label erroneouslyis predicted again for the phrase ?progressiveweight loss?.
The correct label, however, has stilla rather high probability in the predicted labeldistribution.
By means of stacking an additionalclassier onto the first one we hope to be able tocorrect some of the locally made errors a posteri-ori.The setup for the error correction classifierwe experimented with was as follows (it wasperformed only for the segment topic classi-fier trained on ASR output with binary featureweights):1.
The training set of the classifier was clas-Label probabilities (%)True Label Predicted ... 10 11 12 ... 17 18. .
.= PastHistory [11] age PastHistory 0 95 0 0 0= PastHistory [11] 63 PastHistory 0 95 0 0 0= PastHistory [11] and PastHistory 0 95 0 0 1= PastHistory [11] his PastHistory 0 95 0 0 1= PastHistory [11] father PastHistory 0 88 0 0 9= PastHistory [11] died PastHistory 0 90 0 0 8= PastHistory [11] from PastHistory 0 84 0 0 14= PastHistory [11] myocardial infa PastHistory 0 81 0 0 17= PastHistory [11] at PastHistory 0 77 0 0 20= PastHistory [11] age PastHistory 0 78 0 1 19= PastHistory [11] 57 PastHistory 0 78 0 1 19= PastHistory [11] period PastHistory 0 78 0 1 19- ReviewOfSyst[18] review PastHistory 0 76 0 1 20- ReviewOfSyst[18] of PastHistory 0 76 0 1 21- ReviewOfSyst[18] systems PastHistory 0 78 0 0 19- ReviewOfSyst[18] he PastHistory 1 57 0 1 37= ReviewOfSyst[18] has ReviewOfSyst 1 32 0 1 58= ReviewOfSyst[18] had ReviewOfSyst 1 32 0 1 58- ReviewOfSyst[18] progressive PastHistory 1 49 0 1 42- ReviewOfSyst[18] weight loss PastHistory 1 60 0 1 32= ReviewOfSyst[18] period ReviewOfSyst 1 31 0 0 62= ReviewOfSyst[18] his ReviewOfSyst 1 13 0 1 81= ReviewOfSyst[18] appetite ReviewOfSyst 1 13 0 1 81. .
.Figure 4: predicted label probabilitessified, and the predicted label probabilitieswere collected as features.2.
Again, a sliding window (with differentsizes) was used for feature construction.
Fea-tures were set up for each label at each win-dow position and the respective predicted la-bel probability was used as its value.3.
A linear classifier was trained on these fea-tures of the training set4.
This classifier was applied to the results ofclassifying the test set with the original seg-ment topic classifier.Three different window sizes were used on thecorrected reports, only one window was appliedon ASR output (cf.
table 7).
As can be seen, eachcorrected reports ASR outputmicro- macro- micro- macro-context window avg.F1 avg.F1 avg.F1 avg.F1No correction 0.7693 0.4636 0.7413 0.3953[?3, +3] 0.7782 0.4773 - -[?6, +0] 0.7798 0.4754 - -[?3, +4] 0.7788 0.4769 0.7520 0.4055Table 7: A posteriori correction resultscontext variant improved on both microaveragedand macroaveraged F1 in a range of 0,9 to 1.4 per-cent points.
Thus, stacked error correction indeedis possible and able to improve classification re-sults.244 Conclusion and OutlookWe have presented a 3 step approach to seg-ment topic identification in dictations of medi-cal reports.
In the first step, a categorization ofwork-type is performed on the whole report us-ing SVM classification employing semantic fea-tures.
The categorization model yields good per-formance (over 94% accuracy) and is a prerequi-site for subsequent application of work-type spe-cific segment classification models.For segment topic detection, every word was as-signed a class label based on contextual featuresin a sliding window approach.
Here also semanticfeatures were used as a means for feature gener-alisation.
In various experiments, linear modelsusing binary feature weights had the best perfor-mance.
A posteriori error correction via classifierstacking additionally improved the results.When comparing our results to the results ofJancsary et al (2008), who pursue a multi-levelsegmentation aproach using conditional randomfields optimizing over the whole report, the locallyobtained SVM results cannot compete fully.
Onlabel chain 2, which is equivalent to segment top-ics as investigated here, Jancsary et al (2008) re-port an estimated accuracy of 81.45 ?
2.14 % onASR output (after some postprocessing), whereasour results, even with a posteriori error correction,are at least 4 percent points behind.
This is prob-ably due to the fact that the multi-level annotationemployed in Jancsary et al (2008) contains addi-tional information useful for the learning task, andconstraints between the levels improve segmenta-tion behavior at the segment boundaries.
Never-theless, our approach has the merit of employing aframework that can be trained in a fraction of thetime needed for CRF training, and classificationworks locally.An investigation on how to combine these twocomplementary approaches is planned for the fu-ture.
The idea here is to use the probability distri-butions on labels returned by our approach as (ad-ditional) features in the CRF model.
It might bepossible to leave out some other features currentlyemployed in return, thereby reducing model com-plexity.
The benefit we hope to get by doing so areshorter training time for CRF training, and, since,contrary to CRFs, SVMs are a large margin classi-fication method, hopefully the CRF model can beimproved by the present approach.AcknowledgmentsThe work presented here has been carried out inthe context of the Austrian KNet competence net-work COAST.
We gratefully acknowledge fund-ing by the Austrian Federal Ministry of Economicsand Labour, and ZIT Zentrum fuer Innovation undTechnologie, Vienna.
The Austrian Research In-stitute for Artificial Intelligence is supported bythe Austrian Federal Ministry for Transport, Inno-vation, and Technology and by the Austrian Fed-eral Ministry for Science and Research.ReferencesASTM International.
2002.
ASTM E2184-02: Stan-dard specification for healthcare document formats.C.-C. Chang and C.-J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.csie.ntu.edu.tw/ cjlin/libsvmR.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for largelinear classification.
Journal of Machine LearningResearch, 9(2008):1871?1874.C.
Fellbaum.
1998.
WordNet: an electronic lexicaldatabase.
MIT Press, Cambridge, MA.M.
Huber, J. Jancsary, A. Klein, J. Matiasek, H. Trost.2006.
Mismatch interpretation by semantics-drivenalignment.
Proceedings of Konvens 2006.J.
Jancsary, A. Klein, J. Matiasek, H. Trost.
2007.Semantics-based Automatic Literal ReconstructionOf Dictations.
In Alcantara M. and DeclerckT.(eds.
), Semantic Representation of Spoken Lan-guage 2007 (SRSL7) Universidad de Salamanca,Spain, pp.
67-74.J.
Jancsary, J. Matiasek, H. Trost.
2008.
Reveal-ing the Structure of Medical Dictations with Con-ditional Random Fields.
Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing.
Association for ComputationalLinguistics, pp.
1?10.T.
Joachims.
1998.
Text Categorization with Sup-port Vector Machines: Learning with Many Rele-vant Features.
Proceedings of the European Confer-ence on Machine Learning.
Springer, pp.
137?142.D.A.B.
Lindberg, B.L.
Humphreys, A.T. McCray.1993.
The Unified Medical Language System.Methods of Information in Medicine, (32):281-291.Lawrence Philips.
1990.
Hanging on the metaphone.Computer Language, 7(12).V.N.
Vapnik 1995.
The Nature of Statistical LearningTheory.
Springer.25
