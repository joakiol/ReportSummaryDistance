Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1548?1558,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsGood News or Bad News: Using Affect Control Theory to Analyze Readers?Reaction Towards News ArticlesAreej AhothaliDavid R. Cheriton Schoolof Computer ScienceUniversity of WaterlooWaterloo, Ontario, N2L3G1aalhotha@cs.uwaterloo.caJesse HoeyDavid R. Cheriton Schoolof Computer ScienceUniversity of WaterlooWaterloo, Ontario, N2L3G1jhoey@cs.uwaterloo.caAbstractThis paper proposes a novel approach to sen-timent analysis that leverages work in soci-ology on symbolic interactionism.
The pro-posed approach uses Affect Control Theory(ACT) to analyze readers?
sentiment towardsfactual (objective) content and towards its en-tities (subject and object).
ACT is a theory ofaffective reasoning that uses empirically de-rived equations to predict the sentiments andemotions that arise from events.
This theoryrelies on several large lexicons of words withaffective ratings in a three-dimensional spaceof evaluation, potency, and activity (EPA).The equations and lexicons of ACT were eval-uated on a newly collected news-headlinescorpus.
ACT lexicon was expanded using a la-bel propagation algorithm, resulting in 86,604new words.
The predicted emotions for eachnews headline was then computed using theaugmented lexicon and ACT equations.
Theresults had a precision of 82%, 79%, and 68%towards the event, the subject, and object,respectively.
These results are significantlyhigher than those of standard sentiment analy-sis techniques.1 IntroductionNatural language texts are often meant to expressor impact individuals?
emotions.
Recognizing theunderlying emotions expressed in or triggered by atext is essential to understanding the full meaningthat a message conveys.
Sentiment analysis (SA)researchers are increasingly interested in investigat-ing natural language processing techniques as wellas emotion theories to identify sentiment expres-sions in natural language texts.
They typically focuson analyzing subjective documents from the writer?sperspective using frequency-based word representa-tions and mapping them to categorical labels or sen-timent polarity, in which text (e.g., sentence or doc-ument) is associated, respectively, with either a de-scriptive label or a point in a continuum.In this paper, we focus on analyzing objectivestatements.
Unlike subjective statements, that con-tain an explicit opinion or belief about an object oraspect (e.g., ?Seventh Son is a terrible movie?
), ob-jective statements present only factual information(e.g., ?protestors were arrested in NYC?).
Despitethe limited research on fact-based sentiment analy-sis, many factual statements may carry or evoke sen-timents (e.g., news articles, blog comments.
etc.
).Thus, there is a need for a new approach that at-taches sentiment scores to objective (factual) state-ments as well as to the components that make themup.
For example, a sentence like ?x kills y?
willclearly evoke a negative sentiment for the reader, andhighly negative (yet different) sentiments towardseach x and y (angry about x and sorry about y).
Fur-ther, our affective evaluation of each entity partici-pating in the event might affect our judgement of thesituation.
For example, if we know that x is a victimand y is a criminal, then the triggered sentiment willbe more positive in general, positive towards x, butstill negative towards y.The proposed method in this paper builds acontextual model that maps words to a multi-dimensional emotion space by using Affect Con-trol Theory (ACT).
ACT is a social psychological1548theory of human social interaction (Heise, 2007).ACT proposes that peoples?
social perceptions, ac-tions, and emotional experiences are governed by apsychological need to minimize deflections betweenculturally shared fundamental sentiments about so-cial situations and transient impressions resultingfrom the dynamic behaviours of interactants in thosesituations.
Each event in ACT is modeled as atriplet: actor, behavior, and object.
Culturally shared?fundamental?
sentiments about each of these ele-ments are measured in three-dimensions: Evalua-tion, Potency, and Activity (EPA).
The core idea ofACT is that each of the entities (actor, behaviourand object) participating in an event has a fun-damental affective sentiment (EPA value) that isshared among members of a culture, and the com-bination of entities in the event generates a tran-sient impression or feeling that might be differ-ent from the fundamental sentiment.
Transient im-pressions evolve over time according to empiri-cally measured temporal dynamics.
Emotions arefunctions of the difference between fundamentalsentiments and transient impressions.
EPA pro-files of concepts can be measured with the seman-tic differential (Osgood, 1957), a survey techniquewhereby respondents rate affective meanings of con-cepts on numerical scales with opposing adjec-tives at each end (e.g., {good, nice}?
{bad, awful}for E; {weak, little}?
{strong, big} for P; {calm,passive}?
{exciting, active} for A).
Affect controltheorists have compiled databases of a few thousandwords along with average EPA ratings obtained fromsurvey participants who are knowledgeable abouttheir culture (Heise, 2010).
For example, the cultur-ally shared EPA for ?mother?
in Ontario, Canada,is [2.74, 2.04, 0.67], which is quite good, quite pow-erful, and slightly active.
The ?daughter?
EPA is[2.18,?0.01, 1.92], which is quite good, but lesspowerful and more active than ?mother?.ACT is advantageous for sentiment analysis ap-plications, especially those that aim to analyze de-scriptive events or factual content for the follow-ing reasons: (1) EPA space is thought to providea universal representation of individuals?
sentiment;(2) the transient feeling accumulates over the courseof several events and, hence, it can model largerstructures in text; (3) impression formation can becomputed towards different entities in the sentence,giving a more fine-grained description of the senti-ments; (4) the EPA values are empirically driven, re-flecting a real evaluation of human participants; and(5) the interaction between terms in ACT is compat-ible with the linguistic principle of a compositionalsemantic model, which states that the meaning of asentence is a function of its words (Frege, 1892).Our method uses the impression and emotion for-mation equations of ACT to predict human senti-ments towards events.
We decompose each sentenceinto subject-verb-object and then associate subjectwith actor, verb with behaviour, and object with ob-ject in ACT.
We then compute the predicted emotiontowards each of actor, behaviour and object usingACT equations.
We use a semi-supervised methodbased on Wordnet-similarities to assign EPA valuesto words that are not in ACT lexicons.
We evalu-ated the viability of using ACT in sentiment anal-ysis on a news headlines dataset that we collectedand annotated.
This approach yielded a precisionbetween 71% and 82% on the news headline dataset.These results are significantly higher than those re-sults from a method that was trained using bag-of-words and Sentiwordnet lexicon.The rest of the paper is organized as follows: thefirst section presents the related work in emotionselicitation and sentiment analysis fields; the follow-ing two sections describe the proposed method, pro-viding details about affect control theory, impres-sion formation equations, and the lexicon inductionmethod; the last section presents the dataset used inthis paper, and discusses the results obtained usingour proposed methods.2 Related WorkEmotions have been studied extensively in disci-plines like anthropology, psychology, sociology, andmore recently in computer science.
In recent years,fields like affective computing (AC) (Picard, 2000),human-computer interaction (HCI) (Brave and Nass,2002), and sentiment analysis (Feldman, 2013) arealso contributing to this area of research, by compu-tationally modeling and evaluating existing theories.Most of the proposed methods in SA and AC haveused the appraisal theory by representing emotionswith discrete labels such as happy, sad, etc.
(Ek-man, 1992).
Only several studies have adopted di-1549mensional theories by representing emotions (coreaffects) in three- or two-dimensional space: arousal,valence, and sometimes dominance (Russell, 2003).Although psychological evidence show that wordsor any other events are emotionally perceived inmore than one dimension (Barrett, 2006; Russell,2003), multi-dimensional models have rarely beenused in sentiment analysis.In sentiment analysis research, two main machinelearning (ML) methods are used: supervised learn-ing or unsupervised learning methods.
Supervisedlearning methods generally use the occurrence fre-quencies of words that indicate opinion/sentimentand then classify these occurrences as either posi-tive or negative using one of the common classifica-tion methods (e.g., Maximum Entropy, support vec-tor machine (SVM)).
Many combinations of featureshave been tried, such as part-of-speech tagging, n-grams, term weighting, sentiment lexicons, termpresence, and syntactic dependencies (Pang et al,2002; Lin and He, 2009).
In contrast, unsupervisedlearning approaches often determine the semanticorientation (SO) of a sentence or document by calcu-lating the difference between the point-wise mutualinformation (PMI) (Church and Hanks, 1990) for itsphrases or words with sentiment seeds (Turney andLittman, 2002; Turney, 2002).Other approaches have built more-structuredmodels, augmenting the standard bag-of-wordstechnique with appraisal groups, which are repre-sented as sets of attribute values in a semantic taxon-omy (Whitelaw et al, 2005) or considering the inter-action between words by using a tree structured CRFbased on (Nakagawa et al, 2010).
Another studyhas taken the interaction between words into accountby utilizing the compositional formal semantic mod-els based on Frege?s principle (Frege, 1892).
Aframework is proposed in (Coecke et al, 2010) inwhich a sentence vector is a function of the Kro-necker product of its words.
This approach was eval-uated on several datasets and showed promising re-sults and improvement over non-compositional ap-proaches (Grefenstette et al, 2010; Grefenstette andSadrzadeh, 2011; Mitchell and Lapata, 2008).Much sentiment analysis work has focused onextracting emotions from the writer?s perspective;only several recent studies have tackled the prob-lem of predicting readers?
sentiments.
Both (Lin etal., 2008) and (Yang et al, 2009) used bag-of-wordsand linguistic features to classify Chinese news arti-cles (Yahoo!-Kimo news) into one of the user emo-tional ratings (happy, sad, surprise, etc.).
Anotherstudy presents a multi-label classification approachthat uses words?
polarity, and semantic frame fea-tures to classify news article into categorical emo-tions (Bhowmick, 2009).
As a part of the SemEval-2007 task, a number of approaches have been pro-posed to binary-classify news headlines into posi-tive or negative sentiment (Strapparava and Mihal-cea, 2007; Katz et al, 2007; Chaumartin, 2007).3 ACT for Sentiment Analysis3.1 BackgroundAffect Control theory (ACT) is a new version ofsymbolic interactionism (Mead, 1938) created byDavid Heise (Heise, 1987).
ACT proposes that indi-viduals process gestures (including words or events)as symbols or concepts shared among groups ofpeople or culture.
These fundamental meaningsor ?fundamental sentiments?
are defined in a three-dimensional space of EPA profile.
ACT also pro-poses that individuals try to maintain the transientimpressions, which are generated from the interac-tions of the events?
elements (subject-verb-object),close to the fundamental sentiments in the EPAspace.
ACT models emotions as arising because ofthe differences between fundamental sentiments andtransient impressions.
For example, a person whois powerful but is made to feel powerless will feel?angry?.
ACT equations (i.e., impression forma-tion equations) are obtained through empirical stud-ies, and they model the process by which the fun-damental sentiments of elements in the events arecombined to generate a transient impression.The affective meanings in ACT consist of threecomponents: Evaluation (good versus bad); Potency(powerful versus powerless); and Activity (livelyversus inactive).
Each affective meaning is mea-sured on a scale from -4.3 (infinitely bad, power-less, or inactive) to +4.3 (infinitely good, powerful,or lively).
These meanings are attached to conceptscorresponding to identities, behaviors, settings, andmodifiers.
According to ACT, people from the samecultures and gender share the same fundamental sen-timents (EPA) about world concepts (Berger and1550Zelditch, 2002; Heise, 2007).Lexicons of EPA values have been gathered forthousands of words from different languages andcultures including Germany, Japan, Canada and theUSA.
In general, within-cultural agreement aboutEPA meanings of social concepts is high even acrosssubgroups of society, and cultural-average EPA rat-ings from as little as a few dozen survey participantshave been shown to be extremely stable over ex-tended periods of time (Heise, 2010).
These findingsmay seem surprising in light of societal conflicts asevidenced, for instance, by competing political ide-ologies.
Research has consistently shown that thenumber of contested concepts is small relative to thestable and consensual semantic structures that formthe basis of our social interactions and shared cul-tural understanding (Heise, 2010).3.2 Affect Control TheoryIn ACT, each event has at least three elements: ac-tor (subject, S), behavior (verb, V), and object (O).Each of these elements is represented by three values(EPA) that capture the fundamental sentiments theyevoke in terms of evaluation, potency, and activity.The fundamental sentiment of an event according toACT grammar is a nine-dimensional vector:f = {SeSpSeVeVpVaOeOpOa}where e.g.
Serepresents the fundamental sen-timent about the subject (S) on the evaluation (e)dimension.
The transient impression evoked by anevent is another 9D vector:?
= {S?eS?pS?eV?eV?pV?aO?eO?pO?a}where fundamental EPA values are denoted bynon-primed symbols and post-event EPA values aredenoted by primed symbols.
The transient impres-sion ?
is computed by multiplying t, a vector of fea-tures that are combinations of terms from the fun-damental sentiment f , by a matrix M of predictioncoefficients estimated by impression-formation re-search.t = (1 SeSpSaVeVpVaOeOpOaSeVeSeVpSeVaSpVeSpVpSpOaSaVaVeOeVeOpVpOeVpOpVpOaVaOeVaOpSeVeOeSeVpOpSpVpOpSpVpOaSaVaOa)?
=Mt (1)For example, the transient impression of the sub-ject?s valence, S?e, using US male coefficients:S?e= ?.98 + .48 Se?
.015 Sp?
.015 Sa+ .425 Ve?
.069 Vp?
.106 Va+ .055Oe...This part of the equation shows that our evaluationof the subject/actor is affected mainly by how thevalence of this person and action are perceived byothers (positive large coefficients .48 and .425 forSeand Ve).
It also shows that powerful actors (sub-ject) or behaviours (verb) are seen a bit negatively(negative coefficients -.015, -.069 for Spand Vp).We also can incorporate the location (settings),which indicates where the event took place such asschool, country, and etc., can be achieved by addingthe EPA values for the setting and use the coefficientvalues of the subject-verb-object-location (SVOL)grammar instead of subject-verb-object (SVO).Modifiers in ACT are adjectives or attributes thatmodify actor or object (e.g.
?good friend?
or ?abu-sive father?).
The impression generated from combi-nations of identity with modifiers can be calculatedas a linear combination of the EPA values of boththe identity and modifiers.c = B1p+B2i (2)where p = {Pe, Pp, Pa}, i = {Ie, Ip, Ia}, andc = {Ce, Cp, Ca} are the EPA profiles for the modi-fier, identity, and the combination, respectively, andB1and B2are coefficients estimated from surveydata.
For example, the ?father?
affective rating is[1.84, 1.78, 0.02], ?abusive?
is [?2.23, 0.34,?0.02],and ?abusive father?
is [?1.51, 1.37,?0.21].The deflection, which is defined as the discrep-ancy between the fundamental sentiment and thetransient impression, is calculated by the squaredEuclidean distance between the sentiments and im-pressions given the following equation.d =?i(fi?
?i)2(3)The deflection does not indicate positive or nega-tive emotions, but rather indicates whether or not theevent met someone?s expectation.1551In ACT, the emotion triggered by an event is afunction of the fundamental identity for actor orobject, i ?
{Se, Sp, Sa} or i ?
{Oe, Op, Oa},and the transient identity for actor or object, i??
{S?e, S?p, S?a} or i??
{O?e, O?p, O?a}.
ACT uses thefollowing equation to predict emotions, with empir-ically measured coefficients as follows:?
?
E (i??
I i?
?)
(4)where E is a 3?
3 matrix coefficient of the emo-tion profile, I is a 3?3 matrix coefficient for identity,and ?
is a vector of equation constants.3.3 Emotion Elicitation Using ACTWe implement ACT to predict the triggered senti-ment of a single sentence as follows: first we ex-tract the subject, verb, object, setting, and modi-fiers (adjectives of subject and object) and look themup in the augmented ACT lexicon (see section 3.4)to get EPA values (fundamental sentiments, f ) foreach word (MacKinnon, 2006).
We next computethe transient impression ?
using f and Equations 1and 2.
After that, we compute the deflection usingEquation 3, and the emotion towards the subject andobject using Equation 4.
We then map the resultingEPA scores for emotion to the nearest emotions la-bel in ACT.
The ACT dataset (MacKinnon, 2006)has 135 emotion labels, each with an EPA score(e.g., delighted= [2.04, 0.96, 1.48]), and we findthe closest label using a Euclidean distance mea-sure.
We then compare these emotions ?
towardssubject and object to corresponding ground truth inthe news headline dataset (see Section 4) using rootmean squared error (RMSE) and mean absolute er-ror (MAE).
We also discretize the predicted ?
andthe ground truth into negative or positive EPA val-ues, and compare accuracy of the discretized values.To extract the sentence?s quintet (i.e., subject,verb, object, modifiers, and settings), we imple-ment a search algorithm that takes a treebank parsetree (Socher et al, 2013) and returns a (subject,predicate , and object) triplet (Rusu et al, 2007).As Rusu et al?s algorithm searches a parse tree ofgrammatically correct English sentences, we makesome alterations to consider news headlines?
gram-mar.
News headlines are often written to be shortand precise (i.e., often not grammatically correct),and usually consist of several noun phrases withoutarticles or the verb ?to be?.
They are written in thepresent tense for current or past events (e.g, terrorstrikes police base).
The past tense verbs are rarelyused in news headlines, whereas passive voice sen-tences are common.
The passive voice sentences areoften written without an auxiliary verb, which makeit hard for standard parsers to distinguish their verbsfrom past tense verbs and to extract the triplet accu-rately (e.g, Six killed in accident).Our algorithm takes a parse tree and performs abreadth-first search and identifies the last noun de-scendent of the first noun phrase (NP) in the sen-tence as the subject and the previous descendent asthe attributes (in Rusu et al?s algorithm the subjectis the first noun in (NP)).
For example, in the sen-tence Super Bowl-winning quarterback Russell Wil-son divorces wife, the actor/subject ?Russell?
is thelast noun in the first noun phase and the previous ad-jectives and nouns are attributes (modifiers) of thesubject.
To locate the verbs, (similar to Rusu et al?salgorithm) the algorithm searches the deepest verbphrase (VP) and returns the first verb (VB) descen-dent.
If the verb is in the past tense, we transform itto passive voice.
The algorithm (similar to Rusu etal.
?s algorithm) returns the object that is co-locatedwith the verb in the deepest verb phrase (VP).
Toextract the settings, we look for a noun phrase (NP)sub-tree that has a preposition (at, in, on) and returnthe last noun.
This algorithm yields accuracies of43%, 53% , and 26% with the ground truth (users?annotations of the subject, verb, and object).We also consider whether the verb type is tran-sitive, which directly indicates positive or negativesentiment toward something (e.g., x killed y), orintransitive, which transfers sentiments into nouns(e.g., x provides help to y).
For intransitive verbs,we choose the second verb as the behavior (verb)in the sentence (e.g., ?x provides help to y?
will be?x helps y?).
We also used part-of-speech taggingto determine the elements of an event and to iden-tify the places and names.
The gender of the namesis considered by training a na?
?ve Bayes classifier onnames-gender corpus of 5001 female and 2943 malenames1which yielded an accuracy of 86% on clas-sifying names according to their gender.1www.nltk.corpus15523.4 EPA lexicon InductionThe method described in the last section relies ona lexicon that maps words to EPA values.
TheACT lexicon we used originally contains 2,293words (original-EPA-lexicon) (MacKinnon, 2006).We augmented this lexicon by adding the Affec-tive Norm for English Words (ANEW) (Bradley andLang, 2010) data-set that contains 2,476 Englishwords, and the Warriner et al data-set (Warriner etal., 2013) that contains 13,915 words.
ANEW andWarriner et al data-sets were both scaled from therange of [1,9] to the range of [-4.3,4.3] using max-min scaling formula (Han, 2012).
The min-maxnormalization preforms a linear transformation for agiven value xiof A with a minimum and maximumvalue of [minA, maxA] to x?iin range of [minB,maxB] given this formula:x?i=xi?minAmaxA?minA(maxB?minB) +minBAdding ANEW and Warriner lexicons generatedlexicon of 17,347 words (extended-EPA-lexicon).We then randomly divided the extended-EPA-lexicon into a training-EPA-lexicon and a testing-EPA-lexicon, with 5,782 and 11,565 words, respec-tively.
We used the training-EPA-lexicon to addmore words, using a graph-based semi-supervisedlearning method called label-propagation, a tech-nique that has been commonly used for NLP (Chenet al, 2006; Niu et al, 2005; Rao and Ravichan-dran, 2009; Zhu and Ghahramani, 2002; Blair-Goldensohn et al, 2008), and image annotation (Caoet al, 2008; Heckemann et al, 2006).
Label-propagation is a transductive algorithm that propa-gates information from a set of labeled nodes (seedsets) to the rest of the graph through its edges (Zhuand Ghahramani, 2002).
The label-propagation al-gorithm starts by adding all the synonyms and lem-mas in WordNet of a specific part-of-speech (verb,noun, adjective, or adverbs) to the training-EPA-lexicon.
This generates a set of labeled wordsL = (Xl, Yl) (the 5,782 words with EPA labels),and unlabeled words U = (Xu, Yu), from whichwe constructed undirected weighted graph G ={E, V,W}, where V is a set of vertices (all thewords in the set), E is the weighted edges, and W isan n ?
n weight matrix (affinity matrix) n equal tothe size vocabulary |V |.We initialized the labeled nodes/words with theEPA value of the words observed in the trainingset, and the unlabeled nodes/words with zeroes.We computed the weight matrix using the WordNetsimilarity (Wu and Palmer, 1994) between the twowords xiand xj.
Wu and Palmer?s similarity is equalto the depth of the least common subsumer (LCS,the least common ancestor) divided by the summa-tion of the depth of the two words in the WordNettaxonomy.simwup(w1, w2) =2 ?
depth(LCS)depth(w1) + depth(w2)Each edge E ?
(vi, vj) has an associated weightTij, which is the row normalized weight wijof theedge between viand vj.
The labels are then prop-agated to adjacent nodes by computing Y ?
TY .After each iteration the labeled nodes Ylare reset totheir initial values (see Algorithm 1).Algorithm 1 ACT Label Propagationprocedure LABEL PROPAGATION(Synests C)Construct a Graph G = {V,E,W}Initialize T0and Y0matrices, i?
0repeatYi?
TYi?1Yl?
Luntil Y convergesend procedureThe label propagation algorithm generated 167adverbs, 3,809 adjectives, 11,531 verbs, and 81,347nouns, with their EPA ratings in which 10,249of them are in the testing-EPA-lexicon.
To eval-uate the validity of this approach, we compareour generated EPA ratings for these 10,249 wordswith those from the testing-EPA-lexicon (fromACT/ANEW/Warriner datasets).
The results areshown in Tables 1 and 2.
The resulting EPA areequally distributed between -4.3 and +4.3.
Weused two metrics to compare the results: root meansquared error (RMSE) and mean absolute error(MAE).
These metrics were both close to 1.0 for E,P, and A, suggesting that there is a reasonable de-gree of agreement between the induced and manu-ally labeled EPA values.
It is worth mentioning thatdue to the limited numbers of adverbs in the ACT1553lexicon, and because the Wordnet-similarity mea-sure that compares only words of the same part-of-speech, only several adverbs were generated usinglabel-propagation algorithm.POS W RMSE MSAE P A E P AAdjectives 378 1.2 1.2 0.9 0.9 1.0 0.8Adverbs 5 1.0 1.1 1.3 0.7 0.8 0.9Verbs 2,787 1.2 1.1 0.8 1.0 0.9 0.6Noun 7,079 1.3 1.1 0.9 1.0 0.9 0.7Total 10,249 1.3 1.1 0.9 1.0 0.9 0.7Table 1: The results of comparing the induced lexi-con using label propagation and ground truth EPA val-ues (POS= part-of-speech, W= the number of the in-duced words, MAS=mean absolute error, and RMSE=root mean squared error4 DatasetsWe evaluated the proposed method on a newly col-lected news-headlines dataset.
We collected 2080news-headlines from a group of news websitesand archives (BBC, CNN, Reuters, The Telegraph,Times, etc).
The news headlines were selected ran-domly from the period from 1999 to 2014.
ThroughMechanical Turk, we recruited participants locatedin North America with more than 500 approved hitsand an approved rate above 90%.
We asked theparticipants to locate the subject (actor), behavior(verb), and object of each sentence and to indicatetheir emotions towards them and towards the event(as a whole) in the EPA format ?
[?4.3,+4.3](where -4.3 indicates strongly negative EPA valueand +4.3 indicates strongly positive EPA value).
Thedataset was annotated by at least three judges perheadline.
We excluded any ratings that were filledwith blanks, zeros, or similar values in all the fields.We also excluded the answers that did not have theappropriate subject, verb, or object form (e.g., be-havior=Obama, subject=as).
We also excluded allthe answers rated by less than three participants.This screening resulted in 1658 headlines that hada mean EPA rating of 0.80, 1.04, and 1.02.
Of these,995 headlines had a positive evaluation score and663 headlines had a negative evaluation score.
Someexamples from this dataset can be seen in Table 5.Words Testing-EPA-lexicon LP-lexiconIncapable (adj.)
[-1.83, -1.40, -0.54] [-1.56, -1.18, -2.59]Wrongly (adv.)
[-1.96, -0.22, 0.17] [-2.02, -0.23, 0.18]Gauge (v.) [0.12, -0.55, 0.13] [0.18, -1.61, 0.25]Loser (n.) [-1.30, -1.75, 0.30] [-1.14, -1.52, 0.28]Table 2: Words and their EPA ratings from Testing-EPA-lexicon and LP-lexicon=label propagation lexicon5 ResultsOur model (the augmented lexicon, and ACT equa-tions) was evaluated in predicting the evoked senti-ment towards the headlines as a whole by comparingthe discretized evaluation (E) score ?
{0, 1} (where0/1 indicates negative/positive emotions, resp.)
ofthe generated EPA to the ground truth.
This evalu-ation was performed using different configurations(Table 3): (1) Using users?
annotated triplet (ACT-UA) (i.e., subject, verb and object), the modelyielded a precision of 75% compared to the groundtruth.
(2) Using the parse tree triplet (ACT-PTT)(see Section 3.3 for details), the precision droppedto 71%.
(3) Adding the adjectives ( modifiers )and settings to the subject, verb and object, whichwe will called parse tree quintet (ACT-PTQ) yieldeda higher precision, with 82% precision in compar-ison with the corresponding ground truth.
Theseresults were also compared to the results obtainedfrom a standard sentiment classifier (STD-calssifier)that uses occurrence frequencies of positive vs. neg-ative words using SentiWordNet (Das and Bandy-opadhyay, 2010).
This classifier yielded a precisionof 57% in comparison to the ground truth (Table 3).The parse tree quintet (ACT-PTQ) were also usedto evaluate the ACT predicted emotions towards theactor (subject) and the object in the headline (Ta-ble 4).
Three metrics were used in this evalua-tion: precision, MAS, and RMSE.
We used preci-sion to compare the disctized EPA scores ?
{0, 1}and MAS and RMSE to compare the real EPAscores ?
[?4.3,+4.3].
As shown in Table 4, theRMSE and MAS are almost all less than 1.5, andthe precision varies from 67% to 79% across dis-cretized E,P,A for subject and object.
To put theseresults in context, a difference of 1.4 in the EPAspace would equate to the difference between ?ac-cusing?
someone ({?1.03, 0.26, 0.29}) and ?pun-1554Classifier Precision Recall F1-scoreACT-PTQ 82 67 73ACT-UA 75 62 67ACT-PTT 71 63 66STD-classifier 57 51 53Table 3: Results for sentiment classification of newsheadlines dataset using ACT and standard sentiment clas-sification method, ACT-PTQ = ACT using the parsingtree quintet, ACT-UA = ACT using users?
annotation,ACT-PTT= ACT using the parsing tree triplet, STD-classifier= Standard classifier using SentiWordNetEmotions Precision MAS RMSEE P A E P A E P AETS 79 74 76 1.11 1.25 1.27 1.38 1.24 1.33ETO 67 68 67 1.09 1.26 1.27 1.33 1.56 1.54Table 4: Comparison of predicted emotions towards thesubject and object with the ground truth.
ETS/ETO =Emotions towards subject/object, MAS=mean absoluteerror, and RMSE= root mean square errorishing?
someone ({0.19, 0.79, 0.76}), or betweenthe identity of ?mother?
({2.48, 1.96, 1.15}) and?girl?
({1.96, 0.67, 0.99}), or between the emo-tion of ?joyful?
({2.43, 1.97, 1.33}) and ?euphoric?
({1.42, 1.09, 0.99}).
These words seem quite closein an affective sense, which indicates that our senti-ment analysis method is able to uncover sentimentsat a level that is reasonable on an intuitive level, andshows the method?s power in uncovering sentimentsabout specific elements of sentences.6 Discussion and Future WorkHuman emotions are more complicated than severallabels or binary scores.
ACT models emotions ina three-dimensional space which is found to be acomprehensive and universal representation of hu-man emotions, and models emotions towards event-or fact-based sentences as a combination of severalemotions towards their entities (subject and object).To evaluate the effectiveness of using ACT in senti-ment analysis, we chose to analyze news headlinesas they represent real-world statements that describesingle or multiple events/facts.Analysing sentiment in news headlines is a chal-lenging task for several reasons: (1) news headlinesare ungrammatically structured, which makes it hardfor standard parsers to extract their words?
part-of-speech and dependency correctly; (2) they are writ-ten to be short and precise, providing little infor-mation for typical bag-of-word classifiers to workproperly; (3) they are objective, containing wordsthat might not exist in the commonly used sentimentlexicon.
To overcome the limitation of news head-line sentiment analysis, three main contributions thatwe present in this paper: (1) we extracted the sen-tences?
triplets by considering the headline grammarand structure; (2) we augmented ACT lexicon usinglabel-propagation and word similarity; (3) we modelthe interaction between the words in the sentence bymodelling the transient and fundamental sentiments.The label propagation algorithm generated 96,853words in which 10,249 of them are in the testing dataset.
The EPA values of these words were found to bequite close in the affective space to their correspond-ing ground truth, Table 1.
Table 2 also shows sev-eral good examples of the generated EPA and theircorresponding ground truth.
The label-propagationresults could be further improved by adding wordsantonyms and by employing another similarity mea-sure.
The results of predicting the sentiment towardsthe event and their entities as shown in Tables 4, 3,and 5 are computed using only the ACT lexiconsand the ACT impression formation equations.
Withsuch a simple, parsimonious and theoretically well-grounded approach, we are able to compute fine-grained sentiment analysis in a dimensional spacethat is known to be a universal representation of hu-man affect.
Mapping these three-dimensional EPAscores to a specific emotion provides a detailed labelfor objects and subjects within a sentence, as shownin Table 5.
For example, in a sentence like?
Russiasays 4 militants killed in Dagestan siege?, the readerwill be feeling negative, yet different emotions to-wards the subject ?furious?
and the object ?sorry?.Table 5 (obtained with ACT-PTQ) shows the de-flection (d), emotions towards the events (?
), and to-wards the subject and object (ea) and (eo) of someof the examples in the data set.
As shown in Ta-ble 5, we can see that the deflection (d) is very highwhen we do not expect an event to occur (e.g.,?Babydies after being left in car for over 8 hours?).
Thedeflection in this sentence is high (17.42) becausethe EPA value of the object ?Baby?
is equal to1555Headline d ETS ETO Te?
eseoPress sees hope in Mecca.
.
.
.
.talks 2.57 happy reverent 1.33 2.50 1.53 1.59Brazil deploys troops to secure borders for World.
.
.
.
.Cup 2.32 proud apathetic 1.7 1.61 0.66 1.22Gunfire injures three Napoli fans 6.80 furious melancholy -1.13 -0.86 -1.25 0.54Three political candidates slain before Iraqi.
.
.
.vote 11.24 furious sorry -1.33 -1.46 -1.80 0.05Lily Allen wins web music award 2.74 proud reverent 1.67 2.45 1.46 1.36Finland Air crash kills skydivers 12.54 furious cheerless -1.33 -3.20 -3.0 -0.10Bomb kills 18 on military.
.
.bus in Iran 3.40 impatient overwhelmed -1.6 -1.23 -2.3 -0.11Russia says 4 militants killed in Dagestan siege 11.37 furious sorry -0.2 -1.46 -2.34 -0.58Baby dies after being left in car for over 8. .
.
.
.
.hours 17.42 furious overwhelmed -1.67 -2.10 -1.57 0.06Female astronaut sets record 0.79 contented reverent 3.50 0.91 1.37 0.46Table 5: ACT model?s results on news headlines, d=deflection, ETS, ETO= emotion towards subject and object, Te=emotions towards the event (ground truth), ?=emotions towards the event (ACT), and es, eo= the evaluation value ofthe emotion towards subject and object.
Parse elements are coded as: subject, verb, object, and.
.
.
.
.
.
.setting.
[2.40,?2.28, 2.58], which is considered to be quitegood, quite weak, and quite active identity and a caris considered to be a quite positive place, with EPAvalue equal to [1.62, 1.65, 2.01].
While if an eventtook place in a war zone or if the subject has nega-tive evaluation, the deflection will not be very high(e.g., ?Bomb kills 18 on military bus in Iran?)
thedeflection is equal to 3.40 because ?Bomb?
has anegative evaluation.
In Table 5, we can see the emo-tions (?)
and the ground truth evaluation toward theevents (Te) are often quite close.The aforementioned results are obtained by ex-tracting single subject, verb, object, modifier, andsetting.
These results could further improved bytaking adverbs (e.g, ?lived happily?
), phrasal verbs(e.g,?get alng?
and ?get back?
), numbers (e.g,?45killed?
), and negations (e.g.,?no more funding?
)into consideration.
Finally, accumulating the emo-tions of multiple consequent behaviors could also bevery useful.
For example, in the sentence ?Man ar-rested after beating cops in a restaurant?
the behav-ior will be ?arrested?, and ?beating?
is not takeninto account.
We could address this using morecomplex parse trees and accumulating the emotionsof multiple behaviors by considering the previouslygenerated sentiment as the fundamental sentiment.Finally, using ACT predictions to bootstrap super-vised learning could also yield improvements.7 ConclusionsWe have proposed a new direction for senti-ment analysis, employing Affect Control Theory(ACT) to assign different emotions towards events-based/objective statements and their entities (sub-ject, object).
Unlike the majority of sentiment anal-ysis models that are trained on highly subjectivewords to obtain descriptive labels, our model in-corporates ACT, that models emotions as points inthree-dimensional space, and analyzes how objec-tive texts trigger different emotions.
We use a semi-supervised method based on Wordnet similarities tocompute emotional ratings for words not in the ACTlexicons.
Evaluated on a news headline dataset, ourmodel yielded higher accuracy than a widely usedclassifier, with a highest precision of 82%.
We alsoanalyzed the sentiment evaluation of ACT on (ac-tor/subject and the object) in the news headlines,yielding a precision of 79% and 68% when analyz-ing the emotions towards the subject and the object,respectively.
These results have been obtained with-out performing any supervised learning and with-out taking consequent behaviors, phrasal verbs, orsentence negations into account.
Thus, they demon-strate the potential of ACT for sentiment analysis.Affect control theory can also handle consequent be-haviors and modifiers.
In future, we plan to aug-ment our method with more complex levels of detail,gather more extensive datasets, and evaluate ACTfor more precise and detailed sentiments.AcknowledgmentsThe authors thank Tobias Schr?oder and Dan Lizotte fortheir thoughtful discussion and suggestions.
Areej Al-hothali acknowledges the sponsorship of King AbdulAziz University, Jeddah, Saudi Arabia.
Jesse Hoey issupported in part by the Natural Sciences and Engineer-ing Council of Canada (NSERC).1556ReferencesLisa Feldman Barrett.
2006.
Are emotions natural kinds?Perspectives on psychological science, 1(1):28?58.Joseph Berger and Morris Zelditch.
2002.
New Direc-tions in Contemporary Sociological Theories.
Row-man & Littlefield.Plaban Kumar Bhowmick.
2009.
Reader perspec-tive emotion analysis in text through ensemble basedmulti-label classification framework.
Computer andInformation Science, 2(4):P64.Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-ald, Tyler Neylon, George A Reis, and Jeff Reynar.2008.
Building a sentiment summarizer for local ser-vice reviews.
In WWW Workshop on NLP in the Infor-mation Explosion Era, volume 14.MM Bradley and PJ Lang.
2010.
Affective norms forenglish words (anew): Affective ratings of words andinstruction manual (technical report c-2).Scott Brave and Clifford Nass.
2002.
Emotion in human-computer interaction.
The human-computer interac-tion handbook: fundamentals, evolving technologiesand emerging applications, pages 81?96.Liangliang Cao, Jiebo Luo, and Thomas S Huang.
2008.Annotating photo collections by label propagation ac-cording to multiple similarity cues.
In Proceedings ofthe 16th ACM international conference on Multimedia,pages 121?130.
ACM.Franc?ois-R?egis Chaumartin.
2007.
Upar7: Aknowledge-based system for headline sentiment tag-ging.
In Proceedings of the 4th International Work-shop on Semantic Evaluations, pages 422?425.
Asso-ciation for Computational Linguistics.Jinxiu Chen, Donghong Ji, Chew Lim Tan, and ZhengyuNiu.
2006.
Relation extraction using label propaga-tion based semi-supervised learning.
In Proceedingsof the 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Associ-ation for Computational Linguistics, pages 129?136.Association for Computational Linguistics.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Computational linguistics, 16(1):22?29.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a compositionaldistributional model of meaning.
arXiv preprintarXiv:1003.4394.Amitava Das and Sivaji Bandyopadhyay.
2010.Sentiword-net for bangla.
Knowledge Sharing Event-4: Task, 2.Paul Ekman.
1992.
Are there basic emotions?Ronen Feldman.
2013.
Techniques and applicationsfor sentiment analysis.
Communications of the ACM,56(4):82?89.Gottlob Frege.
1892.
On sense and reference.
Ludlow(1997), pages 563?584.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 1394?1404.
Association forComputational Linguistics.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2010.
Con-crete sentence spaces for compositional distributionalmodels of meaning.
arXiv preprint arXiv:1101.0309.Jiawei Han.
2012.
Data mining : concepts andtechniques.
Elsevier/Morgan Kaufmann, AmsterdamBoston.Rolf A Heckemann, Joseph V Hajnal, Paul Aljabar,Daniel Rueckert, and Alexander Hammers.
2006.
Au-tomatic anatomical brain mri segmentation combininglabel propagation and decision fusion.
NeuroImage,33(1):115?126.David R Heise.
1987.
Affect control theory: Conceptsand model.
Journal of Mathematical Sociology, 13(1-2):1?33.David R Heise.
2007.
Expressive order: Confirming sen-timents in social actions.
Springer.David R. Heise.
2010.
Surveying Cultures: DiscoveringShared Conceptions and Sentiments.
Wiley.Phil Katz, Matthew Singleton, and Richard Wicentowski.2007.
Swat-mp: the semeval-2007 systems for task 5and task 14.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, pages 308?313.Association for Computational Linguistics.Chenghua Lin and Yulan He.
2009.
Joint sentiment/topicmodel for sentiment analysis.
In Proceedings of the18th ACM conference on Information and knowledgemanagement, pages 375?384.
ACM.KH-Y Lin, Changhua Yang, and Hsin-Hsi Chen.2008.
Emotion classification of online news arti-cles from the reader?s perspective.
In Web Intelli-gence and Intelligent Agent Technology, 2008.
WI-IAT?08.
IEEE/WIC/ACM International Conference on,volume 1, pages 220?226.
IEEE.Neil J. MacKinnon.
2006.
Mean affective ratings of 2,294 concepts by guelph university undergraduates, on-tario, canada.
In 2001-3 [Computer file].George Herbert Mead.
1938.
The philosophy of the act,volume 3.
Univ of Chicago Pr.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In ACL, pages 236?244.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.2010.
Dependency tree-based sentiment classificationusing crfs with hidden variables.
In Human Language1557Technologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 786?794.
Association forComputational Linguistics.Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.
2005.Word sense disambiguation using label propagationbased semi-supervised learning.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 395?402.
Association forComputational Linguistics.Charles Egerton Osgood.
1957.
The measurement ofmeaning, volume 47.
University of Illinois Press.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing-Volume 10, pages 79?86.
Associa-tion for Computational Linguistics.Rosalind W. Picard.
2000.
Affective computing.
MITpress.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter ofthe Association for Computational Linguistics, pages675?682.
Association for Computational Linguistics.James A Russell.
2003.
Core affect and the psycholog-ical construction of emotion.
Psychological review,110(1):145.Delia Rusu, Lorand Dali, Blaz Fortuna, Marko Grobel-nik, and Dunja Mladenic.
2007.
Triplet extractionfrom sentences.
In Proceedings of the 10th Interna-tional Multiconference?
Information Society-IS, pages8?12.Richard Socher, John Bauer, Christopher D Manning, andAndrew Y Ng.
2013.
Parsing with compositional vec-tor grammars.
In In Proceedings of the ACL confer-ence.
Citeseer.Carlo Strapparava and Rada Mihalcea.
2007.
Semeval-2007 task 14: Affective text.
In Proceedings of the4th International Workshop on Semantic Evaluations,pages 70?74.
Association for Computational Linguis-tics.Peter Turney and Michael L Littman.
2002.
Unsuper-vised learning of semantic orientation from a hundred-billion-word corpus.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th annualmeeting on association for computational linguistics,pages 417?424.
Association for Computational Lin-guistics.Amy Beth Warriner, Victor Kuperman, and Marc Brys-baert.
2013.
Norms of valence, arousal, and domi-nance for 13,915 english lemmas.
Behavior researchmethods, 45(4):1191?1207.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In Proceedings of the 14th ACM international con-ference on Information and knowledge management,pages 625?631.
ACM.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of the 32nd an-nual meeting on Association for Computational Lin-guistics, pages 133?138.
Association for Computa-tional Linguistics.Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-HsiChen.
2009.
Writer meets reader: Emotion analysisof social media from both the writer?s and reader?s per-spectives.
In Proceedings of the 2009 IEEE/WIC/ACMInternational Joint Conference on Web Intelligenceand Intelligent Agent Technology-Volume 01, pages287?290.
IEEE Computer Society.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learn-ing from labeled and unlabeled data with label prop-agation.
Technical report, Technical Report CMU-CALD-02-107, Carnegie Mellon University.1558
