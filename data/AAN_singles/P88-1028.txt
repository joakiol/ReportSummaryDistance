Polynomial Learnability and Locality of Formal GrammarsNaoki Abe*Department of Computer and Information Science,University of Pennsylvania, Philadelphia, PA19104.ABSTRACTWe apply a complexity theoretic notion of feasiblelearnability called "polynomial learnabillty" to the eval-uation of grammatical formalisms for linguistic descrip-tion.
We show that a novel, nontriviai constraint on thedegree of ~locMity" of grammars allows not only con-text free languages but also a rich d~s  of mildy contextsensitive languages to be polynomiaily learnable.
Wediscuss possible implications, of this result O the theoryof naturai language acquisition.1 IntroductionMuch of the formai modeling of natural language acqui-sition has been within the classic paradigm of ~identi-fication in the limit from positive examples" proposedby Gold \[7\].
A relatively restricted class of formal an-guages has been shown to be unleaxnable in this sense,and the problem of learning formal grammars has longbeen considered intractable.
1 The following two contro-versiai aspects of this paradigm, however, leave the im-plications of these negative results to the computationaltheory of language acquisition inconclusive.
First, itplaces a very high demand on the accuracy of the learn-ing that takes place - the hypothesized language mustbe exactly equal to the target language for it to be con-sidered "correct".
Second, it places a very permissivedemand on the time and amount of data that may berequired for the learning - all that is required of thelearner is that it converge to the correct language in thelimit.
2Of the many alternative paradigms of learning pro-posed, the notion of "polynomial learnability ~ recentlyformulated by Blumer et al \[6\] is of particular interestbecause it addresses both of these problems in a unified"Suppor ted  by an IBM graduate  fellowship.
The authorgrateful ly acknowledges his advisor,  Scott  Weinstein, for hisguidance and encouragement  throughout  this research.1 Some interest ing learnable subclasses of regu languageshave been discovered and studied by Angluin \[3\].
lar2For  a comprehensive survey of various parad igms related to"identif ication in the l imit" that  have been proposed to addressthe first issue, see Osheraon,  Stob and Weinstein \[12\].
As for thelatter  issue, Angluin (\[5\], \[4\]) investigates the feasible learnabil-ity of formal languages with the use of powerful oracles such as"MEMBERSHIP"  and "EQUIVALENCE" .way.
This paradigm relaxes the criterion for learning byruling a class of languages to be learnable, if each lan-guage in the class can be approximated, given only pos-itive and negative xamples, a with a desired degree ofaccuracy and with a desired egree of robustness (prob-ability), but puts a higher demand on the complexityby requiring that the learner converge in time polyno-mini in these parameters (of accuracy and robustness)as well as the size (complexity) of the language beinglearned.In this paper, we apply the criterion of polynomiallearnability o subclasses offormal grammars that are ofconsiderable linguistic interest.
Specifically, we presenta novel, nontriviai constraint on gra~nmars called "k-locality", which enables context free grammars and in-deed a rich class of mildly context sensitive grammars tobe feasibly learnable.
Importantly the constraint of k-locality is a nontriviai one because ach k-locai subclassis an exponential class 4 containing infinitely many infi-Rite languages.
To the best of the author's knowledge,~k-locaiity" is the first nontrivial constraint on gram-mars, which has been shown to allow a rich cla~s ofgrammars of considerable linguistic interest to be poly-nomiaily learnable.
We finally mention some recent neg-ative result in this paradigm, and discuss possible im-plications of its contrast with the learnability of k-locaiclasses.2 Polynomial Learnability"Polynomial learnability" is a complexity theoreticnotion of feasible learnability recently formulated byBlumer et al (\[6\]).
This notion generalizes Valiant'stheory of learnable boolean concepts \[15\], \[14\] to infiniteobjects uch as formal anguages.
In this paradigm, thelanguages are presented via infinite sequences of pos-3We hold no part icu lar  stance on the the validity of the claimthat  children make no use of negative xamples.
We do, however,maintain that the investigation of learnability of grammars fromboth positive and negative examples is a worthwhile endeavourfor at  least two reasons: First ,  it has a potent ia l  appl icat ion forthe design of natura l  language systems that  learn.
Second, it ispossible that  chi ldren do  make use of indirect negative informa-tion.4A class of g rammars  G is an exponential class if each sub-class of G with bounded size contains exponential ly (in that  size)many grammars .225itive and negative xamples 5 drawn with an arbitrarybut time invariant distribution over the entire space,that is in our case, ~T*.
Learners are to hypothesizea grammar at each finite initial segment of such a se-quence, in other words, they are functions from finite se-quences of members of ~2"" x {0, 1} to grammars.
6 Thecriterion for learning is a complexity theoretic, approx-imate, and probabilistic one.
A learner is s~id to learnif it can, with an arbitrarily high probability (1 - 8),converge to an arbitrarily accurate (within c) grammarin a feasible number of examples.
=A feasible num-ber of examples" means, more precisely, polynomial inthe size of the grammar it is learning and the degreesof probability and accuracy that it achieves - $ -1 and~-1.
=Accurate within d' means, more precisely, thatthe output grammar can predict, with error probability~, future events (examples) drawn from the same dis-tribution on which it has been presented examples forlearning.
We now formally state this criterion.
7Def in i t ion  2.1 (Po lynomia l  Learnab i l i ty )  A col-lection of languages ?
with an associated 'size' f~nctionwith respect o some f~ed representation mechanism ispolynomially learnable if and onlg if: s3 fE~3 q: a polynomial functionYLtE?Y P: a probability measure on ET*Ve, 6>OV m >_.
q(e- ' ,  8 -~, s i ze (Ld)\ [P ' ({ t  E CX(L~) I P (L ( f ( t~) )AL~)  < e})>_1-6and f is computable in time polynomialin the length of input\]Identification in the LimitErrorTime|trot?
TlmoFigure 1: Convergence behaviourin the limit" and =polynomial learnability ", require dif-ferent kinds of convergence behavior of such a sequence,as is illustrated in Figure 1.Blumer et al (\[6\]) shows an interesting connectionbetween polynomial learnability and data compression.The connection is one way: If there exists a polyno-mial time algorithm which reliably ?compresses ~ anysample of any language in a given collection to a prov-ably small consistent grammar for it, then such an al-ogorlthm polynomially learns that collection.
We statethis theorem in a slightly weaker form.Def in i t ion  2.2 Let ?
be a language collection with anassociated size function "size", and for each n let c,~ ={L E ?
\] size(L) ~ n}.
Then .4 is an Occam algorithmfor ?
with range size ~ f (m,  n) if and only if:If in addition all of f 's output grammars on esamplesequences for languages in c belong to G, then we saythat ?
is polynomially learnable by G.Suppose we take the sequence of the hypotheses(grammars) made by a \]earner on successive initial fi-nite sequences of examples, and plot the =errors" ofthose grammars with respect to the language beinglearned.
The two \]earnability criteria, =identificationawe let ?X(L) denote the set of infinite sequences which con-tain only posit ive and negative xamples for L, so indicated.awe let ~r denote the set of all such functions.7The following presentation uses concepts and notation offormal learning theory, of.
\[12\]aNote the following notation.
The inital segment of a se-quence t up to the n-th element is denoted by t-~.
L denotes omefixed mapping from grammars to languages: If G is a grammar,L(G) denotes the language generated by-it.
If L I is a |anguage,s l zs (L l )  denotes the size of a minimal grammar for LI.
A&Bdenotes the symmetr ic difference, i.e.
(A--B)U(B -A).
Finally,if P is a probabil i ty measure on ~-T ?, then P?
is the cannonicalproduct extension of P.VnENVLE?nVte e.X(L)Vine  N\[.4(~.)
is consistent .ith~?rng(~..)and .4(~..) ?
?
I ( - , - )and .4 runs in time polynomial in \[ tm \[\]Theorem 2.1 (B lumer  et  al.)
I1.4 is an Oceam al-gorithm .for ?
with range size f (n ,  m) ----.
O(n/=m =) forsome k >_ 1, 0 < ct < 1 (i.e.
less than linear in samplesize and polynomial in complexity of language), then .4polynomially learns f-.91n \[6\] the notion of "range dimension" is used in place of" range  size", which is the Vapmk-Chervonenkis dlmension of thehypothesis class.
Here, we use the fact that  the dimension of ahypothesis class with a size bound is at most equal to that  sizebound.10Grammar G is consistent with a sample $ if {= \[ (=, 0) Es} g L(G) ~ r.(a) n {= I (=, 1) ~ s} = ~.2263 K -Loca l  Context  F ree  GrammarsThe notion of "k-locality" of a context free grammar isdefined with respect o a formulation of derivations de-fined originally for TAG's by Vijay-Shanker, Weir, andJosh, \[16\] [17\], which is a generalization of the notionof a parse tree.
In their formulation, a derivation is atree recording the history of rewritings.
Each node ofa derivation tree is labeled by a rewriting rule, and inparticular, the root must be labeled with a rule withthe starting symbol as its left hand side.
Each edgecorresponds to the application of a rewriting; the edgefrom a rule (host rule) to another ule (applied rule) islabeled with the aposition ~ of the nonterminal in theright hand side of the host rule at which the rewritingta~kes place.The degree of locality of a derivation is the num-ber of distinct kinds of rewritings in it - including theimmediate context in which rewritings take place.
Interms of a derivation tree, the degree of locality is thenumber of different kinds of edges in it, where two edgesaxe equivalent just in case the two end nodes are labeledby the same rules, and the edges themselves are labeledby the same node address.Def init ion 3.1 Let D(G) denote the set of all deriva.tion trees of G, and let r E I)(G).
Then, thedegree of locality of r, written locality(r), is defined asfollows, locality(r) ---- card{ (p,q, n) I there is an edge inr from a node labeled with p to another labeled with q,and is itself labeled with ~}The degree of locality of a grammar is the maximum ofthose of M1 its derivations.Def init ion 3.2 A CFG G is called k.local ifma={locallty(r) I r e V(G)} < k.We write k.Local.CFG = {G I G E CFG and G is k.Local} and k.Local.CFL = {L(G) I G E k.Local.CFGExample  3.1 La = { a"bnambm I n ,m E N} EJ.LocaI.CFL since all the derivations of G1 =({S,,-,?l}, {a,b},S, {S - -  SaS1, $1 "* aSlb, Sa - -  A}) generating La havedegree of locality at most J.
For example, the derivationfor the string aZba b has degree of locality J as shownin Figure ~.A crucical property of k-local grammars, which wewill utilize in proving the learnability result, is thatfor each k-local grammar, there exists another k-localg rammar  in a specific normal form, whose size is onlyr "  locality(r) = 4S --481 S12!Sl -m SI b SI --m S1 b2SI ---m SI b S12Sl --m Sl b2$1 -~.S --~1 SI S -~I SII I1 2I ISI -st S1 b S --#a S1 bSl --~ Sl b Sl -m Sl bI l2 2I lSl --m Sl b Sl -0.Figure 2: Degree of locality of a derivation of aSb3ab yGapolynomially larger than the original grammar.
Thenormal form in effect puts the grammar into a disjointunion of small grammars each with at most k rules andk nontenninal occurences.
By ~the disjoint union" ofan arbitrary set of n grammaxs, gl,..., gn, we mean thegrammax obtained by first reanaming nonterminals ineach g~ so that the nonterminal set of each one is dis-joint from that of any other, and then taking the unionof the rules in all those grammars, and finally addingthe rule S -* Si for each staxing symbol S~ of g,, andmaking a brand new symbol S the starting symbol ofthe grAraraar 80 obtained.Lemma 3.1 (K-Local  Normal  Form) For every k-local.CFG H, if n = size(H), then there is a k-loml-CFG G such thatI.
Z (G)= L(H).~.
G is in k.local normal form, i.e.
there is an indexset I such that G = (I2r, U i?~i ,  S, {S -* Si I i EI} U (Ui?IRi)), and if we let Gi -~ (~T, ~,, Si, Ri)for each i E I,  then(a) Each G~ is "k.simple"; Vi E I \[ Ri \[<_k &: NTO(R~) <_ k. 11(b) Each G, has size bounded by size(G); Vi EI size(G,) = O(n)(c) All Gi's have disjoint nonterminal sets;vi, j ~ I(i # j) - -  r., n r~, = ?,.s.
size(G) = O(nk+:).Definit ion 3.3 We let ~ and ~ to be any maps thatsatisfy: If  G is any k.local-CFG in kolocal normal form,11If R is a set of product ion r~nlen,ith~oNeTruOl(eaR.i) denotee thenumber  ol nontermlnm occurre ea227then 4(G) is the set of all of its k.local components (Gabove.)
If 0 = {Gi \[ i G I} is a set of k-simple gram.mars, then ~b(O) is a single grammar that is a "disjointunion" of all of the k-simple grammars in G.4 K -Loca l  Context  F ree  LanguagesAre  Po lynomia l ly  Learnab leIn this section, we present a sketch of the proof of ourmain leaxnability result.Theorem 4.1 For each k G N;k-iocal.CFL is polynomially learnable.
12Proof.
"We prove this by exhibiting an Occam algorithm .A fork-local-CFL with some fixed k, with range size polyno-mial in the size of a minimal grammar and less thanlinear in the sample size.We assume that ,4 is given a labeled m-sample 13SL for some L E k-local-CFL with size(H) = n whereH is its minimal k-local-CFG.
We let length(SL) ffiE,Es length(s) = I.
14 We let S~L and S~" denotethe positive and negative portions of SL respectively,i.e., Sz + = {z \[ 3s E SL such that s = (z, 0)) andS~" = {z \[ 3s E Sr such that s= (z, I)}.
We fix a mini-mal grammar in k-local normal form G that is consistentwith SL with size(G) ~_ p(n) for some fixed polynomialp by Lemma 3.1. and the fact that a minimal consis-tent k-local-CFG is not larger than H. Further, we let0 be the set of all of "k-simple components" of G anddefine L(G) = UoieoL(Gi ).
Then note L(G) = L(G).Since each k-simple component has at most k nonter-minals, we assume without loss of generality that eachG~ in 0 has the same nonterminal set of size k, sayEk = {A1 ..... Ak}.The idea for constructing .4 is straightforward.Step 1.
We generate all possible rules that may bein the portion of G that is relevant to SL +.
That is,if we fix a set of derivations 2), one for each string inSL + from G, then the set of rules that we generate willcontain all the rules that paxticipate in any derivationin /).
(We let ReI(G,S+L) denote the restriction of 0to S + with respect o some/) in this fashion.)
We use12We use the size of a minimal k-local CFG u the size of akolocal-CFL, i.e., VL E k-iocal-CFL size(L) = rain{size(G)G E k-local-CFG L- L(G) = L}.13S?
iS a labeled m-sample for L if S _C graph(char(L)) andcm'd(S) = m. graph(char(L)) is the grap~ of the characteristicfunction of L, ~.e.
is the set {(#, 0} \] z E L} tJ {(z, 1} I z I~ L}.14In the sequel, we refer to the number of str ings in ~ sampleas the sample size, and the total  length of the str ings in a sampleas the sample length.k-locality of G to show that such a set will be polyno-mially bounded in the length of SL +.
Step 2.
We thengenerate the set of all possible grammars having at mostk of these rules.
Since each k-simple component of 0has at most k rules, the generated set of grammars willinclude all of the k-simple components of G. Step 3.We then use the negative portion of the sample, S L tofilter out the "inconsistent" ones.
What  we have at thisstage is a polynomially bounded set of k-simple gram-mars with varying sizes, which do not generate any ofS~, and contain all the k-simple grammars of G. Asso-dated with each k-simple grammar is the portion of SL +that it "covers" and its size.
Step 4.
What  an Occamalgorithm needs to do, then, is to find some subset ofthese k-simple grammmm that "covers" SL +, and has atotal size that is provably only polynomially larger thana minimal total size of a subset that covers SL +, and isless than linear is the sample size, m. We formalizethis as a variant of "Set Cover" problem which we call"Weighted Set Cover~(WSC), and prove the existence ofan approximation algorithm with a performance guar-antee which suffices to ensure that the output of .4 willbe a grammar that is provably only polynomially largerthan the minimal one, and is less than linear in thesample size.
The algorithm runs in time polynomial inthe size of the grammar being learned and the samplelength.Step 1.A crucial consequence of the way k-locality is definedis that the "terminal yield" of any rule body that isused to derive any string in the language could be splitinto at most k + 1 intervals.
(We define the "terminalyield" of a rule body R to be h(R), where h is a homo-morphism that preserves termins2 symbols and deletesnonterminal symbols.
)Def init ion 4.1 (Subylelds) For an arbitrary i E N,an i-tuple of members of E~ u~ = (vl, v2 ..... vi) is saidto be a subyield of s, if there are some uz ..... ui, ui+z EE~.
such that s = uavzu2~...ulviu~+z.
We letSubYields(i,a) = {w E (E~) ffi \[ z ~_ i ~ w is a sub-yield of s}.We then let SubYieldsk(S+L) denote the set of allsubyields of strings in S + that may have come froma rule body in a k-local-CFG, i.e.
subyields that axetuples of at most k + 1 strings.Def init ion 4.2SubYieldsk(S +) = U ,Es+Subyields(k + 1, s).Claim 4.1 ca~d(SubYie/dsk(S,+)) = 0(12'+3).Proof,This is obvious, since given a string s of length a, there228are only O(a 2(k+~)) ways of choosing 2(k -i- 1) differ-ent positions in the string.
This completely specifies allthe elements of SubYieidsk+a(s).
Since the number ofstrings (m) in S + and the length of each string in S +are each bounded by the sample length (1), we have atmost O(l) ?
0(12(k+1)) strings in SubYields~(S+L ).
r~Thus we now have a polynomially generable set ofpossible yields of rule bodies in G. The next step isto generate the set of all possible rules having theseyields.
Now, by k-locality, in may derivation of G wehave at most k distinct "kinds" of rewritings present.So, each rule has at most k useful nonterminal oc-currences mad since G is minimal, it is free of uselessnonterminals.
We generate all possible rules with atmost k nonterminal occurrences from some fixed set ofk nonterminals (Ek), having as terminal subyields, oneof SubYieldsh(S+).
We will then have generated allpossible rules of  Rel(G,S+).
In other words, such aset will provably contain all the rules of ReI(G,S+).We let TFl~ules(Ek) denote the set of "terminal freerules" {Aio - ' *  zlAiaz2....znAi,,Z.+l \[ n < k & Vj <n A~ E Ek} We note that the cardinality of such a setis a function only of k. We then "assign ~members ofSubYields~(S +) to TFRules(Eh), wherever it is possi-ble (or the arities agree).
We let CRules(k, S +) denotethe set of "candidate rules ~ so obtained.Def init ion 4.3 C Rules( k, S +) ={R(wa/za ..... w , / z , )  I a E TFRnles(Ek) & w ESubYieldsk(S +) ~ arity(w) = arity(R) = n}It is easy to see that the number of rules in such a setis also polynomially bounded.Claim 4.2 card(ORulea(k ,  S+ )) = O(l 2k+3)Step 2.Recall that we have assumed that they each have a non-terminal set contained in some fixed set of k nontermi-nMs, Ek.
So if we generate all subsets of CRules(k, S +)with at most k rules, then these will include all the k-simple grammars in G.Def init ion 4.4ccra,.~(k, st) = ~'~(CR~les(k, St ) ) .
'sStep 3.Now we finally make use of the negative portion of thesample, S~', to ensure that we do not include any in-consistent grammars in our candidates.15~k(X)  in general denotes the set of all subsets of X withcardinal i ty  at most k.Definit ion 4.5 FGrams(k, Sz) = {H \[ H ECGra,ns(k, S +) ~, r.(a) n S~ = e~}This filtering can be computed in time polynomial inthe length of St., because for testing consistency of eachgrammar in CGrams(k, + S z ), all that is involved is themembership question for strings in S~" with that gram-mar .Step 4.What we have at this stage is a set of 'subcovers' of SL +,each with a size (or 'weight') associated with it, and wewish to find a subset of these 'subcovers' that cover theentire S +, but has a provably small 'total weight'.
Weabstract his as the following problem.~/E IGHTED-SET-COVER(WSC)INSTANCE: (X, Y, w) where X is a finite set and Y isa subset of ~(X) and w is a function from Y to N +.Intuitively, Y is a set of subcovers of the set X, eachassociated with its 'weight'.NOTATION: For every subset Z of Y, we let couer(g) =t3{z \[ z E Z}, and totahoeight(Z) = E,~z w(z).QUESTION: What subset of Y is a set-cover of X witha minimal total weight, i.e.
find g C_ Y with the follow-ing properties:(i) toner(Z) = X.
(ii) VZ' C_ Y if cover(Z') = X then totalweight(Z') >_totahoeig ht( Z ).We now prove the existence of an approximationalgorithm for this problem with the desired performanceguarantee.Lemma 4.1 There is an algorithm B and a polyno-mial p such that given an arbitrary instance (X, Y, w)of WEIGHTED.SET.COVER with I X I = n, alwaysoutputs Z such that;1.
ZC_Y2.
Z is a cover for X ,  i.e.
UZ = X8.
If Z' is a minimal weight set cover for (X, Y, w),then E~z  to(y) <_ p(Ey~z, w(y)) ?
log n.4.
B runs in time polynomial in the size of the in-stance.Proof:  To exhibit an algorithm with this property, wemake use of the greedy algorithm g for the standard229set-cover problem due to Johnson (\[8\]), with a perfor-mance guarantee.
SET-COVER can be thought of as aspecial case of WEIGHTED-SET-COVER with weightfunction being the constant funtion 1.Theorem 4.2 (David S. JohnRon)There is a greedy algorithm C for SET.COVER suchthat given an arbitrary instance (X, Y) with an optimalsolution Z', outputs a solution Z, such that card(Z) =O(log \[ X \[ xcard(Z')) and runs in time polynomial inthe instance size.Now we present he algorithm for WSC.
The ideaof the algorithm is simple.
It applies C on X and suc-cessive subclasses of Y with bounded weights, upto themaximum weight here is, but using only powers of 2 asthe bounds.
It then outputs one with a minimal totalweight araong those.A lgor i thm B: ((X, Y, w))mazweight := maz{to(y) \[Y E Y)m :-- \[log mazweight\]/* this loop gets an approximate solution using Cfor subsets of Y each defined by putting an upperboundon the weights */For i - -1  tomdo:Y\[i\] := {lr/\[ Y E Y & to(Y) < 2'}s\[,\] := c((x, Y\[,\]))End/*  For *//* this loop replaces all 'bad' (i.e.
does not cover X)solutions with Y - the solution with the maximumtotal weight */For i=  l tomdo:s\[,\] := s\[,\] if cover(s\[i\]) ---- X:= Y otherwiseEnd/*  For */~intotaltoelght := ~i.
{totaltoeight(s\[j\]) I J ?
\[m\]}Return s\[min { i I totaltoeig h t( s\['l) --- mintotaitoeig ht } \]End /* Algorithm B */T ime Analys isClearly, Algorithm B runs in time polynomial inthe instance size, since Algorithm C runs in time poly-nomial in the instance size and there are only m ----~logmazweight\] cMls to it, which certainly does notexceed the instance size.Per formance GuaranteeLet (X, Y, to) be a given instance with card(X) =n.
Then let Z* be an optimal solution of that in-stance, i.e., it is a minimal total weight set cover.
Lettotalweight(Z*) = w' .
Now let m" ---- \[log maz{w(z)  Iz E Z?}
\ ] .
Then m* ~_ rain(n, \[logrnazweight\]).
Sowhen C is called with an instance (X, Y\[m'\]) in them'-th iteration of the first 'For'-loop in the algorithm,every member of Z" is in Y\[m*\].
Hence, the optimalsolution of this instance quals Z ' .
Thus, by the per-formance guarantee of C, s\[m*\] will be a cover of Xwith cardinality at most card(Z ?)
?
log n. Thus, wehave card(s\[m*\]) ~_ card(Z*) ?logn.
Now, for everymember t of sire*l, w(t) ~ 2 '~" _< 2 pOs~'I _~ 2w*.Therefore, totalweight(s\[m*\]) = card(Z') x logn xO(2w*) = O(w*) ?
logn x O(2w'), since w" certainlyis at least as large as card(Z').
Hence, we havetotaltoeight(s\[m*\]) = O(w *= x log n).
Now it is clearthat the output of B will be a cover, and its total weightwill not exceed the total weight of s\[m'\].
We concludetherefore that B((X,  Y, to)) wil l  be a set-cover for X,with total weight bounded above by O(to .= x log n),where to* is the total weight of a minimal weight coverand n f lX  \[.rlNow, to apply algorithm B to our learning problem,we let Y = {S+t.
nL(H) \[ H E FGrams(k, SL)) and de-fine the weight function w : Y --* N + by Vy E Y w(y)  =rain{size(H) \[H E FGrams(k, St)  & St = L(H)N S + }and call B on (S+,Y,w).
We then output the gram-mar 'corresponding' to B((S +, Y, w)).
In other words,we let ~r = {mingrammar(y) \[ y E IJ((S+L,Y,w))}where mingrammar(g) is a minimal-size grammar Hin FGrams(k, SL) such that L(H)N S + = y. Thefinal output 8ra~nmar H will be the =disjoint union"of all the grammars in /~, i.e.
H ---- Ip(H).
H isclearly consistent with SL, and since the minimal to-tal weight solution of this instance of WSC is no largerthan Rel(~, S+~), by the performance guarantee on thealgorithm B, size(H) ~_ p(size( Rel( G, S + ))) x O(log m)for some polynomial p, where m is the sample size.size(O) ~_ size(Rei(G, S+)) is also bounded by a poly-nomial in the size of a minimal grammar consistent withSL.
We therefore have shown the existence of an Occamalgorithm with range size polymomlal in the size of aminimal consistent grammar and less than linear in thesample size.
Hence, Theorem 4.1 has been proved.Q.E.D.5 Extens ion  to  M i ld ly  Context  Sen-s i t ive LanguagesThe learnability of k-local subclasses of CFG may ap-pear to be quite restricted.
It turns out, however, thatthe \]earnability of k-local subclasses extends to a richclass of mildly context sensitive grsmmars which we230call "Ranked Node Rewriting Grammaxs" (RNRG's).RNRG's  are based on the underlying ideas of Tree Ad-joining Grammars (TAG's) :e, and are also a specicalcase of context free tree grammars \[13\] in which unre-stricted use of variables for moving, copying and delet-ing, is not permitted.
In other words each rewritingin this system replaces a "ranked" nontermlnal node ofsay rank \] with an "incomplete" tree containing exactly\] edges that have no descendants.
If we define a hier-archy of languages generated by subclasses of RNRG'shaving nodes and rules with bounded rank \] (RNRLj),then RNRL0 = CFL, and RNRL1 = TAL.
17 It turnsout that each k-local subclass of each RNRLj is poly-nomially learnable.
Further, the constraint of k-localityon RNRG's is an interesting one because not only eachk-local subclass is an exponential class containing in-finitely many infinite languages, but also k-local sub-classes of the RNRG hierarchy become progressivelymore complex as we go higher in the hierarchy.
In pax-t iculax, for each j, RNRG~ can "count up to" 2(j + 1)and for each k _> 2, k-local-RNRGj can also count upto 20' + 1)?
sWe will omit a detailed definition of RNRG's  (see\[2\]), and informally illustrate them by some examples?
sExample  5.1 L1 = {a"b" \[ n E N} E CFL is gen-erated by the following RNRGo grammar, where a isshown in Figure 3.
G: = ({5'}, {s,a,b},| ,  (S}, {S -*~, s - ~(~)})ExampleS .2  L2 = {a"b"c"d" \[ n E N} ETAL  is generated by the following RNRG1 gram-mar, where \[$ is shown in Figure 3.
G2 =({s},  {~, a, b, ~, d}, ~, {(S(~))}, {S - -  ~, S - -  ,(~)})Example  5.3 Ls = {a"b"c"d"e"y" I n E N} f~TAL is generated by the \]allowing RNRG2 gram-mar, where 7 is shown in Figure 3.
G3 =({S} ,{s ,a ,b ,c ,d ,e , f} ,~,{(S(A ,A) )} ,{S  .-* 7, S "-"s(~, ~)}).
An example of a tree in the tree language ofG3 having as its yield 'aabbccddee f f '  is also shown inFigure 3.16Tree adjo in ing grmnmars  were introduced as a formal ismfor l inguistic descript ion by Joehi et al \[10\], \[9\].
Various formaland computat iona l  propert ies of TAG' ?
were studied in \[16\].
Itsl inguistic relevance was demonstrated in \[11\].IZThi ?
hierarchy is different from the hierarchy of "mete,TAL 's"  invented and studied extensively by Weir in \[18\].18A class of _g~rammars G is said to be able to "count up to"j, just  in case -{a~a~...a~ J n 6.
N}  E ~L(G) \[ G E Q} but{a~a~...a~'+1 1 n et?}
?
{L(a) I G e ?
}.19Simpler trees are represented as term structures,  whereasmore involved trees are shown in the figure.
Also note tha~ weuse uppercase letters for nonterminals and lowercase for termi-nals.
Note the use of the special symbol | to indicate an edgewith no descendent.~: 7: derived:?
S b s $ f|b # ?
d # e?
S dIb # ?$Aa s fa s fs $b s c d s eb ~.
c d ~.
eFigure 3: ~, ~, 7 and deriving 'aabbceddeeff' by G3We state the learnabillty result of RNRL j ' s  belowas a theorem, and again refer the reader to \[2\] for details.Note that this theorem sumsumes Theorem 4.1 as thecase j = 0.Theorem 5.1 Vj, k E N k-local-RNRLj is poignomi.ally learnable?
?6 Some Negat ive  Resu l tsThe reader's reaction to the result described above maybe an illusion that the learnability of k-local grammarsfollows from "bounding by k".
On the contrary, wepresent a case where ~bounding by k" not only doesnot help feasible learning, but in some sense makes itharder to learn.
Let us consider Tree Adjoining Gram-mars without local constraints, TAG(wolc) for the sakeof comparison.
2xThen an anlogous argument to the onefor the learn?bUlly of k-local-CFL shows that k-local-TAL(wolc) is polynomlally learnable for any k.Theorem 6.1 Vk E N + k-loeal-TAL(wolc) is polyno.mially learnable.Now let us define subclasses of TAG(wolc) witha bounded number of initial trees; k-inltial-tree-TAG(wolc) is the class of TAG(wolc) with at most kinitial trees.
Then surprisingly, for the case of singleletter alphabet, we already have the following strikingresult.
(For fun detail, see \[1\].
)Theorem 6.2 (i) TAL(wolc) on l-letter alphabet ispolynomially learnable.2?We use the size of a minimal k-local RNRGj  as the size ofa k-local RNRL j ,  i.e., Vj E N VL E k- local -RNRLj  s i ze (L )  =mln{s lz ?
(G)  \[ G E k-local-RNRG~ & L(G)  = L} .21Tree Adjo in ing Grammar  formal ism was never defined with-out  local constrains.231(ii) Vk >_ 3 k.initial.tree-TAL(wolc) on 1.letter al-phabet is not polynomially learnable by k.initial.tres.YA G (wolc ).As a corollary to the second part of the above theorem,we have that k-initial-tree-TAL(wolc) on an arbitraryalphabet is not polynomiaJ\]y learnable (by k-initial-tree-TAG(wolc)).
This is because we would be able to usea learning algorithm for an arbitrary alphabet o con-struct one for the single letter alphabet case.Coro l la ry  6.1 k.initial.tree-TAL(wolc) is not polyno-mially learnable by k-initial.tree- TA G(wolc).The learnability of k-local-TAL(wolc) and the non-learnability of k-initial-tree-TAL(wolc) is an interestingcontrast.
Intuitively, in the former case, the "k-bound"is placed so that the grammar is forced to be an ar-bitrarily ~wide ~ union of boundedly small grammars,whereas, in the latter, the grammar is forced to be aboundedly "narrow" union of arbitrarily large g:am-mars.
It is suggestive of the possibility that in facthuman infants when acquiring her native tongue maystart developing small special purpose grammars for dif-ferent uses and contexts and slowly start to generalizeand compress the large set of similar grammars into asmaller set.7 ConclusionsWe have investigated the use of complexity theory tothe evaluation of grammatical systems as linguistic for-malisms from the point of view of feasible learnabil-ity.
In particular, we have demonstrated that a single,natural and non-trivial constraint of "locality ~on thegrammars allows a rich class of mildly context sensi-tive languages to be feasibly learnable, in a well-definedcomplexity theoretic sense.
Our work differs from re-cent works on efficient learning of formal languages,for example by Angluin (\[4\]), in that it uses only ex-amples and no other powerful oracles.
We hope tohave demonstrated that learning formal grammars neednot be doomed to be necessaxily computationally in-tractable, and the investigation of alternative formula-tions of this problem is a worthwhile ndeavour.References\[1\] Naoki Abe.
Polynomial learnability of semillnearsets.
1988.
UnpubLished manuscript.\[2\] Naoki Abe.
Polynomially leaxnable subclasses ofmildy context sensitive languages.
In Proceedingsof COLING, August 1988.\[3\] Dana Angluin.
Inference of reversible languages.Journal of A.C.M., 29:741-785, 1982.\[4\] Dana Angluin.
Leafing k-bounded contezt.freegrammars.
Technical Report YALEU/DCS/TR-557, Yale University, August 1987.\[5\] Dana Angluin.
Learning RegularSets from Queries and Counter.ezamples.
Techni-cal Report YALEU/DCS/TR-464, Yale University,March 1986.\[6\] A. Blumer, A. Ehrenfeucht, D. Haussler, and M.Waxmuth.
Classifying Learnable Geometric Con-cepts with the Vapnik.Chervonenkis DimensiorLTechnical Report UCSC CRL-86-5, University ofCalifornia at Santa Cruz, March 1986.\[7\] E. Mark Gold.
Language identification i the limit.Information and Control, 10:447-474, 1967.\[8\] David S. Johnson.
Approximation a~gorithms forcombinatorial problems.
Journal of Computer andSystem Sciences, 9:256-278,1974.\[9\] A. K. Joshi.
How much context-sensitivity is neces-sary for characterizing structural description - treeadjoining grammars.
In D. Dowty, L. Karttunen,and A. Zwicky, editors, Natural Language pro.c~sing- Theoretical, Computational, and Psycho-logical Perspoctive~, Cambrldege University Press,1983.\[10\] Aravind K. Joshi, Leon Levy, and Masako Taks-hashl.
Tree adjunct grammars.
Journal of Com-puter and System Sciences, 10:136-163, 1975.\[11\] A. Kroch and A. K. Joshi.
Linguistic relevanceof tree adjoining grammars.
1989.
To appear inLinguistics and Philosophy.\[12\] Daniel N. Osherson, Michael Stob, and Scott We-instein.
Systems That Learn.
The MYI" Press, 1986.\[13\] William C. Rounds..Context-free grammars ontrees.
In ACM Symposium on Theory of Comput-ing, pa4ges 143--148, 1969.\[14\] Leslie G. Variant.
Learning disjunctions of conjunc-tions.
In The 9th IJCAI, 1985.\[15\] Leslie G. Variant.
A theory of the learnable.
Com-munications of A.C.M., 27:1134-1142, 1984.\[16\] K. Vijay-Shanker and A. K. Joshi.
Some compu-tational properties of tree adjoining grammars.
In23rd Meeting of A.C.L., 1985.\[17\] K. Vijay-Shanker, D. J. Weir, and A. K. Joshi.Characterizing structural descriptions produced byvarious grammatical formalisms.
In ~5th Meetingof A.C.L., 1987.\[18\] David J. Weir.
From Contezt-Free Grammars toTree Adjoining Grammars and Beyond - A disser-tation proposal.
Technical Report MS-CIS-87-42,University of Pennsylvania, 1987.232
