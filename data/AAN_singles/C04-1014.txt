Modeling of Long Distance Context DependencyZHOU GuoDongInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore 119613Email: zhougd@i2r.a-star.edu.sgAbstractNgram models are simple in languagemodeling and have been successfully used inspeech recognition and other tasks.
However,they can only capture the short distancecontext dependency within an n-wordswindow where currently the largest practical nfor a natural language is three while much ofthe context dependency in a natural languageoccurs beyond a three words window.
In orderto incorporate this kind of long distancecontext dependency in the ngram model of ourMandarin speech recognition system, thispaper proposes a novel MI-Ngram modelingapproach.
This new MI-Ngram model consistsof two components: a normal ngram modeland a novel MI model.
The ngram modelcaptures the short distance context dependencywithin an n-words window while the MImodel captures the context dependencybetween the word pairs over a long distanceby using the concept of mutual information.That is, the MI-Ngram model incorporates theword occurrences beyond the scope of thenormal ngram model.
It is found that MI-Ngram modeling has much better performancethan the normal word ngram modeling.Experimentation shows that about 20% oferrors can be corrected by using a MI-Trigrammodel compared with the pure word trigrammodel.1 IntroductionLanguage modeling is the attempt tocharacterize, capture and exploit theregularities and constraints in a naturallanguage and has been successfully applied tomany domains.
Among all the languagemodeling approaches, ngram models havebeen most widely used in speech recognition(Jelinek 1990; Gale and Church 1990; Brownet al 1992; Yang et al 1996) and otherapplications.
While ngram models are simplein language modeling and have beensuccessfully used in speech recognition andother tasks, they have obvious deficiencies.For instance, ngram models can only capturethe short-distance dependency within an n-words window where currently the largestpractical N for a natural language is three.In the meantime, it is found that therealways exist many preferred relationshipsbetween words.
Two highly associated wordpairs are ?not only/but also?
and?doctor/nurse?.
Psychological experiments inMeyer D. et al (1975) indicated that thehuman?s reaction to a highly associated wordpair was stronger and faster than that to apoorly associated word pair.
Such preferenceinformation is very useful for natural languageprocessing (Church K.W.
et al 1990; HiddleD.
et al 1993; Rosenfeld R. 1994 and ZhouG.D.
et al1998).
Obviously, the preferencerelationships between words can expand froma short to long distance.
While we can useconventional ngram models to capture theshort distance dependency, the long distancedependency should also be exploited properly.The purpose of this paper is to propose anew modeling approach to capture the contextdependency over both a short distance and along distance and apply it in Mandarin speechrecognition.This paper is organized as follows.
InSection 2, we present the normal ngrammodeling while a new modeling approach,named MI-ngram modeling, is proposed inSection 3.
In Section 4, we will describe itsuse in our Mandarin speech recognitionsystem.
Finally we give a summary of thispaper.2 Ngram ModelingLet , where ?s are thewords that make up the hypothesis, themm wwwwS ...211 == iwprobability of the word string, , can becomputed by using the chain rule:)(SP| 11?ii ww| 11?ii w1?n)| 1?iwniw ?1)...|( 11 ?nn wwwP    for all Vwww ni ?,...,,1 .Given mwwwS ...21= , an ngram modelestimates the log probability of the wordstring, log , by re-writing Equation (2.2): )(SP?=?=miii wwPwPSP2111 )|()()(                      (2.1)By taking a log function to both sides ofEquation (2.1), we have the log probability ofthe word string, log : )(SP?
?=?+=12111 )|(log)(log)(logniiingram wwPwPSP?=?+?+mniinii wwP )|(log11                      (2.6) )(log)(log)(log21=?+= miPwPSP   (2.2)where  is the string length,  is the i -thword in the string .m iwSSo, the classical task of statistical language modeling becomes how to effectively andefficiently predict the next word, given theprevious words, that is to say, to estimateexpressions of the form  .
Forconvenience,  is often written as, where h , is called history.
)(wP)|( 11?ii wwP11?= iw)|( hwP iFrom the ngram model as in Equation(2.3),  we have:)()()()(11111111?+??+????
iniiiniiiiwPwwPwPwwP)()()()()()(11111111iiniiiniiiiiwPwPwwPwPwPwwP?+??+???
?Traditionally, simple statistical models,known as ngram models, have been widelyused in speech recognition.
Within an ngrammodel, the probability of a word occurringnext is estimated based on the  previouswords.
That is to say,)()()(log)()()(log 11111111iiniiiniiiiiwPwPwwPwPwPwwP?+??+????
(2.7)Obviously, the normal ngram model hasthe assumption:)|()|( 1 111?+??
?
i niiii wwPwwP                        (2.3) )1,,()1,,( 1 111 =?= ?
+??
dwwMIdwwMI ii niii  (2.8)For example, in bigram model (n=2) theprobability of a word is assumed to dependonly on the previous word:where)()()(log)1,,( 111111iiiiiiwPwPwwPdwwMI ???
==),( 11 ii ww ?is the mutual information of the word stringpair , and)()()(log)1,, 11111iiniiinii wPwPwwPdw ?+?
?+?+ ==)iw d( 1i niwMI?
?,( 1 1iniw?+?isthe mutual information of the word string pair.
is the distance of the two wordstrings in  the word string pair and is equalto 1 when the two word strings are adjacent.
)|()|( 111 ??
?
iiii wwPwwP                      (2.4)And the probability  can beestimated by using maximum likelihoodestimation (MLE) principle:( iwP)()()|(111???
=iiiii wCwwCwwP                      (2.5)Where  represents the number of times thesequence occurs in the training text.
Inpractice, due to the data sparseness problem,some smoothing technique (e.g.
Good Turingin [Chen and Goodman 1999]) is applied to getmore accurate estimation.
)(?CFor a pair (  over a distance  whereand), BA dA B  are word strings, the mutualinformation  reflects the degree ofpreference relationship between the twostrings over a distance .
Several propertiesof the mutual information are apparent:)d,,( BAMIdObviously, an ngram model assumes thatthe probability of the next word  isindependent of the word string  in thehistory.
The difference between bigram,trigram and other ngram models is the value ofn.
The parameters of an ngram model are thusthe probabilities:iw ?
For the same distance , d),,(),,( dABMIdBAMI ?
.?
For different distances  and , 1d 2d),,(),,( 21 dBAMIdBAMI ?
.?
If  and A B  are independent over adistance d ,  then  .
0),,( =dBAMI)1,,()1,,( === dwXMIdwHMI ii,,1 idww i =(MI )+           (3.4)),,( dBAMI  reflects the change of  theinformation content when two word stringsandAB   are correlated.
That is to say, thehigher the value of ,  the strongeraffinity  and),,( dBAMIA B  have.
Therefore, we can usethe mutual information to measure thepreference relationship degree of a word stringpair.where  ,  and i .
That isto say, the mutual information of the nextword with the history is assumed equal to thesummation of that of the next word with thefirst word in the history and that of the nextword with the rest word string in the history.Then we can re-write Equation (3.3) by usingEquation (3.4),11?= iwH 12?= iwX N>Using an alternative view of equivalence,an ngram model is one that partitions the datainto equivalence classes based on the last n-1words in the history.
Viewed in this way, abigram induces a partition based on the lastword in the history.
A trigram model furtherrefines this partition by considering the next-to-last word and so on.
)|(log HwP i)1,,()(log ii wHMIwP +=),,()1,,()(log 1 iwwMIwXMIwP iii ++=),,()()()(log)(log 1 iwwMIXPwPXwPwP iiii ++=),,()()(log 1 iwwMIXPXwPii +=  As the word trigram model is most widelyused in current research, we will mainlyconsider the word trigram-based model.
By re-writing Equation (2.2), the word trigram modelestimates the log probability of the stringas: )(log SP),,()|(log 1 iwwMIXwP ii +=                    (3.5)That is, we have),,()|(log)|(log 11211 iwwMIwwPwwP iiiii += ??
)|log()(log)(log 121 wwwPSPTrigram +=?=?
?miiii wwP312 )|(log+             (2.9)(3.6)By applying Equation (3.6) repeatedly, wehave a modified estimation of the conditionalprobability: 3 MI-Ngram ModelingGiven  and,  we have12111 ...
??
== ii wwwwH132 ... ?iww12?
== i wwX ),,()|(log)|(log11211iwwMIwwPwwPiiiii+= ??
)|(log 13?= ii wwP),,()1,,( 12 iwwMIiwwMI ii +?+  XwH 1=                        (3.1)and L L L)|()|( 1XwwPHwP ii = .
(3.2))|(log 1 1?+?= i nii wwP ?
?==+?+nikkik kiwwMI1)1,,(  By taking a log function to both sides ofEquation (3.2), we have(3.7))|(log HwP i  Obviously, the first item in equation (3.7)contributes to the log probability of the normalword ngram within an N-words window whilethe second item is the mutual informationwhich contributes to the long distance contextdependency of the next word  with theprevious wordsoutside the n-words window of the normalword ngram model.wii ?
),1( NiNjwj >??
)()(logHPHwP i=)()()(log)(logiii wPHPHwPwP +=)1,,()(log HwMIwP ii +=                           (3.3)Now we assumeBy using Equation (3.7) iteratively,Equation (2.2) can be re-written as: ?
?=?==+?+miikkik kiwwMI431)1,,(                     (3.10))|(log)(log)(log1121?=?+= iimiwwPwPSP?==?+=niiii wwwP2111 )|log()(log)|(log 11nn wwP ++ (log2+=?+ imniwP?==?+=niiii wwwP2111 )|log()(log)|(log 21nn wwP ++)| 11?iwCompared with the normal word ngrammodel, the novel MI-Ngram model alsoincorporates the long distance contextdependency by computing the mutualinformation of the distance dependent wordpairs.
That is, the MI-Ngram modelincorporates the word occurrences beyond thescope of the normal ngram model.Since the number of possible distance-dependent word pairs may be very huge, it isimpossible for the MI-Ngram model toincorporate all the possible distance-dependentword pairs.
Therefore, for the MI-Ngrammodel to be practically useful, how to select areasonable number of word pairs becomesmost important.
Here two approaches are used(Zhou G.D., et al1998):)1,,( 11 ++ + nwwMI n )|(log 112?+=?+ iimniwwPL L L?==?+=niiii wwwP2111 )|log()(log?+=?
?+mniiii wwP112 )|(logOne approach is to restrict the window sizeof possible word pairs by computing andcomparing the conditional perplexities(Shannon C.E.
1951) of the long distance wordbigram models for different distances.Conditional perplexity is a measure of theaverage number of possible choices there arefor a conditional distribution.
The conditionalperplexity of a conditional distribution withthe conditional entropy  is defined tobe 2 .
Given two random variablesH Y X( | )H Y X( | ) X andY , a conditional probability mass function, and a marginal probability massfunction , the conditional entropy ofP y xY X| ( | )P yY ( ) Ygiven X , , is defined as: H Y( | X )(3.8) ?
?+=?==+?+mninikkik kiwwMI1 1)1,,(From Equation (3.8), we can see that thefirst three items are the values computed bythe normal word trigram model as shown inEquation (2.9)  and the forth itemcontributes tosummation of the mutual information of thenext word with the words in the history .Therefore, we call Equation (3.8) as a MI-Ngram model and rewrite it as:?
?+=?==+?mninikkik kiwwMI1 1)1,,(niw ?1 ???
?
?=Xx YyXYYX xyPyxPXYH )|(log),()|( |2,(3.11))(log)(logSPSPNgramNgramMI=?For a large enough corpus, the conditionalperplexity is usually an indication of theamount of information conveyed by the model:the lower the conditional perplexity, the moreinformation it conveys and thus a better model.This is because the model captures as much asit can of that information, and whateveruncertainty remains shows up in theconditional perplexity.
Here, the corpus is theXinHua corpus, which has about 57M(million)characters or 29M words.
For all theexperiments, 80% of the corpus is used for?
?+=?==+?+mninikkik kiwwMI1 1)1,,(                     (3.9)As a special case of N=3, the MI-Trigrammodel estimate the log probability of the stringas follows:)(log)(logSPSPTrigramTrigramMI=?training while the remaining 20% is used fortesting.Table 1 shows that the conditionalperplexity is lowest for d = 1 and increasessignificantly as we move through d = 2, 3, 4, 5and 6.
For d = 7, 8, 9, the conditionalperplexity increases slightly while furtherincreasing d almost does not increase theconditional perplexity.
This suggests thatsignificant information exists only in the last 6words of the history.
In this paper, we restrictthe maximum window size to 10.Table 1: Conditional perplexities of thelong-distance word bigram models fordifferent distancesDistancePerplexity DistancePerplexity1 230 7 14792 575 8 15313 966 9 15804 1157 10 15995 1307 11 16116 1410 20 1647Another approach is to adapt averagemutual information to select a reasonablenumber of distance-dependent word pairs:)()()(log),();(BPAPABPBAPBAAMI =)()()(log),(BPAPBAPBAP++)()()(log),(BPAPBAPBAP+)()()(log),(BPAPBAPBAP          (3.12)Obviously, Equation (3.12) takes the jointprobability into consideration.
That is, thosefrequently occurring word pairs are moreimportant and have much more potential to beincorporated into the MI-Ngram model thanless frequently occurring word pairs.4 ExperimentationWe have evaluated the new MI-Ngram modelin  an experimental speaker-dependentcontinuous Mandarin speech recognitionsystem (Zhou G.D. et al1999).
For basesyllable recognition, 14 cepstral and 14 delta-cepstral coefficients, energy(normalized) anddelta-energy are used as feature parameters toform a feature vector with dimension 30, whilefor tone recognition, the pitch period and theenergy together with their first order andsecond order delta coefficients are used toform a feature vector with dimension 6.
All theacoustic units are modeled by semi-continuousHMMs (Rabiner 1993).
For base syllablerecognition, 138 HMMs are used to model 100context-dependent INITIALs and 38 context-independent FINALs while 5 HMMs are usedto model five different tones in MandarinChinese.
5,000 short sentences are used fortraining and another 600 sentences (6102Chinese characters) are used for testing.
Allthe training and testing data are recorded byone same speaker in an office-like laboratoryenvironment with a sampling frequency of16KHZ.As a reference, the base syllable recognitionrate and the tone recognition rate are shown inTable 2 and Table 3, respectively.
As the wordtrigram model is most widely used in currentresearch, all the experiments have been doneusing a MI-Trigram model which is trained onthe XINHUA news corpus of 29 millionwords(automatically segmented) while thelexicon contains about 28000 words.
As aresult, the perplexities and Chinese characterrecognition rates of different MI-Trigrammodels with the same window size of 10 anddifferent numbers of distance-dependent wordpairs are shown in Table 4.Table 2: The top-n recognition rates of  base syllablesTop-N Base Syllables  1  5 10 15 20Recognition Rate of Base Syllables 88.2 97.6 99.2 99.5 99.8Table 3: The recognition rates of the tonestone 1 tone 2 tone 3 tone 4 tone 5tone 1    90.4     0.8       0.6       0.8       7.4tone 2     8.3   81.1       5.4       0.2       4.9tone 3     5.0   20.9     43.0     29.1      2.0tone 4     4.3     0.2       1.8     93.5      0.2tone 5     24.1     8.6       0.9       8.2    58.2Table 4: The effect of different numbers of word pairs in the MI-Trigram models with the samewindow size 10 on the Chinese character recognition ratesNumber of  word pairs  Perplexity Recognition Rate0 204 90.5100,000 196 91.2200,000 189 91.7400,000 183 92.1600,000 179 92.3800,000 175 92.41,000,000 172 92.51,500,000 171 92.52,000,000 170 92.62,500,000 170 92.53,000,000 168 92.63,500,000 169 92.64,000,000 168 92.7Table 4 shows that the perplexity and therecognition rate rise quickly as the number ofthe long distance-dependent word pairs in theMI-Trigram model increase from 0 to 800,000,and then rise slowly.
This suggests that thebest 800,000 word pairs carry most of the longdistance context dependency and should beincluded in the MI-Ngram model.
It alsoshows that the recognition rate of the MI-Trigram model with 800,000 word pairs is1.9% higher than the pure word trigram model(the MI-Trigram model with 0 long distance-dependent word pairs).
That is to say, about20% of errors can be corrected byincorporating only 800,000 word pairs to theMI-Trigram model compared with the pureword trigram model.It is clear that MI-Ngram modeling hasmuch better performance than normal wordngram modeling.
One advantage of MI-Ngrammodeling is that its number of parameters isjust a little more than that of word ngrammodeling.
Another advantage of MI-Ngrammodeling is that the number of the word pairscan be reasonable in size without losing toomuch of its modeling power.
Compared tongram modeling, MI-Ngram modeling alsocaptures the long distance dependency of wordpairs using the concept of mutual information.4.
CONCLUSIONThis paper proposes a novel MI-Ngrammodeling approach to capture the contextdependency over both a short distance and along distance.
This is done by incorporatinglong distance-dependent word pairs intonormal ngram modeling by using the conceptof mutual information.
It is found that MI-Ngram modeling has much better performancethan word ngram modeling.REFERENCEBrown P.F.
et al (1992).
Class-based Ngrammodels of natural language.
ComputationalLinguistics  18(4), 467-479.Chen S.F.
and Goodman J.
(1999).
Anempirical study of smoothing technique forlanguage modeling.
Computer, Speech andLanguage.
13(5).
pp.359-394.Church K.W.
et al (1991).
Enhanced goodTuring and Cat-Cal: two new methods forestimating probabilities of Englishbigrams.
Computer, Speech and Language5(1), 19-54.Gale W.A.
& Church K.W.
(1990).
Poorestimates of context are worse than none.Proceedings of DARPA Speech andNatural Language Workshop, HiddenValley, Pennsylvania, pp.
293-295.Hindle D. et al (1993).
Structural ambiguityand lexical relations.
ComputationalLinguistics  19(1),  103-120.Jelinek F. (1990).
Self-organized languagemodeling for speech recognition.
InReadings in Speech Recognition.
Edited byWaibel A. and Lee K.F.
Morgan Kaufman.San Mateo.
CA.
pp.450-506.Meyer D.  et al (1975).
Loci of contextualeffects on visual word recognition.
InAttention and Performance V, edited byP.Rabbitt and S.Dornie.
pp.
98-116.Acdemic Press.Rabiner L.R.
et al (1993).
Foundamentals toSpeech Recognition.
Prentice Hall.Rosenfeld R. (1994).
Adaptive statisticallanguage modeling: A Maximum EntropyApproach.
Ph.D. Thesis, Carneige MellonUniversity.Shannon C.E.
(1951).
Prediction and entropyof printed English.
Bell Systems TechnicalJournal   30, 50-64.Yang Y.J.
et al (1996).
Adaptive linguisticdecoding system for Mandarin speechrecognition applications.
ComputerProcessing of Chinese & OrientalLanguages  10(2), 211-224.Zhou G.D. and Lua K.T.
(1998).
Wordassociation and MI-Trigger-basedlanguage modeling, COLING-ACL?98.Montreal Canada, 8-14 August.Zhou G.D. and Lua KimTeng (1999).Interpolation of N-gram and MI-basedTrigger Pair Language Modeling inMandarin Speech Recognition, Computer,Speech and Language, Vol.
13, No.
2,pp.123-135.
