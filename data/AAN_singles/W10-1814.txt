Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 113?117,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCross-lingual Validity of PropBank in the Manual Annotation of FrenchLonneke van der Plas Tanja Samardz?ic?Linguistics DepartmentUniversity of GenevaRue de Candolle 5, 1204 GenevaSwitzerland{Lonneke.vanderPlas,Tanja.Samardzic,Paola.Merlo}@unige.chPaola MerloAbstractMethods that re-use existing mono-lingualsemantic annotation resources to annotatea new language rely on the hypothesis thatthe semantic annotation scheme used iscross-lingually valid.
We test this hypoth-esis in an annotation agreement study.
Weshow that the annotation scheme can beapplied cross-lingually.1 IntroductionIt is hardly a controversial statement that elegantlanguage subtleties and powerful linguistic im-agery found in literary writing are lost in trans-lation.
Yet, translation preserves enough meaningacross language pairs to be useful in many appli-cations and for many text genres.The belief that this layer of meaning which ispreserved across languages can be formally rep-resented and automatically calculated underliesmethods that use parallel corpora for the automaticgeneration of semantic annotations through cross-lingual transfer (Pado?, 2007; Basili et al, 2009).A methodology similar in spirit ?
re-use of theexisting resources in a different language ?
hasalso been applied in developing manually anno-tated resources.
Monachesi et al (2007) annotateDutch sentences using the PropBank annotationscheme (Palmer et al, 2005), while Burchardt etal.
(2009) use the FrameNet framework (Fillmoreet al, 2003) to annotate a German corpus.
In-stead of building special lexicons containing thespecific semantic information needed for the an-notation for each language separately, which is acomplex and time-consuming endeavour in itself,these approaches rely on the lexicons already de-veloped for English.In this paper, we hypothesize that the levelof abstraction that is necessary to develop a se-mantic lexicon/ontology for a single languagebased on observable linguistic behaviour ?
thatis a mono-lingual, item-specific annotation ?
iscross-linguistically valid.
We test this hypothe-sis by manually annotating French sentences usingthe PropBank frame files developed for English.It has been claimed that semantic parallelismacross languages is smaller when using thePropBank semantic annotations instead of theFrameNet scheme, because FrameNet is more ab-stract and less verb-specific (Pado?, 2007).
We areworking with the PropBank annotation scheme,contrary to other works that use the FrameNetscheme, such as Pado?
(2007) and Basili et al(2009).
We choose this annotation for two mainreasons.
First, the primary use of our annotation isto serve as a gold standard in the task of syntactic-semantic parsing.
FrameNet does not have a prop-erly sampled hand-annotated corpus of English,by design.
So we cannot use it for this task.
Sec-ond, in Merlo and Van der Plas (2009), the seman-tic annotations schemes of PropBank and VerbNet(Kipper, 2005) are compared, based on annotationof the SemLink project (Loper et al, 2007).
Theauthors conclude that PropBank is the preferredannotation for a joint syntactic-semantic setting.If the PropBank annotation scheme is cross-lingually valid, annotators can reach a consensusand can do so swiftly.
Thus, cross-lingual valid-ity is measured by how well-defined the manualannotation task is (inter-annotator agreement) andby how hard it is to reach an agreement (pre- andpost-consensus inter-annotator agreement).
In ad-dition, we measure the impact of the level of ab-straction of the predicate labels.
Conversely, howoften labels do not transfer and distributions of dis-agreements are indicators of lack of parallelismacross languages that we study both by quantita-tive and qualitative analysis.To preview the results, we find that the Prop-Bank annotation scheme developed for Englishcan be applied for a large portion of French sen-113tences without adjustments, which confirms itscross-lingual validity.
A high level of inter-annotator agreement is reached when the verb-specific PropBank labels are replaced by less fine-grained verb classes after annotating.
Non-parallelcases are mostly due to idioms and collocations.2 Materials and MethodsOur choices of formal representation and of la-belling scheme are driven by the goal of produc-ing useful annotations for syntactic-semantic pars-ing in a setting based on an aligned corpus.
In thefollowing subsections we describe the annotationscheme and procedure, the corpus, and phases ofannotation.2.1 The PropBank Annotation FrameworkWe use the PropBank scheme for the manual anno-tations.
PropBank is a linguistic resource that con-tains information on the semantic structure of sen-tences.
It consists of a one-million-word corpusof naturally occurring sentences annotated withsemantic structures and a lexicon (the PropBankframe files) that lists all the predicates (verbs) thatcan be found in the annotated sentences and thesets of semantic roles they introduce.Predicates are marked with labels that specifythe sense of the verb in the particular sentence.
Ar-guments are marked with the labels A0 to A5.
Thelabels A0 and A1 have approximately the samevalue with all verbs.
They are used to mark in-stances of typical AGENTS (A0) and PATIENTS(A1).
The value of other numbers varies acrossverbs.
Modifiers are annotated in PropBank withthe label AM.
This label can have different exten-sions depending on the semantic type of the con-stituent, for example locatives and adverbials.2.2 Annotation ProcedureAnnotators have access to PropBank frame filesand guidelines adapted for the current task.
Theframe files provide verb-specific descriptions of allpossible semantic roles and illustrate these roleswith examples as shown for the verb paid in (1)and the verb senses of pay in Table 1.
Annotatorsneed to look up each verb in the frame files to beable to label it with the right verb sense and to beable to allocate the arguments consistently.
(1) [A0 The Latin American nation] has[REL?PAY.01 paid] [A1 very little] [A3 on itsdebt] [AM?TMP since early last year].Frame Semantic rolespay.01 A0: payer or buyerA1: money or attentionA2: person being paid, destination of attentionA3: commodity, paid for whatpay.02 A0: payerpay off A1: debtA2: owed to whom, person paidpay.03 A0: payer or buyerpay out A1: money or attentionA2: person being paid, destination of attentionA3: commodity, paid for whatpay.04 A1: thing succeeding or working outpay.05 A1: thing succeeding or working outpay offpay.06 A0: payerpay down A1: debtTable 1: The PropBank lexicon entry for pay.In our cross-lingual setting, annotators usedthe English PropBank frame files to annotate theFrench sentences.
This means that for every pred-icate they find in the French sentence, they needto translate it, and find an English verb sense thatis applicable to the French verb.
If an appropri-ate entry cannot be found in the frame files for agiven predicate, the annotator is instructed to usethe ?dummy?
label for the predicate and fill in theroles according to their own insights.For the annotation of sentences we use an adap-tation of the user-friendly, freely available TreeEditor (TrEd, Pajas and S?te?pa?nek, 2008).
The toolshows the syntactic analysis and the plain sentencein the same window allowing the user to add se-mantic arcs and labels to the nodes in the syntacticdependency tree.The decision to show syntactic information ismerely driven by the fact that we want to guide theannotator in selecting the heads of phrases duringthe annotation process.
The sentences are parsedby a syntactic parser (Titov and Henderson, 2007)that we trained on syntactic dependency annota-tions for French (Candito et al, 2009).
Althoughthe parser is state-of-the-art (87.2% Labelled At-tachment Score), in case of parse errors, we askannotators to ignore the errors of the parser andput the label on the actual head.2.3 CorpusWe selected the French sentences for the man-ual annotation from the parallel Europarl corpus(Koehn, 2005).
Because translation shifts areknown to pose problems for the automatic cross-lingual transfer of semantic roles (Pado?, 2007)and for machine translation (Ozdowska and Way,1142009), and these are more likely to appear in in-direct translations, we decided to select only thoseparallel sentences, for which we can infer from thelabels used in Europarl that they are direct trans-lations from English to French, or vice versa.
Weselected 1040 sentences for annotation (40 in to-tal for the two training phases, 100 for calibration,and 900 for the main annotation phase.
)12.4 Annotation PhasesThe training procedure described in Figure 1is inspired by the methodology indicated inPado?
(2007).
A set of 130 sentences were anno-tated manually by four annotators with very goodproficiency in both French and English for thetraining and the calibration phase.
The remaining900 sentences are annotated by one annotator (outof those four), a trained linguist.
Inter-annotatoragreement was measured at several points in theannotation process marked with an arrow in Fig-ure 1.
The guidelines were adjusted after the train-ing phase.?
Training phase-TrainingA: 10 sentences, all annotators together-TrainingB: 30 sentences, all annotators individually?-Reach consensus on Training B??
Calibration phase-100 sentences by main annotator, one third of those byeach of the other 3 annotators??
Main annotation phase-900 sentences by main annotatorFigure 1: The annotation phases.3 ResultsCross-lingual validity is measured by comparinginter-annotator agreement at several stages in theannotation, by measuring the agreement on lessspecific predicate labelling, and by a quantitativeand qualitative analysis of non-parallel cases.3.1 Inter-annotator Agreement for SeveralAnnotation PhasesTo assess the quality of the manual annotations wemeasured the agreement between annotators as theaverage F-measure of all pairs of annotators aftereach phase of the annotation procedure.2 The first1As usual practice in preprocessing for automatic align-ment, the datasets were tokenised and lowercased and onlysentence pairs corresponding to a 1-to-1 alignment withlengths ranging from 1 to 40 tokens on both French and En-glish sides were considered.2It is a known fact that measuring annotator agreement us-ing the kappa score is problematic in categorisation tasks thatPredicates ArgumentsLab.
F Unl.
F Lab.
F Unl.
FTrainingB 46 85 62 75TrainingB(cons.)
95 97 91 95Calibration 59 93 69 84Table 2: Percent inter-annotator agreement (F-measure) for labelled/unlabelled predicates andfor labelled/unlabelled argumentsrow of Table 2 shows that the task is hard.
Butthe difference between the first row and the sec-ond row shows that there were many differencesbetween annotators that could be resolved.
Afterdiscussions and individual corrections the scoresare between 91% and 95%.
This indicates thatthe task is well-defined.
Row three shows that theagreement in the calibration phase increases a lotcompared to the last training phase (row 1).
Thismight in part be due to the fact that the guidelineswere adjusted by the end of the training phase, butcould also be because the annotators are gettingmore acquainted to the task and the software.As expected, because annotators used the En-glish PropBank frame files to annotate Frenchverbs, the task of labelling predicates proved moredifficult than labelling semantic roles.
It results inthe lowest agreement scores overall.
In the follow-ing subsections we study the sources of disagree-ment in predicate labelling in more detail.3.2 Inter-annotator Agreement in PredicateLabellingsPredicate labels in PropBank apply to particularverb senses, for example walk.01 for the first senseof the verb walk.
Even though the senses arecoarser than, for example, the senses in Word-Net (Fellbaum, 1998), the labels are rather spe-cific.
This specificity possibly poses problemswhen working in a cross-lingual setting.We compare the agreement reached using Prop-Bank verb sense labels with the agreement reachedusing the verb classifications from VerbNet (Kip-per, 2005) and the mapping to PropBank labelsas provided in the type mappings of the SemLinkproject3 (Loper et al, 2007).
If two annotatorsused two different predicate labels to annotate thedo not have a fixed number of items and categories (Burchardtet al, 2006).
The F-measure is a well-known measure usedfor the evaluation of many task such as syntactic-semanticparsing, the task that is the motivation for this paper.
Thechoice of the F-measure makes the comparison to the perfor-mance of the future parser easier.3(http://verbs.colorado.edu/semlink/)115same verb, but those verb senses belong to thesame verb class, we count those as correct4.The average inter-annotator agreement is rela-tively low when we compare the annotations onthe PropBank verb sense level: 59%.
However, atthe level of verb classes, the inter-annotator agree-ment increases to 81%.
This raises the issue ofwhether we should not label the predicates withverb classes instead of verb senses.
By using Prop-Bank labels for the manual annotation and replac-ing these with verb classes in post-processing, thebenefits are two-fold: We are able to reach a highlevel of cross-lingual parallelism on the annota-tions, while keeping the manual annotation task asspecific and less abstract as possible.3.3 Analysis of Non-Parallel CasesFor a single annotator, the main measure of cross-lingual validity is the percentage of dummy pred-icates in the annotation.
In the sentences from thecalibration and the main annotation phase from themain annotator (1000 sentences in total), we find130 predicates (tokens) for which the annotatorused the ?dummy?
label.Manual inspection reveals that the ?dummy?
la-bel is mainly used for French multi-word expres-sions (82%), most of which can be translated bya single English verb (47%), whereas others can-not, because they are translated by a combinationthat includes a form of ?be?
that is not annotatedin PropBank (25%).
The 47% of multi-word ex-pressions that receive the ?dummy?
label show theannotator?s reluctance to put a single verb label ona French multi-word expression.
The annotationguidelines could be adapted to instruct annotatorsnot to hesitate in such cases.Similarly, collocations and idiomatic expres-sions are the main sources of disagreement inpredicate labellings among annotators.
We canconclude that, as shown in studies on other lan-guage pairs (Burchardt et al, 2009), collocationsand idiomatic expressions were identified as verbuses where the verb?s predicate label cannot betransferred directly from one language to another.4 Discussion and Related WorkBurchardt et al (2009) use English FrameNet to4The mappings from PropBank verb sense labels to Verb-Net verb classes are one-to-many and not complete.
Wecounted a pair as matching if there exists a class to whichboth verb senses belong.
We found a verb class for both verbsenses in about 78% of the cases and discarded the rest.annotate a corpus of German sentences manually.They find that the vast majority of frames can beapplied to German directly.
However, around onethird of the verb senses identified in the Germancorpus were not covered by FrameNet.
Also, anumber of German verbs were found to be under-specified.
Finally, some problems related to treat-ing particular verb uses were identified, such as id-ioms, metaphors, and support verb constructions.Monachesi et al (2007) use PropBank labels forsemi-automatic annotation of a corpus of Dutchsentences.
Semantic roles were first annotatedusing a rule-based semantic parser and then cor-rected by one annotator.
Although not all Dutchverbs could be translated to an equivalent verbsense in English, these cases were assessed as rel-atively rare.
What proved to be problematic wasidentifying the correct label for modifiers.Bittar (2009) makes use of cross-lingual lexi-cal transfer in annotating French verbs with eventtypes, by adapting a small-scale English verb lex-icon with specified event structure (TimeML).The inter-annotator agreement in labelling pred-icates reported in Burchardt et al (2009) reaches85%, while our best score (when falling back toverb classes) is 81%.
However, unlike Burchardtet al (2009) we did not introduce any new Frenchlabels.
We find, like Monachesi et al (2007), thatnon-parallel cases are less frequent than what is re-ported in Burchardt et al (2009), which could bedue to the properties of the annotations schemes.5 ConclusionsWe can conclude that the general task of anno-tating French sentences using English PropBankframe files is well-defined.
Nevertheless, it is ahard task that requires linguistic training.
With re-spect to the disagreements on labelling predicates,we can conclude that a large part can be resolvedif we compare the annotations at the level of verbclasses instead of at the very fine-grained level ofverb senses.
Non-parallel cases are mostly due toidioms and collocations.
Their rate is relativelylow and can be further reduced by adapting anno-tation guidelines.AcknowledgmentsThe research leading to these results has received fund-ing from the EU FP7 programme (FP7/2007-2013) undergrant agreement nr 216594 (CLASSIC project: www.classic-project.org).
We would like to thank Goljihan Kashaeva andJames Henderson for valuable comments.116ReferencesR.
Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti,2009.
Computational Linguistics and Intelligent Text Pro-cessing, chapter Cross-Language Frame Semantics Trans-fer in Bilingual Corpora, pages 332?345.
Springer Berlin/ Heidelberg.A.
Bittar.
2009.
Annotation of events and temporal expres-sions in French texts.
In Proceedings of the third Linguis-tic Annotation Workshop (LAW III), pages 48?51, Suntec,Singapore.A.
Burchardt, K. Erk, A. Frank, A. Kowalski, S.
Pado?, andM.
Pinkal.
2006.
The SALSA corpus: a German cor-pus resource for lexical semantics.
In Proceedings of the5th International Conference on Language Resources andEvaluation (LREC 2006), pages 969?974, Genoa, Italy.A.
Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, andM.
Pinkal, 2009.
Multilingual FrameNets in Computa-tional Lexicography: Methods and Applications, chapterFrameNet for the semantic analysis of German: Annota-tion, representation and automation, pages 209?244.
DeGruyter Mouton, Berlin.M.-H. Candito, B.
Crabbe?, P. Denis, and F. Gue?rin.2009.
Analyse syntaxique du franc?ais : des constitu-ants aux de?pendances.
In Proceedings of la Confe?rencesur le Traitement Automatique des Langues Naturelles(TALN?09), Senlis, France.C.
Fellbaum.
1998.
WordNet, an electronic lexical database.MIT Press.C.
J. Fillmore, R. Johnson, and M.R.L.
Petruck.
2003.
Back-ground to FrameNet.
International journal of lexicogra-phy, 16.3:235?250.K.
Kipper.
2005.
VerbNet: A broad-coverage, comprehen-sive verb lexicon.
Ph.D. thesis, University of Pennsylvnia.P.
Koehn.
2005.
Europarl: A parallel corpus for statisticalmachine translation.
In Proceedings of the MT Summit,pages 79?86, Phuket, Thailand.E.
Loper, S-T Yi, and M. Palmer.
2007.
Combining lexicalresources: Mapping between PropBank and VerbNet.
InProceedings of the 7th International Workshop on Com-putational Semantics (IWCS-7), pages 118?129, Tilburg,The Netherlands.P.
Merlo and L. van der Plas.
2009.
Abstraction and gen-eralisation in semantic role labels: PropBank, VerbNetor both?
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing of theAFNLP, pages 288?296, Suntec, Singapore.P.
Monachesi, G. Stevens, and J. Trapman.
2007.
Addingsemantic role annotation to a corpus of written Dutch.In Proceedings of the Linguistic Annotation Workshop(LAW), pages 77?84, Prague, Czech republic.S.
Ozdowska and A.
Way.
2009.
Optimal bilingual data forFrench-English PB-SMT.
In Proceedings of the 13th An-nual Conference of the European Association for MachineTranslation (EAMT?09), pages 96?103, Barcelona, Spain.S.
Pado?.
2007.
Cross-lingual Annotation Projection Mod-els for Role-Semantic Information.
Ph.D. thesis, SaarlandUniversity.P.
Pajas and J.
S?te?pa?nek.
2008.
Recent advances in a feature-rich framework for treebank annotation.
In Proceedings ofthe 22nd International Conference on Computational Lin-guistics (Coling 2008), pages 673?680, Manchester, UK.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
The Proposi-tion Bank: An annotated corpus of semantic roles.
Com-putational Linguistics, 31:71?105.I.
Titov and J. Henderson.
2007.
A latent variable modelfor generative dependency parsing.
In Proceedings of theInternational Conference on Parsing Technologies (IWPT-07), pages 144?155, Prague, Czech Republic.117
