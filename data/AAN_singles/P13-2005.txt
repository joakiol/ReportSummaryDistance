Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 24?29,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExploiting Topic based Twitter Sentiment for Stock PredictionJianfeng Si* Arjun Mukherjee?
Bing Liu?
Qing Li* Huayi Li?
Xiaotie Deng?
*Department of Computer Science, City University of Hong Kong, Hong Kong, China*{ thankjeff@gmail.com, qing.li@cityu.edu.hk}?Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA?
{ arjun4787@gmail.com, liub@cs.uic.edu, lhymvp@gmail.com}?AIMS Lab, Department of Computer Science, Shanghai Jiaotong University, Shanghai, China?deng-xt@cs.sjtu.edu.cnAbstractThis paper proposes a technique to leveragetopic based sentiments from Twitter to helppredict the stock market.
We first utilize a con-tinuous Dirichlet Process Mixture model tolearn the daily topic set.
Then, for each topicwe derive its sentiment according to its opin-ion words distribution to build a sentimenttime series.
We then regress the stock indexand the Twitter sentiment time series to predictthe market.
Experiments on real-life S&P100Index show that our approach is effective andperforms better than existing state-of-the-artnon-topic based methods.1 IntroductionSocial media websites such as Twitter, Facebook,etc., have become ubiquitous platforms for socialnetworking and content sharing.
Every day, theygenerate a huge number of messages, which giveresearchers an unprecedented opportunity to uti-lize the messages and the public opinions con-tained in them for a wide range of applications(Liu, 2012).
In this paper, we use them for theapplication of stock index time series analysis.Here are some example tweets upon queryingthe keyword ?$aapl?
(which is the stock symbolfor Apple Inc.) in Twitter:1.
?Shanghai Oriental Morning Post confirm-ing w Sources that $AAPL TV will debutin May, Prices range from $1600-$3200,but $32,000 for a 50"wow.?2.
?$AAPL permanently lost its bid for a banon U.S. sales of the Samsung Galaxy Nex-us http://dthin.gs/XqcY74.?3.
?$AAPL is loosing customers.
everybody isbuying android phones!
$GOOG.
?As shown, the retrieved tweets may talk aboutApple?s products, Apple?s competition relation-ship with other companies, etc.
These messagesare often related to people?s sentiments aboutApple Inc., which can affect or reflect its stocktrading since positive sentiments can impactsales and financial gains.
Naturally, this hintsthat topic based sentiment is a useful factor toconsider for stock prediction as they reflect peo-ple?s sentiment on different topics in a certaintime frame.This paper focuses on daily one-day-aheadprediction of stock index based on the temporalcharacteristics of topics in Twitter in the recentpast.
Specifically, we propose a non-parametrictopic-based sentiment time series approach toanalyzing the streaming Twitter data.
The keymotivation here is that Twitter?s streaming mes-sages reflect fresh sentiments of people whichare likely to be correlated with stocks in a shorttime frame.
We also analyze the effect of trainingwindow size which best fits the temporal dynam-ics of stocks.
Here window size refers to thenumber of days of tweets used in model building.Our final prediction model is built using vec-tor autoregression (VAR).
To our knowledge,this is the first attempt to use non-parametriccontinuous topic based Twitter sentiments forstock prediction in an autoregressive framework.2 Related Work2.1 Market Prediction and Social MediaStock market prediction has attracted a great dealof attention in the past.
Some recent researchessuggest that news and social media such as blogs,micro-blogs, etc., can be analyzed to extract pub-lic sentiments to help predict the market (La-vrenko et al, 2000; Schumaker and Chen, 2009).Bollen et al (2011) used tweet based publicmood to predict the movement of Dow Jones*   The work was done when the first author was visitingUniversity of Illinois at Chicago.24Industrial Average index.
Ruiz et al (2012) stud-ied the relationship between Twitter activitiesand stock market under a graph based view.Feldman et al (2011) introduced a hybrid ap-proach for stock sentiment analysis based oncompanies?
news articles.2.2 Aspect and Sentiment ModelsTopic modeling as a task of corpus explorationhas attracted significant attention in recent years.One of the basic and most widely used models isLatent Dirichlet Allocation (LDA) (Blei et al,2003).
LDA can learn a predefined number oftopics and has been widely applied in its extend-ed forms in sentiment analysis and many othertasks (Mei et al, 2007; Branavan et al, 2008; Linand He, 2009; Zhao et al, 2010; Wang et al,2010; Brody and Elhadad, 2010; Jo and Oh, 2011;Moghaddam and Ester, 2011; Sauper et al, 2011;Mukherjee and Liu, 2012; He et al, 2012).The Dirichlet Processes Mixture (DPM) modelis a non-parametric extension of LDA (Teh et al,2006), which can estimate the number of topicsinherent in the data itself.
In this work, we em-ploy topic based sentiment analysis using DPMon Twitter posts (or tweets).
First, we employ aDPM to estimate the number of topics in thestreaming snapshot of tweets in each day.Next, we build a sentiment time series basedon the estimated topics of daily tweets.
Lastly,we regress the stock index and the sentimenttime series in an autoregressive framework.3 ModelWe now present our stock prediction framework.3.1 Continuous DPM ModelComparing to edited articles, it is much harder topreset the number of topics to best fit continuousstreaming Twitter data due to the large topic di-versity in tweets.
Thus, we resort to a non-parametric approach: the Dirichlet Process Mix-ture (DPM) model, and let the model estimate thenumber of topics inherent in the data itself.Mixture model is widely used in clustering andcan be formalized as follows:?
(       )(1)where    is a data point,    is its cluster label, Kis the number of topics,  (       ) is the sta-tistical (topic) models: *  +and     is thecomponent weight satisfying      and?
.In our setting of DPM, the number of mixturecomponents (topics) K is unfixed apriori but es-timated from tweets in each day.
DPM is definedas in (Neal, 2010):(  )(   )                 (2)where    is the parameter of the model thatbelongs to, and   is defined as a Dirichlet Pro-cess with the base measure H and the concentra-tion parameter   (Neal, 2010).We note that neighboring days may share thesame or closely related topics because some top-ics may last for a long period of time coveringmultiple days, while other topics may just last fora short period of time.
Given a set of time-stamped tweets, the overall generative processshould be dynamic as the topics evolve over time.There are several ways to model this dynamicnature (Sun et al, 2010; Kim and Oh, 2011;Chua and Asur, 2012; Blei and Lafferty, 2006;Wang et al, 2008).
In this paper, we follow theapproach of Sun et al (2010) due to its generalityand extensibility.Figure 1 shows the graphical model of our con-tinuous version of DPM (which we call cDPM).As shown, the tweets set is divided into dailybased collections: *         +  *    +are theobserved tweets and *    +are the model pa-rameters (latent topics) that generate these tweets.For each subset of tweets,    (tweets of day  ),we build a DPM on it.
For the first day (   ),the model functions the same as a standard DPM,i.e., all the topics use the same base measure,( ).
However, for later days (   ),besides the base measure,      ( ), we makeuse of topics learned from previous days as pri-ors.
This ensures smooth topic chains or links(details in ?3.2).
For efficiency, we only considertopics of one previous day as priors.We use collapsed Gibbs sampling (Bishop,2006) for model inference.
Hyper-parameters areset to:              ;       as in(Sun et al, 2010; Teh et al, 2006) which havebeen shown to work well.
Because a tweet has atmost 140 characters, we assume that each tweetcontains only one topic.
Hence, we only need toFigure 1: Continuous DPM.
?25sample the topic assignment    for each tweet   .According to different situations with respectto a topic?s prior, for each tweet    in   , theconditional distribution for    given all othertweets?
topic assignments, denoted by    , can besummarized as follows:1.    is a new topic: Its candidate priors containthe symmetric base prior    ( )  and topics*      +learned from            .?
If    takes a symmetric base prior:()(    )(       )?
(      )?
( )(3)where the first part denotes the prior proba-bility according to the Dirichlet Process andthe second part is the data likelihood (thisinterpretation can similarly be applied to thefollowing three equations).?
If    takes one topic k from *      +asits prior:()(    )(       )?
(         ( )     )?
(         ( ))(4)2. k is an existing topic: We already know itsprior.?
If k takes a symmetric base prior:(               )(        ( ))(           ( ))?
()?
()(5)?
If k takes topic        as its prior:(               ).
( )/.
( )/?
(         ( ))?
(         ( ))(6)Notations in the above equations are listed asfollows:?
is the number of topics learned in day t-1.?
|V| is the vocabulary size.?
is the document length of   .?
is the term frequency of word   in   .?
( ) is the probability of word   in pre-vious day?s topic k.?is the number of tweets assigned to topic kexcluding the current one   .
?is the term frequency of word   in topic k,with statistic from    excluded.
While    ( )denotes the marginalized sum of all words intopic k with statistic from    excluded.Similarly, the posteriors on *    ( )+  (topicword distributions) are given according to theirprior situations as follows:?
If topic k takes the base prior:( )   (      ) (         ( )) ?
(7)where      is the frequency of word   in topick and    ( )  is the marginalized sum over allwords.?
otherwise, it is defined recursively as:( )  (          ( )      ) (         ( ))?
(8)where       serves as the topic prior for     .Finally, for each day we estimate the topicweights,    as follows:?
?
(9)where    is the number of tweets in topic k.3.2 Topic-based Sentiment Time SeriesBased on an opinion lexicon   (a list of positiveand negative opinion words, e.g., good and bad),each opinion word,     is assigned with a po-larity label  ( ) as ?+1?
if it is positive and ?-1?if negative.
We spilt each tweet?s text into opin-ion part and non-opinion part.
Only non-opinionwords in tweets are used for Gibbs sampling.Based on DPM, we learn a set of topics fromthe non-opinion words space  .
The correspond-ing tweets?
opinion words share the same topicassignments as its tweet.
Then, we compute theposterior on opinion word probability,( )for topic   analogously to equations (7) and (8).Finally, we define the topic based sentimentscore  (   ) of topic   in day t as a weightedlinear combination of the opinion polarity labels:(   )   ?
( )( );  (   )   ,    -    (10)According to the generative process of cDPM,topics between neighboring days are linked if atopic k takes another topic as its prior.
We regardthis as evolution of topic k. Although there maybe slight semantic variation, the assumption isreasonable.
Then, the sentiment scores for eachtopic series form the sentiment time series {?,S(t-1, k), S(t, k), S(t+1, k), ...}.Figure 2 demonstrates the linking processwhere a triangle denotes a new topic (with basesymmetric prior), a circle denotes a middle topic(taking a topic from the previous day as its prior,0      ?
t-1          t         t+1    ?
NFigure 2: Linking the continuous topics vianeighboring priors.Figure 1: Continuous DPM?...?...??
?26while also supplying prior for the next day) andan ellipse denotes an end topic (no further topicsuse it as a prior).
In this example, two continuoustopic chains or links (via linked priors) exist forthe time interval [t-1, t+1]: one in light grey color,and the other in black.
As shown, there may bemore than one topic chain/link (5-20 in our ex-periments) for a certain time interval1.Thus, wesort multiple sentiment series according to theiraccumulative weights of topics over each link:?.
In our experiments, we try the topfive series and use the one that gives the best re-sult, which is mostly the first (top ranked) serieswith a few exceptions of the second series.
Thetopics mostly focus on hot keywords like: news,stocknews, earning, report, which stimulate ac-tive discussions on the social media platform.3.3 Time Series Analysis with VARFor model building, we use vector autoregression(VAR).
The first order (time steps of historicalinformation to use: lag = 1) VAR model for twotime series *  + and *  + is given by:(11)where * + are the white noises and * + are modelparameters.
We use the ?dse?
library2 in the Rlanguage to fit our VAR model based on leastsquare regression.Instead of training in one period and predictingover another disjointed period, we use a movingtraining and prediction process under slidingwindows3 (i.e., train in [t, t + w] and predict in-dex on t + w + 1) with two main considerations:?
Due to the dynamic and random nature of boththe stock market and public sentiments, we aremore interested in their short term relationship.?
Based on the sliding windows, we have moretraining and testing points.Figure 3 details the algorithm for stock indexprediction.
The accuracy is computed based onthe index up and down dynamics, the function(    ) returns True only if   (our predic-tion) and   (actual value) share the same indexup or down direction.1 The actual topic priors for topic links are governed by thefour cases of the Gibbs Sampler.2 http://cran.r-project.org/web/packages/dse3  This is similar to the autoregressive moving average(ARMA) models.4 DatasetWe collected the tweets via Twitter?s REST APIfor streaming data, using symbols of the Stand-ard & Poor's 100 stocks (S&P100) as keywords.In this study, we focus only on predicting theS&P100 index.
The time period of our dataset isbetween Nov. 2, 2012 and Feb. 7, 2013, whichgave us 624782 tweets.
We obtained the S&P100index?s daily close values from Yahoo Finance.5 Experiment5.1 Selecting a Sentiment MetricBollen et al (2011) used the mood dimension,Calm together with the index value itself to pre-dict the Dow Jones Industrial Average.
However,their Calm lexicon is not publicly available.
Wethus are unable to perform a direct comparisonwith their system.
We identified and labeled aCalm lexicon (words like ?anxious?, ?shocked?,?settled?
and ?dormant?)
using the opinion lexi-con4 of Hu and Liu (2004) and computed the sen-timent score using the method of Bollen et al(2011) (sentiment ratio).
Our pilot experimentsshowed that using the full opinion lexicon of Huand Liu (2004) actually performs consistentlybetter than the Calm lexicon.
Hence, we use theentire opinion lexicon in Hu and Liu (2004).5.2 S&P100INDEX Movement PredictionWe evaluate the performance of our method bycomparing with two baselines.
The first (Index)uses only the index itself, which reduces theVAR model to the univariate autoregressivemodel (AR), resulting in only one index timeseries {  } in the algorithm of Figure 3.4 http://cs.uic.edu/~liub/FBS/opinion-lexicon-English.rarParameter:w: training window size; lag: the order of VAR;Input:   : the date of time series; {  }: sentiment timeseries; {  }: index time series;Output: prediction accuracy.1.
for t = 0, 1, 2, ?, N-w-12.
{3.
= VAR( ,     -  ,     -, lag);4.=      .Predict(x[t+w+1-lag, t+w],y[t+w+1-lag, t+w]);5.       if (     () )rightNum++;6.
}7.
Accuracy = rightNum / (N-w);8.
Return Accuracy;Figure 3: Prediction algorithm and accuracyFigure 1: Continuous DPM27When considering Twitter sentiments, existingworks (Bollen et al, 2011, Ruiz et al, 2012)simply compute the sentiment score as ratio ofpos/neg opinion words per day.
This generates alexicon-based sentiment time series, which isthen combined with the index value series to giveus the second baseline Raw.In summary, Index uses index only with theAR model while Raw uses index and opinionlexicon based time series.
Our cDPM uses indexand the proposed topic based sentiment time se-ries.
Both Raw and cDPM employ the two di-mensional VAR model.
We experiment with dif-ferent lag settings from 1-3 days.We also experiment with different trainingwindow sizes, ranging from 15 - 30 days, andcompute the prediction accuracy for each win-dow size.
Table 1 shows the respective averageand best accuracies over all window sizes foreach lag and Table 2 summarizes the pairwiseperformance improvements of averaged scoresover all training window sizes.
Figure 4 show thedetailed accuracy comparison for lag 1 and lag 3.From Table 1, 2, and Figure 4, we note:i. Topic-based public sentiments from tweetscan improve stock prediction over simple sen-timent ratio which may suffer from backchan-nel noise and lack of focus on prevailing top-ics.
For example, on lag 2, Raw performsworse by 8.6% than Index itself.ii.
cDPM outperforms all others in terms of boththe best accuracy (lag 3) and the average ac-curacies for different window sizes.
The max-imum average improvement reaches 25.0%compared to Index at lag 1 and 15.1% com-pared to Raw at lag 3.
This is due to the factthat cDPM learns the topic based sentimentsinstead of just using the opinion words?
ratiolike Raw, and in a short time period, sometopics are more correlated with the stock mar-ket than others.
Our proposed sentiment timeseries using cDPM can capture this phenome-non and also help reduce backchannel noiseof raw sentiments.iii.
On average, cDPM gets the best performancefor training window sizes within [21, 22], andthe best prediction accuracy is 68.0% on win-dow size 22 at lag 3.6 ConclusionsPredicting the stock market is an important butdifficult problem.
This paper showed that Twit-ter?s topic based sentiment can improve the pre-diction accuracy beyond existing non-topic basedapproaches.
Specifically, a non-parametric topic-based sentiment time series approach was pro-posed for the Twitter stream.
For prediction, vec-tor autoregression was used to regress S&P100index with the learned sentiment time series.
Be-sides the short term dynamics based prediction,we believe that the proposed method can be ex-tended for long range dependency analysis ofTwitter sentiments and stocks, which can renderdeep insights into the complex phenomenon ofstock market.
This will be part of our future work.AcknowledgmentsThis work was supported in part by a grant fromthe National Science Foundation (NSF) undergrant no.
IIS-1111092 and a strategic researchgrant from City University of Hong Kong (pro-ject number: 7002770).Lag Index Raw cDPM1 0.48(0.54) 0.57(0.59) 0.60(0.64)2 0.58(0.65) 0.53(0.62) 0.60(0.63)3 0.52(0.56) 0.53(0.60) 0.61(0.68)Table 1: Average (best) accuracies over alltraining window sizes and different lags 1, 2, 3.Lag Raw vs. Index cDPM vs. Index cDPM vs. Raw1 18.8% 25.0% 5.3%2 -8.6% 3.4% 13.2%3 1.9% 17.3% 15.1%Table 2: Pairwise improvements among Index,Raw and cDPM averaged over all training win-dow sizes.Figure 4: Comparison of prediction accuracy ofup/down stock index on S&P 100 index for dif-ferent training window sizes.0.250.450.6515 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30AccuracyTraining window sizeComparison on Lag 1Index Raw cDPM0.250.350.450.550.650.7518 19 20 21 22 23 24 25 26 27 28 29 30AccuracyTraining Window sizeComparison on Lag 3Index Raw cDPM28ReferencesBishop, C. M. 2006.
Pattern Recognition and MachineLearning.
Springer.Blei, D., Ng, A. and Jordan, M. 2003.
Latent Dirichletallocation.
Journal of Machine Learning Research3:993?1022.Blei, D. and Lafferty, J.
2006.
Dynamic topic models.In Proceedings of the 23rd International Confer-ence on Machine Learning (ICML-2006).Bollen, J., Mao, H. N., and Zeng, X. J.
2011.
Twittermood predicts the stock market.
Journal of Com-puter Science 2(1):1-8.Branavan, S., Chen, H., Eisenstein J. and Barzilay, R.2008.
Learning document-level semantic propertiesfrom free-text annotations.
In Proceedings of theAnnual Meeting of the Association for Computa-tional Linguistics (ACL-2008).Brody, S. and Elhadad, S. 2010.
An unsupervisedaspect-sentiment model for online reviews.
In Pro-ceedings of the 2010 Annual Conference of theNorth American Chapter of the ACL (NAACL-2010).Chua, F. C. T. and Asur, S. 2012.
Automatic Summa-rization of Events from Social Media, TechnicalReport, HP Labs.Feldman, R., Benjamin, R., Roy, B. H. and Moshe, F.2011.
The Stock Sonar - Sentiment analysis ofstocks based on a hybrid approach.
In Proceedingsof 23rd IAAI Conference on Artificial Intelligence(IAAI-2011).He, Y., Lin, C., Gao, W., and Wong, K. F. 2012.Tracking sentiment and topic dynamics from socialmedia.
In Proceedings of the 6th InternationalAAAI Conference on Weblogs and Social Media(ICWSM-2012).Hu, M. and Liu, B.
2004.
Mining and summarizingcustomer reviews.
In Proceedings of the ACMSIGKDD International Conference on KnowledgeDiscovery & Data Mining (KDD-2004).Jo, Y. and Oh, A.
2011.
Aspect and sentiment unifica-tion model for online review analysis.
In Proceed-ings of ACM Conference in Web Search and DataMining (WSDM-2011).Kim, D. and Oh, A.
2011.
Topic chains for under-standing a news corpus.
CICLing (2): 163-176.Lavrenko, V., Schmill, M., Lawrie, D., Ogilvie, P.,Jensen, D. and Allan, J.
2000.
Mining of concur-rent text and time series.
In Proceedings of the 6thKDD Workshop on Text Mining, 37?44.Lin, C. and He, Y.
2009.
Joint sentiment/topic modelfor sentiment analysis.
In Proceedings of ACM In-ternational Conference on Information andKnowledge Management (CIKM-2009).Liu, B.
2012.
Sentiment analysis and opinion mining.Morgan & Claypool Publishers.Mei, Q., Ling, X., Wondra, M., Su, H. and Zhai, C.2007.
Topic sentiment mixture: modeling facetsand opinions in weblogs.
In Proceedings of Interna-tional Conference on World Wide Web (WWW-2007).Moghaddam, S. and Ester, M. 2011.
ILDA: Interde-pendent LDA model for learning latent aspects andtheir ratings from online product reviews.
In Pro-ceedings of the Annual ACM SIGIR Internationalconference on Research and Development in In-formation Retrieval (SIGIR-2011).Mukherjee A. and Liu, B.
2012.
Aspect extractionthrough semi-supervised modeling.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (ACL-2012).Neal, R.M.
2000.
Markov chain sampling methods fordirichlet process mixture models.
Journal of Com-putational and Graphical Statistics, 9(2):249-265.Ruiz, E. J., Hristidis, V., Castillo, C., Gionis, A. andJaimes, A.
2012.
Correlating financial time serieswith micro-blogging activity.
In Proceedings of thefifth ACM international conference on Web searchand data mining (WSDM-2012), 513-522.Sauper, C., Haghighi, A. and Barzilay, R. 2011.
Con-tent models with attitude.
Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics (ACL).Schumaker, R. P. and Chen, H. 2009.
Textual analysisof stock market prediction using breaking financialnews.
ACM Transactions on Information Systems27(February (2)):1?19.Sun, Y.
Z., Tang, J. Han, J., Gupta M. and Zhao, B.2010.
Community Evolution Detection in DynamicHeterogeneous Information Networks.
In Proceed-ings of KDD Workshop on Mining and Learningwith Graphs (MLG'2010), Washington, D.C.Teh, Y., Jordan M., Beal, M. and Blei, D. 2006.
Hier-archical Dirichlet processes.
Journal of the Ameri-can Statistical Association, 101[476]:1566-1581.Wang, C. Blei, D. and Heckerman, D. 2008.
Continu-ous Time Dynamic Topic Models.
Uncertainty inArtificial Intelligence (UAI 2008), 579-586Wang, H., Lu, Y.  and Zhai, C. 2010.
Latent aspectrating analysis on review text data: a rating regres-sion approach.
Proceedings of ACM SIGKDD In-ternational Conference on Knowledge Discoveryand Data Mining (KDD-2010).Zhao, W. Jiang, J. Yan, Y. and Li, X.
2010.
Jointlymodeling aspects and opinions with a MaxEnt-LDA hybrid.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing(EMNLP-2010).29
