Proceedings of ACL-08: HLT, pages 746?754,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsEM Can Find Pretty Good HMM POS-Taggers(When Given a Good Start)?Yoav Goldberg and Meni Adler and Michael ElhadadBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israel{yoavg,adlerm,elhadad}@cs.bgu.ac.ilAbstractWe address the task of unsupervised POS tag-ging.
We demonstrate that good results can beobtained using the robust EM-HMM learnerwhen provided with good initial conditions,even with incomplete dictionaries.
We presenta family of algorithms to compute effectiveinitial estimations p(t|w).
We test the methodon the task of full morphological disambigua-tion in Hebrew achieving an error reduction of25% over a strong uniform distribution base-line.
We also test the same method on the stan-dard WSJ unsupervised POS tagging task andobtain results competitive with recent state-of-the-art methods, while using simple and effi-cient learning methods.1 IntroductionThe task of unsupervised (or semi-supervised) part-of-speech (POS) tagging is the following: given adictionary mapping words in a language to their pos-sible POS, and large quantities of unlabeled textdata, learn to predict the correct part of speech fora given word in context.
The only supervision givento the learning process is the dictionary, which ina realistic scenario, contains only part of the wordtypes observed in the corpus to be tagged.Unsupervised POS tagging has been traditionallyapproached with relative success (Merialdo, 1994;Kupiec, 1992) by HMM-based generative mod-els, employing EM parameters estimation using theBaum-Welch algorithm.
However, as recently noted?This work is supported in part by the Lynn and WilliamFrankel Center for Computer Science.by Banko and Moore (2004), these works made useof filtered dictionaries: dictionaries in which onlyrelatively probable analyses of a given word are pre-served.
This kind of filtering requires serious su-pervision: in theory, an expert is needed to go overthe dictionary elements and filter out unlikely anal-yses.
In practice, counts from an annotated corpushave been traditionally used to perform the filtering.Furthermore, these methods require rather compre-hensive dictionaries in order to perform well.In recent work, researchers try to address thesedeficiencies by using dictionaries with unfilteredPOS-tags, and testing the methods on ?diluted dic-tionaries?
?
in which many of the lexical entries aremissing (Smith and Eisner, 2005) (SE), (Goldwaterand Griffiths, 2007) (GG), (Toutanova and Johnson,2008) (TJ).All the work mentioned above focuses on unsu-pervised English POS tagging.
The dictionaries areall derived from tagged English corpora (all recentwork uses the WSJ corpus).
As such, the setting ofthe research is artificial: there is no reason to per-form unsupervised learning when an annotated cor-pus is available.
The problem is rather approachedas a workbench for exploring new learning methods.The result is a series of creative algorithms, that havesteadily improved results on the same dataset: unsu-pervised CRF training using contrastive estimation(SE), a fully-bayesian HMM model that jointly per-forms clustering and sequence learning (GG), anda Bayesian LDA-based model using only observedcontext features to predict tag words (TJ).
These so-phisticated learning algorithms all outperform thetraditional baseline of EM-HMM based methods,746while relying on similar knowledge: the lexical con-text of the words to be tagged and their letter struc-ture (e.g., presence of suffixes, capitalization andhyphenation).1Our motivation for tackling unsupervised POStagging is different: we are interested in develop-ing a Hebrew POS tagger.
We have access to a goodHebrew lexicon (and a morphological analyzer), anda fair amount of unlabeled training data, but hardlyany annotated corpora.
We actually report resultson full morphological disambiguation for Hebrew, atask similar but more challenging than POS tagging:we deal with a tagset much larger than English (over3,561 distinct tags) and an ambiguity level of about2.7 per token as opposed to 1.4 for English.
Insteadof inventing a new learning framework, we go backto the traditional EM trained HMMs.
We argue thatthe key challenge to learning an effective model isto define good enough initial conditions.
Given suf-ficiently good initial conditions, EM trained modelscan yield highly competitive results.
Such modelshave other benefits as well: they are simple, robust,and computationally more attractive.In this paper, we concentrate on methods for de-riving sufficiently good initial conditions for EM-HMM learning.
Our method for learning initial con-ditions for the p(t|w) distributions relies on a mix-ture of language specific models: a paradigmaticmodel of similar words (where similar words arewords with similar inflection patterns), simple syn-tagmatic constraints (e.g., the sequence V-V is ex-tremely rare in English).
These are complementedby a linear lexical context model.
Such models aresimple to build and test.We present results for unsupervised PoS taggingof Hebrew text and for the common WSJ Englishtest sets.
We show that our method achieves state-of-the-art results for the English setting, even with a rel-atively small dictionary.
Furthermore, while recentwork report results on a reduced English tagset of17 PoS tags, we also present results for the complete45 tags tagset of the WSJ corpus.
This considerablyraises the bar of the EM-HMM baseline.
We alsoreport state-of-the-art results for Hebrew full mor-1Another notable work, though within a slightly differ-ent framework, is the prototype-driven method proposed by(Haghighi and Klein, 2006), in which the dictionary is replacedwith a very small seed of prototypical examples.phological disambiguation.Our primary conclusion is that the problem oflearning effective stochastic classifiers remains pri-marily a search task.
Initial conditions play a domi-nant role in solving this task and can rely on linguis-tically motivated approximations.
A robust learn-ing method (EM-HMM) combined with good initialconditions based on a robust feature set can go along way (as opposed to a more complex learningmethod).
It seems that computing initial conditionsis also the right place to capture complex linguisticintuition without fear that over-generalization couldlead a learner to diverge.2 Previous WorkThe tagging accuracy of supervised stochastic tag-gers is around 96%?97% (Manning and Schutze,1999).
Merialdo (1994) reports an accuracyof 86.6% for an unsupervised token-based EM-estimated HMM, trained on a corpus of about 1Mwords, over a tagset of 159 tags.
Elworthy (1994), incontrast, reports accuracy of 75.49%, 80.87%, and79.12% for unsupervised word-based HMM trainedon parts of the LOB corpora, with a tagset of 134tags.
With (artificially created) good initial condi-tions, such as a good approximation of the tag distri-bution for each word, Elworthy reports an improve-ment to 94.6%, 92.27%, and 94.51% on the samedata sets.
Merialdo, on the other hand, reports an im-provement to 92.6% and 94.4% for the case where100 and 2,000 sentences of the training corpus aremanually tagged.
Later, Banko and Moore (2004)observed that earlier unsupervised HMM-EM re-sults were artificially high due to use of OptimizedLexicons, in which only frequent-enough analysesof each word were kept.
Brill (1995b) proposedan unsupervised tagger based on transformation-based learning (Brill, 1995a), achieving accuraciesof above 95%.
This unsupervised tagger relied onan initial step in which the most probable tag foreach word is chosen.
Optimized lexicons and Brill?smost-probable-tag Oracle are not available in realis-tic unsupervised settings, yet, they show that goodinitial conditions greatly facilitate learning.Recent work on unsupervised POS tagging forEnglish has significantly improved the results on thistask: GG, SE and most recently TJ report the best re-747sults so far on the task of unsupervised POS taggingof the WSJ with diluted dictionaries.
With dictionar-ies as small as 1249 lexical entries the LDA-basedmethod with a strong ambiguity-class model reachesPOS accuracy as high as 89.7% on a reduced tagsetof 17 tags.While these 3 methods rely on the same featureset (lexical context, spelling features) for the learn-ing stage, the LDA approach bases its predictionsentirely on observable features, and excludes the tra-ditional hidden states sequence.In Hebrew, Levinger et al (1995) introduced thesimilar-words algorithm for estimating p(t|w) fromunlabeled data, which we describe below.
Ourmethod uses this algorithm as a first step, and refinesthe approximation by introducing additional linguis-tic constraints and an iterative refinement step.3 Initial Conditions For EM-HMMThe most common model for unsupervised learningof stochastic processes is Hidden Markov Models(HMM).
For the case of tagging, the states corre-spond to the tags ti, and words wi are emitted eachtime a state is visited.
The parameters of the modelcan be estimated by applying the Baum-Welch EMalgorithm (Baum, 1972), on a large-scale corpus ofunlabeled text.
The estimated parameters are thenused in conjunction with Viterbi search, to find themost probable sequence of tags for a given sentence.In this work, we follow Adler (2007) and use a vari-ation of second-order HMM in which the probabilityof a tag is conditioned by the tag that precedes it andby the one that follows it, and the probability of anemitted word is conditioned by its tag and the tagthat follows it2.
In all experiments, we use the back-off smoothing method of (Thede and Harper, 1999),with additive smoothing (Chen, 1996) for the lexicalprobabilities.We investigate methods to approximate the initialparameters of the p(t|w) distribution, from whichwe obtain p(w|t) by marginalization and Bayesianinversion.
We also experiment with constraining thep(t|t?1, t+1) distribution.2Technically this is not Markov Model but a DependencyNet.
However, bidirectional conditioning seem more suitablefor language tasks, and in practice the learning and inferencemethods are mostly unaffected.
See (Toutanova et al, 2003).General syntagmatic constraints We set linguis-tically motivated constraints on the p(t|t?1, t+1)distribution.
In our setting, these are used to forcethe probability of some events to 0 (e.g., ?Hebrewverbs can not be followed by the of preposition?
).Morphology-based p(t|w) approximationLevinger et al (1995) developed a context-freemethod for acquiring morpho-lexical probabilities(p(t|w)) from an untagged corpus.
The method isbased on language-specific rules for constructing asimilar words (SW) set for each analysis of a word.This set is composed of morphological variationsof the word under the given analysis.
For example,the Hebrew token ???
can be analyzed as either anoun (boy) or a verb (gave birth).
The noun SW setfor this token is composed of the definiteness andnumber inflections ????,?????,??????
(the boy, boys,the boys), while the verb SW set is composedof gender and tense inflections ????,????
(she/theygave birth).
The approximated probability of eachanalysis is based on the corpus frequency of its SWset.
For the complete details, refer to the originalpaper.
Cucerzan and Yarowsky (2000) proposeda similar method for the unsupervised estimationof p(t|w) in English, relying on simple spellingfeatures to characterize similar word classes.Linear-Context-based p(t|w) approximationThe method of Levinger et al makes use of Hebrewinflection patterns in order to estimate context freeapproximation of p(t|w) by relating a word to itsdifferent inflections.
However, the context in whicha word occurs can also be very informative withrespect to its POS-analysis (Schu?tze, 1995).
Wepropose a novel algorithm for estimating p(t|w)based on the contexts in which a word occurs.3The algorithm starts with an initial p(t|w) esti-mate, and iteratively re-estimates:p?
(t|c) =?w?W p(t|w)p(w|c)Zp?
(t|w) =?c?RELC p(t|c)p(c|w)allow(t, w)Z3While we rely on the same intuition, our use of contextdiffers from earlier works on distributional POS-tagging like(Schu?tze, 1995), in which the purpose is to directly assign thepossible POS for an unknown word.
In contrast, our algorithmaims to improve the estimate for the whole distribution p(t|w),to be further disambiguated by the EM-HMM learner.748where Z is a normalization factor, W is the set ofall words in the corpus, C is the set of all contexts,andRELC ?
C is a set of reliable contexts, definedbelow.
allow(t, w) is a binary function indicatingwhether t is a valid tag for w. p(c|w) and p(w|c) areestimated via raw corpus counts.Intuitively, we estimate the probability of a taggiven a context as the average probability of a taggiven any of the words appearing in that context, andsimilarly the probability of a tag given a word is theaveraged probability of that tag in all the (reliable)contexts in which the word appears.
At each round,we define RELC , the set of reliable contexts, to bethe set of all contexts in which p(t|c) > 0 for at mostX different ts.The method is general, and can be applied to dif-ferent languages.
The parameters to specify for eachlanguage are: the initial estimation p(t|w), the esti-mation of the allow relation for known and OOVwords, and the types of contexts to consider.4 Application to HebrewIn Hebrew, several words combine into a single to-ken in both agglutinative and fusional ways.
Thisresults in a potentially high number of tags for eachtoken.
On average, in our corpus, the number of pos-sible analyses per known word reached 2.7, with theambiguity level of the extended POS tagset in cor-pus for English (1.41) (Dermatas and Kokkinakis,1995).In this work, we use the morphological analyzerof MILA ?
Knowledge Center for Processing He-brew (KC analyzer).
In contrast to English tagsets,the number of tags for Hebrew, based on all com-binations of the morphological attributes, can growtheoretically to about 300,000 tags.
In practice, wefound ?only?
about 3,560 tags in a corpus of 40Mtokens training corpus taken from Hebrew news ma-terial and Knesset transcripts.
For testing, we man-ually tagged the text which is used in the HebrewTreebank (Sima?an et al, 2001) (about 90K tokens),according to our tagging guidelines.4.1 Initial ConditionsGeneral syntagmatic constraints We define 4syntagmatic constraints over p(t|t?1, t+1): (1) aconstruct state form cannot be followed by a verb,preposition, punctuation, existential, modal, or cop-ula; (2) a verb cannot be followed by the preposition??
s?el (of), (3) copula and existential cannot be fol-lowed by a verb, and (4) a verb cannot be followedby another verb, unless one of them has a prefix, orthe second verb is an infinitive, or the first verb isimperative and the second verb is in future tense.4Morphology-Based p(t|w) approximation Weextended the set of rules used in Levinger et al , inorder to support the wider tagset used by the KC an-alyzer: (1) The SW set for adjectives, copulas, exis-tentials, personal pronouns, verbs and participles, iscomposed of all gender-number inflections; (2) TheSW set for common nouns is composed of all num-ber inflections, with definite article variation for ab-solute noun; (3) Prefix variations for proper nouns;(4) Gender variation for numerals; and (5) Gender-number variation for all suffixes (possessive, nomi-native and accusative).Linear-Context-based p(t|w) approximationFor the initial p(t|w) we use either a uniform distri-bution based on the tags allowed in the dictionary,or the estimate obtained by using the modifiedLevinger et al algorithm.
We use contexts of theform LR=w?1, w+1 (the neighbouring words).
Weestimate p(w|c) and p(c|w) via relative frequencyover all the events w1, w2, w3 occurring at least10 times in the corpus.
allow(t, w) follows thedictionary.
Because of the wide coverage of theHebrew lexicon, we take RELC to be C (allavailable contexts).4.2 EvaluationWe run a series of experiments with 8 distinct ini-tial conditions, as shown in Table 1: our baseline(Uniform) is the uniform distribution over all tagsprovided by the KC analyzer for each word.
TheSyntagmatic initial conditions add the p(t|t?1, t+1)constraints described above to the uniform base-line.
The Morphology-Based and Linear-Contextinitial conditions are computed as described above,while the Morph+Linear is the result of applyingthe linear-context algorithm over initial values com-puted by the Morphology-based method.
We repeat4This rule was taken from Shacham and Wintner(2007).749Initial Condition Dist Context-Free EM-HMMFull Seg+Pos Full Seg+PosUniform 60 63.8 71.9 85.5 89.8Syntagmatic Pair Constraints 60 / / 85.8 89.8Init-Trans 60 / / 87.9 91Morpho-LexicalMorph-Based 76.8 76.4 83.1 87.7 91.6Linear-Context 70.1 75.4 82.6 85.3 89.6Morph+Linear 79.8 79.0 85.5 88 92PairConst+MorphMorph-Based / / / 87.6 91.4Linear-Context / / / 84.5 89.0Morph+Linear / / / 87.1 91.5InitTrans+MorphMorph-Based / / / 89.2 92.3Linear-Context / / / 87.7 90.9Morph+Linear / / / 89.4 92.4Table 1: Accuracy (%) of Hebrew MorphologicalDisambiguation and POS Tagging over various initialconditionsthese last 3 models with the addition of the syntag-matic constraints (Synt+Morph).For each of these, we first compare the computedp(t|w) against a gold standard distribution, takenfrom the test corpus (90K tokens), according to themeasure used by (Levinger et al, 1995) (Dist).
Onthis measure, we confirm that our improved morpho-lexical approximation improves the results reportedby Levinger et al from 74% to about 80% on aricher tagset, and on a much larger test set (90K vs.3,400 tokens).We then report on the effectiveness of p(t|w) asa context-free tagger that assigns to each word themost likely tag, both for full morphological analy-sis (3,561 tags) (Full) and for the simpler task oftoken segmentation and POS tag selection (36 tags)(Seg+Pos).
The best results on this task are 80.8%and 87.5% resp.
achieved on the Morph+Linear ini-tial conditions.Finally, we test effectiveness of the initial con-ditions with EM-HMM learning.
We reach 88%accuracy on full morphological and 92% accuracyfor POS tagging and word segmentation, for theMorph+Linear initial conditions.As expected, EM-HMM improves results (from80% to 88%).
Strikingly, EM-HMM improves theuniform initial conditions from 64% to above 85%.However, better initial conditions bring us muchover this particular local maximum ?
with an errorreduction of 20%.
In all cases, the main improve-ment over the uniform baseline is brought by themorphology-based initial conditions.
When appliedon its own, the linear context brings modest im-provement.
But the combination of the paradigmaticmorphology-based method with the linear contextimproves all measures.A most interesting observation is the detrimentalcontribution of the syntagmatic constraints we in-troduced.
We found that 113,453 sentences of thecorpus (about 5%) contradict these basic and ap-parently simple constraints.
As an alternative tothese common-sense constraints, we tried to use asmall seed of randomly selected sentences (10K an-notated tokens) in order to skew the initial uniformdistribution of the state transitions.
We initialize thep(t|t?1, t+1) distribution with smoothed ML esti-mates based on tag trigram and bigram counts (ig-noring the tag-word annotations).
This small seedinitialization (InitTrans) has a great impact on ac-curacy.
Overall, we reach 89.4% accuracy on fullmorphological and 92.4% accuracy for POS taggingand word segmentation, for the Morph+Linear con-ditions ?
an error reduction of more than 25% fromthe uniform distribution baseline.5 Application to EnglishWe now apply the same technique to English semi-supervised POS tagging.
Recent investigations ofthis task use dictionaries derived from the Penn WSJcorpus, with a reduced tag set of 17 tags5 instead ofthe original 45-tags tagset.
They experiment withfull dictionaries (containing complete POS informa-tion for all the words in the text) as well as ?diluted?dictionaries, from which large portions of the vo-cabulary are missing.
These settings are very dif-ferent from those used for Hebrew: the tagset ismuch smaller (17 vs. ?3,560) and the dictionariesare either complete or extremely crippled.
However,for the sake of comparison, we have reproduced thesame experimental settings.We derive dictionaries from the complete WSJcorpus6, and the exact same diluted dictionaries usedin SE, TJ and GG.5ADJ ADV CONJ DET ENDPUNC INPUNC LPUNCRPUNC N POS PRT PREP PRT TO V VBG VBN WH6The dictionary derived from the WSJ data is very noisy:many of the stop words get wrong analyses stemming from tag-ging mistakes (for instance, the word the has 6 possible analysesin the data-derived dictionary, which we checked manually andfound all but DT erroneous).
Such noise is not expected in a realworld dictionary, and our algorithm is not designed to accomo-date it.
We corrected the entries for the 20 most frequent wordsin the corpus.
This step could probably be done automatically,but we consider it to be a non-issue in any realistic setting.750Syntagmatic Constraints We indirectly incor-porated syntagmatic constraints through a smallchange to the tagset.
The 17-tags English tagsetallows for V-V transitions.
Such a construction isgenerally unlikely in English.
By separating modalsfrom the rest of the verbs, and creating an addi-tional class for the 5 be verbs (am,is,are,was,were),we made such transition much less probable.
Thenew 19-tags tagset reflects the ?verb can not followa verb?
constraint.Morphology-Based p(t|w) approximation En-glish morphology is much simpler compared to thatof Hebrew, making direct use of the Levinger con-text free approximation impossible.
However, somemorphological cues exist in English as well, in par-ticular common suffixation patterns.
We imple-mented our morphology-based context-free p(t|w)approximation for English as a special case of thelinear context-based algorithm described in Sect.3.Instead of generating contexts based on neighboringwords, we generate them using the following 5 mor-phological templates:suff=S The word has suffix S (suff=ing).L+suff=W,S The word appears just after word W ,with suffix S (L+suff=have,ed).R+suff=S,W The word appears just before wordW ,with suffix S (R+suff=ing,to)wsuf=S1,S2 The word suffix is S1, the same stem isseen with suffix S2 (wsuf=,s).suffs=SG The word stem appears with the SG groupof suffixes (suffs=ed,ing,s).We consider a word to have a suffix only if theword stem appears with a different suffix somewherein the text.
We implemented a primitive stemmerfor extracting the suffixes while preserving a us-able stem by taking care of few English orthogra-phy rules (handling, e.g., , bigger ?
big er, nicer?
nice er, happily ?
happy ly, picnicking ?
pic-nic ing).
For the immediate context W in the tem-plates L+suff,R+suff, we consider only the 20 mostfrequent tokens in the corpus.Linear-Context-based p(t|w) approximationWe expect the context based approximation to beparticularly useful in English.
We use the following3 context templates: LL=w?2,w?1, LR=w?1,w+1and RR=w+1,w+2.
We estimate p(w|c) and p(c|w)by relative frequency over word triplets occurring atleast twice in the unannotated training corpus.Combined p(t|w) approximation This approx-imation combines the morphological and linearcontext approximations by using all the above-mentioned context templates together in the iterativeprocess.For all three p(t|w) approximations, we takeRELC to be contexts containing at most 4 tags.allow(t, w) follows the dictionary for known words,and is the set of all open-class POS for unknownwords.
We take the initial p(t|w) for each w to beuniform over all the dictionary specified tags for w.Accordingly, the initial p(t|w) = 0 for w not in thedictionary.
We run the process for 8 iterations.7Diluted Dictionaries and Unknown WordsSome of the missing dictionary elements are as-signed a set of possible POS-tags and correspondingprobabilities in the p(t|w) estimation process.
Otherunknown tokens remain with no analysis at theend of the initial process computation.
For thesemissing elements, we assign an ambiguity class bya simple ambiguity-class guesser, and set p(t|w)to be uniform over all the tags in the ambiguityclass.
Our ambiguity-class guesser assigns for eachword the set of all open-class tags that appearedwith the word suffix in the dictionary.
The wordsuffix is the longest (up to 3 characters) suffix of theword that also appears in the top-100 suffixes in thedictionary.Taggers We test the resulting p(t|w) approxima-tion by training 2 taggers: CF-Tag, a context-freetagger assigning for each word its most probablePOS according to p(t|w), with a fallback to the mostprobable tag in case the word does not appear inthe dictionary or if ?t, p(t|w) = 0.
EM-HMM,a second-order EM-HMM initialized with the esti-mated p(t|w).Baselines As baseline, we use two EM-trainedHMM taggers, initialized with a uniform p(t|w) forevery word, based on the allowed tags in the dic-tionary.
For words not in the dictionary, we takethe allowed tags to be either all the open-class POS7This is the first value we tried, and it seems to work fine.We haven?t experimented with other values.
The same appliesfor the choice of 4 as the RELC threshold.751(uniform(oc)) or the allowed tags according to oursimple ambiguity-class guesser (uniform(suf)).All the p(t|w) estimates and HMM models aretrained on the entire WSJ corpus.
We use the same24K word test-set as used in SE, TJ and GG, as wellas the same diluted dictionaries.
We report the re-sults on the same reduced tagsets for comparison,but also include the results on the full 46 tags tagset.5.1 ResultsTable 2 summarizes the results of our experiments.Uniform initialization based on the simple suffix-based ambiguity class guesser yields big improve-ments over the uniform all-open-class initialization.However, our refined initial conditions always im-prove the results (by as much as 40% error re-duction).
As expected, the linear context is muchmore effective than the morphological one, espe-cially with richer dictionaries.
This seem to indi-cate that in English the linear context is better at re-fining the estimations when the ambiguity classesare known, while the morphological context is incharge of adding possible tags when the ambigu-ity classes are not known.
Furthermore, the bene-fit of the morphology-context is bigger for the com-plete tagset setting, indicating that, while the coarse-grained POS-tags are indicated by word distribu-tion, the finer distinctions are indicated by inflec-tions and orthography.
The combination of linearand morphology contexts is always beneficial.
Syn-tagmatic constraints (e.g., separating be verbs andmodals from the rest of the verbs) constantly im-prove results by about 1%.
Note that the context-freetagger based on our p(t|w) estimates is quite accu-rate.
As with the EM trained models, combining lin-ear and morphological contexts is always beneficial.To put these numbers in context, Table 3 listscurrent state-of-the art results for the same task.CE+spl is the Contrastive-Estimation CRF methodof SE.
BHMM is the completely Bayesian-HMMof GG.
PLSA+AC, LDA, LDA+AC are the mod-els presented in TJ, LDA+AC is a Bayesian modelwith a strong ambiguity class (AC) component, andis the current state-of-the-art of this task.
The othermodels are variations excluding the Bayesian com-ponents (PLSA+AC) or the ambiguity class.While our models are trained on the unannotatedtext of the entire WSJ Treebank, CE and BHMM usemuch less training data (only the 24k words of thetest-set).
However, as noted by TJ, there is no reasonone should limit the amount of unlabeled data used,and in addition other results reported in GG,SE showthat accuracy does not seem to improve as more un-labeled data are used with the models.
We also re-port results for training our EM-HMM tagger on thesmaller dataset (the p(t|w) estimation is still basedon the entire unlabeled WSJ).All the abovementioned models follow the as-sumption that all 17 tags are valid for the unknownwords.
In contrast, we restrict the set of allowedtags for an unknown word to open-class tags.
Closedclass words are expected to be included in a dictio-nary, even a small one.
The practice of allowing onlyopen-class tags for unknown words goes back a longway (Weischedel et al, 1993), and proved highlybeneficial also in our case.Notice that even our simplest models, in whichthe initial p(t|w) distribution for each w is uniform,already outperform most of the other models, and,in the case of the diluted dictionaries, by a widemargin.
Similarly, given the p(t|w) estimate, EM-HMM training on the smaller dataset (24k) is stillvery competitive (yet results improve with more un-labeled data).
When we use our refined p(t|w) dis-tribution as the basis of EM-HMM training, we getthe best results for the complete dictionary case.With the diluted dictionaries, we are outperformedonly by LDA+AC.
As we outperform this model inthe complete dictionary case, it seems that the ad-vantage of this model is due to its much strongerambiguity class model, and not its Bayesian com-ponents.
Also note that while we outperform thismodel when using the 19-tags tagset, it is slightlybetter in the original 17-tags setting.
It could be thatthe reliance of the LDA models on observed surfacefeatures instead of hidden state features is beneficialavoiding the misleading V-V transitions.We also list the performance of our best mod-els with a slightly more realistic dictionary setting:we take our dictionary to include information for allwords occurring in section 0-18 of the WSJ corpus(43208 words).
We then train on the entire unanno-tated corpus, and test on sections 22-24 ?
the stan-dard train/test split for supervised English POS tag-ging.
We achieve accuracy of 92.85% for the 19-tags set, and 91.3% for the complete 46-tags tagset.752Initial Conditions Full dict ?
2 dict ?
3 dict(49206 words) (2141 words) (1249 words)CF-Tag EM-HMM CF-Tag EM-HMM CF-Tag EM-HMMUniform(oc) 81.7 88.7 68.4 81.9 62.5 79.6Uniform(suf) NA NA 76.8 83.4 76.9 81.617tags Morph-Cont 82.2 88.6 73.3 83.9 69.1 81.7Linear-Cont 90.1 92.9 81.1 87.8 78.3 85.8Combined-Cont 89.9 93.3 83.1 88.5 81.1 86.4Uniform(oc) 79.9 91.0 66.6 83.4 60.7 84.7Uniform(suf) NA NA 75.1 86.5 73.1 86.719tags Morph-Cont 80.5 89.2 71.5 86.5 67.5 87.1Linear-Cont 88.4 93.7 78.9 89.0 76.3 86.9Combined-Cont 88.0 93.8 81.1 89.4 79.2 87.4Uniform(oc) 76.7 88.3 61.2 * 55.7 *Uniform(suf) NA NA 64.2 81.9 60.3 79.846tags Morph-Cont 74.8 88.8 65.6 83.0 61.9 80.3Linear-Cont 85.5 91.2 74.5 84.0 70.1 82.2Combined-Cont 85.9 91.4 75.4 85.5 72.4 83.3Table 2: Accuracy (%) of English POS Tagging over various initial conditionsDict InitEM-HMM (24k) LDA LDA+AC PLSA+AC CE+spl BHMMFull 93.8 (91.1) 93.4 93.4 89.7 88.7 87.3?
2 89.4 (87.9) 87.4 91.2 87.8 79.5 79.6?
3 87.4 (85.9) 85 89.7 85.9 78.4 71Table 3: Comparison of English Unsupervised POS Tagging Methods6 ConclusionWe have demonstrated that unsupervised POS tag-ging can reach good results using the robust EM-HMM learner when provided with good initial con-ditions, even with incomplete dictionaries.
We pre-sented a general family of algorithms to compute ef-fective initial conditions: estimation of p(t|w) rely-ing on an iterative process shifting probabilities be-tween words and their contexts.
The parameters ofthis process (definition of the contexts and initial es-timations of p(t|w) can safely encapsulate rich lin-guistic intuitions.While recent work, such as GG, aim to use theBayesian framework and incorporate ?linguisticallymotivated priors?, in practice such priors currentlyonly account for the fact that language related dis-tributions are sparse - a very general kind of knowl-edge.
In contrast, our method allow the incorpora-tion of much more fine-grained intuitions.We tested the method on the challenging taskof full morphological disambiguation in Hebrew(which was our original motivation) and on the stan-dard WSJ unsupervised POS tagging task.In Hebrew, our model includes an improved ver-sion of the similar words algorithm of (Levinger etal., 1995), a model of lexical context, and a smallset of tag ngrams.
The combination of these knowl-edge sources in the initial conditions brings an errorreduction of more than 25% over a strong uniformdistribution baseline.
In English, our model is com-petitive with recent state-of-the-art results, while us-ing simple and efficient learning methods.The comparison with other algorithms indicatesdirections of potential improvement: (1) our initial-conditions method might benefit the other, more so-phisticated learning algorithms as well.
(2) Ourmodels were designed under the assumption of arelatively complete dictionary.
As such, they arenot very good at assigning ambiguity-classes toOOV tokens when starting with a very small dic-tionary.
While we demonstrate competitive resultsusing a simple suffix-based ambiguity-class guesserwhich ignores capitalization and hyphenation infor-mation, we believe there is much room for improve-ment in this respect.
In particular, (Haghighi andKlein, 2006) presents very strong results using adistributional-similarity module and achieve impres-sive tagging accuracy while starting with a mere116 prototypical words.
Experimenting with com-bining similar models (as well as TJ?s ambiguityclass model) with our p(t|w) distribution estimationmethod is an interesting research direction.753ReferencesMeni Adler.
2007.
Hebrew Morphological Disambigua-tion: An Unsupervised Stochastic Word-based Ap-proach.
Ph.D. thesis, Ben-Gurion University of theNegev, Beer-Sheva, Israel.Michele Banko and Robert C. Moore.
2004.
Part-of-speech tagging in context.
In Proceedings of Coling2004, pages 556?561, Geneva, Switzerland, Aug 23?Aug 27.
COLING.Leonard E. Baum.
1972.
An inequality and associ-ated maximization technique in statistical estimationfor probabilistic functions of a Markov process.
In-equalities, 3:1?8.Eric Brill.
1995a.
Transformation-based error-drivenlearning and natural languge processing: A case studyin part-of-speech tagging.
Computational Linguistics,21:543?565.Eric Brill.
1995b.
Unsupervised learning of disam-biguation rules for part of speech tagging.
In DavidYarovsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora, pages1?13, Somerset, New Jersey.
Association for Compu-tational Linguistics.Stanley F. Chen.
1996.
Building Probabilistic Models forNatural Language.
Ph.D. thesis, Harvard University,Cambridge, MA.Silviu Cucerzan and David Yarowsky.
2000.
Languageindependent, minimally supervised induction of lex-ical probabilities.
In ACL ?00: Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 270?277, Morristown, NJ,USA.
Association for Computational Linguistics.Evangelos Dermatas and George Kokkinakis.
1995.
Au-tomatic stochastic tagging of natural language texts.Computational Linguistics, 21(2):137?163.David Elworthy.
1994.
Does Baum-Welch re-estimationhelp taggers?
In Proceeding of ANLP-94.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully bayesian approach to unsupervised part-of-speech tagging.
In Proceeding of ACL 2007, Prague,Czech Republic.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language Technol-ogy Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 320?327, Morristown, NJ, USA.
Association for Computa-tional Linguistics.J.
Kupiec.
1992.
Robust part-of-speech tagging usinghidden Markov model.
Computer Speech and Lan-guage, 6:225?242.Moshe Levinger, Uzi Ornan, and Alon Itai.
1995.
Learn-ing morpholexical probabilities from an untagged cor-pus with an application to Hebrew.
ComputationalLinguistics, 21:383?404.Christopher D. Manning and Hinrich Schutze.
1999.Foundation of Statistical Language Processing.
MITPress.Bernard Merialdo.
1994.
Tagging English textwith probabilistic model.
Computational Linguistics,20:155?171.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the seventh conferenceon European chapter of the Association for Computa-tional Linguistics, pages 141?148, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Danny Shacham and Shuly Wintner.
2007.
Morpho-logical disambiguation of hebrew: A case study inclassifier combination.
In Proceeding of EMNLP-07,Prague, Czech.Khalil Sima?an, Alon Itai, Alon Altman Yoad Winter,and Noa Nativ.
2001.
Building a tree-bank of mod-ern Hebrew text.
Journal Traitement Automatique desLangues (t.a.l.).
Special Issue on NLP and CorpusLinguistics.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages354?362, Ann Arbor, Michigan, June.Scott M. Thede and Mary P. Harper.
1999.
A second-order hidden Markov model for part-of-speech tag-ging.
In Proceeding of ACL-99.Kristina Toutanova and Mark Johnson.
2008.
A bayesianlda-based model for semi-supervised part-of-speechtagging.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural InformationProcessing Systems 20.
MIT Press, Cambridge, MA.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In HLT-NAACL.R.
Weischedel, R. Schwartz, J. Palmucci, M. Meteer, andL.
Ramshaw.
1993.
Coping with ambiguity and un-known words through probabilistic models.
Computa-tional Linguistics, 19:359?382.754
