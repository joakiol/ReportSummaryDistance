Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 58?62,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsMaltOptimizer: An Optimization Tool for MaltParserMiguel BallesterosComplutense University of MadridSpainmiballes@fdi.ucm.esJoakim NivreUppsala UniversitySwedenjoakim.nivre@lingfil.uu.seAbstractData-driven systems for natural languageprocessing have the advantage that they caneasily be ported to any language or domainfor which appropriate training data can befound.
However, many data-driven systemsrequire careful tuning in order to achieveoptimal performance, which may requirespecialized knowledge of the system.
Wepresent MaltOptimizer, a tool developed tofacilitate optimization of parsers developedusing MaltParser, a data-driven dependencyparser generator.
MaltOptimizer performsan analysis of the training data and guidesthe user through a three-phase optimizationprocess, but it can also be used to performcompletely automatic optimization.
Exper-iments show that MaltOptimizer can im-prove parsing accuracy by up to 9 percentabsolute (labeled attachment score) com-pared to default settings.
During the demosession, we will run MaltOptimizer on dif-ferent data sets (user-supplied if possible)and show how the user can interact with thesystem and track the improvement in pars-ing accuracy.1 IntroductionIn building NLP applications for new languagesand domains, we often want to reuse componentsfor tasks like part-of-speech tagging, syntacticparsing, word sense disambiguation and semanticrole labeling.
From this perspective, componentsthat rely on machine learning have an advantage,since they can be quickly adapted to new settingsprovided that we can find suitable training data.However, such components may require carefulfeature selection and parameter tuning in order togive optimal performance, a task that can be dif-ficult for application developers without special-ized knowledge of each component.A typical example is MaltParser (Nivre et al2006), a widely used transition-based dependencyparser with state-of-the-art performance for manylanguages, as demonstrated in the CoNLL sharedtasks on multilingual dependency parsing (Buch-holz and Marsi, 2006; Nivre et al 2007).
Malt-Parser is an open-source system that offers a widerange of parameters for optimization.
It imple-ments nine different transition-based parsing al-gorithms, each with its own specific parameters,and it has an expressive specification languagethat allows the user to define arbitrarily complexfeature models.
Finally, any combination of pars-ing algorithm and feature model can be combinedwith a number of different machine learning al-gorithms available in LIBSVM (Chang and Lin,2001) and LIBLINEAR (Fan et al 2008).
Justrunning the system with default settings whentraining a new parser is therefore very likely toresult in suboptimal performance.
However, se-lecting the best combination of parameters is acomplicated task that requires knowledge of thesystem as well as knowledge of the characteris-tics of the training data.This is why we present MaltOptimizer, a toolfor optimizing MaltParser for a new languageor domain, based on an analysis of the train-ing data.
The optimization is performed in threephases: data analysis, parsing algorithm selec-tion, and feature selection.
The tool can be runin ?batch mode?
to perform completely automaticoptimization, but it is also possible for the user tomanually tune parameters after each of the threephases.
In this way, we hope to cater for users58without specific knowledge of MaltParser, whocan use the tool for black box optimization, aswell as expert users, who can use it interactivelyto speed up optimization.
Experiments on a num-ber of data sets show that using MaltOptimizer forcompletely automatic optimization gives consis-tent and often substantial improvements over thedefault settings for MaltParser.The importance of feature selection and param-eter optimization has been demonstrated for manyNLP tasks (Kool et al 2000; Daelemans et al2003), and there are general optimization tools formachine learning, such as Paramsearch (Van denBosch, 2004).
In addition, Nilsson and Nugues(2010) has explored automatic feature selectionspecifically for MaltParser, but MaltOptimizer isthe first system that implements a complete cus-tomized optimization process for this system.In the rest of the paper, we describe the opti-mization process implemented in MaltOptimizer(Section 2), report experiments (Section 3), out-line the demonstration (Section 4), and conclude(Section 5).
A more detailed description of Malt-Optimizer with additional experimental resultscan be found in Ballesteros and Nivre (2012).2 The MaltOptimizer SystemMaltOptimizer is written in Java and implementsan optimization procedure for MaltParser basedon the heuristics described in Nivre and Hall(2010).
The system takes as input a trainingset, consisting of sentences annotated with depen-dency trees in CoNLL data format,1 and outputsan optimized MaltParser configuration togetherwith an estimate of the final parsing accuracy.The evaluation metric that is used for optimiza-tion by default is the labeled attachment score(LAS) excluding punctuation, that is, the percent-age of non-punctuation tokens that are assignedthe correct head and the correct label (Buchholzand Marsi, 2006), but other options are available.For efficiency reasons, MaltOptimizer only ex-plores linear multiclass SVMs in LIBLINEAR.2.1 Phase 1: Data AnalysisAfter validating that the data is in valid CoNLLformat, using the official validation script fromthe CoNLL-X shared task,2 the system checks the1http://ilk.uvt.nl/conll/#dataformat2http://ilk.uvt.nl/conll/software.html#validateminimum Java heap space needed given the sizeof the data set.
If there is not enough memoryavailable on the current machine, the system in-forms the user and automatically reduces the sizeof the data set to a feasible subset.
After these ini-tial checks, MaltOptimizer checks the followingcharacteristics of the data set:1.
Number of words/sentences.2.
Existence of ?covered roots?
(arcs spanningtokens with HEAD = 0).3.
Frequency of labels used for tokens withHEAD = 0.4.
Percentage of non-projective arcs/trees.5.
Existence of non-empty feature values in theLEMMA and FEATS columns.6.
Identity (or not) of feature values in theCPOSTAG and POSTAG columns.Items 1?3 are used to set basic parameters in therest of phase 1 (see below); 4 is used in the choiceof parsing algorithm (phase 2); 5 and 6 are rele-vant for feature selection experiments (phase 3).If there are covered roots, the system checkswhether accuracy is improved by reattachingsuch roots in order to eliminate spurious non-projectivity.
If there are multiple labels for to-kens with HEAD=0, the system tests which labelis best to use as default for fragmented parses.Given the size of the data set, the system sug-gests different validation strategies during phase1.
If the data set is small, it recommends us-ing 5-fold cross-validation during subsequent op-timization phases.
If the data set is larger, it rec-ommends using a single development set instead.But the user can override either recommendationand select either validation method manually.When these checks are completed, MaltOpti-mizer creates a baseline option file to be used asthe starting point for further optimization.
Theuser is given the opportunity to edit this optionfile and may also choose to stop the process andcontinue with manual optimization.2.2 Phase 2: Parsing Algorithm SelectionMaltParser implements three groups of transition-based parsing algorithms:3 (i) Nivre?s algorithms(Nivre, 2003; Nivre, 2008), (ii) Covington?s algo-rithms (Covington, 2001; Nivre, 2008), and (iii)3Recent versions of MaltParser contains additional algo-rithms that are currently not handled by MaltOptimizer.59Figure 1: Decision tree for best projective algorithm.Figure 2: Decision tree for best non-projective algo-rithm (+PP for pseudo-projective parsing).Stack algorithms (Nivre, 2009; Nivre et al 2009)Both the Covington group and the Stack groupcontain algorithms that can handle non-projectivedependency trees, and any projective algorithmcan be combined with pseudo-projective parsingto recover non-projective dependencies in post-processing (Nivre and Nilsson, 2005).In phase 2, MaltOptimizer explores the parsingalgorithms implemented in MaltParser, based onthe data characteristics inferred in the first phase.In particular, if there are no non-projective depen-dencies in the training set, then only projectivealgorithms are explored, including the arc-eagerand arc-standard versions of Nivre?s algorithm,the projective version of Covington?s projectiveparsing algorithm and the projective Stack algo-rithm.
The system follows a decision tree consid-ering the characteristics of each algorithm, whichis shown in Figure 1.On the other hand, if the training set con-tains a substantial amount of non-projective de-pendencies, MaltOptimizer instead tests the non-projective versions of Covington?s algorithm andthe Stack algorithm (including a lazy and an eagervariant), and projective algorithms in combinationwith pseudo-projective parsing.
The system thenfollows the decision tree shown in Figure 2.If the number of trees containing non-projective arcs is small but not zero, the sys-tem tests both projective algorithms and non-projective algorithms, following the decision treesin Figure 1 and Figure 2 and picking the algorithmthat gives the best results after traversing both.Once the system has finished testing each of thealgorithms with default settings, MaltOptimizertunes some specific parameters of the best per-forming algorithm and creates a new option filefor the best configuration so far.
The user is againgiven the opportunity to edit the option file (orstop the process) before optimization continues.2.3 Phase 3: Feature SelectionIn the third phase, MaltOptimizer tunes the fea-ture model given all the parameters chosen so far(especially the parsing algorithm).
It starts withbackward selection experiments to ensure that allfeatures in the default model for the given pars-ing algorithm are actually useful.
In this phase,features are omitted as long as their removal doesnot decrease parsing accuracy.
The system thenproceeds with forward selection experiments, try-ing potentially useful features one by one.
In thisphase, a threshold of 0.05% is used to determinewhether an improvement in parsing accuracy issufficient for the feature to be added to the model.Since an exhaustive search for the best possiblefeature model is impossible, the system relies ona greedy optimization strategy using heuristics de-rived from proven experience (Nivre and Hall,2010).
The major steps of the forward selectionexperiments are the following:41.
Tune the window of POSTAG n-grams overthe parser state.2.
Tune the window of FORM features over theparser state.3.
Tune DEPREL and POSTAG features overthe partially built dependency tree.4.
Add POSTAG and FORM features over theinput string.5.
Add CPOSTAG, FEATS, and LEMMA fea-tures if available.6.
Add conjunctions of POSTAG and FORMfeatures.These six steps are slightly different dependingon which algorithm has been selected as the bestin phase 2, because the algorithms have differentparsing orders and use different data structures,4For an explanation of the different feature columns suchas POSTAG, FORM, etc., see Buchholz and Marsi (2006) orsee http://ilk.uvt.nl/conll/#dataformat60Language Default Phase 1 Phase 2 Phase 3 DiffArabic 63.02 63.03 63.84 65.56 2.54Bulgarian 83.19 83.19 84.00 86.03 2.84Chinese 84.14 84.14 84.95 84.95 0.81Czech 69.94 70.14 72.44 78.04 8.10Danish 81.01 81.01 81.34 83.86 2.85Dutch 74.77 74.77 78.02 82.63 7.86German 82.36 82.36 83.56 85.91 3.55Japanese 89.70 89.70 90.92 90.92 1.22Portuguese 84.11 84.31 84.75 86.52 2.41Slovene 66.08 66.52 68.40 71.71 5.63Spanish 76.45 76.45 76.64 79.38 2.93Swedish 83.34 83.34 83.50 84.09 0.75Turkish 57.79 57.79 58.29 66.92 9.13Table 1: Labeled attachment score per phase and withcomparison to default settings for the 13 training setsfrom the CoNLL-X shared task (Buchholz and Marsi,2006).but the steps are roughly equivalent at a certainlevel of abstraction.
After the feature selectionexperiments are completed, MaltOptimizer tunesthe cost parameter of the linear SVM using a sim-ple stepwise search.
Finally, it creates a completeconfiguration file that can be used to train Malt-Parser on the entire data set.
The user may nowcontinue to do further optimization manually.3 ExperimentsIn order to assess the usefulness and validity ofthe optimization procedure, we have run all threephases of the optimization on all the 13 data setsfrom the CoNLL-X shared task on multilingualdependency parsing (Buchholz and Marsi, 2006).Table 1 shows the labeled attachment scores withdefault settings and after each of the three opti-mization phases, as well as the difference betweenthe final configuration and the default.5The first thing to note is that the optimizationimproves parsing accuracy for all languages with-out exception, although the amount of improve-ment varies considerably from about 1 percentagepoint for Chinese, Japanese and Swedish to 8?9points for Dutch, Czech and Turkish.
For mostlanguages, the greatest improvement comes fromfeature selection in phase 3, but we also see sig-5Note that these results are obtained using 80% of thetraining set for training and 20% as a development test set,which means that they are not comparable to the test resultsfrom the original shared task, which were obtained using theentire training set for training and a separate held-out test setfor evaluation.nificant improvement from phase 2 for languageswith a substantial amount of non-projective de-pendencies, such as Czech, Dutch and Slovene,where the selection of parsing algorithm can bevery important.
The time needed to run the op-timization varies from about half an hour for thesmaller data sets to about one day for very largedata sets like the one for Czech.4 System DemonstrationIn the demonstration, we will run MaltOptimizeron different data sets and show how the user caninteract with the system while keeping track ofimprovements in parsing accuracy.
We will alsoexplain how to interpret the output of the system,including the final feature specification model, forusers that are not familiar with MaltParser.
By re-stricting the size of the input data set, we can com-plete the whole optimization procedure in 10?15minutes, so we expect to be able to complete anumber of cycles with different members of theaudience.
We will also let the audience contributetheir own data sets for optimization, provided thatthey are in CoNLL format.65 ConclusionMaltOptimizer is an optimization tool for Malt-Parser, which is primarily aimed at applicationdevelopers who wish to adapt the system to anew language or domain and who do not haveexpert knowledge about transition-based depen-dency parsing.
Another potential user group con-sists of researchers who want to perform compar-ative parser evaluation, where MaltParser is oftenused as a baseline system and where the use ofsuboptimal parameter settings may undermine thevalidity of the evaluation.
Finally, we believe thesystem can be useful also for expert users of Malt-Parser as a way of speeding up optimization.AcknowledgmentsThe first author is funded by the Spanish Ministryof Education and Science (TIN2009-14659-C03-01 Project), Universidad Complutense de Madridand Banco Santander Central Hispano (GR58/08Research Group Grant).
He is under the supportof the NIL Research Group (http://nil.fdi.ucm.es)from the same university.6The system is available for download under an open-source license at http://nil.fdi.ucm.es/maltoptimizer61ReferencesMiguel Ballesteros and Joakim Nivre.
2012.
MaltOp-timizer: A System for MaltParser Optimization.
InProceedings of the Eighth International Conferenceon Language Resources and Evaluation (LREC).Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the 10th Conference on Computa-tional Natural Language Learning (CoNLL), pages149?164.Chih-Chung Chang and Chih-Jen Lin, 2001.LIBSVM: A Library for Support Vec-tor Machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Michael A. Covington.
2001.
A fundamental algo-rithm for dependency parsing.
In Proceedings ofthe 39th Annual ACM Southeast Conference, pages95?102.Walter Daelemans, Ve?ronique Hoste, Fien De Meul-der, and Bart Naudts.
2003.
Combined optimiza-tion of feature selection and algorithm parametersin machine learning of language.
In Nada Lavrac,Dragan Gamberger, Hendrik Blockeel, and LjupcoTodorovski, editors, Machine Learning: ECML2003, volume 2837 of Lecture Notes in ComputerScience.
Springer.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
LIBLINEAR: A library for largelinear classification.
Journal of Machine LearningResearch, 9:1871?1874.Anne Kool, Jakub Zavrel, and Walter Daelemans.2000.
Simultaneous feature selection and param-eter optimization for memory-based natural lan-guage processing.
In A. Feelders, editor, BENE-LEARN 2000.
Proceedings of the Tenth Belgian-Dutch Conference on Machine Learning, pages 93?100.
Tilburg University, Tilburg.Peter Nilsson and Pierre Nugues.
2010.
Automaticdiscovery of feature sets for dependency parsing.
InCOLING, pages 824?832.Joakim Nivre and Johan Hall.
2010.
A quick guideto MaltParser optimization.
Technical report, malt-parser.org.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL), pages 99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC), pages 2216?2219.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task of EMNLP-CoNLL 2007, pages 915?932.Joakim Nivre, Marco Kuhlmann, and Johan Hall.2009.
An improved oracle for dependency parsingwith online reordering.
In Proceedings of the 11thInternational Conference on Parsing Technologies(IWPT?09), pages 73?76.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technolo-gies (IWPT), pages 149?160.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34:513?553.Joakim Nivre.
2009.
Non-projective dependencyparsing in expected linear time.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP(ACL-IJCNLP), pages 351?359.Antal Van den Bosch.
2004.
Wrapped progressivesampling search for optimizing learning algorithmparameters.
In Proceedings of the 16th Belgian-Dutch Conference on Artificial Intelligence.62
