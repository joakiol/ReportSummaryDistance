Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317?325,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving nonparameteric Bayesian inference: experiments onunsupervised word segmentation with adaptor grammarsMark JohnsonBrown UniversityProvidence, RIMark Johnson@Brown.eduSharon GoldwaterUniversity of EdinburghEdinburgh EH8 9ABsgwater@inf.ed.ac.ukAbstractOne of the reasons nonparametric Bayesianinference is attracting attention in computa-tional linguistics is because it provides a prin-cipled way of learning the units of generaliza-tion together with their probabilities.
Adaptorgrammars are a framework for defining a va-riety of hierarchical nonparametric Bayesianmodels.
This paper investigates some ofthe choices that arise in formulating adap-tor grammars and associated inference proce-dures, and shows that they can have a dra-matic impact on performance in an unsuper-vised word segmentation task.
With appro-priate adaptor grammars and inference proce-dures we achieve an 87% word token f-scoreon the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction ofover 35% over the best previously reported re-sults for this corpus.1 IntroductionMost machine learning algorithms used in computa-tional linguistics are parametric, i.e., they learn a nu-merical weight (e.g., a probability) associated witheach feature, where the set of features is fixed be-fore learning begins.
Such procedures can be usedto learn features or structural units by embeddingthem in a ?propose-and-prune?
algorithm: a featureproposal component proposes potentially useful fea-tures (e.g., combinations of the currently most usefulfeatures), which are then fed to a parametric learnerthat estimates their weights.
After estimating fea-ture weights and pruning ?useless?
low-weight fea-tures, the cycle repeats.
While such algorithms canachieve impressive results (Stolcke and Omohundro,1994), their effectiveness depends on how well thefeature proposal step relates to the overall learningobjective, and it can take considerable insight andexperimentation to devise good feature proposals.One of the main reasons for the recent interest innonparametric Bayesian inference is that it offers asystematic framework for structural inference, i.e.,inferring the features relevant to a particular prob-lem as well as their weights.
(Here ?nonparamet-ric?
means that the models do not have a fixed set ofparameters; our nonparametric models do have pa-rameters, but the particular parameters in a modelare learned along with their values).
Dirichlet Pro-cesses and their associated predictive distributions,Chinese Restaurant Processes, are one kind of non-parametric Bayesian model that has received consid-erable attention recently, in part because they can becomposed in hierarchical fashion to form Hierarchi-cal Dirichlet Processes (HDP) (Teh et al, 2006).Lexical acquisition is an ideal test-bed for explor-ing methods for inferring structure, where the fea-tures learned are the words of the language.
(Eventhe most hard-core nativists agree that the words of alanguage must be learned).
We use the unsupervisedword segmentation problem as a test case for eval-uating structural inference in this paper.
Nonpara-metric Bayesian methods produce state-of-the-artperformance on this task (Goldwater et al, 2006a;Goldwater et al, 2007; Johnson, 2008).In a computational linguistics setting it is natu-ral to try to align the HDP hierarchy with the hi-erarchy defined by a grammar.
Adaptor grammars,which are one way of doing this, make it easy to ex-plore a wide variety of HDP grammar-based mod-els.
Given an appropriate adaptor grammar, the fea-317tures learned by adaptor grammars can correspondto linguistic units such as words, syllables and col-locations.
Different adaptor grammars encode dif-ferent assumptions about the structure of these unitsand how they relate to each other.
A generic adaptorgrammar inference program infers these units fromtraining data, making it easy to investigate how theseassumptions affect learning (Johnson, 2008).1However, there are a number of choices in the de-sign of adaptor grammars and the associated infer-ence procedure.
While this paper studies the im-pact of these on the word segmentation task, thesechoices arise in other nonparametric Bayesian infer-ence problems as well, so our results should be use-ful more generally.
The rest of this paper is orga-nized as follows.
The next section reviews adaptorgrammars and presents three different adaptor gram-mars for word segmentation that serve as runningexamples in this paper.
Adaptor grammars containa large number of adjustable parameters, and Sec-tion 3 discusses how these can be estimated usingBayesian techniques.
Section 4 examines severalimplementation options within the adaptor grammarinference algorithm and shows that they can makea significant impact on performance.
Cumulativelythese changes make a significant difference in wordsegmentation accuracy: our final adaptor grammarperforms unsupervised word segmentation with an87% token f-score on the standard Brent versionof the Bernstein-Ratner corpus (Bernstein-Ratner,1987; Brent and Cartwright, 1996), which is an er-ror reduction of over 35% compared to the best pre-viously reported results on this corpus.2 Adaptor grammarsThis section informally introduces adaptor gram-mars using unsupervised word segmentation as amotivating application; see Johnson et al (2007b)for a formal definition of adaptor grammars.Consider the problem of learning language fromcontinuous speech: segmenting each utterance intowords is a nontrivial problem that language learn-ers must solve.
Elman (1990) introduced an ideal-ized version of this task, and Brent and Cartwright(1996) presented a version of it where the dataconsists of unsegmented phonemic representationsof the sentences in the Bernstein-Ratner corpus of1The adaptor grammar inference program is available fordownload at http://www.cog.brown.edu/?mj/Software.htm.child-directed speech (Bernstein-Ratner, 1987).
Be-cause these phonemic representations are obtainedby looking up orthographic forms in a pronounc-ing dictionary and appending the results, identifyingthe word tokens is equivalent to finding the locationsof the word boundaries.
For example, the phonemestring corresponding to ?you want to see the book?
(with its correct segmentation indicated) is as fol-lows:y ?u Nw ?a ?n ?t Nt ?u Ns ?i ND ?6 Nb ?U ?kWe can represent any possible segmentation of anypossible sentence as a tree generated by the follow-ing unigram grammar.Sentence ?
Word+Word ?
Phoneme+The nonterminal Phoneme expands to each pos-sible phoneme; the underlining, which identifies?adapted nonterminals?, will be explained below.
Inthis paper ?+?
abbreviates right-recursion through adummy nonterminal, i.e., the unigram grammar ac-tually is:Sentence ?
WordSentence ?
Word SentenceWord ?
PhonemesPhonemes ?
PhonemePhonemes ?
Phoneme PhonemesA PCFG with these productions can represent allpossible segmentations of any Sentence into a se-quence of Words.
But because it assumes that theprobability of a word is determined purely by mul-tiplying together the probability of its individualphonemes, it has no way to encode the fact that cer-tain strings of phonemes (the words of the language)have much higher probabilities than other stringscontaining the same phonemes.
In order to do this,a PCFG would need productions like the followingone, which encodes the fact that ?want?
is a Word.Word ?
w a n tAdaptor grammars can be viewed as a way of for-malizing this idea.
Adaptor grammars learn theprobabilities of entire subtrees, much as in tree sub-stitution grammar (Joshi, 2003) and DOP (Bod,3181998).
(For computational efficiency reasons adap-tor grammars require these subtrees to expand to ter-minals).
The set of possible adapted tree fragmentsis the set of all subtrees generated by the CFG whoseroot label is a member of the set of adapted non-terminals A (adapted nonterminals are indicated byunderlining in this paper).
For example, in the uni-gram adaptor grammar A = {Word}, which meansthat the adaptor grammar inference procedure learnsthe probability of each possible Word subtree.
Thusadaptor grammars are simple models of structurelearning in which adapted subtrees are the units ofgeneralization.One might try to reduce adaptor grammar infer-ence to PCFG parameter estimation by introducinga context-free rule for each possible adapted subtree,but such an attempt would fail because the numberof such adapted subtrees, and hence the number ofcorresponding rules, is unbounded.
However non-parametric Bayesian inference techniques permit usto sample from this infinite set of adapted subtrees,and only require us to instantiate the finite numberof them needed to analyse the finite training data.An adaptor grammar is a 7-tuple(N,W,R, S,?, A,C) where (N,W,R, S,?)
isa PCFG with nonterminals N , terminals W , rulesR, start symbol S ?
N and rule probabilities ?,where ?r is the probability of rule r ?
R, A ?
N isthe set of adapted nonterminals and C is a vectorof adaptors indexed by elements of A, so CX is theadaptor for adapted nonterminal X ?
A.Informally, an adaptor CX nondeterministicallymaps a stream of trees from a base distribution HXwhose support is TX (the set of subtrees whose rootnode is X ?
N generated by the grammar?s rules)into another stream of trees whose support is alsoTX .
In adaptor grammars the base distributions HXare determined by the PCFG rules expanding X andthe other adapted distributions, as explained in John-son et al (2007b).
When called upon to generate an-other sample tree, the adaptor either generates andreturns a fresh tree from HX or regenerates a treeit has previously emitted, so in general the adapteddistribution differs from the base distribution.This paper uses adaptors based on ChineseRestaurant Processes (CRPs) or Pitman-Yor Pro-cesses (PYPs) (Pitman, 1995; Pitman and Yor, 1997;Ishwaran and James, 2003).
CRPs and PYPs non-deterministically generate infinite sequences of nat-ural numbers z1, z2, .
.
., where z1 = 1 and eachzn+1 ?
m+ 1 where m = max(z1, .
.
.
, zn).
In the?Chinese Restaurant?
metaphor samples producedby the adaptor are viewed as ?customers?
and znis the index of the ?table?
that the nth customer isseated at.
In adaptor grammars each table in theadaptor CX is labeled with a tree sampled from thebase distribution HX that is shared by all customersat that table; thus the nth sample tree from the adap-tor CX is the znth sample from HX .CRPs and PYPs differ in exactly how thesequence {zk} is generated.
Suppose z =(z1, .
.
.
, zn) have already been generated and m =max(z).
Then a CRP generates the next table indexzn+1 according to the following distribution:P(Zn+1 = k | z) ?
{nk(z) if k ?
m?
if k = m+ 1where nk(z) is the number of times table k appearsin z and ?
> 0 is an adjustable parameter that deter-mines how often a new table is chosen.
This meansthat if CX is a CRP adaptor then the next tree tn+1it generates is the same as a previously generatedtree t?
with probability proportional to the numberof times CX has generated t?
before, and is a ?fresh?tree t sampled from HX with probability propor-tional to ?XHX(t).
This leads to a powerful ?rich-get-richer?
effect in which popular trees are gener-ated with increasingly high probabilities.Pitman-Yor Processes can control the strength ofthis effect somewhat by moving mass from existingtables to the base distribution.
The PYP predictivedistribution is:P(Zn+1 = k | z) ?{nk(z)?
a if k ?
mma+ b if k = m+ 1where a ?
[0, 1] and b > 0 are adjustable parame-ters.
It?s easy to see that the CRP is a special case ofthe PRP where a = 0 and b = ?.Each adaptor in an adaptor grammar can beviewed as estimating the probability of each adaptedsubtree t; this probability can differ substantiallyfrom t?s probability HX(t) under the base distribu-tion.
Because Words are adapted in the unigramadaptor grammar it effectively estimates the proba-bility of each Word tree separately; the sampling es-timators described in section 4 only instantiate thoseWords actually used in the analysis of Sentences inthe corpus.
While the Word adaptor will generally319prefer to reuse Words that have been used elsewherein the corpus, it is always possible to generate a freshWord using the CFG rules expanding Word into astring of Phonemes.We assume for now that all CFG rules RX ex-panding the nonterminal X ?
N have the sameprobability (although we will explore estimating ?below), so the base distribution HWord is a ?mon-keys banging on typewriters?
model.
That means theunigram adaptor grammar implements the Goldwa-ter et al (2006a) unigram word segmentation model,and in fact it produces segmentations of similar ac-curacies, and exhibits the same characteristic under-segmentation errors.
As Goldwater et al point out,because Words are the only units of generalizationavailable to a unigram model it tends to misanal-yse collocations as words, resulting in a marked ten-dancy to undersegment.Goldwater et al demonstrate that modelling bi-gram dependencies mitigates this undersegmenta-tion.
While adaptor grammars cannot express theGoldwater et al bigram model, they can get muchthe same effect by directly modelling collocations(Johnson, 2008).
A collocation adaptor grammargenerates a Sentence as a sequence of Collocations,each of which expands to a sequence of Words.Sentence ?
Colloc+Colloc ?
Word+Word ?
Phoneme+Because Colloc is adapted, the collocation adap-tor grammar learns Collocations as well as Words.
(Presumably these approximate syntactic, semanticand pragmatic interword dependencies).
Johnsonreported that the collocation adaptor grammar seg-ments as well as the Goldwater et al bigram model,which we confirm here.Recently other researchers have emphasised theutility of phonotactic constraints (i.e., modelingthe allowable phoneme sequences at word onsetsand endings) for word segmentation (Blanchard andHeinz, 2008; Fleck, 2008).
Johnson (2008) pointsout that adaptor grammars that model words as se-quences of syllables can learn and exploit these con-straints, significantly improving segmentation accu-racy.
Here we present an adaptor grammar that mod-els collocations together with these phonotactic con-straints.
This grammar is quite complex, permittingus to study the effects of the various model and im-plementation choices described below on a complexhierarchical nonparametric Bayesian model.The collocation-syllable adaptor grammar gen-erates a Sentence in terms of three levels ofCollocations (enabling it to capture a wider rangeof interword dependencies), and generates Words assequences of 1 to 4 Syllables.
Syllables are subcat-egorized as to whether they are initial (I), final (F) orboth (IF).Sentence ?
Colloc3+Colloc3 ?
Colloc2+Colloc2 ?
Colloc1+Colloc1 ?
Word+Word ?
SyllableIFWord ?
SyllableI (Syllable) (Syllable) SyllableFSyllable ?
Onset RhymeOnset ?
Consonant+Rhyme ?
Nucleus CodaNucleus ?
Vowel+Coda ?
Consonant+SyllableIF ?
OnsetI RhymeFOnsetI ?
Consonant+RhymeF ?
Nucleus CodaFCodaF ?
Consonant+SyllableI ?
OnsetI RhymeSyllableF ?
Onset RhymeFHere Consonant and Vowel expand to all possibleconsonants and vowels respectively, and the paren-theses in the expansion of Word indicate optional-ity.
Because Onsets and Codas are adapted, thecollocation-syllable adaptor grammar learns the pos-sible consonant sequences that begin and end syl-lables.
Moreover, because Onsets and Codas aresubcategorized based on whether they are word-peripheral, the adaptor grammar learns which con-sonant clusters typically appear at word boundaries,even though the input contains no explicit wordboundary information (apart from what it can gleanfrom the sentence boundaries).3 Bayesian estimation of adaptorgrammar parametersAdaptor grammars as defined in section 2 have alarge number of free parameters that have to bechosen by the grammar designer; a rule probabil-ity ?r for each PCFG rule r ?
R and either one ortwo hyperparameters for each adapted nonterminalX ?
A, depending on whether Chinese Restaurant320or Pitman-Yor Processes are used as adaptors.
It?sdifficult to have intuitions about the appropriate set-tings for the latter parameters, and finding the opti-mal values for these parameters by some kind of ex-haustive search is usually computationally impracti-cal.
Previous work has adopted an expedient such asparameter tying.
For example, Johnson (2008) set?
by requiring all productions expanding the samenonterminal to have the same probability, and usedChinese Restaurant Process adaptors with tied pa-rameters ?X , which was set using a grid search.We now describe two methods of dealing with thelarge number of parameters in these models that areboth more principled and more practical than the ap-proaches described above.
First, we can integrateout ?, and second, we can infer values for the adap-tor hyperparameters using sampling.
These meth-ods (the latter in particular) make it practical to usePitman-Yor Process adaptors in complex grammarssuch as the collocation-syllable adaptor grammar,where it is impractical to try to find optimal parame-ter values by grid search.
As we will show, they alsoimprove segmentation accuracy, sometimes dramat-ically.3.1 Integrating out ?Johnson et al (2007a) describe Gibbs samplers forBayesian inference of PCFG rule probabilities ?,and these techniques can be used directly with adap-tor grammars as well.
Just as in that paper, weplace Dirichlet priors on ?
: here ?X is the subvectorof ?
corresponding to rules expanding nonterminalX ?
N , and ?X is a corresponding vector of posi-tive real numbers specifying the hyperparameters ofthe corresponding Dirichlet distributions:P(?
| ?)
= ?X?NDir(?X | ?X)Because the Dirichlet distribution is conjugate to themultinomial distribution, it is possible to integrateout the rule probabilities ?, producing the ?collapsedsampler?
described in Johnson et al (2007a).In our experiments we chose an uniform prior?r = 1 for all rules r ?
R. As Table 1 shows,integrating out ?
only has a major effect on re-sults when the adaptor hyperparameters themselvesare not sampled, and even then it did not havea large effect on the collocation-syllable adaptorgrammar.
This is not too surprising: because theOnset, Nucleus and Coda adaptors in this gram-mar learn the probabilities of these building blocksof words, the phoneme probabilities (which is mostof what ?
encodes) play less important a role.3.2 Slice sampling adaptor hyperparametersAs far as we know, there are no conjugate priors forthe adaptor hyperparameters aX or bX (which cor-responds to ?X in a Chinese Restaurant Process),so it is not possible to integrate them out as we didwith the rule probabilities ?.
However, it is possibleto perform Bayesian inference by putting a prior onthem and sampling their values.Because we have no strong intuitions about thevalues of these parameters we chose uninformativepriors.
We chose a uniform Beta(1, 1) prior on aX ,and a ?vague?
Gamma(10, 0.1) prior on bX = ?X(MacKay, 2003).
(We experimented with other pa-rameters in the Gamma prior, but found no signifi-cant difference in performance).After each Gibbs sweep through the parse trees twe resampled each of the adaptor parameters fromthe posterior distribution of the parameter using aslice sampler 10 times.
For example, we resampleeach bX from:P(bX | t) ?
P(t | bX) Gamma(bX | 10, 0.1)Here P(t | bX) is the likelihood of the current se-quence of sample parse trees (we only need the fac-tors that depend on bX ) and Gamma(bX | 10, 0.1)is the prior.
The same formula is used for samplingaX , except that the prior is now a flat Beta(1, 1) dis-tribution.In general we cannot even compute the normaliz-ing constants for these posterior distributions, so wechose a sampler that does not require this.
We use aslice sampler here because it does not require a pro-posal distribution (Neal, 2003).
(We initially trieda Metropolis-Hastings sampler but were unable tofind a proposal distribution that had reasonable ac-ceptance ratios for all of our adaptor grammars).As Table 1 makes clear, sampling the adaptor pa-rameters makes a significant difference, especiallyon the collocation-syllable adaptor grammar.
Thisis not surprising, as the adaptors in that grammarplay many different roles and there is no reason toto expect the optimal values of their parameters tobe similar.321Condition Word token f-scoresSample average Max.
MarginalBatchinitializationTablelabelresamplingIntegrateout?Sample?
X=b XSamplea Xunigramcolloccolloc-syllunigramcolloccolloc-syll?
?
?
?
?
0.55 0.74 0.85 0.56 0.76 0.87?
?
?
?
0.55 0.72 0.84 0.56 0.74 0.84?
?
?
0.55 0.72 0.78 0.57 0.75 0.78?
?
0.54 0.66 0.75 0.56 0.69 0.76?
?
?
?
0.54 0.70 0.87 0.56 0.74 0.88?
?
?
?
0.55 0.42 0.54 0.57 0.51 0.55?
?
?
?
0.74 0.83 0.88 0.81 0.86 0.89?
?
?
0.75 0.43 0.74 0.80 0.56 0.82?
?
0.71 0.41 0.76 0.77 0.49 0.82?
?
?
0.71 0.73 0.87 0.77 0.75 0.88Table 1: Word segmentation accuracy measured by word token f-scores on Brent?s version of the Bernstein-Ratnercorpus as a function of adaptor grammar, adaptor and estimation procedure.
Pitman-Yor Process adaptors were usedwhen aX was sampled, otherwise Chinese Restaurant Process adaptors were used.
In runs where ?
was not integratedout it was set uniformly, and all ?X = bX were set to 100 they were not sampled.4 Inference for adaptor grammarsJohnson et al (2007b) describe the basic adaptorgrammar inference procedure that we use here.
Thatpaper leaves unspecified a number of implemen-tation details, which we show can make a crucialdifference to segmentation accuracy.
The adaptorgrammar algorithm is basically a Gibbs sampler ofthe kind widely used for nonparametric Bayesian in-ference (Blei et al, 2004; Goldwater et al, 2006b;Goldwater et al, 2006a), so it seems reasonable toexpect that at least some of the details discussed be-low will be relevant to other applications as well.The inference algorithm maintains a vector t =(t1, .
.
.
, tn) of sample parses, where ti ?
TS is aparse for the ith sentence wi.
It repeatedly chooses asentence wi at random and resamples the parse treeti for wi from P(ti | t?i, wi), i.e., conditioned on wiand the parses t?i of all sentences except wi.4.1 Maximum marginal decodingSampling algorithms like ours produce a stream ofsamples from the posterior distribution over parsesof the training data.
It is standard to take the out-put of the algorithm to be the last sample produced,and evaluate those parses.
In some other applica-tions of nonparametric Bayesian inference involv-ing latent structure (e.g., clustering) it is difficult tousefully exploit multiple samples, but that is not thecase here.In maximum marginal decoding we map eachsample parse tree t onto its corresponding word seg-mentation s, marginalizing out irrelevant detail int.
(For example, the collocation-syllable adaptorgrammar contains a syllabification and collocationalstructure that is irrelevant for word segmentation).Given a set of sample parse trees for a sentence wecompute the set of corresponding word segmenta-tions, and return the one that occurs most frequently(this is a sampling approximation to the maximumprobability marginal structure).For each setting in the experiments described inTable 1 we ran 8 samplers for 2,000 iterations (i.e.,passes through the training data), and kept the sam-ple parse trees from every 10th iteration after itera-tion 1000, resulting in 800 sample parses for everysentence.
(An examination of the posterior proba-bilities suggests that all of the samplers using batchinitialization and table label resampling had ?burnt322batch initialization, table label resamplingincremental initialization, table label resamplingbatch initialization, no table label resampling2000150010005000220000215000210000205000200000195000190000185000Figure 1: Negative log posterior probability (lower is bet-ter) as a function of iteration for 24 runs of the collo-cation adaptor grammar samplers with Pitman-Yor adap-tors.
The upper 8 runs use batch initialization but no ta-ble label resampling, the middle 8 runs use incrementalinitialization and table label resampling, while the lower8 runs use batch initialization and table label resampling.in?
by iteration 1000).
We evaluated the word to-ken f-score of the most frequent marginal word seg-mentation, and compared that to average of the wordtoken f-score for the 800 samples, which is also re-ported in Table 1.
For each grammar and setting wetried, the maximum marginal segmentation was bet-ter than the sample average, sometimes by a largemargin.
Given its simplicity, this suggests that max-imum marginal decoding is probably worth tryingwhen applicable.4.2 Batch initializationThe Gibbs sampling algorithm is initialized with aset of sample parses t for each sentence in the train-ing data.
While the fundamental theorem of MarkovChain Monte Carlo guarantees that eventually sam-ples will converge to the posterior distribution, itsays nothing about how long the ?burn in?
phasemight last (Robert and Casella, 2004).
In practiceinitialization can make a huge difference to the per-formance of Gibbs samplers (just as it can with otherunsupervised estimation procedures such as Expec-tation Maximization).There are many different ways in which we couldgenerate the initial trees t; we only study two of theobvious methods here.
Batch initialization assignsevery sentence a random parse tree in parallel.
Inmore detail, the initial parse tree ti for sentence wiis sampled from P(t | wi, G?
), where G?
is the PCFGobtained from the adaptor grammar by ignoring itslast two components A and C (i.e., the adapted non-terminals and their adaptors), and seated at a newtable.
This means that in batch initialization eachinitial parse tree is randomly generated without anyadaptation at all.Incremental initialization assigns the initial parsetrees ti to sentences wi in order, updating the adaptorgrammar as it goes.
That is, ti is sampled from P(t |wi, t1, .
.
.
, ti?1).
This is easy to do in the contextof Gibbs sampling, since this distribution is a minorvariant of the distribution P(ti | t?i, wi) used duringGibbs sampling itself.Incremental initialization is greedier than batchinitialization, and produces initial sample trees withmuch higher probability.
As Table 1 shows, acrossall grammars and conditions after 2,000 iterationsincremental initialization produces samples withmuch better word segmentation token f-score thandoes batch initialization, with the largest improve-ment on the unigram adaptor grammar.However, incremental initialization results insample parses with lower posterior probability forthe unigram and collocation adaptor grammars (butnot for the collocation-syllable adaptor grammar).Figure 1 plots the posterior probabilities of the sam-ple trees t at each iteration for the collocation adap-tor grammar, showing that even after 2,000 itera-tions incremental initialization results in trees thatare much less likely than those produced by batchinitialization.
It seems that with incremental initial-ization the Gibbs sampler gets stuck in a local op-timum which it is extremely unlikely to move awayfrom.It is interesting that incremental initialization re-sults in more accurate word segmentation, eventhough the trees it produces have lower posteriorprobability.
This seems to be because the most prob-able analyses produced by the unigram and, to alesser extent, the collocation adaptor grammars tendto undersegment.
Incremental initialization greed-ily searches for common substrings, and becausesuch substrings are more likely to be short ratherthan long, it tends to produce analyses with shorterwords than batch initialization does.
Goldwater etal.
(2006a) show that Brent?s incremental segmenta-tion algorithm (Brent, 1999) has a similar property.We favor batch initialization because we are in-323terested in understanding the properties of our mod-els (expressed here as adaptor grammars), and batchinitialization does a better job of finding the mostprobable analyses under these models.
However, itmight be possible to justify incremental initializa-tion as (say) cognitively more plausible.4.3 Table label resamplingUnlike the previous two implementation choiceswhich apply to a broad range of algorithms, tablelabel resampling is a specialized kind of Gibbs stepfor adaptor grammars and similar hierarchical mod-els that is designed to improve mobility.
The adap-tor grammar algorithm described in Johnson et al(2007b) repeatedly resamples parses for the sen-tences of the training data.
However, the adaptorgrammar sampler itself maintains of a hierarchy ofChinese Restaurant Processes or Pitman-Yor Pro-cesses, one per adapted nonterminal X ?
A, thatcache subtrees from TX .
In general each of thesesubtrees will occur many times in the parses for thetraining data sentences.
Table label resampling re-samples the trees in these adaptors (i.e., the tablelabels, to use the restaurant metaphor), potentiallychanging the analysis of many sentences at once.For example, each Collocation in the collocationadaptor grammar can occur in many Sentences, andeach Word can occur in many Collocations.
Resam-pling a single Collocation can change the way it isanalysed into Words, thus changing the analysis ofall of the Sentences containing that Collocation.Table label resampling is an additional resam-pling step performed after each Gibbs sweepthrough the training data in which we resample theparse trees labeling the tables in the adaptor for eachX ?
A.
Specifically, if the adaptor CX for X ?
Acurrently contains m tables labeled with the treest = (t1, .
.
.
, tm) then table label resampling re-places each tj , j ?
1, .
.
.
,m in turn with a tree sam-pled from P(t | t?j , wj), where wj is the terminalyield of tj .
(Within each adaptor we actually resam-ple all of the trees t in a randomly chosen order).Table label resampling is a kind of Gibbs sweep,but at a higher level in the Bayesian hierarchy thanthe standard Gibbs sweep.
It?s easy to show that ta-ble label resampling preserves detailed balance forthe adaptor grammars presented in this paper, so in-terposing table label resampling steps with the stan-dard Gibbs steps also preserves detailed balance.We expect table label resampling to have thegreatest impact on models with a rich hierarchi-cal structure, and the experimental results in Ta-ble 1 confirm this.
The unigram adaptor grammardoes not involve nested adapted nonterminals, sowe would not expect table label resampling to haveany effect on its analyses.
On the other hand, thecollocation-syllable adaptor grammar involves a richhierarchical structure, and in fact without table la-bel resampling our sampler did not burn in or mixwithin 2,000 iterations.
As Figure 1 shows, tablelabel resampling produces parses with higher pos-terior probability, and Table 1 shows that table la-bel resampling makes a significant difference in theword segmentation f-score of the collocation andcollocation-syllable adaptor grammars.5 ConclusionThis paper has examined adaptor grammar infer-ence procedures and their effect on the word seg-mentation problem.
Some of the techniques inves-tigated here, such as batch versus incremental ini-tialization, are quite general and may be applica-ble to a wide range of other algorithms, but someof the other techniques, such as table label resam-pling, are specialized to nonparametric hierarchi-cal Bayesian inference.
We?ve shown that samplingadaptor hyperparameters is feasible, and demon-strated that this improves word segmentation accu-racy of the collocation-syllable adaptor grammar byalmost 10%, corresponding to an error reduction ofover 35% compared to the best results presented inJohnson (2008).
We also described and investigatedtable label resampling, which dramatically improvesthe effectiveness of Gibbs sampling estimators forcomplex adaptor grammars, and makes it possibleto work with adaptor grammars with complex hier-archical structure.AcknowledgmentsWe thank Erik Sudderth for suggesting sampling thePitman-Yor hyperparameters and the ACL review-ers for their insightful comments.
This research wasfunded by NSF awards 0544127 and 0631667 toMark Johnson.324ReferencesN.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, editors,Children?s Language, volume 6.
Erlbaum, Hillsdale,NJ.Daniel Blanchard and Jeffrey Heinz.
2008.
Improv-ing word segmentation by simultaneously learningphonotactics.
In CoNLL 2008: Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, pages 65?72, Manchester, England,August.David Blei, Thomas L. Griffiths, Michael I. Jordan, andJoshua B. Tenenbaum.
2004.
Hierarchical topicmodels and the nested chinese restaurant process.In Sebastian Thrun, Lawrence Saul, and BernhardScho?lkopf, editors, Advances in Neural InformationProcessing Systems 16.
MIT Press, Cambridge, MA.Rens Bod.
1998.
Beyond grammar: an experience-basedtheory of language.
CSLI Publications, Stanford, Cal-ifornia.M.
Brent and T. Cartwright.
1996.
Distributional reg-ularity and phonotactic constraints are useful for seg-mentation.
Cognition, 61:93?125.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.
Ma-chine Learning, 34:71?105.Jeffrey Elman.
1990.
Finding structure in time.
Cogni-tive Science, 14:197?211.Margaret M. Fleck.
2008.
Lexicalized phonotacticword segmentation.
In Proceedings of ACL-08: HLT,pages 130?138, Columbus, Ohio, June.
Associationfor Computational Linguistics.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006a.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 673?680, Sydney, Aus-tralia.
Association for Computational Linguistics.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006b.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466,Cambridge, MA.
MIT Press.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2007.
Distributional cues to word boundaries:Context is important.
In David Bamman, TatianaMagnitskaia, and Colleen Zaller, editors, Proceedingsof the 31st Annual Boston University Conference onLanguage Development, pages 239?250, Somerville,MA.
Cascadilla Press.H.
Ishwaran and L. F. James.
2003.
Generalizedweighted Chinese restaurant processes for speciessampling mixture models.
Statistica Sinica, 13:1211?1235.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007a.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 139?146,Rochester, New York, April.
Association for Compu-tational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007b.
Adaptor Grammars: A frameworkfor specifying compositional nonparametric Bayesianmodels.
In B. Scho?lkopf, J. Platt, and T. Hoffman, ed-itors, Advances in Neural Information Processing Sys-tems 19, pages 641?648.
MIT Press, Cambridge, MA.Mark Johnson.
2008.
Using adaptor grammars to identi-fying synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics, Columbus, Ohio.
Association for ComputationalLinguistics.Aravind Joshi.
2003.
Tree adjoining grammars.
In Rus-lan Mikkov, editor, The Oxford Handbook of Compu-tational Linguistics, pages 483?501.
Oxford Univer-sity Press, Oxford, England.David J.C. MacKay.
2003.
Information Theory, Infer-ence, and Learning Algorithms.
Cambridge Univer-sity Press.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31:705?767.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
Annals of Probability, 25:855?900.J.
Pitman.
1995.
Exchangeable and partially exchange-able random partitions.
Probability Theory and Re-lated Fields, 102:145?158.Christian P. Robert and George Casella.
2004.
MonteCarlo Statistical Methods.
Springer.Andreas Stolcke and Stephen Omohundro.
1994.
Induc-ing probabilistic grammars by Bayesian model merg-ing.
In Rafael C. Carrasco and Jose Oncina, editors,Grammatical Inference and Applications, pages 106?118.
Springer, New York.Y.
W. Teh, M. Jordan, M. Beal, and D. Blei.
2006.
Hier-archical Dirichlet processes.
Journal of the AmericanStatistical Association, 101:1566?1581.325
