Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 37?42,Columbus, June 2008. c?2008 Association for Computational LinguisticsArabic Language Modeling with Finite State TransducersIlana HeintzDepartment of LinguisticsThe Ohio State UniversityColumbus, OHheintz.38@osu.edu?AbstractIn morphologically rich languages such asArabic, the abundance of word forms result-ing from increased morpheme combinations issignificantly greater than for languages withfewer inflected forms (Kirchhoff et al, 2006).This exacerbates the out-of-vocabulary (OOV)problem.
Test set words are more likely tobe unknown, limiting the effectiveness of themodel.
The goal of this study is to use theregularities of Arabic inflectional morphologyto reduce the OOV problem in that language.We hope that success in this task will result ina decrease in word error rate in Arabic auto-matic speech recognition.1 IntroductionThe task of language modeling is to predict the nextword in a sequence of words (Jelinek et al, 1991).Predicting words that have not yet been seen is themain obstacle (Gale and Sampson, 1995), and iscalled the Out of Vocabulary (OOV) problem.
Inmorphologically rich languages, the OOV problemis worsened by the increased number of morphemecombinations.Berton et al (1996) and Geutner (1995) ap-proached this problem in German, finding that lan-guage models built on decomposed words reduce theOOV rate of a test set.
In Carki et al (2000), Turk-ish words are split into syllables for language model-ing, also reducing the OOV rate (but not improving?This work was supported by a student-faculty fellowshipfrom the AFRL/Dayton Area Graduate Studies Insititute, andworked on in partnership with Ray Slyh and Tim Anderson ofthe Air Force Research Labs.WER).
Morphological decomposition is also used toboost language modeling scores in Korean (Kwon,2000) and Finnish (Hirsima?ki et al, 2006).We approach the processing of Arabic morphol-ogy, both inflectional and derivational, with finitestate machines (FSMs).
We use a technique that pro-duces many morphological analyses for each word,retaining information about possible stems, affixes,root letters, and templates.
We build our languagemodels on the morphemes generated by the anal-yses.
The FSMs generate spurious analyses.
Thatis, although a word out of context may have severalmorphological analyses, in context only one suchanalysis is correct.
We retain all analyses.
We ex-pect that any incorrect morphemes that are generatedwill not affect the predictions of the model, becausethey will be rare, and the language model introducesbias towards frequent morphemes.
Although manywords in a test set may not have occurred in a train-ing set, the morphemes that make up that word likelywill have occurred.
Using many decompositions todescribe each word sets apart this study from othersimilar studies, including those byWang and Vergyri(2006) and Xiang et al (2006).This study differs from previous research on Ara-bic language modeling and Arabic automatic speechrecognition in two other ways.
To promote cross-dialectal use of the techniques, we use properties ofArabic morphology that we assume to be common tomany dialects.
Also, we treat morphological analy-sis and vowel prediction with a single solution.An overview of Arabic morphology is given inSection 2.
A description of the finite state machineprocess used to decompose the Arabic words into37morphemes follows in Section 3.
The experimentallanguage model training procedure and the proce-dures for training two baseline language models arediscussed in Section 4.
We evaluate all three modelsusing average negative log probability and coveragestatistics, discussed in Section 5.2 Arabic MorphologyThis section describes the morphological processesresponsible for the proliferation of word forms inArabic.
The discussion is based on information fromgrammar textbooks such as that by Haywood andNahmad (1965), as well as descriptions in variousArabic NLP articles, including that by Kirchhoff etal.
(2006).Word formation in Arabic takes place on twolevels.
Arabic is a root-and-pattern language inwhich many vocalic and consonantal patterns com-bine with semantic roots to create surface forms.
Aroot, usually composed of three letters, may encodemore than one meaning.
Only by combining a rootwith a pattern does one create a meaningful and spe-cific term.
The combination of a root with a patternis a stem.
In some cases, a stem is a complete surfaceform; in other cases, affixes are added.The second level of word formation is inflec-tional, and is usually a concatenative process.
In-flectional affixes are used to encode person, number,gender, tense, and mood information on verbs, andgender, number, and case information on nouns.
Af-fixes are a closed class of morphemes, and they en-code predictable information.
In addition to inflec-tion, cliticization is common in Arabic text.
Prepo-sitions, conjunctions, and possessive pronouns areexpressed as clitics.This combination of templatic derivational mor-phology and concatenative inflectional morphology,together with cliticization, results in a rich variationin word forms.
This richness is in contrast with theslower growth in number of English word forms.
Asshown in Table 1, the Arabic stem /drs/, meaning tostudy, combines with the present tense verb pattern?CCuCu?, where the ?C?
represents a root letter, toform the present tense stem drusu.
This stem can becombined with 11 different combinations of inflec-tional affixes, creating as many unique word forms.Table 1 can be expanded with stems from theTransliteration Translation Affixesadrusu I study a-nadrusu we study na-tadrusu you (ms) study ta-tadrusina you (fs) study ta- ,-inatadrusAn you (dual) study ta-, -Antadrusun you (mp) study ya-, -ntadrusna you (fp) study ta-, -nayadrusu he studies ya-tadrusu she studies ta-yadrusan they (dual) study ya-, -Anyadrusun they (mp) study ya-, -nyadrusna they (fp) study ya-, -naTable 1: An Example of Arabic Inflectional Morphologysame root representing different tenses.
For in-stance, the stem daras means studied.
Or, we cancombine the root with a different pattern to obtaindifferent meanings, for instance, to teach or to learn.Each of these stems can combine with the same ordifferent affixes to create additional word forms.Adding a single clitic to the words in Table 1 willdouble the number of forms.
For instance, the wordadrusu, meaning I study, can take the enclitic ?ha?,to express I study it.
Some clitics can be combined,increasing again the number of possible word forms.Stems differ in some ways that do not surface inthe Arabic orthography.
For instance, the pattern?CCiCu?
differs from ?CCuCu?
only in one shortvowel, which is encoded orthographically as a fre-quently omitted diacritic.
Thus, adrisu and adrusuare homographs, but not homophones.
This prop-erty helps decrease the number of word forms, butit causes ambiguity in morphological analyses.
Re-covering the quality of short vowels is a significantchallenge in Arabic natural language processing.This abundance of unique word forms in ModernStandard Arabic is problematic for natural languageprocessing (NLP).
NLP tasks usually require thatsome analysis be provided for each word (or otherlinguistic unit) in a given data set.
For instance,in spoken word recognition, the decoding processmakes use of a language model to predict the wordsthat best fit the acoustic signal.
Only words that havebeen seen in the language model?s training data willbe proposed.
Because of the immense number ofpossible word forms in Arabic, it is highly proba-3801m2m t d A s r3A s r m t d4m t d A s r01m t d A s r2s r m t d A3A4m t d A s rFigure 1: Two templates, mCCC andCCAC as finite staterecognizers, with a small sample alphabet of letters A, d,m, r, s, and t.0m:mt:td:dA:As:sr:r1m:[m2A:A s:s r:r m:m t:t d:d3m:m t:t d:d A:A s:s r:r4m:m] t:t] d:d] A:A] s:s] r:r]m:mt:td:dA:As:sr:rFigure 2: The first template above, now a transducer, withaffixes accepted, and the stem separated by brackets in theoutput.ble that the words in an acoustic signal will not havebeen present in the language model?s training text,and incorrect words will be predicted.
We use in-formation about the morphology of Arabic to createa more flexible language model.
This model shouldencounter fewer unseen forms, as the units we use tomodel the language are the more frequent and pre-dictable morphemes, as opposed to full word forms.As a result, the word error rate is expected to de-crease.3 FSM AnalysesThis section describes howwe derive, for each word,a lattice that describes all possible morphologicaldecompositions for that word.
We start with a groupof templates that define the root consonant positions,long vowels, and consonants for all Arabic regularand augmented stems.
For instance, where C repre-sents a root consonant, three possible templates are02m1[mdrA]3[drAs] sFigure 3: Two analyses of the word ?mdrAs?, as pro-duced by composing a word FSM with the templateFSMs above.CCC, mCCC, and CACC.
We build a finite state rec-ognizer for each of the templates, and in each case,the C arcs are expanded, so that every possible rootconsonant in the vocabulary has an arc at that posi-tion.
The two examples in Figure 1 show the patternsmCCC and CCAC and a short sample alphabet.At the start and end node of each template recog-nizer, we add arcs with self-loops.
This allows anysequence of consonants as an affix.
To track stemboundaries, we add an open bracket to the first stemarc, and a close bracket to the final stem arc.
Thetemplates are compiled into finite state transducers.Figure 2 shows the result of these additions.For each word in the vocabulary, we define a sim-ple, one-arc-per-letter finite state recognizer.
Wecompose this with each of the templates.
Some num-ber of analyses result from each composition.
Thatis, a single template may not compose with the word,may compose with it in a unique way, or may com-pose with the word in several ways.
Each of the suc-cessful compositions produces a finite state recog-nizer with brackets surrounding the stem.
We use ascript to collapse the arcs within the stem to a singlearc.
The result is shown in Figure 3, where the word?mdrAs?
has two analyses corresponding to the twotemplates shown.
We store a lattice as in Figure 3for each word.The patterns that we use to constrain the stemforms are drawn from Haywood and Nahmad(1965).
These patterns also specify the short vowelpatterns that are used with words derived from eachpattern.
An option is to simply add these shortvowels to the output symbols in the template FSTs.However, because several short vowel options mayexist for each template, this would greatly increasethe size of the resulting lattices.
We postpone this ef-fort.
In this work, we focus solely on the usefulnessof the unvoweled morphological decompositions.We do not assess or need to assess the accuracy of39the morphological decompositions.
Our hypothesisis that by having many possible decompositions perword, the frequencies of various affixes and stemsacross all words will lead the model to the strongestpredictions.
Even if the final predictions are not pre-scriptively correct, they may be the most useful de-compositions for the purpose of speech decoding.4 ProcedureWe compare a language model built on multiple seg-mentations as determined by the FSMs describedabove to two baseline models.
We call our exper-imental model FSM-LM; the baseline models useword-based n-grams (WORD), and pre-defined affixsegmentations (AFFIX).
Our data set in this studyis the TDT4 Arabic broadcast news transcriptions(Kong and Graff, 2005).
Because of time and mem-ory constraints, we built and evaluated all models ononly a subsection of the training data, 100 files ofTDT4, balanced across the years of collection, andcontaining files from each of the 4 news sources.
Weuse 90 files for training, comprising about 6.3 mil-lion unvoweled word tokens, and 10 files for testing,comprising about 700K word tokens, and around 5Ksentences.
The size of the vocabulary is 104757.
Weuse ten-fold cross-validation in our evaluations.4.1 Experimental ModelWe extract the vocabulary of the training data, andcompile the word lattices as described in Section 3.The union of all decompositions (a lattice) for eachindividual word is stored separately.For each sentence of training data, we concate-nate the lattices representing each word in that sen-tence.
We use SRILM (Stolcke, 2002) to calculatethe posterior expected n-gram count for morphemesequences up to 4-grams in the sentence-long lattice.The estimated frequency of an n-gram N is calcu-lated as the number of occurrences of that n-gramin the lattice, divided by the number of paths in thelattice.
This is true so long as the paths are equallyweighted; at this point in our study, this is the case.We merge the n-gram counts over all sentencesin all of the training files.
Next, we estimate a lan-guage model based on the n-gram counts, using onlythe 64000 most frequent morphemes, since we ex-pect this vocabulary size may be a limitation of ourASR system.
Also, by limiting the vocabulary sizeof all of our models (including the baseline modelsdescribed below), we can make a fairer comparisonamong the models.
We use Good-Turing smoothingto account for unseen morphemes, all of which arereplaced with a single ?unknown?
symbol.In later work, we will apply our LM statistics tothe lattices, and recalculate the path weights andestimated counts.
In this study, the paths remainequally weighted.We evaluate this model, which we call FSM-LM,with respect to two baseline models.4.2 Baseline ModelsFor the WORD model, we do no manipulation to thetraining or test sets beyond the normalization thatoccurs as a preprocessing step (hamza normaliza-tion, replacement of problematic characters).
Webuild a word-based 4-gram language model usingthe 64000 most frequent words and Good-Turingsmoothing.For the AFFIX model, we first define the charac-ter strings that are considered affixes.
We use thesame list of affixes as in Xiang et al (2006), whichincludes 12 prefixes and 34 suffixes.
We add to thelists all combinations of two prefixes and two suf-fixes.
We extract the vocabulary from the trainingdata, and for each word, propose a single segmenta-tion, based on the following constraints:1.
If the word has an acceptable prefix-stem-suffixdecomposition, such that the stem is at least 3characters long, choose it as the correct decom-position.2.
If only one affix is found, make sure the re-mainder is at least 3 characters long, and is notalso a possible affix.3.
If the word has prefix-stem and stem-suffix de-compositions, use the longest affix.4.
If the longest prefix and longest suffix are equallength, choose the prefix-stem decomposition.We build a dictionary that relates each word to asingle segmentation (or no segmentation).
We seg-ment the training and test texts by replacing eachword with its segmentation.
Morphemes are sepa-rated by whitespace.
The language model is built bycounting 4-grams over the training data, then usingonly the most frequent 64000 morphemes in estimat-ing a language model with Good-Turing smoothing.40WORD AFFIX FSM-LMAvg NegLog Prob4.65 5.30 4.56Coverage (%):Unigram 96.03 99.30 98.89Bigram 17.81 53.13 69.56Trigram 1.52 11.89 27.25Four-gram .37 3.42 9.62Table 2: Average negative log probability and coverageresults for one experimental language model (FSM-LM)and two baseline language models.
Results are averagesover 10 folds.5 EvaluationFor each model, the test set undergoes the same ma-nipulation as the train set; words are left alone forthe WORD model, split into a single segmentationeach for the AFFIX model, or their FSM decompo-sitions are concatenated.Language models are often compared using theperplexity statistic:PP (x1 .
.
.
xn) = 2?
1n?nxi=4logP (xi|xi?3i?1) (1)Perplexity represents the average branching factor ofa model; that is, at each point in the test set, we cal-culate the entropy of the model.
Therefore, a lowerperplexity is desired.In the AFFIX and FSM-LM models, each word issplit into several parts.
Therefore, the value 1n wouldbe approximately three times smaller for these mod-els, giving them an advantage.
To make a more evencomparison, we calculate the geometric mean of then-gram transition probabilities, dividing by the num-ber of words in the test set, not morphemes, as inKirchhoff et al (2006).
The log of this equation is:AvgNegLogProb(x1 .
.
.
xn) =?1Nn?i=4logP (xi|xi?3i?1) (2)where n is the number of morphemes or words in thetest set, depending on the model, and N is the num-ber of words in the test set, and log P (xi|xi?3i?1) is thelog probability of the item xi given the 3-item his-tory (calculated in base 10, as this is how the SRILMToolkit is implemented).
Again, we are looking fora low score.In the FSM-LM, each test sentence is representedby a lattice of paths.
To determine the negative logprobability of the sentence, we score all paths ofthe sentence according to the equations above, andrecord the maximum probability.
This reflects thelikely procedure we would use in implementing thismodel within an ASR task.We see in Table 2 that the average negative logprobability of the FSM-LM is lower than that ofeither the WORD or AFFIX model.
The averageacross 10 folds reflects the pattern of scores for eachfold.
We conclude from this that the FSM modelof predicting morphemes is more effective than -or more conservatively, at least as effective as - astatic decomposition, as in the AFFIX model.
Fur-thermore, we have successfully reproduced the re-sults of Xiang et al (2006) and Kirchhoff et al(2006), among others, that modeling Arabic withmorphemes is more effective than modeling withwhole word forms.We also calculate the coverage of each model: thepercentage of units in the test set that are given prob-abilities in the language model.
For the FSM model,only the morphemes in the best path are counted.The coverage results are reported in Table 2 as theaverage coverage over the 10 folds.
Both the AF-FIX and FSM-LM models showed improved cover-age as compared to the WORD model, as expected.This means that we reduce the OOV problem by us-ing morphemes instead of whole words.
The AF-FIX model has the best coverage of unigrams be-cause only new stems, not new affixes, are proposedin the test set.
That is, the same fixed set of affixesare used to decompose the test set as the train set,however, unseem stems may appear.
In the FSM-LM, there are no restrictions on the affixes, there-fore, unseen affixes may appear in the test set, aswell as new stems, lowering the unigram coverage ofthe test set.
For larger n-grams, however, the FSM-LM model has the best coverage.
This is due tokeeping all decompositions until test time, then al-lowing the language model to define the most likelysequences, rather than specifying a single decompo-sition for each word.A 4-gram of words will tend to cover more con-text than a 4-gram of morphemes; therefore, theword 4-grams will exhibit more sparsity than themorpheme 4-grams.
We compare, for a single train-41WORD AFFIX FSM-LMunigrams 4.97 5.84 5.60bigrams 4.95 5.70 4.61trigrams 4.95 5.69 4.56four-grams 4.95 5.69 4.57Table 3: Comparison of n-gram orders across languagemodel types.test fold, how lower order n-grams compare amongthe models.
The results are shown in Table 3.
Wefind that for lower-order n-grams, the word modelperforms best.
As the n-grams get larger, the spar-sity problem favors the FSM-LM, which has the bestoverall score of all models shown.
Apparently, thefrequencies of 3- and 4-grams are not big enoughto make a big difference in the evaluation.
This islikely due to the small size of our corpus, and weexpect the result would change if we were to use allof the TDT4 corpus, rather than a 100 file portion ofthe corpus.6 Conclusion & Future WorkIt has been shown that reduced perplexity scores donot necessarily correlate with reduced word errorrates in an ASR task (Berton et al, 1996).
This is be-cause the perplexity (or in this case, average negativelog probability) statistic does not take into accountthe acoustic confusability of the items being consid-ered.
However, the average negative log probabilityscore is a useful tool as a proof-of-concept, givingus reason to believe that we may be successful inimplementing this model within an ASR task.The real test of this model is its ability to predictshort vowels.
The average negative log probabilityscores may lead us to believe that the FSM-LM isonly marginally better than the WORD or AFFIXmodel, and the differences may not be apparent inan ASR application.
However, only the FSM-LMmodel allows for the opportunity to predict shortvowels, by arranging the FSMs as finite state trans-ducers with short vowel information encoded as partof the stem patterns.We will continue to tune the language model byapplying the language model weights to the decom-position paths and re-estimating the language model.Also, we will expand the language model to includemore training data.
We will implement the modelwithin an Arabic ASR system, with and withoutshort vowel hypotheses.
Furthermore, we are inter-ested to see how well the application of these tem-plates and this framework will apply to other Arabicdialects.ReferencesA Berton, P Fetter, and P Regel-Brietzmann.
1996.Compound words in large vocabulary German speechrecognition system.
In Proceedings of ICSLP 96,pages 1165?1168.K.
Carki, P. Geutner, and T. Schultz.
2000.
Turkishlvcsr: Towards better speech recognition for aggluti-native languages.
In ICASSP 2000, pages 134?137.William A. Gale and Geoffrey Sampson.
1995.
Good-Turing frequency estimation without tears.
Journal ofQuantitative Linguistics, 22:217?37.P Geutner.
1995.
Using morphology towards betterlarge-vocabulary speech recognition systems.
In Pro-ceedings of ICASSP-95, volume 1, pages 445?448.J.A.
Haywood and H.M. Nahmad.
1965.
A New ArabicGrammar of the Written Language.
Lund Humphries,Burlington, VT.Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja, and Janne Pylkko?nen.
2006.Unlimited vocabulary speech recognition with morphlanguage models applied to Finnish.
Computer Speechand Language, 20:515?541.F.
Jelinek, B. Merialdo, S. Roukos, and M. Strauss.
1991.A dynamic language model for speech recognition.
InProc.
Wkshp on Speech and Natural Language, pages293?295, Pacific Grove, California.
ACL.Katrin Kirchhoff, Dimitra Vergyri, Kevin Duh, JeffBilmes, and Andreas Stolcke.
2006.
Morphology-based language modeling for conversational Arabicspeech recognition.
Computer Speech and Language,20(4):589?608.Junbo Kong and David Graff.
2005.
TDT4 multilingualbroadcast news speech corpus.Oh-Wook Kwon.
2000.
Performance of LVCSR withmorpheme-based and syllable-based recognition units.In Proceedings of ICASSP ?00, volume 3, pages 1567?1570.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proc.
Intl.
Conf.
SpokenLanguage Processing, Denver, Colorado.Wen Wang and Dimitra Vergyri.
2006.
The use of wordn-grams and parts of speech for hierarchical clusterlanguage modeling.
In Proceedings of ICASSP 2006,pages 1057?1060.Bing Xiang, Kham Nguyen, Long Nguyen, RichardSchwartz, and John Makhoul.
2006.
Morphologicaldecomposition for Arabic broadcast news transcrip-tion.
In Proc.
ICASSP 2006, pages 1089?1092.42
