Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879?887,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPComparing Objective and Subjective Measures of Usabilityin a Human-Robot Dialogue SystemMary Ellen Foster and Manuel Giuliani and Alois KnollInformatik VI: Robotics and Embedded SystemsTechnische Universita?t Mu?nchenBoltzmannstra?e 3, 85748 Garching bei Mu?nchen, Germany{foster,giuliani,knoll}@in.tum.deAbstractWe present a human-robot dialogue sys-tem that enables a robot to work togetherwith a human user to build wooden con-struction toys.
We then describe a study inwhich na?
?ve subjects interacted with thissystem under a range of conditions andthen completed a user-satisfaction ques-tionnaire.
The results of this study pro-vide a wide range of subjective and ob-jective measures of the quality of the in-teractions.
To assess which aspects of theinteraction had the greatest impact on theusers?
opinions of the system, we used amethod based on the PARADISE evalua-tion framework (Walker et al, 1997) to de-rive a performance function from our data.The major contributors to user satisfac-tion were the number of repetition requests(which had a negative effect on satisfac-tion), the dialogue length, and the users?recall of the system instructions (both ofwhich contributed positively).1 IntroductionEvaluating the usability of a spoken language dia-logue system generally requires a large-scale userstudy, which can be a time-consuming processboth for the experimenters and for the experimen-tal subjects.
In fact, it can be difficult even todefine what the criteria are for evaluating such asystem (cf.
Novick, 1997).
In recent years, tech-niques have been introduced that are designed topredict user satisfaction based on more easily mea-sured properties of an interaction such as dialoguelength and speech-recognition error rate.
The de-sign of such performance methods for evaluatingdialogue systems is still an area of open research.The PARADISE framework (PARAdigm forDIalogue System Evaluation; Walker et al (1997))describes a method for using data to derive a per-formance function that predicts user-satisfactionscores from the results on other, more easily com-puted measures.
PARADISE uses stepwise mul-tiple linear regression to model user satisfactionbased on measures representing the performancedimensions of task success, dialogue quality, anddialogue efficiency, and has been applied to a widerange of systems (e.g., Walker et al, 2000; Litmanand Pan, 2002; Mo?ller et al, 2008).
If the result-ing performance function can be shown to predictuser satisfaction as a function of other, more eas-ily measured system properties, it will be widelyapplicable: in addition to making it possible toevaluate systems based on automatically availabledata from log files without the need for extensiveexperiments with users, for example, such a per-formance function can be used in an online, incre-mental manner to adapt system behaviour to avoidentering a state that is likely to reduce user satis-faction, or can be used as a reward function in areinforcement-learning scenario (Walker, 2000).Automated evaluation metrics that rate sys-tem behaviour based on automatically computableproperties have been developed in a number ofother fields: widely used measures include BLEU(Papineni et al, 2002) for machine translation andROUGE (Lin, 2004) for summarisation, for exam-ple.
When employing any such metric, it is cru-cial to verify that the predictions of the automatedevaluation process agree with human judgementsof the important aspects of the system output.
Ifnot, the risk arises that the automated measures donot capture the behaviour that is actually relevantfor the human users of a system.
For example,Callison-Burch et al (2006) presented a number of879counter-examples to the claim that BLEU agreeswith human judgements.
Also, Foster (2008) ex-amined a range of automated metrics for evalua-tion generated multimodal output and found thatfew agreed with the preferences expressed by hu-man judges.In this paper, we apply a PARADISE-style pro-cess to the results of a user study of a human-robotdialogue system.
We build models to predict theresults on a set of subjective user-satisfaction mea-sures, based on objective measures that were eithergathered automatically from the system logs or de-rived from the video recordings of the interactions.The results indicate that the most significant con-tributors to user satisfaction were the number ofsystem turns in the dialogues, the users?
ability torecall the instructions given by the robot, and thenumber of times that the user had to ask for in-structions to be repeated.
The former two mea-sures were positively correlated with user satisfac-tion, while the latter had a negative impact on usersatisfaction; however the correlation in all caseswas relatively low.
At the end of the paper, wediscuss possible reasons for these results and pro-pose other measures that might have a larger effecton users?
judgements.2 Task-Based Human-Robot DialogueThis study makes use of the JAST human-robotdialogue system (Rickert et al, 2007) which sup-ports multimodal human-robot collaboration on ajoint construction task.
The user and the robotwork together to assemble wooden constructiontoys on a common workspace, coordinating theiractions through speech, gestures, and facial dis-plays.
The robot (Figure 1) consists of a pairof manipulator arms with grippers, mounted ina position to resemble human arms, and an an-imatronic talking head (van Breemen, 2005) ca-pable of producing facial expressions, rigid headmotion, and lip-synchronised synthesised speech.The system can interact in English or German.The robot is able to manipulate objects in theworkspace and to perform simple assembly tasks.In the system that was used in the current study,the robot instructs the user on building a partic-ular compound object, explaining the necessaryassembly steps and retrieving pieces as required,with the user performing the actual assembly ac-tions.
To make joint action necessary for successin the assembly task, the workspace is divided intoFigure 1: The JAST dialogue robotSYSTEM First we will build a windmill.
Okay?USER Okay.SYSTEM To make a windmill, we must make asnowman.SYSTEM [picking up and holding out red cube]To make a snowman, insert the green boltthrough the end of this red cube and screwit into the blue cube.USER [takes cube, performs action] Okay.SYSTEM [picking up and holding out a smallslat] To make a windmill, insert the yellowbolt through the middle of this short slat andthe middle of another short slat and screw itinto the snowman.USER [takes slat, performs action] Okay.SYSTEM Very good!Figure 2: Sample human-robot dialogue880(a) Windmill (b) Snowman (c) L Shape (d) Railway signalFigure 3: The four target objects used in the experimenttwo areas?one belonging to the robot and one tothe user?so that the robot must hand over somepieces to the user.
Figure 2 shows a sample dia-logue in which the system explains to the user howto build an object called a ?windmill?, which has asub-component called a ?snowman?.3 Experiment DesignThe human-robot system was evaluated via a userstudy in which subjects interacted with the com-plete system; all interactions were in German.
Asa between-subjects factor, we manipulated two as-pects of the generated output: the strategy used bythe dialogue manager to explain a plan to the user,and the type of referring expressions produced bythe system.
Foster et al (2009) give the detailsof these factors and describes the effects of eachindividual manipulation.
In this paper, we concen-trate on the relationships among the different fac-tors that were measured during the study: the effi-ciency and quality of the dialogues, the users?
suc-cess at building the required objects and at learn-ing the construction plans for new objects, and theusers?
subjective reactions to the system.3.1 Subjects43 subjects (27 male) took part in this experi-ment; the results of one additional subject werediscarded due to technical problems with the sys-tem.
The mean age of the subjects was 24.5, with aminimum of 14 and a maximum of 55.
Of the sub-jects who indicated an area of study, the two mostcommon areas were Informatics (12 subjects) andMathematics (10).
On a scale of 1?5, subjectsgave a mean assessment of their knowledge ofcomputers at 3.4, of speech-recognition systemsat 2.3, and of human-robot systems at 2.0.
Thesubjects were compensated for their participationin the experiment.3.2 ScenarioIn this experiment, each subject built the samethree objects in collaboration with the system,always in the same order.
The first targetwas a ?windmill?
(Figure 3a), which has a sub-component called a ?snowman?
(Figure 3b).
Oncethe windmill was completed, the system thenwalked the user through building an ?L shape?
(Figure 3c).
Finally, the robot instructed the userto build a ?railway signal?
(Figure 3d), which com-bines an L shape with a snowman.
During the con-struction of the railway signal, the system askedthe user if they remembered how to build a snow-man and an L shape.
If the user did not remember,the system explained the building process again; ifthey did remember, the system simply told them tobuild another one.3.3 Dependent VariablesWe gathered a wide range of dependent measures:objective measures derived from the system logsand video recordings, as well as subjective mea-sures based on the users?
own ratings of their ex-perience interacting with the system.3.3.1 Objective MeasuresWe collected a range of objective measures fromthe log files and videos of the interactions.
LikeLitman and Pan (2002), we divided our objectivemeasures into three categories based on those usedin the PARADISE framework: dialogue efficiency,dialogue quality, and task success.The dialogue efficiency measures concentratedon the timing of the interaction: the time taken tocomplete the three construction tasks, the numberof system turns required for the complete interac-tion, and the mean time taken by the system to re-spond to the user?s requests.We considered four measures of dialogue qual-ity.
The first two measures looked specifically forsigns of problems in the interaction, using data au-881tomatically extracted from the logs: the number oftimes that the user asked the system to repeat itsinstructions, and the number of times that the userfailed to take an object that the robot attempted tohand over.
The other two dialogue quality mea-sures were computed based on the video record-ings: the number of times that the user looked atthe robot, and the percentage of the total inter-action that they spent looking at the robot.
Weconsidered these gaze-based measures to be mea-sures of dialogue quality since it has previouslybeen shown that, in this sort of task-based interac-tion where there is a visually salient object, par-ticipants tend to look at their partner more oftenwhen there is a problem in the interaction (e.g.,Argyle and Graham, 1976).The task success measures addressed user suc-cess in the two main tasks undertaken in these in-teractions: assembling the target objects followingthe robot?s instructions, and learning and remem-bering to make a snowman and an L shape.
Wemeasured task success in two ways, correspond-ing to these two main tasks.
The user?s success inthe overall assembly task was assessed by count-ing the proportion of target objects that were as-sembled as intended (i.e., as in Figure 3), whichwas judged based on the video recordings.
Totest whether the subjects had learned how to buildthe sub-components that were required more thanonce (the snowman and the L shape), we recordedwhether they said yes or no when they were askedif they remembered each of these components dur-ing the construction of the railway signal.3.3.2 Subjective MeasuresIn addition to the above objective measures, wealso gathered a range of subjective measures.
Be-fore the interaction, we asked subjects to rate theircurrent level on a set of 22 emotions (Ortonyet al, 1988) on a scale from 1 to 4; the subjectsthen rated their level on the same emotional scalesagain after the interaction.
After the interaction,the subjects also filled out a user-satisfaction ques-tionnaire, which was based on that used in the userevaluation of the COMIC dialogue system (Whiteet al, 2005), with modifications to address spe-cific aspects of the current dialogue system and theexperimental manipulations in this study.
Therewere 47 items in total, each of which requestedthat the user choose their level of agreement witha given statement on a five-point Likert scale.
Theitems were divided into the following categories:Mean (Stdev) Min MaxLength (sec) 305.1 (54.0) 195.2 488.4System turns 13.4 (1.73) 11 18Response time (sec) 2.79 (1.13) 1.27 7.21Table 1: Dialogue efficiency resultsOpinion of the robot as a partner 21 items ad-dressing the ease with which subjects wereable to interact with the robotInstruction quality 6 items specifically address-ing the quality of the assembly instructionsgiven by the robotTask success 11 items asking the user to rate howwell they felt they performed on the variousassembly tasksFeelings of the user 9 items asking users to ratetheir feelings while using the systemAt the end of the questionnaire, subjects were alsoinvited to give free-form comments.4 ResultsIn this section, we present the results of each ofthe individual dependent measures; in the follow-ing section, we examine the relationship amongthe different types of measures.
These results arebased on the data from 40 subjects: we excludedresults from two subjects for whom the video datawas not clear, and from one additional subject whoappeared to be ?testing?
the system rather thanmaking a serious effort to interact with it.4.1 Objective MeasuresDialogue efficiency The results on the dialogueefficiency measures are shown in Table 1.
Theaverage subject took 305.1 seconds?that is, justover five minutes?to build all three of the objects,and an average dialogue took 13 system turns tocomplete.
When a user made a request, the meandelay before the beginning of the system responsewas about three seconds, although for one user thistime was more than twice as long.
This responsedelay resulted from two factors.
First, prepar-ing long system utterances with several referringexpressions (such as the third and fourth systemturns in Figure 2) takes some time; second, if auser made a request during a system turn (i.e., a?barge-in?
attempt), the system was not able to re-spond until the current turn was completed.882Mean (Stdev) Min MaxRepetition requests 1.86 (1.79) 0 6Failed hand-overs 1.07 (1.35) 0 6Looks at the robot 23.55 (8.21) 14 50Time looking at robot (%) 27 (8.6) 12 51Table 2: Dialogue quality resultsThese three measures of efficiency were cor-related with each other: the correlation betweenlength and turns was 0.38; between length and re-sponse time 0.47; and between turns and responsetime 0.19 (all p < 0.0001).Dialogue quality Table 2 shows the results forthe dialogue quality measures: the two indica-tions of problems, and the two measures of thefrequency with which the subjects looked at therobot?s head.
On average, a subject asked for aninstruction to be repeated nearly two times perinteraction, while failed hand-overs occurred justover once per interaction; however, as can be seenfrom the standard-deviation values, these mea-sures varied widely across the data.
In fact, 18subjects never failed to take an object from therobot when it was offered, while one subject did sofive times and one six times.
Similarly, 11 subjectsnever asked for any repetitions, while five subjectsasked for repetitions five or more times.1 On aver-age, the subjects in this study spent about a quarterof the interaction looking at the robot head, andchanged their gaze to the robot 23.5 times overthe course of the interaction.
Again, there was awide range of results for both of these measures:15 subjects looked at the robot fewer than 20 timesduring the interaction, 20 subjects looked at therobot between 20 to 30 times, while 5 subjectslooked at the robot more than 30 times.The two measures that count problems weremildly correlated with each other (R2 = 0.26, p <0.001), as were the two measures of looking at therobot (R2 = 0.13, p < 0.05); there was no correla-tion between the two classes of measures.Task success Table 3 shows the success rate forassembling each object in the sequence.
Objectsin italics represent sub-components, as follows:the first snowman was constructed as part of thewindmill, while the second formed part of the rail-way signal; the first L-shape was a goal in itself,1The requested repetition rate was significantly affectedby the description strategy used by the dialogue manager; seeFoster et al (2009) for details.Object Rate MemorySnowman 0.76Windmill 0.55L shape 0.90L shape 0.90 0.88Snowman 0.86 0.70Railway signal 0.71Overall 0.72 0.79Table 3: Task success resultswhile the second was also part of the process ofbuilding the railway signal.
The Rate column indi-cates subjects?
overall success at building the rel-evant component?for example, 55% of the sub-jects built the windmill correctly, while both ofthe L-shapes were built with 90% accuracy.
Forthe second occurrence of the snowman and the L-shape, the Memory column indicates the percent-age of subjects who claimed to remember how tobuild it when asked.
The Overall row at the bottomindicates subjects?
overall success rate at buildingthe three main target objects (windmill, L shape,railway signal): on average, a subject built abouttwo of the three objects correctly.The overall correct-assembly rate was corre-lated with the overall rate of remembering objects:R2 = 0.20, p < 0.005.
However, subjects who saidthat they did remember how to build a snowman oran L shape the second time around were no morelikely to do it correctly than those who said thatthey did not remember.4.2 Subjective MeasuresTwo types of subjective measures were gath-ered during this study: responses on the user-satisfaction questionnaire, and self-assessment ofemotions.
Table 4 shows the mean results for eachcategory from the user-satisfaction questionnaireacross all of the subjects, in all cases on a 5-pointLikert scale.
The subjects in this study gave agenerally positive assessment of their interactionswith the system?with a mean overall satisfactionscore of 3.75?and rated their perceived task suc-cess particularly highly, with a mean score of 4.1.To analyse the emotional data, we averaged allof the subjects?
emotional self-ratings before andafter the experiment, counting negative emotionson an inverse scale, and then computed the differ-ence between the two means.
Table 5 shows the re-sults from this analysis; note that this value was as-sessed on a 1?4 scale.
While the mean emotional883Question category Mean (Stdev)Robot as partner 3.63 (0.65)Instruction quality 3.69 (0.71)Task success 4.10 (0.68)Feelings 3.66 (0.61)Overall 3.75 (0.57)Table 4: User-satisfaction questionnaire resultsMean (Stdev) Min MaxBefore the study 2.99 (0.32) 2.32 3.68After the study 3.05 (0.32) 2.32 3.73Change +0.06 (0.24) ?0.55 +0.45Table 5: Mean emotional assessmentsscore across all of the subjects did not change overthe course of the experiment, the ratings of indi-vidual subjects did show larger changes.
As shownin the final row of the table, one subject?s mean rat-ing decreased by 0.55 over the course of the inter-action, while that of another subject increased by0.45.
There was a slight correlation between thesubjects?
description of their emotional state afterthe experiment and their responses to the question-naire items asking for feelings about the interac-tion: R2 = 0.14, p < 0.01.5 Building Performance FunctionsIn the preceding section, we presented results on anumber of objective and subjective measures, andalso examined the correlation among measures ofthe same type.
The results on the objective mea-sures varied widely across the subjects; also, thesubjects generally rated their experience of usingthe system positively, but again with some varia-tion.
In this section, we examine the relationshipamong measures of different types in order to de-termine which of the objective measures had thelargest effect on users?
subjective reactions to thedialogue system.To determine the relationship among the fac-tors, we employed the procedure used in thePARADISE evaluation framework (Walker et al,1997).
The PARADISE model uses stepwise mul-tiple linear regression to predict subjective usersatisfaction based on measures representing theperformance dimensions of task success, dialoguequality, and dialogue efficiency, resulting in a pre-dictor function of the following form:Satisfaction =n?i=1wi ?N (mi)The mi terms represent the value of each measure,while the N function transforms each measureinto a normal distribution using z-score normali-sation.
Stepwise linear regression produces coef-ficients (wi) describing the relative contribution ofeach predictor to the user satisfaction.
If a predic-tor does not contribute significantly, its wi value iszero after the stepwise process.Using stepwise linear regression, we computeda predictor function for each of the subjective mea-sures that we gathered during our study: the meanscore for each of the individual user-satisfactioncategories (Table 4), the mean score across thewhole questionnaire (the last line of Table 4), aswell as the difference between the users?
emo-tional states before and after the study (the last lineof Table 5).
We included all of the objective mea-sures from Section 4.1 as initial predictors.The resulting predictor functions are shown inTable 6.
The following abbreviations are used forthe factors that occur in the table: Rep for the num-ber of repetition requests, Turns for the number ofsystem turns, Len for the length of the dialogue,and Mem for the subjects?
memory for the com-ponents that were built twice.
The R2 column in-dicates the percentage of the variance that is ex-plained by the performance function, while theSignificance column gives significance values foreach term in the function.Although the R2 values for the predictor func-tions in Table 6 are generally quite low, indicat-ing that the functions do not explain most of thevariance in the data, the factors that remain afterstepwise regression still provide an indication asto which of the objective measures had an effecton users?
opinions of the system.
In general, userswho had longer interactions with the system (interms of system turns) and who said that they re-membered the robot?s instructions tended to givethe system higher scores, while users who askedfor more instructions to be repeated tended to giveit lower scores; for the robot-as-partner questions,the length of the dialogue in seconds also made aslight negative contribution.
None of the other ob-jective factors contributed significantly to any ofthe predictor functions.6 DiscussionThat the factors included in Table 6 were the mostsignificant contributors to user satisfaction is notsurprising.
If a user asks for instructions to be re-884Measure Function R2 SignificanceRobot as partner 3.60+0.53?N (Turns)?0.39?N (Rep)?0.18?N (Len) 0.12 Turns: p < 0.01,Rep: p < 0.05,Length: p?
0.17Instruction quality 3.66?0.22?N (Rep) 0.081 Rep: p < 0.05Task success 4.07+0.20?N (Mem) 0.058 Mem: p?
0.07Feelings 3.63+0.34?N (Turns)?0.32?N (Rep) 0.044 Turns: p?
0.06, Rep:p?
0.08Overall 3.73?0.36?N (Rep)+0.31?N (Turns) 0.062 Rep: p < 0.05,Turns: p?
0.06Emotion change 0.07+0.14?N (Turns)+0.11?N (Mem)?0.090?N (Rep) 0.20 Turns: p < 0.05,Mem: p < 0.01,Rep: p?
0.17Table 6: Predictor functionspeated, this is a clear indication of a problem inthe dialogue; similarly, users who remembered thesystem?s instructions were equally clearly havinga relatively successful interaction.In the current study, increased dialogue lengthhad a positive contribution to user satisfaction; thiscontrasts with results such as those of Litman andPan (2002), who found that increased dialoguelength was associated with decreased user satis-faction.
We propose two possible explanations forthis difference.
First, the system analysed by Lit-man and Pan (2002) was an information-seekingdialogue system, in which efficient access to theinformation is an important criterion.
The currentsystem, on the other hand, has the goal of joint taskexecution, and pure efficiency is a less compellingmeasure of dialogue quality in this setting.
Sec-ond, it is possible that the sheer novelty factor ofinteracting with a fully-embodied humanoid robotaffected people?s subjective responses to the sys-tem, so that subjects who had longer interactionsalso enjoyed the experience more.
Support for thisexplanation is provided by the fact that dialoguelength was only a significant factor in the more?subjective?
parts of the questionnaire, but did nothave a significant impact on the users?
judgementsabout instruction quality or task success.
Otherstudies of human-robot dialogue systems have alsohad similar results: for example, the subjects in thestudy described by Sidner et al (2005) who useda robot that moved while talking reported higherlevels of engagement in the interaction, and alsotended to have longer conversations with the robot.While the predictor functions give useful in-sights into the relative contribution of the objectivemeasures to the subjective user satisfaction, theR2 values are generally lower than those found inother PARADISE-style evaluations.
For example,Walker et al (1998) reported an R2 value of 0.38,the values reported by Walker et al (2000) on thetraining sets ranged from 0.39 to 0.56, Litman andPan (2002) reported an R2 value of 0.71, whilethe R2 values reported by Mo?ller et al (2008)for linear regression models similar to those pre-sented here were between 0.22 and 0.57.
The lowR2 values from this analysis clearly suggest that,while the factors included in Table 6 did affectusers?
opinions?particularly their opinion of therobot as a partner and the change in their reportedemotional state?the users?
subjective judgementswere also affected by factors other than those cap-tured by the objective measures considered here.In most of the previous PARADISE-style stud-ies, measures addressing the performance of theautomated speech-recognition system and otherinput-processing components were included in themodels.
For example, the factors listed by Mo?lleret al (2008) include several measures of word er-ror rate and of parsing accuracy.
However, the sce-nario that was used in the current study requiredminimal speech input from the user (see Figure 2),so we did not include any such input-processingfactors in our models.Other objective factors that might be relevantfor predicting user satisfaction in the current studyinclude a range of non-verbal behaviour from theusers.
For example, the user?s reaction time to in-structions from the robot, the time the users needto adapt to the robot?s movements during hand-over actions (Huber et al, 2008), or the time takenfor the actual assembly of the objects.
Also, othermeasures of the user?s gaze behaviour might be885useful: more global measures such as how oftenthe users look at the robot arms or at the objects onthe table, as well as more targeted measures exam-ining factors such as the user?s gaze and other be-haviour during and after different types of systemoutputs.
In future studies, we will also gather dataon these additional non-verbal behaviours, and weexpect to find higher correlations with subjectivejudgements.7 Conclusions and Future WorkWe have presented the JAST human-robot dia-logue system and described a user study in whichthe system instructed users to build a series of tar-get objects out of wooden construction toys.
Thisstudy resulted in a range of objective and subjec-tive measures, which were used to derive perfor-mance functions in the style of the PARADISEevaluation framework.
Three main factors werefound to affect the users?
subjective ratings: longerdialogues and higher recall performance were as-sociated with increased user satisfaction, while di-alogues with more repetition requests tended to beassociated with lower satisfaction scores.
The ex-plained variance of the performance functions wasgenerally low, suggesting that factors other thanthose measured in this study contributed to theuser satisfaction scores; we have suggested severalsuch factors.The finding that longer dialogues were associ-ated with higher user satisfaction disagrees withthe results of many previous PARADISE-styleevaluation studies.
However, it does confirm andextend the results of previous studies specificallyaddressing interactions between users and embod-ied agents: as in the previous studies, the users inthis case seem to view the agent as a social entitywith whom they enjoy having a conversation.A newer version of the JAST system is currentlyunder development and will shortly undergo a userevaluation.
This new system will support an ex-tended set of interactions where both agents knowthe target assembly plan, and will will also in-corporate enhanced components for vision, objectrecognition, and goal inference.
When evaluat-ing this new system, we will include similar mea-sures to those described here to enable the eval-uations of the two systems to be compared.
Wewill also gather additional objective measures inorder to measure their influence on the subjectiveresults.
These additional measures will includethose mentioned at the end of the preceding sec-tion, as well as measures targeted at the revisedscenario and the updated system capabilities?forexample, an additional dialogue quality measurewill assess how often the goal-inference systemwas able to detect and correctly respond to an errorby the user.AcknowledgementsThis research was supported by the Euro-pean Commission through the JAST2 (IST-FP6-003747-IP) and INDIGO3 (IST-FP6-045388)projects.
Thanks to Pawel Dacka for his help inrunning the experiment and analysing the data.ReferencesM.
Argyle and J.
A. Graham.
1976.
The Cen-tral Europe experiment: Looking at persons andlooking at objects.
Environmental Psychologyand Nonverbal Behavior, 1(1):6?16.
doi:10.1007/BF01115461.A.
J. N. van Breemen.
2005. iCat: Experimentingwith animabotics.
In Proceedings of the AISB2005 Creative Robotics Symposium.C.
Callison-Burch, M. Osborne, and P. Koehn.2006.
Re-evaluating the role of BLEU in ma-chine translation research.
In Proceedings ofEACL 2006.
ACL Anthology E06-1032.M.
E. Foster.
2008.
Automated metrics that agreewith human judgements on generated output foran embodied conversational agent.
In Proceed-ings of INLG 2008.
ACL Anthology W08-1113.M.
E. Foster, M. Giuliani, A. Isard, C. Matheson,J.
Oberlander, and A. Knoll.
2009.
Evaluatingdescription and reference strategies in a coop-erative human-robot dialogue system.
In Pro-ceedings of IJCAI 2009.M.
Huber, M. Rickert, A. Knoll, T. Brandt, andS.
Glasauer.
2008.
Human-robot interaction inhanding-over tasks.
In Proceedings of IEEERO-MAN 2008. doi:10.1109/ROMAN.2008.4600651.C.
Y. Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In Proceedingsof the ACL 2004 Workshop on Text Summariza-tion.
ACL Anthology W04-1013.2http://www.euprojects-jast.net/3http://www.ics.forth.gr/indigo/886D.
J. Litman and S. Pan.
2002.
Designing andevaluating an adaptive spoken dialogue sys-tem.
User Modeling and User-Adapted Inter-action, 12(2?3):111?137.
doi:10.1023/A:1015036910358.S.
Mo?ller, K.-P. Engelbrecht, and R. Schleicher.2008.
Predicting the quality and usability ofspoken dialogue systems.
Speech Communica-tion, 50:730?744.
doi:10.1016/j.specom.2008.03.001.D.
G. Novick.
1997.
What is effective-ness?
In Working notes, CHI ?97 Work-shop on HCI Research and Practice AgendaBased on Human Needs and Social Responsi-bility.
http://www.cs.utep.edu/novick/papers/eff.chi.html.A.
Ortony, G. L. Clore, and A. Collins.
1988.
TheCognitive Structure of Emotions.
CambridgeUniversity Press.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.2002.
BLEU: A method for automatic evalua-tion of machine translation.
In Proceedings ofACL 2002.
ACL Anthology P02-1040.M.
Rickert, M. E. Foster, M. Giuliani, T. By,G.
Panin, and A. Knoll.
2007.
Integrating lan-guage, vision and action for human robot dialogsystems.
In Proceedings of HCI International2007.
doi:10.1007/978-3-540-73281-5_108.C.
L. Sidner, C. Lee, C. D. Kidd, N. Lesh, andC.
Rich.
2005.
Explorations in engagementfor humans and robots.
Artificial Intelligence,166(1?2):140?164.
doi:10.1016/j.artint.2005.03.005.M.
Walker, C. Kamm, and D. Litman.
2000.
To-wards developing general models of usabilitywith PARADISE.
Natural Language Engineer-ing, 6(3?4):363?377.M.
A. Walker.
2000.
An application of reinforce-ment learning to dialogue strategy selection ina spoken dialogue system for email.
Journal ofArtificial Intelligence Research, 12:387?416.M.
A. Walker, J. Fromer, G. D. Fabbrizio, C. Mes-tel, and D. Hindle.
1998.
What can I say?
: Eval-uating a spoken language interface to email.In Proceedings of CHI 1998. doi:10.1145/274644.274722.M.
A. Walker, D. J. Litman, C. A. Kamm, andA.
Abella.
1997.
PARADISE: A framework forevaluating spoken dialogue agents.
In Proceed-ings of ACL/EACL 1997.
ACL Anthology P97-1035.M.
White, M. E. Foster, J. Oberlander, andA.
Brown.
2005.
Using facial feedback to en-hance turn-taking in a multimodal dialogue sys-tem.
In Proceedings of HCI International 2005.887
