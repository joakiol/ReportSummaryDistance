RECOGNIZING AND RESPONDING TO PLAN-ORIENTED MISCONCEPTIONSAlex Quil iciMichael  G. DyerMargot  F lowersArtificial Intelligence LaboratoryComputer Science DepartmentUniversity of CaliforniaLos Angeles, CA 90024This paper discusses the problem of recognizing and responding to plan-oriented misconceptions inadvice-seeking dialogs, concentrating on the problems of novice computer users.
A cooperative responseis one that not only corrects the user's mistaken belief, but also addresses the missing or mistaken userbeliefs that led to it.
Responding appropriately to a potentially incorrect user belief is presented as aprocess of 1. checking whether the advisor holds the user's belief; 2. confirming the belief as amisconception by finding an explanation for why the advisor does not hold this belief; 3. detecting themistaken beliefs underlying the misconception by trying to explain why the user holds the incorrectbelief, and 4. providing these explanations to the user.
An explanation is shown to correspond to a setof advisor beliefs, and searching for an explanation to proving whether various abstract configurationsof advisor beliefs hold.
A taxonomy of domain-independent explanations for potential user misconcep-tions involving plan applicability conditions, preconditions, and effects is presented.1 INTRODUCTIONA novice computer user having trouble performing atask often describes his problem to an expert, who isthen expected to explain its cause and provide itssolution.
Consider, for example, the following advice-seeking dialog between a novice UNIX user and hismore experienced counterpart.User: I tried to remove a file with the " rm"  com-mand.
But the file was not removed and the errormessage was permission denied.
I checked and Iown the file.
What's wrong?Advisor: To remove a file, you need to be able towrite into the directory containing it.
You do notneed to own the file.User: How do I make that directory writeable?Advisor: To make a directory writeable, use the"chmod +w"  command.User: Wait a second while I try it.
No, "chmod +w"prints the error message "permission denied".Advisor: To make a directory writeable, you need toown it.User: So to remove a file, I have to own the directorythat contains it.Advisor: No, to remove a file, you need to have writepermission on the directory that contains it.
Youdo not need to own the directory that contains it.You need to own that directory when you do notalready have write permission on it.User: So how do I remove the file?Advisor: Send mail to whomever has write permis-sion on the directory, asking him to remove the filefor you.Participating as the advisor in such a dialog requires theability to recognize and respond to missing or mistakenuser beliefs about plan applicability conditions, precon-ditions, and effects.
The advisor above recognizes twouser misconceptions.
The user first incorrectly believesthat owning a file is a precondition to removing it, andthen incorrectly believes that the precondition is owningthe directory containing it.
This advisor also noticesseveral gaps in the user's knowledge.
The user has noplan for making a directory writeable, does not knowwhy the advisor's plan for doing so failed, and has noplan for removing a file when the directory in which thefile resides is not writeable.A cooperative advisor response to a missing userCopyright 1988 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/88/0100e-?$03.0038 Computational Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsbelief simply provides that belief.
The advisor aboveprovides the missing beliefs that the plan for making adirectory writeable is to use the "chmod +w"  com-mand, that owning a directory is a precondition tomaking it writeable, and that the plan for removing a filewhen all else fails is to send mail requesting its removalto some person who has write permission on the direc-tory in which it resides.However, for a mistaken belief the advisor must notonly point out that the belief is incorrect and provide acorrection, but must also address the missing or mis-taken beliefs that are the source of this misconception.Above, this is done by pointing out that the actualprecondition is being able to write into the directory thatcontains it, and by explaining that owning that directoryis necessary only if it is not already writeable.In this paper we examine the problem of detectingand responding to plan-oriented user misconceptions.This problem can be broken into several subproblems:1.
Mapping the user's natural language problem de-scription into a set of user beliefs.2.
Determining which of these beliefs are incorrect.3.
Inferring the missing or mistaken user beliefs thatmight have led to these incorrect beliefs.4.
Selecting the advisor beliefs to present o the user asa conversationally cooperative response.5.
Mapping these advisor beliefs into a natural anguageresponse.Here we provide a computational model of (2), (3), and(4).
The input is a set of potentially incorrect userbeliefs.
The output is a set of advisor beliefs to presentto the user which correct any mistaken user beliefs andaddress the missing or mistaken user beliefs that mayhave led to them.
We consider three types of userbeliefs: those involving plan applicability conditions(whether a particular plan should be used to achieve agoal), enablements (whether a particular state mustexist before a plan can achieve a goal), and effects(whether a state will exist as a result of a plan'sexecution).2 AN EXPLANATION-BASED APPROACHHow can an advisor determine whether a particular userbelief is mistaken and understand how the user came tobelieve it?
And furthermore, how can the advisor de-termine the contents of a cooperative response to theuser's misconception?An advisor presented with a user belief must doseveral things.
He must first determine whether heshares the user's belief.
If he does, it is clearly not amisconception.
Assuming that he does not share thatbelief, the advisor must confirm that it is, in fact, amisconception, and then decide which user beliefs ledto it.
(If the advisor cannot confirm that the user's beliefis mistaken, it could become a new advisor belief.
)We suggest an explanation-based approach to ac-complish these tasks.
To confirm that the user's belief isa misconception, the advisor tries to find an explanationfor why he does not hold the user's belief.
To infer theproblematic user beliefs underlying the user's mistakenbelief, the advisor tries to find an explanation for whythe user does hold this belief.
These two explanationsconstitute the advisor's response to the user.We illustrate our approach by showing how theadvisor arrives at the response found in this exchangefrom our introductory dialog.User: So to remove a file, I have to own the directorythat contains it.Advisor: No, to remove a file, you need to have writepermission on the directory that contains it.
Youdo not need to own the directory that contains it.You need to own that directory when you do notalready have write permission on it.Here the user's belief is that a file cannot be removedwithout owning the directory in which it resides.The advisor first tries to verify that he holds theuser's belief.
In this case he cannot, so he must now tryto determine the reason why he does not hold thisbelief.
The explanation the advisor finds is that theuser's belief is contradicted by his belief that a file canbe removed without owning the directory in which itresides, and that the user's belief can be replaced by hisbelief that a file cannot be removed without writepermission on that directory.At this point the advisor has confirmed that theuser's belief is a misconception.
Now he must try todiscover which user beliefs led to this error.
To do so,the advisor tries to understand why the user holds thiserroneous belief.
His explanation is that the user isunaware that to remove a file it is really only necessaryto have write permission on the directory containing it,and that owning that directory is necessary only if onedoes not already have write permission on it.
In otherwords, the user is unaware that owning a directory isnot a precondition for removing a file, but a precondi-tion for achieving one of its preconditions.Once the advisor finds these explanations, he pres-ents them to the user as the response to his misconcep-tion.
The response corrects the user's misconception bypointing out that the user's claimed precondition forremoving a file is incorrect, by providing the actualprecondition for removing a file, and by providing themissing user beliefs that led to the user's misconcep-tion.2.1 OTHER WORK IN EXPLANATION-BASEDUNDERSTANDINGOur approach derives from work in explanation-basedstory understanding (Schank 1986, Dyer 1983, Wilensky1983, 1978, Cullingford 1978, Schank and Abelson1977).
The basic idea is that to understand a particularinput, such as a person's action, we have to explain whyit has occurred.
One way to find an explanation for aperson's action is to relate it to known goals the personComputational Linguistics, Volume 14, Number 3, September 1988 39Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsis trying to achieve.
Suppose, for example, that a storyunderstander reads that a hungry character bought arestaurant guidebook (Wilensky 1983).
One explanationfor this action is that hungry people want to eat, to eatyou have to be near food, to be near food you haveknow where it is and then go there.
The guidebook sayswhere the food is.
This explanation can be constructedeither by using rules to build a reasoning chain or byapplying pre-existing schemas that capture the relation-ship.Our task may be thought of as trying to understandwhy an actor (either the user or advisor) does or doesnot hold a particular belief, a task similar to that facedby explanation-based story understanders.
Because ofthe similarities in our tasks, we use the same approach,trying to construct a potential explanation for the beliefswe are trying to understand.2.2 THE REST OF THE PAPERSubsequent sections of the paper present our approachin more detail.
First, we describe the representation weuse to represent plan-oriented user and advisor beliefs.Then, we examine the process by which the necessaryexplanations are found and provide a taxonomy ofexplanations for the types of beliefs we consider.
Fi-nally, we show how our approach compares with otherwork in detecting and correcting user misconceptions.belief(user, R) Advisor believes that usermaintains Rbelief(advisor, R) Advisor believes that advisormaintains RIn this paper we do not discuss beliefs involving otherrelationships, such as a belief that an object has aparticular property.
In addition, for readability we donot use the belief predicate here, but instead precede alist of planning relationships with either "the userbelieves" or "the advisor believes".3.11 REPRESENTING PLANNING RELATIONSHIPSThe planning relation can be one of the relations be-tween actions and states hown below.
Here A denotesan action, which is either a primitive operator whoseexecution results in a set of state changes, or a plan,which is a sequence of these operators.
S, S1, and $2denote states, which are descriptions of properties ofobjects.causes(A,S)!causes(A,S)enables(S1,A,S2)!enables(S1,A,S2)Executing A has an effect SExecuting A does not have effectSS1 is necessary for A to have $2as an effectSI is unnecessary for A to have$2 as an effect3 REPRESENTING USER AND ADVISOR BELIEFSThe mistaken user beliefs that we consider involve planapplicability conditions, enablements, and effects.
Inthis section we describe how these beliefs are repre-sented.
In essence, we make use of existing frameworksfor representing planning knowledge, except hat we arecareful to distinguish between user and advisor beliefs.Traditional planning systems (Fikes and Nilsson1974, Sacerdoti 1974) represent an agent's planningknowledge as a data base of operators associated withapplicability conditions, preconditions, and effects.Since these systems have only one agent, the planner,the entries in the data base are implicitly assumed torepresent that agent's beliefs.
However, because usermisconceptions occur when the user's planning knowl-edge differs from the advisor's, systems that deal withuser misconceptions must explicitly distinguish be-tween advisor beliefs about what the user knows andadvisor beliefs about what the advisor knows.Our representation forbeliefs (Abelson 1973, 1979) issimilar to that used by existing systems that keep trackof the possibly contradictory knowledge of multipleparticipants (Alvarado 1987; Alvarado, Dyer, and Flow-ers 1986; Flowers, McGuire, and Birnbaum 1982; Pol-lack 1986).
A belief relation represents an advisor'sbelief that an actor maintains that a particular planapplicability condition, precondition, or effect holds.The actor is either the user or the advisor.applies(A,S)!applies(A,S)precludes(S 1,$2)!precludes(S 1 ,$2)goal(A,S)These relationshipsA is a correct or normal plan forachieving oal state SA is not a plan for achieving SS1 and $2 cannot existsimultaneouslyS1 and $2 can existsimultaneouslyActor A wants to achieve Sare derived from existing represen-tations.
SPIRIT's (Pollack 1986) representation forplanning knowledge uses gen to represent a state result-ing in an action and cgen to represent a state resulting inan action only if some other state exists.
Causes andenables are identical in semantics to gen and cgen.Applies, which has no analog in SPIRIT, is similar to theintends relation in BORIS (Dyer 1983).
The differencebetween causes and applies is in whether the action isintended to cause the state that results from its execu-tion to exist.
"Causes" represents cause-effect rela-tions which are nonintentional, while "applies" repre-sents a cause-effect relation between an action(sequence) or plan which is intended to achieve adesired state (a goal).
An action causes a state wheneverthe state results from its execution.
An action applies toa state when an actor believes the action will cause thedesired state to occur.40 Computational Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented MisconceptionsTo see why this distinction is necessary, considertwo actions that can be used by a user who wants toremove one of his files: typing " rm"  followed by thefile's name, and typing "rm *".
Both have removing thefile as one of their effects, but the latter also causes allother files to be removed as well, an effect hat is not theuser's goal.
Only "rm file" applies to removing a file,although both actions have an effect that causes the fileto be removed.To further illustrate the semantics of these relations,we show how they can be used to represent the firstexchange in our example dialog.User: I tried to remove a file with the " rm"  com-mand.
But the file was not removed and the errormessage was permission denied.
I checked and Iown the file.
What's wrong?Advisor: To remove a file, you need to be able towrite into the directory containing it.
You do notneed to own the file.Three of the user's beliefs in this exchange are: 1. the" rm"  command is used when one wants to remove afile; 2. one has to own a file to remove it, and 3. an errormessage resulted when the plan was executed.
In termsof these planning relations, the user's beliefs are:applies(using " rm fi le", the file's removal)enables(owning the file, using "rm file",the file's removal)causes(using " rm"  on the user's file,an error message)The advisor holds several similar beliefs, except hat hebelieves that to remove a file it is necessary to havewrite permission on the directory containing it.
In termsof the planning relationships, the advisor's beliefs are:applies(using " rm fi le", the file's removal)enables(directory write permission, using " rm" ,the file's removal)causes(using " rm"  on the user's file,an error message)(The paper is not concerned with representing notionssuch as "the file's removal" or "write permission onthe directory containing the file".
The details of therepresentation for such things may be found in Quilici(1985).
)The user and advisor in this exchange share onebelief that we have not represented.
This belief is thatusing " rm"  did not cause the user's file to be removed.To represent beliefs that a state did not result from anaction, that a plan is not applicable to a goal, or that astate is not an enablement condition of an action havinganother state as a result, we use !causes, !applies, and/enables, respectively.
The belief above is representedwith !causes, a belief that "mkdir"  is not used toremove a file is represented with !applies, and a beliefthat " rm"  does not require owning the directory con-taining the file is represented with !enables.
!causes(using " rm"  on the user's file,the file's removal)!applies(using "mkdir f i le", the file's removal)!enables(owning the directory, using " rm" ,the file's removal)It is also necessary to be able to represent the notionthat a state's existence caused a planning failure.
Con-sider the following exchange:User: I accidentally hit the up arrow key and itdeleted 20 unanswered mail messages.
How can Iget them back?Advisor: Hitting the up arrow does not delete yourmessages, but does result in your being discon-nected from the etherplexer.
You could not accessyour mail messages because they were moved to"mbox" .
The mail program requires that yourmail messages be in "mai lbox".Here the advisor believes that the user's mail messagesare inaccessible because they are not in the location themail program expects them to be.
The belief that themail program requires the mail messages to be in the file"mai lbox" can be represented using "enables".
Theadvisor's belief that the mail messages being in the file"mbox"  prevents the mail program from accessing isrepresented with precludes, which captures the notionthat two states are mutually exclusive.enables(messages in "mai lbox",  use "mai l" ,display messages)precludes(messages in "mbox" ,messages in "mai lbox")"precludes" and "!precludes" relations between statescan be inferred using rules such as "an object cannot bein two places at once.
"The one other relation we find useful is goal, which isused in representing a belief that an actor wants toachieve a particular state.
In the example above, theadvisor believes that one goal of the user is accessinghis mail messages.
The advisor's belief is:goal(user, access user's mail messages)Most user modeling systems use a similar relation toexplicitly represent that a state is a user's goal.3.2 SUMMARY OF THE REPRESENTATIONThe main focus of our work is in trying to detect andrespond to user misconceptions.
To do so, it is neces-sary to have some representation for user and advisorplanning knowledge.
Our representation is based onthat used by traditional planning systems.
The mostimportant difference is that we take care to distinguishbetween things the advisor believes and things theadvisor thinks the user believes.
We also distinguishComputational Linguistics, Volume 14, Number 3, September 1988 41Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsbetween actions that are intended to achieve a state andactions that happen to have a particular state as one oftheir effects.
And we find it necessary to representbeliefs that two states cannot exist at the same time andthat achieving a particular state is a goal of the user.4 EXPLANATION-BASED MISCONCEPTION RECOGNITIONAND RESPONSEOur approach to recognizing and responding to a poten-tially incorrect user belief revolves around the advisortrying to do several things.
First, the advisor tries toverify that he does not share the user's belief.
Next, theadvisor tries to confirm that the user's belief is amisconception.
The advisor does so by finding anexplanation for why he does not share the user's belief.After the belief is confirmed as a misconception, theadvisor tries to detect its source.
He does this by findinga potential explanation for why the user holds thatincorrect belief, based on a taxonomy of abstract expla-nation classes.
Finally, the advisor presents these ex-planations to the user as a cooperative response.But what exactly is an explanation?
And what knowl-edge does the advisor need to find one?
And finally,how is an explanation found?4.1 EXPLANATIONS AS SETS OF BELIEFSAn explanation is a set of advisor beliefs that accountsfor why a particular belief is or is not held.
An advisor,presented with a potentially incorrect user belief, has tofind two explanations.The first explanation confirms that the user's belief isa misconception.
To find this explanation the advisortries to find a set of advisor beliefs that justify his notholding the user's belief.
For instance, the user in ourearlier example had an incorrect belief that owning adirectory is a precondition for removing a file.enables(own directory, use "rm file",the file's removal)Two advisor beliefs constitute an explanation for whythe advisor does not hold this belief.
The first is theadvisor's contradictory belief that owning a directory isnot a precondition for removing a file.
The other is hisbelief that the actual precondition is write permission onthe directory containing the file.
!enables(own directory, use "rm file",the file's removal)enables(writeable directory, use "rm file",the file's removal)These two beliefs confirmed that the user's belief wasmistaken.The other explanation explains why the user holdsthis incorrect belief.
To find this explanation the advisortries to find a set of advisor beliefs that capture thesource of the user's misconception.
Two advisor beliefsprovide a possible explanation for the incorrect userbelief above.
The first is that one has to own a directoryto make it writeable.
The other is that having a writeabledirectory is the precondition to removing a file.enables(own directory, use "chmod",obtain writeable directory)enables(writeable directory, use " rm",the file's removal)The user's not sharing these advisor beliefs explains theuser's misconception, which is that the user does notrealize that owning a directory is merely a preconditionto obtaining write permission on the directory, which isthe actual precondition to removing the file.4.2 REQUIRED ADVISOR KNOWLEDGETo find an explanation the advisor must have threetypes of knowledge: 1. a set of domain-specific beliefs;2. a set of rules for inferring additional beliefs, and 3. aset of abstract explanation patterns.
All of these mustcome from past advisor experience or past advisorinteraction with users.
However, here we simply as-sume their existence and leave understanding how theyare obtained for future research.The first type of required knowledge is a set ofdomain-specific beliefs about plan applicability condi-tions, preconditions, and effects.
Examples of theseinclude beliefs that " rm"  is used to remove a file, andthat it is necessary to have write permission on thedirectory containing the file.
Without these types ofbeliefs it would be impossible for the advisor to correctuser misconceptions about he preconditions for remov-ing a file.
This category of knowledge includes beliefssuch as a belief that " rm"  is not used to remove adirectory.
These negated beliefs--!applies, !enables,!causes, and so on--are especially useful in detectingmisconceptions.
An advisor, with the explicit belief that" rm" is not applicable to removing a directory, cantrivially detect hat a user belief that " rm"  is applicableto removing a directory is incorrect.These domain-specific beliefs are assumed to derivefrom past advisor experiences.
An advisor who success-fully uses " rm" to remove a file will believe that using" rm" is applicable to the goal of removing a file.
Anadvisor who uses " rm"  to try to remove a directory andhas it fail will believe that " rm"  is not applicable toremoving a directory.
The negated beliefs correspond tothe bug lists kept by many tutoring and planning sys-tems (Anderson, Boyle, and Yost 1985; Brown andBurton 1978; Burton 1982; Stevens, Collins, and Goldin1982).The second type of advisor knowledge is a set ofrules that help infer negated domain-specific beliefs,such as a belief that a particular action does not result ina particular state, or that a given plan is not useful for aparticular goal.
These rules are needed because theadvisor cannot be expected to have a complete set ofthese beliefs.
One such rule, for example, suggests that42 Computatiottal Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions" i f  a state S is not among the known states that resultfrom an action A's execution, assume that A is notapplicable to achieving S."  There are similar rules forthe other types of beliefs.The third and final type of knowledge is a taxonomyof potential explanations for why an actor might ormight not hold a belief.
Each type of planning relation--applies, enables, and effects--is associated with twosets of potential explanations.
One set provides reasonswhy an actor might hold a particular belief involvingthat planning relation.
The other set provides reasonswhy an actor might not.The inference rules and potential explanations differfor each type of planning relation.
Associated with eachtype of planning relation is:1. a set of rules for inferring its negation (which proveuseful in finding explanations for why the belief is oris not held),2. a potential explanation for why an actor does nothold a belief involving that planning relationship, and3.
a set of potential explanations for why an actor doeshold a belief involving that planning relationship.For example, applies is associated with a set of rules forinferring that an actor holds a particular !applies belief,a potential explanation for why an actor does not hold agiven applies belief, and a set of potential explanationsfor why an actor does hold a given applies belief.5 POTENTIAL EXPLANATIONSThe advisor must be able to find a reason for why aparticular belief is or is not held.
One way to do so is1.
classify the belief, and 2. try to verify one of thepotential explanations associated with that class ofbelief.
A potential explanation is an abstract pattern ofplanning relationships.
The idea is that to verify apotential explanation, the advisor tries to prove, eitherby memory search or by deductive reasoning, that eachof these planning relationships hold.There are two types of potential explanations.
Thefirst explains why an actor does not hold a belief.
Theother explains why an actor does.
In this section wedescribe the potential explanations associated with theplanning relationships we have examined.
The followingsection discusses in detail how they are used.5.1 POTENTIAL EXPLANATIONS FOR NOT HOLDINGA BELIEFThe potential explanations for why the advisor does nothold an instance of one the plan-oriented beliefs areshown below.
Each of these potential explanationssuggests that to confirm that a user's belief is a miscon-ception, the advisor must try to verify that one of hisbeliefs contradicts the user's belief, and that one of hisbeliefs can replace it.
The only difference between thepotential explanations is in the type of belief beingcontradicted or replaced.Unshared Potential EnglishUser Belief Exp lanat ion  Descriptionapplies(Ap, Sg) !applies(Ap,Sg) Plan is not used toachieve Goalapplies(A,Sg) Other plan is used toachieve Goalenables(Sp,Ap,Sg) !enables(Sp,Ap,Sg) State is not preconditionof Actionenables(S,Ap,Sg) Other state isprecondition fActioncauses(Ap,Sp) !causes(Ap,Sp) Action does not causestatecauses(A,Sp) Other action does causestateConsider our earlier example in which the user's beliefis that a precondition of removing a file is owning thedirectory containing it.
The potential explanation sug-gests trying to prove that the advisor holds two beliefs:that owning a directory is not a precondition of remov-ing a file, and that some other state is.
Here, the advisorfinds that he believes that owning a directory is not aprecondition of removing a file (either by finding thatrelationship in his knowledge base or by deducing it).The advisor also finds that directory write permission isa precondition of removing a file.
These beliefs explainwhy the advisor does not hold the user's belief, con-firming it as a misconception.A similar process is used to confirm that the advisordoes not hold a user's applies or causes belief.
Considerthe following exchange:User: I tried to display my file with the " Is"  com-mand but it just printed the file's name.Advisor: The " Is"  command is not used to displaythe contents of files, the "more"  command is."
Is"  is used to list the names of your files.The user's potentially incorrect belief is that " Is"  isapplicable to achieving the goal of displaying a file'scontents.
The potential explanation for why an advisordoes not hold this belief is that the advisor does notbelieve that using " Is"  is applicable to this user's goal,and that using " Is"  is applicable to some other goal.
Sothe advisor tries to verify (again, by either search ordeduction) that " Is"  is not applicable to displaying thefile's contents, and he tries to verify that some otherplan does.
Here the advisor finds that "more"  is usedinstead.Finally, consider the following exchange:User: I deleted a file by typing " remove" .Advisor: No, typing " remove"  did not delete yourfile.
Typing " rm"  deleted it.
Typing " remove"cleans up your old mail messages.The user's potentially mistaken belief is that typingremove results in a file being deleted.
The potentialexplanation for why the advisor does not share thisbelief is that the advisor instead believes that typing" remove"  does not result in a file being deleted and thatsome other action does.
The advisor verifies that typing" remove"  does not cause a file to be deleted and that" rm"  is an action that does.Computational Linguistics, Volume 14, Number 3, September 1988 43Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions5.2 EXPLANATIONS FOR HOLDING A BELIEFThe potential explanations we have examined so farexplain why an actor does not hold a particular belief.There are also potential explanations for why an actordoes hold an incorrect belief.
We now present a taxon-omy of these explanations for each of the three types ofbeliefs.5.2.1 EXPLANATIONS FOR INCORRECT APPLIESThere are four potential explanations for why a userholds an incorrect applies belief of the form applies(Ap,Sp).
Recall that to recognize that this type of user beliefis incorrect the advisor found two beliefs of the form!applies(Ap, Sp) and applies(A, Sp).
Here are the po-tential explanations along with English descriptions foreach.Class Potential EnglishOf Mistake Explanation DescriptionPlan !causes(Ap,Sp) No effect achieves goalAchievesDifferent applies(Ap,S) Plan applies to other goalGoalPlan Missing !causes(Ap,S) User plan does not an effectEffect causes(A,S) that other plan hasUnachievable enables(S,Ap,Sp) Some state enables PlanPlanEnablement !causes(A,Sp) No action achieves this statePlan Thwarts causes(Ap,S) User plan has effectUser Goal precludes(S, p) that thwarts the user's goalgoal(user, Sp)!causes(Ap,S) Advisor plan does not havethat effectThe first, "P lan Achieves Different Goal" ,  explainsone of our earlier examples.
The explanation is that theuser is unaware that his plan does not have an effect thatachieves his goal, and that his plan is, in fact, used toachieve some other goal.User: I tried to display my file with the " I s "  com-mand but it just printed the file's name.Advisor: The " I s "  command is not used to displaythe contents of files, the "more"  command is."
I s "  is used to list the names of your files.The user's incorrect belief that using " I s "  displays a filearises because the user is unaware of two things.
Thefirst is that using " I s "  does not display the contents offiles; the other is that " I s "  is applicable to listing thenames of files.The second, "Plan Missing Ef fect" ,  suggests that theuser is unaware that his plan P1 does not have one of theeffects that the plan P2 (that achieves his goal) has.User: I tried to remove my directory and I got anerror message "directory not empty" .
But " I s "didn't list any files.Advisor: Use "Is -a"  to list all of your files. "
I s "cannot be used to list all of your files because " I s "does not list those files whose names begin with aperiod.The user's mistaken belief is that " I s "  should be used tolist all file names.
This belief arises because the user isunaware that " I s "  does not have an effect that causes itto list files whose names begin with a period, an effectthat the correct plan (Is -a) has.The third, "Unachievable Plan Enablement" ,  sug-gests that the user is unaware his plan will not workbecause there is no plan to achieve one of its enable-ments.User: So to read Margot's mail, all I have to do is"more  f lowers/mail".Advisor: No, only " f lowers"  can read her mail.The user mistakenly believes that his plan of using"more"  to examine Margot's mail file will allow him toread her mail.
The advisor believes that "more"  has aneffect of displaying a user's mail, that one of its enable-ments is that you have to be that particular user, andthat no plan has an effect that achieves this enablement.The last, "P lan Thwarts User Goal" ,  suggests thatthe user is unaware that another plan achieves theuser's goal without an additional effect that the user'splan has.User: To list files whose names begin with a number,I pipe " I s "  to "grep \[0-9\]".Advisor: Use "Is \[0-9\]*" instead.
It is more efficient.The user's mistaken belief is that piping " I s "  to "grep"is the most appropriate plan for listing files whosenames begin with a digit.
The user 's  misconceptionarises because he is unaware that the plan of using"Is\[0-9\]*" not only achieves his goal, but also does notthwart his other goal of using his time efficiently.5.2.2 EXPLANATIONS FOR AN INCORRECT ENABLESJust as there are several different sources of usermisconceptions about a plan's applicability to a goal,there are also several different sources of user miscon-ceptions about whether a state is a precondition to aplan achieving a goal: that is, a user belief of the formenables(Se, Ap, Sp).
Recall that to recognize that thistype of belief is incorrect he advisor found two beliefsof the form !enables(Se, Ap, Sp) and enables(S, Ap,Sp).
Here are the potentialEnglish descriptions for each.explanations along withClass Potential EnglishOf Mistake Explanation DescriptionEnablement For enables(Se,A,S) State enables actualSubgoal enablementEnablement For causes(A,Sp) Plan achieves user's goalOnly One Plan !enables(Se,A,Sp) without claimedenablementEnablement Too causes(A1,Se) User enablement resultsSpeci f ic  causes(AI,S) from action that achievescauses(A2,S) real enablement!causes(A2,Se) Other action achieves realenablement without user'senablement aseffectThe first, "Enablement For Subgoal",  explains theuser's mistake in our introductory exchange.
The ex-44 Computational Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsplanation is that the user is unaware that his precondi-tion is not a precondition of the goal itself, but of one ofits preconditions.User: So to remove a file, I have to own the directorythat contains it.Advisor: No, to remove a file, you need to have writepermission on the directory that contains it.
Youdo not need to own the directory that contains it.You need to own that directory when you do notalready have write permission on it.This user is unaware that owning a directory is aprecondition for achieving write permission on it, andthat having write permission is a precondition for re-moving a file.The second, "Enablement for One Plan", suggeststhat the user is unaware that a plan without his claimedprecondition achieves his goal.User: So I can only edit files when I 'm on a smartterminal?Advisor: Only if you edit with " re" .
"v i "  works fineon a dumb terminal.The user's incorrect belief is that it is necessary to havea smart terminal to edit a file.
This belief arises becausethe user is unaware that only one plan, using "v i " ,requires a smart terminal, and that there are other plansthat do not.The last, "Enablement Too Specific", suggests thatthe user is unaware that his precondition is less generalthan the actual precondition for achieving his goal.User: So I have to remove a file to create a file?Advisor: You do not have to remove a file to createa file.
You must have enough free space.
Remov-ing a file is only one way to obtain it.
You couldalso ask the system administrator for more space.The user mistakenly believes that it is necessary toremove an existing file before a new file can be created.The advisor believes that the precondition is sufficientspace for the new file, which can be achieved either byexecuting a plan for removing a file or by executing theplan of requesting more space.5.2.3 EXPLANATIONS FOR INCORRECT CAUSESOne final class of user misconception is an incorrectbelief that a particular state results from a plan's exe-cution; that is, a user belief of the form causes(Ap, Sp).Recall that to recognize that this type of belief isincorrect the advisor found beliefs of the form !cau-ses(Ap, Sp) and causes(A, Sp).
There are three potentialexplanations for this type of mistaken belief.Class Potential EnglishOf Mistake Explanation DescriptionPlan has Other applies(Ap,So) Action used to cause otherEffect effectEffect Requires enables(S,Ap,Sp) State required for ActionEnablement !causes(A,S) and no way to achieve StateEffect Inferred causes(Ap,So) Action causes other effectFrom Other precludes(So, ) That effect precludes a stateEffect precludes(Sp, S) that is precluded by user'seffectThe first, "Effect From Another Plan", accounts for anearlier example.
The explanation is that the user isunaware that the user's action actually has a differenteffect.User: I deleted a file by typing " remove" .Advisor: No, typing " remove"  did not delete yourfile.
Typing " rm"  deleted it.
Typing " remove"deletes a mail message from the mail program.The user's mistaken belief is typing " remove"  deletes afile.
The user is unaware that typing " remove"  actuallythrows away old mail messages.The second, "Effect Requires Unfulfilled Enable-ment" ,  suggests that the user is unaware that a partic-ular state is required for the plan to have the claimedeffect.User: I was cleaning out my account when I acciden-tally deleted all the command files by typing" rm" .Advisor: You can't delete the command files with" rm"  unless you are the system administrator.The user incorrectly believes that typing " rm"  resultedin the removal of various system files.
The advisorbelieves that it is necessary for the user to be the systemadministrator for this effect to occur.The last, "Effect Inferred From Other Effect" ,  ac-counts for another one of earlier examples.
It suggeststhat the user is unaware that one effect of his plan hasincorrectly led him to believe what was another effect ofthe plan.User: I accidentally hit the up arrow key and itdeleted 20 unanswered mail messages.
How can Iget them back?Advisor: Hitting the up arrow does not delete yourmessages, but does result in your being discon-nected from the etherplexer.
You could not accessyour mail messages because they were moved to"mbox" .
The mail program requires that yourmail messages be in "mai lbox".The user incorrectly believes that one effect of hittinguparrow was that his mail messages were deleted.
Thisbelief occurs because the user is unaware that one effectof hitting uparrow is that files are moved to a differentlocation, which makes them seem inaccessible.6 A DETAILED PROCESS MODELWe have presented three sets of potential explanationsand briefly sketched how they are used.
In this sectionwe provide a more detailed view of the process bywhich an explanation is found.An advisor presented with a user belief has threegoals.
First, he wants to know whether he shares theuser's belief.
Second, he wants to confirm that theComputational Linguistics, Volume 14, Number 3, September 1988 45Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsuser's belief is indeed a misconception.
Third, he wantsto infer the reasons behind the user's mistake.The advisor accomplishes the first by trying to verifythat he holds the user's belief.
He accomplishes thesecond by trying to find an explanation for why he doesnot hold the user's belief.
He accomplishes the third bytrying to find an explanation for why the user does holdthat belief.Two questions need to be answered.
How does theadvisor verify that he holds a particular belief?
And howdoes the advisor explain why he does not hold a belief,or why the user does?6.1 VERIFYING AN ADVISOR BELIEFVerifying whether or not the advisor believes that aparticular planning relationship holds takes two steps.First, the advisor searches his memory for the desiredpiece of planning knowledge.
Then, if it is not found, theadvisor applies the set of rules associated with thatplanning relationship to try and prove that it holds.Once the advisor has proved that the planning relation-ship holds, either by search or by reasoning, that pieceof knowledge is noted to be an advisor belief.Consider, for example, the process of verifying thatthe advisor holds a belief that owning a directory is nota precondition of removing a file.
If this fact is alreadyknown from past experience, the advisor will recognizeit during memory search.
If not, the advisor can try todeduce it.
One rule that applies here says that "it" a stateS is not one of the known states that are preconditionsto an action A for achieving a goal state, then assumethat S is not a precondition."
Here, this means that ifowning a directory is not among the known precondi-tions for removing afile, assume it is not a preconditionfor removing a file.6.2 FINDING AN EXPLANATIONThe advisor must be able to explain why an actor doesor does not hold a particular belief.
Finding an expla-nation is accomplished by hypothesizing one associatedwith the given class of belief and then trying to confirmit.
The advisor:1.
Classifies the belief according to its type: applies,enables, or effects.2.
Selects one of the potential explanations associatedwith that class of belief.
The potential explanation isan abstract configuration of planning relationships.3.
Instantiates this potential explanation with informa-tion from the user's belief.4.
Tries to verify each of the planning relationshipswithin the potential explanation.
If all can be veri-fied, this potential explanation is the desired expla-nation.5.
Repeats the process until one of the potential expla-nations associated with this belief's type is verifiedor all potential explanations have been tried and havefailed.The result of the process of finding an explanation isthat thte advisor has verified that he holds a particularset of beliefs.
These beliefs constitute the desired ex-planatkm.6.3 AN EXAMPLEThis section is a detailed look at the advisor's process-ing of the user belief that owning a directory is aprecondition of removing a file.enables(user own directory, use "rm",  the file'sremoval)First, the advisor tries to verify that he holds the user'sbelief.
He cannot.Next, the advisor tries to confirm that the user'sbelief is, in fact, a misconception.
He does this by tryingto explain why he does not hold this user belief.
Henotes that it can be classified as a belief that some stateSp (owning the directory) is a precondition to achievingsome other state Sg (removing a file).
The potentialexplanation for why the advisor does not hold this typeof belief is that he believes that Sp is not a preconditionof achieving Sg, and that some other state S is aprecondition ofSg.
By instantiating this potential expla-nation, the advisor determines that he must checkwhether he holds beliefs that:!enables(owning a directory, use "rm file", the file'sremoval)enables(S, use "rm file", removing a file)The advisor finds that he believes that owning a direc-tory is not a precondition of removing a file (either byfinding that relationship in memory or by deducing it).The advisor also finds that write permission on adirectory is a precondition of removing a file (that is,that S can be instantiated with write permission on adirectory).
These matching beliefs confirm that theuser's belief is a misconception.Now, the advisor has to try to find an explanation forwhy the user holds this mistaken belief.
One potentialexplanation is that the user is unaware that Sp isactually a precondition of achieving a state S, which isa precondition to achieving Sg.
In this case, instantiat-ing Sp and Sg leads to the advisor to try and verify thathe holds two beliefs:enables(S, use "rm file", the file's removal)enables(owning a directory, A, S)These beliefs are verified when the advisor finds thathaving written permission on a directory is a precondi-tion to removing a file, and that owning a directory is aprecondition to obtaining written permission on thedirectory.
The potential explanation suggests that theuser's misconception resulted from his being unawareof these two advisor beliefs.FinaUy, the advisor presents the resulting beliefs tothe user.
The user is informed of the beliefs used to46 Computati~mal Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsconfirm the user's misconception a d the beliefs used toexplain its source.6.4 THE POINT OF POTENTIAL EXPLANATIONSHaving a taxonomy of potential explanations lessensthe amount of reasoning the advisor must do to detectand respond to the user's misconceptions.To see why, consider an advisor trying to understandhow the user arrived at the mistaken belief that aprecondition of removing a file is owning the directorycontaining it.
The advisor is trying to find some connec-tion between the user's enablement and removing afile.The potential explanations suggest how to find specific,likely-to-be-useful connections.
For example, the po-tential explanation "Enablement for Subgoal" suggestsexamining whether achieving any of the preconditionsof removing a file requires owning a directory.Without a set of potential explanations, it becomesnecessary to reason from a set of rules that describelikely differences between user and advisor beliefs.
Onerule might be that a user may incorrectly attribute anenablement of one action to another action.
Anotherrule might be that a user may incorrectly attribute theresult of one action to another action.
From a set ofsuch rules the advisor must somehow deduce the causeof the user's mistake.
By using potential explanationsthe problem becomes instead one of guided memorysearch rather than reasoning from first principles.7 RELATED WORKTwo approaches have been used to detect and correctmisconceptions.
The first approach is used by manyintelligent tutoring systems (Anderson, Boyle, and Yost1985; Brown and Burton 1978; Burton 1982; Stevens,Collins, and Goldin 1982).
These systems locate mis-taken beliefs in a data base of error-explanation pairsand provide the associated explanation.
A basic prob-lem with this approach is that, because there is noinformation about the underlying causes of the errors,these systems can handle only those misconceptionsknown in advance.The other approach avoids the difficulty inherent inenumerating all possible misconceptions within a do-main by using strategies that address an entire class ofmisconceptions.
The user's misconception is classifiedaccording to the abstract reasoning error likely to haveled to it.
This approach shares many features withrecognizing abstract thematic situations (such as irony)in narratives, where such situations are defined in termsof abstract planning errors made by the narrative char-acters (Dyer 1983; Dyer, Flowers, and Reeves 1987;Dolan and Dyer 1986).
Once an appropriate strategy isfound, it can be used to generate advice (in narratives,this advice may be in the form of adages).
In advisorysystems, this approach as been applied to both object-and plan-oriented misconceptions.7.1 OBJECT-ORIENTED MISCONCEPTIONSROMPER (McCoy 1985, and this issue) corrects usermisconceptions dealing with whether an object is aninstance of a particular class of objects or possesses aparticular property.User: I thought whales were fish.ROMPER: No, they are mammals.
You may havethought they were fish because they are fin-bearing and live in the water.
However, they aremammals since, while fish have gills, whalesbreathe through lungs and feed their young withmilk.ROMPER classifies a user's misconception aseither amisclassification r misattribution a d then selects oneof several strategies associated with each class of mis-conception to generate a response.
Each strategy ad-dresses a different type of reasoning error, and isselected based on ROMPER's own beliefs about objectsand its model of the user's relevant beliefs.
One suchstrategy isuseful when the advisor believes that X isa Z,the user mistakenly believes that X isa Y, and theadvisor believes that X and Y share certain attributes.The strategy suggests presenting these shared attributesas a possible reason for the misclassification, and point-ing out the unshared attributes that lead the advisor tobelieve that X isa Z.Despite dealing with a very different class of miscon-ceptions, ROMPER's approach is similar to ours.
Themajor difference is that our explanation-based approachseparates the beliefs needed to confirm the user's beliefas a misconception from those needed to understandwhy the user holds it.
The strategy above divides intotwo explanations.
The first confirms that a user beliefthat X isa Y is incorrect if the advisor believes that X isaZ because X and Z share certain attributes.
The othersuggests that the user may hold this belief because Xand Y share certain attributes.
The advantage to ourapproach is that the information regarding the beliefsthat confirm that the user has a misconception can beseparated from the explanations for why the user holdsthe belief, and unnecessary duplication of tests isavoided.7.2 PLAN-ORIENTED MISCONCEPTIONSTwo efforts have examined etecting and responding toplan-oriented misconceptions.Joshi, Webber, and Weishedel (1984) suggest using astrategy-based approach to provide cooperative re-sponses to problematic planning requests.
They con-sider "How do I do X?"
questions in which X can beinferred to be a subgoal of a more important goal Y.User: How can I drop cs577?System: It is too late in the quarter to drop it.
But youcan avoid failing by taking an incomplete andfinishing your work next quarter.They provide several strategies, listed below, for select-ing the contents of a reasonable r sponse, with strategyselection based on the advisor's beliefs about whichComputational Linguistics, Volume 14, Number 3, September 1988 47Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsplans achieve a particular goal and the achievability oftheir preconditions.Situation Response1.
Unachievable Precondition E of X Provide E(a) Plan P achieves Y Provide P(b) No plan to achieve Y Point this out2.
X doesn't help achieve Y Point this out(a) Plan P achieves Y Provide P(b) No plan to achieve Y Point this out3.
Plan P better way to achieve Y Provide P4.
X only way to achieve Y Point this out5.
Plan P involving uncontrollable Provide Pevent E achieves YOne such strategy, useful when the advisor believes thatX cannot be achieved because of an impossible-to-achieve precondition, is to point out the troublesomeprecondition and suggest an alternate plan that achievesY.Our work differs from theirs in several respects.
Themain difference is that they focus on correcting theuser's misconception i stead of trying to explain why itoccurred.
Only one strategy above is concerned withproviding an explanation that addresses the source of auser misconception (i this case, an inappropriate plan).The other strategies describe situations in which achiev-ing X is inappropriate and an alternate plan for Y existsand should be presented to the user as a correction.
Inaddition, they did not consider esponding to incorrectbeliefs about plan preconditions or effects.The other effort, SPIRIT (Pollack 1986), tries todetect he inappropriate plans underlying queries madeby users of a computer mail program and the mistakenuser beliefs underlying those plans.User: I want to prevent Tom from reading my file.How can I set the permissions on it to faculty-readonly?System: You can make the file readable by facultyonly using "set permission".
However, Tom canstill read it because he's the system administrator.User misconceptions about he applicability and execu-tability of plans are detected by reasoning about thelikely differences between the advisor's beliefs and theuser's, with various rules used to infer these differ-ences.
One such rule, used to detect he source of themisconception above, states that an advisor who be-lieves that an act has a particular esult under certainconditions can infer that the user has a similar beliefmissing one of the required conditions.SPIRIT has a task similar to ours but takes a verydifferent approach, trying to determine the cause of theuser's error through reasoning from first principlesrather than memory search.
In addition, SPIRIT cannotdetect or respond to mistakes involving plan applicabil-ity conditions or preconditions.
Finally, SPIRIT doesnot specify how knowledge of the cause of the user'smistaken belief affects the information to be included ina cooperative response, something that falls naturallyout of our model.7.3 UNIX ADVISORSFinally, there are two other elated research efforts, UC(Wilensky et al 1986, Wilensky, Arens, and Chin 1984)and SC.
(Kemke 1986), that address providing advice tonovice UNIX users.
Neither system, however, detectsor responds to misconceptions.
Instead, both are con-cerned with tailoring a response to a question to reflectthe user's level of expertise.
UC's user modeling com-ponent, KNOME (Chin 1986), analyzes a user's ques-tions to determine which stereotypical c ass the userbelongs to and then uses this information to providemore details and possibly more examples to less expe-rienced users.Novice: What does the "rwho" command o?UC: Rwho lists all users on the network, their tty,their login time, and their idle time.Expert: What does the "rwho" command o?UC: Rwho is like who, except rwho lists all users onthe network.SC's user modeling component, SCUM (Nessen 1987),takes an approach similar to UC's, also using stereo-typical information.
These approaches are complemen-tary to ours.8 IMPLEMENTATION DETAILSThe theory discussed in this paper is embodied inAQUA, a computer program currently under develop-ment at UCLA.
The current version of AQUA isimplemented in T (Rees, Adams, and Meehan 1984),using RHAPSODY (Turner and Reeves 1987), a graph-ical AI tools environment with Prolog-like unificationand backtracking capabilities, and runs on an ApolloDN460 workstation.
Given a set of user beliefs involv-ing plan applicability conditions, preconditions, or ef-fects, AQUA determines which of these user beliefs areincorrect and what missing or mistaken user beliefs arelikely to have led to them, and then produces a set ofadvisor beliefs that capture the content of the advisor'sresponse.
AQUA's domain of expertise is in the basicplans used to manipulate and access files, directories,and electronic mail.
It has been used to detect andrespond to at least wo different incorrect user beliefs ineach class of misconception that we have identified.More detailed escriptions of the program's implemen-tation can be found in Quilici, Flowers, and Dyer (1986),and in Quilici (1985).9 LIMITATIONS AND FUTURE WORKOur approach to determining why an actor does or doesnot hold a particular belief has been to let potentialexplanations direct the search for the advisor beliefsthat serve as an appropriate explanation.
Our focus hasbeen on discovering and representing these explana-tions.
The limitations of our approach arise in areas wehave ignored, each of which is an interesting area ofresearch.48 Computational Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions9.1 INFERRING THE SET OF USER BELIEFSOur model assumes that the user's problem descriptionhas somehow been parsed into a set of beliefs.
How-ever, users rarely explicitly state their beliefs, leavingthe advisor to the difficult task of inferring them.Consider our introductory exchange.User: I tried to remove a file with the "rm" com-mand.
But the file was not removed and the errormessage was permission denied.
I checked and Iown the file.
What's wrong?Advisor: To remove a file, you need to be able towrite into the directory containing it.
You do notneed to own the file.Here the advisor must infer the user's beliefs that 1.using "rm" is applicable to removing afile; that 2. using"rm" did not cause the file's removal; that 3. using"rm" resulted in an error message, and that 4. owninga file is a precondition to removing it.Inferring the first belief requires arule such as "if theuser tries to achieve a state with a particular action,assume the user believes that action achieves thatstate."
The second belief can be inferred from the rulethat "if an utterance describes the nonexistence of astate that is a believed result of an action, assume thatthe user believes that the action did not cause thestate."
A similar rule can be used to infer the thirdbelief.Inferring the final belief, that owning a file is aprecondition to its removal, is a difficult ask.
Becausethere are a potentially-infinite number of incorrect userbeliefs about the preconditions of removing a file, theadvisor cannot simply match owning a file against a listof incorrect preconditions.
Because the user may havebeen discussing other plans and other goals the advisorcannot simply assume that any utterance after a plan'sfailure refers to its preconditions.
Instead, the advisorneeds to infer this user belief from the knowledge thatthe user did some sort of verify-action, the knowledgethat one plan for dealing with a plan failure is to try toverify that the enablements of the plan have beenachieved, and the knowledge that both owning the fileand having write permission are different instantiationsof having sufficient permission.Inferring beliefs like these, that involve the user'splans and goals and the relationships between, evenwhen they differ from the advisor's, is currently anactive area of research (Carberry, this issue; Kautz andAllen 1986, Goodman 1986, Wilensky et al 1986, Quilici1985).9.2 RETRIEVING ADVISOR BELIEFSOur potential explanations suggest patterns of beliefsthat the advisor should search for.
However, we havenot specified how this search of the advisor's memory isactually carried out, how a belief in memory can beretrieved efficiently, or how the beliefs are actuallyacquired through experience.
AQUA's organization ofplan-oriented beliefs is discussed in Quilici (1988, 1985).It is based on earlier work (Kolodner 1985, Schank1982) in taking experiences and indexing them appro-priately for efficient search and retrieval, especially thatinvolving indexing memory around various planningfailures (Kolodner and Cullingford 1986, Quilici 1985,Hammond 1984, Dyer 1983).Because the advisor may need to verify a belief thatis not stored directly in memory, memory search maynot be sufficient.
Suppose the advisor is trying to verifythat owning a directory is not required to remove a file.The advisor may be able to deduce this belief from apast experience inwhich he removed a file from/trap, adirectory owned by the system administrator.
Similarly,the advisor may be able to deduce that write permissionis needed to remove a file from his beliefs that writepermission is needed to make changes on objects andthat removing a file involves making a change to adirectory.
This requires more powerful reasoning capa-bilities than AQUA's simple rules for inferring negatedbeliefs.Finally, AQUA assumes the existence of a taxonomyof planning failures.
We have left the automatic creationof this taxonomy from advisor experiences to futureresearch.
Initial work in recognizing and indexing ab-stract configurations of planning relations is discussedin Dolan and Dyer (1985, 1986).9.3 OTHER CLASSES OF MISCONCEPTIONSWe are currently studying how well the classes ofmisconceptions described here account for responses tomisconceptions in domains other than the problems ofnovice computer users, such as the domain of simpleday-to-day planning.
In addition, we are examiningother classes of planning misconceptions.
For example,to respond to an incorrect user belief such as "rm"cannot be used to remove a file, the advisor needspotential explanations for why an action does not applyto a particular goal state.We do not yet know whether our approach is suitablefor generating responses to misconceptions that are notdirectly related to plan-goal interactions, uch as mis-takes in referring to an object.
Consider the followingexchange:User: Diana is up, but I cannot access my file.Advisor: Your files are on Rhea, not Diana.
Theymoved your files yesterday because your file sys-tem was full.Here the user's problem is that he is incorrectly using"Diana" to refer to the machine his files are on.
We areexamining whether our approach is extendable to re-spond to these types of user misconceptions.9.4 RESPONSE GENERATIONThe response we provide is a set of advisor beliefs thatis as complete as possible.
We make no attempt to useknowledge about other user beliefs to modify our re-sponse to provide only the most relevant beliefs.
How-ever, if the advisor can infer that a user knows that hisComputational Linguistics, Volume 14, Number 3, September 1988 49Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsplan has failed (perhaps because of the error message acommand produces), he need not inform the user thathis plan is incorrect.
One straightforward way to ex'tendour model is to have the advisor filter out those beliefshe can infer the user has.The advisor should use information about the user totailor their response based on the user's level of exper-tise.
Recall the following exchange:User: I tried to remove my directory and I got anerror message "directory not empty".
But " Is"didn't list any files.Advisor: Use "Is -a" to list all of your files. "
Is"cannot be used to list all of your because " Is"  doesnot list those files whose names begin with aperiod.An advisor who knows the user is a novice might wantto augment his response with an explanation that " -a"is a command option and that command options causechanges in the normal behavior of commands.
Severalresearchers are working on tailoring the response to theuser based on knowledge about the user's expertise(Paris, this issue; Chin 1986).10 CONCLUSIONSWe have presented an explanation-based approach tothe problem of recognizing and responding to usermisconceptions.
The advisor confirms that a user'sbelief is a misconception by finding an explanation forwhy he does not hold the user's belief.
The advisorinfers its source by finding an explanation for themistaken belief.
The process of finding an explanationwas presented as one of hypothesizing and trying toverify a small set of potential explanations associatedwith each type of user belief.
In essence, the model usesinformation about likely sources of different classes ofuser misconceptions to recognize user mistakes andinfer their underlying causes.This approach is attractive for several reasons.
First,because it has information about classes of abstractmisconceptions, it can handle misconceptions of whichit has no prior knowledge, as long as they fall into oneof these classes.
A smaller set of potential explanationscan account for a large number of specific user mis-takes.
Second, the model makes use of knowledge ofthe types of misconceptions u ers are likely to make tocircumvent he need for general deductive reasoning.The model can easily be augmented to first checkwhether it has knowledge of specific misconceptions, asdo tutoring systems, and to use general reasoning whenit is confronted with a misconception that cannot beexplained by any of its potential explanations, as doesSPIRIT.
Finally, our approach leads to responses im-ilar (and sometimes more informative) than those of theUNIX advisors we have observed.ACKNOWLEDGMENTSThe work reported here was supported in part by a grant from theLockheed Software Technology Center (Austin, TX).
Special thanksgo to Mike Gasser, John Reeves, and Ron Sumida for fighting theirway through several earlier incoherent and uninteresting versions ofthis paper.
Comments by two anonymous reviewers have also greatlyimproved its organization and content.REFERENCESAbelson, R. 1973 The Structure of Belief Systems.
In Schank R.C.and C.olby K.M.
(eds.
), Computer Models of Thought and Lan-guage.
Freeman, San Francisco, CA.Abelson, R. 1979 Differences between Beliefs and Knowledge Sys-tems.
Cognitive Science 3: 355-366.Alvarado, S. 1987 Understanding Editorial Text: A computer model ofreasoning comprehension.
Ph.D. thesis.
University of California,Los Angeles, CA.Alvarado, S., Dyer, M., and Flowers, M. 1986 Editorial Comprehen-sion in OpED through Argument Units.
In Proceedings of the 1986National Conference on Artificial Intelligence, Philadelphia, PA:250-256.Anderson, J.R., Boyle, C.F., and Yost, G. 1985 The Geometry Tutor.In Proceedings of the 1985 Joint Conference on Artificial Intelli-gent:e, Los Angeles, CA: 1-7.Brown, J.S.
and Burton, R.R.
1978 Diagnostic models for proceduralbugs in basic mathematical skills.
Cognitive Science 2: 155-192.Burton, R.R.
1982 Diagnosing bugs in a simple procedural skill.Intelligent Tutoring Systems, Academic Press, London, England:157-183.Carberry, S. Plan Recognition and User Modeling.
this issue.Chin, D. 1986 User modeling in UC, the UNIX consultant.
InProceedings of the CHI-86 Conference, Boston, MA.Cullingford, S. 1978 Script Application: Computer Understanding ofNewspaper Stories, Ph.D. thesis.
Yale University, New Haven,CT.Dolan, C. and Dyer, M. 1985 Learning Planning Heuristics throughObservation.
In Proceedings of the 1985 Joint Conference onArtificial Intelligence, Los Angeles, CA: 606-602.Dolan, C. and Dyer, M. 1986 Encoding Planning Knowledge forRecognition, Construction, and Learning.
In Proceedings of the8th Annual Cognitive Science Society: 488--499.Dyer, M. 1983 In-Depth Understanding: A Computer Model ofNarrative Comprehension, MIT Press, Cambridge, MA.Dyer, M., Flowers, M., and Reeves, J.F.
1987 Recognizing Situa-tional Ironies: A Computer Model of Irony Recognition andNaiTative Understanding.
In Advances in Computing and theHumanities: forthcoming.Fikes, R.E.
and Nilsson, N.J. 1971 STRIPS: A New Approach to theApplication of Theorem Proving to Problem Solving.
ArtificialIntelligence 2: 189-208.Flowers, M., McGuire, R., and Birnbaum, L. 1982 Adversary Argu-ments and the Logic of Personal Attacks.
In Strategies for NaturalLanguage Processing.
Lawrence Erlbaum, Hillsdale, NJ: 275-294.Goodman, B.
1986 Miscommunication a d Plan Recognition.
Unpub-lished manuscript from UM86.
International Workshop on UserModeling, Maria Laach, West Germany.Hammond, K. 1984 Indexing and Causality: The Organization ofPlans and Strategies in Memory.
Technical Report 351, YaleUniversity, New Haven, CT.?
Kautz, H. and Allen, J.
1986 Generalized Plan Recognition.
InProceedings of the 1986 National Conference on Artificial Intelli-gence, Los Angeles, CA: 423-427.Kemke, C. 1986 The SINIX Consultant: Requirements, Design andImplementation f an Intelligent Help System for a UNIX Deriv-50 Computational Linguistics, Volume 14, Number 3, September 1988Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptionsative.
Report No.
11, AI Laboratory, Dept.
of Computer Science,Univ.
of Saarbrficken, W. Germany.Joshi, A., Webber, B., Weishedel, R. 1984 Living up to expectations:computing expert responses.
In Proceedings of the 1984 NationalConference on Artificial Intelligence, Dallas, TX: 169-175.Kolodner, J.L.
1984 Retrieval and Organizational Strategies in Con-ceptual Memory, Lawrence Erlbaum, Hillsdale, NJ.Kolodner, J.L.
and Cullingford, R.E.
1986 Towards a MemoryArchitecture that Supports Reminding.
In Proceedings of the 8thAnnual Cognitive Science Society: 467--477.McCoy, K. Reasoning on a highlighted user model to respond tomisconceptions, this issue.McCoy, K. 1985 Responding to Object-Oriented Misconceptions.Ph.D.
Thesis.
University of Pennsylvania, Philadelphia, PA.Nessen, E. 1987 SCUM: User modeling in the SINIX consultant.Memo No.
18, AI Laboratory, Dept.
of Computer Science,University of Saarbrficken, Saarbriicken, West Germany.Paris, C. Tailoring object descriptions to the user's level of expertise.this issue.Pollack, M. 1986 A model of plan inference that distinguishes betweenthe beliefs of actors and observers.
In Proceedings of 24th meetingof the Association of Computational Linguistics, New York, NY.Pollack, M. 1986 Inferring domain plans in question-answering.Ph.D.
Thesis.
University of Pennsylvania, Philadelphia, PA.Quilici, A.
1988 AQUA: A system that detects and responds to usermisconceptions.
In Kobsa Alfred and Wahlster Wolfgang (eds.
),User Modeling and Dialog Systems, Springer Verlag, Berlin, NewYork.Quilici, A., Dyer, M., and Flowers, M. 1986 AQUA: An intelligentUNIX advisor.
In Proceedings of the 1986 European Conferenceon Artificial Intelligence, Brighton, England: 33-38.Quilici, A.
1985 Human problem understanding and advice giving: Acomputer model.
Technical Report No.
85-00069, Computer Sci-ence Department, University of California, Los Angeles, CA.Rees, J.A., Adams, N.L., and Meehan, J.R. 1984 The Tmanual.
YaleUniversity, New Haven, CT.Sacerdoti, E. 1974 Planning in a hierarchy of abstraction spaces.Artificial Intelligence 5(2): 115-135.Schank, R.C.
1982 Dynamic Memory.
Cambridge University Press,Cambridge, MA.Schank, R.C., and Abelson, R. 1977 Scripts, Plans, Goals, andUnderstanding, Lawrence Erlbaum, Hillsdale, NJ.Schank, R.C.
1986 Explanation Patterns: Understanding Mechani-cally and Creatively, Lawrence Erlbaum, Hillsdale, NJ.Stevens, A., Collins, A., and Goldin, S.E.
1982 Misconceptions instudents' understanding.
Intelligent Tutoring Systems, AcademicPress, London, England: 13-24.Turner, S.R.
and Reeves, J.F.
1987 The Rhapsody User's Manual.Technical Note UCLA-AI-86-10, Artificial Intelligence Labora-tory, University of California, Los Angeles, CA.Wilensky, R., Mayfield, J., Albert, A., Chin, D., Cox, C., Luria, M.,Martin, J., and Wu, D. 1986 UC: A Progress Report.
TechnicalReport UCB/CSB 87/303, Computer Science Division (EECS),University of California, Berkeley, CA.Wilensky, R., Arens, Y., and Chin, D. 1984 Talking to UNIX inEnglish: An Overview of UC, Communications ofthe ACM: 574-593.Wilensky, R. 1983 Planning and Understanding, Addison Wesley,Reading, MA.Wilensky, R. 1978 Understanding Goal-Based Stories.
Ph.D. thesis.Technical Report 140, Yale University, New Haven, CT.Computational Linguistics, Volume 14, Number 3, September 1988 51
