Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 296?305,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPHow well does active learning actually work?
Time-based evaluation ofcost-reduction strategies for language documentation.Jason BaldridgeDepartment of LinguisticsThe University of Texas at Austinjbaldrid@mail.utexas.eduAlexis PalmerComputational LinguisticsSaarland Universityapalmer@coli.uni-sb.deAbstractMachine involvement has the potential tospeed up language documentation.
We as-sess this potential with timed annotationexperiments that consider annotator exper-tise, example selection methods, and sug-gestions from a machine classifier.
Wefind that better example selection and la-bel suggestions improve efficiency, but ef-fectiveness depends strongly on annota-tor expertise.
Our expert performed bestwith uncertainty selection, but gained lit-tle from suggestions.
Our non-expert per-formed best with random selection andsuggestions.
The results underscore theimportance both of measuring annotationcost reductions with respect to time and ofthe need for cost-sensitive learning meth-ods that adapt to annotators.1 IntroductionData annotated with linguistically interesting la-bels is used in a wide variety of contexts.
Com-putational linguists generally use annotated dataas training and evaluation material for natural lan-guage processing systems; corpus linguists use itto test hypotheses about language; documentarylinguists create interlinear glossed texts to pre-serve examples of endangered languages and hy-potheses about the grammars of those languages.Regardless of the context, creating annotated datais costly in terms of time and/or money.
Since bothtime and money are undeniably in limited supply,there is a widely shared desire to reduce this cost.Reducing cost involves strategies that do morewith fewer human-annotated labels and/or reducethe per-label cost.
An example of the former is ac-tive learning, which focuses annotation effort ondata points selected by the learner(s) for their ex-pected utility in developing a more accurate model(Settles, 2009).
Examples of the latter includeproviding suggestions from a machine labeler andusing extremely cheap human labelers, e.g.
withthe Amazon Mechanical Turk (Snow et al, 2008).Different techniques may be more or less appli-cable depending on the language being annotated,the kind of labels which are desired (tags, syntac-tic structures, etc.
), and the desired use of the an-notated data (e.g., for training models, testing lin-guistic hypotheses, or preserving a language).This paper discusses experiments that measurethe effectiveness of machine-aided annotation forlanguage documentation using both active learn-ing simulation experiments and annotation ex-periments which involve actual documentary lin-guists interacting with machine example selec-tion and label suggestion.
Specifically, we dealwith the task of labeling morphemes of the Mayanlanguage Uspanteko with fine-grained parts-of-speech.
We also run active learning simulationexperiments for part-of-speech tagging for Dan-ish, Dutch, English, Swedish, and Uspanteko toshow the validity of our models and methods in astandard setting.
For Uspanteko, we provide re-sults from annotation experiments in which anno-tation cost is measured in terms of the actual an-notation time required while varying three factors:(1) example selection, (2) machine label sugges-tions, and (3) annotator expertise.Our findings indicate that there is consider-able promise for reducing the cost of produc-ing IGT, but they also demonstrate considerablevariation due to the interaction of these factors.This suggests different prescriptions for appropri-ate strategies in different contexts.
Most clearly,the worst performing strategy?by far?is thatused in nearly all documentary work: sequentialannotation without automation.
Also, our expertannotator did best with examples picked by un-certainty selection, while our non-expert did bestwith random selection aided by machine label sug-296Language #words-tr #words-dev #tags #sents-tr #sents-dev Avg.sent Avg.tr.sent Avg.dev.sentDanish 62825 31561 10 3570 1618 18.18 17.60 19.50Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44English 167593 131768 45 6945 5527 24.00 24.13 23.84Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length.gestions.
This difference confirms the importanceof cost-sensitive active learning strategies that arenot just learner-guided, but also take into accountmodeling of the annotators (Settles et al, 2008;Haertel et al, 2008; Vijayanarasimhan and Grau-man, 2008).
Finally, we confirm the importanceof using actual annotation time to measure annota-tion cost: a unit-cost assumption?even at a fine-grained level?can dramatically misrepresent theactual effectiveness of different strategies.2 Task and dataAnnotation task: language documentationThe amount of money spent on obtaining humanannotations is an extremely important concern inmuch language annotation.
However, there is afurther urgency for annotation in the case of lan-guage documentation: languages are dying at therate of two each month.
By the end of this cen-tury, half of the approximately 6000 extant spokenlanguages will cease to be transmitted effectivelyfrom one generation of speakers to the next (Crys-tal, 2000).
Recorded and transcribed texts anno-tated with detailed linguistic information create animportant multi-faceted record of these languages,but there are few trained linguists with adequatetime and appropriate levels of funding relative tothe size of the problem.
Annotation cost?in bothtime and money?is thus keenly felt in the workof documenting and describing endangered lan-guages.
Active learning and automated label sug-gestions could help deal with this language docu-mentation bottleneck.We focus on one stage of language documen-tation, the production of interlinear glossed text(IGT), a standard form of annotation that in-volves both morphological and grammatical anal-ysis.
IGT is generally created following transcrip-tion and translation of recorded speech, with theannotations often being provided by trained anno-tators with varying levels of expertise.
The resultis generally a small amount of IGT annotated dataand a greater amount of unannotated data.Data We use a collection of 32 interlinearglossed texts (IGT) in the Mayan language Uspan-teko.
This corpus was cleaned up and adapted byPalmer et al (2009) from an original collection of67 texts that were collected, transcribed, translatedand annotated by the OKMA language documen-tation project (Pixabaj et al, 2007).Two core tasks in creating IGT are morpholog-ical analysis and tagging morphemes with theirglosses (labels indicating part-of-speech and/orgrammatical function).
We deal with the latter taskand assume texts are morphologically segmented.Standard four-line IGT has morphemes on one lineand their glosses on the next.
The gloss line in-cludes labels for grammatical morphemes (e.g.
PLor COM) and translations of stems (e.g.
hablar oridioma).
The following is an Uspanteko example:(1) TEXT: Kita?
tinch?ab?ej laj inyolj iinMORPH:GLOSS:POS:kita?NEGPARTt-in-ch?abe-jINC-E1S-hablar-SCTAM-PERS-VT-SUFlajPREPPREPin-yol-jA1S-idioma-SCPERS-S-SUFiinyoPRONTRANS: ?No le hablo en mi idioma.
?We use a single layer that is a combination of theGLOSS and POS layers (Palmer et al, 2009).
For(1), the morphemes and labels for our task are:(2) kita?NEGt-INCin-E1Sch?abeVT-jSClajPREPin-A1SyolS-jSCiinPRONWe also consider POS-tagging for Danish,Dutch, English, and Swedish; the English is fromsections 00-05 (as training set) and 19-21 (as de-velopment set) of the Penn Treebank (Marcus etal., 1993), and the other languages are from theCoNLL-X dependency parsing shared task (Buch-holz and Marsi, 2006).1We split the original train-ing data into training and development sets.
Ta-ble 1 shows the number of words and sentencesin each split of each dataset, as well as the num-ber of possible labels and the average sentencelength.
The Uspanteko data is counted in mor-phemes rather than words; also, the Uspantekotexts are divided at the clause rather than sentencelevel.
This gives the corpus a much lower averageclause length than the other languages (Table 1).1The subset of the Penn Treebank was chosen to be ofcomparable size to the CoNLL datasets.2973 Model and methodsClassification model.
We use a standard maxi-mum entropy classifier for tagging Danish, Dutch,English, and Swedish words with POS-tags andtagging Uspanteko morphemes with Gloss/POStags.
The label for a word/morpheme is pre-dicted based on the word/morpheme itself plusa window of two units before and after.
Stan-dard part-of-speech tagging features (Ratnaparkhi,1998; Curran and Clark, 2003) are extracted fromthe morpheme to help with predicting labels forpreviously unseen morphemes.
This is a strongbut standard model; better, more complex modelscould be used, but the gains are likely to be small.Thus, we opted for simplicity in our model so as tofocus more on the interaction between the annota-tor and different levels of machine involvement.The accuracy of the tagger on the datasets whentrained on all available training material is givenin the following table, along with accuracy of aunigram model (learned from the training set andconstrained by a tag dictionary for known words).Unigram ModelDanish 91.62% 95.58%Dutch 90.92% 93.57%English 87.87% 93.25%Swedish 84.91% 87.74%Uspanteko 77.84% 79.39%Sample selection.
We consider three sampleselection methods: sequential, random, anduncertainty.
Sequential selection is importantto consider as it is the default in documentaryprojects.
It is sub-optimal for corpora with con-tiguous sub-domains, since it necessitates workingthrough many similar examples before getting topossibly more informative examples.
Random se-lection is a model-free method that avoids the sub-domain trap by sampling freely from the entirecorpus.
It generally works better than sequentialselection and provides a strong baseline againstwhich to compare learner-guided selection.Uncertainty selection (Cohn et al, 1995) iden-tifies examples the model is least confident about.We measure uncertainty as the entropy of the la-bel distribution predicted by the maximum en-tropy model for each example.
Uncertainty fora clause is calculated as the average entropy permorpheme; clauses with the highest average en-tropy are selected for labeling.A recent development in active learning is cost-sensitive selection that is guided not only by thelearner but also by the expected cost of labeling anexample based on its likely complexity and/or thereliability of the annotator.
Settles et al (2008)provide empirical validation for cost-related in-tuitions; for example, that cost of annotation isstatic neither per example nor per annotator.
Also,they show that taking annotation cost into accountcan improve active learning effectiveness, but thatlearning to predict annotation cost is not yet well-understood.
A cost-sensitive Return on Investmentheuristic is developed in Haertel et al (2008) andtested in a simulated POS-tagging context.
Ourexperiments do not employ cost-sensitive selec-tion, but our results?from live (non-simulated)active learning experiments of real-world scale?empirically support the need to consider cost-sensitive selection if better cost reductions are tobe achieved.Annotation setup.
We compare results fromtwo annotators with different levels of exposure toUspanteko.
Both are documentary linguists withextensive field experience.
Our expert annota-tor is a native speaker of K?ichee?, a closely re-lated Mayan language, and has worked extensivelyon Uspanteko.
Our non-expert annotator had noprior experience with Uspanteko and only limitedexposure to Mayan languages.
During annotation,he used an Uspanteko-Spanish dictionary.For each selection method, we consider twoconditions for providing classifier labels: a do-suggest (ds) condition where the labels predictedby the machine learner are shown to the annotator,and a no-suggest (ns) condition where the annota-tor does not see the predictions.
With ds, the anno-tator is shown the most probable label and a rankedlist of all labels assigned a probability greater thanhalf that of the best label.
For ns, the annotatorsees a frequency-ranked list of labels previouslyseen in training data for the given morpheme.Annotators improve as they see more examples.To minimize the impact of this learning process,annotation is done in rounds.
Each round con-sists of sixty clauses?six batches of ten each forthe six experimental cases.
The annotator is freeto break between batches.
Following annotation,the newly-labeled clauses are added to the train-ing data, and a new model is trained and evaluated.Both annotators completed fifty-six rounds of an-notation.
See Palmer et al (2009) for more detailson the annotation setup.298Measuring annotation cost.
Active learningstudies usually simulate annotation and use a unitcost assumption that each word, sentence, con-stituent, document, etc.
takes the same time to an-notate.
This is often the only option since corporatypically do not retain annotation time, but it islikely to exaggerate the annotation cost reductionsachieved.
This is exacerbated with active learn-ing: the informative examples it seeks to find aretypically harder to annotate (Hachey et al, 2005).Baldridge and Osborne (2008) correlate a unitcost in terms of discriminants (decisions madeby annotators about valid parses) to annotationtime.
This is a better approximation than unit costswhere such a relationship cannot be established.However, it is based on a static measurement ofannotation time, and clearly the time taken to an-notate an example is not a function of the examplealone.
Annotation time is actually dynamic in thatit is dependent on how many and what kinds ofexamples have already been annotated.
An ?infor-mative?
example is likely to take longer to anno-tate if selected early than it would after the anno-tator has seen many other examples.Thus, it is important to measure annotation timeembedded in the context of a particular annota-tion experiment with the sample selection/labelingstrategies of interest.
In our annotation experi-ments, we measure the exact time taken to anno-tate each example by each annotator and use thisas the cost metric, inspired by Ngai and Yarowsky(2000).
In the simulation studies, as we are un-able to measure time, we measure cost by sen-tence/clause and word/morpheme.Learning curve comparison.
We are interestedin comparative evaluation of many different exper-imental settings, across which we vary selectionmethods, use of label suggestions, and annotators.To achieve this, it is useful to have a summaryvalue for comparing the results from two individ-ual experiments.
One such measure is the percent-age error reduction (PER), measured over a dis-crete set of points on the first 20% of the points onthe learning curve (Melville and Mooney, 2004).2We use a new related measure, which we callthe overall percentage error reduction (OPER),that uses the entire area under the curves given by2This is justified in standard conditions, sampling from afinite corpus: active learning runs out of interesting examplesafter considering a fraction of the data, so the curve is artifi-cially pulled down by the remaining, boring examples.fitted nonlinear regression models rather than av-eraging over a subset of data points.
Specifically,we fit a modified Michaelis-Menton model:f(cost, (K,Vm, A)) =Vm(A + cost)K + costThe (original) parameters Vmand K respectivelycorrespond to the horizontal asymptote and thecost where accuracy is halfway between 0 and Vm.The additional parameter A allows for a better fitto our data by allowing for less sharp elbows andletting cost be zero.
Model parameters were de-termined with nls in R (Ritz and Streibig, 2008).With the fitted regression models, it is straight-forward to calculate the area under the curve be-tween a start cost ciand end cost cjby taking theintegral from cito cj.
The overall accuracy forthe experiment is given by dividing that area by100 ?
(cj?
ci).
Call this the overall curve accu-racy (OCA).
Then, for experiment A compared toexperiment B, OPER(A,B) =OCAA?OCAB100?OCAB.
Forthe simulation experiments we calculate OPER foronly the first 20% of cost units, like Melville andMooney.
For the annotation experiments, we cal-culate it for the minimum amount of time spent onany of the experiments (which ended up using lessthan 10% of all available morphemes).4 Simulation experimentsWe verify that our tagger and dataset behave asexpected in standard active learning experimentsby running simulations on the Uspanteko data set,and on POS-tagging for Danish, Dutch, English,and Swedish.
Here, we vary only the selectionmethod: sequential, random, or uncertainty.For each language, we randomly select a seedset of 10 labeled sentences.
The number of exam-ples selected to be labeled in each round beginsat 10 and doubles after every 20 rounds.
For randand unc, each batch of examples is selected from apool (size of 1000) that is itself randomly selectedfrom the entire set of remaining unlabeled exam-ples.
rand and unc experiments for each languageare replicated 5 times; splines and regressions arecomputed over all runs for each condition.Figure 1 gives learning curves for the Uspan-teko simulations, with cost measured in terms of(a) clauses and (b) morphemes.
Both graphs showthe usual behavior found in active learning exper-iments.
rand and unc both rise more quickly thanseq, and unc is well above rand.
The relation-ship between the methods is the same regardless2990 2000 4000 600050556065707580Number of clauses selectedAccuracyon all tokensUncertaintyRandomSequential0 10000 20000 30000 4000050556065707580Number of morphemes selectedAccuracyon all tokensUncertaintyRandomSequential(a) (b)Figure 1: Learning curves for simulations; (a) clause cost and (b) morphemes cost.
The dashed verticallines indicate (a) #clauses=1485 and (b) #morphemes=8695 (to compare OPER values).randsequncsequncrandUspanteko-Clauses 5.86 13.27 7.86Uspanteko-Morphs 7.47 11.68 4.55Table 2: OPER values for Uspanteko simulations,comparing clause and morpheme cost.ABindi-cates we compute OPER(A,B).of the cost metric, but the relative differences incost-savings are not, which we see when we lookat OPER values.The dashed vertical lines in the two graphs cor-respond to the 20% mark used to calculate OPERvalues, which are given in Table 2.
Most impor-tantly, note the much larger OPER for unc overrand with clause cost (7.86 vs 4.55).
Also notethat OPER(rand,seq) is lower with clause cost?this indicates that the beginning portions of thecorpus contain longer sentences with more mor-phemes, an accident which overstates how wellseq would likely work in general.Since rand is unbiased with respect to pick-ing longer sentences, the large increase ofOPER(unc,rand) from 4.55 to 7.86 is a clear in-dication of the well-known?but not always at-tended to?tendency of uncertainty sampling toselect longer sentences.
Consequently, one shouldat least use sub-sentence cost in order not to over-state the gains from active learning.
The annota-tion experiments in the next section take this wordrandsequncsequncrandDanish 4.58 6.95 2.48Dutch 21.95 23.68 2.20English 6.55 8.00 1.56Swedish 9.56 9.29 -0.30Uspanteko 7.47 11.68 4.55Table 3: OPER values for morpheme cost for sim-ulations.ABindicates we compute OPER(A,B).of caution one step further: even sub-sentence cost(morpheme cost, in our setting) can overestimategains since the morphemes selected are actuallyharder to annotate and thus take more time.Table 3 gives overall percentage error reduc-tions (OPER) between different selection methodsbased on word/morpheme cost, for each language.For all languages, rand and unc are better thanseq.
Only in the case of Swedish is there no ben-efit from unc over rand.
For Dutch, the largegains over seq for both rand and unc accuratelyreflect the heterogeneity of the underlying Alpinocorpus.3Most importantly, for Uspanteko, thereare large reductions from unc to rand to seq, mir-roring the clear trends in Figure 1b.These simulations have an unrealistic ?perfect?annotator, the corpus.
Next, we discuss resultswith real annotators?who may be fallible or may(reasonably) beg to differ with the corpus analysis.3http://www.let.rug.nl/vannoord/trees/3000 1000 2000 3000 4000203040506070Morphemes annotatedAccuracyon all tokensNon?Expert, No Suggest, SequentialNon?Expert, Suggest, RandomExpert, No Suggest, SequentialExpert, No Suggest, Uncertainty0 5000 10000 15000 20000 25000203040506070Cumulative annotation timeAccuracyon all tokensNon?Expert, No Suggest, SequentialNon?Expert, Suggest, RandomExpert, No Suggest, SequentialExpert, No Suggest, Uncertainty(a) (b)Figure 2: A sample of the learning curves with (a) morpheme cost and (b) time cost.
Morpheme costranks strategies for a given annotator similarly to time cost, but it gives dramatically different resultsfrom time cost when used to compare different annotators.5 Annotation experimentsWith two annotators (expert, non-expert), threeselection methods (seq, rand, unc), and two ma-chine labeling settings (ns, ds), we obtain 12 dif-ferent experiments.
Each experiment measures ac-curacy in terms of all words and unknown wordsand cost in terms of clauses, morphemes and time;this produces six views on every experiment.
Inthis paper we focus on one view: accuracy over allwords with time-based evaluation of cost.As with the simulations, clause cost in the an-notation experiments overestimates the cost reduc-tions.
For morpheme cost, the annotation experi-ments show that (a) it also overstates cost reduc-tions compared to time, and (b) it can mis-staterelative effectiveness when comparing annotators.The big picture.
Figure 2 shows curves for fourexperiments: seq-ns for both annotators4and themost effective overall condition for each annota-tor.
Figure 2a uses morpheme cost evaluation; onthat metric, both annotators appear to be aboutequally effective with seq-ns and much more ef-fective with machine involvement (unc or ds) thanwithout.
Additionally, the non-expert?s rand-dsappears to beat the expert?s unc-ns.
However, thetime cost evaluation in Figure 2b tells a dramat-ically different story.
Each annotator?s machine-4Recall that sequential annotation is the default mode forproducing IGT, so this strategy is of particular interest.involved experiment is much better than their seq-ns, but now the expert?s best is clearly better thanthe non-expert?s.
We see this as clear evidence forthe need for cost-sensitive learning over vanilla ac-tive learning (as we do here).5The non-expert with rand-ds caught up to andsurpassed the unaided expert in about six hourstotal annotation time, and he caught up to herunc-ns curve after 35 hours.
This is encourag-ing since often language documentation projectshave participants with a wide range of expertiselevels, and these results suggest that assistancefrom machine learning, if done properly, may in-crease the effectiveness of participants with lesslanguage-specific expertise.
We are also encour-aged, with respect to the effectiveness of activelearning, that the expert?s best performance is ob-tained with uncertainty-based selection.Within annotator comparisons.
Figure 3shows both actual measurements and the fittednonlinear regression curves used to computeOPER.
Figure 3a, the expert without suggestions,exhibits typical active learning behavior similar tothat seen in the simulation experiments.
Figure 3b,5It is also clear to see that, unsurprisingly, the expert spentmuch less time to complete the 56 rounds than the non-expert.In general, the expert annotator was much quicker, particu-larly in early rounds, averaging 4.1 seconds per morphemeannotated against the non-expert?s 8.0 second average.
SeePalmer et al (2009) for more details.301llllllllllllllllllllllllllllllllllllllllllllllllllllll0 5000 10000 15000 20000 25000 300003040506070Cumulative annotation timeAccuracyon all tokensl Expert, No Suggest, UncertaintyExpert, No Suggest, RandomExpert, No Suggest, Sequentiallllllllllllllll lllllll lllllllllllllllllllllllllllllllllll0 5000 10000 15000 20000 25000 300003040506070Cumulative annotation timeAccuracyon all tokensl Non?expert, Suggest, UncertaintyNon?expert, Suggest, RandomNon?expert, Suggest, SequentialNon?expert, No Suggest, Sequential(a) (b)Figure 3: Sample measurements and fitted nonlinear regression curves for (a) the expert and (b) thenon-expert.
Note that the scale is consistent for comparability.
The dashed vertical lines indicate 12,500seconds (about 35 hours), which is the upper limit used in computing OPER values for Table 4.the non-expert with suggestions, shows that in theds conditions the non-expert was less effectivewith unc.
This is not unexpected: uncertaintyselects harder examples that will either takelonger to annotate or are easier to get wrong,especially if the annotator trusts the classifier andespecially on examples the classifier is uncertainabout.
Nonetheless, in all ds cases, the non-expertperforms better than with seq-ns.OPER.
Table 4 provides OPER values fromtime 0 to 12,500 seconds (about 35 hours), theminimum amount of annotation time logged in anyone of the twelve experiments.6The table mixesthree types of comparison: (1) the boxed valueson the diagonal give OPER for the expert versusthe non-expert given the same selection and sug-gestion conditions; (2) the upper (right) trianglegives OPER for the expert versus herself for dif-ferent conditions; and (3) the lower (left) trian-gle is the non-expert versus himself.
For exam-ple: (1) the expert obtained an 11.52 OPER versusthe non-expert when both used rand-ns; (2) theexpert obtained a 10.52 OPER by using rand-dsrather than seq-ns; and (3) the non-expert obtaineda 5.93 OPER over rand-ns by using rand-ds.A number of patterns emerge.
Quite unsurpris-6Stopping at 12,500 seconds ensures a fair comparison,for example, between the expert and the non-expert becauseit requires no extrapolation of the expert?s performance.XXXXXXnon-expexpseq-ns rand-ns unc-ns seq-ds rand-ds unc-dsseq-ns 15.99 8.85 14.17 6.34 10.52 14.50rand-ns 13.46 11.52 5.83 -2.76 1.83 6.20unc-ns 19.20 6.63 10.76 -9.12 -4.25 0.39seq-ds 10.24 -3.72 -11.09 12.34 4.46 8.72rand-ds 18.59 5.93 -0.76 9.30 7.67 4.45unc-ds 11.19 -2.62 -9.91 1.06 -9.09 19.13Table 4: Overall percentage error reduction(OPER) comparisons, with timing cost.
See ex-planation of table in the OPER subsection.ingly, the values on the diagonal show that the ex-pert is more effective than the non-expert in allconditions.
Also, every other condition is more ef-fective than seq-ns for both annotators (first rowfor the expert, first column for the non-expert).unc-ns and rand-ds are particularly effective forthe non-expert, giving OPERs of 19.20 and 18.59over seq-ns, respectively.
These reductions, big-ger than the expert?s reductions of 14.17 and 10.52for the same conditions, considerably reduce thelarge gap in seq-ns effectiveness between the twoannotators (see Figure 2b).The expert actually gains very little from ds forboth rand and unc: adding suggestions gave OP-ERs of just 1.83 and .39, respectively.
In con-trast, the non-expert obtains an improvement of5.93 OPER when suggestions are used with rand,302but performs worse when used with unc (-9.91OPER).
Even more striking: the non-expert?sunc-ds is worse than rand-ns (-2.62 OPER), acompletely model-free setting.
These variationsdemonstrate the importance of modeling annotatorfallibility and sensitivity to cost, as well as char-acteristics of the annotation task itself, if learner-guided selection and suggestion are to be used(Donmez and Carbonell, 2008; Arora et al, 2009).Annotator accuracy.
Another factor whichmust be considered when annotation is done byhuman annotators (rather than being simulated)is the accuracy of the humans?
labels.
Table 5shows the overall accuracy of the annotators?
la-bels for each condition (after 56 rounds) as mea-sured against the original OKMA annotations.Unsurprisingly, unc selection picks examples thatare more difficult to annotate: accuracy for bothannotators suffers in both unc-ns and unc-ds.It may seem surprising that the non-expert?s ac-curacies are generally higher than the expert?s.The main reason for this is that the non-experttook nearly twice as long to annotate his examples,so each one was done with more care.
However,this difference also highlights challenges that arisewhen we bring active learning into non-simulatedannotation contexts.
The typical assumption isthat gold standard labeled data represents a true,fixed target, against which annotator or machine-predicted labels should be measured.
In languagedocumentation, though, the analysis of the lan-guage is continually evolving, and analysis andannotation each inform the other.
In fact, the ex-pert recognized (in the morphological segmenta-tion) several linguistic phenomena for which theanalysis has changed since the original OKMA an-notations were done.
As she changed her analy-ses, her labels diverged from those of the originalcorpus?another reason for her ?lower?
accuracy.This is to say that the ground truth of the currentOKMA annotations we had to work with can beviewed as one (valid) stage in the iterative reanal-ysis process that language documentation is.Error analysis.
Preliminary analysis of ?errors?made by the annotators supports the idea thatthe results seen in Table 5 are heavily influencedby changes in the expert?s analysis of the lan-guage.
Some duplicate clause annotation oc-curred for each annotator, because each of thetwelve annotator-selection-suggestion conditionsexpert non-expertseq-ns 73.17% 75.09%rand-ns 69.90% 74.37%unc-ns 61.23% 60.04%seq-ds 67.48% 73.13%rand-ds 68.34% 73.03%unc-ds 59.79% 60.27%Table 5: Overall accuracy of annotators?
labels,measured against OKMA annotations.drew from the same global set of unlabeled ex-amples.
This duplication allows us to measurethe consistency of each annotator on labeling suchduplicate clauses.
Table 6 shows the percentageof morphemes labeled consistently by each anno-tator.
Numbers for the expert appear in the top(right) triangle, and for the non-expert in the bot-tom (left) triangle.
Overall intra-annotator consis-tency is much higher for the expert (88.38%) thanfor the non-expert (81.64%), suggesting that theexpert maintained a more consistent mental modelof the language, but one which disagrees in someareas with the original annotations.Another key error source comes from differ-ences in use of one individual label: the annota-tors could assign a label that does not appear inthe original corpus.
This is yet another issue thatdoes not?in fact, cannot?arise in simulated ac-tive learning.
The label ESP was introduced for la-beling Spanish loans or insertions (such as the dis-course marker entonces) which do not have a clearfunction in Uspanteko grammar.
Such tokens areinconsistently labeled in the original corpus, usu-ally with catch-all categories like particle or ad-verb.
The annotators felt that the best analysis wasto mark the tokens as of Spanish origin.
The expertannotator used the ESP label for 2086 of 24129 to-kens (8.65%) versus 221 of 22819 tokens (0.97%)for the non-expert.
Any such token labeled withESP is scored as incorrect when compared to theOKMA standard, so this label alone accounts formore than 7% of the expert annotator?s total error.Finally, Table 7 presents inter-annotator agree-ment measured as percent agreement on mor-phemes in clauses labeled by both annotators.Note that in general agreement seems to be low-est for clauses duplicated in unc conditions, sup-porting the expected result that uncertainty-basedselection does indeed select clauses that are moredifficult for human annotators to label.303PPPPPPnonexpseq-ns rand-ns unc-ns seq-ds rand-ds unc-dsseq-ns ?
95.00% (41) 87.10% (56) 92.39% (60) 91.02% (28) 88.83% (51)rand-ns 90.11% (49) ?
90.91% (57) 87.57% (35) 90.94% (50) 89.53% (57)unc-ns 80.80% (44) 81.68% (54) ?
81.35% (41) 89.10% (40) 87.82% (332)seq-ds 90.00% (54) 87.94% (44) 77.97% (48) ?
86.13% (42) 82.14% (42)rand-ds 90.15% (52) 86.64% (45) 79.46% (62) 81.43% (44) ?
87.06% (49)unc-ds 84.15% (47) 78.55% (52) 77.68% (328) 78.81% (35) 77.95% (60) ?Table 6: Annotation consistency, expert and non-expert, (number of duplicate clauses, of 560 possible)PPPPPPnonexpseq-ns rand-ns unc-ns seq-ds rand-ds unc-dsseq-ns 69.91% (523) 70.82% (42) 62.42% (48) 72.35% (54) 74.25% (28) 67.82% (47)rand-ns 71.32% (48) 83.94% (39) 66.56% (47) 66.15% (43) 73.75% (42) 67.55% (52)unc-ns 66.31% (48) 67.87% (53) 62.31% (301) 58.87% (51) 73.31% (40) 61.10% (298)seq-ds 73.35% (60) 75.56% (34) 56.39% (37) 60.02% (540) 66.00% (44) 61.01% (36)rand-ds 68.67% (50) 76.40% (63) 66.67% (58) 65.88% (47) 76.33% (42) 66.99% (64)unc-ds 65.41% (50) 67.98% (55) 60.43% (263) 58.13% (38) 70.74% (57) 60.40% (275)Table 7: IAA: expert v. non-expert, percentage of morphemes in agreement, (number of duplicateclauses, of 560 possible)6 ConclusionThrough actual annotation experiments that con-trol for several factors, we have evaluated the po-tential of incorporating active learning and labelsuggestions to speed up morpheme glossing in arealistic language documentation context.
Someconfigurations of learner-guided example selec-tion and machine label suggestions perform farbetter than the standard strategy of sequential se-lection without suggestions.
However, the effec-tiveness of any given strategy depends on annota-tor expertise.
The impact of differences betweenannotators directly bears on the point made byDonmez and Carbonell (2008) that if cost reduc-tions are to be reliably obtained with active learn-ing techniques, annotators?
fallibility, unreliabil-ity, and sensitivity to cost must be modeled.Our results suggest some possible prescriptionsfor tuning techniques according to annotator ex-pertise.
However, even if we can estimate a rela-tive level of expertise, following such broad pre-scriptions is unlikely to be more robust than an ap-proach which adapts selection and suggestion tothe individual annotator, perhaps working withinan annotation group.
Indeed, it seems that dealingwith variation in annotators/oracles may be moreimportant than devising better selection strategies.The difference in performance due to expertisesuggests that using multiple annotators to checkrelative annotation rate and accuracy of differentannotators could be a key ingredient in any actu-ally deployed active learning system.
This couldprovide for better modeling of individual anno-tators as part of an annotation group they can becompared against, allowing the system, for exam-ple, to throttle active selection if an annotator ap-pears to be too slow or inaccurate.Another major issue we highlight is the uncer-tainty around the question of whether active learn-ing works in practical applications.
Respondentsto the survey of Tomanek and Olsson (2009) in-dicated that this uncertainty?will active learn-ing work?
what methods or techniques will workbest?
?is one of the reasons active learning is notwidely used in actual annotation.
In addition, cre-ating the necessary software infrastructure to buildan active learning enabled annotation system?a system which must interface robustly betweendata, annotator, and machine classifier, yet stillbe easy to use?is a substantial hurdle.
It seemsunlikely that there will be much uptake until a)consistent, large cost reductions can be shown inactual annotation studies, and b) appropriate, tun-able, widely-available software exists.AcknowledgmentsThis work is funded by NSF grant BCS 06651988?Reducing Annotation Effort in the Documenta-tion of Languages using Machine Learning andActive Learning.?
Thanks to Eric Campbell, Ka-trin Erk, Michel Jacobson, Taesun Moon, TelmaKaan Pixabaj, and Elias Ponvert.304ReferencesShilpa Arora, Eric Nyberg, and Carolyn P. Ros?e.
2009.Estimating annotation cost for active learning in amulti-annotator environment.
In Proceedings of theNAACL HLT Workshop on Active Learning for Nat-ural Language Processing, pages 18?26, Boulder,CO.Jason Baldridge and Miles Osborne.
2008.
Activelearning and logarithmic opinion pools for HPSGparse selection.
Natural Language Engineering,14(2):199?222.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.In Proceedings of the Tenth Conference on Com-putational Natural Language Learning (CoNLL-X),pages 149?164, New York City, June.
Associationfor Computational Linguistics.David A. Cohn, Zoubin Ghahramani, and Michael I.Jordan.
1995.
Active learning with statistical mod-els.
In G. Tesauro, D. Touretzky, and T. Leen, ed-itors, Advances in Neural Information ProcessingSystems, volume 7, pages 705?712.
The MIT Press.David Crystal.
2000.
Language Death.
CambridgeUniversity Press, Cambridge.James R. Curran and Stephen Clark.
2003.
Investigat-ing GIS and smoothing for maximum entropy tag-gers.
In Proceedings of the 10th Conference of theEuropean Association for Computational Linguis-tics, pages 91?98.Pinar Donmez and Jaime G. Carbonell.
2008.
Proac-tive learning: Cost-sensitive active learning withmultiple imperfect oracles.
In Proceedings ofCIKM08, Napa Valley, CA.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In Proceedings of the 9th Confer-ence on Computational Natural Language Learning,Ann Arbor, MI.Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger,and James L. Carroll.
2008.
Return on invest-ment for active learning.
In Proceedings of the NIPSWorkshop on Cost-Sensitive Learning.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional linguistics, 19:313?330.Prem Melville and Raymond J. Mooney.
2004.
Di-verse ensembles for active learning.
In Proceed-ings of the 21st International Conference on Ma-chine Learning, pages 584?591, Banff, Canada.Grace Ngai and David Yarowsky.
2000.
Rule writ-ing or annotation: cost-efficient resource usage forbase noun phrase chunking.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics, pages 117?125, Hong Kong.Alexis Palmer, Taesun Moon, and Jason Baldridge.2009.
Evaluating automation strategies in languagedocumentation.
In Proceedings of the NAACL HLT2009 Workshop on Active Learning for Natural Lan-guage Processing, pages 36?44, Boulder, CO.Telma Can Pixabaj, Miguel Angel Vicente M?endez,Mar?
?a Vicente M?endez, and Oswaldo Ajcot Dami?an.2007.
Text Collections in Four Mayan Languages.Archived in The Archive of the Indigenous Lan-guages of Latin America.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania, Philadelphia, PA.Christian Ritz and Jens Carl Streibig.
2008.
NonlinearRegression with R. Springer.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active learning with real annotation costs.
In Pro-ceedings of the NIPS Workshop on Cost-SensitiveLearning.Burr Settles.
2009.
Active learning literature survey.Technical Report Computer Sciences Technical Re-port 1648, University of Wisconsin-Madison.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - but is itgood?
Evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of EMNLP 2008,pages 254?263.Katrin Tomanek and Fredrik Olsson.
2009.
A WebSurvey on the Use of Active learning to support an-notation of text data.
In Proceedings of the NAACLHLT Workshop on Active Learning for Natural Lan-guage Processing, pages 45?48, Boulder, CO.Sudheendra Vijayanarasimhan and Kristen Grauman.2008.
Multi-level active prediction of useful im-age annotations for recognition.
In Proceedings ofNIPS08, Vancouver, Canada.305
