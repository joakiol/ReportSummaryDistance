Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 81?88,Avignon, France, April 23 - 24 2012. c?2012 Association for Computational LinguisticsEstimating and visualizing language similarities using weightedalignment and force-directed graph layoutGerhard Ja?gerUniversity of Tu?bingen, Department of Linguisticsgerhard.jaeger@uni-tuebingen.deAbstractThe paper reports several studies aboutquantifying language similarity via pho-netic alignment of core vocabulary items(taken from Wichman?s Automated Simi-larity Judgement Program data base).
Itturns out that weighted alignment accord-ing to the Needleman-Wunsch algorithmyields best results.For visualization and data exploration pur-poses, we used an implementation of theFruchterman-Reingold algorithm, a versionof force directed graph layout.
This soft-ware projects large amounts of data pointsto a two- or three-dimensional structure insuch a way that groups of mutually similaritems form spatial clusters.The exploratory studies conducted alongthese ways lead to suggestive results thatprovide evidence for historical relation-ships beyond the traditionally recognizedlanguage families.1 IntroductionThe Automated Similarity Judgment Program(Wichmann et al, 2010) is a collection of 40-itemSwadesh lists from more than 5,000 languages.The vocabulary items are all given in a uniform,if coarse-grained, phonetic transcription.In this project, we explore various ways to com-pute the pairwise similarities of these languagesbased on sequence alignment of translation pairs.As the 40 concepts that are covered in the database are usually thought to be resistant againstborrowing, these similarities provide informationabout genetic relationships between languages.To visualize and explore the emerging pat-terns, we make use of Force Directed Graph Lay-out.
More specifically, we use the CLANS1 im-plementation of the Fruchterman-Reingold algo-rithm (Frickey and Lupas, 2004).
This algorithmtakes a similarity matrix as input.
Each data pointis treated as a physical particle.
There is a re-pelling force between any two particles ?
youmay think of the particles as electrically chargedwith the same polarity.
Similarities are treated asattracting forces, with a strength that is positivelyrelated to the similarity between the correspond-ing data points.All data points are arranged in a two- or three-dimensional space.
The algorithm simulates themovement of the particles along the resultingforce vector and will eventually converge towardsan energy minimum.In the final state, groups of mutually similardata items form spatial clusters, and the distancebetween such clusters provides information abouttheir cumulative similarity.This approach has proven useful in bioinfor-matics, for instance to study the evolutionary his-tory of protein sequences.
Unlike more com-monly used methods like SplitsTree (or other phy-logenetic tree algorithms), CLANS does not as-sume an underlying tree structure; neither does itcompute a hypothetical phylogenetic tree or net-work.
The authors of this software package, Tan-cred Frickey and Andrei Lupas, argue that thisapproach is advantageous especially in situationswere a large amount of low-quality data are avail-able:?An alternative approach [...] is thevisualization of all-against-all pairwise1Cluster ANalysis of Sequences; freely available fromhttp://www.eb.tuebingen.mpg.de/departments/1-protein-evolution/software/clans81similarities.
This method can han-dle unrefined, unaligned data, includ-ing non-homologous sequences.
Un-like phylogenetic reconstruction it be-comes more accurate with an increas-ing number of sequences, as the largernumber of pairwise relationships aver-age out the spurious matches that arethe crux of simpler pairwise similarity-based analyses.?
(Frickey and Lupas2004, 3702)This paper investigates two issues, that are re-lated to the two topics of the workshop respec-tively:?
Which similarity measures over languagepairs based on the ASJP data are apt to sup-ply information about genetic relationshipsbetween languages??
What are the advantages and disadvantagesof a visualization method such as CLANS, ascompared to the more commonly used phy-logenetic tree algorithms, when applied tolarge scale language comparison?2 Comparing similarity measures2.1 The LDND distance measureIn Bakker et al (2009) a distance measure is de-fined that is based on the Levenshtein distance (=edit distance) between words from the two lan-guages to be compared.
Suppose two languages,L1 and L2, are to be compared.
In a first step,the normalized Levenshtein distances between allword pairs from L1 and L2 are computed.
(Ide-ally this should be 40 word pairs, but some dataare missing in the data base.)
This measure is de-fined asnld(x, y).=dLev(x, y)max(l(x), l(y)).
(1)The normalization term ensures that wordlength does not affect the distance measure.If L1 and L2 have small sound inventories witha large overlap (which is frequently the case fortonal languages), the distances between wordsfrom L1 and L2 will be low for non-cognatesbecause of the high probability of chance simi-larities.
If L1 and L2 have large sound inven-tories with little overlap, the distance betweenFigure 1: Simple alignmentnon-cognates will be low in comparison.
To cor-rect for this effect, Bakker et al (2009) normal-ize the distance between two synonymous wordsfrom L1 and L2 by defining the normalized Lev-enshtein distance by the average distance betweenall words from L1 and L2 that are non synony-mous (39?
40 = 1, 560 pairs if no data are miss-ing).
The NDLD distance between L1 and L2 isdefined as the average doubly normalized Leven-shtein distance between synonymous word pairsfrom L1 and L2.
(LDND is a distance measurerather than a similarity measure, but it is straight-forward to transform the one type of measure intothe other.
)In the remainder of this section, I will proposean improvement of LDND in two aspects:?
using weighted sequence alignment based onphonetic similarity, and?
correcting for the variance of alignments us-ing an information theoretic distance mea-sure.2.2 Weighted alignmentThe identity-based sequence alignment that un-derlies the computation of the Levenshtein dis-tance is rather coarse grained because it does notconsider different degrees of similarities betweensounds.
Consider the comparison of the Englishword hand (/hEnd/ in the ASJP transcription) toits German translation hand (/hant/) on the onehand and its Spanish translation mano (/mano/) onthe other hand.
As the comparison involves twoidentical and two non-identical sounds in eachcase (see Figure 1), the normalized Levenshteindistance is 0.5 in both cases.
It seems obviousthough that /hEnd/ is much more similar to /hant/than to /mano/, i.e.
it is much more likely to findan /a/ corresponding to an /E/ in words that arecognate, and and /d/ corresponding to a /t/, thanan /h/ corresponding to an /m/ or a /t/ to an /o/.82There is a parallel here to problems in bioin-formatics.
When aligning two protein sequences,we want to align molecules that are evolu-tionarily related.
Since not every mutation isequally likely, not all non-identity alignments areequally unlikely.
The Needleman-Wunsch algo-rithm (Needleman and Wunsch, 1970) takes asimilarity matrix between symbols as an input.Given two sequences, it computes the optimalglobal alignment, i.e.
the alignment that max-imizes the sum of similarities between alignedsymbols.Following Henikoff and Henikoff (1992), thestandard approach in bioinformatics to align pro-tein sequences with the Needleman-Wunsch algo-rithm is to use the BLOSUM (Block SubstitutionMatrix), which contains the log odds of aminoacid pairs, i.e.Sij ?
logpijqi ?
qj(2)Here S is the substitution matrix, pij is theprobability that amino acid i is aligned withamino acid j, and qi/qj are the relative frequen-cies of the amino acids i/j.This can straightforwardly be extrapolated tosound alignments.
The relative frequencies qi foreach sound i can be determined simply by count-ing sounds in the ASJP data base.The ASJP data base contains information aboutthe family and genus membership of the lan-guages involved.
This provides a key to estimatepij .
If two word x and y have the same meaningand come from two languages belonging to thesame family, there is a substantial probability thatthey are cognates (like /hEnd/ and /hant/ in Figure1).
In this case, some of the sounds are likely tobe unchanged.
This in turn enforces alignment ofnon-identical sounds that are historically related(like /E/-/a/ and /d/-/T/ in the example).Based on this intuition, I estimated p in the fol-lowing way:2?
Pick a family F at random that contains atleast two languages.?
Pick two languages L1 and L2 that both be-long to G.2A similar way to estimate sound similarities is proposedin Prokic (2010) under the name of pointwise mutual infor-mation in the context of a dialectometric study.qGX aEeou 3iT8L lrdt 4N5n mvw bfp gkhx 7!CczSs yZj?4 ?2 0 2 4 6 8 10Figure 2: Sound similarities?
Pick one of the forty Swadesh concepts thathas a corresponding word in both languages.?
Align these two words using the Levenshteindistance algorithm and store all alignmentpairs.This procedure was repeated 100,000 times.
Ofcourse most of the word pairs involved are notcognates, but it can be assumed in these cases, thealignments are largely random (except for univer-sal phonotactic patterns), such that genuine cog-nate alignments have a sufficiently large effect.Note that language families vary considerablyin size.
While the data base comprises more than1,000 Austronesian and more than 800 Niger-Congo languages, most families only consist of ahandful of languages.
As the procedure describedabove samples according to families rather thanlanguages, languages that belong to small familiesare over-represented.
This decision is intentional,because it prevents the algorithm from overfittingto the historically contingent properties of Aus-tronesian, Niger-Congo, and the few other largefamilies.The thus obtained log-odds matrix is visual-ized in Figure 2 using hierarchical clustering.
Theoutcome is phonetically plausible.
Articulatorilysimilar sounds ?
such as the vowels, the alve-olar sound, the labial sounds, the dental soundsetc.
?
form clusters, i.e.
they have high log-oddsamongst each other, while the log-odds betweensounds from different clusters are low.83Using weighted alignment, the similarity scorefor /hEnd/ ?
/hant/ comes out as ?
4.1, while/hEnd/ ?
/mano/ has a score of ?
0.2.2.3 Language specific normalizationThe second potential drawback of the LDNDmeasure pertains to the second normalization stepdescribed above.
The distances between trans-lation pairs are divided by the average distancebetween non-translation pairs.
This serves toneutralize the impact of the sound inventories ofthe languages involved ?
the distances betweenlanguages with small and similar sound invento-ries are generally higher than those between lan-guages with large and/or different sound invento-ries.Such a step is definitely necessary.
However,dividing by the average distance does not take theeffect of the variance of distances (or similarities)into account.
If the distances between words fromtwo languages have generally a low variance, theeffect of cognacy among translation pairs is lessvisible than otherwise.As an alternative, I propose the following simi-larity measure between words.
Suppose s is someindependently defined similarity measure (suchas the inverse normalized Levenshtein distance,or the Needleman-Wunsch similarity score).
Forsimplicity?s sake, L1 and L2 are identified withthe set of words from the respective languages inthe data base:si(x, y|L1, L2).= ?log |{(x?,y?)?L1?L2|s(x?,y?
)?s(x,y)}||L1|?|L2|The fraction gives the relative frequency ofword pairs that are at least as similar to each otherthan x to y.
If x and y are highly similar, thisexpression is close to 0.
Conversely, if they areentirely dissimilar, the expression is close to 0.The usage of the negative logarithm is mo-tivated by information theoretic considerations.Suppose you know a word x from L1 and youhave to pick out its translation from the words inL2.
A natural search procedure is to start withthe word from L2 which is most similar to x, andthen to proceed according to decreasing similar-ity.
The number of steps that this will take (or,up to a constant factor, the relative frequency ofword pairs that are more similar to each other thanx to its translation) is a measure of the distancebetween x and its translation.
Its logarithm corre-sponds (up to a constant factor) to the number ofbits that you need to find x?s translation.
Its nega-tion measures the amount of information that yougain about some word if you know its translationin the other language.The information theoretic similarity betweentwo languages is defined as the average similar-ity between its translation pairs.2.4 ComparisonThese considerations lead to four different simi-larity/distance measures:?
based on Levenshtein distance vs. based onNeedleman-Wunsch similarity score, and?
normalization via dividing by average scorevs.
information theoretic similarity measure.To evaluate these measures, I defined a goldstandard based on the know genetic affiliations oflanguages:gs(L1, L2).= 2 if L1 and L2belong to the same genusgs(L1, L2).= 1 if L1 and L2belong to the same familybut not the same genusgs(L1, L2).= 0 elseThree tests were performed for each metric.2,000 different languages were picked at randomand arranged into 1,000 pairs, and the four metricswere computed for each pair.
First, the correlationof these metrics with the gold standard was com-puted.
Second, a logistic regression model wasfitted, where a language pair has the value 1 if thelanguages belong to the same genus, and 0 oth-erwise.
Third, the same was repeated with fam-ilies rather than genera.
In both cases, the log-likelihood of another sample of 1,000 languagepairs according to the thus fitted models was com-puted.Table 1 gives the outcomes of these tests.
Theinformation theoretic similarity measure basedon the Needleman-Wunsch alignment score per-forms best in all three test.
It achieves the high-est correlation with the gold standard (the corre-lation coefficient for LDND is negative because itis a distance metric while the other measures are84metric correlation log-likelihood genus log-likelihood familyLDND ?0.62 ?116.0 ?583.6Levenshteini 0.61 ?110.5 ?530.5NW normalized 0.62 ?108.1 ?518.5NWi 0.64 ?106.7 ?514.5Table 1: Tests of the different similarity measuressimilarity metrics; only the absolute value mat-ters for the comparison), and it assigns the high-est log-likelihood on the test set both for familyequivalence and for genus equivalence.
We canthus conclude that this metric provides most in-formation about the genetic relationship betweenlanguages.3 Visualization using CLANSThe pairwise similarity between all languages inthe ASJP database (excluding creoles and artifi-cial languages) was computed according to thismetric, and the resulting matrix was fed intoCLANS.
The outcome of two runs, using the sameparameter settings, are given in Figure 3.
Eachcircle represents one language.
The circles arecolored according to the genus affiliation of thecorresponding language.
Figure 4 gives the leg-end.In both panels, the languages organize intoclusters.
Such clusters represent groups with ahigh mutual similarity.
With few exceptions, alllanguages within such a cluster belong to the samegenus.
Obviously, some families (such as Aus-tronesian ?
shown in dark blue ?
and Indo-European ?
shown in brown ?
have a high co-herence and neatly correspond to a single com-pact cluster.
Other families such as Australian ?shown in light blue ?
and Niger-Congo ?
shownin red ?
are more scattered.As can be seen from the two panels, the algo-rithm (which is initialized with a random state)may converge to different stable states with dif-ferent global configurations.
For instance, Indo-European is located somewhere between Aus-tronesian, Sino-Tibetan ?
shown in yellow ?,Trans-New-Guinea (gray) and Australian in theleft panel, but between Austronesian, Austro-Asiatic (orange) and Niger-Congo (red) in theright panel.
Nonetheless, some larger patternsare recurrent across simulations.
For instance, theTai-Kadai languages (light green) always end upFigure 4: Legend for Figure 385Figure 3: Languages of the worldin the proximity of the Austronesian languages.Likewise, the Nilo-Saharan languages (pink) donot always form a contiguous cluster, but they arealways near the Niger-Congo languages.It is premature to draw conclusions aboutdeep genetic relationships from such observa-tions.
Nonetheless, they indicate the presenceof weak but non-negligible similarities betweenthese families that deserve investigation.
Visual-ization via CLANS is a useful tool to detect suchweak signals in an exploratory fashion.4 The languages of EurasiaWorking with all 5,000+ languages at once intro-duces a considerable amount of noise.
In partic-ular the languages of the Americas and of PapuaNew Guinea do not show stable relationships toother language families.
Rather, they are spreadover the entire panel in a seemingly random fash-ion.
Restricting attention to the languages ofEurasia (also including those Afro-Asiatic lan-guages that are spoken in Africa) leads to morepronounced global patterns.In Figure 5 the outcome of two CLANS runs isshown.
Here the global pattern is virtually iden-tical across runs (modulo rotation).
The Dravid-ian languages (dark blue) are located at the cen-ter.
Afro-Asiatic (brown), Uralic (pink), Indo-European (red), Sino-Tibetan (yellow), Hmong-Mien (light orange), Austro-Asiatic (orange), andTai-Kadai (yellowish light green) are arrangedFigure 6: Legend for Figure 586Figure 5: The languages of Eurasiaaround the center.
Japanese (light blue) is locatedfurther to the periphery outside Sino-Tibetan.Outside Indo-European the families Chukotko-Kamchatkan (light purple), Mongolic-Tungusic(lighter green), Turkic (darker green)3 Kartvelian(dark purple) and Yukaghir (pinkish) are fur-ther towards the periphery beyond the Turkiclanguages.
The Caucasian languages (both theNorth Caucasian languages such as Lezgic andthe Northwest-Caucasian languages such as Abk-haz) are located at the periphery somewhere be-tween Indo-European and Sino-Tibetan.
Bu-rushaski (purple) is located near to the Afro-Asiatic languages.Some of these pattern coincide with proposalsabout macro-families that have been made in theliterature.
For instance the relative proximity ofIndo-European, Uralic, Chukotko-Kamchatkan,Mongolic-Tungusic, the Turkic languages, andKartvelian is reminiscent of the hypothetical Nos-tratic super-family.
Other patterns, such as theconsistent proximity of Japanese to Sino-Tibetan,is at odds with the findings of historical linguis-tics and might be due to language contact.
Otherpatterns, such as the affinity of Burushaski to theAfro-Asiatic languages, appear entirely puzzling.3According to the categorization used in ASJP, the Mon-golic, Tungusic, and Turkic languages form the genus Al-taic.
This classification is controversial in the literature.In CLANS, Mongolic/Tungusic consistently forms a singlecluster, and likewise does Turkic, but there is no indicationthat there is a closer relation between these two groups.5 ConclusionCLANS is a useful tool to aid automatic languageclassification.
An important advantage of thissoftware is its computational efficiency.
Produc-ing a cluster map for a 5,000 ?
5,000 similaritymatrix hardly takes more than an hour on a reg-ular laptop, while it is forbidding to run a phy-logenetic tree algorithm with this hardware andthis amount of data.
Next to this practical ad-vantage, CLANS presents information in a formatthat facilitates the discovery of macroscopic pat-terns that are not easily discernible with alterna-tive methods.
Therefore it is apt to be a usefuladdition to the computational toolbox of moderndata-oriented historical and typological languageresearch.AcknowledgmentsI would like to thank Andrei Lupas, David Er-schler, and S?ren Wichmann for many inspiringdiscussions.
Furthermore I am grateful for thecomments I received from the reviewers for thisworkshop.ReferencesDik Bakker, Andre?
Mu?ller, Viveka Velupillai, S?renWichmann, Cecil H. Brown, Pamela Brown, DmitryEgorov, Robert Mailhammer, Anthony Grant, andEric W. Holman.
2009.
Adding typology to lexico-87statistics: a combined approach to language classi-fication.
Linguistic Typology, 13:167?179.Tancred Frickey and Andrei N. Lupas.
2004.
Clans:a java application for visualizing protein fami-lies based on pairwise similarity.
Bioinformatics,20(18):3702?3704.Steven Henikoff and Jorja G. Henikoff.
1992.
Aminoacid substitution matrices from protein blocks.
Pro-ceedings of the National Academy of Sciences,89(22):10915?9.Saul B. Needleman and Christian D. Wunsch.
1970.A general method applicable to the search for simi-larities in the amino acid sequence of two proteins.Journal of Molecular Biology, 48:443453.Jelena Prokic.
2010.
Families and Resemblances.Ph.D.
thesis, Rijksuniversiteit Groningen.S?ren Wichmann, Andre?
Mu?ller, Viveka Velupil-lai, Cecil H. Brown, Eric W. Holman, PamelaBrown, Sebastian Sauppe, Oleg Belyaev, MatthiasUrban, Zarina Molochieva, Annkathrin Wett,Dik Bakker, Johann-Mattis List, Dmitry Egorov,Robert Mailhammer, David Beck, and HelenGeyer.
2010.
The ASJP Database (version 13).http://email.eva.mpg.de/?wichmann/ASJPHomePage.htm.88
