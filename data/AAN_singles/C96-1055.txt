Role of Word Sense Disambiguation i  Lexical Acquisition:Predicting Semantics from Syntactic CuesBonn ie  J. Dor r  and Doug JonesDepar tment  of Computer  Sc ience andIns t i tu te  for Advanced Computer  Stud iesUn ivers i ty  of Mary landA.V.
Wi l l i ams Bu i ld ingCol lege Park ,  MD 20742{bonnie, j ones}~umiacs, umd.
eduABSTRACTThis paper addresses the issue of word-sense ambiguityin extraction from machine-readable resources for the con-struction of large-scale knowledge sources.
We describe twoexperiments: one which ignored word-sense distinctions, re-sulting in 6.3% accuracy for semantic lassification of verbsbased on (Levin, 1993); and one which exploited word-sensedistinctions, resulting in 97.9% accuracy.
These experimentswere dual purpose: (1) to validate the central thesis of thework of (Levin, 1993), i.e., that verb semantics and syntacticbehavior are predictably related; (2) to demonstrate hat a15-fold improvement can be achieved in deriving semanticinformation from syntactic ues if we first divide the syntac-tic cues into distinct groupings that correlate with differentword senses.
Finally, we show that we can provide effectiveacquisition techniques for novel word senses using a combi-nation of online sources.1 In t roduct ionThis paper addresses the issue of word-sense ambigu-ity in extraction from machine-readable resources forthe construction of large-scale knowledge sources.
Wedescribe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for seman-tic classification of verbs based on (Levin, 1993); andone which exploited word-sense distinctions, resultingin 97.9% accuracy.
These experiments were dual pur-pose: (l) to validate the central thesis of the work of(Levin, 1993), i.e., that verb semantics and syntactic be-havior are predictably related; (2) to demonstrate hata 15-fold improvement can be achieved in deriving se-mantic information from syntactic ues if we first dividethe syntactic ues into distinct groupings that correlatewith different word senses.
Finally, we show that wecan provide effective acquisition techniques for novelword senses using a combination of online sources, inparticular, Longman's Dictionary of Contemporary En-glish (LDOCE) (Procter, 1978), Levin's verb classifica-tion scheme (Levin, 1993), and WordNet (Miller, 1985).We have used these techniques to build a database of10,000 English verb entries containing semantic infor-mation that we are currently porting into languagessuch as Arabic, Spanish, and Korean for multilingualNLP tasks such as foreign language tutoring and ma-chine translation.3222 Automat ic  Lex ica l  Acqu is i t ion  fo rNLP  TasksAs machine-readable resources (i.e., online dictionaries,thesauri, and other knowledge sources) become read-ily available to NLP researchers, automated acquisitionhas become increasingly more attractive.
Several re-searchers have noted that the average time needed toconstruct a lexical entry can be as much as 30 min-utes (see, e.g., (Neff and McCord, 1990; Copestakc etal., 1995; Walker and Amsler, 1986)).
Given that weare aiming for large-scale lexicons of 20-60,000 words,automation of the acquisition process has become a ne-cessity.Previous research in automatic acquisition focuscsprimarily on the use of statistical techniques, such asbilingual alignment (Church and Hanks, 1990; Kla-vans and Tzoukermann, 1996; Wu and Xia, 1995), orextraction of syntactic constructions from online dic-tionaries and corpora (Brant, 1993; Dorr, Garman,and Weinberg, 1995).
Others who have taken a moreknowledge-based (interlingual) approach (Lonsdale, Mi-tamura, and Nyberg, 1996) do not provide a meansfor systematically deriving the relation between sur-face syntactic structures and their underlying semanticrepresentations.
Those who have taken more argumentstructures into account, e.g., (Copestake t al., 1995),do not take full advantage of the systematic relation be-tween syntax and semantics during lexical acquisition.We adopt the central thesis of Levin (1993), i.e., thatthe semantic class of a verb and its syntactic behav-ior are predictably related.
We base our work on acorrelation between semantic classes and patterns ofgrammar codes in the Longman's Dictionary of Con-temporary English (LDOCE) (Procter, 1978).
Whilethe LDOCE has been used previously in automatic x-traction tasks (Alshawi, 1989; Farwell, Guthrie, andWilks, 1993; Boguraev and Briscoe, 1989; ,Wilks et al,1989; Wilks et al, 1990) these tasks are primarily con-cerned with the extraction of other types of informa-tion including syntactic phrase structure and broad ar-gument restrictions or with the derivation of semanticstructures from definition analyses.
The work of San-filippo and Poznanski (1992) is more closely related toour approach in that it attempts to recover a syntactic-semantic relation from machine-readable dictionaries.Itowever, they claim that the semantic lassification ofverbs based on standard machine-readable dictionaries(e.g., the LDOCE) is % hopeless pursuit \[since\] stan-dard dictionaries are simply not equipped to offer thiskind of information with consistency and exhaustive-ness.
"Others have also argued that the task of simplify-in K lexical entries on the basis of broad semantic lassmembership is complex and, perhaps, infeasible (see,e.g., Boguraev and llriscoe (1989)).
tlowever, a numberof researchers (l,'ilhnore, 1968; Grimshaw, 1990; Gru-ber, 1965; Guthrie et al, 1991; Hearst, 1991; Jackend-otr, 1983; Jackendoff, 1990; l,evin, 1993; Pinker, t989;Yarowsky, 1992) have demonstrated conclusively thatthere is a clear relationship between syntactic contextand word senses; it is our aim to exploit this relationshipfor the acquisition of semantic lexicons.3 Syntax-Semantics Relation: -VerbClassification Based on SyntacticBehaviorThe central thesis of (Levin, 1993) is that the seman-tics of a verb and its syntactic behavior are predictablyrelated.
As a demonstration that such predictable rela-tionships are not confined to an insignificant portion ofthe vocabulary, Levin surveys 4183 verbs, grouped into191 semantic lasses in Part Two of her book.
The syn-tactic behavior of these classes is illustrated with 1668example sentences, an average of 8 sentences per (:lass.Given the scope of bevin's work, it is not easy toverify the central thesis. '
lb this end, we created adatabase of Levin's verb classes and example sentencesfrom each class, and wrote a parser to extract, basic syn-tactic patterns from tire sentences.1 We then character-ized each semantic lass by a set of syntactic patterns,which we call a syntactic signature, and used the re-suiting database as the basis of two experiments, bothdesigned to to discover whether the syntactic signaturestell us anything about the meaning of the verbs.
2 '\['hefirst experiment, which we label Verb-Based, ignoresword-sense distinctions by assigning one syntactic sig-nature to each verb, regardless of whether it occurredin multiple classes.
The second experiment, which welabel Class-Based, implicitly takes word-sense distinc-tions into account by considering each occurrence of averb individually and assigning it a single syntactic sig-nature according to class membership.The remainder of this section describes the assign-rnent of signatures to semantic busses and the two ex-periments for determining the relation of syntactic in-formation to semantic btsses.
We will see that our clas-sitication technique shows a 15-fold improvement in theexperiment where we implicitly account for word-sensedistinctions.1Both the database and the parser are encoded in Quin-tus Prolog.2The design of this experiment is inspired by the workof (Dubois and Saint-Dizier, 1995).
In particular, we departfrom the alternation-based data in (Levin, 1993), which isprimarily binary in that sentences are presented in pairswhich constitute an alternation.
Following Saint-Dizier'swork, we construct N-ary syntactic haracterizations.
Thechoice is of no empirieM consequence, but it simplifms theexperiment by eliminating the problem of naming the syn-tactic patterns.Verbs: break, chip, crack, crash, crush, fracture, rip,shatter, slnash, snap, sl)linter, split, tearExample Sentences:Crystal vases break easily.The hammer broke the window.The window broke.q'ony broke her arm.
'l?ony broke his finger.
"lbny broke the crystal vase.qbny broke the cup against he wall.q'ony broke the glass to 1)ieces.Tony broke the piggy bank open.Tony broke the window with a hanuner.Tony broke the window.
*Tony broke at tit(; window.
*qbny broke herself on the arm.
*Tony broke himself.
*qbny broke the wall with the cup.A break.Derived Syntactic Signature:1-\[np,v\] 1-\[np,v,np\] 1 - \ [np ,v ,np ,ad ject iw\ ]1- \[np, v, np ,pp(against) \] l-\[np,v,np,pp(to)\]1- \[np, v, np,pp (with) \] 1- \[np, v, pess, np\]1- \ [np,v,adv(easi ly) \ ]  l - in\ ]O-\[np,v,np,pp(with)\] 0- \ [np,v ,se l f \ ]O-\[np,v,seH,pp(on)\] 0- \ [np,v,pp(at) \ ]Table 1: Syntactic Signatm:e for Change of State breaksubclass3.1 Ass lgntnent  of  S ignaturesFor tile first experiment below, we construct a verb-based syntactic signature, while for the second exl)eri-ment, we constructed a class-based signature.The first step for constructing a signature is todecide what syntactic information to extract for ttret)asic syntactic patterns that make up the signature.It turns out that a very simple strategy works well,namely, flat parses that contain lists of the major cat-egories in the sentence, the verb, and a handfifl ofother elements.
The "parse", then, for the sentenceTony broke the c rys ta l  vase is simply the syntac-tic pattern \[np,v,np\].
For Tony broke the vase topieces we get \[np,v,np,pp(to)\].
Note that the ppnode is marked with its head preposition.
Table l showsan example class, the break subclass of the Change ofState verbs (45.1), along with example sentences andthe derived syntactic signature based on sentence pat-terns.
Positive example sentences are denoted by thenumber 1 in the sentence patterns and negative xamplesentences are denoted by the number 0 (correspondingto sentences marked with a *).3.2 Exper iment  1: Verb -based  ApproachIn the first experiment, we ignored word sense distinc-tions and considered each verb only once, regardless ofwhether it occurred in multiple classes.
In fact;, 46%of the verbs appear more than once.
In some cases,the verb appears to have a related sense even though itappears in different classes.
For example, the verb rollappears in two subclasses of Manner of Motion Verbsthat are distinguished on the basis of whether the gram-matical subject is animate or inanimate.
In other cases,tile verb may have (largely) unrelated senses.
For ex-ample, the verb move is both a Manner of Motion verb323and verb of Psychological State.To compose the syntactic signatures for each verb,we collect all of the syntactic patterns associated withevery class a particular verb appears in, regardless of thedifferent classes are semantically related.
A syntacticsignature for a verb, by definition, is the union of theframes extracted from every example sentence for eachverb.
The outline of the verb-based experiment is asfollows:1.
Automatically extract syntactic information from theexample sentences.2.
Group the verbs according to their syntactic signature.3.
Determine where the two ways of grouping verbs over-lap:(a) the semantic lassification given by Levin.
(1)) the syntactic classification based on the derivedsyntactic signatures.To return to the Change of State verbs, we now con-sider the syntactic signature of the verb break, ratherthan the signature of the semantic lass as a unit.
Theverb break belongs not only to the Change of Stateclass, but also four other classes: 10.6 Cheat, 23.2 Split,40.8.3 Hurl, and 48.1.1 Appear.
Each of these classes ischaracterized syntactically with a set of sentences.
Theunion of the syntactic patterns corresponding to thesesentences forms the syntactic signature for the verb.
Soalthough the signature for the Change of State class has13 frames, the verb break has 39 frames from the otherclasses it appears in.Conceptually, it is helpful to consider the differencebetween the intension of a function versus its exten-sion.
In this case, we are interested in the functionsthat group the verbs syntactically and semantically.
In-tensionally speaking, the definition of the function thatgroups verbs semantically would have something to dowith the actual meaning of the verbs.
~ Likewise, the in-tension of the function that groups verbs syntacticallywould be defined in terms of something strictly syntac-tic, such as subcategorization frames.
But the inten-sions of these functions are matters of significant he-oretical investigation, and although much has been ac-complished in this ~rea, the question of mapping syntaxto semantics and vice versa is an open research topic.Therefore, we can turn to the extensions of the func-tions: the actual groupings of verbs, based on these twoseparate criteria.
The semantic extensions are sets ofverb tokens, and likewise, the syntactic extensions aresets of verb tokens.
To the extent that these functionsmap between syntax and semantics intensionally, theywill pick out the same verbs extensionally.So for the verb-based experiment, our technique forestablishing the relatedness between the syntactic signa-tures and the semantic lasses, is mediated by the verbsthemselves.
We compare the two orthogonal groupingsof the inventory of verbs: the semantic lasses definedby Levin and the sets of verbs that correspond to eachof the derived syntactic signatures.
When these twogroupings overlap, we have discovered a mapping fromthe syntax of the verbs to their semantics, via the verbtokens.
More specifically, we define the overlap index3An example of the intensional characterization of theLevin classes are the definitions of Lexical Conceptual Struc-tures which correspond to each of Levin's semantic lasses.See (Dorr and Voss, to appear).as the number of overlapping verbs divided by the av-erage of the number of verbs in the semantic lass andthe number of verbs in the syntactic signature.
Thus anoverlap index of 1.00 is a complete overlap and an over-lap of 0 is completely disjoint.
In this experiment, hesets of verbs with a high overlap index are of interest.When we parsed the 1668 example sentences in PartTwo of Levin's book (including the negative xamples),these sentences reduce to 282 unique patterns.
The 191sets of sentences listed with each of the 191 semanticclasses in turn reduces to 748 distinct syntactic signa-tures.
Since there are far more syntactic signatures thanthe 191 semantic lasses, it is clear that the mappingbetween signatures and semantic classes is not direct,.Only 12 mappings have complete overlaps.
That means6.3% of the 191 semantic lasses have a complete over-lap with a syntactic signature.The results of this experiment are shown in Table 2.Three values are shown for each of the six variations inthe experiment: the mean overlap, the median overlap,and the percentage of perfect overlaps (overlaps of value1.00).
In every case, the median is higher than themean.
Put another way, there is always a cluster ofgood overlaps, but the general tendency is to have fairlypoor overlaps.The six variations of the experiment are as follows.The first distinction is whether or not to count the neg-ative evidence.
We note that the use of negative xam-ples, i.e., plausible uses of the verb in contexts whichare disallowed, was a key component of this experi-ment.
There are 1082 positive examples and 586 nega-tive examples.
Although this evidence is useful, it is notavailable in dictionaries, corpora, or other convenientresources that could be used to extend Levin's classi-fication.
Thus, to extend our approach to novel wordsenses (i.e., words not occurring in Levin), we wouldnot be able to use negative evidence.
For this reason,we felt it necessary to determine the importance of nega-tive evidence for building uniquely identifying syntacticsignatures.
As one might expect, throwing out the neg-ative evidence degrades the usefulness of the signaturesacross the board.
The results which had the negativeevidence are shown in the left-hand column of numbersin Table 2, and the results which had only positive evi-dence are shown in the right-hand side.The second, three-way, distinction involves preposi-tions, and breaks the two previous distinctions involv-ing negative vidence into three sub-cases.
Because wewere interested in the role of prepositions in the sig-natures, we also ran the experiment with two differentparse types: ones that ignored the actual prepositionsin the pp's, and ones that ignored all information exceptfor the values of the prepositions.
Interestingly, we stillgot useful results with these impoverished parses, al-though fewer semantic lasses had uniquely-identifyingsyntactic signatures under these conditions.
These re-sults are shown in the three major rows of Table 2.The best result, using both positive and negative v-idence to identify semantic classes, gives 6.3% of theverbs having perfect overlaps relating semantic lassesto syntactic signatures.
See Table 2 for the full results.3.3 Experiment 2" Class-based ApproachIn this experiment, we attempt o discover whether eachclass-based syntactic signature uniquely identifies a sin-324Verb-based Exper iment  (No Disami)iguation):ed,~sitions~i~ ed)sitionsqYgfy)sitionsOverlapMedianMeanPerfectMedianMeanPerfectMedianMeanPerfectWith NoNegative NegativeEvidence EvidenceO.lO 0.090.17 0.176.3% 5.2%0.t0 0.090.17 O.
166.3% 4.2%0.10 0.090.16 0.7153.1% 3.1%Table 2: Verb-Ba~sed ResultsClass-based Exper iment  (Disambiguated Verbs)- With NoNegative NegativeOverlap Evidence EvidenceMarkedPrepositions~lgnoredPret)ositions~3nlyPrepositions-TVledianMeanPerfectMedianMeanPerfectMedianMeanPerfect1.00 1.000.99 0.9397.9% 88.0%-1.00 1.000.96 0.6987.4% 52.4%1.00 0.540.82 0.5766.5% 42.9%'fable 3: Cla~ss-Based l{esnltsgle semantic lass.
By h)cnsing on the classes, the verbsare implicitly disambiguated: the word sense is by def-inition the sense of the verb as a member of a givenclass.
To compare these signatures with the previousverb-based signatures, it may be helpfnl to note thata verb-based signature is the union of all of the class~based signatures of the semantic lasses that the verbappears m.'Fhe outline for this class-based exl)eriment is as fol-lows:1.
Automatically extract syntactic information from tileexample sentences to yMd the syntactic signatnre forthe class.2.
Determine which semantic classes have uniquely-identifying syntactic signatures.If we use the class-based syntactic signatures contain-ing t)rcposition-marked pp's and both positive and neg-ative evidence, the 1668 example sentences reduce to282 syntactic patterns, just as before.
But now thereare 189 class-based syntactic signatures, as comparedwith 748 verb-based signatures from before.
187 of themmriquely identify a semantic (:lass, meaning that 97.9%of the classes have uniquely identifying syntactic signa-tures.
Four of the semantic lasses do not have enoughsyntactic information to distinguish them uniquely.
4Although the effects of the various distinctions werepresent in the verb-based experiment, these effects aremuch clearer in the class-based experiments.
The effectsof negative and positive evidence, as well as the threeways of handling prepositions how up much clearerhere, as is clear in Table 4.In the class-based experiment, we counted the per-centage of semantic classes that had uniquely ide.nti-fying signatures.
In the verb-based experiment, wecounted the number of perfect overlaps (i.e., index of1.00) between the verbs as grouped in the semanticclasses and grouped by syntactic signature.
The over-all results of the suite of experiments, illustrating tilerole of disambiguation, egative vidence, and preposi-tions, is shown in Table 4.
There were three ways oftreating prepositions: (i) mark the pp with the prepo-sition, (ii) ignore the preposition, and (iii) keel) onlythe prepositions.
For these different strategies, we seethe percentage of perfect overlaps, as well as both tire4Two of these classes correspond to one of the two non-unique signatures, and two (:orrespond to the other non-unique signature.median and mean overlap ratios for each experiment.
'Fhese data show that the most important factor in theexperiments i word-sense disambiguation.Marked Prepositionsignored PrepositionsOnly Prepositions~W~{h Dismnl)lguationMarked PrepositionsIgnored PrepositionsOnly PrepositionsWith NoNegative NegativeEvidence Evidence6.3% 5.2%6.3% 4.2%3.1% 3.1%97.9% 88.
{)%87.4% 52.4%66.5% 42.9%Table 4: Overall Results4 Semant ic  C lass i f i ca t ion  o f  Nove lWordsAs we saw above, word sense disambiguation is criticalto tile success of any \[exical acquisition algorithm.
TheLevin-based verbs are already disambiguated by virtueof their membership in different classes.
The difficulty,then, is to disambiguate and classify verbs that do notoccur in Levin.
Our current direction is to make useof the results of tire first two experiments, i.e., the re-lation t)etween syntactic patterns and semantic lasses,but to use two additional techniques for disambiguationand classification of non-Levin verbs: (1) extraction ofsynonym sets provided in WordNet (Miller, 1985), anonline lexical database containing thesaurus-like rela-tions such as synonymy; and (2) selection of appropri-ate synonyms based on correlations between syntacticinformation in l ,ongman's Dictionary of ContemporaryEnglish (LDOCF,) (Procter, 1978) and semantic lassesin Levin.
'Phe basic idea is to first determine tire mostlikely candidates for semantic lassification of a verb byexamining the verb's synonym sets, many of which in-tersect directly with the verbs classified by Leviu.
The"closest" synonyms are then selected fl'om these sets bycomparing the LDOCE grammar codes of tire unknownword with those associated with each synonym candi-date.
The use of LDOCE as a syntactic filter on tiresemantics derived from WordNet is tire key to resolv-ing word-sense ambiguity during the acquisition pro-cess.
The fldl acquisition algorithm is as follows:325Given a verb, check I,evin (:lass.1.
If in Levitt, classify directly.2.
if not in Levin, find synonym set from WordNet.
(a) if synonym in Levin, select, the class thathas the closest match with canonical LDOCEcodes.
(b) If no synonyms in Levin or canonical LDOCEcodes are completely mismatched, hypothesizenew class.Note that this algorithm assmnes that there is a"canonicM" set of LDOCE codes tbr each of Levin'ssemantic lasses.
Table 5 describes the significance ofa subset of the syntactic codes in LDOCE.
(The totalnmnber of codes is 174.)
We have developed a relationbetween LDOCE codes and Levin classes, in mnch thesame way that we associated syntactic signatures withthe semantic lasses in the earlier experiments.
Thesecanonical codes are for syntactic filtering (checking forthe closest match) in the classification algorithm.As an example of how the word-sense disambigua-tion process and classifcation, consider the non-Levinverb attempt.
The LDOCE specification for this verbis: T1 T3 T4 WV5 N. Using the synonymy featureof WordNet, the algorithm automatically extracts tirecandidate classes associated with the synonyms of thisword: (1) Class 29.6 "Masquerade Verbs" (ace), (2)Class 29.8 "Captain Verbs" (pioneer), (3) Class 31.1"Amuse Verbs" (try), (4) Class 35.6 "Ferret Verbs"(seek), and (5) Class 55.2 "Complete Verbs" (initiate).The synonyms for each of these classes have the follow-ing LDOCE encodiugs, respectively: (1) I I -FOIl I-ONI-UPON LI L9 T1 N; (2) L9 T1 N; (3) I T1 T3 T4 WV4N; (4) ~ bAF'\['EI~ I-FOR T1 T3; and (5) T1 T I - INTON.
The largest intersection with the syntactic odes forattempt occurs with the verb try (TI T3 T4 N).
How-ever, Levin's class 31.1 is not the correct class for at-tempt since this sense of try has a "negative amuse"meaning (e.g., John's behavior tried my patience.
Infact, the (:odes T1 'l'3 '1'4 are not part of the canonicalclass-code mapping associated with class 31.1.
Thus, at-tempt falls under case 2(b) of the algorithm, and a newclass is hypothesized.
This is a case where word-sensedisambiguation has allowed us to classify a new wordand to enhance Levin's verb classification by adding anew class to the word try as well.
In our experiment;s,our algorithm found severM additional non-Levin verbsthat fell into this newly hypothesized (;lass, includingaspire, attempt, dare, decide, desire, elect, need, andswear.We have automatically classified 10,000 "unknown"verbs, i.e., those not occurring in the Levin classifica-tion, using this technique.
These verbs are taken fromi e , translations provided in bilin- English "glosses" ( .
.  )
.gual dictionaries for Spanish and Arabic)  As a pre-liminary measure of success, we picked out 84 L1)OCEcontrol vocabulary verbs, (i.e., primitive words used fordefning dictionary entries) and hand-checked our re-sults.
We found that 69 verbs were classifed correctly,SThe Spanish-English dictionary was built at the Univer-sity of Maryland; The Arabic-English dictionary was pro-duced by Alpnet, a company in Utah that develops transla-tion aids.
We are Mso in the process of developing bilingualdictionaries for Korean and French, and we will be portingour LCS acquisition technology to these languages in thenear future.i.e., 82% accuracy.5 SummaryWe have conducted two experiments with the intent ofaddressing the issue of word-sense ambiguity in extrac-tion from machine-readable resources for the construetion of large-scale knowledge sources.
In the first exper-iment, verbs that appeared in different classes collectedthe syntactic information fl'om each class it appearedin.
Therefore, the syntactic signature was coml)osedfrom all of the example sentences fi'om every (:lass theverb appeared in.
In some cases, the verbs were seanan-tically unrelated and consequently the mat)ping fromsyntax to semantics was muddied.
'\['he second exper-iment attelnpted to determine a relationship betweena semantic lass and the syntactic information associ-ated with each class.
Not surprisingly, but not insignif-icantly, this relationship was very clear, since this ex-periment avoided the problem of word sense ambiguity.These experiments served to validate Levin's claim thatverb semantics and syntactic behavior are predictablyrelated and also demonstrated that a significant con>ponent of any lexical acquisition program is the abilityto perform word-sense disambiguation.We have used the results of our first two experimentsto help in constructing and augmenting online dictio-naries for novel verb senses.
We have used the samesyntactic signatures to categorize new verbs into Lcvin'sclasses on the basis of WordNet and 1,1)O(?1!3.
We arecurrently porting these results to new languages usingonline bilingual lexicons.AcknowledgementsThe research reported herein was supported, in part,by Army l{,esearch Office contract I)AAL03-91-C-0034through Battelle Corporation, NSF NYI IRl-9357731,Alfred P. Sloan Research Fellow Award BR3336, and aGeneral Research Board Semester Award.ReferencesAlshawi, H. 1989.
Analysing the Dictionary l)efini-tions.
In B. Boguraev and T. Briscoe, editor, Compuorational Lexicography for Natural Language Prvcess-ing.
Longman, London, pages 153 169.Boguraev, B. and T. Briscoe.
1989.
Utilising theLDOCE Grammar Codes.
In B. Boguraev and T.Briscoe, editor, Computational Lexicography for" Nat-ural Language Processing.
Longman, London, pages85-116.Brent, M. 1993.
Unsupervised Learning of Lexical Syn-tax.
Computational Linguistics, 19:243-262.Chm'ch, K. and P. Hanks.
1990.
Word AssociationNorlns, Mutual Information and Lexicography.
Co'm-pntational Linguistics, 1(5:22 29.Copestake, A., T. Briscoe, 1 ).
Vossen, A. Ageno,I.
Castellon, F. l{ibas, (J. t{igau, 1t.
l{odr{guez, andA.
Samiotou.
1995.
Acquisition of LexicM Transla-tion Relations from MRDS.
Machine Translation, 9.l)orr, B., J. Garlnan, and A. Weinberg.
1995. l,'romSyntactic Encodings to '\['hematic Roles: BuildingLexieal Entries lbr Interlingual MT.
Machine 7'rans-lation, 9.326LDOCE Code Argmnentg Adjuncts ExmnI)leII-AI,'TERI- I,'O 1/.I-ONnI-UPON\], 1L9 -q,\] .
.
.
.
.
.
.
.
.
.
.
.
.T1-IN~I'OT3q'4NPADV/PPNPNPt 'P~af~T q , ~ .
\ ] -  - -PP\[on\]Olivier is acting t, onightShe sought ;ffter the truthThey sought for the right ottolie acted on our suggestion'\['he drug acted upon the pa.in11(.'
a('ts the exl)erienced man'l'he \])by a('.ts well- I i ) loneered tit(: new bum-WeT~it~,eTd-him into tit(: grouplie.
tr led to do it7qh(7 Grh.~dl eating the new foodWV4 ing adjectiwd I've ha.d a, trying day-ed adjectiwd WV5 lie was convicted for attelnpte(1 murderTabh'.
5: Sample Syntactic Codes used in IA)OCI';l)orr, B. and C. Voss.
to appear.
A Multiq,evel Approach to lnterlingual MT: I)etining the Interfacebetween l{epresentt~tional l, nguages, l'nlcr'nationalJou'rnal of l,,',pert Systcms.Dubois, \]).
and 1 ).
Saint-Dizier.
1995.
Construction el,repres6ntation de classes sdmantiques de verbes: unecoop&ation entre syntaxe et cognition, manuscript,IRIT- CNRS, Toulouse, li'rance.Farwell, D., 1,.
Guthrie, trod Y. Wilks.
1993.
Automat-ically Creating Lexical Entries for UI,'FRA, a Multi-lingual MT System.
Machine 7Y'anslaliou, 8(3).Fillmore, C.,I.
1968.
The Case lbr (?ase.
In 1,',.
Bach andt~.
'1'./larms, editor, Universals in Linguistic Th, cory.\[lolt, l{,inehart, a11d Winston, p~ges 1 88.Grimshaw, J.
1990.
Argument b'lruclure.
MIT Press,Cambridge, MA.Gruber, J.S.
1965.
Studies in Lcxical Rela?ions.
Ph.I).thesis, MIT, Cambridge, MA.Guthrie, J., L. Guthrie, Y. Wilks, and 11.
Aidinciad.19911.
Subjeet-l)ependent Co occurrence and WordSense l)isambiguation.
In l~roceedings oJlhc 29th An-nual Meeting of lhe Associalion for CompulalionalLinguistics, pages 146 152, (Jniversity of California,Berkeley, CA.Ilearst, M. 1991.
Noun llomograph l)isambiguationUsing 1,oeal ('Jontext in Large 'Pext Corl)ora.
In UsingCorpora, University of Waterloo, Waterloo, Ontario.,l~u;kendoff, R,.
1983.
Semantics aud Cognition.
MtTPress, Cambridge, MA.Jackendoff, R. 1990. fiemanlic Structures.
MIT Press,Cambridge, MA.Klawms, J.L.
and 1,;.
q'zoukermmm.
1996.
Dictionar-ies and (k)rpora: (Jolnbiniug (~ort)us aud Machine,-readable Dictionary l)ata lbr Building 13ilingual I,exicons.
Machine 'lYanslatiou, 10.Levin, B.
1993. l'h~.glid~, Verb Classes and Alternation.s:A Preliminary Investigation.
Chicttgo, II,.l,onsdale, I)., T. Mitamura, and F,.
Nyberg.
19!)6.
Ac.quisition of l,arge Lexicous for Practical Knowledge-Based MT.
Machine Translation,'9.Miller, (\].
1985.
WOIU)NF, T: A /)ictiouary lh:owser.In Proceedings of the First lnlcrualional Conferenceon htJbrmalion, i'n l)ala, University of Waterloo (',en-tre for the New ()li)l), Waterloo, Ontario.Neff, M. and M. Mc(;ord.
1990.
Acquiring I,exh:atI)ata f'ronl Machinc-II.eadable )ictionary I~esourccsfor Machine Translation.
\]ii Third l'nter'nalional Con-ference on Theoretical and Methodological issues inMachine Translation oJ'Nahwal Languages (7'MI-90),A us t in, 'l'exas.Pinker, S. 1989. hearnability and Cognilion: The Ac-quisitiou of Argument Structure.
MIq' Press, Cam-bridge, MA.Procter, P. 1978.
Lou, gman Dictionary of Conlcmpoora W I5'uglish.
l,onginan, l,ondon.Sanfilippo, A. and V. I'ozmmsk\[.
1992.
The Acquisi-tion of \[,exieal Knowledge from (~olnbine(l Machine-Readable Dictionary Ih-.som'('es.
In Proceedings ofth.e Applied Natural Languaqc Processing Co'nfcrencc,pages 80 87, Tren{;o, Italy.Walker, 1).
and R,.
Ams\]er.
1986.
The Use of Machine--readable Dictionaries in Sublanguage Atmlysis.
In1L Grishman and IL Kittredge, editors, AnalyzingLanguage in t{csl'ricled Domains.
l,awrence ErlbaumAssociates, llillsdale, New Jersey, pages 69 83.Wilks, Y., 71).
Fass, C.M.
Guo, ,1.1';.
M('Domdd, andT.
l'|ate.
1990.
Providing Machine 't'ractable \[)ic-~ionary 'lbols.
Machine 7'ranslalion, 5(2):99 154.Wilks, Y., 1).
Fass, C.M.
Guo, a .E.
M('l)onald, '\['.
P\]al,e,and B.M.
Slator.
1989.
A Tractable Machine Dicl;io-nary as a. I{.esource for Computational Semanl, ics.
InB.
Bogura.ev and T. Briscoe, editor, ComputaiionalLexicograph, y for' Natural Language Processing.
\[,oug--man, l,ondon, pages 85 116.Wu, \]).
and X. Xia.
19!)5.
I,m'ge-Seale Automatic Ex-traction o\[' an l,\]nglish-Chinese, Translation l,exieon.Machine Translalion, 9.Yarowsky, 1).
1992.
Word-Sense l)isambigual, io|~:, Using Statistical Models of Roget's Categories Trainc.,don l,arge Corpora.
In Proceedings of the l/ou'rlce~lhlntc'rnatioual UonJi:renc~ on Compulalional Linguis-lie.s, pages 454 460, Nantes, l~'rancc.327
