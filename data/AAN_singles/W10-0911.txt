Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMachine Reading at the University of WashingtonHoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,Stephen Soderland, Dan Weld, Fei Wu, Congle ZhangDepartment of Computer Science & EngineeringUniversity of WashingtonSeattle, WA 98195{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.eduAbstractMachine reading is a long-standing goal of AIand NLP.
In recent years, tremendous progresshas been made in developing machine learningapproaches for many of its subtasks such asparsing, information extraction, and questionanswering.
However, existing end-to-end so-lutions typically require substantial amount ofhuman efforts (e.g., labeled data and/or man-ual engineering), and are not well poised forWeb-scale knowledge acquisition.
In this pa-per, we propose a unifying approach for ma-chine reading by bootstrapping from the easi-est extractable knowledge and conquering thelong tail via a self-supervised learning pro-cess.
This self-supervision is powered by jointinference based on Markov logic, and is madescalable by leveraging hierarchical structuresand coarse-to-fine inference.
Researchers atthe University of Washington have taken thefirst steps in this direction.
Our existing workexplores the wide spectrum of this vision andshows its promise.1 IntroductionMachine reading, or learning by reading, aims toextract knowledge automatically from unstructuredtext and apply the extracted knowledge to end taskssuch as decision making and question answering.
Ithas been a major goal of AI and NLP since theirearly days.
With the advent of the Web, the billionsof online text documents contain virtually unlimitedamount of knowledge to extract, further increasingthe importance and urgency of machine reading.In the past, there has been a lot of progress inautomating many subtasks of machine reading bymachine learning approaches (e.g., components inthe traditional NLP pipeline such as POS taggingand syntactic parsing).
However, end-to-end solu-tions are still rare, and existing systems typically re-quire substantial amount of human effort in manualengineering and/or labeling examples.
As a result,they often target restricted domains and only extractlimited types of knowledge (e.g., a pre-specified re-lation).
Moreover, many machine reading systemstrain their knowledge extractors once and do notleverage further learning opportunities such as ad-ditional text and interaction with end users.Ideally, a machine reading system should strive tosatisfy the following desiderata:End-to-end: the system should input raw text, ex-tract knowledge, and be able to answer ques-tions and support other end tasks;High quality: the system should extract knowledgewith high accuracy;Large-scale: the system should acquire knowledgeat Web-scale and be open to arbitrary domains,genres, and languages;Maximally autonomous: the system should incurminimal human effort;Continuous learning from experience: thesystem should constantly integrate new infor-mation sources (e.g., new text documents) andlearn from user questions and feedback (e.g.,via performing end tasks) to continuouslyimprove its performance.These desiderata raise many intriguing and chal-lenging research questions.
Machine reading re-search at the University of Washington has explored87a wide spectrum of solutions to these challenges andhas produced a large number of initial systems thatdemonstrated promising performance.
During thisexpedition, an underlying unifying vision starts toemerge.
It becomes apparent that the key to solvingmachine reading is to:1.
Conquer the long tail of textual knowledge viaa self-supervised learning process that lever-ages data redundancy to bootstrap from thehead and propagates information down the longtail by joint inference;2.
Scale this process to billions of Web documentsby identifying and leveraging ubiquitous struc-tures that lead to sparsity.In Section 2, we present this vision in detail, iden-tify the major dimensions these initial systems haveexplored, and propose a unifying approach that sat-isfies all five desiderata.
In Section 3, we reivewmachine reading research at the University of Wash-ington and show how they form synergistic efforttowards solving the machine reading problem.
Weconclude in Section 4.2 A Unifying Approach for MachineReadingThe core challenges to machine reading stem fromthe massive scale of the Web and the long-tailed dis-tribution of textual knowledge.
The heterogeneousWeb contains texts that vary substantially in subjectmatters (e.g., finance vs. biology) and writing styles(e.g., blog posts vs. scientific papers).
In addition,natural languages are famous for their myraid vari-ations in expressing the same meaning.
A fact maybe stated in a straightforward way such as ?kale con-tains calcium?.
More often though, it may be statedin a syntactically and/or lexically different way thanas phrased in an end task (e.g., ?calcium is found inkale?).
Finally, many facts are not even stated ex-plicitly, and must be inferred from other facts (e.g.,?kale prevents osteoporosis?
may not be stated ex-plicitly but can be inferred by combining facts suchas ?kale contains calcium?
and ?calcium helps pre-vent osteoporosis?).
As a result, machine readingmust not rely on explicit supervision such as manualrules and labeled examples, which will incur pro-hibitive cost in the Web scale.
Instead, it must beable to learn from indirect supervision.Figure 1: A unifying vision for machine reading: boot-strap from the head regime of the power-law distribu-tion of textual knowledge, and conquer the long tail ina self-supervised learning process that raises certainty onsparse extractions by propagating information via jointinference from frequent extractions.A key source of indirect supervision is metaknowledge about the domains.
For example, theTextRunner system (Banko et al, 2007) hinges onthe observation that there exist general, relation-independent patterns for information extraction.
An-other key source of indirect supervision is data re-dundancy.
While a rare extracted fact or inferencepattern may arise by chance of error, it is much lesslikely so for the ones with many repetitions (Downeyet al, 2010).
Such highly-redundant knowledge canbe extracted easily and with high confidence, andcan be leveraged for bootstrapping.
For knowledgethat resides in the long tail, explicit forms of redun-dancy (e.g., identical expressions) are rare, but thiscan be circumvented by joint inference.
For exam-ple, expressions that are composed with or by sim-ilar expressions probably have the same meaning;the fact that kale prevents osteoporosis can be de-rived by combining the facts that kale contains cal-cium and that calcium helps prevent osteoporosis viaa transitivity-through inference pattern.
In general,joint inference can take various forms, ranging fromsimple voting to shrinkage in a probabilistic ontol-ogy to sophisticated probabilistic reasoning basedon a joint model.
Simple ones tend to scale bet-ter, but their capability in propagating informationis limited.
More sophisticated methods can uncoverimplicit redundancy and propagate much more in-88formation with higher quality, yet the challenge ishow to make them scale as well as simple ones.To do machine reading, a self-supervised learningprocess, informed by meta knowledege, stipulateswhat form of joint inference to use and how.
Effec-tively, it increases certainty on sparse extractions bypropagating information from more frequent ones.Figure 1 illustrates this unifying vision.In the past, machine reading research at the Uni-versity of Washington has explored a variety of so-lutions that span the key dimensions of this uni-fying vision: knowledge representation, bootstrap-ping, self-supervised learning, large-scale joint in-ference, ontology induction, continuous learning.See Section 3 for more details.
Based on this ex-perience, one direction seems particularly promisingthat we would propose here as our unifying approachfor end-to-end machine reading:Markov logic is used as the unifying framework forknowledge representation and joint inference;Self-supervised learning is governed by a jointprobabilistic model that incorporates a smallamount of heuristic knowledge and large-scalerelational structures to maximize the amountand quality of information to propagate;Joint inference is made scalable to the Web bycoarse-to-fine inference.Probabilistic ontologies are induced from text toguarantee tractability in coarse-to-fine infer-ence.
This ontology induction and popula-tion are incorporated into the joint probabilisticmodel for self-supervision;Continuous learning is accomplished by combin-ing bootstrapping and crowdsourced contentcreation to synergistically improve the readingsystem from user interaction and feedback.A distinctive feature of this approach is its empha-sis on using sophisticated joint inference.
Recently,joint inference has received increasing interest inAI, machine learning, and NLP, with Markov logic(Domingos and Lowd, 2009) being one of the lead-ing unifying frameworks.
Past work has shown thatit can substantially improve predictive accuracy insupervised learning (e.g., (Getoor and Taskar, 2007;Bakir et al, 2007)).
We propose to build on these ad-vances, but apply joint inference beyond supervisedlearning, with labeled examples supplanted by indi-rect supervision.Another distinctive feature is that we proposeto use coarse-to-fine inference (Felzenszwalb andMcAllester, 2007; Petrov, 2009) as a unifyingframework to scale inference to the Web.
Es-sentially, coarse-to-fine inference leverages thesparsity imposed by hierarchical structures thatare ubiquitous in human knowledge (e.g., tax-onomies/ontologies).
At coarse levels (top levels ina hierarchy), ambiguities are rare (there are few ob-jects and relations), and inference can be conductedvery efficiently.
The result is then used to prune un-promising refinements at the next level.
This processcontinues down the hierarchy until decision can bemade.
In this way, inference can potentially be spedup exponentially, analogous to binary tree search.Finally, we propose a novel form of continuouslearning by leveraging the interaction between thesystem and end users to constantly improve the per-formance.
This is straightforward to do in our ap-proach given the self-supervision process and theavailability of powerful joint inference.
Essentially,when the system output is applied to an end task(e.g., answering questions), the feedback from useris collected and incorporated back into the systemas a bootstrap source.
The feedback can take theform of explicit supervision (e.g., via communitycontent creation or active learning) or indirect sig-nals (e.g., click data and query logs).
In this way,we can bootstrap an online community by an initialmachine reading system that provides imperfect butvaluable service in end tasks, and continuously im-prove the quality of system output, which attractsmore users with higher degree of participation, thuscreating a positive feedback loop and raising the ma-chine reading performance to a high level that is dif-ficult to attain otherwise.3 Summary of Progress to DateThe University of Washington has been one of theleading places for machine reading research and hasproduced many cutting-edge systems, e.g., WIEN(first wrapper induction system for information ex-traction), Mulder (first fully automatic Web-scalequestion answering system), KnowItAll/TextRunner(first systems to do open-domain information extrac-89tion from the Web corpus at large scale), Kylin (firstself-supervised system for Wikipedia-based infor-mation extraction), UCR (first unsupervised corefer-ence resolution system that rivals the performance ofsupervised systems), Holmes (first Web-scale jointinference system), USP (first unsupervised systemfor semantic parsing).Figure 2 shows the evolution of the major sys-tems; dashed lines signify influence in key ideas(e.g., Mulder inspires KnowItAll), and solid linessignify dataflow (e.g., Holmes inputs TextRunner tu-ples).
These systems span a wide spectrum in scal-ability (assessed by speed and quantity in extrac-tion) and comprehension (assessed by unit yield ofknowledge at a fixed precision level).
At one ex-treme, the TextRunner system is highly scalable, ca-pable of extracting billions of facts, but it focuses onshallow extractions from simple sentences.
At theother extreme, the USP and LOFT systems achievemuch higher level of comprehension (e.g., in a taskof extracting knowledge from biomedical papers andanswering questions, USP obtains more than threetimes as many correct answers as TextRunner, andLOFT obtains more than six times as many correctanswers as TextRunner), but are much less scalablethan TextRunner.In the remainder of the section, we review theprogress made to date and identify key directions forfuture work.3.1 Knowledge Representation and JointInferenceKnowledge representations used in these systemsvary widely in expressiveness, ranging from sim-ple ones like relation triples (<subject, relation,object>; e.g., in KnowItAll and TextRunner), toclusters of relation triples or triple components (e.g.,in SNE, RESOLVER), to arbitrary logical formulasand their clusters (e.g., in USP, LOFT).
Similarly,a variety forms of joint inference have been used,ranging from simple voting to heuristic rules to so-phisticated probabilistic models.
All these can becompactly encoded in Markov logic (Domingos andLowd, 2009), which provides a unifying frameworkfor knowledge representation and joint inference.Past work at Washington has shown that in su-pervised learning, joint inference can substantiallyimprove predictive performance on tasks related tomachine reading (e.g., citation information extrac-tion (Poon and Domingos, 2007), ontology induc-tion (Wu and Weld, 2008), temporal informationextraction (Ling and Weld, 2010)).
In addition, ithas demonstrated that sophisticated joint inferencecan enable effective learning without any labeledinformation (UCR, USP, LOFT), and that joint in-ference can scale to millions of Web documents byleveraging sparsity in naturally occurring relations(Holmes, Sherlock), showing the promise of our uni-fying approach.Simpler representations limit the expressivenessin representing knowledge and the degree of sophis-tication in joint inference, but they currently scalemuch better than more expressive ones.
A key direc-tion for future work is to evaluate this tradeoff morethoroughly, e.g., for each class of end tasks, to whatdegree do simple representations limit the effective-ness in performing the end tasks?
Can we automatethe choice of representations to strike the best trade-off for a specific end task?
Can we advance jointinference algorithms to such a degree that sophisti-cated inference scales as well as simple ones?3.2 BootstrappingPast work at Washington has identified and lever-aged a wide range of sources for bootstrapping.
Ex-amples include Wikipedia (Kylin, KOG, IIA, WOE,WPE), Web lists (KnowItAll, WPE), Web tables(WebTables), Hearst patterns (KnowItAll), heuristicrules (TextRunner), semantic role labels (SRL-IE),etc.In general, potential bootstrap sources can bebroadly divided into domain knowledge (e.g., pat-terns and rules) and crowdsourced contents (e.g., lin-guistic resources, Wikipedia, Amazon MechanicalTurk, the ESP game).A key direction for future work is to combinebootstrapping with crowdsourced content creationfor continuous learning.
(Also see Subsection 3.6.
)3.3 Self-Supervised LearningAlthough the ways past systems conduct self-supervision vary widely in detail, they can be di-vided into two broad categories.
One uses heuristicrules that exploit existing semi-structured resourcesto generate noisy training examples for use by su-pervised learning methods and with cotraining (e.g.,90			 		 !"#$#%& "'     $($&)*++(*Figure 2: The evolution of major machine reading systems at the University of Washington.
Dashed lines signifyinfluence and solid lines signify dataflow.
At the top are the years of publications.
ShopBot learns comparison-shopping agents via self-supervision using heuristic knowledge (Doorenbos et al, 1997); WIEN induces wrappersfor information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmericket al, 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted frommultiple search query results (Kwok et al, 2001); KnowItAll conducts open-domain information extraction via self-supervision bootstrapping from Hearst patterns (Etzioni et al, 2005); Opine builds on KnowItAll and mines productreviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin pop-ulates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEXconducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al, 2007a); REALMimproves sparse open-domain information extraction via relational clustering and language modeling (Downey et al,2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); Tex-tRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Bankoet al, 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional re-lations (Ritter et al, 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackerset al, 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference usingMarkov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin?sextractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount ofdomain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos,2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kokand Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures(Cafarella et al, 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions thatare interesting to people (Lin et al, 2009); USP jointly learns a semantic parser and extracts knowledge via recursiverelational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact repre-sentation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritteret al, 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction viajoint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves theefficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddonand Domingos, 2010).
SHERLOCK induces new inference rules via relational learning (Schoenmackers et al, 2010);SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is ahybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than eithersystem (Christensen et al, 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu andWeld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generatedynamic, relation-specific lexicon features (Hoffmann et al, 2010).91TextRunner, Kylin, KOG, WOE, WPE).
Anotheruses unsupervised learning and often takes a partic-ular form of relational clustering (e.g., objects asso-ciated with similar relations tend to be the same andvice versa, as in REALM, RESOLVER, SNE, UCR,USP, LDA-SP, LOFT, etc.
).Some distinctive types of self-supervision in-clude shrinkage based on an ontology (KOG,LOFT, OLPI), probabilistic inference via hand-crafted or learned inference patterns (Holmes, Sher-lock), and cotraining using relation-specific andrelation-independent (open) extraction to reinforcesemantic coherence (Wu et al, 2008).A key direction for future work is to develop aunifying framework for self-supervised learning bycombining the strengths of existing methods andovercoming their limitations.
This will likely takethe form of a new learning paradigm that combinesexisting paradigms such as supervised learning, rela-tional clustering, semi-supervised learning, and ac-tive learning into a unifying learning framework thatsynergistically leverages diverse forms of supervi-sion and information sources.3.4 Large-Scale Joint InferenceTo apply sophisticated joint inference in machinereading, the major challenge is to make it scal-able to billions of text documents.
A general solu-tion is to identify and leverage ubiquitous problemstructures that lead to sparsity.
For example, orderof magnitude reduction in both memory and infer-ence time can be achieved for relational inferenceby leveraging the fact that most relational atoms arefalse, which trivially satisfy most relational formulas(Singla and Domingos, 2006; Poon and Domingos,2008a); joint inference with naturally occurring tex-tual relations can scale to millions of Web pages byleveraging the fact that such relations are approxi-mately functional (Schoenmackers et al, 2008).More generally, sparsity arises from hierarchicalstructures (e.g., ontologies) that are naturally exhib-ited in human knowledge, and can be leveraged todo coarse-to-fine inference (OLPI).The success of coarse-to-fine inference hinges onthe availability and quality of hierarchical structures.Therefore, a key direction for future work is to auto-matically induce such hierarchies.
(Also see nextsubsection.)
Moreover, given the desideratum ofcontinuous learning from experience, and the speedyevolution of the Web (new contents, formats, etc.
),it is important that we develop online methods forself-supervision and joint inference.
For example,when a new text document arrives, the reading sys-tem should not relearn from scratch, but should iden-tify only the relevant pieces of knowledge and con-duct limited-scoped inference and learning accord-ingly.3.5 Ontology InductionAs mentioned in previous subsections, ontologiesplay an important role in both self-supervision(shrinkage) and large-scale inference (coarse-to-fineinference).
A distinctive feature in our unifying ap-proach is to induce probabilistic ontologies, whichcan be learned from noisy text and support jointinference.
Past systems have explored two differ-ent approaches to probabilistic ontology induction.One approach is to bootstrap from existing onto-logical structures and apply self-supervision to cor-rect the erroneous nodes and fill in the missing ones(KOG).
Another approach is to integrate ontologyinduction with hierarchical smoothing, and jointlypursue unsupervised ontology induction, populationand knowledge extraction (LOFT).A key direction for future work is to combinethese two paradigms.
As case studies in ontologyintegration, prior research has devised probabilisticschema mappings and corpus-based matching algo-rithms (Doan, 2002; Madhavan, 2005; Dong et al,2007), and has automatically constructed mappingsbetween the Wikipedia infobox ?ontology?
and theFreebase ontology.
This latter endeavor illustratedthe complexity of the necessary mappings: a simpleattribute in one ontology may correspond to a com-plex relational view in the other, comprising threejoin operations; searching for such matches yieldsa search space with billions of possible correspon-dences for just a single attribute.Another key direction is to develop general meth-ods for inducing multi-facet, multi-inheritance on-tologies.
Although single-inheritance, tree-like hier-archies are easier to induce and reason with, natu-rally occurring ontologies generally take the form ofa lattice rather than a tree.923.6 Continuous LearningEarly work at Washington proposed to constructknowledge bases by mass collaboration (Richard-son and Domingos, 2003).
A key challenge is tocombine inconsistent knowledge sources of varyingquality, which motivated the subsequent develop-ment of Markov logic.
While this work did not domachine reading, its emphasis on lifelong learningfrom user feedback resonates with our approach oncontinuous learning.Past work at Washington has demonstrated thepromise of our approach.
For example, (Banko andEtzioni, 2007) automated theory formation based onTextRunner extractions via a lifelong-learning pro-cess; (Hoffmann et al, 2009) show that the pairingof Kylin and community content creation benefitsboth by sharing Wikipedia edits; (Soderland et al,2010) successfully adapted the TextRunner open-domain information extraction system to specific do-mains via active learning.Our approach also resonates with the never-ending learning paradigm for ?Reading the Web?
(Carlson et al, 2010).
In future work, we intend tocombine our approach with related ones to enablemore effective continuous learning from experience.4 ConclusionThis paper proposes a unifying approach to ma-chine reading that is end-to-end, large-scale, maxi-mally autonomous, and capable of continuous learn-ing from experience.
At the core of this approachis a self-supervised learning process that conquersthe long tail of textual knowledge by propagating in-formation via joint inference.
Markov logic is usedas the unifying framework for knowledge represen-tation and joint inference.
Sophisticated joint in-ference is made scalable by coarse-to-fine inferencebased on induced probabilistic ontologies.
This uni-fying approach builds on the prolific experience incutting-edge machine reading research at the Uni-versity of Washington.
Past results demonstrate itspromise and reveal key directions for future work.5 AcknowledgementThis research was partly funded by ARO grantW911NF-08-1-0242, AFRL contract FA8750-09-C-0181, DARPA contracts FA8750-05-2-0283,FA8750-07-D-0185, HR0011-06-C-0025, HR0011-07-C-0060, NBCH-D030010 and FA8750-09-C-0179, NSF grants IIS-0803481 and IIS-0534881,and ONR grant N00014-08-1-0670 and N00014-08-1-0431, the WRF / TJ Cable Professorship, a giftfrom Google, and carried out at the University ofWashington?s Turing Center.
The views and con-clusions contained in this document are those of theauthors and should not be interpreted as necessarilyrepresenting the official policies, either expressed orimplied, of ARO, DARPA, NSF, ONR, or the UnitedStates Government.ReferencesG.
Bakir, T. Hofmann, B.
B. Scho?lkopf, A. Smola,B.
Taskar, S. Vishwanathan, and (eds.).
2007.
Pre-dicting Structured Data.
MIT Press, Cambridge, MA.M.
Banko and O. Etzioni.
2007.
Strategies for lifelongknowledge extraction from the web.
In Proceedings ofthe Sixteenth International Conference on KnowledgeCapture, British Columbia, Canada.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings ofthe Twentieth International Joint Conference on Artifi-cial Intelligence, pages 2670?2676, Hyderabad, India.AAAI Press.Michael J. Cafarella, Jayant Madhavan, and Alon Halevy.2008.
Web-scale extraction of structured data.
SIG-MOD Record, 37(4):55?61.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the Third ACM Interna-tional Conference on Web Search and Data Mining.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2010.
Semantic role labeling for openinformation extraction.
In Proceedings of the First In-ternational Workshop on Formalisms and Methodol-ogy for Learning by Reading.Anhai Doan.
2002.
Learning to Map between StructuredRepresentations of Data.
Ph.D. thesis, University ofWashington.Pedro Domingos and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Morgan & Claypool, San Rafael, CA.Xin Dong, Alon Halevy, and Cong Yu.
2007.
Data inte-gration with uncertainty.
VLDB Journal.Robert B. Doorenbos, Oren Etzioni, and Daniel S. Weld.1997.
A scalable comparison-shopping agent for theworld-wide web.
In Proceedings of AGENTS-97.93Doug Downey, Matthew Broadhead, and Oren Etzioni.2007a.
Locating complex named entities in web text.In Proceedings of the Twentieth International JointConference on Artificial Intelligence.Doug Downey, Stefan Schoenmackers, and Oren Etzioni.2007b.
Sparse information extraction: Unsupervisedlanguage models to the rescue.
In Proceedings ofthe Forty Fifth Annual Meeting of the Association forComputational Linguistics.Doug Downey, Oren Etzioni, and Stephen Soderland.2010.
Analysis of a probabilistic model of redundancyin unsupervised information extraction.
In ArtificialIntelligence.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsuper-vised named-entity extraction from the web: An exper-imental study.
Artificial Intelligence, 165(1):91?134.Pedro F. Felzenszwalb and David McAllester.
2007.
Thegeneralized A* architecture.
Journal of Artificial In-telligence Research, 29.Lise Getoor and Ben Taskar, editors.
2007.
Introductionto Statistical Relational Learning.
MIT Press, Cam-bridge, MA.Raphael Hoffmann, Saleema Amershi, Kayur Patel, FeiWu, James Fogarty, and Daniel S. Weld.
2009.Amplifying community content creation using mixed-initiative information extraction.
In Proceedings ofCHI-09.Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.2010.
Learning 5000 relational extractors.
In submis-sion.Chloe Kiddon and Pedro Domingos.
2010.
Ontologicallifted probabilistic inference.
In submission.Stanley Kok and Pedro Domingos.
2008.
Extracting se-mantic networks from text via relational clustering.
InProceedings of the Nineteenth European Conferenceon Machine Learning, pages 624?639, Antwerp, Bel-gium.
Springer.Nicholas Kushmerick, Daniel S. Weld, and RobertDoorenbos.
1997.
Wrapper induction for informationextraction.
In Proceedings of the Fifteenth Interna-tional Joint Conference on Artificial Intelligence.Cody Kwok, Oren Etzioni, and Daniel S. Weld.
2001.Scaling question answering to the web.
In Proceed-ings of the Tenth International Conference on WorldWide Web.Thomas Lin, Oren Etzioni, and James Fogarty.
2009.Identifying interesting assertions from the web.
InProceedings of the Eighteenth Conference on Informa-tion and Knowledge Management.Xiao Ling and Daniel S. Weld.
2010.
Temporal infor-mation extraction.
In Proceedings of the Twenty FifthNational Conference on Artificial Intelligence.Jayant Madhavan.
2005.
Using known schemas andmappings to construct new semantic mappings.
Ph.D.thesis, University of Washington.Slav Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, Department of ComputerScience, University of Berkeley.Hoifung Poon and Pedro Domingos.
2007.
Joint infer-ence in information extraction.
In Proceedings of theTwenty Second National Conference on Artificial In-telligence, pages 913?918, Vancouver, Canada.
AAAIPress.Hoifung Poon and Pedro Domingos.
2008a.
A generalmethod for reducing the complexity of relational in-ference and its application to mcmc.
In Proceedingsof the Twenty Third National Conference on ArtificialIntelligence, pages 1075?1080, Chicago, IL.
AAAIPress.Hoifung Poon and Pedro Domingos.
2008b.
Joint un-supervised coreference resolution with Markov logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 649?658, Honolulu, HI.
ACL.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10, Singapore.
ACL.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontological induction from text.
In submission.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of the Joint Conference on Human Lan-guage Technology and Empirical Methods in NaturalLanguage Processing.Matt Richardson and Pedro Domingos.
2003.
Buildinglarge knowledge bases by mass collaboration.
In Pro-ceedings of the Second International Conference onKnowledge Capture, pages 129?137, Sanibel Island,FL.
ACM Press.Alan Ritter, Doug Downey, Stephen Soderland, and OrenEtzioni.
2008.
It?s a contradiction ?
no, it?s not: Acase study using functional relations.
In Proceedingsof the 2008 Conference on Empirical Methods in Nat-ural Language Processing.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alocation method for selectional preferences.In submission.Stefan Schoenmackers, Oren Etzioni, and Daniel S.Weld.
2008.
Scaling textual inference to the web.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing.Stefan Schoenmackers, Jesse Davis, Oren Etzioni, andDaniel S. Weld.
2010.
Learning first-order hornclauses from web text.
In submission.94Parag Singla and Pedro Domingos.
2006.
Memory-efficient inference in relational domains.
In Proceed-ings of the Twenty First National Conference on Arti-ficial Intelligence.Stephen Soderland, Brendan Roof, Bo Qin, Mausam, andOren Etzioni.
2010.
Adapting open information ex-traction to domain-specific relations.
In submission.Fei Wu and Daniel S. Weld.
2007.
Automatically se-mantifying wikipedia.
In Proceedings of the SixteenthConference on Information and Knowledge Manage-ment, Lisbon, Portugal.Fei Wu and Daniel S. Weld.
2008.
Automatically refin-ing the wikipedia infobox ontology.
In Proceedingsof the Seventeenth International Conference on WorldWide Web, Beijing, China.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using wikipedia.
In submission.Fei Wu, Raphael Hoffmann, and Daniel S. Weld.
2008.Information extraction from Wikipedia: Moving downthe long tail.
In Proceedings of the FourteenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, Las Vegas, NV.Alexander Yates and Oren Etzioni.
2007.
Unsupervisedresolution of objects and relations on the web.
In Pro-ceedings of Human Language Technology (NAACL).95
