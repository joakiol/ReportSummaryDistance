Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 75?83,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPA Cohesion Graph Based Approach for Unsupervised Recognition ofLiteral and Non-literal Use of Multiword ExpressionsLinlin Li and Caroline SporlederSaarland UniversityPostfach 15 11 5066041 Saarbr?uckenGermany{linlin,csporled}@coli.uni-saarland.deAbstractWe present a graph-based model for rep-resenting the lexical cohesion of a dis-course.
In the graph structure, vertices cor-respond to the content words of a text andedges connecting pairs of words encodehow closely the words are related semanti-cally.
We show that such a structure can beused to distinguish literal and non-literalusages of multi-word expressions.1 IntroductionMultiword expressions (MWEs) are defined as?idiosyncratic interpretations that cross wordboundaries or spaces?
(Sag et al, 2001).
Suchexpressions are pervasive in natural language;they are estimated to be equivalent in numberto simplex words in mental lexicon (Jackendoff,1997).
MWEs exhibit a number of lexical, syn-tactic, semantic, pragmatic and statistical idiosyn-crasies: syntactic peculiarities (e.g., by and large,ad hoc), semantic non-compositionality (e.g., asin kick the bucket (die) and red tape (bureau-cracy)), pragmatic idiosyncrasies (the expressionis sometimes associated with a fixed pragmaticpoint, e.g., good morning, good night), variationin syntactic flexibility (e.g., I handed in my thesis= I handed my thesis in vs. Kim kicked the bucket6= *the bucket was kicked by Kim), variation inproductivity (there are various levels of productiv-ity for different MWEs, e.g., kick/*beat/*hit thebucket, call/ring/phone/*telephone up).These idiosyncrasies pose challenges for NLPsystems, which have to recognize that an expres-sion is anMWE to deal with it properly.
Recogniz-ing MWEs has been shown to be useful for a num-ber of applications such as information retrieval(Lewis and Croft, 1990; Rila Mandala and Tanaka,2000; Wacholder and Song, 2003) and POS tag-ging (Piao et al, 2003).
It has also been shownthat MWEs account for 8% of parsing errors withprecision grammars (Baldwin et al, 2004).
Fur-thermore, MWE detection is used in informationextraction (Lin, 1998b) and an integral componentof symbolic MT systems (Gerber and Yang, 1997;Bond and Shirai, 1997).However, the special properties of MWEs canalso be exploited to recognize MWEs automati-cally.
There have been many studies on MWEs:identification (determining whether multiple sim-plex words form a MWE in a given token context,e.g.
put the sweater on vs. put the sweater onthe table), extraction (recognizing MWEs as wordunits at the type level), detecting or measuringcompositionality of MWEs, semantic interpreta-tion (interpreting the semantic association amongcomponents in MWEs).To extract MWEs, various methods have beenproposed that exploit the syntactic and lexicalfixedness exhibited by MWEs, or apply variousstatistical measures across all co-occurrence vec-tors between the whole expression and its com-ponent parts (see Section 2).
These methods canbe used to automatically identify potentially id-iomatic expressions at a type level, but they do notsay anything about the idiomaticity of an expres-sion in a particular context.
While some idioms(e.g., ad hoc) are always used idiomatically, thereare numerous others that can be used both idiomat-ically (see Example 1) and non-idiomatically (seeExample 2).
(1) When the members of De la Guarda aren?thanging around, they?re yelling andbouncing off the wall.
(2) Blinded by the sun, Erstad leaped at thewall, but the ball bounced off the wall wellbelow his glove.Our work aims to distinguish the literal andnon-literal usages of idiomatic expressions in a75discourse context (so-called token based classifi-cation).
It is therefore different from type-basedapproaches which aim to detect the general id-iomaticity of an expression rather than its actualusage in a particular context.We utilize the cohesive structure of a discourse(Halliday and Hasan, 1976) to distinguish literal ornon-literal usage of MWEs.
The basic idea is thatthe component words of an MWE contribute to thecohesion of the discourse in the literal case, whilein the non-literal case they do not.
For instance, inthe literal use of break the ice in Example 3, thecontent word ice contributes to the overall seman-tic connectivity of the whole sentence by the factthat ice is semantically related to water.
In con-trast, in the non-literal example in 4, the word icedoes not contribute to the overall cohesion as it ispoorly connected to all the other (content) wordsin this specific context (play, party, games).
(3) The water would break the ice into floeswith its accumulated energy.
(4) We played a couple of party games tobreak the ice.Our approach bears similarities to Hirst and St-Onge?s (1998) method for detecting malapropismsbased on their non-participation in cohesivechains.
However, computing such chains requiresa pre-defined similarity threshold which governswhether a word is placed in a particular chain.
Set-ting this threshold typically requires a manually la-beled development set, which makes this methodweakly supervised.
We propose an alternative,parameter-free method in which we model the co-hesive structure of a discourse as a graph structure(called cohesion graph), where the vertices of thegraph correspond to the content words of the textand the edges encode the semantic relatedness be-tween pairs of words.
To distinguish between lit-eral and non-literal use of MWEs, we look at howthe average relatedness of the graph changes whenthe component words of the MWE are excluded orincluded in the graph (see Section 3).1We first introduced the cohesion graph methodin Sporleder and Li (2009).
In the present paper,1By modeling lexical cohesion as a graph structure, wefollow earlier approaches in information retrieval, notably bySalton and colleagues (Salton et al, 1994).
The difference isthat these works aim at representing similarity between largertext segments (e.g., paragraphs) in a so-called ?text?
or ?para-graph relation map?, whose vertices correspond to a text seg-ment and whose edges represent the similarity between thesegments (modeled as weighted term overlap).we provide a formalization of the graph and ex-periment with different vertex and edge weight-ing schemes.
We also report on experiments withvarying the size of the input context and also withpruning the graph structure automatically.2 Related WorkType-based MWE classification aims to extractmultiword expression types in text from observa-tions of the token distribution.
It aims to pickup on word combinations which occur with com-paratively high frequencies when compared to thefrequencies of the individual words (Evert andKrenn, 2001; Smadja, 19993).
The lexical andsyntactic fixedness property can also be utilized toautomatically extract MWEs (Baldwin and Villav-icencio, 2002).The study of semantic compositionality ofMWEs focuses on the degree to which the seman-tics of the parts of an MWE contribute towards themeaning of the whole.
The aim is a binary classi-fication of the MWEs as idiosyncratically decom-posable (e.g.
spill the beans) or non-decomposable(e.g.
kick the bucket).
Several approaches havebeen proposed.
Lin (1999) uses the substitutiontest2and mutual information (MI) to determinethe compositionality of the phrase.
An obviouschange of the MI value of the phrase in the sub-stitution test is taken as the evidence of the MWEsbeing non-compositional.
Bannard et al (2003)assume that compositional MWEs occur in sim-ilar lexical context as their component parts.
Theco-occurrence vector representations of verb parti-cle construction (VPC) and the component wordsare utilized to determine the compositionality ofthe MWE.There have also been a few token-based classi-fication approaches, aimed at classifying individ-ual instances of a potential idiom as literal or non-literal.
Katz and Giesbrecht (2006) make use oflatent semantic analysis (LSA) to explore the locallinguistic context that can serve to identify multi-word expressions that have non-compositionalmeaning.
They measure the cosine vector similar-ity between the vectors associated with an MWEas a whole and the vectors associated with its con-stituent parts and interpret it as the degree to whichthe MWE is compositional.
They report an av-2The substitution test aims to replace part of the idiom?scomponent words with semantically similar words, and testhow the co-occurrence frequency changes.76erage accuracy of 72%, but the data set used intheir evaluation is small.
Birke and Sarkar (2006)use literal and non-literal seed sets acquired with-out human supervision to perform bootstrappinglearning.
The new instances of potential idiomsare always labeled according to the closest set.While their approach is unsupervised clustering,they do rely on some resources such as databasesof idioms.
Cook et al (2007) and Fazly et al(2009) rely crucially on the concept of canonicalform (CForm).
It is assumed that for each idiomthere is a fixed form (or a small set of those) cor-responding to the syntactic pattern(s) in which theidiom normally occurs.
The canonical form al-lows for inflection variation of the heard verb butnot for other variations (such as nominal inflec-tion, choice of determiner etc.).
It has been ob-served that if an expression is used idiomaticallyit typically occurs in its canonical form (Riehe-mann, 2001).
Fazly and her colleagues exploit thisbehavior and propose an unsupervised method fortoken-based idiom classification in which an ex-pression is classified as idiomatic if it occurs incanonical form and literal otherwise.
The canon-ical forms are determined automatically using astatistical, frequency-based measure.
They alsodeveloped statistical measures to measure the lex-ical and syntactic fixedness of a given expression,which is used to automatically recognize expres-sion types, as well as their token identification incontext.
They report an average accuracy of 72%for their canonical form (CForm) classifier.3 Cohesion GraphIn this section, we first give a formal definition ofthe cohesion graph that is used for modeling dis-course connectivity, then we define the discourseconnectivity.
Finally, we introduced our graph-based classifier for distinguishing literal and non-literal use of MWEs.3.1 Cohesion Graph StructureA cohesion graph (CG) is an undirected completegraph3G = (V,E), whereV : is a set of nodes {v1, v2, ..., vn}, where eachnode vi= (ti, idi) represents a unique token in thediscourse.
tiis the string form of the token, and ididenotes the position of the token in the context.3In the mathematical field of graph theory, a completegraph is a simple graph in which every pair of distinct verticesis connected by an edge.
The complete graph on n verticeshas n(n?
1)/2 edges.E: is a set of edges {e12, e13, ..., e(n)(n?1)},such that each edge eijconnects a pair of nodes(vi, vj).
n is the total number of tokens in the dis-course that the graph models.
The value of eijrep-resents the semantic relatedness of the two tokensti, tjthat eijconnects:eij= h(ti, tj) (5)where h is a semantic relatedness assignmentfunction.
The explicit form of h will be discussedin the next section.eiis the average semantic relatedness of the to-ken tiin the discourse.
It represents the averagerelatedness score of a certain token to its surround-ing context:ei=n?j=1,j 6=i?ij?
eij(6)where ?ijis the weight of the edge eij, with theconstraint,n?j=1,j 6=i?ij= 1.The edge weight function ?ijallows us toweight the relatedness between two tokens, for ex-ample based on their distance in the text.
The mo-tivation for this is that the closer two tokens occurtogether, the more likely it is that their relatednessis not accidental.
For instance, the idiom breakthe ice in Example 7 could be misclassified as lit-eral due to there being a high relatedness score be-tween ice and snow.
The weight function is in-troduced so that relatedness with tokens that arecloser to MWE component words counts more.
(7) The train was canceled because of the windand snow.
All the people in the small villagetrain station felt upset.
Suddenly, one guybroke the ice and proposed to play a game.The weight function ?ijis defined in terms ofthe inverse of the distance ?
between the two tokenpositions idiand idj:?ij=?
(idi, idj)?j?
(idi, idj)(8)As the semantic relatedness among the MWEcomponent words does not contain any informa-tion of how these component words are seman-tically involved in the context, we do not countthe edges between the MWE component words77(as e45in Figure 1).
We set al the weightsfor connecting MWE component words to be 0,?
(idmwe?i, idmwej) = 0.c(G): is defined as the discourse connectivityof the cohesion graph.
It represents the semanticrelatedness score of the discourse.c(G) =n?i=1(?i?
ei) (9)where n is the total number of tokens in thediscourse, ?iis the weight of the average seman-tic relatedness of the token tiwith the constraint?i?i= 1.
It represents the importance of therelatedness contribution of a specific token tiinthe discourse.
For instance, the word Monday inExample 12 should be assigned less weight thanthe word bilateral as it is not part of the centraltheme(s) of the discourse.
This is often the casefor time expressions.
?iis defined as:?i=salience(ti)?jsalience(tj)(10)To model the salience of a token for the se-mantic context of the text we use a tf.idf -basedweighting scheme.
Since we represent word to-kens rather than word types in the cohesion graph,we do not need to model the term frequency tfseparately, instead we set salience to the log valueof the inverse document frequency idf :salience(ti) = log|D||{d : ti?
d}|(11)whereD is the total number of documents in ourdata set and |{d : ti?
d}| is the number of docu-ments in which tioccurs.
Terms which are relatedto the sub-topics of a document will typically onlyoccur in a few texts in the collection, hence theiridf (and often also their tf ) is high and they willthus be given more weight in the graph.
Termswhich are not related to the central themes of atext, such as temporal expressions, will be givena lower weight.
A complication arises for compo-nent words of the MWE: these occur in all of ourexamples and thus will receive a very low idf.
Thisis an artifact of the data and not what we want asit means that the average connectivity of the graphvirtually always increases if the MWE is excluded,causing the classifier to over-predict ?non-literal?.To counteract this effect, we set |{d : ti?
d}| ofthese words uniformly to 1.
(12) ?Gujral will meet Sharif on Monday anddiscuss bilateral relations,?
the Press Trustof India added.
The minister said Sharif andGujral would be able to ?break the ice?
overKashmir.3.2 Graph-based ClassifierThe cohesion graph based classifier compares thecohesion graph connectivity of the discourse in-cluding the MWE component words with the con-nectivity of the discourse excluding the MWEcomponent words to check how well the MWEcomponent words are semantically connected tothe context.
If the cohesion graph connectivityincreases by including MWE component words,the MWE is thought to be semantically well re-lated to its discourse.
It is classified as literal (oth-erwise as non-literal).
In other words, the cohe-sion graph based algorithm detects the strength ofrelatedness between the MWE component wordsand their context by calculating the discourse con-nectivity gain, and classifies instances as literal ornon-literal based on this gain.
This process is de-scribed as Formula 13 (if ?c > 0, it is literal;otherwise it is non-literal):?c = c(G)?
c(G?)
(13)where, c(G) is the discourse connectivity of thecontext with MWE component words (as shownwith the complete graph in Figure 1 ); c(G?)
isthe discourse connectivity of the context withoutMWE component words (as shown with the sub-graph {v1, v2, v3} in Figure 1).Figure 1: Cohesion Graph for identifying literal ornon-literal usage of MWEs784 Modeling Semantic RelatednessIn Section 3.1, we did not define how we modelthe semantic relatedness between two tokens(h(ti, tj)).
Modeling semantic relatedness be-tween two terms is currently an area of active re-search.
There are two main approaches.
Methodsbased on manually built lexical knowledge bases,such as WordNet, compute the shortest path be-tween two concepts in the knowledge base and/orlook at word overlap in the glosses (see Budan-itsky and Hirst (2006) for an overview).
Distri-butional approaches, on the other hand, rely ontext corpora, and model relatedness by comparingthe contexts in which two words occur, assumingthat related words occur in similar context (e.g.,Hindle (1990), Lin (1998a), Mohammad and Hirst(2006)).
More recently, there has also been re-search on using Wikipedia and related resourcesfor modeling semantic relatedness (Ponzetto andStrube, 2007; Zesch et al, 2008).WordNet-based approaches are unsuitable forour purposes as they only model so-called ?classi-cal relations?
like hypernymy, antonymy etc.
Forour task, we need to model a wide range of re-lations, e.g., between ice and water.
Hence weopted for a distributional approach.
We experi-mented with two different approaches, one (DV )based on syntactic co-occurrences in a large textcorpus and the other (NGD) based on search en-gine page counts.Dependency Vectors (DV) is a distributionalapproach which does not look simply at word co-occurrences in a fixed-size window but takes intoaccount syntactic (dependency) relations betweenwords (Pad?o and Lapata, 2007).
Each target wordis represented by a co-occurrence vector where di-mension represents a chosen term and the vectorcontains the co-occurrence information betweenthat word and the chosen terms in a corpus (weused the BNC in our experiments).
A variety ofdistance measures can be used to compute the sim-ilarity of two vectors; here we use the cosine sim-ilarity which is defined as:simcos(?
?x ,?
?y ) =nXi=1xiyivuutnXi=1x2ivuutnXi=1y2i(14)Normalized Google Distance (NGD) uses thepage counts returned by a search engine as prox-ies for word co-occurrence and thereby quantifiesthe strength of a relationship between two words(see Cilibrasi and Vitanyi (2007)).
The basic ideais that the more often two terms occur together rel-ative to their overall occurrence the more closelythey are related.
NGD is defined as follows:NGD(x, y) =max{log f(x), log f(y)} ?
log f(x, y)log M ?min{log f(x), log f(y)}(15)where x and y are the two words whose associ-ation strength is computed, f(x) is the page countreturned by the search engine for the term x (andlikewise for f(y) and y), f(x, y) is the page countreturned when querying for ?x AND y?
(i.e., thenumber of pages that contain both, x and y), andM is the number of web pages indexed by thesearch engine.
When querying for a term we queryfor a disjunction of all its inflected forms.4As itis difficult to obtain a specific and reliable numberfor the number of pages indexed by a search en-gine, we approximated it by setting it to the num-ber of hits obtained for the word the.
The assump-tion is that the word the occurs in all English lan-guage web pages (Lapata and Keller, 2005).Using web counts rather than bi-gram countsfrom a corpus as the basis for computing semanticrelatedness has the advantage that the web is a sig-nificantly larger database than any compiled cor-pus, which makes it much more likely that we canfind information about the concepts we are look-ing for (thus alleviating data sparseness).
How-ever, search engine counts are notoriously unre-liable (Kilgariff, 2007; Matsuo et al, 2007) andwhile previous studies have shown that web countscan be used as reliable proxies for corpus-basedcounts for some applications (Zhu and Rosenfeld,2001; Lapata and Keller, 2005) it is not clear thatthis also applies when modeling semantic related-ness.
We thus carried out a number of experimentstesting the reliability of page counts (Section 4.1)and comparing the NGD measure to a standarddistributional approach (Section 4.2).4The inflected forms were generated by apply-ing the morph tools developed at the University ofSussex (Minnen et al, 2001) which are availableat: http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html794.1 Search Engine StabilityWe first carried out some experiments to test thestability of the page counts returned by two of themost widely-used search engines, Google and Ya-hoo.
For both search engines, we found a numberof problems.5Total number of pages indexed The total num-ber of the web pages indexed by a search enginevaries across time and the numbers provided aresomewhat unreliable.
This is a potential problemfor NGD because we need to fix the value of M inFormula 15.
As an approximative solution, we setit to the number of hits obtained for the word the,assuming that it will occur in all English languagepages (Lapata and Keller, 2005).Page count variation The number of page hitsfor a given term also varies across time (see exam-ple (4.1) for two queries for Jim at different timest1 and t2).
However, we found that the variance inthe number of pages tends to be relatively stableover short time spans, hence we can address thisproblem by carrying out all queries in one quicksession without much delay.
However, this meanswe cannot store page counts in a database and re-use them at a later stage; for each new examplewhich we want to classify at a later stage, we haveto re-compute all relevant counts.
(16) Hits(Jim, t1) = 763,000,000Hits(Jim, t2) = 757,000,000Problems with conjunction and disjunctionThe search engines?
AND and OR operators areproblematic and can return counter-intuitive re-sults (see Table 1).
This is a potential problemfor us because we have to query for conjunctionsof terms and disjunctions of inflected forms.
Forthe time being we ignored this problem as it is notstraightforward to solve.OPT = AND OPT = ORcar 3,590,000,000car OPT car 4,670,000,000 3,550,000,000car OPT car OPT car 3,490,000,000 3,530,000,000Table 1: Operator test for YahooProblems with high-frequency terms We alsofound that both the Google and Yahoo API seemto have problems with high frequency words, withthe Google SOAP API throwing an exception and5See also the discussions in Jean V?eronis blog: http://aixtal.blogspot.com and the comments in Kilgariff(2007).the Yahoo API returning the same 10-digit num-ber for every high frequency word.
This might bea data overflow problem.
We addressed this prob-lem by excluding high frequency words.When comparing Yahoo and Google we foundthat Yahoo?s page counts tend to be more consis-tent than Google?s.
We therefore opted for Yahooin our further experiments.4.2 NGD vs. Co-occurrence VectorsIn principle, we believe that the web-based ap-proach for computing relatedness is more suitablefor our task since it gives us access to more dataand allows us to also model relations based on (up-to-date) world knowledge.
However, the questionarises whether the stability problems observed inthe previous section have a negative effect on theperformance of the NGD measure.
To test this, weconducted a small study in which we comparedthe relatedness scores obtained by NGD and thesemantic vector space model to the human ratingscompiled by Finkelstein et al (2002).6We used Spearman?s correlation test (Spear-man, 1904) to compare the ranked human ratingsto the ranked ratings obtained by NGD and thevector space method.
The (human) inter-annotatoragreement varies a lot for different pairs of annota-tors (between 0.41 and 0.82 by Spearman?s corre-lation test), suggesting that deciding on the seman-tic relatedness between arbitrary pairs of wordsis not an easy task even for humans.
In gen-eral, the NGD-human agreement is comparable tothe human-human agreement.
The agreement be-tween the NGD and average human agreement ishigher than some human-human agreements.
Fur-thermore, we found that NGD actually outper-forms the dependency vector method on this dataset.7Hence, we decided to use NGD in the fol-lowing experiments.5 ExperimentsWe tested our graph-based classifiers on a manu-ally annotated data set, which we describe in Sec-6The data sets are available at: http://www.cs.technion.ac.il/?gabr/resources/data/wordsim353/7There may be several reasons for this.
Apart from thefact that NGD has access to a larger data set, it may also bethat syntactic co-occurrence information is not ideal for mod-eling this type of relatedness; co-occurrence information in afixed window might be more useful.
Furthermore, we did notspend much time on finding an optimal parameter setting forthe dependency vector method.80tion 5.1.
We report on our experiments and resultsin Section 5.2.5.1 DataThroughout the experiments we used the data setfrom Sporleder and Li (2009).
The data consist of17 potentially idiomatic expressions from the En-glish Gigaword corpus, which were extracted withfive paragraphs of context and manually annotatedas ?literal?
or ?non-literal?
(see Table 2).
The inter-annotator agreement on a doubly annotated sam-ple of the data was 97% and the kappa score 0.7(Cohen, 1960).expression literal non-lit.
allback the wrong horse 0 25 25bite off more than one can chew 2 142 144bite one?s tongue 16 150 166blow one?s own trumpet 0 9 9bounce off the wall* 39 7 46break the ice 20 521 541drop the ball* 688 215 903get one?s feet wet 17 140 157pass the buck 7 255 262play with fire 34 532 566pull the trigger* 11 4 15rock the boat 8 470 478set in stone 9 272 281spill the beans 3 172 175sweep under the carpet 0 9 9swim against the tide 1 125 126tear one?s hair out 7 54 61all 862 3102 3964Table 2: Idiom statistics (* indicates expressionsfor which the literal usage is more common thanthe non-literal one)5.2 The Influence of Context Size andWeighting SchemeTo gain some insights into the performance of thegraph-based classifier, we experimented with dif-ferent context sizes and weighting schemes.
In ad-dition to the basic cohesion graph approach withfive paragraphs of context (CGA), we tested avariant which only uses the current paragraph ascontext (CGApara) to determine how sensitive theclassifier is to the context size.
We also experi-mented with three weighting schemes.
The ba-sic classifier (CGA) uses uniform edge and nodeweights.
CGAewuses edge weights based on theinverse distance between the tokens.
CGAnwusesnode weights based on idf .
Finally, CGAew+nwuses both edge and node weights.We also carried out a pruning experiment inwhich we removed nodes from the graph that areonly weakly connected to the context (called weakcohesion nodes).
We hypothesize that these donot contribute much to the overall connectivity butmay add noise.
Pruning can thus be seen as amore gentle version of node weighting, in whichwe only remove the top n outliers rather than re-weight all nodes.
For comparison we also imple-mented a baseline (BASE), which always assignsthe majority class (?non-literal?
).Table 3 shows the results for the classifiers dis-cussed above.
In addition to accuracy, which isnot very informative as the class distribution in ourdata set is quite skewed, we show the precision,recall, and F-score for the minority class (literal).All classifiers obtain a relatively high accuracy butvary in the precision, recall and F-Score values.Method LPrec.
LRec.
LF?=1Acc.Base ?
?
?
0.78CGA 0.50 0.69 0.58 0.79CGApara0.42 0.67 0.51 0.71CGAprun0.49 0.72 0.58 0.78CGAew0.51 0.63 0.57 0.79CGAnw0.48 0.68 0.56 0.77CGAew+nw0.49 0.61 0.54 0.78Table 3: Accuracy (Acc.
), literal precision(LPrec.
), recall (LRec.
), and F-Score (LF?=1) forthe classifierIt can be seen that the basic cohesion graphclassifier (CGA) outperforms the baseline on ac-curacy.
Moreover, it is reasonably good at iden-tifying literal usages among the majority of non-literal occurrences, as witnessed by an F-score of58%.
To obtain a better idea of the behavior ofthis classifier, we plotted the distribution of theMWE instances in the classifier?s feature space,where the first dimension represents the discourseconnectivity of the context with MWE componentwords (c(G)) and the second represents the dis-course connectivity of the context without MWEcomponent words (c(G?)).
The graph-based clas-sifier, which calculates the connectivity gain (seeEquation 13), is a simple linear classifier in whichthe line y = x is chosen as the decision boundary.Examples above that line are classified as ?literal?,examples below as ?non-literal?.
Figure 2 showsthe true distribution of literal and non-literal exam-ples in our data set.
It can be seen that most non-literal examples are indeed below the line whilemost literal ones are above it (though a certainnumber of literal examples can also be found be-81low the line).
So, in general we would expect ourclassifier to have a reasonable performance.Figure 2: Decision boundaries of the cohesiongraphReturning to the results in Table 3, we findthat a smaller context worsens the performanceof the classifier (CGApara).
Pruning the 3 leastconnected nodes (CGAprun) does not lead to asignificant change in performance.
Edge weight-ing (CGAew), node weighting (CGAnw) and theircombination (CGAew+nw), on the other hand,seem to have a somewhat negative influence onthe literal recall and F-score.
It seems that theweighting scheme scales down the influence ofMWE component words.
As a result, the prod-uct of the weight and the relatedness value for theidiom component words are lower than the aver-age, which leads to the negative contribution ofthe idiom words to the cohesion graph (over pre-dicting non-literal usage).
We need to investigatemore sophisticated weighting schemes to assignbetter weights to idiom component words in thefuture.
The negative performance of the weight-ing scheme may be also due to the fact that weused a relatively small context of five paragraphs.8Both the idf and the distance weighting shouldprobably be defined on larger contexts.
For ex-ample, the distance between two tokens within aparagraph probably has not such a large effect onwhether their relatedness score is reliable or ac-cidental.
Hence it might be better to model theedge weight as the distance in terms of paragraphsrather than words.
The idf scores, too, might bemore reliable if more context was used.8Note that we used news texts which typically have veryshort paragraphs.6 ConclusionIn this paper, we described an approach for token-based idiom classification.
Our approach is basedon the observation that literally used expressionstypically exhibit strong cohesive ties with the sur-rounding discourse, while idiomatic expressionsdo not.
Hence, idiomatic use of MWEs can bedetected by the absence of such ties.We propose a graph-based method which ex-ploits this behavior to classify MWEs as literalor non-literal.
The method compares how theMWE component words contribute the overall se-mantic connectivity of the graph.
We provided aformalization of the graph and experimented withvarying the context size and weighting scheme fornodes and edges.
We found that the method gener-ally works better for larger contexts; the weightingschemes proved somewhat unsuccessful, at leastfor our current context size.
In the future, we planto experiment with larger context sizes and moresophisticated weighting schemes.AcknowledgmentsThis work was funded by the Cluster of Excellence ?Multi-modal Computing and Interaction?.ReferencesT.
Baldwin, A. Villavicencio.
2002.
Extracting the un-extractable: a case study on verb-particles.
In Proc.of CoNLL-02.T.
Baldwin, E. M. Bender, D. Flickinger, A. Kim,S.
Oepen.
2004.
Road-testing the english resourcegrammar over the british national corpus.
In Proc.LREC-04, 2047?2050.C.
Bannard, T. Baldwin, A. Lascarides.
2003.
A sta-tistical approach to the semantics of verb-particles.In Proc.
ACL 2003 Workshop on Multiword Expres-sions.J.
Birke, A. Sarkar.
2006.
A clustering approachfor the nearly unsupervised recognition of nonliterallanguage.
In Proceedings of EACL-06.F.
Bond, S. Shirai.
1997.
Practical and efficient or-ganization of a large valency dictionary.
In Work-shop on Multilingual Information Processing Natu-ral Language Processing Pacific Rim Symposium.A.
Budanitsky, G. Hirst.
2006.
Evaluating WordNet-based measures of semantic distance.
Computa-tional Linguistics, 32(1):13?47.R.
L. Cilibrasi, P. M. Vitanyi.
2007.
The Google sim-ilarity distance.
IEEE Trans.
Knowledge and DataEngineering, 19(3):370?383.J.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Mea-surements, 20:37?46.82P.
Cook, A. Fazly, S. Stevenson.
2007.
Pulling theirweight: Exploiting syntactic forms for the automaticidentification of idiomatic expressions in context.
InProceedings of the ACL-07 Workshop on A BroaderPerspective on Multiword Expressions.S.
Evert, B. Krenn.
2001.
Methods for the qualitativeevaluation of lexical association measures.
In Proc.ACL-01.A.
Fazly, P. Cook, S. Stevenson.
2009.
Unsupervisedtype and token identification of idiomatic expres-sions.
Computational Linguistics, 35(1):61?103.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
ACMTransactions on Information Systems, 20(1):116?131.L.
Gerber, J. Yang.
1997.
Systran mt dictionary devel-opment.
In Proc.
Fifth Machine Translation Summit.M.
Halliday, R. Hasan.
1976.
Cohesion in English.Longman House, New York.D.
Hindle.
1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL-90,268?275.G.
Hirst, D. St-Onge.
1998.
Lexical chains as rep-resentations of context for the detection and correc-tion of malapropisms.
In C. Fellbaum, ed., Word-Net: An electronic lexical database, 305?332.
TheMIT Press.R.
Jackendoff.
1997.
The Architecture of the LanguageFaculty.
MIT Press.G.
Katz, E. Giesbrecht.
2006.
Automatic identifi-cation of non-compositional multi-word expressionsusing latent semantic analysis.
In Proceedings of theACL/COLING-06 Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties.A.
Kilgariff.
2007.
Googleology is bad science.
Com-putational Linguistics, 33(1):147?151.M.
Lapata, F. Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions onSpeech and Language Processing, 2:1?31.D.
D. Lewis, W. B. Croft.
1990.
Term clustering ofsyntactic phrase.
In Proceedings of SIGIR-90, 13thACM International Conference on Research and De-velopment in Information Retrieval.D.
Lin.
1998a.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of ACL-98.D.
Lin.
1998b.
Using collocation statistics in informa-tion extraction.
In Proc.
MUC-7.D.
Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,317?324.Y.
Matsuo, H. Tomobe, T. Nishimura.
2007.
Robustestimation of google counts for social network ex-traction.
In AAAI-07.G.
Minnen, J. Carroll, D. Pearce.
2001.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.S.
Mohammad, G. Hirst.
2006.
Distributional mea-sures of concept-distance: A task-oriented evalua-tion.
In Proceedings of EMNLP-06.S.
Pad?o, M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.S.
S. L. Piao, P. Rayson, D. Archer, A. Wilson,T.
McEnery.
2003.
Extracting multiword expres-sions with a semantic tagger.
In Proc.
of the ACL2003 Workshop on Multiword Expressions, 49?56.S.
P. Ponzetto, M. Strube.
2007.
Knowledge derivedfrom Wikipedia for computing semantic relatedness.Journal of Artificial Intelligence Research, 30:181?212.S.
Riehemann.
2001.
A Constructional Approach toIdioms and Word Formation.
Ph.D. thesis, StanfordUniversity.T.
T. Rila Mandala, H. Tanaka.
2000.
Query expansionusing heterogeneous thesauri.
Inf.
Process.
Man-age., 36(3).I.
A.
Sag, T. Baldwin, F. Bond, A. Copestake,D.
Flickinger.
2001.
Multiword expressions: a painin the neck for NLP.
In Lecture Notes in ComputerScience.G.
Salton, J. Allan, C. Buckley, A. Singhal.
1994.Automatic analysis, theme generation and sum-marization of machine-readable texts.
Science,264(3):1421?1426.F.
Smadja.
19993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19(1):143?177.C.
Spearman.
1904.
The proof and measurement ofassociation between two things.
Amer.
J. Psychol,72?101.C.
Sporleder, L. Li.
2009.
Unsupervised recognition ofliteral and non-literal use of idiomatic expressions.In Proceedings of EACL-09.N.
Wacholder, P. Song.
2003.
Toward a task-basedgold standard for evaluation of NP chunks and tech-nical terms.
In Proc HLT-NAACL.T.
Zesch, C. M?uller, I. Gurevych.
2008.
Using wik-tionary for computing semantic relatedness.
In Pro-ceedings of AAAI-08, 861?867.X.
Zhu, R. Rosenfeld.
2001.
Improving trigram lan-guage modeling with the world wide web.
In Pro-ceedings of ICASSP-01.83
