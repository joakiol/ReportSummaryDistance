Proceedings of NAACL HLT 2009: Short Papers, pages 193?196,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAnalysing Recognition Errors in Unlimited-Vocabulary Speech RecognitionTeemu Hirsima?ki and Mikko KurimoAdaptive Informatics Research CentreHelsinki University of TechnologyP.O.
Box 5400, 02015, TKK, Finlandteemu.hirsimaki@tkk.fiAbstractWe analyze the recognition errors made bya morph-based continuous speech recognitionsystem, which practically allows an unlim-ited vocabulary.
Examining the role of theacoustic and language models in erroneousregions shows how speaker adaptive training(SAT) and discriminative training with mini-mum phone frame error (MPFE) criterion de-crease errors in different error classes.
An-alyzing the errors with respect to word fre-quencies and manually classified error typesreveals the most potential areas for improvingthe system.1 IntroductionLarge vocabulary speech recognizers have becomevery complex.
Understanding how the parts of thesystem affect the results separately or together is farfrom trivial.
Still, analyzing the recognition errorsmay suggest how to reduce the errors further.There exist previous work on analyzing recogni-tion errors.
Chase (1997) developed error regionanalysis (ERA), which reveals whether the errorsare due to acoustic or language models.
Greenberget al (2000) analyzed errors made by eight recog-nition systems on the Switchboard corpus.
The er-rors correlated with the phone misclassification andspeech rate, and conclusion was that the acousticfront ends should be improved further.
Duta et al(2006) analyzed the main errors made by the 2004BBN speech recognition system.
They showed thaterrors typically occur in clusters and differ betweenbroadcast news (BN) and conversational telephonespeech (CTS) domains.
Named entities were a com-mon cause for errors in the BN domain, and hesita-tion, repeats and partially spoken words in the CTSdomain.This paper analyzes the errors made by a Finnishmorph-based continuous recognition system (Hir-sima?ki et al, 2009).
In addition to partitioning theerrors using ERA, we compare the number of let-ter errors in different regions and analyze what kindof errors are corrected when speaker adaptive train-ing and discriminative training are taken in use.
Themost potential error sources are also studied by par-titioning the errors according to manual error classesand word frequencies.2 Data and Recognition SystemThe language model training data used in the experi-ments consist of 150 million words from the FinnishKielipankki corpus.
Before training the n-grammodels, the words of the training data were splitinto morphs using the Morfessor algorithm, whichhas been shown to improve Finnish speech recogni-tion (Hirsima?ki et al, 2006).
The resulting morphlexicon contains 50 000 distinct morphs.
A growingalgorithm (Siivola et al, 2007) was used for traininga Kneser-Ney smoothed high-order variable-lengthn-gram model containing 52 million n-grams.The acoustic phoneme models were trained on theFinnish SpeechDat telephone speech database: 39hours from 3838 speakers for training, 46 minutesfrom 79 speakers for development and another simi-lar set for evaluation.
Only full sentences were usedand sentences with severe noise or mispronuncia-tions were removed.193AM scoreLM scoreLM scoreAM scoreHyp.Ref.
tiedontiedon valtavalta tietienmullistamullistaaa####?423?127?10.8?6.62?136?39.7?114?33.0?15.3?0.01?269?181?36.5?18.7?36.5?18.7?242?203?11.1?1.55?133?12.9?136?39.7?10.8?6.62?423?127AM: ?398.3  LM: ?214.01  TOT: ?612.31AM: ?386.1  LM: ?217.45  TOT: ?603.55Figure 1: An example of a HYP-AM error region.
Thescores are log probabilities.
Word boundaries are denotedby ?#?.
The error region only contains one letter error (aninserted ?n?
).The acoustic front-end consist of 39-dimensionalfeature vectors (Mel-frequency cepstral coefficientswith first and second time-derivatives), global max-imum likelihood linear transform, decision-tree tiedHMM triphones with Gaussian mixture models, andcepstral mean subtraction.Three models are trained: The first one is a max-imum likelihood (ML) model without any adap-tation.
The second model (ML+SAT) enhancesthe ML model with three iterations of speakeradaptive training (SAT) using constrained maxi-mum likelihood linear regression (CMLLR) (Gales,1998).
In recognition, unsupervised adaptationis applied in the second pass.
The third model(ML+SAT+MPFE) adds four iterations of discrim-inative training with minimum phone frame error(MPFE) criterion (Zheng and Stolcke, 2005) to theML+SAT model.3 Analysis3.1 Error Region AnalysisError Region Analysis (Chase, 1997) can be usedto find out whether the language model (LM), theacoustic model (AM) or both can be blamed foran erroneous region in the recognition output.
Fig-ure 1 illustrates the procedure.
For each utter-ance, the final hypothesis is compared to the forcedalignment of the reference transcript and segmentedinto correct and error regions.
An error region isa contiguous sequence of morphs that differ fromthe corresponding reference morphs with respect tomorph identity, boundary time-stamps, AM score,Letter errorsRegion ML ML+SAT ML+SAT+MPFEHYP-BOTH 962 909 783HYP-AM 1059 709 727HYP-LM 623 597 425REF-TOT 82 60 15Total 2726 2275 1950LER (%) 6.8 5.6 4.8Table 1: SpeechDat: Letter errors for different trainingmethods and error regions.
The reference transcript con-tains 40355 letters in total.LM score, or n-gram history1.By comparing the AM and LM scores in the hy-pothesis and reference regions, the regions can bedivided in classes.
We denote the recognition hy-pothesis as HYP, and the reference transcript as REF.The relevant classes for the analysis are the follow-ing.
REF-TOT: the reference would have better to-tal score, but it has been erroneously pruned.
HYP-AM: the hypothesis has better score, but only AMfavors HYP over REF.
HYP-LM: the hypothesis hasbetter score, but only LM favors HYP over REF.HYP-BOTH: both the AM and LM favor HYP.Since the error regions are independent, the let-ter error rate2 (LER) can be computed separately foreach region.
Table 1 shows the error rates for threedifferent acoustic models: ML training, ML+SAT,andML+SAT+MPFE.We see that SAT decreases allerror types, but the biggest reduction is in the HYP-AM class.
This should be expected.
In the ML case,the Gaussian mixtures contain much variance due todifferent unnormalized speakers, and since the testset contains only unseen speakers, many errors areexpected for some speakers.
Adapting the models tothe test set is expected to increase the acoustic scoreof the reference transcript, and since in the HYP-AMregions the LM already prefers REF, corrections be-cause of SAT are most probable there.On the other hand, adding MPFE after SAT seems1A region may be defined as an error region even if the tran-scription is correct (only the segmentation differs).
However,since we are going to analyze the number of letter errors in theerror regions, the ?correct?
error regions do not matter.2The words in Finnish are often long and consist of severalmorphs, so the performance is measured in letter errors insteadof word errors to have finer resolution for the results.194Letter errorsClass label Total HYP-BOTH HYP-AM HYP-LM REF-TOT Class descriptionForeign 156 89 61 6 Foreign proper nameInflect 143 74 26 43 Small error in inflectionPoor 131 37 84 10 Poor pronunciation or repairNoise 124 21 97 6 Error segment contains some noiseName 81 29 29 23 Finnish proper nameDelete 65 29 9 27 Small word missingAcronym 53 44 6 3 AcronymCompound 42 11 8 23 Word boundary missing or insertedCorrect 37 15 19 3 Hypothesis can be considered correctRare 27 11 3 13 Reference contains a very rare wordInsert 9 3 6 Small word inserted incorrectlyOther 1082 421 379 277 5 Other errorTable 2: Manual error classes and the number of letter errors for the ML+SAT+MPFE system.to reduce HYP-BOTH and HYP-LM errors, but notHYP-AM errors.
The number of search errors (REF-TOT) also decreases.All in all, for all models, there seems to be moreHYP-AM errors than HYP-LM errors.
Chase (1997)lists the following possible reasons for the HYP-AM regions: noise, speaker pronounces badly, pro-nunciation model is poor, some phoneme modelsnot trained to discriminate, or reference is plainlywrong.
The next section studies these issues further.3.2 Manual Error ClassificationNext, the letter errors in the error regions weremanually classified according to the most probablecause.
Table 2 shows the classes, the total numberof letter errors for each class, and the errors dividedto different error region types.All errors that did not seem to have an obviouscause are put under the class Other.
Some of the er-rors were a bit surprising, since the quality of theaudio and language seemed perfectly normal, butstill the recognizer got the sentences wrong.
On theother hand, the class also contains regions where thespeech is very fast or the signal level is quite low.The largest class with a specific cause is Foreign,which contains about 8 % of all letter errors.
Cur-rently, the morph based recognizer does not haveany foreign pronunciation modeling, so it is naturalthat words like Ching, Yem Yung, Villeneuve, Schu-macher, Direct TV, Thunderbayssa are not recog-nized correctly, since the mapping between the writ-ten form and pronunciation does not follow the nor-mal Finnish convention.
In Table 2 we see, that theacoustic model prefers the incorrect hypothesis in al-most all cases.
A better pronunciation model wouldbe essential to improve the recognition.
However,integrating exceptions in pronunciation to morph-based recognition is not completely straightforward.Another difficulty with foreign names is that theyare often rare words, so they will get low languagemodel probability anyway.The errors in the Acronym class are pretty muchsimilar to foreign names.
Since the letter-by-letterpronunciation is not modelled, the acronyms usuallycause errors.The next largest class is Inflect, which containserrors where the root of the word is correctly rec-ognized, but the inflectional form is slightly wrong(for example: autolla/autolle, kirjeeksi/kirjeiksi).
Inthese errors, it is usually the language model thatprefers the erroneous hypothesis.The most difficult classes to improve are perhapsPoor and Noise.
For bad pronunciations and repairsit is not even clear what the correct answer shouldbe.
Should it be the word the speaker tried to say,or the word that was actually said?
As expected, thelanguage model would have preferred the correct hy-pothesis in most cases, but the acoustic model havechosen the wrong hypothesis.The Name and Rare are also difficult classes.Contrary to the foreign names and acronyms, thepronunciation model is not a problem.1950500010000Lettersinreference0200400Letter errors0?1 1?3 3?7 7?15  ?31  ?63  ?127  ?255  ?511 ?4116 New051015Letter error rate(%)Subset of training data vocabulary (x 1000)Figure 2: Frequency analysis of the SAT+MPFE system.Number of letters in reference (top), number of letter er-rors (middle), and letter error rate (bottom) partitionedaccording to word frequencies.
The leftmost bar corre-sponds to the 1000 most frequent words, the next bar tothe 2000 next frequent words, and so on.
The rightmostbar corresponds to words not present in the training data.The Compound errors are mainly in HYP-LM re-gions, which is natural since there is usually lit-tle acoustic evidence at the word boundary.
Fur-thermore, it is sometimes difficult even for humansto know if two words are written together or not.Sometimes the recognizer made a compound worderror because the compound word was often writtenincorrectly in the language model training data.3.3 Frequency AnalysisIn order to study the effect of rare words in more de-tail, the words in the test data were grouped accord-ing their frequencies in the LM training data: Thefirst group contained all the words that were amongthe 1000 most common words, the next group con-tained the next 2000 words, then 4000, and so on,until the final group contained all words not presentin the training data.Figure 2 shows the number of letters in the ref-erence (top), number of letter errors (middle), andletter error rate (bottom) for each group.
Quite ex-pectedly, the error rates (bottom) rise steadily for theinfrequent words and is highest for the new wordsthat were not seen in the training data.
But lookingat the absolute number of letter errors (middle), themajority occur in the 1000 most frequent words.4 ConclusionsSAT and MPFE training seem to correct differenterror regions: SAT helps when the acoustic modeldominates and MPFE elsewhere.
The manual errorclassification suggests that improving the pronunci-ation modeling of foreign words and acronyms is apotential area for improvement.
The frequency anal-ysis shows that a major part of the recognition errorsoccur still in the 1000 most common words.
Onesolution might be to develop methods for detectingwhen the problem is in acoustics and to trust the lan-guage model more in these regions.AcknowledgmentsThis work was partly funded from the EC?s FP7project EMIME (213845).ReferencesLin Chase.
1997.
Error-Responsive Feedback Mecha-nisms for Speech Recognizers.
Ph.D. thesis, RoboticsInstitute, Carnegie Mellon University.Nicolae Duta, Richard Schwartz, and John Makhoul.2006.
Analysis of the errors produced by the 2004BBN speech recognition system in the DARPA EARSevaluations.
IEEE Trans.
Audio, Speech Lang.
Pro-cess., 14(5):1745?1753.M.
J. F. Gales.
1998.
Maximum likelihood linear trans-formations for HMM-based speech recognition.
Com-puter Speech and Language, 12(2):75?98.Steven Greenberg, Shuangyu Chang, and Joy Hollen-back.
2000.
An introduction to the diagnostic eval-uation of the Switchboard-corpus automatic speechrecognition systems.
In Proc.
NIST Speech Transcrip-tion Workshop.Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja, and Janne Pylkko?nen.
2006.Unlimited vocabulary speech recognition with morphlanguage models applied to Finnish.
Computer Speechand Language, 20(4):515?541.Teemu Hirsima?ki, Janne Pylkko?nen, and Mikko Kurimo.2009.
Importance of high-order n-gram models inmorph-based speech recognition.
IEEE Trans.
Audio,Speech Lang.
Process., 17(4):724?732.Vesa Siivola, Teemu Hirsima?ki, and Sami Virpioja.
2007.On growing and pruning Kneser-Ney smoothed n-gram models.
IEEE Trans.
Audio, Speech Lang.
Pro-cess., 15(5):1617?1624.Jing Zheng and Andreas Stolcke.
2005.
Improved dis-criminative training using phone lattices.
In Proc.
In-terspeech, pages 2125?2128.196
