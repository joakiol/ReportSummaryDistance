Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924?933,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAlgorithm Selection and Model Adaptation for ESL Correction TasksAlla Rozovskaya and Dan RothUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{rozovska,danr}@illinois.eduAbstractWe consider the problem of correcting errorsmade by English as a Second Language (ESL)writers and address two issues that are essen-tial to making progress in ESL error correction- algorithm selection and model adaptation tothe first language of the ESL learner.A variety of learning algorithms have beenapplied to correct ESL mistakes, but oftencomparisons were made between incompara-ble data sets.
We conduct an extensive, faircomparison of four popular learning methodsfor the task, reversing conclusions from ear-lier evaluations.
Our results hold for differenttraining sets, genres, and feature sets.A second key issue in ESL error correctionis the adaptation of a model to the first lan-guage of the writer.
Errors made by non-nativespeakers exhibit certain regularities and, as weshow, models perform much better when theyuse knowledge about error patterns of the non-native writers.
We propose a novel way toadapt a learned algorithm to the first languageof the writer that is both cheaper to imple-ment and performs better than other adapta-tion methods.1 IntroductionThere has been a lot of recent work on correct-ing writing mistakes made by English as a SecondLanguage (ESL) learners (Izumi et al, 2003; Eeg-Olofsson and Knuttson, 2003; Han et al, 2006; Fe-lice and Pulman, 2008; Gamon et al, 2008; Tetreaultand Chodorow, 2008; Elghaari et al, 2010; Tetreaultet al, 2010; Gamon, 2010; Rozovskaya and Roth,2010c).
Most of this work has focused on correctingmistakes in article and preposition usage, which aresome of the most common error types among non-native writers of English (Dalgish, 1985; Bitcheneret al, 2005; Leacock et al, 2010).
Examples belowillustrate some of these errors:1.
?They listen to None*/the lecture carefully.?2.
?He is an engineer with a passion to*/for what hedoes.
?In (1) the definite article is incorrectly omitted.
In(2), the writer uses an incorrect preposition.Approaches to correcting preposition and articlemistakes have adopted the methods of the context-sensitive spelling correction task, which addressesthe problem of correcting spelling mistakes that re-sult in legitimate words, such as confusing theirand there (Carlson et al, 2001; Golding and Roth,1999).
A candidate set or a confusion set is definedthat specifies a list of confusable words, e.g., {their,there}.
Each occurrence of a confusable word in textis represented as a vector of features derived from acontext window around the target, e.g., words andpart-of-speech tags.
A classifier is trained on textassumed to be error-free.
At decision time, for eachword in text, e.g.
there, the classifier predicts themost likely candidate from the corresponding con-fusion set {their, there}.Models for correcting article and preposition er-rors are similarly trained on error-free native Englishtext, where the confusion set includes all articlesor prepositions (Izumi et al, 2003; Eeg-Olofssonand Knuttson, 2003; Han et al, 2006; Felice andPulman, 2008; Gamon et al, 2008; Tetreault andChodorow, 2008; Tetreault et al, 2010).924Although the choice of a particular learning al-gorithm differs, with the exception of decision trees(Gamon et al, 2008), all algorithms used are lin-ear learning algorithms, some discriminative (Hanet al, 2006; Felice and Pulman, 2008; Tetreaultand Chodorow, 2008; Rozovskaya and Roth, 2010c;Rozovskaya and Roth, 2010b), some probabilistic(Gamon et al, 2008; Gamon, 2010), or ?counting?
(Bergsma et al, 2009; Elghaari et al, 2010).While model comparison has not been the goalof the earlier studies, it is quite common to com-pare systems, even when they are trained on dif-ferent data sets and use different features.
Further-more, since there is no shared ESL data set, sys-tems are also evaluated on data from different ESLsources or even on native data.
Several conclusionshave been made when comparing systems devel-oped for ESL correction tasks.
A language modelwas found to outperform a maximum entropy classi-fier (Gamon, 2010).
However, the language modelwas trained on the Gigaword corpus, 17 ?
109 words(Linguistic Data Consortium, 2003), a corpus sev-eral orders of magnitude larger than the corpus usedto train the classifier.
Similarly, web-based modelsbuilt on Google Web1T 5-gram Corpus (Bergsma etal., 2009) achieve better results when compared to amaximum entropy model that uses a corpus 10, 000times smaller (Chodorow et al, 2007)1.In this work, we compare four popular learningmethods applied to the problem of correcting prepo-sition and article errors and evaluate on a commonESL data set.
We compare two probabilistic ap-proaches ?
Na?
?ve Bayes and language modeling; adiscriminative algorithm Averaged Perceptron; and acount-based method SumLM (Bergsma et al, 2009),which, as we show, is very similar to Na?
?ve Bayes,but with a different free coefficient.
We train ourmodels on data from several sources, varying train-ing sizes and feature sets, and show that there aresignificant differences in the performance of thesealgorithms.
Contrary to previous results (Bergsma etal., 2009; Gamon, 2010), we find that when trainedon the same data with the same features, AveragedPerceptron achieves the best performance, followedby Na?
?ve Bayes, then the language model, and fi-nally the count-based approach.
Our results hold for1These two models also use different features.training sets of different sizes, genres, and featuresets.
We also explain the performance differencesfrom the perspective of each algorithm.The second important question that we address isthat of adapting the decision to the source languageof the writer.
Errors made by non-native speakersexhibit certain regularities.
Adapting a model sothat it takes into consideration the specific error pat-terns of the non-native writers was shown to be ex-tremely helpful in the context of discriminative clas-sifiers (Rozovskaya and Roth, 2010c; Rozovskayaand Roth, 2010b).
However, this method requiresgenerating new training data and training a separateclassifier for each source language.
Our key contri-bution here is a novel, simple, and elegant adaptationmethod within the framework of the Na?
?ve Bayesalgorithm, which yields even greater performancegains.
Specifically, we show how the error patternsof the non-native writers can be viewed as a differentdistribution on candidate priors in the confusion set.Following this observation, we train Na?
?ve Bayes ina traditional way, regardless of the source languageof the writer, and then, only at decision time, changethe prior probabilities of the model from the onesobserved in the native training data to the ones corre-sponding to error patterns in the non-native writer?ssource language (Section 4).
A related idea has beenapplied in Word Sense Disambiguation to adjust themodel priors to a new domain with different sensedistributions (Chan and Ng, 2005).The paper has two main contributions.
First, weconduct a fair comparison of four learning algo-rithms and show that the discriminative approachAveraged Perceptron is the best performing model(Sec.
3).
Our results do not support earlier conclu-sions with respect to the performance of count-basedmodels (Bergsma et al, 2009) and language mod-els (Gamon, 2010).
In fact, we show that SumLMis comparable to Averaged Perceptron trained witha 10 times smaller corpus, and language model iscomparable to Averaged Perceptron trained with a 2times smaller corpus.The second, and most significant, of our contribu-tions is a novel way to adapt a model to the sourcelanguage of the writer, without re-training the model(Sec.
4).
As we show, adapting to the source lan-guage of the writer provides significant performanceimprovement, and our new method also performs925better than previous, more complicated methods.Section 2 presents the theoretical component ofthe linear learning framework.
In Section 3, wedescribe the experiments, which compare the fourlearning models.
Section 4 presents the key result ofthis work, a novel method of adapting the model tothe source language of the learner.2 The ModelsThe standard approach to preposition correctionis to cast the problem as a multi-class classifica-tion task and train a classifier on features definedon the surrounding context2.
The model selectsthe most likely candidate from the confusion set,where the set of candidates includes the top n mostfrequent English prepositions.
Our confusion setincludes the top ten prepositions3: ConfSet ={on, from, for, of, about, to, at, in, with, by}.
Weuse p to refer to a candidate preposition fromConfSet.Let preposition context denote the preposition andthe window around it.
For instance, ?a passion towhat he?
is a context for window size 2.
We usethree feature sets, varying window size from 2 to 4words on each side (see Table 1).
All feature setsconsist of word n-grams of various lengths span-ning p and all the features are of the form s?kps+m,where s?k and s+m denote k words before and mwords after p; we show two 3-gram features for il-lustration:1. a passion p2.
passion p whatWe implement four linear learning models: thediscriminative method Averaged Perceptron (AP);two probabilistic methods ?
a language model (LM)and Na?
?ve Bayes (NB); and a ?counting?
methodSumLM (Bergsma et al, 2009).Each model produces a score for a candidate inthe confusion set.
Since all of the models are lin-ear, the hypotheses generated by the algorithms dif-fer only in the weights they assign to the features2We also report one experiment on the article correctiontask.
We take the preposition correction task as an example;the article case is treated in the same way.3This set of prepositions is also considered in other works,e.g.
(Rozovskaya and Roth, 2010b).
The usage of the ten mostfrequent prepositions accounts for 82% of all preposition errors(Leacock et al, 2010).Feature Preposition context N-gramset lengthsWin2 a passion [to] what he 2,3,4Win3 with a passion [to] what he does 2,3,4Win4 engineer with a passion [to] what he does .
2,3,4,5Table 1: Description of the three feature sets used inthe experiments.
All feature sets consist of word n-gramsof various lengths spanning the preposition and vary byn-gram length and window size.Method Free Coefficient Feature weightsAP bias parameter mistake-drivenLM ?
?
prior(p)?vl?vr?vr ?
log(P (u|vr))NB log(prior(p)) log(P (f |p))SumLM |F (S, p)| ?
log(C(p)) log(P (f |p))Table 2: Summary of the learning methods.
C(p) de-notes the number of times preposition p occurred in train-ing.
?
is a smoothing parameter, u is the rightmost wordin f , vl ?
vr denotes all concatenations of substrings vland vr of feature f without u.
(Roth, 1998; Roth, 1999).
Thus a score computedby each of the models for a preposition p in the con-text S can be expressed as follows:g(S, p) = C(p) +?f?F (S,p)wa(f), (1)where F (S, p) is the set of features active in con-text S relative to preposition p, wa(f) is the weightalgorithm a assigns to feature f ?
F , and C(p) isa free coefficient.
Predictions are made using thewinner-take-all approach: argmaxpg(S, p).
The al-gorithms make use of the same feature set F anddiffer only by how the weights wa(f) and C(p) arecomputed.
Below we explain how the weights aredetermined in each method.
Table 2 summarizes thefour approaches.2.1 Averaged PerceptronDiscriminative classifiers represent the most com-mon learning paradigm in error correction.
AP (Fre-und and Schapire, 1999) is a discriminative mistake-driven online learning algorithm.
It maintains a vec-tor of feature weights w and processes one trainingexample at a time, updating w if the current weightassignment makes a mistake on the training exam-ple.
In the case of AP, the C(p) coefficient refers tothe bias parameter (see Table 2).926We use the regularized version of AP in Learn-ing Based Java4 (LBJ, (Rizzolo and Roth, 2007)).While classical Perceptron comes with a generaliza-tion bound related to the margin of the data, Aver-aged Perceptron also comes with a PAC-like gener-alization bound (Freund and Schapire, 1999).
Thislinear learning algorithm is known, both theoreti-cally and experimentally, to be among the best linearlearning approaches and is competitive with SVMand Logistic Regression, while being more efficientin training.
It also has been shown to produce state-of-the-art results on many natural language applica-tions (Punyakanok et al, 2008).2.2 Language ModelingGiven a feature f = s?kps+m, let u denote therightmost word in f and vl ?
vr denote all concate-nations of substrings vl and vr of feature f withoutu.
The language model computes several probabil-ities of the form P (u|vr).
If f =?with a passionp what?, then u =?what?, and vr ?
{?with a pas-sion p?, ?a passion p?, ?passion p?, ?p?
}.
In prac-tice, these probabilities are smoothed and replacedwith their corresponding log values, and the totalweight contribution of f to the scoring function ofp is?vl?vr?vr ?
log(P (u|vr)).
In addition, thisscoring function has a coefficient that only dependson p: C(p) = ?
?
prior(p) (see Table 2).
The priorprobability of a candidate p is:prior(p) =C(p)?q?ConfSetC(q), (2)where C(p) and C(q) denote the number oftimes preposition p and q, respectively, occurred inthe training data.
We implement a count-basedLM with Jelinek-Mercer linear interpolation as asmoothing method5 (Chen and Goodman, 1996),where each n-gram length, from 1 to n, is associatedwith an interpolation smoothing weight ?.
Weightsare optimized on a held-out set of ESL sentences.Win2 and Win3 features correspond to 4-gramLMs and Win4 to 5-gram LMs.
Language modelsare trained with SRILM (Stolcke, 2002).4LBJ can be downloaded from http://cogcomp.cs.illinois.edu.5Unlike other LM methods, this approach allows us to trainLMs on very large data sets.
Although we found that backoffLMs may perform slightly better, they still maintain the samehierarchy in the order of algorithm performance.2.3 Na?
?ve BayesNB is another linear model, which is often hard tobeat using more sophisticated approaches.
NB ar-chitecture is also particularly well-suited for adapt-ing the model to the first language of the writer (Sec-tion 4).
Weights in NB are determined, similarly toLM, by the feature counts and the prior probabilityof each candidate p (Eq.
(2)).
For each candidatep, NB computes the joint probability of p and thefeature space F , assuming that the features are con-ditionally independent given p:g(S, p) = log{prior(p) ?
?f?F (S,p)P (f |p)}= log(prior(p)) ++?f?F (S,p)log(P (f |p)) (3)NB weights and its free coefficient are also summa-rized in Table 2.2.4 SumLMFor candidate p, SumLM (Bergsma et al, 2009)6produces a score by summing over the logs of allfeature counts:g(S, p) =?f?F (S,p)log(C(f))=?f?F (S,p)log(P (f |p)C(p))= |F (S, p)|C(p) +?f?F (S,p)log(P (f |p))where C(f) denotes the number of times n-gramfeature f was observed with p in training.
It shouldbe clear from equation 3 that SumLM is very similarto NB, with a different free coefficient (Table 2).3 Comparison of Algorithms3.1 Evaluation DataWe evaluate the models using a corpus of ESL es-says, annotated7 by native English speakers (Ro-zovskaya and Roth, 2010a).
For each preposition6SumLM is one of several related methods proposed in thiswork; its accuracy on the preposition selection task on nativeEnglish data nearly matches the best model, SuperLM (73.7%vs.
75.4%), while being much simpler to implement.7The annotation of the ESL corpus can be downloaded fromhttp://cogcomp.cs.illinois.edu.927Source Prepositions Articleslanguage Total Incorrect Total IncorrectChinese 953 144 1864 150Czech 627 28 575 55Italian 687 43 - -Russian 1210 85 2292 213Spanish 708 52 - -All 4185 352 4731 418Table 3: Statistics on prepositions and articles in theESL data.
Column Incorrect denotes the number ofcases judged to be incorrect by the annotator.
(article) used incorrectly, the annotator indicated thecorrect choice.
The data include sentences by speak-ers of five first languages.
Table 3 shows statistics bythe source language of the writer.3.2 Training CorporaWe use two training corpora.
The first corpus,WikiNYT, is a selection of texts from EnglishWikipedia and the New York Times section of theGigaword corpus and contains 107 preposition con-texts.
We build models of 3 sizes8: 106, 5 ?
106, and107.To experiment with larger data sets, we use theGoogle Web1T 5-gram Corpus, which is a collec-tion of n-gram counts of length one to five over acorpus of 1012 words.
The corpus contains 2.6 ?1010prepositions.
We refer to this corpus as GoogleWeb.We stress that GoogleWeb does not contain com-plete sentences, but only n-gram counts.
Thus, wecannot generate training data for AP for feature setsWin3 and Win4: Since the algorithm does not as-sume feature independence, we need to have 7 and9-word sequences, respectively, with a prepositionin the middle (as shown in Table 1) and their corpusfrequencies.
The other three models can be eval-uated with the n-gram counts available.
For exam-ple, we compute NB scores by obtaining the countof each feature independently, e.g.
the count for leftcontext 5-gram ?engineer with a passion p?
and rightcontext 5-gram ?p what he does .
?, due to the con-ditional independence assumption that NB makes.On GoogleWeb, we train NB, SumLM, and LM withthree feature sets: Win2, Win3, and Win4.From GoogleWeb, we also generate a smallertraining set of size 108: We use 5-grams witha preposition in the middle and generate a new8Training size refers to the number of preposition contexts.count, proportional to the size of the smaller cor-pus9.
For instance, a preposition 5-gram with acount of 2600 in GoogleWeb, will have a count of10 in GoogleWeb-108.3.3 ResultsOur key results of the fair comparison of the fouralgorithms are shown in Fig.
1 and summarized inTable 4.
The table shows that AP trained on 5 ?
106preposition contexts performs as well as NB trainedon 107 (i.e., with twice as much data; the perfor-mance of LM trained on 107 contexts is better thanthat of AP trained with 10 times less data (106), butnot as good as that of AP trained with half as muchdata (5?106); AP outperforms SumLM, when the lat-ter uses 10 times more data.
Fig.
1 demonstrates theperformance results reported in Table 4; it shows thebehavior of different systems with respect to preci-sion and recall on the error correction task.
We gen-erate the curves by varying the decision threshold onthe confidence of the classifier (Carlson et al, 2001)and propose a correction only when the confidenceof the classifier is above the threshold.
A higher pre-cision and a lower recall are obtained when the de-cision threshold is high, and vice versa.Key resultsAP > NB > LM > SumLMAP ?
2 ?NB5 ?AP > 10 ?
LM > APAP > 10 ?
SumLMTable 4: Key results on the comparison of algorithms.2 ?NB refers to NB trained with twice as much data asAP ; 10 ?
LM refers to LM trained with 10 times moredata asAP ; 10?SumLM refers to SumLM trained with10 times more data as AP .
These results are also shownin Fig.
1.We now show a fair comparison of the four algo-rithms for different window sizes, training data andtraining sizes.
Figure 2 compares the models trainedon WikiNY T -107 corpus for Win4.
AP is the su-perior model, followed by NB, then LM, and finallySumLM.Results for other training sizes and feature10 set9Scaling down GoogleWeb introduces some bias but we be-lieve that it should not have an effect on our experiments.10We have also experimented with additional POS-based fea-tures that are commonly used in these tasks and observed simi-lar behavior.9280102030405060  0102030405060PRECISIONRECALLSumLM-107LM-107NB-107AP-106AP-5*106 AP-107Figure 1: Algorithm comparison across differenttraining sizes.
(WikiNYT, Win3).
AP (106 prepositioncontexts) performs as well as SumLM with 10 times moredata, and LM requires at least twice as much data toachieve the performance of AP.configurations show similar behavior and are re-ported in Table 5, which provides model compari-son in terms of Average Area Under Curve (AAUC,(Hanley and McNeil, 1983)).
AAUC is a measurecommonly used to generate a summary statistic andis computed here as an average precision value over12 recall points (from 5 to 60):AAUC =112?12?i=1Precision(i ?
5)The Table also shows results on the article correc-tion task11.Training data Feature Performance (AAUC)set AP NB LM SumLMWikiNY T -5 ?
106 Win3 26 22 20 13WikiNY T -107 Win4 33 28 24 16GoogleWeb-108 Win2 30 29 28 15GoogleWeb Win4 - 44 41 32ArticleWikiNY T -5 ?
106 Win3 40 39 - 30Table 5: Performance Comparison of the four algo-rithms for different training data, training sizes, and win-dow sizes.
Each row shows results for training data of thesame size.
The last row shows performance on the articlecorrection task.
All other results are for prepositions.11We do not evaluate the LM approach on the article correc-tion task, since with LM it is difficult to handle missing articleerrors, one of the most common error types for articles, but theexpectation is that it will behave as it does for prepositions.0102030405060  0102030405060PRECISIONRECALLSumLM LM NB APFigure 2: Model Comparison for training data of thesame size: Performance of models for feature set Win4trained on WikiNY T -107.3.3.1 Effects of Window SizeWe found that expanding window size from 2 to 3is helpful for all of the models, but expanding win-dow to 4 is only helpful for the models trained onGoogleWeb (Table 6).
Compared to Win3, Win4 hasfive additional 5-gram features.
We look at the pro-portion of features in the ESL data that occurred intwo corpora: WikiNY T -107 and GoogleWeb (Ta-ble 7).
We observe that only 4% of test 5-grams oc-cur inWikiNY T -107.
This number goes up 7 timesto 28% for GoogleWeb, which explains why increas-ing the window size is helpful for this model.
Bycomparison, a set of native English sentences (dif-ferent from the training data) has 50% more 4-gramsand about 3 times more 5-grams, because ESL sen-tences often contain expressions not common for na-tive speakers.Training data Performance (AAUC)Win2 Win3 Win4GoogleWeb 35 39 44Table 6: Effect of Window Size in terms ofAAUC.
Per-formance improves, as the window increases.4 Adapting to Writer?s Source LanguageIn this section, we discuss adapting error correctionsystems to the first language of the writer.
Non-native speakers make mistakes in a systematic man-ner, and errors often depend on the first language ofthe writer (Lee and Seneff, 2008; Rozovskaya and929Test Train N-gram length2 3 4 5ESL WikiNY T -107 98% 66% 22% 4%Native WikiNY T -107 98% 67% 32% 13%ESL GoogleWeb 99% 92% 64% 28%Native-B09 GoogleWeb - 99% 93% 70%Table 7: Feature coverage for ESL and native data.Percentage of test n-gram features that occurred in train-ing.
Native refers to data from Wikipedia and NYT.
B09refers to statistics from Bergsma et al (2009).Roth, 2010a).
For instance, a Chinese learner ofEnglish might say ?congratulations to this achieve-ment?
instead of ?congratulations on this achieve-ment?, while a Russian speaker might say ?congrat-ulations with this achievement?.A system performs much better when it makes useof knowledge about typical errors.
When trainedon annotated ESL data instead of native data, sys-tems improve both precision and recall (Han et al,2010; Gamon, 2010).
Annotated data include boththe writer?s preposition and the intended (correct)one, and thus the knowledge about typical errors ismade available to the system.Another way to adapt a model to the first languageis to generate in native training data artificial errorsmimicking the typical errors of the non-native writ-ers (Rozovskaya and Roth, 2010c; Rozovskaya andRoth, 2010b).
Henceforth, we refer to this method,proposed within the discriminative framework AP,as AP-adapted.
To determine typical mistakes, errorstatistics are collected on a small set of annotatedESL sentences.
However, for the model to use theselanguage-specific error statistics, a separate classi-fier for each source language needs to be trained.We propose a novel adaptation method, whichshows performance improvement over AP-adapted.Moreover, this method is much simpler to imple-ment, since there is no need to train per source lan-guage; only one classifier is trained.
The methodrelies on the observation that error regularities canbe viewed as a distribution on priors over the cor-rection candidates.
Given a preposition s in text, theprior for candidate p is the probability that p is thecorrect preposition for s. If a model is trained on na-tive data without adaptation to the source language,candidate priors correspond to the relative frequen-cies of the candidates in the native training data.More importantly, these priors remain the same re-gardless of the source language of the writer or ofthe preposition used in text.
From the model?s per-spective, it means that a correction candidate, forexample to, is equally likely given that the author?spreposition is for or from, which is clearly incorrectand disagrees with the notion that errors are regularand language-dependent.We use the annotated ESL data and defineadapted candidate priors that are dependent on theauthor?s preposition and the author?s source lan-guage.
Let s be a preposition appearing in text bya writer of source language L1, and p a correctioncandidate.
Then the adapted prior of p given s is:prior(p, s, L1) =CL1(s, p)CL1(s),where CL1(s) denotes the number of times s ap-peared in the ESL data by L1 writers, and CL1(s, p)denotes the number of times p was the correct prepo-sition when s was used by an L1 writer.Table 8 shows adapted candidate priors for twoauthor?s choices ?
when an ESL writer used on andat ?
based on the data from Chinese learners.
Onekey distinction of the adapted priors is the high prob-ability assigned to the author?s preposition: the newprior for on given that it is also the preposition foundin text is 0.70, vs. the 0.07 prior based on the nativedata.
The adapted prior of preposition p, when p isused, is always high, because the majority of prepo-sitions are used correctly.
Higher probabilities arealso assigned to those candidates that are most oftenobserved as corrections for the author?s preposition.For example, the adapted prior for at when the writerchose on is 0.10, since on is frequently incorrectlychosen instead of at.To determine a mechanism to inject the adaptedpriors into a model, we note that while all of ourmodels use priors in some way, NB architecture di-rectly specifies the prior probability as one of its pa-rameters (Sec.
2.3).
We thus train NB in a traditionalway, on native data, and then replace the prior com-ponent in Eq.
(3) with the adapted prior, languageand preposition dependent, to get the score for p ofthe NB-adapted model:g(S, p) = log{prior(p, s, L1) ?
?f?F (S,p)P (f |p)}930Candidate Global Adapted priorprior author?s prior author?s priorchoice choiceof 0.25 on 0.03 at 0.02to 0.22 on 0.06 at 0.00in 0.15 on 0.04 at 0.16for 0.10 on 0.00 at 0.03on 0.07 on 0.70 at 0.09by 0.06 on 0.00 at 0.02with 0.06 on 0.04 at 0.00at 0.04 on 0.10 at 0.75from 0.04 on 0.00 at 0.02about 0.01 on 0.03 at 0.00Table 8: Examples of adapted candidate priors fortwo author?s choices ?
on and at ?
based on the er-rors made by Chinese learners.
Global prior denotesthe probability of the candidate in the standard modeland is based on the relative frequency of the candidatein native training data.
Adapted priors are dependent onthe author?s preposition and the author?s first language.Adapted priors for the author?s choice are very high.Other candidates are given higher priors if they often ap-pear as corrections for the author?s choice.We stress that in the new method there is no needto train per source language, as with previous adap-tion methods.
Only one model is trained, and onlyat decision time, we change the prior probabilities ofthe model.
Also, while we need a lot of data to trainthe model, only one parameter depends on annotateddata.
Therefore, with rather small amounts of data, itis possible to get reasonably good estimates of theseprior parameters.In the experiments below, we compare four mod-els: AP, NB AP-adapted and NB-adapted.
AP-adapted is the adaptation through artificial errorsand NB-adapted is the method proposed here.
Bothof the adapted models use the same error statistics ink-fold cross-validation (CV): We randomly partitionthe ESL data into k parts, with each part tested onthe model that uses error statistics estimated on theremaining k ?
1 parts.
We also remove all prepo-sition errors that occurred only once (23% of all er-rors) to allow for a better evaluation of the adaptedmodels.
Although we observe similar behavior onall the data, the models especially benefit from theadapted priors when a particular error occurred morethan once.
Since the majority of errors are not dueto chance, we focus on those errors that the writerswill make repeatedly.Fig.
3 shows the four models trained onWikiNY T -107.
First, we note that the adapted01020304050607080  01020304050PRECISIONRECALLNB-adaptedAP-adapted AP NBFigure 3: Adapting to Writer?s Source Language.
NB-adapted is the method proposed here.
AP-adapted andNB-adapted results are obtained using 2-fold CV, with50% of the ESL data used for estimating the new priors.All models are trained on WikiNY T -107.models outperform their non-adapted counterpartswith respect to precision.
Second, for the recallpoints less than 20%, the adapted models obtain verysimilar precision values.
This is interesting, espe-cially because NB does not perform as well as AP, aswe also showed in Sec.
3.3.
Thus, NB-adapted notonly improves over NB, but its gap compared to thelatter is much wider than the gap between the AP-based systems.
Finally, an important performancedistinction between the two adapted models is theloss in recall exhibited by AP-adapted ?
its curve isshorter because AP-adapted is very conservative anddoes not propose many corrections.
In contrast, NB-adapted succeeds in improving its precision over NBwith almost no recall loss.To evaluate the effect of the size of the data usedto estimate the new priors, we compare the perfor-mance of NB-adapted models in three settings: 2-fold CV, 10-fold CV, and Leave-One-Out (Figure 4).In 2-fold CV, priors are estimated on 50% of the ESLdata, in 10-fold on 90%, and in Leave-One-Out onall data but the testing example.
Figure 4 shows theaveraged results over 5 runs of CV for each setting.The model converges very quickly: there is almostno difference between 10-fold CV and Leave-One-Out, which suggests that we can get a good estimateof the priors using just a little annotated data.Table 9 compares NB and NB-adapted for twocorpora: WikiNY T -107 and GoogleWeb.
Since9310102030405060708090  0102030405060PRECISIONRECALLNB-adapted-LeaveOneOutNB-adapted-10-foldNB-adapted-2-fold NBFigure 4: How much data are needed to estimateadapted priors.
Comparison of NB-adapted modelstrained on GoogleWeb that use different amounts of datato estimate the new priors.
In 2-fold CV, priors are es-timated on 50% of the data; in 10-fold on 90% of thedata; in Leave-One-Out, the new priors are based on allthe data but the testing example.GoogleWeb is several orders of magnitude larger,the adapted model behaves better for this corpus.So far, we have discussed performance in termsof precision and recall, but we can also discuss itin terms of accuracy, to see how well the algorithmis performing compared to the baseline on the task.Following Rozovskaya and Roth (2010c), we con-sider as the baseline the accuracy of the ESL databefore applying the model12, or the percentage ofprepositions used correctly in the test data.
FromTable 3, the baseline is 93.44%13.
Compared tothis high baseline, NB trained on WikiNY T -107achieves an accuracy of 93.54, and NB-adaptedachieves an accuracy of 93.9314.Training data AlgorithmsNB NB-adaptedWikiNY T -107 29 53GoogleWeb 38 62Table 9: Adapting to writer?s source language.
Re-sults are reported in terms of AAUC.
NB-adapted is themodel with adapted priors.
Results for NB-adapted arebased on 10-fold CV.12Note that this baseline is different from the majority base-line used in the preposition selection task, since here we havethe author?s preposition in text.13This is the baseline after removing the singleton errors.14We select the best accuracy among different values that canbe achieved by varying the decision threshold.5 ConclusionWe have addressed two important issues in ESLerror correction, which are essential to makingprogress in this task.
First, we presented an exten-sive, fair comparison of four popular linear learningmodels for the task and demonstrated that there aresignificant performance differences between the ap-proaches.
Since all of the algorithms presented hereare linear, the only difference is in how they learnthe weights.
Our experiments demonstrated that thediscriminative approach (AP) is able to generalizebetter than any of the other models.
These resultscorrect earlier conclusions, made with incompara-ble data sets.
The model comparison was performedusing two popular tasks ?
correcting errors in articleand preposition usage ?
and we expect that our re-sults will generalize to other ESL correction tasks.The second, and most important, contribution ofthe paper is a novel method that allows one toadapt the learned model to the source language ofthe writer.
We showed that error patterns can beviewed as a distribution on priors over the correc-tion candidates and proposed a method of injectingthe adapted priors into the learned model.
In ad-dition to performing much better than the previousapproaches, this method is also very cheap to im-plement, since it does not require training a separatemodel for each source language, but adapts the sys-tem to the writer?s language at decision time.AcknowledgmentsThe authors thank Nick Rizzolo for many helpfuldiscussions.
The authors also thank Josh Gioja, NickRizzolo, Mark Sammons, Joel Tetreault, YuanchengTu, and the anonymous reviewers for their insight-ful comments.
This research is partly supported bya grant from the U.S. Department of Education.ReferencesS.
Bergsma, D. Lin, and R. Goebel.
2009.
Web-scalen-gram models for lexical disambiguation.
In 21st In-ternational Joint Conference on Artificial Intelligence,pages 1507?1512.J.
Bitchener, S. Young, and D. Cameron.
2005.
The ef-fect of different types of corrective feedback on ESLstudent writing.
Journal of Second Language Writing.A.
Carlson, J. Rosen, and D. Roth.
2001.
Scaling upcontext sensitive text correction.
In Proceedings of the932National Conference on Innovative Applications of Ar-tificial Intelligence (IAAI), pages 45?50.Y.
S. Chan and H. T. Ng.
2005.
Word sense disambigua-tion with distribution estimation.
In Proceedings ofIJCAI 2005.S.
Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Pro-ceedings of ACL 1996.M.
Chodorow, J. Tetreault, and N.-R. Han.
2007.
Detec-tion of grammatical errors involving prepositions.
InProceedings of the Fourth ACL-SIGSEM Workshop onPrepositions, pages 25?30, Prague, Czech Republic,June.
Association for Computational Linguistics.G.
Dalgish.
1985.
Computer-assisted ESL research.CALICO Journal, 2(2).J.
Eeg-Olofsson and O. Knuttson.
2003.
Automaticgrammar checking for second language learners - theuse of prepositions.
Nodalida.A.
Elghaari, D. Meurers, and H. Wunsch.
2010.
Ex-ploring the data-driven prediction of prepositions inenglish.
In Proceedings of COLING 2010, Beijing,China.R.
De Felice and S. Pulman.
2008.
A classifier-based ap-proach to preposition and determiner error correctionin L2 English.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 169?176, Manchester, UK, August.Y.
Freund and R. E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277?296.M.
Gamon, J. Gao, C. Brockett, A. Klementiev,W.
Dolan, D. Belenko, and L. Vanderwende.
2008.Using contextual speller techniques and languagemodeling for ESL error correction.
In Proceedings ofIJCNLP.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing.
In NAACL, pages 163?171,Los Angeles, California, June.A.
R. Golding and D. Roth.
1999.
A Winnow basedapproach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107?130.N.
Han, M. Chodorow, and C. Leacock.
2006.
Detectingerrors in English article usage by non-native speakers.Journal of Natural Language Engineering, 12(2):115?129.N.
Han, J. Tetreault, S. Lee, and J. Ha.
2010.
Us-ing an error-annotated learner corpus to develop andESL/EFL error correction system.
In LREC, Malta,May.J.
Hanley and B. McNeil.
1983.
A method of comparingthe areas under receiver operating characteristic curvesderived from the same cases.
Radiology, 148(3):839?843.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-hara.
2003.
Automatic error detection in the Japaneselearners?
English spoken data.
In The Companion Vol-ume to the Proceedings of 41st Annual Meeting ofthe Association for Computational Linguistics, pages145?148, Sapporo, Japan, July.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Morgan and Claypool Publishers.J.
Lee and S. Seneff.
2008.
An analysis of grammaticalerrors in non-native speech in English.
In Proceedingsof the 2008 Spoken Language Technology Workshop.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).N.
Rizzolo and D. Roth.
2007.
Modeling DiscriminativeGlobal Inference.
In Proceedings of the First Inter-national Conference on Semantic Computing (ICSC),pages 597?604, Irvine, California, September.
IEEE.D.
Roth.
1998.
Learning to resolve natural language am-biguities: A unified approach.
In Proceedings of theNational Conference on Artificial Intelligence (AAAI),pages 806?813.D.
Roth.
1999.
Learning in natural language.
In Proc.
ofthe International Joint Conference on Artificial Intelli-gence (IJCAI), pages 898?904.A.
Rozovskaya and D. Roth.
2010a.
Annotating ESLerrors: Challenges and rewards.
In Proceedings of theNAACL Workshop on Innovative Use of NLP for Build-ing Educational Applications.A.
Rozovskaya and D. Roth.
2010b.
Generating con-fusion sets for context-sensitive error correction.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).A.
Rozovskaya and D. Roth.
2010c.
Training paradigmsfor correcting errors in grammar and usage.
In Pro-ceedings of the NAACL-HLT.A.
Stolcke.
2002.
Srilm-an extensible language mod-eling toolkit.
In Proceedings International Confer-ence on Spoken Language Processing, pages 257?286,November.J.
Tetreault and M. Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages865?872, Manchester, UK, August.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Usingparse features for preposition selection and error de-tection.
In ACL.933
