Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 230?235,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDetecting Retries of Voice Search QueriesRivka LevitanColumbia University?rlevitan@cs.columbia.eduDavid ElsonGoogle Inc.elson@google.comAbstractWhen a system fails to correctly recog-nize a voice search query, the user will fre-quently retry the query, either by repeat-ing it exactly or rephrasing it in an attemptto adapt to the system?s failure.
It is de-sirable to be able to identify queries asretries both offline, as a valuable qualitysignal, and online, as contextual informa-tion that can aid recognition.
We presenta method than can identify retries offlinewith 81% accuracy using similarity mea-sures between two subsequent queries aswell as system and user signals of recogni-tion accuracy.
The retry rate predicted bythis method correlates significantly with agold standard measure of accuracy, sug-gesting that it may be useful as an offlinepredictor of accuracy.1 IntroductionWith ever more capable smartphones connectingusers to cloud-based computing, voice has been arapidly growing modality for searching for infor-mation online.
Our voice search application con-nects a speech recognition service with a searchengine, providing users with structured answers toquestions, Web results, voice actions such as set-ting an alarm, and more.
In the multimodal smart-phone interface, users can press a button to ac-tivate the microphone, and then speak the querywhen prompted by a beep; after receiving results,the microphone button is available if they wish tofollow up with a subsequent voice query.Traditionally, the evaluation of speech recogni-tion systems has been carried by preparing a testset of annotated utterances and comparing the ac-curacy of a system?s transcripts of those utterances?This work was done while the first author was an internat Google Inc.against the annotations.
In particular, we seek tomeasure and minimize the word error rate (WER)of a system, with a WER of zero indicating perfecttranscription.
For voice search interfaces such asthe present one, though, query-level metrics likeWER only tell part of the story.
When a user is-sues two queries in a row, she might be seeking thesame information for a second time due to a sys-tem failure the first time.
When this happens, froman evaluation standpoint it is helpful to break downwhy the first query was unsuccessful: it might bea speech recognition issue (in particular, a mis-taken transcription), a search quality issue (wherea correct transcript is interpreted incorrectly by thesemantic understanding systems), a user interfaceissue, or another factor.
As a second voice querymay also be a new query or a follow-up query, asopposed to a retry of the first query, the detectionof voice search retry pairs in the query steam isnon-trivial.Correctly identifying a retry situation in thequery stream has two main benefits.
The firstinvolves offline evaluation and monitoring.
Wewould like to know the rate at which users wereforced to retry their voice queries, as a measure ofquality.
The second has a more immediate ben-efit for individual users: if we can detect in realtime that a new voice search is really a retry of aprevious voice search, we can take immediate cor-rective action, such as reranking transcription hy-potheses to avoid making the same mistake twice,or presenting alternative searches in the user inter-face to indicate that the system acknowledges it ishaving difficulty.In this paper, we describe a method for the clas-sification of subsequent voice searches as eitherretry pairs of a certain type, or non-retry pairs.
Weidentify four salient types of retry pairs, describea test set and identify the features we extracted tobuild an automatic classifier.
We then describe themodels we used to build the classifier and their rel-230ative performance on the task, and leave the issueof real-time corrective action to future work.2 Related WorkPrevious work in voice-enabled information re-trieval has investigated the problem of identifyingvoice retries, and some has taken the additionalstep of taking corrective action in instances wherethe user is thought to be retrying an earlier utter-ance.
Zweig (2009) describes a system switchingapproach in which the second utterance is recog-nized by a separate model, one trained differentlythan the primary model.
The ?backup?
system isfound to be quite effective at recognizing thoseutterances missed by the primary system.
Retrycases are identified with joint language modelingacross multiple transcripts, with the intuition thatretry pairs tend to be closely related or exact dupli-cates.
They also propose a joint acoustic model inwhich portions of both utterances are averaged forfeature extraction.
Zweig et al (2008) similarlycreate a joint decoding model under the assump-tion that a discrete sent of entities (names of busi-nesses with directory information) underlies bothqueries.
While we follow this work in our usage ofjoint language modeling, our application encom-passes open domain voice searches and voice ac-tions (such as placing calls), so we cannot use sim-plifying domain assumptions.Other approaches include Cevik, Weng and Lee(2008), who use dynamic time warping to de-fine pattern boundaries using spectral features, andthen consider the best matching patterns to be re-peated.
Williams (2008) measures the overlap be-tween the two utterances?
n-best lists (alternate hy-potheses) and upweights hypotheses that are com-mon to both attempts; similarly, Orlandi, Culy andFranco (2003) remove hypotheses that are seman-tically equivalent to a previously rejected hypoth-esis.
Unlike these approaches, we do not assume astrong notion of dialog state to maintain per-statemodels.Another consequence of the open-domain na-ture of our service is that users are conditionedto interact with the system as they would with asearch engine, e.g., if the results of a search donot satisfy their information need, they rephrasequeries in order to refine their results.
This canhappen even if the first transcript was correct andthe rephrased query can be easily confused for aretry of a utterance where the recognition failed.Figure 1: Retry annotation decision tree.For purposes of latently monitoring the accuracyof the recognizer from usage logs, this is a signifi-cant complicating factor.3 Data and AnnotationOur data consists of pairs of queries sampled fromanonymized session logs.
We consider a pair ofvoice searches (spoken queries) to be a potentialretry pair if they are consecutive; we assume thata voice search cannot be a retry of another voicesearch if a typed search occurs between them.
Wealso exclude pairs for which either member has norecognition result.
For the purpose of our analy-sis, we further restricted our data to query pairswhose second member had been previously ran-domly selected for transcription.
A set of 8,254query pairs met these requirements and are consid-ered potential retry pairs.
1,000 randomly selectedpairs from this set were separated out and anno-tated by the authors, leaving a test set of 7,254 po-tential retry pairs.
Among the annotated develop-ment set, 18 inaudible or unintelligible pairs werediscarded, for a final development set of 982 pairs.The problem as we have formulated it requiresa labeling system that identifies repetitions andrephrases as retries, while excluding query pairsthat are superficially similar but have differentsearch intents.
Our system includes five labels.Figure 1 shows the guidelines for annotation thatdefine each category.The first distinction is between query pairs withthe same search intent (?Is the user looking forthe same information??)
and those with differentsearch intents.
We define search intent as the re-sponse the user wants and expects from the sys-tem.
If the second query?s search intent is differ-ent, it is by definition no retry.The second distinction we make is betweencases where the first query was recognized cor-231rectly and those where it was not.
Althougha query that was recognized correctly may beretried?for example, the user may want to bereminded of information she already received(other)?we are only interested in cases where thesystem is in error.If the search intent is the same for both queries,and the system incorrectly recognized the first,we consider the second query a retry.
We dis-tinguish between cases where the user repeatedthe query exactly, repetition, and where the userrephrased the query in an attempt to adapt to thesystem?s failure, rephrase.
This category includesmany kinds of rephrasings, such as adding or drop-ping terms, or replacing them with synonyms.The rephrased query may be significantly differ-ent from the original, as in the following example:Q1.
Navigate to chaparral ease.
(?Navigate to Chiappar-elli?s.?)Q2.
Chipper rally?s Little Italy Baltimore.
(?Chiappar-elli?s Little Italy Baltimore.?
)The rephrased query dropped a term (?Navigateto?)
and added another (?Little Italy Baltimore?
).This example illustrates another difficulty of thedata: the unreliability of the automatic speechrecognition (ASR) means that terms that are infact identical (?Chiapparelli?s?)
may be recog-nized very differently (?chaparral ease?
or ?chip-per rally?s?).
In the next example, the recognitionhypotheses of two identical queries have only asingle word in common:Q1.
I get in the house Google.
(?I did it Google?)Q2.
I did it crash cool.
(?I did it Google?
)Conversely, recognition hypotheses that arenearly identical are not necessarily retries.
Often,these are ?serial queries,?
a series of queries theuser is making of the same form or on the sametopic, often to test the system.Q1.
How tall is George Clooney?Q2.
How old is George Clooney?Q1.
Weather in New York.Q2.
Weather in Los Angeles.These complementary problems mean that wecannot use na?
?ve text similarity features to identifyretries.
Instead, we combine features that modelthe first query?s likely accuracy to broader similar-ity features to form a more nuanced picture of alikely retry.The five granular retry labels were collapsedinto binary categories: search retry, other, and noretry were mapped to NO RETRY; and repetitionand rephrase were mapped to RETRY.
The label(a) Granular labels(b) Collapsed (binary) labelsFigure 2: Retry label distribution.distribution of the final dataset is shown in Figure2.4 FeaturesThe features we consider can be divided into threemain categories.
The first group of features, sim-ilarity, is intended to measure the similarity be-tween the two queries, as similar queries are (withthe above caveats) more likely to be retries.
Wecalculate the edit distance between the two tran-scripts at the character and word level, as well asthe two most similar phonetic rewrites.
We includeboth raw and normalized values as features.
Wealso count the number of unigrams the two tran-scripts have in common and the length, absoluteand relative, of the longest unigram overlap.As we have shown in the previous section, sim-ilarity features alone cannot identify a retry, sinceASR errors and user rephrases can result in recog-nition hypotheses that are significantly differentfrom the original query, while a nearly identicalpair of queries can have different search intents.Our second group of features, correctness, goesup a level in our labeling decision tree (Figure 1)and attempts to instead answer the question: ?Wasthe first query transcribed incorrectly??
We usethe confidence score assigned by the recognizer tothe first recognition hypothesis as a measure of thesystem?s opinion of its own performance.
Sincethis score, while informative, may be inaccurate,we also consider signals from the user that mightindicate the accuracy of the hypothesis.
A booleanfeature indicates whether the user interacted withany of the results (structured or unstructured) thatwere presented by the system in response to thefirst query, which should constitute an implicit ac-ceptance of the system?s recognition hypothesis.The length of the interval between the two queriesis another feature, since a query that occurs imme-diately after another is likely to be a retry.
We alsoinclude the difference and ratio of the two queries?speaking rate, roughly calculated as the numberof vowels divided by the audio duration in sec-232onds, since a speaker is likely to hyperarticulate(speak more loudly and slowly) after being misun-derstood ((Wade et al, 1992; Oviatt et al, 1996;Levow, 1998; Bell and Gustafson, 1999; Soltauand Waibel, 1998)).The third feature group, recognizability, at-tempts to model the characteristics of a query thatis likely to be misrecognized (for the first queryof the pair) or is likely to be a retry of a previ-ous query (for the second query).
We look at thelanguage model (LM) score and the number of al-ternate pronunciations of the first query, predictingthat a misrecognized query will have a lower LMscore and more alternate pronunciations.
In ad-dition, we look at the number of characters andunigrams and the audio duration of each query,with the intuition that the length of a query maybe correlated with its likelihood of being retried(or a retry).
This feature group also includestwo heuristic features intended to flag the ?serialqueries?
mentioned before: the number of capital-ized words in each query, and whether each onebegins with a question word (who, what, etc.
).5 Prediction task5.1 Experimental ResultsA logistic regression model was trained on thesefeatures to predict the collapsed binary categoriesof NO RETRY (search retry, other, no retry) vs.RETRY (rephrase, repetition).
The results of run-ning this model with each combination of the fea-ture groups are shown in Table 1.Features Precision Recall F1 AccuracySimilarity 0.54 0.65 0.59 0.72Correctness 0.53 0.67 0.59 0.73Recognizability 0.49 0.63 0.55 0.70Sim.
& Corr.
0.67 0.71 0.69 0.77Sim.
& Rec.
0.62 0.70 0.66 0.76Corr.
& Rec.
0.65 0.71 0.68 0.77All Features 0.70 0.76 0.73 0.81Table 1: Results of the binary prediction task.Individually, each feature group peformed sig-nificantly better than the baseline strategy of al-ways predicting NO RETRY (62.4%).
Each pairof feature groups performed better than any indi-vidual group, and the final combination of all threefeature groups had the highest precision, recall,and accuracy, suggesting that each aspect of theretry conceptualization provides valuable informa-tion to the model.Of the similarity features, the ones that con-tributed significantly in the final model were char-acter edit distance (normalized) and phoneme editdistance (raw and normalized); as expected, re-tries are associated with more similar query pairs.Of the correctness features, high recognizer con-fidence, the presence of a positive reaction fromthe user such as a link click, and a long inter-val between queries were all negatively associatedwith retries.
The significant recognizability fea-tures included length of the first query in charac-ters (longer queries were less likely to be retried)and the number of capital letters in each query (asour LM is case-sensitive): queries transcribed withmore capital letters were more likely to be retried,but less likely to themselves be retries.
In addition,the language model likelihood for the first querywas, as expected, significantly lower for retries.Interestingly, the score of the second query waslower for retries as well.
This accords with ourfinding that retries of misrecognized queries arethemselves misrecognized 60%-70% of the time,which highlights the potential value of correctiveaction informed by the retry context.Several features, though not significant in themodel, are significantly different between theRETRY and NO RETRY categories, which affordsus further insight into the characteristics of a retry.T -tests between the two categories showed that alledit distance features?character, word, reduced,and phonetic; raw and normalized?are signifi-cantly more similar between retry query pairs.1Similarly, the number of unigrams the two querieshave in common is significantly higher for retries.The duration of each member of the query pair,in seconds and word count, is significantly moresimilar between retry pairs, and each member of aretry pair tends to be shorter than members of a noretry pair.
Finally, members of NO RETRY querypairs were significantly more similar in speakingrate, and the relative speaking rate of the secondquery was significantly slower for RETRY pairs,possibly due to hyperarticulation.5.2 AnalysisFigure 3 shows a breakdown of the true granularlabels versus the predicted binary labels.
The pri-mary source of error is the REPHRASE category,which is identified as a retry with only 16.5% ac-1T -tests reported here use a conservative significancethreshold of p < 0.00125 to control for family-wise type Ierror (?data dredging?
effects).233Figure 3: Performance on each of the granular categories.curacy.
This result reflects the fact that althoughrephrases conceptually belong in the retry cate-gory, their characteristics are materially different.Most notably, all edit distance features are signif-icantly greater for rephrases.
Differences in du-ration between the two queries in a pair, in sec-onds and words, are significantly greater as well.Rephrases also are significantly longer, in secondsand words, than strict retries.
The model includ-ing only correctness and recognizability featuresdoes significantly better on rephrases than the fullmodel, identifying them as retries with 25.6% ac-curacy, confirming that the similarity features arethe primary culprit.
Future work may address thisissue by including features crafted to examine thesimilarity between substrings of the two queries,rather than the query as a whole, and by expand-ing the similarity definition to include synonyms.To test the model?s performance with a larger,unseen dataset, we looked at how many retriesit detected in the test set of potential retry pairs(n=7,254).
We do not have retry annotations forthis larger set, but we have transcriptions for thefirst member of each query pair, enabling us to cal-culate the word error rate (WER) of each query?srecognition hypothesis, and thus obtain groundtruth for half of our retry definition.
A perfectmodel should never predict RETRY when the firstquery is transcribed correctly (WER==0).
Asshown in Figure 4, our model assigns a RETRYlabel to approximately 14% of the queries follow-ing an incorrectly recognized search, and only 2%of queries following a correctly recognized search.While this provides us with only a lower bound onour model?s error, this significant correlation withan orthogonal accuracy metric shows that we havemodeled at least this aspect of retries correctly, andsuggests a correlation between retry rate and tradi-tional WER-based evaluation.Figure 4: Performance on unseen data.
A perfect modelwould have a predicted retry rate of 0 when WER==0.6 ConclusionWe have presented a method for characterizing re-tries in an unrestricted voice interface to a searchsystem.
One particular challenge is the lack ofsimplifying assumptions based on domain andstate (as users may consider the system to bestateless when issuing subsequent queries).
Weintroduce a labeling scheme for retries that en-compasses rephrases?cases in which the user re-worded her query to adapt to the system?s error?as well as repetitions.Our model identifies retries with 81% accuracy,significantly above baseline.
Our error analysisconfirms that user rephrasings complicate the bi-nary class separation; an approach that modelstypical typed rephrasings may help overcome thisdifficulty.
However, our model?s performance to-day correlates strongly with an orthogonal accu-racy metric, word error rate, on unseen data.
Thissuggests that ?retry rate?
is a reasonable offlinequality metric, to be considered in context amongother metrics and traditional evaluation based onword error rate.AcknowledgmentsThe authors thank Daisy Stanton and MaryamKamvar for their helpful comments on this project.ReferencesLinda Bell and Joakim Gustafson.
1999.
Repetitionand its phonetic realizations: Investigating a swedishdatabase of spontaneous computer-directed speech.In Proceedings of ICPhS, volume 99, pages 1221?1224.Mert Cevik, Fuliang Weng, and Chin-Hui Lee.
2008.Detection of repetitions in spontaneous speech in di-234alogue sessions.
In Proceedings of the 9th AnnualConference of the International Speech Communi-cation Association (INTERSPEECH 2008), pages471?474, Brisbane, Australia.Gina-Anne Levow.
1998.
Characterizing and recog-nizing spoken corrections in human-computer di-alogue.
In Proceedings of the 17th internationalconference on Computational linguistics-Volume 1,pages 736?742.
Association for Computational Lin-guistics.Marco Orlandi, Christopher Culy, and Horacio Franco.2003.
Using dialog corrections to improve speechrecognition.
In Error Handling in Spoken LanguageDialogue Systems.
International Speech Communi-cation Association.Sharon Oviatt, G-A Levow, Margaret MacEachern,and Karen Kuhn.
1996.
Modeling hyperarticu-late speech during human-computer error resolu-tion.
In Spoken Language, 1996.
ICSLP 96.
Pro-ceedings., Fourth International Conference on, vol-ume 2, pages 801?804.
IEEE.Hagen Soltau and Alex Waibel.
1998.
On the influ-ence of hyperarticulated speech on recognition per-formance.
In ICSLP.
Citeseer.Elizabeth Wade, Elizabeth Shriberg, and Patti Price.1992.
User behaviors affecting speech recognition.In ICSLP.Jason D. Williams.
2008.
Exploiting the asr n-best by tracking multiple dialog state hypotheses.In Proceedings of the 9th Annual Conference ofthe International Speech Communication Associa-tion (INTERSPEECH 2008), pages 191?194, Bris-bane, Australia.Geoffrey Zweig, Dan Bohus, Xiao Li, and PatrickNguyen.
2008.
Structured models for jointdecoding of repeated utterances.
In Proceed-ings of the 9th Annual Conference of the Interna-tional Speech Communication Association (INTER-SPEECH 2008), pages 1157?1160, Brisbane, Aus-tralia.Geoffrey Zweig.
2009.
New methods for theanalysis of repeated utterances.
In Proceed-ings of the 10th Annual Conference of the Inter-national Speech Communication Association (IN-TERSPEECH 2009), pages 2791?2794, Brighton,United Kingdom.235
