A Tree.Trellis Based Fast Search for Finding the N Best SentenceHypotheses in Continuous Speech RecognitionFrank K. SoongEng-Fong Huang*AT&T Bell LaboratoriesMurray Hill, New Jersey 07974ABSTRACTIn this paper a new, tree-trellis based fast search forfinding the N best sentence hypotheses in continuousspeech recognition is proposed.
The search consists oftwo parts: a forward, time-synchronous, trellis search anda backward, time asynchronous, tree search.
In the firstmodule the well known Viterbi algorithm is used forfinding the best hypothesis and for preparing a map of allpartial paths scores time synchronously.
In the secondmodule a tree search is used to grow partial pathsbackward and time asynchronously.
Each partial path inthe backward tree search is rank ordered in a stack by thecorresponding full path score, which is computed byadding the partial path score with the best possible scoreof the remaining path obtained from the trellis path map.In each path growing cycle, the current best partial path,which is at the top of the stack, is extended by one arc(word).
The new tree-trellis earch is different from thetraditional time synchronous Viterbi search in its abilityfor finding not just the best but the N-best paths ofdifferent word content.
The new search is also differentfrom the A* algorithm, or the stack algorithm, in itscapability for providing an exact, full path score estimateof any given partial (i.e., incomplete) path before itscompletion.
When compared with the best candidateViterbi search, the search complexities for finding the N-best strings are rather low, i.e., only a fraction morecomputation is needed.I.
IntroductionIn a spoken language understanding (recognition) system,the search space of possible sentence hypotheses aredetermined by many factors such as the size ofrecognition vocabulary, the rigidity of grammar, thesystem dependency on specific speakers, etc.
When theseconstraining factors are relaxed, the space can be verylarge and the effort for finding a global optimalhypothesis may become expensive or even prohibitive.Sometimes, even for a smaller scale problem, certain* On leave from Telecommunication Labs, Chung-li, Taiwan.language constraints can not be easily incorporated into alow level, acoustic search.
For example, a check-sumgrammar used for detecting error in a digit string, due toits nonlinear and modulo arithmetic nature, can not bebuilt into a continuous digit recognizer except for sometrivial cases.To reduce the search effort, a spoken language system isin general divided into two stages: a continuous peechrecognition system followed by higher level languageprocessing modules.
First, the frame level acousticinformation is processed by a continuous speechrecognizer.
The output of a continuous speech recognizer,sn'ings of symbols or words, are then fed into higher levellanguage modules such as a sentence parser, semanticanalysis modules, etc.
for further processing.Unforttmately, such a division was usually done at a priceof sacrificing the optimality of solutions and the finaloutput, in most cases, is only suboptimal or not optimal atall.
This compromise is not really necessary and globaloptimality can be obtained if the following two conditionsare satisfied: first, the reduction of the search space in thefirst stage should not too greedy to cause any hard errorsso that the optimal solution are dropped from beingconsidered for further processing in the first stage; andsecond, the output of all processing modules should berank ordered according to some universal optimalitycriteria like the likelihood scores.To fulffill the above two conditions, it is important to todevise an efficient search for the N-best sentencehypotheses in the first stage where N should be adaptivelyadjustable.
By doing so, we can preserve the optimalityof final results while reducing the search space to amanageable size, i.e., an N element subset of all possiblehypotheses.
Existing efforts for finding N-best sentencehypotheses were devised in the level building algorithmby Meyer and Rabiner \[1\], a frame synchronous networksearch by Lee and Rabiner \[2\] and recently a sentencehypothesis earch by Steinbiss \[3\].
The resultant N-besthypotheses, however, are only optimal under constrainedconditions: lower ranked sentence candidates are derived12from the sentence segmentation of higher rankedcandidates.
Due to the constraints, the top-N candidatesthus derived are not exact.Recently an exact, top-N candidate search was proposedby Chow and Schwartz \[4\].
The top-N string hypothesesare obtained in a Viterbi-like, breadth-first earch set-up inthe first stage and they are then processed by knowledgesources in the second stage.
Another approach, proposedby Paul \[5\], is to use stack-based decodingalgorithms \[6,7,8\] as common tools for a continuousspeech recognizer and natural language processingmodules.
Likelihood scores are used as a common metricshared by both lhe recognizer and language modules.
Inthe same paper, a natural interface was proposed to link acontinuous peech recognizer and higher level languagemodules.In this paper, we present anewly proposed, fast tree-trellisbased N-best search \[9\] for finding the top-N sentencehypotheses in a continuous speech recognizer.
The searchA ?
uses an algorithm for finding the top, N sentencehypotheses.
However, different from other A or the stackalgorithm where heuristic ways are used to evaluate pathscores before completion and optimalities of the solutionsare compromised, the new algorithm is an exact N-bestsearch and only exact, not heuristic, path scores are used.The new algorithm generates the N-best hypothesessequentially.
Furthermore, the number of hypotheses neednot to be preset and the algorithm can terminate at anytime whenever a string hypothesis accepted as a validsentence.
The search is also computationally efficient dueto its tree-trellis earch nature.
The new algorithm onlyneeds a fraction more computation than a normal Viterbisearch for finding the top N sentence hypotheses.The rest of the paper is organized as follows.
In the nextsection, we present the fast tree-trellis algorithm.
InSection HI we discuss the optimality of the algorithm.
InSection IV we presem results obtained from testing thealgorithms on two different applications: connected igitrecognition using check-sum rules and the DARPAResource Management task using the word-pair grzmmarand the original finite state automata of DARPAsentences.
In Section V we confirm the efficiency of thetree-trellis earch by presenting a CPU time breakdown ofdifferem computation modules.If.
The Tree-Trellis N-Best SearchThe proposed algorithm, as its name indicates, is a fastsearch by combining a tree search based A* \[I0\] (or"stack") algorithm \[6,7,8\] with a trellis search basedmodified Viterbi algorithm.
A block diagram of thealgorithm is shown in Fig.
I.ACOUSTIC OBSERVA'I'IONS1, TRELUSSEARCH(FORWARD) 1"I IPA'r.
I ST,~ \[STRINGFRAME-ASYNCHRONOUS \[TREE j=SEARCH (BACKWARD) IN-BEST STRINGSHIGHER LEVELLANGUAGEPROCESSINGUKELJHOOD '1RECOGNIZED STRING(UNDERSTOOD)Fig.
I.
Block diagram of the trcc-tmRis searchAs shown in the diagram, first, input acousticobservations are compared with HMM models and acorresponding log likelihood map is generated.
The trellissearch is then performed time (frame) synchronously inthe forward (left-to-right) direction and the search isguided by a given grammar and/or any knowledge sourcewhich is incotporated into the Viterbi search.
In additionto output the best sentence hypothesis, a path map of allpartial paths is registered in the Viterbi search.
Thepartial path map contains scores of all partial paths thatlead to any grammar node at every time instant.At the end of the trellis search, a new, tree search forfinding the best N sentence hypotheses is initiated.
Thesearch is performed backward in time and frameasynchronously.
It is a best-first search implemented byusing an A* search or the stack algorithm.
The N-bestcandidates are found one at a time and each candidate isthen fed sequentially to higher level processing modules.The final result of the whole system is a recognized(understood) string.
In the following subsections wepresent individual modules of the trce-trctlls search indetail.11.1 Modified Viterbi Algorithm (MVA)The Viterbi algorithm (trellis search) is modified in thenew tree-trellis search to generate a partial path map.The map is needed by the A* tree search.
The modified13Viterbi algorithm is given as follows.Trell is Search (Modified Viterbi) AlgorithmINITIALIZE: (1) path scores; (2) arc ranking indices;(3) backpointers (optional)LOOP I: loop over time indices from left to fightLOOP II: loop over grammar nodesLOOP III: loop over arcs of a grammar nodeLOOP IV: loop over states of an arc (word)Evaluate dynamic programming recursionUpdate accumulated likelihood arrays- -  Update backpointer arrays (optional)LOOP IV controlFor every grammar node,- -  sort accumulated likelihood path scores- -  register arc ranking index arrays- -  register "from frame" arrays (optional)LOOP III controlLOOP II controlLOOP I controlAfter all bookkeeping arrays are initiated, four nestedloops are performed.
The dynamic programming startsfirst from the outermost loop, a loop over the time indicesfrom left to right frame synchronously, over a loop of allgrammar nodes, then over all arcs (words) of a grammarnode and finally, over the innermost loop of all ,statesassociated with an arc (word).
Since the best path will beobtained in the backward tree search as the first sentencehypothesis output, it is not necessary to register anybackpointer arrays and all backtracking operations areonly optional.
The arc (word) ranking index arrays arerecorded only when the number of possible arcs (words)at a node exceeds N, the number of sentences hypothesesto be found.
In addition to the best partial path, an theother partial paths that lead to a grammar node, arerecorded in the modified Viterbi algorithm.II.2 Tree Search A ?
Algorithm for Finding the N,bestStringsAt the end of the modified Viterbi search, a backward treesearch is initiated from a terminal node and the search isperformed time asynchronously in a backward (fight-to-left) direction.
The tree search is implemented using anA* search or the stack algorithm.
However, differentfrom a typical A ?
search where the incomplete portion ofa partial path is estimated (or predicted) using ~someheuristics, the tree search here uses the partial path mapprepared in the first stage trellis search ar 4 the score ofthe incomplete portion of a path in the search tree is thenexactly known.
The backward partial paths are rankordered in the stack based upon the exact scores of theircorresponding full but yet incomplete paths.
The tree-trellis search algorithm has other advantages over abreadth-first N-best search in its ability to outputsequentially the N-best hypotheses, one at a time,according to descending likelihood scores.
The backwardtree search can be best illustrated by a conceptual diagramdepicted in Fig.
2.o-  .
.
.
.
.MAIN STACK JI/ I / I SECONDARY STACKFig.
2.
Conceptual diagram of path growing in a treeTwo stacks, a main stack and a secondary stack, are twolist data structures for storing partial hypotheses.
In themain stack, all partial paths, are rank ordered according totheir likelihood scores.
The best partial path at the top ofthe main stack gets extended in every path growing cycle.As shown in the figure, the top entry in the main stack isfirst split into two parts, the best one word (arc) extensionand the set of remaining one word (arc) extensions.These two extensions are stored temporaarily in thesecondary stack and then reinserted back into the mainstack so that all main stack entries are still rank ordered.The modified A ?
algorithm used to grow a tree issummarized as foUows:14N-Best Tree Search (A ?)
AlgorithmINITIALIZE: put the root node in a rankordered list (main stack) to form a null partial pathLOOP: best first path growing loopTake the top entry (the best partial path) off the main stackIF the top entry is a single path (i.e., not a group of partial paths),THENIF the best partial path is complete (i.e., leads to aterminal node), THENoutput he path and increment the outputhypothesis counter by oneIF output counter equals N, THENstopENDIFELSEENDIFSplit the partial path into two sets: the best one-arc extension and theremaining one-arc extensions.Use the partial path map provided by the Viterbi algorithm in evaluatingthe one-arc extensions.Store the two sets temporarily into the secondary suck and then reinsertthem back into the main stack such that the main stack is still rankordered.
Ranking is based upon complete path scores.ELSESplit the set of partial paths into two sets: the best partial path and theremaining partial paths in the set.Store them temporarily into the secondary stack and then reinsert hem backinto the main stack such that the main stack is still rank ordered.ENDIFLOOP CONTROLH.3 Partial Path Merging at a Grammar NodeThe search of the N-best paths in continuous peechrecognition is somewhat more complicated than a typicalgraph search problem due to the time varying nature ofthe graph, i.e., the cost of a path varies with time.
In otherwords, a single path in a graph is actually a set of pathsof different time signatures (trajectories).
Since weconsider only paths of different word content, paths of thesame word content but with different rajectories have tobe compared first and only the best path is retained and itis then compared with other best paths of different wordcoment.
In search of the N-best sentence hypotheses, thebest paths between the start node and the terminal nodecan pass any given node in-between at any time instantsshould be compared.
These paths can be filrther dividedinto two sets of partial paths: backward partial pathsgrown in the tree search with forward partial paths grownin the trellis search at a specific grammar node asillustrated in Fig.
3.15WORD I11MEBACKWARD t "T-- T R E E S/ / /I / / I ~' 1 / J '~I I ,,' / / "  s ' "  .4 ~/ / / / / .
.~- "  \/ //// ~, ; .
;A"  I-TRELLISI !
/ /  - f"~"t-1 TIMEFORWARDM th GRAMMARNODEFig.
3.
Merging forward and backward partial pathsAs shown in the figure, solid lines represent partial pathsgrown by the backward tree search while broken linesrepresent partial paths grown by the forward trellis search.To be specific, partial paths leading to a grammar node,say the N m node, from a terminal node and partial pathsstemming from the root node, passing a grammar nodewhich is a predecessor f the N-th node, say the M-thnode, along the arc of word i are merged at the N-thnode at matched time instants.
The best path is the onewith the maximum summed likelihood scores.Ill.
Optimafity of the Tree-Trellis SearchThe optimality of the A* search has been proven before,e.g., \[10\].
It is stated as the admissibility of the A*algorithm.
That is, if there is a path from the root nodeto a terminal node, A* terminates by finding an optimalpath.
There is also an interesting property that associateswith any node n chosen for path growing.
That is, thepath score, a summation of the computed partial pathscore from the root node to the node n and the estimatedincomplete partial path score from node n to the terminalnode, is equal to or better than the optimal path scorebetween the root node and the terminal node.
Theequality holds in our modified A* algorithm because theexact rather than an estimated score of the incompletepath between and the terminal node is precomputed inthe first stage Viterbi search and is readily available.
Thisequality maximizes the A" search efficiency andminimizes the main stack size to N, the number ofcandidates.
The search efficiency is maximized becausethe exact score for the incomplete partial path is usedinstead of an estimated upperbound.In terms of the storage, a main stack of size N, issufficient o maintain the N-best hypotheses within thesearch procedure because any partial path on the stackdoes not change its optimal (complete) path scorethroughout the search.IV.
Applications to Check-Sum Based ConnectedDigit RecognitionThe development of the fast tree-tretlis search for findingthe N-best candidate sentence strings was originallymotivated by a continuous digit recognition application.The American Express (AMEX) credit card companyinitiated a project recently to automate its procedure inverifying the merchant I.D., the credit card number andauthorizing the dollar purchase mount  via telephonecalls.
Currently all verifications and authorizations arecarried out by human operators and the final goal of theproject is to replace as many human operators as possibleby automatic speech recognizers while maintaincomparable recognition performance.
Both credit cardnumbers and merchant I.D.
's are fixed length digit strings,i.e., 10 digits for a merchant I.D.
and 15 digits for a creditcard number.The last digit of each digit string (merchant I.D.
or acredit card, is a check-sum digit, which is a nonlinear,modulo, combination of the previous digits.
The check-sum digit is installed for security reasons and the exactformulas for generating the check-sum digit axe not givenhere.
The check-sum rules, despite their simplicity, cannot be incorporated irectly into a continuous speechrecognizer.
Because xcept by using an exhaustive, henceprohibitive, finite state network, the check sum formulascan not be tested before all digits in a string are available(recognized).
Consequently, the search for the correctdigit string is thus divided into two stages: a continuousdigit recognition and a check-sum test.
The fast tree-treU.is search for finding the top-N candidates i then idealfor this two-stage processing.
The sentence hypothesesare found sequentially and they are tested against thecheck-sum rules.
If at any time, a string passes thecheck-sum rules, search stops.
Otherwise, the next digitstring with a lower likelihood score is then fetched andtested.Two digits strings (credit card number and merchantI.D.)
from each speaker were recorded over each local orlong distance telephone call to the American Expressprocessing facilities in Phoenix, Arizona.
1,800 digitstrings recorded by 900 speakers constitute the training16data base.
Separate 100 strings of merchant I.D.
's and 114strings of credit card numbers recorded by a different setof speakers form the test data base.
Various "real world"conditions are present in the recordings, including:speakers with a strong foreign accent, music or humanconversations in the background, tone noise, cross-channelinterference from different elephone lines, etc.
The SNRof the recordings i  around 25 to 30 dB.In constructing word-based HMMs, 13 words were chosento form the vocabulary, which consists of the ten digits,i.e., {"0"  to "9"},  "oh",  silence, and extraneous speech.Two HMM models were built for each word in thevocabulary.
For each state in a word models, a 64 mixturecomponent continuous Gaussian mixture probabilitydensity function was trained.
Both the best string Viterbisearch and the top-N, tree-trellis earch were used torecognized the strings in the test data base.The recognition results are tabulated in Table I by stringaccuracies.Type Top 1 Top 10 + check sumCredit Card 84 98Merchant I.D.
82 97Tab le  L Recognition String Accuracy (%) for theAMEX TrialsThe string accuracies of the best word hypothesis obtainedfrom an unmodified Viterbi decoding are 84% and 82%for the credit card and merchant I.D.
recognition,respectively.
However, when we used the check-sumrules to check the top-10 candidate obtained from thetree-treUis earch the string accuracy was improved from84% to 98% for the credit card number ecognition andfrom 82% to 97% for the merchant I.D.
recognition.
Thishigh level of recognition performance has been reportedbefore e.g., \[11\], but it was achieved with a cleanmicrophone data base recorded in a sound booth.
Theresults of this experiment demonstrate that highperformance connected igit recognition in a real worldenvironment is achievable when simple error detectionrules are used in conjunction with the new tree-treUissearch algorithm.
As an example, the top-10 candidatesof a merchant I.D.
recognition trial is listed according totheir correspond ecreasing likelihood scores.1 4 2 8 4 11 2 7 11 5time for likelihood map = 55.29time for Viterbi = 37.061 19.25 1 4 2 8 4 11 2 7 11 52 19.19 1 4 2 7 4 11 2 7 0 53 19.18 1 4 2 8 4 I1 2 7 0 54 19.15 1 4 2 9 4 11 2 7 11 55 19.15 1 4 2 8 4 11 7 7 11 56 19.14 1 4 2 8 4 11 2 7 11 47 19.14 1 4 2 8 4 11 0 7 11 58 19.13 1 4 2 7 4 11 2 7 0 59 19.12 1 4 2 6 4 11 2 7 11 510 19.12 1 4 2 2 4 11 2 7 11 5time for multi-candidate tree search = 5.89In this example, while the best digit string happens to bethe correct string, the rest 9 candidate suing are differentfrom the correct string only by one or two digits and aillikelihood scores are very dose.
Also shown are the CPUlime breakdown for computing the likelihood map, thetrellis (Viterbi) search and the-tree search.
The timerequired in the tree search for finding the top-10 candidatestrings is only about 15% of the amount needed in theforward trellis search.
A different example of merchantI.D.
recognition is depicted in Fig.
4, where the 10-bestdigit strings are displayed with corresponding wordboundaries, word likelihood scores (average) and theirrankings.s t r  2?
:  1 4 2 ?
S 0 2 5 S 1j .
.
.
.
.
:o ::2 : ' s  ;;?
'1 ', ; ; ; ;  .
.
.
.
.
~ f l~._..,,-.,-..: :,  ', ,, ,,., : :,.,....,-~ ;._,.,;,.,.,.,,..~, ;z ;?
;2 ;;0 is :o ,..., ~!s ;;s i~ ;: ' ' :! "
;.... ._~.......~ .
.
.
.
.
.i~ ;, 12 ~;0 ;s :o ~!,  i ls ;i, ;, ;il ;, :2 ::o :s ;o ;i' ~i s i~ s i ~ ir=-" -=5 r - .
- - - - - i  ~ .
.
.
.
.
_ j  , , ,  , ,;a ;?
:2 ::0 :5 ;o ::o : : s  ;;s ~l ;; - - ' ,~- - -4 '  ': 1 ,_.._,, 1_._~ ," ' - - ,  L .
.
- , - - - -  .
.
.
.
;~ i?
;2 ;io ;s ;o ii' :!s ;i, ;, ;J , * - , e j ,  , ,;~ ;' i '  ii ?
:, :o : : ,  :: '  ;;' i* io , ,11 :~ :2 ::o :5 :o :: j  ::s ;is ;t ;, ; , ; ;  ; ; 0, ; ; .
.
.
.11 ;' \]2 ilo is 1* ::2 .
:i' ;;s ;?
;i i i ** * i1 t , .
', ',* :2 ::o '.s 'o "7 '.
:s ',',* ;t 'DCg, P ~ ' ' t0 .0T l tbg  |FPJU41E ~ 10181Fig.
4.
Multi-candidates and their segmentations499.0d1|)456?|g1017The segmented but unmarked portions in the figure wererecognized as either silence or extraneous peech, Thecorrect string, or the second best recognized string, isdifferent from the best recognized string by a single digitconfusion ("2"  with "7").
Similar competing wordtokens can be collected for training discriminative HMMmodels \[12-15\].V.
Applications to the DARPA Resource ManagementTaskThe new search procedure was also applied to theDARPA resource management task and some preliminaryresults are reported here.
Forty seven context-independentHMM phone models were trained by using 3,200sentences (40 sentences/talker).
Each phone HMM has 3states and the output probability density function (pdf) ofeach state is characterized by a 32-component, Gaussianmixture densities.
A 150-sentence test set used byLee \[16\] was used as a test set and recognition stringaccuracies are given in Table III.Type Top 1 : Top 100 + FSN checkCMU150 38 75Table II.
String accuracy (%) for the DARPA taskWhen a beam-search based Viterbi search was used todecode an input string under a word-pair grammarconstraint, a sentence string accuracy of 38% wasobtained (perplexity 60).
But when we used the tree-trellis search and incorporated the finite state network(i.e., a perplexity 9) as the second stage processing, thesentence accuracy was almost doubled to 75% as shownin the table.
This result, in principle, can be similarlyachieved by using a full finite state grammar search butwith much higher search complexities.
An example of thetop-10 candidates obtained in the search is given asfollows along with their CO1Tesponding average loglikelihood (per frame).The correct string is the 7-th candidate with an averagelog likelihood score of 11.806, only 0.03 less than thestring with the highest score.
Almost all major contentwords, especially those with longer durations, arerecognized in the top-10 strings.
The extra computationeffort for finding the top--10 candidates in the tree searchis 1.8 sec, about 2.5% of the time needed for the forwardtrellis beam search.VI.
Computation Breakdown of the Tree-TrellisAlgorithmBy breaking the search into a treUis (modified Viterbi)and a tree search, the N-top sentence hypotheses can beobtained sequentially and the search effort is greatlyreduced compared with other breadth-first based N-topcandidate search.
We used the internal timing routines ofan Alliant computer to measure the CPU time spent oneach individual module in the AMEX digit recognitiontrials.
The breakdown is given in terms of percentage inTable m.Type Pementage (%)Likelihood Map 56Trellis 38Tree ( lop 10 + checksum) 6Table I I l. Computation breakdown (%) for the newsearchHOW SOON CAN ESTEEM CHOP TO ATLANTIC FLEETtime for likelihood map = 16.79time for1 11.8362 11.8243 11.8234 11.8235 11.8116 11.8117 11.8068 11.8049 11.80410 11.799Viterbi beam search = 71.29HOW SOON CAN ESTEEM IN SOUTH TWO ATLANTIC FLEETHOW SOON CAN ESTEEM A SOUTH TWO ATLANTIC FLEETHOW SOON CAN ESTEEM IN SOUTH TWO ATLANTIC FI.EET TOHOW SOON CAN ESTEEM IN SOUTH TWO ATLANTIC FLEET INHOWHOWHOWHOWHOWHOWSOON CAN ESTEEM A SOUTH TWO ATLANTIC FLEET TOSOON CAN ESTEEM A SOUTH TWO ATLANTIC FLEET INSOON CAN ESTEEM CHOP TO ATLANTIC FLEET ***SOON CAN ESTEEM TO SOUTH TWO ATLANTIC FLEETSOON CAN ESTEEM THE SOUTH TWO ATLANTIC FLEETSOON CAN ESTEEM IN SOUTH ATLANTIC FLEETtime for multi-candidate n e search = 1.7918As shown in the table, the likelihood map computation,for the specific task (AMEX uials) and the HMMs usedconstitutes about 56% of the total CPU time.
Moreefficient algorithm can be implemented, e.g., a partialrather than a full table of likelihood functions can becomputed on demand, but it was not incorporated in ourimplementation.
The trellis search, or the modifiedViterbi algorithm, consumes about 38% of the CPU timewhile the final top-10 candidate, tree-search needs the rest6%.
The check-sum test, based upon very simplearithmetics, takes virtually no CPU time at all.
The extracomputational needed for finding the top N candidates ofthe tree-trellis earch is minimal.VII.
ConclusionIn this paper, we propose a new, tree-trellis based, fastsearch algorithm for finding the top N sentencehypotheses in continuous speech recognition.
Thealgorithm uses a bi-directional search consisting of aforward, time synchronous, trellis search and a backward,time asynchronous, tree search.
In the new algorithm, dueto the partial path map prepared in the trellis search, thebackward tree search is highly efficient and a shallowstack, i.e., of a size N, is needed.
The algorithm has beentested successffially on two different data bases: theAmerican Express credit service data base and theDARPA resource management data base.
In the formerdata base, multiple candidate digit strings weresuccessively generated and tested against some check-sumrules, the digit string accuracy was improved by 14-15%.For the DARPA database, when the finite state grammarwas used to screen out invalid sentence hypotheses in thetop 100 candidates, the string error was reduced by morethan a half.
It was also shown in the experiments hat thetop N candidates were obtained with a minimalcomputational overhead.References\[1\] Meyer, C. S. and Rabiner, L. R., "Connected DigitRecognition Using a Level Building DTW Algorithm,"IEEE trans, on ASSP, Vol.
ASSP-29, pp.
351-363, June1981.\[2\] Lee, C. H. and Rabiner, L. R., "A Frame SynchronousLevel Building Algorithm for Connected WordRecognition," Comput.
Speech Language, Vol.
1, no.
1,pp.
29-45, Mar.
1986.\[3\] Steinbiss, "Sentence Hypothesis Generation in aContinuous Speech Recognition System," Proc.European Conf.
on Speech Comm.
and Tecl.
pp.
51-54,Paris, Sept. 1989.\[4\] Chow, Y. and Schwartz, R., "The N-Best Algorithm:An Efficient Procedure for Finding top N SentenceHypotheses," Proc.
Speech and Natural LanguageWorkshop, Oct., 1989, pp.
199-202, also Proc.ICASSP-90, pp.
81-84, Apr.
1990, NM.\[5\] Paul, D., "A CSR-NL Interface Specification,Version 1.5," Proc.
Speech and Natural LanguageWorkshop, Oct. 1989, pp.
203-214.\[6\] Jelinek, F. "A Fast Sequential Decoding AlgorithmUsing a Stack," IBM J. Res.
Develop., Vol.
13,pp.
675-685, Nov. 1969.\[7\] Jelinek, F., Bahl, L. R., Mercer, R. L., "Design of aLinguistic Statistical Decoder for the Recognition ofContinuous Speech," IEEE Trans.
on InformationTheory, Vol.
rl'-21, No.
3, pp.
250-256, May 1975.\[8\] Sturtevant, D., "A Stack Decoder for ContinuousSpeech Recognition," Proc.
Speech and NaturalLanguage Workshop, Oct. 1989, pp.
193-198.\[9\] Soong, F. K. and Huang, E.-F., "A Fast Tree-TrellisSearch for Finding the N-Best Sentence Hypotheses inContinuous Speech Recognition," J. Acoust.
Soc.
AM.S-I, Vol.
87, pp.
105-106, May 1990.\[10\] Nilsson, N., Problem-Solving Methods in ArtificialIntelligence, NY, NY, McGraw Hill, 1971.\[11\] Rabiner, L. R., Wilpon, J. G., Soong, F. K., "HighPerformance Connected Digit Recognition Using HiddenMarkov Models," ~.1~1~ Trans.
on AcousL Speech andSig.
Proc., Vol.
37, No.
8, pp.
1214-1225, Aug. 1989.\[12\] Bahl, L., Brown, P. DeSousa, P., and Mercer, R., "ANew Algorithm for the Estimation of Hidden MarkovModel Parameters," Proc.
ICASSP-88, ppA93-496, Apr.1988.\[13\] Doddington, G., "Phonetically Sensitive Discriminantsfro Improved Speech Recognition," Proc.
ICASSP-89,pp.
556-559, May 1989.\[14\] Huang.
E.-F. and Soong, F. K., "A ProbabilisticAcoustic Map Based Discriminative H/VIM Training,"Proc.
ICASSP-90, pp.693-696, Apr.
1990.\[15\] Chow, Y.-L., "Maximum Mutual InformationEstimation of H/vIM Parameters for Continuous SpeechRecognition Using the N-Best Algorithm," Proc.ICASSP-90, pp.701-704, Apr.
1900.\[16\] Lee, K.-F., Large Vocabulary Speaker-IndependentContinuous Speech Recognition: The Sphinx System,Ph.D Dissertation, Computer Science Department,Carnegie-Mellon Univ., Apr.
1988.19
