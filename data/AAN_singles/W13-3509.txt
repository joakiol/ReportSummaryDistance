Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75?83,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsLearning Adaptable Patterns for Passage RerankingAliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2(1)DISI, University of Trento, 38123 Povo (TN), Italy{severyn,m.nicosia,moschitti}@disi.unitn.it(2)QCRI, Qatar Foundation, 5825 Doha, Qataramoschitti@qf.org.qaAbstractThis paper proposes passage rerankingmodels that (i) do not require manual fea-ture engineering and (ii) greatly preserveaccuracy, when changing application do-main.
Their main characteristic is theuse of relational semantic structures rep-resenting questions and their answer pas-sages.
The relations are established us-ing information from automatic classifiers,i.e., question category (QC) and focusclassifiers (FC) and Named Entity Recog-nizers (NER).
This way (i) effective struc-tural relational patterns can be automati-cally learned with kernel machines; and(ii) structures are more invariant w.r.t.
dif-ferent domains, thus fostering adaptability.1 IntroductionA critical issue for implementing Question An-swering (QA) systems is the need of designinganswer search and extraction modules specific tothe target application domain.
These modules en-code handcrafted rules based on syntactic patternsthat detect the relations between a question and itscandidate answers in text fragments.
Such rulesare triggered when patterns in the question and thepassage are found.
For example, given a ques-tion1:What is Mark Twain?s real name?and a relevant passage, e.g., retrieved by a searchengine:Samuel Langhorne Clemens, betterknown as Mark Twain.the QA engineers typically apply a syntactic parserto obtain the parse trees of the above two sen-tences, from which, they extract rules like:1We use this question/answer pair from TREC QA as arunning example in the rest of the paper.if the pattern ?What is NP2?s ADJname?
is in the question and the pat-tern ?NP1 better known as NP2?is in the answer passage then associatethe passage with a high score2.Machine learning has made easier the task ofQA engineering by enabling the automatic learn-ing of answer extraction modules.
However, newfeatures and training data have to be typically de-veloped when porting a QA system from a domainto another.
This is even more critical consideringthat effective features tend to be as much complexand similar as traditional handcrafted rules.To reduce the burden of manual feature engi-neering for QA, we proposed structural modelsbased on kernel methods, (Moschitti et al 2007;Moschitti and Quarteroni, 2008; Moschitti, 2008)with passages limited to one sentence.
Their mainidea is to: (i) generate question and passage pairs,where the text passages are retrieved by a searchengine; (ii) assuming those containing the correctanswer as positive instance pairs and all the oth-ers as negative ones; (iii) represent such pairs withtwo syntactic trees; and (ii) learn to rank answerpassages by means of structural kernels applied totwo trees.
This enables the automatic engineeringof structural/lexical semantic patterns.More recently, we showed that such models canbe learned for passages constituted by multiplesentences on very large-scale (Severyn and Mos-chitti, 2012).
For this purpose, we designed a shal-low syntactic representation of entire paragraphsby also improving the pair representation using re-lational tags.In this paper, we firstly use our model in (Sev-eryn and Moschitti, 2012) as the current baselineand compare it with more advanced structures de-rived from dependency trees.2If the point-wise answer is needed rather than the entirepassage, the rule could end with: returns NP175Search EngineKernel-basedrerankerRerankedanswersCandidateanswersQueryEvaluationUIMA pipelineNLPannotatorsFocus andQuestionclassifierssyntactic/semanticgraphq/a similarityfeaturestrain/testdataFigure 1: Kernel-based Answer Passage Reranking systemSecondly, we enrich the semantic representa-tion of QA pairs with the categorical informa-tion provided by automatic classifiers, i.e., ques-tion category (QC) and focus classifiers (FC) andNamed Entity Recognizers (NER).
FC determinesthe constituent of the question to be linked to thenamed entities (NEs) of the answer passage.
Thetarget NEs are selected based on their compatibil-ity with the category of the question, e.g., an NEof type PERSON is compatible with a category ofa question asking for a human (HUM).Thirdly, we tested our models in a cross-domainsetting since we believe that: (i) the enriched rep-resentation is supposed to increase the capabilityof learning effective structural relational patternsthrough kernel machines; and (ii) such structuralfeatures are more invariant with respect to differ-ent domains, fostering their adaptability.Finally, the results show that our methodsgreatly improve on IR baseline, e.g., BM25, by40%, and on previous reranking models, up to10%.
In particular, differently from our previouswork such models can effectively use NERs andthe output of different automatic modules.The rest of the paper is organized as follows,Sec.
2 describes our kernel-based reranker, Sec.
3illustrates our question/answer relational struc-tures; Sec.
5 briefly describes the feature vectors,and finally Sec.
6 reports the experimental resultson TREC and Answerbag data.2 Learning to rank with kernels2.1 QA systemOur QA system is based on a rather simple rerank-ing framework as displayed in Figure 1: given aquery question a search engine retrieves a list ofcandidate passages ranked by their relevancy.
Var-ious NLP components embedded in the pipeline asUIMA3 annotators are then used to analyze eachquestion together with its candidate answers, e.g.,part-of-speech tagging, chunking, named entityrecognition, constituency and dependency pars-ing, etc.
These annotations are then used toproduce structural models (described in Sec.
3),which are further used by a question focus detectorand question type classifiers to establish relationallinks for a given question/answer pair.
The result-ing tree pairs are then used to train a kernel-basedreranker, which outputs the model to refine the ini-tial ordering of the retrieved answer passages.2.2 Tree kernelsWe use tree structures as our base representationsince they provide sufficient flexibility in repre-sentation and allow for easier feature extractionthan, for example, graph structures.
We rely onthe Partial Tree Kernel (PTK) (Moschitti, 2006) tohandle feature engineering over the structural rep-resentations.
The choice of PTK is motivated byits ability to generate rich feature spaces over bothconstituency and dependency parse trees.
It gen-eralizes a subset tree kernel (STK) (Collins andDuffy, 2002) that maps a tree into the space ofall possible tree fragments constrained by the rulethat the sibling nodes from their parents cannot beseparated.
Different from STK where the nodesin the generated tree fragments are constrained toinclude none or all of their direct children, PTKfragments can contain any subset of the features,i.e., PTK allows for breaking the production rules.Consequently, PTK generalizes STK, thus gener-ating an extremely rich feature space, which re-sults in higher generalization ability.2.3 Preference reranking with kernelsTo enable the use of kernels for learning torank with SVMs, we use preference reranking(Joachims, 2002), which reduces the task to bi-nary classification.
More specifically, the problemof learning to pick the correct candidate hi froma candidate set {h1, .
.
.
, hk} is reduced to a bi-nary classification problem by creating pairs: pos-itive training instances ?h1, h2?, .
.
.
, ?h1, hk?
andnegative instances ?h2, h1?, .
.
.
, ?hk, h1?.
This setcan then be used to train a binary classifier.
Atclassification time the standard one-versus-all bi-narization method is applied to form all possible3http://uima.apache.org/76pairs of hypotheses.
These are ranked accordingto the number of classifier votes they receive: apositive classification of ?hk, hi?
gives a vote tohk whereas a negative one votes for hi.A vectorial representation of such pairs is thedifference between the vectors representing thehypotheses in a pair.
However, this assumes thatfeatures are explicit and already available whereaswe aim at automatically generating implicit pat-terns with kernel methods.
Thus, for keeping im-plicit the difference between such vectors we usethe following preference kernel:PK(?h1, h2?, ?h?1, h?2?)
= K(h1, h?1)+K(h2, h?2)?K(h1, h?2)?K(h2, h?1),(1)where hi and h?i refer to two sets of hypothe-ses associated with two rankings and K is a ker-nel applied to pairs of hypotheses.
We representthe latter as pairs of question and answer passagetrees.
More formally, given two hypotheses, hi =?hi(q), hi(a)?
and hi = ?h?i(q), h?i(a)?, whosemembers are the question and answer passagetrees, we define K(hi, h?i) as TK(hi(q), h?i(q)) +TK(hi(a), h?i(a)), where TK can be any tree ker-nel function, e.g., STK or PTK.To enable traditional feature vectors it is enoughto add the product (~xh1 ?
~xh2) ?
(~xh?1 ?
~xh?2) tothe structural kernel PK , where ~xh is the featurevector associated with the hypothesis h.We opted for a simple kernel sum over a prod-uct, since the latter rarely works in practice.
Al-though in (Moschitti, 2004) the kernel product hasbeen shown to provide some improvement whenapplied to tree kernels over a subcategorizationframe structure, in general, it seems to work wellonly when the tree structures are small and derivedrather accurately (Giordani and Moschitti, 2009;Giordani and Moschitti, 2012).3 Structural models of Q/A pairsFirst, we briefly describe a shallow tree represen-tation that we use as our baseline model and thenpropose a new dependency-based representation.3.1 Shallow tree structuresIn a shallow syntactic representation first exploredfor QA in (Severyn and Moschitti, 2012) eachquestion and its candidate answer are encoded intoa tree where part-of-speech tags are found at thepre-terminal level and word lemmas at the leaflevel.
To encode structural relationships for agiven q/a pair a special REL tag is used to linkthe related structures.
The authors adopt a sim-ple strategy to establish such links: lemmas sharedbetween a question and and answer get their par-ents (POS tags) and grandparents (chunk labels)marked by a REL tag.3.2 Dependency-based structuresGiven the ability of PTK to generate a rich setof structural features from a relatively flat shal-low tree representation, we propose to use depen-dency relations between words to derive an al-ternative structural model.
In particular, we usea variation of the dependency tree, where depen-dency relations are altered in such a way that thewords are always at the leaf level.
This reorder-ing of the nodes in the dependency tree, s.t.
wordsdo not form long chains, which is typical in thestandard dependency tree representation, is essen-tial for PTK to extract meaningful fragments.
Wealso add part-of-speech tags between the wordsand the nodes encoding their grammatical roles(provided by the original dependency parse tree).Again a special REL tag is used in the same man-ner as in the shallow representation to establishstructural links between a question and an answer.Fig.
2 (top) gives an example of a dependency-based structure for our example q/a pair.4 Relational LinkingThe use of a special tag to mark the related frag-ments in the question and answer tree represen-tations has been shown to yield more accuraterelational models (Severyn and Moschitti, 2012).However, previous approach was based on a na?
?vehard matching between word lemmas.Below we propose a novel strategy to estab-lish relational links using named entities extractedfrom the answer along with question focus andcategory classifiers.
In particular, we use a ques-tion category to link the focus word of a questionwith the named entities extracted from the candi-date answer.
For this purpose, we first introduceour tree kernel-based models for building a ques-tion focus and category classifiers.4.1 Question focus detectionThe question focus is typically a simple noun rep-resenting the entity or property being sought bythe question (Prager, 2006).
It can be used tosearch for semantically compatible candidate an-77NER: Person NER: PersonfocusFigure 2: Dependency-based structure DEP (top) for the q/a pair.
Q: What is Mark Twain?s real name?
A: Samuel LanghorneClemens, better known as Mark Twain.
Arrows indicate the tree fragments in the question and its answer passage linked by therelational REL tag.
Shallow tree structure CH (bottom) with a typed relation tag REL-FOCUS-HUM to link a question focusword name with the named entities of type Person corresponding to the question category (HUM).swers in document passages, thus greatly reduc-ing the search space (Pinchak, 2006).
While sev-eral machine learning approaches based on man-ual features and syntactic structures have beenrecently explored, e.g.
(Quarteroni et al 2012;Damljanovic et al 2010; Bunescu and Huang,2010), we opt for the latter approach where treekernels handle automatic feature engineering.In particular, to detect the question focus wordwe train a binary SVM classifier with tree ker-nels applied to the constituency tree representa-tion.
For each given question we generate a setof candidate trees where the parent (node with thePOS tag) of each candidate focus word is anno-tated with a special FOCUS tag.
Trees with thecorrectly tagged focus word constitute a positiveexample, while the others are negative examples.To detect the focus for an unseen question we clas-sify the trees obtained after tagging each candidatefocus word.
The tree yielding the highest classifi-cation score reveals the target focus word.4.2 Question classificationQuestion classification is the task of assigning aquestion to one of the pre-specified categories.
Weuse the coarse-grain classes described in (Li andRoth, 2002): six non-overlapping classes: Abbre-viations (ABBR), Descriptions (DESC, e.g.
def-initions or explanations), Entity (ENTY, e.g.
an-imal, body or color), Human (HUM, e.g.
groupor individual), Location (LOC, e.g.
cities or coun-tries) and Numeric (NUM, e.g.
amounts or dates).These categories can be used to determine the Ex-pected Answer Type for a given question and findthe appropriate entities found in the candidate an-swers.
Imposing such constraints on the potentialanswer keys greatly reduces the search space.Previous work in Question Classification re-veals the power of syntactic/semantic tree repre-sentations coupled with tree kernels to train thestate-of-the-art models (Bloehdorn and Moschitti,2007).
Hence, we opt for an SVM multi-classifierusing tree kernels to automatically extract thequestion class.
To build a multi-class classifierwe train a binary SVM for each of the classes andapply a one-vs-all strategy to obtain the predictedclass.
We use constituency trees as our input rep-resentation.4.3 Linking focus word with named entitiesusing question classQuestion focus captures the target informationneed posed by a question, but to make this pieceof information effective, the focus word needs tobe linked to the target candidate answer.
The focusword can be lexically matched with words presentin an answer, or the match can be established us-ing semantic information.
Clearly, the latter ap-proach is more appealing since it helps to allevi-ate the lexical gap problem which makes the na?ivestring matching of words between a question andits answer less reliable.Hence, we propose to exploit a question cate-gory (automatically identified by a question typeclassifier) along with named entities found in theanswer to establish relational links between thetree structures of a given q/a pair.
In particu-lar, once the question focus and question category78Table 1: Question classes ?
named entity types.Question Category Named Entity typesHUM PersonLOC LocationNUM Date, Time, Money, PercentageENTY Organization, Personare determined, we link the focus word wfocus inthe question, with all the named entities whosetype matches the question class.
Table 1 providesthe correspondence between question classes andnamed entity types.
We perform tagging at thechunk level and use two types of relational tags:plain REL-FOCUS and a tag typed with a ques-tion class, e.g., REL-FOCUS-HUM.
Fig.
2 (bot-tom) shows an example q/a pair where the typedrelational tag is used in the shallow syntactic treerepresentation to link the chunk containing thequestion focus name with the named entities of thecorresponding type Person (according to the map-ping defined in Table 1), i.e.
samuel langhorneclemens and mark twain.5 Feature vector representationWhile the primary focus of our study is on thestructural representations and relations betweenq/a pairs we also include basic features widelyused in QA:Term-overlap features.
A cosine similarity be-tween a question and an answer: simCOS(q, a),where the input vectors are composed of: (i) n-grams (up to tri-grams) of word lemmas and part-of-speech tags, and (ii) dependency triplets.
Forthe latter, we simply hash the string value of thepredicate defining the triple together with its argu-ment, e.g.
poss(name, twain).PTK score.
For the structural representations wealso define a similarity based on the PTK score:simPTK(q, a) = PTK(q, a), where the inputtrees can be both dependency trees and shallowchunk trees.
Note that this similarity is computedbetween the members of a q/a pair, thus, it is verydifferent from the one defined in Eq.
1.NER relatedness represents a match between aquestion category and the related named entitytypes extracted from the candidate answer.
Itcounts the proportion of named entities in the an-swer that correspond to the question type returnedby the question classifier.In our study feature vectors serve a complemen-tary purpose, while the main focus is to study thevirtue of structural representations for reranking.The effect of a more extensive number of pairwisesimilarity features in QA has been studied else-where, e.g., (Surdeanu et al 2008).6 ExperimentsWe report the results on two QA collections: fac-toid open-domain QA corpus from TREC and acommunity QA corpus Answerbag.
Since we fo-cus on passage reranking we do not carry out an-swer extraction.
The goal is to rank the passagecontaining the right answer in the top position.6.1 CorporaTREC QA.
In the TREC QA tasks, answer pas-sages containing correct information nuggets, i.e.answer keys, have to be extracted from a given textcorpus, typically a large corpus from newswire.In our experiments, we opted for questions from2002 and 2003 years, which totals to 824 ques-tions.
AQUAINT newswire corpus4 is used forsearching the supporting answers.Answerbag is a community-driven QA collectionthat contains a large portion of questions that have?professionally researched?
answers.
Such an-swers are provided by the website moderators andallow for training high quality models.
From theoriginal corpus containing 180k question/answerpairs, we use 1k randomly sampled questions fortesting and 10k for training.Question Focus.
We use 3 datasets for train-ing and evaluating the performance of our fo-cus detector: SeCo-600 (Quarteroni et al 2012),Mooney GeoQuery (Damljanovic et al 2010) andthe dataset from (Bunescu and Huang, 2010).
TheSeCo dataset contains 600 questions from whichwe discarded a subset of multi-focus questionsand non-interrogative queries.
The Mooney Geo-Query contains 250 question targeted at geograph-ical information in the U.S.
The first two datasetsare very domain specific, so we also carried outexperiments with the dataset from (Bunescu andHuang, 2010), which contains the first 2000 ques-tions from the answer type dataset from Li andRoth annotated with focus words.
We removedquestions with implicit and multiple focuses.Question Classification.
We used the UIUICdataset (Li and Roth, 2002)5 which contains 59524http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/5although the QC dataset from (Li and Roth, 2002) in-cludes additional 50 fine grain classes we opted for using only6 coarse classes that are sufficient to capture the coarse se-mantic answer type of the candidate answer.
This choice alsoresults in a more accurate multi-class classifier.79factoid questions from different sources (USC,TREC 8, TREC 9, TREC 10).
For training theclassifiers we excluded questions from TREC 8 toensure there is no overlap with the data used fortesting models trained on TREC QA.6.2 Models and MetricsOur models are built applying a kernel-basedreranker to the output of a search engine.6.2.1 BM25We use Terrier6 search engine, which providesBM25 scoring model for indexing and retrieval.For the TREC QA 2002 and 2003 task we indexAQUAINT corpus treating paragraphs as docu-ments.
The resulting index contains about 12 mil-lion items.
For the Answerbag we index the entirecollection of 180k answers.
We retrieve a list oftop 50 candidate answers for each question.6.2.2 Reranking modelsTo train our reranking models we used SVM-light-TK7, which encodes structural kernels in SVM-light (Joachims, 2002) solver.
In particular, weuse PTK on the relational tree structures combinedwith the polynomial kernel of degree 3 applied tothe feature vectors.
Therefore, different represen-tations lead to different models described below.CH - our basic shallow chunk tree (Severyn andMoschitti, 2012) used as a baseline structuralreranking model.DEP - dependency tree augmented with POS tagsand reorganized relations suitable for PTK.V - reranker model using similarity features de-fined in Sec.
5.DEP+V, CH+V - a combination of tree structuresand similarity feature vectors.+FC+QC - relational linking of the question focusword and named entities of the corresponding typeusing Focus and Question classifiers.+TFC+QC - a typed relational link refined a ques-tion category.86.2.3 MetricsWe report the following metrics, most commonlyused in QA: Precision at rank 1 (P@1), i.e.,6http://terrier.org/7http://disi.unitn.it/moschitti/Tree-Kernel.htm8?
is used for showing the results of DEP, DEP+V andCH+V structural representations that are significantly betterthan the baseline model CH, while ?
indicates improvementof +QC+FC and +QC+TFC tagging applied to basic struc-tural representations, e.g.
CH+V and DEP+V.Table 2: Structural representations on TREC QA.MODELS MAP MRR P@1BM25 0.22 28.02 18.17V 0.22 28.40 18.54STRUCTURAL REPRESENTATIONSCH (S&M, 2012) 0.28 35.63 24.88CH+V 0.30?
37.45?
27.91?DEP 0.30?
37.87?
28.05?DEP+V 0.30?
37.64?
28.05?REFINED RELATIONAL TAGCH+V+QC+FC 0.32?
39.48?
29.63?CH+V+QC+TFC 0.32?
39.49?
30.00?DEP+V+QC+FC 0.31?
37.49 28.56DEP+V+QC+TFC 0.31?
38.05?
28.93?the percentage of questions with a correct an-swer at rank 1, Mean Reciprocal Rank (MRR),and Mean Average Precision (MAP).
The reportedmetrics are averages after performing a 5-foldcross-validation.
We used a paired t-test at 95%confidence to compare the performance of ourmodels to a baseline.6.3 Passage Reranking ResultsWe first evaluate the impact of two different syn-tactic representations using shallow and depen-dency trees.
Then, we evaluate the accuracy boostwhen such structures are enriched with automati-cally derived tags, e.g., question focus and ques-tion category and NEs found in the answer pas-sage.6.3.1 Structural representationsTable 2 reveals that using V model results in asmall improvement over BM25 baseline.
Indeed,similarity scores that are most often based onword-overlap measures even when computed overvarious q/a representations are fairly redundant tothe search engine similarity score.
Instead, usingthe structural representations, CH and DEP, givesa bigger boost in the performance.
Interestingly,having more features in the CH+V model resultsin further improvement while DEP+V seems to re-main insensitive to additional features provided bythe V model.6.3.2 Semantically Enriched StructuresIn the following set of experiments we explore an-other strategy for linking structures for a givenq/a pair.
We automatically detect the questionfocus word and link it to the related named en-tities in the answer, selected accordingly to thequestion category identified by the question clas-sifier (QC+FC).
Further refining the relational link80Table 3: Accuracy (%) of focus classifiers.DATASET ST STK STK+BOW PTKMOONEY 73.0 81.9 81.5 80.5SECO-600 90.0 94.5 94.5 90.0BUNESCU 89.7 98.3 98.2 96.9Table 4: Accuracy (%) of question classifiers.DATASET STK+BOW PTKLI & ROTH 86.1 82.2TREC TEST 79.3 78.1with the question category yields QC+TFC model.First, we report the results of training our questionfocus detector and question category classifier.Focus classifier results.
Table 3 displays the ac-curacies obtained by the question focus detectoron 3 datasets using different kernels: the ST (sub-tree kernel where fragments contain full subtreesincluding leaves), STK, STK+bow (bag-of-wordsfeature vector is added) and PTK.
As we can see,using STK model yields the best accuracy and weuse it in our pipeline to automatically detect thefocus.Question classifier results.
Table 4 contains theaccuracies of the question classifier on the UIUICdataset and the TREC questions that we also usefor testing our reranker models.
STK+bow per-forms better than PTK, since here the input rep-resentation is a plain constituency tree, for whichSTK is particularly suited.
Hence, we use thismodel to predict the question category.Ranking results.
Table 2 (bottom) summarizesthe performance of the CH+V and DEP+V modelswhen coupled with QC+FC and QC+TFC strate-gies to establish the links between the structuresin a given q/a pair.
CH structural representationwith QC+FC yields an interesting improvement,while further refining the relational tag by addinga question category (QC+TFC) gives slightly bet-ter results.Integrating the refined relational tag into theDEP based structures results more problematic,since the dependency tree is less suitable for repre-senting multi-word expressions, named entities inour case.
Hence, using the relational tag to markthe nodes spanning such multi-word entities in thedependency structure may result in less meaning-ful features than in CH model, where words in aphrase are naturally grouped under a chunk node.A more detailed discussion on the merits of eachmodel is provided in the Sec.
6.5.Table 5: Cross-domain experiment: training on Answerbagand testing on TREC QA.MODELS MAP MRR P@1BM25 0.22 27.91 18.08V 0.23 28.86 18.90BASIC STRUCTURAL REPRESENTATIONSCH (S&M, 2012) 0.24 30.25 20.42CH+V 0.25?
31.31?
21.28?DEP+V 0.26?
33.26?
22.21?REFINED RELATIONAL TAGCH+V+QC+TFC 0.27?
33.53?
22.81?DEP+V+QC+TFC 0.29?
34.25?
23.45?6.4 Learning cross-domain pairwisestructural relationshipsTo test the robustness of the syntactic patterns au-tomatically learned by our structural models, weconduct a cross-domain experiment, i.e.
we traina model on Answerbag data and test it on TREC.
Itshould be noted that unlike TREC data, where theanswers are simply passages containing the cor-rect answer phrase, answers in Answerbag specif-ically address a given question and are generatedby humans.
Additionally, TREC QA contains onlyfactoid questions, while Answerbag is a commu-nity QA corpus with a large portion of non-factoidquestions.
Interestingly, the results demonstratethe robustness of our syntactic relational modelwhich captures patterns shared across different do-mains, e.g.
TREC and Answerbag data.Table 5 shows that: (i) models based on depen-dency structures result in a better generalizationability extracting more robust syntactic patterns;and (ii) the strategy to link the question focus withthe related named entities in the answer providesan interesting improvement over the basic struc-tural representations.6.5 Error AnalysisConsider our running example q/a pair fromSec.
1.
As the first candidate answer, thesearch engine retrieves the following incorrectpassage: ?The autobiography of Mark Twain?,Mark Twain.
It is relatively short and mentions thekeywords {Mark, Twain} twice, which apparentlyresults in a high score for the BM25 model.
In-stead, the search engine ranks the correct answer atposition 34.
After reranking using the basic CH+Vmodel the correct answer is promoted by 20 posi-tions.
While using the CH+V+QC+FC model thecorrect answer advances to position 6.
Below, weprovide the intuition behind the merits of QC+FCand QC+TFC encoding question focus and ques-81tion category into the basic models.The model learned by the reranker represents acollection of q/a pairs from the training set (sup-port vectors) which are matched against each can-didate q/a pair.
We isolated the following pairfrom the model that has a high structural similaritywith our running example:Q: What is Barbie?s full name?A: The toy is called after Barbie MillicentRoberts from Willows.Despite differences in the surface forms ofthe words, PTK extracts matching patterns,e.g.
[S NP [VP VBN] [PP IN] REL-NP],which yields a high similarity score boosting therank of the correct candidate.
However, wenote that at the same time an incorrect candi-date answer, e.g.
Mark Twain was accused ofracist language., exhibits similar patterns and alsogets a high rank.
The basic structural repre-sentation is not able to encode essential differ-ences from the correct answer candidate.
Thisposes a certain limitation on the discriminativepower of CH and DEP representations.
Intro-ducing a focus tag changes the structural repre-sentation of both q/a pairs, s.t.
the correct q/apair preserves the pattern (after identifying wordname as focus and question category as HUM,it is transformed to [S REL-FOCUS-NP [VPVBN] [PP IN] REL-FOCUS-NP]), while itis absent in the incorrect candidate.
Thus, linkingthe focus word with the related NEs in the answerhelps to discriminate between structurally similaryet semantically different candidates.Another step towards a more fine-grained struc-tural representation is to specialize the relationalfocus tag (QC+TFC model).
We propose to aug-ment the focus tag with the question category toavoid matches with other structurally similar butsemantically different candidates.
For example, aq/a pair found in the list of support vectors:Q: What is Mark Twain?s place of birth?A: Mark Twain was raised in Hannibal Missouri.would exhibit high structural similarity even whenrelational focus is used (since the relational tagdoes not incorporate the question class LOC), butrefining the focus tag with the question class elim-inates such cases.7 Related WorkPrevious studies similar to ours carry out pas-sage reranking by exploiting structural informa-tion, e.g.
using subject-verb-object relations (At-tardi et al 2001; Katz and Lin, 2003).
Un-fortunately, the large variability of natural lan-guage makes such triples rather sparse thus dif-ferent methods explore soft matching (i.e., lexicalsimilarity) based on answer types and named en-tity types (Aktolga et al 2011).
Passage rerankingusing classifiers of question and answer pairs wereproposed in (Radlinski and Joachims, 2006; Jeonet al 2005).Regarding kernel methods, our work in (Mos-chitti et al 2007; Severyn and Moschitti, 2012)was the first to exploit tree kernels for modelinganswer reranking.
However, such method lacksthe use of important relational information be-tween a question and a candidate answer, whichis essential to learn accurate relational patterns.
Incontrast, this paper relies on shallow and depen-dency trees encoding the output of question andfocus classifiers to connect focus word and NEs ofthe answer passage.
This provides more effectiverelational information, which allows our model tosignificantly improve on previous rerankers.8 ConclusionsThis paper shows a viable research direction inthe automatic QA engineering.
One of its maincharacteristics is the use of structural kernel tech-nology to induce features from structural seman-tic representations of question and answer pas-sage pairs.
The same technology is also used toconstruct question and focus classifiers, which areused to derive relational structures.An interesting result of this paper is that to de-sign an answer passage reranker for a new do-main, we can use off-the-shelf syntactic parsersand NERs along with little training data for theQC and FC classifiers.
This is due to the factthat: (i) the kernel technology is able to automat-ically extract effective structural patterns; and (ii)the extracted patterns are rather robust, e.g., mod-els learned on Answerbag improve accuracy onTREC test data.AcknowledgementsThis research is partially supported by the EU?s 7thFramework Program (FP7/2007-2013) (#288024LIMOSINE project) and an Open CollaborativeResearch (OCR) award from IBM Research.82ReferencesElif Aktolga, James Allan, and David A. Smith.
2011.Passage reranking for question answering using syn-tactic structures and answer types.
In ECIR.Giuseppe Attardi, Antonio Cisternino, FrancescoFormica, Maria Simi, and Ro Tommasi.
2001.Piqasso: Pisa question answering system.
In TREC,pages 599?607.Stephan Bloehdorn and Alessandro Moschitti.
2007.Combined syntactic and semantic kernels for textclassification.
In ECIR.Razvan Bunescu and Yunfeng Huang.
2010.
Towardsa general model of answer typing: Question focusidentification.
In CICLing.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels overDiscrete Structures, and the Voted Perceptron.
InACL.Danica Damljanovic, Milan Agatonovic, and HamishCunningham.
2010.
Identification of the questionfocus: Combining syntactic analysis and ontology-based lookup through the user interaction.
In LREC.Alessandra Giordani and Alessandro Moschitti.
2009.Syntactic structural kernels for natural language in-terfaces to databases.
In Proceedings of ECMLPKDD, ECML PKDD ?09.
Springer-Verlag.Alessandra Giordani and Alessandro Moschitti.
2012.Translating questions to sql queries with generativeparsers discriminatively reranked.
In Proceedingsof The 24rd International Conference on Computa-tional Linguistics, India.
Coling 2012.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In CIKM.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD),pages 133?142.Boris Katz and Jimmy Lin.
2003.
Selectively using re-lations to improve precision in question answering.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In COLING.Alessandro Moschitti and Silvia Quarteroni.
2008.Kernels on linguistic structures for answer extrac-tion.
In ACL.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploit-ing syntactic and shallow semantic kernels for ques-tion/answer classification.
In ACL.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow statistic parsing.
In Proceedingsof the 42nd Meeting of the Association for Compu-tational Linguistics (ACL?04), Main Volume, pages335?342, Barcelona, Spain, July.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM.Christopher Pinchak.
2006.
A probabilistic answertype model.
In In EACL.John M. Prager.
2006.
Open-domain question-answering.
Foundations and Trends in InformationRetrieval, 1(2):91?231.Silvia Quarteroni, Vincenzo Guerrisi, and Pietro LaTorre.
2012.
Evaluating multi-focus natural lan-guage queries over data services.
In LREC.Filip Radlinski and Thorsten Joachims.
2006.
Querychains: Learning to rank from implicit feedback.CoRR.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In SIGIR.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2008.Learning to rank answers on large online QA collec-tions.
In Proceedings of ACL-HLT.83
