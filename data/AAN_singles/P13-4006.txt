Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 31?36,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDISSECT - DIStributional SEmantics Composition ToolkitGeorgiana Dinu and Nghia The Pham and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, Italy)(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.itAbstractWe introduce DISSECT, a toolkit tobuild and explore computational modelsof word, phrase and sentence meaningbased on the principles of distributionalsemantics.
The toolkit focuses in partic-ular on compositional meaning, and im-plements a number of composition meth-ods that have been proposed in the litera-ture.
Furthermore, DISSECT can be use-ful to researchers and practitioners whoneed models of word meaning (withoutcomposition) as well, as it supports var-ious methods to construct distributionalsemantic spaces, assessing similarity andeven evaluating against benchmarks, thatare independent of the composition infras-tructure.1 IntroductionDistributional methods for meaning similarity arebased on the observation that similar words oc-cur in similar contexts and measure similaritybased on patterns of word occurrence in large cor-pora (Clark, 2012; Erk, 2012; Turney and Pan-tel, 2010).
More precisely, they represent words,or any other target linguistic elements, as high-dimensional vectors, where the dimensions repre-sent context features.
Semantic relatedness is as-sessed by comparing vectors, leading, for exam-ple, to determine that car and vehicle are very sim-ilar in meaning, since they have similar contextualdistributions.
Despite the appeal of these meth-ods, modeling words in isolation has limited ap-plications and ideally we want to model semanticsbeyond word level by representing the meaning ofphrases or sentences.
These combinations are in-finite and compositional methods are called for toderive the meaning of a larger construction fromthe meaning of its parts.
For this reason, the ques-tion of compositionality within the distributionalparadigm has received a lot of attention in recentyears and a number of compositional frameworkshave been proposed in the distributional seman-tic literature, see, e.g., Coecke et al(2010) andMitchell and Lapata (2010).
For example, in suchframeworks, the distributional representations ofred and car may be combined, through various op-erations, in order to obtain a vector for red car.The DISSECT toolkit (http://clic.cimec.unitn.it/composes/toolkit)is, to the best of our knowledge, the first toprovide an easy-to-use implementation of manycompositional methods proposed in the literature.As such, we hope that it will foster further workon compositional distributional semantics, as wellas making the relevant techniques easily availableto those interested in their many potential applica-tions, e.g., to context-based polysemy resolution,recognizing textual entailment or paraphrasedetection.
Moreover, the DISSECT tools toconstruct distributional semantic spaces fromraw co-occurrence counts, to measure similarityand to evaluate these spaces might also be ofuse to researchers who are not interested in thecompositional framework.
DISSECT is freelyavailable under the GNU General Public License.2 Building and composing distributionalsemantic representationsThe pipeline from corpora to compositional mod-els of meaning can be roughly summarized as con-sisting of three stages:11.
Extraction of co-occurrence counts from cor-pora In this stage, an input corpus is used to ex-tract counts of target elements co-occurring withsome contextual features.
The target elementscan vary from words (for lexical similarity), topairs of words (e.g., for relation categorization),1See Turney and Pantel (2010) for a technical overview ofdistributional methods for semantics.31to paths in syntactic trees (for unsupervised para-phrasing).
Context features can also vary fromshallow window-based collocates to syntactic de-pendencies.2.
Transformation of the raw counts Thisstage may involve the application of weightingschemes such as Pointwise Mutual Information,feature selection, dimensionality reduction meth-ods such as Singular Value Decomposition, etc.The goal is to eliminate the biases that typicallyaffect raw counts and to produce vectors whichbetter approximate similarity in meaning.3.
Application of composition functionsOnce meaningful representations have beenconstructed for the atomic target elements ofinterest (typically, words), various methods, suchas vector addition or multiplication, can be usedfor combining them to derive context-sensitiverepresentations or for constructing representationsfor larger phrases or even entire sentences.DISSECT can be used for the second andthird stages of this pipeline, as well as to measuresimilarity among the resulting word or phrase vec-tors.
The first step is highly language-, task- andcorpus-annotation-dependent.
We do not attemptto implement all the corpus pre-processing andco-occurrence extraction routines that it wouldrequire to be of general use, and expect instead asinput a matrix of raw target-context co-occurrencecounts.2 DISSECT provides various methods tore-weight the counts with association measures,dimensionality reduction methods as well as thecomposition functions proposed by Mitchell andLapata (2010) (Additive, Multiplicative and Dila-tion), Baroni and Zamparelli (2010)/Coecke et al(2010) (Lexfunc) and Guevara (2010)/Zanzotto etal.
(2010) (Fulladd).
In DISSECT we define andimplement these in a unified framework and in acomputationally efficient manner.
The focus ofDISSECT is to provide an intuitive interface forresearchers and to allow easy extension by addingother composition methods.3 DISSECT overviewDISSECT is written in Python.
We provide manystandard functionalities through a set of power-2These counts can be read from a text file containing twostrings (the target and context items) and a number (the corre-sponding count) on each line (e.g., maggot food 15) orfrom a matrix in format word freq1 freq2 ...#create a semantic space from counts in#dense format("dm"): word freq1 freq2 ..ss = Space.build(data="counts.txt",format="dm")#apply transformationsss = ss.apply(PpmiWeighting())ss = ss.apply(Svd(300))#retrieve the vector of a target elementprint ss.get_row("car")Figure 1: Creating a semantic space.ful command-line tools, however users with ba-sic Python familiarity are encouraged to use thePython interface that DISSECT provides.
Thissection focuses on this interface (see the onlinedocumentation on how to perform the same oper-ations with the command-line tools), that consistsof the following top-level packages:#DISSECT packagescomposes.matrixcomposes.semantic_spacecomposes.transformationcomposes.similaritycomposes.compositioncomposes.utilsSemantic spaces and transforma-tions The concept of a semantic space(composes.semantic space) is at thecore of the DISSECT toolkit.
A semanticspace consists of co-occurrence values, storedas a matrix, together with strings associated tothe rows of this matrix (by design, the targetlinguistic elements) and a (potentially empty)list of strings associated to the columns (thecontext features).
A number of transforma-tions (composes.transformation) canbe applied to semantic spaces.
We implementweighting schemes such as positive PointwiseMutual Information (ppmi) and Local Mu-tual Information, feature selection methods,dimensionality reduction (Singular Value De-composition (SVD) and Nonnegative MatrixFactorization (NMF)), and new methods canbe easily added.3 Going from raw counts to atransformed space is accomplished in just a fewlines of code (Figure 1).3The complete list of transformations currently sup-ported can be found at http://clic.cimec.unitn.it/composes/toolkit/spacetrans.html#spacetrans.32#load a previously saved spacess = io_utils.load("ss.pkl")#compute cosine similarityprint ss.get_sim("car", "book",CosSimilarity())#the two nearest neighbours of "car"print ss.get_neighbours("car", 2,CosSimilarity())Figure 2: Similarity queries in a semantic space.Furthermore DISSECT allows the pos-sibility of adding new data to a seman-tic space in an online manner (using thesemantic space.peripheral spacefunctionality).
This can be used as a way to effi-ciently expand a co-occurrence matrix with newrows, without re-applying the transformations tothe entire space.
In some other cases, the user maywant to represent phrases that are specializationof words already existing in the space (e.g.,slimy maggot and maggot), without distorting thecomputation of association measures by countingthe same context twice.
In this case, adding slimymaggot as a ?peripheral?
row to a semantic spacethat already contains maggot implements thedesired behaviour.Similarity queries Semantic spaces are used forthe computation of similarity scores.
DISSECTprovides a series of similarity measures such as co-sine, inverse Euclidean distance and Lin similarity,implemented in the composes.similaritypackage.
Similarity of two elements can be com-puted within one semantic space or across twospaces that have the same dimensionality.
Figure2 exemplifies (word) similarity computations withDISSECT.Composition functions Composition functionsin DISSECT (composes.composition) takeas arguments a list of element pairs to be com-posed, and one or two spaces where the elementsto be composed are represented.
They return a se-mantic space containing the distributional repre-sentations of the composed items, which can befurther transformed, used for similarity queries, orused as inputs to another round of composition,thus scaling up beyond binary composition.
Fig-ure 3 shows a Multiplicative composition exam-ple.
See Table 1 for the currently available com-position models, their definitions and parameters.Model Composition function ParametersAdd.
w1~u+ w2~v w1(= 1), w2(= 1)Mult.
~u ~v -Dilation ||~u||22~v + (??
1)?~u,~v?~u ?
(= 2)Fulladd W1~u+W2~v W1,W2 ?
Rm?mLexfunc Au~v Au ?
Rm?mTable 1: Currently implemented compositionfunctions of inputs (u, v) together with parame-ters and their default values in parenthesis, wheredefined.
Note that in Lexfunc each functor wordcorresponds to a separate matrix or tensor Au (Ba-roni and Zamparelli, 2010).Parameter estimation All composition modelsexcept Multiplicative have parameters to be esti-mated.
For simple models with few parameters,such as as Additive, the parameters can be passedby hand.
However, DISSECT supports automatedparameter estimation from training examples.
Inparticular, we extend to all composition methodsthe idea originally proposed by Baroni and Zam-parelli (2010) for Lexfunc and Guevara (2010) forFulladd, namely to use corpus-extracted examplevectors of both the input (typically, words) andoutput elements (typically, phrases) in order to op-timize the composition operation parameters.
Theproblem can be generally stated as:??
= arg min?||P ?
fcomp?
(U, V )||Fwhere U, V and P are matrices containing inputand output vectors respectively.
For example Umay contain adjective vectors such as red, blue,V noun vectors such as car, sky and P corpus-extracted vectors for the corresponding phrasesred car, blue sky.
fcomp?
is a composition func-tion and ?
stands for a list of parameters that thiscomposition function is associated with.4 We im-plement standard least-squares estimation meth-ods as well as Ridge regression with the optionfor generalized cross-validation, but other meth-ods such as partial least-squares regression can beeasily added.
Figure 4 exemplifies the Fulladdmodel.Composition output examples DISSECT pro-vides functions to evaluate (compositional) distri-butional semantic spaces against benchmarks inthe composes.utils package.
However, as amore qualitatively interesting example of what canbe done with DISSECT, Table 2 shows the nearest4Details on the extended corpus-extracted vector estima-tion method in DISSECT can be found in Dinu et al(2013).33#instantiate a multiplicative modelmult_model = Multiplicative()#use the model to compose words from input space input_spacecomp_space = mult_model.compose([("red", "book", "my_red_book"),("red", "car", "my_red_car")],input_space)#compute similarity of: 1) two composed phrases and 2) a composed phrase and a wordprint comp_space.get_sim("my_red_book", "my_red_car", CosSimilarity())print comp_space.get_sim("my_red_book", "book", CosSimilarity(), input_space)Figure 3: Creating and using Multiplicative phrase vectors.#training data for learning an adjective-noun phrase modeltrain_data = [("red","book","red_book"), ("blue","car","blue_car")]#train a fulladd modelfa_model = FullAdditive()fa_model.train(train_data, input_space, phrase_space)#use the model to compose a phrase from new words and retrieve its nearest neighb.comp_space = fa_model.compose([("yellow", "table", "my_yellow_table")], input_space)print comp_space.get_neighbours("my_yellow_table", 10, CosSimilarity())Figure 4: Estimating a Fulladd model and using it to create phrase vectors.Target Method Neighboursflorist Corpus Harrod, wholesaler, stockistflora + -istFulladd flora, fauna, ecologistLexfunc ornithologist, naturalist, botanistAdditive flora, fauna, ecosystemTable 3: Compositional models for morphol-ogy.
Top 3 neighbours of florist using its (low-frequency) corpus-extracted vector, and when thevector is obtained through composition of floraand -ist with Fulladd, Lexfunc and Additive.neighbours of false belief obtained through com-position with the Fulladd, Lexfunc and Additivemodels.
In Table 3, we exemplify a less typical ap-plication of compositional models to derivationalmorphology, namely obtaining a representation offlorist compositionally from distributional repre-sentations of flora and -ist (Lazaridou et al 2013).4 Main featuresSupport for dense and sparse representationsCo-occurrence matrices, as extracted from text,tend to be very sparse structures, especially whenusing detailed context features which include syn-tactic information, for example.
On the otherhand, dimensionality reduction operations, whichare often used in distributional models, lead tosmaller, dense structures, for which sparse rep-resentations are not optimal.
This is our motiva-tion for supporting both dense and sparse repre-sentations.
The choice of dense vs. sparse is ini-tially determined by the input format, if a spaceis created from co-occurrence counts.
By default,DISSECT switches to dense representations af-ter dimensionality reduction, however the user canfreely switch from one representation to the other,in order to optimize computations.
For this pur-pose DISSECT provides wrappers around matrixoperations, as well as around common linear alge-bra operations, in the composes.matrix pack-age.
The underlying Python functionality is pro-vided by numpy.array and scipy.sparse.Efficient computations DISSECT is optimizedfor speed since most operations are cast as matrixoperations, that are very efficiently implementedin Python?s numpy and scipy modules5.
Ta-bles 4 and 5 show running times for typical DIS-SECT operations: application of the ppmi weight-ing scheme, nearest neighbour queries and estima-tion of composition function parameters (on a 2.15For SVD on sparse structures, we use sparsesvd(https://pypi.python.org/pypi/sparsesvd/).For NMF, we adapted http://www.csie.ntu.edu.tw/?cjlin/nmf/ (Lin, 2007).34Target Method Neighboursbelief Corpus moral, dogma, worldview, religion, world-view, morality, theism, tenet, agnosticism, dogmaticfalse beliefFulladd pantheist, belief, agnosticism, religiosity, dogmatism, pantheism, theist, fatalism, deism, mind-setLexfunc self-deception, untruth, credulity, obfuscation, misapprehension, deceiver, disservice, falsehoodAdditive belief, assertion, falsity, falsehood, truth, credence, dogma, supposition, hearsay, denialTable 2: Top nearest neighbours of belief and of false belief obtained through composition with theFulladd, Lexfunc and Additive models.Method Fulladd Lexfunc Add.
DilationTime (s.) 2864 787 46 68Table 4: Composition model parameter estimationtimes (in seconds) for 1 million training points in300-dimensional space.Matrix size (nnz) Ppmi Query100Kx300 (30M) 5.8 0.5100Kx100K (250M) 52.6 9.5Table 5: Running times (in seconds) for 1) appli-cation of ppmi weighting and 2) querying for thetop neighbours of a word (cosine similarity) fordifferent matrix sizes (nnz: number of non-zeroentries, in millions).GHz machine).
The price to pay for fast computa-tions is that data must be stored in main memory.We do not think that this is a major inconvenience.For example, a typical symmetric co-occurrencematrix extracted from a corpus of several billionwords, defining context in terms of 5-word win-dows and considering the top 100K?100K mostfrequent words, contains?
250 million entries andrequires only 2GB of memory for (double preci-sion) storage.Simple design We have opted for a very simpleand intuitive design as the classes interact invery natural ways: A semantic space storesthe actual data matrix and structures to indexits rows and columns, and supports similarityqueries and transformations.
Transformationstake one semantic space as input to returnanother, transformed, space.
Composition func-tions take one or more input spaces and yielda composed-elements space, which can furtherundergo transformations and be used for similarityqueries.
In fact, DISSECT semantic spaces alsosupport higher-order tensor representations, notjust vectors.
Higher-order representations areused, for example, to represent transitive verbsand other multi-argument functors by Coeckeet al(2010) and Grefenstette et al(2013).See http://clic.cimec.unitn.it/composes/toolkit/composing.html foran example of using DISSECT for estimatingsuch tensors.Extensive documentation The DISSECTdocumentation can be found at http://clic.cimec.unitn.it/composes/toolkit.We provide a tutorial which guides the userthrough the creation of some toy semantic spaces,estimation of the parameters of compositionmodels and similarity computations in semanticspaces.
We also provide a full-scale exampleof intransitive verb-subject composition.
Weshow how to go from co-occurrence counts tocomposed representations and make the data usedin the examples available for download.Comparison to existing software In terms ofdesign choices, DISSECT most resembles theGensim toolkit (R?ehu?r?ek and Sojka, 2010).
How-ever Gensim is intended for topic modeling, andtherefore diverges considerably from DISSECT inits functionality.
The SSpace package of Jurgensand Stevens (2010) also overlaps to some degreewith DISSECT in terms of its intended use, how-ever, like Gensim, it does not support composi-tional operations that, as far as we know, are anunique feature of DISSECT.5 Future extensionsWe implemented and are currently testing DIS-SECT functions supporting other compositionmethods, including the one proposed by Socheret al(2012).
Adding further methods is our top-priority goal.
In particular, several distributionalmodels of word meaning in context share impor-tant similarities with composition models, and weplan to add them to DISSECT.
Dinu et al(2012)show, for example, that well-performing, simpli-fied variants of the method in Thater et al(2010),Thater et al(2011) and Erk and Pado?
(2008) canbe reduced to relatively simple matrix operations,making them particularly suitable for a DISSECTimplementation.35DISSECT is currently optimized for the compo-sition of many phrases of the same type.
This is inline with most of the current evaluations of com-positional models, which focus on specific phe-nomena, such as adjectival modification, noun-noun compounds or intransitive verbs, to name afew.
In the future we plan to provide a module forcomposing entire sentences, taking syntactic treesas input and returning composed representationsfor each node in the input trees.Finally, we intend to make use of the exist-ing Python plotting libraries to add a visualizationmodule to DISSECT.6 AcknowledgmentsWe thank Angeliki Lazaridou for helpful dis-cussions.
This research was supported by theERC 2011 Starting Independent Research Grantn.
283554 (COMPOSES).ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Stephen Clark.
2012.
Vector space models of lexicalmeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, 2nd edition.Blackwell, Malden, MA.
In press.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.Georgiana Dinu, Stefan Thater, and So?ren Laue.
2012.A comparison of models of word meaning in con-text.
In Proceedings of NAACL HLT, pages 611?615, Montreal, Canada.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
A general framework for the estimation ofdistributional composition functions.
In Proceed-ings of ACL Workshop on Continuous Vector SpaceModels and their Compositionality, Sofia, Bulgaria.In press.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP, pages 897?906, Honolulu,HI.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.
In Proceedings ofIWCS, pages 131?142, Potsdam, Germany.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.David Jurgens and Keith Stevens.
2010.
The S-Spacepackage: an open source package for word spacemodels.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 30?35, Uppsala, Sweden.Angeliki Lazaridou, Marco Marelli, Roberto Zampar-elli, and Marco Baroni.
2013.
Compositional-lyderived representations of morphologically complexwords in distributional semantics.
In Proceedings ofACL, Sofia, Bulgaria.
In press.Chih-Jen Lin.
2007.
Projected gradient methods forNonnegative Matrix Factorization.
Neural Compu-tation, 19(10):2756?2779.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Radim R?ehu?r?ek and Petr Sojka.
2010.
Software frame-work for topic modelling with large corpora.
In Pro-ceedings of the LREC 2010 Workshop on New Chal-lenges for NLP Frameworks, pages 45?50, Valletta,Malta.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of ACL, pages 948?957, Uppsala, Sweden.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and ef-fective vector model.
In Proceedings of IJCNLP,pages 1134?1143, Chiang Mai, Thailand.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.36
