Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 161?164,Prague, June 2007. c?2007 Association for Computational LinguisticsGYDER: maxent metonymy resolutionRicha?rd FarkasUniversity of SzegedDepartment of InformaticsH-6720 Szeged, A?rpa?d te?r 2.rfarkas@inf.u-szeged.huEszter SimonBudapest U. of TechnologyDept.
of Cognitive ScienceH-1111 Budapest, Stoczek u 2.esimon@cogsci.bme.huGyo?rgy SzarvasUniversity of SzegedDepartment of InformaticsH-6720 Szeged, A?rpa?d te?r 2.szarvas@inf.u-szeged.huDa?niel VargaBudapest U. of TechnologyMOKK Media ResearchH-1111 Budapest, Stoczek u 2.daniel@mokk.bme.huAbstractThough the GYDER system has achievedthe highest accuracy scores for themetonymy resolution shared task atSemEval-2007 in all six subtasks, we don?tconsider the results (72.80% accuracy fororg, 84.36% for loc) particularly impres-sive, and argue that metonymy resolutionneeds more features.1 IntroductionIn linguistics metonymy means using one term, orone specific sense of a term, to refer to another,related term or sense.
For example, in ?the penis mightier than the sword?
pen refers to writing,the force of ideas, while sword refers to militaryforce.
Named Entity Recognition (NER) is ofkey importance in numerous natural language pro-cessing applications ranging from information ex-traction to machine translation.
Metonymic usageof named entities is frequent in natural language.On the basic NER categories person, place,organisation state-of-the-art systems generallyperform in the mid to the high nineties.
These sys-tems typically do not distinguish between literal ormetonymic usage of entity names, even though thiswould be helpful for most applications.
Resolvingmetonymic usage of proper names would thereforedirectly benefit NER and indirectly all NLP tasks(such as anaphor resolution) that require NER.Markert and Nissim (2002) outlined a corpus-based approach to proper name metonymy as a se-mantic classification problem that forms the basisof the 2007 SemEval metonymy resolution task.Instances like ?He was shocked by Vietnam?
or?Schengen boosted tourism?
were assigned to broadcategories like place-for-event, sometimesignoring narrower distinctions, such as the fact thatit wasn?t the signing of the treaty at Schengen butrather its actual implementation (which didn?t takeplace at Schengen) that boosted tourism.
But thecorpus makes clear that even with these (sometimescoarse) class distinctions, several metonymy typesseem to appear extremely rarely in actual texts.The shared task focused on two broad named en-tity classes as metonymic sources, location andorg, each having several target classes.
For moredetails on the data sets, see the task description pa-per Markert and Nissim (2007).Several categories (e.g.
place-for-event,organisation-for-index) did not contain asufficient number of examples for machine learn-ing, and we decided early on to accept the fact thatthese categories will not be learned and to concen-trate on those classes where learning seemed feasi-ble.
The shared task itself consisted of 3 subtasksof different granularity for both organisation and lo-cation names.
The fine-grained evaluation aimedat distinguishing between all categories, while themedium-grained evaluation grouped different typesof metonymic usage together and addressed literal /mixed / metonymic usage.
The coarse-grained sub-task was in fact a literal / nonliteral two-class classi-fication task.Though GYDER has obtained the highest accu-racy for the metonymy shared task at SemEval-2007in all six subtasks, we don?t consider the results161(72.80% accuracy for org, 84.36% for loc) par-ticularly impressive.
In Section 3 we describe thefeature engineering lessons learned fromworking onthe task.
In Section 5 we offer some speculative re-marks on what it would take to improve the results.2 LearningGYDER (the acronym was formed from the initialsof the author?
first names) is a maximum entropylearner.
It uses Zhang Le?s 1 maximum entropytoolkit, setting the Gaussian prior to 1.
We used ran-dom 5-fold cross-validation to determine the useful-ness of a particular feature.
Due to the small num-ber of instances and features, the learning algorithmalways converged before 30 iterations, so the cross-validation process took only seconds.We also tested the classic C4.5 decision tree learn-ing algorithm Quinlan (1993), but our early exper-iments showed that the maximum entropy learnerwas consistently superior to the decision tree clas-sifier for this task, yielding about 2-5% higher accu-racy scores on average on both tasks (on the trainingset, using cross-validation).3 Feature EngineeringWe tested several features describing orthographic,syntactic, or semantic characteristics of the PossiblyMetonymic Words (PMWs).
Here we follow Nissimand Markert (2005), who reported three classes offeatures to be the most relevant for metonymy res-olution: the grammatical annotations provided forthe corpus examples by the task organizers, the de-terminer, and the grammatical number of the PMW.We also report on some features that didn?t work.3.1 Grammatical annotationsWe used the grammatical annotations provided foreach PMW in several ways.
First, we used as afeature the type of the grammatical relation and theword form of the related word.
(If there was morethan one related word, each became a feature.)
Toovercome data sparseness, it is useful to general-ize from individual headwords Markert and Nissim(2003).
We used three different methods to achievethis:1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.htmlFirst, we used Levin?s (1993) verb classificationindex to generalize the headwords of the most rele-vant grammatical relations (subject and object).
Theadded feature was simply the class assigned to theverb by Levin.We also used WordNet (Fellbaum 1998) to gen-eralize headwords.
First we gathered the hypernympath from WordNet for each headword?s sense#1 inthe train corpus.
Based on these paths we collectedsynsets whose tree frequently indicated metonymicsense.
We indicated with a feature if the headwordin question was in one of such collected subtrees.Third, we have manually built a very small verbclassification ?Trigger?
table for specific cases.
E.g.announce, say, declare all trigger the same feature.This table is the only resource in our final systemthat was manually built by us, so we note that on thetest corpus, disabling this ?Trigger?
feature does notalter org accuracy, and decreases loc accuracy by0.44%.3.2 DeterminersFollowing Nissim and Markert (2005), we distin-guished between definite, indefinite, demonstrative,possessive, wh and other determiners.
We alsomarked if the PMW was sentence-initial, and thusnecessarily determinerless.
This feature was usefulfor the resolution of organisation PMWs so we usedit only for the org tasks.
It was not straightforward,however, to assign determiners to the PMWswithoutproper syntactic analysis.
After some experiments,we linked the nearest determiner and the PMW to-gether if we found only adjectives (or nothing) be-tween them.3.3 NumberThis feature was particularly useful to separatemetonymies of the org-for-product class.
Weassumed that only PMWs ending with letter s mightbe in plural form, and for themwe compared the websearch result numbers obtained by the Google API.We ran two queries for each PMWs, one for the fullname, and one for the name without its last charac-ter.
If we observed a significant increase in the num-ber of hits returned by Google for the shorter phrase,we set this feature for plural.1623.4 PMW word formWe included the surface form of the PMW as a fea-ture, but only for the org domain.
Cross-validationon the training corpus showed that the use of thisfeature causes an 1.5% accuracy improvement fororganisations, and a slight degradation for locations.The improvement perfectly generalized to the testcorpora.
Some company names are indeed morelikely to be used in a metonymic way, so we be-lieve that this feature does more than just exploit-ing some specificity of the shared task corpora.
Wenote that the ranking of our system would have beenunaffected even if we didn?t use this feature.3.5 Unsuccessful featuresHere we discuss those features where cross-validation didn?t show improvements (and thus werenot included in the submitted system).Trigger words were automatically collected lists ofword forms and phrases that more frequentlyappeared near metonymic PMWs.Expert triggers were similar trigger words orphrases, but suggested by a linguist expert tobe potentially indicative for metonymic usage.We experimented with sample-level, sentence-level and vicinity trigger phrases.Named entity labels given by a state-of-the-artnamed entity recognizer (Szarvas et al 2006).POS tags around PMWs.Ortographical features such as capitalisation andand other surface characteristics for the PMWand nearby words.Individual tokens of the potentially metonymicphrase.Main category of Levin?s hierarchical classification.Inflectional category of the verb nearest to the PMWin the sentence.4 ResultsTable 1. shows the accuracy scores of our submittedsystem on fine classification granularity.
As a base-line, we also evalute the system without the Word-Net, Levin, Trigger and PMW word form features.This baseline system is quite similar to the one de-scribed by Nissim and Markert (2005).
We also pub-lish the majority baseline scores.run majority baseline submittedorg train 5-fold 63.30 77.51 80.92org test 61.76 70.55 72.80loc train 5-fold 79.68 85.58 88.36loc test 79.41 83.59 84.36Table 1: Accuracy of the submitted systemWe could not exploit the hierarchical structure ofthe fine-grained tag set, and ended up treating it astotally unstructured even for the mixed class, unlikeNissim and Markert, who apply complicated heuris-tics to exploit the special semantics of this class.For the coarse and medium subtasks of the locdomain, we simply coarsened the fine-grained re-sults.
For the coarse and medium subtasks ofthe org domain, we coarsened the train corpus tomedium coarseness before training.
This idea wasbased on observations on training data, but wasproven to be unjustified: it slightly decreased thesystem?s accuracy on the medium subtask.coarse medium finelocation 85.24 84.80 84.36organisation 76.72 73.28 72.80Table 2: Accuracy of the GYDER system for eachdomain / granularityIn general, the coarser grained evaluation did notshow a significantly higher accuracy (see Table 2.
),proving that the main difficulty is to distinguish be-tween literal and metonymic usage, rather than sepa-rating metonymy classes from each other (since dif-ferent classes represent significantly different usage/ context).
Because of this, data sparseness remaineda problem for coarse-grained classification as well.Per-class results of the submitted system forboth domains are shown on Table 3.
Notethat our system never predicted loc values fromthe four small classes place-for-event andproduct, object-for-name and other asthese had only 26 instances altogether.
Sincewe never had significant results for the mixedcategory, in effect the loc task ended up a bi-nary classification task between literal andplace-for-people.163loc class # prec rec fliteral 721 86.83 95.98 91.17place-for-people 141 68.22 51.77 58.87mixed 20 25.00 5.00 8.33othermet 11 - 0.0 -place-for-event 10 - 0.0 -object-for-name 4 - 0.0 -place-for-product 1 - 0.0 -org class # prec rec fliteral 520 75.76 90.77 82.59org-for-members 161 65.99 60.25 62.99org-for-product 67 82.76 35.82 50.00mixed 60 43.59 28.33 34.34org-for-facility 16 100.0 12.50 22.22othermet 8 - 0.0 -object-for-name 6 50.00 16.67 25.00org-for-index 3 - 0.0 -org-for-event 1 - 0.0 -Table 3: Per-class accuracies for both domainsWhile in the org set the system also ig-nores the smallest categories othermet,org-for-index and event (a total of 11instances), the six major categories literal,org-for-members, org-for-product,org-for-facility, object-for-name,mixed all receive meaningful hypotheses.5 Conclusions, Further DirectionsThe features we eventually selected performed wellenough to actually achieve the best scores in all sixsubtasks of the shared task, and we think they areuseful in general.
But it is worth emphasizing thatmany of these features are based on the grammaticalannotation provided by the task organizers, and assuch, would require a better dependency parser thanwe currently have at our disposal to create a fullyautomatic system.That said, there is clearly a great deal of merit toprovide this level of annotation, and we would liketo speculate what would happen if even more de-tailed annotation, not just grammatical, but also se-mantical, were provided manually.
We hypothesizethat the metonymy task would break down into thetask of identifying several journalistic cliches suchas ?location for sports team?, ?capital city for gov-ernment?, and so on, which are not yet alays dis-tinguished by the depth of the annotation.It would be a true challenge to create a data setof non-cliche metonymy cases, or a corpus largeenough to represent rare metonymy types and chal-lenging non-cliche metonymies better.We feel that at least regarding the corpus used forthe shared task, the potential of the grammatical an-notation for PMWs was more or less well exploited.Future systems should exploit more semantic knowl-edge, or the power of a larger data set, or preferablyboth.AcknowledgementWe wish to thank Andra?s Kornai for help andencouragement, and the anonymous reviewers forvaluable comments.ReferencesChristiane Fellbaum ed.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Beth Levin.
1993.
English Verb Classes and Alterna-tions.
A Preliminary Investigation.
The University ofChicago Press.Katja Markert and Malvina Nissim.
2002.
Metonymyresolution as a classification task.
Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2002).
Philadelphia, USA.Katja Markert and Malvina Nissim.
2003.
Syntactic Fea-tures and Word Similarity for Supervised MetonymyResolution.
Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics(ACL2003).
Sapporo, Japan.Malvina Nissim and Katja Markert.
2005.
Learningto buy a Renault and talk to BMW: A supervisedapproach to conventional metonymy.
InternationalWorkshop on Computational Semantics (IWCS2005).Tilburg, Netherlands.Katja Markert and Malvina Nissim.
2007.
SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007.
In Proceedings of SemEval-2007.Ross Quinlan.
1993.
C4.5: Programs for machine learn-ing.
Morgan Kaufmann.Gyo?rgy Szarvas, Richa?rd Farkas and Andra?s Kocsor.2006.
Multilingual Named Entity Recognition Sys-tem Using Boosting and C4.5 Decision Tree LearningAlgorithms.
Proceedings of Discovery Science 2006,DS2006, LNAI 4265 pp.
267-278.
Springer-Verlag.164
