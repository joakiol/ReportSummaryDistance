Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 37?45,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLarge-scale Semantic Networks: Annotation and EvaluationVa?clav Nova?kInstitute of Formal and Applied LinguisticsCharles University in Prague, Czech Republicnovak@ufal.mff.cuni.czSven HartrumpfComputer Science DepartmentUniversity of Hagen, GermanySven.Hartrumpf@FernUni-Hagen.deKeith Hall?Google ResearchZu?rich, Switzerlandkbhall@google.comAbstractWe introduce a large-scale semantic-networkannotation effort based on the MutliNet for-malism.
Annotation is achieved via a pro-cess which incorporates several independenttools including a MultiNet graph editing tool,a semantic concept lexicon, a user-editableknowledge-base for semantic concepts, and aMultiNet parser.
We present an evaluationmetric for these semantic networks, allowingus to determine the quality of annotations interms of inter-annotator agreement.
We usethis metric to report the agreement rates for apilot annotation effort involving three annota-tors.1 IntroductionIn this paper we propose an annotation frame-work which integrates the MultiNet semantic net-work formalism (Helbig, 2006) and the syntactico-semantic formalism of the Prague Dependency Tree-bank (Hajic?
et al, 2006) (PDT).
The primary goal ofthis task is to increase the interoperability of thesetwo frameworks in order to facilitate efforts to an-notate at the semantic level while preserving intra-sentential semantic and syntactic annotations as arefound in the PDT.The task of annotating text with global semanticinteractions (e.g., semantic interactions within somediscourse) presents a cognitively demanding prob-lem.
As with many other annotation formalisms,?Part of this work was completed while at the Johns Hop-kins University Center for Language and Speech Processing inBaltimore, MD USA.we propose a technique that builds from cognitivelysimpler tasks such as syntactic and semantic anno-tations at the sentence level including rich morpho-logical analysis.
Rather than constraining the se-mantic representations to those compatible with thesentential annotations, our procedure provides thesyntacitco-semantic tree as a reference; the annota-tors are free to select nodes from this tree to createnodes in the network.
We do not attempt to measurethe influence this procedure has on the types of se-mantic networks generated.
We believe that using asoft-constraint such as the syntactico-semantic tree,allows us to better generate human labeled seman-tic networks with links to the interpretations of theindividual sentence analyses.In this paper, we present a procedure for com-puting the annotator agreement rate for MultiNetgraphs.
Note that a MultiNet graph does not rep-resent the same semantics as a syntactico-semanticdependency tree.
The nodes of the MultiNet graphare connected based on a corpus-wide interpretationof the entities referred to in the corpus.
These globalconnections are determined by the intra-sententialinterpretation but are not restricted to that inter-pretation.
Therefore, the procedure for computingannotator agreement differs from the standard ap-proaches to evaluating syntactic and semantic de-pendency treebanks (e.g., dependency link agree-ment, label agreement, predicate-argument structureagreement).As noted in (Bos, 2008), ?Even though the de-sign of annotation schemes has been initiated forsingle semantic phenomena, there exists no anno-tation scheme (as far as I know) that aims to inte-37grate a wide range of semantic phenomena all atonce.
It would be welcome to have such a resourceat ones disposal, and ideally a semantic annotationscheme should be multi-layered, where certain se-mantic phenomena can be properly analysed or leftsimply unanalysed.
?In Section 1 we introduce the theoretical back-ground of the frameworks on which our annotationtool is based: MultiNet and the TectogrammaticalRepresentation (TR) of the PDT.
Section 2 describesthe annotation process in detail, including an intro-duction to the encyclopedic tools available to the an-notators.
In Section 3 we present an evaluation met-ric for MultiNet/TR labeled data.
We also present anevaluation of the data we have had annotated usingthe proposed procedure.
Finally, we conclude witha short discussion of the problems observed duringthe annotation process and suggest improvements asfuture work.1.1 MultiNetThe representation of the Multilayered ExtendedSemantic Networks (MultiNet), which is describedin (Helbig, 2006), provides a universal formalismfor the treatment of semantic phenomena of natu-ral language.
To this end, they offer distinct ad-vantages over the use of the classical predicatecalculus and its derivatives.
For example, Multi-Net provides a rich ontology of semantic-concepttypes.
This ontology has been constructed to belanguage independent.
Due to the graphical inter-pretation of MultiNets, we believe manual anno-tation and interpretation is simpler and thus morecognitively compatible.
Figure 1 shows the Multi-Net annotation of a sentence from the WSJ corpus:?Stephen Akerfeldt, currently vice president fi-nance, will succeed Mr.
McAlpine.
?In this example, there are a few relationships that il-lustrate the representational power of MultiNet.
Themain predicate succeed is a ANTE dependent of thenode now, which indicates that the outcome of theevent described by the predicate occurs at some timelater than the time of the statement (i.e., the succes-sion is taking place after the current time as capturedby the future tense in the sentence).
Intra-sententialcoreference is indicated by the EQU relationship.From the previous context, we know that the vicepresident is related to a particular company, MagnaInternational Inc.
The pragmatically defined rela-tionship between Magna International Inc. and vicepresident finance is captured by the ATTCH (con-ceptual attachment) relationship.
This indicates thatthere is some relationship between these entities forwhich one is a member of the other (as indicated bythe directed edge).
Stephen Akerfeldt is the agent ofthe predicate described by this sub-network.The semantic representation of natural languageexpressions by means of MultiNet is generally in-dependent of the considered language.
In contrast,the syntactic constructs used in different languagesto express the same content are obviously not iden-tical.
To bridge the gap between different languageswe employ the deep syntactico-semantic representa-tion available in the Functional Generative Descrip-tion framework (Sgall et al, 1986).1.2 Prague Dependency TreebankThe Prague Dependency Treebank (PDT) presents alanguage resource containing a deep manual analy-sis of texts(Sgall et al, 2004).
The PDT containsannotations on three layers:Morphological A rich morphological annotation isprovided when such information is available inthe language.
This includes lemmatization anddetailed morphological tagging.Analytical The analytical layer is a dependencyanalysis based purely on the syntactic interpre-tation.Tectogrammatical The tectogrammatical annota-tion provides a deep-syntactic (syntactico-semantic) analysis of the text.
The formal-ism abstracts away from word-order, functionwords (syn-semantic words), and morphologi-cal variation.The units of each annotation level are linked withcorresponding units on the preceding level.
Themorphological units are linked directly with theoriginal tokenized text.
Linking is possible as mostof these interpretations are directly tied to the wordsin the original sentence.
In MultiNet graphs, addi-tional nodes are added and nodes are removed.The PDT 2.0 is based on the long-standingPraguian linguistic tradition, adapted for the current38Figure 1: MultiNet annotation of sentence ?Stephen Akerfeldt, currently vice president finance, will succeed Mr.McAlpine.?
Nodes C4 and C8 are re-used from previous sentences.
Node C2 is an unexpressed (not explicitly statedin the text) annotator-created node used in previous annotations.computational-linguistics research needs.
The theo-retical basis of the tectogrammatical representationlies in the Functional Generative Description of lan-guage systems (Sgall et al, 1986).
Software toolsfor corpus search, lexicon retrieval, annotation, andlanguage analysis are included.
Extensive documen-tation in English is provided as well.2 Integrated Annotation ProcessWe propose an integrated annotation procedureaimed at acquiring high-quality MultiNet semanticannotations.
The procedure is based on a combi-nation of annotation tools and annotation resources.We present these components in the this section.2.1 Annotation ToolThe core annotation is facilitated by the cedittool1, which uses PML (Pajas and S?te?pa?nek, 2005),an XML file format, as its internal representa-tion (Nova?k, 2007).
The annotation tool is anapplication with a graphical user interface imple-mented in Java (Sun Microsystems, Inc., 2007).
The1The cedit annotation tool can be downloaded fromhttp://ufal.mff.cuni.cz/?novak/files/cedit.zip.cedit tool is platform independent and directly con-nected to the annotators?
wiki (see Section 2.4),where annotators can access the definitions of indi-vidual MultiNet semantic relations, functions and at-tributes; as well as examples, counterexamples, anddiscussion concerning the entity in question.
If thewiki page does not contain the required information,the annotator is encouraged to edit the page withhis/her questions and comments.2.2 Online LexiconThe annotators in the semantic annotation projecthave the option to look up examples of MultiNetstructures in an online version of the semanticallyoriented computer lexicon HaGenLex (Hartrumpf etal., 2003).
The annotators can use lemmata (insteadof reading IDs formed of the lemma and a numer-ical suffix) for the query, thus increasing the recallof related structures.
English and German input issupported with outputs in English and/or German;there are approximately 3,000 and 25,000 seman-tic networks, respectively, in the lexicon.
An exam-ple sentence for the German verb ?borgen.1.1?
(?toborrow?)
plus its automatically generated and val-39Figure 2: HaGenLex entry showing an example sentencefor the German verb ?borgen.1.1?
(?to borrow?).
Thesentence is literally ?The man borrows himself moneyfrom the friend.
?idated semantic representation is displayed in Fig-ure 2.
The quality of example parses is assured bycomparing the marked-up complements in the ex-ample to the ones in the semantic network.
In therare case that the parse is not optimal, it will not bevisible to annotators.2.3 Online ParserSometimes the annotator needs to look up a phraseor something more general than a particular nounor verb.
In this case, the annotator can usethe workbench for (MultiNet) knowledge bases(MWR (Gno?rlich, 2000)), which provides conve-nient and quick access to the parser that translatesGerman sentences or phrases into MultiNets.2.4 Wiki Knowledge BaseAwiki (Leuf and Cunningham, 2001) is used collab-oratively to create and maintain the knowledge baseused by all the annotators.
In this project we useDokuwiki (Badger, 2007).
The entries of individ-ual annotators in the wiki are logged and a feed ofchanges can be observed using an RSS reader.
Thecedit annotation tool allows users to display appro-priate wiki pages of individual relation types, func-tion types and attributes directly from the tool usingtheir preferred web browser.3 Network EvaluationWe present an evaluation which has been carriedout on an initial set of annotations of English arti-cles from The Wall Street Journal (covering thoseannotated at the syntactic level in the Penn Tree-bank (Marcus et al, 1993)).
We use the annotationfrom the Prague Czech-English Dependency Tree-bank (Cur??
?n et al, 2004), which contains a large por-tion of the WSJ Treebank annotated according to thePDT annotation scheme (including all layers of theFGD formalism).We reserved a small set of data to be used to trainour annotators and have excluded these articles fromthe evaluation.
Three native English-speaking anno-tators were trained and then asked to annotate sen-tences from the corpus.
We have a sample of 67sentences (1793 words) annotated by two of the an-notators; of those, 46 sentences (1236 words) wereannotated by three annotators.2 Agreement is mea-sured for each individual sentences in two steps.First, the best match between the two annotators?graphs is found and then the F-measure is computed.In order to determine the optimal graph match be-tween two graphs, we make use of the fact thatthe annotators have the tectogrammatical tree fromwhich they can select nodes as concepts in theMulti-Net graph.
Many of the nodes in the annotatedgraphs remain linked to the tectogrammatical tree,therefore we have a unique identifier for these nodes.When matching the nodes of two different annota-tions, we assume a node represents an identical con-cept if both annotators linked the node to the sametectogrammatical node.
For the remaining nodes,we consider all possible one-to-one mappings andconstruct the optimal mapping with respect to the F-measure.Formally, we start with a set of tectogrammaticaltrees containing a set of nodes N .
The annotation isa tuple G = (V,E, T,A), where V are the vertices,E ?
V ?
V ?P are the directed edges and their la-bels (e.g., agent of an action: AGT ?
P ), T ?
V ?Nis the mapping from vertices to the tectogrammati-cal nodes, and finally A are attributes of the nodes,which we ignore in this initial evaluation.3 Analo-gously, G?
= (V ?, E?, T ?, A?)
is another annotation2The data associated with this experiment can be down-loaded from http://ufal.mff.cuni.cz/?novak/files/data.zip.
Thedata is in cedit format and can be viewed using the cedit editorat http://ufal.mff.cuni.cz/?novak/files/cedit.zip.3We simplified the problem also by ignoring the mappingfrom edges to tectogrammatical nodes and the MultiNet edgeattribute knowledge type.40of the same sentence and our goal is to measure thesimilarity s(G,G?)
?
[0, 1] of G and G?.To measure the similarity we need a set ?
of ad-missible one-to-one mappings between vertices inthe two annotations.
A mapping is admissible ifit connects vertices which are indicated by the an-notators as representing the same tectogrammaticalnode:?
={?
?
V ?
V ????
(1)?n?Nv?Vv?
?V ?(((v,n)?T?
(v?,n)?T ?)?(v,v?)??)?
?v?Vv?,w?
?V ?(((v,v?)???(v,w?)??)?(v?=w?))?
?v,w?Vv?
?V ?(((v,v?)???(w,v?)??)?
(v=w))}In Equation 1, the first condition ensures that ?
isconstrained by the mapping induced by the links tothe tectogrammatical layer.
The remaining two con-ditions guarantee that ?
is a one-to-one mapping.We define the annotation agreement s as:sF (G,G?)
= max???
(F (G,G?, ?
))where F is the F1-measure:Fm(G,G?, ?)
= 2 ?m(?
)|E|+ |E?|wherem(?)
is the number of edges that match giventhe mapping ?.We use four versions of m, which gives us fourversions of F and consequently four scores s for ev-ery sentence:Directed unlabeled: mdu(?)
=?????{(v,w,?)?E????v?,w?
?V ?,??
?P((v?, w?, ??)?
E??
(v, v?)
?
?
?
(w,w?)
?
?)}????
?Undirected unlabeled: muu(?)
=?????{(v,w,?)?E????v?,w?
?V ?,??
?P(((v?, w?, ??)
?
E?
?
(w?, v?, ??)
?
E?)?
(v, v?)
?
?
?
(w,w?)
?
?)}????
?Directed labeled: mdl(?)
=?????{(v,w,?)?E????v?,w?
?V ?
((v?, w?, ?)?
E??
(v, v?)
?
?
?
(w,w?)
?
?)}????
?Undirected labeled: mul(?)
=?????{(v,w,?)?E????v?,w?
?V ?
(((v?, w?, ?)
?
E?
?
(w?, v?, ?)
?
E?)?
(v, v?)
?
?
?
(w,w?)
?
?)}????
?These four m(?)
functions give us four possibleFm measures, which allows us to have four scoresfor every sentence: sdu, suu, sdl and sul.Figure 3 shows that the inter-annotator agreementis not significantly correlated with the position of thesentence in the annotation process.
This suggeststhat the annotations for each annotator had achieveda stable point (primarily due to the annotator trainingprocess).10 20 30 40 500.20.40.60.81.0Sentence lengthInter?annotator F?measure?
Undirected UnlabeledAnnotatorsCB?CWSM?CWSM?CBFigure 4: Inter-annotator agreement depending on thesentence length.
Each point represents a sentence.Figure 4 shows that the agreement is not corre-lated with the sentence length.
It means that longer410 10 20 30 400.20.40.60.81.0IndexUndirectedUnlabeledF?measureAnnotatorsCB?CWSM?CWSM?CB0 10 20 30 400.00.20.40.6IndexUndirectedLabeled F?measureAnnotatorsCB?CWSM?CWSM?CBFigure 3: Inter-annotator agreement over time.
Left: unlabeled, right: labeled.
Each point represents a sentence; CB,CW, and SM are the annotators?
IDs.sentences are not more difficult than short sentences.The variance decreases with the sentence length asexpected.In Figure 5 we show the comparison of directedand labeled evaluations with the undirected unla-beled case.
By definition the undirected unlabeledscore is the upper bound for all the other scores.The directed score is well correlated and not verydifferent from the undirected score, indicating thatthe annotators did not have much trouble with de-termining the correct direction of the edges.
Thismight be, in part, due to support from the formal-ism and its tool cedit: each relation type is speci-fied by a semantic-concept type signature; a relationthat violates its signature is reported immediately tothe annotator.
On the other hand, labeled score issignificantly lower than the unlabeled score, whichsuggests that the annotators have difficulties in as-signing the correct relation types.
The correlationcoefficient between suu and sul (approx.
0.75) isalso much lower than than the correlation coefficientbetween suu and sdu (approx.
0.95).Figure 6 compares individual annotator pairs.
Thescores are similar to each other and also have a sim-ilar distribution shape.Undirected Unlabeled F?measureDensity0.00.51.01.52.02.50.2 0.4 0.6 0.8 1.0CB ?
CW 0.00.51.01.52.02.5SM ?
CB0.00.51.01.52.02.5SM ?
CWFigure 6: Comparison of individual annotator pairs.A more detailed comparison of individual anno-tator pairs is depicted in Figure 7.
The graph showsthat there is a significant positive correlation be-tween scores, i.e.
if two annotators can agree on the420.2 0.4 0.6 0.8 1.00.20.40.60.8Undirected Unlabeled F?measureDirected UnlabeledF?measureAnnotatorsCB?CWSM?CWSM?CB0.2 0.4 0.6 0.8 1.00.00.20.40.6Undirected Unlabeled F?measureUndirectedLabeled F?measureAnnotatorsCB?CWSM?CWSM?CBFigure 5: Left: Directed vs. undirected inter-annotator agreement.
Right: Labeled vs. unlabeled inter-annotator agree-ment.
Each point represents a sentence.annotation, the third is likely to also agree, but thiscorrelation is not a very strong one.
The actual cor-relation coefficients are shown under the main diag-onal of the matrix.Sample Annotators Agreement F-measuresuu sdu sul sdlSmaller CB-CW 61.0 56.3 37.1 35.0Smaller SM-CB 54.9 48.5 27.1 25.7Smaller SM-CW 58.5 50.7 31.3 30.2Smaller average 58.1 51.8 31.8 30.3Larger CB-CW 64.6 59.8 40.1 38.5Table 1: Inter-annotator agreement in percents.
The re-sults come from the two samples described in the firstparagraph of Section 3.Finally, we summarize the raw result in Table 1.Note that we report simple annotator agreementhere.4 Conclusion and Future WorkWe have presented a novel framework for the anno-tation of semantic network for natural language dis-course.
Additionally we present a technique to eval-uate the agreement between the semantic networksannotated by different annotators.Our evaluation of an initial dataset reveals thatgiven the current tools and annotation guidelines, theannotators are able to construct the structure of thesemantic network (i.e., they are good at building thedirected graph).
They are not, however, able to con-sistently label the semantic relations between the se-mantic nodes.
In our future work, we will investigatethe difficulty in labeling semantic annotations.
Wewould like to determine whether this is a product ofthe annotation guidelines, the tool, or the formalism.Our ongoing research include the annotation ofinter-sentential coreference relationships betweenthe semantic concepts within the sentence-basedgraphs.
These relationships link the local structures,allowing for a complete semantic interpretation ofthe discourse.
Given the current level of consistencyin structural annotation, we believe the data will beuseful in this analysis.43CB_CW0.2 0.4 0.6 0.8llllllllllllll ll llllllll lll llll llll lllllllllll0.20.40.60.81.0llllllllllllllll llllllll llllllllllllllllllllll0.20.40.60.80.34 SM_CWllllll lll lllllllllllllllll lllllllllllllllllll0.2 0.4 0.6 0.8 1.00.55 0.560.2 0.4 0.6 0.80.20.40.60.8SM_CBUndirected Unlabeled F?measure with Correlation CoefficientsFigure 7: Undirected, unlabeled F-measure correlation of annotator pairs.
Each cell represents two different pairs ofannotators; cells with graphs show scatter-plots of F-scores for the annotator pairs along with the optimal linear fit;cells with values show the correlation coefficient (each point in the plot corresponds to a sentence).
For example,the top row, right-most column, we are comparing the F-score agreement of annotators CB and CW with that of theF-score agreement of annotators SM and CB.
This should help identify an outlier in the consistency of the annotations.AcknowledgmentThis work was partially supported by CzechAcademy of Science grants 1ET201120505 and1ET101120503; by Czech Ministry of Educa-tion, Youth and Sports projects LC536 andMSM0021620838; and by the US National ScienceFoundation under grant OISE?0530118.
The viewsexpressed are not necessarily endorsed by the spon-sors.ReferencesMike Badger.
2007.
Dokuwiki ?
A Practical OpenSource Knowledge Base Solution.
Enterprise OpenSource Magazine.Johan Bos.
2008.
Let?s not Argue about Semantics.
InEuropean Language Resources Association (ELRA),editor, Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08), Mar-rakech, Morocco, may.Jan Cur??
?n, Martin C?mejrek, Jir???
Havelka, and VladislavKubon?.
2004.
Building parallel bilingual syntacti-cally annotated corpus.
In Proceedings of The FirstInternational Joint Conference on Natural LanguageProcessing, pages 141?146, Hainan Island, China.Carsten Gno?rlich.
2000.
MultiNet/WR: A KnowledgeEngineering Toolkit for Natural Language Informa-tion.
Technical Report 278, University Hagen, Hagen,Germany.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and MarieMikulova?.
2006.
Prague Dependency Treebank 2.0.44CD-ROM, Linguistic Data Consortium, LDC CatalogNo.
: LDC2006T01, Philadelphia, Pennsylvania.Sven Hartrumpf, Hermann Helbig, and Rainer Osswald.2003.
The Semantically Based Computer Lexicon Ha-GenLex ?
Structure and Technological Environment.Traitement Automatique des Langues, 44(2):81?105.Hermann Helbig.
2006.
Knowledge Representation andthe Semantics of Natural Language.
Springer, Berlin,Germany.Bo Leuf and Ward Cunningham.
2001.
The Wiki Way.Quick Collaboration on the Web.
Addison-Wesley,Reading, Massachusetts.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn treebank.
ComputationalLinguistics, 19(2):313?330.Va?clav Nova?k.
2007.
Cedit ?
semantic networks man-ual annotation tool.
In Proceedings of Human Lan-guage Technologies: The Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-HLT), pages 11?12,Rochester, New York, April.
Association for Compu-tational Linguistics.Petr Pajas and Jan S?te?pa?nek.
2005.
A Generic XML-Based Format for Structured Linguistic Annotationand Its Application to Prague Dependency Treebank2.0.
Technical Report 29, UFAL MFF UK, Praha,Czech Republic.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Semantic and Prag-matic Aspects.
D. Reidel, Dordrecht, The Netherlands.Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?.
2004.Deep syntactic annotation: Tectogrammatical repre-sentation and beyond.
In Adam Meyers, editor, Pro-ceedings of the HLT-NAACL 2004 Workshop: Fron-tiers in Corpus Annotation, pages 32?38, Boston,Massachusetts, May.
Association for ComputationalLinguistics.Sun Microsystems, Inc. 2007.
Java Platform, StandardEdition 6. http://java.sun.com/javase/6/webnotes/README.html.45
