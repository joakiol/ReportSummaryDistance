Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 19?24,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsImprovements to Syntax-based Machine Translation using EnsembleDependency ParsersNathan David Green and Zdene?k Z?abokrtsky?Charles University in PragueInstitute of Formal and Applied LinguisticsFaculty of Mathematics and PhysicsPrague, Czech Republic{green,zabokrtsky}@ufal.mff.cuni.czAbstractDependency parsers are almost ubiqui-tously evaluated on their accuracy scores,these scores say nothing of the complex-ity and usefulness of the resulting struc-tures.
The structures may have more com-plexity due to their coordination structureor attachment rules.
As dependency parsesare basic structures in which other systemsare built upon, it would seem more reason-able to judge these parsers down the NLPpipeline.We show results from 7 individual parsers,including dependency and constituentparsers, and 3 ensemble parsing tech-niques with their overall effect on a Ma-chine Translation system, Treex, for En-glish to Czech translation.
We show thatparsers?
UAS scores are more correlatedto the NIST evaluation metric than to theBLEU Metric, however we see increasesin both metrics.1 IntroductionEnsemble learning (Dietterich, 2000) has beenused for a variety of machine learning tasks andrecently has been applied to dependency parsingin various ways and with different levels of suc-cess.
(Surdeanu and Manning, 2010; Haffariet al 2011) showed a successful combination ofparse trees through a linear combination of treeswith various weighting formulations.
To keeptheir tree constraint, they applied Eisner?s algo-rithm for reparsing (Eisner, 1996).Parser combination with dependency trees hasbeen examined in terms of accuracy (Sagae andLavie, 2006; Sagae and Tsujii, 2007; Zemanand Z?abokrtsky?, 2005; Holan and Z?abokrtsky?,2006).
Other methods of parser combinationshave shown to be successful such as using oneparser to generate features for another parser.
Thiswas shown in (Nivre and McDonald, 2008), inwhich Malt Parser was used as a feature to MSTParser.
The result was a successful combination ofa transition-based and graph-based parser, but didnot address adding other types of parsers into theframework.We will use three ensemble approaches.
First afixed weight ensemble approach in which edgesare added together in a weighted graph.
Sec-ond, we added the edges using weights learnedthrough fuzzy clustering based on POS errors.Third, we will use a meta-classifier that uses anSVM to predict the correct model for edge usingonly model agreements without any linguistic in-formation added.
Parsing accuracy and machinetranslation has been examined in terms of BLEUscore (Quirk and Corston-Oliver, 2006).
How-ever, we believe our work is the first to examinethe NLP pipeline for ensemble parsing for both de-pendency and constituent parsers as well as exam-ining both BLEU and NIST scores?
relationship totheir Unlabeled Accuracy Score(UAS).2 Methodology2.1 AnnotationTo find the maximum effect that dependency pars-ing can have on the NLP pipeline, we annotatedEnglish dependency trees to form a gold standard.Annotation was done with two annotators usinga tree editor, Tred (Pajas and Fabian, 2011), ondata that was preprocessed using MST parser.
Forthe annotation of our gold data, we used the stan-dard developed by the Prague Dependency Tree-bank (PDT) (Hajic?, 1998).
PDT is annotated onthree levels, morphological, analytical, and tec-togrammatical.
For our gold data we do not touchthe morphological layer, we only correct the ana-lytical layer (i.e.
labeled dependency trees).
Formachine translation experiments later in the paper19we allow the system to automatically generate anew tectogrammatical layer based on our new an-alytical layer annotation.
Because the Treex ma-chine translation system uses a tectogrammaticallayer, when in doubt, ambiguity was left to the tec-togrammatical (t-layer in Figure 1) to handle.2.1.1 Data SetsFor the annotation experiments we use data pro-vided by the 2012 Workshop for Machine Trans-lation (WMT2012).
The data which consistsof 3,003 sentences was automatically tokenized,tagged, and parsed.
This data set was also chosensince it is disjoint from the usual dependency train-ing data, allowing researchers to use it as a out-of-domain testing set.
The parser used was an imple-mentation of MST parser.
We then hand correctedthe analytical trees to have a ?Gold?
standard de-pendency structure.
Analytical trees were anno-tated on the PDT standard.
Most changes involvedcoordination construction along with prepositionalphrase attachment.
We plan to publicly release thisdata and corresponding annotations in the near fu-ture1.Having only two annotators has limited usto evaluating our annotation only through spotchecking and through comparison with other base-lines.
Annotation happened sequentially one afteranother.
Possible errors were additionally detectedthrough automatic means.
As a comparison wewill evaluate our gold data set versus other parsersin respect to their performance on previous datasets, namely the Wall Street Journal (WSJ) section23.2.2 Translation2.2.1 Data SetsAll the parsers were trained on sections 02-21 ofthe WSJ, except the Stanford parser which alsouses section 01.
We retrained MST and Maltparsers and used pre-trained models for the otherparsers.
Machine translation data was used fromWMT 2010, 2011, and 2012.
Using our goldstandard we are able to evaluate the effective-ness of different parser types from graph-base,transition-based, constituent conversion to ensem-ble approaches on the 2012 data while finding datatrends using previous years data.1When available the data and description will be atwww.nathangreen.com/wmtdata2.2.2 Translation ComponentsTo examine the effects of dependency parsingdown the NLP pipeline, we now turn to syntaxbased machine translation.
Our dependency mod-els will be evaluated using the Treex translationsystem (Popel and Z?abokrtsky?, 2010).
This sys-tem, as opposed to other popular machine transla-tion systems, makes direct use of the dependencystructure during the conversion from source to tar-get languages via a tectogrammatical tree transla-tion approach.Figure 1: Treex syntax-based translation scenario(Popel and Z?abokrtsky?, 2010)We use the different parsers in separate trans-lation runs each time in the same Treex parsingblock.
So each translation scenario only differs inthe parser used and nothing else.
As can be seenin Figure 1, we are directly manipulating the An-alytical portion of Treex.
The parsers used are asfollows:?
MST: Implementation of Ryan McDonald?sMinimum spanning tree parser (McDonald etal., 2005)?
MST with chunking: Same implementationas above but we parse the sentences based onchunks and not full sentences.
For instancethis could mean separating parentheticals orseparating appositions (Popel et al 2011)?
Malt: Implementation of Nivre?s Malt Parsertrained on the Penn Treebank (Nivre, 2003)?
Malt with chunking: Same implementationas above but with chunked parsing?
ZPar: Yue Zhang?s statistical parser.
Weused the pretrained English model (en-glish.tar.gz) available on the ZPar website forall tests (Zhang and Clark, 2011)?
Charniak: A constituent based parser(ec50spfinal model) in which we transform20the results using the Pennconverter (Johans-son and Nugues, 2007)?
Stanford: Another constituent basedparser (Klein and Manning, 2003) whoseoutput is converted using Pennconverter aswell (wsjPCFG.ser.gz model)?
Fixed Weight Ensemble: A stacked en-semble system combining five of the parsersabove (MST, Malt, ZPar, Charniak, Stan-ford).
The weights for each tree are as-signed based on UAS score found in tun-ing data, section 22 of the WSJ (Green andZ?abokrtsky?, 2012)?
Fuzzy Cluster: A stacked ensemble systemas well but weights are determined by a clus-ter analysis of POS errors found in the sametuning data as above (Green and Z?abokrtsky?,2012)?
SVM: An ensemble system in which each in-dividual edge is picked by a meta classifierfrom the same 5 parsers as the other ensemblesystems.
The SVM meta classifier is trainedon results from the above tuning data (Greenet al 2012a; Green et al 2012b).2.2.3 EvaluationFor Machine Translation we report two automaticevaluation scores, BLEU and NIST.
We examineparser accuracy using UAS.
This paper comparesa machine translation system integrating 10 differ-ent parsing systems against each other, using thebelow metrics.The BLEU (BiLingual Evaluation Understudy)and NIST(from the National Institute of Standardsand Technology), are automatic scoring mecha-nisms for machine translation that are quick andcan be reused as benchmarks across machinetranslation tasks.
BLEU and NIST are calculatedas the geometric mean of n-grams multiplied by abrevity penalty, comparing a machine translationand a reference text (Papineni et al 2002).
NISTis based upon the BLEU n-gram approach how-ever it is also weighted towards discovering more?informative?
n-grams.
The more rare an n-gramis, the higher the weight for a correct translation ofit will be.Made a standard in the CoNLL shared taskscompetition, UAS studies the structure of a depen-dency tree and assesses how often the output hasthe correct head and dependency arcs (Buchholzand Marsi, 2006).
We report UAS scores for eachparser on section 23 of the WSJ.3 Results and Discussion3.1 Type of Changes in WMT AnnotationSince our gold annotated data was preprocessedwith MST parser, our baseline system at the time,we started with a decent baseline and only hadto change 9% of the dependency arcs in the data.These 9% of changes roughly increase the BLEUscore by 7%.3.2 Parser AccuracyAs seen in previous Ensemble papers (Farkas andBohnet, 2012; Green et al 2012a; Green et al2012b; Green and Z?abokrtsky?, 2012; Zeman andZ?abokrtsky?, 2005), parsing accuracy can be im-proved by combining parsers?
outputs for a varietyof languages.
We apply a few of these systems, asdescribed in Section 2.2.2, to English using mod-els trained for both dependencies and constituents.3.2.1 Parsers vs our Gold StandardOn average our gold data differed in head agree-ment from our base parser 14.77% of the time.When our base parsers were tested on the WSJsection 23 data they had an average error rate of12.17% which is roughly comparable to the differ-ence with our gold data set which indicates overallour annotations are close to the accepted standardfrom the community.
The slight difference in per-centage fits into what is expect in annotator errorand in the errors in the conversion process of theWSJ by Pennconverter.3.3 Parsing Errors Effect on MT3.3.1 MT Results in WMT with EnsembleParsersWMT 2010As seen in Table 1, the highest resulting BLEUscore for the 2010 data set is from the fixed weightensemble system.
The other two ensemble sys-tems are beaten by one component system, Char-niak.
However, this changes when comparingNIST scores.
Two of the ensemble method havehigher NIST scores than Charniak, similar to theirUAS scores.WMT 2011The 2011 data corresponded the best with UASscores.
While the BLEU score increases for all21Parser UAS NIST(10/11/12) BLEU(10/11/12)MST 86.49 5.40/5.58/5.19 12.99/13.58/11.54MST w chunking 86.57 5.43/5.63/5.23 13.43/14.00/11.96Malt 84.51 5.37/5.57/5.14 12.90/13.48/11.27Malt w chunking 87.01 5.41/5.60/5.19 13.39/13.80/11.73ZPar 76.06 5.26/5.46/5.08 11.91/12.48/10.53Charniak 92.08 5.47/5.65/5.28 13.49/13.95/12.26Stanford 87.88 5.40/5.59/5.18 13.23/13.63/11.74Fixed Weight 92.58 5.49/5.68/5.29 13.53/14.04/12.23Fuzzy Cluster 92.54 5.47/5.68/5.26 13.47/14.06/12.06SVM 92.60 5.48/5.68/5.28 13.45/14.11/12.22Table 1: Scores for each machine translation run for each dataset (WMT 2010, 2011 and 2012)the ensemble systems, the order of systems byUAS scores corresponds exactly to the systems or-dered by NIST score and corelates strongly (Table2).
Unlike the 2010 data, the MST parser was thehighest base parser in terms of the BLEU metric.WMT 2012The ensemble increases are statistically significantfor both the SVM and the Fixed Weight systemover the MST with chunking parser with 99% con-fidence, our previous baseline and best scoringbase system from 2011 in terms of BLEU score.We examine our data versus MST with chunkinginstead of Charniak since we have preprocessedour gold data set with MST, allowing us a directcomparison in improvements.
The fuzzy clustersystem achieves a higher BLEU evaluation scorethan MST, but is not significant.
In pairwise testsit wins approximately 78% of the time.
This is thefirst dataset we have looked at where the BLEUscore is higher for a component parser and not anensemble system, although the NIST score is stillhigher for the ensemble systems.NIST BLEU2010 0.98 0.932011 0.98 0.942012 0.95 0.97Table 2: Pearson correlation coefficients for eachyear and each metric when measured against UAS.Statistics are taken from the WMT results in Table1.
Overall NIST has the stronger correlation toUAS scores, however both NIST and BLEU showa strong relationship.3.3.2 Human Manual Evaluation: SVM vsthe Baseline SystemWe selected 200 sentences at random from our an-notations and they were given to 7 native Czechspeakers.
77 times the reviewers preferred theSVM system, 48 times they preferred the MSTsystem, and 57 times they said there was no differ-ence between the sentences.
On average each re-viewer looked at 26 sentences with a median of 30sentences.
Reviewers were allowed three options:sentence 1 is better, sentence 2 is better, both sen-tences are of equal quality.
Sentences were dis-played in a random order and the systems wererandomly shuffled for each question and for eachuser.+ = -+ 12 12 0= 3 7- 7Table 3: Agreement for sentences with 2 or moreannotators for our baseline and SVM systems.
(-,-)all annotators agreed the baseline was better, (+,+)all annotators agreed the SVM system was better,(+,-) the annotators disagreed with each otherTable 3 indicates that the SVM system was pre-ferred.
When removing annotations marked asequal, we see that the SVM system was preferred24 times to the Baseline?s 14.Although a small sample, this shows that usingthe ensemble parser will at worse give you equalresults and at best a much improved result.223.3.3 MT Results with Gold DataIn the perfect situation of having gold standard de-pendency trees, we obtained a NIST of 5.30 anda BLEU of 12.39.
For our gold standard systemrun, the parsing component was removed and re-placed with our hand annotated data.
These arethe highest NIST and BLEU scores we have ob-tained including using all base parsers or any com-binations of parsers.
This indicates that while anold problem which is a ?solved?
problem for somelanguages, Parsing is still worth researching andimproving for its cascading effects down the NLPpipeline.4 ConclusionWe have shown that ensemble parsing techniqueshave an influence on syntax-based machine trans-lation both in manual and automatic evaluation.Furthermore we have shown a stronger correlationbetween parser accuracy and the NIST rather thanthe more commonly used BLEU metric.
We havealso introduced a gold set of English dependencytrees based on the WMT 2012 machine translationtask data, which shows a larger increase in bothBLEU and NIST.
While on some datasets it is in-conclusive whether using an ensemble parser withbetter accuracy has a large enough effect, we doshow that practically you will not do worse usingone and in many cases do much better.5 AcknowledgmentsThis research has received funding from theEuropean Commission?s 7th Framework Pro-gram (FP7) under grant agreement n?
238405(CLARA).
Additionally, this work has been us-ing language resources developed and/or storedand/or distributed by the LINDAT-Clarin projectof the Ministry of Education of the Czech Repub-lic (project LM2010013).ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.In Proceedings of the Tenth Conference on Com-putational Natural Language Learning, CoNLL-X?06, pages 149?164, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Thomas G. Dietterich.
2000.
Ensemble Methodsin Machine Learning.
In Proceedings of the FirstInternational Workshop on Multiple Classifier Sys-tems, MCS ?00, pages 1?15, London, UK.
Springer-Verlag.Jason Eisner.
1996.
Three New Probabilistic Mod-els for Dependency Parsing: An Exploration.
InProceedings of the 16th International Conferenceon Computational Linguistics (COLING-96), pages340?345, Copenhagen, August.Richa?rd Farkas and Bernd Bohnet.
2012.
Stacking ofDependency and Phrase Structure Parsers.
In Pro-ceedings of COLING 2012, pages 849?866, Mum-bai, India, December.
The COLING 2012 Organiz-ing Committee.Nathan Green and Zdene?k Z?abokrtsky?.
2012.
Hy-brid Combination of Constituency and DependencyTrees into an Ensemble Dependency Parser.
In Pro-ceedings of the EACL 2012 Workshop on Innovativehybrid approaches to the processing of textual data,Avignon, France.Nathan Green and Zdene?k Z?abokrtsky?.
2012.
Ensem-ble Parsing and its Effect on Machine Translation.Technical Report 48.Nathan Green, Septina Dian Larasati, and Zdene?kZ?abokrtsky?.
2012a.
Indonesian DependencyTreebank: Annotation and Parsing.
In Proceed-ings of the 26th Pacific Asia Conference on Lan-guage, Information, and Computation, pages 137?145, Bali,Indonesia, November.
Faculty of Com-puter Science, Universitas Indonesia.Nathan Green, Loganathan Ramasamy, and Zdene?kZ?abokrtsky?.
2012b.
Using an SVM Ensemble Sys-tem for Improved Tamil Dependency Parsing.
InProceedings of the ACL 2012 Joint Workshop onStatistical Parsing and Semantic Processing of Mor-phologically Rich Languages, pages 72?77, Jeju,Republic of Korea, July 12.
Association for Com-putational Linguistics.Gholamreza Haffari, Marzieh Razavi, and AnoopSarkar.
2011.
An Ensemble Model that CombinesSyntactic and Semantic Clustering for Discrimina-tive Dependency Parsing.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 710?714, Portland, Oregon, USA, June.Association for Computational Linguistics.Jan Hajic?.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
In EvaHajic?ova?, editor, Issues of Valency and Meaning.Studies in Honor of Jarmila Panevova?, pages 12?19.Prague Karolinum, Charles University Press.Toma?s?
Holan and Zdene?k Z?abokrtsky?.
2006.
Com-bining Czech Dependency Parsers.
In Proceedingsof the 9th international conference on Text, Speechand Dialogue, TSD?06, pages 95?102, Berlin, Hei-delberg.
Springer-Verlag.23Richard Johansson and Pierre Nugues.
2007.
Ex-tended Constituent-to-dependency Conversion forEnglish.
In Proceedings of NODALIDA 2007, pages105?112, Tartu, Estonia, May 25-26.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, HLT ?05, pages 523?530, Morristown, NJ,USA.
Association for Computational Linguistics.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing Graph-Based and Transition-Based DependencyParsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.
Association forComputational Linguistics.Joakim Nivre.
2003.
An Efficient Algorithm for Pro-jective Dependency Parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT, pages 149?160.Petr Pajas and Peter Fabian.
2011.
TrEd 2.0 - newlyrefactored tree editor.
http://ufal.mff.cuni.cz/tred/,Institute of Formal and Applied Linguistics, MFFUK.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, ACL ?02, pages 311?318, Morristown, NJ, USA.
Association for Com-putational Linguistics.Martin Popel and Zdene?k Z?abokrtsky?.
2010.
Tec-toMT: modular NLP framework.
In Proceedings ofthe 7th international conference on Advances in nat-ural language processing, IceTAL?10, pages 293?304, Berlin, Heidelberg.
Springer-Verlag.Martin Popel, David Marec?ek, Nathan Green, andZdenek Zabokrtsky.
2011.
Influence of parserchoice on dependency-based mt.
In Proceedings ofthe Sixth Workshop on Statistical Machine Transla-tion, pages 433?439, Edinburgh, Scotland, July.
As-sociation for Computational Linguistics.Chris Quirk and Simon Corston-Oliver.
2006.
The im-pact of parse quality on syntactically-informed sta-tistical machine translation.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?06, pages 62?69,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Kenji Sagae and Alon Lavie.
2006.
Parser Combina-tion by Reparsing.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Companion Volume: Short Papers, pages 129?132,New York City, USA, June.
Association for Compu-tational Linguistics.Kenji Sagae and Jun?ichi Tsujii.
2007.
DependencyParsing and Domain Adaptation with LR Modelsand Parser Ensembles.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages1044?1050, Prague, Czech Republic, June.
Associ-ation for Computational Linguistics.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: cheapand good?
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 649?652, Stroudsburg, PA,USA.
Association for Computational Linguistics.Daniel Zeman and Zdene?k Z?abokrtsky?.
2005.
Improv-ing Parsing Accuracy by Combining Diverse Depen-dency Parsers.
In In: Proceedings of the 9th Inter-national Workshop on Parsing Technologies.Yue Zhang and Stephen Clark.
2011.
Syntactic Pro-cessing Using the Generalized Perceptron and BeamSearch.
Computational Linguistics, 37(1):105?151.24
