CONTEXT-FREENESS AND THE COMPUTER PROCESSING OF HUMAN LANGUAGESGeoffrey K. PullumCowell CollegeUniversity of California, Santa CruzSanta Cruz, California 95064ABSTRACTContext-free grammars, far from having insufficientexpressive power for the description of human fan K -uages, may he overly powerful, along three dimen-sions; (i) weak generative capacity: there existsan interesting proper subset of the CFL's, theprofligate CFL's, within which no human languageappears to fall; (2) strong generative capacity:human languages can be appropr ia te ly  descr ibed  interms of a proper subset of the CF-PSG's, namelythose with the ECPO property; (3) time complexity:the recent controversy about the importance of alow deterministic polynomial time bound on therecognition problem for human languages is mis-directed, since an appropriately restrictive theorywould guarantee even more, namely a linear bound.0.
INTRODUCTIONMany computationally inclined linguists appearto think that in order to achieve adequate gr~----rsfor human languages we need a hit more power thanis offered by context-free phrase structure gram-mars (CF-PSG's), though not a whole lot more.
Inthis paper, I am concerned with the defense of amore conservative view: that even CF-PSG's shouldbe regarded as too powerful, in three computation-ally relevant respects: weak generative capacity,strong generative capacity, and time complexity ofrecognition.
All three of these matters should beof concern to theoretical linguists; the study ofwhat mathematically definable classes humanlanguages fall into does not exhaust scientificlinguistics, hut it can hardly he claimed to heirrelevant to it.
And it should be obvious thatall three issues also have some payoff in terms ofcertain computationally interesting, if ratherindirect, implications.I.
WEAK GENERATIVE CAPACITYWeak generative capacity (WGC) results are heldby some linguists (e.g.
Chomsky (1981)) to be unim-portant.
Nonetheless, they cannot be ignored bylinguists who are interested in setting their workin a context of (even potential) computationalimplementation (which, of course, some linguistsare not).
To paraphrase Montague, we might saythat linguistically (as opposed to psycholinguisti-cally) there is no important theoretical differencebetween natural languages and high-level program-ming languages.
Mediating programs (e.g.
a com-piler or interpreter), of considerable complexity,will be needed for the interpretation of computerinput in either Prolog or Japanese.
In the lattercase the level of complexity will be much higher,but the assumption is that we are talking quantita-tively, not qualitatively.
And if we are seriouslyinterested in the computational properties ofeither kind of language, we will be interested intheir language-theoretic properties, as well asproperties of the grammars that define them and theparsers that accept them.The most important language-theoretic class con-sidered by designers of programming languages, com-pilers, etc.
is the context-free languages(CFL's).
Ginsburg (1980, 7) goes so far as to sayon behalf of formal language theorists, "We live ordie on the context-free languages.")
The class ofCFL's is very rich.
Although there are simplydefinable languages well known to be non-CF,linguists often take CFL's to be non-CF in error.Several examples are cited in Pullum and Gazdar(1982).
For another example, see Dowry, Wall andPeters (1980; p.81), where exercise 3 invites thereader to prove a certain artificial language non-CF.
The exercise is impossible, for the languagei__% a CFL, as noted by William H. Baxter (personalcommunication to Gerald Gazdar).From this point on, it will he useful to be ableto refer to certain types of formal language bynames.
I shall use the terms defined in \[i) thru(3), among others.
(i) Triple Counting Languages:languages that can be mapped by a homomorphismonto some language of the form~ b n ~1 nZl~(2) String Matching Languages:languages that can be mapped by a homomorphismonto some language of the form{xxlx is in some infinite language A}(3) String Contrasti~ Languages:languages that can be mapped by a homomorphismonto some language of the form{xcy\[x and y are in some infinite language Aand x ~ y}Programming languages are virtually alwaysdesigned to be CF, except that there is a mootpoint concerning the implications of obligatoryinitial declaration of variables as in ALGOL orPascal, since if variables (identifiers) can bealphanumeric strings of arbitrary length, a syntac-tic guarantee that each variable has been declaredis tantamount to a syntax for a string matchinglanguage.
The following view seems a sensible oneto take about such cases: languages like ALGOL orPascal are CF, but not all ALGOL or Pascal programscompile or run.
Programs using undeclared vari-ables make no sense either to the compiler or tothe CPU.
But they are still programs, providedthey conform in all other ways to the syntax of thelanguage in question, just as a program whichalways goes into an infinite loop and thus nevergives any output is a program.
Aho and Ullmann(1977, 140) take such a view:the syntax of ALGOL...does not get  down tothe leve l  of characters  in a name.
Ins tead ,a l l  names are represented  by a token such asi d, and i t  i s  le f t  to the bookkeeping phase ofthe compi ler  to keep t rack  of dec la ra t ions  anduses of par t i cu la r  names.The bookkeeping has Co be done, of course, even inthe case of languages like LISP whose syntax doesnot demand a list of declarations at the start ofeach program.Var ious e f fo r ts  have been made in the l ingu is t i cl i te ra ture  to show that  some human language has anin f in i te ,  appropr ia te ly  ext rac tab le  subset  that  i sa t r ip le  count ing  language or a s t r ing  matchinglanguage.
(By appropr ia te ly  ext rac tab le  I meani so lab le  v ia  e i ther  homomorphism or in tersect ionwith a regular set.)
But all the published claimsof this sort are fallacious (Pullum and Gazdar1982).
This lends plausibility to the hypothesisthat human languages are all CF.
Stronger claimsthan this (e.g.
that human languages are regular, orfinite cardinality) have seldom seriously defended.I now want to propose one, however.I propose that human languages are never profli-gate CYL's in the sense given by the followingdefinition.
(i) A CFL is profligate if all CF-PSG'sgenerating it have nonterminal vocabulariesstrictly larger than their terminalvocabularies.
(ii) A CFL is profligate if it is the image of aprofligate language under some homomorphism.\[OPEN PROBLEM: Is profligacy decidable for anarbitrary CFL?
I conjecture that it is not, but Ihave not been able  to prove th i s .
\ ]Clearly, only an infinite CPL can be profligate,and clearly the most commonly cited infinite CFL'sare not profligate.
For instance, {!nbn~n ~ 0} isnot profligate, because it has two terminal symbolsbut there is a grammar for it that has only onenonterminal symbol, namely S. (The rules are: (S--> aSb, S --> e}.)
However, profligate CFL's doexist.
There are even regular languages that areprofligate: a simple example (due to ChristopherCuly) is (A* + ~*).More interesting is the fact that some stringcontrasting languages as defined above are profli-gate.
Consider the string contrasting language overthe vocabulary {~, k, K} where A = (A + ~)*.
Astring xcv in (~ + b)*~(~ + A)* will be in thislanguage if any one of the following is met:(a) ~ is longer than Z;(b) K is shorter than ~;(c) ~ is the same length as ~ but there is ansuch that the ith symbol of K is distinctfrom the ith symbol of ~.The interesting Condition here is (c).
The grammarhas to generate, for all ~ and for all pairs <u, v>of symbols in the terminal vocabulary, all thosestrings in (a + b)*c(a + b)* such that the ~th sym-bol is ~ and the ~th symbol after ~ is Z.
There isno bound on l, so recursion has tO be involved.But it must be recursion through a category thatpreserves a record of which symbol is cruciallygoing to be deposited at the ~th position in theterminal string and mismatched with a distinct sym-bol in the second half.
A CF-PSG that does thiscan be constructed (see Pullum and Gazdar 1982,478, for a grammar for a very similar language).But such a grammar has to use recursive nontermi-nals, one for each terminal, to carry down informa-tion about the symbol to be deposited at a certainpoint in the string.
In the language just giventhere are only two relevant terminal symbols, butif there were a thousand symbols that could appearin the ~ and ~ strings, then the vocabulary ofrecursive nonterminals would have to be increasedin proportion.
(The second clause in the defini-tion of profligacy makes it irrelevant whetherthere are other terminals in the language, like gin the language cited, that do not have to partici-pate in the recursive mechanisms just referred to.
)For a profligate CFL, the argument that a CF-PSGis a cumbersome and inelegant form of grammar mightwell have to be accepted.
A CF-PSG offers, in somecases at least, an appallingly inelegant hypothesisas to the proper description of such a language,and would be rejected by any linguist or program-mer.
The discovery that some human language isprofligate would therefore provide (for the firsttime, I claim) real grounds for a rejection of CF-PSG's on the basis of strong generative capacity(considerations of what structural descriptions areassigned to strings) as opposed to weak (whatlanguage is generated).However, no human language has been shown to bea profligate CFL.
There is one relevant argumentin the literature, found in Chomsky (1963).
Theargument is based on the nonidentity of consti-tuents allegedly required in comparative clauseconstructions like (4).
(4) She is more competent as \[a designer ofprogramming languages\] than he is as\[a designer of microchips\].Chomsky took sentences like (5) to be ungrammati-cal, and thus assumed that the nonidentity betweenthe bracketed phrases in the previous example hadto be guaranteed by the grammar.
(5) She is more competent as \[a designer ofprogramming languages\] than he is as\[a designer of programming languages|.Chomsky took this as an argument for non-CF-ness inEnglish, since he thought all string contrastinglanguages were non-CF (see Chomsky 1963, 378-379),but it can be reinterpreted as an attempt to showthat English is (at least) profligate.
(It couldeven be reconstituted as a formally valid argumentthat English was non-CF if supplemented by ademonstration that the class of phrases from whichthe bracketed sequences are drawn is not only"infinite but non-regular; of.
Zwicky and Sadock.
)However, the argument clearly collapses on empir-ical grounds.
As pointed out by Pullum and Gazdar(1982, 476-477), even Chomsky now agrees thatstrings like (5) are grammatical (though they needa contrastive context and the appropriate intona-tion to make them readily acceptable to infor-mants).
Hence these examples do not show thatthere is a homomorphism mapping English onto someprofligate string contrasting language.The interesting thing about this, if it iscorrect, is that it suggests that human languagesnot only never demand the syntactic string com-parison required by string matching languages, theynever call for syntactic string comparision overinfinite sets of strings at all, whether forsymbol-by-symbol checking of identity (which typi-cally makes the language non-CF) or for specifyinga mismatch between symbols (which may not make thelanguage non-CF, but typically makes it profli-gate).There is an important point about profligacythat" I should make at this point.
My claim thathuman languages are non-profligate entails thateach human language has at least one CF-PSG inwhich the nonterminal vocabulary has cardinalitystrictly less than the terminal vocabulary, but notthat the best granzaar to implement for it willnecessarily meet this condition.
The point isimportant, because the phrase structure grammarsemployed in natural language processing generallyhave complex nouterminals consisting of sizeablefeature bundles.
It is not uncommon for a largenatural language processing system to employ thirty .or forty binary features (or a rough equivalent interms of multi-valued features), i.e.
about as manyfeatures as are employed for phonological descript-ion by Chomsky and Halle (19681.
The GPSG systemdescribed in Gawron et al (1982) has employedfeatures on this sort of scale at all points in itsdevelopment, for example.
Thirty or forty binaryfeatures yields between a billion and a trillionlogically distinguishable nonterminals (if allvalues for each feature are compatible with allcombinations of values for all other features).Because economical techniques for rapid checking ofrelevant feature values are built into the parsersnormally used for such grammars, the size of thepotentially available nonterminal vocabulary is nota practical concern.
In principle, if the goal ofcapturing generalizations and reducing the size ofthe grammar formulation were put aside, the nonter-minal vocabulary could be vastly reduced by replac-ing rule schemata by long lists of distinct rulesexpanding the same nonterminal.Naturally, no claim has been made here that pro-fligate CFL's are computationally intractable.
NoCFL's are intractable in the theoretical sense, andintractability in practice is so closely tied todetails of particular machines and programmingenvironments as to be pointless to talk about interms divorced from actual measurements of size forgrammars, vocabularies, and address spaces.
I havebeen concerned only to point out that there is aninteresting proper subset of the infinite CFL'swithin which the human languages seem to fall.One further thing may be worth pointing out.The kind of string contrasting languages I havebeen concerned with above are strictly nondeter-ministic.
The deterministic CFL's (DCFL's) areclosed under complementation.
But the cor~ I _ntof(6) {xcvJx and ~ are in (& + ~)* and ~ # ~}in (~ + b)*E(& + ~)* is (7a), identical to (7b), astring matching language.(7)a.
{xcvl~ and ~ are in (~ + b)* and x = ~}b.
{xcx\[x is in (a + b)*}If (7a) \[=(Yb)\] is non-CF and is the complement of(6), then (6) is not a DCFL.\[OPEN PROBLEM: Are there any nonregular profligateDCFL's?\]2.
STRONG GENERATIVE CAPACITYI now turn to a claim involving strong genera-tive capacity (SGC).
In addition to claiming thathuman languages are non-profligate CFL's, I want tosuggest that every human language has a linguisti-cally adequate grammar possessing the ExhaustiveConstant Partial Ordering (ECPO) property of Gazdarand Pullum (1981).
A grammar has this property ifthere is a single partial ordering of the nontermi-hal vocabulary which no right hand side of any ruleviolates.
The ECPO CF-PSG's are a nonempty propersubset of the CF-PSG's.
The claim that humanlanguages always have ECPO CF-PSG's is a claimabout the strong generative capacity that anappropriate theory of human language should have---one of the first such claims to have been seriouslyadvanced, in fact.
It does not affect weakgenerative capacity; Shieber (1983a) proves thatevery CFL has an ECPO grammar.
It is always poss-ible to construct an ECPO grammar for any CFL ifone is willing to pay the price of inventing newnonterminals ad hoc to construct it.
The contentof the claim lies in the fact that linguists demandindependent motivation for the nonterminals theypostulate, so that the possibility of creating newones just to guarantee ECPO-ness is not always areasonable one.\[OPEN PROBLEM: Could there be a non-profligate CFLwhich had #(N) < #T (i.e.
nonterminal vocabularystrictly smaller than terminal vocabulary) for atleast one of its non-ECPO grammars, but whose ECPOgrammars always had #(N) > #(T)?\]When the linguist's criteria of evaluation arekept in mind, it is fairly clear what sort of factsin a human language would convince linguists toabandon the ECPO claim.
For example, if Englishhad PP - S" order in verb phrases (explain to him~a~ he'll have to leave) but had S" - PP order inadjectives (so that lucky for us we found you hadthe form lucky we found you for us), the grammar ofEnglish would not have the ECPO property.
But suchfacts appear not to turn up in the languages we knowabout.The ECPO claim has interesting consequencesrelating to patterns of constituent order and howthese can be described in a fully general way.
Ifa g r~r  has the ECPO property, it can be statedin what Gazdar and Pullum call ID/LP format, andthis renders numerous significant generalizationselegantly capturable.
There are also some poten-tially interesting implications for parsing, stu-died by Shieber (1983a), who shows that a modifiedEarley algorithm can be used to parse ID/LP formatgr----mrs d i rec t ly?One putative challenge to any claim that CF-PSG's can be strongly adequate descriptions forhuman languages comes from Dutch and has been d is -cussed recent ly  by Bresnan, Kaplan, Peters ,  andZaenen (1982).
Dutch has construct ions l ike(7) dat Jan Pier Marie zag leren zwemmenthat Jan Pier Marie saw teach swim"that Jan saw Pier teach Marie to swim"These seem to involve crossing dependencies over adomain of potentially arbitrary length, a confi-guration that is syntactically not expressible by aCF-PSG.
In the special case where the dependencyinvolves stringwise ~dentity, a language with thissort of structure reduces to something like {xx\[~is in ~*}, a string matching language.
However,analysis reveals that, as Bresnan et el.
accept,the actual dependencies in Dutch are not syntactic.Grammaticality of a string like (7) is not in gen-eral affected by interchanging the NP's with oneanother, since it does not matter to the ~th verbwhat the ith NP might he.
What is crucial is that(in cases with simple transitive verbs, as above)the ~th predicate (verb) takes the interpretationof the i-lth noun phrase as its argument.Strictly, this does not bear on the issue of SGC inany way that can be explicated without makingreference to semantics.
What is really at issue iswhether a CF-PSG can assign syntactic qtructures tosentences of Dutch in a way that supports semanticinterpretation.Certain recent work within the framework of gen-eralized phrase structure gran~mar suggests to methat there is a very strong probability of theanswer being yes.
One interesting development isto be found in Culy (forthcoming), where it isshown that it is possible for a CFL-inducing syntaxin ID/LP format to assign a "flat" constituentstructure to strings like Pier Marie za~ lerenzwemmen ('saw Pier teach Marie to swim'), andassign them the correct semantics.Ivan Sag, in unpublished work, has developed adifferent account, in which strings like za~ lerenzwemmen ('saw teach to swim') are treated as com-pound verbs whose semantics is only satisfied ifthey are provided with the appropriate number of NPsisters.
Whereas Culy has the syntax determine therelative numbers of NP's and verbs, Sag is explor-ing the assumption that this is unnecessary, sincethe semantic interpretation procedure can carrythis descriptive burden.
Under this view too,there is nothing about the syntax of Dutch thatmakes it non-CF, and there is not necessarily any-thing in the grammar that makes it non-ECPO.Henry Thompson "also discusses the Dutch problemfrom the GPSG standpoint (in this volume).One other interesting line of work being pursued(at Stanford, like the work of Culy and of Sag) isdue to Carl Pollard (Pollard, forthcoming, providesan introduction).
Pollard has developed a general-ization of context-free grammar which is definednot on trees but on "headed strings", i.e.
stringswith a mark indicating that one distinguished ele-ment of the string is the "head", and which com-bines constituents not only by concatenation butalso by "head wrap".
This operation is analogousto Emmon Bach's notion "right (or left) wrap" butnot equivalent to it.
It involves wrapping a con-stituent ~ around a constituent B so that the headis to the left (or right) of B and the rest of ~ isto the right (or left) of ~.
Pollard has shownthat this provides for an elegant syntactic treat-ment of the Dutch facts.
I mention his workbecause I want to return to make a point about itin the immediately following section.3.
TIME COMPLEXITY OF RECOGNITIONThe time complexity of the recognition problem(TCR) for human languages is like WGC questions inbeing decried as irrelevant by some linguists, butagain, it is hardly one that serious computationalapproaches can legitimately ignore.
Gazdar (1981)has recently reminded the linguistic community ofthis, and has been answered at great length byBerwick and Weinberg (1982).
Gazdar noted that iftransformational grammars (TG's) were stripped ofall their transformations, they became CFL-inducing, which meant that the series of worksshowing CFL's to have sub-cubic recognition timesbecame relevant to them.
gerwick and Weinberg'spaper represents a concerted eff6rt to discreditany such suggestion by insisting that (a) it isn'tonly the CFL's that have low polynomial recognitiontime results, and (b) it isn't clear that anyasymptotic recognition time results have practicalimplications for human language use (or for com-puter modelling of it).Both points should be quite uncontroversial, ofcourse, and it is only by dint of inaccurate attri-bution that Berwick and Weinberg manage to suggestthat Gazdar denies them.
However, the two pointssimply do not add up to a reason for not being con-cerned with TCR results.
Perfectly straightforwardconsiderations of theoretical restrictiveness dic-tate that if the languages recognizable in polyno-mial time are a proper subset of those recognizablein exponential time (or whatever), it is desirableto explore the hypothesis that the human languagesfall within the former class rather than just thelatter.Certainly, it is not just CFL's that have beenshown to be efficiently recognizable in determinis-tic time on a Turing machine.
Not only everycontext-free grammar but also every context-sensitive grammar that can actually be exhibitedgenerates a language that can be recognized indeterministic linear time on a two-tape Turingmachine.
It is certainly not the case that all thecontext-sensitive languages are linearly recogniz-able; it can be shown (in a highly indirect way)that there must be some that are not.
But all theexamples ever constructed generate linearly recog-nizable languages.
And it is still unknown whetherthere are CFL's not linearly recognizable.It is therefore not at all necessary that ahuman language should be a CFL in order to be effi-ciently recognizable.
But the claims about recog-nizability of CFL's do not stop at saying that bygood fortune there happens to be a fast recognitionalgorithm for each member of the class of CFL's.The claim, rather, is that there is ~ single,universal algorithm that works for every member ofthe class and has a low deterministic polynomialtime complexity.
That is what cannot be said ofthe context-sensitive languages.Nonetheless, there are well-understood classesof gr~-m-rs and automata for which it can be said.For example, Pollard, in the course of the workmentioned above, has shown that if one or other ofleft head wrap and right head wrap is permitted inthe theory of generalized context-free grammar,recognizability in deterministic time ~5 isguaranteed, and if both left head wrap and righthead wrap are allowed in gr---.-rs (with individualgr-----rs free to have either or both), then in thegeneral case the upper bound for recognition timeis ~7o These are, while not sub-cubic, still lowdeterministic polynomial time bounds.
Pollard'ssystem contrasts in this regard with the lexical-functional g ra~ar  advocated by Bresnan eta l .
,which is currently conjectured to have an NP-complete recognition problem.I remain cautious about welcoming the move thatPollard makes because as yet his non-CFL-inducingsyntactic theory does not provide an explanationfor the fact that human languages always seem toturn out to be CFL's.
It should be pointed out,however, that it is true of every grammaticaltheory that not every grammar defined as possibleis held to be likely to turn up in practice, so itis not inconceivable that the gr-----rs of humanlanguages might fall within the CFL-inducing propersubset of Pollard-style head gra=mars.Of course, another possibility is that it mightturn out that some human language ultimately pro-vides evidence of non-CY-ness, and thus of a needfor mechanisms at least as powerful as Pollard's.Bresman eta l .
mention at the end of their paperon Dutch a set of potential candidates: the socalled "free word order" or "nonconfigurational"languages, particularly Australian languages likeDyirbal and Walbiri, which can allegedly distributeelements of a phrase at random throughout a sen-tence in almost any order.
I have certain doubtsabout the interpretation of the empirical materialon these languages, but I shall not pursue chathere.
I want instead to show that, counter to thenaive intuition that wild word order would neces-sarily lead to gross parsing complexity, even ram-pantly free word order in a language does notnecessarily indicate a parsing problem that exhi-bits itself in TCR terms.Let us call transposition of adjacent terminalsymbols scrambling, and let us refer to the closureof a language ~ under scrambling as the scramble of2- The scramble of a CFL (even a regular one) canhe non-CF.
For example, the scramble of the regu-lar language (abe)* is non-CF, although (abc)*itself is regular.
(Of course, the scramble of aCFL is not always non-CF.
The scramble of a*b*c*is (~, b, !
)*, and both are regular, hence CF.
)Suppose for the sake of discussion that there is ahuman language that is closed under scrambling (orhas an appropriately extractable infinite subsetthat is).
The example just cited, the scramble of(abc)*, is a fairly clear case of the sort of thingthat might be modeled in a human language that wasclosed under scrambling.
Imagine, for example, thecase of a language in which each transitive clausehad a verb (~), a nominative noun phrase (~), andan accusative noun phrase (~), and free word orderpermitted the ~, b, and ~ from any number ofclauses to occur interspersed in any orderthroughout the sentence.
If we denote the numberof ~'s in a string Z by Nx(Z), we can say ~nat thescramble of (abc)* is (8).
(8 ){~J~ is in (~, b, &)* and N_a(~) = N b(~) = N=(~)}Attention was first drawn to this sort of languageby Bach (1981), and I shall therefore call it aBach lan~uaze.
What TCR properties does a Bachlanguage have?
The one in (8), at least, can beshown to be recognizable in linear time.
The proofis rather trivial, since it is just a corollary ofa previously known result.
Cook (1971) shows thatany language that is recognized by a two-way deter-ministic pushdown stack automaton (2DPDA) is recog-nizable in linear time on a Turing machine.
In theAppendix, I give an informal description of a 2DPDAthat will recognize the language in (81.
Giventhis, the proof that (8) is linearly recognizableis trivial.?
Thus even if my WGC and SGC conjectures werefalsified by discoveries about free word orderlanguages (which I consider that they have notbeen), there would still be no ground for tolerat-ing theories of grammar and parsing that fail toimpose a linear time bound on recognition.
Andrecent work of Shieber (1983b) shows that there areinteresting avenues in natural language parsing tobe explored using deterministic context-freeparsers that do work in linear time.In the light of the above remarks, some of thepoints made by Berwick and Weinberg look ratherpeculiar.
For example, Berwick and Weinberg argueat length that things are really so complicated inpractical implementations that a cubic bound onrecognition time might not make much difference;for short sentences a theory that only guaranteesan exponential time bound might do just as well.This is, to begin with, a very odd response to bemade by defenders of TG when confronted by atheoretically restrictive claim.
If someone madethe theoretical claim that some problem had thetime complexity of the Travelling Salesman problem,and was met by the response that real-life travel-ling salesmen do not visit very many cities beforereturning to head office, I think theoretical com-puter scientists would have a right to be amused.Likewise, it is funny to see practical implementa-tion considerations brought to bear in defending TGagainst the phrase structure backlash, when (a) noformalized version of modern TG exists, let alnebeing available for implementation, and (b) largephrase structure grammars.are being implemented oncomputers and shown to run very fast (see e.g.
Slo-cum 1983, who reports an all-paths, bottom-upparser actually running in linear time using a CF-PSG with 400 rules and i0,000 lexical entries).Berwick and Weinberg seem to imply that datapermitting a comparison of CF-PSG with TG areavailable.
This is quite untrue, as far as I know.I therefore find it nothing short of astonishing tofind Chomsky (1981, 234), taking a very similarposition, affirming that because the size of thegrammar LS a constant  fac tor  in  TCR ca lcu la t ions ,and poss ib ly  a la rge  one,The rea l  empi r i ca l  content  of ex i s t ingresu l t s .
.
,  may we l l  be that  grammars arepre fer red  i f  they are not too complex inthe i r  ru le  s t ruc ture .
I f  parsab i l i ty  i s  afac tor  in language evo lu t ion ,  we wouldexpect  i t  to p re fer  "shor t  g rammars ' - - - suchas t rans format iona l  g r - -~- rs  based on theprojection pr inc ip le  or the bindingtheory...TG's based on the "projection principle" and the'~inding theory" have yet to be formulated withsufficient explicitness for it to be determinedwhether they have a rule structure at all, letalone a simple one, and the existence of parsingalgorithms for them, of any sort whatever, has notbeen demonstrated.The rea l  reason  to re jec t  a cubic recogn i t ion -t ime guarantee  as a goal  to be a t ta ined  by syntac -t i c  theory  const ruct ion  is  not that  the quest  i spo in t less ,  but ra ther  that  i t  i s  not  near ly  ambi-t ious  enough a goa l .
Anyone who set t les  fo r  acubic  TC~ bound may be set t l ing  fo r  a theory  a lo tlaxer  than i t  could be.
(This accusat ion  would beleve l lab le  equa l ly  at  TG, lex ica l - funct iona l  gram-mar, Pollard's genera l i zed  context-free gr-----r,and genera l i zed  phrase  s t ruc ture  gr~- - , - r  ascur rent ly  conce ived . )
C loser  to what i s  ca l led  forwould be a theory  that  de f ines  human gr - , , , , - r s  assome proper  subset  of the ECPO CF-FSG's that  gen-e ra te  in f in i te ,  uonpro f l igate ,  l inear - t ime recog-n i zab le  languages.
Jus t  as the descr ip t ion  ofALGOL-60 in BNF formal ism had a ga lvan iz ing  e f fec ton theoret i ca l  computer sc ience  (Ginsburg 1980, 6-7), precise specification of a theory of this sortmight sharpen quite considerably our view of thecomputational issues involved in natural languageprocessing.
And it would simultaneously be of con-siderable l i ngu is t i c  in teres t ,  at  leas t  fo r  thosewho accept  that  we need a sharper  theory  of natura llanguage than the vague ly -out l ined  ecorat ive  nota -t ions  for  Turing machines that  are so o f ten  takenfor  theor ies  in l ingu is t i cs .ACKNOWLEDGEMENTI thank Chris Culy, Carl Pollard, Stuart Shieber,Tom Waaow, and Arnold Zwicky for useful conversa-tions and helpful comments.
The research reportedhere was in no way supported by Rewlett-Packard.ReferencesAho, A. V. and J. D. Ullmann (1977).
principles ofC~mviler Design.
Addison-Wesley.Bach, E. (1981).
Discontinuous constituents ingeneralized categorial grammars.
NELS II, 1-12.Berwick, R., and A. Weinberg (1982).
Parsing effi-ciency and the evaluation of grammaticaltheories.
L_!I 13.165-191.Bresnan, J. W.; R. M. Kaplan; S. Peters; and A.Zaenen (1982).
Cross-serial dependencies inDutch.
L_I.I13.613-635.Chomsky, N. (1963) Formal properties of grammars.In R. D. Luce, R. R. Bush, and E.
Galanter,eds., H~ndbook of Mathematical Psychology If.John Wiley.Chomsky, N. (1981).
Knowledge of language: itselements and origins.
Phil.
Trans.
of the RoyalSgc.
of Loud.
B 295, 223-234.Cook, S. A.
(1971).
Linear time simulation ofdeterministic two-way pushdown automata.Proceedinzs of  the 19711FIP Conference, 75-80.North-Holland.Culy, C. D. (forthcoming).
An extension of phrasestructure rules and an application to naturallanguage.
Stanford MA thesis, of Linguistics,Stanford University.Dowry, D. Ro; R?
Wall; and P. S. Peters (1980).Introduction t_~oMonta~ue Semantics.
D. Reidel.Gawron, J. M., et al (1982).
The GPSG linguisticsystem.
Proc.
20th Ann.
Meetin~ of ACL 74-81.Gazdar, G. (1981).
Unbounded ependenc ies  andcoordinate structure.
LI  12.155-184.Gazdar, G. and G. K. Pullum (1981).
Subcategoriza-tion, constituent order, and the notion "head'.In M. Moortgat~ H. v. d. Hulst, and T. Hoekstra(edso), Th__fie Scope of Lexical Rules, 107-123.Foris?Ginsburg, S. (1980).
Methods for specifying formallanguages--past-present-future.
In R. V.
Book,ed., Formal Lan~uaRe Theory: Perspectives and.
Qpen Problems, 1-47.
Academic Press.Pollard, C. J.
(forthcoming).
Generalizedcontext-free grammars, head grammars, andnatural language.Pullum, G. K. (1982).
Free word order and phrasestructure rules.
~ELS 12, 209-220.Pullum, G. K. and Gazdar, G. (1982).
Naturallanguages and context-free languages.
Lin~.
andPhil.
4.471-504.Shieber, S. M. (1983a).
Direct parsing of ID/LPgrn----rs.
Unpublished, SRI, Menlo Park, CA.Shieber, S. M. (1983b).
Sentence disambiguation bya shift-reduce parsing technique.
In thisvolume.Slocum, J.
(1983).
A status report on the LECmachine translation system.
Cgnf.
on AppliedNat.
Lan~.
Proc.
166-173.
ACL, Menlo Park, CA.Zwicky, A. M. and J. M. Sadock (forthcoming).
Anote on xv languages.
Submitted to Lin~.
and~l .Appendix: a 2DPDA that recognizes a Bach languageThe language {~\[~ is in (~ + ~ + ~)* and Na(x) =N_b(x) = N_.c(E)} is accepted by a 2DPDA with a singlesymbol ~ in its stack vocabulary, {~, ~, ~} asinput vocabulary, four states, and the followinginstruction set.
State I: move rightward, reading~'s, b's, and E's, and adding a ~ to the stack eachtime ~ appears on the input tape.
On encounteringright end marker in state i, go to state 2.
State2: move left, popping a ~ each time a ~ appears.On reaching left end marker in state 2 with emptystack (which will mean Na(~) = Nb(~)), go to state3.
State 3: move right, pushing a ~ on the stackfor every ~ encountered.
On reaching right endmarker in state 3, go to state 4.
State 4: moveleft, popping a ~ for each E encountered.
Onreaching left end marker in state 4 with emptystack (which will mean Na(w) = Nc(w)), accept.
