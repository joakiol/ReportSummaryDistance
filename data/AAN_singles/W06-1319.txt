Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 134?143,Sydney, July 2006. c?2006 Association for Computational LinguisticsBalancing Conicting Factors in Argument InterpretationIngrid Zukerman, Michael Niemann and Sarah GeorgeFaculty of Information TechnologyMonash UniversityClayton, VICTORIA 3800, AUSTRALIA{ingrid,niemann,sarahg}@csse.monash.edu.auAbstractWe present a probabilistic approach for theinterpretation of arguments that casts theselection of an interpretation as a modelselection task.
In selecting the best model,our formalism balances conflicting fac-tors: model complexity against data fit,and structure complexity against beliefreasonableness.
We first describe our ba-sic formalism, which considers interpreta-tions comprising inferential relations, andthen show how our formalism is extendedto suppositions that account for the beliefsin an argument, and justifications that ac-count for the inferences in an interpreta-tion.
Our evaluations with users show thatthe interpretations produced by our systemare acceptable, and that there is strong sup-port for the postulated suppositions andjustifications.1 IntroductionThe source-channel approach has been often usedfor word-based language tasks, such as speechrecognition and machine translation (Epstein,1996; Och and Ney, 2002).
According to this ap-proach, an addressee receives a noisy channel (lan-guage or speech wave), and decodes this channelto derive the source (idea).
The selected source isthat with the maximum posterior probability.In this paper, we apply the source-channel ap-proach to the interpretation of arguments.
Thisapproach enables us to cast argument interpreta-tion as a trade-off between conflicting factors, vizmodel complexity against data fit, and structurecomplexity against belief reasonableness.
Thistrade-off is inspired by the Minimum MessageLength (MML) Criterion ?
a model selectionmethod that is the basis for several machine learn-ing techniques (Wallace, 2005).
According to thistrade-off, a more complex model might fit the databetter, but the plausibility (priors) of the modelmust be taken into account to avoid over-fitting.1Our argument interpretation mechanism hasbeen implemented in a system called BIAS(Bayesian Interactive Argumentation System).BIAS presents to a user a set of facts about theworld (evidence), and the user constructs an argu-ment about a particular goal proposition in lightof this evidence.
BIAS then generates an interpre-tation of the user?s argument, i.e., it tries to un-derstand the argument.
When people try to under-stand an interlocutor?s discourse, their interpreta-tion is in terms of their own beliefs and inferencepatterns.
Likewise, our system?s interpretationsare in terms of its underlying knowledge repre-sentation ?
a Bayesian network (BN).
The inter-pretations generated by BIAS include inferencesthat connect the propositions in a user?s argument,suppositions that postulate a user?s beliefs that arenecessary to make sense of the argument, and ex-planatory extensions that justify the inferences inthe interpretation (and in the argument).
BIASdoes not generate its own arguments, rather, it in-tegrates these components to make sense of theuser?s argument.In this paper, we first describe our basic for-malism, which is used to calculate the probabilityof interpretations that include only inferences, andthen show how progressive enhancements of thisformalism are used for more informative interpre-tations.In Section 2, we explain what is an argumentinterpretation, and describe briefly the interpreta-tion process.
Next, we discuss our probabilisticformalism for selecting an interpretation, which isthe focus of this paper.
In Section 4, we present1Other model selection criteria such as Akaike Informa-tion Criterion (AIC) and Bayes Information Criterion (BIC)(Box et al, 1994) also argue for model parsimony, but theydo so by penalizing models with more free parameters.134the results of our evaluations, followed by a dis-cussion of related work, and concluding remarks.2 Argument interpretationWe define an interpretation of a user?s argument asthe tuple {SC, IG,EE}, where SC is a supposi-tion conguration, IG is an interpretation graph,and EE are explanatory extensions.?
A Supposition Configuration is a set of sup-positions attributed to the user (in addition toor instead of shared beliefs) to account for thebeliefs in his or her argument.?
An Interpretation Graph is a domain struc-ture, in our case a subnet of the domain BN,that connects the nodes mentioned in the argu-ment.
The nodes and arcs that are included inan interpretation graph but were not mentionedby the user fill in additional detail from the BN,bridging inferential leaps in the argument.?
Explanatory Extensions are domain struc-tures (subnets of the domain BN) that are addedto an interpretation graph to justify an infer-ence.
Contrary to suppositions, these explana-tions contain propositions believed by the userand the system.
The presentation of these ex-planations is motivated by the results of ourearly trials, where people objected to belief dis-continuities between the antecedents and theconsequent of inferences, i.e., increases in cer-tainty or large changes in certainty (Zukermanand George, 2005).To illustrate these components, consider the ex-ample in Figure 1.
The top segment containsa short argument, and the bottom segment con-tains its interpretation.
The middle segment con-tains an excerpt of the domain BN which in-cludes the interpretation; the probabilities of somenodes are indicated with linguistic terms.2 The in-terpretation graph, which appears inside a lightgray bubble in the BN excerpt, includes the ex-tra node GreenInGardenAtTimeOfDeath (boxed).Note that the propagated beliefs in this interpre-tation graph do not match those in the argument.To address this problem, the system supposes thatthe user believes that TimeOfDeath11=TRUE, in-stead of the BN belief of Probably (boldfaced and2We use the terms Very Probable, Probable, Possible andtheir negations, and Even Chance.
These terms, which aresimilar to those used in (Elsaesser, 1987), are most consis-tently understood by people according to our user surveys.ARGUMENTMr Greenhe                  had the opportunity to kill Mr Body, butpossiblypossiblybeing in the garden at 11 implies thatprobablyhe                 did murder Mr Body.notINTERPRETATIONHence, hesupposing that the time of death is 11Mr Green                   being in the garden at 11, andhad the opportunity to kill Mr Body, butpossiblyprobablyMr Green probably was in the garden at the time of death.implies thatpossibly notMr Green probably did not have the means.Therefore, he                  did          murder Mr Body.GreenLadderAtWindowGreenVisitBodyLastNightNbourHeardGreen&BodyGreenHadOpportunityGreenMurderedBodyArgueLastNightGreenInGardenAt11TimeOfDeath11.
.
..   .
.GreenHadMotive.
.
.EXCERPT OF DOMAIN BNProbablyProbablyNot ProbablyNot EvenChanceProbablyProbablyNotGreenHadMeansTimeOfDeathGreenInGardenAtFigure 1: Sample argument, BN excerpt and inter-pretationgray-boxed).
This fixes the mismatch between theprobabilities in the argument and those in the in-terpretation, but one problem remains: in early tri-als we found that people objected to belief discon-tinuities, such as the ?jump in belief?
from pos-sibly having opportunity to possibly not murder-ing Mr Body (this jump appears both in the origi-nal argument and in the interpretation, whose be-liefs now match those in the argument as a re-sult of the supposition).
This prompts the gen-eration of the explanatory extension GreenHad-Means[ProbablyNot] (white boldfaced and dark-gray boxed).
The three elements added during theinterpretation process ?
the extra node in the inter-pretation graph, the supposition and the explana-tory extension ?
appear in boldface italics in theinterpretation at the bottom of the figure.2.1 Proposing InterpretationsThe problem of finding the best interpretation isexponential.
In previous work, we proposed ananytime algorithm to propose interpretation graphsand supposition configurations until time runs out(George et al, 2004).
Here we apply our algorithmto generate interpretations comprising suppositionconfigurations (SC), interpretation graphs (IG)and explanatory extensions (EE) (Figure 2).Supposition configurations are proposed first, asinstantiated beliefs affect the plausibility of inter-135Algorithm GenerateInterpretations(Arg)while {there is time}{1.
Propose a supposition configuration SC thataccounts for the beliefs stated in the argument.2.
Propose an interpretation graph IG that con-nects the nodes in Arg under supposition con-figuration SC .3.
Propose explanatory extensions EE for inter-pretation graph IG under supposition config-uration SC if necessary.4.
Calculate the probability of interpretation{SC, IG,EE}.5.
Retain the top N (=6) most probable interpre-tations.
}Figure 2: Anytime algorithm for generating inter-pretationspretation graphs, which in turn affect the need forexplanatory extensions.
The proposal of supposi-tion configurations, interpretation graphs and ex-planatory extensions is driven by the probabilityof these components.
In each iteration, we gener-ate candidates for a component, calculate the prob-ability of these candidates in the context of theselections made in the previous steps, and proba-bilistically select one of these candidates.
That is,higher probability candidates have a better chanceof being selected than lower probability ones (ourselection procedures are described in George et al,2004).
For example, say that in Step 1, we selectedsupposition configuration SCa.
Next, in Step 2,the probability of candidate IGs is calculated inthe context of the domain BN and SCa, and oneof the IGs is probabilistically selected, say IGb.Similarly, in Step 3, one of the candidate EEs isselected in the context of SCa and IGb.
In the nextiteration, we probabilistically select an SC (whichcould be a previously chosen one), and so on.
Togenerate diverse interpretations, if SCa is selectedagain, a different IG will be chosen.3 Probabilistic formalismFollowing (Wallace, 2005), our approach requiresthe specification of three elements: backgroundknowledge, model and data.
Background knowl-edge is everything known to the system prior to in-terpreting a user?s argument, e.g., domain knowl-edge, shared beliefs with the user, and dialoguehistory; the data is the argument; and the modelis the interpretation.We posit that the best interpretation is that withthe highest posterior probability.IntBest = argmaxi=1,...,qPr(SCi, IGi, EEi|Arg)where q is the number of interpretations.After applying Bayes rule, this probability isrepresented as follows.3Pr(SCi, IGi, EEi|Arg) = (1)?
Pr(SCi, IGi, EEi)?Pr(Arg|SCi, IGi, EEi)where ?
is a normalizing constant that ensures thatthe probabilities of the interpretations sum to 1(?= 1?nj=1Pr(SCj ,IGj ,EEj)?Pr(Arg|SCj ,IGj ,EEj)).The first factor represents model complexity,and the second factor represents data t.?
Model complexity measures how difficult it isto produce the model (interpretation) from thebackground knowledge.
The higher/lower thecomplexity of a model, the lower/higher itsprobability.?
Data fit measures how well the data (argument)matches the model (interpretation).
The bet-ter/worse the match between the argument andan interpretation, the higher/lower the proba-bility that the speaker intended this interpreta-tion when he or she uttered the argument.Model ComplexityModel complexity is a function {B,M}?
[0, 1]that represents the prior probability of the modelM (i.e., the interpretation) in terms of the back-ground knowledge B.
The calculation of modelcomplexity depends on the type of the model: nu-merical or structural.The probability of a numerical model dependson the similarity between the numerical values (ordistributions) in the model and those in the back-ground knowledge.
The higher/lower this similar-ity, the higher/lower the probability of the model.For instance, a supposition configuration SC com-prising beliefs that differ significantly from thosein the background knowledge will lower the prob-ability of an interpretation.
One of the functionswe have used to calculate belief probabilities is theZipf distribution, where the parameter is the differ-ence between beliefs, e.g., between the supposed3In principle, Pr(SCi, IGi, EEi|Arg) can be calculateddirectly.
However, it is not clear how to incorporate the priorsof an interpretation in the direct calculation.136beliefs and the corresponding beliefs in the back-ground knowledge (Zukerman and George, 2005).That is, the probability of a supposed belief inproposition P according to model M (bel M (P )),in light of the belief in P according to backgroundknowledge B (bel B(P )), isPr(bel M (P )|bel B(P ))= ?|bel M (P )?bel B(P )|?where ?
is a normalizing constant, and ?
deter-mines the penalty assigned to the discrepancy be-tween the beliefs in P .
For example,Pr(bel M (P )=TRUE|bel B(P )=Probable) >Pr(bel M (P )=TRUE|bel B(P )=EvenChance)as TRUE is closer to Probable than to EvenChance.The probability of a structural model (e.g., aninterpretation graph) is obtained from the proba-bilities of the elements in the structure (e.g., nodesand arcs) in light of the background knowledge.The simplest calculation assumes that the proba-bility of including nodes and arcs in an interpreta-tion graph is uniform.
That is, the probability ofan interpretation graph comprising n nodes and aarcs is a function of?
the probability of n,?
the probability of selecting n particular nodesfrom N nodes in the domain BN: (Nn)?1,?
the probability of a, and?
the probability of selecting a particular arcsfrom the arcs that connect the n selected nodes.This calculation generally prefers small modelsto larger models.4Data fitData fit is a function {M,D} ?
[0, 1] that rep-resents the probability of the data D (argument)given the model M (interpretation).
This proba-bility hinges on the similarity between the modeland the data ?
the closer the data is to the model,the higher is the probability of the data.The calculation of the similarity between nu-merical data and a numerical model is the sameas the calculation of the similarity between a nu-merical model and background knowledge.The similarity between structural data and astructural model is a function of the number andtype of operations required to convert the modelinto the data, e.g., node and arc insertions and4In the rare cases where n > N/2, smaller models do notyield lower probabilities.deletions.
For the example in Figure 1, to con-vert the interpretation graph into the argument, wemust delete one node (GreenInGardenAtTimeOf-Death) and its incident arcs.
The more operationsneed to be performed, the lower the similarity be-tween the data and the model, and the lower theprobability of the data given the model.We now discuss our basic probabilistic formal-ism, which accounts for interpretation graphs, fol-lowed by two enhancements: (1) a more complexmodel that accounts for suppositions; and (2) in-creases in background knowledge that yield a pref-erence for larger interpretation graphs under cer-tain circumstances, and account for explanatoryextensions.3.1 Basic formalism: Interpretation graphsIn the basic formalism, the model contains only aninterpretation graph.
Thus, Equation 1 is simplyPr(IGi|Arg) = ?
Pr(IGi)?
Pr(Arg|IGi) (2)The difference in the calculations of modelcomplexity and data fit for numerical and struc-tural information warrants the separation of struc-ture and belief, which yieldsPr(IGi|Arg) = ?
Pr(bel IGi, struc IGi)?Pr(bel Arg, struc Arg|bel IGi, struc IGi)After applying the chain rule of probabilityPr(IGi|Arg) =?
Pr(bel IGi|struc IGi) ?
Pr(struc IGi) ?Pr(bel Arg|struc Arg, bel IGi, struc IGi)?Pr(struc Arg|bel IGi, struc IGi)Note that Pr(bel IGi|struc IGi) does not cal-culate the probability of (or belief in) the nodesin IGi.
Rather, it calculates how probable arethese beliefs in light of the structure of IGiand the expectations from the background knowl-edge.
For instance, if the belief in a node is p,it calculates the probability of p. This proba-bility depends on the closeness between the be-liefs in IGi and the expected ones.
Since thebeliefs in IGi are obtained algorithmically bymeans of Bayesian propagation from the back-ground knowledge, they match precisely the ex-pectations.
Hence, Pr(bel IGi|struc IGi) = 1.We also make the following simplifying as-sumptions for situations where the interpretationis known (given): (1) the probability of the beliefsin the argument depends only on the beliefs in the137Table 1: Probability ?
Basic formalismModel complexity (against background)?
Pr(struc IGi) ?
structural complexity(model size)Data fit with model?
Pr(struc Arg|struc IGi) ?
structural discrepancyPr(bel Arg|bel IGi) numerical discrepancyinterpretation (and not on its structure or the ar-gument?s structure), and (2) the probability of theargument structure depends only on the interpreta-tion structure (and not on its beliefs).
This yieldsPr(IGi|Arg) = ?
Pr(struc IGi)?
(3)Pr(bel Arg|bel IGi) ?
Pr(struc Arg|struc IGi)Table 1 summarizes the calculation of theseprobabilities separated according to model com-plexity and data fit.
It also shows the trade-offbetween structural model complexity and struc-tural data fit.
As seen at the start of Section 3,smaller structures generally have a lower modelcomplexity than larger ones.
However, an increasein structural model complexity (indicated by the ?next to the structural complexity and the ?
nextto the resultant probability of the model) may re-duce the structural discrepancy between the argu-ment structure and the structure of the interpreta-tion graph (indicated by the ?
next to the structuraldiscrepancy and the ?
next to the probability of thestructural data-fit).
For instance, the smallest pos-sible interpretation for the argument in Figure 1consists of a single node, but this interpretation hasa very poor data fit with the argument.3.2 A more informed modelIn order to postulate suppositions that account forthe beliefs in an argument, we expand the basicmodel to include supposition configurations (be-liefs attributed to the user in addition to or insteadof the beliefs shared with the system).
Now themodel comprises the pair {SCi, IGi}, and Equa-tion 2 becomesPr(SCi, IGi|Arg) = (4)?
Pr(SCi, IGi)?
Pr(Arg|SCi, IGi)Similar probabilistic manipulations to thoseperformed in Section 3.1 yieldPr(SCi, IGi|Arg) = (5)?
Pr(struc IGi|SCi)?Pr(SCi)?Pr(bel Arg|SCi, bel IGi)?Pr(struc Arg|struc IGi)Table 2: Probability ?
More informed modelModel complexity (against background)Pr(struc IGi|SCi) structural complexity?
Pr(SCi) ?numerical discrepancyData fit with modelPr(struc Arg|struc IGi) structural discrepancy?
Pr(bel Arg|SCi,bel IGi) ?numerical discrepancy(Recall that suppositions pertain to beliefs only,i.e., they don?t have a structural component.
)Table 2 summarizes the calculation of theseprobabilities separated according to model com-plexity and data fit (the elements that differ fromthe basic model are boldfaced).
It also shows thetrade-off between belief model complexity and be-lief data fit.
Making suppositions has a highermodel complexity (lower probability) than notmaking suppositions (where SCi matches the be-liefs in the domain BN).
However, as seen in theexample in Figure 1, making a supposition that re-duces or eliminates the discrepancy between thebeliefs in the argument and those in the interpre-tation increases the belief data-fit considerably, atthe expense of a more complex belief model.3.3 Additional background knowledgeAn increase in our background knowledge meansthat we take into account additional factors aboutthe world.
This extra knowledge in turn maycause us to prefer interpretations that were pre-viously discarded.
We have considered two ad-ditions to background knowledge: dialogue his-tory, and users?
preferences regarding inferencepatterns.Dialogue historyDialogue history influences the salience of anode, and hence the probability that it was in-cluded in a user?s argument.
We have modeledsalience by means of an activation function thatdecays with time (Anderson, 1983), and used thisfunction to moderate the probability of includinga node in an interpretation (instead of using a uni-form distribution).
We have experimented withtwo activation functions: (1) a function where thelevel of activation of a node is based on the fre-quency and recency of the direct activation of thisnode; and (2) a function where the level of activa-tion of a node depends on its similarity with all the(activated) nodes, together with the frequency andrecency of their activation (Zukerman and George,1382005).To illustrate the influence of salience, com-pare the preferred interpretation graph inFigure 1 (in the light gray bubble) withan alternative path through NbourHeard-Green&BodyArgueLastNight and GreenVisit-BodyLastNight.
The preferred path has 4 nodes,while the alternative one has 5 nodes, and hencea lower probability.
However, if the nodes in thelonger path had been recently mentioned, theirsalience could overcome the size disadvantage.Thus, although the chosen interpretation graphmay have a worse data fit than the smallest graph,it still may have the best overall probability inlight of the additional background knowledge.Inference patternsIn a formative evaluation of an earlier versionof our system, we found that people objectedto inferences that had increases in certainty orlarge changes in certainty (Zukerman and George,2005).
An example of an increase in certainty isA [Probably] implies B [VeryProbably].A large change in certainty is illustrated byA [VeryProbably] implies B [EvenChance].We then conducted another survey to deter-mine the types of inferences considered acceptableby people (from the standpoint of the beliefs inthe antecedents and the consequent).
The resultsfrom our preliminary survey prompted us to dis-tinguish between three types of inferences: Both-Sides, SameSide and AlmostSame.?
BothSides inferences have antecedents with be-liefs on both ?sides?
of the consequent (infavour and against), e.g.,A[VeryProbably] & B[ProbablyNot] impliesC[EvenChance].?
All the antecedents in SameSide inferenceshave beliefs on ?one side?
of the consequent,but at least one antecedent has the same belieflevel as the consequent, e.g.,A[VeryProbably] & B[Possibly] impliesC[Possibly].?
All the antecedents in AlmostSame inferenceshave beliefs on one side of the consequent, butthe closest antecedent is one level ?up?
fromthe consequent, e.g.,A[VeryProbably] & B[Possibly] impliesC[EvenChance].Our survey contained six evaluation sets, whichwere done by 50 people.
Each set contained an ini-tial statement (we varied the polarity of the state-ment in the various sets), three alternative argu-ments that explain this statement, and the optionto say that no argument is a good explanation.
Therespondents were asked to rank these options inorder of preference.All the evaluation sets contained one argumentthat was objectionable according to our prelim-inary survey (there was an increase in belief ora large change in belief from the antecedent tothe consequent).
The two other arguments, eachof which comprises a single inference, were dis-tributed among the six evaluation sets as follows.?
Three sets had one BothSides inference andone SameSide inference, each with two an-tecedents.?
Two sets had one SameSide inference, andone AlmostSame inference, each with two an-tecedents.?
One set had one SameSide inference with twoantecedents, and one BothSides inference com-prising three antecedents.In order to reduce the effect of the respondents?domain bias, we generated two versions of the sur-vey, where for each evaluation set we swapped theantecedent propositions in one of the inferenceswith the antecedent propositions in the other.Our survey showed that people prefer BothSidesinferences (which contain antecedents for andagainst the consequent).
They also prefer Same-Side to AlmostSame for antecedents with beliefsin the negative range (VeryProbNot, ProbNot andPossNot); and they did not distinguish betweenSameSide and AlmostSame for antecedents withbeliefs in the positive range.
Further, BothSides in-ferences with three antecedents were preferred toSameSide inferences with two antecedents.
Thisindicates that persuasiveness carries more weightthan parsimony.These general preferences are incorporated intoour background knowledge as expectations fora range of acceptable beliefs in the consequentsof inferences in light of their antecedents.
Thefarther the actual beliefs in the consequents arefrom the expectations, the lower the probabilityof these beliefs.
Hence, it is no longer true thatPr(bel IGi|SCi, struc IGi) = 1 (Section 3.1), aswe now have a belief expectation that goes beyondBayesian propagation.
As done at the start of Sec-tion 3, the probability of the beliefs in an inter-pretation is a function of the discrepancy between139these beliefs and expected beliefs.
We calculatethis probability using a variant of the Zipf distri-bution adjusted for ranges of beliefs.Explanatory extensions are added to an inter-pretation in order to overcome these belief dis-crepancies, yielding an expanded model that com-prises the tuple {SCi, IGi, EEi}.
Equation 2 nowbecomesPr(SCi, IGi, EEi|Arg) = (6)?
Pr(SCi, IGi, EEi) ?
Pr(Arg|SCi, IGi, EEi)We make simplifying assumptions similar tothose made in Section 3.1, i.e., given the interpre-tation graph and supposition configuration, the be-liefs in the argument depend only on the beliefs inthe interpretation, and the argument structure de-pends only on the interpretation structure.
Theseassumptions, together with probabilistic manipu-lations similar to those performed in Section 3.1,yieldPr(SCi, IGi, EEi|Arg) = (7)?
Pr(struc IGi|SCi)?Pr(SCi)?Pr(bel IGi|SCi, struc IGi, bel EEi, struc EEi)?Pr(struc EEi|SCi, struc IGi, bel EEi)?Pr(bel EEi|SCi, struc IGi, struc EEi)?Pr(bel Arg|SCi, bel IGi)?Pr(struc Arg|struc IGi)The calculation of the probability of an ex-planatory extension is the same as the calcu-lation for structural model complexity at thestart of Section 3.
However, the nodes inan explanatory extension are selected from thenodes directly connected to the interpretationgraph.
In addition, as for the basic model (Sec-tion 3.1), the beliefs in the nodes in explana-tory extensions are obtained algorithmically bymeans of Bayesian propagation.
Hence, thereis no discrepancy with expected beliefs, i.e.,Pr(bel EEi|SCi, struc IGi, struc EEi) = 1.Table 3 summarizes the calculation of theseprobabilities (the elements that differ from the ba-sic model and the enhanced model are boldfaced).It also shows the trade-off between structural andbelief model complexity.
Presenting explana-tory extensions has a higher structural complex-ity (lower probability) than not presenting them.However, explanatory extensions can reduce thenumerical discrepancy between the beliefs in aninterpretation and the beliefs expected from thebackground knowledge, thereby increasing the be-lief probability of the interpretation.
For instance,Table 3: Probability ?
Additional backgroundknowledgeModel complexity (against background)Pr(struc IGi|SCi) structural complexityPr(SCi) numerical discrepancy?
Pr(struc EEi|SCi,struc IGi, bel EEi) ?
structural complexity?
Pr(bel IGi|SCi, struc IGi,bel EEi, struc EEi) ?
numerical discrepancyData fit with modelPr(struc Arg|struc IGi) structural discrepancyPr(bel Arg|SCi, bel IGi) numerical discrepancyTable 4: Summary of Trade-offs?
Pr model structure (IG) ?
?
Pr struct.
data fit?
Pr model belief (SC) ?
?
Pr belief data fit?
Pr model structure (EE)?
?
Pr model beliefin the example in Figure 1, the added explanatoryextension eliminates the unacceptable jump in be-lief.Table 4 summarizes the trade-offs discussed inthis section.4 EvaluationWe evaluated separately each component of aninterpretation ?
interpretation graph, suppositionconfiguration and explanatory extensions.4.1 Interpretation graphWe prepared four evaluation sets, each of whichwas done by about 20 people (Zukerman andGeorge, 2005).
In three of the sets, the partici-pants were given a simple argument and a few can-didate interpretations (ranked highly by our sys-tem).
The fourth set featured a complex argument,and only one interpretation (other candidates hadmuch lower probabilities).
The participants wereasked to give each interpretation a score between1 (Very UNreasonable) and 5 (Very reasonable).Table 5 shows the results obtained for the inter-pretation selected by our formalism for each set,which was the top scoring interpretation.
The firstTable 5: Evaluation results: Interpretation graphSet # 1 2 3 4Avg.
score 3.38 3.68 3.35 4.00Std.
dev.
1.45 1.11 1.39 1.02Stat.
sig.
(p) 0.08 0.15 0.07 NA140row shows the average score given by our subjectsto this interpretation, the second row shows thestandard deviation, and the third row the statisticalsignificance, derived using a paired Z-test againstalternative options (no alternatives were presentedfor the fourth set).
Our results show that the inter-pretations generated by our system were generallyacceptable, but that some people gave low scores.Our subjects?
feedback indicated that these scoreswere mainly due to mismatches between beliefs inthe argument and in its interpretation, and due tobelief discontinuities.
This led to the addition ofsuppositions and explanatory extensions.4.2 Supposition configurationWe prepared four evaluation sets, each of whichwas done by 34 people (George et al, 2005).
Eachset consisted of a short argument, plus a list of sup-position options as follows: (a) four suppositionsthat had a reasonably high probability accordingto our formalism, (b) the option to make a free-form supposition in line with the domain BN, and(c) the option to suppose nothing.
We then askedour subjects to indicate which of these options wasrequired for the argument to make sense.
Specif-ically, they had to rank their preferred options inorder of preference (but they did not have to rankoptions they disliked).
Overall, there was strongsupport for the supposition preferred by our for-malism.
In three of the evaluation sets, it wasranked first by most of the trial subjects (30/34,19/34, 20/34), with no other option a clear second.Only in the fourth set, the supposition preferred byour formalism was equal-first with another option,but still was ranked first 10 times (out of 34).4.3 Explanatory extensionsWe constructed two evaluation sets, each of whichwas done by 20 people.
Each set consisted of ashort argument and two alternative interpretations(with and without explanatory extensions).
Therewas strong support for the explanatory extensionsproposed by our formalism, with 57.5% of ourtrial subjects favouring the interpretations with ex-planatory extensions, compared to 37.5% of thesubjects who preferred the interpretations withoutsuch extensions, and 5% who were indifferent.5 Related ResearchAn important aspect of discourse understandinginvolves filling in information that was omitted bythe interlocutor.
In this paper, we have presenteda probabilistic formalism that balances conflictingfactors when filling in three types of informationomitted from an argument.
Interpretation graphsfill in details in the argument?s inferences, sup-position configurations make sense of the beliefsin the argument, and explanatory extensions over-come belief discontinuities.Our approach resembles the work of Hobbs etal.
(1993) in several respects.
They employedan abductive approach where a model (interpre-tation) is inferred from evidence (sentence); theymade assumptions as necessary; and used guid-ing criteria pertaining to the model and the datafor choosing between candidate models.
There arealso significant differences between our work andtheirs.
Their interpretation focused on problems ofreference and disambiguation in single sentences,while ours focuses on a longer discourse and therelations between the propositions therein.
Thisdistinction also determines the nature of the task,as they try to find a concise model that explainsas much of the data as possible (e.g., one refer-ent that fits many clues), while we try to find arepresentation for a user?s argument.
Additionally,their domain knowledge is logic-based, while oursis Bayesian; and they used weights to apply theirhypothesis selection criteria, while our criteria areembodied in a probabilistic framework.Plan recognition systems also generate one ormore interpretations of a user?s utterances, em-ploying different resources to fill in informationomitted by the user, e.g., (Allen and Perrault,1980; Litman and Allen, 1987; Carberry and Lam-bert, 1999; Raskutti and Zukerman, 1991).
Theseplan recognition systems used a plan-based ap-proach to propose interpretations.
The first threesystems applied different types of heuristics to se-lect an interpretation, while the fourth system useda probabilistic approach moderated by heuristicsto select the interpretation with the highest prob-ability.
We use a probabilistic domain repre-sentation in the form of a BN (rather than planlibraries), and apply a probabilistic mechanismthat represents explicitly the contribution of back-ground knowledge, model complexity and data fitto the generation of an interpretation.
Our mech-anism, which can be applied to other domain rep-resentations, balances different types of complex-ities and discrepancies to select the interpretationwith the highest posterior probability.Several researchers used maximum posterior141probability as the criterion for selecting an inter-pretation (Charniak and Goldman, 1993; Gertneret al, 1998; Horvitz and Paek, 1999).
They usedBNs to represent a probability distribution over theset of possible explanations for the observed facts,and selected the explanation (a node in the BN ora value of a node) with the highest probability.
Wealso use BNs as our domain representation, but our?explanation?
of the facts (the user?s argument) isa Bayesian subnet (rather than a single node) sup-plemented by suppositions.
Additionally, we cal-culate the probability of an interpretation on thebasis of the fit between the argument and the inter-pretation, and the complexity of the interpretationin light of the background knowledge.Our work on positing suppositions is related toresearch on presuppositions (Kaplan, 1982; Gur-ney et al, 1997) ?
a type of supposition impliedby the wording of a statement.
Like our sup-positions, presuppositions are necessary to makesense of what is being said, but they operate ata different knowledge level than our suppositions.This aspect of our work is also related to researchon the recognition of flawed plans (Quilici, 1989;Pollack, 1990; Chu-Carroll and Carberry, 2000).These researchers used a plan-based approach toidentify erroneous beliefs that account for a user?sstatements or plan, while we use a probabilistic ap-proach.
Our approach supports the considerationof many possible options, and integrates supposi-tions into a broader reasoning context.Finally, the research reported in (Joshi et al,1984; van Beek, 1987; Zukerman and Mc-Conachy, 2001) considers the addition of informa-tion to planned discourse to prevent a user?s erro-neous inferences from this discourse.
Our mech-anism adds explanatory extensions to an interpre-tation to prevent inferences that are objectionabledue to discontinuities in belief.
Since such non-sequiturs may also be present in system-generatedarguments, the approach presented here may be in-corporated into argument-generation systems.6 ConclusionWe have offered a probabilistic approach to the in-terpretation of arguments that casts the selectionof an interpretation as a model selection task.
Inso doing, our formalism balances conflicting fac-tors: model complexity against data fit, and struc-ture complexity against belief reasonableness.
Wehave demonstrated the use of our basic formalismfor the selection of an interpretation graph, andshown how a more complex model and additionalbackground knowledge account respectively forthe inclusion of suppositions and explanatory ex-tensions in an interpretation.
Our user evaluationsshow that the interpretation graphs produced byour formalism are generally acceptable, and thatthere is strong support for the suppositions and ex-planatory extensions it proposes.ReferencesJ.F.
Allen and C.R.
Perrault.
1980.
Analyzing inten-tion in utterances.
Artificial Intelligence, 15(3):143?178.J.
R. Anderson.
1983.
The Architecture of Cogni-tion.
Harvard University Press, Cambridge, Mas-sachusetts.G.E.P.
Box, G.M.
Jenkins, and G.C.
Reinsel.
1994.Time Series Analysis: Forecasting and Control.Prentice Hall.S.
Carberry and L. Lambert.
1999.
A process modelfor recognizing communicative acts and modelingnegotiation subdialogues.
Computational Linguis-tics, 25(1):1?53.E.
Charniak and R. Goldman.
1993.
A Bayesianmodel of plan recognition.
Artificial Intelligence,64(1):53?79.J.
Chu-Carroll and S. Carberry.
2000.
Conflict res-olution in collaborative planning dialogues.
In-ternational Journal of Human Computer Studies,6(56):969?1015.C.
Elsaesser.
1987.
Explanation of probabilistic infer-ence for decision support systems.
In Proceedingsof the AAAI-87 Workshop on Uncertainty in Artifi-cial Intelligence, pages 394?403, Seattle, Washing-ton.M.E.
Epstein.
1996.
Statistical Source Channel Mod-els for Natural Language Understanding.
Ph.D. the-sis, Department of Computer Science, New YorkUniversity, New York, New York.S.
George, I. Zukerman, and M. Niemann.
2004.
Ananytime algorithm for interpreting arguments.
InPRICAI2004 ?
Proceedings of the Eighth PacificRim International Conference on Artificial Intelli-gence, pages 311?321, Auckland, New Zealand.S.
George, I. Zukerman, and M. Niemann.
2005.
Mod-eling suppositions in users?
arguments.
In UM05 ?Proceedings of the 10th International Conference onUser Modeling, pages 19?29, Edinburgh, Scotland.A.
Gertner, C. Conati, and K. VanLehn.
1998.
Pro-cedural help in Andes: Generating hints using a142Bayesian network student model.
In AAAI98 ?
Pro-ceedings of the Fifteenth National Conference on Ar-tificial Intelligence, pages 106?111, Madison, Wis-consin.J.
Gurney, D. Perlis, and K. Purang.
1997.
Interpretingpresuppositions using active logic: From contexts toutterances.
Computational Intelligence, 13(3):391?413.J.
R. Hobbs, M. E. Stickel, D. E. Appelt, and P. Martin.1993.
Interpretation as abduction.
Artificial Intelli-gence, 63(1-2):69?142.E.
Horvitz and T. Paek.
1999.
A computational archi-tecture for conversation.
In UM99 ?
Proceedings ofthe Seventh International Conference on User Mod-eling, pages 201?210, Banff, Canada.A.
Joshi, B. L. Webber, and R. M. Weischedel.
1984.Living up to expectations: Computing expert re-sponses.
In AAAI84 ?
Proceedings of the Fourth Na-tional Conference on Artificial Intelligence, pages169?175, Austin, Texas.S.
J. Kaplan.
1982.
Cooperative responses from aportable natural language query system.
ArtificialIntelligence, 19:165?187.D.
Litman and J.F.
Allen.
1987.
A plan recognitionmodel for subdialogues in conversation.
CognitiveScience, 11(2):163?200.F.J.
Och and H. Ney.
2002.
Discriminative training andmaximum entropy models for statistical machinetranslation.
In ACL?02 ?
Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics, pages 295?302, Philadelphia, Pennsyl-vania.M.E.
Pollack.
1990.
Plans as complex mental atti-tudes.
In P. Cohen, J. Morgan, and M.E.
Pollack, ed-itors, Intentions in Communication, pages 77?103.MIT Press.A.
Quilici.
1989.
Detecting and responding toplan-oriented misconceptions.
In A. Kobsa andW.
Wahlster, editors, User Models in Dialog Sys-tems, pages 108?132.
Springer-Verlag.B.
Raskutti and I. Zukerman.
1991.
Generation and se-lection of likely interpretations during plan recogni-tion.
User Modeling and User Adapted Interaction,1(4):323?353.P.
van Beek.
1987.
A model for generating better ex-planations.
In Proceedings of the Twenty-Fifth An-nual Meeting of the Association for ComputationalLinguistics, pages 215?220, Stanford, California.C.S.
Wallace.
2005.
Statistical and Inductive Inferenceby Minimum Message Length.
Springer, Berlin,Germany.I.
Zukerman and S. George.
2005.
A probabilisticapproach for argument interpretation.
User Model-ing and User-Adapted Interaction, Special Issue onLanguage-Based Interaction, 15(1-2):5?53.I.
Zukerman and R. McConachy.
2001.
WISHFUL:A discourse planning system that considers a user?sinferences.
Computational Intelligence, 1(17):1?61.143
