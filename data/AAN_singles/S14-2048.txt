Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 294?299,Dublin, Ireland, August 23-24, 2014.GPLSI: Supervised Sentiment Analysis in Twitter using SkipgramsJavi Fern?andez, Yoan Guti?errez, Jos?e M. G?omez, Patricio Mart?
?nez-BarcoDepartment of Software and Computing SystemsUniversity of Alicante{javifm,ygutierrez,jmgomez,patricio}@dlsi.ua.esAbstractIn this paper we describe the system sub-mitted for the SemEval 2014 Task 9 (Sen-timent Analysis in Twitter) Subtask B. Ourcontribution consists of a supervised ap-proach using machine learning techniques,which uses the terms in the dataset as fea-tures.
In this work we do not employ anyexternal knowledge and resources.
Thenovelty of our approach lies in the useof words, ngrams and skipgrams (not-adjacent ngrams) as features, and how theyare weighted.1 IntroductionThe Web 2.0 has become one of the most im-portant sources of data to extract useful and het-erogeneous knowledge from.
Texts can providefactual information, such as descriptions and listsof features, and opinion-based information, whichwould include reviews, emotions, or feelings.
Thissubjective information can be expressed throughdifferent textual genres, such as blogs, forums, so-cial networks and microblogs.An example of microblogging social network isTwitter1, which has gained much popularity in thelast years.
This website enables its users to sendand read text-based messages of up to 140 char-acters, known as tweets.
This site can be a vastsource of subjective information in real time; mil-lions of users share opinions on different aspectsof their everyday life.
Extracting this subjectiveinformation has a great value for both general andexpert users.
However, it is difficult to exploit itaccordingly, mainly because of the short length ofThis work is licensed under a Creative Commons Attribu-tion 4.0 International Licence.
Page numbers and proceed-ings footer are added by the organisers.
Licence details:creativecommons.org/licenses/by/4.0/1http://twitter.comthe tweets, the informality, and the lack of context.Sentiment Analysis (SA) systems must be adaptedto face the challenges of this new textual genre.International competitions related to the assess-ment of SA systems in Twitter have taken place.Some of them include the TASS workshop in theSEPLN conference (Villena-Rom?an et al., 2013),the RepLab workshop in the CLEF conference(Amig?o et al., 2012), and the Sentiment Analysisin Twitter task (Task 2) in the last SemEval work-shop (Nakov et al., 2013).In this paper we describe the system submit-ted for the SemEval 2014 Sentiment Analysis inTwitter task (Task 9 Subtask B)2(Rosenthal et al.,2014).
This task consists of performing an au-tomatic sentiment analysis to determine whethera message expresses a positive, negative, or neu-tral sentiment.
The organisers of this task providethree datasets: training, development training, anddevelopment test.
The participants can use thetraining and development training datasets to trainand validate their models, but the development testdataset can only be used for validation.
The sizeand distribution of polarities of these datasets isshown in Table 1.
Once their systems are ready,the participants must classify each text in the offi-cial test corpus and send the results to the organis-ers, who will perform the official evaluation.Polarity Train Dev Train Dev TestPositive 2,148 362 1,572Neutral 2,915 448 1,640Negative 836 187 601Total 5,899 997 3,813Table 1: Dataset distribution in number of tweets.The goal of the present work is to create a re-liable polarity classifier, built only from a trainingset without any external knowledge and resources.2http://alt.qcri.org/semeval2014/294Our contribution consists of a supervised approachusing machine learning techniques, which uses theterms in the dataset as features.
The novelty of ourapproach lies in the feature generation and weight-ing, using not only single words and ngrams asfeatures but also skipgrams.
This approach is de-scribed in detail in Section 3.
Subsequently, inSection 4 we show the assessment of our modelin the competition.
Finally, the conclusions andfuture work are presented in Section 5.
The fol-lowing Section 2 shows some relevant backgroundrelated to this work.2 Related WorkThe goal of Sentiment analysis (SA) is to identifythe opinions expressed in text and classify textsaccordingly (Dadvar et al., 2011).
Two main ap-proaches can be followed (Annett and Kondrak,2008; Liu, 2010; Taboada et al., 2011): lexical ap-proaches (unsupervised SA) and machine learningapproaches (supervised SA).
Lexical approachesfocus on building dictionaries and lexicons of la-belled words.
This labeling gives a score for eachword, that indicates how strong is the relation be-tween that word and each polarity.
The most com-mon way to classify a text using these scores isby adding the positive values and subtracting thenegative values of the terms in that text.
If the to-tal score is positive, that text is classified as pos-itive, otherwise it is classified as negative.
Thesedictionaries can be created manually (Stone et al.,1966) or automatically (Turney, 2002).
Examplesof lexicons are WordNet Affect (Strapparava andValitutti, 2004), SentiWordNet (Esuli and Sebas-tiani, 2006), MicroWNOP (Cerini et al., 2007) orJRC Tonality (Balahur et al., 2009).
However, itis very difficult to collect and maintain a univer-sal sentiment lexicon because different words maybe used in different domains (Qiu et al., 2009) andsome words are domain dependent (Turney, 2002).The second approach uses machine learningtechniques.
These techniques require the previouscreation of a corpus containing a set of classifiedtexts to train a classifier, which can then be appliedto classify a set of unclassified texts.
The majorityof the researches employ Support Vector Machines(Mullen and Collier, 2004; Prabowo et al., 2009;Wilson et al., 2005) or Na?
?ve Bayes (Pang and Lee,2004; Wiebe et al., 2005; Tan et al., 2009) classi-fiers because they usually obtain the best results.In this approach, texts are represented as vectorsof features, and depending on the features usedthe system can reach better results (bag-of-wordsand lexeme-based features are the more commonlyused (Pang and Lee, 2008)).
These classifiers per-form very well in the domain that they are trainedon, but their performance drops when the sameclassifier is used in a different domain (Pang andLee, 2008; Tan et al., 2009).The problem of the domain dependence is com-mon to both approaches.
When the lexicons andclassifiers generated are used in a domain differentfrom the one they were built for, they use to per-form worse (Turney, 2002; Pang and Lee, 2008;Qiu et al., 2009; Tan et al., 2009).
Creating adomain-specific lexicon or classifier means mak-ing a manual effort.
Although some studies tryto overcome this problem by generating the lexi-cons automatically (Turney, 2002), learning fromunannotated texts (Wiebe et al., 2005) or using hy-brid approaches (Andreevskaia and Bergler, 2008;Bollen et al., 2011; Zhang and Ye, 2008), a min-imal intervention from experts in the domain isneeded.
In this study we use the machine learningapproach due to the promising results obtained inprevious works (Boldrini et al., 2009; Fern?andezet al., 2011; Fern?andez et al., 2013).3 MethodologyOur contribution consists of a supervised approachusing machine learning techniques, which uses theterms in the dataset as features.
In summary, ourapproach starts making a basic normalisation ofeach tweet in the dataset (see Section 3.1).
Next,these texts are tokenised to extract their terms, andthese terms are combined to create skipgrams (seeSection 3.2).
Finally, these skipgrams are em-ployed as features for a supervised machine learn-ing algorithm (see Section 3.3).3.1 Basic normalisationWe perform a very basic normalisation, as wedo not want to lose the potential subjective infor-mation given by the not normalised original text.Each tweet in the dataset is normalised followingthese steps:1.
Lower case conversion.
All the characters inthe tweet text are converted to lower case.2.
Character repetition removal.
If the samecharacter is repeated more than 3 times, therest of repetitions are removed, so we can295still recognize if a word had repeated char-acters.
For example, the words gooood andgooooood would be normalised to goood, butthe word good would remain the same.
Weassume the ambiguity of some words like theone in the example, which can refer to thewords good and god.3.
Usernames and hashtags substitution.
Wedo not consider usernames and hashtags asthey are not usually the words that representa subjective sentence, they use to be the topicof the tweet.
They are not removed com-pletely but they are replaced by the stringsUSERNAME and HASHTAG.So excited to go to #Alicante tomorrowwith the best friend everrrrr @John!!!!
?so excited to go to #alicante tomorrowwith the best friend everrrrr @john!!!!
?so excited to go to #alicante tomorrowwith the best friend everrr @john!!!
?so excited to go to HASHTAG tomorrowwith the best friend everrr USERNAME!!
!Figure 1: Example of normalisation process.3.2 TokenisationOnce we have normalised the texts, we extract alltheir terms.
In this work, we consider a term as agroup of adjacent characters of the same type (let-ters, numbers or punctuation symbols).
For exam-ple, the text want2go!!
would be tokenised to theterms want, 2, go, and !!.
Note that we employ allthe terms extracted, not filtering any of them.Finally, we obtain the skipgrams of the terms inthe text.
Skipgrams are a technique largely used inthe field of speech processing, whereby n-gramsare formed (bigrams, trigrams, etc.)
but in addi-tion to allowing adjacent sequences of words, italso allows tokens to be skipped (Guthrie et al.,2006).
More specifically, in a k-skip-n-gram, n de-termines the maximum number of terms, and k themaximum number of skips allowed.
In this wayskipgrams are new terms that retain part of the se-quentiality of the terms, but in a more flexible waythan ngrams.
Note that a ngram can be describedas a skipgram where k = 0.
An example is shownin Figure 2.Normalised tweetso excited to go to HASHTAG tomorrowwith the best friend everrr USERNAME!!!
?Single terms(so) (excited) (to) (go) (to) (HASHTAG) (with)(the) (best) (friend) (everrr) (USERNAME) (!!!
)?Skipgrams (n = 2, k = 1)(so) (so excited) (so to) (excited) (excited to)(excited go)(to) (to go) (to to) (go) (go to) (go HASHTAG) (to)(to HASHTAG) (to with) (HASHTAG) (HASHTAGwith) (HASHTAG the) (with) (with the) (with best)(the) (the best) (the friend) (best) (best friend)(best everrr) (friend) (friend everrr) (friendUSERNAME) (everrr) (everrr USERNAME)(everrr !!!)
(USERNAME) (USERNAME !!!
)Figure 2: Example of tokenisation process.3.3 Supervised LearningTo build our model we employed Support VectorMachines (SVM) as the supervised machine learn-ing algorithm, as it has been proved to be effectiveon text categorisation tasks and robust on largefeature spaces (Sebastiani, 2002; Mohammad etal., 2013).
More specifically, we used the Weka3(Hall et al., 2009) LibSVM (Chang and Lin, 2011)implementation with the default parameters (lin-ear kernel, C = 1,  = 0.1).The skipgrams extracted in the previous step areemployed as features for the SVM.
The weight ofeach feature in each text will be calculated depend-ing on the skipgram it represents, using the for-mula in Equation 1.w(s, t) =terms(s)terms(s) + skips(s, t)(1)Wherew(s, t) represents the weight of the skip-gram s in the text t, terms is a function thatreturns the number of terms in skipgram s, andskips is a function that returns the number of skipsof the skipgram s in the text t. This formula givesmore importance to the skipgrams with a lowernumber of skips.
In the example of the Figure 2,the skipgram best friend would have a weight of2/(2 + 0) = 1, while skipgram best everrr wouldhave a weight of 2/(2 + 1) = 0.66.3http://www.cs.waikato.ac.nz/ml/weka/296Parameters P R F1 ScoreBaseline 0.630 0.604 0.580 0.447Words n = 1 0.611 0.612 0.604 0.530Ngrams n = 2 0.617 0.620 0.618 0.557n = 3 0.620 0.621 0.621 0.564n = 4 0.620 0.621 0.620 0.565n = max 0.621 0.622 0.621 0.566Skipgrams n = 2, k = 1 0.623 0.625 0.624 0.571n = 2, k = 2 0.626 0.624 0.626 0.572n = 2, k = max 0.627 0.624 0.625 0.575n = 3, k = 1 0.620 0.616 0.617 0.566n = 3, k = 2 0.625 0.614 0.618 0.564n = 3, k = max 0.636 0.588 0.599 0.544Table 2: Experiments performed and scores obtained.4 EvaluationWe performed a series of experiments employ-ing both the training corpus and the developmenttraining corpus to create our model, and the devel-opment test corpus to validate it.
We used as base-line the system presented to the workshop TASS2012 (Fern?andez et al., 2013), which also usesskipgrams and scores them depending on theirdensity but, instead of using the skipgrams as fea-tures of a machine learning model, the polarity ofeach text is determined by a combination of thescores of its skipgrams.The results of our experiments are shown in Ta-ble 2.
In this table we show the weighted precision(P), the weighted recall (R), the weighted F-score(F1) and the score obtained using the scorer toolprovided by the workshop organisers (Score).
Thenotation n = max indicates there was no limitwith the number of terms, and k = max indi-cates there was no restriction with the number ofskips.
As we can see, the presented approach out-performs the baseline proposed and the best resultsare obtained using skipgrams, specifically whenn = 2 and k = max.
These are the parametersof the system submitted to the competition.Our main observation is that incrementing thenumber of terms increases the precision of the sys-tem.
A possible explanation for this might be thatngrams/skipgrams with a greater number of wordsare more specific and representative of a givenpolarity.
In addition, using skipgrams insted ofngrams also improves the precision.
However, nosignificant differences were found between exper-iments with a different number of skips.In Table 3 we can see the official results ob-tained in the SemEval 2014 competition.
Thebest rank was obtained in the experiments with theTwitter 2014 Sarcasm dataset.Dataset Rank ScoreLive Journal 34 0.573SMS 2013 35 0.466Twitter 2013 28 0.575Twitter 2014 30 0.561Twitter 2014 Sarcasm 8 0.539Table 3: Official SemEval 2014 Subtask B results.5 ConclusionsIn this paper we described the system submittedfor the SemEval 2014 Task 9 (Sentiment Analysisin Twitter).
It consists of a supervised approachusing machine learning techniques, without em-ploying any external knowledge and resources.The novelty of our approach lies in the feature gen-eration and weighting, using not only single wordsand ngrams as features but also skipgrams.
In theexperiments performed we showed that employ-ing skipgrams instead of single words or ngramsimproves the results for these datasets.
This factsuggests that our approach is promising and en-courages us to continue with our research.As future work, we plan to find new methodsto combine the weights of the skipgrams, evaluateour approaches on different corpora and differentdomains (in order to check their robustness), andstart adding external knowledge and resources.297AcknowledgementsThis research work has been partially funded bythe University of Alicante, Generalitat Valenciana,Spanish Government and the European Com-mission through the projects, ?Tratamiento in-teligente de la informaci?on para la ayuda a la tomade decisiones?
(GRE12-44), ATTOS (TIN2012-38536-C03-03), LEGOLANG (TIN2012-31224),SAM (FP7-611312), FIRST (FP7-287607) andACOMP/2013/067.ReferencesEnrique Amig?o, Adolfo Corujo, Julio Gonzalo, EdgarMeij, and Maarten de Rijke.
2012.
Overview ofRepLab 2012: Evaluating Online Reputation Man-agement Systems.
In Conference and Labs of theEvaluation Forum (CLEF 2012).Alina Andreevskaia and Sabine Bergler.
2008.
WhenSpecialists and Generalists Work Together: Over-coming Domain Dependence in Sentiment Tagging.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics on HumanLanguage Technologies (ACL HLT 2008), pages290?298.Michelle Annett and Grzegorz Kondrak.
2008.
AComparison of Sentiment Analysis Techniques: Po-larizing Movie Blogs.
In Proceedings of the21st Canadian Conference on Artificial Intelligence(CCAI 2008), pages 25?35.Alexandra Balahur, Ralf Steinberger, Erik Van DerGoot, Bruno Pouliquen, and Mijail Kabadjov.
2009.Opinion Mining on Newspaper Quotations.
In 2009IEEE/WIC/ACM International Joint Conference onWeb Intelligence and Intelligent Agent Technology,pages 523?526.Ester Boldrini, Javier Fern?andez Mart?
?nez,Jos?e Manuel G?omez Soriano, PatricioMart?
?nez Barco, et al.
2009.
Machine learn-ing techniques for automatic opinion detection innon-traditional textual genres.Johan Bollen, Alberto Pepe, and Huina Mao.
2011.Modeling Public Mood and Emotion: Twitter Senti-ment and Socio-Economic Phenomena.
In Fifth In-ternational AAAI Conference on Weblogs and SocialMedia (ICWSM 2011).Sabrina Cerini, Valentina Compagnoni, Alice Demon-tis, Maicol Formentelli, and G Gandini.
2007.Micro-WNOp: A Gold Standard for the Evaluationof Automatically Compiled Lexical Resources forOpinion Mining.
Language resources and linguis-tic theory: Typology, second language acquisition,English linguistics, pages 200?210.Chih-chung Chang and Chih-jen Lin.
2011.
LIBSVM: A Library for Support Vector Machines.
ACMTransactions on Intelligent Systems and Technology(TIST), 2:1?39.Maral Dadvar, Claudia Hauff, and FMG De Jong.2011.
Scope of Negation Detection in SentimentAnalysis.
In Proceedings of the Dutch-Belgian In-formation Retrieval Workshop (DIR 2011), pages16?20.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A Publicly Available Lexical Resourcefor Opinion Mining.
In Proceedings of LREC, vol-ume 6, pages 417?422.Javi Fern?andez, Ester Boldrini, Jos?e M. G?omez,and Patricio Mart??nez-Barco.
2011.
Evaluat-ing EmotiBlog Robustness for Sentiment AnalysisTasks.
In Natural Language Processing and Infor-mation Systems, pages 290?294.Javi Fern?andez, Yoan Guti?errez, Jos?e M. G?omez, Patri-cio Mart?
?nez-Barco, Andr?es Montoyo, and RafaelMu?noz.
2013.
Sentiment Analysis of SpanishTweets Using a Ranking Algorithm and Skipgrams.In XXIX Congreso de la Sociedad Espa?nola deProcesamiento de Lenguaje Natural (SEPLN 2013),pages 133?142.David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,and Yorick Wilks.
2006.
A Closer Look at Skip-gram Modelling.
In 5th international Conference onLanguage Resources and Evaluation (LREC 2006),pages 1?4.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The WEKA Data Mining Software: anUpdate.
ACM SIGKDD Explorations Newsletter,11(1):10?18.Bing Liu.
2010.
Sentiment Analysis and Subjectiv-ity.
In Handbook of Natural Language Processing,pages 1?38.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In Pro-ceedings of the International Workshop on SemanticEvaluation (SemEval-2013).Tony Mullen and Nigel Collier.
2004.
Sentiment Anal-ysis using Support Vector Machines with Diverse In-formation Sources.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2004), pages 412?418.Preslav Nakov, Sara Rosenthal, Alan Ritter, andTheresa Wilson.
2013.
SemEval-2013 Task 2:Sentiment Analysis in Twitter.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval-2013), volume 2, pages 312?320.298Bo Pang and Lillian Lee.
2004.
A Sentimental Educa-tion: Sentiment Analysis Using Subjectivity Sum-marization Based on Minimum Cuts.
In Proceed-ings of the 42nd annual meeting on Association forComputational Linguistics (ACL 2004), page 271.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Rudy Prabowo, Mike Thelwall, and Wulfruna Street.2009.
Sentiment Analysis: A Combined Approach.Journal of Informetrics, 3:143?157.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding Domain Sentiment Lexiconthrough Double Propagation.
In Proceedings of the21st international Joint Conference on Artificial In-telligence (IJCAI 2009), pages 1199?1204.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval-2014).Fabrizio Sebastiani.
2002.
Machine Learning in Au-tomated Text Categorization.
ACM Computing Sur-veys (CSUR), 34(1):1?47, March.Philip J.
Stone, Dexter C. Dunphy, and Marshall S.Smith.
1966.
The General Inquirer: A ComputerApproach to Content Analysis.Carlo Strapparava and Alessandro Valitutti.
2004.WordNet Affect: an Affective Extension of Word-Net.
In LREC, volume 4, pages 1083?1086.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional Linguistics, 37(2):267?307.Songbo Tan, Xueqi Cheng, Yuefen Wang, and HongboXu.
2009.
Adapting Naive Bayes to Domain Adap-tation for Sentiment Analysis.
Advances in Informa-tion Retrieval, pages 337?349.Peter D. Turney.
2002.
Thumbs Up or ThumbsDown?
Semantic Orientation Applied to Unsuper-vised Classification of Reviews.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL 2002), pages 417?424.Julio Villena-Rom?an, Eugenio Mart?
?nez-C?amara, SaraLana-Serrano, and Jos?e Carlos Gonz?alez-Crist?obal.2013.
TASS - Workshop on Sentiment Analysisat SEPLN.
Procesamiento del Lenguaje Natural,50:37?44.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating Expressions of Opinions andEmotions in Language.
Language resources andevaluation, 39(2-3):165?210.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005.
OpinionFinder: A System for Subjec-tivity Analysis.
In Proceedings of HLT/EMNLP onInteractive Demonstrations, pages 34?35.Min Zhang and Xingyao Ye.
2008.
A GenerationModel to Unify Topic Relevance and Lexicon-basedSentiment for Opinion Retrieval.
In Proceedings ofthe 31st annual international ACM SIGIR Confer-ence on Research and Development in InformationRetrieval (SIGIR 2008), pages 411?418, New York,New York, USA.
ACM Press.299
