Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818?827,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsEncoding Relation Requirements for Relation Extraction via JointInferenceLiwei Chen1, Yansong Feng?1, Songfang Huang2, Yong Qin2and Dongyan Zhao11ICST, Peking University, Beijing, China2IBM China Research Lab, Beijing, Chinachenliwei,fengyansong,zhaodongyan@pku.edu.cnhuangsf,qinyong@cn.ibm.comAbstractMost existing relation extraction modelsmake predictions for each entity pair lo-cally and individually, while ignoring im-plicit global clues available in the knowl-edge base, sometimes leading to conflictsamong local predictions from different en-tity pairs.
In this paper, we proposea joint inference framework that utilizesthese global clues to resolve disagree-ments among local predictions.
We ex-ploit two kinds of clues to generate con-straints which can capture the implicit typeand cardinality requirements of a relation.Experimental results on three datasets, inboth English and Chinese, show that ourframework outperforms the state-of-the-art relation extraction models when suchclues are applicable to the datasets.
And,we find that the clues learnt automaticallyfrom existing knowledge bases performcomparably to those refined by human.1 IntroductionIdentifying predefined kinds of relationship be-tween pairs of entities is crucial for many knowl-edge base related applications(Suchanek et al,2013).
In the literature, relation extraction (RE) isusually investigated in a classification style, whererelations are simply treated as isolated class labels,while their definitions or background informationare sometimes ignored.
Take the relation Capitalas an example, we can imagine that this relationwill expect a country as its subject and a city asobject, and in most cases, a city can be the capitalof only one country.
All these clues are no doubthelpful, for instance, Yao et al (2010) explicitlymodeled the expected types of a relation?s argu-ments with the help of Freebase?s type taxonomyand obtained promising results for RE.
?Yansong Feng is the corresponding author.However, properly capturing and utilizing suchtyping clues are not trivial.
One of the hurdles hereis the lack of off-the-shelf resources and such cluesoften have to be coded by human experts.
Manyknowledge bases do not have a well-defined typingsystem, let alne fine-grained typing taxonomieswith corresponding type recognizers, which arecrucial to explicitly model the typing requirementsfor arguments of a relation, but rather expensiveand time-consuming to collect.
Similarly, the car-dinality requirements of arguments, e.g., a personcan have only one birthdate and a city can only belabeled as capital of one country, should be con-sidered as a strong indicator to eliminate wrongpredictions, but has to be coded manually as well.On the other hand, most previous relation ex-tractors process each entity pair (we will use en-tity pair and entity tuple exchangeably in the restof the paper) locally and individually, i.e., the ex-tractor makes decisions solely based on the sen-tences containing the current entity pair and ig-nores other related pairs, therefore has difficultiesto capture possible disagreements among differententity pairs.
However, when looking at the outputof a multi-class relation predictor globally, we caneasily find possible incorrect predictions such as auniversity locates in two different cities, two dif-ferent cities have been labeled as capital for onecountry, a country locates in a city and so on.In this paper, we will address how to derive andexploit two categories of these clues: the expectedtypes and the cardinality requirements of a rela-tion?s arguments, in the scenario of relation extrac-tion.
We propose to perform joint inference uponmultiple local predictions by leveraging implicitclues that are encoded with relation specific re-quirements and can be learnt from existing knowl-edge bases.
Specifically, the joint inference frame-work operates on the output of a sentence level re-lation extractor as input, derives 5 types of con-straints from an existing KB to implicitly capture818the expected type and cardinality requirements fora relation?s arguments, and jointly resolve the dis-agreements among candidate predictions.
We for-malize this procedure as a constrained optimiza-tion problem, which can be solved by many opti-mization frameworks.
We use integer linear pro-gramming (ILP) as the solver and evaluate ourframework on English and Chinese datasets.
Theexperimental results show that our framework per-forms better than the state-of-the-art approacheswhen such clues are applicable to the datasets.
Wealso show that the automatically learnt clues per-form comparably to those refined manually.In the rest of the paper, we first review relatedwork in Section 2, and in Section 3, we describeour framework in detail.
Experimental setup andresults are discussed in Section 4.
We concludethis paper in Section 5.2 Related WorkSince traditional supervised relation extractionmethods (Soderland et al, 1995; Zhao and Gr-ishman, 2005) require manual annotations and areoften domain-specific, nowadays many efforts fo-cus on semi-supervised or unsupervised methods(Banko et al, 2007; Fader et al, 2011).
Distantsupervision (DS) is a semi-supervised RE frame-work and has attracted many attentions (Bunescu,2007; Mintz et al, 2009; Yao et al, 2010; Sur-deanu et al, 2010; Hoffmann et al, 2011; Sur-deanu et al, 2012).
DS approaches can predictcanonicalized (predefined in KBs) relations forlarge amount of data and do not need much hu-man involvement.
Since the automatically gener-ated training datasets in DS often contain noises,there are also research efforts focusing on reduc-ing the noisy labels in the training data (Takamatsuet al, 2012).
To bridge the gaps between the rela-tions extracted from open information extractionand the canonicalized relations in KBs, Yao et al(2012) and Riedel et al (2013) propose a universalschema which is a union of KB schemas and nat-ural language patterns, making it possible to in-tegrate the unlimited set of uncanonicalized rela-tions in open settings with the relations in existingKBs.As far as we know, few works have managedto take the relation specific requirements for ar-guments into account, and most existing worksmake predictions locally and individually.
TheMultiR system allows entity tuples to have morethan one relations, but still predicts each entitytuple locally (Hoffmann et al, 2011).
Surdeanuet al (2012) propose a two-layer multi-instancemulti-label (MIML) framework to capture the de-pendencies among relations.
The first layer is amulti-class classifier making local predictions forsingle sentences, the output of which are aggre-gated by the second layer into the entity pair level.Their approach only captures relation dependen-cies, while we learn implicit relation backgroundsfrom knowledge bases, including argument typeand cardinality requirements.
Riedel et al (2013)propose to use latent vectors to estimate the pref-erences between relations and entities.
These canbe considered as the latent type information of therelations?
arguments, which is learnt from variousdata sources.
In contrast, our approach learn im-plicit clues from existing KBs, and jointly opti-mize local predictions among different entity tu-ples to capture both relation argument type cluesand cardinality clues.
Li et al (2011) and Li et al(2013) use co-occurring statistics among relationsor events to jointly improve information extrac-tion performances in ACE tasks, while we mineexisting KBs to collect global clues to solve lo-cal conflicts and find the optimal aggregation as-signments, regarding existing knowledge facts.
deLacalle and Lapata (2013) encode general domainknowledge as FOL rules in a topic model whileour instantiated constraints are directly operated inan ILP model.
Zhang et al (2013) utilize relationcardinality to create negative samples for distantsupervision while we use both implicit type cluesand relation cardinality expectations to discoverpossible inconsistencies among local predictions.3 The FrameworkOur framework takes a set of entity pairs and theirsupporting sentences as its input.
We first traina preliminary sentence level extractor which canoutput confidence scores for its predictions, e.g.,a maximum entropy or logistic regression model,and use this local extractor to produce local predic-tions.
In order to implicitly capture the expectedtype and cardinality requirements for a relation?sarguments, we derive two kinds of clues from anexisting KB, which are further utilized to discoverthe disagreements among local candidate predic-tions.
Our objective is to maximize the overallconfidence of all the selected predictions.8193.1 Generating Candidate RelationsSince we will focus on the open domain relationextraction, we still follow the distant supervisionparadigm to collect our training data guided bya KB, and train the local extractor accordingly.Specifically, we train a sentence level extractor us-ing the maximum entropy model.
Given a sen-tence containing an entity pair, the model willoutput the confidence of this sentence represent-ing certain relationship (from a predefined relationset) between the entity pair.
Formally R repre-sents the relation set we are working on, T is theset of entity tuples that we will predict in the testset.Keep in mind that our local extractor is trainedon noisy training data, which, we admit, is notfully reliable.
As we observed in a pilot experi-ment that there is a good chance that the predic-tions ranked in the second or third may still becorrect, we select top three predictions as the can-didate relations for each mention in order to intro-duce more potentially correct output.On the other hand, we should discard the pre-dictions whose confidences are too low to be true,where we set up a threshold of 0.1.
For a tuple t,we obtain its candidate relation set by combiningthe candidate relations of all its mentions, and rep-resent it as Rt.
For a candidate relation r ?
Rtanda tuple t, we define Mrtas all t?s mentions whosecandidate relations contain r. Now the confidencescore of a relation r ?
Rtbeing assigned to tuplet can be calculated as:conf(t, r) =?m?MrtMEscore(m, r) (1)where MEscore(m, r) is the confidence of mentionm representing relation r output by our prelimi-nary extractor.Traditionally, both lexical features and syntac-tic features are used in relation extraction.
Lexi-cal features are the word chains between the sub-jects and objects in the sentences, while syntacticfeatures are the dependency paths from the sub-jects to the objects on the dependency graphs ofthe supporting sentences.
However, lexical fea-tures are usually too specific to frequently appearin the test data, while the reliability of syntacticfeatures depends heavily on the quality of depen-dency parsing tools.
Generally, we expect morepotentially correct relations to be put into the can-didate relation set for further consideration.
So inconflictconflictconflictconflictCapital: 0.5LargestCity: 0.4LocationCity: 0.0511->01USA, New YorkLocationCity: 0.8FoundationPlace: 0.1511New York University, New YorkCapital: 0.95LocationCity: 0.0311->0USA, Washington D.C.Nationality: 0.7BirthPlace: 0.211Richard Fuld,USACapital: 0.3 1->0Germany, Washington D.C.conflictLocationCountry: 0.5LocationCity: 0.31->01Columbia University, New YorkconflictFigure 1: The different types of disagreements wewill investigate in the candidate relations.
Theclues of detecting these inconsistencies can belearnt from a knowledge base.addition to lexical and syntactic features, we alsouse n-gram features to train our preliminary rela-tion extraction model.
N-gram features are consid-ered as more ambiguous compared to traditionallexical and syntactic features, and may introduceincorrect predictions, thus improving the recall atthe cost of precision.3.2 Disagreements among the CandidatesThe candidate relations we obtained in the pre-vious subsection inevitably include many incor-rect predictions.
Ideally we should discard thosewrong predictions to produce more accurate re-sults.As discussed earlier, we will exploit from theknowledge base two categories of clues that im-plicitly capture relations?
backgrounds: their ex-pected argument types and argument cardinalities,based on which we can discover two categoriesof disagreements among the candidate predictions,summarized as argument type inconsistencies andviolations of arguments?
uniqueness, which havebeen rarely considered before.
We will discussthem in detail, and describe how to learn the cluesfrom a KB afterwards.Implicit Argument Types Inconsistencies:Generally, the argument types of the correctpredictions should be consistent with each other.Given a relation, its arguments sometimes arerequired to be certain types of entities.
Forexample, in Figure 1, the relation LargestCityrestricts its subject to be either countries or states,and its object to be cities.
If the predictionsamong different entity tuples require the sameentity to belong to different types, we call this820an argument type inconsistency.
Take <USA,New York> and <USA, Washington D.C.> as anexample.
In Figure 1, <USA, New York> hasa candidate relation LargestCity which restrictsUSA to be either countries or states, while <USA,Washington D.C.> has a prediction LocationCitywhich indicates a disagreement in terms of USA?stype because the latter prediction expects USA tobe an organization located in a city.
This warnsthat at least one of the two candidate relations isincorrect.The previous scenario shows that the subjectsof two candidate relations may disagree with eachother.
From Figure 1, we can observe two moresituations: the first one is that the objects ofthe two candidate relations are inconsistent witheach other, for example <New York University,New York> with the prediction LocationCity and<Columbia University, New York> with the pre-diction LocationCountry.
The second one isthat the subject of one candidate relation do notagree with another prediction?s object, for exam-ple <Richard Fuld, USA> with the prediction Na-tionality and <USA, New York> with the pre-diction LocationCity.
Although we have not as-signed explicit types to these entities, we can stillexploit the inconsistencies implicitly with the helpof shared entities.
Note that the implicit argumenttyping clues here mean whether two relations canshare arguments, but NOT enumate what types ex-plicitly their arguments should have.We formalize all the relation pairs that disagreewith each other as follows.
These relation pairscan be divided into three subcategories.
We repre-sent the relation pairs (ri, rj) that are inconsistentin terms of subjects as Csr, the relations pairs thatare inconsistent in terms of objects as Cro, the re-lation pairs that are inconsistent in terms of one?ssubject and the other one?s object as Crer.It is worth mentioning that disagreements in-side a tuple are also included here.
For instance,an entity tuple <USA, Washington D.C.> in Fig-ure 1 has two candidate relations, Capital and Lo-cationCity.
These two predictions are inconsistentwith each other with respect to the type of USA.They implicitly consider USA as ?country?
and?organization?, respectively.Violations of Arguments?
Uniqueness: Theprevious categories of disagreements are all basedon the implicit type information of the relations?arguments, Now we make use of the clues of ar-gument cardinality requirements.
Given a subject,some relations should have unique objects.
Forexample, in Figure 1, given USA as the subject ofthe relation Capital, we can only accept one pos-sible object, because there is great chance that acountry only have one capital.
On the other hand,given Washington D.C. as the object of the relationCapital, we can only accept one subject, since usu-ally a city can only be the capital of one countryor state.
If these are violating in the candidates,we could know that there may be some incorrectpredictions.
We represent the relations expectingunique objects as Cou, and the relations expectingunique subjects as Csu.3.3 Obtaining the Global CluesNow, the issue is how to obtain the clues usedin the previous subsection.
That is, how we de-termine which relations expect certain types ofsubjects, which relations expect certain types ofobjects, etc.
These knowledge can be definitelycoded by human, or learnt from a KB.Most existing knowledge bases represent theirknowledge facts in the form of (<subject, rela-tion, object>) triple, which can be seen as re-lational facts between entity tuples.
Usually thetriples in a KB are carefully defined by experts.
Itis rare to find inconsistencies among the triples inthe knowledge base.
The clues are therefore learntfrom KBs, and further refined manually if needed.Given two relations r1and r2, we query the KBfor all tuples bearing the relation r1or r2.
We useSiand Oito represent ri?s (i ?
{1, 2}) subject setand object set, respectively.
We adopt the point-wise mutual information (PMI) to estimate the de-pendency between the argument sets of two rela-tions:PMI(A,B) = logp(A,B)p(A)p(B)(2)where p(A,B) is number of the entities both inA and B, p(A) and p(B) are the numbers ofthe entities in A and B, respectively.
For anypair of relations from R ?
R, we calculate fourscores: PMI(S1, S2), PMI(O1, O2), PMI(S1, O2)and PMI(S2, O1).
To make more stable esti-mations, we set up a threshold for the PMI.
IfPMI(S1, S2) is lower than the threshold, we willconsider that r1and r2cannot share a subject.Things are similar for the other three scores.
Thethreshold is set to -3 in this paper.821We can also learn the uniqueness of argumentsfor relations.
For each pre-defined relation in R,we collect all the triples containing this relation,and count the portion of the triples which onlyhave one object for each subject, and the por-tion of the triples which only have one subjectfor each object.
The relations whose portions arehigher than the threshold will be considered tohave unique argument values.
This threshold isset to 0.8 in this paper.3.4 Integer Linear Program FormulationAs discussed above, given a set of entity pairs andtheir candidate relations output by a preliminaryextractor, our goal is to find an optimal configura-tion for all those entities pairs jointly, solving thedisagreements among those candidate predictionsand maximizing the overall confidence of the se-lected predictions.
This is an NP-hard optimiza-tion problem.
Many optimization models can beused to obtain the approximate solutions.In this paper, we propose to solve the problemby using an ILP tool, IBM ILOG Cplex1.
Firstly,for each tuple t and one of its candidate relationsr, we define a binary decision variable drtindicat-ing whether the candidate relation r is selected bythe solver.
Our objective is to maximize the totalconfidence of all the selected candidates, and theobjective function can be written as:max?t?T ,r?Rtconf(t, r)drt+?
?t,r?Rt,m?MrtmaxMEscore(m, r)drtwhere conf(t, r) is the confidence of the tuple tbearing the candidate relation r. The first compo-nent is the sum of the original confidence scores ofall the selected candidates, and the second one isthe sum of the maximal mention-level confidencescores of all the selected candidates.
The latter isdesigned to encourage the model to select the can-didates with higher individual mention-level con-fidence scores.We add the constraints with respect to the dis-agreements described in Section 3.2.
For the sakeof clarity, we describe the constraints derived fromeach scenario of the two categories of disagree-ments separately.The subject-relation constraints avoid the dis-agreements between the predictions of two tuples1www.cplex.comsharing a subject.
These constraints can be repre-sented as:drtiti+ drtjtj?
1 (3)?ti, tj: subj(ti) = subj(tj) ?
(rti, rtj) ?
Csrwhere tiand tjare two tuples in T , subj(ti) is thesubject of ti, rtiis a candidate relation of ti, rtjisa candidate relation of tj.The object-relation constraints avoid the incon-sistencies between the predictions of two tuplessharing an object.
Formally we add the followingconstraints:drtiti+ drtjtj?
1 (4)?ti, tj: obj(ti) = obj(tj) ?
(rti, rtj) ?
Crowhere ti?
T and tj?
T are two tuples, obj(ti)is the object of ti.The relation-entity-relation constraints ensurethat if an entity works as subject and object in twotuples tiand tjrespectively, their relations agreewith each other.
The constraints we add are:drtiti+ drtjtj?
1 (5)?ti, tj: obj(ti) = subj(tj) ?
(rti, rtj) ?
CrerThe object uniqueness constraints ensure thatthe relations requiring unique objects do not bearmore than one object given a subject.?t?Tuple(r),subj(t)=edrt?
1 (6)?e ?
r ?
Couwhere e is an entity, Tuple(r) are the tuples whosecandidate relations contain r.The subject uniqueness constraints ensure thatgiven an object, the relations expecting uniquesubjects do not bear more than one subject.?t?Tuple(r),obj(t)=edrt?
1 (7)?e ?
r ?
CsuBy adopting ILP, we can combine the localinformation including MaxEnt confidence scoresand the implicit relation backgrounds that are em-bedded into global consistencies of the entity tu-ples together.
After the optimization problem issolved, we will obtain a list of selected candidaterelations for each tuple, which will be our finaloutput.8224 Experiments4.1 DatasetsWe evaluate our approach on three datasets, in-cluding two English datasets and one Chinesedataset.The first English dataset, Riedel?s dataset, is theone used in (Riedel et al, 2010; Hoffmann et al,2011; Surdeanu et al, 2012), with the same split.It uses Freebase as the knowledge base and NewYork Time corpus as the text corpus, includingabout 60,000 entity tuples in the training set, andabout 90,000 entity tuples in the testing set.We generate the second English dataset, DB-pedia dataset, by mapping the triples in DBpedia(Bizer et al, 2009) to the sentences in New YorkTime corpus.
We map 51 different relations to thecorpus and result in about 50,000 entity tuples,134,000 sentences for training and 30,000 entitytuples, 53,000 sentences for testing.For the Chinese dataset, we derive knowledgefacts and construct a Chinese KB from the In-foboxes of HudongBaike, one of the largest Chi-nese online encyclopedias.
We collect four na-tional economic newspapers in 2009 as our corpus.28 different relations are mapped to the corpus andthis results in 60,000 entity tuples, 120,000 sen-tences for training and 40,000 tuples, 83,000 sen-tences for testing.4.2 Baselines and CompetitorsThe baseline we use in this paper is Mintz++,which is described in (Surdeanu et al, 2012).
Itis a modification of the model proposed by Mintzet al (2009).
The model predicts for each mentionseparately, and allows multi-label outputs for anentity tuple by OR-ing the outputs of its mentions.As we described in Section 3.1, originally weselect the top three predicted relations as the can-didates for each mention.
In order to investigatewhether it is necessary to use up to three candi-dates, we implement two variants of our approach,which select the top one and top two relations ascandidates for each mention, and represented asILP-1cand and ILP-2cand, respectively.We also use two distant supervision approachesfor the comparison.
The first one is MultiR (Hoff-mann et al, 2011), a novel joint model that candeal with the relation overlap issue.
The secondone, MIML-RE (Surdeanu et al, 2012), is one ofthe state-of-the-art MIML relation extraction sys-tems.
We tune the models of MultiR and MIML-RE so that they fit our datasets.4.3 Overall PerformanceFirst we compare our framework and its vari-ants with the baseline and the state-of-the-art REmodels.
Following previous works, we use thePrecision-Recall curve as the evaluation criterionin our experiment.
The results are summarizedin Figure 2.
For the constraints, we first manu-ally select an average of 20 relation pairs for eachsubcategory of the first kind of clues, and all therelations with unique argument values in R. Wealso show how automatically learnt clues performin Section 4.5.Figure 2 shows that compared with the baseline,our framework performs consistently better in theDBpedia dataset and the Chinese dataset.
Mintz++proves to be a strong baseline on both datasets.
Ittends to result in a high recall, and its weakness oflow precision is perfectly fixed by the ILP model.Our ILP model and its variants all outperformMintz++ in precision in both datasets, indicatingthat our approach helps filter out incorrect predic-tions from the output of MaxEnt model.
Com-pared with MultiR, our framework obtains betterresults in both datasets.
Especially in the Chinesedataset, the improvement in precision reaches ashigh as 10-16% at the same recall points.
Ourframework performs better compared to MIML-RE in the English dataset.
On the Chinese dataset,our framework outperforms MIML-RE except inthe low-recall portion (<10%) of the P-R curve.All these results show that embedding the relationbackground information into RE can help elim-inate the wrong predictions and improve the re-sults.However, in the Riedel?s dataset, Mintz++, theMaxEnt relation extractor, does not perform well,and our framework cannot improve its perfor-mance.
In order to find out the reasons, we manu-ally investigate the dataset.
The top three relationsof this dataset are /location/location/contains,/people/person/nationality and/people/person/place lived.
About two-thirds ofthe entity tuples belongs to these three relations,and the outputs of the local extractor usuallybias even more to the large relations.
What isworse, we cannot find any clues from the topthree relations because their arguments?
types aretoo general.
Things are similar for many other8230 0.1 0.2 0.3 0.4 0.50.20.40.60.81recallprecisionMintz++ILP?1candILP?2candILP(a) The DBpedia Dataset0 0.1 0.2 0.3 0.4 0.5 0.600.20.40.60.81recallprecisionMintz++ILP?1candILP?2candILP(b) The Riedel?s Dataset0 0.1 0.2 0.3 0.4 0.5 0.60.30.40.50.60.70.80.91recallprecisionMintz++ILP?1candILP?2candILP(c) The Chinese Dataset0 0.1 0.2 0.3 0.4 0.50.20.40.60.81recallprecisionMintz++MultiRMIML?REILP(d) The DBpedia Dataset0 0.1 0.2 0.3 0.4 0.5 0.600.20.40.60.81recallprecisionMintz++MultiRMIML?REILP(e) The Riedel?s Dataset0 0.1 0.2 0.3 0.4 0.5 0.60.30.40.50.60.70.80.91recallprecisionMintz++MultiRMIML?REILP(f) The Chinese DatasetFigure 2: Overall performances of our framework and its variants, the baselines and the state-of-the-artapproaches on the three datasets.relations in this dataset.
Although we may findsome clues any way, they are too few to makeany improvement.
Hence, our framework doesnot perform well due to the poor performance ofMaxEnt extractor and the lack of clues.
To solvethis problem, we think of addressing the selectionpreferences between relations and entities pro-posed in (Riedel et al, 2013), which should beour future work.We notice that in all three datasets our variantILP-1cand is shorter than Mintz++ in recall, in-dicating we may incorrectly discard some predic-tions.
Compared to ILP-2cand and original ILP,ILP-1cand leads to slightly lower precision butmuch lower recall, showing that selecting morecandidates may help us collect more potentiallycorrect predictions.
Comparing ILP-2cand andoriginal ILP, the latter hardly makes any improve-ment in precision, but is slightly longer in re-call, indicating using three candidates can still col-lect some more potentially correct predictions, al-though the number may be limited.In order to study how our framework improvesthe performances on the DBpedia dataset and theChinese dataset, we further investigate the num-ber of incorrect predictions eliminated by ILP andthe number of incorrect predictions corrected byILP.
We also examine the number of correct pre-Table 1: Details of the improvements made by ILPin the DBpedia and Chinese datasets.Datasets Incorrect Predictions Wrong Predictioins Correct PredictionsEliminated Corrected Newly IntroducedDBpedia 268 61 1426Chinese 1506 14 283dictions newly introduce by ILP, which were NAin Mintz++.
We summarize the results in Table 1.The results show that our framework can reducethe incorrect predictions and introduce more cor-rect predictions at the same time.
We also findan interesting results: in the DBpedia dataset, ILPis more likely to introduce correct predictions tothe results, while in the Chinese dataset it tends toreduce more incorrect predictions, which may becaused by the differences between performancesof Mintz++ on the two datasets, where it gets ahigher recall on the Chinese dataset.Following Surdeanu et al (2012), we also listthe peak F1 score (highest F1 score) for eachmodel in Table 2.
Different from (Surdeanu et al,2012), we use all the entity pairs instead of theones with more than 10 mentions.
We can observethat our model obtains the best performance in theDBpedia dataset and the Chinese dataset.
In theDBpedia dataset, it is 3.6% higher than Mintz++,8247.9% higher than MIML-RE and 13.9% higherthan MultiR.
In the Chinese dataset, Mintz++,MultiR and MIML-RE performs similarly in termsof the highest F1 score, while our model gainsabout 8% improvement.
In the Riedel?s dataset,our framework hardly obtains any improvementcompared with Mintz++.We also investigate the impacts of the con-straints used in ILP, which are derived based on thetwo kinds of clues and can encode relation defini-tion information into our framework.
Experimen-tal results in Table 2 shows that in the DBpediadataset, the highest F1 score increases from 35.2%to 38.3% with the help of both kinds of clues,while in the Chinese dataset the improvement isfrom 44.4% to 52.8%.
In the Riedel?s dataset wedo not see any improvements since there are al-most no clues.
Furthermore, using constraints de-rived from only one kind of clues can also improvethe performance, but not as well as using both ofthem.4.4 Adapting MultiR Sentence LevelExtractor to Our FrameworkThe preliminary relation extractor of our optimiza-tion framework is not limited to the MaxEnt ex-tractor, and can take any sentence level relationextractor with confidence scores.
We also fit Mul-tiR?s mention level extractor into our framework.As shown in Figure 3, in the DBpedia datasetand the Chinese dataset, in most parts of the curve,ILP optimized MultiR outperforms original Mul-tiR.
We think the reason is that our frameworkmake use of global clues to discard the incorrectpredictions.
The results are not as high as whenwe use MaxEnt as the preliminary extractor.
Wethink one reason is that MultiR does not performwell in these two datasets.
Furthermore, the confi-dence scores which MultiR outputs are not nor-malized to the same scale, which brings us dif-ficulties in setting up a confidence threshold toselect the candidates.
As a result, we only usethe top one result as the candidate since includingtop two predictions without thresholding the confi-dences performs bad, indicating that a probabilis-tic sentence-level extractor is more suitable for ourframework.
We also notice that in the Riedel?sdataset our framework does not improve the per-formance significantly, and we have discussed thereasons in Section 4.3.
( a)( b)Figure 4: F1 score v.s.
number of relations (usedto introduce the related learnt clues into the ILPframework) on the DBpedia dataset (a) and theChinese dataset (b).00.20.40.60.20.40.60.81recallprecisionManual Auto(a) The DBpedia Dataset00.20.40.60.20.40.60.81recallprecisionManual Auto(b) The Chinese DatasetFigure 5: Performances of manually selected cluesand automatically learnt clues on two datasets.4.5 Examining the Automatically LearntCluesNow we evaluate the performance of automati-cally collected clues used in our model.
Sincethere are almost no clues in the Riedel?s dataset,we only investigate the other two datasets.
We addclues according to their related relations?
propor-tions in the local predictions.
For example, Coun-try and birthPlace take up about 30% in the localpredictions, we thus add clues that are related tothese two relations, and then move on with newclues related to other relations according to thoserelations?
proportions in the local predictions.As is shown in Figure 4, in both datasets, theclues related to more local predictions will solvemore inconsistencies, thus are more effective.Adding the first two relations improves the modelsignificantly, and as more relations are added, the825Table 2: Results of the highest F1 score on all three datasets.DBpedia Riedel ChineseMethod P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)Mintz++ 40.2 30.5 34.7 35.3 23.2 27.9 43.3 45.7 44.4MultiR 60.4 15.3 24.4 32.3 25.1 28.2 53.5 38.2 44.6MIML-RE 51.3 21.6 30.4 41.5 19.9 26.9 49.2 41.3 44.9ILP 37.4 39.2 38.3 35.5 23.2 28.0 52.6 52.9 52.8ILP-No-Constraint 34.1 36.3 35.2 35.3 23.2 28.0 43.3 45.7 44.4ILP-Type-Inconsistent 36.3 39.2 37.7 35.5 23.2 28.0 49.5 49.0 49.2ILP-Cardinality 35.3 37.8 36.5 35.4 23.2 28.0 50.3 48.8 49.60 0.05 0.1 0.150.60.70.80.91recallprecisionILP Optimized MultiROriginal MultiR(a) The DBpedia Dataset0 0.05 0.1 0.15 0.2 0.25 0.30.30.40.50.60.70.80.91recallprecisionILP Optimized MultiROriginal MultiR(b) The Riedel?s Dataset0 0.1 0.2 0.3 0.40.50.60.70.80.91recallprecisionILP Optimized MultiROriginal MultiR(c) The Chinese DatasetFigure 3: The results of original MultiR and ILP optimized MultiR on the three datasets.performances keep increasing until approachingthe still state.
It is worth mentioning that whensufficient learnt clues are added into the model, theresults are comparable to those based on the cluesrefined manually, as shown in Figure 5.
This indi-cates that the clues can be collected automatically,and further used to examine whether predicted re-lations are consistent with the existing ones in theKB, which can be considered as a form of qualitycontrol.5 ConclusionsIn this paper, we make use of the global clues de-rived from KB to help resolve the disagreementsamong local relation predictions, thus reduce theincorrect predictions and improve the performanceof relation extraction.
Two kinds of clues, includ-ing implicit argument type information and argu-ment cardinality information of relations are in-vestigated.
Our framework outperforms the state-of-the-art models if we can find such clues in theKB.
Furthermore, our framework is scalable forother local sentence level extractors in addition tothe MaxEnt model.
Finally, we show that the cluescan be learnt automatically from the KB, and leadto comparable performance to manually refinedones.For future work, we will investigate other kindsof clues and attempt a joint optimization frame-work that could host entity disambiguation, rela-tion extraction and entity linking together.
Wewill also adopt selection preference between en-tities and relations since sometimes we may notfind useful clues.AcknowledgmentsWe would like to thank Heng Ji, Dong Wang andKun Xu for their useful discussions and the anony-mous reviewers for their helpful comments whichgreatly improved the work.
This work was sup-ported by the National High Technology R&DProgram of China (Grant No.
2012AA011101),National Natural Science Foundation of China(Grant No.
61272344, 61202233, 61370055) andthe joint project with IBM Research.ReferencesMichele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In Pro-ceedings of IJCAI, IJCAI?07, pages 2670?2676.Christian Bizer, Jens Lehmann, Georgi Kobilarov,S?oren Auer, Christian Becker, Richard Cyganiak,826and Sebastian Hellmann.
2009.
Dbpedia - a crys-tallization point for the web of data.
Web Semant.,7:154?165, September.Razvan C. Bunescu.
2007.
Learning to extract rela-tions from the web using minimal supervision.
InProceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL07.Oier Lopez de Lacalle and Mirella Lapata.
2013.
Un-supervised relation extraction with general domainknowledge.
In EMNLP, pages 415?425.
ACL.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1535?1545, Stroudsburg, PA,USA.
Association for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceedingsof the 49th ACL-HLT - Volume 1, HLT ?11, pages541?550, Stroudsburg, PA, USA.
ACL.Qi Li, Sam Anzaroot, Wen-Pin Lin, Xiang Li, andHeng Ji.
2011.
Joint inference for cross-documentinformation extraction.
In Proceedings of the 20thACM International Conference on Information andKnowledge Management, CIKM ?11, pages 2225?2228, New York, NY, USA.
ACM.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In ACL, pages 73?82.
The Association forComputer Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th IJCNLP of the AFNLP: Volume 2 -Volume 2, ACL ?09, pages 1003?1011.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Machine Learning and Knowl-edge Discovery in Databases, volume 6323 of Lec-ture Notes in Computer Science, pages 148?163.Springer Berlin / Heidelberg.Sebastian Riedel, Limin Yao, Benjamin M. Marlin, andAndrew McCallum.
2013.
Relation extraction withmatrix factorization and universal schemas.
In JointHuman Language Technology Conference/AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT-NAACL?13), June.Stephen Soderland, David Fisher, Jonathan Aseltine,and Wendy Lehnert.
1995.
Crystal inducing a con-ceptual dictionary.
In Proceedings of the 14th IJCAI- Volume 2, IJCAI?95, pages 1314?1319, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Fabian Suchanek, James Fan, Raphael Hoffmann, Se-bastian Riedel, and Partha Pratim Talukdar.
2013.Advances in automated knowledge base construc-tion.
In SIGMOD Records journal, March.Mihai Surdeanu, David McClosky, Julie Tibshirani,John Bauer, Angel X. Chang, Valentin I. Spitkovsky,and Christopher D. Manning.
2010.
A simple dis-tant supervision approach for the TAC-KBP slot fill-ing task.
In Proceedings of the Third Text Anal-ysis Conference (TAC 2010), Gaithersburg, Mary-land, USA, November.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-ati, and Christopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.In EMNLP-CoNLL, pages 455?465.
ACL.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervi-sion for relation extraction.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers - Volume 1, ACL?12, pages 721?729, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Collective cross-document relation extractionwithout labelled data.
In Proceedings of EMNLP,EMNLP ?10, pages 1013?1023, Stroudsburg, PA,USA.
ACL.Limin Yao, Sebastian Riedel, and Andrew McCal-lum.
2012.
Probabilistic databases of universalschema.
In Proceedings of the Joint Workshop onAutomatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ?12,pages 116?121, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Xingxing Zhang, Jianwen Zhang, Junyu Zeng, JunYan, Zheng Chen, and Zhifang Sui.
2013.
Towardsaccurate distant supervision for relational facts ex-traction.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 810?815, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 419?426, Stroudsburg, PA, USA.Association for Computational Linguistics.827
