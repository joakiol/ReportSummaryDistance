Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 258?268,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLiberal Event Extraction and Event Schema InductionLifu Huang1, Taylor Cassidy2, Xiaocheng Feng3,Heng Ji1, Clare R. Voss2, Jiawei Han4, Avirup Sil51Rensselaer Polytechnic Institute,2US Army Research Lab,3Harbin Institute of Technology,4Univerisity of Illinois at Urbana-Champaign,5IBM T.J. Watson Research Center1{huangl7,jih}@rpi.edu,2{taylor.cassidy.civ,clare.r.voss.civ}@rpi.edu,3xcfeng@ir.hit.edu.cn,4hanj@illinois.edu,5avi@us.ibm.comAbstractWe propose a brand new ?Liberal?
EventExtraction paradigm to extract events anddiscover event schemas from any inputcorpus simultaneously.
We incorporatesymbolic (e.g., Abstract Meaning Repre-sentation) and distributional semantics todetect and represent event structures andadopt a joint typing framework to simulta-neously extract event types and argumentroles and discover an event schema.
Ex-periments on general and specific domainsdemonstrate that this framework can con-struct high-quality schemas with manyevent and argument role types, covering ahigh proportion of event types and argu-ment roles in manually defined schemas.We show that extraction performance us-ing discovered schemas is comparable tosupervised models trained from a largeamount of data labeled according to pre-defined event types.
The extraction qualityof new event types is also promising.1 IntroductionEvent extraction aims at identifying and typ-ing trigger words and participants (arguments).It remains a challenging and costly task.
Thefirst question is what to extract?
The TIP-STER (Onyshkevych et al, 1993), MUC (Grish-man and Sundheim, 1996), CoNLL (Tjong et al,2003; Pradhan et al, 2011), ACE1and TAC-KBP (Ji and Grishman, 2011) programs found thatit was feasible to manually define an event schemabased on the needs of potential users.
An ACEevent schema example is shown in Figure 1.
Thisprocess is very expensive because consumers and1http://www.itl.nist.gov/iad/mig/tests/ace/expert linguists need to examine a lot of data be-fore specifying the types of events and argumentroles and writing detailed annotation guidelinesfor each type in the schema.
Manually-definedevent schemas often provide low coverage and failto generalize to new domains.
For example, noneof the aforementioned programs include ?dona-tion?
and ?evacuation?
in their schema in spite oftheir potential relevance to users.In this paper we propose Liberal Event Extrac-tion, a new paradigm to take humans out of theloop and enable systems to extract events in a moreliberal fashion.
It automatically discovers a com-plete event schema, customized for a specific inputcorpus.
Figure 1 compares the ACE event extrac-tion paradigm and our proposed Liberal event ex-traction paradigm.We use the following examples to explain andmotivate our approach, where event triggers are inbold and arguments are in italics and underlined:E1.
Two Soldiers were killed and one injured in theclose-quarters fighting in Kut.E2.
Bill Bennet?s glam gambling loss changed myopinion.E3.
Gen. Vincent Brooks announced the captureof Barzan Ibrahim Hasan al-Tikriti, telling re-porters he was an adviser to Saddam.E4.
This was the Italian ship that was captured byPalestinian terrorists back in 1985.E5.
Ayman Sabawi Ibrahim was arrested in Tikritand was sentenced to life in prison.We seek to cluster the event triggers and eventarguments so that each cluster represents a type.We rely on distributional similarity for our clus-tering distance metric.
The distributional hypoth-esis (Harris, 1954) states that words often occur-ring in similar contexts tend to have similar mean-ings.
We formulate the following distributional258Traditional Event ExtractionConflict LifeAttack Marry Die ?
Injure?Type:Subtype:Argument:DemonstrateEntity Time Place?
?Agent Victim Time?Guidelines DocumentsSen 1: The Indian army stated that 4 Islamic militantswere killed in 2 separate gun battles 20021228.Sen 2: The embassy stated the British government isopposed to the death penalty in all circumstances.Event : killed, Type: Die,  Arguments: 4 Islamic militants (Victim)NullLinguistic Resource DocumentsSen 1: The Indian army stated that 4 Islamic militants were killed in 2 separate gun battles 20021228.Sen 2: The embassy stated the British government is opposed to the death penalty in all circumstances.Event 2: killed, Type: Kill,  Arguments: 4 Islamic militants (Victim)Event 1: stated, Type: State, Arguments: embassy (Agent), opposed (Topic)Event 1: stated, Type: State,  Arguments: Indian army (Agent), killed (Topic)Event 3: battles, Type: Battle,  Arguments: 4 Islamic militants (Agent), 20021228 (Time)Event 2: opposed, Type: Oppose, Arguments: British government (Patient), death penalty (Theme)Attack ImprisonBattle ?
Demand StateType:Trigger Cluster:Arguments:Agent Time Place?
?
Agent Patient Topic Time Place?Opposeattackstrikehitbombs ?imprisonprisonerssentence??
demandurgepressured ??
?
antiopposed ?MannerLiberal Event ExtractionFigure 1: Comparison between ACE Event Extraction and Liberal Event Extraction.hypotheses specifically for event extraction, anddevelop our approach accordingly.Hypothesis 1: Event triggers that occur in sim-ilar contexts and share the same sense tend to havesimilar types.Following the distributional hypothesis, whenwe simply learn general word embeddings froma large corpus for each word, we obtain similarwords like those shown in Table 1.
We can seesimilar words, such as those centered around ?in-jure?
and ?fight?, are converging to similar types.However, for words with multiple senses such as?fire?
(shooting or employment termination), simi-lar words may indicate multiple event types.
Thus,we propose to apply Word Sense Disambiguation(WSD) and learn a distinct embedding for eachsense (Section 2.3).injure Score fight Score fire Scoreinjures 0.602 fighting 0.792 fires 0.686hurt 0.593 fights 0.762 aim 0.683harm 0.592 battle 0.702 enemy 0.601maim 0.571 fought 0.636 grenades 0.597injuring 0.561 Fight 0.610 bombs 0.585endanger 0.543 battles 0.590 blast 0.566dislocate 0.529 Fighting 0.588 burning 0.562kill 0.527 bout 0.570 smoke 0.558Table 1: Top-8 Most Similar Words (in 3 Clusters)Hypothesis 2: Beyond the lexical semantics ofa particular event trigger, its type is also depen-dent on its arguments and their roles, as well asother words contextually connected to the trigger.For example, in E4, the fact that the patient roleis a vehicle (?Italian ship?
), and not a person (asin E3 and E5), suggests that the event trigger ?cap-tured?
has type ?Transfer-Ownership?
as opposedto ?Arrest?.
In E2, we know the ?loss?
event oc-curs in a gambling scenario, so we can determineits type as loss of money, not loss of life.We therefore propose to enrich each trigger?srepresentation by incorporating the distributionalrepresentations of various words in the trigger?scontext.
Not all context words are relevant to eventtrigger type prediction, while those that are vary intheir predictive value.
We propose to use seman-tic relations, derived from a meaning representa-tion for the text, to carefully select arguments andother words in an event trigger?s context.
Thesewords are then incorporated into a ?global?
eventstructure for a trigger mention.
We rely on seman-tic relations to (1) specify how the distributionalsemantics of relevant context words contribute tothe overall event structure representation; (2) de-termine the order in which distributional semanticsof relevant context words are incorporated into theevent structure (Section 2.4).2 Approach2.1 OverviewInput DocumentsFrameNetLexical UnitsCandidate Trigger &Argument IdentificationEvent Schema & Event Extraction ResultsAMR ParsingEvent Structure SemanticComposition & RepresentationUnlabeledCorpusWord SenseDisambiguationDistributional SemanticRepresentationWord Sense based Triggerand Argument RepresentationJoint Trigger and Argument ClusteringEvent Type NamingArgument Role NamingAMR/PropBank/FrameNet/VerbNet/OntoNotes RoleDescriptionsFigure 2: Liberal Event Extraction Overview.Figure 2 illustrates the overall framework of259Liberal Event Extraction.
Given a set of input doc-uments, we first extract semantic relations, applyWSD and learn word sense embeddings.
Next, weidentify candidate triggers and arguments.For each event trigger, we apply a series of com-positional functions to generate that trigger?s eventstructure representation.
Each function is specificto a semantic relation, and operates over vectors inthe embedding space.
Argument representationsare generated as a by-product.Trigger and argument representations are thenpassed to a joint constraint clustering framework.Finally, we name each cluster of triggers, andname each trigger?s arguments using mappings be-tween the meaning representation and semanticrole descriptions in FrameNet, VerbNet (Kipper etal., 2008) and Propbank (Palmer et al, 2005).We compare settings in which semantic re-lations connecting triggers to context wordsare derived from three meaning representations:Abstract Meaning Representation (AMR) (Ba-narescu et al, 2013), Stanford Typed Depen-dencies (Marie-Catherine et al, 2006), andFrameNet (Baker and Sato, 2003).
We derive se-mantic relations automatically for these three rep-resentations using CAMR (Wang et al, 2015a),Stanford?s dependency parser (Manning, 2003),and SEMAFOR (Das et al, 2014), respectively.2.2 Candidate Trigger and ArgumentIdentificationGiven a sentence, we consider all noun and verbconcepts that are assigned an OntoNotes (Hovy etal., 2006) sense by WSD as candidate event trig-gers.
Any remaining concepts that match both averbal and a nominal lexical unit in the FrameNetcorpus are considered candidate event triggers aswell.
This mainly helps to identify more nominaltriggers like ?pickpocket?
and ?sin?.2For each candidate event trigger, we consideras candidate arguments all concepts for which oneof a manually-selected set of semantic relationsholds between it and the event trigger.
For thesetting in which AMR serves as our meaning rep-resentation, we selected a subset of all AMR rela-tions that specify event arguments, as shown in Ta-ble 2.
Note that some AMR relations generally donot specify event arguments, e.g.
?mode?, whichcan indicate sentence illocutionary force, or ?snt?2For consistency, we use the same trigger identificationprocedure regardless of which meaning representation is usedto derive semantic relations.which is used to combine multiple sentences intoone AMR graph.3When FrameNet is the mean-ing representation we allow all frame relations toidentify arguments.
For dependencies, we manu-ally mapped dependency relations to AMR rela-tions and use Table 2.Categories RelationsCore roles ARG0, ARG1, ARG2, ARG3, ARG4Non-core roles mod, location, poss, manner, topic,medium, instrument, duration, prep-XTemporal year, duration, decade, weekday, timeSpatial destination, path, locationTable 2: Event-Related AMR Relations.In E1, for example, ?killed?, ?injured?
and?fighting?
are identified as candidate triggers, andthree concept sets are identified as candidate argu-ments using AMR relations: ?
{Two Soldiers, verylarge missile}?, ?
{one, Kut}?
and ?
{Two Soldiers,Kut}?, as shown in Figure 3.2.3 Trigger Sense and ArgumentRepresentationBased on Hypothesis 1, we learn sense-based em-beddings from a large data set, using the Con-tinuous Skip-gram model (Mikolov et al, 2013).Specifically, we first apply WSD to link each wordto its sense in WordNet using a state-of-the-arttool (Zhong and Ng, 2010), and map WordNetsense output to OntoNotes senses.4We map eachtrigger candidate to its OntoNotes sense and learna distinct embedding for each sense.
We use gen-eral lexical embeddings for arguments.2.4 Event Structure Composition andRepresentationBased on Hypothesis 2, we aim to exploit linguis-tic knowledge to incorporate inter-dependenciesbetween event and argument role types into ourevent structure representation.
Many meaningrepresentations could provide such informationto some degree.
We illustrate our method forbuilding event structures using semantic relationsfrom meaning representations using AMR.
In Sec-tion 3.4 we compare results using Stanford TypedDependencies and FrameNet in place of AMR.Let?s take E2 as an example.
Based on AMRannotation and Table 2, we extract semantically re-3For relation details, see https://github.com/amrisi/amr-guidelines/blob/master/amr.md4WordNet-OntoNotes mapping fromhttps://catalog.ldc.upenn.edu/LDC2011T03260E1:  Two Soldiers were killed by a very large missile and one injured in the close-quarters fighting in Kut.
[Event:Die] [Event:Injure] [Event:Attack]PlacePlace PlaceVictimVictimAttackerInstrumentkill-01:ARG1 :instrumentinjure-01 fight-01 :location:ARG0Figure 3: Event Trigger and Argument Annotations and AMR Parsing Results of E1.exchange for 15000 U.S. dollars.
Event: ship,    Arguments: man(Agent), Austrialia(Destination),heroin(Theme)S2: State media didn?t identify the 2 convicts hanged in Zahedan but statedthat they had been found guilty of transporting 5.25 kilograms of heroin.Event: transporting,   Arguments: they(Agent), heroin(Theme)S1: The construction of the facility started in 790000, but stopped afterthe 910000 Soviet collapse when Tajikistan slid into a 5 year civil warthat undermined its economy.
Event:construction,  Arguments: facility(Product), 790000(Time)S2: The closed Soviet-era military facility was fou-nded in 570000 andcollects and analyzes all information gathered from Russia's military spysatellites.
Event: founded,    Arguments: Soviet-era facility(Product), 570000(Time)Event Type: Buildsentenced him to death in 1997.Event: death,   Arguments: him(Theme), 1997(Time)S2: A newspaper report on January 1, 2008 that Iran hanged twoconvicted drug traffickers in the south-eastern city of Zahedan.S1: Colombian Government was alarmed because uranium is theprimary basis for generating weapons of mass destruction.Event:alarmed, Arguments:Columbian Government(Experiencer)Event Type: ThreatenS2: Cluster bomblets have been criticized by human rights groupsbecause they kill indiscriminately and because unexplodedordinance poses a threat to civilians similar to that of land mines.Event:threat,  Arguments:ordinance(Cause), civilian(Experiencer)Event: hanged,   Arguments: Iran(Agent), drug traf-fickers(Theme), southeastern city of Zahedan(Place)losegambleglamBill Bennet:op1 :op2 :mod:mod:possZ1=fmod(Wmod,Xga,Yl)=XTgaWmodYl+bReconstruct: (X?ga,Y?l)=Z1W?mod+b?Z2=fmod(Wmod,Xgl,Z1)Z4=fposs(Wposs,Z3,Z2)Reconstruct: (Z?3,Z?2)=Z4W?poss+b?Z3=Avarage(VBill, VBennet) Z1Z2Z4X?gamble Y?loseZ?3 Z?2Reconstruct: (X?gl,Z?1)=Z2W?mod+b?X?glam Z?1AMR annotationEvent Structure RepresentationEvent Structure for ?lose?
:instance:mod:mod:poss:op1 :op2Bill Bennetglamgamble lose:ARG0 (x8 / lose-1:poss (x3 / person:name (n1 / name:op1 "Bill":op2 "Bennet")):mod (x6 / glam):mod (x7 / gamble-01))Figure 4: Partial AMR and Event Structure for E2.lated words for the event trigger with sense ?lose-1?
and construct the event structure for the wholeevent, as shown in Figure 4.We design a Tensor based Recursive Auto-Encoder (TRAE) (Socher et al, 2011) frameworkto utilize a tensor based composition function foreach of a subset of the AMR semantic relationsand compose the event structure representationbased on multiple functional applications.
Thissubset was manually selected by the authors as theset of relations that link a trigger to concepts thathelp to determine its type.
Similarly, we selected asubset of dependency and FrameNet relations us-ing the same criteria for experiments using thosemeaning representations.Figure 4 shows an instance of a TRAE appliedto an event structure to generate its representation.For each semantic relation type r, such as ?
:mod?,we define the output of a tensor product Z via thefollowing vectorized notation:Z = fmod(X,Y,W[1:d]r, b) = [X;Y ]TW[1:d]r[X;Y ] + bwhere Wmod?
R2d?2d?dis a 3-order tensor, andX,Y ?
Rdare two input word vectors.
b ?
Rdisthe bias term.
[X;Y ] denotes the concatenation oftwo vectors X and Y .
Each slice of the tensor actsas a coefficient matrix for one entry Ziin Z:Zi= fmod(X,Y,W[i]r, b) = [X;Y ]TW[i]r[X;Y ] + biWe use the statistical mean to compose thewords connected by ?:op?
relations (e.g.
?Bill?and ?Bennet?
in Figure 4).After composing the vectors ofX and Y , we ap-ply an element-wise sigmoid activation function tothe composed vector and generate the hidden layerrepresentations Z.
One way to optimize Z is to tryto reconstruct the vectors X and Y by generatingX?and Y?fromZ, and minimizing the reconstruc-tion errors between the input VI= [X,Y ] and out-put layers VO= [X?, Y?].
The error is computedbased on Euclidean distance function:E(VI, VO) =12||VI?
VO||2For each pair of words X and Y , the recon-struction error back-propagates from its outputlayer to input layer through parameters ?r=(W?r, b?r,Wr, br).
Let ?Obe the residual error ofthe output layer, and ?Hbe the error of the hiddenlayer:?O= ?(VI?
VO) ?
f?sigmoid(VOH)?H= (d?k=1?kO?
(W?kr+ (W?kr)T) ?
VOH) ?
f?sigmoid(VIH)where VIHand VOHdenote the input and outputof the hidden layer, and VOH= Z. W?kris the kthslice of tensor W?r.To minimize the reconstruction errors, we uti-lize gradient descent to iteratively update parame-ters ?r:?E(?r)?W?kr= ?kO?
(VOH)T?
VOH?E(?r)?b?r= ?(VI?
VO) ?
f?sigmoid(VOH)?E(?r)?Wkr= ?kH?
(VI)T?
VI?E(?r)?br= (d?k=1?kO?
(W?kr+ (W?kr)T) ?
VOH) ?
f?sigmoid(VIH)After computing the composition vector of Z1based on X and Y , for the next layer, it com-poses Z1and another new word vector such as261Xgl.
For each type of relation r, we randomlysample 2,000 pairs to train optimized parameters?r.
For each event structure tree, we iterativelyrepeat the same steps for each layer.
For multiplearguments at each layer, we compose them in theorder of their distance to the trigger: the closestargument is composed first.2.5 Joint Trigger and Argument ClusteringBased on the representation vectors generatedabove, we compute the similarity between eachpair of triggers and arguments, and cluster theminto types.
Recall that a trigger?s arguments areidentified as in section 2.2.
We observe that, fortwo triggers t1and t2, if their arguments have thesame type and role, then they are more likely tobelong to the same type, and vice versa.
Thereforewe introduce a constraint function f , to enforceinter-dependent triggers and arguments to have co-herent types:f(P1,P2) = log(1 +|L1?
L2||L1?
L2|)where P1and P2are triggers.
Elements ofLiare pairs of the form (r, id(a)), where id(a)is the cluster ID for argument a that stands inrelation r to Pi.
For example, let P1and P2betriggers ?capture?
and ?arrested?
(c.f.
Figure 5).If Barzan Ibrahim Hasan al-Tikriti and AymanSabawi Ibrahim share the same cluster ID, the pair(arg1, id(Barzan Ibrahim Hasan al-Tikriti)) willbe a member of L1?
L2.
This argument overlapis evidence that ?capture?
and ?arrested?
have thesame type.
We define f where Piare arguments,and elements Liare defined analogously to above.capturecaptured arrestedsentencedBarzan IbrahimHasan al-TikritiTikritAyman SabawiIbrahimPalestinianterroristsprisonItalian ship:arg1:arg1:location:arg1:arg0 :arg1:locationFigure 5: Joint Constraint Clustering for E3,4,5.Given a trigger set T and their corresponding ar-gument set A, we compute the similarity betweentwo triggers t1and t2and two arguments a1anda2by:sim(t1, t2) = ?
?
simcos(Et1g, Et2g)+(1?
?)
?
?r?Rt1?Rt2simcos(Et1r, Et2r)|Rt1?Rt2|+ f(t1, t2)sim(a1, a2) = simcos(Ea1g, Ea2g) + f(a1, a2)where Etgrepresents the trigger sense vector andEagis the argument vector.
Rtis the AMR re-lation set in the event structure of t, and Etrde-notes the vector resulting from the last applicationof the compositional function corresponding to thesemantic relation r for trigger t. ?
is a regulariza-tion parameter that controls the trade-off betweenthese two types of representations.
In our experi-ment ?
= 0.6.We design a joint constraint clustering ap-proach, which iteratively produces new clusteringresults based on the above constraints.
To find aglobal optimum, which corresponds to an approx-imately optimal partition of the trigger set into Kclusters CT= {CT1, CT2, ..., CTK}, and a partition of theargument set intoM clusters CA= {CA1, CA2, ..., CAM},we minimize the agreement across clusters and thedisagreement within clusters:arg minKT,KA,?O = (DTinter+DTintra) + (DAinter+DAintra)DPinter=K?i6=j=1?u?CPi,v?CPjsim(Pu,Pv)DPintra=K?i=1?u,v?CPi(1?
sim(Pu,Pv))We incorporate the Spectral Clustering algo-rithm (Luxburg, 2007) into joint constraint clus-tering process to get the final optimized clusteringresults.
The detailed algorithm is summarized inAlgorithm 1.2.6 Event Type and Argument Role NamingFor each trigger cluster, we utilize the triggerwhich is nearest to the centroid of the cluster asthe event type name.
For a given event trigger, weassign a role name to each of its arguments (iden-tified as in section 2.2).
This process depends onwhich meaning representation was used to selectthe arguments.For AMR, we first map the event trigger?sOntoNotes sense to PropBank, VerbNet, andFrameNet.
We assign each argument a rolename as follows.
We map AMR core roles(e.g.
?
:ARG0?, ?ARG1?)
to FrameNet if possi-ble, otherwise to VerbNet if possible, and finallyto PropBank roles if a mapping to VerbNet is notavailable.5.
Nearly 5% of AMR core roles can5OntoNotes 5.0 provides a mapping;https://catalog.ldc.upenn.edu/LDC2013T19262Algorithm 1 Joint Constraint Clustering AlgorithmInput: Trigger set T , argument set A, their lexical em-bedding ETg, EAg, event structure representation ETR, and theminimal (KminT, KminA) and maximal (KmaxT, KmaxA) num-ber of clusters for triggers and arguments;Output: The optimal clustering results: CTand CA;?
Omin=?, CT= ?, CA= ??
For KT= KminTto KT= KmaxT, KA= KminAtoKA= KmaxA?
Clustering with Spectral Clustering Algorithm:?
CTcurr= spectral(T,ETg, ETR,KT)?
CAcurr= spectral(A,EAg,KA)?
Ocurr= O(CTcurr, CAcurr)?
if Ocurr< Omin?
Omin= Ocurr, CT= CTcurr, CA= CAcurr?
while iterate time ?
10?
CTcurr= spectral(T,ETg, ETR,KT, CAcurr)?
CAcurr= spectral(A,EAg,KA, CTcurr)?
Ocurr= O(CTcurr, CAcurr)?
if Ocurr< Omin?
Omin=Ocurr, CT= CTcurr, CA= CAcurr?
return Omin, CT, CA;be mapped to FrameNet roles and 55% can bemapped to VerbNet roles, and the remaining canbe mapped to PropBank.
Table 3 shows somemapping examples.
We map non-core roles fromAMR to FrameNet, as shown in Table 4.When Stanford Typed Dependencies are usedfor meaning representation we construct a manualmapping AMR relations and use the above proce-dure.
When FrameNet is used for meaning repre-sentation we simply keep the FrameNet role namefor argument role naming.Concept AMRCoreRoleFrameNetRoleVerbNetRolePropBankDescriptionfire.1 ARG0 Agent Agent Shooterfire.1 ARG1 Projectile Theme Gun/projectileextrude.1 ARG0 Agent Extruder, agentextrude.1 ARG1 Theme Entity extrudedextrude.1 ARG2 Source Extruded fromblood.1 ARG0 Agentblood.1 ARG1 Theme, one bledTable 3: Core Role Mapping Examples.3 Evaluation3.1 DataWe used the August 11, 2014 English Wikipediadump to learn trigger sense and argument embed-dings.
For evaluation we choose a subset of ERE(Entity Relation Event) corpus (50 documents)which has perfect AMR annotations so we canAMR None-Core Role FrameNet Roletopic Topicinstrument Instrumentmanner Mannerposs Possessorprep-for, prep-to, prep-on-behalf Purposetime, decade, year, weekday, duration Timemod, cause, prep-as Explanationprep-by, medium, path Meanslocation, destination, prep-in PlaceTable 4: None-Core Role Mapping.compare the impact of perfect AMR and systemgenerated AMR.
To compare with state-of-the-artevent extraction on Automatic Content Extraction(ACE2005) data, we follow the same evaluationsetting in previous work (Ji and Grishman, 2008;Liao and Grishman, 2010; Hong et al, 2011) anduse 40 newswire documents as our test set.3.2 Schema DiscoveryFigure 6 shows some examples as part of the eventschema discovered from the ERE data set.
Eachcluster denotes an event type, with a set of eventmentions and sentences.
Each event mention isalso associated with some arguments and theirroles.
The event and argument role annotationsfor sample sentences may serve as an example-based corpus-customized ?annotation guideline?for event extraction.Table 5 compares the coverage of event schemadiscovered by our approach, using AMR as mean-ing representation, with the predefined ACE andERE event schemas.
Besides the types defined inACE and ERE, this approach discovers many newevent types such as Build and Threaten as dis-played in Figure 6.
Our approach can also discovernew argument roles for a given event type.
Forexample, for Attack events, besides five types ofexisting arguments (Attacker, Target, Instrument,Time, and Place) defined in ACE, we also dis-cover a new type of argument Purpose.
For ex-ample, in ?The Dutch government, facing strongpublic anti-war pressure, said it would not com-mit fighting forces to the war against Iraq butadded it supported the military campaign to dis-arm Saddam.
?, ?disarm Saddam?
is identified asthe Purpose for the Attack event triggered by?campaign?.
Note that while FrameNet specifiesPurpose as an argument role for the Attack, suchinformation specific to Attack is not part of AMR.263S1: The court official stated that on 18 March 2008 Luong stated to judgesthat she was hired by an unidentified man to ship the heroin to Australia inexchange for 15000 U.S. dollars.
Event: ship,    Arguments: man(Agent), Austrialia(Destination),heroin(Theme)S2: State media didn?t identify the 2 convicts hanged in Zahedan but statedthat they had been found guilty of transporting 5.25 kilograms of heroin.Event: transporting,   Arguments: they(Agent), heroin(Theme)Event Type: TransportS1: The construction of the facility started in 790000, but stopped afterthe 910000 Soviet collapse when Tajikistan slid into a 5 year civil warthat undermined its economy.
Event:construction,  Arguments: facility(Product), 790000(Time)S2: The closed Soviet-era military facility was fou-nded in 570000 andcollects and analyzes all information gathered from Russia's military spysatellites.
Event: founded,    Arguments: Soviet-era facility(Product), 570000(Time)Event Type: BuildEvent Type: DieS1: Police in the strict communist country discovered his metha-mphetamine manufacturing plant disguised as a soap factory andsentenced him to death in 1997.Event: death,   Arguments: him(Theme), 1997(Time)S2: A newspaper report on January 1, 2008 that Iran hanged twoconvicted drug traffickers in the south-eastern city of Zahedan.S1: Colombian Government was alarmed because uranium is theprimary basis for generating weapons of mass destruction.Event:alarmed, Arguments:Columbian Government(Experiencer)Event Type: ThreatenS2: Cluster bomblets have been criticized by human rights groupsbecause they kill indiscriminately and because unexplodedordinance poses a threat to civilians similar to that of land mines.Event:threat,  Arguments:ordinance(Cause), civilian(Experiencer)Event: hanged,   Arguments: Iran(Agent), drug traf-fickers(Theme), southeastern city of Zahedan(Place)S1: Ras acts as a molecular switch that is activated upon GTP loading anddeactivated upon hydrolysis of GTP to GDP.Event: hydrolysis   Arguments: GTP (Patient), GDP (Result)Event Type: DissociateS2: Activation requires dissociation of protein-bound GDP , an intrinsica-lly slow process that is accelerated by guanine nucleotide exchange factors.Event: dissociation   Arguments: GDP (Patient)S3: His - ubiquitinated proteins were purified by Co2+ metal affinitychromatography in 8M urea denaturing conditions.Event: denaturing  Arguments: proteins(Patient)Figure 6: Example Output of the Event Schema.DataACE EREHuman SystemAMR Overlap Human PerfectAMR Overlap SystemAMR Overlap# of Events 440 2,395 331 580 3,765 517 2,498 477# of Event Types 33 134 N/A 26 137 N/A 120 N/A# of Arguments 883 4,361 587 1,231 6,195 919 4,288 801Table 5: Schema Coverage Co parison on ACE and ERE.3.3 Event Extraction for All TypesTo evaluate the performance of the whole eventschema, we randomly sample 100 sentences fromERE data set and ask two linguistic experts tofully annotate the events and arguments.
As astarting point, annotators were given output fromour Schema Discovery using gold standard AMR.For each sentence, they saw event triggers andcorresponding arguments.
Their job was to cor-rect this output by marking incorrectly identifiedevents and arguments, and adding missing eventsand arguments.
The inter-annotator agreement is83% for triggers and 79% for arguments.To evaluate trigger and argument identification,we automatically compare this gold standard withsystem output (see Table 6).
To evaluate trig-ger and argument typing, annotators manuallychecked system output and assessed whether thetype name was reasonable (see Table 6).
Note thatautomatic comparison between system and goldstandard output is not appropriate for typing; fora given cluster, there is no definitive ?best?
name.We found that most event triggers not recov-ered by our system are multi-word expressionssuch as ?took office?
or adverbs such as ?previ-ously?
and ?formerly?.
For argument identifica-tion, our approach fails to identify some argumentsthat require world knowledge to extract.
For ex-ample, in ?Anti-corruption judge Saul Pena statedMontesinos has admitted to the abuse of authoritycharge?, ?Saul Pena?
is not identified as a Adju-dicator argument of event ?charge?
because it hasno direct semantic relations with the event trigger.3.4 Impact of Semantic Information andMeaning RepresentationsTable 7 assesses the impact of various types ofsemantic information, and also compares the ef-fectiveness of each type of meaning representationfor the typing task only.
We note that F-measuredrops 14.4 points if only WSD based embeddingsare not used.
In addition, AMR relations specify-ing both core and non-core roles are informativefor learning distinct compositional operators.
Tocompare typing results across meaning representa-tions, we use triggers identified by both the AMRand FrameNet parsers.
Using Stanford Typed De-pendencies, relations are likely too coarse-grainedor lack sufficient semantic information.
Thus, ourapproach cannot leverage the inter-dependencybetween event trigger type and argument role toachieve pure trigger clusters.
Compared with de-pendency relations, the fine-grained AMR seman-tic relations such as :location, :manner, :topic, :in-strument appear to be more informative to inferthe argument roles.
For example, in sentence ?Ap-proximately 25 kilometers southwest of Sringar2 militants were killed in a second gun battle.?,?gun?
is identified as an Instrument for ?battle?event based on the AMR relation :instrument.
Incontrast, dependency parsing identifies ?gun?
as a264MethodTrigger Identification (%) Trigger Typing (%) Arg Identification (%) Arg Typing (%)P R F1P R F1P R F1P R F1Perfect AMR 87.0 98.7 92.5 70.0 79.5 74.5 94.0 83.7 88.6 72.4 64.4 68.2System AMR 93.0 67.2 78.0 69.8 50.5 58.6 95.7 59.6 73.4 68.9 42.9 52.9Table 6: Overall Performance of Liberal Event Extraction on ERE data for All Event Types.MethodTrigger F1(%) Arg F1(%)P R F1P R F1Perfect AMR 70 79.5 74.5 72.4 64.4 68.2w/o StructureRepresentation52.8 59.4 55.9 52.1 48.0 50.0w/o WSD basedembeddings62.8 57.4 60.1 61.9 50.3 55.5w/o None-Core Roles 61.5 72.2 66.5 61.3 58.0 59.6w/o Core Roles 57.3 49.7 53.2 63.6 49.5 55.7System AMR 69.8 50.5 58.6 68.9 42.9 52.9Replace AMR withDependency Parsing45.9 61.9 52.7 63.9 18.2 28.4Replace AMR withFrameNet Parsing43.1 57.1 49.2 78.1 7.1 13.0Table 7: Impact of semantic information and rep-resentations on typing for ERE data.compound modifier of ?battle?.
Note that we useda static mapping to map dependency relations toAMR relations (see section 2.6), whereas ideallythis mapping would be context-dependent.
Creat-ing a context-dependent mapping would constitutesignificant steps toward building an AMR parser.Using FrameNet results in low recall for argu-ment typing.
SEMAFOR?s output often does notidentify all the arguments identified by our annota-tors.
Many triggers are associated with zero or oneargument, thus there is not enough data to learn theevent structure representation.
In addition, most ofthe arguments from identified by SEMAFOR arelong phrases.
Because no internal structure is as-signed, we simply average all single token?s vec-tors to represent the phrase.
However, the highprecision may be due to the fact that FrameNet re-lations are designed to specify semantic roles.3.5 Event Extraction for ACE/ERE TypesWe manually select the event triggers in the ACEand ERE evaluation sets discovered by our AMR-based approaches that are ACE/ERE events basedon their annotation guidelines.
If a trigger doesn?talready have a gold standard ACE/ERE annota-tion we provide one.
For each such event we usecore roles and Instrument/Possessor/Time/Placerelations to detect arguments.
Each trigger andargument role type is assessed manually if anACE/ERE annotation does not exist.
We evalu-ate our approach for trigger and argument typingby comparing system output to manual annota-tion, considering synonymous labels to be equiva-lent (e.g., our approach?s kill type ACE?s die).
Wecompare our approach with the following state-of-the-art supervised methods which are trained from529 ACE documents or 336 ERE documents:?
DMCNN: A dynamic multi-pooling convolu-tional neural network based on distributed wordrepresentations (Chen et al, 2015).?
Joint: A structured perceptron model based onsymbolic semantic features (Li et al, 2013).?
LSTM: A long short-term memory neuralnetwork (Hochreiter and Schmidhuber, 1997)based on distributed semantic features.Table 8 shows the results.
On ACE events, bothDMCNN and Joint methods outperform our ap-proach for trigger and argument extraction.
How-ever, when moving to ERE event schema, althoughre-trained based on ERE labeled data, their perfor-mance still degrades significantly.
These previousmethods heavily rely on the quality and quantity ofthe training data.
When the training data is not ad-equate (the ERE training documents contain 1,068events and 2,448 arguments, while ACE trainingdocuments contain more than 4,700 events and9,700 arguments), the performance is low.
In con-trast, our approach is unsupervised and can au-tomatically identify events, arguments and assigntypes/roles, and is not tied to one event schema.3.6 Event Extraction for Biomedical DomainTo demonstrate the portability of our approach toa new domain, we conduct our experiment on 14biomedical articles (755 sentences) with perfectAMR annotations (Garg et al, 2016).
We utilize aword2vec model6trained from all paper abstractsfrom PubMed7and full-text documents from thePubMed Central Open Access subset.
To evaluatethe performance, we randomly sample 100 sen-tences and ask a biomedical scientist to assess thecorrectness of each event and argument role.
Ourapproach achieves 83.1% precision on trigger la-beling (619 events in total) and 78.4% precisionon argument labeling (1,124 arguments in total).6http://bio.nlplab.org/7http://www.ncbi.nlm.nih.gov/pubmed265MethodERE: Trigger F1(%) ERE: Arg F1(%) ACE: Trigger F1(%) ACE: Arg F1(%)P R F1P R F1P R F1P R F1LSTM 41.5 46.8 44.1 9.9 11.6 10.7 66.0 60 62.8 29.3 32.6 30.8Joint 42.3 41.7 42.0 61.8 23.2 33.7 73.7 62.3 67.5 64.7 44.4 52.7DMCNN - - - - - - 75.6 63.6 69.1 68.8 46.9 53.5LiberalPerfectAMR79.8 50.5 61.8 48.9 32.9 39.3 - - - - - -LiberalSystemAMR88.5 42.6 57.5 47.6 30.0 36.8 80.7 50.1 61.8 51.9 39.4 44.8Table 8: Performance on ERE and ACE events.It demonstrates that our approach can be rapidlyadapted to a new domain and discover domain-richevent schema.
An example schema for an eventtype ?Dissociate?
is shown in Figure 7.exchange for 15000 U.S. dollars.
Event: ship,Arguments: man(Agent), Austrialia(Destination),heroin(Theme)S2: State media didn?t identify the 2 convicts hanged in Zahedan but statedthat they had been found guilty of transporting 5.25 kilograms of heroin.Event: transporting,   Arguments: they(Agent), heroin(Theme)S1: The construction of the facility started in 790000, but stopped afterthe 910000 Soviet collapse when Tajikistan slid into a 5 year civil warthat undermined its economy.
Event:construction,Arguments: facility(Product), 790000(Time)S2: The closed Soviet-era military facility was fou-nded in 570000 andcollects and analyzes all information gathered from Russia's military spysatellites.
Event: founded,    Arguments: Soviet-era facility(Product), 570000(Time)Event Type: Buildsentenced him to death in 1997.Event: death,   Arguments: him(Theme), 1997(Time)S2: A newspaper report on January 1, 2008 that Iran hanged twoconvicted drug traffickers in the south-eastern city of Zahedan.S1: Colombian Government was alarmed because uranium is theprimary basis for generating weapons of mass destruction.Event:alarmed, Arguments:Columbian Government(Experiencer)Event Type: ThreatenS2: Cluster bomblets have been criticized by human rights groupsbecause they kill indiscriminately and because unexplodedordinance poses a threat to civilians similar to that of land mines.Event:threat,Arguments:ordinance(Cause), civilian(Experiencer)Event: hanged,   Arguments: Iran(Agent), drug traf-fickers(Theme), southeastern city of Zahedan(Place)S1: Ras acts as a molecular switch that is activated upon GTPloading and deactivated upon hydrolysis of GTP to GDP.Event: hydrolysis   Arguments:GTP (Patient), (GDP) (Result)Event Type: DissociateS2: Activation requires dissociation of protein-bound GDP , anintrinsically slow process that is accelerated by guanine nucleotideexchange factors.Event: dissociation   Arguments: GDP (Patient)S3: His - ubiquitinated proteins were purified by Co2+ metalaffinity chromatography in 8M urea denaturing conditions.Event: denaturing  Arguments: proteins(Patient)Figure 7: Example Output of the DiscoveredBiomedical Event Schema.4 Related WorkMost of previous event extraction work focusedon learning supervised models based on symbolicfeatures (Ji and Grishman, 2008; Miwa et al,2009; Liao and Grishman, 2010; Liu et al, 2010;Hong et al, 2011; McClosky et al, 2011; Se-bastian and Andrew, 2011; Chen and Ng, 2012;Li et al, 2013) or distributional features throughdeep learning (Chen et al, 2015; Nguyen andGrishman, 2015).
They usually rely on a pre-defined event schema and a large amount of train-ing data.
Compared with other paradigms suchas Open Information Extraction (Etzioni et al,2005; Banko et al, 2007; Banko et al, 2008;Etzioni et al, 2011; Ritter et al, 2012), Pre-emptive IE (Shinyama and Sekine, 2006), On-demand IE (Sekine, 2006) and semantic framebased event discovery (Kim et al, 2013), our ap-proach can explicitly name each event type andargument role.
Some recent work focused onuniversal schema discovery (Chambers and Juraf-sky, 2011; Pantel et al, 2012; Yao et al, 2012;Yao et al, 2013; Chambers, 2013; Nguyen et al,2015).
However, the schemas discovered fromthese methods are rather static and they are notcustomized for any specific input corpus.Our work is also related to efforts at composingword embeddings using syntactic structures (Her-mann and Blunsom, 2013; Socher et al, 2013a;Socher et al, 2013b; Bowman et al, 2014; Zhaoet al, 2015).
Our trigger sense representation issimilar to Word Sense Induction (Navigli, 2009;Bordag, 2006; Pinto et al, 2007; Brody and La-pata, 2009; Manandhar et al, 2010; Navigli andLapata, 2010; Van de Cruys and Apidianaki, 2011;Wang et al, 2015b).
Besides word sense, we ex-ploit related concepts to enrich trigger representa-tion.5 Conclusions and Future WorkWe proposed a novel Liberal event extractionframework which combines the merits of symbolicsemantics and distributed semantics.
Experimentson news and biomedical domain demonstrate thatthis framework can discover explicitly defined richevent schemas which cover not only most types inexisting manually defined schemas, but also newevent types and argument roles.
The granularityof event types is also customized for specific inputcorpus.
And it can produce high-quality event an-notations simultaneously without using annotatedtraining data.
In the future, we will extend thisframework to other Information Extraction tasks.AcknowledgementsWe would like to thank Kevin Knight and JonathanMay (ISI) for sharing biomedical AMR annota-tions.
This work was supported by the U.S. ARLNS-CTA No.
W911NF-09-2-0053 and DARPADEFT No.
FA8750-13-2-0041, and in part by NSFIIS-1523198, IIS-1017362, IIS-1320617 and IIS-1354329, and NIH BD2K grant 1U54GM114838.The views and conclusions contained in this doc-ument are those of the authors and should not beinterpreted as representing the official policies, ei-ther expressed or implied, of the U.S. Govern-ment.
The U.S. Government is authorized to re-produce and distribute reprints for Governmentpurposes notwithstanding any copyright notationhere on.266ReferencesC.
F. Baker and H. Sato.
2003.
The framenet data andsoftware.
In Proc.
ACL2003.L.
Banarescu, C. Bonial, S. Cai, M. Georgescu,K.
Griffitt, U. Hermjakob, K. Knight, P. Koehn,M.
Palmer, and N. Schneider.
2013.
Abstractmeaning representation for sembanking.
In Proc.ACL2013 Workshop on Linguistic Annotation andInteroperability with Discourse.M.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfor the web.
In Proc.
IJCAI2007.M.
Banko, O. Etzioni, and T. Center.
2008.
The trade-offs between open and traditional relation extraction.In Proc.
ACL-HLT2008.S.
Bordag.
2006.
Word sense induction: Triplet-based clustering and automatic evaluation.
In Proc.EACL2006.S.
Bowman, C. Potts, and C. Manning.
2014.
Recur-sive neural networks for learning logical semantics.CoRR, abs/1406.1827.S.
Brody and M. Lapata.
2009.
Bayesian word senseinduction.
In Proc.
EACL2009.N.
Chambers and D. Jurafsky.
2011.
Template-basedinformation extraction without the templates.
InProc.
ACL-HLT2011.N.
Chambers.
2013.
Event schema induction with aprobabilistic entity-driven model.
In EMNLP, vol-ume 13, pages 1797?1807.C.
Chen and V. Ng.
2012.
Joint modeling for chineseevent extraction with rich linguistic features.
In InCOLING.
Citeseer.Y.
Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao.
2015.Event extraction via dynamic multi-pooling convo-lutional neural networks.
In Proc.
ACL2015.Dipanjan Das, Desai Chen, Andr?e FT Martins, NathanSchneider, and Noah A Smith.
2014.
Frame-semantic parsing.
Computational Linguistics,40(1):9?56.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe web: An experimental study.
Artificial Intelli-gence, 165:91?134.O.
Etzioni, A. Fader, J. Christensen, S. Soderland, andM.
Mausam.
2011.
Open information extraction:The second generation.
In Proc.
IJCAI2011, vol-ume 11, pages 3?10.S.
Garg, A. Galstyan, U. Hermjakob, and D. Marcu.2016.
Extracting biomolecular interactions usingsemantic parsing of biomedical text.
In Proc.
AAAI.R.
Grishman and B. Sundheim.
1996.
Message un-derstanding conference-6: A brief history.
In Proc.COLING1996.Z.
Harris.
1954.
Distributional structure.
Word,10(23):146?162.K.
Hermann and P. Blunsom.
2013.
The role of syntaxin vector space models of compositional semantics.In Proc.
ACL2013.S.
Hochreiter and J. Schmidhuber.
1997.
Long short-term memory.
Neural computation, 9(8):1735?1780.Y.
Hong, J. Zhang, B. Ma, J. Yao, G. Zhou, and Q. Zhu.2011.
Using cross-entity inference to improve eventextraction.
In Proc.
ACL, pages 1127?1136.
Asso-ciation for Computational Linguistics.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: the 90% solution.In Proc.
NAACL2006.H.
Ji and R. Grishman.
2008.
Refining event extractionthrough cross-document inference.
In ACL.H.
Ji and R. Grishman.
2011.
Knowledge base pop-ulation: Successful approaches and challenges.
InProc.
ACL2011.H.
Kim, X. Ren, Y.
Sun, C. Wang, and J. Han.
2013.Semantic frame-based document representation forcomparable corpora.
In ICDM.K.
Kipper, A. Korhonen, N. Ryant, and M. Palmer.2008.
A large-scale classification of englishverbs.
Language Resources and Evaluation Jour-nal, 42(1):21?40.Q.
Li, H. Ji, and L. Huang.
2013.
Joint event extrac-tion via structured prediction with global features.In Proc.
ACL, pages 73?82.S.
Liao and R. Grishman.
2010.
Using document levelcross-event inference to improve event extraction.In Proc.
ACL.B.
Liu, L. Qian, H. Wang, and G. Zhou.
2010.Dependency-driven feature-based learning for ex-tracting protein-protein interactions from biomedi-cal text.
In Proc.
COLING.U.
Luxburg.
2007.
A tutorial on spectral clustering.Statistics and computing, 17(4):395?416.S.
Manandhar, I. Klapaftis, D. Dligach, and S. Pradhan.2010.
Semeval-2010 task 14: Word sense induction& disambiguation.
In Proc.
ACL2010 internationalworkshop on semantic evaluation.Dan Klein Christopher D Manning.
2003.
Natural lan-guage parsing.
In Advances in Neural InformationProcessing Systems 15: Proceedings of the 2002Conference, volume 15, page 3.
MIT Press.267D.
M. Marie-Catherine, Bill M., and Christopher D.M.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proceedings LREC,pages 449,454.D.
McClosky, M. Surdeanu, and C. D. Manning.
2011.Event extraction as dependency parsing.
In ACL,pages 1626?1635.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013.Efficient estimation of word representations in vec-tor space.
CoRR, abs/1301.3781.M.
Miwa, R. Stre, Y. Miyao, and J. Tsujii.
2009.
A richfeature vector for protein-protein interaction extrac-tion from multiple corpora.
In Proc.
EMNLP.R.
Navigli and M. Lapata.
2010.
An experimentalstudy of graph connectivity for unsupervised wordsense disambiguation.
Pattern Analysis and Ma-chine Intelligence, 32(4):678?692.R.
Navigli.
2009.
Word sense disambiguation: A sur-vey.
ACM Computing Surveys (CSUR), 41(2):10.T.
Nguyen and R. Grishman.
2015.
Event detec-tion and domain adaptation with convolutional neu-ral networks.
Volume 2: Short Papers, page 365.K.
Nguyen, X. Tannier, O. Ferret, and R. Besanc?on.2015.
Generative event schema induction with en-tity disambiguation.
In Proc.
ACL.B.
Onyshkevych, M. E. Okurowski, and L. Carlson.1993.
Tasks, domains, and languages for informa-tion extraction.
In TIPSTER.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.P.
Pantel, T. Lin, and M. Gamon.
2012.
Mining entitytypes from query logs via user intent modeling.
InProc.
ACL2012.D.
Pinto, P. Rosso, and H. Jimenez-Salazar.
2007.Upv-si: Word sense induction using self term ex-pansion.
In Proc.
ACL2007 International Workshopon Semantic Evaluations.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
Conll-2011shared task: Modeling unrestricted coreference inontonotes.
In Proc.
CONLL2011.A.
Ritter, O. Etzioni, and S. Clark.
2012.
Opendomain event extraction from twitter.
In Proc.SIGKDD2012, pages 1104?1112.
ACM.R.
Sebastian and M. Andrew.
2011.
Fast and robustjoint models for biomedical event extraction.
InEMNLP.S.
Sekine.
2006.
On-demand information extraction.In Proc.
COLING-ACL2006.Y.
Shinyama and S. Sekine.
2006.
Preemptive infor-mation extraction using unrestricted relation discov-ery.
In Proc.
HLT-NAACL2006.R.
Socher, J. Pennington, E. H. Huang, A. Y. Ng, andC.
D. Manning.
2011.
Semi-supervised recursiveautoencoders for predicting sentiment distributions.In Proc.
EMNLP, pages 151?161.R.
Socher, A. Karpathy, Q. V. Le, C. Manning, andA.
Y. Ng.
2013a.
Grounded compositional se-mantics for finding and describing images with sen-tences.
TACL2013.R.
Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning,A.
Ng, and C. Potts.
2013b.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proc.
EMNLP2013.E.
Tjong, K. Sang, and F. Meulder.
2003.
Introductionto the conll-2003 shared task: language-independentnamed entity recognition.
In Proc.
CONLL2003.T.
Van de Cruys and M. Apidianaki.
2011.
Latent se-mantic word sense induction and disambiguation.
InProc.
ACL-HLT2011.C.
Wang, N. Xue, and S. Pradhan.
2015a.
Boost-ing transition-based amr parsing with refined actionsand auxiliary analyzers.
In Proc.
ACL2015.J.
Wang, M. Bansal, K. Gimpel, B. Ziebart, andT.
Clement.
2015b.
A sense-topic model for wordsense induction with unsupervised data enrichment.TACL, 3:59?71.L.
Yao, S. Riedel, and A. McCallum.
2012.
Prob-abilistic databases of universal schema.
In Proc.NIPS2012 Joint Workshop on Automatic KnowledgeBase Construction and Web-scale Knowledge Ex-traction.L.
Yao, S. Riedel, and A. McCallum.
2013.
Uni-versal schema for entity type prediction.
In Proc.NIPS2013 Workshop on Automated Knowledge BaseConstruction.Y.
Zhao, Z. Liu, and M. Sun.
2015.
Phrase type sen-sitive tensor indexing model for semantic composi-tion.
In Proc.
AAAI2015.Z.
Zhong and H. T. Ng.
2010.
It makes sense: Awide-coverage word sense disambiguation systemfor free text.
In Proceedings of the ACL 2010 SystemDemonstrations, pages 78?83.268
