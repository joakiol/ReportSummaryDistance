Coling 2010: Poster Volume, pages 1408?1416,Beijing, August 2010Accelerated Training of Maximum Margin Markov Models for SequenceLabeling: A Case Study of NP Chunking?Xiaofeng YU Wai LAMInformation Systems LaboratoryDepartment of Systems Engineering & Engineering ManagementThe Chinese University of Hong Kong{xfyu,wlam}@se.cuhk.edu.hkAbstractWe present the first known empirical resultson sequence labeling based on maximum mar-gin Markov networks (M3N ), which incorpo-rate both kernel methods to efficiently deal withhigh-dimensional feature spaces, and probabilisticgraphical models to capture correlations in struc-tured data.
We provide an efficient algorithm, thestochastic gradient descent (SGD), to speedup thetraining procedure of M3N .
Using official datasetfor noun phrase (NP) chunking as a case study,the resulting optimizer converges to the same qual-ity of solution over an order of magnitude fasterthan the structured sequential minimal optimization(structured SMO).
Our model compares favorablywith current state-of-the-art sequence labeling ap-proaches.
More importantly, our model can be eas-ily applied to other sequence labeling tasks.1 IntroductionThe problem of annotating or labeling observationsequences arises in many applications across a va-riety of scientific disciplines, most prominently innatural language processing, speech recognition,information extraction, and bioinformatics.
Re-cently, the predominant formalism for modelingand predicting label sequences has been based ondiscriminative graphical models and variants.Among such models, maximum marginMarkov networks (M3N ) and variants ( Taskaret al (2003); Taskar (2004); Taskar et al (2005))have recently gained popularity in the machinelearning community.
While theM3N frameworkmakes extensive use of many theoretical results?The work described in this paper is substantially sup-ported by grants from the Research Grant Council of theHong Kong Special Administrative Region, China (ProjectNo: CUHK4128/07) and the Direct Grant of the Fac-ulty of Engineering, CUHK (Project Codes: 2050442 and2050476).
This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing andInterface Technologies.available for Markov networks, it largely dis-penses with the probabilistic interpretation.
M3Nthus combines the advantages of both worlds, thepossibility to have a concise model of the relation-ships present in the data via log-linear Markovnetworks over a set of label variables and thehighly accurate predictions based on maximummargin estimation of the model parameters.Traditionally, M3N can be trained usingthe structured sequential minimal optimization(structured SMO), a coordinate descent methodfor solving quadratic programming (QP) prob-lems (Taskar et al, 2003).
Clearly, however, thepolynomial number of constraints in the QP prob-lem associated with the M3N can still be verylarge, making the structured SMO algorithm slowto converge over the training data.
This currentlylimits the scalability and applicability ofM3N tolarge-scale real world problems.Stochastic gradient methods (e.g., Lecun etal.
(1998); Bottou (2004)), on the other hand,are online and scale sub-linearly with the amountof training data, making them very attractive forlarge-scale datasets.
In stochastic (or online) gra-dient descent (SGD), the true gradient is approx-imated by the gradient of the cost function onlyevaluated on a single training example.
The pa-rameters are then adjusted by an amount propor-tional to this approximate gradient.
Therefore, theparameters of the model are updated after eachtraining example.
For large-scale datasets, onlinegradient descent can be much faster than standard(or batch) gradient descent.In this paper, we marry the above two tech-niques and show how SGD can be used to signif-icantly accelerate the training of M3N .
And we1408then apply our model to the well-established se-quence labeling task: noun phrase (NP) chunking.Experimental results show the validity and effec-tiveness of our approach.
We now summarize theprimary contributions of this paper as follows:?
We exploit M3N to NP chunking on thestandard evaluation dataset, achieving fa-vorable performance against recent top-performing systems.
The M3N frameworkallows arbitrary features of observation se-quence, as well as the important benefits ofkernels.
To the best of our knowledge, this isthe first known empirical study on NP chunk-ing usingM3N in the NLP community.?
We provide the efficient SGD algorithm toaccelerate the training procedure of M3N ,and experimental results show that it con-verges over an order of magnitude faster thanthe structured SMO without sacrificing per-formance.?
Our model is easily extendable to other se-quence labeling tasks, such as part-of-speechtagging and named entity recognition.
Basedon the promising results on NP chunking,we believe that our model will significantlyfurther the applicability of margin-based ap-proaches to large-scale sequence labelingtasks.2 Maximum Margin Markov Networksfor Sequence LabelingIn sequence labeling, the output is a sequence oflabels y = (y1, .
.
.
, yT ) which corresponds to anobservation sequence x = (x1, .
.
.
, xT ).
Supposeeach individual label can take values from set ?,then the problem can be considered as a multiclassclassification problem with |?|T different classes.InM3N , a pairwise Markov network is definedas a graph G = (Y,E).
Each edge (i, j) ?
E isassociated with a potential function?ij(x, yi, yj) = exp(l?k=1wk?k(x, yi, yj))= exp(w>?
(x, yi, yj)) (1)where ?
(x, yi, yj) is a pairwise basis function.
Alledges in the graph denote the same type of in-teraction, so that we can define a feature map?k(x, y) = ?
(i,j)?E ?k(x, yi, yj).
The networkencodes the following conditional probability dis-tribution (Taskar et al, 2003):P (y|x) ?
?
(i,j)?E?ij(x, yi, yj) = exp(w>?
(x, y))(2)where ?
(x, y) = [?1?2 .
.
.
?|?|?trans]> isused to learn a weight vector w. ?k =?ni=1 ?i(x)I(yi = k),?k ?
{1, 2, .
.
.
, |?|} and?trans = [c11c12 .
.
.
cTT ]> where cij is the num-ber of observed transitions from the ith alphabetto the jth alphabet in ?.Similar to SVMs (Vapnik, 1995),M3N tries tofind a projection to maximize the margin ?.
Onthe other hand, M3N also attempts to minimize?w?
to minimize the generalization error.
Sup-pose ?tx(y) = ?ni=1 ?tx(yi) =?ni=1 I(yi 6=(t(x))i) where t((x))i is the true label of the ithsequence xi, and ?
?x(y) = ?
(x, t(x)) ?
?
(x, y)where t(x) is the true label of the observation se-quence x.
We can get a quadratic program (QP)using a standard transformation to eliminate ?
asfollows:min 12?w?2; (3)s.t.
w>?
?x(y) ?
?tx(y),?x ?
S,?y ?
?.However, the sequence data is often not separa-ble by the defined hyperplane.
In such cases, wecan introduce slack variables ?x which are guaran-teed to be non-negative to allow some constraints.Thus the complete primal form of the optimiza-tion problem can be formulated by:min 12?w?2 + C?x?x; (4)s.t.
w>?
?x(y) ?
?tx(y)?
?x,?x ?
S,?y ?
?.where C is called the capacity in the support vectorliterature and presents a way to trade-off the train-ing error and margin size.
One should note that thenumber of constraints is?Ti=1 |?i|, an extremelylarge number.
And the corresponding dual formu-1409lation can be defined as:max?x,y?x(y)?tx(y)?
12??x,y?x(y)??x(y)?2;s.t.
?y?x(y) = C,?x;?x(y) ?
0,?x, y.
(5)where ?x(y) is a dual variable.As well as loss functions, kernels might havesubstantial influence on the performance of a clas-sification system.
M3N is capable of incorpo-rating many different kinds of kernel functions toreduce computations in the high-dimensional fea-ture space H. This is sometimes referred to asthe ?kernel trick?
(Scho?lkopf and Smola, 2002;Shawe-Taylor and Cristianini, 2004).
A linearkernel can be defined as?
((x, y), (x?, y?))
= ??
(x, y), ?
(x?, y?
)?H (6)For a polynomial kernel,?
((x, y), (x?, y?
))= (s ?
??
(x, y), ?
(x?, y?
)?H + r)d, (7)and for a neural kernel,?
((x, y), (x?, y?
))= tanh(s ?
??
(x, y), ?
(x?, y?
)?H + r), (8)where s, d, and r are coefficients in kernel func-tions.3 Stochastic Gradient DescentFor M3N optimization, Taskar et al (2003) hasproposed a reparametrization of the dual variablesto take advantage of the network structure of thelabeling sequence problem.
The dual QP is thensolved using the structured sequential minimaloptimization (structured SMO) analogous to theSMO used for SVMs (Platt, 1998).
However, theresulting number of constraints in the QP makethe structured SMO algorithm slow to converge,or even prohibitively expensive for large-scale realworld problems.
In this section we will presentstochastic gradient descent (SGD) method, andshow SGD can significantly speedup the trainingofM3N .3.1 Regularized Loss MinimizationRecall that for M3N , the goal is to finda linear hypothesis hw such that hw(x) =argmaxy??
w>?
(x, y).
The parameters w arelearned by minimizing a regularized lossL(w; {(xi, yi)}Ti=1, C) =m?i=1`(w, xi, yi)+C2 ?w?2.
(9)The function `measures the loss incurred in us-ing w to predict the label of xi.
Following (Taskaret al, 2003), `(w, xi, yi) is a variant of the hingeloss, and can be defined as follows:`(w, xi, yi) = maxy??
[e(xi, yi, y)?
w ?
(?
(xi, yi)?
?
(xi, y))], (10)where e(xi, yi, y) is some non-negative measureof the error incurred in predicting y instead of yias the label of xi.
We assume that e(xi, yi, y) = 0for all i, so that no loss is incurred for correctprediction, and therefore `(w, xi, yi) is alwaysnon-negative.
This loss function corresponds tothe M3N approach, which explicitly penalizestraining examples for which, for some y 6= yi,w ?
(?
(xi, yi) ?
?
(xi, y)) < e(xi, yi, y).
And thefunction L is convex in w for `(w, xi, yi).
There-fore, minimization of L can be re-cast as opti-mization of the following dual convex problem:w?
= argminw?imaxy??
[e(xi, yi, y)?
w ?
(?
(xi, yi)?
?
(xi, y))] + C2 ?w?2.
(11)3.2 The SGD AlgorithmTo perform parameter estimation, we need to min-imize L(w; {(xi, yi)}Ti=1, C).
For this purpose wecompute its gradient G(w):G(w) = ?
?w(L(w; {(xi, yi)}Ti=1, C))= ?
?w(m?i=1`(w, xi, yi) + C2 ?w?2) (12)In addition to the gradient, second-order meth-ods based on Newton steps also require computa-tion and inversion of the Hessian H(w).
Taking1410the gradient of Equation 12 wrt.
w yields:H(w) = ?
?wG(w) =?2?w2L (13)Explicitly computing the full Hessian is timeconsuming.
Instead we can make use of the dif-ferentialdG(w) = H(w)dw (14)to efficiently compute the product of the Hessianwith a chosen vector v =: dw by forward-modealgorithmic differentiation (Pearlmutter, 1994).These Hessian-vector products can be computedalong with the gradient at only 2-3 times thecost of the gradient computation alone.
We de-note G(w) = ?wL, and each iteration of theSGD algorithm consists in drawing an example(xi, yi) at random and applying the parameter up-date rule (Robbins and Monroe, 1951):wt+1 ?
wt ?
?
?
?wL (15)where ?
is the learning rate in the algorithm.The SGD algorithm has been shown to be fast,reliable, and less prone to reach bad local minima.In this algorithm, the weights are updated afterthe presentation of each example, according to thegradient of the loss function (Lecun et al, 1998).The convergence is very fast when the training ex-amples are redundant since only a few examplesare needed to perform.
This algorithm can get agood estimation after considerably few iterations.3.3 Choosing Learning Rate ?The learning rate ?
is crucial to the speed ofSGD algorithm.
Ideally, each parameter weightwi should have its own learning rate ?i.
Becauseof possible correlations between input variables,the learning rate of a unit should be inversely pro-portional to the square root of the number of in-puts to the unit.
If shared weights are used, thelearning rate of a weight should be inversely pro-portional to the square root of the number of con-nection sharing that weight.For one-dimensional sequence labeling task,the optimal learning rate yields the fastest conver-gence in the direction of highest curvature is (Bot-tou, 2004):?opt = (?2L?w2 )?1 = (H(w))?1, (16)and the maximum learning rate is ?max = 2?opt.The simple SGD update offers lots of engineer-ing opportunities.
In practice, however, at any mo-ment during the training procedure, we can selecta small subset of training examples and try vari-ous learning rates on the subset, then pick the onethat most reduces the cost and use it on the fulldataset.3.4 The SGD ConvergenceThe convergence of stochastic algorithms actuallyhas been studied for a long time in adaptive signalprocessing.
Given a suitable choice of the learn-ing rate ?t, the standard (batch) gradient descentalgorithm is known to converge to a local mini-mum of the cost function.
However, the randomnoise introduced by SGD disrupts this determinis-tic picture and the specific study of SGD conver-gence usually is fairly complex (Benveniste et al,1987).It is reported that for the convex case, if sev-eral assumptions and conditions are valid, thenthe SGD algorithm converges almost surely to theoptimum w?
1.
For the general case where thecost function is non-convex and has both localand global minima, if four assumptions and twolearning rate assumptions hold, it is guaranteedthat the gradient ?wL converges almost surely tozero (Bottou, 2004).
We omit the details of theconvergence theorem and corresponding proofsdue to space limitation.3.5 SGD SpeedupUnfortunately, many of sophisticated gradientmethods are not robust to noise, and scale badlywith the number of parameters.
The plain SGDalgorithm can be very slow to converge.
Inspiredby stochastic meta-descent (SMD) (Schraudolph,1999), the convergence speed of SGD can be fur-ther improved with gradient step size adaptationby using second-order information.
SMD is ahighly scalable local optimizer.
It shines whengradients are stochastically approximated.In SMD, the learning rate ?
is simultaneously1One may argue that SGD on many architectures doesnot result in a global optima.
However, our goal is to obtaingood performance on future examples in learning rather thanachieving a global optima on the training set.1411INPUT: training set S {(x1, y1), .
.
.
, (xT, yT)};factor ?
; number of iterations N .INITIALIZE: w0, v0 = 0, ?0.FOR t = 1, 2, .
.
.
, NChoose a random example (xi, yi) ?
SCompute the gradient ?t = Gt and HtvtSet vt+1 = ?vt ?
?t ?
(Gt + ?Htvt)Update the parameter vector:wt+1 ?
wt ?
?t ?
?tAdapt the gradient step size:?t+1 = ?t ?max(12 , 1?
?Gt+1 ?
vt+1)OUTPUT: wN+1Figure 1: Pseudo-code for the SGD algorithm.adapted via a multiplicative update with ?
:?t+1 = ?t ?max(12 , 1?
?Gt+1 ?
vt+1), (17)where the vector v (v =: dw) captures the long-term dependencies of parameters.
v can be com-puted by the simple iterative update:vt+1 = ?vt ?
?t ?
(Gt + ?Htvt), (18)where the factor 0 ?
?
?
1 governs the time scaleover which long-term dependencies are taken intoaccount, and Htvt can be calculated efficientlyalongside the gradient by forward-mode algorith-mic differentiation via Equation 14.
This Hessian-vector product is computed implicitly and it is thekey to SMD?s efficiency.
The pseudo-code for theSGD algorithm is shown in Figure 1.4 Experiments: A Case Study of NPChunking4.1 DataOur data comes from the CoNLL 2000 shared task(Sang and Buchholz, 2000).
The dataset is di-vided into a standard training set of 8,936 sen-tences and a testing set of 2,012 sentences.
Thisdata consists of the same partitions of the WallStreet Journal corpus (WSJ) as the widely useddata for NP chunking: sections 15-18 as trainingdata (211,727 tokens) and section 20 as test data(47,377 tokens).
And the annotation of the datahas been derived from the WSJ corpus.wt??
= wwt matches [A-Z]wt matches [A-Z]+wt matches [A-Z][a-z]+wt matches [A-Z]+[a-z]+[A-Z]+[a-z]wt matches .*[0-9].
*wt contains dash ?-?
or dash-based ?-based?wt is capitalized, all-caps, single capital letter,or mixed capitalizationwt contains years, year-spans or fractionswt is contained in a lexicon of words with POSp (from the Brill tagger)pt = pqk(x, t+ ?)
for all k and ?
?
[?3, 3]Table 1: Input feature template qk(x, t) for NPchunking.
In this table wt is the token (word) atposition t, pt is the POS tag at position t, w rangesover all words in the training data, and p rangesover all POS tags.4.2 FeaturesWe follow some top-performing NP chunking sys-tems and perform holdout methodology to designfeatures for our model, resulting in a rich featureset including POS features provided in the officialCoNLL 2000 dataset (generated by the Brill tag-ger (Brill, 1995), with labeling accuracy of around95-97%), some contextual and morphological fea-tures.
Table 1 lists our feature set for NP chunk-ing.4.3 Experimental ResultsWe trained linear-chain conditional random fields(CRFs) (Lafferty et al, 2001) as the baseline.
Thewell known limited memory quasi-Newton BFGSalgorithm (L-BFGS) (Liu and Nocedal, 1989) wasapplied to learn the parameters for CRFs.
Toavoid over-fitting, we penalized the log-likelihoodby the commonly used zero-mean Gaussian priorover the parameters.
This gives us a competitivebaseline CRF model for NP chunking.
To makefair and accurate comparison, we used the sameset of features listed in Table 1 for bothM3N andCRFs.
All experiments were performed on theLinux platform, with a 3.2GHz Pentium 4 CPUand 4 GB of memory.1412Model Training Method Kernel Function Iteration Training Time(s) P(%) R(%) F?=1M3N structured SMO linear kernel: ?a, b?H 100 1176 94.59 94.22 94.40M3N structured SMO polynomial(quadratic): (?a, b?H + 1)2 100 30792 94.88 94.49 94.68M3N structured SMO polynomial(cubic): (?a, b?H + 1)3 100 30889 94.47 94.01 94.24M3N structured SMO polynomial(biquadratic): (?a, b?H + 1)4 100 31556 93.90 93.77 93.83M3N structured SMO neural kernel: tanh(0.1 ?
?a, b?H) 20 7395 94.42 94.02 94.22CRFs L-BFGS ?
100 352 94.55 94.09 94.32Table 2: M3N vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL2000 official dataset.
M3N was trained using the structured SMO algorithm.Model Training Method Kernel Function Iteration Training Time(s) P(%) R(%) F?=1M3N SGD linear kernel: ?a, b?H 100 89 94.58 94.21 94.39M3N SGD polynomial(quadratic): (?a, b?H + 1)2 100 1820 94.89 94.50 94.69M3N SGD polynomial(cubic): (?a, b?H + 1)3 100 1831 94.47 94.01 94.24M3N SGD polynomial(biquadratic): (?a, b?H + 1)4 100 1857 93.91 93.76 93.83M3N SGD neural kernel: tanh(0.1 ?
?a, b?H) 20 477 94.40 94.01 94.20CRFs L-BFGS ?
100 352 94.55 94.09 94.32Table 3: M3N vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL2000 official dataset.
M3N was trained using the SGD algorithm.System F?=1SVMs (polynomial kernel) (Kudo and Mat-sumoto, 2000)93.79SVM combination (Kudo and Matsumoto,2001)94.39Generalized winnow (Zhang et al, 2002) 94.38Voted perceptron (Collins, 2002) 94.09CRFs (Sha and Pereira, 2003) 94.38Second order CRFs (McDonald et al, 2005) 94.29Chunks from the Charniak Parser (Holling-shead et al, 2005)94.20Second order latent-dynamic CRFs + improvedA* search based inference (Sun et al, 2008)94.34Our approach 94.69Table 4: NP chunking: Comparison with some ex-isting state-of-the-art systems.Similar to other discriminative graphical mod-els such as CRFs, the modeling flexibility ofM3N permits the feature functions to be com-plex, arbitrary, nonindependent, and overlappingfeatures, allowing the multiple features describedin Table 1 to be directly exploited.
Moreover,M3N is capable of incorporating multiple kernelfunctions (see Section 2) which allow the efficientuse of high-dimensional feature spaces during theexperiments.The resulting number of features is 7,835,439,and both M3N and CRFs were trained to predict47,366 tokens with 12,422 noun phrases in thetesting set.
For simplicity, we denote a = ?
(x, y),and b = ?
(x?, y?
), and the linear kernel can berewritten as ?
(a, b) = ?a, b?H.
We performedholdout methodology to find optimal values forcoefficients s, d, and r in M3N kernel functions.For polynomial kernels, we varied d from 2 to 4,resulting in quadratic, cubic, and biquadratic ker-nels, respectively.
Finally, we chose optimizedvalues: s = 1, r = 1 for polynomial kernels, ands = 0.1, r = 0 for neural kernels.
The capacity CforM3N was set to 1 in our experiments.Table 2 shows comparative performance andtraining time for M3N (trained with structuredSMO) and CRFs, while Table 3 shows compar-ative performance and training time for M3N(trained with SGD) and CRFs 2.
ForM3N , whentrained with quadratic kernel and structured SMO,the best F-measure of 94.68 was achieved, leadingto an improvement of 0.36 compared to the CRFbaseline.
What follows is the linear kernel thatobtained 94.40 F-measure.
The cubic and neu-ral kernels obtained close performance, while thebiquadratic kernel led to the worst performance.However, the structured SMO is very computa-tionally intensive, especially for polynomial ker-nels.
For example, CRFs converged in 352 sec-2We used Taku Kudo?s CRF++ toolkit (available athttp://crfpp.sourceforge.net/) in our experiments.
The M3Nmodel, and the structured SMO and SGD training algorithmswere also implemented using C++.1413-10 0 10 20 30 40 50 60 70 80 90 100-240-220-200-180-160-140-120-100-80ObjectivefunctionvalueIteration(a)M3N, structured SMOM3N, SGD-10 0 10 20 30 40 50 60 70 80 90 100-045-040-035-030-025ObjectivefunctionvalueIteration(E)M3N, structured SMOM3N, SGD-2 0 2 4 6 8 10 12 14 16 18 20-220-200-180-160-140-120-100ObjectivefunctionvalueIteration(F)M3N, structured SMOM3N, SGDFigure 2: Convergence speed comparison for structured SMO and SGD algorithms.
The X axis showsnumber of training iterations, and the Y axis shows objective function value.
(a) TheM3N model wastrained using linear kernel.
(b) The M3N model was trained using polynomial(quadratic) kernel.
(c)TheM3N model was trained using neural kernel.onds, whileM3N (polynomial kernels) took morethan 8.5 hours to finish training.As can be seen in Table 3, the SGD algorithmsignificantly accelerated the training procedure ofM3N without sacrificing performance.
When thelinear kernel was used, M3N finished training in89 seconds, more than 13 times faster than themodel trained with structured SMO.
And it is evenmuch faster than the CRF model trained with L-BFGS.
More importantly, SGD obtained almostthe same performance as structured SMO with allM3N kernel functions.Table 4 gives some representative NP chunkingresults for previous work and for our best modelon the same dataset.
These results showed that ourmodel compares favorably with existing state-of-the-art systems 3.Figure 2 compares the convergence speed ofstructured SMO and SGD algorithms for theM3N model.
Linear (Figure 2 (a)), polyno-mial(quadratic) (Figure 2 (b)) and neural kernels(Figure 2 (c)) were used 4.
We calculated objec-tive function values during effective training iter-ations.
It can be seen that both structured SMOand SGD algorithms converge to the same objec-tive function value for different kernels, but SGDconverges considerably faster than the structuredSMO.Figure 3 (a) demonstrates the effect of trainingset size on performance for NP chunking.
We3Note that it is difficult to compare strictly, since reportedresults sometimes leave out details (e.g., feature sets, signifi-cance tests, etc) needed for accurate comparison.4For cubic and biquadratic kernels, the curves are verysimilar to that of quadratic kernel, and we omitted them forspace.increased the training set size from 1,000 sen-tences to 8,000 sentences, with an incrementalstep of 1,000.
And the testing set was fixed tobe 2,012 sentences.
The M3N models (with dif-ferent kernels) were trained using the SGD algo-rithm.
It is particularly interesting to know thatthe performance boosted for all the models whenincreasing the training set size.
Using linear andquadratic kernels, M3N model significantly andconsistently outperforms the CRF model for dif-ferent training set sizes.
The cubic and neuralkernels lead to almost the same performance forM3N , which is slightly lower than the CRF base-line.
As illustrated by the curves, M3N (trainedwith quadratic kernel) achieved the best perfor-mance and larger training set size leads to betterimprovement for this model when compared to theCRFmodel, whileM3N (trained with biquadratickernel) obtained the worst performance among allthe models.Accordingly, Figure 3 (b) shows the impact ofincreasing the training set size on training time forNP chunking.
Increasing training set size leadsto an increase in the computational complexity oftraining procedure for all models.
For the M3Nmodel, it is faster when trained with linear kernelthan the CRF model.
And the three polynomialkernels (quadratic, cubic and biquadratic) haveroughly the same training time.
For CRFs and(M3N , neural kernel), the training time is closeto each other.
For example, when the trainingset contains 1,000 sentences, the training time forCRFs, (M3N , linear kernel), (M3N , quadratickernel), (M3N , cubic kernel), (M3N , biquadratickernel), and (M3N , neural kernel) is 24s, 7s, 72s,14141N 2N 3N 4N 5N 6N 7N 8N 9N905910915920925930935940945950)-PeasureNuPber of traininJ sentences(a)&5)sM3N, linear NernelM3N, Tuadratic NernelM3N, cubic NernelM3N, biTuadratic NernelM3N, neural Nernel1N 2N 3N 4N 5N 6N 7N 8N 9N02004006008001000120014001600180020007raininJtiPesNuPber of traininJ sentences(E)&5)sM3N, linear NernelM3N, Tuadratic NernelM3N, cubic NernelM3N, biTuadratic NernelM3N, neural NernelFigure 3: (a) Effect of training set size on performance for NP chunking.
The training set size wasincreased from 1,000 sentences to 8,000 sentences, with an incremental step of 1,000.
The testing setcontains 2,012 sentences.
All the M3N models (with different kernels) were trained using the SGDalgorithm.
(b) Effect of training set size on training time for NP chunking.72s, 74s, and 30s.
When trained on 8,000 sen-tences, the numbers become 336s, 79s, 1679s,1689s, 1712s, and 411s, respectively.5 Related WorkThe M3N framework and its variants have gen-erated much interest and great progress has beenmade, as evidenced by their promising resultsevaluated in handwritten character recognition,collective hypertext classification (Taskar et al,2003), parsing (Taskar et al, 2004), and XMLtag relabeling (Spengler, 2005).
However, all theabove mentioned research work used structuredSMO algorithm for parameter learning, which canbe computationally intensive, especially for verylarge datasets.Recently, similar stochastic gradient methodshave been applied to train log-linear models suchas CRFs (Vishwanathan et al, 2006).
However,the maximum margin loss has a discontinuity inits derivative, making optimization of such modelssomewhat more involved than log-linear ones.
Wefirst exploit SGD method for fast parameter learn-ing of M3N and achieve state-of-the-art perfor-mance on the NP chunking task in the NLP com-munity.Several algorithms have been proposed totrain max-margin models, including cutting planeSMO (Tsochantaridis et al, 2005), exponenti-ated gradient (Bartlett et al, 2004; Collins et al,2008), extragradient (Taskar et al, 2006), andsubgradient (Shalev-Shwartz et al, 2007).
Somemethods are similar to SGD in that they all pro-cess a single training example at a time.
TheSGD methods directly optimize the primal prob-lem, and at each update use a single example toapproximate the gradient of the primal objectivefunction.
Some of the proposed algorithms, suchas exponentiated gradient corresponds to block-coordinate descent in the dual, and uses the exactgradient with respect to the block being updated.We plan to implement and compare some of thesealgorithms with SGD forM3N .6 Conclusion and Future WorkWe have presented the first known empirical studyon sequence labeling based on M3N .
We havealso provided the efficient SGD algorithm andshown how it can be applied to significantlyspeedup the training procedure of M3N .
As acase study, we performed extensive experimentson standard dataset for NP chunking, showing thepromising and competitiveness of our approach.Several interesting issues, such as the convergencespeed of the SGD algorithm, the effect of train-ing set size on performance for NP chunking, andthe effect of training set size on training time,were also investigated in our experiments.
Forthe future work, we plan to further the scalabilityand applicability of our approach and evaluate iton other large-scale real world sequence labelingtasks, such as POS tagging and NER.1415ReferencesPeter L. Bartlett, Ben Taskar, Michael Collins, and DavidMcallester.
Exponentiated gradient algorithms for large-margin structured classification.
In Proceedings of NIPS-04, pages 113?120.
MIT Press, 2004.A.
Benveniste, M. Metivier, and P. Priouret.
Algorithmesadaptatifs et approximations stochastiques.
Masson,1987.Le?on Bottou.
Stochastic learning.
In Olivier Bousquet andUlrike von Luxburg, editors, Advanced Lectures on Ma-chine Learning, Lecture Notes in Artificial Intelligence,LNAI 3176, pages 146?168.
Springer Verlag, Berlin,2004.Eric Brill.
Transformation-based error-driven learning andnatural language processing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543?565, 1995.Michael Collins, Amir Globerson, Terry Koo, Xavier Car-reras, and Peter L. Bartlett.
Exponentiated gradient al-gorithms for conditional random fields and Max-marginMarkov networks.
Journal of Machine Learning Re-search, 9:1775?1822, 2008.Michael Collins.
Discriminative training methods for hiddenMarkov models: Theory and experiments with perceptronalgorithms.
In Proceedings of HLT/EMNLP-02, pages 1?8, 2002.Kristy Hollingshead, Seeger Fisher, and Brian Roark.Comparing and combining finite-state and context-freeparsers.
In Proceedings of HLT/EMNLP-05, pages 787?794, Vancouver, British Columbia, Canada, 2005.Taku Kudo and Yuji Matsumoto.
Use of support vector learn-ing for chunk identification.
In Proceedings of CoNLL-2000 and LLL-2000, pages 142?144, Lisbon, Portugal,2000.Taku Kudo and Yuji Matsumoto.
Chunking with support vec-tor machines.
In Proceedings of HLT/NAACL-01, pages1?8, 2001.John Lafferty, Andrew McCallum, and Fernando Pereira.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofICML-01, pages 282?289, 2001.Yann Lecun, Le?on Bottou, Yoshua Bengio, and PatrickHaffner.
Gradient-based learning applied to documentrecognition.
Proceedings of the IEEE, 86(11):2278?2324,Nov 1998.Dong C. Liu and Jorge Nocedal.
On the limited memoryBFGS method for large scale optimization.
MathematicalProgramming, 45:503?528, 1989.Ryan McDonald, Koby Crammer, and Fernando Pereira.Flexible text segmentation with structured multilabel clas-sification.
In Proceedings of HLT/EMNLP-05, pages 987?994, Vancouver, British Columbia, Canada, 2005.Barak A. Pearlmutter.
Fast exact multiplication by the Hes-sian.
Neural Computation, 6(1):147?160, 1994.John C. Platt.
Fast training of support vector machines us-ing sequential minimal optimization.
Advances in KernelMethods: Support Vector Learning, pages 41?64, 1998.H.
Robbins and S. Monroe.
A stochastic approximationmethod.
Annals of Mathematical Statistics, 22:400?407,1951.Erik Tjong Kim Sang and Sabine Buchholz.
Introduction tothe CoNLL-2000 shared task: Chunking.
In Proceedingsof CoNLL-2000, pages 127?132, Lisbon, Portugal, 2000.Bernhard Scho?lkopf and Alexander J. Smola.
Learning withKernels.
MIT Press, Cambridge, MA, 2002.Nicol N. Schraudolph.
Local gain adaptation in stochas-tic gradient descent.
In Proceedings of the 9th Interna-tional Conference on Artificial Neural Networks, pages569?574, 1999.Fei Sha and Fernando Pereira.
Shallow parsing with condi-tional random fields.
In Proceedings of HLT/NAACL-03,pages 213?220, 2003.Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
Pe-gasos: Primal estimated sub-GrAdient SOlver for SVM.In Proceedings of ICML-07, pages 807?814, New York,NY, USA, 2007.John Shawe-Taylor and Nello Cristianini.
Kernel Methodsfor Pattern Analysis.
Cambridge University Press, Cam-bridge, UK, 2004.Alex Spengler.
Maximum margin Markov networks forXML tag relabelling.
Master?s thesis, University of Karl-sruhe, 2005.Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, andJun?ichi Tsujii.
Modeling latent-dynamic in shallow pars-ing: A latent conditional model with improved inference.In Proceedings of COLING-08, pages 841?848, Manch-ester, UK, 2008.Ben Taskar, Carlos Guestrin, and Daphne Koller.
Max-margin Markov networks.
In Proceedings of NIPS-03.MIT Press, 2003.Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, andChristopher Manning.
Max-margin parsing.
In Proceed-ings of HLT/EMNLP-04, pages 1?8, 2004.Ben Taskar, Vassil Chatalbashev, Daphne Koller, and CarlosGuestrin.
Learning structured prediction models: A largemargin approach.
In Proceedings of ICML-05, pages 896?903, Bonn, Germany, 2005.Ben Taskar, Simon Lacoste-Julien, and Michael I. Jordan.Structured prediction via the extragradient method.
InProceedings of NIPS-06.
MIT Press, 2006.Ben Taskar.
Learning Structured PredictionModels: A LargeMargin Approach.
PhD thesis, Stanford University, De-cember 2004.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
Large margin methods forstructured and interdependent output variables.
Journalof Machine Learning Research, 6:1453?1484, 2005.Vladimir N. Vapnik.
The Nature of Statistical Learning The-ory.
Springer-Verlag, Inc., New York, USA, 1995.S.
V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.Schmidt, and Kevin P. Murphy.
Accelerated training ofconditional random fields with stochastic gradient meth-ods.
In Proceedings of ICML-06, pages 969?976, Pitts-burgh, Pennsylvania, 2006.Tong Zhang, Fred Damerau, and David Johnson.
Text chunk-ing based on a generalization of winnow.
Journal of Ma-chine Learning Research, 2:615?637, 2002.1416
