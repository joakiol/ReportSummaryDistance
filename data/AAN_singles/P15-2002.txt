Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 8?14,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOn metric embedding for boosting semantic similarity computationsJulien Subercaze, Christophe Gravier, Frederique LaforestUniversit?e de Lyon, F-42023, Saint-Etienne, France,CNRS, UMR5516, Laboratoire Hubert Curien, F-42000, Saint-Etienne, France,Universit?e de Saint-Etienne, Jean Monnet, F-42000, Saint-Etienne, France.firstname.lastname@univ-st-etienne.frAbstractComputing pairwise word semantic simi-larity is widely used and serves as a build-ing block in many tasks in NLP.
In thispaper, we explore the embedding of theshortest-path metrics from a knowledgebase (Wordnet) into the Hamming hyper-cube, in order to enhance the computa-tion performance.
We show that, althoughan isometric embedding is untractable, itis possible to achieve good non-isometricembeddings.
We report a speedup ofthree orders of magnitude for the task ofcomputing Leacock and Chodorow (LCH)similarity while keeping strong correla-tions (r = .819, ?
= .826).1 IntroductionAmong semantic relatedness measures, seman-tic similarity encodes the conceptual distance be-tween two units of language ?
this goes beyondlexical ressemblance.
When words are the speechunits, semantic similarity is at the very core ofmany NLP problems.
It has proven to be essen-tial for word sense disambiguation (Mavroeidis etal., 2005; Basile et al, 2014), open domain ques-tion answering (Yih et al, 2014), and informa-tion retrieval on the Web (Varelas et al, 2005),to name a few.
Two established strategies to es-timate pairwise word semantic similarity includesknowledge-based and distributional semantics.Knowledge-based approaches exploit the struc-ture of the taxonomy ((Leacock and Chodorow,1998; Hirst and St-Onge, 1998; Wu and Palmer,1994)), its content ((Banerjee and Pedersen,2002)), or both (Resnik, 1995; Lin, 1998).
Inthe earliest applications, Wordnet-based semanticsimilarity played a predominant role so that se-mantic similarity measures reckon with informa-tion from the lexical hierarchy.
It therefore ignorescontextual information on word occurrences andrelies on humans to encode such hierarchies ?
atedious task in practice.
In contrast, well-knowndistributional semantics strategies encode seman-tic similarity using the correlation of statistical ob-servations on the occurrences of words in a textualcorpora (Lin, 1998).While providing a significant impact on abroad range of applications, (Herbelot and Gane-salingam, 2013; Lazaridou et al, 2013; Beltagyet al, 2014; Bernardi et al, 2013; Goyal et al,2013; Lebret et al, 2013), distributional semantics?
similarly to knowledge-based strategies ?
strug-gle to process the ever-increasing size of textualcorpora in a reasonable amount of time.
As an an-swer, embedding high-dimensional distributionalsemantics models for words into low-dimensionalspaces (henceforth word embedding (Collobertand Weston, 2008)) has emerged as a popularmethod.
Word embedding utilizes deep learn-ing to learn a real-valued vector representation ofwords so that any vector distance ?
usually thecosine similarity ?
encodes the word-to-word se-mantic similarity.
Although word embedding wassuccessfully applied for several NLP tasks (Her-mann et al, 2014; Andreas and Klein, 2014; Clin-chant and Perronnin, 2013; Xu et al, 2014; Liand Liu, 2014; Goyal et al, 2013), it implies aslow training phase ?
measured in days (Collobertand Weston, 2008; Mnih and Kavukcuoglu, 2013;Mikolov et al, 2013), though re-embedding wordsseems promising (Labutov and Lipson, 2013).There is another usually under-considered issue:the tractability of the pairwise similarity computa-tion in the vector space for large volume of data.Despite these limitations, the current enthusiasmfor word embedding certainly echoes the needfor lightning fast word-to-word semantic similar-ity computation.In this context, it is surprising that embeddingsemantic similarity of words in low dimensional8spaces for knowledge-based approaches is under-studied.
This oversight may well condemn theword-to-word semantic similarity task to remaincorpus-dependant ?
i.e.
ignoring the backgroundknowledge provided by a lexical hierarchy.In this paper, we propose an embedding ofknowledge base semantic similarity based on theshortest path metric (Leacock and Chodorow,1998), into the Hamming hypercube of size n (thesize of targeted binary codes).
The Leacock andChodorow semantic similarity is one of the mostmeaningful measure.
It yields the second rankfor highest correlation with the data collected by(Miller and Charles, 1991), and the first one withinedge centric approaches, as shown by (Seco et al,2004).
This method is only surpassed by the infor-mation theoretic based similarity from (Jiang andConrath, 1997).
A second study present similarresult (Budanitsky and Hirst, 2006), while a thirdone ranks this similarity measure at the first rankfor precision in paraphrase identification (Mihal-cea et al, 2006).The hypercube embedding technique benefitsfrom the execution of Hamming distance within afew cycles on modern CPUs.
This allows the com-putation of several millions distances per second.Multi-index techniques allows the very fast com-putation of top-k queries (Norouzi et al, 2012) onthe Hamming space.
However, the dimension ofthe hypercube (i.e.
the number of bits used torepresent an element) should obey the thresholdof few CPU words (64, 128 .
.
.
, bits) to maintainsuch efficiency (Heo et al, 2012).An isometric embedding requires a excessivelyhigh number of dimensions to be feasible.
How-ever, in this paper we show that practical em-beddings exist and present a method to constructthem.
The best embedding presents very strongcorrelations (r = .819, ?
= .829) with the Lea-cock & Chodorow similarity measure (LCH in therest of this paper).
Our experiments against thestate-of-the art implementation including cachingtechniques show that performance is increased byup to three orders of magnitude.2 Shortest path metric embeddingLet us first introduce few notations.
We denoteHn2as an n-dimensional hypercube whose nodes arelabeled by the 2nbinary n-tuples.
The nodes areadjacent if and only if their corresponding n-tuplesdiffer in exactly one position, i.e.
their Hammingdistance (`1) is equal to one.
In what follows, Qndenotes the metric space composed ofHn2with `1.We tackle the following problem: We aimat defining a function f that maps every nodew of the taxonomy (Wordnet for Leacock &Chodorow) intoQnso that for every pair of nodes:?
(wi, wj), d(wi, wj) = ?
?
`1(f(wi), f(wj)),where ?
is a scalar.
For practical purposes, theconstruction of the mapping should also be rea-sonable in terms of time complexity.Theoretical limitations Wordnet with its hyper-nym relation forms a partially ordered set (poset).The first approach is to perform an isometric em-bedding from the poset with shortest path distanceinto the Hamming hypercube.
Such a mappingwould exactly preserve the original distance in theembedding.
As proven by (Deza and Laurent,1997), poset lattices, with their shortest path met-ric, can be isometrically embedded into the hyper-cube, but the embedding requires 2ndimensions.The resulting embedding would not fit in the mem-ory of any existing computer, for a lattice havingmore than 60 nodes.
Using Wordnet, with tens ofthousands synsets, this embedding is untractable.The bound given by Deza et al is not tight, how-ever it would require a more than severe improve-ment to be of any practical interest.Tree embedding To reduce the dimensionality,we weaken the lattice into a tree.
We build atree from the Wordnet?s Hyponyms/Hypernymsposet by cutting 1,300 links, which correspond toroughly one percent of the edges in the original lat-tice.
The nature of the cut to be performed can besubject to discussion.
In this preliminary research,we used a simple approach.
Since hypernyms areordered, we decided to preserve only the first hy-pernym ?
semantically more relevant, or at leaststatistically ?
and to cut edges to other hypernyms.000000010010000 0100001010 01001AB C FD E000001100 010AB C FD EAB C FD EFigure 1: Construction of isometric embedding ona sample tree.
For this six nodes tree, the embed-ding requires five bits.9Our experiments in Table 1 shows that using theobtained tree instead of the lattice keeps a highcorrelation (r = .919, ?
= .931) with the origi-nal LCH distance, thus validating the approach.
(Wilkeit, 1990) showed that any k-ary tree ofsize n can be embedded into Qn?1.
We give anisometric embedding algorithm, which is linearin time and space, exhibiting a much better timecomplexity than Winkler?s generic approach forgraphs, running in O(n5) (Winkler, 1984).
Start-ing with an empty binary signature, the algorithmis the following: at each step of a depth-first pre-order traversal: if the node has k children, we setthe signature for the i-th child by appending k ze-roes to the parent?s signature and by setting the i-thof the k bits to one.
An example is given in Figure1.
However, when using real-world datasets suchas Wordnet, the embedding still requires severalthousands of bits to represent a node.
This dimen-sion reduction to tens of kilobits per node remainsfar from our goal of several CPU words, and callsfor a task-specific approach.Looking at the construction of the isometric em-bedding, the large dimension results from the ap-pending of bits to all nodes in the tree.
This resultsin a large number of bits that are rarely set to one.At the opposite, the optimal embedding in termsof dimension is given by the approach of (Chenand Stallmann, 1995) that assigns gray codes toeach node.
However, the embedding is not isomet-ric and introduces a very large error.
As shown inTable 1, this approach gives the most compact em-bedding with dlog2(87,000)e = 17 bits, but leadsto poor correlations (r = .235 and ?
= .186).An exhaustive search is also out of reach: fora fixed dimension n and r nodes in the tree, thenumber of combinations C is given by:C =(2n)!(n?
r)!Even with the smallest value of n = 17 and r =87,000, we have C > 1010,000.
With n = 64, toalign to a CPU word, C > 10100,000.3 Non-isometric EmbeddingOur approach is a trade-off between the isomet-ric embedding and the pre-order gray code solu-tion.
When designing our algorithm, we had todecide which tree distance we will preserve, eitherbetween parent and children, or among siblings.Therefore, we take into account the nature of thetree that we aim to embed into the hypercube.
LetB C D E FA0000000001 00010 00100 01000 10000B C D E FA000001 011 010 110 101B C D E FA00000001 0010 0100 1000 1001Isometric Pre-order Gray CodeRMSE=.66, r=-0.07,  ?=-0.12Additional bit and sortingRMSE=.33, r=.55,  ?=-.57B C D E FA000001 010 100 101 011Value sortingRMSE=.6, r=.19,  ?=.16Figure 2: Approaches to reduce the tree embed-ding dimensions.first analyse the characteristics of the tree obtainedfrom the cut.
The tree has an average branchingfactor of 4.9, with a standard deviation of 14 and96% of the nodes have a branching factor lesserthan 20.
At the opposite, the depth is very stablewith an average of 8.5, a standard deviation of 2,and a maximum of 18.
Consequently, we decideto preserve the parent-children distance over thevery unstable siblings distance.
To lower the di-mensions, we aim at allocating less than k bits fora node with k children, thus avoiding the signatureextension taking place for every node in the iso-metric approach.
Our approach uses the followingprinciples.Branch inheritance: each node inherits thesignature from its father, but contrary to isometricembedding, the signature extension does not ap-ply to all the nodes in the tree.
This guarantees thecompactness of the structure.Parentship preservation: when allocating lessbits than required for the isometric embedding,we introduce an error.
Our allocation favours asmuch as possible the parentship distance at theexpense of the sibling distance.
As a first allo-cation, for a node with k children, we allocatedlog2(k + 1)e bits for the signatures, in order toguarantee the unicity of the signature.
Each childnode is assigned a signature extension using agray code generation on the dlog2(k + 1)e bits.The parent node simply extends its signature withdlog2(k + 1)e zeroes, which is much more com-pact than the k bits from the isometric embeddingalgorithm.Word alignment: The two previous techniquesgive a compact embedding for low-depth trees,which is the case of Wordnet.
The dimension D10of the embedding is not necessarily aligned toa CPU word size W : kW ?
D ?
(k + 1)W .We want to exploit the potential (k + 1)W ?
Dbits that are unused but still processed by theCPU.
For this purpose we rank the nodes alonga value v(i), i ?
N to decide which nodes areallowed to use extra bits.
Since our approachfavours parent/child distance, we want to allowadditional bits for nodes that are both close tothe root and the head of a large branch.
To bal-ance the two values, we use the following formula:v(i) = (maxdepth?
depth(i)) ?
log(sizebranch(i))We therefore enable our approach to take fulladvantage of the otherwise unused bits.In order to enhance the quality of the embed-ding, we also introduce two potential optimiza-tions:The first is called Children-sorting: we allocatea better preserving signature to children havinglarger descents.
A better signature is among theavailable the 2dlog2(k+1)eavailable, the one that re-duces the error with the parent node.
We rank thechildren by the size of their descent and assign thesignatures accordingly.The second optimization is named Value-sorting and is depicted in Figure 2.
Among the2dlog2(k+1)eavailable signatures, only k + 1 willbe assigned (one for the parent and k for the chil-dren).
For instance in the case of 5 children asdepicted in Figure 2, we allocate 3 bits for 6 signa-tures.
We favor the parentship distance by select-ing first the signatures where one bit differs fromthe parent?s one.4 ExperimentsIn this section, we run two experiments to eval-uate both the soundness and the performance ofour approach.
In the first experiment, we test thequality of our embedding against the tree distanceand the LCH similarity.
The goal is to assess thesoundness of our approach and to measure the cor-relation between the approximate embedding andthe original LCH similarity.In the second experiment we compare the com-putational performance of our approach against anoptimized in-memory library that implements theLCH similarity.Our algorithm called FSE for Fast SimilarityEmbedding, is implemented in Java and avail-able publicly1.
Our testbed is an Intel Xeon E31Source code, binaries and instructions to reproduce80 90 100 110 120 1300.750.80.850.9Embedding dimensionPearson?srCombinedBase + value sortingBase + children sortingFSE-BaseFigure 3: FSE: influence of optimizations and di-mensions on the correlation over the tree distanceon Wordnet.1246v3 with 16GB of memory, a 256Go PCI Ex-press SSD.
The system runs a 64-bit Linux 3.13.0kernel with Oracle?s JDK 7u67.The FSE algorithm is implemented in variousflavours.
FSE-Base denotes the basic algorithm,containing none of the optimizations detailed inthe previous section.
FSE-Base can be aug-mented with either or both of the optimizations.This latter version is denoted FSE-Best.4.1 EmbeddingWe first measure the correlation of the embeddeddistance with the original tree distance, to validatethe approach and to determine the gain induced bythe optimizations.
Figure 3 shows the influenceof dimensions and optimizations on the Pearson?sproduct moment correlation r. The base versionreaches r = .77 for an embedding of dimension128.
Regarding the optimizations, children sort-ing is more efficient than value sorting, exceptedfor dimensions under 90.
Finally, combined opti-mizations (FSE-Best) exhibit a higher correlation(r = .89) than the other versions.We then measure the correlation with the Lea-cock & Chodorow similarity measure.
We com-pare our approach to the gray codes embeddingfrom (Chen and Stallmann, 1995) as well as theisometric embedding.
We compute the correlationon 5 millions distances from the Wordnet-Corenoun pairs2(Table 1).
As expected, the embed-the experiments are available at http://demo-satin.telecom-st-etienne.fr/FSE/2https://wordnet.princeton.edu/wordnet/download/standoff/11Embedding Bits Pearson?s r Spearman?s ?Chen et al 17 .235 .186FSE-Base 84 .699 .707FSE-Best 128 .819 .829Isometric 84K .919 .931Table 1: Correlations between LCH, isometric em-bedding, and FSE for all distances on all Wordnet-Core noun pairs (p-values ?
10?14).Algorithm MeasureAmount of pairs (n)103104105106107WS4J 103?
ms 0.156 1.196 11.32 123.89 1,129.3FSE-Best ms 0.04 0.59 14.15 150.58 1,482speedup ?3900 ?2027 ?800 ?822 ?762Table 2: Running time in milliseconds for pairwisesimilarity computations.ding obtained using gray codes present a very lowcorrelation with the original distance.Similarly to the results obtained on the tree dis-tance correlation, FSE-Best exhibits the highestscores with r = .819 and ?
= .829, not far fromthe theoretical bound of r = .919 and ?
= .931for the isometric embedding of the same tree.
Ourapproach requires 650 times less bits than the iso-metric one, while keeping strong guarantees on thecorrelation with the original LCH distance.4.2 SpeedupTable 4.2 presents the computation time of theLCH similarity.
This is computed using WS4J3, anefficient library that enables in-memory caching.Because of the respective computational com-plexities of the Hamming distance and the shortestpath algorithms, FSE unsurprisingly boosts LCHsimilarity computation by orders of magnitudes.When the similarity is computed on a small num-ber of pairs (a situation of the utmost practical in-terest), the factor of improvement is three ordersof magnitude.
This factor decreases to an amountof 800 times for very large scale applications.
Thereason of the decrease is that WS4J caching mech-anism becomes more efficient for larger numbersof comparisons.
As the caching system storesshortest path between nodes, these computed val-ues are more likely to be a subpath of anotherquery when the number of queries grows.3https://code.google.com/p/ws4j/5 ConclusionWe proposed in this paper a novel approach basedon metric embedding to boost the computation ofshortest-path based similarity measures such asthe one of Leacock & Chodorow.
We showed thatan isometric embedding of the Wordnet?s hyper-nym/hyponym lattice does not lead to a practicalsolution.
To tackle this issue, we weaken the lat-tice structure into a tree by cutting less relevantedges.
We then devised an algorithm and severaloptimizations to embed the tree shortest-path dis-tance in a word-aligned number of bits.
Such anembedding can be used to boost NLP core algo-rithms ?
this was demonstrated here on the com-putation of LCH for which our approach offers afactor of improvement of three orders of magni-tude, with a very strong correlation.AcknowledgementsThis work is supported by the OpenCloudwareproject.
OpenCloudware is funded by the FrenchFSN (Fond national pour la Soci?et?e Num?erique),and is supported by P?oles Minalogic, Systematicand SCS.ReferencesJacob Andreas and Dan Klein.
2014.
How much doword embeddings encode about syntax.
In Associa-tion for Computational Linguistics (ACL).Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using wordnet.
In Computational linguis-tics and intelligent text processing, pages 136?145.Springer.Pierpaolo Basile, Annalina Caputo, and Giovanni Se-meraro.
2014.
An enhanced lesk word sense dis-ambiguation algorithm through a distributional se-mantic model.
In Proceedings of COLING, pages1591?1600.Islam Beltagy, Katrin Erk, and Raymond Mooney.2014.
Semantic parsing using distributional seman-tics and probabilistic logic.
Association for Compu-tational Linguistics (ACL), page 7.Raffaella Bernardi, Georgiana Dinu, Marco Marelli,and Marco Baroni.
2013.
A relatedness benchmarkto test the role of determiners in compositional dis-tributional semantics.
In Association for Computa-tional Linguistics (ACL), pages 53?57.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.12Woei-Kae Chen and Matthias FM Stallmann.
1995.On embedding binary trees into hypercubes.Journal of Parallel and Distributed Computing,24(2):132?138.St?ephane Clinchant and Florent Perronnin.
2013.
Ag-gregating continuous word embeddings for informa-tion retrieval.
Association for Computational Lin-guistics (ACL).Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.M.
Deza and M. Laurent.
1997.
Geometry of Cuts andMetrics.
Springer, 588 pages.Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-maya Sachan, Shashank Srivastava, and Eduard HHovy.
2013.
A structured distributional semanticmodel for event co-reference.
In Association forComputational Linguistics (ACL), pages 467?473.Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-FuChang, and Sung-Eui Yoon.
2012.
Spherical hash-ing.
In Computer Vision and Pattern Recognition(CVPR), 2012 IEEE Conference on, pages 2957?2964.
IEEE.Aur?elie Herbelot and Mohan Ganesalingam.
2013.Measuring semantic content in distributional vec-tors.
In Association for Computational Linguistics(ACL), pages 440?445.Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic frame iden-tification with distributed word representations.
InAssociation for Computational Linguistics (ACL).Graeme Hirst and David St-Onge.
1998.
Lexicalchains as representations of context for the detec-tion and correction of malapropisms.
WordNet: Anelectronic lexical database, 305:305?332.Jay J Jiang and David W Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
Proceedings of the 10th Research on Com-putational Linguistics International Conference.Igor Labutov and Hod Lipson.
2013.
Re-embeddingwords.
In ACL (2), pages 489?493.Angeliki Lazaridou, Marco Marelli, Roberto Zampar-elli, and Marco Baroni.
2013.
Compositional-lyderived representations of morphologically complexwords in distributional semantics.
In Associationfor Computational Linguistics (ACL), pages 1517?1526.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and wordnet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.R?emi Lebret, Jo?el Legrand, and Ronan Collobert.2013.
Is Deep Learning Really Necessary for WordEmbeddings?
Technical report, Idiap.Chen Li and Yang Liu.
2014.
Improving text normal-ization via unsupervised model and discriminativereranking.
Association for Computational Linguis-tics (ACL), page 86.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In ICML, volume 98, pages 296?304.Dimitrios Mavroeidis, George Tsatsaronis, MichalisVazirgiannis, Martin Theobald, and GerhardWeikum.
2005.
Word sense disambiguation forexploiting hierarchical thesauri in text classification.In Knowledge Discovery in Databases: PKDD2005, pages 181?192.
Springer.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In AAAI, vol-ume 6, pages 775?780.Tomas Mikolov, Kai Chenand, Greg Corradoand, andJeffrey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
In Proceedings ofthe International Conference on Learning Represen-tations.George A Miller and Walter G Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andcognitive processes, 6(1):1?28.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 2265?2273.
Curran Associates, Inc.Mohammad Norouzi, Ali Punjani, and David J Fleet.2012.
Fast search in hamming space with multi-index hashing.
In Computer Vision and PatternRecognition (CVPR), 2012 IEEE Conference on,pages 3108?3115.
IEEE.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
InProceedings of the 14th International Joint Confer-ence on Artificial Intelligence - Volume 1, IJCAI?95,pages 448?453, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Nuno Seco, Tony Veale, and Jer Hayes.
2004.
An in-trinsic information content metric for semantic sim-ilarity in wordnet.
In ECAI, volume 16, page 1089.Giannis Varelas, Epimenidis Voutsakis, ParaskeviRaftopoulou, Euripides GM Petrakis, and Evange-los E Milios.
2005.
Semantic similarity methodsin wordnet and their application to information re-trieval on the web.
In Proceedings of the 7th an-nual ACM international workshop on Web informa-tion and data management, pages 10?16.
ACM.13Elke Wilkeit.
1990.
Isometric embeddings in ham-ming graphs.
Journal of Combinatorial Theory, Se-ries B, 50(2):179?197.Peter M Winkler.
1984.
Isometric embedding in prod-ucts of complete graphs.
Discrete Applied Mathe-matics, 7(2):221?225.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, pages 133?138.
Association for Com-putational Linguistics.Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao.
2014.Product feature mining: Semantic clues versus syn-tactic constituents.
In Association for Computa-tional Linguistics (ACL), pages 336?346.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation questionanswering.
In Proceedings of ACL.14
