Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213?1223,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatic Knowledge Acquisition for Case Alternationbetween the Passive and Active Voices in JapaneseRyohei Sasano1 Daisuke Kawahara2 Sadao Kurohashi2 Manabu Okumura11 Precision and Intelligence Laboratory, Tokyo Institute of Technology2 Graduate School of Informatics, Kyoto University{sasano,oku}@pi.titech.ac.jp, {dk,kuro}@i.kyoto-u.ac.jpAbstractWe present a method for automatically acquir-ing knowledge for case alternation betweenthe passive and active voices in Japanese.
Byleveraging several linguistic constraints on al-ternation patterns and lexical case frames ob-tained from a large Web corpus, our methodaligns a case frame in the passive voice to acorresponding case frame in the active voiceand finds an alignment between their cases.We then apply the acquired knowledge to acase alternation task and prove its usefulness.1 IntroductionPredicate-argument structure analysis is one of thefundamental techniques for many natural languageapplications such as recognition of textual entail-ment, information retrieval, and machine transla-tion.
In Japanese, the relationship between a pred-icate and its argument is usually represented by us-ing case particles1 (Kawahara and Kurohashi, 2006;Taira et al 2008; Yoshikawa et al 2011).
However,since case particles vary depending on the voices,we have to take case alternation into account to rep-resent predicate-argument structure.
There are thustwo major types of representations: one uses surfacecases, and the other uses normalized-cases for thebase form of predicates.
For example, while the Ky-oto University Text Corpus (Kawahara et al 2004),one of the major Japanese corpora that contains an-notations of predicate-argument structures, adopts1Japanese is a head-final language.
Word order does notmark syntactic relations.
Instead, postpositional case particlesfunction as case markers.the former representation, the NAIST Text Corpora(Iida et al 2007), another major Japanese corpus,adopts the latter representation.Examples (1) and (2) describe the same event inthe passive and active voices, respectively.
Whenwe use surface cases to represent the relationship be-tween the predicate and its argument in Example (1),the case of ??
(woman)?
is ga2 and the case of ??(man)?
is ni.2 On the other hand, when we use thenormalized-cases for the base form, the case of ??(woman)?
is wo2 and the case of ??
(man)?
is ga,which are the same as the surface cases in the activevoice as in Example (2).
(1) ??
??
???????
?woman-ga man-ni was pushed down(A woman was pushed down by a man.
)(2) ??
??
??????
?man-ga woman-wo pushed down(A man pushed down a woman.
)Both representations have their own advantages.Surface case analysis is easier than normalized-caseanalysis, especially when we consider omitted ar-guments, which are also called zero anaphors (Na-gao and Hasida, 1998).
In Japanese, zero anaphorafrequently occurs, and the omitted unnormalized-case of a zero anaphor is often the same as thesurface case of its antecedent (Sasano and Kuro-hashi, 2011).
Therefore, surface case analysis suitszero anaphora resolution.
On the other hand, when2Ga, wo, and ni are typical Japanese postpositional case par-ticles.
In most cases, they indicate nominative, accusative, anddative, respectively.1213we focus on the resulting predicate argument struc-tures, the normalized-case structure is more useful.Specifically, since a normalized-case structure rep-resents the same meaning in the same representa-tion, normalized-case analysis is useful for recog-nizing textual entailment and information retrieval.Therefore, we need a system that first analyzessurface cases and then alternates the surface caseswith normalized-cases.
In particular, we focus onthe transformation of the passive voice into the ac-tive voice in this paper.
Passive-to-active voicetransformation in English can be performed system-atically, which does not depend on lexical infor-mation in most cases.
However, in Japanese, themethod of transformation depends on lexical infor-mation.
For example, while the case particle ni inExample (1) is alternated with ga in the active voice,the case particle ni in Example (3) is not alternated inthe active voice as in Example (4) even though boththeir predicates are ????????
(be pusheddown).?
(3) ??
??
???????
?woman-ga sea-ni was pushed down(A woman was pushed down into the sea.
)(4) ??
??
??????
?woman-wo sea-ni pushed down(?
pushed down a woman into the sea.
)The ni case in Example (1) indicates agent.
Onthe other hand, the ni case in Example (3) indicatesdirection.
To determine the difference is importantfor many NLP applications including machine trans-lation.
In fact, Google Translate (GT)3 translatesExamples (1) and (3) as ?Woman was pushed downin the man?
and ?Woman was pushed down in thesea,?
respectively, which may be because GT cannotdistinguish between the roles of ni in Examples (1)and (3).
(5) ??
??
????
?prize-ga man-ni was awarded(A prize was awarded to a man.
)In example (5), although the ni-case argument??
(man)?
is the same as in Example (1), the caseparticle ni indicates recipient and is not alternatedin the active voice.
These examples show that case3http://translate.google.com, accessed 2013-2-20.alternation between the passive and active voices inJapanese depends on not only predicates but also ar-guments, and we have to consider their combina-tions.
Since it is impractical to manually describethe case alternation rules for all combinations ofpredicates and arguments, we have to acquire suchknowledge automatically.Thus, in this paper, we present a method for ac-quiring the knowledge for case alternation betweenthe passive and active voices in Japanese.
Ourmethod leverages several linguistic constraints on al-ternation patterns and lexical case frames obtainedfrom a large Web corpus, which are constructed foreach meaning and voice of each predicate.2 Related WorkLevin (1993) grouped English verbs into classes onthe basis of their shared meaning components andsyntactic behavior, defined in terms of diathesis al-ternations.
Hence, diathesis alternations have beenthe topic of interest for a number of researchersin the field of automatic verb classification, whichaims to induce possible verb frames from corpora(e.g., McCarthy 2000; Lapata and Brew 2004; Joa-nis et al2008; Schulte im Walde et al2008; Li andBrew 2008; Sun and Korhonen 2009; Theijssen et al2012).
Baroni and Lenci (2010) used distributionalslot similarity to distinguish between verbs undergo-ing the causative-inchoative alternations, and verbsthat do not alternate.There is some work on passive-to-active voicetransformation in Japanese.
Baldwin and Tanaka(2000) empirically identified the range and fre-quency of basic verb alternation, including active-passive alternation, in Japanese.
They automaticallyextracted alternation types by using hand-craftedcase frames but did not evaluate the quality.
Kondoet al(2001) dealt with case alternation between thepassive and active voices as a subtask of paraphras-ing a simple sentence.
They manually introducedcase alternation rules on the basis of verb types andcase patterns and transformed passive sentences intoactive sentences.Murata et al(2006) developed a machine-learning-based method for Japanese case alterna-tion.
They extracted 3,576 case particles in passivesentences from the Kyoto University Text Corpus1214Case particle Grammatical functionga nominativewo accusativeni dativede locative, instrumentalkara ablativeno genitiveTable 1: Examples of Japanese postpositional case parti-cles and their typical grammatical functions.and tagged their cases in the active voice.
Then,they trained SVM classifiers using the tagged cor-pus.
Their features for training SVM were madeby using several lexical resources such as IPAL(IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo(NLRI, 1993), and the output of Kondo et alsmethod.3 Lexicalized Case FramesTo acquire knowledge for case alternation, we ex-ploit lexicalized case frames that are automaticallyconstructed from 6.9 billion Web sentences by usingKawahara and Kurohashi (2002)?s method.
In short,their method first parses the input sentences, andthen constructs case frames by collecting reliablemodifier-head relations from the resulting parses.These case frames are constructed for each predi-cate like PropBank frames (Palmer et al 2005), foreach meaning of the predicate like FrameNet frames(Fillmore et al 2003), and for each voice.
However,neither pseudo-semantic role labels such as Arg1 inPropBank nor information about frames defined inFrameNet are included in these case frames.
Eachcase frame describes surface cases that each predi-cate has and instances that can fill a case slot, whichis fully lexicalized like the subcategorization lexiconVALEX (Korhonen et al 2006).We list some Japanese postpositional case parti-cles with their typical grammatical functions in Ta-ble 1 and show examples of case frames in Table2.4 Ideally, one case frame is constructed for eachmeaning and voice of the target predicate.
However,since Kawahara and Kurohashi?s method is unsuper-vised, several case frames are actually constructed4Niyotte in Table 2 is a Japanese functional phrase that in-dicates agent in this case.
We treat niyotte as a case particle inthis paper for the sake of simplicity.Case Frame: ???????
?-4 (be pushed down-4)?{??
(woman):5,?
(I):2,?
(woman):2, ?
?
?
}-ga{?
(sea):229,?
(bottom):115,?
(pond):51, ?
?
?
}-ni{??(stepmother):2,????
(Pegasus):2, ?
?
?
}-niyotte?
?
?Case Frame: ???????
?-5 (be pushed down-5)?{??
(Kyoko):3,??
(manager):1, ?
?
?
}-ga{??
(someone):143,???
(somebody):85, ?
?
?
}-ni{??
(stair):20,?
(ship):7,?
(cliff):7, ?
?
?
}-kara?
?
?Case Frame: ?????
?-2 (push down-2)?{?
(man):14,??
(lion):5,?
(tiger):3, ?
?
?
}-ga{?(child):316,??(child):81,?
(person):51, ?
?
?
}-wo{?
(sea):580,?
(ravine):576,?
(river):352 ?
?
?
}-ni?
?
?Case Frame: ?????
?-4 (push down-4)?{??
(someone):14,????
(lion):5, ?
?
?
}-ga{?
(person):257,?
(I):214,?
(child):137, ?
?
?
}-wo{?
(cliff):53,??
(stair):28, ?
?
?
}-kara?
?
?Table 2: Examples of case frames for ????????
(be pushed down)?
and ??????
(push down).
?Words in curly braces denote instances that can fill cor-responding cases and the numbers following these wordsdenote their frequency in the corpus.for each meaning and voice.
For example, 59 andeight case frames were respectively constructed forthe predicate in the passive voice ????????
(be pushed down)?
and in the active voice ??????
(push down)?
from 6.9 billion Web sentences.Table 2 shows the 4th and 5th case frames for ????????
(be pushed down)?
and the 2nd and 4thcase frames for ??????
(push down).
?Table 3 shows an example of case frames for???
(hit),?
which includes no-case.
Here, theJapanese postpositional case particle ?no?
roughlycorresponds to ?of,?
that is, ?X no Y?
means ?Y ofX,?
and thus no-case is not an argument of the targetpredicate.
While Kawahara and Kurohashi?s methodbasically collects arguments of the target predicate,the phrase of no-case that modifies the direct objectof the predicate is also collected as no-case.
Thisis because, as we will show in the next section, thisphrase can be represented as ga-case in the passivevoice.1215Case Frame: ??
?-2 (hit-2)?{?
(man):51,?
(fist):30,??
(someone):23, ?
?
?
}-ga{??
(myself):360,?
(I):223, ?
?
?
}-no{?
(head):5424,?
(face):3215, ?
?
?
}-wo{?
(fist):316,??
(palm):157,??
(fist):126, ?
?
?
}-de?
?
?Table 3: An example of case frames for ???
(hit).
?4 Passive-Active Transformation inJapaneseMorphologically speaking, the passive voice inJapanese is expressed by using the auxiliary verbs???
(reru)?
and ????
(rareru),?
whose pastforms are ???
(reta)?
and ????
(rareta),?
re-spectively.
For example, the verb in the base form??????
(tsukiotosu, push down)?
is trans-formed into the past passive form ????????
(tsukiotosa-reta, was pushed down).?
Case al-ternations accompany passive-active transformationin Japanese.
There are only two case alternationsat most in passive-active transformation.
One is thecase represented as ga in the passive voice, and theother is the case represented as ga in the active voice.Japanese passive sentences can be classified intothree types in accordance with what is representedas ga-case in the passive voice: direct passive, in-direct passive, and possessor passive.In direct passive sentence, the object of the pred-icate in the active voice is represented as ga-case.Examples (1), (3), and (5) are all direct passive sen-tences.
The case that is represented as ga in the ac-tive voice is usually represented as ni, niyotte, kara,or de in the passive sentence.
In the first sentence ofExamples (6) and (7),5 ga-cases in the active voiceare represented as niyotte and kara, respectively.
Onthe other hand, ga-case in the passive sentence is al-ternated with wo or ni as shown with broken lines inthe second sentence of Examples (6) and (7).
(6) P: ???......
?????
??????cause-ga.....
man-niyotte was identified(The cause was identified by a man.
)A: ??
???......
????
?man-ga cause-wo...... identified(A man identified the cause.)5?P?
denotes a passive sentence and ?A?
denotes the corre-sponding active sentence in these examples.
(7) P: ??......
???
????????man-ga.....
woman-kara was talked to(A man was talked to by a woman.
)A: ??
??......
?????
?woman-ga man-ni.... talked to(A woman talked to a man.
)Indirect passive is also called adversative pas-sive, in which an indirectly influenced agent is repre-sented with ga. For example, ??
(I),?
the argumentrepresented with ga in the first sentence of Exam-ple (8), does not appear in the active voice, i.e.
thesecond sentence of Example (8).
In the case of in-direct passive, ga-case in the active sentence is al-ways alternated with ni-case in the passive sentenceas shown with solid lines in Examples (8).
(8) P: ??......
???
?????I-ga.....
child-ni was cried(I?ve got a child crying.
)A: ???
????
(A child cried.
)child-ga criedPossessor passive is similar to indirect passive inthat the argument represented with ga-case does notappear as an argument of the predicate in the ac-tive voice.
Therefore, possessor passive is some-times treated as a kind of indirect passive.
How-ever, in the case of possessor passive, the argumentappears in the active sentence as a possessor of thedirect object.
For example, the ga-case argument??
(woman)?
in the passive sentence of Example(9) does not appear as an argument of the predicate????
(hit)?
in the active sentence but appears inthe phrase that modifies the direct object ??
(head)?with the case particle no, which indicates that ??(woman)?
is the possessor of ??
(head).?
(9) P: ??......
??
??
?????woman-ga.....
man-ni head-wo was hit(A woman was hit on the head by a man.)A:??
??......
??
???
?man-ga woman-no..... head-wo hit(A man hit the head of a woman.
)In conclusion, the number of case alternation pat-terns accompanying passive-active transformation inJapanese is limited.
Ga-case in the passive voice can1216be alternated only with either wo, ni, or no, or doesnot appear in the active voice.
Ga-case in the activevoice can be represented only by ni, niyotte, kara,or de in the passive voice.
Hence, it is sufficient toconsider only their combinations.5 Knowledge Acquisition for CaseAlternation5.1 Task DefinitionOur objective is to acquire knowledge for case al-ternation between the passive and active voices inJapanese.
We leverage lexical case frames obtainedfrom a large Web corpus by using Kawahara andKurohashi (2002)?s method and align cases of a caseframe in the passive voice and cases of a case framein the active voice.
As described in Section 2, sev-eral case frames are constructed for each voice ofeach predicate.
Our task consists of the followingtwo subtasks:1.
Identify a corresponding case frame in the ac-tive voice.2.
Find an alignment between cases of caseframes in the passive and active voice.Figure 1 shows the overview of our task.
If a caseframe in the passive voice is input, we identify a cor-responding case frame in the active voice, and findan alignment between cases by using the algorithmdescribed in Section 5.3.
In this example, an activecase frame ?????
?-4 (push down-4)?
is iden-tified as a corresponding case frame for the inputpassive case frame ???????
?-5 (be pusheddown-5)?
and ga, ni, and kara-cases in the passivecase frame are aligned to wo, ga, and kara-cases inthe active case frame, respectively.5.2 Clues for Knowledge AcquisitionWe exploit three clues for corresponding case frameidentification and case alignment as follows:1.
Semantic similarity between the instances ofthe aligned cases: simSEM .2.
Case distribution similarity between the corre-sponding case frames: simDIST .3.
Preference of alternation patterns: fPP .&DVH)UDPH SXVKGRZQ^DZRUG 	ZRUGV `JD^,SHUVRQ`ZR^ERWWRPKHOO`QL&DVH)UDPH SXVKGRZQ^PDQ OLRQ `JD^FKLOGFKLOG`ZR^VHDUDYLQH ULYHU`QL&DVH)UDPH SXVKGRZQ^UDSLVW DZRUG `JD^,SHRSOH	IXQ`ZR^ERWWPIHDU`QL&DVH)UDPH SXVKGRZQ^ VRPHRQH !"# OLRQ `ga^SHUVRQ,FKLOG`wo^$ FOLII%& VWDLU`kara&DVH)UDPH '()EHSXVKHGGRZQ^*.\RWR +,PDQDJHU `ga^ VRPHRQH-.
VRPHERG\`ni^%&VWDLU/VKLS$FOLII`kara,QSXWDFDVHIUDPHLQWKHSDVVLYHYRLFH&DVHIUDPHVIRU?SXVKGRZQ?LQWKHDFWLYHYRLFH ,GHQWLI\DFRUUHVSRQGLQJFDVHIUDPH )LQGDQDOLJQPHQWEHWZHHQFDVHVFigure 1: The overview of our task.Semantic similarity The instances of the alignedcases should be similar.
For example, the instancesof the ga-case of the case frame ???????
?-5 (be pushed down-5)?
and the wo-case of thecase frame ?????
?-4 (push down-4),?
whichare considered to be aligned and represent patient,are similar.
Thus, we exploit semantic similaritysimSEM between the instances of the correspondingcases.We first define an asymmetric similarity measurebetween C1and C2, each of which is a set of caseslot instances, as follows:sima(C1, C2) =1|C1|?i1?C1maxi2?C2(sim(i1, i2)),where sim(i1, i2) is the similarity between instances.In this study, we apply a distributional similaritymeasure (Lin, 1998), which was computed fromthe Web corpus used to construct the case frames.We next define a symmetric similarity measure be-tween C1and C2as an average of sima(C1, C2) andsima(C2, C1).sims(C1, C2)=12(sima(C1, C2)+sima(C2, C1)).Then we define semantic similarity of a casealignment A between case frames CF1and CF2.simSEM (A) =1NN?i=1sims(C1,i, C2,a(i)),1217where N denotes the number of case slots of CF1,C1,idenotes a set of instances of the i-th case slot ofCF1, and C2,a(i)denotes the set of the aligned caseinstances of CF2.
A denotes the alignment {c1,1?c2,a(1), c1,2?c2,a(2), .
.
.
, c1,N?c2,a(N)} where cn,idenotes the case name that corresponds to Cn,i.Case distribution similarity Although argumentsare often omitted in Japanese, arguments that areusually mentioned explicitly in the passive voicewill be also explicitly mentioned in the active voice.Hence, the frequency distribution of cases can be aclue for case alignment.
In this study, we exploit thefollowing cosine similarity of frequency distributionas case distribution similarity:simDIST (A)=cos((|C1,1|, .
.
.
, |C1,N |),(|C2,a(1)|, .
.
.
, |C2,a(N)|)).As an example, consider the alignment between apassive case6 ????
?-1 (be selected-1)?
and thecorresponding active case frame ??
?-13 (select-13)?
in Table 4.
The alignment A1= {ga ?wo, ni?ni,NIL?ga} is considered to be correct.However, if we consider only the semantic similar-ity, an alignment A2= {ga ?
ni, ni ?
ga, wo ?wo} is selected, because the alignment A2has thehighest semantic similarity.
On the other hand, thecase distribution similaritysimDIST (A1) = cos((17722, 122273, 0),(33338, 800, 382))?
0.167is much larger thansimDIST (A2) = cos((17722, 122273, 96),(800, 382, 33338))?
0.016.Thus, the alignment A1would be selected by con-sidering the case distribution similarity.Preference of alternation patterns Some alter-nation patterns often appear, and others do not.For example, as Murata et al(2006) reported,whereas 96.47% of ga-case is alternated with wo-case in passive-active transformation in Japanese,6This case frame should not have wo-case.
However, sincewe constructed case frames automatically, some case frameshave improper cases.Case Frame: ????
?-1 (be selected-1)?{??
(player):1119,??
(work):983, ?
?
?
}-ga:17722{??
(representative):18295, ?
?
?
}-ni:122273{??
(work):5,??
(mayor):3, ?
?
?
}-wo:96?
?
?Case Frame: ??
?-13 (select-13)?{?
(I):14,??
(teacher):18, ?
?
?
}-ga:382{???
(award):42,????
(single):17, ?
?
?
}-ni:800{?
(tune):16666,??
(work):9967, ?
?
?
}-wo:33338?
?
?Table 4: Case frames ????
?-1 (be selected-1)?and ??
?-13 (select-13).?
The numbers following casenames denote the total numbers of case slot instances.only 27.38% of ni-case is alternated with ga-case.Therefore, when we can use development data, weexploit a weighting factor fPP (A) that is deter-mined on the development data and takes into ac-count the preference of alternation patterns.
We de-fine fPP (A) as follows:fPP (A)=w(ga?cga to)?w(cto ga?ga), (i)where cga tois the case in the active voice to whichga-case in the passive voice is aligned, cto gais thecase in the passive voice which is aligned to ga-case in the active voice, and w(c1?
c2) denotes theweight of the case alternation ?c1?c2.
?5.3 AlgorithmAlgorithm 1 presents our algorithm for identifyinga corresponding case frame and finding an align-ment between cases in pseudo-code.
Our algo-rithm first makes all possible combinations of acase frame in the active voice (cfactive), a case inthe active voice to which ga-case in the passivevoice is aligned (cga to), and a case in the passivevoice which is aligned to ga-case in the active voice(cto ga) on the basis of the linguistic constraints,and then evaluates the score for the combinations{cfactive, cga to, cto ga} by the following equation:score=simSEM (A)?simDIST (A)?
?fPP (A), (ii)where ?
is a parameter that controls the impact ofthe case distribution similarity.7 When we can use7Since fPP (A) is defined with a set of weights of case alter-nation patterns, fPP (A) contains these weights implicitly, andthus there is only a single explicit weight in equation (ii).1218Algorithm 1: Identifying a corresponding caseframe and finding an alignment between cases.Input: a case frame in the passive voice: cfpassive, anda set of case frames in the active voice: CFSactiveOutput: a case frame and an alignment between cases: A1: max score = 0, A = ()2: for each cfactive ?
CFSactive3: for each cga to ?
{wo, ni, no, NIL}4: for each cto ga ?
{ni, niyotte, kara, de, NIL}5: if (!occur(cga to, cto ga)) then continue6: A?
= (cfactive, cga to, cto ga)7: score=simSEM (A?
)?simDIST (A?)?
?fPP (A?
)8: if (score > max score) then9: (max score, A) = (score, A?
)10: end for11: end for12: end fordevelopment data, we tune ?
on the developmentdata; otherwise we set ?
= 1.
Since some combi-nations of cga toand cto ganever occur, our algo-rithm filters them out in line 5 of the algorithm.
Af-ter checking all combinations, the combination withthe highest score is output.6 Evaluation of the Acquired KnowledgeWe applied our algorithm to the case frames thatare automatically constructed from a corpus consist-ing of about 6.9 billion Japanese sentences from theWeb.
Of course, these case frames contain improperones, that is, several frames mix several meaningsor usages of the predicates.
Thus, it is difficult toevaluate the acquired knowledge itself.
Instead, weevaluate the usefulness of the acquired knowledgeon a case alternation task between the passive andactive voices.6.1 Setting and Algorithm for Case AlternationWe basically used the same data as Murata etal.
(2006).
As mentioned in Section 2, they extracted3,576 case particles in passive sentences from theKyoto University Text Corpus, and tagged theircases in the active voice.
Since they treated posses-sor passive as a kind of indirect passive, they did notadopt the case alternation between ga and no.
In ad-dition, their data included some annotation errors.We thus modified 21 annotations,8 five of which8The modified version of the data is publicly available athttp://alaginrc.nict.go.jp/case/src/kaku1.1.tar.gz.were changed to the case alternation between ga andno.
Note that there were some cases where multiplepossible case particles were tagged to one instance.We adopted evaluation metrics called ?Eval.
B?
byMurata et al that is, we judged the output to be cor-rect when the output was included in possible an-swers.
We performed experiments on the followingthree types of data settings.1.
Experiments without either development ortraining data.2.
Experiments with development data.3.
Experiments with training data.Experiments without either development ortraining data In the first setting, we aligned theinput passive case frame to one of the active caseframes of the same predicate only by using simSEMand simDIST with the parameter ?
= 1.
Therefore,this setting is fully unsupervised.
In this setting, theinput surface cases are alternated as follows:1.
If a passive sentence is input, perform syntac-tic and surface case structure analysis by us-ing Kawahara and Kurohashi (2006)?s model.9Their model identified a proper case frame foreach predicate, and assigned arguments in theinput sentence to case slots of the case frame.2.
By using the acquired knowledge for case alter-nation, alternate input surface cases with casesin the active voice.We call this model Model 1.
For example, if Ex-ample (10) is input, the ga-case argument is assignedto the ga-case of the case frame ???????
?-5(be pushed down-5).?
Since this case is aligned tothe wo-case of the case frame ?????
?-4 (pushdown-4)?
as shown in Figure 1, this ga-case is alter-nated with wo-case.
(10) ??
???????
?woman-ga was pushed down(A woman was pushed down.
)9KNP: http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP1219Algorithm 2: Pseudo-code of the hill-climbingalgorithm for tuning the parameter vector x.1: x = (1.0, 1.0, .
.
.
, 1.0)2: acc = faccuracy(x), pre acc = 03: while acc > pre acc4: pre acc = acc5: for i ?
{0, .
.
.
, |x| ?
1}6: acc+= faccuracy(x0, .
.
.
, xi + 0.1, .
.
.
, x|x|?1)7: acc?= faccuracy(x0, .
.
.
, xi ?
0.1, .
.
.
, x|x|?1)8: if acc+>acc and acc+>acc?then xi =xi+0.18: else if acc?> acc then xi = xi ?
0.19: acc = faccuracy(x)10: end for11: end whileExperiments with development data In the sec-ond setting, we aligned the input passive case frameto one of the active case frames of the same pred-icate by using simSEM , simDIST , and fPP with ?tuned on the development data.
In advance, we di-vided the tagged data into two parts just as Murataet al(2006) did, both of which contained 1,788 caseparticles, and performed 2-fold cross-validation.
Weused one part for development and the other for test-ing, and vice versa.We tuned w(ga ?
cga to), w(cto ga?
ga) inEquation (i), and ?
in Equation (ii) by a simplehill-climbing strategy.
Since the candidate cases forcga toare ni, niyotte, kara, de, and NIL, and the can-didate cases for cto gaare wo, ni, no, and NIL, wedefined parameter vector x as follows:x=(w(ga?ni),w(ga?niyotte),w(ga?kara),w(ga?de),w(ga?NIL),w(wo?ga),w(ni?ga),w(wo?no),w(NIL?ga), ?
).Algorithm 2 shows the hill-climbing algorithm fortuning the parameter vector x, where faccuracy(x) isa function that returns the case alternation accuracyon the development data with parameter x.
This al-gorithm varies one parameter at a time with a step-size of 0.1 until there is no accuracy improvementin the development data.
After acquiring knowledgefor case alternation with the tuned parameter, we ap-plied the same method for case alternation as the firstsetting.
We call this model Model 2.Experiments with training data In the third set-ting, we also performed 2-fold cross validation, thatis, we used one part of the divided tagged corpusModel AccuracysimSEM simDISTParametertuningModel 1S 0.902 (3,224/3,576)Model 1D 0.857 (3,063/3,576)Model 1   0.906 (3,239/3,576)Model 2S  0.928 (3,320/3,576)Model 2D  0.927 (3,314/3,576)Model 2    0.938 (3,353/3,576)Baseline 0.883 (3,159/3,576)Table 5: Experimental results of case alternation withouttraining data.for training and the other for testing, and vice versa.Although we basically applied Murata et al(2006)?smethod, which is based on SVMs, we added the out-put of Model 2 as a new feature.Specifically, we first tuned the parameter vector xon the training data and acquired the knowledge forcase alternation with the tuned parameter.
By us-ing the acquired knowledge, we alternated the inputcases in both the training and test data and obtainedthe resulting case of Model 2.
Note that, we did notuse any annotations for the test data in this process.We then trained the SVMs on the training data andapplied them to the test data using the resulting caseas a new feature.
We call this model Model 3.6.2 Results and DiscussionTable 5 shows the results of the experiments withouttraining data.
Baseline is a system that outputs themost frequently alternated cases in the developmentdata, which was also used by Murata et al(2006).The baseline score was higher than that reported byMurata et albecause we modified 21 annotations.We also performed experiments without using casedistribution similarity or semantic similarity.
Wecall these models in the first setting Model 1SandModel 1D, and these models in the second settingModel 2Sand Model 2D, respectively.Although Models 1S, 1D, and 1 were fully un-supervised models, Models 1Sand 1 significantly10outperformed the baseline model (p-values of Mc-Nemar (1947)?s test were smaller than 0.00001).
Onthe other hand, the difference between Models 1S10In this paper, we call a difference significant if the p-valueof McNemar (1947)?s test is less than 0.01.1220Model Accuracy(Murata et al 2006) 0.944 (3,376/3,576)Model 3 0.956 (3,417/3,576)Table 6: Comparison between Murata et al(2006)?smethod and our method with training data.and 1 is not statistically significant, and thus the ef-fect of the case distribution similarity was not con-firmed by these experiments.Models 2S, 2D, and 2 were models with parame-ter tuning.
Parameter tuning significantly improvedthe performance.
In addition, the difference betweenModels 2Sand 2 and the difference between Models2Dand 2 were both significant (p-values of McNe-mar?s test were 0.00032 and 0.00039, respectively),and thus we confirmed the usefulness of the two sim-ilarity measures.
The parameter ?
that controls theimpact of the case distribution similarity was tunedto 0.3, which means semantic similarity between theinstances of the aligned cases is more important thancase distribution similarity for this task.Table 6 compares Murata et als method and ourmethod with training data.
We used Murata et alsmethod without feature selection because it achievedthe highest performance on this setting.
Theirmethod?s score was higher than that they reported,again due to the corpus modification.
The differencebetween their method and our method was signifi-cant (p-value of McNemar?s test was 0.00011), andwe confirmed the usefulness of the acquired knowl-edge for case alternation.Table 7 shows an example of case alternation be-tween the passive and active voices.
When the pas-sive sentence was input, the argument ??????......(Mr.
Matsuki-ga......)?
was first assigned to ga-caseof the case frame ????
?-2 (be hit-2).?
Since thiscase was aligned to no-case of the case frame ??
?-2 (hit-2),?
the input ga-case was alternated withno-case.
On the other hand, the cases of the other ar-guments ?????
(bat-de)?
and ???
(head-wo)?were output as they were in the passive sentence.We now list three error causes observed in our ex-periments of the case alternation task:1) The passive voice in Japanese is expressed by us-ing the auxiliary verbs ???
(reru)?
and ????(rareru).?
However, these auxiliary verbs can rep-Input Text:?
?
?
?????.....
??????
??
?????
?
?Mr.
Matsuki-ga..... metal bat-de head-wo was hit(.
.
.
Mr. Matsuki was hit on the head with a metal bat .
.
.
)Identified passive case frame:Case Frame: ????
?-2 (be hit-2)?{???
(someone):2,??
(member):1, ?
?
?
}-niyotte{??
(woman):5,??
(girl)):4, ?
?
?
}-ga.....{?
(head):3944,?
(face):1186, ?
?
?
}-wo{??
(blunt weapon):84,???
(bat):45, ?
?
?
}-de?
?
?Corresponding active case frame and case alignment:Case alignment: {niyotte?ga, ga ?no, wo?wo, de?de}Case Frame: ??
?-2 (hit-2)?{?
(man):51,?
(fist):30,??
(someone):23, ?
?
?
}-ga{??
(myself):360,?
(I):223, ?
?
?
}-no.....{?
(head):5424,?
(face):3215, ?
?
?
}-wo{?
(fist):316,??
(palm):157,?
(fist):43, ?
?
?
}-de?
?
?Table 7: An example of case alternation.
The input ga-case was alternated with no-case.resent several other meanings, such as honorific andpossibility.
Since Kawahara and Kurohashi (2002)?smethod does not distinguish between these mean-ings, our case frames sometimes contain impropercases such as wo-case in case frame ????
?-1(be selected-1)?
in Table 4.2) In some passive sentences, there are two surfaceni-cases as in Example (11).
However, our methoddoes not assume such sentences, and thus cannotdeal with them properly.
(11) ??
?????
?????
?man-ni office-ni was sent(?
was sent to the office by a man.
)3) Agent of a predicate can be represented by us-ing several types of case particles in the passivevoice.
For example, ???
(company)?
in Exam-ple (12) is the agent of ?????
(employed),?which can be represented by either of ni, niyotte,and kara in the passive voice.
Since Kawahara andKurohashi (2002)?s method can not recognize theexchangeablity of case particles, some case framescontain several cases of the same semantic role.However, since our method enforces a one-to-onealignments, only one of these cases is properlyaligned to the corresponding case in the active voice.1221(12) ???
??
????
?company-ga man-wo employed(The company employed a man.
)6.3 Application to Alternation between theCausative and Active VoicesTo confirm the applicability of our framework toother types of alternation than the active-passive al-ternation, we applied our framework to case alter-nation between the causative and active voices.
Thecausative voice in Japanese is a grammatical voiceand is expressed by using the auxiliary verbs ???(seru)?
and ????
(saseru).?
We basically usedthe same algorithm as Algorithm 1 for acquiringthe knowledge for case alternation, but used differ-ent constraints on case alternation patterns becausepossible case alternation patterns are different fromthose of active-passive alternation.
Specifically, wereplaced the third and fourth lines of Algorithm 1with ?for each cto ga?
{NIL, ni}?
and ?for eachcga to?
{wo, ni},?
respectively, based on linguisticanalysis of active-causative alternation in Japanese.We used a part of the data created by Murata andIsahara (2003) to evaluate the usefulness of the ac-quired knowledge.
Their data consists of 4,671 caseparticles in passive or causative sentences from theKyoto University Text Corpus with their cases inthe active voice.
We first extracted 524 case par-ticles that were extracted from causative sentences.Since the annotation quality was not very high, wemanually checked all tags and modified inappropri-ate ones.
We then performed 2-fold cross valida-tion experiments.
Table 8 shows experimental re-sults.
Baseline is a system that outputs the most fre-quently alternated cases in the training data.
The dif-ference between Murata et al(2006)?s model11 andour method was significant (p-value of McNemar?stest was 0.0019), and we confirmed the applicabilityof our framework to active-causative alternation.7 Conclusions and Future DirectionsWe have presented a method for automatically ac-quiring knowledge for case alternation between thepassive and active voices in Japanese.
Our method11In this experiment, we used the same features as those usedby Murata and Isahara (2003).Model AccuracyBaseline 0.781 (409/524)Murata et al(2006)?s model 0.836 (438/524)Our method with training data 0.872 (457/524)Table 8: Experimental results of case alternation betweenthe causative and active voices.aligned an input case frame in the passive voice toa corresponding case frame in the active voice andfound an alignment between their cases.
We thenapplied the acquired knowledge to a case alternationtask and proved its usefulness.The knowledge we have to manually construct isonly the knowledge of linguistic constraints on casealternation patterns.
The other types of knowledgeare automatically acquired from a large raw cor-pus.
Thus, although this paper focused on the active-passive alternation in Japanese, our framework is ap-plicable to the other types of case alternation and toother languages, especially similar languages suchas Korean.
We plan to apply our framework to othertypes of case alternation such as case alternation be-tween intransitive and transitive verbs.AcknowledgmentsThis work was supported by JSPS KAKENHI GrantNumber 23800025 and 25730131.ReferencesTimothy Baldwin and Hozumi Tanaka.
2000.
Verb alter-nations and Japanese ?
how, what and where?
In Proc.of PACLIC 14, pages 3?14.Marco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistic, 36(4):673?721.Charles J. Fillmore, Christopher R. Johnson, and Miriam.R.
L. Petruck.
2003.
Background to FrameNet.
Inter-national Journal of Lexicography, 16(3):235?250.Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-sumoto.
2007.
Annotating a Japanese text corpus withpredicate-argument and coreference relations.
In Proc.of ACL?07 Workshop: Linguistic Annotation Work-shop, pages 132?139.IPA.
1987.
Japanese verbs : A guide to the IPA lexiconof basic Japanese verbs.Eric Joanis, Suzanne Stevenson, and David James.
2008.A general feature space for automatic verb classifica-tion.
Natural Language Engineering, 14(3):337?367.1222Daisuke Kawahara and Sadao Kurohashi.
2002.
Fertil-ization of case frame dictionary for robust Japanesecase analysis.
In Proc.
of COLING?02, pages 425?431.Daisuke Kawahara and Sadao Kurohashi.
2006.
Afully-lexicalized probabilistic model for Japanese syn-tactic and case structure analysis.
In Proc.
of HLT-NAACL?06, pages 176?183.Daisuke Kawahara, Ryohei Sasano, and Sadao Kuro-hashi.
2004.
Toward text understanding: Integrat-ing relevance-tagged corpora and automatically con-structed case frames.
In Proc.
of LREC?04, pages1833?1836.Keiko Kondo, Satoshi Sato, and Manabu Okumura.2001.
Paraphrasing by case alternation (in Japanese).Journal of Information Processing Society of Japan,42(3):465?477.Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.2006.
A large subcategorization lexicon for nat-ural language processing applications.
In Proc.ofLREC?06, pages 3000?3006.Mirella Lapata and Chris Brew.
2004.
Verb class dis-ambiguation using informative priors.
ComputationalLinguistics, 30(1):45?73.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press.Jianguo Li and Chris Brew.
2008.
Which are the bestfeatures for automatic verb classification.
In Proc.
ofACL-HLT?08, pages 434?442.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
of ACL-COLING?98, pages768?774.Diana McCarthy.
2000.
Using semantic preferences toidentify verbal participation in role switching alterna-tions.
In Proc.
of NAACL?00.Quinn McNemar.
1947.
Note on the sampling error ofthe difference between correlated proportions or per-centages.
Psychometrika, 12:153?157.Masaki Murata and Hitoshi Isahara.
2003.
Conver-sion of Japanese passive/causative sentences into ac-tive sentences using machine learning.
In Proc.
of CI-CLing?03, pages 115?125.Masaki Murata, Toshiyuki Kanamaru, Tamotsu Shirado,and Hitoshi Isahara.
2006.
Machine-learning-basedtransformation of passive Japanese sentences into ac-tive by separating training data into each input particle.In Proc.
of COLING-ACL?06, pages 587?594.Katashi Nagao and Koiti Hasida.
1998.
Automatic textsummarization based on the global document annota-tion.
In Proc.
of ACL?98, pages 917?921.NLRI.
1993.
Bunrui Goi Hyo (in Japanese).
ShuueiPublishing.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1):71?105.Ryohei Sasano and Sadao Kurohashi.
2011.
A discrim-inative approach to japanese zero anaphora resolutionwith large-scale lexicalized case frames.
In Proc.
ofIJCNLP?11, pages 758?766.Sabine Schulte im Walde, Christian Hying, ChristianScheible, and Helmut Schmid.
2008.
Combining EMtraining and the MDL principle for an automatic verbclassification incorporating selectional preferences.
InProc.
of ACL-HLT?08, pages 496?504.Lin Sun and Anna Korhonen.
2009.
Improvingverb clustering with automatically acquired selectionalpreferences.
In Proc.
of EMNLP?09, pages 638?647.Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008.A Japanese predicate argument structure analysis us-ing decision lists.
In Proc.
of EMNLP?08, pages 523?532.Daphne Theijssen, Lou Boves, Hans van Halteren, andNelleke Oostdijk.
2012.
Evaluating automatic anno-tation: automatically detecting and enriching instancesof the dative alternation.
Language Resources andEvaluation, 46(4):565?600.Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Mat-sumoto.
2011.
Jointly extracting Japanese predicate-argument relation with markov logic.
In Proc.
of IJC-NLP?11, pages 1125?1133.1223
