Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 468?471,Prague, June 2007. c?2007 Association for Computational LinguisticsUVA: Language Modeling Techniques for Web People SearchKrisztian BalogISLA, University of Amsterdamkbalog@science.uva.nlLeif AzzopardiUniversity of Glasgowleif@dcs.gla.ac.ukMaarten de RijkeISLA, University of Amsterdammdr@science.uva.nlAbstractIn this paper we describe our participation inthe SemEval 2007 Web People Search task.Our main aim in participating was to adaptlanguage modeling tools for the task, and toexperiment with various document represen-tations.
Our main finding is that single passclustering, using title, snippet and body torepresent documents, is the most effectivesetting.1 IntroductionThe goal of the Web People Search task at SemEval2007 was to disambiguate person names in a websearching scenario (Artiles et al, 2007).
Participantswere presented with the following setting: given alist of documents retrieved from a web search engineusing a person?s name as a query, group documentsthat refer to the same individual.Our aim with the participation was to adapt lan-guage modeling techniques to this task.
To thisend, we employed two methods: single pass cluster-ing (SPC) and probabilistic latent semantic analysis(PLSA).
Our main finding is that the former leads tohigh purity, while the latter leads to high inverse pu-rity scores.
Furthermore, we experimented with var-ious document representations, based on the snip-pets and body text.
Highest overall performance wasachieved with the combination of both.The remainder of the paper is organized as fol-lows.
In Section 2 we present the two approaches weemployed for clustering documents.
Next, in Sec-tion 3 we discuss document representation and pre-processing.
Section 4 reports on our experiments.We conclude in Section 5.2 Modeling2.1 Single Pass ClusteringWe employed single pass clustering (Hill., 1968) toautomatically assign pages to clusters, where we as-sume that each cluster is a set of pages related to oneparticular sense of the person.The process for assignment was performed as fol-lows: The first document was taken and assignedto the first cluster.
Then each subsequent documentwas compared against each cluster with a similaritymeasure based on the log odds ratio (initially, therewas only the initial one created).
A document wasassigned to the most likely cluster, as long as thesimilarity score was higher than a threshold ?
; oth-erwise, the document was assigned to a new cluster,unless the maximum number of desired clusters ?had been reached; in that case the document was as-signed to the last cluster (i.e., the left overs).The similarity measure we employed was the logodds ratio to decide whether the document was morelikely to be generated from that cluster or not.
Thisapproach follows Kalt (1996)?s work on documentclassification using the document likelihood by rep-resenting the cluster as a multinomial term distribu-tion (i.e., a cluster language model) and predictingthe probability of a document D, given the clusterlanguage model, i.e., p(D|?C).
It is assumed thatthe terms t in a document are sampled independentlyand identically, so the log odds ratio is calculated as468follows:logO(D,C) = logp(D|?C)p(D|?C?
)(1)= log?t?D p(t|?C)n(t,D)?t?D p(t|?C?
)n(t,D),where n(t,D) is the number of times a term ap-pears in a document, and the ?C represents the lan-guage model that represents not being in the cluster.Note this is similar to a well-known relevance mod-eling approach, where the clusters are relevance andnon-relevance, except, here, it is applied in the con-text of classification as done by Kalt (1996).The cluster language model was estimated by per-forming a linear interpolation between the empiricalprobability of a term occurring in the cluster p(t|C)and the background model p(t), the probability ofa term occurring at random in the collection, i.e.,p(t|?C) = ?
?
p(t|C) + (1 ?
?)
?
p(t), where ?
wasset to 0.5.1 The ?not in the cluster?
language modelwas approximated by using the background modelp(t).
The similarity threshold above (used for de-ciding whether to assign a document to an existingcluster) was set to ?
= 1, and ?
was set to 100.2.2 Probabilistic Latent Semantic AnalysisThe second method for disambiguation we em-ployed was probabilistic latent semantic analysis(PLSA) (Hofmann, 1999).
PLSA clusters docu-ments based on the term-document co-occurrencewhich results in semantic decomposition of the termdocument matrix into a lower dimensional latentspace.
Formally, PLSA can be defined as:p(t, d) = p(d)?zp(t|z)p(z|d), (2)where p(t, d) is the probability of term t and doc-ument d co-occurring, p(t|z) is the probability of aterm given a latent topic z and p(z|d) is the probabil-ity of a latent topic in a document.
The prior prob-ability of the document, p(d), was assumed to beuniform.
This decomposition can be obtained auto-matically using the EM algorithm (Hofmann, 1999).Once estimated, we assumed that each latent topicrepresents one of the different senses of the person,1This value was not tuned but selected based on best per-forming range suggested by Lavrenko and Croft (2001).so the document is assigned to one of the person-topics.
Here, we made the assignment based on themaximum p(z|d), so if p(z|d) = max p(z|d), then dwas assigned to z.In order to automatically select the number ofperson-topics, we performed the following processto decide when the appropriate number of person-topics (defined by k) have been identified: (1) weset k = 2 and computed the log-likelihood of the de-composition on a held out sample of data; (2) we in-cremented k and computed the log-likelihood; if thelog-likelihood had increased over a given threshold(0.001) then we repeated step 2, else (3) we stoppedas we have maximized the log-likelihood of the de-compositions, with respect to the number person-topics.
This point was assumed to be the optimalwith respect to the number of person senses.
Since,we are focusing on identifying the true number ofclasses, this should result in higher inverse purity,whereas with the single pass clustering the numberof clusters is not restricted, and so we would expectsingle pass clustering to produce more clusters butwith a higher purity.We used Lemur2 and the PennAspect implemen-tation of PLSA (Schein et al, 2002) for our exper-iments, where the parameters for PLSA where setas follows.
For each k we performed 10 initializa-tions where the best initialization in terms of log-likelihood was selected.
The EM algorithm wasrun using tempering with up to 100 EM Steps.
Fortempering the setting suggested in (Hofmann, 1999)were used.
The models were estimated on 90% ofthe data and 10% of the data was held out in order tocompute the log-likelihood of the decompositions.3 Document RepresentationThis section describes the various document repre-sentations we considered, and preprocessing stepswe applied.For each document, we considered the title, snip-pet, and body text.
Title and snippet were pro-vided by the output of the search engine results(person name.xml files), while the body textwas extracted from the crawled index.html files.2http://www.lemurproject.org469Method Title+Snippet Body Title+Snippet+BodyPur InvP F0.5 F0.2 Pur InvP F0.5 F0.2 Pur InvP F0.5 F0.2Train dataSPC 0.903 0.298 0.422 0.336 0.776 0.416 0.482 0.434 0.768 0.438 0.506 0.456PLSA 0.589 0.833 0.636 0.716 0.591 0.656 0.563 0.592 0.579 0.724 0.588 0.641Test dataSPC 0.867 0.541 0.640 0.575 0.818 0.570 0.647 0.596 0.810 0.607 0.669 0.628PLSA 0.292 0.892 0.383 0.533 0.311 0.869 0.413 0.563 0.305 0.923 0.405 0.566Table 1: Results of the clustering methods using various document representations.3.1 Acquiring Plain-Text Content from HTMLOur aim is to extract the plain-text content fromHTML pages and to leave out blocks or segmentsthat contain little or no useful textual information(headers, footers, navigation menus, adverts, etc.
).To this end, we exploit the fact that most web-pages consist of blocks of text content with rel-atively little markup, interspersed with navigationlinks, images with captions, etc.
These segments ofa page are usually separated by block-level HTMLtags.
Our extractor first generates a syntax tree fromthe HTML document.
We then traverse this treewhile bookkeeping the stretch of uninterrupted non-HTML text we have seen.
Each time we encounter ablock-level HTML tag we examine the buffer of textwe have collected, and if it is longer than a threshold,we output it.
The threshold for the minimal length ofbuffer text was empirically set to 10.
In other words,we only consider segments of the page, separatedby block-level HTML tags, that contain 10 or morewords.3.2 IndexingWe used a standard (English) stopword list but wedid not apply stemming.
A separate index was builtfor each person, using the Lemur toolkit.
We createdthree index variations: title+snippet, body,and title+snippet+body.In our official run we used thetitle+snippet+body index; however, inthe next section we report on all three variations.4 ResultsTable 1 reports on the results of our experiments us-ing the Single Pass Clustering (SPC) and Probabilis-tic Latent Semantic Analysis (PLSA) methods withvarious document representations.
The measures(purity, inverse purity, and F-score with ?
= 0.5and ?
= 0.2) are presented for both the train andtest data sets.The results clearly demonstrate the difference inthe behaviors of the two clustering methods.
SPCassigns people to the same cluster with high preci-sion, as is reflected by the high purity scores.
How-ever, it is overly restrictive, and documents that be-long to the same person are distributed into a numberof clusters, which should be further merged.
Thisexplains the low inverse purity scores.
Further ex-periments should be performed to evaluate to whichextent this restrictive behavior could be controlledby the ?
parameter of the method.In contrast with SPC, the PLSA method producesfar fewer clusters per person.
These clusters maycover multiple referents of a name, as is witnessedby the low purity scores.
On the other hand, inversepurity scores are very high, which means referentsare usually not dispersed among clusters.As to the various document representations, wefound that highest overall performance was achievedwith the combination of title, snippet, and body text.Since the data was not homogenous, it would beinteresting to see how performance varies on the dif-ferent names.
We leave this analysis to further work.Our official run employed the SPC method, usingthe title+snippet+body index.
The resultsof our official submission are presented in Table 2.Our purity score was the highest of all submissions,and our system was ranked overall 4th, based on theF?=0.5 measure.470Pur InvP F0.5 F0.2Lowest 0.30 0.60 0.40 0.55Highest 0.81 0.95 0.78 0.83Average 0.54 0.82 0.60 0.69UVA 0.81 0.60 0.67 0.62Table 2: Official submission results and statistics.5 ConclusionsWe have described our participation in the SemEval2007 Web People Search task.
Our main aim in par-ticipating was to adapt language modeling tools forthe task, and to experiment with various documentrepresentations.
Our main finding is that single passclustering, using title, snippet and body to representdocuments, is the most effective setting.We explored the two very different clusteringschemes with contrasting characteristics.
Lookingforward, possible improvements might be pursuedby combining the two approaches into a more robustsystem.6 AcknowledgmentsKrisztian Balog was supported by the NetherlandsOrganization for Scientific Research (NWO) un-der project number 220-80-001.
Maarten de Rijkewas supported by NWO under project numbers017.001.190, 220-80-001, 264-70-050, 354-20-005,600.065.120, 612-13-001, 612.000.106, 612.066.-302, 612.069.006, 640.001.501, 640.002.501, andby the E.U.
IST programme of the 6th FP for RTDunder project MultiMATCH contract IST-033104.ReferencesJ.
Artiles, J. Gonzalo, and S. Sekine.
2007.
TheSemEval-2007 WePS Evaluation: Establishing abenchmark for the Web People Search Task.
In Pro-ceedings of Semeval 2007, Association for Computa-tional Linguistics.D.
R. Hill.
1968.
A vector clustering technique.
InSamuelson, editor, Mechanised Information Storage,Retrieval and Dissemination, North-Holland, Amster-dam.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proc.
of Uncertainty in Artificial Intelli-gence, UAI?99, Stockholm.T.
Kalt.
1996.
A new probabilistic model of text classifi-cation and retrieval.
Technical Report CIIR TR98-18,University of Massachusetts, January 25, 1996.V.
Lavrenko and W. B. Croft.
2001.
Relevance-basedlanguage models.
In Proceedings of the 24th annualinternational ACM SIGIR conference, pages 120?127,New Orleans, LA.
ACM Press.Andrew I. Schein, Alexandrin Popescul, Lyle H. Un-gar, and David M. Pennock.
2002.
Meth-ods and metrics for cold-start recommendations.In SIGIR ?02: Proceedings of the 25th an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 253?260, New York, NY, USA.
ACMPress.
See http://www.cis.upenn.edu/datamining/software dist/PennAspect/.471
