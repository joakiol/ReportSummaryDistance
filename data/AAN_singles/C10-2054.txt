Coling 2010: Poster Volume, pages 472?480,Beijing, August 2010Mining Large-scale Comparable Corpora from Chinese-EnglishNews CollectionsDegen Huang1                Lian Zhao2                Lishuang Li 3               Haitao Yu4Department of Computer Science and TechnologyDalian University of Technology1huangdg@dlut.edu.cn              3lils@dlut.edu.cn2zhaolian@mail.dlut.edu.cn        4gengshenspirit@163.comAbstractIn this paper, we explore a CLIR-basedapproach to construct large-scale Chi-nese-English comparable corpora, whichis valuable for translation knowledgemining.
The initial source and targetdocument sets are crawled from newswebsite and standardized uniformly.Keywords are extracted from the sourcedocument firstly, and then the extractedkeywords are translated and combined asquery words through certain criteria toretrieve against the index created usingtarget document set.
Meanwhile, themapping correlations between source andtarget documents are developed accord-ing to the value of similarity calculatedby the retrieval tool.
Two methods areevaluated to filter the comparable docu-ment pairs so as to ensure the quality ofthe comparable corpora.
Experimental re-sults indicate that our approach is effec-tive on the construction of Chinese-English comparable corpora.1 IntroductionParallel corpora are key resource for statisticalmachine translation, in which machine learningtechniques are used to learn translation knowl-edge.
Sufficient data is necessary for the data-driven approaches to estimate the model parame-ters reliably.
However, as Munteanu (2006)stated, beyond a few resource-rich language pairssuch as English-Chinese or English-French and asmall number of contexts like parliamentary de-This work was supported by Microsoft Research Asia.bates or legal texts, parallel corpora remain ascarce resource, despite the proposition of auto-mated methods to collect parallel corpora fromthe Web.
Researches on comparable corpora aremotivated by the scarcity of parallel corpora.Compared with parallel corpora, comparablecorpora are more abundant, up-to-date and ac-cessible.Comparable corpora are defined as pairs ofmonolingual corpora selected according to thesame set of criteria, but in different languages orlanguage varieties.
When creating comparablecorpora, the key process is to align the sourcedocument with relevant target documents.
Earlywork by Braschler and Sc?uble (1998) employedcontent descriptors and publication dates to alignGerman and Italian news stories.
Resnik (1999)mined comparable corpora on the assumptionthat the pages which are comparable of eachother share a similar structure (headers, para-graphs, etc.)
when text is presented in many lan-guages in the Web.
Tao and Zhai (2005) acquiredcomparable bilingual text corpora based on theobservation that terms that are translations ofeach other or share the same topic tend to co-occur in the comparable corpora at thesame/similar time periods.
Recently, Talvensaariet al (2007) introduced a CLIR-based approachto align two document collections with differentlanguages.
All the target documents were in-dexed with Lemur.
Then appropriate keywordswere extracted from the source language docu-ments and translated into the target language asquery words to retrieve similar target documents.As we know, the problems may vary with thelanguage of documents when using CLIR-basedapproach to construct comparable corpora, suchas keyword extraction, out-of-vocabulary key-word translation and so on.
This paper is a fur-ther endeavor to CLIR-based approach for com-472Figure 1.
The general architecture of comparable corpora constructionparable corpora construction.
We focus on theconstruction of Chinese-English comparable cor-pora, explore and address the issues during theconstruction.
Experimental results show that ourmethod is better through a rough comparisonwith Talvensaari et al (2007) and it also outper-forms our reconstruction of Tao and Zhai (2005)in respect to the quality of comparable corpora.This paper is organized as follows.
In section 2,the general architecture of our system is de-scribed, and each module is illuminated in detail.Section 3 reports and analyzes the experimentalresults followed by conclusions in section 4.2 System ArchitectureFigure 1 shows the general architecture of ourcomparable corpora construction system.
It con-sists of two components: component I and com-ponent II.
Component I is mainly composed by aweb crawler, which is used to harvest source andtarget documents from selected web sites.
Wecan get the final source and target document setsthrough content extraction and noise filtering.The core of the system is component II, whichaligns a source document with target documentshaving comparable contents.
It implements onthe two document sets generated by component I.Component II is composed of three modules:keyword extraction, keyword translation, andretrieval & filtering.
The methods for three mod-ules are detailed respectively.2.1 Keyword ExtractionA keyword is described as a meaningful and sig-nificant expression containing one or more words.Appropriate keywords briefly describe the themeof a document.
In this paper, keywords areviewed as basic units of search indexes in orderto retrieve closely related documents.
Generally,phrases can capture the main idea of a documentmore effectively, inasmuch as they have moreinformation than single words (an independentlinguistic unit after word segmentation for Chi-nese).Existing approaches for keyword extractioncould be distinguished into two main categories:supervised or unsupervised methods.
Supervisedmachine learning algorithms were widely used inkeyword extraction such as Na?ve Bayes (Franket al, 1999; Witten et al, 1999), SVM (Zhang etal., 2006), CRF (Zhang et al, 2008), etc.
Theseapproaches had excellent stability.
However, itwas difficult for us to construct a big-enoughgolden annotated corpus to train a good classifier,especially for news web pages.
Unsupervisedmethods hinged on evaluating various features toselect keywords, such as word frequency (Luhn,1957), word co-occurrence (Matsuo and Ishizuka,2004), and TF*IDF (Li et al, 2007).
The inher-ent problem in these methods was that most oftheir work came in the judgment whether a can-didate was a keyword or not, but they had notpaid sufficient attention to the identification ofphrase candidates.
Wan and Xiao (2008) pro-posed a method for keyphrase extraction fromsingle document.
However, it simply combinedthe adjacent candidate words to a multi-wordphrase.Based on the above observation, our approachfor keyword extraction focuses more on the con-struction of phrasal candidates.
It is mainly basedon MWE (Multi-Word Expression) extractiontogether with relevant word ranking method.473MWE is a special lexical unit including com-pound terms, idioms and collocations, etc.
Theprocess of keyword extraction in this papermainly depends on the following stages.Stage 1: The generation of phrasal candidates(1) The extraction of MWEs from the preproc-essed documentDocument preprocessing is a procedure ofmorphological analysis including segmentationand part of speech tagging for Chinese.
Themethod based on the marginal probabilities de-tailed in (Luo and Huang, 2009) is adopted inthis part.We extract MWEs using LocalMaxs selectionalgorithm together with a relevance measure cal-culation method (FSCP) proposed by Silva et al(1999).
Suffix arrays and related structures in(Aires et al, 2008) are used to compute the FSCPvalue so as to raise efficiency.
And the initial col-lection of MWEs named G for the document isgenerated after filtered by stopword list.
(2) The acquisition of new MWEs through themodification for segmentationAs a matter of fact, the results of segmentationfor the document usually have some errors espe-cially for out-of-vocabulary (OOV) words whichare segmented to single Chinese characters inmost cases.
Inaccurate segmentation leads tosome faults for keyword extraction.
As stated in(Liu et al, 2007), OOV words can be identifiedby the method of MWEs extraction mentionedabove.
Therefore, we modify the segmentationlike this: any MWE in G is merged to one wordif it only consists of single Chinese charactersand its frequency > freq.
The changes before andafter merging are shown in Table 1.
Because themethod of MWE extraction is based on statisticaltechniques, so low frequency of MWE will resultin poor performance.
But large value for freqmeans that very few MEWs can satisfy the fre-quency restriction.
In our experiments, we setfreq=2.
The extraction process is called again toidentify MWEs from the document with modi-fied segmentation.
Consequently, new collectionof MWEs is acquired.Additionally, some simple rules are definedaccording to language features to filter MWEs.In this paper, our method is tailored to extractkeywords from news web pages which containsome special symmetric marks like ?
?, ??.
Thewords in a specially marked area are usually im-portant to the document.
So we extract wordswithin each paired marks and view them as aMWE on the condition that it contains two ormore than two words.
All of the MWEs areviewed as phrasal candidates and filtered bystopword list.Stage 2: The generation of single words candi-datesOur method also generates single word candi-dates with the account that both phrase and sin-gle word can be served as a keyword.
The proc-ess of single word selection is independent ofMWE extraction.
The candidate words are re-stricted to nouns, verbs, strings (like WTO) andmerged words as discussed in the previous stage.But the word will be removed if it only appearsonce in the document or is contained in thestopword list.Stage 3: Keyword selection based on candidatesrankingAs for MWE candidates, we calculate theweight for them using Formula 1 which refers tothe formula used to sort NP phrases in (Brace-well et al, 2008).
But the weight of len is re-duced.1( ) log(1 ( ))MWEleniiWeight MWE len ftf wlen == + +??
(1)Where len is the length of MWE (in number ofwords); fMWE is the frequency of the MWE withinin the document; tf(wi) is the frequency of wordwi.
The following rules are used to rank MWEs:MWE Segmentation before mergingSegmentationafter mergingPosbefore mergingPosafter merging?
?
?/ ?/ ?/ ?/ ?/ ?
?/ ?/ ?/?/ ?/ ?
?/ ?/?
?/ ?/ ?/?/n ?/n ?/n ?/n?/vl ?
?/n ?/us?/a?/n ?/n ?
?/oov?/vl ?
?/n ?/us?/a?
?
?/ ?/ ?
?/ ?/ ?
?/ ?
?/ ?/ ?
/jb ?
/n ??
/b ?/n?
?/oov ?
?/b?/nTable 1.
Changes before and after merging474(a) more frequent MWEs are ranked higher; (b)MWEs with larger weight are ranked higher.
Inorder to avoid redundancy, we remove the re-dundant MWEs with lower rank.Single word candidates are ranked as follows:(a) the single word w with larger TF*IDF valueis ranked higher; (b) the pos score for w in de-scending order is: named entity, merged words,nouns, strings, verbs.
In the end, top-a MWEsand top-b single words are chosen to form thekeyword set of the document.Stage 4: Parameters evaluation and experimentalresultsThe max number of keywords extracted fromeach document is limited to ten (a+b=10) and werun our approach on the dataset which includeone hundred Chinese documents from the corpusof NTCIR-5 since they are also news articles.For evaluation of the results, the keywords ex-tracted by our method are compared with themanually extracted keywords (at most ten key-words are assigned to each document).
The F-measure is used as evaluation metric.
It is de-fined like this: F=(P+R)/2; P=nummatch/numsystem;R=nummatch/nummanual.
Where nummatch is thecount of keywords extracted by our methodmatching with manually extracted keywords;numsystem is the count of keywords extracted byour method; nummanual is the count of keywordsassigned by human.Figure 2 shows the performance curves for ourextraction method.
In this figure, a ranges from 0to 10 while b is 10 to 0.
It performs best when a= 4 and b = 6.
So the two values are adopted inthis paper.Figure 2.
F-measure varies with the value of aWe test our approach on another dataset whichalso contains one hundred documents.
In the ex-periments, the max number of keywords is set toten.
Table 2 shows the results of keyword extrac-tion under three different conditions respectively.
(A) Only extracts single words as keywordswhile just MWEs with (B).
(C) The method pre-sented in this paper which makes a proper com-bination of MWEs and single words.P R FA (single words)  24.2% 28.5% 26.4%B (MWEs)  18.1% 23.0% 20.6%C (A+B)  34.2% 43.6% 38.9%Table 2.
Keyword extraction results2.2 Keyword TranslationAs for keyword translation, there are three mainapproaches: translation based on dictionary, par-allel corpora and machine translation.
Dictionarybased approach is adopted in our system by tak-ing the acquisition of translation resource intoaccount.Word Sense Disambiguation (WSD) and OOVproblem are the main difficulties in CLIR (CrossLanguage Information Retrieval) task.
A typicalbilingual dictionary will provide a set of alterna-tive translations for a given keyword, so how tochoose the optimal translation is called WordSense Disambiguation.
Actually some keywordscan not be found and translated due to the cover-age limitation of a bilingual dictionary, which iscalled OOV problem.In this paper, the keyword is given up if itssize of translations gained from the bilingual dic-tionary is larger than two for the convenience ofWSD.
Additionally, both of the translations aretreated as synonyms and equal weight is assignedto them when retrieval.To address the OOV problem, researchers pro-posed methods using snippets returned by asearch engine.
For example, Wang et al (2004)introduced a statistics-based approach calledSCPCD to mine translations from the returnedsnippets.
Different from (Wang et al, 2004),Zhou et al (2007) used a pattern-based approachto analyze the mixed-languages snippets.Leveraging on previous work, we analyze theco-occurrence mode of the OOV term and thecorresponding translation in the returned snippets.Table 3 shows the typical co-occurrence modescollected during experiments, where the Englishwords in bold are the corresponding translationsof the underlined Chinese OOV terms.
From Ta-ble 3, we can see the translations in number 1, 2and 3 are included in the symmetric symbols,like bracket, quotation marks.
However, the475Serialnumber Segments extracted from the returned snippets1 ??????????????
?The Bridges of Madison County??????
?2 ??????????
?The Bridges of Madison County?-52??
?...3 ??????????????
?cowboy diplomacy????????????
?4 ...????????????
Bayesian Network for Data Mining-???
?...5 ????????????
?The End of Cowboy Diplomacy????????
?6 ?????????.
The Bridges of Madison County.
Forrest Gump ???
?...Table 3.
Chinese OOV and the corresponding translation in returned snippetstranslations in number 4, 5, and 6 are embeddedin the partial sentence while there are noise Eng-lish words.
In order to get the correct translation,the partial sentence needs to be segmented.
Byabove analysis, we integrate the SCPCD methodand the pattern-based method so as to extractmore correct translations.
The SCPCD methodcan be used to determine the boundaries forOOVs like number 4, 5, and 6; while pattern-based method makes use of the symmetric sym-bols like number 1, 2 and 3.
Table 4 shows theexperimental results for OOV translation meth-ods.
The average top-n inclusion rate is adoptedas a metric.
For a set of test OOV terms, its top-ninclusion rate is defined as the percentage of theOOVs whose translations can be found in thefirst n extracted translations.Pattern SCPCD Pattern + SCPCDTop-1 40.0% 49.2% 68.1%Top-3 41.5% 55.4% 70.2%Table 4.
The performance comparison of differ-ent OOV translation methodsThe test dataset used is the Chinese topicterms in CLIR task of NTCIR-5.
The search en-gine is Google.
The bilingual dictionary used byus is LDC_CE_DICT 2.0.
And we only adapt thepattern with symmetric symbols, which has thehighest precision proposed by Cao et al (2007).2.3 Retrieval and FilteringThe process of retrieval is to construct the align-ment relationship between source and targetdocument pairs.
It is a core module in our systemsince the quality of comparable corpora is greatlyinfluenced by alignment level which depends onthe relevance between document pairs.
Our in-tention here is to retrieve high relevant targetdocuments for the source documents.
Open-source toolkit Indri is introduced to assist theretrieval process.
Indri is a part of the Lemur pro-ject1.
On the basis of Lemur, it combines infer-ence networks with language modeling.
And it?swidely adopted by institution for scientific re-search since it is effective, flexible, usable andpowerful.
So it is employed by us to retrieve re-lated documents.
A query for each source docu-ment is formed by the translated keywords withIndri query language and then run against thetarget collection.The essential of alignment is to compare thesimilarity between source and target documentpairs.
In order to reduce the workload of compar-ing, Pooling method is applied to assist the com-paring process.
We choose the top r documentsreturned by Indri retrieval system to build therelated document pool.
And g (g<=r) documentsin the pool are selected to form the alignmentdocument pairs together with the source docu-ment.
In our experiments, we set r=10 and g=1.In the process of alignment, three features areused to filter the alignment pairs for the sake ofpruning the low relevant pairs.
The first is publi-cation date contained in documents.
The secondis similarity calculated by Indri between thequery and the target document when retrieval.The last is KSD (Keyword similarity betweendocument pairs) which is defined by our system.In this paper, we propose two methods to filterthe alignment pairs by using various features.
(1) DSF filteringThis method depends on two features: dateand similarity.
At first, we give a priority to thetarget documents that have the closest date to thesource document during the top-r documentssearching.
A date-window size d is defined tomeasure the date difference.
We set d=1 in thispaper.
That is to say, the target documents with1 Lemur toolkit is developed by Carnegie Mellon Universityand University of Massachusetts.
The open source code isavailable at http://www.lemurproject.org.476exactly the same date as the source document,and one day earlier or later are considered to beclosest.
Then, we select g documents with largersimilarity from the related document pool.
Fi-nally, we rank all of the alignment pairs with thescore of similarity and set a similarity threshold sto filter further.
It should be noted that there aren ?
g alignment pairs, where n is the number ofsource documents having non-empty relateddocument pool.
(2) DSKF filteringThis method utilizes all of the features: date,similarity and KSD.
As for KSD, it integratestwo factors.
One is NTK, namely the number oftranslated keywords appeared in the targetdocument, since the target document is moresimilar to the source document as increasing ofNTK.
The other is FIS, namely frequency infor-mation score.
Inspired by paper (Tao and Zhai,2005), we use the score of FIS to measure thecorrelations between the keywords in sourcedocument and translated keywords in targetdocument which represent the matching forsource and target document pair.
We define ds asthe source document, dt as the target document,ks as the set of keywords extracted from ds, kts asthe set of translated keywords.
Formula 2 is usedto compute the score of FIS:1 ( 25( , ) ( )25( , ) ( ) / ( ( , )))ktsLenFIS i s iii t i i iScore BM x d IDF xBM y d IDF y norm Dif x y== ?
???
(2)Where, ktsLen is the size of kts, yi is an elementin kts, xi is the element in ks while yi is the trans-lation of xi.
Moreover, BM25(w, d) is the normal-ized frequency of word w in document d. It hasbeen considered as one of the most effectivematching functions for retrieval.
IDF stands forInverse Document Frequency which is alsocommonly used in information retrieval.
Dif(x, y)is defined as the difference between BM25(x, ds)and BM25(y, dt).
Formula 2 penalizes large dif-ference due to the conditions like this: any key-word in source document appears many timeswhile its translation appears rarely in targetdocument.
The process of its normalization is runby Formula 3 which makes the score less sensi-tive to the absolute value:1, 1( ) ,scorenorm score score else<?= ??
(3)Furthermore, the final KSD score is got bysimply adding the normalized scores of NTK andFIS which are dealt with Formula 3.
Actually, thetwo filtering methods differ principally in the laststep.
DSKF sorts all of the alignment pairs ac-cording to the KSD score while it is similarity inDSF.
We also set a KSD threshold k for DSKFmethod to filter further.
The values for s and kwill be investigated in the following experiments.3 ExperimentsIn this section, we first introduce how to acquirethe source and target document sets.
Then oursystem is tested on the two sets.
The experimen-tal results are reported and analyzed finally.3.1 Experiment SetupTo test the effectiveness of the proposed system,large-scale of Chinese and English news webpages are crawled respectively from XinHuaNetand used as the document resource.
The reasonsfor choosing news pages are:(1) Many websites, like portal website, newsagency, government and so on, provide large-scale news reports.
At the same time, a large pro-portion of the reports can be crawled politely, sodocument acquisition is relatively easy.
(2) The news pages include various contents,such as politics, economy, sports, so the corporamade up of news pages can avoid the limitationsof domain-specific corpora.All the news pages are processed uniformly.The core content of each web page crawled isextracted and several tags describing the headlineand publication date are added.
Meanwhile, theoriginal contents are kept with no change.
Table5 shows the basic information of document sets.Year Number of source documentsNumber of targetdocuments2003 23747 33902004 25660 29432005 47333 115782006 28572 253202007 25036 252472008 14021 242922009 7476 10887Total 171845 103657Table 5.
The composition of source document setand target document set3.2 Results and DiscussionThe quality of comparable corpora highly de-477pends on the alignment level between source andtarget document pairs.
Braschler and Sc?uble(1998) used five levels of relevance to assess thealignments as follows:(1) Same story.
The two documents deal withthe same event.
(2) Related story.
The two documents dealwith the same event or topic from a slightly dif-ferent viewpoint.
Alternatively, the other docu-ment may concern the same event or topic, butthe topic is only a part of a broader story or thearticle is comprised of multiple stories.
(3) Shared aspect.
The documents deal withrelated events.
They may share locations or per-sons.
(4) Common terminology.
The events or topicsare not directly related, but the documents sharea considerable amount of terminology.
(5) Unrelated.
The similarities between thedocuments are slight or nonexistent.We randomly select 500 source documentspublished in 2009 as the test dataset.
Experi-ments with different parameters are constructedbased on this dataset.
The quality of each align-ment pair is manually assessed using the five-level relevance as discussed above.
What shouldto be pointed out is that parameter s and k are notabsolute values, but percentile rank level in ourwork.
For instance, k = 10 means that we onlychoose the alignment pairs whose KSD scorerank in top ten percent among all of the results.Table 6 shows the results filtered by DSFmethod with different values of s (s1 < s2 < s3 <s4 ).
Table 7 shows the results filtered by DSKFmethod with various values of k (k1 < k2 < k3 <k4).
In order to evaluate the results conveniently,two standards are established: (a) the number ofhigh relevant pairs created, which is the count ofdocument pairs in Level 1 and 2; (b) the qualityof the whole alignments, that is to say the per-centage of alignment pairs with Level 1 and 2.Seen from Table 6 and 7, DSKF is better thanDSF by considering the two standards.
Com-pared with DSF, more high relevant pairs are leftfiltered by DSKF when they have the same totalnumber of pairs.
In other words, the DSKFmethod is more powerful to make high relevantpairs in higher rank so as to reduce alignmentpairs which are rarely relevant.
Therefore, DSKFis adopted in our system.
Taking the first crite-rion into account, we give up the parameter k1, k2.Parameter k4 is not the best considering the sec-ond criterion.
Ultimately, k3 is chosen as the finalvalue for k. At this point, the number of align-ment pairs in Level 1 and 2 is close to the maxi-mum.
Meanwhile, the percentage of high align-ments reaches 68.5%.Among the surveyed related work, Talvensaariet al (2007) created Swedish-English compara-ble corpora based on CLIR techniques and itsframework of construction is similar to ours.However, the two systems are different in thefollowing aspects:s1 = 10 s2 = 30 s3 =50 s4 =70 Level Number % Number % Number % Number %Leve1 1 23 46.9% 54 36.5% 83 33.5% 96 27.7%Level 2 18 36.7% 43 29.1% 62 25.0% 81 23.3%Level 3 4 8.2% 21 14.2% 40 16.1% 57 16.4%Level 4 4 8.2% 19 12.8% 41 16.5% 60 17.3%Level 5 0 0.0% 11 7.4% 22 8.9% 53 15.3%Total 49 100% 148 100% 248 100% 347 100%Table 6.
The distribution results filtered by DSF with different s parametersk1 = 10 k2 = 30 k3 = 50 k4 =70 Level Number % Number % Number % Number %Level 1 33 67.3% 78 52.7% 93 37.5% 98 28.2%Level 2 15 30.6% 52 35.1% 77 31.0% 89 25.6%Level 3 1 2.0% 9 6.1% 37 14.9% 62 17.9%Level 4 0 0.0% 9 6.1% 34 13.7% 60 17.3%Level 5 0 0.0% 0 0.0% 7 2.8% 38 11.0%Total 49 100% 148 100% 248 100% 347 100%Table 7.
The distribution results filtered by DSKF with different k parameters478(1) The language is different.
We focus onbuilding comparable corpora of Chinese-Englishwhile they were Swedish-English.
(2) A series of sub problems are different dueto language difference.
As for keyword extrac-tion, we propose a method to select both keyphrases and single words, while they used RATF(Relative Average Term Frequency) method.
ForOOV problem, we combine the SCPCD methodwith the pattern-based method to extract OOVtranslations from snippets returned by a searchengine.
However, the classified s-gram matchingtechnique was utilized by Talvensaari et al (2007)to translate OOV words.
(3) Talvensaari et al (2007) filtered theiralignment pairs mainly depending on date andsimilarity, while we introduce new feature KSDto extend the original feature set.Talvensaari et al (2007) also randomly chose500 source documents and assessed the qualityof alignments using the same five-level relevance.In addition to this, we implement the methodof Tao and Zhai (2005) which is a purely statisti-cal-based and language independent approach.The source and target documents published in2009 are employed to test the method.
The samesample as our system including 500 Chinesedocuments is chosen to make a further compari-son with our work.
We align each source docu-ment with one target document through theBM25Corr model in (Tao and Zhai, 2005).
Thealignment pairs are ranked according to mappingscores calculated by the BM25Corr model.
Andwe select the top N (N = 248) alignment pairs forthe benefit of comparison.Table 8 shows the distribution results for thethree systems.
As illustrated in Table 8, we canroughly conclude that our approach creates morealignment pairs with the same number of sourcedocuments when compared with Talvensaari et al(2007).
Meanwhile, the percentage of high rele-vant document pairs is larger.Likewise, our system outperforms BM25Corrin that it aligns more high relevant documentspairs when they use the same sample of test cor-pora and create the same total number of pairs.Obviously, the quality of comparable corporagained by our system is better than BM25Corr.All the experimental results and analysis men-tioned above indicate that our method is effectiveto create alignment pairs.
Up to now, both thesource and target documents published in 2007-2009 years are used to build comparable corporathrough our proposed system.
It includes 23102alignment pairs after filtered by DSKF.Talvensaari et al(2007)Our System(DSKF filtering)BM25Corr(Top N = 248) LevelNumber % Number % Number %Level 1 21 21.6% 93 37.5% 1 0.4%Level 2 20 20.6% 77 31.0% 2 0.8%Level 3 33 34.0% 37 14.9% 3 1.2%Level 4 19 19.6% 34 13.7% 5 2.0%Level 5 4 4.1% 7 2.8% 237 95.6%Total 97 100% 248 100% 248 100%Table 8.
The distribution results for Talvensaari et al (2007), Our System, and BM25Corr4 ConclusionsIn this paper, we propose a CLIR-based approachto create large-scale Chinese-English comparablecorpora.
Firstly, we harvest the original sourceand target document sets from news website us-ing open-source crawler.
Then the core contentof each document is extracted through discrimi-nating noise contents.
Next, we delve into theapproaches of problems such as keyword extrac-tion and OOV translation followed by the proc-ess of retrieval to develop mapping correlationsbetween source and target documents.
Finally,three features as publication date, similarityscore and KSD value are used to filter thealigned document pairs.
Experimental resultsshow that our approach is effective to mine Chi-nese-English document pairs with comparablecontents.
In the future, we will optimize the ap-proach for every module in the construction ofcomparable corpora for the sake of improvingthe performance of the whole system.
What?smore, it will be worth consideration to minemappings between terms which can be served asa feature for the process of developing mappingsbetween document pairs in turn.479ReferencesAires, Jos?, Gabriel Lopes, and Joaquim FerreiraSilva.
2008.
Efficient Multi-word Expressions Ex-tractor Using Suffix Arrays and Related Structures.In Proceeding of the 2nd ACM workshop on Imp-roving non english web searching, pp.
1-8.Bracewell, David B., Fuji Ren, and Shingo Kuroiwa.2008.
Mining News Sites to Create Special Do-main News Collections.
International Journal ofComputational Intelligence, 4(1): 56-63.Braschler, Martin, and Peter Sc?uble.
1998.
Multilin-gual Information Retrieval Based on DocumentAlignment Techniques.
In Proceedings of the 2ndEuropean Conference on Research and AdvancedTechnology for Digital Libraries, pp.
183-197.Cao Guihong, Jianfeng Gao, and Jianyun Nie.
A Sys-tem to Mine Large-Scale Bilingual Dictionariesfrom Monolingual Web pages.
2007.
In Proceed-ings of Machine Translation Summit XI, pp.
57-64.Frank, Eibe, Gordon W. Paynter, and Ian H. Witten.1999.
Domain-Specific Keyphrase Extraction.
InProceedings of the 16th International Joint Con-ference on Artificial Intelligence, pp.
668-673.Li Juanzi, Qi?na Fan, and Kuo Zhang.
2007.
KeywordExtraction Based on tf/idf for Chinese NewsDocument.
Wuhan University Journal of NaturalSciences, 12(5): 917-921.Liu Tao, Bingquan Liu, Xiaolong Wang, and MinghuiLi.
2007.
The Effectiveness Study of Local Maxi-mum Feature for Chinese Unknown Word Identifi-cation.
Journal of Chinese Language and Comput-ing, 17(1): 15-26.Luhn, Hans Peter.
1957.
A Statistical Approach toMechanized Encoding and Searching of LiteraryInformation.
IBM Journal of Research and Devel-opment, 1(4): 309-317.Luo Yanyan, and Degen Huang.
2009.
Chinese WordSegmentation Based on the Marginal ProbabilitiesGenerated by CRFs.
Journal of Chinese Informa-tion Processing, 23(5): 3-8.Matsuo, Yutaka and Mitsuru Ishizuka.
2004.
Key-word Extraction from a Single Document UsingWord Co-occurrence Statistical Information.
Inter-national Journal on Artificial Intelligence Tools,13(1): 157-169.Munteanu, Dragos Stefan.
2006.
Exploiting Compa-rable Corpora.
Doctoral Thesis.
UMI OrderNo.3257825.
University of Southern California.Resnik, Philip.
1999.
Mining the web for bilingualtext.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics, pp.527-534.Silva, Joaquim Ferreira, Ga?l Dias, Sylvie Guillor?,and Jos?
Gabriel Pereira Lopes.
1999.
Using Lo-calMaxs Algorithm for the Extraction of Contigu-ous and Non-contiguous Multiword Lexical Units.In Proceedings of the 9th Portuguese Conferenceon Artificial Intelligence, pp.
113-132.Talvensaari, Tuomas, Jorma Laurikkala, KalervoJ?rvelin, Martti Juhola and Heikki Keskustalo.2007.
Creating and Exploiting a Comparable Cor-pus in Cross-Language Information Retrieval.ACM Transactions on Information Systems,25(1):1-21.Tao Tao, and Chengxiang Zhai.
2005.
Mining Com-parable Bilingual Text Corpora for Cross-Language Information Integration.
In Proceedingsof the 11th ACM SIGKDD international conferenceon Knowledge discovery in data mining, pp.
691-696.Wan Xiaojun, and Jianguo Xiao.
2008.
CollabRank:Towards a Collaborative Approach to Single-Document Keyphrase Extraction.
In Proceeding ofthe 22nd International Conference on Computa-tional Linguistics, pp.
969-976.Wang Jenq Haur, Jie Wen Teng, Pu Jen Cheng, WenHsiang Lu, and Lee Feng Chien.
2004.
TranslatingUnknown Cross-Lingual Queries in Digital Librar-ies using a Web-based Approach.
In Proceedingsof the 4th ACM/IEEE-CS joint Conference on Digi-tal Libraries, pp.
108-116.Witten, Ian H., Gordon W. Paynter, Eibe Frank, CarlGutwin, and Craig G. Nevill-Manning.
1999.
KEA:Practical automatic keyphrase extraction.
In Pro-ceedings of the 4th ACM Conference on Digital Li-braries, pp.
254-255.Zhang Chengzhi, Huilin Wang, Yao Liu, Dan Wu, YiLiao, and Bo Wang.
2008.
Automatic KeywordExtraction from Documents Using ConditionalRandom Fields.
Journal of Computational Infor-mation Systems, 4(3): 1169-1180.Zhang Kuo, Hui Xu, Jie Tang, and Juanzi Li.
2006.Keyword Extraction Using Support Vector Ma-chines.
In Proceedings of the 7th InternationalConference on Web-Age Information Management,pp.
85-96.Zhou Dong, Mark Truran, Tim Brailsford, and HelenAshman.
2007.
NTCIR-6 Experiments Using Pat-tern Matched Translation Extraction.
In Proceed-ings of 6th NTCIR Workshop Meeting, pp.
145-151.480
