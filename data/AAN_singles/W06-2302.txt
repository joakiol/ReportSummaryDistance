Another Evaluation of Anaphora Resolution Algorithms and aComparison with GETARUNS?
Knowledge Rich ApproachRodolfo Delmonte, Antonella Bristot, MarcoAldo Piccolino Boniforti, Sara TonelliDepartment of Language SciencesUniversit?
Ca?
Foscari ?
Ca?
Bembo30120, Venezia, Italydelmont@unive.itAbstractIn this paper we will present an evaluation ofcurrent state-of-the-art algorithms for AnaphoraResolution based on a segment of Susannecorpus (itself a portion of Brown Corpus), amuch more comparable text type to what isusually required at an international level fors u c h a p p l i c a t i o n d o m a i n s a sQuestion/Answering, Information Extraction,Text Understanding, Language Learning.
Theportion of text chosen has an adequate sizewhich lends itself to significant statisticalmeasurements: it is portion A, counting 35,000tokens and some 1000 third person pronominalexpressions.
The algorithms will then becompared to our system, GETARUNS, whichincorporates an AR algorithm at the end of apipeline of interconnected modules thatinstantiate standard architectures for NLP.
F-measure values reached by our system aresignificantly higher (75%) than the other ones.1 IntroductionThe problem of anaphora resolution (hence AR)looms more and more as a prominent one inunrestricted text processing due to the need torecover semantically consistent information in mostcurrent NLP applications.
This problem does notlend itself easily to a statistical approach so thatrule-based approaches seem the only viablesolution.We present a new evaluation of three state-of-the-artalgorithms for anaphora resolution ?
GuiTAR,JavaRAP, MARS ?
on the basis of a portion ofSusan Corpus (derived from Brown Corpus) a muchricher testbed than the ones previously used forevaluation, and in any case a much morecomparable source with such texts as newspaperarticles and stories.
Texts used previously rangedfrom scientific manuals to descriptive scientifictexts and were generally poor on pronouns and richon nominal descriptions.
Two of the algorithms ?GuiTAR and JavaRAP - use Charniak?s parseroutput, which contributes to the homogeneity of thetype of knowledge passed to the resolutionprocedure.
MARS, on the contrary, uses a moresophisticated input, the one provided by ConnexorFDG-parser.
The algorithms will then be comparedto our system, GETARUNS, which incorporated anAR algorithm at the end of a pipeline ofinterconnected modules that instantiate standardarchitectures for NLP.
The version of the algorithmpresented here is a newly elaborated one, and isdevoted to unrestricted text processing.
It is anupgraded version from the one discussed inDelmonte (1999;2002a;2002b) and tries toincorporate as much as possible of the moresophisticated version implemented in the completeGETARUN (see Delmonte 1990;1991;1992;1994;2003;2004).The paper is organized as follows: in section 2below we briefly discuss architectures and criteriafor AR of the three algorithms evaluated.
In section3 we present our system.
Section 4 is dedicated to acompared evaluation and a general discussion.2 The Anaphora Resolution AlgorithmsWe start by presenting a brief overview of threestate-of-the-art algorithms for anaphora resolution ?GuiTAR, JavaRAP, MARS.2.1 JavaRAPAs reported by the authors (Long Qiu, Min-YenKan, Tat-Seng Chua, 2004) of the JAVAimplementation, head-dependent relations requiredby RAP are provided by looking into the structural?argument domain?
for arguments and into thestructural ?adjunct domain?
for adjuncts.
Domaininformation is important to establish disjunctionrelations, i.e.
to tell whether a third person pronouncan look for antecedents within a certain structuraldomain or not.
According to Binding Principles,Anaphors (i.e.
reciprocal and reflexive pronouns),3must be bound ?
search for their binder-antecedent ?in their same binding domain ?
roughlycorresponding to the notion of structural?argument/adjunct domain?.
Within the samedomains, Pronouns must be free.
Head-argument orhead-adjunct relation is determined whenever two ormore NPs are sibling of the same VP.Additional information is related to agreementfeatures, which in the case of pronominalexpressions are directly derived.
As for nominalexpressions, features are expressed in case they areeither available on the verb ?
for SUBJect NPs?
orelse if they are expressed on the noun and someother tricks are performed for conjoined nouns.Gender is looked up in the list of names available onthe web.
This list is also used to provide thesemantic feature of animacy.RAP is also used to find pleonastic pronouns, i.e.pronouns which have no referents.
To detectconditions for pleonastic pronouns a list of patternsis indicated, which used both lexical and structuralinformation.Salience weight is produced for each candidateantecedent from a set of salience factors.
Thesefactors include main Grammatical Relations,Headedness, non Adverbiality, belonging to thesame sentence.
The information is computed againby RAP, directly on the syntactic structure.
Theweight computed for each noun phrase is divided bytwo in case the distance from the current sentenceincreases.
Only NPs contained within a distance ofthree sentences preceding the anaphor areconsidered by JavaRAP.2.2 GuiTARThe authors (Poesio, M. and Mijail A. Kabadjov2004) present their algorithm as an attempt atproviding a domain independent anaphoraresolution module, ?that developers of NLEapplications can pick off the shelf in the way oftokenizers, POS taggers, parsers, or NamedEntity classifiers?.
For these reasons, GuiTAR hasbeen designed to be as independent as possible fromother modules, and to be as modular as possible,thus ?allowing for the possibility of replacingspecific components (e.g., the pronoun resolutioncomponent)?.The authors have also made an attempt at specifyingwhat they call the Minimal Anaphoric Syntax(MAS) and have devised a markup language basedon GNOME mark-up scheme.
In MAS, NominalExpressions constitute the main processing units,and are identified with the tag NE <ne>, which havea CAT attribute, specifying the NP type: the-np,pronoun etc., as well as Person, Number and Genderattributes for agreement features.
Also the internalstructure of the NP is marked with Mod andNPHead tags.The pre-processing phase uses a syntactic guesserwhich is a chunker of NPs based on heuristics.
AllNEs add up to a discourse model ?
or better HistoryList - which is then used as the basic domain whereDiscourse Segments are contained.
Each DiscourseSegment in turn may be constituted by one or moreUtterances.
Each Utterance in turn contains a list offorward looking centers Cfs.The Anaphora Resolution algorithm implemented isthe one proposed by MARS which will becommented below.
The authors also implemented asimple algorithm for resolving Definite Descriptionson the basis of the History List by a same headmatching approach.2.3 MARSThe approach is presented as a knowledge pooranaphora resolution algorithm (Mitkov R.[1995;1998]), which makes use of POS and NPchunking, it tries to individuate pleonastic ?it?occurrences, and assigns animacy.
The weightingalgorithm seems to contain the most originalapproach.
It is organized with a filtering approachby a series of indicators that are used to boost orreduce the score for antecedenthood to a given NP.The indicators are the following ones:FNP (First NP); INDEF (Indefinite NP); IV(Indicating Verbs); REI (Lexical Reiteration); SH(Section Heading Preference); CM (CollocationMatch); PNP (Prepositional Noun Phrases); IR(Immediate Reference); SI (Sequential Instructions);RD (Referential Distance); TP (Term Preference),As the author comments, antecedent indicators(preferences) play a decisive role in tracking downthe antecedent from a set of possible candidates.Candidates are assigned a score (-1, 0, 1 or 2) foreach indicator; the candidate with the highestaggregate score is proposed as the antecedent.The authors comment is that antecedent indicatorshave been identified empirically and are relatedto salience (definiteness, givenness, indicatingverbs, lexical reiteration, section headingpreference, "non- prepositional" noun phrases), tostructural matches (collocation, immediatereference), to referential distance or to preferenceof terms.
However it is clear that most of theindicators have been suggested for lack of betterinformation, in particular no syntactic constituencywas available.In a more recent paper (Mitkov et al, 2003) MARShas been fully reimplemented and the indicatorsupdated.
The authors seem to acknowledge the factthat anaphora resolution is a much more difficulttask than previous work had suggested, In4unrestricted text analysis, the tasks involved in theanaphora resolution process contribute a lot ofuncertainty and errors that may be the cause for lowperformance measures.The actual algorithm uses the output of Connexor?sFDG Parser, filters instances of ?it?
and eliminatespleonastic cases, then produces a list of potentialantecedents by extracting nominal and pronominalheads from NPs preceding the pronoun.
Constraintsare then applied to this list in order to produce the?set of competing candidates?
to be consideredfurther, i.e.
those candidates that agree in numberand gender with the pronoun, and also obeysyntactic constraints.
They also introduced the useof Genetic Algorithms in the evaluation phase.The new version of MARS includes three newindicators which seem more general and applicableto any text, so we shall comment on them.Frequent Candidates (FC) ?
this is a boosting scorefor most frequent three NPs; Syntactic Parallelism(SP) ?
this is a boosting score for NPs with the samesyntactic role as the pronoun, roles provided by theFDG-Parser; Boost Pronoun (BP) ?
pronouncandidates are given a bonus (no indication ofconditions for such a bonus).The authors also reimplemented in a significant waythe indicator First NPs which has been renamed,?Obliqueness (OBL) ?
score grammatical functions,SUBJect > OBJect > IndirectOBJect > Undefined?.MARS has a procedure for automatically identifyingpleonastic pronouns: the classification is done bymeans of 35 features organized into 6 types and areexpressed by a mixture of lexical and grammaticalheuristics.
The output should be a fine-grainedcharacterization of the phenomenon of the use ofpleonastic pronouns which includes, among others,discourse anaphora, clause level anaphora andidiomatic cases.In the same paper, the authors deal with two moreimportant topics: syntactic constraints and animacyidentification.3 GETARUNSIn a number of papers (Delmonte 1990;1991;1992;1994; 2003;2004) and in a book (Delmonte1992) we described our algorithms and thetheoretical background which inspired it.
Whereasthe old version of the system had a limitedvocabulary and was intended to work only in limiteddomains with high precision, the current version ofthe system has been created to cope withunrestricted text.
In Delmonte (2002), we reportedpreliminary results obtained on a corpus ofanaphorically annotated texts made available byR.Mitkov on his website.
Both definite descriptionsand pronominal expressions were considered,success rate was at 75% F-measure.
In those casewe used a very shallow and robust parser whichproduced only NP chunks which were then used tofire anaphoric processes.
However the texts makingup the corpus were technical manuals, where thescope and usage of pronominal expressions is verylimited.The current algorithm for anaphora resolution workson the output of a complete deep robust parserwhich builds an indexed linear list of dependencystructures where clause boundaries are clearlyindicated; differently from Connexor, our systemelaborates both grammatical relations and semanticroles information for arguments and adjuncts.Semantic roles are very important in the weightingprocedures.
Our system also produces implicitgrammatical relations which are either controlledSUBJects of untensed clauses, arguments oradjuncts of relative clauses.As to the anaphoric resolution algorithm, it is basedon the original Sidner?s (1983:Chapter 5) andWebber?s (1983:Chapter 6) intuitions on Focussingin Discourse.
We find distributed, local approachesto anaphora resolution more efficient thanmonolithic, global ones.
In particular we believethat due to the relevance of structural constraints inthe treatment of locally restricted classes ofpronominal expressions, it is more appropriate toactivate different procedures which by dealingseparately with non-locally restricted classes alsoafford separate evaluation procedures.
There arealso at least two principled reasons for theseparation into two classes.The first reason is a theoretical one.
Linguistictheory has long since established without any doubtthe existence in most languages of the world of atleast two classes: the class of pronouns which mustbe bound locally in a given domain and the class ofpronouns which must be left free in the samedomain ?
as a matter of fact, English also has a thirdclass of pronominals, the so-called long-distancesubject-of-consciousness bound pronouns (seeZribi-Hertz A., 1989);The second reason is empirical.
Anaphora resolutionis usually carried out by searching antecedentsbackward w.r.t.
the position of the current anaphoricexpression.
In our approach, we proceed in a clauseby clause fashion, weighting each candidateantecedent w.r.t.
that domain, trying to resolve itlocally.
Weighting criteria are amenable on the onehand to linear precedence constraints, with scoresassigned on a functional/semantic basis.
On theother hand, these criteria may be overrun by afunctional ranking of clauses which requires to treatmain clauses differently from secondary clauses,5and these two differently from complement clauses.On the contrary, global algorithms neglectaltogether such requirements: they weight eachreferring expression w.r.t.
the utterance, linearprecedence is only physically evaluated, nofunctional correction is introduced.3.1 Referential Policies and AlgorithmsThere are also two general referential policyassumption that we adopt in our approach: The firstone is related to pronominal expressions, the secondone to referring expressions or entities to be assertedin the History List, and are expressed as follows:- no more than two pronominal expressions areallowed to refer back in the previous discourseportion;- at discourse level, referring expressions arestored in a push-down stack according toPersistence principles.Persistence principles respond to psychologicalprinciples and limit the topicality space available touser w.r.t.
a given text.
It has a bidimensionalnature: it is determined both in relation to an overalltopicality frequency value and to an utterancenumber proximity value.Only ?persistent?
referring expressions are allowedto build up the History List, where persistence isestablished on the basis of the frequency oftopicality for each referring expression which mustbe higher than 1.
All referring expression asserted asTopic (Secondary, Potential) only once arediscarded in case they appeared at a distancemeasured in 5 previous utterances.
Proximatereferring expressions are allowed to be asserted inthe History List.In particular, if Mitkov considers the paragraph asthe discourse unit most suitable for coreferring andcospecifying operation at discourse level, we preferto adopt a parameterized procedure which isdefinable by the user and activated automatically: itcan be fired within a number that can vary fromevery 10 up to 50 sentences.
Our procedure has thetask to prune the topicality space and reduce thenumber of perspective topic for Main andSecondary Topic.
Thus we garbage-collect all non-relevant entities.
This responds to the empiricallyvalidated fact that as the distance between first andsecond mention of the same referring expressionincreases, people are obliged to repeat the samelinguistic description, using a definite expression ora bare NP.
Indefinites are unallowed and may onlyserve as first mention; they can also be used asbridging expression within opaque propositions.
Thefirst procedure is organized as follows:A.
For each clause,1.
we collect all referential expressions andweight them (see B below for criteria) ?
thisis followed by an automatic ranking;2. then we subtract pronominal expressions;3. at clause level, we try to bind personal andpossessive pronouns obeying specificstructural properties; we also bind reflexivepronouns and reciprocals if any, which mustbe bound obligatorily in this domain;4. when binding a pronoun, we check fordisjointness w.r.t.
a previously boundpronoun if any;5. all unbound pronouns and all remainingpersonal pronouns are asserted as?externals?, and are passed up to the higherclause levels;B. Weighting is carried out by taking into accountthe following linguistic properties associated to eachreferring expression:1.
Grammatical Function with usual hierarchy(SUBJ > ARG_MOD > OBJ > OBJ2 > IOBJ >NCMOD);2.
Semantic Roles, as they have been labelled inFrameNet, and in our manually producedfrequency lexicon of English;3.
Animacy: we use 75 semantic features derivedfrom WordNet, and reward Human andInstitution/Company labelled referringexpressions;4.
Functional Clause Type is further used tointroduce penalties associated to those referringexpressions  which don?t belong to main clause.C.
Then we turn at the higher level ?
if any -, andwe proceed as in A., in addition1.
we try to bind pronouns passed up by the lowerclause levelso if successful, this will activate a retract of the?external?
label and a label of?antecedenthood?
for the current pronounwith a given antecedent;o the best antecedent is chosen by recursivelytrying to match features of the pronoun withthe first available antecedent previouslyranked by weighting;o here again whenever a pronoun is bound wecheck for disjointness at utterance level.D.
This is repeated until all clauses are examinedand all pronouns are scrutinised and bound or leftfree.E.
Pronouns left free ?
those asserted as externals ?will be matched tentatively with the best candidatesprovided this time by a ?centering-like?
algorithm.Step A. is identical and is recursively repeated untilall clauses are processed.6Then, we move to step B. which in this case will useall referring expressions present in the utterance,rather than only those available locally.Fig.
1 GETARUNS AR algorithm3.2 Focussing RevisitedOur version of the focussing algorithm followsSidner?s proposal (Sidner C., 1983; Grosz B., SidnerC., 1986), to use a Focus Stack, a certain FocusAlgorithm with Focus movements and datastructures to allow for processing simple inferentialrelations between different linguistic descriptionsco-specifying or coreferring to a given entity.Our Focus Algorithm is organized as follows: foreach utterance, we assert three ?centers?
that we callMain, Secondary and the first Potential Topic,which represent the best three referring expressionsas they have been weighted in the candidate listused for pronominal binding; then we also keep alist of Potential Topics for the remaining bestcandidates.
These three best candidates repositoriesare renovated at each new utterance, and are usedboth to resolve pronominal and nominalcospecification and coreference: this is done both incase of strict identity of linguistic description and ofnon-identity.
The second case may occur eitherwhen derivational morphological properties allowthe two referring expressions to be matchedsuccessfully, or when a simple hyponym/hypernymrelation is entertained by two terms, one of which iscontained in the list of referring expressionscollected from the current sentence, and the other isamong one of the entities stored in the focus list.The Main Topic may be regarded the ForwardLooking Center in the centering terminology or theCurrent Focus.
All entities are stored in the HistoryList (HL) which is a stack containing theirmorphological and semantic features: this is not tobe confused with a Discourse Model - what we didin the deep complete system anaphora resolutionmodule ?
which is a highly semantically wroughtelaboration of the current text.
In the HL every newentity is assigned a semantic index which identifiesit uniquely.
To allow for Persistence evaluation, wealso assert rhetorical properties associated to eachentity, i.e.
we store the information of topicality (i.e.whether it has been evaluated as Main, Secondary orPotential Topic), together with the semantic ID andthe number of the current utterance.
This issubsequently used to measure the degree ofPersistence in the overall text of a given entity, asexplained below.In order to decide which entity has to become Main,Secondary or Potential Topic we proceed asfollows:- we collect all entities present in the History Listwith their semantic identifier and feature listand proceed to an additional weightingprocedure;- nominal expressions, they are divided up intofour semantic types: definite, indefinite, bareNPs, quantified NPs.
Both definite andindefinite NP may be computed as new or oldentity according to contextual conditions aswill be discussed below and are given arewarding score;- we enumerate for each entity its persistence inthe previous text, and keep entities which havefrequency higher than 1, we discard the others;- we recover entities which have been asserted inthe HL in proximity to the current utterance, upto four utterances back;- we use this list to ?resolve?
referringexpressions contained in the current utterance;- if this succeeds, we use the ?resolved?
entitiesas new Main, Secondary, and Potential Topicsand assert the rest in the Potential Topics stack;- if this fails ?
also partially ?
we use the bestcandidates in the weighted list of referringexpressions to assert the new Topics.
It may bethe case that both resolved and current bestcandidates are used, and this is by far the mostcommon case.4.
Evaluation and General DiscussionEvaluating anaphora resolution systems calls for areformulation of the usual parameters of Precisionand Recall as introduced in IR/IE field: in that case,there are two levels that are used as valuable results;a first stage where systems are measured for their7capacity to retrieve/extract relevant items from thecorpus/web (coverage-recall).
Then a second stagefollows in which systems are evaluated for theircapacity to match the content of the query(accuracy-precision).
In the field of IR/IE items tobe matched are usually constituted by words/phrasesand pattern-matching procedures are the norm.However, for AR systems this is not sufficient andNLP heavy techniques are used to get valuableresults.
As Mitkov also notes, this phase jeopardizesthe capacity of AR systems to reach satisfactoryaccuracy scores simply because of its intrinsicweakness: none of the off-the-shelf parsers currentlyavailable overcomes 90% accuracy.To clarify these issues, we present here below twoTables: in the first one we report data related to thevexed question of whether pleonastic ?it?
should beregarded as part of the task of anaphora resolutionor rather part of a separate classification task ?
assuggested in a number of papers by Mitkov.
In theformer case, they should contribute to the overallanaphora resolution evaluation metrics; in the lattercase they should be compute separately as a case ofclassification over all occurrences of ?it?
in thecurrent dataset and discarded from the overall count.Even though we don?t agree fully with Mitkov?sposition, we find it useful to deal with ?it?
separate,due to its high inherent ambiguity.
Besides, it is truethat the AR task is not like any InformationRetrieval task.In Table 1 below we reported figures for ?it?
inorder to evaluate the three algorithms in relation tothe classification task.
Then in Table 2. we reportgeneral data where we computed the two types ofaccuracy reported in the literature.
In Table 1 wesplit results for ?it?
into Wrong Reference vs WrongClassification: following Mitkov, in case we onlycomputed anaphora related cases and disregardedthose cases of ?it?
which were wrongly classified asexpletives.
Expletive ?it?
present in the text are 189:so at first we computed coverage and accuracy withthe usual formula that we report below.
Then wesubtracted wrongly classified cases from the numberof total ?it?
found in one case (following Mitkovwho claims that wrongly classified ?it?
found by thesystem should not count; in another case, thisnumber is subtracted from the total number of ?it?to be found in the text.
Only for MARS we thencomputed different measures of Coverage andAccuracy.
If we regard this approach worthpursuing, we come up with two Adjusted Accuracymeasures which are related to the revised totalnumbers of anaphors by the two subtractionsindicated above.We computed manually all third person pronominalexpressions and came up with a figure 982 which isTable 1.
Expletive ?it?
compared resultsMARS JavaRAP GuiTAR GETARUNSCoverage 163  (86.2%) 188  (99.5%) 188  (99.5%) 171  (91%)Accuracy 1 63  (33.3%) 73  (38.6%) 75  (39.7%) 87  (46 %)Wrong Classification 44163-44=119189-44=14549189-49=14064189-64=12553189-53=136Wrong Reference 56 66 49 32Accuracy 2 63  (38.6%)Adjusted Accuracy 2 63  (52.9%)Adjusted Accuracy 3 63  (43.4%) 73  (52.1%) 75  (60%) 87 (64 %)only confirmed by one of the three systemsconsidered: JavaRAP.
Pronouns considered are thefollowing one, lower case and upper case included:Possessives ?
his, its, her, hers, their, theirsPersonals ?
he, she, it, they, him, her, it, them(where ?it?
and ?her?
have to be disambiguated)Reflexives ?
himself, itself, herself, themselvesThere are 16 different wordforms.
As can be seenfrom the table below, apart from JavaRAP, none ofthe other systems considered comes close to 100%coverage.Computing general measures for Precision andRecall we have three quantities (see also Poesio &Kabadjov):?
total number of anaphors present in the text;?
anaphors identified by the system;?
correctly resolved anaphors.Formulas related to Accuracy/Success Rate orPrecision are as follows: Accuracy1 = number ofsuccessfully resolved anaphors/number of allanaphors; Accuracy2 = number of successfullyresolved anaphors/number of anaphors found(attempted to be resolved).
Recall - which shouldcorrespond to Coverage - we come up with formula:R= number of anaphors found /number of allanaphors to be resolved (present in the text).
Finallythe formula for F-measure is as follows:2*P*R/(P+R) where P is chosen as Accuracy 2.8Table 2.
Overall results Coverage/AccuracyCOVERAGE ACCURACY 1 ACCURACY 2 F-measureMARS 936  (95.3%) 403/982  (41.5%) 403/903 (43%) 59.26%JavaRAP 981  (100%) 490/982  (49.9%) 490/981 (50%) 66.7%GUITAR 824  (84.8%) 445/982  (45.8%) 445/824 (54%) 65.98%GETARUNS 885  (90.1%) 555/982  (56.5%) 555/885 (62.7%) 73.94%In absolute terms best accuracy figures have beenobtained by GETARUNS, followed by JavaRAP.
Soit is still thanks to the classic Recall formula thatthis result stands out clearly.
We also producedanother table which can however only be workedout for our system, which uses a distributedapproach.
We managed to separate pronominalexpressions in relation to their contribution at thedifferent levels of anaphora resolution considered:clause level, utterance level, discourse level.
Atclause level, only those pronouns which must bebound locally are checked, as is the case withreflexive pronouns, possessives, some cases ofexpletive ?it?
: both arguments and adjuncts maycontribute the appropriate antecedent.
At utterancelevel, in case the sentence is complex or there ismore than one clause, also personal subject/objectpronouns may be bound (if only preferentially so).Eventually, those pronouns which do not find anantecedent are regarded discourse level pronouns.We collapsed under CLAUSE all pronouns bound atclause and utterance level; DISCOURSE containsonly sentence external pronouns.
Expletives havebeen computed in a separate column.Table 3.
GETARUNS pronouns collapsed at structural levelCLAUSE DISCOURSE EXPLETIVES TOTALSPronouns found 410 366 109 885Correct 266 222 67 555Errors made 144 144 42 330As can be noticed easily, the highest percentage ofpronouns found is at Clause level: this is nothowever the best performance of the system, whichon the contrary performs better at discourse level.Expletives contribute by far the highest correctresult.
We also found correctly 47 ?there?
expletivesand 6 correctly classified pronominal ?there?
whichhowever have been left unbound.
The system alsofound 48 occurrences of deictic discourse bound?this?
and ?that?, which corresponds to the fullcoverage.Finally, nominal expressions: the History List (HL)has been incremented up to 2243 new entities.
Thesystem identified 2773 entities from the HL bymatching their linguistic description.
The overallnumber of resolution actions taken by the DiscourseLevel algorithm is 1861: this includes both cases ofnominal and pronominal expressions.
However,since only 366 can be pronouns, the remaining 1500resolution actions have been carried out on nominalexpressions present in the HL.
If we compare theseresults to the ones computed by GuiTAR, whichassign semantic indices to NamedEntitiesdisregarding their status of anaphora, we can seethat the whole text is made up of 12731 NEs.GuiTAR finds 1585 cases of identity relationsbetween a NE and an antecedent.
However,GuiTAR introduces always new indices and createslocal antecedent-referring expression chains ratherthan repeating the same index of the chain head.
Inthis way, it is difficult if not impossible to computehow many times the text corefers/cospecifies to thesame referring expressions.
On the contrary, in ourcase, this can be easily computed by counting howmany times the same semantic index is beingrepeated in a ?resolution?
or ?identity?
action of theanaphora resolution algorithm.
For instance, theJury is coreferred/cospecified 12 times; Price Danielalso 12 times and so on.5.
ConclusionsThe error rate of both Charniak?s and Connexor?s asreported in the literature, is approximately the same,20%; this notwithstanding, MARS has a slightlyreduced coverage when compared with JavaRAP,96%.
GuiTAR has the worst coverage, 85%.
As toaccuracy, none of the three algorithms overruns50%: JavaRAP has the best score 49.9%.
HoweverGETARUNS has 63% correct score, with 90%coverage.There are at least three reasons why our system has abetter performance: one is the presence of a richerfunctional and semantic information as explainedabove, which comes with augmented head-dependent structures.
Second reason is the decisionto split the referential process into two and treatutterance level pronominal expressions separatelyfrom discourse level ones.
Third reason is the wayin which discourse level anaphora resolution is9organized: our version of the Centering algorithmhinges on a record of a list of best antecedentsweighted on the basis of their behaviour in HistoryList and on their intrinsic semantic properties.
Thesethree properties of our AR algorithm can be dubbedthe Knowledge Rich approach.F-measures approximates very closely what weobtained in a previous experiment: however, as awhole it is an insufficient score to insure adequateconfidence in semantic substitution of anaphoricitems by the head of the antecedent.
Improvementsneed to come from parsing and the lexicalcomponent.AcknowledgementsThanks to three anonymous reviewers who helped usimprove the overall layout of the paper.ReferencesDelmonte R. 1990.
Semantic Parsing with an LFG-basedLexicon and Conceptual Representations, Computers& the Humanities, 5-6, pp.461-488.Delmonte R. and D.Bianchi 1991.
Binding Pronominalswith an LFG Parser, Proceeding of the SecondInternational Workshop on Parsing Technologies,Cancun(Messico), ACL 1991, pp.59-72.Delmonte R., D.Bianchi 1992.
Quantifiers in Discourse,in Proc.
ALLC/ACH'92, Oxford(UK), OUP, pp.
107-114.Delmonte R. 1992.
Linguistic and Inferential Processingin Text Analysis by Computer, UP, Padova.Delmonte R. and D.Bianchi 1994.
Computing DiscourseAnaphora from Grammatical Representation, inD.Ross & D.Brink(eds.
), Research in HumanitiesComputing 3, Clarendon Press, Oxford, 179-199.Delmonte R. and D.Bianchi 1999.
Determining EssentialProperties of Linguistic Objects for Unrestricted TextAnaphora Resolution, Proc.
Workshop on Proceduresin Discourse, Pisa, pp.10-24.Delmonte R., L.Chiran, and C.Bacalu, (2000).
TowardsAn Annotated Database For Anaphora Resolution,LREC, Atene, pp.63-67.Delmonte R. 2002a.
From Deep to Shallow AnaphoraResolution: What Do We Lose, What Do We Gain, inProc.
International Symposium RRNLP, Alicante,pp.25-34.Delmonte R. 2002b.
From Deep to Shallow AnaphoraResolution:, in Proc.
DAARC2002 , 4th DiscourseAnaphora and Anaphora Resolution Colloquium,Lisbona, pp.57-62.Delmonte, R. 2003.
Getaruns: a Hybrid System forSummarization and Question Answering.
In Proc.Natural Language Processing (NLP) for Question-Answering, EACL, Budapest, pp.
21-28.Delmonte R. 2004.
Evaluating GETARUNS Parser withGREVAL Test Suite, In Proc.
ROMAND - 20thCOLING, University of Geneva, pp.
32-41.Di Eugenio B.
1990.
Centering Theory and the Italianpronominal system, COLING, Helsinki.Grosz B. and C. Sidner 1986.
Attention, Intentions, andthe Structure of Discourse, Computational Linguistics12 (3), 175-204.Kennedy, C. and B. Boguraev, 1996.
Anaphora foreveryone: Pronominal anaphora resolution without aparser.
In Proc.
of the 16th COLING, Budapest.Long Qiu, Min-Yen Kan, and Tat-Seng Chua, 2004.
APublic Reference Implementation of the RAPAnaphora Resolution Algorithm, In Proceedings of theLanguage Resources and Evaluation Conference 2004(LREC 04), Lisbon, Portugal, pp.1-4.Mitkov R. 1995.
Two Engines are better than one:Generating more power and confidence in the searchfor the antecedent, Proceedings of Recent Advances inNatural Language Processing, Tzigov Chark, 87-94.Mitkov, R. 1998.
Robust Pronoun Resolution withlimited knowledge.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING?98)/ACL?98 Conference, pp.869-875, Montreal, Canada.Mitkov, R., R. Evans, and C. Orasan.
2002.
A New, FullyAutomatic Version of Mitkov?s Knowledge-PoorPronoun Resolution Method, Proceedings of CICLing-2002, pp.1-19.Poesio, M. and R. Vieira, 1998.
A corpus-basedinvestigation of definite description use.Computational  Linguistics, 24(2):183?216.Poesio, M. and Mijail A. Kabadjov 2004.
A General-Purpose, off-the-shelf Anaphora Resolution Module:Implementation and Preliminary EvaluationProceedings of the Language Resources andEvaluation Conference 2004 (LREC 04), Lisbon,Portugal, pp.1-4.Sidner C. 1983.
Focusing in the Comprehension ofDefinite Anaphora, in Brady M., Berwick R.(eds.
),Computational Models of Discourse, MIT Press,Cambridge, MA, 267-330.Webber B.
1983.
So can we Talk about Now?, in BradyM., Berwick R.(eds.
), Computational Models ofDiscourse, MIT Press, Cambridge, MA, 331-371.Webber B. L. 1991.
Structure and Ostension in theInterpretation of Discourse Deixis, in Language andCognitive Processes 6 (2):107-135.Zribi-Hertz A.
1989.
Anaphor Binding and NarrativePoint of View: English reflexive pronouns in sentenceand discourse, Language, 65(4):695-727.10
