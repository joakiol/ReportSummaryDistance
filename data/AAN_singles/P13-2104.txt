Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 585?590,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDependency Parser Adaptation with Subtreesfrom Auto-Parsed Target Domain DataXuezhe MaDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195, USAxzma@uw.eduFei XiaDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195, USAfxia@uw.eduAbstractIn this paper, we propose a simple andeffective approach to domain adaptationfor dependency parsing.
This is a fea-ture augmentation approach in which thenew features are constructed based on sub-tree information extracted from the auto-parsed target domain data.
To demon-strate the effectiveness of the proposed ap-proach, we evaluate it on three pairs ofsource-target data, compared with sever-al common baseline systems and previousapproaches.
Our approach achieves signif-icant improvement on all the three pairs ofdata sets.1 IntroductionIn recent years, several dependency parsing algo-rithms (Nivre and Scholz, 2004; McDonald et al,2005a; McDonald et al, 2005b; McDonald andPereira, 2006; Carreras, 2007; Koo and Collins,2010; Ma and Zhao, 2012) have been proposedand achieved high parsing accuracies on severaltreebanks of different languages.
However, theperformance of such parsers declines when train-ing and test data come from different domain-s.
Furthermore, the manually annotated treebanksthat these parsers rely on are highly expensive tocreate.
Therefore, developing dependency pars-ing algorithms that can be easily ported from onedomain to another?say, from a resource-rich do-main to a resource-poor domain?is of great im-portance.Several approaches have been proposed for thetask of parser adaptation.
McClosky et at.
(2006)successfully applied self-training to domain adap-tation for constituency parsing using the rerank-ing parser of Charniak and Johnson (2005).
Re-ichart and Rappoport (2007) explored self-trainingwhen the amount of the annotated data is smalland achieved significant improvement.
Zhang andWang (2009) enhanced the performance of depen-dency parser adaptation by utilizing a large-scalehand-crafted HPSG grammar.
Plank and van No-ord (2011) proposed a data selection method basedon effective measures of domain similarity for de-pendency parsing.There are roughly two varieties of domain adap-tation problem?fully supervised case in whichthere are a small amount of labeled data in thetarget domain, and semi-supervised case in whichthere are no labeled data in the target domain.
Inthis paper, we present a parsing adaptation ap-proach focused on the fully supervised case.
It is afeature augmentation approach in which the newfeatures are constructed based on subtree infor-mation extracted from the auto-parsed target do-main data.
For evaluation, we run experimentson three pairs of source-target domains?WSJ-Brown, Brown-WSJ, and WSJ-Genia.
Our ap-proach achieves significant improvement on al-l these data sets.2 Our Approach for Parsing AdaptationOur approach is inspired by Chen et al (2009)?swork on semi-supervised parsing with addition-al subtree-based features extracted from unlabeleddata and by the feature augmentation method pro-posed by Daume III (2007).
In this section, wefirst summarize Chen et al?s work and explainhow we extend that for domain adaptation.
Wewill then highlight the similarity and differencebetween our work and Daume?s method.2.1 Semi-supervised parsing withsubtree-based featuresOne of the most well-known semi-supervisedparsing methods is self-training, where a parsertrained from the labeled data set is used to parseunlabeled data, and some of those auto-parsed dataare added to the labeled data set to retrain the pars-585ing models.
Chen et al (2009)?s approach differsfrom self-training in that partial information (i.e.,subtrees), instead of the entire trees, from the auto-parsed data is used to re-train the parsing models.A subtree is a small part of a dependencytree.
For example, a first-order subtree is a singleedge consisting of a head and a dependent, and asecond-order sibling subtree is one that consists ofa head and two dependents.
In Chen et al (2009),they first extract all the subtrees in auto-parsed da-ta and store them in a list Lst.
Then they countthe frequency of these subtrees and divide theminto three groups according to their levels of fre-quency.
Finally, they construct new features forthe subtrees based on which groups they belongsto and retrain a new parser with feature-augmentedtraining data.12.2 Parser adaptation with subtree-basedFeaturesChen et al (2009)?s work is for semi-supervisedlearning, where the labeled training data and thetest data come from the same domain; the subtree-based features collected from auto-parsed data areadded to all the labeled training data to retrain theparsing model.
In the supervised setting for do-main adaptation, there is a large amount of labeleddata in the source domain and a small amount oflabeled data in the target domain.
One intuitiveway of applying Chen?s method to this setting is tosimply take the union of the labeled training datafrom both domains and add subtree-based featuresto all the data in the union when re-training theparsing model.
However, it turns out that addingsubtree-based features to only the labeled trainingdata in the target domain works better.
The stepsof our approach are as follows:1.
Train a baseline parser with the small amountof labeled data in the target domain and usethe parser to parse the large amount of unla-beled sentences in the target domain.2.
Extract subtrees from the auto-parsed dataand add subtree-based features to the labeledtraining data in the target domain.3.
Retrain the parser with the union of the la-beled training data in the two domains, wherethe instances from the target domain are aug-mented with the subtree-based features.1If a subtree does not appear in Lst, it falls to the fourthgroup for ?unseen subtrees?.To state our feature augmentation approachmore formally, we use X to denote the input s-pace, and Ds and Dt to denote the labeled da-ta in the source and target domains, respective-ly.
Let X ?
be the augmented input space, and ?sand ?t be the mappings from X to X ?
for the in-stances in the source and target domains respec-tively.
The mappings are defined by Eq 1, where0 =< 0, 0, .
.
.
, 0 >?
X is the zero vector.
?s(xorg) = < xorg,0 >?t(xorg) = < xorg,xnew > (1)Here, xorg is the original feature vector in X ,and xnew is the vector of the subtree-based fea-tures extracted from auto-parsed data of the targetdomain.
The subtree extraction method used inour approach is the same as in (Chen et al, 2009)except that we use different thresholds when di-viding subtrees into three frequency groups: thethreshold for the high-frequency level is TOP 1%of the subtrees, the one for the middle-frequencylevel is TOP 10%, and the rest of subtrees belongto the low-frequency level.
These thresholds arechosen empirically on some development data set.The idea of distinguishing the source and tar-get data is similar to the method in (Daume III,2007), which did feature augmentation by defin-ing the following mappings:2?s(xorg) = < xorg,0 >?t(xorg) = < xorg,xorg > (2)Daume III showed that differentiating featuresfrom the source and target domains improved per-formance for multiple NLP tasks.
The differencebetween that study and our approach is that ournew features are based on subtree information in-stead of copies of original features.
Since the newfeatures are based on the subtree information ex-tracted from the auto-parsed target data, they rep-resent certain properties of the target domain andthat explains why adding them to the target dataworks better than adding them to both the sourceand target data.3 ExperimentsFor evaluation, we tested our approach on threepairs of source-target data and compared it with2The mapping in Eq 2 looks different from the one pro-posed in (Daume III, 2007), but it can be proved that the twoare equivalent.586several common baseline systems and previousapproaches.
In this section, we first describe thedata sets and parsing models used in each of thethree experiments in section 3.1.
Then we pro-vide a brief introduction to the systems we havereimplemented for comparison in section 3.2.
Theexperimental results are reported in section 3.3.3.1 Data and ToolsIn the first two experiments, we used the Wal-l Street Journal (WSJ) and Brown (B) portion-s of the English Penn TreeBank (Marcus et al,1993).
In the first experiment denoted by ?WSJ-to-B?, WSJ corpus is used as the source domainand Brown corpus as the target domain.
In thesecond experiment, we use the reverse order ofthe two corpora and denote it by ?B-to-WSJ?.
Thephrase structures in the treebank are converted intodependencies using Penn2Malt tool3 with the stan-dard head rules (Yamada and Matsumoto, 2003).For the WSJ corpus, we used the standard datasplit: sections 2-21 for training and section 23 fortest.
In the experiment of B-to-WSJ, we random-ly selected about 2000 sentences from the trainingportion of WSJ as the labeled data in the target do-main.
The rest of training data in WSJ is regardedas the unlabeled data of the target domain.For Brown corpus, we followed Reichart andRappoport (2007) for data split.
The training andtest sections consist of sentences from all of thegenres that form the corpus.
The training portionconsists of 90% (9 of each 10 consecutive sen-tences) of the data, and the test portion is the re-maining 10%.
For the experiment of WSJ-to-B,we randomly selected about 2000 sentences fromtraining portion of Brown and use them as labeleddata and the rest as unlabeled data in the target do-main.In the third experiment denoted by ?
?WSJ-to-G?, we used WSJ corpus as the source domain andGenia corpus (G)4 as the target domain.
FollowingPlank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanuet al, 2008) which are also from WSJ sections2-21 but converted into dependency structure bythe LTH converter (Johansson and Nugues, 2007).The Genia corpus is converted to CoNLL formatwith LTH converter, too.
We randomly selected3http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html4Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gzSource Targettraining training unlabeled testWSJ-to-B 39,832 2,182 19,632 2,429B-to-WSJ 21,814 2,097 37,735 2,416WSJ-to-G 39,279 1,024 13,302 1,360Table 1: The number of sentences for each data setused in our experimentsabout 1000 sentences from the training portion ofGenia data and use them as the labeled data of thetarget domain, and the rest of training data of Ge-nia as the unlabeled data of the target domain.
Ta-ble 1 shows the number of sentences of each dataset used in the experiments.The dependency parsing models we used in thisstudy are the graph-based first-order and second-order sibling parsing models (McDonald et al,2005a; McDonald and Pereira, 2006).
To be morespecific, we use the implementation of MaxPars-er5 with 10-best MIRA (Crammer et al, 2006; M-cDonald, 2006) learning algorithm and each pars-er is trained for 10 iterations.
The feature sets offirst-order and second-order sibling parsing mod-els used in our experiments are the same as theones in (Ma and Zhao, 2012).
The input to Max-Parser are sentences with Part-of-Speech tags; weuse gold-standard POS tags in the experiments.Parsing accuracy is measured with unlabeled at-tachment score (UAS) and the percentage of com-plete matches (CM) for the first and second experi-ments.
For the third experiment, we also report la-beled attachment score (LAS) in order to comparewith the results in (Plank and van Noord, 2011).3.2 Comparison SystemsFor comparison, we re-implemented the follow-ing well-known baselines and previous approach-es, and tested them on the three data sets:SrcOnly: Train a parser with the labeled datafrom the source domain only.TgtOnly: Train a parser with the labeled datafrom the target domain only.Src&Tgt: Train a parser with the labeled datafrom the source and target domains.Self-Training: Following Reichart and Rap-poport (2007), we train a parser with theunion of the source and target labeled data,parse the unlabeled data in the target domain,5http://sourceforge.net/projects/maxparser/587add the entire auto-parsed trees to the man-ually labeled data in a single step withoutchecking their parsing quality, and retrain theparser.Co-Training: In the co-training system, we firsttrain two parsers with the labeled data fromthe source and target domains, respectively.Then we use the parsers to parse unlabeleddata in the target domain and select sentencesfor which the two parsers produce identicaltrees.
Finally, we add the analyses for thosesentences to the union of the source and tar-get labeled data to retrain a new parser.
Thisapproach is similar to the one used in (Sagaeand Tsujii, 2007), which achieved the highestscores in the domain adaptation track of theCoNLL 2007 shared task (Nivre et al, 2007).Feature-Augmentation: This is the approachproposed in (Daume III, 2007).Chen et al (2009): The algorithm has been ex-plained in Section 2.1.
We use the union ofthe labeled data from the source and targetdomains as the labeled training data.
Theunlabeled data needed to construct subtree-based features come from the target domain.Plank and van Noord (2011): This system per-forms data selection on a data pool consistingof large amount of labeled data to get a train-ing set that is similar to the test domain.
Theresults of the system come from their paper,not from the reimplementation of their sys-tem.Per-corpus: The parser is trained with the largetraining set from the target domain.
For ex-ample, for the experiment of WSJ-to-B, allthe labeled training data from the Brown cor-pus is used for training, including the subsetof data which are treated as unlabeled in ourapproach and other comparison systems.
Theresults serve as an upper bound of domainadaptation when there is a large amount oflabeled data in the target domain.3.3 ResultsTable 2 illustrates the results of our approach withthe first-order parsing model in the first and sec-ond experiments, together with the results of thecomparison systems described in section 3.2.
TheWSJ-to-B B-to-WSJUAS CM UAS CMSrcOnlys 88.8 43.8 86.3 26.5TgtOnlyt 86.6 38.8 88.2 29.3Src&Tgts,t 89.1 44.3 89.4 31.2Self-Trainings,t 89.2 45.1 89.8 32.1Co-Trainings,t 89.2 45.1 89.8 32.7Feature-Augs,t 89.1 45.1 89.8 32.8Chen (2009)s,t 89.3 45.0 89.7 31.8this papers,t 89.5 45.5 90.2 33.4Per-corpusT 89.9 47.0 92.7 42.1Table 2: Results with the first-order parsing modelin the first and second experiments.
The super-script indicates the source of labeled data used intraining.WSJ-to-B B-to-WSJUAS CM UAS CMSrcOnlys 89.8 47.3 88.0 30.4TgtOnlyt 87.7 42.2 89.7 34.2Src&Tgts,t 90.2 48.2 90.9 36.6Self-Trainings,t 90.3 48.8 91.0 37.1Co-Trainings,t 90.3 48.5 90.9 38.0Feature-Augs,t 90.0 48.4 91.0 37.4Chen (2009)s,t 90.3 49.1 91.0 37.6this papers,t 90.6 49.6 91.5 38.8Per-corpusT 91.1 51.1 93.6 47.9Table 3: Results with the second-order siblingparsing model in the first and second experiments.results with the second-order sibling parsing mod-el is shown in Table 3.
The superscript s, t and Tindicates from which domain the labeled data areused in training: tag s refers to the labeled data inthe source domain, tag t refers to the small amountof labeled data in the target domain, and tag T in-dicates that all the labeled training data from thetarget domain, including the ones that are treatedas unlabeled in our approach, are used for training.Table 4 shows the results in the third experimen-t with the first-order parsing model.
We also in-clude the result from (Plank and van Noord, 2011),which use the same parsing model as ours.
Notethat this result is not comparable with other num-bers in the table as it uses a larger set of labeleddata, as indicated by the ?
superscript.All three tables show that our system out-performs the comparison systems in all three588WSJ-to-GUAS LASSrcOnlys 83.8 82.0TgtOnlyt 87.0 85.7Src&Tgts,t 87.2 85.9Self-Trainings,t 87.3 86.0Co-Trainings,t 87.3 86.0Feature-Augs,t 87.9 86.5Chen (2009)s,t 87.5 86.2this papers,t 88.4 87.1Plank (2011)?
- 86.8Per-corpusT 90.5 89.7Table 4: Results with first-order parsing model inthe third experiment.
?Plank (2011)?
refers to theapproach in Plank and van Noord (2011).experiments.6 The improvement of our ap-proach over the feature augmentation approachin Daume III (2007) indicates that adding subtree-based features provides better results than makingseveral copies of the original features.
Our systemoutperforms the system in (Chen et al, 2009), im-plying that adding subtree-based features to onlythe target labeled data is better than adding themto the labeled data in both the source and targetdomains.Considering the three steps of our approach inSection 2.2, the training data used to train the pars-er in Step 1 can be from the target domain only orfrom the source and target domains.
Similarly, inStep 3 the subtree-based features can be added tothe labeled data from the target domain only orfrom the source and target domains.
Therefore,there are four combinations.
Our approach is theone that uses the labeled data from the target do-main only in both steps, and Chen?s system useslabeled data from the source and target domains inboth steps.
Table 5 compares the performance ofthe final parser in the WSJ-to-Genia experimen-t when the parser is created with one of the fourcombinations.
The column label and the row labelindicate the choice in Step 1 and 3, respectively.The table shows the choice in Step 1 does not havea significant impact on the performance of the fi-nal models; in contrast, the choice in Step 3 doesmatter?
adding subtree-based features to the la-beled data in the target domain only is much betterthan adding features to the data in both domains.6The results of Per-corpus are better than ours but it usesa much larger labeled training set in the target domain.TgtOnly Src&TgtTgtOnly 88.4/87.1 88.4/87.1Src&Tgt 87.6/86.3 87.5/86.2Table 5: The performance (UAS/LAS) of the fi-nal parser in the WSJ-to-Genia experiment whendifferent training data are used to create the finalparser.
The column label and row label indicatethe choice of the labeled data used in Step 1 and 3of the process described in Section 2.2.4 ConclusionIn this paper, we propose a feature augmentationapproach for dependency parser adaptation whichconstructs new features based on subtree informa-tion extracted from auto-parsed data from the tar-get domain.
We distinguish the source and targetdomains by adding the new features only to thedata from the target domain.
The experimental re-sults on three source-target domain pairs show thatour approach outperforms all the comparison sys-tems.For the future work, we will explore the po-tential benefits of adding other types of featuresextracted from unlabeled data in the target do-main.
We will also experiment with various waysof combining our current approach with other do-main adaptation methods (such as self-trainingand co-training) to further improve system perfor-mance.ReferencesXavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CONLL, pages 957?961.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminative r-eranking.
In Proceedings of the 43rd Meeting ofthe Association for Computional Linguistics (ACL2005), pages 132?139.Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimo-to, and Kentaro Torisawa.
2009.
Improving de-pendency parsing with subtrees from auto-parseddata.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 570?579, Singapore, August.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Jornal of MachineLearning Research, 7:551?585.589Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics (A-CL 2007), pages 256?263, Prague, Czech Republic,June.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
In Proceedings of NODALIDA, Tartu, Estonia.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of 48thMeeting of the Association for Computional Linguis-tics (ACL 2010), pages 1?11, Uppsala, Sweden, July.Xuezhe Ma and Hai Zhao.
2012.
Fourth-order depen-dency parsing.
In Proceedings of COLING 2012:Posters, pages 785?796, Mumbai, India, December.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for pars-er adaptation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics (COLING-ACL 2006), pages337?344, Sydney, Australia, July.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of European Association forComputational Linguistics (EACL-2006), pages 81?88, Trento, Italy, April.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training ofdependency parsers.
In Proceedings of the 43rdAnnual Meeting on Association for ComputationalLinguistics (ACL-2005), pages 91?98, Ann Arbor,Michigan, USA, June 25-30.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage (HLT/EMNLP 05), pages 523?530, Van-couver, Canada, October.Ryan McDonald.
2006.
Discriminative learning span-ning tree algorithm for dependency parsing.
Ph.D.thesis, University of Pennsylvania.Joakim Nivre and Mario Scholz.
2004.
Determinis-tic dependency parsing of english text.
In Proceed-ings of the 20th international conference on Com-putational Linguistics (COLING?04), pages 64?70,Geneva, Switzerland, August 23-27.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech, June.Barbara Plank and Gertjan van Noord.
2011.
Effec-tive measures of domain similarity for parsing.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT 2011), pages 1566?1576, Portland, Oregon, USA, June.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics (ACL-2007), pages 616?623,Prague, Czech Republic, June.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependen-cy parsing and domain adaptation with LR modelsand parser ensembles.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages1044?1050, Prague, Czech Republic, June.Mihai Surdeanu, Richard Johansson, Adam Meyers, L-luis Marquez, and Joakim Nivre.
2008.
The conll-2008 shared task on joint parsing of syntactic andsemantic dependencies.
In Proceedings of the 12thConference on Computational Natural LanguageLearning (CoNLL-2008), pages 159?177, Manch-ester, UK, Augest.Hiroyasu Yamada and Yuji Matsumoto.
2003.
S-tatistical dependency analysis with support vectormachines.
In Proceedings of the 8th Internation-al Workshop on Parsing Technologies (IWPT-2003),pages 195?206, Nancy, France, April.Yi Zhang and Rui Wang.
2009.
Cross-domain depen-dency parsing using a deep linguistic grammar.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing(ACL-IJCNLP 2009), pages 378?386, Suntec, Sin-gapore, August.590
