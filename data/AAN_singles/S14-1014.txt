Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 99?109,Dublin, Ireland, August 23-24 2014.Generating Simulations of Motion Events from Verbal DescriptionsJames PustejovskyComputer Science Dept.Brandeis UniversityWaltham, MA USAjamesp@cs.brandeis.eduNikhil KrishnaswamyComputer Science Dept.Brandeis UniversityWaltham, MA USAnkrishna@brandeis.eduAbstractIn this paper, we describe a computationalmodel for motion events in natural lan-guage that maps from linguistic expres-sions, through a dynamic event interpreta-tion, into three-dimensional temporal sim-ulations in a model.
Starting with themodel from (Pustejovsky and Moszkow-icz, 2011), we analyze motion events us-ing temporally-traced Labelled TransitionSystems.
We model the distinction be-tween path- and manner-motion in an op-erational semantics, and further distin-guish different types of manner-of-motionverbs in terms of the mereo-topological re-lations that hold throughout the process ofmovement.
From these representations,we generate minimal models, which arerealized as three-dimensional simulationsin software developed with the game en-gine, Unity.
The generated simulationsact as a conceptual ?debugger?
for the se-mantics of different motion verbs: thatis, by testing for consistency and infor-mativeness in the model, simulations ex-pose the presuppositions associated withlinguistic expressions and their composi-tions.
Because the model generation com-ponent is still incomplete, this paper fo-cuses on an implementation which mapsdirectly from linguistic interpretations intothe Unity code snippets that create the sim-ulations.1 IntroductionSemantic interpretation requires access to bothknowledge about words and how they compose.As the linguistic phenomena associated with lexi-cal semantics have become better understood, sev-eral assumptions have emerged across most mod-els of word meaning.
These include the following:(1) a. Lexical meaning involves some sort of?componential analysis?, either throughpredicative primitives or a system oftypes.b.
The selectional properties of predicatorscan be explained in terms of these com-ponents;c. An understanding of event semantics andthe different role of event participantsseems crucial for modeling linguistic ut-terances.As a starting point in lexical semantic analysis,a standard methodology in both theoretical andcomputational linguistics is to identify features ina corpus that differentiate the data in meaningfulways; meaningful in terms of prior theoretical as-sumptions or in terms of observably differentiatedbehaviors.
Combining these strategies we might,for instance, take a theoretical constraint that wehope to justify through behavioral distinctions inthe data.
An example of this is the theoreticalclaim that motion verbs can be meaningfully di-vided into two classes: manner- and path-orientedpredicates (Talmy, 1985; Jackendoff, 1983; Talmy,2000).
These constructions can be viewed as en-coding two aspects of meaning: how the move-ment is happening and where it is happening.
Theformer strategy is illustrated in (2a) and the latterin (2b) (where m indicates a manner verb, and pindicates a path verb).
(2) a.
The ball rolledm.b.
The ball crossedpthe room.With both of the verb types, adjunction can makereference to the missing aspect of motion, by intro-ducing a path (as in (3a)) or the manner of move-ment (in (3b)).
(3) a.
The ball rolledmacross the room.b.
The ball crossedpthe room rolling.Differences in syntactic distribution and grammat-ical behavior in large datasets, in fact, correlate99fairly closely with the theoretical claims made bylinguists using small introspective datasets.The path-manner classification is a case wherethere are data-derived distinctions that corre-late nicely with theoretically inspired predictions.More often than not, however, lexical semanticdistinctions are formal stipulations in a linguisticmodel, that often have no observable correlationsto data.
For example, an examination of the man-ner of movement class from Levin (1993) illus-trates this point.
The verbs below are all Levin-class manner of motion verbs:(4) MANNER OF MOTION VERBS: drive, walk,run, crawl, fly, swim, drag, slide, hop, rollAssuming the two-way distinction between pathand manner predication of motion mentionedabove, these verbs do, in fact, tend to pattern ac-cording to the latter class in the corpus.
Giventhat they are all manner of motion verbs, however,any data-derived distinctions that emerge withinthis class will have to be made in terms of addi-tional syntactic or semantic dimensions.
While itis most likely possible to differentiate, for exam-ple, the verbs slide from roll, or walk from hop inthe corpus, given enough data, it is important torealize that conceptual and theoretical modeling isoften necessary to reveal the factors that semanti-cally distinguish such linguistic expressions, in thefirst place.We argue that this problem can be approachedwith the use of minimal model generation.
AsBlackburn and Bos (2008) point out, theoremproving (essentially type satisfaction of a verb inone class as opposed to another) provides a ?nega-tive handle?
on the problem of determining consis-tency and informativeness for an utterance, whilemodel building provides a ?positive handle?
onboth.
For our concerns, simulation constructionprovides a positive handle on whether two man-ner of motion processes are distinguished in themodel.
Further, the simulation must specify howthey are distinguished, the analogue to informa-tiveness.In this paper, we argue that traditional lexicalmodeling can benefit greatly from examining howsemantic interpretations are contextually and con-ceptually grounded.
We explore a dynamic in-terpretation of the lexical semantic model devel-oped in Generative Lexicon Theory (Pustejovsky,1995; Pustejovsky et al., 2014).
Specifically, weare interested in using model building (Blackburnand Bos, 2008; Konrad, 2004; Gardent and Kon-rad, 2000) and simulation generation (Coyne andSproat, 2001; Siskind, 2011) to reveal the concep-tual presuppositions inherent in natural languageexpressions.
In this paper, we focus our attentionon motion verbs, in order to distinguish betweenmanner and path motion verbs, as well as to modelmereotopological distinctions within the mannerclass.2 Situating Motion in Space and TimeThe interpretation of motion in language has beenone of the most researched areas in linguisticsand Artificial Intelligence (Kuipers, 2000; Freksa,1992; Galton, 2000; Levinson, 2003; Mani andPustejovsky, 2012).
Because of their grammaticaland semantic import, linguistic interest in identi-fying where events happen has focused largely onmotion verbs and the role played by paths.
Jack-endoff (1983), for example, elaborates a semanticsfor motion verbs incorporating explicit referenceto the path traversed by the mover, from source todestination (goal) locations.
Talmy (1983) devel-ops a similar conceptual template, where the pathfollowed by the figure is integral to the conceptu-alization of the motion against a ground.
Hence,the path can be identified as the central element indefining the location of the event (Talmy, 2000).Related to this idea, both Zwarts (2005) and Puste-jovsky and Moszkowicz (2011) develop mecha-nisms for dynamically creating the path traversedby a mover in a manner of motion predicate, suchas run or drive.
Starting with this approach, thelocalization of a motion event, therefore, is atleast minimally associated with the path createdby virtue of the activity.In addition to capturing the spatial trace of theobject in motion, several researchers have pointedout that identifying the shape of the path dur-ing motion is also critical for fully interpretingthe semantics of movement.
Eschenbach et al.
(1999) discusses the orientation associated withthe trajectory, something they refer to as orientedcurves.
Motivated more by linguistic considera-tions, Zwarts (2006) introduces the notion of anevent shape, which is the trajectory associatedwith an event in space represented by a path.
Hedefines a shape function, which is a partial func-tion assigning unique paths to those events involv-ing motion or extension in physical space.
Thiswork suggests that the localization of an event100makes reference to orientational as well as config-urational factors, a view that is pursued in Puste-jovsky (2013b).
This forces us to look at the var-ious spatio-temporal regions associated with theevent participants, and the interactions betweenthem.These issues are relevant to our present con-cerns, because in order to construct a simulation, amotion event must be embedded within an appro-priate minimal embedding space.
This must suf-ficiently enclose the event localization, while op-tionally including room enough for a frame of ref-erence visualization of the event (the viewer?s per-spective).
We return to this issue later in the paperwhen constructing our simulation from the seman-tic interpretation associated with motion events.3 Modeling Motion in Language3.1 Theoretical AssumptionsThe advantage of adopting a dynamic interpre-tation of motion is that we can directly distin-guish path predication from manner of motionpredication in an operational semantics (Millerand Charles, 1991; Miller and Johnson-Laird,1976) that maps nicely to a simulation environ-ment.
Models of processes using updating typi-cally make reference to the notion of a state tran-sition (van Benthem, 1991; Harel, 1984).
This isdone by distinguishing between formulae, ?, andprograms, pi.
A formula is interpreted as a clas-sical propositional expression, with assignment ofa truth value in a specific model.
We will inter-pret specific models by reference to specific states.A state is a set of propositions with assignmentsto variables at a specific index.
Atomic programsare input/output relations ( [[pi]] ?
S ?
S), andcompound programs are constructed from atomicones following rules of dynamic logic (Harel et al.,2000).For the present discussion, we represent the dy-namics of actions in terms of Labeled TransitionSystems (LTSs) (van Benthem, 1991).1An LTSconsists of a triple, ?S,Act,?
?, where: S is theset of states; Act is a set of actions; and?
is a to-tal transition relation:??
S?Act?S.
An action,?
?
Act, provides the labeling on an arrow, mak-ing it explicit what brings about a state-to-state1This is consistent with the approach developed in (Fer-nando, 2009; Fernando, 2013).
This approach to a dynamicinterpretation of change in language semantics is also in-spired by Steedman (2002).transition.
As a shorthand for (e1, ?, e2) ?
?, wewill also use e1???
e2.
If reference to the statecontent (rather than state name) is required for in-terpretation purposes (van Benthem et al., 1994),then as shorthand for ({?
}e1, ?, {??
}e2) ?
?, weuse, ?e1???
??e2.
Finally, when referringto temporally-indexed states in the model, whereei@i indicates the state eiinterpreted at time i, asshorthand for ({?
}e1@i, ?, {??
}e2@i+1) ?
?, wewill use, ?ie1???
?
?i+1e2, as described in Puste-jovsky (2013).3.2 Distinguishing Path and Manner MotionWe will assume that change of location of an ob-ject can be viewed as a special instance of a first-order program, which we will refer to as ?
(Puste-jovsky and Moszkowicz, 2011).2(5) x := y (?-transition, where loc(z) is valuebeing updated)?x assumes the value given to y in the nextstate.?
?M, (i, i+ 1), (u, u[x/u(y)])?
|= x := yiff ?M, i, u?
|= loc(z) = x ?
?M, i +1, u[x/u(y)]?
|= loc(z) = yGiven a simple transition, a process can be viewedas simply an iteration of ?
(Fernando, 2009).However, as (Pustejovsky, 2013a) points out, sincemost manner motion verbs in language are ac-tually directed processes, simple decompositionsinto change-of-location are inadequate.
That is,they are guarded transitions where the test is notjust non-coreference, but makes reference to val-ues on a scale, C, and ensures that it continues inan order-preserving change through the iterations.When this test references the values on a scale, C,we call this a directed ?-transition (~?
), e.g., x 4 y,x < y:(6) ~?
=dfC?xei???
ei+1.
(7) loc(z) = xe0~???
loc(z) = y1 e1~???
.
.
.loc(z) = yn enThis now provides us with our dynamic interpre-tation of directed manner of motion verbs, suchas slide, swim, roll, where we have an iteration ofassignments of locations, undistinguished except2Cf.
Groenendijk and Stokhof (1990) for dynamic updat-ing, and Naumann (2001) for a related analysis.101that the values are order-preserving according to ascalar constraint.This is quite different from the dynamic inter-pretation of path predicates.
Following (Galton,2004; Pustejovsky and Moszkowicz, 2011), pathpredicates such as arrive and leave make refer-ence to a ?distinguished location?, not an arbi-trary location.
For example, the ball enters theroom is satisified when the distinguished location,D, (the room) is successfully tested as the loca-tion for the moving object.
That is, the locationis tested against the current location for an object((loc(x) 6= D)?
), and retested until it is satisfied((loc(x) = D)?).
(8)(loc(x)6=D)?xloc(z) = xe0~???
(loc(x) 6=D)?xloc(z) = y1 e1~???
.
.
.
(loc(x)=D)?xloc(z) = yn enWhile beyond the scope of the present discus-sion, it is worth noting that the model of eventstructure adopted here for motion verbs fits wellwith most of the major semantic and syntactic phe-nomena associated with event classes and Aktion-sarten.33.3 Mereotopological Distinctions in MannerGiven the formal distinction between path andmanner predicates as described above, let us ex-amine how to differentiate meaning within themanner class.
Levin (1993) differentiates thisclass in terms of argument alternation patterns,and identifies the following verb groupings: ROLL,RUN, EPONYMOUS VEHICLE, WALTZ, ACCOM-PANY, and CHASE verbs.
While suggestive, thesedistinctions are only partially useful towards actu-ally teasing apart the semantic dimensions alongwhich we identify the contributing factors of man-ner.Mani and Pustejovsky (2012) suggest a differ-ent strategy involving the identification of seman-tic parameters that clearly differentiate verb sensesfrom each other within this class.
One parameterexploited quite extensively within the motion classinvolves the mereotopological contraints that in-here throughout the movement of the object (Ran-dell et al., 1992; Asher and Vieu, 1995; Gal-ton, 2000).
Using this parameter, we are able todistinguish several of Levin?s classes of manner3Cf.
(Pustejovsky, 2013a) and (Krifka, 1992).as well as some novel ones, as described in (9),where a class is defined by the constraints that holdthroughout the event (where EC is ?externally con-nected?, and DC is ?disconnected?).
(9) For Figure (F) relative to Ground (G):a. EC(F,G), throughout motion:b. DC(F,G), throughout motion:c. EC(F,G) followed by DC(F,G), through-out motion:d. Sub-part(F?,F), EC(F?,G) followed byDC(F?,G), throughout motion:e. Containment of F in a Vehicle (V).For example, consider the semantic distinction be-tween the verbs slide and hop or bounce.
When thelatter are employed in induced (directed) motionconstructions (Levin, 1993; Jackendoff, 1996),they take on the meaning of manner of motionverbs.
Distinguishing between a sliding and hop-ping motion involves inspecting the next-statecontent in the motion n-gram: namely, there is acontinuous satisfaction of EC(F,G) throughout themotion for slide and a toggling effect (on-off) forthe predicates bounce and hop, as shown in (10).
(10)?DC(x,G)?xloc(z) = xe0~??
?DC(x,G)?xloc(z) = y1 e1~???
?DC(x,G)?xloc(z) = y2 e2With the surface as the ground argument, theseverbs are defined in terms of two transitions.4BAs1AAs3s2l1 l2 l3Figure 1: Slide MotionBAs1AAs3s2l1 l2 l3Figure 2: Hop Motion4Many natural language predicates require reference to atleast three states.
These include the semelfactives mentionedabove, as well as blink and iterative uses of knock and clap(Vendler, 1967; Dowty, 1979; Rothstein, 2008).102Distinguishing between a sliding motion and arolling motion is also fairly straightforward.
Wehave the entailments that result from each kind ofmotion, given a set of initial conditions, as in thefollowing short sentence describing the motion ofa ball relative to a floor (the domain for our eventsimulations).?
The ball slid.
: At the termination of the ac-tion, object ball has moved relative to a sur-face in a manner that is [+translate].That is, the movement is a translocationacross absolute space, but other attributes(such as the ball?s orientation) do not change.?
The ball rolled.
: At the termination of theaction, object ball has moved relative to asurface in a manner that is [+translate]and [+rotate].
Here, the translocationacross space is preserved, with the additionof an orientation change.We can further decompose these features, cast-ing the [+translate] in terms of the trans-lation?s dimensionality.
For both the ball slidand the ball rolled, it is required that the ball re-main in the contact with the relevant surface, thuswe can enforce a [-3-dimensional] con-straint on the [+translate] feature.
Thus,we arrive at the following differentiating se-mantic constraints for these verbs: (a) slide,[+translate], [-3-dimensional]; (b)roll, [+translate], [-3-dimensional],[+rotate].
This is illustrated below over threestates of execution.Bs1AabcBs2AbacBs3AcbaFigure 3: Roll MotionIn our approach to conceptual modeling, we hy-pothesize that between the members of any pair ofmotion verbs, there exists at least one distinctivefeature of physical motion that distinguishes thetwo predicates.
While this may be too strong, itis helpful in our use of simulations for debuggingthe lexical semantics of linguistic expressions.5Inorder to quantify the qualitative distinctions be-tween motion predicates and identify the preciseprimitive components of a motion verb, we builda real-time simulation, within which the individ-ual features of a single motion verb can be definedand isolated in three-dimensional space.The idea of constructing simulations from lin-guistic utterances is, of course, not new.
Thereare two groups of researchers who have developedrelated ideas quite extensively: simulation theo-rists, working in the philosophy of mind, such asAlvin Goldman and Robert Gordon; and cogni-tive scientists and linguists, such as Jerry Feldman,Ron Langacker, and Ben Bergen.
According toGoldman (1989), simulation provides a process-driven theory of mind and mental attribution, dif-fering from the theory-driven models proposed byChurchland and others (Churchland, 1991).
Fromthe cognitive linguistics tradition, simulation se-mantics has come to denote the mental instanti-ation of an interpretation of any linguistic utter-ance (Feldman, 2006; Bergen et al., 2007; Bergen,2012).
While these communities do not seem toreference each other, it is clear from our perspec-tive, that they are both pursuing similar programs,where distinct linguistic utterances correspond togenerated models that have differentiated struc-tures and behaviors (Narayanan, 1999; Siskind,2011; Goldman, 2006).4 Simulations as Minimal ModelsThe approach to simulation construction intro-duced in the previous section is inspired by workin minimal model generation (Blackburn and Bos,2008; Konrad, 2004).
Type satisfaction in thecompositional process mirrors the theorem prov-ing component, while construction of the specificmodel helps us distinguish what is inherent in thedifferent manner of motion events.
This latter as-pect is the ?positive handle?, (Blackburn and Bos,2008) which demonstrates the informativeness ofa distinction in our simulation.Simulation software must be able to map a pred-icate to a known behavior, its arguments to objectsin the scene, and then prompt those objects to ex-ecute the behavior.
A simple input sentence needs5Obviously, true synonyms in the lexicon would not bedistinguishable in a model.103to be tagged and parsed and transformed into pred-icate/argument representation, and from there intoa dynamic event structure, as in (Pustejovsky andMoszkowicz, 2011).
The event structure is inter-preted as the transformation executed over the ob-ject or objects in each frame, and then rendered.Ball1/NNP crossed/VBD Floor/NNPSBJ OBJBall1/NNP rolled/VBDSBJTable 1: Dependency parses for Ball1 crossedFloor (top) and Ball1 rolled (bottom).We currently use only proper names to refer toobjects in the scene, to simplify model generation,hence Ball1 and Floor.
This facilitates easy objectidentification in this prototype development stage.Given a tagged and dependency parsed sen-tence, we can the transform the parse into a pred-icate formula, using the root of the parse as thepredicate, the subject as a singleton first argument,and all objects as an optional stack of subsequentarguments.1.
pred := cross 1. pred := roll2.
x := Ball1 2. x := Ball13.
y.push(Floor)cross(Ball1,[Floor]) roll(Ball1)Table 2: Transformation to predicate formula forBall1 crossed Floor and Ball1 rolled.The resulting predicates are represented in Ta-ble 3 as expressions in Dynamic Interval Tempo-ral Logic (DITL) (Pustejovsky and Moszkowicz,2011), which are equivalent to the LTS expres-sions used above.cross(Ball1,Floor)loc(Ball1) := y, target(Ball1) := z; b := y;(y := w; y 6= w; d(b,y) < d(b,w),d(b,z) > d(z,w), IN(y,Floor))+roll(Ball1)loc(Ball1) := y, rot(Ball1) := z; bloc:= y,brot:= z; (y := w; y 6= w; d(bloc,y) < d(bloc,w),IN(y,Floor))+, (z := v; z 6= v; z-brot< v-brot)+Table 3: DITL expressions for Ball1 crossed Floorand Ball1 rolled.The DITL expression forms the basis of thecoded behavior.
The first two initialization stepsare coded into the behavior?s start function whilethe the third, Kleene iterated step, is encoded inthe behavior?s update function.5 Generating SimulationsWe use the freely-available game engine, Unity,(Goldstone, 2009) to handle all underlying graph-ics processing, and limited our object library tosimple primitive shapes of spheroids, rectangularprisms, and planes.
For every instance of an ob-ject, the game engine maintains a data structure forthe object?s virtual representation.
Table 4 showsthe data structure for Entity, the superclass ofall movable objects.Entity:position: 3-vector rotation: 3-vectorscale: 3-vector transform: Matrixcollider =center: 3-vectormin: 3-vectormax: 3-vectorradius: floatgeometry: MeshcurrentBehavior: BehaviorTable 4: Data structure of motion-capable entities.The position and scale of the object arerepresented as 3-vectors of floating point numbers.The rotation is represented as the Euler anglesof the object?s current rotation, also a 3-vector.This 3-vector is computed as a quaternion for ren-dering purposes.
The transform matrix com-poses the position, scale, and quaternion rotationinto the complete transformation applied to the ob-ject at any given frame.
The geometry is a mesh.The points, edges, faces, and texture attributes thatcomprise the mesh are all immutable at the mo-ment so the mesh type is considered atomic forour purposes.
The collider contains the coor-dinates of the center of the object, minimum andmaximum extents of the object?s boundaries, andradius of the boundaries (for spherical objects).Behaviors can only be executed over Entityinstances, so we also provide each one with acurrentBehavior property, referencing thecode to be executed over the object every framethat said behavior is being run.
This code performsa transformation over the object at every step, gen-erating a new state in a dynamic model of theevent denoted by the a given predicate.
Thus, theevent6is decomposed into frame-by-frame trans-formations representing the ?-transition from Sec-tion 3.2.We generate example simulations of behaviorsin a sample environment, shown in Figure 4, that6These events are linguistic events, and not the same as?events?
as used in software development or with event han-dlers.104consists of a sealed four-walled room that containsa number of primitive objects.Figure 4: Sample environment in top-down andperspective views.The behaviors currently coded into our softwaremap directly from DITL to the simulation.
Thevarious parts of the DITL formula that describes agiven behavior are coded into the behavior?s startor update functions in Unity.
Below is one suchC# code snippet: the per-frame transformation forroll.
(11) transform.rotation = new Vector3(0.0,0.0,transform.rotation.z+(rotSpeed*deltaTime));transform.position = new Vector3(transform.position.x-radius*deltaTime,transform.position.y,transform.position.z);This ?translates?
the DITL expression (y := w; y6= w; d(bloc,y) < d(bloc,w))+, (z := v; z 6= v; z-brot< v-brot),IN(y,Floor)+while explicitly calculat-ing the value of the precise differences in locationand rotation between each frame or time step.
Thevariables moveSpeed, rotSpeed and radiusare given explicit value.
deltaTime refers to thetime elapsed between frames.Translating a DITL formula into executablecode makes evident the differences in minimalverb pairs, such as the ball (or box) rolled and theball (or box) slid.
When an object rolls, one areaon the object must remain in contact with the sup-porting surface, and that area must be adjacent tothe area contacting the surface in the previous timestep.
When an object slides, the same area on theobject must contact the supporting surface.
Com-pare the per-frame transformation for slide belowto the given transformation for roll.
(12) transform.position = new Vector3(transform.position.x-radius*deltaTime,transform.position.y,transform.position.z);This maps the DITL expression (y := w; y 6= w;d(bloc,y) < d(bloc,w),IN(y,Floor))+.
Here, the ob-ject?s location changes along a path leading awayfrom the start location, but does not rotate as inroll.DITL expressions and their coded equivalentscan also be composed into new, more specific mo-tions.
The cross formula from Section 4 can becomposed with that for roll to describe a ?rollacross?
motion.In a model, a path verb such as cross doesnot necessarily need an explicit manner of mo-tion specified.
In a simulation, the manner needsto be given a value, requiring the composition ofthe path verb (e.g., cross) with one of a certainsubsets of manner verbs specifying how the ob-ject moves relative to the supporting surface.
Be-low are DITL expressions and code implementa-tions for two cross predicates, the first a cross mo-tion while sliding, the second a cross motion whilerolling.
(13) loc(Ball1) := y, target(Ball1) := z; b := y;(y := w; y 6= w; d(b,y) < d(b,w), d(b,z) >d(z,w), IN(y,Floor))+offset = transform.position-destination;offset = Vector3.Normalize(offset);transform.position = new Vector3(transform.position.x-offset.x*radius*deltaTime,transform.position.y,transform.position.z-offset.z*radius*deltaTime);At each frame, the distance between the object?scurrent position and its previously computed des-tination is computed again, and the update movesthe object away from its current position (d(b,y) <d(b,w)) toward the destination (d(b,z) > d(z,w)).Since no other manner of motion is specified, theobject does not turn or rotate as it moves, but sim-ply ?slides.
?105(14) loc(Ball1) := y, target(Ball1) := z; b := y; (y:= w; y 6= w; d(bloc,y) < d(bloc,w), d(bloc,z)> d(z,w), (u := v; u 6= v; u-brot< v-brot),IN(y,Floor))+offset = transform.position-destination;offset = Vector3.Normalize(offset);transform.rotation = new Vector3(0.0,arccos(offset.z)*(360/PI*2),transform.rotation.z+(rotSpeed*deltaTime));transform.position = new Vector3(transform.position.x-offset.x*radius*deltaTime,transform.position.y,transform.position.z-offset.z*radius*deltaTime);Here the update is the same as above, butwith the introduction of the rolling motion.
Inboth code snippets, the non-changing value oftransform.position.y implicitly maps theIN RCC condition in the DITL formulas, andkeeps the moving object attached to the floor.If there exists a behavior corresponding to thepredicate (by name) on an entity bearing the nameof the predicate?s first (subject) argument, thetransformation encoded in that behavior is per-formed over the entity until an end condition spe-cific to the behavior is met.
The resulting animatedmotion depicts the manner of motion denoted bythe predicate.
Given a predicate of arity greaterthan 1, the simulator tries to prompt a behavior onthe first argument that can be run using parametersof the subsequent arguments.A cross behavior, for example, divides thesupporting surface into regions and attempts tomove the crossing object from one region to thethe opposite region.
In figure 5, the bounds ofFloor completely surround the bounds of Ball2(IN(Ball2,Floor) in RCC8).
This configurationmakes it possible for the simulation to compute amotion moving the Ball2 object from one side ofthe Floor to the other.The left side of figure 5 shows a ball rolling anda box sliding, a depiction of two predicates: Box1slid and Ball1 rolled.
The right side depicts Ball2crossed Floor (from the rear center to the frontcenter).
The starting state of each scene is over-laid semi-transparently while the in-progress stateis fully opaque.6 Discussion and ConclusionIn this paper, we describe a model for mappingnatural language motion expressions into a 3Dsimulation environment.
Our strategy has been touse minimal simulation generation as a conceptualFigure 5: Roll and slide motions in progress (top),and cross motion in progress (bottom).debugging tool, in order to tease out the semanticdifferences between linguistic expressions; specif-ically those between verbs that are members ofconventionally homogeneous classes, according tolinguistic analysis.It should be pointed out that our goal is differentfrom WordsEye (Coyne and Sproat, 2001).
Whilewe are interested in using simulation generationto differentiate semantic distinctions in both lex-ical classes and compositional constructions, thegoal behind WordsEye is to provide an enhancedinterface to allow non-specialists create 3D sceneswithout being familiar with special software mod-els for everyday objects and relations.
There areobvious synergies between these two goals thatcan be pursued.The simulations we create provide an interpre-tation of the given motion predicate over the givenentity, but not the only interpretation.
Just asCoyne et al.
(2010) does for static objects in theWordsEye system, we must apply some implicitconstraints to our motion predicates to allow themto be visually simulated.
For instance, in the rolland slide examples given in Figure 5, both objectsare moving in the same direction?parallel to theback wall of the room object.
Had the objects beenmoving perpendicular to the back wall or in anyother direction, as long as they remained in con-106tact with the floor at all times, the simulated mo-tion would still be considered a ?roll?
(if rotatingaround an axis parallel to the floor), or a ?slide?
(if not), regardless of what the precise direction ofmotion is.
Minimal pairs in a model have to becompared and contrasted in a discriminative way,and thus in modeling a slide predicate versus a rollpredicate, knowing that the distinction is one ofrotation parallel to the surface is enough to distin-guish the two predicates in a model.In a simulation, the discriminative process re-quires that the two contrasting behaviors look dif-ferent, and as such, the simulation software mustbe able to completely render a scene for eachframe from behavior start to behavior finish, andso every variable for every object being renderedmust have an assigned value, including the posi-tion of the object from frame to frame.
If thesevalues are left unspecified, the software either failsto compile or throws an exception.
Thus, we areforced to arbitrarily choose a direction of motion(as well as direction of rotation, speed of rota-tion, speed of motion, etc.).
As long as all non-changing variables are kept consistent between aminimal pair of behaviors, we can evaluate thequantitative and qualitative differences betweenthe values that do change.
As simulations re-quire values to be assigned to variables that can beleft unspecified in an ordinary modeling process,simulations expose presuppositions about the se-mantics of motion verbs and of compositions thatwould not be necessary in a model alone.In order to evaluate the appropriateness of agiven simulation, we are currently experimentingwith a strategy often used in classification and an-notation tasks, namely pairwise similarity judg-ments (Rumshisky et al., 2012; Pustejovsky andRumshisky, 2014).
This involves presenting a userwith a simple discrimination task that has a re-duced cognitive load, comparing the similarity ofthe example to the target instances.
In the presentcontext, a subject is shown a specific simulationresulting from the translation from textual input,through DITL, to the visualization.
A set of ac-tivity or event descriptions is given, and the sub-ject is then asked to select which best describesthe simulation shown; e.g., ?Is this a sliding?
?, ?Isthis a rolling??.
The results of this experiment arepresently being evaluated.The system is currently in the prototype stageand needs to be expanded in three main areas: ob-ject library, parsing pipeline, and predicate han-dling.
Our object and behavior libraries are cur-rently limited to geometric primitives and the mo-tions that can be applied over them.
While roll,slide, and cross behaviors can be scripted forspheres and cubes and shapes derived from them,a predicate like walk cannot be supported on thecurrent infrastructure.
Thus, we intend to expandthe object library to include more complex inan-imate objects (tables, chairs, or other householdobjects) as well as animate objects.
Having an ob-ject library containing forms capable of executinggreater numbers of predicates will allow us to im-plement those predicates.The parsing pipeline described in Section 4 isonly partially implemented, with the only com-pleted parts being the latter stages, relating a for-mulas to a scripted behavior and its arguments.
Weintend to expand the parsing pipeline to include allthe steps described in this paper: taking input asa simple natural language sentence, tagging andparsing it to extract the constituent parts of a pred-icate/argument representation, and using that out-put to prompt a behavior in software as a dynamicevent structure.
More robust parsing will affordus the opportunity to expand the diversity of pred-icates that the software can handle as well (Mc-Donald and Pustejovsky, 2014).
While currentlylimited to unary and binary predicates, we needto extend the capability to ternary predicates andpredicates of greater arity, including the use of ad-junct phrases and indirect objects.
We are in theprocess of developing an implementation that usesBoxer (Curran et al., 2007) so that we can createfirst-order models from the dynamic expressionsused here.AcknowledgementsWe would like to thank David McDonald for com-ments and discussion.
We would also like to thankthe reviewers for several substantial suggestionsfor improvements and clarifications to the paper.All remaining errors are of course the responsi-bilities of the authors.
This work was supportedin part by the Department of the Navy, Office ofNaval Research under grant N00014-13-1-0228.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the Office of Naval Research.107ReferencesNicholas Asher and Laure Vieu.
1995.
Towards a ge-ometry of common sense: a semantics and a com-plete axiomatisation of merotopology.
In Proceed-ings of IJCAI95, Montreal, Canada.Benjamin K. Bergen, Shane Lindsay, Teenie Matlock,and Srini Narayanan.
2007.
Spatial and linguisticaspects of visual imagery in sentence comprehen-sion.
Cognitive Science, 31(5):733?764.Benjamin K Bergen.
2012.
Louder than words: Thenew science of how the mind makes meaning.
BasicBooks.Patrick Blackburn and Johan Bos.
2008.
Computa-tional semantics.
THEORIA.
An International Jour-nal for Theory, History and Foundations of Science,18(1).Paul M Churchland.
1991.
Folk psychology and theexplanation of human behavior.
The future of folkpsychology: Intentionality and cognitive science,pages 51?69.Bob Coyne and Richard Sproat.
2001.
Wordseye: anautomatic text-to-scene conversion system.
In Pro-ceedings of the 28th annual conference on Computergraphics and interactive techniques, pages 487?496.ACM.Bob Coyne, Owen Rambow, Julia Hirschberg, andRichard Sproat.
2010.
Frame semantics in text-to-scene generation.
In Knowledge-Based and Intel-ligent Information and Engineering Systems, pages375?384.
Springer.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale nlp with c&cand boxer.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics Companion Volume Proceedings of the Demoand Poster Sessions, pages 33?36, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.David R Dowty.
1979.
Word meaning and Mon-tague grammar: The semantics of verbs and times ingenerative semantics and in Montague?s PTQ, vol-ume 7.
Springer.C.
Eschenbach, C. Habel, L. Kulik, et al.
1999.
Rep-resenting simple trajectories as oriented curves.
InFLAIRS-99, Proceedings of the 12th InternationalFlorida AI Research Society Conference, pages 431?436.Jerome Feldman.
2006.
From molecule to metaphor:A neural theory of language.
MIT press.Tim Fernando.
2009.
Situations in ltl as strings.
Infor-mation and Computation, 207(10):980?999.Tim Fernando.
2013.
Segmenting temporal intervalsfor tense and aspect.
In The 13th Meeting on theMathematics of Language, page 30.Christian Freksa.
1992.
Using orientation informationfor qualitative spatial reasoning.
Springer.Antony Galton.
2000.
Qualitative Spatial Change.Oxford University Press, Oxford.Antony Galton.
2004.
Fields and objects in space,time, and space-time.
Spatial Cognition and Com-putation, 4(1).Claire Gardent and Karsten Konrad.
2000.
Interpretingdefinites using model generation.
Journal of Lan-guage and Computation, 1(2):193?209.Alvin I Goldman.
1989.
Interpretation psycholo-gized*.
Mind & Language, 4(3):161?185.Alvin I Goldman.
2006.
Simulating minds: The phi-losophy, psychology, and neuroscience of mindread-ing.
Oxford University Press.Will Goldstone.
2009.
Unity Game Development Es-sentials.
Packt Publishing Ltd.Jeroen Groenendijk and Martin Stokhof.
1990.
Dy-namic predicate logic.
Linguistics and Philosophy,14:39?100.David Harel, Dexter Kozen, and Jerzy Tiuyn.
2000.Dynamic Logic.
The MIT Press, 1st edition.David Harel.
1984.
Dynamic logic.
In M. Gabbayand F. Gunthner, editors, Handbook of Philosophi-cal Logic, Volume II: Extensions of Classical Logic,page 497?604.
Reidel.Ray Jackendoff.
1983.
Semantics and Cognition.
MITPress.Ray Jackendoff.
1996.
The proper treatment of mea-suring out, telicity, and perhaps even quantificationin english.
Natural Language & Linguistic Theory,14(2):305?354.Karsten Konrad.
2004.
Model generation for naturallanguage interpretation and analysis, volume 2953.Springer.Manfred Krifka.
1992.
Thematic relations as links be-tween nominal reference and temporal constitution.Lexical matters, 2953.Benjamin Kuipers.
2000.
The spatial semantic hierar-chy.
Artificial Intelligence, 119(1):191?233.Beth Levin.
1993.
English verb class and alternations:a preliminary investigation.
University of ChicagoPress.S.C.
Levinson.
2003.
Space in Language and Cog-nition: Explorations in Cognitive Diversity.
Lan-guage, culture, and cognition.
Cambridge UniversityPress.Inderjeet Mani and James Pustejovsky.
2012.
Inter-preting Motion: Grounded Representations for Spa-tial Language.
Oxford University Press.108David McDonald and James Pustejovsky.
2014.
Onthe representation of inferences and their lexicaliza-tion.
In Advances in Cognitive Systems, volume 3.G.
Miller and W. Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and Cogni-tive Processes, 6(1):1?28.George A Miller and Philip N Johnson-Laird.
1976.Language and perception.
Belknap Press.Srinivas Narayanan.
1999.
Reasoning about actions innarrative understanding.
IJCAI, 99:350?357.Ralf Naumann.
2001.
Aspects of changes: a dynamicevent semantics.
Journal of semantics, 18:27?81.James Pustejovsky and Jessica Moszkowicz.
2011.The qualitative spatial dynamics of motion.
TheJournal of Spatial Cognition and Computation.James Pustejovsky and Anna Rumshisky.
2014.
Deepsemantic annotation with shallow methods.
LRECTutorial, May.James Pustejovsky, Anna Rumshisky, Olga Batiukova,and Jessica Moszkowicz.
2014.
Annotation of com-positional operations with glml.
In Harry Bunt, ed-itor, Computing Meaning, pages 217?234.
SpringerNetherlands.J.
Pustejovsky.
1995.
The Generative Lexicon.
Brad-ford Book.
Mit Press.James Pustejovsky.
2013a.
Dynamic event structureand habitat theory.
In Proceedings of the 6th Inter-national Conference on Generative Approaches tothe Lexicon (GL2013), pages 1?10.
ACL.James Pustejovsky.
2013b.
Where things happen: Onthe semantics of event localization.
In Proceedingsof ISA-9: International Workshop on Semantic An-notation.David Randell, Zhan Cui, and Anthony Cohn.
1992.A spatial logic based on regions and connections.
InMorgan Kaufmann, editor, Proceedings of the 3rdInternation Conference on Knowledge Representa-tion and REasoning, pages 165?176, San Mateo.Susan Rothstein.
2008.
Two puzzles for a theory oflexical aspect: Semelfactives and degree achieve-ments.
Event structures in linguistic form and in-terpretation, 5:175.Anna Rumshisky, Nick Botchan, Sophie Kushkuley,and James Pustejovsky.
2012.
Word sense inven-tories by non-experts.
In LREC, pages 4055?4059.Jeffrey Mark Siskind.
2011.
Grounding the lexi-cal semantics of verbs in visual perception usingforce dynamics and event logic.
arXiv preprintarXiv:1106.0256.Mark Steedman.
2002.
Plans, affordances, and combi-natory grammar.
Linguistics and Philosophy, 25(5-6):723?753.Leonard Talmy.
1983.
How language structures space.In Herbert Pick and Linda Acredolo, editors, Spa-tial Orientation: Theory, Research, and Application.Plenum Press.Leonard Talmy.
1985.
Lexicalization patterns: seman-tic structure in lexical forms.
In T. Shopen, editor,Language typology and semantic description Vol-ume 3:, pages 36?149.
Cambridge University Press.Leonard Talmy.
2000.
Towards a cognitive semantics.MIT Press.Johan van Benthem, Jan van Eijck, and Vera Ste-bletsova.
1994.
Modal logic, transition systemsand processes.
Journal of Logic and Computation,4(5):811?855.Johannes Franciscus Abraham Karel van Benthem.1991.
Logic and the flow of information.Z.
Vendler.
1967.
Linguistics in philosophy.
CornellUniversity Press Ithaca.J.
Zwarts.
2005.
Prepositional aspect and the algebraof paths.
Linguistics and Philosophy, 28(6):739?779.J.
Zwarts.
2006.
Event shape: Paths in the semanticsof verbs.109
