Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835?841,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproving Multi-Modal Representations Using Image Dispersion:Why Less is Sometimes MoreDouwe Kiela*, Felix Hill*, Anna Korhonen and Stephen ClarkUniversity of CambridgeComputer Laboratory{douwe.kiela|felix.hill|anna.korhonen|stephen.clark}@cl.cam.ac.ukAbstractModels that learn semantic representationsfrom both linguistic and perceptual in-put outperform text-only models in manycontexts and better reflect human conceptacquisition.
However, experiments sug-gest that while the inclusion of perceptualinput improves representations of certainconcepts, it degrades the representationsof others.
We propose an unsupervisedmethod to determine whether to includeperceptual input for a concept, and showthat it significantly improves the ability ofmulti-modal models to learn and representword meanings.
The method relies solelyon image data, and can be applied to a va-riety of other NLP tasks.1 IntroductionMulti-modal models that learn semantic conceptrepresentations from both linguistic and percep-tual input were originally motivated by parallelswith human concept acquisition, and evidence thatmany concepts are grounded in the perceptual sys-tem (Barsalou et al, 2003).
Such models extractinformation about the perceptible characteristicsof words from data collected in property normingexperiments (Roller and Schulte im Walde, 2013;Silberer and Lapata, 2012) or directly from ?raw?data sources such as images (Feng and Lapata,2010; Bruni et al, 2012).
This input is combinedwith information from linguistic corpora to pro-duce enhanced representations of concept mean-ing.
Multi-modal models outperform language-only models on a range of tasks, including mod-elling conceptual association and predicting com-positionality (Bruni et al, 2012; Silberer and Lap-ata, 2012; Roller and Schulte im Walde, 2013).Despite these results, the advantage of multi-modal over linguistic-only models has only beendemonstrated on concrete concepts, such aschocolate or cheeseburger, as opposed to abstractconcepts such as such as guilt or obesity.
Indeed,experiments indicate that while the addition ofperceptual input is generally beneficial for repre-sentations of concrete concepts (Hill et al, 2013a;Bruni et al, 2014), it can in fact be detrimentalto representations of abstract concepts (Hill et al,2013a).
Further, while the theoretical importanceof the perceptual modalities to concrete represen-tations is well known, evidence suggests this is notthe case for more abstract concepts (Paivio, 1990;Hill et al, 2013b).
Indeed, perhaps the most influ-ential characterization of the abstract/concrete dis-tinction, the Dual Coding Theory (Paivio, 1990),posits that concrete representations are encodedin both the linguistic and perceptual modalitieswhereas abstract concepts are encoded only in thelinguistic modality.Existing multi-modal architectures generallyextract and process all the information from theirspecified sources of perceptual input.
Since per-ceptual data sources typically contain informationabout both abstract and concrete concepts, such in-formation is included for both concept types.
Thepotential effect of this design decision on perfor-mance is significant because the vast majority ofmeaning-bearing words in everyday language cor-respond to abstract concepts.
For instance, 72% ofword tokens in the British National Corpus (Leechet al, 1994) were rated by contributors to the Uni-versity of South Florida dataset (USF) (Nelson etal., 2004) as more abstract than the noun war, aconcept that many would consider quite abstract.In light of these considerations, we proposea novel algorithm for approximating conceptualconcreteness.
Multi-modal models in which per-ceptual input is filtered according to our algorithmlearn higher-quality semantic representations thanprevious approaches, resulting in a significant per-formance improvement of up to 17% in captur-835ing the semantic similarity of concepts.
Further,our algorithm constitutes the first means of quan-tifying conceptual concreteness that does not relyon labor-intensive experimental studies or annota-tors.
Finally, we demonstrate the application ofthis unsupervised concreteness metric to the se-mantic classification of adjective-noun pairs, anexisting NLP task to which concreteness data hasproved valuable previously.2 Experimental ApproachOur experiments focus on multi-modal modelsthat extract their perceptual input automaticallyfrom images.
Image-based models more natu-rally mirror the process of human concept acquisi-tion than those whose input derives from exper-imental datasets or expert annotation.
They arealso more scalable since high-quality tagged im-ages are freely available in several web-scale im-age datasets.We use Google Images as our image source,and extract the first n image results for each con-cept word.
It has been shown that images fromGoogle yield higher-quality representations thancomparable sources such as Flickr (Bergsma andGoebel, 2011).
Other potential sources, such asImageNet (Deng et al, 2009) or the ESP GameDataset (Von Ahn and Dabbish, 2004), either donot contain images for abstract concepts or do notcontain sufficient images for the concepts in ourevaluation sets.2.1 Image Dispersion-Based FilteringFollowing the motivation outlined in Section 1, weaim to distinguish visual input corresponding toconcrete concepts from visual input correspond-ing to abstract concepts.
Our algorithm is moti-vated by the intuition that the diversity of imagesreturned for a particular concept depends on itsconcreteness (see Figure 1).
Specifically, we an-ticipate greater congruence or similarity among aset of images for, say, elephant than among im-ages for happiness.
By exploiting this connection,the method approximates the concreteness of con-cepts, and provides a basis to filter the correspond-ing perceptual information.Formally, we propose a measure, image disper-sion d of a concept word w, defined as the aver-age pairwise cosine distance between all the imagerepresentations { ~w1.
.
.
~wn} in the set of imagesfor that concept:Figure 1: Example images for a concrete (elephant?
little diversity, low dispersion) and an abstractconcept (happiness ?
greater diversity, high dis-persion).Figure 2: Computation of PHOW descriptors us-ing dense SIFT for levels l = 0 to l = 2 and thecorresponding histogram representations (Boschet al, 2007).d(w) =12n(n?
1)?i<j?n1?~wi?
~wj| ~wi|| ~wj|(1)We use an average pairwise distance-based met-ric because this emphasizes the total variationmore than e.g.
the mean distance from the cen-troid.
In all experiments we set n = 50.Generating Visual Representations Visualvector representations for each image were ob-tained using the well-known bag of visual words(BoVW) approach (Sivic and Zisserman, 2003).BoVW obtains a vector representation for an836image by mapping each of its local descriptorsto a cluster histogram using a standard clusteringalgorithm such as k-means.Previous NLP-related work uses SIFT (Fengand Lapata, 2010; Bruni et al, 2012) or SURF(Roller and Schulte im Walde, 2013) descriptorsfor identifying points of interest in an image,quantified by 128-dimensional local descriptors.We apply Pyramid Histogram Of visual Words(PHOW) descriptors, which are particularly well-suited for object categorization, a key componentof image similarity and thus dispersion (Bosch etal., 2007).
PHOW is roughly equivalent to run-ning SIFT on a dense grid of locations at a fixedscale and orientation and at multiple scales (seeFig 2), but is both more efficient and more accu-rate than regular (dense) SIFT approaches (Boschet al, 2007).
We resize the images in our datasetto 100x100 pixels and compute PHOW descriptorsusing VLFeat (Vedaldi and Fulkerson, 2008).The descriptors for the images were subse-quently clustered using mini-batch k-means (Scul-ley, 2010) with k = 50 to obtain histograms ofvisual words, yielding 50-dimensional visual vec-tors for each of the images.Generating Linguistic Representations Weextract continuous vector representations (also of50 dimensions) for concepts using the continu-ous log-linear skipgram model of Mikolov et al(2013a), trained on the 100M word British Na-tional Corpus (Leech et al, 1994).
This modellearns high quality lexical semantic representa-tions based on the distributional properties ofwords in text, and has been shown to outperformsimple distributional models on applications suchas semantic composition and analogical mapping(Mikolov et al, 2013b).2.2 Evaluation Gold-standardsWe evaluate models by measuring the Spearmancorrelation of model output with two well-knowngold-standards reflecting semantic proximity ?
astandard measure for evaluating the quality of rep-resentations (see e.g.
Agirre et al (2009)).To test the ability of our model to captureconcept similarity, we measure correlations withWordSim353 (Finkelstein et al, 2001), a selec-tion of 353 concept pairs together with a similar-ity rating provided by human annotators.
Word-Sim has been used as a benchmark for distribu-tional semantic models in numerous studies (seee.g.
(Huang et al, 2012; Bruni et al, 2012)).As a complementary gold-standard, we use theUniversity of South Florida Norms (USF) (Nelsonet al, 2004).
This dataset contains scores for freeassociation, an experimental measure of cognitiveassociation, between over 40,000 concept pairs.The USF norms have been used in many previousstudies to evaluate semantic representations (An-drews et al, 2009; Feng and Lapata, 2010; Sil-berer and Lapata, 2012; Roller and Schulte imWalde, 2013).
The USF evaluation set is partic-ularly appropriate in the present context becauseconcepts in the dataset are also rated for concep-tual concreteness by at least 10 human annotators.We create a representative evaluation set of USFpairs as follows.
We randomly sample 100 con-cepts from the upper quartile and 100 conceptsfrom the lower quartile of a list of all USF con-cepts ranked by concreteness.
We denote thesesets C, for concrete, and A for abstract respec-tively.
We then extract all pairs (w1, w2) in theUSF dataset such that bothw1andw2are inA?C.This yields an evaluation set of 903 pairs, of which304 are such that w1, w2?
C and 317 are suchthat w1, w2?
A.The images used in our experiments andthe evaluation gold-standards can be down-loaded from http://www.cl.cam.ac.uk/?dk427/dispersion.html.3 Improving Multi-ModalRepresentationsWe apply image dispersion-based filtering as fol-lows: if both concepts in an evaluation pair havean image dispersion below a given threshold, boththe linguistic and the visual representations are in-cluded.
If not, in accordance with the Dual Cod-ing Theory of human concept processing (Paivio,1990), only the linguistic representation is used.For both datasets, we set the threshold as themedian image dispersion, although performancecould in principle be improved by adjusting thisparameter.
We compare dispersion filtered rep-resentations with linguistic, perceptual and stan-dard multi-modal representations (concatenatedlinguistic and perceptual representations).
Sim-ilarity between concept pairs is calculated usingcosine similarity.As Figure 3 shows, dispersion-filtered multi-modal representations significantly outperform8370.1450.5320.4770.5420.1890.2290.2030.2470.00.10.20.30.40.5Similarity ?
WordSim 353 Free association ?
USF (903)Evaluation SetCorrelationModel RepresentationsLinguistic onlyImage onlyStandard multi?modalDispersion filteredFigure 3: Performance of conventional multi-modal (visual input included for all concepts) vs.image dispersion-based filtering models (visual in-put only for concepts classified as concrete) on thetwo evaluation gold-standards.standard multi-modal representations on bothevaluation datasets.
We observe a 17% increase inSpearman correlation on WordSim353 and a 22%increase on the USF norms.
Based on the corre-lation comparison method of Steiger (1980), bothrepresent significant improvements (WordSim353,t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1).
Inboth cases, models with the dispersion-based filteralso outperform the purely linguistic model, whichis not the case for other multi-modal approachesthat evaluate on WordSim353 (e.g.
Bruni et al(2012)).4 Concreteness and Image DispersionThe filtering approach described thus far improvesmulti-modal representations because image dis-persion provides a means to distinguish concreteconcepts from more abstract concepts.
Since re-search has demonstrated the applicability of con-creteness to a range of other NLP tasks (Turney etal., 2011; Kwong, 2008), it is important to exam-ine the connection between image dispersion andconcreteness in more detail.4.1 Quantifying ConcretenessTo evaluate the effectiveness of image dispersionas a proxy for concreteness we evaluated our al-gorithm on a binary classification task based onthe set of 100 concrete and 100 abstract conceptsA?C introduced in Section 2.
By classifying con-0.1840.2570.290.0540.1890.1670.00.10.20.30.4'concrete' pairs (304) 'abstract' pairs (317)Concept TypeCorrelationRepresentation ModalityLinguisticVisualLinguistic+VisualFigure 4: Visual input is valuable for representingconcepts that are classified as concrete by the im-age dispersion algorithm, but not so for conceptsclassified as abstract.
All correlations are with theUSF gold-standard.cepts with image dispersion below the median asconcrete and concepts above this threshold as ab-stract we achieved an abstract-concrete predictionaccuracy of 81%.While well-understood intuitively, concreteness isnot a formally defined notion.
Quantities such asthe USF concreteness score depend on the sub-jective judgement of raters and the particular an-notation guidelines.
According to the Dual Cod-ing Theory, however, concrete concepts are pre-cisely those with a salient perceptual representa-tion.
As illustrated in Figure 4, our binary clas-sification conforms to this characterization.
Theimportance of the visual modality is significantlygreater when evaluating on pairs for which bothconcepts are classified as concrete than on pairs oftwo abstract concepts.Image dispersion is also an effective predic-tor of concreteness on samples for which the ab-stract/concrete distinction is less clear.
On a differ-ent set of 200 concepts extracted by random sam-pling from the USF dataset stratified by concrete-ness rating (including concepts across the con-creteness spectrum), we observed a high correla-tion between abstractness and dispersion (Spear-man ?
= 0.61, p < 0.001).
On this more diversesample, which reflects the range of concepts typi-cally found in linguistic corpora, image dispersionis a particularly useful diagnostic for identifying838Concept Image Dispersion Conc.
(USF)shirt .488 6.05bed .495 5.91knife .560 6.08dress .578 6.59car .580 6.35ego 1.000 1.93nonsense .999 1.90memory .999 1.78potential .997 1.90know .996 2.70Table 1: Concepts with highest and lowest imagedispersion scores in our evaluation set, and con-creteness ratings from the USF dataset.the very abstract or very concrete concepts.
AsTable 1 illustrates, the concepts with the lowestdispersion in this sample are, without exception,highly concrete, and the concepts of highest dis-persion are clearly very abstract.It should be noted that all previous approachesto the automatic measurement of concreteness relyon annotator ratings, dictionaries or manually-constructed resources.
Kwong (2008) proposesa method based on the presence of hard-codedphrasal features in dictionary entries correspond-ing to each concept.
By contrast, S?anchez et al(2011) present an approach based on the positionof word senses corresponding to each concept inthe WordNet ontology (Fellbaum, 1999).
Turneyet al (2011) propose a method that extends a largeset of concreteness ratings similar to those in theUSF dataset.
The Turney et al algorithm quanti-fies the concreteness of concepts that lack such arating based on their proximity to rated conceptsin a semantic vector space.
In contrast to each ofthese approaches, the image dispersion approachrequires no hand-coded resources.
It is thereforemore scalable, and instantly applicable to a widerange of languages.4.2 Classifying Adjective-Noun PairsFinally, we explored whether image dispersioncan be applied to specific NLP tasks as an effec-tive proxy for concreteness.
Turney et al (2011)showed that concreteness is applicable to the clas-sification of adjective-noun modification as eitherliteral or non-literal.
By applying a logistic regres-sion with noun concreteness as the predictor vari-able, Turney et al achieved a classification accu-racy of 79% on this task.
This model relies on sig-nificant supervision in the form of over 4,000 hu-man lexical concreteness ratings.1Applying im-age dispersion in place of concreteness in an iden-tical classifier on the same dataset, our entirely un-supervised approach achieves an accuracy of 63%.This is a notable improvement on the largest-classbaseline of 55%.5 ConclusionsWe presented a novel method, image dispersion-based filtering, that improves multi-modal repre-sentations by approximating conceptual concrete-ness from images and filtering model input.
Theresults clearly show that including more percep-tual input in multi-modal models is not always bet-ter.
Motivated by this fact, our approach providesan intuitive and straightforward metric to deter-mine whether or not to include such information.In addition to improving multi-modal represen-tations, we have shown the applicability of the im-age dispersion metric to several other tasks.
Toour knowledge, our algorithm constitutes the firstunsupervised method for quantifying conceptualconcreteness as applied to NLP, although it does,of course, rely on the Google Images retrieval al-gorithm.
Moreover, we presented a method toclassify adjective-noun pairs according to modi-fication type that exploits the link between imagedispersion and concreteness.
It is striking that thisapparently linguistic problem can be addressedsolely using the raw data encoded in images.In future work, we will investigate the precisequantity of perceptual information to be includedfor best performance, as well as the optimal filter-ing threshold.
In addition, we will explore whetherthe application of image data, and the interactionbetween images and language, can yield improve-ments on other tasks in semantic processing andrepresentation.AcknowledgmentsDK is supported by EPSRC grant EP/I037512/1.FH is supported by St John?s College, Cambridge.AK is supported by The Royal Society.
SC is sup-ported by ERC Starting Grant DisCoTex (306920)and EPSRC grant EP/I037512/1.
We thank theanonymous reviewers for their helpful comments.1The MRC Psycholinguistics concreteness ratings (Colt-heart, 1981) used by Turney et al (2011) are a subset of thoseincluded in the USF dataset.839ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 19?27, Boulder, Colorado.Mark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological review, 116(3):463.Lawrence W Barsalou, W Kyle Simmons, Aron K Bar-bey, and Christine D Wilson.
2003.
Groundingconceptual knowledge in modality-specific systems.Trends in cognitive sciences, 7(2):84?91.Shane Bergsma and Randy Goebel.
2011.
Using vi-sual information to predict lexical preference.
InRANLP, pages 399?405.Anna Bosch, Andrew Zisserman, and Xavier Munoz.2007.
Image classification using random forests andferns.
In Proceedings of ICCV.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 136?145.
Asso-ciation for Computational Linguistics.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.Max Coltheart.
1981.
The MRC psycholinguisticdatabase.
The Quarterly Journal of ExperimentalPsychology, 33(4):497?505.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei.
2009.
Imagenet: A large-scale hi-erarchical image database.
In Computer Vision andPattern Recognition, 2009.
CVPR 2009.
IEEE Con-ference on, pages 248?255.
IEEE.Christiane Fellbaum.
1999.
WordNet.
Wiley OnlineLibrary.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 91?99.
Asso-ciation for Computational Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: Theconcept revisited.
In Proceedings of the 10th inter-national conference on World Wide Web, pages 406?414.
ACM.Felix Hill, Douwe Kiela, and Anna Korhonen.
2013a.Concreteness and corpora: A theoretical and practi-cal analysis.
CMCL 2013.Felix Hill, Anna Korhonen, and Christian Bentz.2013b.
A quantitative empirical analysis of the ab-stract/concrete distinction.
Cognitive science, 38(1).Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 873?882.
Asso-ciation for Computational Linguistics.Oi Yee Kwong.
2008.
A preliminary study on the im-pact of lexical concreteness on word sense disam-biguation.
In PACLIC, pages 235?244.Geoffrey Leech, Roger Garside, and Michael Bryant.1994.
Claws4: the tagging of the british nationalcorpus.
In Proceedings of the 15th conferenceon Computational linguistics-Volume 1, pages 622?628.
Association for Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of Inter-national Conference of Learning Representations,Scottsdale, Arizona, USA.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Douglas L Nelson, Cathy L McEvoy, and Thomas ASchreiber.
2004.
The University of South Floridafree association, rhyme, and word fragment norms.Behavior Research Methods, Instruments, & Com-puters, 36(3):402?407.Allan Paivio.
1990.
Mental representations: A dualcoding approach.
Oxford University Press.Stephen Roller and Sabine Schulte im Walde.
2013.A multimodal LDA model integrating textual, cog-nitive and visual modalities.
In Proceedings of the2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1146?1157, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.David S?anchez, Montserrat Batet, and David Isern.2011.
Ontology-based information content compu-tation.
Knowledge-Based Systems, 24(2):297?303.D Sculley.
2010.
Web-scale k-means clustering.
InProceedings of the 19th international conference onWorld wide web, pages 1177?1178.
ACM.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof the 2012 Joint Conference on Empirical Methods840in Natural Language Processing and ComputationalNatural Language Learning, pages 1423?1433.
As-sociation for Computational Linguistics.J.
Sivic and A. Zisserman.
2003.
Video Google: a textretrieval approach to object matching in videos.
InProceedings of the Ninth IEEE International Con-ference on Computer Vision, volume 2, pages 1470?1477, Oct.James H Steiger.
1980.
Tests for comparing ele-ments of a correlation matrix.
Psychological Bul-letin, 87(2):245.Peter D Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of the 2011 Conference on the Empiri-cal Methods in Natural Language Processing, pages680?690.A.
Vedaldi and B. Fulkerson.
2008.
VLFeat: An openand portable library of computer vision algorithms.http://www.vlfeat.org/.Luis Von Ahn and Laura Dabbish.
2004.
Labelingimages with a computer game.
In Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 319?326.
ACM.841
