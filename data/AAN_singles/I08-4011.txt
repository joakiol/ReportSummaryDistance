A Two-Stage Approach to Chinese Part-of-Speech TaggingAitao ChenYahoo!
Inc.701 First AvenueSunnyvale, CA 94089aitao@yahoo-inc.comYa ZhangYahoo!
Inc.701 First AvenueSunnyvale, CA 94089yazhang@yahoo-inc.comGordon SunYahoo!
Inc.701 First AvenueSunnyvale, CA 94089gzsun@yahoo-inc.comAbstractThis paper describes a Chinese part-of-speech tagging system based on the maxi-mum entropy model.
It presents a noveltwo-stage approach to using the part-of-speech tags of the words on both sides ofthe current word in Chinese part-of-speechtagging.
The system is evaluated on fourcorpora at the Fourth SIGHAN Bakeoff inthe close track of the Chinese part-of-speech tagging task.1 IntroductionA part-of-speech tagger typically assigns a tag toeach word in a sentence sequentially from left toright or in reverse order.
When the words aretagged from left to right, the part-of-speech tagsassigned to the previous words are available to thetagging of the current word, but not the tags of thefollowing words.
And when words are tagged fromright to left, only the tags of the words on the rightside are available to the tagging of the currentword.
We expect the use of the tags of the wordson both sides of the current word should improvethe tagging of the current word.
In this paper, wepresent a novel two-stage approach to using thetags of the words on both sides of the current wordin tagging the current word.
We train two maxi-mum entropy part-of-speech taggers on the sametraining data.
The difference between the two tag-gers is that the second tagger uses features involv-ing the tags of the words on both sides of the cur-rent word, while the first tagger uses the tags ofonly the previous words.
Both taggers assign tagsto words from left to right.
In tagging a new sen-tence, the first tagger is applied to the testing data,and then the second tagger is applied to the outputof the first tagger to produce the final results.We participated in the Chinese part-of-speechtagging task at the Fourth International ChineseLanguage Processing Bakeoff.
Our Chinese part-of-speech taggers were trained only on the trainingdata provided to the participants, and evaluated onfour corpora in the close track of the part-of-speechtagging task.
The words in both the training andtesting data sets are already segmented into words.2 Maximum Entropy POS TaggerMaximum entropy model is a machine learningalgorithm that has been applied to a range of natu-ral language processing tasks, including part-of-speech tagging (Ratnaparkhi, 1996).
Our Chinesepart-of-speech taggers are based on the maximumentropy model.2.1 Maximum Entropy ModelThe conditional maximum entropy model (Berger,et.
al., 1996) has the form)),(exp()|( )(1 yxfxypkkkxZ ?= ?where ?= y xypxZ )|()( is a normalization fac-tor, and k?
is a weight parameter associated withfeature ).,( yxfk  In the context of part-of-speechtagging, y is the POS tag assigned to a word, and xrepresents the contextual information regarding theword in consideration, such as the surroundingwords.
A feature is a real-valued, typically binary,function.
For example, we may define a binary fea-ture which takes the value 1 if the current word ofX is ?story?
and its POS tag is ?NNS?
; and 0 other-wise.
Given a set of training examples, the loglikelihood of the model with Gaussian prior (Chenand Rosenfeld, 1999) has the form82Sixth SIGHAN Workshop on Chinese Language ProcessingconstxypLkkiii +?= ??
22)()(2)|(log)( ??
?Malouf (2002) compared iterative procedures suchas Generalized Iterative Scaling (GIS) and Im-proved Iterative Scaling (IIS) with numerical opti-mization techniques like limited-memory BFGS(L-BFGS) for estimating the maximum entropymodel parameters and found that L-BFGS outper-forms the other methods.
The use of L-BFGS re-quires the computation of the gradient of the loglikelihood function.
The first derivative with re-spect to parameter k?
is given by2~ ),(),()(????
kkpkpkyxfEyxfEL ??=?
?where the first term kp fE~  is the feature expecta-tion with the empirical model, and the second termkp fE is the feature expectation with respect to themodel.
In our model training, we used L-BFGS toestimate the model parameters by maximizing)(?L on the training data.2.2 FeaturesThe feature templates used in our part-of-speechtaggers are presented in Table 1 and Table 2.Word2112 ,,,, ++??
iiiii wwwww1111,211112,,,,+?+?+++??
?iiiiiiiiiiiiiwwwwwwwwwwwwwTag121, ???
iii tttWord/Tagiiii wtwt 21 , ?
?Special FirstChar, LastChar, Length,ForeighWordTable 1: Feature templates used in the first stagePOS tagger.Tag11211 ,, +?+++ iiiii tttttWord/Tag21, ++ iiii twtwTable 2: Additional feature templates used in thesecond stage POS tagger.The features are grouped into four categories.
Thefirst category contains features involving word to-kens only; the second category consists of featuresinvolving tags only; the third category has featuresinvolving both word tokens and tags.
And the lastcategory has four special features.
In the featuretemplates, wi denotes the current word, wi-2 thesecond word to the left, wi-1 the previous word,wi+1 the next word, wi+2 the second word to theright of the current word, and ti denotes the part-of-speech tag assigned to the word wi.
The FirstCharrefers to the initial character of a word, and theLastChar the final character of a word.
The Lengthdenotes the length of a word in terms of byte.
Andthe feature ForeignWord indicates whether or not aword is a foreign word.
Table 2 shows additionalfeature templates involving the part-of-speech tagsof the following one or two words.
The featuresinvolving the tags of the words in the right con-texts are used only in the second maximum entropyPOS tagger.
Features are generated from the train-ing data according to the feature templates pre-sented in Table 1 and Table 2.2.3 Training ModelsThe four training corpora we received for the Chi-nese part-of-speech tagging task include the Aca-demia Sinica corpus (CKIP), the City Universityof Hong Kong corpus (CityU), the National Chi-nese Corpus (NCC), and the Peking Universitycorpus (PKU).
The CKIP corpus and the CityUcorpus contain texts in traditional Chinese, whilethe NCC corpus and the PKU corpus contain textsin simplified Chinese.
The texts in all four trainingcorpora are segmented into words according todifferent word segmentation guidelines.
And thewords in all training corpora are labeled with part-of-speech tags using different tag sets.Two maximum entropy POS taggers weretrained on each of the four corpora using our ownimplementation of the maximum entropy model.The first-stage POS tagger was trained with onlythe feature templates presented in Table 1, whilethe second-stage POS tagger with the feature tem-plates presented in both Table 1 and Table 2.All the first-stage POS taggers, one for eachcorpus, were trained with the same feature tem-plates shown in Table 1, and all the second-stagePOS taggers were trained with the same featuretemplates shown in Table 1 and Table 2.
The fea-ture templates are not necessarily optimal for eachindividual corpus.
For simplicity, we chose to ap-ply the same feature templates to all four corpora.83Sixth SIGHAN Workshop on Chinese Language ProcessingThe same parameter settings were applied in thetraining of all eight POS taggers.
More specifi-cally, no feature selection was performed.
All fea-tures, including features occurring just once in thetraining data, were retained.
The sigma square2?
was set to 5.0.
And the training process wasterminated when the ratio of the likelihood differ-ence between the current iteration and the previousiteration over the likelihood of the current iterationis below the pre-defined threshold or the maximumnumber of iterations, which was set to 400, isreached.
Both the first-stage POS tagger and thesecond-stage POS tagger were trained on the samecorpus.2.4 Testing the ModelsThe POS tagger assigns a part-of-speech tag toeach word in a new sentence such that the tag se-quence maximizes the probability p(Y|X), where Xis the input sentence, and Y the POS tags assignedto X.
The decoder implements the beam searchprocedure described in (Ratnaparkhi, 1996).
Ateach word position, the decoder keeps the top nbest tag sequences up to that position.
The decoderalso uses a word/tag dictionary, consisting of thewords in the training data and the tags assigned toeach word in the training data.
During the decod-ing phase, if a word in the new sentence is found inthe training data, only the tags that are assigned tothat word in the training corpus are considered.Otherwise, all the tags in the tag set are consideredfor a new word.
So the tagger will not assign to aword, found in the training data, a tag that is neverassigned to that word in the training data, even ifthat word should be assigned a new tag that wasnever assigned to the word in the training data.
Aword/tag dictionary is automatically built by col-lecting all the words in the training corpus and thetags assigned to every word in the training corpus.The final output is produced in two steps.
Thefirst-stage POS tagger is applied on the testing data,and then the second-stage POS tagger is applied onthe output of the first POS tagger.
The second-stage tagger uses features involving POS tags ofthe following one or two words.
The features in-volving the tags of following one or two wordsmay be erroneous, since the tags assigned to thefollowing one or two words by the first-stage tag-ger may be incorrect.3 Evaluation ResultsFive corpora are provided for the Chinese part-of-speech tagging task at the forth SIGHAN bakeoff.We selected four corpora, two in simplified Chi-nese and two in traditional Chinese.Corpus Training size(tokens)TagsetsizeNo.
of tagsper tokentypeCityU 1,092,687 44 1.2587CKIP 721,551 60 1.1086NCC 535,023 60 1.0658PKU 1,116,754 103 1.1194Table 3:  Training corpus size.Table 3 shows the training corpus size, the tagsetsize, and the average number of tags per token type.The NCC tagset has 60 tags, but nine of the tagsoccurred only once in the training corpus.
In allfour corpora, most of the unique tokens have onlya single tag.
The percentage of token types havingsingle tag is 83.29% in CityU corpus; 91.09 inCKIP corpus; 94.67 in NCC corpus; and 90.27% inPKU corpus.
The proportion of token types havingsingle tag in CityU corpus is much lower than inNCC corpus.
In the NCC corpus, the organizationnames, location names, and a sequence of Englishwords are all treated as single token, and these longsingle tokens are not ambiguous and are assignedto a single part-of-speech tag in the corpus.corpus Baseline  TestingsizeToken/tag OOV-RCityU 0.8433 184,314 0.0921CKIP 0.8865 91,071 0.0897NCC 0.9159 102,344 0.0527PKU 0.8805 156,407 0.0594Table 4: The testing data size and the baseline per-formance.The baseline performance is computed by assign-ing the most likely tag to each word in the testingdata.
When a word in the testing data is found inthe training corpus, it is assigned the tag that ismost frequently assigned to that word in the train-ing corpus.
A new word in the testing data is as-signed the most frequent tag found in the trainingcorpus, which is the common noun in all four cor-pora.
The baseline performances of the four testing84Sixth SIGHAN Workshop on Chinese Language Processingdata sets are presented in Table 4, which alsoshows the percentage of new token/tag in the test-ing data sets.Our POS taggers are evaluated on four testingdata sets, one corresponding to each training cor-pus.
We trained eight POS taggers, two on eachtraining corpus, and submitted eight runs in totalon the Chinese part-of-speech tagging task, tworuns on each testing data set.
The first run, labeled?a?
in Table 5, is produced using the first-stagetagger, and the second run, labeled ?b?
in Table 5,is the output of the second-stage tagger, which isapplied to the output of the first tagger.
For all ofour runs, only the provided training data are used.Table 5 shows the official evaluation results of theeight runs we submitted in the close track.
Thethird column, labeled ?Total-A?, shows the accu-racy of the eight runs.
The accuracy is the propor-tion of correctly tagged words in a testing data set.Only one tag is assigned to every word in the test-ing data set.
The remaining three labels, ?IV-R?,?OOV-R?, and ?MT-R?, may be defined in TheFourth SIGHAN Bakeoff overview paper.Corpus RunIDTotal-AIV-R OOV-RMT-RCityU a 0.8929 0.9367 0.4608 0.8705CityU b 0.8951 0.9389 0.4637 0.8745CKIP a 0.9286 0.9618 0.5875 0.9099CKIP b 0.9295 0.9629 0.5869 0.9123NCC a 0.9525 0.9717 0.6059 0.9135NCC b 0.9541 0.9738 0.5998 0.9195PKU a 0.9420 0.9648 0.5813 0.9148PKU b 0.9450 0.9679 0.5818 0.9252Table 5: Official evaluation results of eight runs inthe close track of the Chinese part-of-speech tag-ging task.4 DiscussionsA Chinese verb can function as a noun, and viceversa, without suffix change.
In PKU corpus, averb is labeled with the tag ?v?, and a verb thatfunctions as a noun is labeled with the tag ?vn?.
Inthe PKU-b run, almost half of the incorrectlytagged verbs (v) were tagged as verbal noun (vn),and slightly more than half of the incorrectlytagged verbal nouns (vn) were tagged as verb (v).The accuracy of our best runs on all four corporais much higher than the baseline performance.
Onthe PKU corpus, the accuracy is increased from thebaseline performance of 0.8805 to 0.9450, an im-provement of 7.33% over the baseline.
The sec-ond-stage tagging increased the accuracy on allfour corpora.
On the PKU corpus, the accuracy isincreased by about 0.32% over the first-stage tag-ging.
The improvement may not seem to be large;however, it corresponds to an error reduction by5.4%.That the accuracy on the CityU corpus is thelowest among all four corpora is not surprising,given that the CityU testing data set has the highestout-of-vocabulary rate, and the CityU training cor-pus has the highest average number of tags as-signed to each token type.
Furthermore, the CityUtraining corpus has the lowest percentage of tokenswith only one tag.
The POS tagging task on CityUcorpus seems to be most challenging among thefour corpora.5 ConclusionsWe have described a Chinese part-of-speech taggerwith maximum entropy modeling.
The tagger withrich lexical and morphological features signifi-cantly outperforms the baseline system which as-signs to a word the most likely tag assigned to thatword in the training corpus.
The use of featuresinvolving the part-of-speech tags of the followingwords further improves the performance of thetagger.ReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
Computational Lin-guistics, 22(1):39-71.Stanley F. Chen, and Ronald Rosenfeld.
1999.
A Gaus-sion Prior for Smoothing Maximum Entropy Models,Technical Report CMU-CS-99-108, Carnegie MellonUniversity.Rober Malouf.
2002.
A Comparison of Algorithms forMaximum Entropy Parameter Estimation, Proceed-ings of CoNLL-2002.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-of-Speech Tagging, Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pp 133-142.85Sixth SIGHAN Workshop on Chinese Language Processing
