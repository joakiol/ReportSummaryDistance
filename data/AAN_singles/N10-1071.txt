Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474?482,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning Dense Models of Query Similarity from User Click LogsFabio De Bona?Friedrich Miescher Laboratoryof the Max Planck SocietyTu?bingen, Germanyfabio@tuebingen.mpg.deStefan RiezlerGoogle ResearchZu?rich, Switzerlandriezler@google.comKeith HallGoogle ResearchZu?rich, Switzerlandkbhall@google.comMassimiliano CiaramitaGoogle ResearchZu?rich, Switzerlandmassi@google.comAmac?
Herdag?delen?University of TrentoRovereto, Italyamac@herdagdelen.comMaria Holmqvist?Linkopings UniversityLinkopings, Swedenmarho@ida.liu.seAbstractThe goal of this work is to integrate querysimilarity metrics as features into a densemodel that can be trained on large amountsof query log data, in order to rank queryrewrites.
We propose features that incorpo-rate various notions of syntactic and semanticsimilarity in a generalized edit distance frame-work.
We use the implicit feedback of userclicks on search results as weak labels in train-ing linear ranking models on large data sets.We optimize different ranking objectives in astochastic gradient descent framework.
Ourexperiments show that a pairwise SVM rankertrained on multipartite rank levels outperformsother pairwise and listwise ranking methodsunder a variety of evaluation metrics.1 IntroductionMeasures of query similarity are used for a widerange of web search applications, including queryexpansion, query suggestions, or listings of relatedqueries.
Several recent approaches deploy userquery logs to learn query similarities.
One set of ap-proaches focuses on user reformulations of queriesthat differ only in one phrase, e.g., Jones et al(2006).
Such phrases are then identified as candi-date expansion terms, and filtered by various signalssuch as co-occurrence in similar sessions, or log-likelihood ratio of original and expansion phrase.Other approaches focus on the relation of queriesand search results, either by clustering queries based?The work presented in this paper was done while the au-thors were visiting Google Research, Zu?rich.on their search results, e.g., Beeferman and Berger(2000), or by deploying the graph of queries and re-sults to find related queries, e.g., Sahami and Heil-man (2006).The approach closest to ours is that of Jones et al(2006).
Similar to their approach, we create a train-ing set of candidate query rewrites from user querylogs, and use it to train learners.
While the datasetused in Jones et al (2006) is in the order of a fewthousand query-rewrite pairs, our dataset comprisesaround 1 billion query-rewrite pairs.
Clearly, man-ual labeling of rewrite quality is not feasible for ourdataset, and perhaps not even desirable.
Instead, ourintent is to learn from large amounts of user querylog data.
Such data permit to learn smooth mod-els because of the effectiveness of large data sets tocapture even rare aspects of language, and they alsoare available as in the wild, i.e., they reflect the ac-tual input-output behaviour that we seek to automate(Halevy et al, 2009).
We propose a technique to au-tomatically create weak labels from co-click infor-mation in user query logs of search engines.
Thecentral idea is that two queries are related if theylead to user clicks on the same documents for a largeamount of documents.
A manual evaluation of asmall subset showed that a determination of positiveversus negative rewrites by thresholding the numberof co-clicks correlates well with human judgementsof similarity, thus justifying our method of elicitinglabels from co-clicks.Similar to Jones et al (2006), the features of ourmodels are not based on word identities, but insteadon general string similarity metrics.
This leads todense rather than sparse feature spaces.
The dif-474ference of our approach to Jones et al (2006) liesin our particular choice of string similarity metrics.While Jones et al (2006) deploy ?syntactic?
fea-tures such as Levenshtein distance, and ?semantic?features such as log-likelihood ratio or mutual in-formation, we combine syntactic and semantic as-pects into generalized edit-distance features wherethe cost of each edit operation is weighted by vari-ous term probability models.Lastly, the learners used in our approach are appli-cable to very large datasets by an integration of lin-ear ranking models into a stochastic gradient descentframework for optimization.
We compare severallinear ranking models, including a log-linear prob-ability model for bipartite ranking, and pairwise andlistwise SVM rankers.
We show in an experimen-tal evaluation that a pairwise SVM ranker trained onmultipartite rank levels outperforms state-of-the-artpairwise and listwise ranking methods under a vari-ety of evaluation metrics.2 Query Similarity Measures2.1 Semantic measuresIn several of the similarity measures we describe be-low, we employ pointwise mutual information (PMI)as a measure of the association between two terms orqueries.
Let wi and wj be two strings that we wantto measure the amount of association between.
Letp(wi) and p(wj) be the probability of observing wiand wj in a given model; e.g., relative frequenciesestimated from occurrence counts in a corpus.
Wealso define p(wi, wj) as the joint probability of wiand wj ; i.e., the probability of the two strings occur-ring together.
We define PMI as follows:PMI(wi, wj) = logp(wi, wj)p(wi)p(wj).
(1)PMI has been introduced by Church and Hanks(1990) as word assosiatio ratio, and since thenbeen used extensively to model semantic similar-ity.
Among several desirable properties, it correlateswell with human judgments (Recchia and Jones,2009).2.2 Taxonomic normalizationsAs pointed out in earlier work, query transitions tendto correlate with taxonomic relations such as gener-alization and specialization (Lau and Horvitz, 1999;Rieh and Xie, 2006).
Boldi et al (2009) show howknowledge of transition types can positively impactquery reformulation.
We would like to exploit thisinformation as well.
However, rather than building adedicated supervised classifier for this task we try tocapture it directly at the source.
First, we notice howstring features; e.g., length, and edit distance alreadymodel this phenomenon to some extent, and in factare part of the features used in Boldi et al (2009).However, these measures are not always accurateand it is easy to find counterexamples both at theterm level (e.g., ?camping?
to ?outdoor activities?
isa generalization) and character level (?animal pic-tures?
to ?cat pictures?
is a specialization).
Sec-ondly, we propose that by manipulating PMI we candirectly model taxonomic relations to some extent.Rather than using raw PMI values we re-normalize them.
Notice that it is not obvious in ourcontext how to interpret the relation between stringsco-occurring less frequently than random.
Suchnoisy events will yield negative PMI values sincep(wi, wj) < p(wi)p(wj).
We enforce zero PMI val-ues for such cases.
If PMI is thus constrained tonon-negative values, normalization will bound PMIto the range between 0 and 1.The first type of normalization, called joint nor-malization, uses the negative log joint probabilityand is defined asPMI(J)(wi, wj) = PMI(wi, wj)/?log(p(wi, wj)).The jointly normalized PMI(J) is a symmetricmeasure between wi and wj in the sense thatPMI(J)(wi, wj) = PMI(J)(wj , wi).
Intuitively itis a measure of the amount of shared informationbetween the two strings relative to the sum of indi-vidual strings information.
The advantages of thejoint normalization of PMI have been noticed be-fore (Bouma, 2009).To capture asymmetries in the relation betweentwo strings, we introduce two non-symmetric nor-malizations which also bound the measure between0 and 1.
The second normalization is called special-ization normalization and is defined asPMI(S)(wi, wj) = PMI(wi, wj)/?
log(p(wi)).The reason we call it specialization is that PMI(S)favors pairs where the second string is a specializa-475tion of the first one.
For instance, PMI(S) is at itsmaximum when p(wi, wj) = p(wj) and that meansthe conditional probability p(wi|wj) is 1 which is anindication of a specialization relation.The last normalization is called the generalizationnormalization and is defined in the reverse directionasPMI(G)(wi, wj) = PMI(wi, wj)/?
log(p(wj)).Again, PMI(G) is a measure between 0 and 1 and isat its maximum value when p(wj |wi) is 1.The three normalizations provide a richer rep-resentation of the association between two strings.Furthermore, jointly, they model in an information-theoretic sense the generalization-specialization di-mension directly.
As an example, for the querytransition ?apple?
to ?mac os?
PMI(G)=0.2917 andPMI(S)=0.3686; i.e., there is more evidence for aspecialization.
Conversely for the query transition?ferrari models?
to ?ferrari?
we get PMI(G)=1 andPMI(S)=0.5558; i.e., the target is a ?perfect?
gener-alization of the source1.2.3 Syntactic measuresLet V be a finite vocabulary and ?
be the nullsymbol.
An edit operation: insertion, deletion orsubstitution, is a pair (a, b) ?
{V ?
{?}
?
V ?{?}}
\ {(?, ?)}.
An alignment between two se-quences wi and wj is a sequence of edit oper-ations ?
= (a1, b1), ..., (an, bn).
Given a non-negative cost function c, the cost of an alignment isc(?)
=?ni=1 c(?i).
The Levenshtein distance, oredit distance, defined over V , dV (wi, wj) betweentwo sequences is the cost of the least expensive se-quence of edit operations which transforms wi intowj (Levenshtein, 1966).
The distance computationcan be performed via dynamic programming in timeO(|wi||wj |).
Similarity at the string, i.e., characteror term, level is an indicator of semantic similar-ity.
Edit distance captures the amount of overlap be-tween the queries as sequences of symbols and hasbeen previously used in information retrieval (Boldiet al, 2009; Jones et al, 2006).We use two basic Levenshtein distance models.The first, called Edit1 (E1), employs a unit cost func-tion for each of the three operations.
That is, given1The values are computed from Web counts.a finite vocabulary T containing all terms occurringin queries:?a, b ?
T, cE1(a, b) = 1 if(a 6= b), 0 else.The second, called Edit2 (E2), uses unit costs forinsertion and deletion, but computes the character-based edit distance between two terms to decide onthe substitution cost.
If two terms are very similarat the character level, then the cost of substitution islower.
Given a finite vocabulary T of terms and afinite vocabulary A of characters, the cost functionis defined as:?a, b ?
T, cE2(a, b) = dA(a, b) ifa ?
b 6= ?, 1 else.where dA(a, b) is linearly scaled between 0 and 1dividing by max(|a|, |b|).We also investigate a variant of the edit distancealgorithm in which the terms in the input sequencesare sorted, alphabetically, before the distance com-putation.
The motivation behind this variant is theobservation that linear order in queries is not alwaysmeaningful.
For example, it seems reasonable to as-sume that ?brooklyn pizza?
and ?pizza brooklyn?denote roughly the same user intent.
However, thepair has an edit distance of two (delete-insert), whilethe distance between ?brooklyn pizza?
and the lessrelevant ?brooklyn college?
is only one (substitute).The sorted variant relaxes the ordering constraint.2.4 Generalized measuresIn this section we extend the edit distance frame-work introduced in Section 2.3 with the semanticsimilarity measures described in Section 2.1, usingthe taxonomic normalizations defined in Section 2.2.Extending the Levenshtein distance frameworkto take into account semantic similarities betweenterms is conceptually simple.
As in the Edit2 modelabove we use a modified cost function.
We introducea cost matrix encoding individual costs for term sub-stitution operations; the cost is defined in terms ofthe normalized PMI measures of Section 2.2, recallthat these measures range between 0 and 1.
Given anormalized similarity measure f , an entry in a costmatrix S for a term pair (wi, wj) is defined as:s(wi, wj) = 2?
2f(wi, wj) + 476We call these models SEdit (SE), where S specifiesthe cost matrix used.
Given a finite term vocabularyT and cost matrix S, the cost function is defined as:?a, b ?
T, cSE(a, b) = s(a, b) ifa ?
b 6= ?, 1 else.The cost function has the following properties.Since insertion and deletion have unit cost, a termis substituted only if a substitution is ?cheaper?
thandeleting and inserting another term, namely, if thesimilarity between the terms is not zero.
The correction, coupled with unit insertion and deletioncost, guarantees that for an unrelated term pair acombination of insertion and deletion will always beless costly then a substitution.
Thus in the compu-tation of the optimal alignment, each operation costranges between 0 and 2.As a remark on efficiency, we notice that here thesemantic similarities are computed between terms,rather than full queries.
At the term level, cachingtechniques can be applied more effectively to speedup feature computation.
The cost function is imple-mented as a pre-calculated matrix, in the next sec-tion we describe how the matrix is estimated.2.5 Cost matrix estimationIn our experiments we evaluated two differentsources to obtain the PMI-based cost matrices.
Inboth cases, we assumed that the cost of the substitu-tion of a term with itself (i.e.
identity substitution)is always 0.
The first technique uses a probabilis-tic clustering model trained on queries and clickeddocuments from user query logs.
The second modelestimates cost matrices directly from user sessionlogs, consisting of approximately 1.3 billion U.S.English queries.
A session is defined as a sequenceof queries from the same user within a controlledtime interval.
Let qs and qt be a query pair observedin the session data where qt is issued immediatelyafter qs in the same session.
Let q?s = qs \ qt andq?t = qt \ qs, where \ is the set difference opera-tor.
The co-occurrence count of two terms wi andwj from a query pair qs, qt is denoted by ni,j(qs, qt)and is defined as:ni,j(qs, qt) =??
?1 if wi = wj ?
wi ?
qs ?
wj ?
qt1/(|q?s| |q?t|) if wi ?
q?s ?
wj ?
q?t0 else.In other words, if a term occurs in both queries,it has a co-occurrence count of 1.
For all other termpairs, a normalized co-occurrence count is computedin order to make sure the sum of co-occurrencecounts for a term wi ?
qs sums to 1 for a givenquery pair.
The normalization is an attempt to avoidthe under representation of terms occurring in bothqueries.The final co-occurrence count of two arbitraryterms wi and wj is denoted by Ni,j and it is definedas the sum over all query pairs in the session logs,Ni,j =?qs,qt ni,j(qs, qt).
Let N =?wi,wjNi,j bethe sum of co-occurrence counts over all term pairs.Then we define a joint probability for a term pair asp(wi, wj) =Ni,jN .
Similarly, we define the single-occurrence counts and probabilities of the termsby computing the marginalized sums over all termpairs.
Namely, the probability of a termwi occurringin the source query is p(i, ?)
=?wjNi,j/N andsimilarly the probability of a term wj occurring inthe target query is p(?, j) =?wiNi,j/N .
Pluggingin these values in Eq.
(1), we get the PMI(wi, wj)for term pair wi and wj , which are further normal-ized as described in Section 2.2.More explanation and evaluation of the featuresdescribed in this section can be found in Ciaramitaet al (2010).3 Learning to Rank from Co-Click Data3.1 Extracting Weak Labels from Co-ClicksSeveral studies have shown that implicit feedbackfrom clickstream data is a weaker signal than humanrelevance judgements.
Joachims (2002) or Agrawalet al (2009) presented techniques to convert clicksinto labels that can be used for machine learning.Our goal is not to elicit relevance judgments fromuser clicks, but rather to relate queries by pivoting oncommonly clicked search results.
The hypothesis isthat two queries are related if they lead to user clickson the same documents for a large amount of docu-ments.
This approach is similar to the method pro-posed by Fitzpatrick and Dent (1997) who attemptto measure the relatedness between two queries byusing the normalized intersection of the top 200 re-trieval results.
We add click information to thissetup, thus strengthening the preference for preci-sion over recall in the extraction of related queries.477Table 1: Statistics of co-click data sets.train dev testnumber of queries 250,000 2,500 100average number ofrewrites per query 4,500 4,500 30percentage of rewriteswith ?
10 coclicks 0.2 0.2 43In our experiments we created two ground-truthranking scenarios from the co-click signals.
In a firstscenario, called bipartite ranking, we extract a setof positive and a set of negative query-rewrite pairsfrom the user logs data.
We define positive pairs asqueries that have been co-clicked with at least 10 dif-ferent results, and negative pairs as query pairs withfewer than 10 co-clicks.
In a second scenario, calledmultipartite ranking, we define a hierarchy of levelsof ?goodness?, by combining rewrites with the samenumber of co-clicks at the same level, with increas-ing ranks for higher number of co-clicks.
Statisticson the co-click data prepared for our experiments aregiven in Table 1.For training and development, we collectedquery-rewrite pairs from user query logs that con-tained at least one positive rewrite.
The training setconsists of about 1 billion of query-rewrite pairs; thedevelopment set contains 10 million query-rewritepairs.
The average number of rewrites per query isaround 4,500 for the training and development set,with a very small amount of 0.2% positive rewritesper query.
In order to confirm the validity of our co-click hypothesis, and for final evaluation, we heldout another sample of query-rewrite pairs for man-ual evaluation.
This dataset contains 100 queries foreach of which we sampled 30 rewrites in descendingorder of co-clicks, resulting in a high percentage of43% positive rewrites per query.
The query-rewritepairs were annotated by 3 raters as follows: First theraters were asked to rank the rewrites in descend-ing order of relevance using a graphical user inter-face.
Second the raters assigned rank labels and bi-nary relevance scores to the ranked list of rewrites.This labeling strategy is similar to the labeling strat-egy for synonymy judgements proposed by Ruben-stein and Goodenough (1965).
Inter-rater agree-ments on binary relevance judgements, and agree-ment between rounded averaged human relevancescores and assignments of positive/negative labelsby the co-click threshold of 10 produced a Kappavalue of 0.65 (Siegel and Castellan, 1988).3.2 Learning-to-Rank Query Rewrites3.2.1 NotationLet S = {(xq, yq)}nq=1 be a training sampleof queries, each represented by a set of rewritesxq = {xq1, .
.
.
, xq,n(q)}, and set of rank labelsyq = {yq1, .
.
.
, yq,n(q)}, where n(q) is the num-ber of rewrites for query q.
For full rankings ofall rewrites for a query, a total order on rewrites isassumed, with rank labels taking on values yqi ?
{1, .
.
.
, n(q)}.
Rewrites of equivalent rank can bespecified by assuming a partial order on rewrites,where a multipartite ranking involves r < n(q) rele-vance levels such that yqi ?
{1, .
.
.
, r} , and a bipar-tite ranking involves two rank values yqi ?
{1, 2}with relevant rewrites at rank 1 and non-relevantrewrites at rank 2.Let the rewrites in xq be identified by the integers{1, 2, .
.
.
, n(q)}, and let a permutation piq on xq bedefined as a bijection from {1, 2, .
.
.
, n(q)} onto it-self.
Let ?q denote the set of all possible permuta-tions on xq, and let piqi denote the rank position ofxqi.
Furthermore, let (i, j) denote a pair of rewritesin xq and let Pq be the set of all pairs in xq.We associate a feature function ?
(xqi) with eachrewrite i = 1, .
.
.
, n(q) for each query q. Further-more, a partial-order feature map as used in Yue etal.
(2007) is created for each rewrite set as follows:?
(xq, piq) =1|Pq|?(i,j)?Pq?(xqi)??
(xqj)sgn(1piqi?1piqj).The goal of learning a ranking over the rewritesxq for a query q can be achieved either by sorting therewrites according to the rewrite-level ranking func-tion f(xqi) = ?w, ?
(xqi)?, or by finding the permu-tation that scores highest according to a query-levelranking function f(xq, piq) = ?w, ?
(xq, piq)?.In the following, we will describe a varietyof well-known ranking objectives, and extensionsthereof, that are used in our experiments.
Optimiza-tion is done in a stochastic gradient descent (SGD)framework.
We minimize an empirical loss objec-tiveminw?xq ,yq`(w)478by stochastic updatingwt+1 = wt ?
?tgtwhere ?t is a learning rate, and gt is the gradientgt = ?`(w)where?`(w) =???w1`(w),?
?w2`(w), .
.
.
,?
?wn`(w)?.3.2.2 Listwise Hinge LossStandard ranking evaluation metrics such as(Mean) Average Precision (Manning et al, 2008)are defined on permutations of whole lists and arenot decomposable over instances.
Joachims (2005),Yue et al (2007), or Chakrabarti et al (2008) haveproposed multivariate SVM models to optimize suchlistwise evaluation metrics.
The central idea is toformalize the evaluation metric as a prediction lossfunction L, and incorporate L via margin rescal-ing into the hinge loss function, such that an up-per bound on the prediction loss is achieved (seeTsochantaridis et al (2004), Proposition 2).The loss function is given by the following list-wise hinge loss:`lh(w) = (L(yq, pi?q )?
?w, ?
(xq, yq)?
?
(xq, pi?q )?
)+where pi?q is the maximizer of themaxpiq?
?q\yq L(yq, pi?q ) +?w, ?
(xq, pi?q )?ex-pression, (z)+ = max{0, z} and L(yq, piq) ?
[0, 1]denotes a prediction loss of a predicted ranking piqcompared to the ground-truth ranking yq.2In this paper, we use Average Precision (AP) asprediction loss function s.t.LAP (yq, piq) = 1?AP (yq, piq)where AP is defined as follows:AP (yq, piq) =?n(q)j=1 Prec(j) ?
(|yqj ?
2|)?n(q)j=1 (|yqj ?
2|),P rec(j) =?k:piqk?piqj(|yqk ?
2|)piqj.2We slightly abuse the notation yq to denote the permutationon xq that is induced by the rank labels.
In case of full rankings,the permutation piq corresponding to ranking yq is unique.
Formultipartite and bipartite rankings, there is more than one pos-sible permutation for a given ranking, so that we let piq denotea permutation that is consistent with ranking yq .Note that the ranking scenario is in this case bipartitewith yqi ?
{1, 2}.The derivatives for `lh are as follows:?
?wk`lh =??
?0 if(?w, ?
(xq, yq)?
?
(xq, pi?q )?
)> L(yq, pi?q ),?
(?k(xq, yq)?
?k(xq, pi?q )) else.SGD optimization involves computing pi?q for eachfeature and each query, which can be done effi-ciently using the greedy algorithm proposed by Yueet al (2007).
We will refer to this method as theSVM-MAP model.3.2.3 Pairwise Hinge Loss for Bipartite andMultipartite RankingJoachims (2002) proposed an SVM method thatdefines the ranking problem as a pairwise classifi-cation problem.
Cortes et al (2007) extended thismethod to a magnitude-preserving version by penal-izing a pairwise misranking by the magnitude of thedifference in preference labels.
A position-sensitivepenalty for pairwise ranking SVMs was proposedby Riezler and De Bona (2009) and Chapelle andKeerthi (2010), and earlier for perceptrons by Shenand Joshi (2005).
In the latter approaches, the mag-nitude of the difference in inverted ranks is accruedfor each misranked pair.
The idea is to impose anincreased penalty for misrankings at the top of thelist, and for misrankings that involve a difference ofseveral rank levels.Similar to the listwise case, we can view thepenalty as a prediction loss function, and incor-porate it into the hinge loss function by rescalingthe margin by a pairwise prediction loss functionL(yqi, yqj).
In our experiments we used a position-sensitive prediction loss functionL(yqi, yqj) = |1yqi?1yqj|defined on the difference of inverted ranks.
Themargin-rescaled pairwise hinge loss is then definedas follows:`ph(w) =?
(i,j)?Pq(L(yqi, yqj)?
?w, ?(xqi)?
?(xqj)?
sgn(1yqi?1yqj))+479Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5Random 51.8 48.7 50.4 45.6 45.6 46.6Best-feature 71.9 70.2 74.5 70.2 68.1 68.7SVM-bipart.
73.7 73.7 74.7 79.4 70.1 70.1SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0Log-linear 74.7 75.1 75.7 75.3 72.2 71.3SVM-pos.-sens.
75.7 76.0 76.6 82.5 72.9 73.0SVM-multipart.
76.5 77.3 77.2 83.5 74.2 73.6The derivative of `ph is calculated as follows:?
?wk`lp =????????
?0 if (?w, ?(xqi)?
?
(xqj)?sgn( 1yqi ?1yqj)) > L(yqi, yqj),?(?k(xqi)?
?k(xqj))sgn( 1yqi ?1yqj)else.Note that the effect of inducing a position-sensitive penalty on pairwise misrankings appliesonly in case of full rankings on n(q) rank levels,or in case of multipartite rankings involving 2 <r < n(q) rank levels.
Henceforth we will refer tomargin-rescaled pairwise hinge loss for multipartiterankings as the SVM-pos.-sens.
method.Bipartite ranking is a special case whereL(yqi, yqj) is constant so that margin rescaling doesnot have the effect of inducing position-sensitivity.This method will be referred to as the SVM-bipartitemodel.Also note that for full ranking or multipartiteranking, predicting a low ranking for an instancethat is ranked high in the ground truth has a dominoeffect of accruing an additional penalty at eachrank level.
This effect is independent of margin-rescaling.
The method of pairwise hinge lossfor multipartite ranking with constant margin willhenceforth be referred to as the SVM-multipartitemodel.Computation in SGD optimization is dominatedby the number of pairwise comparisons |Pq| foreach query.
For full ranking, a comparison of|Pq| =(n(q)2)pairs has to be done.
In the caseof multipartite ranking at r rank levels, each in-cluding |li| rewrites, pairwise comparisons betweenrewrites at the same rank level can be ignored.This reduces the number of comparisons to |Pq| =?r?1i=1?rj=i+1 |li||lj |.
For bipartite ranking of ppositive and n negative instances, |Pq| = p ?
n com-parisons are necessary.3.2.4 Log-linear Models for Bipartite RankingA probabilistic model for bipartite ranking can bedefined as the conditional probability of the set ofrelevant rewrites, i.e., rewrites at rank level 1, givenall rewrites at rank levels 1 and 2.
A formalization inthe family of log-linear models yields the followinglogistic loss function `llm that was used for discrim-inative estimation from sets of partially labeled datain Riezler et al (2002):`llm(w) = ?
log?xqi?xq |yqi=1e?w,?(xqi)??xqi?xqe?w,?
(xqi)?.The gradient of `llm is calculated as a difference be-tween two expectations:?
?wk`llm = ?pw [?k|xq; yqi = 1] + pw [?k|xq] .The SGD computation for the log-linear model isdominated by the computation of expectations foreach query.
The logistic loss for bipartite ranking ishenceforth referred to as the log-linear model.4 Experimental ResultsIn the experiments reported in this paper, we trainedlinear ranking models on 1 billion query-rewritepairs using 60 dense features, combined of the build-ing blocks of syntactic and semantic similarity met-rics under different estimations of cost matrices.
De-velopment testing was done on a data set that washeld-out from the training set.
Final testing was car-ried out on the manually labeled dataset.
Data statis-tics for all sets are given in Table 1.480Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.Best-feature SVM-bipart.
SVM-MAP Log-linear SVM-pos.-sens.
SVM-multipart.Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005SVM-bipart.
- - 0.324 < 0.005 < 0.005 < 0.005SVM-MAP - - - 0.374 < 0.005 < 0.005Log-linear - - - - 0.053 < 0.005SVM-pos.-sens.
- - - - - < 0.005SVM-multipart.
- - - - - -Model selection was performed by adjustingmeta-parameters on the development set.
Wetrained each model at constant learning rates ?
?
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variantafter every fifth out of 100 passes over the trainingset.
The variant with the highest MAP score on thedevelopment set was chosen and evaluated on thetest set.
This early stopping routine also served forregularization.Evaluation results for the systems are reported inTable 2.
We evaluate all models according to the fol-lowing evaluation metrics: Mean Average Precision(MAP), Normalized Discounted Cumulative Gainwith a cutoff at rank 10 (NDCG@10), Area-under-the-ROC-curve (AUC), Precision@n3.
As baselineswe report a random permutation of rewrites (ran-dom), and the single dense feature that performedbest on the development set (best-feature).
The latteris the log-probability assigned to the query-rewritepair by the probabilistic clustering model used forcost matrix estimation (see Section 2.5).
P-valuesare reported in Table 3 for all pairwise compar-isons of systems (except the random baseline) us-ing an Approximate Randomization test where strat-ified shuffling is applied to results on the query level(see Noreen (1989)).
The rows in Tables 2 and 3are ranked according to MAP values of the systems.SVM-multipartite outperforms all other ranking sys-tems under all evaluation metrics at a significancelevel ?
0.995.
For all other pairwise comparisonsof result differences, we find result differences ofsystems ranked next to each other to be not statis-tically significant.
All systems outperform the ran-dom and best-feature baselines with statistically sig-nificant result differences.
The distinctive advantageof the SVM-multipartite models lies in the possibil-3For a definition of these metrics see Manning et al (2008)ity to rank rewrites with very high co-click num-bers even higher than rewrites with reasonable num-bers of co-clicks.
This preference for ranking thetop co-clicked rewrites high seems the best avenuefor transferring co-click information to the humanjudgements encoded in the manually labeled test set.Position-sensitive margin rescaling does not seem tohelp, but rather seems to hurt.5 DiscussionWe presented an approach to learn rankings of queryrewrites from large amounts of user query log data.We showed how to use the implicit co-click feed-back about rewrite quality in user log data to trainranking models that perform well on ranking queryrewrites according to human quality standards.
Wepresented large-scale experiments using SGD opti-mization for linear ranking models.
Our experimen-tal results show that an SVM model for multipartiteranking outperforms other linear ranking models un-der several evaluation metrics.
In future work, wewould like to extend our approach to other models,e.g., sparse combinations of lexicalized features.ReferencesR.
Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,and P. Tsaparas.
2009.
Generating labels from clicks.In Proceedings of the 2nd ACM International Con-ference on Web Search and Data Mining, Barcelona,Spain.Doug Beeferman and Adam Berger.
2000.
Agglom-erative clustering of a search engine query log.
InProceedings of the 6th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing (KDD?00), Boston, MA.P.
Boldi, F. Bonchi, C. Castillo, and S. Vigna.
2009.From ?Dango?
to ?Japanese cakes?
: Query reformula-481tion models and patterns.
In Proceedings of Web Intel-ligence.
IEEE Cs Press.G.
Bouma.
2009.
Normalized (pointwise) mutual in-formation in collocation extraction.
In Proceedings ofGSCL.Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, andChiru Bhattacharayya.
2008.
Structured learning fornon-smooth ranking losses.
In Proceedings of the 14thACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD?08), Las Vegas, NV.Olivier Chapelle and S. Sathiya Keerthi.
2010.
Efficientalgorithms for ranking with SVMs.
Information Re-trieval Journal.Kenneth Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 16(1):22?29.Massimiliano Ciaramita, Amac?
Herdag?delen, DanielMahler, Maria Holmqvist, Keith Hall, Stefan Riezler,and Enrique Alfonseca.
2010.
Generalized syntacticand semantic models of query reformulation.
In Pro-ceedings of the 33rd ACM SIGIR Conference, Geneva,Switzerland.Corinna Cortes, Mehryar Mohri, and Asish Rastogi.2007.
Magnitude-preserving ranking algorithms.
InProceedings of the 24th International Conference onMachine Learning (ICML?07), Corvallis, OR.Larry Fitzpatrick and Mei Dent.
1997.
Automatic feed-back using past queries: Social searching?
In Pro-ceedings of the 20th Annual International ACM SIGIRConference, Philadelphia, PA.Alon Halevy, Peter Norvig, and Fernando Pereira.
2009.The unreasonable effectiveness of data.
IEEE Intelli-gent Systems, 24:8?12.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the 8thACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD?08), New York, NY.Thorsten Joachims.
2005.
A support vector method formultivariate performance measures.
In Proceedings ofthe 22nd International Conference on Machine Learn-ing (ICML?05), Bonn, Germany.Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
InProceedings of the 15th International World Wide Webconference (WWW?06), Edinburgh, Scotland.T.
Lau and E. Horvitz.
1999.
Patterns of search: analyz-ing and modeling web query refinement.
In Proceed-ings of the seventh international conference on Usermodeling, pages 119?128.
Springer-Verlag New York,Inc.V.I.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
Soviet PhysicsDoklady, 10(8):707?710.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
An Introduction.
Wiley, NewYork.G.
Recchia and M.N.
Jones.
2009.
More data trumpssmarter algorithms: comparing pointwise mutual in-formation with latent semantic analysis.
BehavioralResearch Methods, 41(3):647?656.S.Y.
Rieh and H. Xie.
2006.
Analysis of multiple queryreformulations on the web: the interactive informationretrieval context.
Inf.
Process.
Manage., 42(3):751?768.Stefan Riezler and Fabio De Bona.
2009.
Simple riskbounds for position-sensitive max-margin ranking al-gorithms.
In Proceedings of the Workshop on Ad-vances in Ranking at the 23rd Annual Conferenceon Neural Information Processing Systems (NIPS?09),Whistler, Canada.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), Philadelphia, PA.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communicationsof the ACM, 10(3):627?633.Mehran Sahami and Timothy D. Heilman.
2006.
A web-based kernel function for measuring the similarity ofshort text snippets.
In Proceedings of the 15th Inter-national World Wide Web conference (WWW?06), Ed-inburgh, Scotland.Libin Shen and Aravind K. Joshi.
2005.
Ranking andreranking with perceptron.
Journal of Machine Learn-ing Research, 60(1-3):73?96.Sidney Siegel and John Castellan.
1988.
NonparametricStatistics for the Behavioral Sciences.
Second Edition.MacGraw-Hill, Boston, MA.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proceedings of the 21st InternationalConference on Machine Learning (ICML?04), Banff,Canada.Yisong Yue, Thomas Finley, Filip Radlinski, andThorsten Joachims.
2007.
A support vector methodfor optimizing average precision.
In Proceedings ofthe 30th Annual International ACM SIGIR Confer-ence, Amsterdam, The Netherlands.482
