Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsLIMSI @ WMT?12Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2Univ.
Paris-Sud1 and LIMSI-CNRS2rue John von Neumann, 91403 Orsay cedex, France{firstname.lastname}@limsi.frAbstractThis paper describes LIMSI?s submissions tothe shared translation task.
We report resultsfor French-English and German-English inboth directions.
Our submissions use n-code,an open source system based on bilingualn-grams.
In this approach, both the transla-tion and target language models are estimatedas conventional smoothed n-gram models; anapproach we extend here by estimating thetranslation probabilities in a continuous spaceusing neural networks.
Experimental resultsshow a significant and consistent BLEU im-provement of approximately 1 point for allconditions.
We also report preliminary experi-ments using an ?on-the-fly?
translation model.1 IntroductionThis paper describes LIMSI?s submissions to theshared translation task of the Seventh Workshopon Statistical Machine Translation.
LIMSI partic-ipated in the French-English and German-Englishtasks in both directions.
For this evaluation, weused n-code, an open source in-house StatisticalMachine Translation (SMT) system based on bilin-gual n-grams1.
The main novelty of this year?sparticipation is the use, in a large scale system, ofthe continuous space translation models describedin (Hai-Son et al, 2012).
These models estimate then-gram probabilities of bilingual translation unitsusing neural networks.
We also investigate an alter-native approach where the translation probabilitiesof a phrase based system are estimated ?on-the-fly?1http://ncode.limsi.fr/by sampling relevant examples, instead of consider-ing the entire training set.
Finally we also describethe use in a rescoring step of several additional fea-tures based on IBM1 models and word sense disam-biguation information.The rest of this paper is organized as follows.
Sec-tion 2 provides an overview of the baseline systemsbuilt with n-code, including the standard transla-tion model (TM).
The continuous space translationmodels are then described in Section 3.
As in ourprevious participations, several steps of data pre-processing, cleaning and filtering are applied, andtheir improvement took a non-negligible part of ourwork.
These steps are summarized in Section 5.The last two sections report experimental results ob-tained with the ?on-the-fly?
system in Section 6 andwith n-code in Section 7.2 System overviewn-code implements the bilingual n-gram approachto SMT (Casacuberta and Vidal, 2004; Marin?o et al,2006; Crego and Marin?o, 2006).
In this framework,translation is divided in two steps: a source reorder-ing step and a (monotonic) translation step.
Sourcereordering is based on a set of learned rewrite rulesthat non-deterministically reorder the input words.Applying these rules result in a finite-state graph ofpossible source reorderings, which is then searchedfor the best possible candidate translation.2.1 FeaturesGiven a source sentence s of I words, the best trans-lation hypothesis t?
is defined as the sequence of Jwords that maximizes a linear combination of fea-330ture functions:t?
= argmaxt,a{M?m=1?mhm(a, s, t)}(1)where ?m is the weight associated with feature func-tion hm and a denotes an alignment between sourceand target phrases.
Among the feature functions, thepeculiar form of the translation model constitute oneof the main difference between the n-gram approachand standard phrase-based systems.
This will be fur-ther detailled in section 2.2 and 3.In addition to the translation model, fourteenfeature functions are combined: a target-languagemodel (Section 5.3); four lexicon models; six lexi-calized reordering models (Tillmann, 2004; Cregoet al, 2011) aiming at predicting the orientation ofthe next translation unit; a ?weak?
distance-baseddistortion model; and finally a word-bonus modeland a tuple-bonus model which compensate for thesystem preference for short translations.
The fourlexicon models are similar to the ones used in stan-dard phrase-based systems: two scores correspondto the relative frequencies of the tuples and two lexi-cal weights are estimated from the automatic wordalignments.
The weights vector ?
is learned us-ing a discriminative training framework (Och, 2003)(Minimum Error Rate Training (MERT)) using thenewstest2009 as development set and BLEU (Pap-ineni et al, 2002) as the optimization criteria.2.2 Standard n-gram translation modelsn-gram translation models rely on a specific de-composition of the joint probability of a sentencepair P (s, t): a sentence pair is assumed to bedecomposed into a sequence of L bilingual unitscalled tuples defining a joint segmentation: (s, t) =u1, ..., uL2.
In the approach of (Marin?o et al, 2006),this segmentation is a by-product of source reorder-ing obtained by ?unfolding?
initial word alignments.In this framework, the basic translation units aretuples, which are the analogous of phrase pairs andrepresent a matching u = (s, t) between a sources and a target t phrase (see Figure 1).
Using then-gram assumption, the joint probability of a seg-2From now on, (s, t) thus denotes an aligned sentence pair,and we omit the alignment variable a in further developments.mented sentence pair decomposes as:P (s, t) =L?i=1P (ui|ui?1, ..., ui?n+1) (2)During the training phase (Marin?o et al, 2006), tu-ples are extracted from a word-aligned corpus (us-ing MGIZA++3 with default settings) in such away that a unique segmentation of the bilingualcorpus is achieved.
A baseline n-gram translationmodel is then estimated over a training corpus com-posed of tuple sequences using modified Knesser-Ney Smoothing (Chen and Goodman, 1998).2.3 InferenceDuring decoding, source sentences are representedin the form of word lattices containing the mostpromising reordering hypotheses, so as to reproducethe word order modifications introduced during thetuple extraction process.
Hence, only those reorder-ing hypotheses are translated and they are intro-duced using a set of reordering rules automaticallylearned from the word alignments.In the example in Figure 1, the rule [prix no-bel de la paix ; nobel de la paix prix] repro-duces the invertion of the French words that is ob-served when translating from French into English.Typically, part-of-speech (POS) information is usedto increase the generalization power of these rules.Hence, rewrite rules are built using POS rather thansurface word forms (Crego and Marin?o, 2006).3 SOUL translation modelsA first issue with the model described by equa-tion (2) is that the elementary units are bilingualpairs.
As a consequence, the underlying vocabulary,hence the number of parameters, can be quite large,even for small translation tasks.
Due to data sparsityissues, such model are bound to face severe estima-tion problems.
Another problem with (2) is that thesource and target sides play symmetric roles: yet,in decoding, the source side is known and only thetarget side must be predicted.3.1 A word factored translation modelTo overcome these issues, the n-gram probability inequation (2) can be factored by decomposing tuples3http://www.kyloo.net/software/doku.php331s?8: ?t?8: tos?9: recevoirt?9: receives?10: let?10: thes?11: nobel de la paixt?11: nobel peaces?12: prixt?12: prizeu8u9u10u11u12S :   ....T :   ....?
recevoir le prix nobel de la paixorg :   ............Figure 1: Extract of a French-English sentence pair segmented into bilingual units.
The original (org) French sentenceappears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into asequence of L bilingual units (tuples) u1, ..., uL.
Each tuple ui contains a source and a target phrase: si and ti.in two parts (source and target), and by taking wordsas the basic units of the n-gram TM.
This may seemto be a regression with respect to current state-of-the-art SMT systems, as the shift from the word-based model of (Brown et al, 1993) to the phrase-based models of (Zens et al, 2002) is usually con-sidered as a major breakthrough of the recent years.Indeed, one important motivation for consideringphrases was to capture local context in translationand reordering.
It should however be emphasizedthat the decomposition of phrases into words is onlyre-introduced here as a way to mitigate the param-eter estimation problems.
Translation units are stillpairs of phrases, derived from a bilingual segmen-tation in tuples synchronizing the source and targetn-gram streams.
In fact, the estimation policy de-scribed in section 4 will actually allow us to take intoaccount larger contexts than is possible with conven-tional n-gram models.Let ski denote the kth word of source tuple si.Considering the example of Figure 1, s111 denotesthe source word nobel, s411 the source word paix.We finally denote hn?1(tki ) the sequence made ofthe n?
1 words preceding tki in the target sentence:in Figure 1, h3(t211) thus refers to the three wordscontext receive the nobel associated with t211 peace.Using these notations, equation (2) is rewritten as:P (a, s, t) =L?i=1[ |ti|?k=1P(tki |hn?1(tki ), hn?1(s1i+1))?|si|?k=1P(ski |hn?1(t1i ), hn?1(ski ))] (3)This decomposition relies on the n-gram assump-tion, this time at the word level.
Therefore, thismodel estimates the joint probability of a sentencepair using two sliding windows of length n, one foreach language; however, the moves of these win-dows remain synchronized by the tuple segmenta-tion.
Moreover, the context is not limited to the cur-rent phrase, and continues to include words from ad-jacent phrases.
Using the example of Figure 1, thecontribution of the target phrase t11 = nobel, peaceto P (s, t) using a 3- gram model is:P(nobel|[receive, the], [la, paix])?P(peace|[the, nobel], [la, paix]).A benefit of this new formulation is that the vo-cabularies involved only contain words, and are thusmuch smaller that tuple vocabularies.
These modelsare thus less at risk to be plagued by data sparsity is-sues.
Moreover, the decomposition (3) now involvestwo models: the first term represents a TM, the sec-ond term is best viewed as a reordering model.
Inthis formulation, the TM only predicts the targetphrase, given its source and target contexts.P (s, t) =L?i=1[ |si|?k=1P(ski |hn?1(ski ), hn?1(t1i+1))?|ti|?k=1P(tki |hn?1(s1i ), hn?1(tki ))] (4)4 The principles of SOULIn section 3.1, we defined a n-gram translationmodel based on equations (3) and (4).
A major diffi-culty with such models is to reliably estimate theirparameters, the numbers of which grow exponen-tially with the order of the model.
This problemis aggravated in natural language processing due to332the well-known data sparsity issue.
In this work,we take advantage of the recent proposal of (Le etal., 2011).
Using a specific neural network architec-ture (the Structured OUtput Layer or SOUL model),it becomes possible to handle large vocabulary lan-guage modeling tasks.
This approach was experi-mented last year for target language models only andis now extended to translation models.
More detailsabout the SOUL architecture can be found in (Le etal., 2011), while its extension to translation modelsis more precisely described in (Hai-Son et al, 2012).The integration of SOUL models for large SMTtasks is carried out using a two-pass approach: thefirst pass uses conventional back-off n-gram trans-lation and language models to produce a k-best list(the k most likely translations); in the second pass,the probability of a m-gram SOUL model is com-puted for each hypothesis and the k-best list is ac-cordingly reordered.
In all the following experi-ments, we used a context size for SOUL of m = 10,and used k = 300.
The two decompositions of equa-tions (3) and (4) are used by introducing 4 scoresduring the rescoring step.5 Corpora and data pre-processingConcerning data pre-processing, we started from oursubmissions from last year (Allauzen et al, 2011)and mainly upgraded the corpora and the associatedlanguage-dependent pre-processing routines.5.1 Pre-processingWe used in-house text processing tools for the to-kenization and detokenization steps (De?chelotte etal., 2008).
Previous experiments have demonstratedthat better normalization tools provide better BLEUscores: all systems are thus built in ?true-case?.Compared to last year, the pre-processing of utf-8characters was significantly improved.As German is morphologically more complexthan English, the default policy which consists intreating each word form independently is plaguedwith data sparsity, which severely impacts bothtraining (alignment) and decoding (due to unknownforms).
When translating from German into En-glish, the German side is thus normalized using aspecific pre-processing scheme (described in (Al-lauzen et al, 2010; Durgar El-Kahlout and Yvon,2010)), which aims at reducing the lexical redun-dancy by (i) normalizing the orthography, (ii) neu-tralizing most inflections and (iii) splitting complexcompounds.
All parallel corpora were POS-taggedwith the TreeTagger (Schmid, 1994); in addition, forGerman, fine-grained POS labels were also neededfor pre-processing and were obtained using the RF-Tagger (Schmid and Laws, 2008).5.2 Bilingual corporaAs for last year?s evaluation, we used all the avail-able parallel data for the German-English languagepair, while only a subpart of the French-English par-allel data was selected.
Word alignment modelswere trained using all the data, whereas the transla-tion models were estimated on a subpart of the par-allel data: the UN corpus was discarded for this stepand about half of the French-English Giga corpuswas filtered based on a perplexity criterion as in (Al-lauzen et al, 2011)).For French-English, we mainly upgraded thetraining material from last year by extracting thenew parts from the common data.
The wordalignment models trained last year were then up-dated by running a forced alignment 4 of the newdata.
These new word-aligned data was added tolast year?s parallel corpus and constitute the train-ing material for the translation models and featurefunctions described in Section 2.
Given the largeamount of available data, three different bilingualn-gram models are estimated, one for each source ofdata: News-Commentary, Europarl, and the French-English Giga corpus.
These models are then addedto the weighted mixture defined by equation (1).
ForGerman-English, we simply used all the availableparallel data to train one single translation models.5.3 Monolingual corpora and language modelsFor the monolingual training data, we also used thesame setup as last year.
For German, all the train-ing data allowed in the constrained task were di-vided into several sets based on dates or genres:News-Commentary, the news crawled from the Webgrouped by year, and Europarl.
For each subset,a standard 4-gram LM was estimated using inter-polated Kneser-Ney smoothing (Kneser and Ney,4The forced alignment step consists in an additional EM it-eration.3331995; Chen and Goodman, 1998).
The resultingLMs are then linearly combined using interpolationcoefficients chosen so as to minimize the perplexityof the development set.
The German vocabulary iscreated using all the words contained in the paralleldata and expanded to reach a total of 500k words byincluding the most frequent words observed in themonolingual News data for 2011.For French and English, the same monolingualcorpora as last year were used5.
We did not observeany perplexity decrease in our attempts to includethe new data specifically provided for this year?sevaluation.
We therefore used the same languagemodels as in (Allauzen et al, 2011).6 ?On-the-fly?
systemWe also developped an alternative approach imple-menting ?on-the-fly?
estimation of the parameter ofa standard phase-based model, using Moses (Koehnet al, 2007) as the decoder.
Implementing on-the-fly estimation for n-code, while possible in the-ory, is less appealing due to the computational costof estimating a smoothed language model.
Givenan input source file, it is possible to compute onlythose statistics which are required to translate thephrases it contains.
As in previous works on on-the-fly model estimation for SMT (Callison-Burchet al, 2005; Lopez, 2008), we compute a suffixarray for the source corpus.
This further enablesto consider only a subset of translation examples,which we select by deterministic random sampling,meaning that the sample is chosen randomly withrespect to the full corpus but that the same sampleis always returned for a given value of sample size,hereafter denoted N .
In our experiments, we usedN = 1, 000 and computed from the sample and theword alignments (we used the same tokenization andword alignments as in all other submitted systems)the same translation6 and lexical reordering modelsas the standard training scripts of the Moses system.Experiments were run on the data sets used forWMT English-French machine translation evalua-tion tasks, using the same corpora and optimization5The fifth edition of the English Gigaword (LDC2011T07)was not used.6An approximation is used for p(f |e), and coherent transla-tion estimation is used; see (Lopez, 2008).procedure as in our other experiments.
The only no-table difference is our use of the Moses decoder in-stead of the n-gram-based system.
As shown in Ta-ble 1, our on-the-fly system achieves a result (31.7BLEU point) that is slightly worst than the n-codebaseline (32.0) and slightly better than the equiva-lent Moses baseline (31.5), but does it much faster.Model estimation for the test file is reduced to 2hours and 50 minutes, with an additional overheadfor loading and writing files of one and a half hours,compared to roughly 210 hours for our baseline sys-tems under comparable hardware conditions.7 Experimental results7.1 n-code with SOULTable 1 summarizes the experimental results sub-mitted to the shared translation for French-Englishand German-English in both directions.
The perfor-mances are measured in terms of BLEU on new-stest2011, last year?s test set, and this year?s testset newstest2012.
For the former, BLEU scores arecomputed with the NIST script mteva-v13.pl, whilewe provide for newstest2012 the results computedby the organizers 7.
The Baseline results are ob-tained with standard n-gram models estimated withback-off, both for the bilingual and monolingual tar-get models.
With standard n-gram estimates, the or-der is limited to n = 4.
For instance, the n-codeFrench-English baseline achieves a 0.5 BLEU pointimprovement over a Moses system trained with thesame data setup in both directions.From Table 1, it can be observed that addingthe SOUL models (translation models and targetlanguage model) consistently improves the base-line, with an increase of 1 BLEU point.
Con-trastive experiments show that the SOUL target LMdoes not bring significant gain when added to theSOUL translation models.
For instance, a gain of0.3 BLEU point is observed when translating fromFrench to English with the addition of the SOUL tar-get LM.
In the other translation directions, the differ-ences are negligible.7All results come from the official website: http://matrix.statmt.org/matrix/.334Direction System BLEUtest2011 test2012?en2fr Baseline 32.0 28.9+ SOUL TM 33.4 29.9on-the-fly 31.7 28.6fr2en Baseline 30.2 30.4+ SOUL TM 31.1 31.5en2de Baseline 15.4 16.0+ SOUL TM 16.6 17.0de2en Baseline 21.8 22.9+ SOUL TM 22.8 23.9Table 1: Experimental results in terms of BLEU scoresmeasured on the newstest2011 and newstest2012.
Fornewstest2012, the scores are provided by the organizers.7.2 Experiments with additional featuresFor this year?s evaluation, we also investigated sev-eral additional features based on IBM1 models andword sense disambiguation (WSD) information inrescoring.
As for the SOUL models, these featuresare added after the n-best list generation step.In previous work (Och et al, 2004; Hasan, 2011),the IBM1 features (Brown et al, 1993) are foundhelpful.
As the IBM1 model is asymmetric, twomodels are estimated, one in both directions.
Con-trary to the reported results, these additional featuresdo not yield significant improvements over the base-line system.
We assume that the difficulty is to addinformation to an already extensively optimized sys-tem.
Moreover, the IBM1 models are estimated onthe same training corpora as the translation system,a fact that may explain the redundancy of these ad-ditional features.In a separate series of experiments, we also addWSD features calculated according to a variation ofthe method proposed in (Apidianaki, 2009).
Foreach word of a subset of the input (source lan-guage) vocabulary, a simple WSD classifier pro-duces a probability distribution over a set of trans-lations8.
During reranking, each translation hypoth-esis is scanned and the word translations that matchone of the proposed variant are rewarded using anadditional score.
While this method had given some8The difference with the method described in (Apidianaki,2009) is that no sense clustering is performed, and each transla-tion is represented by a separate weighted source feature vectorwhich is used for disambiguationsmall gains on a smaller dataset (IWSLT?11), we didnot observe here any improvement over the base-line system.
Additional analysis hints that (i) mostof the proposed variants are already covered by thetranslation model with high probabilities and (ii) thatthese variants are seldom found in the reference sen-tences.
This means that, in the situation in whichonly one reference is provided, the hypotheses witha high score for the WSD feature are not adequatelyrewarded with the actual references.8 ConclusionIn this paper, we described our submissions toWMT?12 in the French-English and German-English shared translation tasks, in both directions.As for our last year?s participation, our main sys-tems are built with n-code, the open source Statis-tical Machine Translation system based on bilingualn-grams.
Our contributions are threefold.
First, wehave experimented a new kind of translation mod-els, where the bilingual n-gram distribution are es-timated in a continuous space with neural networks.As shown in past evaluations with target languagemodel, there is a significant reward for using thiskind of models in a rescoring step.
We observed that,in general, the continuous space translation modelyields a slightly larger improvement than the targettranslation model.
However, their combination doesnot result in an additional gain.We also reported preliminary results with a sys-tem ?on-the-fly?, where the training data are sam-pled according to the data to be translated in orderto train contextually adapted system.
While this sys-tem achieves comparable performance to our base-line system, it is worth noticing that its total train-ing time is much smaller than a comparable Mosessystem.
Finally, we investigated several additionalfeatures based on IBM1 models and word sense dis-ambiguation information in rescoring.
While thesemethods have sometimes been reported to help im-prove the results, we did not observe any improve-ment here over the baseline system.AcknowledgmentThis work was partially funded by the French Stateagency for innovation (OSEO) in the Quaero Pro-gramme.335ReferencesAlexandre Allauzen, Josep M. Crego, I?lknur Durgar El-Kahlout, and Franc?ois Yvon.
2010.
LIMSI?s statis-tical translation systems for WMT?10.
In Proc.
of theJoint Workshop on Statistical Machine Translation andMetricsMATR, pages 54?59, Uppsala, Sweden.Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,Guillaume Wisniewski, and Franc?ois Yvon.
2011.LIMSI @ WMT11.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 309?315, Edinburgh, Scotland, July.
Association for Com-putational Linguistics.Marianna Apidianaki.
2009.
Data-driven semantic anal-ysis for multilingual WSD and lexical selection intranslation.
In Proceedings of the 12th Conference ofthe European Chapter of the ACL (EACL 2009), pages77?85, Athens, Greece, March.
Association for Com-putational Linguistics.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19(2):263?311.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proceedings of the 43rd Annual Meetingof the Association for Computational Linguistics(ACL?05), pages 255?262, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Stanley F. Chen and Joshua T. Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, Computer Sci-ence Group, Harvard Un iversity.Josep M. Crego and Jose?
B. Marin?o.
2006.
Improvingstatistical MT by coupling reordering and decoding.Machine Translation, 20(3):199?215.Josep M. Crego, Franc?ois Yvon, and Jose?
B. Marin?o.2011.
N-code: an open-source Bilingual N-gram SMTToolkit.
Prague Bulletin of Mathematical Linguistics,96:49?58.Ilknur Durgar El-Kahlout and Franc?ois Yvon.
2010.
Thepay-offs of preprocessing for German-English Statis-tical Machine Translation.
In Marcello Federico, IanLane, Michael Paul, and Franc?ois Yvon, editors, Pro-ceedings of the seventh International Workshop onSpoken Language Translation (IWSLT), pages 251?258.Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-nard, and Franc?ois Yvon.
2008.
LIMSI?s statisti-cal translation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.Hai-Son, Alexandre Allauzen, and Franc?ois Yvon.
2012.Continuous space translation models with neural net-works.
In NAACL ?12: Proceedings of the 2012 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology.Sas?a Hasan.
2011.
Triplet Lexicon Models for Statisti-cal Machine Translation.
Ph.D. thesis, RWTH AachenUniversity.Reinhard Kneser and Herman Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acoustics,Speech, and Signal Processing, ICASSP?95, pages181?184, Detroit, MI.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011.
Structured outputlayer neural network language model.
In Proceedingsof ICASSP?11, pages 5524?5527.Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 505?512, Manchester, UK, August.Coling 2008 Organizing Committee.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrick Lambert, Jose?
A.R.
Fonollosa, andMarta R. Costa-Jussa`.
2006.
N-gram-based machinetranslation.
Computational Linguistics, 32(4):527?549.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 161?168, Boston, Massachusetts, USA,336May 2 - May 7.
Association for Computational Lin-guistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL ?03: Proc.
ofthe 41st Annual Meeting on Association for Computa-tional Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL ?02: Proc.
ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318.
Association forComputational Linguistics.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees and anapplication to fine-grained POS tagging.
In Proceed-ings of the 22nd International Conference on Com-putational Linguistics (Coling 2008), pages 777?784,Manchester, UK, August.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proc.
of InternationalConference on New Methods in Language Processing,pages 44?49, Manchester, UK.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004, pages 101?104.
Association forComputational Linguistics.Richard Zens, Franz Josef Och, and Hermann Ney.
2002.Phrase-based statistical machine translation.
In KI?02: Proceedings of the 25th Annual German Con-ference on AI, pages 18?32, London, UK.
Springer-Verlag.337
