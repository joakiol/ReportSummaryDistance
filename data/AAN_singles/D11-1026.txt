Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLinear Text Segmentation Using Affinity PropagationAnna KazantsevaSchool of Electrical Engineeringand Computer Science,University of Ottawaankazant@site.uottawa.caStan SzpakowiczSchool of Electrical Engineeringand Computer Science,University of Ottawa &Institute of Computer Science,Polish Academy of Sciencesszpak@site.uottawa.caAbstractThis paper presents a new algorithm for lin-ear text segmentation.
It is an adaptation ofAffinity Propagation, a state-of-the-art clus-tering algorithm in the framework of factorgraphs.
Affinity Propagation for Segmenta-tion, or APS, receives a set of pairwise simi-larities between data points and produces seg-ment boundaries and segment centres ?
datapoints which best describe all other data pointswithin the segment.
APS iteratively passesmessages in a cyclic factor graph, until conver-gence.
Each iteration works with informationon all available similarities, resulting in high-quality results.
APS scales linearly for realisticsegmentation tasks.
We derive the algorithmfrom the original Affinity Propagation formu-lation, and evaluate its performance on topi-cal text segmentation in comparison with twostate-of-the art segmenters.
The results sug-gest that APS performs on par with or outper-forms these two very competitive baselines.1 IntroductionIn complex narratives, it is typical for the topic toshift continually.
Some shifts are gradual, others ?more abrupt.
Topical text segmentation identifies themore noticeable topic shifts.
A topical segmenter?soutput is a very simple picture of the document?sstructure.
Segmentation is a useful intermediate stepin such applications as subjectivity analysis (Stoy-anov and Cardie, 2008), automatic summarization(Haghighi and Vanderwende, 2009), question an-swering (Oh, Myaeng, and Jang, 2007) and others.That is why improved quality of text segmentationcan benefit other language-processing tasks.We present Affinity Propagation for Segmenta-tion (APS), an adaptation of a state-of-the-art clus-tering algorithm, Affinity Propagation (Frey andDueck, 2007; Givoni and Frey, 2009).1 The origi-nal AP algorithm considerably improved exemplar-based clustering both in terms of speed and the qual-ity of solutions.
That is why we chose to adapt it tosegmentation.
At its core, APS is suitable for seg-menting any sequences of data, but we present it inthe context of segmenting documents.
APS takes asinput a matrix of pairwise similarities between sen-tences and, for each sentence, a preference valuewhich indicates an a priori belief in how likelya sentence is to be chosen as a segment centre.APS outputs segment assignments and segment cen-tres ?
data points which best explain all other pointsin a segment.
The algorithm attempts to maximizenet similarity ?
the sum of similarities between alldata points and their respective segment centres.APS operates by iteratively passing messages ina factor graph (Kschischang, Frey, and Loeliger,2001) until a good set of segments emerges.
Eachiteration considers all similarities ?
takes into ac-count all available information.
An iteration in-cludes sending at most O(N2) messages.
For themajority of realistic segmentation tasks, however,the upper bound is O(MN) messages, where Mis a constant.
This is more computationally ex-pensive than the requirements of locally informedsegmentation algorithms such as those based onHMM or CRF (see Section 2), but for a globally-informed algorithm the requirements are very rea-sonable.
APS is an instance of loopy-belief propaga-tion (belief propagation on cyclic graphs) which has1An implementation of APS in Java, and the data sets, can bedownloaded at ?www.site.uottawa.ca/?ankazant?.284been used to achieved state-of-the-art performancein error-correcting decoding, image processing anddata compression.
Theoretically, such algorithmsare not guaranteed to converge or to maximize theobjective function.
Yet in practice they often achievecompetitive results.APS works on an already pre-compiled similaritiymatrix, so it offers flexibility in the choice of simi-larity metrics.
The desired number of segments canbe set by adjusting preferences.We evaluate the performance of APS on threetasks: finding topical boundaries in transcripts ofcourse lectures (Malioutov and Barzilay, 2006),identifying sections in medical textbooks (Eisen-stein and Barzilay, 2008) and identifying chapterbreaks in novels.
We compare APS with two recentsystems: the Minimum Cut segmenter (Malioutovand Barzilay, 2006) and the Bayesian segmenter(Eisenstein and Barzilay, 2008).
The comparisonis based on the WindowDiff metric (Pevzner andHearst, 2002).
APS matches or outperforms thesevery competitive baselines.Section 2 of the paper outlines relevant researchon topical text segmentation.
Section 3 briefly cov-ers the framework of factor graphs and outlines theoriginal Affinity Propagation algorithm for cluster-ing.
Section 4 contains the derivation of the newupdate messages for APSeg.
Section 5 describes theexperimental setting, Section 6 reports the results,Section 7 discusses conclusions and future work.2 Related WorkThis sections discusses selected text segmentationmethods and positions the proposed APS algorithmin that context.Most research on automatic text segmentation re-volves around a simple idea: when the topic shifts,so does the vocabulary (Youmans, 1991).
We canroughly subdivide existing approaches into two cat-egories: locally informed and globally informed.Locally informed segmenters attempt to identifytopic shifts by considering only a small portion ofcomplete document.
A classical approach is Text-Tiling (Hearst, 1997).
It consists of sliding two ad-jacent windows through text and measuring lexicalsimilarity between them.
Drops in similarity corre-spond to topic shifts.
Other examples include textsegmentation using Hidden Markov Models (Bleiand Moreno, 2001) or Conditional Random Fields(Lafferty, McCallum, and Pereira, 2001).
Locallyinformed methods are often very efficient becauseof lean memory and CPU time requirements.
Due toa limited view of the document, however, they caneasily be thrown off by short inconsequential digres-sions in narration.Globally informed methods consider ?the big pic-ture?
when determining the most likely location ofsegment boundaries.
Choi (2000) applies divisiveclustering to segmentation.
Malioutov and Barzilay(2006) show that the knowledge about long-rangesimilarities between sentences improves segmenta-tion quality.
They cast segmentation as a graph-cutting problem.
The document is represented as agraph: nodes are sentences and edges are weightedusing a measure of lexical similarity.
The graph iscut in a way which maximizes the net edge weightwithin each segment and minimizes the net weightof severed edges.
Such Minimum Cut segmentationresembles APS the most among others mentioned inthis paper.
The main difference between the two isin different objective functions.Another notable direction in text segmentationuses generative models to find segment boundaries.Eisenstein and Barzilay (2008) treat words in a sen-tence as draws from a multinomial language model.Segment boundaries are assigned so as to maximizethe likelihood of observing the complete sequence.Misra et al (2009) use a Latent Dirichlet aloca-tion topic model (Blei, Ng, and Jordan, 2003) to findcoherent segment boundaries.
Such methods outputsegment boundaries and suggest lexical distributionassociated with each segment.
Generative modelstend to perform well, but are less flexible than thesimilarity-based models when it comes to incorpo-rating new kinds of information.Globally informed models generally perform bet-ter, especially on more challenging datasets such asspeech recordings, but they have ?
unsurprisingly ?higher memory and CPU time requirements.The APS algorithm described in this paper com-bines several desirable properties.
It is unsupervisedand, unlike most other segmenters, does not requirespecifying the desired number of segments as an in-put parameter.
On each iteration it takes into accountthe information about a large portion of the docu-285ment (or all of it).
Because APS operates on a pre-compiled matrix of pair-wise sentence similarities, itis easy to incorporate new kinds of information, suchas synonymy or adjacency.
It also provides some in-formation as to what the segment is about, becauseeach segment is associated with a segment centre.3 Factor graphs and affinity propagationfor clustering3.1 Factor graphs and the max-sum algorithmThe APS algorithm is an instance of belief propa-gation on a cyclic factor graph.
In order to explainthe derivation of the algorithm, we will first brieflyintroduce factor graphs as a framework.Many computational problems can be reduced tomaximizing the value of a multi-variate functionF (x1, .
.
.
, xn) which can be approximated by a sumof simpler functions.
In Equation 1, H is a set ofdiscrete indices and fh is a local function with argu-ments Xh ?
{x1, .
.
.
, xn}:F (x1, .
.
.
, xn) =?h?Hfh(Xh) (1)Factor graphs offer a concise graphical represen-tation for such problems.
A global function F whichcan be decomposed into a sum of M local functionfh can be represented as a bi-partite graph with Mfunction nodes and N variable nodes (M = |H|).Figure 1 shows an example of a factor graph forF (x1, x2, x3, x4) = f1(x1, x2, x3)+ f2(x2, x3, x4).The factor (or function) nodes are dark squares, thevariable nodes are light circles.The well-known max-sum algorithm (Bishop,2006) seeks a configuration of variables which max-imizes the objective function.
It finds the maximumin acyclic factor graphs, but in graphs with cyclesneither convergence nor optimality are guaranteed(Pearl, 1982).
Yet in practice good approximationscan be achieved.
The max-sum algorithm amountsto propagating messages from function nodes tovariable nodes and from variable nodes to functionnodes.
A message sent from a variable node x to afunction node f is computed as a sum of the incom-ing messages from all neighbours of x other than f(the sum is computed for each possible value of x):?x?f =?f ?
?N(x)\f?f ?
?x (2)Figure 1: Factor graph for F (x1, x2, x3, x4)= f1(x1, x2, x3) + f2(x2, x3, x4).f1 f2x1 x2 x3 x4N(x) is the set of all function nodes which are x?sneighbours.
The message reflects the evidence aboutthe distribution of x from all functions which have xas an argument, except for the function correspond-ing to the receiving node f .A message ?f?x from function f to variable x iscomputed as follows:?f?x = maxN(f)\x(f(x1, .
.
.
, xm) +?x??N(f)\x?x?
?f )(3)N(f) is the set of all variable nodes which are f ?sneighbours.
The message reflects the evidence aboutthe distribution of x from function f and its neigh-bours other than x.A common message-passing schedule on cyclicfactor graphs is flooding: iteratively passing allvariable-to-function messages, then all function-to-variable messages.
Upon convergence, the summarymessage reflecting final beliefs about the maximiz-ing configuration of variables is computed as a sumof all incoming function-to-variable messages.3.2 Affinity PropagationThe APS algorithm described in this paper is a mod-ification of the original Affinity Propagation algo-rithm intended for exemplar-based clustering (Freyand Dueck, 2007; Givoni and Frey, 2009).
This sec-tion describes the binary variable formulation pro-posed by Givoni and Frey, and lays the groundworkfor deriving the new update messages (Section 4).Affinity Propagation for exemplar-based cluster-ing is formulated as follows: to cluster N datapoints, one must specify a matrix of pairwise sim-ilarities {SIM(i, j)}i,j?
{1,...,N},i 6=j and a set ofself-similarities (so-called preferences) SIM(j, j)which reflect a priori beliefs in how likely each datapoint is to be selected as an exemplar.
Preferencevalues occupy the diagonal of the similarity matrix.The algorithm then assigns each data point to an ex-emplar so as to maximize net similarity ?
the sum of286Figure 2: Factor graph for affinity propagation.E1 Ej ENI1IiINc11 c1j c1Nci1 cij ciNcN1 cNj cNNS11 S1j S1NSi1 Sij SiNSN1 SNj SNNsimilarities between all points and their respectiveexemplars; this is expressed by Equation 7.
Figure 2shows a schematic factor graph for this problem,with N2 binary variables.
cij = 1 iff point j is anexemplar for point i.
Function nodes Ej enforce acoherence constraint: a data point cannot exemplifyanother point unless it is an exemplar for itself:Ej(c1j , .
.
.
, cNj) =???????
if cjj = 0 ?
cij = 1for some i 6= j0 otherwise(4)An I node encodes a single-cluster constraint: eachdata point must belong to exactly one exemplar ?and therefore to one cluster:Ii(ci1, .
.
.
, ciN ) ={??
if ?j cij 6= 10 otherwise (5)An S node encodes user-defined similaritiesbetween data-points and candidate exemplars(SIM(i, j) is the similarity between points i andj):Sij(cij) ={SIM(i, j) if cij = 10 otherwise (6)Equation 7 shows the objective function which wewant to maximize: a sum of similarities betweendata points and their exemplars, subject to the twoconstraints (coherence and single-cluster per point).S(c11, .
.
.
, cNN ) =?i,jSi,j(cij) +?iIi(ci1, .
.
.
, ciN )(7)+?jEj(c1j , .
.
.
, cNj)According to Equation 3, the computation of a sin-gle factor-to-variable message involves maximizingover 2n configurations.
E and I , however, are bi-nary constraints and evaluate to ??
for most con-figurations.
This drastically reduces the number ofconfigurations which can maximize the message val-ues.
Given this simple fact, Givoni and Frey (2009)show how to reduce the necessary update messagesto only two types of scalar ones: availabilities (?
)and responsibilities (?
).2A responsibility message ?ij , sent from a variablenode cij to function node Ej , reflects the evidenceof how likely j is to be an exemplar for i given allother potential exemplars:?ij = SIM(i, j)?maxk 6=j (SIM(i, k) + ?ik) (8)An availability message ?ij , sent from a functionnode Ej to a variable node cij , reflects how likelypoint j is to be an exemplar for i given the evidencefrom all other data points:?ij =?????????
?k 6=jmax[?kj , 0] if i = jmin[0, ?jj +?k/?
{i,j}max[?kj , 0]] if i 6= j(9)Let ?ij(l) be the message value corresponding to set-ting variable cij to l, l ?
{0, 1}.
Instead of sendingtwo-valued messages (corresponding to the two pos-sible values of the binary variables), we can sendthe difference for the two possible configurations:?ij = ?ij(1)?
?ij(0) ?
effectively, a log-likelihoodratio.2Normally, each iteration of the algorithm sends five typesof two-valued messages: to and from functions E and I anda message from functions S. Fortunately, the messages sentto and from E factors to the variable nodes subsume the threeother message types and it is not necessary to compute themexplicitly.
See (Givoni and Frey, 2009, p.195) for details.287Figure 3: Examples of valid configuration of hiddenvariables {cij} for clustering and segmentation.
(a) Clustering (b) SegmentationThe algorithm converges when the set of pointslabelled as exemplars remains unchanged for a pre-determined number of iterations.
When the al-gorithm terminates, messages to each variable areadded together.
A positive final message indicatesthat the most likely value of a variable cij is 1 (pointj is an exemplar for i), a negative message indicatesthat it is 0 (j is not an exemplar for i).4 Affinity Propagation for SegmentationThis section explains how we adapt the AffinityPropagation clustering algorithm to segmentation.In this setting, sentences are data points and werefer to exemplars as segment centres.
Given a doc-ument, we want to assign each sentence to a segmentcentre so as to maximize net similarity.The new formulation relies on the same underly-ing factor graph (Figure 2).
A binary variable nodecij is set to 1 iff sentence j is the segment centre forsentence i.
When clustering is the objective, a clus-ter may consist of points coming from anywhere inthe data sequence.
When segmentation is the ob-jective, a segment must consist of a solid block ofpoints around the segment centre.
Figure 3 shows,for a toy problem with 5 data points, possible validconfigurations of variables {cij} for clustering (3a)and for segmentation (3b).To formalize this new linearity requirement, weelaborate Equation 4 into Equation 10.
Ej evaluatesto ??
in three cases.
Case 1 is the original coher-ence constraint.
Case 2 states that no point k maybe in the segment with a centre is j, if k lies beforethe start of the segment (the sequence c(s?1)j = 0,csj = 1 necessarily corresponds to the start of thesegment).
Case 3 handles analogously the end ofthe segment.Ej =???????????????????????
1. if cjj = 0 ?
cij = 1 for some i 6= j2.
if cjj = 1 ?
csj = 1 ?
c(s?1)j = 0?
ckj = 1 for some s < j, k < s?
13. if cjj = 1 ?
cej = 1 ?
c(e+1)j = 0?
ckj = 1 for some e > j, k > e+ 10 otherwise(10)The E function nodes are the only changed part ofthe factor graph, so we only must re-derive ?
mes-sages (availabilities) sent from factors E to variablenodes.
A function-to-variable message is computedas shown in Equation 11 (elaborated Equation 3),and the only incoming messages to E nodes are re-sponsibilities (?
messages):?f?x = maxN(f)\x(f(x1, .
.
.
, xm) +?x??N(f)\x?x?
?f ) =(11)maxcij , i 6=j((Ej(c1j , .
.
.
, cNj) +?cij , i 6=j?ij(cij)))We need to compute the message values for thetwo possible settings of binary variables ?
denotedas ?ij(1) and ?ij(0) ?
and propagate the difference?ij = ?ij(1) - ?ij(0).Consider the case of factor Ej sending an ?
mes-sage to the variable node cjj (i.e., i = j).
If cjj = 0then point j is not its own segment centre and theonly valid configuration is to set al other cij to 0:?jj(0) = maxcij ,i 6=j(Ej(c1j , .
.
.
, cNj) +?cij ,i 6=j?ij(cij))(12)=?i 6=j?ij(0)To compute ?ij(1) (point j is its own segmentcentre), we only must maximize over configurationswhich will not correspond to cases 2 and 3 in Equa-tion 10 (other assignments are trivially non-optimalbecause they would evaluate Ej to ??).
Let thestart of a segment be s, 1 ?
s < j and the end ofthe segment be e, j + 1 < e ?
N .
We only need toconsider configurations such that all points betweens and e are in the segment while all others are not.288The following picture shows a valid configuration.31 s j e NTo compute the message ?ij(1), i = j, we have:?jj(1) =jmaxs=1[s?1?k=1?kj(0) +j?1?k=s?kj(1)]+ (13)Nmaxe=j[e?k=j+1?kj(1) +N?k=e+1?kj(0)]Subtracting Equation 12 from Equation 13, we get:?jj = ?jj(1)?
?jj(0) = (14)jmaxs=1(j?1?k=s?kj) +Nmaxe=j(e?k=j+1?kj)Now, consider the case of factor Ej sending an ?message to a variable node cij other than segmentexemplar j (i.e., i 6= j).
Two subcases are possible:point i may lie before the segment centre j (i < j),or it may lie after the segment centre (i > j).The configurations which may maximize ?ij(1)(the message value for setting the hidden variableto 1) necessarily conform to two conditions: pointj is labelled as a segment centre (cjj = 1) and allpoints lying between i and j are in the segment.This corresponds to Equation 15 for i < j and toEquation 16 for i > j. Pictorial examples of corre-sponding valid configurations precede the equations.1 s i j e N?ij, i<j(1) =imaxs=1[s?1?k=1?kj(0) +i?1?k=s?kj(1)]+(15)j?k=i+1?kj(1) +Nmaxe=j[e?k=j+1?kj(1) +N?k=e+1?kj(0)]3Variables cij set to 1 are shown as shaded circles, to 0 ?
aswhite circles.
Normally, variables form a column in the factorgraph; we transpose them to save space.1 s j i e N?ij, i>j(1) =jmaxs=1[s?1?k=1?kj(0) +j?1?k=s?kj(1)]+(16)i?1?k=j?kj(1) +Nmaxe=i[e?k=i+1?kj(1) +N?k=e+1?kj(0)]To compute the message value for setting thehidden variable cij to 0, we again distinguishbetween i < j and i > j and consider whether cjj= 1 or cjj = 0 (point j is / is not a segment centre).For cjj = 0 the only optimal configuration is cij = 0for all i 6= j.
For cjj = 1 the set of possible optimalconfigurations is determined by the position of pointi with respect to point j.
Following the same logicas in the previous cases we get Equation 17 fori < j and Equation 18 for i > j.1 i s j e N?ij(0) = max(?k/?i,j?kj(0), (17)i?1?k=1?kj(0) +jmaxs=i+1[s?1?k=i+1?kj(0) +j?1?k=s?kj(1)]+?jj(1) +Nmaxe=j[e?k=j+1?kj(1) +N?k=e+1?kj(0)])1 s j e i N?ij(0) = max(?k/?i,j?kj(0), (18)jmaxs=1[s?1?k=1?kj(0) +j?1?k=s?kj(1)]+?jj(1) +i?1maxe=j[e?k=j+1?kj(1) +i?1?k=e+1?kj(0)]N?k=i+1?kj(0))Due to space constraints, we will omit the detailsof subtracting Equation 17 from 15 and Equation 18from 16.
The final update rules for both i < j and289Algorithm 1 Affinity Propagation for Segmentation1: input: 1) a set of pairwise similarities {SIM(i, j)}(i,j)?
{1,...,N}2 , SIM(i, j) ?
R; 2) a set of prefer-ences (self-similarities) {SIM(i, i)}i?
{1,...,N} indicating a priori likelihood of point i being a segmentcentre2: initialization: ?i, j : ?ij = 0 (set al availabilities to 0)3: repeat4: iteratively update responsibilities (?)
and availabilities (?
)5:?i, j : ?ij = SIM(i, j) + maxk 6=j (SIM(i, k)?
?ik)6:?i, j : ?ij =??????????????????????????????????????????????????
?jmaxs=1(j?1?k=s?kj) +Nmaxe=j(e?k=j+1?kj) if i = jmin[ imaxs=1i?1?k=s?kj +j?k=i+1?kj +Nmaxe=je?k=j+1?kj ,imaxs=1i?1?k=s?kj +jmins=i+1s?1?k=i+1?kj ] if i < jmin[ jmaxs=1j?1?k=s?kj +i?1?k=j?kj +Nmaxe=ie?k=i+1?kj ,i?1mine=ji?1?k=e+1?kj +Nmaxe=ie?k=i+1?kj ] if i > j7: until convergence8: compute the final configuration of variables: ?i, j j is the exemplar for i iff ?ij + ?ij > 09: output: exemplar assignmentsi > j appear in Algorithm 1, where we summarizethe whole process.The equations look cumbersome but they are triv-ial to compute.
Every summand corresponds to find-ing the most likely start or end of the segment, tak-ing into account fixed information.
When computingmessages for any given sender node, we can remem-ber the maximizing values for neighbouring recipi-ent nodes.
For example, after computing the avail-ability message from factor Ej to cij , we must onlyconsider one more responsibility value when com-puting the message from Ej to variable c(i+1)j .
Thecost of computing a message is thus negligible.When the matrix is fully specified, each iterationrequires passing 2N2 messages, so the algorithmruns in O(N2) time and requires O(N2) memory(to store the similarities, the availabilities and theresponsibilities).
When performing segmentation,however, the user generally has some idea aboutthe average or maximum segment length.
In suchmore realistic cases, the input matrix of similaritiesis sparse ?
it is constructed by sliding a window ofsize M .
M usually needs to be at least twice themaximum segment length or thrice the average seg-ment length.
Each iteration, then, involves sending2MN messages and the storage requirements arealso O(MN).As is common in loopy belief propagation algo-rithms, both availability and responsibility messagesare dampened to avoid overshooting and oscillating.The dampening factor is ?
where 0.5 ?
?
< 1.newMsg = ?
?
oldMsg+ (1?
?
)newMsg (19)The APS algorithm is unsupervised.
It only benefits290from a small development set to fine-tune a few pa-rameters: preference values and the dampening fac-tor.
APS does not require (nor allow) specifying thenumber of segments beforehand.
The granularity ofsegmentation is adjusted through preference values;this reflect how likely each sentence is to be selectedas a segment centre.
(This translates into the cost ofadding a segment.
)Because each message only requires the knowl-edge about one column or row of the matrix, the al-gorithm can be easily parallelized.5 Experimental SettingDatasets.
We evaluate the performance of theAPS algorithm on three datasets.
The first, com-piled by Malioutov and Barzilay (2006), consistsof manually transcribed and segmented lectures onArtificial Intelligence, 3 development files and 19test files.
The second dataset consists of 227 chap-ters from medical textbooks (Eisenstein and Barzi-lay, 2008), 5 of which we use for development.
Inthis dataset the gold standard segment boundariescorrespond to section breaks specified by the au-thors.
The third dataset consists of 85 works of fic-tion downloaded from Project Gutenberg, 3 of whichare used for development.
The segment boundariescorrespond to chapter breaks or to breaks betweenindividual stories.
They were inserted automaticallyusing HTML markup in the downloaded files.The datasets exhibit different characteristics.
Thelecture dataset and the fiction dataset are challeng-ing because they are less cohesive than medical text-books.
The textbooks are cognitively more difficultto process and the authors rely on repetition of ter-minology to facilitate comprehension.
Since lexicalrepetition is the main source of information for textsegmentation, we expect a higher performance onthis dataset.
Transcribed speech, on the other hand,is considerably less cohesive.
The lecturer makes aneffort to speak in ?plain language?
and to be com-prehensible, relying less on terminology.
The use ofpronouns is very common, as is the use of examples.Repeated use of the same words is also uncom-mon in fiction.
In addition, the dataset was compiledautomatically using HTML markup.
The markupis not always reliable and occasionally the e-bookproofreaders skip it altogether, which potentiallyadds noise to the dataset.Baselines.
We compare the performance ofAPS with that of two state-of-the-art segmenters: theMinimum Cut segmenter (Malioutov and Barzilay,2006) and the Bayesian segmenter (Eisenstein andBarzilay, 2008).
The authors have made Java imple-mentations publicly available.
For the Minimum Cutsegmenter, we select the best parameters using thescript included with that distribution.
The Bayesiansegmenter automatically estimates all necessary pa-rameters from the data.Preprocessing and the choice of similarity met-ric.
As described in Section 4, the APS algorithmtakes as inputs a matrix of pairwise similarities be-tween sentences in the document and also, for eachsentence, a preference value.This paper focuses on comparing globally in-formed segmentation algorithms, and leaves for fu-ture work the exploration of best similarity metrics.To allow fair comparison, then, we use the samemetric as the Minimum Cut segmenter, cosine sim-ilarity.
Each sentence is represented as a vector oftoken-type frequencies.
Following (Malioutov andBarzilay, 2006), the frequency vectors are smoothedby adding counts of words from the adjacent sen-tences and then weighted using a tf.idf metric (fordetails, see ibid.)
The similarity between sentencevectors s1 and s2 is computed as follows:cos(s1, s2) =s1 ?
s2||s1|| ?
||s2||(20)The representation used by the Bayesian segmenteris too different to be incorporated into our model di-rectly, but ultimately it is based on the distributionof unigrams in documents.
This is close enough toour representation to allow fair comparison.The fiction dataset consists of books: novels orcollections of short stories.
Fiction is known to ex-hibit less lexical cohesion.
That is why ?
whenworking on this dataset ?
we work at the paragraphlevel: the similarity is measured not between sen-tences but between paragraphs.
We use this repre-sentation with all three segmenters.All parameters have been fine-tuned on the devel-opment portions of the datasets.
For APS algorithmper se we needed to set three parameters: the size ofthe sliding window for similarity computations, thedampening factor ?
and the preference values.
The291BayesSeg MinCutSeg APSAI 0.443 0.437 0.404Clinical 0.353 0.382 0.371Fiction 0.377 0.381 0.350Table 1: Results of segmenting the three datasets us-ing the Bayesian segmenter, the Minimum Cut seg-menter and APS.parameters for the similarity metric (best variationof tf.idf, the window size and the decay factor forsmoothing) were set using the script provided in theMinimum Cut segmenter?s distribution.Evaluation metric.
We have measured the per-formance of the segmenters with the WindowDiffmetric (Pevzner and Hearst, 2002).
It is computedby sliding a window through reference and throughsegmentation output and, at each window position,comparing the number of reference breaks to thenumber of breaks inserted by the segmenter (hypo-thetical breaks).
It is a penalty measure which re-ports the number of windows where the referenceand hypothetical breaks do not match, normalizedby the total number of windows.
In Equation 21,ref and hyp denote the number of reference and hy-pothetical segment breaks within a window.winDiff = 1N ?
kN?k?i=1(|ref ?
hyp| 6= 0) (21)6 Experimental Results and DiscussionTable 1 compares the performance of the three seg-menters using WindowDiff values.
On the lectureand fiction datasets, the APS segmenter outperformsthe others by a small margin, around 8% over thebetter of the two.
It is second-best on the clinicaltextbook dataset.
According to a one-tailed pairedt-test with 95% confidence cut-off, the improvementis statistically significant only on the fiction dataset.All datasets are challenging and the baselines arevery competitive, so drawing definitive conclusionsis difficult.
Still, we can be fairly confident thatAPS performs at least as well as the other two seg-menters.
It also has certain advantages.One important difference between APS and theother segmenters is that APS does not require thenumber of segments as an input parameter.
This isvery helpful, because such information is generallyunavailable in any realistic deployment setting.
Theparameters are fine-tuned to maximize WindowDiffvalues, so this results in high-precision, low-recallsegment assignments; that is because WindowDifffavours missing boundaries over near-hits.APS also outputs segment centres, thus providingsome information about a segment?s topic.
We havenot evaluated how descriptive the segment centresare; this is left for future work.APS performs slightly better than the other seg-menters but not by much.
We hypothesize that oneof the reasons is that APS relies on the presence ofdescriptive segment centres which are not necessar-ily present for large, coarse-grained segments suchas chapters in novels.
It is possible for APS to havean advantage performing fine-grained segmentation.7 Conclusions and Future WorkIn this paper we have presented APS ?
a new algo-rithm for linear text segmentation.
APS takes intoaccount the global structure of the document andoutputs segment boundaries and segment centres.
Itscales linearly in the number of input sentences, per-forms competitively with the state-of-the-art and iseasy to implement.
We also provide a Java imple-mentation of the APS segmenter.We consider two main directions for future work:using more informative similarity metrics and mak-ing the process of segmentation hierarchical.
Wechose to use cosine similarity primarily to allow faircomparison and to judge the algorithm itself, in iso-lation from the information it uses.
Cosine similarityis a very simple metric which cannot provide an ad-equate picture of topic fluctuations in documents.
Itis likely that dictionary-based or corpus-based simi-larity measures would yield a major improvement inperformance.Reliance on descriptive segment centres mayhandicap APS?s performance when looking forcoarse-grained segments.
One possible remedy is tolook for shorter segments first and then merge them.One can also modify the algorithm to perform hier-archical segmentation: consider net similarity withlow-level segment centres as well as with high-levelones.
We plan to explore both possibilities.292AcknowledgementsWe thank Inmar Givoni for explaining the detailsof binary Affinity Propagation and for comment-ing on our early ideas in this project.
Many thanksto Yongyi Mao for a helpful discussion on the useAffinity Propagation for text segmentation.ReferencesBishop, Christopher M. 2006.
Pattern Recognition andMachine Learning.
Springer.Blei, David and Pedro Moreno.
2001.
Topic Segmenta-tion with an Aspect Hidden Markov Model.
In Pro-ceedings of the 24th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 343?348.
ACM Press.Blei, David M., Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Choi, Freddy Y. Y.
2000.
Advances in Domain Inde-pendent Linear Text Segmentation.
In Proceedings ofNAACL, pages 26?33.Eisenstein, Jacob and Regina Barzilay.
2008.
BayesianUnsupervised Topic Segmentation.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 334?343, Honolulu,Hawaii, October.Frey, Brendan J. and Delbert Dueck.
2007.
Clusteringby Passing Messages Between Data Points.
Science,315:972?976.Givoni, Inmar E. and Brendan J. Frey.
2009.
A BinaryVariable Model for Affinity Propagation.
Neural Com-putation, 21:1589?1600.Haghighi, Aria and Lucy Vanderwende.
2009.
Explor-ing Content Models for Multi-Document Summariza-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 362?370, Boulder, Colorado, June.Hearst, Marti A.
1997.
TextTiling: segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23:33?64, March.Kschischang, Frank R., Brendan J. Frey, and Hans-ALoeliger.
2001.
Factor graphs and the sum-productalgorithm.
In IEEE Transactions on Information The-ory, Vol 47, No 2, pages 498?519, February.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of ICML-01, pages 282?289.Malioutov, Igor and Regina Barzilay.
2006.
MinimumCut Model for Spoken Lecture Segmentation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages 25?32, Sydney, Australia, July.Misra, Hemant, Franc?ois Yvon, Joemon M. Jose, andOlivier Cappe?.
2009.
Text segmentation via topicmodeling: an analytical study.
In 18th ACM Con-ference on Information and Knowledge Management,pages 1553?1556.Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-GilJang.
2007.
Semantic passage segmentation based onsentence topics for question answering.
InformationSciences, an International Journal, 177:3696?3717,September.Pearl, Judea.
1982.
Reverend Bayes on inference en-gines: A distributed hierarchical approach.
In Pro-ceedings of the American Association of Artificial In-telligence National Conference on AI, pages 133?136,Pittsburgh, PA.Pevzner, Lev and Marti A. Hearst.
2002.
A Critique andImprovement of an Evaluation Metric for Text Seg-mentation.
Computational Linguistics, 28(1):19?36.Stoyanov, Veselin and Claire Cardie.
2008.
Topic identi-fication for fine-grained opinion analysis.
In COLING?08 Proceedings of the 22nd International Conferenceon Computational Linguistics - Volume 1, pages 817?824.Youmans, Gilbert.
1991.
A new tool for discourse anal-ysis: The vocabulary-management profile.
Language,67(4):763?789.293
