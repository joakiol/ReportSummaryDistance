Integrated Shallow and Deep Parsing: TopP meets HPSGAnette Frank, Markus Beckerz, Berthold Crysmann, Bernd Kiefer and Ulrich Scha?ferDFKI GmbH School of Informaticsz66123 Saarbru?cken, Germany University of Edinburgh, UKfirstname.lastname@dfki.de M.Becker@ed.ac.ukAbstractWe present a novel, data-driven methodfor integrated shallow and deep parsing.Mediated by an XML-based multi-layerannotation architecture, we interleave arobust, but accurate stochastic topologicalfield parser of German with a constraint-based HPSG parser.
Our annotation-basedmethod for dovetailing shallow and deepphrasal constraints is highly flexible, al-lowing targeted and fine-grained guidanceof constraint-based parsing.
We conductsystematic experiments that demonstratesubstantial performance gains.11 IntroductionOne of the strong points of deep processing (DNLP)technology such as HPSG or LFG parsers certainlylies with the high degree of precision as well asdetailed linguistic analysis these systems are ableto deliver.
Although considerable progress has beenmade in the area of processing speed, DNLP systemsstill cannot rival shallow and medium depth tech-nologies in terms of throughput and robustness.
Asa net effect, the impact of deep parsing technologyon application-oriented NLP is still fairly limited.With the advent of XML-based hybrid shallow-deep architectures as presented in (Grover and Las-carides, 2001; Crysmann et al, 2002; Uszkoreit,2002) it has become possible to integrate the addedvalue of deep processing with the performance androbustness of shallow processing.
So far, integrationhas largely focused on the lexical level, to improveupon the most urgent needs in increasing the robust-ness and coverage of deep parsing systems, namely1This work was in part supported by a BMBF grant to theDFKI project WHITEBOARD (FKZ 01 IW 002).lexical coverage.
While integration in (Grover andLascarides, 2001) was still restricted to morphologi-cal and PoS information, (Crysmann et al, 2002) ex-tended shallow-deep integration at the lexical levelto lexico-semantic information, and named entityexpressions, including multiword expressions.
(Crysmann et al, 2002) assume a vertical,?pipeline?
scenario where shallow NLP tools provideXML annotations that are used by the DNLP systemas a preprocessing and lexical interface.
The per-spective opened up by a multi-layered, data-centricarchitecture is, however, much broader, in that it en-courages horizontal cross-fertilisation effects amongcomplementary and/or competing components.One of the culprits for the relative inefficiency ofDNLP parsers is the high degree of ambiguity foundin large-scale grammars, which can often only be re-solved within a larger syntactic domain.
Within a hy-brid shallow-deep platform one can take advantageof partial knowledge provided by shallow parsers topre-structure the search space of the deep parser.
Inthis paper, we will thus complement the efforts madeon the lexical side by integration at the phrasal level.We will show that this may lead to considerable per-formance increase for the DNLP component.
Morespecifically, we combine a probabilistic topologicalfield parser for German (Becker and Frank, 2002)with the HPSG parser of (Callmeier, 2000).
TheHPSG grammar used is the one originally developedby (Mu?ller and Kasper, 2000), with significant per-formance enhancements by B. Crysmann.In Section 2 we discuss the mapping probleminvolved with syntactic integration of shallow anddeep analyses and motivate our choice to combinethe HPSG system with a topological parser.
Sec-tion 3 outlines our basic approach towards syntacticshallow-deep integration.
Section 4 introduces vari-ous confidence measures, to be used for fine-tuningof phrasal integration.
Sections 5 and 6 report onexperiments and results of integrated shallow-deepparsing, measuring the effect of various integra-tion parameters on performance gains for the DNLPcomponent.
Section 7 concludes and discusses pos-sible extensions, to address robustness issues.2 Integrated Shallow and Deep ProcessingThe prime motivation for integrated shallow-deepprocessing is to combine the robustness and effi-ciency of shallow processing with the accuracy andfine-grainedness of deep processing.
Shallow analy-ses could be used to pre-structure the search space ofa deep parser, enhancing its efficiency.
Even if deepanalysis fails, shallow analysis could act as a guideto select partial analyses from the deep parser?s chart?
enhancing the robustness of deep analysis, and theinformativeness of the combined system.In this paper, we concentrate on the usage of shal-low information to increase the efficiency, and po-tentially the quality, of HPSG parsing.
In particu-lar, we want to use analyses delivered by an effi-cient shallow parser to pre-structure the search spaceof HPSG parsing, thereby enhancing its efficiency,and guiding deep parsing towards a best-first analy-sis suggested by shallow analysis constraints.The search space of an HPSG chart parser canbe effectively constrained by external knowledgesources if these deliver compatible partial subtrees,which would then only need to be checked for com-patibility with constituents derived in deep pars-ing.
Raw constituent span information can be usedto guide the parsing process by penalizing con-stituents which are incompatible with the precom-puted ?shape?.
Additional information about pro-posed constituents, such as categorial or featuralconstraints, provide further criteria for prioritis-ing compatible, and penalising incompatible con-stituents in the deep parser?s chart.An obvious challenge for our approach is thus toidentify suitable shallow knowledge sources that candeliver compatible constraints for HPSG parsing.2.1 The Shallow-Deep Mapping ProblemHowever, chunks delivered by state-of-the-art shal-low parsers are not isomorphic to deep syntacticanalyses that explicitly encode phrasal embeddingstructures.
As a consequence, the boundaries ofdeep grammar constituents in (1.a) cannot be pre-determined on the basis of a shallow chunk analy-sis (1.b).
Moreover, the prevailing greedy bottom-upprocessing strategies applied in chunk parsing do nottake into account the macro-structure of sentences.They are thus easily trapped in cases such as (2).
(1) a.
[CLThere was [NPa rumor [CLit was goingto be bought by [NPa French company [CLthatcompetes in supercomputers]]]]].b.
[CLThere was [NPa rumor]] [CLit was goingto be bought by [NPa French company]] [CLthat competes in supercomputers].
(2) Fred eats [NPpizza and Mary] drinks wine.In sum, state-of-the-art chunk parsing does nei-ther provide sufficient detail, nor the required accu-racy to act as a ?guide?
for deep syntactic analysis.2.2 Stochastic Topological ParsingRecently, there is revived interest in shallow anal-yses that determine the clausal macro-structure ofsentences.
The topological field model of (German)syntax (Ho?hle, 1983) divides basic clauses into dis-tinct fields ?
pre-, middle-, and post-fields ?
delim-ited by verbal or sentential markers, which consti-tute the left/right sentence brackets.
This model ofclause structure is underspecified, or partial as tonon-sentential constituent structure, but provides atheory-neutral model of sentence macro-structure.Due to its linguistic underpinning, the topologi-cal field model provides a pre-partitioning of com-plex sentences that is (i) highly compatible withdeep syntactic analysis, and thus (ii) maximally ef-fective to increase parsing efficiency if interleavedwith deep syntactic analysis; (iii) partiality regardingthe constituency of non-sentential material ensuresrobustness, coverage, and processing efficiency.
(Becker and Frank, 2002) explored a corpus-based stochastic approach to topological field pars-ing, by training a non-lexicalised PCFG on a topo-logical corpus derived from the NEGRA treebank ofGerman.
Measured on the basis of hand-correctedPoS-tagged input as provided by the NEGRA tree-bank, the parser achieves 100% coverage for length 40 (99.8% for all).
Labelled precision and recallare around 93%.
Perfect match (full tree identity) isabout 80% (cf.
Table 1, disamb +).In this paper, the topological parser was provideda tagger front-end for free text processing, using theTnT tagger (Brants, 2000).
The grammar was portedto the efficient LoPar parser of (Schmid, 2000).
Tag-ging inaccuracies lead to a drop of 5.1/4.7 percent-CL-V2VF-TOPIC LK-VFIN MF RK-VPART NFART NN VAFIN ART ADJA NN VAPP CL-SUBCLDer,1 Zehnkampf,2 ha?tte,3 eine,4 andere,5 Dimension,6 gehabt,7 ,The decathlon would have a other dimension had LK-COMPL MF RK-VFINKOUS PPER PROAV VAPP VAFINwenn,9 er,10 dabei,11 gewesen,12 wa?re,13 .if he there been had .<TOPO2HPSG type=?root?
id=?5608?><MAP CONSTR id=?T1?
constr=?v2 cp?
confent=?0.87?
left=?W1?
right=?W13?/><MAP CONSTR id=?T2?
constr=?v2 vf?
confent=?0.87?
left=?W1?
right=?W2?/><MAP CONSTR id=?T3?
constr=?vfronted vfin+rk?
confent=?0.87?
left=?W3?
right=?W3?/><MAP CONSTR id=?T6?
constr=?vfronted rk-complex?
confent=?0.87?
left=?W7?
right=?W7?/><MAP CONSTR id=?T4?
constr=?vfronted vfin+vp+rk?
confent=?0.87?
left=?W3?
right=?W13?/><MAP CONSTR id=?T5?
constr=?vfronted vp+rk?
confent=?0.87?
left=?W4?
right=?W13?/><MAP CONSTR id=?T10?
constr=?extrapos rk+nf?
confent=?0.87?
left=?W7?
right=?W13?/><MAP CONSTR id=?T7?
constr=?vl cpfin compl?
confent=?0.87?
left=?W9?
right=?W13?/><MAP CONSTR id=?T8?
constr=?vl compl vp?
confent=?0.87?
left=?W10?
right=?W13?/><MAP CONSTR id=?T9?
constr=?vl rk fin+complex+finlast?
confent=?0.87?
left=?W12?
right=?W13?/></TOPO2HPSG>DerDZehnkampfN?NP-NOM-SGhaetteVeineDandereAP-ATTDimensionN?N?NP-ACC-SGgehabtVEPSwennCerNP-NOM-SGdabeiPPgewesenVwaereV-LEVVSCP-MODEPSEPSEPS/NP-NOM-SGS/NP-NOM-SGSFigure 1: Topological tree w/param.
cat., TOPO2HPSG map-constraints, tree skeleton of HPSG analysisdis- cove- perfect LP LR 0CB 2CBamb rage match in % in % in % in %+ 100.0 80.4 93.4 92.9 92.1 98.9  99.8 72.1 88.3 88.2 87.8 97.9Table 1: Disamb: correct (+) / tagger ( ) PoS input.Eval.
on atomic (vs. parameterised) category labels.age points in LP/LR, and 8.3 percentage points inperfect match rate (Table 1, disamb  ).As seen in Figure 1, the topological trees abstractaway from non-sentential constituency ?
phrasalfields MF (middle-field) and VF (pre-field) directlyexpand to PoS tags.
By contrast, they perfectly ren-der the clausal skeleton and embedding structure ofcomplex sentences.
In addition, parameterised cate-gory labels encode larger syntactic contexts, or ?con-structions?, such as clause type (CL-V2, -SUBCL,-REL), or inflectional patterns of verbal clusters (RK-VFIN,-VPART).
These properties, along with theirhigh accuracy rate, make them perfect candidates fortight integration with deep syntactic analysis.Moreover, due to the combination of scramblingand discontinuous verb clusters in German syntax, adeep parser is confronted with a high degree of localambiguity that can only be resolved at the clausallevel.
Highly lexicalised frameworks such as HPSG,however, do not lend themselves naturally to a top-down parsing strategy.
Using topological analyses toguide the HPSG will thus provide external top-downinformation for bottom-up parsing.3 TopP meets HPSGOur work aims at integration of topological andHPSG parsing in a data-centric architecture, whereeach component acts independently2 ?
in contrastto the combination of different syntactic formalismswithin a unified parsing process.3 Data-based inte-gration not only favours modularity, but facilitatesflexible and targeted dovetailing of structures.3.1 Mapping Topological to HPSG StructuresWhile structurally similar, topological trees are notfully isomorphic to HPSG structures.
In Figure 1,e.g., the span from the verb ?ha?tte?
to the end of thesentence forms a constituent in the HPSG analysis,while in the topological tree the same span is domi-nated by a sequence of categories: LK, MF, RK, NF.Yet, due to its linguistic underpinning, the topo-logical tree can be used to systematically predictkey constituents in the corresponding ?target?
HPSG2See Section 6 for comparison to recent work on integratedchunk-based and dependency parsing in (Daum et al, 2003).3As, for example, in (Duchier and Debusmann, 2001).analysis.
We know, for example, that the span fromthe fronted verb (LK-VFIN) till the end of its clauseCL-V2 corresponds to an HPSG phrase.
Also, thefirst position that follows this verb, here the leftmostdaughter of MF, demarcates the left edge of the tra-ditional VP.
Spans of the vorfeld VF and clause cat-egories CL exactly match HPSG constituents.
Cate-gory CL-V2 tells us that we need to reckon with afronted verb in position of its LK daughter, here 3,while in CL-SUBCL we expect a complementiser inthe position of LK, and a finite verb within the rightverbal complex RK, which spans positions 12 to 13.In order to communicate such structural con-straints to the deep parser, we scan the topologicaltree for relevant configurations, and extract the spaninformation for the target HPSG constituents.
Theresulting ?map constraints?
(Fig.
1) encode a brackettype name4 that identifies the target constituent andits left and right boundary, i.e.
the concrete span inthe sentence under consideration.
The span is en-coded by the word position index in the input, whichis identical for the two parsing processes.5In addition to pure constituency constraints, askilled grammar writer will be able to associate spe-cific HPSG grammar constraints ?
positive or neg-ative ?
with these bracket types.
These additionalconstraints will be globally defined, to permit fine-grained guidance of the parsing process.
This andfurther information (cf.
Section 4) is communicatedto the deep parser by way of an XML interface.3.2 Annotation-based IntegrationIn the annotation-based architecture of (Crysmannet al, 2002), XML-encoded analysis results of allcomponents are stored in a multi-layer XML chart.The architecture employed in this paper improveson (Crysmann et al, 2002) by providing a centralWhiteboard Annotation Transformer (WHAT) thatsupports flexible and powerful access to and trans-formation of XML annotation based on standardXSLT engines6 (see (Scha?fer, 2003) for more de-tails on WHAT).
Shallow-deep integration is thusfully annotation driven.
Complex XSLT transforma-tions are applied to the various analyses, in order to4We currently extract 34 different bracket types.5We currently assume identical tokenisation, but could ac-commodate for distinct tokenisation regimes, using map tables.6Advantages we see in the XSLT approach are (i) minimisedprogramming effort in the target implementation language forXML access, (ii) reuse of transformation rules in multiple mod-ules, (iii) fast integration of new XML-producing components.extract or combine independent knowledge sources,including XPath access to information stored inshallow annotation, complex XSLT transformationsto the output of the topological parser, and extractionof bracket constraints.3.3 Shaping the Deep Parser?s Search SpaceThe HPSG parser is an active bidirectional chartparser which allows flexible parsing strategies by us-ing an agenda for the parsing tasks.7 To compute pri-orities for the tasks, several information sources canbe consulted, e.g.
the estimated quality of the parti-cipating edges or external resources like PoS taggerresults.
Object-oriented implementation of the prior-ity computation facilitates exchange and, moreover,combination of different ranking strategies.
Extend-ing our current regime that uses PoS tagging for pri-oritisation,8 we are now utilising phrasal constraints(brackets) from topological analysis to enhance thehand-crafted parsing heuristic employed so far.Conditions for changing default priorities Ev-ery bracket pair brxcomputed from the topologicalanalysis comes with a bracket type x that defines itsbehaviour in the priority computation.
Each brackettype can be associated with a set of positive and neg-ative constraints that state a set of permissible or for-bidden rules and/or feature structure configurationsfor the HPSG analysis.The bracket types fall into three main categories:left-, right-, and fully matching brackets.
A right-matching bracket may affect the priority of taskswhose resulting edge will end at the right bracketof a pair, like, for example, a task that wouldcombine edges C and F or C and D in Fig.
2.Left-matching brackets work analogously.
For fullymatching brackets, only tasks that produce an edgethat matches the span of the bracket pair can be af-fected, like, e.g., a task that combines edges B and Cin Fig.
2.
If, in addition, specified rule as well as fea-ture structure constraints hold, the task is rewardedif they are positive constraints, and penalised if theyare negative ones.
All tasks that produce crossingedges, i.e.
where one endpoint lies strictly inside thebracket pair and the other lies strictly outside, arepenalised, e.g., a task that combines edges A and B.This behaviour can be implemented efficientlywhen we assume that the computation of a task pri-7A parsing task encodes the possible combination of a pas-sive and an active chart edge.8See e.g.
(Prins and van Noord, 2001) for related work.brxbrxAB CD EFFigure 2: An example chart with a bracket pair oftype x.
The dashed edges are active.ority takes into account the priorities of the tasks itbuilds upon.
This guarantees that the effect of chang-ing one task in the parsing process will propagateto all depending tasks without having to check thebracket conditions repeatedly.For each task, it is sufficient to examine the start-and endpoints of the building edges to determine ifits priority is affected by some bracket.
Only fourcases can occur:1.
The new edge spans a pair of brackets: a match2.
The new edge starts or ends at one of the brack-ets, but does not match: left or right hit3.
One bracket of a pair is at the joint of the build-ing edges and a start- or endpoint lies strictlyinside the brackets: a crossing (edges A and Bin Fig.
2)4.
No bracket at the endpoints of both edges: usethe default priorityFor left-/right-matching brackets, a match behavesexactly like the corresponding left or right hit.Computing the new priority If the priority of atask is changed, the change is computed relative tothe default priority.
We use two alternative confi-dence values, and a hand-coded parameter(x), toadjust the impact on the default priority heuristics.confent(brx) specifies the confidence for a concretebracket pair brxof type x in a given sentence, basedon the tree entropy of the topological parse.
confprspecifies a measure of ?expected accuracy?
for eachbracket type.
Sec.
4 will introduce these measures.The priority p(t) of a task t involving a bracketbrxis computed from the default priority ~p(t) by:p(t) = ~p(t)  (1 confent(brx)  confpr(x) (x))4 Confidence MeasuresThis way of calculating priorities allows flexible pa-rameterisation for the integration of bracket con-straints.
While the topological parser?s accuracy ishigh, we need to reckon with (partially) wrong anal-yses that could counter the expected performancegains.
An important factor is therefore the confi-dence we can have, for any new sentence, into thebest parse delivered by the topological parser: Ifconfidence is high, we want it to be fully consideredfor prioritisation ?
if it is low, we want to lower itsimpact, or completely ignore the proposed brackets.We will experiment with two alternative confi-dence measures: (i) expected accuracy of particularbracket types extracted from the best parse deliv-ered, and (ii) tree entropy based on the probabilitydistribution encountered in a topological parse, asa measure of the overall accuracy of the best parseproposed ?
and thus the extracted brackets.94.1 Confpr: Accuracy of map-constraintsTo determine a measure of ?expected accuracy?
forthe map constraints, we computed precision and re-call for the 34 bracket types by comparing the ex-tracted brackets from the suite of best deliveredtopological parses against the brackets we extractedfrom the trees in the manually annotated evalua-tion corpus in (Becker and Frank, 2002).
We obtain88.3% precision, 87.8% recall for brackets extractedfrom the best topological parse, run with TnT frontend.
We chose precision of extracted bracket typesas a static confidence weight for prioritisation.Precision figures are distributed as follows: 26.5%of the bracket types have precision  90% (93.1%in avg, 53.5% of bracket mass), 50% have pre-cision  80% (88.9% avg, 77.7% bracket mass).20.6% have precision  50% (41.26% in avg, 2.7%bracket mass).
For experiments using a thresholdon confpr(x) for bracket type x, we set a thresholdvalue of 0.7, which excludes 32.35% of the low-confidence bracket types (and 22.1% bracket mass),and includes chunk-based brackets (see Section 5).4.2 Confent: Entropy of Parse DistributionWhile precision over bracket types is a static mea-sure that is independent from the structural complex-ity of a particular sentence, tree entropy is defined asthe entropy over the probability distribution of theset of parsed trees for a given sentence.
It is a use-ful measure to assess how certain the parser is aboutthe best analysis, e.g.
to measure the training utilityvalue of a data point in the context of sample selec-tion (Hwa, 2000).
We thus employ tree entropy as a9Further measures are conceivable: We could extract brack-ets from some n-best topological parses, associating them withweights, using methods similar to (Carroll and Briscoe, 2002).10203040506070809000.20.40.60.81in%Normalized entropyprecisionrecallcoverageFigure 3: Effect of different thresholds of normal-ized entropy on precision, recall, and coverageconfidence measure for the quality of the best topo-logical parse, and the extracted bracket constraints.We carry out an experiment to assess the effectof varying entropy thresholds  on precision and re-call of topological parsing, in terms of perfect matchrate, and show a way to determine an optimal valuefor .
We compute tree entropy over the full prob-ability distribution, and normalise the values to bedistributed in a range between 0 and 1.
The normali-sation factor is empirically determined as the highestentropy over all sentences of the training set.10Experimental setup We randomly split the man-ually corrected evaluation corpus of (Becker andFrank, 2002) (for sentence length  40) into a train-ing set of 600 sentences and a test set of 408 sen-tences.
This yields the following values for the train-ing set (test set in brackets): initial perfect matchrate is 73.5% (70.0%), LP 88.8% (87.6%), and LR88.5% (87.8%).11 Coverage is 99.8% for both.Evaluation measures For the task of identifyingthe perfect matches from a set of parses we give thefollowing standard definitions: precision is the pro-portion of selected parses that have a perfect match?
thus being the perfect match rate, and recall is theproportion of perfect matches that the system se-lected.
Coverage is usually defined as the proportionof attempted analyses with at least one parse.
We ex-tend this definition to treat successful analyses witha high tree entropy as being out of coverage.
Fig.
3shows the effect of decreasing entropy thresholds on precision, recall and coverage.
The unfilteredset of all sentences is found at =1.
Lowering  in-10Possibly higher values in the test set will be clipped to 1.11Evaluation figures for this experiment are given disregard-ing parameterisation (and punctuation), corresponding to thefirst row of figures in table 1.82848688909294960.160.180.20.220.240.260.280.3in%Normalized entropyprecisionrecallf-measureFigure 4: Maximise f-measure on the training set todetermine best entropy thresholdcreases precision, and decreases recall and coverage.We determine f-measure as composite measure ofprecision and recall with equal weighting (=0.5).Results We use f-measure as a target function onthe training set to determine a plausible .
F-measureis maximal at =0.236 with 88.9%, see Figure 4.Precision and recall are 83.7% and 94.8% resp.while coverage goes down to 83.0%.
Applying thesame  on the test set, we get the following results:80.5% precision, 93.0% recall.
Coverage goes downto 80.6%.
LP is 93.3%, LR is 91.2%.Confidence Measure We distribute the comple-ment of the associated tree entropy of a parse tree tras a global confidence measure over all brackets brextracted from that parse: confent(br) = 1 ent(tr).For the thresholded version of confent(br), we setthe threshold to 1   = 1  0:236 = 0:764.5 ExperimentsExperimental Setup In the experiments we usethe subset of the NEGRA corpus (5060 sents,24.57%) that is currently parsed by the HPSG gram-mar.12 Average sentence length is 8.94, ignoringpunctuation; average lexical ambiguity is 3.05 en-tries/word.
As baseline, we performed a run with-out topological information, yet including PoS pri-oritisation from tagging.13 A series of tests exploresthe effects of alternative parameter settings.
We fur-ther test the impact of chunk information.
To this12This test set is different from the corpus used in Section 4.13In a comparative run without PoS-priorisation, we estab-lished a speed-up factor of 1.13 towards the baseline used inour experiment, with a slight increase in coverage (1%).
Thiscompares to a speed-up factor of 2.26 reported in (Daum et al,2003), by integration of PoS guidance into a dependency parser.end, phrasal fields determined by topological pars-ing were fed to the chunk parser of (Skut and Brants,1998).
Extracted NP and PP bracket constraints aredefined as left-matching bracket types, to compen-sate for the non-embedding structure of chunks.Chunk brackets are tested in conjunction with topo-logical brackets, and in isolation, using the labelledprecision value of 71.1% in (Skut and Brants, 1998)as a uniform confidence weight.14Measures For all runs we measure the absolutetime and the number of parsing tasks needed to com-pute the first reading.
The times in the individualruns were normalised according to the number ofexecuted tasks per second.
We noticed that the cov-erage of some integrated runs decreased by up to1% of the 5060 test items, with a typical loss ofaround 0.5%.
To warrant that we are not just tradingcoverage for speed, we derived two measures fromthe primary data: an upper bound, where we asso-ciated every unsuccessful parse with the time andnumber of tasks used when the limit of 70000 pas-sive edges was hit, and a lower bound, where weremoved the most expensive parses from each run,until we reached the same coverage.
Whereas theupper bound is certainly more realistic in an applica-tion context, the lower bound gives us a worst caseestimate of expectable speed-up.Integration Parameters We explored the follow-ing range of weighting parameters for prioritisation(see Section 3.3 and Table 2).We use two global settings for the heuristic pa-rameter.
Settingto 12without using any confi-dence measure causes the priority of every affectedparsing task to be in- or decreased by half its value.Settingto 1 drastically increases the influence oftopological information, the priority for rewardedtasks is doubled and set to zero for penalized ones.The first two runs (rows with  P  E) ignoreboth confidence parameters (confpr=ent=1), measur-ing only the effect of higher or lower influence oftopological information.
In the remaining six runs,the impact of the confidence measures confpr=entistested individually, namely +P  E and  P +E, bysetting the resp.
alternative value to 1.
For two runs,we set the resp.
confidence values that drop belowa certain threshold to zero (PT, ET) to exclude un-14The experiments were run on a 700 MHz Pentium III ma-chine.
For all runs, the maximum number of passive edges wasset to the comparatively high value of 70000.factor msec (1st) taskslow-b up-b low-b up-b low-b up-bBaseline     524 675 3813 4749Integration of topological brackets w/ parameters P  E122.21 2.17 237 310 1851 2353 P  E1 2.04 2.10 257 320 2037 2377+P  E122.15 2.21 243 306 1877 2288PT  E122.20 2.30 238 294 1890 2268 P +E122.27 2.23 230 302 1811 2330 P ET122.10 2.00 250 337 1896 2503+P  E1 2.06 2.12 255 318 2021 2360PT  E1 2.08 2.10 252 321 1941 2346PT with chunk and topological bracketsPT  E122.13 2.16 246 312 1929 2379PT with chunk brackets onlyPT  E120.89 1.10 589 611 4102 4234Table 2: Priority weight parameters and resultscertain candidate brackets or bracket types.
For runsincluding chunk bracketing constraints, we chosethresholded precision (PT) as confidence weightsfor topological and/or chunk brackets.6 Discussion of ResultsTable 2 summarises the results.
A high impact onbracket constraints (1) results in lower perfor-mance gains than using a moderate impact (12)(rows 2,4,5 vs. 3,8,9).
A possible interpretation isthat for high, wrong topological constraints andstrong negative priorities can mislead the parser.Use of confidence weights yields the best per-formance gains (with12), in particular, thresholdedprecision of bracket types PT, and tree entropy+E, with comparable speed-up of factor 2.2/2.3 and2.27/2.23 (2.25 if averaged).
Thresholded entropyET yields slightly lower gains.
This could be due toa non-optimal threshold, or the fact that ?
while pre-cision differentiates bracket types in terms of theirconfidence, such that only a small number of brack-ets are weakened ?
tree entropy as a global measurepenalizes all brackets for a sentence on an equal ba-sis, neutralizing positive effects which ?
as seen in+/ P ?
may still contribute useful information.Additional use of chunk brackets (row 10) leadsto a slight decrease, probably due to lower preci-sion of chunk brackets.
Even more, isolated use ofchunk information (row 11) does not yield signifi-010002000300040005000600070000 5 10 15 20 25 30 35baseline+PT ?
(0.5)12867 12520 11620 92900100200300400500600#sentencesmsecFigure 5: Performance gain/loss per sentence lengthcant gains over the baseline (0.89/1.1).
Similar re-sults were reported in (Daum et al, 2003) for inte-gration of chunk- and dependency parsing.15For PT -E12, Figure 5 shows substantial per-formance gains, with some outliers in the range oflength 25?36.
962 sentences (length >3, avg.
11.09)took longer parse time as compared to the baseline(with 5% variance margin).
For coverage losses, weisolated two factors: while erroneous topological in-formation could lead the parser astray, we also foundcases where topological information prevented spu-rious HPSG parses to surface.
This suggests thatthe integrated system bears the potential of cross-validation of different components.7 ConclusionWe demonstrated that integration of shallow topo-logical and deep HPSG processing results in signif-icant performance gains, of factor 2.25?at a highlevel of deep parser efficiency.
We show that macro-structural constraints derived from topological pars-ing improve significantly over chunk-based con-straints.
Fine-grained prioritisation in terms of con-fidence weights could further improve the results.Our annotation-based architecture is now easilyextended to address robustness issues beyond lexicalmatters.
By extracting spans for clausal fragmentsfrom topological parses, in case of deep parsing fail-15(Daum et al, 2003) report a gain of factor 2.76 relative to anon-PoS-guided baseline, which reduces to factor 1.21 relativeto a PoS-prioritised baseline, as in our scenario.ure the chart can be inspected for spanning anal-yses for sub-sentential fragments.
Further, we cansimplify the input sentence, by pruning adjunct sub-clauses, and trigger reparsing on the pruned input.ReferencesM.
Becker and A. Frank.
2002.
A Stochastic TopologicalParser of German.
In Proceedings of COLING 2002,pages 71?77, Taipei, Taiwan.T.
Brants.
2000.
Tnt - A Statistical Part-of-Speech Tag-ger.
In Proceedings of Eurospeech, Rhodes, Greece.U.
Callmeier.
2000.
PET ?
A platform for experimenta-tion with efficient HPSG processing techniques.
Nat-ural Language Engineering, 6 (1):99 ?
108.C.
Carroll and E. Briscoe.
2002.
High precision extrac-tion of grammatical relations.
In Proceedings of COL-ING 2002, pages 134?140.B.
Crysmann, A. Frank, B. Kiefer, St. Mu?ller, J. Pisko-rski, U. Scha?fer, M. Siegel, H. Uszkoreit, F. Xu,M.
Becker, and H.-U.
Krieger.
2002.
An IntegratedArchitecture for Deep and Shallow Processing.
InProceedings of ACL 2002, Pittsburgh.M.
Daum, K.A.
Foth, and W. Menzel.
2003.
ConstraintBased Integration of Deep and Shallow Parsing Tech-niques.
In Proceedings of EACL 2003, Budapest.D.
Duchier and R. Debusmann.
2001.
Topological De-pendency Trees: A Constraint-based Account of Lin-ear Precedence.
In Proceedings of ACL 2001.C.
Grover and A. Lascarides.
2001.
XML-based datapreparation for robust deep parsing.
In Proceedings ofACL/EACL 2001, pages 252?259, Toulouse, France.T.
Ho?hle.
1983.
Topologische Felder.
Unpublishedmanuscript, University of Cologne.R.
Hwa.
2000.
Sample selection for statistical gram-mar induction.
In Proceedings of EMNLP/VLC-2000,pages 45?52, Hong Kong.S.
Mu?ller and W. Kasper.
2000.
HPSG analysis ofGerman.
In W. Wahlster, editor, Verbmobil: Founda-tions of Speech-to-Speech Translation, Artificial Intel-ligence, pages 238?253.
Springer, Berlin.R.
Prins and G. van Noord.
2001.
Unsupervised pos-tagging improves parsing accuracy and parsing effi-ciency.
In Proceedings of IWPT, Beijing.U.
Scha?fer.
2003.
WHAT: An XSLT-based Infrastruc-ture for the Integration of Natural Language Process-ing Components.
In Proceedings of the SEALTS Work-shop, HLT-NAACL03, Edmonton, Canada.H.
Schmid, 2000.
LoPar: Design and Implementation.IMS, Stuttgart.
Arbeitspapiere des SFB 340, Nr.
149.W.
Skut and T. Brants.
1998.
Chunk tagger: statisticalrecognition of noun phrases.
In ESSLLI-1998 Work-shop on Automated Acquisition of Syntax and Parsing.H.
Uszkoreit.
2002.
New Chances for Deep LinguisticProcessing.
In Proceedings of COLING 2002, pagesxiv?xxvii, Taipei, Taiwan.
