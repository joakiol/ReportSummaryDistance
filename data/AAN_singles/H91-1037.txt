Partial Parsing:A Report on Work in ProgressRalph Weischedel, Damaris Ayuso, R. Bobrow, Sean Boisen,Robert lngria, Jeff PalmucciBBN Systems and Technologies10 Moulton St.Cambridge, MA 02138AbstractThis paper eports a handful of experiments designed to test thefeasibility of applying well-known partial parsing techniques to theproblem of automatic data base update from an open-ended sourceof messages, and the feasiblity of automatically learning semanticknowledge from annotated examples.
The challenges arise fromthe incompleteness of any lexicon, sentences that average over 20words in length, and the lack of a complete semantics.IntroductionTraditionally natural language processing (NLP) has focussed onobtaining complete syntactic analyses of all input and on semanticanalysis based on handcrafted knowledge.
However, grammars areincomplete; text often contains new words; and there are errors intext.
Furthermore, as research activities tackle broader domainsand if the research results are to scale up to realistic applications,handcrafting knowledge must give way to automatic knowledgebase construction.An alternative totraditional parsers is represented in FIDDITCH(Hindle, 1983), MITFP (deMarcken 1990), and CASS (Abney,1990).
Instead of requiring complete parses, a forest is frequentlyproduced, each tree in the forest representing a non-overlappingfragment of the input.
However, algorithms for finding thesemantics of the whole from the disjoint fragments have notpreviously been developed nor evaluated.We are comparing several differing algorithms from varioussites to evaluate both the effectiveness of such a strategy incorrectly predicting fragments and the effectiveness ofsyntactic/semantic algorithms for combining fragments.
Onequestion our research is trying to answer is how well the linguisticexpression of entities and the relational structures among them canbe recovered for data base update without determining lobalsyntactic structure and without full information regarding thevocabulary items.
A second question is how well an algorithm tolearn lexical semantic knowledge from examples will perform.Application ContextFor message processing, insistence on complete syntacticanalysis is usually unnecessary, since much of the input isn'tdirectly relevant o updating a data base, routing a message, orprioritizing it.An example article from the Third Message UnderstandingConference (MUC-3), illustrates how complete analysis isunnecessary; the first sixteen paragraphs relate the results of asummit between the presidents of Peru and Bolivia.
Thoseparagraphs would not add anything to the MUC-3 data base onterrorist acts.
However, the final two sentences of the articlemention, almost incidentally, that a bomb exploded near thesummit, and therefore, do provide data to be added to the terrorismdata base.Even at the sentential level, one is not likely to be able toreliably compute a full semantic interpretation.
For instance, in thesentence below from the aforementioned article, only the materialin italics actually contributes to the desired, pre-defined data baseupdate.A BOMB EXPLODED TODAY AT DAWN IN THE PERUVIANTOWN OF YUNGUYO, NEAR THE LAKE, VERY NEARWHERE THE PRESIDENTIAL SUMMIT WAS TO TAKEPLACE.In a task such as MUC-3 the goal is to identify pre-definedclasses of entities, e.g., terrorist events, and dates, and therelationships among them, e.g., the perpetrator f a given terroristact.
Below, we have listed the first seven of nineteen pre-specifiedclasses of data to be extracted from the messages of MUC-3.0.
MESSAGE ID: identifier1.
TEMPLATE ID: identifier2.
DATE OF INCIDENT: date3.
TYPE OF INCIDENT: set element e.g., KIDNAPPING,ATI'EMPTED KIDNAPPING, KIDNAPPING THREAT ....4.
CATEGORY OF INCIDENT: set element, e.g, TERRORISTACT, STATE-SPONSORED VIOLENCE5.
PERPETRATOR: ID OF INDIV(S): a string6.
PERPETRATOR: ID OF ORG(S): a stringSystem Architecture AssumptionsFor the purposes of this discussion, we assume a fairly standardsystem architecture as shown in Figure 1 below.
We furtherassume that a domain model exists for the pre-specified data to beextracted.
That is, every class of entides of importance is specifiedin a frame representation indicating subclass-superclassrelationships and all other important binary relationships amongthem.Processing sentences to the point of finding semanticinterpretations is the topic of this paper.
Pilot experiments are204reported here on alternative algorithms to find interpretablefragments even when no global syntactic or semantic analysis canbe found.
We are particularly exploring probabilistic models forthis processing, and have described experiments with variousprobabilistie models elsewhere (Aynso, et al 1990; Meteer, et al,1991).The discourse component has two roles.
One is resolvingreferences.
The second role is to use hypotheses regarding whatdomain-specific events are being described in each paragraph orarticle.
Particular events correspond totemplates.
Once a templatehas been hypothesized, and once all text has been processed, if thetemplate requires (or expects) certain information that has not yetbeen found, the discourse processor looks for values of the rightsemantic type and plausible within the discourse structure of thearticle.
This process will be described elsewhere.The output emplates consist of three types of fields: set fill (apre-specified finite set of alternatives), string (a literal,uninterpreted string from the input), and denumerably infiniteentities (integers, dates, identifiers, etc.
).NaturalLanguageInput1MorphologicalProcessingJSemanticInterpretationDiscourseProcessing1I Application Mapping1Output?
Template?
SQL?
ProgramI I Algorithmic componentO Knowledge BaseFigure 1individuals, and organizations of primary interest in the domain.Normally these entities appear as noun phrases in the text.Therefore, abasic concern is to reliably identify noun phrases thatdenote ntities of interest, even if neither full syntactic nor fullsemantic analysis is possible.Two of our experiments have focussed on the identification ofcore noun phrases, a primary way of expressing entities in text.
Acore NP is defined syntactically as the maximal simple nounphrase, i.e., the largest one containing no post-modifiers.
Here aresome examples of core NPs (marked by italics) within their fullnoun phrases:a jo int  venture with the Chinese government to build anautomobile-parts as embly planta $50.9 mill ion loss from discontinued operations in the thirdquarter because of the proposed saleSuch complex, full NPs require too many linguistic decisions tobe directly processed without detailed syntactic and semanticknowledge about each word, an assumption which need not be truefor open-ended text.We tested two differing algorithms on text from the Wall StreetJournal (WSJ).
Using BBN's part of speech tagger (POST), taggedtext was parsed using the full unification grammar of Delphi to fredonly core NPs, 695 in 100 sentences.
Hand-scoring of the resultsindicated that 85% of the core NPs were identified correctly.Subsequent analysis suggested that half the errors could beremoved with only a little additional work, suggesting that over90% performance is achievable.In a related test, we explored the bracketings produced byChurch's PARTS program (Church, 1988).
We extracted 200sentences of WSJ text by taking every tenth sentence from acollection of manually corrected parse trees (data from theTREEBANK Project at the University of Pennsylvania).
Weevaluated the NP bracketings in these 200 sentences by hand, andtried to classify the errors.
Of 1226 phrases in the 200 sentences,131 were errors, for a 10.7% error rate.
The errors were classifiedby hand as follows:?
Two consecutive but unrelated phrases grouped as one: 10?
Phrase consisted of a single word, which was not an NP: 70?
Missed phrases (those that should have been bracketed but werenot): 12?
Ellided head (e.g.
part of a conjoined premodifier toan NP): 4?
Missed premodffiers: 4?
Head of phrase was verb form that was missed: 4?
Other: 27The 90% success rate in both tests suggests that identification fcore NPs can be achieved using only local information and withminimal knowledge of the words.
Next we consider the issue ofwhat semantics should be assigned and how reliably that can beaccomplished.Finding Core Noun PhrasesIn a task such as MUC-3 one fundamental application goal is toidentify pre-defined classes of entities, e.g., dates, locations,Semantics of Core Noun PhrasesIn trying to extract pre-specified data from open-ended text suchas a newswire, it is clear that full semantic interpretation f suchtexts is not on the horizon.
However, our hypothesis that it need205not be for automatic data base update.
The type of information tobe extracted permits some partial understanding.
For semanticprocessing, minimally, for each noun phrase (NP), one would liketo identify the class in the domain model that is the smallest pre-defined class containing the NPs denotation.
For each clause, onewould like to identify the corresponding event class or state ofaffairs denoted.Our pilot experiment focussed on the reliability of identifyingthe minimal c ass for each noun phrase.Assigning asemantic lass to a core noun phrase can be handledvia some structural rules.
Usually the semantic lass of the headword is correct for the semantic lass not only of the core nounphrase but also of the complete noun phrase it is part of.Additional rules cover exceptions, such as "set of ...".
Theseheuristics correctly predicted the semantic lass of the whole nounphrase 99% of the time in the sample of over 1000 noun phrasesfrom WSJ that were correctly predicted by Church's PARTSprogram.Furthermore, ven some of the NP's whose left boundary wasnot predicted correctly by PARTS, nevertheless were assigned thecorrect semantic lass.
One consequence of this is that the correctsemantic lass of a complex noun phrase can be predicted even ifsome of the words in the noun phrase are unknown and even ff itsfull structure is unknown.
Thus, fully correct identification ofcorenoun phrase boundaries and noun phrase boundaries may not benecessary to accurately produce data base updates.Finding Relations/Combining FragmentsThough finding the entities of interest is fundamental tothe task,finding relationships of interest among them is also critical.
Forinstance, in MUC-3 one must identify terrorist events in any ofnine Latin American countries, the perpetrators of the event, thevictims, if any, the date, the location, any structural damage, and soon.The experiments reported above were run by mid-summer, 1990.In fall, 1990, a more complete alternative, the MIT Fast Parser(MITFP), became available to us.
It finds fragments using astochastic part of speech algorithm and a nearly deterministicparser.
It produces fragments averaging 3-4 words in length.
Anexample output follows.
(S (NP (DETERMINER "A") (N "BOMB"))(VP (AUX (NP (MONTH "TODAY"))(PP (PREP "AT")(NP (N "DAWN"))))(VP (V "EXPLODED"))))(PP(PP (PREP "IN")(NP (NP (DETERMINER "THE")(N "PERUVIAN")(N "TOWN"))(PP (PREP "OF")(NP (N "YUNGUYO")))))(PUNCT ","))(PP (PP (PREP "NEAR")(NP (DETERMINER "THE")(N "LAKE")))(PUNCT ","))(ADJP (DEGREESPEC "VERY")(ADJP (ADJ "NEAR")))(ADV "WHERE")(NP (DETERMINER "THE")(ADJP (ADJ "PRESIDENTIAL"))(N "SUMMrr"))(VP (AUX) (VP (V "WAS")))(VP (AUX "TO")(VP (V "TAKE")(NP (Y "PLACE"))))(PUNCT ".
")Figure 2 Example Output from MITFPCertain sequences of fragments appear frequently, as illustratedin the tables below.
One frequently occurring pair is an S followedby a PP (prepositional phrase).
Since there is more than one waythe parser could attach the PP, and syntactic grounds alone forattaching the PP would yield poor performance, semanticpreferences applied by a post-process that combines fragments arecalled for.Pair OccurrencesS PP 104NP VP 89VP VP 72S VP 65PP PP 62PP NP 58NP PP 56VP PP 54PP VP 48NP NP 34Table 1 Most Frequently Occurring Pairs (In 2500 Pairs)Triple OccurrencesNP PUNCT NP 53VP PUNCT S 20S PUNCT S 19NP PUNCT S 19S PUNCT NP 17VP PUNCT N 12NP PUNCT PP 10NP PUNCT VP 9Table 2 Frequently Occurring Fragment Pairs SurroundingPunctuationIn our approach, the first step is to compute a semanticinterpretation for each fragment found without assuming that themeaning of each word is known.
For instance, as described above,the semantic lass for any noun phrase can be computed providedthe head noun has semantics in the domain.Based on the data above, a reasonable approach is an algorithmthat moves left-to-right through the set of fragments produced byM1TFP, deciding to attach fragments (or not) based on semantic206criteria.
To avoid requiring acomplete, global analysis, awindowtwo constituents wide is used to fred patterns of possible relationsamong phrases.
For example, an S followed by a PP invokes anaction of finding all points along the "right edge" of the S treewhere a PP could attach, applying the fragment combining patternsat each such spot, and ranking the alternatives.As evident in Table 2, MITFP frequently does not attachpunctuation.
This is to be expected, since punctuation is used inmany ways, and there is no deterministic basis grounds forattaching the constituent following the punctuation to theconstituent preceding it.
Therefore, if the pair being examined bythe combining algorithms ends in punctuation, the algorithm looksat the constituent following it, trying to combine it with theconstituent left of the punctuation.A similar case is when the pair ends in a conjunction.
Here thealgorithm tries to combine the constituent to the right of theconjunction with that on the left of the conjunction.Learning Semantic InformationSince the norm will be that there are several ways to combine apair of fragments, we plan to test several alternative heuristics forranking the alternatives.
Probabilistic methods eem particularlypowerful and appropriate.
Thus far, we have tested this hypothesison propositional phrase attachment.Such semantic kffowledge called selection restrictions or caseframes governs what phrases make sense with a particular verb ornoun (what arguments go with a particular verb or noun).Traditionally such semantic knowledge is handcrafted, thoughsome software aids exist to enable greater productivity (Ayuso etal., 1987; Bates, 1989; Grishman et al, 1986; Weischedel, et al,1989).Instead of handrafting this semantic knowledge, our goal is tolearn that knowledge from examples, using a three step process:1.
Simple manual semantic annotation,2.
Supervised training based on parsed sentences,3.
Estimation of probabilities.Simple Manual Semantic AnnotationGiven a sample of text, we annotate each noun, verb, and propernoun in the sample with the semantic class corresponding to it inthe domain modal.
For instance, dawn would be annotated <time>,explode would be <explosion event>, and Yunguyo would be<city>.
For our experiment, 560 nouns and 170 verbs were definedin this way.
We estimate that this semantic annotation proceededat about 90 words/hour.Supervised TrainingFrom the TREEBANK project at the University of Pennsylvania,we used 20,000 words of MUC-3 texts that had been bracketedaccording to major syntactic ategory.
The bracketed constituentsfor the sentence used in Figure 2 appears in Figure 3 below.
( (s(NP a bomb)(VP explodedtoday(PP at(NP dawn) )(PP in(NP the Peruvian town(PP of(NP yunguyo) ) )(PP near(NP the lake) )(SBAR (WHPP verynear(WHADVP where) )(S (NP the presidential summit)(VP was(s (NP *)to(VP take(NP place) ) ) )  ) ) ) )F igure  3 Example  of  TREEBANK AnalysisFrom the example one can clearly infer that bombs can explode,or more properly, that bomb can be the logical subject of explode,that at dawn can modify explode, etc.
Naturally goodgeneralizations based on the instances are more valuable than theinstances themselves.Since we have a hierarchical domain model, and since themanual semantic annotation states the relationship between lexicalitems and concepts in the domain model, we can use the domainmodel hierarchy as a given set of categories for generalization.However, the critical issue is selecting the right level ofgeneralization given the set of examples in the supervised trainingset.We have chosen aknown statistical procedure (Katz, 1987) thatselects the minimum level of generalization such that there issufficient data in the training set to support discrimination f casesof attaching phrases (arguments) to their head.
This leads us to thenext topic, estimation of probabilities from the supervised trainingset.Estimation of ProbabilitiesThe case relation, or selection restriction, to be learned is of theform X P O, where X is a head word or its semantic lass; P is acase, e.g., logical subject, logical object, a preposition, etc.
; and Ois a head word or its semantic class.One factor in the probability that O attaches to X with case P isp' (X I P, O), an estimate of the likelihood of X given P and O. Wechose to model a second factor p(d)l, the probability of anattachment where d words separate the head word X from thephrase to be attached (intuitively, the notion of attachmentdistance).207Since a 20,000 word corpus is not much data, we used ageneralization algorithm (Katz, 1987) to automatically move up thehierarchical domain model from X to its parent, and from O to itsparent.The ExperimentBy examining the table of triples X P O that were learned, itwasclear that meaningful information was induced from the examples.For instance, \[<attack> against <building>\] and \[<attack> against<residence>\] were learned, which correspond to two cases ofimportance in the MUC domain.However, we ran a far more meaningful evaluation of what waslearned by measuring how effective the learned information wouldbe at predicting 166 prepositional phrase attachments that were notmade by the MITFP.
For example, in Figure 1, fragment 2 couldbe attached syntactically tofragment 1 at three places: modifyingdawn, modifying today, or modifying explode.Closest attachment, a purely syntactic onstraint, worked quiteeffectively, having a 25% error rate.
Using the semanticprobabilities alone p' (X I P, O) had poorer performance, a 34%error rate.
However, the richer probability model p' (X I P, O) *p(d) outperformed beth the purely semantic model and the purelysyntactic model (closest attachment), yielding an 18% error ate.As a consequence, useful semantic information was learned bythe training algorithm.However, the degree of reduction of error rate should not betaken as the final word, for the following reasons:20,000 words of training data is much less than one wouldwant.
An additional 70,000 words of training data shouldsoon be available through TREEBANK.Since many of the head words in the 20,000 word corpus arenot of import in the MUC-3 domain, their semantic type isvague, i.e., <unknown event>, <unknown entity>, etc.Related WorkIn addition to the work discussed earlier on tools to increase theportability of natural anguage systems, another ecent paper(Hindle and Rooth, 1990) is directly related to our goal of inferringcase frame information from examples.Hindle and Rooth focussed only on prepositional phraseattachment using a probabilistic model, whereas our work appliesto all case relations.
Their work used an unsupervised trainingcorpus of 13 million words to judge the strength of prepositionalaffinity to verbs, e.g., how likely it is for to to attach to the wordgo, for from to attach to the word leave, or for to to attach to theword flight.
This lexical affinity is measured independent of theobject of the preposition.
By contrast, we are exploring inductionof semantic relations from supervised training, where very littletraining may be available.
Furthermore, we are looking at triplesof head word (or semantic class), syntactic case, and head word (orsemantic class).In Hindle and Rooth's test, they evaluated their probabilitymodel in the limited case of verb - noun phrase - prepositionalphrase.
Therefore, no model at all would be at least 50% accurate.In our test, many of the test cases involved three or more possibleattachment points fro the prepositional phrase, providing a morerealistic test.An interesting next step would be to combine these twoprobabilistic models (perhaps via linear weights) in order to get thebenefit of domain-specific knowledge, as we have explored, andthe benefits of domain-independent knowledge, as Hindle andRooth have explored.ConclusionsTwo traditional approaches to applying natural languageprocessing techniques are complete syntactic analysis and script-based analysis.
In Proteus (Grishman, 1989), complete syntacticanalysis is applied.
If no complete analysis of a sentence can befound, the largest S found anchored at the left end is analyzed,ignoring whatever occurs to the right.
A second alternative isscript-based analysis e.g., as represented in FRUMP (de Jong,1979).
This technique emphasizes emantic and domainexpectations, minimizing dependence onsyntactic analysis.In our approach to open-ended text processing, there are threesteps:1.
Probabilistically based syntactic analysis produces aforest of non-overlapping fragments, if no single tree canbe found.2.
A semantic interpreter assigns emantic representationsto the trees of the forest.3.
Fragments are combined using a probability modelreflexting both syntactic and semantic preferences.However, the most innovative aspect of our approach is theautomatic induction of semantic knowledge from annotatedexamples.
The use of probabilistic models offers the inductionprocedure a decision criterion for making generalizations from thecorpus of examples.The partial parsing approach offers an alternative.
By findingfragments based only on syntactic knowledge, and by starting anew fragment when a constituent cannot be detenninisticallyattached, one has some partial analysis of the whole input.
How tocompute semantic analysis for any constituent is well understood inany compositional semantics.
An algorithm that combines thesemantically interpreted fragments eems to gain the power ofsemantically guided analysis without sacrificing syntactic analysis.Fragments that cannot be combined can still be employed withdiscourse processing and script-based expectations to identify theentities and relations among them for data base update.Our pilot experiments indicate that the approach to textprocessing and the induction algorithm are both feasible andpromising.AcknowledgementsThe work reported here was supported by the AdvancedResearch Projects Agency and was monitored by the Rome Air208Development Center under Contract No.
F30602-87-D-0093.
Theviews and conclusions contained in this document are those of theauthors and should not be interpreted as necessarily representingthe official policies, either expressed or implied, of the DefenseAdvanced Research Projects Agency or the United StatesGovernment.ReferencesAbney, S., Rapid Incremental Parsing with Repair, Proceedings ofthe Sixth Annual Conference of the UW Centre for the NewOxford English Dictionary and Text Research: Electronic TextResearch,.
UW Centre for the New Oxford English Dictionary andText Research, 1990, 1-9.Ayuso, D. et al, Towards Understanding Text with a Very LargeVocabulary., Proceedings of the Speech and Natural LanguageWorkshop, Morgan Kaufman Publishers, Inc., 1990, 354-358.Ayuso, D.M., Shaked, V., and Weischedel, R.M.
An Environmentfor Acquiring Semantic Information.
In Proceedings ofthe 25thAnnual Meeting of the Association for Computational Linguistics,pages 32-40.
ACL, 1987.Bates, M. Rapid Porting of the Parlance TM Natural LanguageInterface.
Proceedings of the Speech and Natural LanguageWorkshop, Morgan Kaufmann Publishers, Inc., pages 83-88, 1989.Church, K. A Stochastic Parts Program and Noun Phrase Parser forUnrestricted Text.
Proceedings of the Second Conference onApplied Natural Language Processing, ACL, 1988, 136-143.Church, K., Gale, W.A., Hanks, P., and Hindle, D. Parsing, WordAssociations and Typical Predicate-Argument Relations.Proceedings ofthe Speech and Natural Language Workshop Oct.1989, 75-81.De Jong, G.F.
Skimming Stories in Real Time: An Experiment inIntegrated Understanding.
Yale University, Research Report No.158, May 1979.de Marcken, C.G.
Parsing the LOB Corpus.
Proceedings ofthe28th Annual Meeting of the Association for ComputationalLinguistics 1990, 243-251.Grishman, R., and Sterling, J.
Preference Semantics for MessageUnderstanding.
Proceedings ofthe Speech and Natural LanguageWorkshop Oct. 1989, 71-74.Grishman, R., Hirschman, L., Nhan, N.T., Discovery Proceduresfor Sublanguage Selectional Patterns: Initial Experiments,Computational Linguistics, 1986, 205 -215.Hindle, D., and Rooth, M. Structural Ambiguity and LexicalRelations.
Proceedings of the Speech and Natural LanguageWorkshop, Morgan Kaufman Publishers, Inc., 1990, 257-262.Katz, S.M.
Estimation of Probabilities from Sparse Data for theLanguage Model Component of a Speech Recognizer.
IEEETransactions on Acoustics, Speech, and Signal Processing, Vol.ASSP-35 No.
3, March 1987.Meteer, M., Schwartz, R., and Weischedel, R. Empirical Studies inPart of Speech Labelling., Proceedings ofthe Speech and NaturalLanguage Workshop, 1991, this volume.Stallard, D. Unification-Based Semantic Interpretation in the BBNSpoken Language System.
Proceedings ofthe Speech and NaturalLanguage Workshop Oct. 1989, 39-46.Weischedel, R.M., Bobrow, R., Ayuso, D.M., and Ramshaw, L.Portability in the Janus Natural Language Interface.
In Speech andNatural Language, pages 112-117.
Morgan Kaufmarm PublishersInc., Oct. 1989.209
