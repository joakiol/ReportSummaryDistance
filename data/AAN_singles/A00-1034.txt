A Hybrid Approach for Named Entity and Sub-Type Tagging*Rohini SrihariCymfony Net, Inc.5500 Main StreetWilliamsville, NY 14260rohini @ cymfony.comCheng Niu and Wei LiCymfony Net, Inc.5500 Main StreetWilliamsville, NY 14260chengniu@cymfony.comwei@cymfony.comAbstractThis paper presents a hybrid approach fornamed entity (NE) tagging which combinesMaximum Entropy Model (MaxEnt), HiddenMarkov Model (HMM) and handcraftedgrammatical rules.
Each has innate strengthsand weaknesses; the combination results in avery high precision tagger.
MaxEnt includesexternal gazetteers in the system.
Sub-categorygeneration is also discussed.IntroductionNamed entity (NE) tagging is a task in whichlocation names, person names, organizationnames, monetary amounts, time and percentageexpressions are recognized and classified inunformatted text documents.
This task providesimportant semantic information, and is a criticalfirst step in any information extraction system.Intense research has been focused onimproving NE tagging accuracy using severaldifferent echniques.
These include rule-basedsystems \[Krupka 1998\], Hidden Markov Models(HMM) \[Bikel et al 1997\] and MaximumEntropy Models (MaxEnt) \[Borthwick 1998\].
Asystem based on manual rules may provide thebest performance; however these requirepainstaking intense skilled labor.. Furthermore,shifting domains involves significant effort andmay result in performance degradation.
Thestrength of HMM models lie in their capacity formodeling local contextual information.
HMMshave been widely used in continuous peechrecognition, part-of-speech tagging, OCR, etc.,and are generally regarded as the most successfulstatistical modelling paradigm in these domains.MaxEnt is a powerful tool to be used in situationswhere several ambiguous information sourcesneed to be combined.
Since statistical techniquessuch as HMM are only as good as the data theyare trained on, they are required to use back-offmodels to compensate for unreliable statistics.
Incontrast o empirical back-off models used inHMMs, MaxEnt provides a systematic methodby which a statistical model consistent with allobtained knowledge can be trained.
\[Borthwicket al 1998\] discuss atechnique for combining theoutput of several NE taggers in a black boxfashion by using MaxEnt.
They demonstrate thesuperior performance of this system; however,the system is computationally inefficient sincemany taggers need to be run.In this paper we propose ahybrid method forNE tagging which combines all the modellingtechniques mentioned above.
NE tagging is acomplex task and high-performance systems arerequired in order to be practically usable.Furthermore, the task demonstratescharacteristics that can be exploited by all threetechniques.
For example, time and monetaryexpressions are fairly predictable and henceprocessed most efficiently with handcraftedgrammar rules.
Name, location and organizationentities are highly variable and thus lendthemselves tostatistical training algorithms suchas HMMs.
Finally, many conflicting pieces ofinformation regarding the class of a tag are* This work was supported in part by the SBIR grant F30602-98-C-0043 from Air Force Research Laboratory(AFRL)/IFED.247frequently present.
This includes informationfrom less than perfect gazetteers.
For this, aMaxEnt approach works well in utilizing diversesources of information in determining the finaltag.
The structure of our system is shown inFigure 1.I II to=~m, I~mmI Il I o , ,=- .
.
, II MJE3;,= G,,: ;,IE1I~1 Sb~lued hE Tasp"The first module is a rule-based taggercontaining pattern match rules, or templates, fortime, date, percentage, and monetaryexpressions.
These tags include the standardMUC tags \[Chinchor 1998\], as well as severalother sub-categories defined by our organization.More details concerning the sub-categories arepresented later.
The pattern matcher is based onFinite State Transducer (FST) technology\[Roches & Schabes 1997\] that has beenimplemented in-house.
The subsequent modulesare focused on location, person and organizationnames.
The second module assigns tentativeperson and location tags based on external personand location gazetteers.
Rather than relying onsimple lookup of the gazetteer which is very errorprone, this module employs MaxEnt to build astatistical model that incorporates gazetteers withcommon contextual information.
The coremodule of the system is a bigram-based HMM\[Bikel et a1.1997\].
Rules designed to correcterrors in NE segmentation are incorporated into aconstrained HMM network.
These rules serve asconstraints on the HMM model and enable it toutilize information beyond bigrams and removeobvious errors due to the limitation of the trainingcorpus.
HMM generates the standard MUC tags,person, location and organization.
Based onMaxEnt, the last module derives sub-categoriessuch as city, airport, government, etc.
from thebasic tags.Section 1 describes the FST rule module.Section 2 discusses combining gazetteerinformation using MaxEnt.
The constrainedHMM is described in Section 3.
Section 4discusses ub-type generation by MaxEnt.
Theexperimental results and conclusion arepresented finally.1 FST-based Pattern Matching Rules forTextract NEThe most attractive feature of the FST (FiniteState Transducer) formalism lies in its superiortime and space efficiency \[Mohri 1997\] \[Roche& Schabes 1997\].
Applying a deterministic FSTdepends linearly only on the input size of the text.Our experiments also show that an FST rulesystem is extraordinarily robust.
In addition, ithas been verified by many research programs\[Krupka & Hausman 1998\] \[Hobbs 1993\]\[Silberztein 1998\] \[Srihari 1998\] \[Li & Srihari2000\], that FST is also a convenient tool forcapturing linguistic phenomena, especially foridioms and semi-productive expressions like timeNEs and numerical NEs.The rules which we have currentlyimplemented include a grammar for temporalexpressions (time, date, duration, frequency, age,etc.
), a grammar for numerical expressions(money, percentage, length, weight, etc.
), and agrammar for other non-MUC NEs (e.g.
contactinformation like address, email).The following sample pattern rules give anidea of what our NE grammars look like.
Theserules capture typical US addresses, like: 5500Main St., Williamsville, NY14221; 12345 XyzAvenue, Apt.
678, Los Angeles, CA98765-4321.The following notation is used: @ for macro; Ifor logical OR; + for one or more; (...) foroptionality.9 -~-number =uppercase =0111213141516171819@0_9+AIBICIDIEIFIGIHIIIJIKILIMINIOIPIQIRISITUIVIWIXIYIZ248lowercase = a \[ b \[ c I d \[ e I f l g I h \[i I J I k \[ I Imln lo lp lq l r l s l t lu lv lw\ [x ly l zletter = @uppercase \[ @lowercaseword = @letter+delimiter = (",") .... +zip = @0_9 @0_9 @09 @0_9 @0_9("-" @0_9 @0_9 @0_9 @0_9)street = \[\[St l ST I Rd I RD I Dr I DRIAve\[AVE \] C.")\] I Street\[Road\[Drive\[Avenuecity = @word (@word)state = @uppercase (".")
@uppercase (".
")us-- USA IU.S.AIUSIU.S.I(The) United States (of America)street_addr = @number @word @streetapt_addr = \[APT C.") I Apt (".")
\[Apartment\] @numberlocal_addr = @ street_addr(@delimiter @apt_addr)address = @ local_addr@delimiter @city@delimiter @state @zip(@delimiter @us)Our work is similar to the research on FSTlocal grammars at LADL/University Paris VII\[Silberztein 1998\] 1, but that research was notturned into a functional rule based NE system.The rules in our NE grammars coverexpressions with very predictable patterns.
Theywere designed to address the weaknesses of ourstatistical NE tagger.
For example, the followingmissings (underlined) and mistagging originallymade by our statistical NE tagger have all beencorrectly identified by our temporal NEgrammar.began <TIMEX TYPE="DATE">Dec.
15,the</TIMEX> space agencyon Jan. 28, <TIMEXTYPE="DATE"> 1986</TIMEX>,in September <TIMEXTYPE="DATE">1994</TIMEX>on <TIMEX1 They have made public their esearch results at theirwebsite (http://www.ladl.jussieu.fr/index.html),including a grammar for certain temporal expressionsand a grammar for stock exchange sub-language.TYPE="TIME">Saturday at</TIMEX> 2:42a.m.
ES<ENAMEXTYPE="PERSON">T.</ENAMEX>He left the United States in <TIMEXTYPE="DATE">1984 and</TIMEX> movedin early <TIMEX TYPE="DATE"> 1962and</TIMEX>in <TIMEX TYPE="DATE">1987 theBonn</TIMEX> government ruled2 Incorporating Gazetteers with theMaximum Entropy ModelWe use two gazetteers in our system, one forperson and one for location.
The person gazetteerconsists of 3,000 male names, 5,000 femalenames and 14,000 family names.
The locationgazetteer consists of 250,000 location ames withtheir categories uch as CITY, PROVINCE,COUNTRY, AIRPORT, etc.
The containing andbeing-contained relationship among locations isalso provided.The following is a sample line in the locationgazetteer, which denotes "Aberdeen" as a city in"California", and "California" as a province of"United States".Aberdeen (CITY) California (PROVINCE)United States (COUNTRY)Although gazetteers obviously contain usefulname entity information, a straightforward wordmatch approach may even degrade the systemperformance since the information fromgazetteers i  too ambiguous.
There are a lot ofcommon words that exist in the gazetteers, uchas 'T', "A", "Friday", "June", "Friendship", etc.Also, there is large overlap between personnames and location names, such as "Clinton","Jordan", etc.Here we propose a machine learningapproach to incorporate the gazetteer informationwith other common contextual information basedon MaxEnt.
Using MaxEnt, the system maylearn under what situation the occurrence ingazetteers is a reliable vidence for a name entity.We first define "LFEATURE" based onoccurrence in the location gazetteer as follows:249COUNTRYUSSTATEMULTITOKENof multiple tokens)BIGCITYin OXFD dictionary)COEXIST(country name)(US state name)(a location ame consisting(a location ame occurring(where COEXIST(A,B) istrue iff A and B are in the same US state, or inthe same foreign country)OTHERThere is precedence from the firstLFEATURE to the last one.
Each token in theinput document is assigned a unique"LFEATURE".
We also define "NFEATURE"based on occurrence in the name gazetteer asfollows:FAMILYMALEFEMALEFAMILYANDMALEname)FAMILYANDFEMALEname)OTHER(family name)(male name)(female name)(family and male(family and femaleWith these two extra features, every token inthe document is regarded as a three-componentvector (word, LFEATURE, NFEATURE).
Wecan build a statistical model to evaluate theconditional probability based on these contextualand gazetteer features.
Here "tag" represents oneof the three possible tags (Person, Location,Other), and history represents any possiblecontextual history.
Generally, we have:p (tag, history)tag(1)A maximum entropy solution for probability hasthe form \[Rosenfeld 1994\] \[Ratnaparkhi 1998\]H ~/i (history,tag)p(tag,history) =Z(history)Z(history) = ~.~ 1-I \[~t'ifi(hist?ry'tag )tag i(e)(3)where fi (history, tag) are binary-valued featurefunctions that are dependent on whether thefeature is applicable to the current contextualhistory.
Here is an example of our featurefunction:f(history,tag)={~ ifcurrenttokenisaeountryname, ndtagisloeatiOnotherwise(4)In (2) and (3) a i are weights associated tofeaturefunctions.The weight evaluation scheme is as follows:We first compute the average value of eachfeature function according to a specific trainingcorpus.
The obtained average observations areset as constraints, and the Improved IterativeScaling (IIS) algorithm \[Pietra et al 1995\] isemployed to evaluate the weights.
The resultingprobability distribution (2) possesses themaximum entropy among all the probabilitydistributions consistent with the constraintsimposed by feature function average values.In the training stage, our gazetteer modulecontains two sub-modules: feature functioninduction and weight evaluation \[Pietra et al1995\].
The structure is shown in Figure 2.Rule ~|ect|on Module \[~elect next rule reduce the entropy most"-~ Evaluate weiEht for each Selected ruleIteraUve $?atinB (US) tFig.2, Structure ofMaxEnt learning ProcessWe predefine twenty-four feature functiontemplates.
The following are some examples andothers have similar structures:10 if LFEATURE = , and tag = _ f (history, tag) =else250f(history, tag)={lof(history, tag)={~f(history,tag)={lof(history, tag)={;i f  NFEATURE = _ ,  and  tag  = _e l sei f  cur rent  word  = _ ,  and  tag  = _e l sei f  p rev ious  word  = _ ,  and  tag  = _e l sei f  fo l low ing  word  = _ ,  and  tag  = _e l sewhere the symbol .... denotes any possiblevalues which may be  inserted into that field.Different fields will be filled different values.Then, using a training corpus containing230,000 tokens, we set up a feature functioncandidate space based on the feature functiontemplates.
The "Feature Function InductionModule" can select next feature function thatreduces the Kullback-Leibler divergence themost \[Pietra et al 1995\].
To make the weightevaluation computation tractable at the featurefunction induction stage, when trying a newfeature function, all previous computed weightsare held constant, and we only fit one newconstraint hat is imposed by the candidatefeature function.
Once the next feature functionis selected, we recalculate the weights by IIS tosatisfy all the constraints, and thus obtain the nexttentative probability.
The feature functioninduction module will stop when theLog-likelihood gain is less than a pre-setthreshold.The gazetteer module recognizes the personand location names in the document despite thefact that some of them may be embedded in anorganization ame.
For example, "New YorkFire Department" may be tagged as<LOCATION> New York </NE> FireDepartment.
In the input stream for HMM, eachtoken being tagged as location is accordinglytransformed into one of the built-in tokens"CITY", "PROVINCE", "COUNTRY".
TheHMM may group "CITY Fire Department" intoan organization ame.
A similar technique isapplied for person names.Since the tagged tokens from the gazetteermodule are regarded by later modules as eitherperson or location names, we require that thecurrent module generates results with the highestpossible precision.
For each tagged token we willcompute the entropy of the answer.
If the entropyis higher than a pre-set hreshold, the system willnot be certain enough about the answer, and theword will be untagged.
The missed location orperson names may be recognized by thefollowing HMM module.3 Improving NE Segmentation throughconstrained HMMOur original HMM is similar to the Nymble\[Bikel et al 1997\] system that is based on bigramstatistics.
To correct some of the leading errors,we incorporate manual segmentation rules withHMM.
These syntactic rules may provideinformation beyond bigram and balance thelimitation of the training corpus.Our manual rules focus on improving the NEsegmentation.
For example, in the tokensequence "College of William and Mary", wehave rules based on global sequence checking todetermine if the words "and" or "of" are commonwords or parts of organization name.The output of the rules are some constraintson the HMM transition etwork, such as "sametags for tokens A, B", or "common word fortoken A".
The Viterbi algorithm will select heoptimized path that is consistent with suchconstraints.The manual rules are divided into threecategories: (i) preposition disambiguation, (ii)spurious capitalized word disambiguation, and(iii) spurious NE sequence disambiguation.The rules of preposition disambiguation areresponsible for determination of boundariesinvolving prepositions ("of", "and", "'s", etc.
).For example, for the sequence "A of B", we havethe following rule: A and B have same tags if thelowercase of A and B both occur in OXFDdictionary.
A "global word sequence checking"\[Mikheev, 1999\] is also employed.
For thesequence "Sprint and MCI", we search thedocument globally.
If the word "Sprint" or251"MCI" occurs individually somewhere lse, wemark "and" as a common word.The rules of spurious capitalized worddisambiguation are designed to recognize thefirst word in the sentence.
If the first word isunknown in the training corpus, but occurs inOXFD as a common word in lowercase, HHM'sunknown word model may be not accurateenough.
The rules in the following paragraph aredesigned to treat such a situation.If the second word of the same sentence is inlowercase, the first word is tagged as a commonword since it never occurs as an isolated NEtoken in the training corpus unless it has beenrecognized as a NE elsewhere in the document.If the second word is capitalized, we will checkglobally if the same sequence occurs somewhereelse.
If so, the HMM is constrained to assign thesame tag to the two tokens.
Otherwise, thecapitalized token is tagged as a common word.The rules of spurious NE sequencedisambiguation are responsible for findingspurious NE output from HMM, addingconstraints, and re-computing NE by HMM.
Forexample, in a sequence "Person Organization",we will require the same output ag for these twotokens and run HMM again.4 NE Sub-Type Tagging using MaximumEntropy ModelThe output document from constrained HMMcontains MUC-standard NE.tags such as person,location and organization.
However, for a realinformation extraction system, theMUC-standard NE tag may not be enough andfurther detailed NE information might benecessary.
We have predefined the followingsub-types for person, location and organization:Person: Military PersonReligious PersonManWomanLocation: CityProvinceCountryContinentLakeRiverMountainRoadRegionDistrictAirportOrganization: CompanyGovernmentArmySchoolAssociationMass MediumIf a NE is not covered by any of the abovesub-categories, it should remain a MUC-standardtag.
Obviously, the sub-categorization requiresmuch more information beyond bigram thanMUC-standard tagging.
For example, it is hardto recognize CNN as a Mass Media company bybigram if the token "CNN" never occurs in thetraining corpus.
External gazetteer information iscritical for some sub-category recognition, andtrigger word models may also play an importantrole.With such considerations, we use theMaximum entropy model for sub-categorization,since MaxEnt is powerful enough to incorporateinto the system gazetteer or other informationsources which might become available at somelater time.Similar to the gazetteer module in Section 2,the sub-categorization module in the trainingstage contains two sub-modules, (i) featurefunction induction and (ii) weight evaluation.We have the following seven feature functiontemplates:10 if MUC_tag = _, and tag = _f (history, tag) = else{ 10 if MUC_tag = _, LFEATURE = _, and tag = _f (history, tag) = else1 if contain word(__), MUC tag(history) = _,and tag =f (history, tag ) = - - -0 else10 if Previous_Word = _, MUC_tag = _,and tag = _f (history, tag)= elsef(history, tag)= {10 if following_Word= _,MUC_tag = _ ,ande lse  tag=_f(history, tag)={lo i fMUC_tag= ,contain_male_name, and tag252= 1l  if  ,oc_ta  =_,co.
.in_fema,e_.ame,a.d,ag=_ f (history, tag ) to elseWe have trained 1,000 feature functions bythe feature function induction module accordingto the above templates.Because much more external gazetteerinformation is necessary for thesub-categorization and there is an overlapbetween male and female name gazetteers, theresult from the current MaxEnt module is notsufficiently accurate.
Therefore, a conservativestrategy has been applied.
If the entropy of theoutput answer is higher than a threshold, we willback-off to the MUC-standard tags.
UnlikeMUC NE categories, local contextualinformation is not sufficient forsub-categorization.
In the future more externalgazetteers focusing on recognition ofgovernment, company, army, etc.
will beincorporated into our system.
And we areconsidering using trigger words \[Rosenfeld,1994\] to recognize some sub-categories.
Forexample, "psalms" may be a trigger word for"religious person", and "Navy" may be a triggerword for "military person".Experiment and ConclusionWe have tested our system on MUC-7 dry rundata; this data consists of 22,000 words andrepresents articles from The New York Times.Since a key was provided with the data, it ispossible to properly evaluate the performance ofour NE tagger.
The scoring program computesboth the precision and recall, and combines thesetwo measures into f-measure as the weightedharmonic mean \[Chinchor, 1998\].
The formulasare as follows:number of correct responses Precision =number esponsesnumber of  correct responsesRecall =number correct in keyF - (/32 + 1)Precision * Recall(f l2Recall) + PrecisionThe score of our system is as follows:Recall PrecisionOrganization 95 95Person 96Location 96Percentage9394Date 92 91Time 92 91Money 100 86100 75F-measure =93.39If the gazetteer module is removed from oursystem, and the constrained HMM is restored tothe standard HMM, the f-measures for person,location, and organization are as follows:Recall PrecisionOrganization 94 92Person 95 91Location 95 92Obviously, our gazetteer model andconstrained HMM have greatly increased thesystem accuracy on the recognition of persons,locations, and organizations.
Currently, there aresome errors in our gazetteers.
Some commonwords such as "Changes", "USER","Administrator", etc.
are mistakenly included inthe person name gazetteer.
Also, too manyperson names are included into the locationgazetteer.
By cleaning up the gazetteers, we cancontinue improving the precision on person nameand locations.We also ran our NE tagger on the formal testfiles of MUC-7.
The following are the results:Recall PrecisionPerson 92 95Organization 85 86Location 90 92Date 95 85253Time 79 72Money 95 82Percentage 97 80-Overall F-measure 89There is some performance degradation ithe formal test.
This decrease is because that theformal test is focused on satellite and rocketdomains in which our system has not beentrained.
There are some person/location namesused as spacecraft or robot names (ex.
Mir, Alvin,Columbia...), and there are many high-techcompany names which do not occur in our HMMtraining corpus.
Since the finding of organizationnames totally relies on the HMM model, it suffersmost from domain shift (10% degradation).
Thisdifference implies that gazetteer information maybe useful in overcoming the domain dependency.This paper has demonstrated improvedperformance in an NE tagger by combiningsymbolic and statistical approaches.
MaxEnt hasbeen demonstrated to be a viable technique forintegrating diverse sources of information andhas been used in NE sub-categorization.A.
Ratnaparkhi, Maximum Entropy Models forNatural Language Ambiguity resolution, PHDthesis, Univ.
of Pennsylvania, (1998)S. D. Pietra, Vincent Della Pietra, and John Lafferty,Inducing Features of Random Fields, Tech Report,Carnegie Mellon University, (1995)A. Mikheev, A Knowledge-free Method forCapitalized Word Disambiguation, in Proceedingsof the 37th Annual Meeting of the Association forComputational Linguistics, (1999), pp.
159-166J.
R. Hobbs, 1993.
FASTUS: A System for ExtractingInformation from Text, Proceedings ofthe DARPAworkshop on Human Language Technology",Princeton, NJ, pp.
133-137.Emmanuel Roche & Yves Schabes, 1997.
Finite-StateLanguage Processing, The MIT Press, Cambridge,MA.Li, W & Srihari, R. 2000.
Flexible InformationExtraction Learning Algorithm, Final TechnicalReport, Air Force Research Laboratory, RomeResearch Site, New YorkM.
Silberztein, 1998.
Tutorial Notes: Finite StateProcessing with INTEX, COLING-ACL'98,Montreal (also available athttp://www.ladl.\] ussieu.fr)M. Mohri,.
1997.
Finite-State Transducers inLanguage and Speech Processing, ComputationalLinguistics, Vol.
23, No.
2, pp.
269-311.R.
Srihari, 1998.
A Domain Independent EventExtraction Toolkit, AFRL-IF-RS-TR-1998-152Final Technical Report, Air Force ResearchLaboratory, Rome Research Site, New YorkReferencesG.
R Krupka and K. Hausman, "IsoQuest Inc:Description of the NetOwl "Fext Extraction Systemas used for MUC-7" in Proceedings of SeventhMachine Understanding Conference (MUC-7)(1998)D. M. Bikel, "Nymble: a high-performance learningname-finder" in Proceedings of the FifthConference on Applied Natural LanguageProcessing, 1997, pp.
194-201, Morgan KaufmannPublishers.A.
Borthwick, et al, Description of the MENE namedEntity System, In Proceedings of the SeventhMachine Understanding Conference (MUC-7)(1998)R. Rosenfeld, Adaptive Statistical language Modeling,PHD thesis, Carnegie Mellon University, (1994)254
