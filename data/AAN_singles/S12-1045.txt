First Joint Conference on Lexical and Computational Semantics (*SEM), pages 340?346,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsFBK: Exploiting Phrasal and Contextual Cluesfor Negation Scope DetectionMd.
Faisal Mahbub Chowdhury ?
??
Fondazione Bruno Kessler (FBK-irst), Trento, Italy?
University of Trento, Italychowdhury@fbk.euAbstractAutomatic detection of negation cues alongwith their scope and corresponding negatedevents is an important task that could bene-fit other natural language processing (NLP)tasks such as extraction of factual informationfrom text, sentiment analysis, etc.
This paperpresents a system for this task that exploitsphrasal and contextual clues apart from vari-ous token specific features.
The system wasdeveloped for the participation in the Task 1(closed track) of the *SEM 2012 Shared Task(Resolving the Scope and Focus of Negation),where it is ranked 3rd among the participatingteams while attaining the highest F1 score fornegation cue detection.1 IntroductionNegation is a linguistic phenomenon that can al-ter the meaning of a textual segment.
While auto-matic detection of negation expressions (i.e.
cues)in free text has been a subject of research interestfor quite some time (e.g.
Chapman et al (2001),Elkin et al (2005) etc), automatic detection of fullscope of negation is a relatively new topic (Moranteand Daelemans, 2009; Councill et al, 2010).
Detec-tion of negation cues, their scope and correspondingnegated events in free text could improve accuracy inother natural language processing (NLP) tasks suchas extraction of factual information from text, senti-ment analysis, etc (Jia et al, 2009; Councill et al,2010).In this paper, we present a system that was de-veloped for the participation in the Scope Detectiontask of the *SEM 2012 Shared Task1.
The proposedsystem exploits phrasal and contextual clues apartfrom various token specific features.
Exploitationof phrasal clues is not new for negation scope de-tection.
But the way we encode this information(i.e.
the features for phrasal clues) is novel and dif-fers completely from the previous work (Councill etal., 2010; Morante and Daelemans, 2009).
More-over, the total number of features that we use is alsocomparatively lower.
Furthermore, to the best of ourknowledge, automatic negated event/property iden-tification has not been explored prior to the *SEM2012 Shared Task.
So, our proposed approach forthis particular sub-task is another contribution of thispaper.The remainder of this paper is organised as fol-lows.
First, we describe the scope detection taskas well as the accompanying datasets in Section 2.Then in Section 3, we present how we approach thetask.
Following that, in Section 4, various empiri-cal results and corresponding analyses are discussed.Finally, we summarize our work and discuss how thesystem can be further improved in Section 5.2 Task Description: Scope DetectionThe Scope Detection task (Task 1) of *SEM 2012Shared Task deals with intra-sentential (i.e.
con-text is single sentence) negations.
According tothe guidelines of the task (Morante and Daelemans,2012; Morante et al, 2011), the scope of a nega-tion cue(s) is composed of all negated concepts andnegated event/property, if any.
Negation cue(s) is1http://www.clips.ua.ac.be/sem2012-st-neg/340Training Development TestTotal sentence 3644 787 1089Negation sentences 848 144 235Negation cues 984 173 264Cues with scopes 887 168 249Tokens in scopes 6929 1348 1805Negated events 616 122 173Table 1: Various statistics of the training, developmentand test datasets.not considered as part of the scope.
Cues and scopesmay be discontinuous.The organisers provided three sets of data ?
train-ing, development and test datasets, all consisting ofstories by Conan Doyle.
The training dataset con-tains Chapters 1-14 from The Hound of theBaskervilles.
While development datasetcontains The Adventures of WisteriaLodge.
For testing, two other stories, TheAdventure of the Red Circle and TheAdventure of the Cardboard Box, werereleased during the evaluation period of the sharedtask.
Table 1 shows various statistics regarding thedatasets.In the training and development data, all occur-rences of negation are annotated.
For each negationcue, the cue and corresponding scope are marked,as well as the negated event/property, if any.
Thedata is provided in CoNLL-2005 Shared Task for-mat.
Table 2 shows an example of annotated datawhere ?un?
is the negation cue, ?his own conven-tional appearance?
is the scope, and ?conventional?is the negated property.The test data has a format similar to the trainingdata except that only the Columns 1?7 (as shown inTable 2) are provided.
Participating systems have tooutput the remaining column(s).During a random checking we have found at least2 missing annotations2 in the development data.
So,there might be few wrong/missing annotations in theother datasets, too.There were two tracks in the task.
For the closed2Annotations for the following negation cues (and their cor-responding scope/negated events) in the development data aremissing ?
{cue: ?no?, token no.
: 8, sentence no.
: 237, chap-ter: wisteria01} and {cue: ?never?, token no.
: 3, sentence no.
:358, chapter: wisteria02}.track, systems have to be built strictly with infor-mation contained in the given training corpus.
Thisincludes the automatic annotations that the organiz-ers provide for different levels of analysis (POS tags,lemmas and parse trees).
For the open track, sys-tems can be developed making use of any kind ofexternal tools and resources.We participated in the closed track of the scopedetection task.3 Our ApproachWe approach the subtasks (i.e.
cue, scope andnegated event detection) of the Task 1 as sequenceidentification problems and train three different 1storder Conditional Random Field (CRF) classifiers(i.e.
one for each of them) using the MALLET ma-chine learning toolkit (McCallum, 2002).
All theseclassifiers use ONLY the information available in-side the training corpus (i.e.
training and develop-ment datasets) as provided by the task organisers,which is the requirement of the closed track.3.1 Negation Cue DetectionAt first, our system automatically collects a vocab-ulary of all the positive tokens (i.e.
those which arenot negation cues) of length greater than 3 charac-ters, after excluding negation cue affixes (if any),from the training data and uses them to extract fea-tures that could be useful to identify potential nega-tion cues which are subtokens (e.g.
*un*able).
Wealso create a list of highly probable negation ex-pressions (henceforth, NegExpList) from the train-ing data based on frequencies.
The list consists ofthe following terms ?
nor, neither, without, nobody,none, nothing, never, not, no, nowhere, and non.Negation cue subtokens are identified if the tokenitself is predicted as a negation cue by the classi-fier and has one of the following affixes that are col-lected from the training data ?
less, un, dis, im, in,non, ir.Lemmas are converted to lower case inside thefeature set.
Additional post-processing is done toannotate some obvious negation expressions that areseen inside the training data but sometimes missedby the classifier during prediction on the develop-ment data.
These expressions include neither, no-body, save for, save upon, and by no means.
A spe-341wisteria01 60 0 Our Our PRP$ (S(NP*wisteria01 60 1 client client NN *)wisteria01 60 2 looked look VBD (VP*wisteria01 60 3 down down RB (ADVP*)wisteria01 60 4 with with IN (PP*wisteria01 60 5 a a DT (NP(NP*wisteria01 60 6 rueful rueful JJ *wisteria01 60 7 face face NN *)wisteria01 60 8 at at IN (PP*wisteria01 60 9 his his PRP$ (NP* hiswisteria01 60 10 own own JJ * ownwisteria01 60 11 unconventional unconventional JJ * un conventional conventionalwisteria01 60 12 appearance appearance NN *))))) appearanceTable 2: Example of the data provided for *SEM 2012 Shared Task.Feature name DescriptionPOSi Part-of-speech of tokeniLemmai Lemma form of tokeniLemmai?1 Lemma form of tokeni?1hasNegPrefix If tokeni has a negationprefix and is found inside theautomatically created vocabularyhasNegSuffix If tokeni has a negationsuffix and is found inside theautomatically created vocabularymatchesNegExp If tokeni is found in NegExpListTable 3: Feature set for negation cue classifiercial check is done for the phrase ?none the less?which is marked as a non-negation expression insidethe training data.Finally, a CRF model is trained using the col-lected features (see Table 3) and used to predictnegation cue on test instance.3.2 Scope and Negated Event DetectionOnce the negation cues are identified, the next tasksare to detect scopes of the cues and negated eventswhich are approached independently using separateclassifiers.
If a sentence has multiple negation cues,we create separate training/test instance of the sen-tence for each of the cues.Tables 4 and 5 show the feature sets that are usedto train classifiers.
Both the feature sets exclusivelyuse various phrasal clues, e.g.
whether the (clos-est) NP, VP, S or SBAR containing the token un-der consideration (i.e.
tokeni) and that of the nega-tion cue are different.
Further phrasal clues that areexploited include whether the least common phraseof tokeni has no other phrase as child, and alsolist of the counts of different common phrasal cat-egories (starting from the root of the parse tree) thatcontain tokeni and the cue.
These latter two typesof phrasal clue features are found effective for thenegated event detection but not for scope detection.We also use various token specific features (e.g.lemma, POS, etc) and contextual features (e.g.lemma of the 1st word of the corresponding sen-tence, position of the token with respect to the cue,presence of conjunction and special characters be-tween tokeni and the cue, etc).
Finally, new fea-tures are created by combining different features ofthe neighbouring tokens within a certain range of thetokeni.
The range values are selected empirically.Once scopes and negated events are identified(separately), the prediction output of all the threeclassifiers are merged to produce the full negationscope.Initially, a number of features is chosen by doingmanual inspection (randomly) of the scopes/negatedevents in the training data as well analysing syntac-tic structures of the corresponding sentences.
Someof those features (e.g.
POS of previous token forscope detection) which are found (empirically) asnot useful for performance improvement have beendiscarded.342Feature name: DescriptionLemma1 Lemma of the 1st wordof the sentencePOSi Part-of-speech of tokeniLemmai Lemma of tokeniLemmai?1 Lemma of tokeni?1isCue If tokeni is negation cueisCueSubToken If a subtoken of tokeniis negation cueisCcBetCueAndCurTok If there is a conjunctionbetween tokeni and cueisSpecCharBetCueAndCurTok If there is anon-alphanumeric tokenbetween tokeni and cuePosition Position of tokeni : before,after or same w.r.t.
the cueisCueAndCurTokInDiffNP If tokeni and cuebelong to different NPsisCueAndCurTokInDiffVP If tokeni and cuebelong to different VPsisCueAndCurTokInDiffSorSBAR If tokeni and cue belongto different S or SBARFeatureConjunctions New features by combiningthose of tokeni?2 to tokeni+2Table 4: Feature set for negation scope classifier.
Boldfeatures are the phrasal clue features.We left behind two verifications unintentionallywhich should have been included.
One of them isto take into account whether a sentence is a fac-tual statement or a question before negated event de-tection.
The other is to check whether a predictednegated event is found inside the predicted scope ofthe corresponding negation cue.4 Results and DiscussionsIn this section, we discuss various empirical re-sults on the development data and test data.
De-tails regarding the evaluation criteria are describedin Morante and Blanco (2012).4.1 Results on the Development DatasetOur feature sets are selected after doing a number ofexperiments by combining various potential featuretypes.
In these experiments, the system is trainedon the training data and tested on development data.Feature name DescriptionLemma1 Lemma of the 1st wordof the sentencePOSi Part-of-speech of tokeniLemmai Lemma of tokeniPOSi?1 POS of tokeni?1isCue If tokeni is negation cueisCueSubToken If a subtoken of tokeniis negation cueisSpecCharBetCueAndCurTok If there is anon-alphanumeric tokenbetween tokeni and cueIsModal If POS of tokeni is MDIsDT If POS of tokeni is DTisCueAndCurTokInDiffNP If tokeni and cuebelong to different NPsisCueAndCurTokInDiffVP If tokeni and cuebelong to different VPsisCueAndCurTokInDiffSorSBAR If tokeni and cue belongto different S or SBARbelongToSamePhrase If the least common phrase oftokeni and cue do notcontain other phraseCPcatBetCueAndCurTok All common phrase categories(and their counts) thatcontain tokeni and cueFeatureConjunctions New features by combiningthose of tokeni?3 to tokeni+1Table 5: Feature set for negated event classifier.
Boldfeatures are the phrasal clue features.Due to time limitation we could not do parametertuning for CRF model training which we assumecould further improve the results.Table 8 shows the results3 on the developmentdata using the feature sets described in Section 3.There are two noticeable things in these results.Firstly, there is a very high F1 score (93.29%) ob-tained for negation cue identification.
And secondly,the precision obtained for scope detection (97.92%)is very high as well.Table 6 shows the results (of negated event iden-3All the results reported in this paper, apart from the oneson test data which are directly obtained from the organisers,reported in this paper are computed using the official evaluationscript provided by the organisers.343TP FP FN Prec.
Rec.
F1Using only 71 16 46 81.61 60.68 69.61contextual and tokenspecific featuresAfter adding phrasal 81 17 34 82.65 70.43 76.05clue featuresTable 6: Negated event detection results on developmentdata with and without the 5 phrasal clue feature types.The results are obtained using gold annotation of nega-tion cues.
Note that, TP+FN is not the same.
However, since theseresults are computed using the official evaluation script, we are not surewhy there is this mismatch.Using negation cues annotated by our systemTP FP FN Prec.
Rec.
F1Scope detection 94 2 74 97.92 55.95 71.21Event detection 63 19 51 76.83 55.26 64.28Using gold annotations of negation cuesTP FP FN Prec.
Rec.
F1Scope detection 103 0 65 100.00 61.31 76.02Event detection 81 17 34 82.65 70.43 76.05Table 7: Scope and negated event detection results ondevelopment data with and without gold annotations ofnegation cues.
Note that, for negated events, TP+FN is not the same.However, since these results are computed using the official evaluationscript, we are not sure why there is this mismatch.tification) obtained before and after the usage of ourproposed 5 phrasal clue feature types (using gold an-notation of negation cues).
As we can see, there is asignificant improvement in recall (almost 10 points)due to the usage of phrasal clues which ultimatelyleads to a considerable increase (almost 6.5 points)of F1 score.4.2 Results on the Official Test DatasetTable 9 shows official results of our system in the*SEM 2012 Shared Task (closed track) of scope de-tection, as provided by the organisers.
It should benoted that the test dataset is almost 1.5 times biggerthan the combined training corpus (i.e.
training +development data).
Despite this fact, the results ofcue and scope detection on the test data are almostsimilar as those on the development data.
How-ever, there is a sharp drop (almost 4 points lower F1score) in negated event identification, primarily dueto lower precision.
This resulted in a lower F1 score(almost 4.5 points) for full negation identification.4.3 Further Analyses of the Results andFeature SetsOur analyses of the empirical results (conductedon the development data) suggest that negation cueidentification largely depends on the token itselfrather than its surrounding syntactic construction.Although context (i.e.
immediate neighbouring to-kens) are also important, the significance of a vo-cabulary of positive tokens (for the identification ofnegation cue subtokens) and the list of negation cueexpressions is quite obvious.
In a recently publishedstudy, Morante (2010) listed a number of negationcues and argued that their total number are actuallynot exhaustive.
We refrained from using the cueslisted in that paper (instead we built a list automati-cally from the training data) since additional knowl-edge/resource outside the training data was not al-lowed for the closed track.
But we speculate thatusage of such list of expressions as well as an exter-nal dictionary of (positive) words can further boostthe high performance that we already achieved.Since scope and negation event detection are de-pendent on the correct identification of cues, wehave done separate evaluation on the developmentdata using the gold cues (instead of predicting thecues first).
As the results in Table 7 show, there is aconsiderable increment in the results for both scopeand event detection if the correct annotation of cuesare available.The general trend of errors that we have observedin scope detection is that the more distant a token isfrom the negation cue in the phrase structure tree (ofthe corresponding sentence) the harder it becomesfor the classifier to predict whether the token shouldbe included in the scope or not.
For example, in thesentence ?I am not aware that in my whole life sucha thing has ever happened before.?
of the devel-opment data, the negation cue ?not?
has scope overthe whole sentence.
But the scope classifier fails toinclude the last 4 words in the scope.
Perhaps syn-tactic dependency can provide complementary infor-mation in such cases.As for the negated event identification errors, themajority of the prediction errors (on the develop-ment data) occurred for verb and noun tokens whichare mostly immediately preceded by the negationcue.
Information of syntactic dependency should be344Gold System TP FP FN Prec.
(%) Rec.
(%) F1 (%)Cues: 173 156 153 2 20 98.71 88.44 93.29Scopes (cue match): 168 150 94 2 74 97.92 55.95 71.21Scopes (no cue match): 168 150 94 2 74 97.92 55.95 71.21Scope tokens (no cue match): 1348 1132 1024 108 324 90.46 75.96 82.58Negated (no cue match): 122 90 63 19 51 76.83 55.26 64.28Full negation: 173 156 67 2 106 97.10 38.73 55.37Cues B: 173 156 153 2 20 98.08 88.44 93.01Scopes B (cue match): 168 150 94 2 74 62.67 55.95 59.12Scopes B (no cue match): 168 150 94 2 74 62.67 55.95 59.12Negated B (no cue match): 122 90 63 19 51 70.00 55.26 61.76Full negation B: 173 156 67 2 106 42.95 38.73 40.73# Sentences: 787 # Negation sentences: 144 # Negation sentences with errors: 97% Correct sentences: 87.55 % Correct negation sentences: 32.64Table 8: Results on the development data.
In the ?B?
variant of the results, Precision = TP / System, instead ofPrecision = TP / (TP + FP).helpful to reduce such errors, too.5 ConclusionsIn this paper, we presented our approach for nega-tion cue, scope and negated event detection task(closed track) of *SEM 2012 Shared Task, whereour system ranked 3rd among the participatingteams for full negation detection while obtaining thebest F1 score for negation cue detection.
Interest-ingly, according to the results provided by the organ-isers, our system performs better than all the systemsof the open track except one (details of these resultsare described in (Morante and Blanco, 2012)).The features exploited by our system includephrasal and contextual clues as well as token spe-cific information.
Empirical results show that thesystem achieves very high precision for scope de-tection.
The results also imply that the novel phrasalclue features exploited by our system improve iden-tification of negated events significantly.We believe the system can be further improvedin a number of ways.
Firstly, this can be done byincorporating linguistic knowledge as described inMorante (2010).
Secondly, we did not take into ac-count whether a sentence is a factual statement ora question before negated event detection.
We alsodid not check whether a predicted negated event isfound inside the predicted scope of the correspond-ing negation cue.
These verifications should in-crease the results more.
Finally, previous work re-ported that usage of syntactic dependency informa-tion helps in scope detection (Councill et al, 2010).Hence, this could be another possible direction forimprovement.AcknowledgmentsThe author would like to thank Alberto Lavelli andthe anonymous reviewers for various useful feed-back regarding the manuscript.ReferencesWW Chapman, W Bridewell, P Hanbury, GF Cooper,and BG Buchanan.
2001.
A Simple Algorithm forIdentifying Negated Findings and Diseases in Dis-charge Summaries.
Journal of Biomedical Informat-ics, 34(5):301?10.I Councill, R McDonald, and L Velikovich.
2010.
WhatsGreat and Whats Not: Learning to Classify the Scopeof Negation for Improved Sentiment Analysis.
In Pro-ceedings of the Workshop on Negation and Speculationin Natural Language Processing, pages 51?59, Upp-sala, Sweden.P Elkin, S Brown, B Bauer, C Husser, W Carruth,L Bergstrom, and D Wahner-Roedler.
2005.
A con-trolled trial of automated classification of negationfrom clinical notes.
BMC Medical Informatics andDecision Making, 5(1):13.L Jia, C Yu, and W Meng.
2009.
The Effect of Negationon Sentiment Analysis and Retrieval Effectiveness.
In345Gold System TP FP FN Prec.
(%) Rec.
(%) F1 (%)Cues: 264 263 241 17 23 93.41 91.29 92.34Scopes (cue match): 249 249 145 18 104 88.96 58.23 70.39Scopes (no cue match): 249 249 145 18 104 88.96 58.23 70.39Scope tokens (no cue match): 1805 1825 1488 337 317 81.53 82.44 81.98Negated (no cue match): 173 154 93 52 71 64.14 56.71 60.20Full negation: 264 263 96 17 168 84.96 36.36 50.93Cues B: 264 263 241 17 23 91.63 91.29 91.46Scopes B (cue match): 249 249 145 18 104 58.23 58.23 58.23Scopes B (no cue match): 249 249 145 18 104 58.23 58.23 58.23Negated B (no cue match): 173 154 93 52 71 60.39 56.71 58.49Full negation B: 264 263 96 17 168 36.50 36.36 36.43# Sentences: 1089 # Negation sentences: 235 # Negation sentences with errors: 151% Correct sentences: 84.94 % Correct negation sentences: 35.74Table 9: Results on the *SEM 2012 Shared Task (closed track) test data provided by the organisers.
In the ?B?
variantof the results, Precision = TP / System, instead of Precision = TP / (TP + FP).Proceedings of the 18th ACM Conference on Informa-tion and Knowledge Management (CIKM 2009), pages1827?1830, Hong Kong, China.AK McCallum.
2002.
MALLET: A machine learningfor language toolkit.
http://mallet.cs.umass.edu,.R Morante and E Blanco.
2012.
*SEM 2012 SharedTask: Resolving the Scope and Focus of Negation.In Proceedings of the First Joint Conference on Lexi-cal and Computational Semantics (*SEM 2012), Mon-treal, Canada.R Morante and W Daelemans.
2009.
A MetalearningApproach to Processing the Scope of Negation.
InProceedings of CoNLL 2009, pages 28?36, Boulder,Colorado, USA.R Morante and W Daelemans.
2012.
ConanDoyle-neg:Annotation of Negation in Conan Doyle Stories.
InProceedings of the 8th International Conference onLanguage Resources and Evaluation (LREC 2012), Is-tanbul, Turkey.R Morante, S Schrauwen, and W Daelemans.
2011.
An-notation of Negation Cues and Their Scope Guidelinesv1.0.
Technical Report CLiPS Technical Report 3,CLiPS, Antwerp, Belgium.R Morante.
2010.
Descriptive Analysis of NegationCue in Biomedical Texts.
In Proceedings of the 7thInternational Conference on Language Resources andEvaluation (LREC 2010), Malta.346
