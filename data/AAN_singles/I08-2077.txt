Named Entity Recognition in Bengali: A Conditional Random FieldApproachAsif EkbalDepartment of CSEJadavpur UniversityKolkata-700032, Indiaasif.ekbal@gmail.comRejwanul HaqueDepartment of CSEJadavpur UniversityKolkata-700032, Indiarejwanul@gmail.comSivaji BandyopadhyayDepartment of CSEJadavpur UniversityKolkata-700032, Indiasivaji cse ju@yahoo.comAbstractThis paper reports about the development ofa Named Entity Recognition (NER) systemfor Bengali using the statistical ConditionalRandom Fields (CRFs).
The system makesuse of the different contextual informationof the words along with the variety of fea-tures that are helpful in predicting the var-ious named entity (NE) classes.
A portionof the partially NE tagged Bengali news cor-pus, developed from the archive of a lead-ing Bengali newspaper available in the web,has been used to develop the system.
Thetraining set consists of 150K words and hasbeen manually annotated with a NE tagsetof seventeen tags.
Experimental results ofthe 10-fold cross validation test show the ef-fectiveness of the proposed CRF based NERsystem with an overall average Recall, Pre-cision and F-Score values of 93.8%, 87.8%and 90.7%, respectively.1 IntroductionNamed Entity Recognition (NER) is an impor-tant tool in almost all Natural Language Process-ing (NLP) application areas.
Proper identifica-tion and classification of named entities (NEs) arevery crucial and pose a very big challenge to theNLP researchers.
The level of ambiguity in NERmakes it difficult to attain human performance.NER has applications in several domains includ-ing information extraction, information retrieval,question-answering, automatic summarization, ma-chine translation etc.The current trend in NER is to use the machine-learning approach, which is more attractive in thatit is trainable and adoptable and the maintenanceof a machine-learning system is much cheaper thanthat of a rule-based one.
The representative ma-chine-learning approaches used in NER are Hid-den Markov Model (HMM) (BBN?s IdentiFinderin (Bikel et al, 1999)), Maximum Entropy (NewYork University?s MENE in (Borthwick, 1999)) andConditional Random Fields (CRFs) (Lafferty et al,2001; McCallum and Li, 2003).There is no concept of capitalization in Indianlanguages (ILs) like English and this fact makesthe NER task more difficult and challenging in ILs.There has been very little work in the area of NERin ILs.
In Indian languages particularly in Ben-gali, the work in NER can be found in (Ekbal andBandyopadhyay, 2007a; Ekbal and Bandyopadhyay,2007b) with pattern directed shallow parsing ap-proach and in (Ekbal et al, 2007c) with HMM.Other than Bengali, a CRF based NER system canbe found in (Li and McCallum, 2004) for Hindi.2 Conditional Random FieldsConditional Random Fields (CRFs) (Lafferty et al,2001) are used to calculate the conditional proba-bility of values on designated output nodes givenvalues on other designated input nodes.
The con-ditional probability of a state sequence S =<s1, s2, .
.
.
, sT > given an observation sequenceO =< o1, o2, .
.
.
, oT > is calculated as:P?
(s|o) =1Z0exp(T?t=1?k?k ?
fk(st?1, st, o, t)),589where, fk(st?1, st, o, t) is a feature function whoseweight ?k, is to be learned via training.
The val-ues of the feature functions may range between?
?, .
.
.+?, but typically they are binary.
To makeall conditional probabilities sum up to 1, we mustcalculate the normalization factor,Z0 =?sexp(T?t=1?k?k ?
fk(st?1, st, o, t)),which as in HMMs, can be obtained efficiently bydynamic programming.To train a CRF, the objective function to be maxi-mized is the penalized log-likelihood of the state se-quences given the observation sequences:L?
=N?i=1log(P?
(s(i)|o(i))) ?
?k?2k2?2 ,where {< o(i), s(i) >} is the labeled training data.The second sum corresponds to a zero-mean, ?2-variance Gaussian prior over parameters, whichfacilitates optimization by making the likelihoodsurface strictly convex.
Here, we set parameters?
to maximize the penalized log-likelihood usingLimited-memory BFGS (Sha and Pereira, 2003), aquasi-Newton method that is significantly more ef-ficient, and which results in only minor changes inaccuracy due to changes in ?.When applying CRFs to the NER problem, an ob-servation sequence is a token of a sentence or docu-ment of text and the state sequence is its correspond-ing label sequence.
While CRFs generally can usereal-valued functions, in our experiments maximumof the features are binary valued.
A feature functionfk(st?1, st, o, t) has a value of 0 for most cases andis only set to be 1, when st?1, st are certain statesand the observation has certain properties.
We haveused the C++ based OpenNLP CRF++ package 1.3 Named Entity Recognition in BengaliBengali is one of the widely used languages all overthe world.
It is the seventh popular language in theworld, second in India and the national language ofBangladesh.
A partially NE tagged Bengali newscorpus (Ekbal and Bandyopadhyay, 2007d), devel-oped from the archive of a widely read Bengali news1http://crfpp.sourceforge.netpaper available in the web, has been used in thiswork to identify and classify NEs.
The corpus con-tains around 34 million word forms in ISCII (IndianScript Code for Information Interchange) and UTF-8 format.
The location, reporter, agency and differ-ent date tags (date, ed, bd, day) in the partially NEtagged corpus help to identify some of the location,person, organization and miscellaneous names, re-spectively, that appear in some fixed places of thenewspaper.
These tags cannot detect the NEs withinthe actual news body.
The date information obtainedfrom the news corpus provides example of miscella-neous names.
A portion of this partially NE taggedcorpus has been manually annotated with the seven-teen tags as described in Table 1.NE tag Meaning ExamplePER Single-word sachin/ PERperson nameLOC Single-word jadavpur/LOClocation nameORG Single-word infosys/ ORGorganization nameMISC Single-word 100%/ MISCmiscellaneous nameB-PER Beginning, Internal sachin/B-PERI-PER or End of a multiword ramesh/I-PERE-PER person name tendulkar/E-PERB-LOC Beginning, Internal or mahatma/B-LOCI-LOC End of a multiword gandhi/I-LOCE-LOC location name road/E-LOCB-ORG Beginning, Internal or bhaba/B-ORGI-ORG End of a multiword atomic/I-ORGE-ORG organization name research/I-ORGcenter/E-ORGB-MISC Beginning, Internal or 10e/B-MISCI-MISC End of a multiword magh/ I-MISCE-MISC miscellaneous name 1402/E-MISCNNE Words that are not NEs neta/NNETable 1: Named Entity Tagset3.1 Named Entity TagsetA CRF based NER system has been developedin this work to identify NEs in Bengali and clas-sify them into the predefined four major categories,namely, ?Person name?, ?Location name?, ?Organi-zation name?
and ?Miscellaneous name?.
In order to590properly denote the boundaries of NEs and to applyCRF in NER task, sixteen NE and one non-NE tagshave been defined as shown in Table 1.
In the out-put, sixteen NE tags are replaced appropriately withthe four major NE tags by some simple heuristics.3.2 Named Entity FeaturesFeature selection plays a crucial role in CRF frame-work.
Experiments were carried out to find out themost suitable features for NER in Bengali.
Themain features for the NER task have been iden-tified based on the different possible combinationof available word and tag context.
The featuresalso include prefix and suffix for all words.
Theterm prefix/suffix is a sequence of first/last fewcharacters of a word, which may not be a lin-guistically meaningful prefix/suffix.
The use ofprefix/suffix information works well for highly in-flected languages like the Indian languages.
Inaddition, various gazetteer lists have been devel-oped for use in the NER task.
We have consid-ered different combination from the following setfor inspecting the best feature set for NER task:F={wi?m, .
.
.
, wi?1, wi, wi+1, .
.
.
wi+n, |prefix| ?n, |suffix| ?
n, previous NE tag, POS tags, Firstword, Digit information, Gazetteer lists}.Following are the details of the set of features thatwere applied to the NER task:?
Context word feature: Previous and next words ofa particular word might be used as a feature.?
Word suffix: Word suffix information is helpfulto identify NEs.
This feature can be used in twodifferent ways.
The first and the nai?ve one is, afixed length word suffix of the current and/or the sur-rounding word(s) might be treated as feature.
Thesecond and the more helpful approach is to modifythe feature as binary valued.
Variable length suf-fixes of a word can be matched with predefined listsof useful suffixes for different classes of NEs.
Thedifferent suffixes that may be particularly helpful indetecting person (e.g., -babu, -da, -di etc.)
and lo-cation names (e.g., -land, -pur, -lia etc.)
have beenconsidered also.
Here, both types of suffixes havebeen used.?
Word prefix: Prefix information of a word is alsohelpful.
A fixed length prefix of the current and/orthe surrounding word(s) might be treated as features.?
Part of Speech (POS) Information: The POS ofthe current and/or the surrounding word(s) can beused as features.
Multiple POS information of thewords can be a feature but it has not been used in thepresent work.
The alternative and the better way isto use a coarse-grained POS tagger.Here, we have used a CRF-based POS tagger,which was originally developed with the help of 26different POS tags2, defined for Indian languages.For NER, we have considered a coarse-grained POStagger that has only the following POS tags:NNC (Compound common noun), NN (Com-mon noun), NNPC (Compound proper noun), NNP(Proper noun), PREP (Postpositions), QFNUM(Number quantifier) and Other (Other than theabove).The POS tagger is further modified with twoPOS tags (Nominal and Other) for incorporatingthe nominal POS information.
Now, a binary val-ued feature ?nominalPOS?
is defined as: If the cur-rent/previous/next word is ?Nominal?
then the ?nom-inalPOS?
feature of the corresponding word is set to1; otherwise, it is set to 0.
This ?nominalPOS?
fea-ture has been used additionally with the 7-tag POSfeature.
Sometimes, postpositions play an importantrole in NER as postpositions occur very frequentlyafter a NE.
A binary valued feature ?nominalPREP?is defined as: If the current word is nominal and thenext word is PREP then the feature ?nomianlPREP?of the current word is set to 1, otherwise set to 0.?
Named Entity Information: The NE tag of the pre-vious word is also considered as the feature.
This isthe only dynamic feature in the experiment.?
First word: If the current token is the first word ofa sentence, then the feature ?FirstWord?
is set to 1.Otherwise, it is set to 0.?
Digit features: Several binary digit featureshave been considered depending upon the presenceand/or the number of digits in a token (e.g., Con-tainsDigit [token contains digits], FourDigit [tokenconsists of four digits], TwoDigit [token consistsof two digits]), combination of digits and punctu-ation symbols (e.g., ContainsDigitAndComma [to-ken consists of digits and comma], ConatainsDigi-tAndPeriod [token consists of digits and periods]),combination of digits and symbols (e.g., Contains-DigitAndSlash [token consists of digit and slash],2http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf591ContainsDigitAndHyphen [token consists of digitsand hyphen], ContainsDigitAndPercentage [tokenconsists of digits and percentages]).
These binaryvalued features are helpful in recognizing miscella-neous NEs such as time expressions, monetary ex-pressions, date expressions, percentages, numericalnumbers etc.?
Gazetteer Lists: Various gazetteer lists have beendeveloped from the partially NE tagged Bengalinews corpus (Ekbal and Bandyopadhyay, 2007d).These lists have been used as the binary valued fea-tures of the CRF.
If the current token is in a particu-lar list then the corresponding feature is set to 1 forthe current and/or the surrounding word(s); other-wise, set to 0.
The following is the list of gazetteers:(i) Organization suffix word (94 entries): This listcontains the words that are helpful in identifying or-ganization names (e.g., kong, limited etc).
The fea-ture ?OrganizationSuffix?
is set to 1 for the currentand the previous words.
(ii) Person prefix word (245 entries): This is usefulfor detecting person names (e.g., sriman, sree, sri-mati etc.).
The feature ?PersonPrefix?
is set to 1 forthe current and the next two words.
(iii) Middle name (1,491 entries): These words gen-erally appear inside the person names (e.g., chandra,nath etc.).
The feature ?MiddleName?
is set to 1 forthe current, previous and the next words.
(iv) Surname (5,288 entries): These words usuallyappear at the end of person names as their parts.
Thefeature ?SurName?
is set to 1 for the current word.
(v) Common location word (547 entries): This listcontains the words that are part of location namesand appear at the end (e.g., sarani, road, lane etc.
).The feature ?CommonLocation?
is set to 1 for thecurrent word.
(vi) Action verb (221 entries): A set of action verbslike balen, ballen, ballo, shunllo, haslo etc.
oftendetermines the presence of person names.
The fea-ture ?ActionVerb?
is set to 1 for the previous word.
(vii) Frequent word (31,000 entries): A list of mostfrequently occurring words in the Bengali news cor-pus has been prepared using a part of the corpus.The feature ?RareWord?
is set to 1 for those wordsthat are not in this list.
(viii) Function words (743 entries): A list of func-tion words has been prepared manually.
The feature?NonFunctionWord?
is set to 1 for those words thatare not in this list.
(ix) Designation words (947 entries): A list of com-mon designation words has been prepared.
Thishelps to identify the position of the NEs, partic-ularly person names (e.g., neta, sangsad, kheloaretc.).
The feature ?DesignationWord?
is set to 1 forthe next word.
(x) Person name (72, 206 entries): This list containsthe first name of person names.
The feature ?Person-Name?
is set to 1 for the current word.
(xi) Location name (7,870 entries): This list containsthe location names and the feature ?LocationName?is set to 1 for the current word.
(xii) Organization name (2,225 entries): This listcontains the organization names and the feature ?Or-ganizationName?
is set to 1 for the current word.
(xiii) Month name (24 entries): This contains thename of all the twelve different months of both En-glish and Bengali calendars.
The feature ?Month-Name?
is set to 1 for the current word.
(xiv) Weekdays (14 entries): It contains the name ofseven weekdays in Bengali and English both.
Thefeature ?WeekDay?
is set to 1 for the current word.4 Experimental ResultsA partially NE tagged Bengali news corpus (Ekbaland Bandyopadhyay, 2007d) has been used to cre-ate the training set for the NER experiment.
Out of34 million wordforms, a set of 150K wordforms hasbeen manually annotated with the 17 tags as shownin Table 1 with the help of Sanchay Editor 3, a texteditor for Indian languages.
Around 20K NE taggedcorpus has been selected as the development set andthe rest 130K wordforms has been used as the train-ing set of the CRF based NER system.We define the baseline model as the one wherethe NE tag probabilities depend only on the cur-rent word: P (t1, t2, .
.
.
, tn|w1, w2, .
.
.
, wn) =?i=1,...,n P (ti, wi).In this model, each word in the test data will beassigned the NE tag which occurred most frequentlyfor that word in the training data.
The unknownword is assigned the NE tag with the help of vari-ous gazetteers and NE suffix lists.Ninety-five different experiments were conductedtaking the different combinations from the set ?F?
to3Sourceforge.net/project/nlp-sanchay592Feature (word, tag) FS(in %)pw, cw, nw, FirstWord 71.31pw2, pw, cw, nw, nw2, FirstWord 72.23pw3, pw2, pw, cw, nw, nw2, nw3, 71.12FirstWordpw2, pw, cw, nw, nw2, FirstWord, pt 74.91pw2, pw, cw, nw, nw2, FirstWord, pt, 77.61|pre| ?
4, |suf| ?
4pw2, pw, cw, nw, nw2, FirstWord, pt, 79.70|suf| ?
3, |pre| ?
3pw2, pw, cw, nw, nw2, FirstWord, pt, 81.50|suf| ?
3, |pre| ?
3, Digit featurespw2, pw, cw, nw, nw2, FirstWord, pt, 83.60|suf| ?
3, |pre| ?
3, Digit features, pp,cp, nppw2, pw, cw, nw, nw2, FirstWord, pt, 82.20|suf| ?
3, |pre| ?
3, Digit features,pp2, pp, cp, np, np2pw2, pw, cw, nw, nw2, FirstWord, pt, 83.10|suf| ?
3, |pre| ?
3, Digit features, pp, cppw2, pw, cw, nw, nw2, FirstWord, pt, 83.70|suf| ?
3, |pre| ?
3, Digit features, cp, nppw2, pw, cw, nw, nw2, FirstWord, pt, 89.30|suf| ?
3,|pre| ?
3, Digit features, pp,cp, np, nominalPOS, nominalPREP,Gazetteer listsTable 2: Results on Development Setidentify the best suited set of features for the NERtask.
From our empirical analysis, we found that thefollowing combination gives the best result with 744iterations:F=[wi?2, wi?1, wi, wi+1, wi+2, |prefix| ?
3,|sufix| ?
3, NE information of the previous word,POS information of the window three, nominalPOSof the current word, nominalPREP, FirstWord, Digitfeatures, Gazetteer lists].The meanings of the notations, used in experi-mental results, are defined as below:cw, pw, nw: Current, previous and next word; pwi,nwi: Previous and the next ith word from the currentword; pre, suf: Prefix and suffix of the current word;pt: NE tag of the previous word; cp, pp, np: POS tagof the current, previous and the next word; ppi, npi:POS tag of the previous and the next ith word.Evaluation results of the system for the develop-ment set in terms of overall F-Score (FS) are pre-sented in Table 2.
It is observed from Table 2 thatword window [?2,+2] gives the best result with?FirstWord?
feature only and the further increase ofthe window size reduces the overall F-Score value.Results of Table 2 (3rd and 5th rows) show thatthe inclusion of NE information of the previousword increases the overall F-Score by 2.68%.
It isalso indicative from the evaluation results that theperformance of the system can be improved by in-cluding the prefix and suffix features.
Results (6thand 7th rows) also show the fact that prefix and suf-fix of length upto three of the current word is moreeffective.
In another experiment, it has been also ob-served that the surrounding word suffixes and/or pre-fixes do not increase the F-Score value.
The overallF-Score value is further improved by 1.8% (7th and8th rows) with the inclusion of various digit features.Results (8th and 9th rows) show that POS in-formation of the words improves the overall F-scoreby 2.1%.
In the above experiment, the POS tag-ger was developed with 26 POS tags.
Experimen-tal results (9th, 10th, 11th and 12th rows) suggestthat the POS tags of the previous, current and thenext words, i.e., POS information of the window[?1,+1] is more effective than POS information ofthe window [?2,+2], [?1, 0] or [0,+1].
In anotherexperiment, we also observed that the POS informa-tion of the current word alone is less effective thanthe window [?1,+1].
The modified POS tagger thatis developed with 7 POS tags increases the overall F-Score to 85.2%, while other set of features are keptunchanged.
So, it can be decided that smaller POStagset is more effective than the larger POS tagsetin NER.
We have observed from two separate ex-periments that the overall F-Score values can furtherbe improved by 0.4% and 0.2%, respectively, withthe ?nominalPOS?
and ?nominalPREP?
features.
Fi-nally, an overall F-Score value of 89.3% is obtainedby including the gazetteer lists.The best set of features is identified by trainingthe system with 130K wordforms and testing withthe development set of 20K wordforms.
Now, thedevelopment set is included as part of the train-ing set and resultant training set is thus consists of150K wordforms.
The training set has 20,455 per-son names, 11,668 location names, 963 organization593names and 11,554 miscellaneous names.
We haveperformed 10-fold cross validation test on this train-ing set.
The Recall, Precision and F-Score valuesfor the 10 different experiments in the 10-fold crossvalidation test are presented in Table 3.
The over-all average Recall, Precision and F-Score values are93.8%, 87.8% and 90.7%, respectively.The other existing Bengali NER systems alongwith the baseline model are also trained and testedunder the same experimental setup.
The baselinemodel has demonstrated the overall F-Score value of56.3%.
The overall F-Score value of the CRF basedNER system is 90.7%, which is an improvement ofmore than 6% over the HMM based system, best re-ported Bengali NER system (Ekbal et al, 2007c).The reason behind the rise in overall F-Score valuemight be its better capability than HMM to capturethe morphologically rich and overlapping features ofBengali language.
The system has been evaluatedalso for the four individual NE classes and it hasshown the average F-Score values of 91.2%, 89.7%,87.1% and 99.2%, respectively, for person, location,organization and miscellaneous names.5 ConclusionIn this paper, we have developed a NER system us-ing CRF with the help of a partially NE tagged Ben-gali news corpus, developed from the archive of aleading Bengali newspaper available in the web.
Ex-perimental results with the 10-fold cross validationtest have shown reasonably good Recall, Precisionand F-Score values.
It has been shown that the con-textual window [-2, +2], prefix and suffix of lengthupto three, first word of the sentence, POS informa-tion of the window [-1, +1], current word, NE infor-mation of the previous word, different digit featuresand the various gazetteer lists are the best-suited fea-tures for the Bengali NER.Analyzing the performance using other methodslike MaxEnt and Support Vector Machines (SVMs)will be other interesting experiments.ReferencesDaniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An Algorithm that Learns What?sin a Name.
Machine Learning, 34(1-3):211?231.A.
Borthwick.
1999.
Maximum Entropy Approach toTest set no.
Recall Precision FS (%)1 92.4 87.3 89.782 92.3 87.4 89.783 91.4 86.6 88.944 95.2 87.7 91.295 91.6 86.7 89.086 92.2 87.1 89.587 94.5 87.9 91.088 93.8 89.3 91.499 96.9 88.4 92.4510 97.7 89.6 93.47Average 93.8 87.8 90.7Table 3: Results for the 10-fold Cross ValidationTestNamed Entity Recognition.
Ph.D. thesis, New YorkUniversity.A.
Ekbal and S. Bandyopadhyay.
2007a.
Lexical PatternLearning from Corpus Data for Named Entity Recog-nition.
In Proceedings of ICON, pages 123?128, India.A.
Ekbal and S. Bandyopadhyay.
2007b.
Pattern BasedBootstrapping Method for Named Entity Recognition.In Proceedings of ICAPR, pages 349?355, India.A.
Ekbal and S. Bandyopadhyay.
2007d.
A Web-based Bengali News Corpus for Named Entity Recog-nition.
Language Resources and Evaluation Journal(accepted).A.
Ekbal, S.K.
Naskar, and S. Bandyopadhyay.
2007c.Named Entity Recognition and Transliteration in Ben-gali.
Named Entities: Recognition, Classificationand Use, Special Issue of Lingvisticae InvestigationesJournal, 30(1):95?114.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional Random Fields: Proba-bilistic Models for Segmenting and Labeling SequenceData.
In ICML, pages 282?289.Wei Li and Andrew McCallum.
2004.
Rapid Develop-ment of Hindi Named Entity Recognition using Con-ditional Random Fields and Feature Induction.
ACMTALIP, 2(3):290?294.A.
McCallum and W. Li.
2003.
Early results for NamedEntity Recognition with Conditional Random Fields,Feature Induction and Web-enhanced Lexicons.
InProceedings of CoNLL, pages 188?191, Canada.Fei Sha and Fernando Pereira.
2003.
Shallow Parsingwith Conditional Random Fields.
In Proceedings ofNAACL ?03, pages 134?141, Canada.594
