Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 20?29,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsEnriching Parallel Corpora for Statistical Machine Translationwith Semantic Negation RephrasingDominikus WetzelDepartment of Computational LinguisticsSaarland Universitydwetzel@coli.uni-sb.deFrancis BondLinguistics and Multilingual StudiesNanyang Technological Universitybond@ieee.orgAbstractThis paper presents an approach to improvingperformance of statistical machine translationby automatically creating new training datafor difficult to translate phenomena.
In partic-ular this contribution is targeted towards tack-ling the poor performance of a state-of-the-artsystem on negated sentences.
The corpus ex-pansion is achieved by high quality rephrasingof existing sentences to their negated counter-parts making use of semantic transfer.
Themethod is designed to work on both sides ofthe parallel corpus while preserving the align-ment.
Our results show an overall improve-ment of 0.16 BLEU points, with a statisti-cally significant increase of 1.63 BLEU pointswhen tested on only negated test data.1 IntroductionHaving large and good quality parallel corpora is vi-tal for the quality of statistical machine translation(SMT) systems.
However, these corpora are expen-sive to create.
Furthermore, certain phenomena arenot very frequent and hence underrepresented in ex-isting parallel corpora, such as negated sentences,questions, etc.
Due to the lack of such training data,the SMT systems do not perform as well as theycould.
Especially when it comes to negation, it isimportant that the basic semantics is preserved, i.e.
anegated statement should not be translated as a pos-itive one and vice versa.Given a state-of-the-art baseline Japanese-EnglishSMT system, a separate evaluation on the seman-tic level of negative only vs. positive only test datareveals the considerably poorer performance on thenegative test set.
This tendency and the importanceof preserving a negated statement motivates experi-ments with improving performance on negative sen-tences.Providing more training data for negative sen-tences should even out the discrepancy of the perfor-mance between the above mentioned negative andpositive test data.
We present a method where alarge amount of negative training data is obtained byrephrasing the original training data.
The rephras-ing is performed on the semantic level to ensurehigh reliability and quality of the generated data.Simple rewriting based on the surface or syntacticlevel would require complex language specific rules,which is not desirable.Working on the semantic structure exploits thefact that these representations abstract away fromlanguage specific structures.
Thus, our approachcan be easily implemented for other languages, pro-vided there are grammars available for both lan-guages involved in the desired parallel corpus.
TheDELPH-IN project1 provides various such gram-mars.This paper first describes related work in the fol-lowing section.
Section 3 presents a semantic analy-sis of the data with respect to negation and providessome distributional statistics.
In Section 4 we elab-orate on the functionality of our rephrasing systemand present different methods for corpus expansion.The experimental setup and the results are in Sec-tion 5.
A discussion and our conclusion are given inSection 6 and Section 7, respectively.2 Related WorkThere has been plenty of work on paraphrasing datain order to overcome the limitations that insuffi-ciently large or underrepresented phenomena in par-1www.delph-in.net20allel corpora impose on SMT.Callison-Burch et al (2006) tackle the problemof unseen phrases in SMT by adding source lan-guage paraphrases to the phrase table with appropri-ate probabilities.
Both are obtained from additionalparallel corpora, where the translations of the sameforeign language phrase are considered paraphrases.He et al (2011) use a statistical framework forparaphrase generation of the source language.
Alog-linear model similar to the one used in phrase-based SMT provides paraphrases which are rankedbased on novelty and fluency.
The training corpusis then expanded by either adding the first best para-phrase, or n-best paraphrases.
The target language isjust copied to provide the required target side of theparaphrase.Marton et al (2009) and Gao and Vogel (2011)create new information by means of shallow seman-tic methods.
The former present an approach toovercome the problem of unknown words in a lowresource experiment.
They base their monolingualparaphrasing on semantic similarity measures.
Intheir setting they achieve significantly better trans-lations.
Gao and Vogel (2011) expand the parallelcorpus by creating new information from existingdata.
With the use of a monolingual semantic rolelabeller one side of the parallel corpus is labelled.Role-to-word rules are extracted.
In sentences con-taining the frames and semantic roles for which re-placement rules exist, the corresponding words aresubstituted.
A support vector machine is used forfiltering the generated paraphrases.An approach where paraphrases are obtained viageneration from semantic structures is presented inNichols et al (2010).
It exploits the fact that the gen-erator produces multiple surface realizations.
Thebasic set up is similar to our work, however our ap-proach additionally manipulates, i.e.
rephrases thesemantics before generation.
Furthermore, we im-plement parallel rephrasing, changing the meaningof both source and target text simultaneously.There is, on the other hand, little work in phrase-based SMT especially targeting negated sentences.Collins et al (2005) approach the problem of prop-erly translating negation in their general reorderingsetting.
Transformation rules are applied to syntac-tic trees, so that the source language word order hasa closer resemblance to the target language word or-der.
In particular, the German negation is moved to-wards the same position as the English one.
Thishowever presumes the existence of at least somenegated training data.3 Analysis of the Semantic StructureThe linguistic analysis is performed based on theHead-Driven Phrase Structure Grammar (HPSG)formalism established in the DELPH-IN project.
Inparticular we consider the language pair Japanese-English.
Hence, the broad-coverage grammar Jacyfor Japanese (Bender and Siegel, 2004) and the En-glish Resource Grammar (ERG) (Flickinger, 2000)are used respectively to parse the data and obtain thesemantics for each sentence.3.1 Negation in Minimal Recursion SemanticsThe formalism that is used to represent the seman-tics in the DELPH-IN grammars is Minimal Recur-sion Semantics (MRS) (Copestake et al, 2005).
Perdefinition, an MRS structure consists of a top han-dle, a bag of elementary predicates (EP) and a bag ofconstraints on handles.
EPs represent verbs, their ar-guments, negations, quantifiers, among others.
Fur-thermore, each EP has a handle with which it canbe identified.
Constraints on handles are used to re-strict EPs such that they are outscoped by negationsor quantifiers.In a negated sentence, the negated verb isoutscoped by the negation relation EP.
Technically,the negation relation with handle hn takes as its ar-gument (ARG1) a handle (hx) which is equal mod-ulo quantifiers to the handle of the verb (hv), writtenas the handle constraint: hx =q hv.
For visualiza-tion, an example is given, which shows the relevantparts of such a negated structure for the sentence?This may not suit your taste.?
(Figure 1).
There,the negated verb has the handle h8.
The negationrelation EP with handle h10 outscopes this via theconstraint h12 =q h8.The rephrasing we propose can be achieved withlittle or no knowledge about the specific implemen-tation choices of the individual grammar.
Collectinga few sample sentences that appear to be negated inthe original data ?
by performing a simple surfacestring matching ?
is enough to reveal the principleof how negation is implemented.
Because negation21< e2,{ h8: _MAY_V_MODAL_REL( ARG0 e2, ARG1 h9 ),h10: NEG_REL( ARG0 e11, ARG1 h12),h13: _suit_v_1_rel( ARG0 e14, ARG1 x4, ARG2 x15),... }{ h6 =q h3,h12 =q h8,h9 =q h13,... } >Figure 1: A visualization of the English MRS structure from the sentence ?This may not suit your taste.
?.The irrelevant parts have been omitted.
The necessary parts in the corresponding Japanese MRS are thesame.JapaneseEnglish neg rel no neg relneg rel 8.5% 1.4%no neg rel 9.7% 80.4%Table 1: Distribution of negation measured by thepresence or absence of a negation relation (neg rel)for those sentences with parses in both languages.is represented at the semantic level, both the ERGand Jacy have very similar analyses, even thoughthe syntactic realization is very different (negationin English involves a negative marker such as notand the use of an auxiliary verb such as do, while inJapanese it is realized by an auxiliary verb nai).3.2 Data and Distribution of NegationsThe data we use in this work is the Japanese-English parallel Tanaka corpus (Tanaka, 2001; Bondet al, 2008).
We used the version distributed withJacy, which has approximately 150,000 sentencepairs randomly ordered and divided into 100 pro-files of 1,500 sentences each (the last one is a lit-tle short).
We summarize the distribution of negatedsentence pairs in Table 1.
The data we consider forthese statistics excludes development and test pro-files (000?005).
84.5% of the input sentence pairscan be parsed successfully (110,759 out of 139,150).The table also shows mixed cases where one lan-guage had a negation relation EP, whereas the otherdid not.
Mixed cases are especially frequent whenthe Japanese side has a negation relation.
Thesecases have two main causes: lexical negation suchas ?She missed the bus.?
being translated with theequivalent of ?She did not catch the bus.?
; and id-ioms, such as ikanakereba naranai ?I must go (lit:go-not-if not-become)?
where the Japanese expres-sion of modality includes a negation.
Instances ofthe latter type form the majority, and should be han-dled in a newer version of the grammar, they are notconsidered further in this work.4 Method: MRS Rephrasing & CorpusExpansionThe basic setup of the whole rephrasing system con-sists of parsing, MRS manipulation, generation andfinally parallel corpus compilation.
In the follow-ing sections, the individual processing modules aredescribed in detail.4.1 ParsingParsing is done using PET (Callmeier, 2000) abottom-up chart parser for unification-based gram-mars using the English and Japanese GrammarsERG and Jacy.
Since our approach builds on seman-tic rephrasing, only the MRS structure is required.We only use the best (first) parse returned by theparser.4.2 RephrasingThis module takes an MRS structure as input andrephrases it if possible by adding a negation rela-tion EP to the highest scoping predicate.
Adding thenegation relation in our current form does not ex-plore alternatives, where the negation has scope over22other EPs in the MRS, nor are more refined changesfrom positive to negative polarity items considered.Before inserting the negation relation EP into theexisting MRS structure with its required handle con-straint, we have to identify the EP we want to negate.The event that is introduced by the highest scopingverb is used.
The event variable e2 is directly acces-sible at the top of the MRS structure (cf.
Figure 1).The corresponding EP that we want to negate has theevent variable as value of its ARG0 attribute.
ThisEP has a handle h8 that has to be outscoped by thenegation by means of a handle constraint.
Hence, anew negation relation EP (in the example it got thehandle h10) is inserted with the following condition:Its ARG1 attribute value has to be token identical tothe left side of a =q constraint.
The right side is setto the just identified handle h8 of the verb.4.3 GenerationThe same grammars used for parsing can also beused by the generator of the Lexical KnowledgeBuilder Environment (Copestake, 2002) to gener-ate an n-best list of surface realizations given anMRS structure.
However, we only consider the high-est ranked realization.
For the English generation,a generation ranking model is provided within theDELPH-IN project, thus providing a more confidentn-best list.
For the current Japanese grammar, nosuch model is available.An example of a successful generation can befound in Table 2.
On the English side, two surfacevariations are generated.
The Japanese realizationsshow more variations in honorification and aspect.We can only negate sentence pairs in both lan-guages for 13.3% of the training data (18,727).
Thisis mainly because of the brittleness of the Japanesegeneration (Goodman and Bond, 2009).
Further,there are multiple ways of negating sentences andwe do not always select the correct one.4.4 Expanded Parallel Corpus CompilationThe method for assembling the expanded version ofthe parallel corpus for the use as training or devel-opment data directly influences translation quality.This is also demonstrated in Nichols et al (2010),where various versions of padding out the data andpreserving the word distribution are compared.
Thereported differences in performance suggest the im-portance of the method.
Therefore, we have experi-mented with the following versions:?
Append: The obtained negated sentence pairsare added to the original corpus.
Only the high-est ranked realization per sentence for each lan-guage is considered.
Thus they are aligned witheach other.
This leads to the addition of the fol-lowing sentence pair where bilingual negationwas successful:(en original,jp original)(en negated 1,jp negated 1) added?
Padding: In order to preserve the word dis-tribution as mentioned above, we addition-ally padded out the sentence pairs by copying,where no bilingual negation was possible:(en original,jp original)(en original,jp original) added?
Replace: For emphasizing the impact ofnegated sentences, a variant of Append wascompiled.
Instead of adding the original pair ofa successful bilingual negation the former wasreplaced by the latter:(en negated 1,jp negated 1) substi-tutedAnother way of testing the quality of the gener-ated rephrases is to include them in the languagemodel training.
The expectation is that when therephrases are of good quality, then the languagemodel will be better and in turn should have posi-tive result on the overall SMT.5 Experiments & EvaluationWe experiment with the phrase-based statistical ma-chine translation toolkit Moses (Koehn et al, 2007)in order to train a Japanese - English system andto show the influence of the expanded parallel cor-pora obtained with negation rephrasing on transla-tion performance.5.1 DataThe Tanaka corpus is used as a basis for our exper-iments.
We tokenize and truecase the English side,the Japanese side is already tokenized and there areno case distinctions.
Sentences longer than 40 to-kens are removed.
For evaluation, the English partis recased and detokenized.23English Japaneseoriginal I aim to be a writer.
???????????
?negated I don?t aim to be a writer.
???????????
?I do not aim to be a writer.
??????????????????????????????????????????????????????
?Table 2: English and Japanese generations of a successfully rephrased sentence pair.The sentence and token statistics for the originalTanaka corpus and our various extensions are listedin Table 3.
The original corpus version acts as base-line data with profiles 006?100 as training and 000?002 as development data.
For the extended systems,the training data as described in Section 4.4 is used.The same methods are applied on the developmentportion of the Tanaka corpus for tuning.
The full testdata has 42,305 English and 53,242 Japanese tokensand 4,500 sentences and is equal to the Tanaka cor-pus profiles 003?005.The language model training data is in almost allcases equal to the original English Tanaka trainingdata.
Only in the Append + neg LM experiment, thetraining data for the language model is equal to theAppend training data, except that it is slightly larger,since long sentences have not been filtered out.
Theexpanded language model training data consists of1,476,231 tokens and 160,069 sentences.5.2 Different Test SetsIn order to find out the performance of the baselineand the extended systems on negative sentences, thetest data has to be split up into several subsets, mostnotably neg-strict and pos-strict.
The former onlycontains negated sentences, the latter only positivesentences.
The definition of both is based on the ex-istence of a negation relation EP in the semantics ofthe sentence.
In order to obtain the semantic struc-ture, the sentence pairs have to be parsed success-fully.
This also means, we will have some sentencepairs for which we cannot make a decision.
There-fore, we provide a third test subset biparse, whichcontains all the parsable sentence pairs.
This set re-veals the big jump of BLEU score compared to thefourth test set al, which is the regular test set of theTanaka corpus.
A combined dataset with pos-strict-neg-strict is provided, which is the union of the firsttwo sets.5.3 SetupWe use Moses (SVN revision 4293) with Giza++(Och and Ney, 2003) and the SRILM toolkit 1.5.12(Stolcke, 2002).
The language model is trained asa 5-order model with Kneser-Ney discounting.
TheGiza++ alignment heuristic grow-diag-final-and isused.
All systems are tuned with MERT (Och,2003).
Several tunings for each system are run, thebest performing ones are reported here.5.4 ResultsThe results of our experiments can be seen in Ta-ble 4.
The baseline is outperformed by our two bestvariations Append and Append + neg LM with re-spect to the entire test set.
The differences in BLEUpoints are 0.14 and 0.16, which are not statisticallysignificant according to the paired bootstrap resam-pling method (Koehn, 2004).When looking at the test set neg-strict that onlycontains negated sentences, our improvement ismuch more apparent.
The gain of our best perform-ing model Append + neg LM compared to the base-line is at 1.63 BLEU points, which is statisticallysignificant (p < 0.05).
On the other hand there isa statistically insignificant drop of 0.30 with pos-strict.The model with the expanded language modeltraining data (Append + neg LM) always performs24Tokens Sentencestrain dev train devBaseline 1,300,821 / 1,641,591 42,248 / 52,822 141,147 4,500Append 1,469,569 / 1,841,139 47,905 / 59,400 159,874 5,121Padding 2,628,757 / 3,293,246 85,422 / 105,952 282,294 9,000Replace 1,327,936 / 1,651,655 43,174 / 53,130 141,147 4,500Table 3: Counts of tokens and sentences of the original Tanaka corpus and our expanded versions.
Tokensare split up in English/Japanese counts.better than the model under the same conditions ex-cept language model training data (Append).When padding out the original data to preserve theword distribution in Padding, the effect of the addi-tional negated training pairs is not strong enough.Both scores on the entire test set, as well as on thenegation specific test set drop below the baseline.This version performs slightly better overall com-pared to Replace, however, on neg-strict it is a lotworse.We manually checked the neg-strict test data setof our best performing system Append + neg LMversus the baseline, checking only whether the nega-tion was translated or not (ignoring the overall qual-ity).
For 146 sentences, both systems correctlytranslated the negation.
For 76 sentences both sys-tems failed to translate the negation.
For 33 sen-tences Append + neg LM translated the negationwhere the baseline system did not, and for 30 sen-tences the baseline system translated the negationbut Append + neg LM did not.
Overall, we reducedthe number of critical negation errors from 99 to 96.Some example sentences are given in Figure 2.6 DiscussionFor identifying the performance of a state-of-the-artbaseline system on negated sentences, we have splitthe test data into several distinct sets.
The transla-tion quality drops considerably by about 3 BLEUpoints when looking at the negative data comparedto the parsable test data biparse.
This big declineand the difference between performance on negativevs.
positive test data shows that there is great poten-tial to improve SMT systems by tackling this prob-lem.
Our approach is successful in handling nega-tions better and thus diminishing the discrepancy ofthe two sets.As the results show, there is only a small decreaseof BLEU score points on the positive test data.
Andon the negative test data, the increase is substan-tially higher.
Nevertheless, the overall performancein terms of BLEU only reflects this high increase toa certain degree.
This can be attributed to the factthat the test data has a similar distribution to that ofthe training data, i.e.
the proportion of negative sen-tences is low.
Thus, the big increase gets diluted inthe overall test data.The results further show that improvement on thenegative test data set comes at the cost of a slightdegradation of performance on the positive data setand hence also on the full test set.
This behaviouris not surprising due to the fact that a positive andits negative correspondent only vary very little whenlooking at the surface structure.
The models trainedwith our extended data are aimed at providing onemodel which provides a balance between this gainand the loss.This notion suggests that one would benefit fromproviding two separate translation models, one fornegated input data and one for positive data.
In thissetting, the ample amount of negative training datathat we generated through rephrasing could be ex-ploited even more.
A yet higher increase of BLEUscore is expected.
This of course requires a prepro-cessing step that confidently splits up the data ac-cordingly.
However, since we have the grammars athand that can reliably determine whether there is asemantic negation relation in the input, this step canbe solved easily.
One small disadvantage with thisidea is that a decision can only be made if the gram-25Test data sets all biparse neg-strict pos-strict pos-strict-neg-strictSentence counts 4500 3399 285 2684 2969Baseline 22.87 25.76 22.77 26.60 26.25Append 23.01 25.78 24.04 26.22 26.25Append + neg LM 23.03 25.88 24.40 26.30 26.28Padding 22.74 25.54 22.62 26.35 26.06Replace 22.55 25.35 23.36 26.00 25.84Table 4: Japanese-English translation evaluation results of the baseline and our extended systems.mar of the input language produces a parse for theinput sentence.
This however can be circumventedby backing off to the well balanced model presentedin this work.
In other words, we use a positive modelfor positive sentences, a negative model for negativesentences and a balanced model if we are not sure.Our method depends on two large-scale deep se-mantic grammars.
However, developing such gram-mars has been made much more efficient with theemergence of the Grammar Matrix (Bender et al,2002).
There is is already a large collection of work-ing grammars, which can readily be tried out.
Inaddition to the ERG and Jacy, there are grammarsfor German, French, Korean, Modern Greek, Nor-wegian, Spanish, Portuguese, and more, with vary-ing levels of coverage.2Because parsing, rephrasing and generation donot have 100% coverage, we cannot produce negatedversions of all sentences.
The rephrasing can onlywork when both sides of a sentence pair are parsable.Furthermore, not every rephrased sentence pair canbe successfully realized.
However, we still manageto build far more negated training data than is oth-erwise available: more than doubling the amount.This could be further increased by a little more workon the generation, especially for Jacy.
In addition,we have not made use of all the generated data, i.e.lower ranked realizations have been discarded eventhough they may still be useful.Furthermore, we have shown in the experiment re-sults that using our expanded version for languagemodel training is also of great benefit, since wecould achieve not only an overall increase, but es-pecially one on negated test data.2moin.delph-in.net/GrammarCatalogue7 Conclusion & Future WorkWe have presented an approach which alleviatesthe negation translation difficulties of phrase-basedSMT.
We have tackled the problem by automati-cally expanding the training data with negated sen-tence pairs.
The additional data has been obtainedby rephrasing existing data based on the semanticstructure of the input.Our experiments with the phrase-based SMT sys-tem Moses show small improvements over the base-line considering the entire test data.
A more dis-tinct look at only negated sentences in the test datashows a statistically significant improvement of 1.63BLEU points.
The best performing model representsa good balance of a high BLEU score increase on thenegated test data vs. a statistically insignificant de-crease on the positive test data, yet achieving a smalloverall improvement.
Furthermore, it was shown,that expanding not only the translation training data,but also the language model training data boosts per-formance even more.Our method works on the semantic level and canbe easily adapted to other languages.
Having ac-cess to a deep semantic structure opens possible ex-tensions along our idea.
On the one hand negationrephrasing could be refined in order to have a highergeneration rate.
On the other hand, other phenomenacould also be tackled in the same way: e.g.
rephras-ing declarative statements to interrogatives.Just for negation, the corpora expanded with ourhigh quality negations could be combined with thesyntactic reordering strategies presented in Section 2such that the negation reordering rule has more train-ing data and thus a bigger influence on the overallperformance.26AcknowledgementsThis research was supported in part by the ErasmusMundus Action 2 program MULTI of the EuropeanUnion, grant agreement number 2009-5259-5.ReferencesBender, E. M., Flickinger, D., and Oepen, S. (2002).The grammar matrix: An open-source starter-kitfor the rapid development of cross-linguisticallyconsistent broad-coverage precision grammars.
InProceedings of the Workshop on Grammar Engi-neering and Evaluation at the 19th InternationalConference on Computational Linguistics, pages8?14, Taipei, Taiwan.Bender, E. M. and Siegel, M. (2004).
Implement-ing the syntax of Japanese numeral classifiers.
InProceedings of the IJC-NLP-2004.Bond, F., Kuribayashi, T., and Hashimoto, C.(2008).
Construction of a free Japanese tree-bank based on HPSG.
In 14th Annual Meetingof the Association for Natural Language Process-ing, pages 241?244, Tokyo.
(in Japanese).Callison-Burch, C., Koehn, P., and Osborne, M.(2006).
Improved statistical machine translationusing paraphrases.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 17?24, New York City,USA.
Association for Computational Linguistics.Callmeier, U.
(2000).
PET - a platform for exper-imentation with efficient HPSG processing tech-niques.
Natural Language Engineering, 6(1):99?108.Collins, M., Koehn, P., and Kucerova, I.
(2005).Clause Restructuring for Statistical MachineTranslation.
In Proceedings of the 43rd AnnualMeeting of the ACL, Ann Arbor, Michigan.
ACL.Copestake, A.
(2002).
Implementing Typed FeatureStructure Grammars.
CSLI Publications.Copestake, A., Flickinger, D., Pollard, C., and Sag,I.
A.
(2005).
Minimal Recursion Semantics ?
AnIntroduction.
Research on Language and Compu-tation, 3:281?332.Flickinger, D. (2000).
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.
(Special Issue on Effi-cient Processing with HPSG).Gao, Q. and Vogel, S. (2011).
Corpus expansionfor statistical machine translation with semanticrole label substitution rules.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Tech-nologies, pages 294?298, Portland, Oregon, USA.Association for Computational Linguistics.Goodman, M. W. and Bond, F. (2009).
Using gen-eration for grammar analysis and error detection.In Joint conference of the 47th Annual Meetingof the Association for Computational Linguisticsand the 4th International Joint Conference onNatural Language Processing of the Asian Fed-eration of Natural Language Processing, pages109?112, Singapore.He, W., Zhao, S., Wang, H., and Liu, T. (2011).Enriching smt training data via paraphrasing.
InProceedings of 5th International Joint Conferenceon Natural Language Processing, pages 803?810,Chiang Mai, Thailand.
Asian Federation of Natu-ral Language Processing.Koehn, P. (2004).
Statistical Significance Tests forMachine Translation Evaluation.
In Proceed-ings of EMNLP 2004, pages 388?395, Barcelona,Spain.
Association for Computational Linguis-tics.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,Federico, M., Bertoldi, N., Cowan, B., Shen, W.,Moran, C., Zens, R., Dyer, C., Bojar, O., Con-stantin, A., and Herbst, E. (2007).
Moses: OpenSource Toolkit for Statistical Machine Transla-tion.
In Annual Meeting of the ACL.Marton, Y., Callison-Burch, C., and Resnik, P.(2009).
Improved statistical machine translationusing monolingually-derived paraphrases.
In Pro-ceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages381?390, Singapore.
Association for Computa-tional Linguistics.Nichols, E., Bond, F., Appling, D. S., and Mat-sumoto, Y.
(2010).
Paraphrasing Training Datafor Statistical Machine Translation.
Journal ofNatural Language Processing, 17(3):101?122.27Och, F. J.
(2003).
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting of ACL, pages 160?167.Och, F. J. and Ney, H. (2003).
A Systematic Com-parison of Various Statistical Alignment Models.Computational Linguistics, 29:19?51.Stolcke, A.
(2002).
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
Intl.
Conf.
onSpoken Language Processing, volume 2, pages901?904, Denver.Tanaka, Y.
(2001).
Compilation of a multilingualparallel corpus.
In Proceedings of PACLING2001, pages 265?268, Kyushu.28Japanese ??????????????
?Baseline They played tennis yesterday.Append + neg LM They do not play tennis yesterday.Reference Yesterday they didn?t play tennis, because it rained.
(a) Baseline fails to translate the negation.Japanese ????????????????????????
?Baseline He is sure to break your promise, I?m sure.Append + neg LM He never breaks his word, I?m sure.Reference I?m sure he won?t fail to keep his word.
(b) Correct translation by our system with valid variation of wording.Japanese ?????????????????????
?Baseline I was when I came home, he was asleep.Append + neg LM I came home when he is not asleep.Reference He wasn?t sleeping when I came home.
(c) Baseline omits the negation.Japanese ??????????????
?Baseline Money with me.Append + neg LM I don?t have any money with me.Reference I don?t have any money with me.
(d) Baseline omits subject, verb and negation.Japanese ??????????????????
?Baseline The????
in Japan, I cannot see it.Append + neg LM The????
in Japan.Reference The Southern Cross is not to be seen in Japan.
(e) Our system does not translate a part of the sentence.Japanese ???????????
?Baseline Don?t speak in a loud voice.Append + neg LM You must speak in a loud voice.Reference You must not speak loudly.
(f) Our system omits the negation.Japanese ?????????
?Baseline She has no friends.Append + neg LM She is a friend of mine.Reference She doesn?t have a boy friend.
(g) Our system does not produce a negation.
The object is incorrectly trans-lated in both systems.Figure 2: Sentences from the neg-strict test set showing differences between the baseline and our bestperforming system Append + neg LM.
Examples in (a?d) show improvements, (e?g) show degradations.29
