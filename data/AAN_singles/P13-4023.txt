Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 133?138,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsHYENA-live: Fine-Grained Online Entity Type Classification fromNatural-language TextMohamed Amir Yosef1 Sandro Bauer2 Johannes Hoffart1Marc Spaniol1 Gerhard Weikum1(1) Max-Planck-Institut fu?r Informatik, Saarbru?cken, Germany(2) Computer Laboratory, University of Cambridge, UK{mamir|jhoffart|mspaniol|weikum}@mpi-inf.mpg.desandro.bauer@cl.cam.ac.ukAbstractRecent research has shown progress inachieving high-quality, very fine-grainedtype classification in hierarchical tax-onomies.
Within such a multi-level typehierarchy with several hundreds of types atdifferent levels, many entities naturally be-long to multiple types.
In order to achievehigh-precision in type classification, cur-rent approaches are either limited to certaindomains or require time consuming multi-stage computations.
As a consequence, ex-isting systems are incapable of performingad-hoc type classification on arbitrary inputtexts.
In this demo, we present a novel Web-based tool that is able to perform domainindependent entity type classification underreal time conditions.
Thanks to its efficientimplementation and compacted feature rep-resentation, the system is able to processtext inputs on-the-fly while still achievingequally high precision as leading state-of-the-art implementations.
Our system offersan online interface where natural-languagetext can be inserted, which returns seman-tic type labels for entity mentions.
Furthermore, the user interface allows users to ex-plore the assigned types by visualizing andnavigating along the type-hierarchy.1 IntroductionMotivationWeb contents such as news, blogs and other so-cial media are full of named entities.
Each en-tity belongs to one or more semantic types as-sociated with it.
For instance, an entity such asBob Dylan should be assigned the types Singer,Musician, Poet, etc., and also the correspond-ing supertype(s) (hypernyms) in a type hierarchy,in this case Person.
Such fine-grained typing ofentities in texts can be a great asset for variousNLP tasks including semantic role labeling, sensedisambiguation and named entity disambiguation(NED).
For instance, noun phrases such as ?song-writer Dylan?, ?Google founder Page?, or ?rocklegend Page?
can be easily mapped to the entitiesBob Dylan, Larry Page, and Jimmy Page if their re-spective types Singer, BusinessPerson, andGuitarist are available (cf.
Figure 1 for an il-lustrative example).with 100,000$, Google wasFunded"founded by Brin and Page "his firstplayed on" guitar in 1952Page "Business_peopleEntrepreneurEntertainerMusicianFigure 1: Fine-grained entity type classificationProblem StatementType classification is not only be based on hier-archical sub-type relationships (e.g.
MusicianisA Person), but also has to do on multi-labeling.Within a very fine-grained type hierarchy, many en-tities naturally belong to multiple types.
For exam-ple, a guitarist is also a musician and a person, butmay also be a singer, an actor, or even a politician.Consequently, entities should not only be assignedthe most (fine-grained) label associated to them,but with all labels relevant to them.
So we facea hierarchical multi-label classification problem(Tsoumakas et al 2012).ContributionThis paper introduces HYENA-live, which allowsan on-the-fly computation of semantic types for en-tity mentions, based on a multi-level type hierarchy.Our approach uses a suite of features for a givenentity mention, such as neighboring words and bi-133grams, part-of-speech tags, and also phrases from alarge gazetteer derived from state-of-the-art knowl-edge bases.
In order to perform ?live?
entity typeclassification based on ad-hoc text inputs, severalperformance optimizations have been undertakento operate under real-time conditions.2 Entity Type Classification SystemsState-of-the-art tools for named entity recognitionsuch as the Stanford NER Tagger (Finkel et al2005) compute semantic tags only for a small set ofcoarse-grained types: Person, Location, andOrganization (plus tags for non-entity phrasesof type time, money, percent, and date).
However,we are not aware of any online tool that performsfine-grained typing of entity mentions.
The mostcommon workaround to perform entity classifica-tion is a two-stage process: in first applying an on-line tool for Named-Entity Disambiguation (NED),such as DBpedia Spotlight (Mendes et al 2011)or AIDA (Yosef et al 2011; Hoffart et al 2011),in order to map the mentions onto canonical enti-ties and subsequently query the knowledge base fortheir types.
In fact, (Ling and Weld, 2012) followedthis approach when comparing their entity classi-fication system results against those obtained byan adoption of the Illinois?
Named-Entity Linkingsystem (NEL) (Ratinov et al 2011) and reachedthe conclusion that while NEL performed decentlyfor prominent entities, it could not scale to coverlong tail ones.
Specifically, entity typing via NEDhas three major drawbacks:1.
NED is an inherently hard problem, especiallywith highly ambiguous mentions.
As a conse-quence, accurate NED systems come at a highcomputation costs.2.
NED only works for those mentions that cor-respond to a canonical entity within a knowl-edge base.
However, this fails for all out-of-knowledge-base entities like unregistered per-sons, start-up companies, etc.3.
NED heavily depends on the quality of the un-derlying knowledge base.
Yet, only very fewknowledge bases have comprehensive classlabeling of entities.
Even more, in the bestcase, coverage drops sharply for relatively un-common entities.We decided to adopt one of the existing ap-proaches to make it suitable for online querying.We considered five systems.
In the rest of thissection we will briefly describe each of them.
(Fleischman and Hovy, 2002) is one of the earli-est approaches to perform entity classification intosubtypes of PERSON.
They developed a decision-tree classifier based on contextual features that canbe automatically extracted from the text.
In orderto account for scarcity of labeled training data, theytapped on WordNet synonyms to achieve highercoverage.
While their approach is fundamentallysuitable, their type system is very restricted.
In or-der to account for more fine-grained classes, morefeatures need to be added to their feature set.
(Ekbal et al 2010) considered 141 subtypes ofWordNet class PERSON and developed a maximumentropy classifier exploiting the words surroundingthe mentions together with their POS tags and othercontextual features.
Their type hierarchy is fine-grained, but still limited to sub classes of PERSON.In addition, their experimental results have beenflagged as non-reproducible in the ACL Anthology.
(Altaf ur Rahman and Ng, 2010) considered atwo-level type hierarchy consisting of 29 top-levelclasses and a total of 92 sub-classes.
These includemany non-entity types such as date, time, percent,money, quantity, ordinal, cardinal, etc.
They in-corporated a hierarchical classifier using a rich fea-ture set and made use of WordNet sense tagging.However, the latter requires human interception,which is not suitable for ad-hoc processing of out-of-domain texts.
(Ling and Weld, 2012) developed FIGER,which classifies entity mentions onto a two-leveltaxonomy based on the Freebase knowledge base(Bollacker et al 2008).
This results in a two-levelhierarchy with top-level topics and 112 types.
Theytrained a CRF for the joint task of recognizing en-tity mentions and inferring type tags.
Althoughthey handle multi-label assignment, their test datais sparse.
Many classes are absent and plenty ofinstances come with only a single label (e.g.
216of the 562 entities were of type PERSON withoutsubtypes).
Further, their results are instance based,which does not guarantee that the quality of theirsystem will be reproducible for all the 112 types intheir taxonomy.
(Yosef et al 2012) is the most recent work inmulti-label type classification.
The HYENA sys-tem incorporates a large hierarchy of 505 classes134organized under 5 top level classes, with 100 de-scendant classes under each of them.
The hierarchyreaches a depth of up to 9 levels in some parts.The system is based on an SVM classifier using acomprehensive set of features and provides resultsfor all classes of a large data set.
In their exper-iments the superiority of the system in terms ofprecision and recall has been shown.
However, themain drawback of HYENA comes from its largehierarchy and the extensive set of features extractedfrom the fairly large training corpus it requires.
Asa result, on-the-fly type classification with HYENAis impossible in its current implementation.We decided to build on top of HYENA sys-tem by spotting the bottlenecks in the architec-ture and modifying it accordingly to be suitablefor online querying.
In Section 3 we explain indetails HYENA?s type taxonomy and their featureportfolio.
Later on, we explain the engineeringundertaken in order to develop the on-the-fly typeclassification system HYENA-live (cf.
Section 4).3 Type Hierarchy and Feature Set3.1 Fine-grained TaxonomyThe type system is an automatically gathered fine-grained taxonomy of 505 classes.
The classes areorganized under 5 top level classes, with 100 de-scendant classes under each.
The YAGO knowl-edge base (Hoffart et al 2013) is selected to de-rive the taxonomy from because of its highly pre-cise classification of entities into WordNet classes,which is a result of the accurate mapping YAGOhas from Wikipedia Categories to WordNet synsets.We start with five top classes namely PERSON,LOCATION, ORGANIZATION, EVENT andARTIFACT.
Under each top class, the most 100prominent descendant classes are picked.
Promi-nence is estimated by the number of YAGO entitiestagged with this class.
This results in a very-finegrained taxonomy of 505 types, represented as adirected acyclic graph with 9 levels in its deepestparts.
While the classes are picked from the YAGOtype system, the approach is generic and can beapplied to derive type taxonomies from otherknowledge bases such as Freebase or DBpedia(Auer et al 2007) as in (Ling and Weld, 2012).3.2 Feature SetFor the sake of generality and applicability to ar-bitrary text, we opted for features that can be au-tomatically extracted from the input text withoutany human interaction, or manual annotation.
Theextracted features fall under five categories, whichwe briefly explain in the rest of this section.Mention StringWe derive four features from the entity mentionstring.
The mention string itself, a noun phraseconsisting of one or more consecutive words.
Theother three features are unigrams, bigrams, andtrigrams that overlap with the mention string.Sentence Surrounding MentionWe also exploit a bounded-size window around themention to extract four features: all unigrams, bi-grams, and trigrams.
Two versions of those featuresare extracted, one to account for the occurrence ofthose tokens around the mention, and another to ac-count for the position at which they occurred withrespect to the mention (before or after).
In addition,unigrams are also included with their absolute dis-tance ignoring whether before of after the mention.Our demo is using a conservative threshold for thesize of the window which is three tokens on eachside of the mention.Mention ParagraphWe also leverage the entire paragraph of the men-tion.
This gives additional topical cues about themention type (e.g., if the paragraph is about a mu-sic concert, this is a cue for mapping people namesto musician types).
We create three features here:unigrams, bigrams, and trigrams without includingany distance information.
In our demo, we extractthose features from a bounded window of size 2000characters before and after the mention.Grammatical FeaturesWe exploit the semantics of the text by extractingfour features.
First, we use part-of-speech tags ofthe tokens in a size-bounded window around themention in distance and absolute distance versions.Second and third, we create a feature for the firstoccurrence of a ?he?
or ?she?
pronoun in the samesentence and in the subsequent sentence followingthe mention, along with the distance to the mention.Finally, we use the closest verb-preposition pairpreceding the mention as another feature.Gazetteer FeaturesWe leverage YAGO2 knowledge base even furtherby building a type-specific gazetteer of words oc-135# of articles 50,000# of instances (all types) 1,613,340# of location instances 489,003 (30%)# of person instances 426,467 (26.4%)# of organization instances 219,716 (13.6%)# of artifact instances 204,802 (12.7%)# of event instances 176,549 (10.9%)# instances in 1 top-level class 1,131,994 (70.2%)# instances in 2 top-level classes 182,508 (11.3%)# instances in more than 2 top-level classes 6,492 (0.4%)# instances not in any class 292,346 (18.1%)Table 1: Properties of the labeled data used for training HYENA-livecurring in the names of the entities of that type.YAGO2 knowledge base comes with an exten-sive dictionary of name-entity pairs extracted fromWikipedia redirects and link-anchor texts.
We con-struct, for each type, a binary feature that indicatesif the mention contains a word occurring in thistype?s gazetteer.
Note that this is a fully automatedfeature construction, and it does by no means de-termine the mention type(s) already, as most wordsoccur in the gazetteers of many different types.
Forexample, ?Alice?
occurs in virtually every subclassof Person but also in city names like ?Alice Springs?and other locations, as well as in songs, movies,and other products or organizations.4 System Implementation4.1 OverviewAs described in Section 3, HYENA classifies men-tions of named entities onto a hierarchy of 505types using large set of features.
A random sub-set of the English Wikipedia has been used fortraining HYENA.
By exploiting Wikipedia anchorlinks, mentions of named entities are automati-cally disambiguated to their correct entities.
EachWikipedia named entity has a corresponding YAGOentity labeled with an accurate set of types, andhence we effortlessly obtain a huge training dataset (cf.
data properties in Table 1).We build type-specific classifiers using the SVMsoftware LIBLINEAR (cf.
http://liblinear.bwaldvogel.de/).
Each model comes with a com-prehensive feature set.
While larger models (withmore features) improve the accuracy, they signifi-cantly affect the applicability of the system.
A sin-gle model file occupies around 150MB disk spaceleading to a total of 84.7GB for all models.
Asa consequence, there is a substantial setup timeto load all models in memory and a high-memoryserver (48 cores with 512GB of RAM) is requiredfor computation.
An analysis showed that each sin-gle feature contributes to the overall performanceof HYENA, but only a tiny subset of all features isrelevant for a single classifier.
Therefore, most ofthe models are extremely sparse.4.2 Sparse Models RepresentationThere are several workarounds applicable to batchmode operations, e.g.
by performing classificationsper level only.
However, this is not an option foron-the-fly computations.
For that reason we optedfor a sparse-model representation.LIBLINEAR model files are normalized textualfiles: a header (data about the model and the to-tal number of features), followed by listing theweights assigned to each feature (line number in-dicates the feature ID).
Each model file has beenpost-processed to produce 2 files:?
A compacted model file containing only fea-tures of non-zero weights.
Its header reflectsthe reduced number of features.?
A meta-data file.
It maps the new features IDsto the original feature IDs.Due to the observed sparsity in the model files,particularly at deeper levels, there is a significantdecrease in disk space consumption for the com-pacted model files and hence in the memory re-quirements.4.3 Sparse Models ClassificationBy switching to the sparse model representation thearchitecture of the whole system is affected.
In par-ticular, modified versions of feature vectors needto be generated for each classifier; this is because136,nSXt7e[t)eatXre([traFtor&lassifiFation0odels6Sarse 0odels0eta'ata6Sarse 0odel5eSresentation3ost3roFessing)eatXre 9eFtor&lassifier'eFision0odel6SeFifiF)eatXre 9eFtorFigure 2: Modified system architecture designed for handling sparse modelsa lot of features have been omitted from specificclassifiers (those with zero weights).
Consequently,the feature IDs need to be mapped to the new fea-ture space of each classifier.
The conceptual designof the new architecture is illustrated in Figure 4.2.5 Demo PresentationHYENA-live has been fully implemented as a Webapplication.
Figure 5 shows the user interface ofHYENA-live in a Web browser:1) On top, there is a panel where a user can inputany text, e.g.
by copy-and-paste from news ar-ticles.
We employ the Stanford NER Tagger toidentify noun phrases as candidates of entitymentions.
Alternatively, users can flag entitymentions by double brackets (e.g.
?Harry isthe opponent of [[you know who]]?).
For thesake of simplicity, detected entity mentions byHYENA-live are highlighted in yellow.
Eachmention is clickable to study its type classifi-cation results.2) The output of type classification is shown in-side a tabbed widget.
Each tab correspondsto a detected mention by the system and tabsare sorted by the order of occurrence in theinput text.
To open a tab, the tab header or thecorresponding mention in the input area needsto be clicked.3) The type classification of a mention is shownas a color-coded interactive tree.
While theoriginal type hierarchy is a directed acyclicgraph, for the ease of navigation the classifi-cation output has been converted into a tree.In order to do so, nodes that belong to morethan a parent have been duplicated.
There arethree different types of nodes:?
Green Nodes: referring to a class that hasbeen accepted by the classifier.
Thesenodes can be further expanded in orderto check which sub-classes have beenaccepted or rejected by HYENA-live.?
Red Nodes: corresponding to a class thatwas rejected by the classifier, and henceHYENA-live did not traverse deeper totest its sub-classes.?
White Nodes: matching classes that havenot been tested.
These nodes are eitherknown upfront (e.g.
ENTITY) or theirsuper class was rejected by the system.It is worth noting that HYENA-live automati-cally adjusts the layouting so that as much aspossible of the hierarchy is shown to the user.For the sake of explorability, this is being dy-namically adjusted once the user decides tonavigate along a certain (child-)node.The system is available online at:d5gate.ag5.mpi-sb.mpg.de/webhyena/.The data transfer between the client and the serveris done via JSON objects.
Hence, we also provideHYENA-live as a JSON compliant entity classi-fication Web-service.
As a result, the back-endbecomes easily interchangeable (e.g.
by a differentclassification technique or a different type taxon-omy) with minimum modifications required on theuser interface side.AcknowledgmentsThis work is supported by the 7th Framework IST programmeof the European Union through the focused research project(STREP) on Longitudinal Analytics of Web Archive data(LAWA) under contract no.
258105.137Figure 3: Interactively exploring the types of the ?Battle of Waterloo?
in the HYENA-live interfaceReferencesMd.
Altaf ur Rahman and Vincent Ng.
2010.
Inducingfine-grained semantic classes via hierarchical andcollective classification.
In COLING, pages 931?939.So?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, and Zachary Ives.
2007.
Dbpedia: A nu-cleus for a web of open data.
In ISWC, pages 11?15.Springer.Kurt D. Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In SIGMOD, pages 1247?1250.Asif Ekbal, Eva Sourjikova, Anette Frank, and Si-mone P. Ponzetto.
2010.
Assessing the challenge offine-grained named entity recognition and classifica-tion.
In Named Entities Workshop, pages 93?101.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In ACL, pages 363?370.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In COLING,pages 1?7.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen Fu?rstenau, Manfred Pinkal, Marc Span-iol, Bilyana Taneva, Stefan Thater, and GerhardWeikum.
2011.
Robust disambiguation of namedentities in text.
In EMNLP, pages 782?792.Johannes Hoffart, Fabian M. Suchanek, KlausBerberich, and Gerhard Weikum.
2013.
YAGO2: Aspatially and temporally enhanced knowledge basefrom wikipedia.
Artificial Intelligence, 194(0):28 ?61.Xiao Ling and Daniel S. Weld.
2012.
Fine-grainedentity recognition.
In AAAI, pages 94?100.Pablo N. Mendes, Max Jakob, Andre?s Garc?
?a-Silva,and Christian Bizer.
2011.
Dbpedia spotlight:shedding light on the web of documents.
In I-SEMANTICS, pages 1?8.Lev-Arie Ratinov, Dan Roth, Doug Downey, and MikeAnderson.
2011.
Local and global algorithms fordisambiguation to wikipedia.
In ACL, pages 1375?1384.Grigorios Tsoumakas, Min-Ling Zhang, and Zhi-HuaZhou.
2012.
Introduction to the special issue onlearning from multi-label data.
Machine Learning,88(1-2):1?4.Mohamed Amir Yosef, Johannes Hoffart, Ilaria Bor-dino, Marc Spaniol, and Gerhard Weikum.
2011.AIDA: An online tool for accurate disambiguationof named entities in text and tables.
PVLDB,4(12):1450?1453.Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-fart, Marc Spaniol, and Gerhard Weikum.
2012.HYENA: Hierarchical Type Classification for EntityNames.
In COLING, pages 1361?1370.138
