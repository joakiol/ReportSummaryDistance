Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19?27,COLING 2010, Beijing, August 2010.Source-side Syntactic Reordering Patterns with Functional Words forImproved Phrase-based SMTJie Jiang, Jinhua Du, Andy WayCNGL, School of Computing, Dublin City University{jjiang,jdu,away}@computing.dcu.ieAbstractInspired by previous source-side syntacticreordering methods for SMT, this paperfocuses on using automatically learnedsyntactic reordering patterns with func-tional words which indicate structural re-orderings between the source and targetlanguage.
This approach takes advan-tage of phrase alignments and source-sideparse trees for pattern extraction, and thenfilters out those patterns without func-tional words.
Word lattices transformedby the generated patterns are fed into PB-SMT systems to incorporate potential re-orderings from the inputs.
Experimentsare carried out on a medium-sized cor-pus for a Chinese?English SMT task.
Theproposed method outperforms the base-line system by 1.38% relative on a ran-domly selected testset and 10.45% rela-tive on the NIST 2008 testset in termsof BLEU score.
Furthermore, a systemwith just 61.88% of the patterns filteredby functional words obtains a comparableperformance with the unfiltered one on therandomly selected testset, and achieves1.74% relative improvements on the NIST2008 testset.1 IntroductionPrevious work has shown that the problem ofstructural differences between language pairs inSMT can be alleviated by source-side syntacticreordering.
Taking account for the integrationwith SMT systems, these methods can be dividedinto two different kinds of approaches (Elming,2008): the deterministic reordering and the non-deterministic reordering approach.To carry out the deterministic approach, syntac-tic reordering is performed uniformly on the train-ing, devset and testset before being fed into theSMT systems, so that only the reordered sourcesentences are dealt with while building duringthe SMT system.
In this case, most work is fo-cused on methods to extract and to apply syntac-tic reordering patterns which come from manuallycreated rules (Collins et al, 2005; Wang et al,2007a), or via an automatic extraction process tak-ing advantage of parse trees (Collins et al, 2005;Habash, 2007).
Because reordered source sen-tence cannot be undone by the SMT decoders (Al-Onaizan et al, 2006), which implies a systematicerror for this approach, classifiers (Chang et al,2009b; Du & Way, 2010) are utilized to obtainhigh-performance reordering for some specializedsyntactic structures (e.g.
DE construction in Chi-nese).On the other hand, the non-deterministic ap-proach leaves the decisions to the decoders tochoose appropriate source-side reorderings.
Thisis more flexible because both the original andreordered source sentences are presented in theinputs.
Word lattices generated from syntacticstructures for N-gram-based SMT is presentedin (Crego et al, 2007).
In (Zhang et al, 2007a;Zhang et al, 2007b), chunks and POS tags areused to extract reordering rules, while the gener-ated word lattices are weighted by language mod-els and reordering models.
Rules created from asyntactic parser are also utilized to form weightedn-best lists which are fed into the decoder (Li etal., 2007).
Furthermore, (Elming, 2008; Elm-19ing, 2009) uses syntactic rules to score the outputword order, both on English?Danish and English?Arabic tasks.
Syntactic reordering information isalso considered as an extra feature to improve PB-SMT in (Chang et al, 2009b) for the Chinese?English task.
These results confirmed the effec-tiveness of syntactic reorderings.However, for the particular case of Chinesesource inputs, although the DE construction hasbeen addressed for both PBSMT and HPBSMTsystems in (Chang et al, 2009b; Du & Way,2010), as indicated by (Wang et al, 2007a), thereare still lots of unexamined structures that im-ply source-side reordering, especially in the non-deterministic approach.
As specified in (Xue,2005), these include the bei-construction, ba-construction, three kinds of de-construction (in-cluding DE construction) and general prepositionconstructions.
Such structures are referred withfunctional words in this paper, and all the con-structions can be identified by their correspond-ing tags in the Penn Chinese TreeBank.
It is in-teresting to investigate these functional words forthe syntactic reordering task since most of themtend to produce structural reordering between thesource and target sentences.Another related work is to filter the bilingualphrase pairs with closed-class words (Sa?nchez-Mart?
?nez, 2009).
By taking account of the wordalignments and word types, the filtering processreduces the phrase tables by up to a third, but stillprovide a system with competitive performancecompared to the baseline.
Similarly, our idea is touse special type of words for the filtering purposeon the syntactic reordering patterns.In this paper, our objective is to exploitthese functional words for source-side syntac-tic reordering of Chinese?English SMT in thenon-deterministic approach.
Our assumption isthat syntactic reordering patterns with functionalwords are the most effective ones, and others canbe pruned for both speed and performance.To validate this assumption, three systems arecompared in this paper: a baseline PBSMT sys-tem, a syntactic reordering system with all pat-terns extracted from a corpus, and a syntactic re-ordering system with patterns filtered with func-tional words.
To accomplish this, firstly the lat-tice scoring approach (Jiang et al, 2010) is uti-lized to discover non-monotonic phrase align-ments, and then syntactic reordering patterns areextracted from source-side parse trees.
After that,functional word tags specified in (Xue, 2005) areadopted to perform pattern filtering.
Finally, boththe unfiltered pattern set and the filtered one areused to transform inputs into word lattices topresent potential reorderings for improving PB-SMT system.
A comparison between the threesystems is carried out to examine the performanceof syntactic reordering as well as the usefulness offunctional words for pattern filtering.The rest of this paper is organized as follows:in section 2 we describe the extraction process ofsyntactic reordering patterns, including the latticescoring approach and the extraction procedures.Then section 3 presents the filtering process usedto obtain patterns with functional words.
Afterthat, section 4 shows the generation of word lat-tices with patterns, and experimental setup and re-sults included related discussion are presented insection 5.
Finally, we give our conclusion and av-enues for future work in section 6.2 Syntactic reordering patternsextractionInstead of top-down approaches such as (Wanget al, 2007a; Chang et al, 2009a), we use abottom-up approach similar to (Xia et al, 2004;Crego et al, 2007) to extract syntactic reorderingpatterns from non-monotonic phrase alignmentsand source-side parse trees.
The following stepsare carried out to extract syntactic reordering pat-terns: 1) the lattice scoring approach proposedin (Jiang et al, 2010) is used to obtain phrasealignments from the training corpus; 2) reorder-ing regions from the non-monotonic phrase align-ments are used to identify minimum treelets forpattern extraction; and 3) the treelets are trans-formed into syntactic reordering patterns whichare then weighted by their occurrences in thetraining corpus.
Details of each of these steps arepresented in the rest of this section.2.1 Lattice scoring for phrase alignmentsThe lattice scoring approach is proposed in (Jianget al, 2010) for the SMT data cleaning task.20To clean the training corpus, word alignmentsare used to obtain approximate decoding results,which are then used to calculate BLEU (Papineniet al, 2002) scores to filter out low-scoring sen-tences pairs.
The following steps are taken inthe lattice scoring approach: 1) train an initialPBSMT model; 2) collect anchor pairs contain-ing source and target phrase positions from wordalignments generated in the training phase; 3)build source-side lattices from the anchor pairsand the translation model; 4) search on the source-side lattices to obtain approximate decoding re-sults; 5) calculate BLEU scores for the purpose ofdata cleaning.Note that the source-side lattices in step 3 comefrom anchor pairs, so each edge in the lattices con-tain both the source and target phrase positions.Thus the outputs of step 4 contain phrase align-ments on the training corpus.
These phrase align-ments are used to identify non-monotonic areasfor the extraction of reordering patterns.2.2 Reordering patternsNon-monotonic regions of the phrase alignmentsare examined as potential source-side reorderings.By taking a bottom-up approach, the reorderingregions are identified and mapped to minimumtreelets on the source parse trees.
After that, syn-tactic reordering patterns are transformed fromthese minimum treelets.In this paper, reordering regions A and B indi-cating swapping operations on the source side areonly considered as potential source-side reorder-ings.
Thus, given reordering regions AB, this im-plies (1):AB ?
BA (1)on the source-side word sequences.
Referring tothe phrase alignment extraction in the last section,each non-monotonic phrase alignment producesone reordering region.
Furthermore, for each re-ordering region identified, all of its sub-areas in-dicating non-monotonic alignments are also at-tempted to produce more reordering regions.To represent the reordering region using syn-tactic structure, given the extracted reordering re-gions AB, the following steps are taken to mapthem onto the source-side parse trees, and to gen-erate corresponding patterns:1.
Generate a parse tree for each of the sourcesentences.
The Berkeley parser (Petrov,2006) is used in this paper.
To obtain sim-pler tree structures, right-binarization is per-formed on the parse trees, while tags gener-ated from binarization are not distinguishedfrom the original ones (e.g.
@V P and V Pare the same).2.
Map reordering regions AB onto the parsetrees.
Denote NA as the set of leaf nodes inregion A and NB for region B.
The mappingis carried out on the parse tree to find a mini-mum treelet T , which satisfies the followingtwo criteria: 1) there must exist a path fromeach node in NA ?
NB to the root node ofT ; 2) each leaf node of T can only be theancestor of nodes in NA or NB (or none ofthem).3.
Traverse T in pre-order to obtain syntacticreordering pattern P .
Label all the leaf nodesof T with A or B as reorder options, whichindicate that the descendants of nodes withlabel A are supposed to be swapped withthose with label B.Instead of using subtrees, we use treelets torefer the located parse tree substructures, sincetreelets do not necessarily go down to leaf nodes.Since phrase alignments cannot always be per-fectly matched with parse trees, we also expandAB to the right and/or the left side with a limitednumber of words to find a minimum treelet.
Inthis situation, a minimum number of ancestors ofexpanded tree nodes are kept in T but they are as-signed the same labels as those from which theyhave been expanded.
In this case, the expandedtree nodes are considered as the context nodes ofsyntactic reordering patterns.Figure 1 illustrates the extraction process.
Notethe symbol @ indicates the right-binarization sym-bols (e.g.
@V P in the figure).
In the figure, treeT (surrounded by dashed lines) is the minimumtreelet mapped from the reordering region AB.Leaf node NP is labeled by A, V P is labeled byB, and the context node P is also labeled by A.Leaf nodes labeled A or B are collected into nodesequences LA or LB to indicate the reordering op-21A BTFigure 1: Reordering pattern extractionerations.
Thus the syntactic reordering pattern Pis obtained from T as in (2):P = {V P (PP (P NP ) V P )|O = {LA, LB}}(2)where the first part of P is the V P with its treestructure, and the second part O indicates the re-ordering scheme, which implies that source wordscorresponding with descendants of LA are sup-posed to be swapped with those of LB .2.3 Pattern weights estimationWe use preo to represent the chance of reorderingwhen a treelet is located by a pattern on the parsetree.
It is estimated by the number of reorderingsfor each of the occurrences of the pattern as in (3):preo(P ) =count{reorderings of P}count{observation of P} (3)By contrast, one syntactic pattern P usually con-tains several reordering schemes (specified in for-mula (2)), each of them weighted as in (4):w(O,P ) = count{reorderings of O in P}count{reorderings of P}(4)Generally, a syntactic reordering pattern is ex-pressed as in (5):P = {tree | preo | O1, w1, ?
?
?
, On, wn} (5)where tree is the tree structures of the pattern,preo is the reordering probability, Oi and wi arethe reordering schemes and weights (1 ?
i ?
n).3 Patterns with functional wordsSome of the patterns extracted may not benefitthe final system since the extraction process iscontrolled by phrase alignments rather than syn-tactic knowledge.
Inspired by the study of DEconstructions (Chang et al, 2009a; Du & Way,2010), we assume that syntactic reorderings areindicated by functional words for the Chinese?English task.
To incorporate the knowledge offunctional words into the extracted patterns, in-stead of directly specifying the syntactic struc-ture from the linguistic aspects, we use functionalword tags to filter the extracted patterns.
In thiscase, we assume that all patterns containing func-tional words tend to produce meaningful syntacticreorderings.
Thus the filtered patterns carry the re-ordering information from the phrase alignmentsas well as the linguistic knowledge.
Thus thenoise produced in phrase alignments and the sizeof pattern set can be reduced, so that the speed andthe performance of the system can be improved.The functional word tags used in this paper areshown in Table 1, which come from (Xue, 2005).We choose them as functional words because nor-mally they imply word reorders between Chineseand English sentence pairs.Tag DescriptionBA ba-constructionDEC de (1st kind) in a relative-clauseDEG associative de (1st kind)DER de (2nd kind) in V-de const.
& V-de-RDEV de (3rd kind) before VPLB bei in long bei-constructionP preposition excluding bei and baSB bei in short bei-constructionTable 1: Syntactic reordering tags for functionalwordsNote that there are three kinds of de-constructions, but only the first kind is the DEconstruction in (Chang et al, 2009a; Du & Way,2010).
After the filtering process, both the unfil-tered pattern set and the filtered one are used tobuild different syntactic reordering PBSMT sys-tems for comparison purpose.224 Word lattice constructionBoth the devset and testset are transformed intoword lattices by the extracted patterns to incor-porate potential reorderings.
Figure 2 illustratesthis process: treelet T ?
is matched with a pat-tern, then its leaf nodes {a1, ?
?
?
am} ?
LA (span-ning {w1, ?
?
?
, wp}) are swapped with leaf nodes{b1, ?
?
?
, bn} ?
LB (spanning {v1, ?
?
?
, vq}) onthe generated paths in the word lattice.T?a1am b1bn... ...... ...w1 w2 ... wp v1 v2 vq...w1 w2 ... wp v1 v2 vq...w2...wpv1v2 ...... ...vq w1Sub parse treematched witha patternSource sidesentenceGeneratedlatticeFigure 2: Incorporating potential reorderings intolatticesWe sort the matched patterns by preo in formula(5), and only apply a pre-defined number of re-orderings for each sentence.
For each lattice node,if we denote E0 as the edge from the original sen-tence, while patterns {P1, ?
?
?
, Pi, ?
?
?
, Pk} are ap-plied to this node, then E0 is weighted as in (6):w(E0) = ?
+k?i=1{(1?
?
)k ?
{1?
preo(Pi)}}(6)where preo(Pi) is the pattern weight in formula(3), and ?
is the base probability to avoid E0 be-ing equal to zero.
Suppose {Es, ?
?
?
, Es+r?1} aregenerated by r reordering schemes of Pi, then Ejis weighted as in (7):w(Ej) =(1 ?
?
)k ?preo(Pi)?ws?j+1(Pi)?rt=1 wt(Pi)(7)where wt(Pi) is the reordering scheme in formula(5), and s <= j < s + r. Reordering patternswith the same root lattice node share equal proba-bilities in formula (6) and (7).5 Experiments and resultsWe conducted our experiments on a medium-sizedcorpus FBIS (a multilingual paragraph-alignedcorpus with LDC resource number LDC2003E14)for the Chinese?English SMT task.
The Cham-pollion aligner (Ma, 2006) is utilized to performsentence alignment.
A total number of 256,911sentence pairs are obtained, while 2,000 pairs fordevset and 2,000 pairs for testset are randomly se-lected, which we call FBIS set.
The rest of thedata is used as the training corpus.The baseline system is Moses (Koehn etal., 2007), and GIZA++1 is used to performword alignment.
Minimum error rate training(MERT) (Och, 2003) is carried out for tuning.
A5-gram language model built via SRILM2 is usedfor all the experiments in this paper.Experiments results are reported on two differ-ent sets: the FBIS set and the NIST set.
For theNIST set, the NIST 2005 testset (1,082 sentences)is used as the devset, and the NIST 2008 test-set (1,357 sentences) is used as the testset.
TheFBIS set contains only one reference translationfor both devset and testset, while NIST set hasfour references.5.1 Pattern extraction and filtering withfunctional wordsThe lattice scoring approach is carried out withthe same baseline system as specified above toproduce the phrase alignments.
The initial PB-SMT system in the lattice scoring approach istuned with the FBIS devset to obtain the weights.As specified in section 2.1, phrase alignments aregenerated in the step 4 of the lattice scoring ap-proach.From the generated phrase alignments andsource-side parse trees of the training corpus,we obtain 48,285 syntactic reordering patterns(57,861 reordering schemes) with an averagenumber of 11.02 non-terminals.
For computa-tional efficiency, any patterns with number of non-terminal less than 3 and more than 9 are pruned.This procedure leaves 18,169 syntactic reorderingpatterns (22,850 reordering schemes) with a aver-1http://fjoch.com/GIZA++.html2http://www.speech.sri.com/projects/srilm/23age number of 7.6 non-terminals.
This pattern setis used to built the syntactic reordering PBSMTsystem without pattern filtering, which here afterwe call the ?unfiltered system?.Using the tags specified in Table 1, the ex-tracted syntactic reordering patterns without func-tional words are filtered out, while only 6,926 syn-tactic reordering patterns (with 9,572 reorderingschemes) are retained.
Thus the pattern set arereduced by 61.88%, and over half of them arepruned by the functional word tags.
The filteredpattern set is used to build the syntactic reorder-ing PBSMT system with pattern filtering, whichwe refer as the ?filtered system?.Type Tag Patterns Percentba-const.
BA 222 3.20%bei-const.
LB 97 2.79%SB 96de-const.
(1st) DEC 1662 60.11%DEG 2501de-const.
(2nd) DER 52 0.75%de-const.
(3rd) DEV 178 2.57%preposition P 2591 37.41%excl.
ba & beiTable 2: Statistics on the number of patterns foreach type of functional wordStatistics on the patterns with respect to func-tional word types are shown in Table 2.
The num-ber of patterns for each functional word in the fil-tered pattern set are illustrated, and percentages offunctional word types are also reported.
Note thatsome patterns contain more than one kind of func-tional word, so that the percentages of functionalword types do not sum to one.As demonstrated in Table 2, the first kind of de-construction takes up 60.11% of the filtered pat-tern set, and is the main type of patterns used inour experiment.
This indicates that more than halfof the patterns are closely related to the DE con-struction examined in (Chang et al, 2009b; Du& Way, 2010).
However, the general preposi-tion construction (excluding bei and ba) accountsfor 37.41% of the filtered patterns, which impliesthat it is also a major source of syntactic reorder-ing.
By contrast, other constructions have muchsmaller amount of percentages, so have a minorimpact on our experiments.5.2 Word lattice constructionAs specified in section 4, for both unfiltered andthe filtered systems, both the devset and testsetare converted into word lattices with the unfilteredand filtered syntactic reordering patterns respec-tively.
To avoid a dramatic increase in size of thelattices, the following constraints are applied: foreach source sentence, the maximum number of re-ordering schemes is 30, and the maximum span ofa pattern is 30.For the lattice construction, the base probabil-ity in (6) and (7) is set to 0.05.
The two syntac-tic reordering PBSMT systems also incorporatethe built-in reordering models (distance-based andlexical reordering) of Moses, and their weights inthe log-linear model are tuned with respect to thedevsets.The effects of the pattern filtering by functionalwords are also reported in Table 3.
For both theFBIS and NIST sets, the average number of nodesin word lattices are illustrated before and after pat-tern filtering.
From the table, it is clear that thepattern filtering procedure dramatically reducesthe input size for the PBSMT system.
The reduc-tion is up to 37.99% for the NIST testset.Data set Unfiltered Filtered ReducedFBIS dev 183.13 131.38 28.26%FBIS test 183.68 136.56 25.65%NIST dev 175.78 115.89 34.07%NIST test 149.13 92.48 37.99%Table 3: Comparison of the average number ofnodes in word lattices5.3 Results on FBIS setThree systems are compared on the FBIS set:the baseline PBSMT system, and the syntacticreordering systems with and without pattern fil-tering.
Since the built-in reordering models ofMoses are enabled, several values of the distor-tion limit (DL) parameter are chosen to validateconsistency.
The evaluation results on the FBISset are shown in Table 4.As shown in Table 4, the syntactic reorderingsystems with and without pattern filtering outper-24System DL BLEU NIST METEBaseline0 22.32 6.45 52.516 23.67 6.63 54.0710 24.52 6.66 54.0412 24.57 6.69 54.31Unfiltered0 23.92 6.60 54.306 24.57 6.68 54.6410 24.98 6.71 54.6712 24.84 6.69 54.65Filtered0 23.71 6.60 54.116 24.65 6.68 54.6110 24.87 6.71 54.8412 24.91 6.7 54.51Table 4: Results on FBIS testset (DL = distortionlimit, METE=METEOR)form the baseline system for each of the distortionlimit parameters in terms of the BLEU, NIST andMETEOR scores (scores in bold face).
By con-trast, the filtered systems has a comparable perfor-mance with the unfiltered system: for some of thedistortion limits, the filtered systems even outper-forms the unfiltered system (scores in bold face,e.g.
BLEU and NIST for DL=12, METEOR forDL=10).The best performance of the baseline systemis obtained with distortion limit 12 (underlined);the best performance of the unfiltered system isachieved with distortion limit 10 (underlined);while for the filtered system, the best BLEU scoreis accomplished with distortion limit 12 (under-lined), and the best NIST and METEOR scoresare shown with distortion limit 10 (underlined).Thus the unfiltered system outperforms the base-line by 0.41 (1.67% relative) BLEU points, 0.02(0.30% relative) NIST points and 0.36 (0.66%relative) METEOR points.
By contrast, the fil-tered system outperforms the baseline by 0.34(1.38% relative) BLEU points, 0.02 (0.30% rel-ative) NIST points and 0.53 (0.98% relative) ME-TEOR points.Compared with the unfiltered system, patternfiltering with functional words degrades perfor-mance by 0.07 (0.28% relative) in term of BLEU,but improves the system by 0.17 (0.31% rela-tive) in term of METEOR, while the two systemsachieved the same best NIST score.These results indicates that the filtered systemhas a comparable performance with the unfilteredone on the FBIS set, while both of them outper-form the baseline.5.4 Results on NIST setThe evaluation results on the NIST set are illus-trated in Table 5.System DL BLEU NIST METEBaseline0 14.43 5.75 45.036 15.61 5.88 45.7510 15.73 5.78 45.2712 15.89 6.16 45.88Unfiltered0 16.77 6.54 47.166 17.25 6.67 47.6510 17.15 6.64 47.7812 16.88 6.56 47.17Filtered0 16.79 6.64 47.676 17.55 6.71 48.0610 17.51 6.72 48.1512 17.37 6.72 48.08Table 5: Results on NIST testset (DL = distortionlimit, METE=METEOR)From Table 5, the unfiltered system outper-forms the baseline system for each of the distor-tion limits in terms of the BLEU, NIST and ME-TEOR scores (scores in bold face).
By contrast,the filtered system also outperform the unfilteredsystem for each of the distortion limits in terms ofthe three evaluation methods (scores in bold face).The best performance of the baseline systemis obtained with distortion limit 12 (underlined),while the best performance of the unfiltered sys-tem is obtained with distortion limit 6 for BLEUand NIST, and 10 for METEOR (underlined).
Forthe filtered system, the best BLEU score is shownwith distortion limit 6, and the best NIST and ME-TEOR scores are accomplished with distortionlimit 10 (underlined).
Thus the unfiltered systemoutperforms the baseline by 1.36 (8.56% relative)BLEU points, 0.51 (8.28% relative) NIST pointsand 1.90 (4.14% relative) METEOR points.
Bycontrast, the filtered system outperforms the base-line by 1.66 (10.45% relative) BLEU points, 0.56(9.52% relative) NIST points and 2.27 (4.95% rel-ative) METEOR points.25Compared with the unfiltered system, patternswith functional words boost the performance by0.30 (1.74% relative) in term of BLEU, 0.05(0.75% relative) in term of NIST, and 0.37 (0.77%relative) in term of METEOR.These results demonstrate that the pattern filter-ing improves the syntactic reordering system onthe NIST set, while both of them significantly out-perform the baseline.5.5 DiscussionExperiments in the previous sections demonstratethat: 1) the two syntactic reordering systems im-prove the PBSMT system by providing potentialreorderings obtained from phrase alignments andparse trees; 2) patterns with functional words playa major role in the syntactic reordering process,and filtering the patterns with functional wordsmaintains or even improves the system perfor-mance for Chinese?English SMT task.
Further-more, as shown in the previous section, patternfiltering prunes the whole pattern set by 61.88%and also reduces the sizes of word lattices by upto 37.99%, thus the whole syntactic reorderingprocedure for the original inputs as well as thetuning/decoding steps are sped up dramatically,which make the proposed methods more useful inthe real world, especially for online SMT systems.From the statistics on the filtered pattern setin Table 2, we also argue that the first kindof de-construction and general preposition (ex-cluding bei and ba) are the main sources ofChinese?English syntactic reordering.
Previouswork (Chang et al, 2009b; Du & Way, 2010)showed the advantages of dealing with the DEconstruction.
In our experiments too, even thoughall the patterns are automatically extracted fromphrase alignments, these two constructions stilldominate the filtered pattern set.
This result con-firms the effectiveness of previous work on DEconstruction, and also highlights the importanceof the general preposition construction in this task.6 Conclusion and future workSyntactic reordering patterns with functionalwords are examined in this paper.
The aim is toexploit these functional words within the syntacticreordering patterns extracted from phrase align-ments and parse trees.
Three systems are com-pared: a baseline PBSMT system, a syntactic re-ordering system with all patterns extracted from acorpus and a syntactic reordering system with pat-terns filtered with functional words.
Evaluationresults on a medium-sized corpus showed that thetwo syntactic reordering systems consistently out-perform the baseline system.
The pattern filteringwith functional words prunes 61.88% of patterns,but still maintains a comparable performance withthe unfiltered one on the randomly select testset,and even obtains 1.74% relative improvement onthe NIST 2008 testset.In future work, the structures of patterns con-taining functional words will be investigated toobtain fine-grained analysis on such words in thistask.
Furthermore, experiments on larger corporaas well as on other language pairs will also be car-ried out to validation our method.AcknowledgementsThis research is supported by Science FoundationIreland (Grant 07/CE/I1142) as part of the Centrefor Next Generation Localisation (www.cngl.ie) atDublin City University.
Thanks to Yanjun Ma forthe sentence-aligned FBIS corpus.ReferencesYaser Al-Onaizan and Kishore Papineni 2006.
Dis-tortion models for statistical machine translation.Coling-ACL 2006: Proceedings of the 21st Inter-national Conference on Computational Linguisticsand 44th Annual Meeting of the Association forComputational Linguistics, pages 529-536, Sydney,Australia.Pi-Chuan Chang, Dan Jurafsky, and ChristopherD.Manning 2009a.
Disambiguating DE forChinese?English machine translation.
Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, pages 215-223, Athens, Greece.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, andChristopher D. Manning.
2009b.
Discriminativereordering with Chinese grammatical features.
Pro-ceedings of SSST-3: Third Workshop on Syntax andStructure in Statistical Translation, pages 51-59,Boulder, CO.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
ACL-2005: 43rd Annual meeting of the26Association for Computational Linguistics, pages531-540, University of Michigan, Ann Arbor, MI.Josep M. Crego, and Jose?
B. Marin?o.
2007.
Syntax-enhanced N-gram-based SMT.
MT Summit XI,pages 111-118, Copenhagen, Denmark.Jinhua Du and Andy Way.
2010.
The Impact ofSource-Side Syntactic Reordering on HierarchicalPhrase-based SMT.
EAMT 2010: 14th Annual Con-ference of the European Association for MachineTranslation, Saint-Raphae?l, France.Jakob Elming.
2008.
Syntactic reordering integratedwith phrase-based SMT.
Coling 2008: 22nd In-ternational Conference on Computational Linguis-tics, Proceedings of the conference, pages 209-216,Manchester, UK.Jakob Elming, and Nizar Habash.
2009.
Syntac-tic reordering for English-Arabic phrase-based ma-chine translation.
Proceedings of the EACL 2009Workhop on Computational Approaches to SemiticLanguages, pages 69-77, Athens, Greece.Nizar Habash.
2007.
Syntactic preprocessing for sta-tistical machine translation.
MT Summit XI, pages215-222, Copenhagen, Denmark.Jie Jiang, Andy Way, Julie Carson-Berndsen.
2010.Lattice Score-Based Data Cleaning For Phrase-Based Statistical Machine Translation.
EAMT2010: 14th Annual Conference of the European As-sociation for Machine Translation, Saint-Raphae?l,France.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical MachineTranslation.
ACL 2007: proceedings of demo andposter sessions, pp.
177-180, Prague, Czech Repub-lic.Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,Minghui Li, and Yi Guan 2007.
A probabilisticapproach to syntax-based reordering for statisticalmachine translation.
ACL 2007: proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics, pages 720-727, Prague, CzechRepublic.Xiaoyi Ma.
2006.
Champollion: A Robust Paral-lel Text Sentence Aligner.
LREC 2006: Fifth In-ternational Conference on Language Resources andEvaluation, pp.489-492, Genova, Italy.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
ACL-2003:41st Annual meeting of the Association for Compu-tational Linguistics, pp.
160-167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method For AutomaticEvaluation of Machine Translation.
ACL-2002:40th Annual meeting of the Association for Compu-tational Linguistics, pp.311-318, Philadelphia, PA.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
Coling-ACL 2006:Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 433-440, Sydney, Australia.Felipe Sa?nchez-Mart?
?nez and Andy Way.
2009.Marker-based filtering of bilingual phrase pairs forSMT.
EAMT-2009: Proceedings of the 13th An-nual Conference of the European Association forMachine Translation, pages 144-151, Barcelona,Spain.Chao Wang, Michael Collins, and Philipp Koehn.2007a.
Chinese syntactic reordering for statisticalmachine translation.
EMNLP-CoNLL-2007: Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages737-745, Prague, Czech Republic.Fei Xia, and Michael McCord 2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
Coling 2004: 20th InternationalConference on Computational Linguistics, pages508-514, University of Geneva, Switzerland.Nianwen Xue, Fei Xia, Fu-dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2), pages 207-238.Richard Zens, Franz Josef Och, and Hermann Ney.2002.
Phrase-based statistical machine translation.Proceedings of the 47th Annual Meeting of the ACLand the 4th IJCNLP, pages 333-341, Suntec, Singa-pore.Yuqi Zhang, Richard Zens, and Hermann Ney 2007a.Chunk-level reordering of source language sen-tences with automatically learned rules for statisti-cal machine translation.
SSST, NAACL-HLT-2007AMTA Workshop on Syntax and Structure in Statis-tical Translation, pages 1-8, Rochester, NY.Yuqi Zhang, Richard Zens, and Hermann Ney 2007b.Improved chunk-level reordering for statistical ma-chine translation.
IWSLT 2007: International Work-shop on Spoken Language Translation, pages 21-28,Trento, Italy.27
