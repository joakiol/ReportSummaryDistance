Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21?30,Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational LinguisticsImproving Word Translation Disambiguation byCapturing Multiword Expressions with DictionariesLars Bungum, Bjo?rn Gamba?ck, Andre?
Lynum, Erwin MarsiNorwegian University of Science and TechnologySem S?lands vei 7?9; NO?7491 Trondheim, Norway{bungum,gamback,andrely,emarsi}@idi.ntnu.noAbstractThe paper describes a method for identifyingand translating multiword expressions using abi-directional dictionary.
While a dictionary-based approach suffers from limited recall,precision is high; hence it is best employedalongside an approach with complementingproperties, such as an n-gram language model.We evaluate the method on data from theEnglish-German translation part of the cross-lingual word sense disambiguation task in the2010 semantic evaluation exercise (SemEval).The output of a baseline disambiguation sys-tem based on n-grams was substantially im-proved by matching the target words and theirimmediate contexts against compound andcollocational words in a dictionary.1 IntroductionMultiword expressions (MWEs) cause particularlexical choice problems in machine translation(MT), but can also be seen as an opportunity to bothgeneralize outside the bilingual corpora often usedas training data in statistical machine translation ap-proaches and as a method to adapt to specific do-mains.
The identification of MWEs is in generalimportant for many language processing tasks (Saget al 2002), but can be crucial in MT: since the se-mantics of many MWEs are non-compositional, asuitable translation cannot be constructed by trans-lating the words in isolation.
Identifying MWEscan help to identify idiomatic or otherwise fixed lan-guage usage, leading to more fluent translations, andpotentially reduce the amount of lexical choice anMT system faces during target language generation.In any translation effort, automatic or otherwise,the selection of target language lexical items to in-clude in the translation is a crucial part of the fi-nal translation quality.
In rule-based systems lex-ical choice is derived from the semantics of thesource words, a process which often involves com-plex semantic composition.
Data-driven systemson the other hand commonly base their translationsnearly exclusively on cooccurrences of bare wordsor phrases in bilingual corpora, leaving the respon-sibility of selecting lexical items in the translationentirely to the local context found in phrase trans-lation tables and language models with no explicitnotion of the source or target language semantics.Still, systems of this type have been shown to pro-duce reasonable translation quality without explic-itly considering word translation disambiguation.Bilingual corpora are scarce, however, and un-available for most language pairs and target do-mains.
An alternative approach is to build systemsbased on large monolingual knowledge sources andbilingual lexica, as in the hybrid MT system PRE-SEMT (Sofianopoulos et al 2012).
Since sucha system explicitly uses a translation dictionary, itmust at some point in the translation process decidewhich lexical entries to use; thus a separate wordtranslation disambiguation module needs to be in-corporated.
To research available methods in such amodule we have identified a task where we can usepublic datasets for measuring how well a method isable to select the optimal of many translation choicesfrom a source language sentence.21In phrase-based statistical MT systems, the trans-lation of multiword expressions can be a notablesource of errors, despite the fact that those systemsexplicitly recognize and use alignments of sequen-tial chunks of words.
Several researchers have ap-proached this problem by adding MWE translationtables to the systems, either through expanding thephrase tables (Ren et al 2009) or by injecting theMWE translations into the decoder (Bai et al 2009).Furthermore, there has been some interest in auto-matic mining of MWE pairs from bilingual corporaas a task in itself: Caseli et al(2010) used a dic-tionary for evaluation of an automatic MWE extrac-tion procedure using bilingual corpora.
They alsoargued for the filtering of stopwords, similarly to theprocedure described in the present paper.
Sharoffet al(2006) showed how MWE pairs can be ex-tracted from comparable monolingual corpora in-stead of from a parallel bilingual corpus.The methodology introduced in this paper em-ploys bilingual dictionaries as a source of multi-word expressions.
Relationships are induced be-tween the source sentence and candidate transla-tion lexical items based on their correspondence inthe dictionary.
Specifically, we use a determinis-tic multiword expression disambiguation procedurebased on translation dictionaries in both directions(from source to target language and vice versa),and a baseline system that ranks target lexical itemsbased on their immediate context and an n-gramlanguage model.
The n-gram model represents ahigh-coverage, low-precision companion to the dic-tionary approach (i.e., it has complementary proper-ties).
Results show that the MWE dictionary infor-mation substantially improves the baseline system.The 2010 Semantic Evaluation exercise (Sem-Eval?10) featured a shared task on Cross-LingualWord Sense Disambiguation (CL-WSD), where thefocus was on disambiguating the translation of a sin-gle noun in a sentence.
The participating systemswere given an English word in its context and askedto produce appropriate substitutes in another lan-guage (Lefever and Hoste, 2010b).
The CL-WSDdata covers Dutch, French, Spanish, Italian and Ger-man; however, since the purpose of the experimentsin this paper just was to assess our method?s abil-ity to choose the right translation of a word given itscontext, we used the English-to-German part only.The next section details the employed disam-biguation methodology and describes the data setsused in the experiments.
Section 3 then reports onthe results of experiments applying the methodologyto the SemEval datasets, particularly addressing theimpact of the dictionary MWE correspondences.
Fi-nally, Section 4 sums up the discussion and points toissues that can be investigated further.2 MethodologyThe core of the disambiguation model introducedin this paper is dictionary-based multiword extrac-tion.
Multiword extraction is done in both a directand indirect manner: Direct extraction uses adjacentwords in the source language in combination withthe word to be translated, if the combination has anentry in the source-to-target language (SL?TL) dic-tionary.
Indirect extraction works in the reverse di-rection, by searching the target-to-source (TL?SL)dictionary and looking up translation candidates forthe combined words.
Using a dictionary to identifymultiword expressions after translation has a low re-call of target language MWEs, since often there ei-ther are no multiword expressions to be discovered,or the dictionary method is unable to find a trans-lation for an MWE.
Nevertheless, when an MWEreally is identified by means of the dictionary-basedmethod, the precision is high.Due to the low recall, relying on multiword ex-pressions from dictionaries would, however, not besufficient.
Hence this method is combined with ann-gram language model (LM) based on a large tar-get language corpus.
The LM is used to rank trans-lation candidates according to the probability of then-gram best matching the context around the transla-tion candidate.
This is a more robust but less preciseapproach, which servers as the foundation for thehigh-precision but low-recall dictionary approach.In the actual implementation, the n-gram methodthus first provides a list of its best suggestions(currently top-5), and the dictionary method thenprepends its candidates to the top of this list.
Con-sequently, n-gram matching is described beforedictionary-based multiword extraction in the follow-ing section.
First, however, we introduce the datasets used in the experiments.22(a) AGREEMENT in the form of an exchange of letters betweenthe European Economic Community and the Bank for Interna-tional Settlements concerning the mobilization of claims held bythe Member States under the medium-term financial assistancearrangements{bank 4; bankengesellschaft 1; kreditinstitut 1; zentralbank 1; fi-nanzinstitut 1}(b) The Office shall maintain an electronic data bank with the par-ticulars of applications for registration of trade marks and entriesin the Register.
The Office may also make available the contentsof this data bank on CD-ROM or in any other machine-readableform.
{datenbank 4; bank 3; datenbanksystem 1; daten 1}(c) established as a band of 1 km in width from the banks of ariver or the shores of a lake or coast for a length of at least 3 km.
{ufer 4; flussufer 3}Table 1: Examples of contexts for the English word bankwith possible German translations2.1 The CL-WSD DatasetsThe data sets used for the SemEval?10 Cross-Lingual Word Sense Disambiguation task were con-structed by making a ?sense inventory?
of all pos-sible target language translations of a given sourcelanguage word based on word-alignments in Eu-roparl (Koehn, 2005), with alignments involving therelevant source words being manually checked.
Theretrieved target words were manually lemmatisedand clustered into translations with a similar sense;see Lefever and Hoste (2010a) for details.Trial and test instances were extracted from twoother corpora, JRC-Acquis (Steinberger et al 2006)and BNC (Burnard, 2007).
The trial data for eachlanguage consists of five nouns (with 20 sentencecontexts per noun), and the test data of twenty nouns(50 contexts each, so 1000 in total per language,with the CL-WSD data covering Dutch, French,Spanish, Italian and German).
Table 1 provides ex-amples from the trial data of contexts for the Englishword bank and its possible translations in German.Gold standard translations were created by hav-ing four human translators picking the contextuallyappropriate sense for each source word, choosing 0?3 preferred target language translations for it.
Thetranslations are thus restricted to those appearing inEuroparl, probably introducing a slight domain bias.Each translation has an associated count indicatinghow many annotators considered it to be among theirtop-3 preferred translations in the given context.bank, bankanleihe, bankanstalt, bankdarlehen, bankenge-sellschaft, bankensektor, bankfeiertag, bankgesellschaft, bankin-stitut, bankkonto, bankkredit, banknote, blutbank, daten, daten-bank, datenbanksystem, euro-banknote, feiertag, finanzinstitut,flussufer, geheimkonto, geldschein, gescha?ftsbank, handelsbank,konto, kredit, kreditinstitut, nationalbank, notenbank, sparkasse,sparkassenverband, ufer, weltbank, weltbankgeber, west-bank,westbank, westjordanien, westjordanland, westjordanufer, west-ufer, zentralbankTable 2: All German translation candidates for bank asextracted from the gold standardIn this way, for the English lemma bank, for ex-ample, the CL-WSD trial gold standard for Germancontains the word Bank itself, together with 40 othertranslation candidates, as shown in Table 2.
Eightof those are related to river banks (Ufer, but also,e.g., Westbank and Westjordanland), three concerndatabases (Datenbank), and one is for blood banks.The rest are connected to different types of finan-cial institutions (such as Handelsbank and Finanz-institut, but also by association Konto, Weldbank-geber, Banknote, Geldschein, Kredit, etc.
).2.2 N-Gram Context MatchingN-gram matching is used to produce a ranked listof translation candidates and their contexts, both inorder to provide robustness and to give a baselineperformance.
The n-gram models were built usingthe IRSTLM toolkit (Federico et al 2008; Bungumand Gamba?ck, 2012) on the DeWaC corpus (Baroniand Kilgarriff, 2006), using the stopword list fromNLTK (Loper and Bird, 2002).
The n-gram match-ing procedure consists of two steps:1.
An nth order source context is extracted and thetranslations for each SL word in this contextare retrieved from the dictionary.
This includesstopword filtering of the context.2.
All relevant n-grams are inspected in orderfrom left to right and from more specific (5-grams) to least specific (single words).For each part of the context with matching n-gramsin the target language model, the appropriate targettranslation candidates are extracted and ranked ac-cording to their language model probability.
Thisresults in an n-best list of translation candidates.23Since dictionary entries are lemma-based, lemma-tization was necessary to use this approach in com-bination with the dictionary enhancements.
Thesource context is formed by the lemmata in the sen-tence surrounding the focus word (the word to bedisambiguated) by a window of up to four wordsin each direction, limited by a 5-gram maximumlength.
In order to extract the semantically most rel-evant content, stopwords are removed before con-structing this source word window.
For each of the1?5 lemmata in the window, the relevant translationcandidates are retrieved from the bilingual dictio-nary.
The candidates form the ordered translationcontext for the source word window.The following example illustrates how the trans-lation context is created for the focus word ?bank?.First the relevant part of the source language sen-tence with the focus word in bold face:(1) The BIS could conclude stand-by creditagreements with the creditor countries?
cen-tral bank if they should so request.For example, using a context of two words in frontand two words after the focus word, the followingsource language context is obtained after a prepro-cessing involving lemmatization, stopword removal,and insertion of sentence start (<s>) and end mark-ers (</s>):(2) country central bank request </s>From this the possible n-grams in the target side con-text are generated by assembling all ordered com-binations of the translations of the source languagewords for each context length: the widest contexts(5-grams) are looked up first before moving on tonarrower contexts, and ending up with looking uponly the translation candidate in isolation.Each of the n-grams is looked up in the languagemodel and for each context part the n-grams are or-dered according to their language model probability.Table 3 shows a few examples of such generated n-grams with their corresponding scores from the n-gram language model.1 The target candidates (ital-ics) are then extracted from the ordered list of targetlanguage n-grams.
This gives an n-best list of trans-1There are no scores for 4- and 5-grams; as expected whenusing direct translation to generate target language n-grams.n n-gram LM score5 land mittig bank nachsuchen </s> Not found4 mittig bank nachsuchen </s> Not found3 mittig bank nachsuchen Not found3 kredit anfragen </s> -0.2662912 mittig bank -3.3825602 zentral blutbank -5.1448701 bank -3.673000Table 3: Target language n-gram examples from look-ups of stopword-filtered lemmata country central bankrequest reported in log scores.
The first 3 n-grams werenot found in the language model.lation candidates from which the top-1 or top-5 canbe taken.
Since multiple senses in the dictionary canrender the same literal output, duplicate translationcandidates are filtered out from the n-best list.2.3 Dictionary-Based Context MatchingAfter creating the n-gram based list of translationcandidates, additional candidates are produced bylooking at multiword entries in a bilingual dictio-nary.
The existence of multiword entries in the dic-tionary corresponding to adjacent lemmata in thesource context or translation candidates in the targetcontext is taken as a clear indicator for the suitabilityof a particular translation candidate.
Such entries areadded to the top of the n-best list, which representsa strong preference in the disambiguation system.Dictionaries are used in all experiments to look uptranslation candidates and target language transla-tions of the words in the context, but this approach ismining the dictionaries by using lookups of greaterlength.
Thus is, for example, the dictionary entryCommunity Bank translated to the translation candi-date Commerzbank; this translation candidate wouldbe put on top of the list of prioritized answers.Two separate procedures are used to find such in-dicators, a direct procedure based on the source con-text and an indirect procedure based on the weakertarget language context.
These are detailed in pseu-docode in Algorithms 1 and 2, and work as follows:Source Language (SL) Method (Algorithm 1)If there is a dictionary entry for the source wordand one of its adjacent words, search the setof translations for any of the translation candi-dates for the word alone.
Specifically, transla-24Algorithm 1 SL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) .
rlist is original ranking2: comblemmas?
list(previouslemma(b) + b, b + nextlemma(b)) .
Find adjacent lemmata3: for lem ?
comblemmas do4: c?
sl-dictionary-lookup(lem) .
Look up lemma in SL?TL dict.5: if c ?
tcands then rlist?
list(c + rlist) .
Push lookup result c onto rlist if in tcands6: end if7: end for8: return rlist .
Return new list with lemmata whose translations were in tcands on top9: end procedureAlgorithm 2 TL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands[The ready-made TL tcands from the dataset are looked up in TL-SL direction.
It is necessary to keep a list of thereverse-translation of the individual tcand as well as the original tcand itself, in order to monitor which tcand it was.If the SL context is found in either of these reverse lookups the matching tcand is ranked high.
]1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) .
rlist is original ranking2: for cand ?
tcands do .
Assemble list of TL translations3: translist?
list(cand, tl-dictionary-lookup(cand)) + translist4: .
Append TL?SL lookup results of tcands with cand as id5: end for6: for cand, trans ?
translist do7: if previouslemma(b)?nextlemma(b) ?
trans then .
If trans contains either SL lemma8: rlist?
list(cand) + rlist .
append this cand onto rlist9: end if10: end for11: return rlist12: .
Return tcands list; top-ranking tcands whose SL-neighbours were found in TL?SL lookup13: end proceduretions of the combination of the source word andan adjacent word in the context are matchedagainst translation candidates for the word.Target Language (TL) Method (Algorithm 2)If a translation candidate looked up in the re-verse direction matches the source word alongwith one or more adjacent words, it is a goodtranslation candidate.
TL candidates are lookedup in a TL?SL dictionary and multiword resultsare matched against SL combinations of disam-biguation words and their immediate contexts.For both methods the dictionary entry for the tar-get word or translation candidate is matched againstthe immediate context.
Thus both methods resultin two different lookups for each focus word, com-bining it with the previous and next terms, respec-tively.
This is done exhaustively for all combina-tions of translations of the words in the context win-dow.
Only one adjacent word was used, since veryfew of the candidates were able to match the contexteven with one word.
Hence, virtually none wouldbe found with more context, making it very unlikelythat larger contexts would contribute to the disam-biguation procedure, as wider matches would alsomatch the one-word contexts.Also for both methods, translation candidates areonly added once, in case the same translation candi-date generates hits with either (or both) of the meth-ods.
Looking at the running example, stopword fil-tered and with lemmatized context:(3) country central bank requestThis example generates two source language multi-word expressions, central bank and bank request.
Inthe source language method, these word combina-25tions are looked up in the dictionary where the zen-tralbank entry is found for central bank, which isalso found as a translation candidate for bank.The target language method works in the reverseorder, looking up the translation candidates in theTL?SL direction and checking if the combined lem-mata are among the candidates?
translations into thesource language.
In the example, the entry zentral-bank:central bank is found in the dictionary, match-ing the source language context, so zentralbank isassumed to be a correct translation.2.4 DictionariesTwo English-German dictionaries were used in theexperiments, both with close to 1 million entries(translations).
One is a free on-line resource, whilethe other was obtained by reversing an existing pro-prietary German-English dictionary made availableto the authors by its owners:?
The GFAI dictionary (called ?D1?
in Section 3below) is a proprietary and substantially ex-tended version of the Chemnitz dictionary, with549k EN entries including 433k MWEs, and552k DE entries (79k MWEs).
The Chem-nitz electronic German-English dictionary2 it-self contains over 470,000 word translationsand is available under a GPL license.?
The freely available CC dictionary3 (?D2?
be-low) is an internet-based German-English andEnglish-German dictionary built through usergenerated word definitions.
It has 565k/440k(total/MWE) EN and 548k/210k DE entries.Note that the actual dictionaries are irrelevant to thediscussion at hand, and that we do not aim to pointout strengths or weaknesses of either dictionary, norto indicate a bias towards a specific resource.3 ResultsExperiments were carried out both on the trial andtest data described in Section 2.1 (5 trial and 20 testwords; with 20 resp.
50 instances for each word; intotal 1100 instances in need of disambiguation).
Theresults show that the dictionaries yield answers with2http://dict.tu-chemnitz.de/3http://www.dict.cc/high precision, although they are robust enough tosolve the SemEval WSD challenge on their own.For measuring the success rate of the developedmodels, we adopt the ?Out-Of-Five?
(OOF) score(Lefever and Hoste, 2010b) from the SemEval?10Cross-Lingual Word Sense Disambiguation task.The Out-Of-Five criterion measures how well thetop five candidates from the system match the topfive translations in the gold standard:OOF (i) =?a?Aifreq i(a)|Hi|where Hi denotes the multiset of translations pro-posed by humans for the focus word in each sourcesentence si (1 ?
i ?
N , N being the numberof test items).
Ai is the set of translations producedby the system for source term i.
Since each transla-tion has an associated count of how many annotatorschose it, there is for each si a function freq i return-ing this count for each term in Hi (0 for all otherterms), and max freq i gives the maximal count forany term in Hi.
For the first example in Table 1:????????????????????
?H1 = {bank, bank, bank, bank, zentralbank,bankengesellschaft, kreditinstitut, finanzinstitut}freq1(bank) = 4. .
.freq1(finanzinstitut) = 1maxfreq1 = 4and the cardinality of the multiset is: |H1| = 8.
Thisequates to the sum of all top-3 preferences given tothe translation candidates by all annotators.For the Out-Of-Five evaluation, the CL-WSD sys-tems were allowed to submit up to five candidatesof equal rank.
OOF is a recall-oriented measurewith no additional penalty for precision errors, sothere is no benefit in outputting less than five can-didates.
With respect to the previous example fromTable 1, the maximum score is obtained by systemoutput A1 = {bank, bankengesellschaft, kreditinstitut,zentralbank, finanzinstitut}, which gives OOF (1) =(4 + 1 + 1 + 1 + 1)/8 = 1, whereas A2 = {bank,bankengesellschaft, nationalbank, notenbank, sparkasse}would give OOF (1) = (4 + 1)/8 = 0.625.44Note that the maximum OOF score is not always 1 (i.e., itis not normalized), since the gold standard sometimes containsmore than five translation alternatives.26Source language Target language AllDictionary D1 D2 comb D1 D2 comb combTop 8.89 6.99 8.89 22.71 24.43 25.34 24.67Low 0.00 0.00 0.00 0.00 0.00 0.00 0.00Mean 2.71 0.99 3.04 8.35 7.10 9.24 10.13Table 4: F1-score results for individual dictionariesSource language Target language AllDictionary D1 D2 comb D1 D2 comb combcoach 1.00 0.00 1.00 0.21 0.00 0.21 0.21education 0.83 0.67 0.83 0.47 0.62 0.54 0.53execution 0.00 0.00 0.00 0.17 0.22 0.17 0.17figure 1.00 0.00 1.00 0.51 0.57 0.55 0.55job 0.88 0.80 0.94 0.45 0.78 0.46 0.44letter 1.00 0.00 1.00 0.66 0.75 0.62 0.66match 1.00 1.00 1.00 0.80 0.50 0.80 0.80mission 0.71 0.33 0.71 0.46 0.37 0.36 0.36mood 0.00 0.00 0.00 0.00 0.00 0.00 0.00paper 0.68 0.17 0.68 0.53 0.35 0.55 0.55post 1.00 1.00 1.00 0.39 0.48 0.45 0.48pot 0.00 0.00 0.00 1.00 1.00 1.00 1.00range 1.00 1.00 1.00 0.28 0.37 0.30 0.30rest 1.00 0.67 1.00 0.60 0.56 0.56 0.58ring 0.09 0.00 0.09 0.37 0.93 0.38 0.38scene 1.00 0.00 1.00 0.50 0.42 0.44 0.50side 1.00 0.00 1.00 0.21 0.16 0.23 0.27soil 1.00 0.00 1.00 0.72 0.58 0.66 0.69strain 0.00 0.00 0.00 0.51 0.88 0.55 0.55test 1.00 1.00 1.00 0.62 0.52 0.57 0.61Mean 0.84 0.74 0.84 0.50 0.56 0.49 0.51Table 5: Precision scores for all terms filtering out thoseinstances for which no candidates were suggestedFor assessing overall system performance inthe experiments, we take the best (?Top?
), worst(?Low?
), and average (?Mean?)
of the OOF scoresfor all the SL focus words, with F1-score reportedas the harmonic mean of the precision and recall ofthe OOF scores.
Table 4 shows results for each dic-tionary approach on the test set, with ?D1?
beingthe GFAI dictionary, ?D2?
the CC dictionary, and?comb?
the combination of both.
Target languagelook-up contributes more to providing good transla-tion candidates than the source language methodol-ogy, and also outperforms a strategy combining alldictionaries in both directions (?All comb?
).Filtering out the instances for which no candi-date translation was produced, and taking the aver-age precision scores only over these, gives the re-sults shown in Table 5.
Markedly different preci-sion scores can be noticed, but the source languageSource language Target languageDictionary D1 D2 D1 D2Mean 3.25 1.5 12.65 11.45Total 223 256 1,164 880Table 6: Number of instances with a translation candidate(?Mean?)
and the total number of suggested candidatesMost Most Freq 5-gram 5-gram All Dict VSMFreq Aligned + Dict Comb ModelTop 51.77 68.71 52.02 52.74 24.67 55.92Low 1.76 9.93 14.09 15.40 0.00 10.73Mean 21.18 34.61 30.36 36.38 10.13 30.30Table 7: Overview of results (F1-scores) on SemEval datamethod again has higher precision on the sugges-tions it makes than the target language counterpart.As shown in Table 6, this higher precision is offsetby lower coverage, with far fewer instances actuallyproducing a translation candidate with the dictionarylookup methods.
There is a notable difference in theprecision of the SL and TL approaches, coincidingwith more candidates produced by the latter.
Severalwords in Table 5 give 100% precision scores for atleast one dictionary, while a few give 0% precisionfor some dictionaries.
The word ?mood?
even has0% precision for both dictionaries in both directions.Table 7 gives an overview of different approachesto word translation disambiguation on the dataset.For each method, the three lines again give boththe best and worst scoring terms, and the meanvalue for all test words.
The maximum attainablescore for each of those would be 99.28, 90.48 and95.47, respectively, but those are perfect scores notreachable for all items, as described above (OOF-scoring).
Instead the columns Most Freq and MostFreq aligned give the baseline scores for the Sem-Eval dataset: the translation most frequently seenin the corpus and the translation most frequentlyaligned in a word-aligned parallel corpus (Europarl),respectively.
Then follows the results when usingonly a stopword-filtered 5-gram model built with theIRSTLM language modeling kit (Federico and Cet-tolo, 2007), and when combining the 5-gram modelwith the dictionary approach (5-gram + Dict).The next column (All Dict Comb) shows how thedictionary methods fared on their own.
The com-27bined dictionary approach has low recall (see Ta-ble 6) and does not alone provide a good solution tothe overall problem.
Due to high precision, however,the approach is able to enhance the n-gram methodthat already produces acceptable results.
Finally, thecolumn VSM Model as comparison gives the resultsobtained when using a Vector Space Model for wordtranslation disambiguation (Marsi et al 2011).Comparing the dictionary approach to state-of-the-art monolingual solutions to the WTD problemon this dataset shows that the approach performs bet-ter for the Lowest and Mean scores of the terms, butnot for the Top scores (Lynum et al 2012).
As canbe seen in Table 7, the vector space model producedthe overall best score for a single term.
However, themethod combining a 5-gram language model withthe dictionary approach was best both at avoidingreally low scores for any single term and when com-paring the mean scores for all the terms.4 Discussion and ConclusionThe paper has presented a method for using dictio-nary lookups based on the adjacent words in boththe source language text and target language candi-date translation texts to disambiguate word transla-tion candidates.
By composing lookup words by us-ing both neighbouring words, improved disambigua-tion performance was obtained on the data from theSemEval?10 English-German Cross-Lingual WordSense Disambiguation task.
The extended use ofdictionaries proves a valuable source of informa-tion for disambiguation, and can introduce low-costphrase-level translation to quantitative Word SenseDisambiguation approaches such as N-gram or Vec-tor Space Model methods, often lacking the phrases-based dimension.The results show clear differences between thesource and target language methods of using dictio-nary lookups, where the former has very high preci-sion (0.84) but low coverage, while the TL methodcompensates lower precision (0.51) with markedlybetter coverage.
The SL dictionary method pro-vided answers to only between 1.5 and 3.25 of 50instances per word on average, depending on the dic-tionary.
This owes largely to the differences in algo-rithms, where the TL method matches any adjacentlemma to the focus word with the translation of thepre-defined translation candidates, whereas the SLmethod matches dictionaries of the combined lem-mata of the focus word and its adjacent words to thesame list of translation candidates.
False positivesare expected with lower constraints such as these.On the SemEval data, the contribution of the dictio-nary methods to the n-grams is mostly in improvingthe average score.The idea of acquiring lexical information fromcorpora is of course not new in itself.
So did, e.g.,Rapp (1999) use vector-space models for the pur-pose of extracting ranked lists of translation can-didates for extending a dictionary for word trans-lation disambiguation.
Chiao and Zweigenbaum(2002) tried to identify translational equivalencesby investigating the relations between target andsource language word distributions in a restricteddomain, and also applied reverse-translation filteringfor improved performance, while Sadat et al(2003)utilised non-aligned, comparable corpora to inducea bilingual lexicon, using a bidirectional method(SL?TL, TL?SL, and a combination of both).Extending the method to use an arbitrary size win-dow around all words in the context of each focusword (not just the word itself) could identify moremultiword expressions and generate a more accuratebag-of-words for a data-driven approach.
Differ-ences between dictionaries could also be explored,giving more weight to translations found in two ormore dictionaries.
Furthermore, the differences be-tween the SL and TL methods could explored fur-ther, investigating in detail the consequences of us-ing a symmetrical dictionary, in order to study theeffect that increased coverage has on results.
Test-ing the idea on more languages will help verify thevalidity of these findings.AcknowledgementsThis research has received funding from NTNU and fromthe European Community?s 7th Framework Programmeunder contract nr 248307 (PRESEMT).
Thanks to theother project participants and the anonymous reviewersfor several very useful comments.28ReferencesBai, M.-H., You, J.-M., Chen, K.-J., and Chang,J.
S. (2009).
Acquiring translation equivalences ofmultiword expressions by normalized correlationfrequencies.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 478?486, Singapore.
ACL.Baroni, M. and Kilgarriff, A.
(2006).
Largelinguistically-processed web corpora for multiplelanguages.
In Proceedings of the 11th Conferenceof the European Chapter of the Association forComputational Linguistics, pages 87?90, Trento,Italy.
ACL.Bungum, L. and Gamba?ck, B.
(2012).
Efficient n-gram language modeling for billion word web-corpora.
In Proceedings of the 8th InternationalConference on Language Resources and Evalua-tion, pages 6?12, Istanbul, Turkey.
ELRA.
Work-shop on Challenges in the Management of LargeCorpora.Burnard, L., editor (2007).
Reference Guide for theBritish National Corpus (XML Edition).
BNCConsortium, Oxford, England.
http://www.natcorp.ox.ac.uk/XMLedition/URG.Caseli, H. d. M., Ramisch, C., das Grac?asVolpe Nunes, M., and Villavicencio, A.
(2010).Alignment-based extraction of multiword expres-sions.
Language Resources and Evaluation, 44(1-2):59?77.
Special Issue on Multiword expression:hard going or plain sailing.Chiao, Y.-C. and Zweigenbaum, P. (2002).
Look-ing for candidate translational equivalents in spe-cialized comparable corpora.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, volume 2, pages 1?5,Philadelphia, Pennsylvania.
ACL.
Also publishedin AMIA Annual Symposium 2002, pp.
150?154.Federico, M., Bertoldi, N., and Cettolo, M. (2008).Irstlm: an open source toolkit for handling largescale language models.
In INTERSPEECH, pages1618?1621.
ISCA.Federico, M. and Cettolo, M. (2007).
Efficient han-dling of n-gram language models for statisticalmachine translation.
In Proceedings of the 45thAnnual Meeting of the Association for Compu-tational Linguistics, pages 88?95, Prague, CzechRepublic.
ACL.
2nd Workshop on Statistical Ma-chine Translation.Koehn, P. (2005).
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings ofthe 10th Machine Translation Summit, pages 79?86, Phuket, Thailand.Lefever, E. and Hoste, V. (2010a).
Constructionof a benchmark data set for cross-lingual wordsense disambiguation.
In Proceedings of the 7thInternational Conference on Language Resourcesand Evaluation, pages 1584?1590, Valetta, Malta.ELRA.Lefever, E. and Hoste, V. (2010b).
SemEval-2010Task 3: Cross-lingual word sense disambiguation.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages15?20, Uppsala, Sweden.
ACL.
5th InternationalWorkshop on Semantic Evaluation.Loper, E. and Bird, S. (2002).
NLTK: the natu-ral language toolkit.
In Proceedings of the ACL-02 Workshop on Effective tools and methodolo-gies for teaching natural language processing andcomputational linguistics - Volume 1, ETMTNLP?02, pages 63?70, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.LREC06 (2006).
Proceedings of the 5th Interna-tional Conference on Language Resources andEvaluation, Genova, Italy.
ELRA.Lynum, A., Marsi, E., Bungum, L., and Gamba?ck,B.
(2012).
Disambiguating word translations withtarget language models.
In Proceedings of the15th International Conference on Text, Speechand Dialogue, pages 378?385, Brno, Czech Re-public.
Springer.Marsi, E., Lynum, A., Bungum, L., and Gamba?ck,B.
(2011).
Word translation disambiguation with-out parallel texts.
In Proceedings of the Inter-national Workshop on Using Linguistic Informa-tion for Hybrid Machine Translation, pages 66?74, Barcelona, Spain.Rapp, R. (1999).
Automatic identification of wordtranslations from unrelated English and Germancorpora.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 519?526, Madrid, Spain.
ACL.29Ren, Z., Lu?, Y., Cao, J., Liu, Q., and Huang, Y.(2009).
Improving statistical machine translationusing domain bilingual multiword expressions.
InProceedings of the 47th Annual Meeting of theAssociation for Computational Linguistics, pages47?54, Singapore.
ACL.
Workshop on MultiwordExpressions: Identification, Interpretation, Dis-ambiguation and Applications.Sadat, F., Yoshikawa, M., and Uemura, S. (2003).Learning bilingual translations from comparablecorpora to cross-language information retrieval:Hybrid statistics-based and linguistics-based ap-proach.
In Proceedings of the 41th Annual Meet-ing of the Association for Computational Linguis-tics, pages 57?64, Sapporo, Japan.
ACL.
6thInternational Workshop on Information Retrievalwith Asian languages; a shorter version publishedin ACL Annual Meeting 2003, pp.
141?144.Sag, I., Baldwin, T., Bond, F., Copestake, A., andFlickinger, D. (2002).
Multiword expressions:A pain in the neck for NLP.
In Gelbukh, A.,editor, Computational Linguistics and IntelligentText Processing: Proceedings of the 3rd Interna-tional Conference, number 2276 in Lecture Notesin Computer Science, pages 189?206, MexicoCity, Mexico.
Springer-Verlag.Sharoff, S., Babych, B., and Hartley, A.
(2006).
Us-ing collocations from comparable corpora to findtranslation equivalents.
In LREC06 (2006), pages465?470.Sofianopoulos, S., Vassiliou, M., and Tambouratzis,G.
(2012).
Implementing a language-independentMT methodology.
In Proceedings of the 50thAnnual Meeting of the Association for Computa-tional Linguistics, pages 1?10, Jeju, Korea.
ACL.First Workshop on Multilingual Modeling.Steinberger, R., Pouliquen, B., Widiger, A., Ignat,C., Erjavec, T., Tufis?, D., and Varga, D. (2006).The JRC-Acquis: A multilingual aligned parallelcorpus with 20+ languages.
In LREC06 (2006),pages 2142?2147.30
