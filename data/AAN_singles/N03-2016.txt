Cognates Can Improve Statistical Translation ModelsGrzegorz KondrakDepartment of Computing ScienceUniversity of Alberta221 Athabasca HallEdmonton, AB, Canada T6G 2E8kondrak@cs.ualberta.eduDaniel Marcu and Kevin KnightInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA, 90292marcu,knight@isi.eduAbstractWe report results of experiments aimed at im-proving the translation quality by incorporatingthe cognate information into translation mod-els.
The results confirm that the cognate iden-tification approach can improve the quality ofword alignment in bitexts without the need forextra resources.1 IntroductionIn the context of machine translation, the term cognatesdenotes words in different languages that are similarin their orthographic or phonetic form and are possibletranslations of each other.
The similarity is usually dueeither to a genetic relationship (e.g.
English night andGerman nacht) or borrowing from one language to an-other (e.g.
English sprint and Japanese supurinto).
Ina broad sense, cognates include not only genetically re-lated words and borrowings but also names, numbers, andpunctuation.
Practically all bitexts (bilingual parallel cor-pora) contain some kind of cognates.
If the languages arerepresented in different scripts, a phonetic transcriptionor transliteration of one or both parts of the bitext is apre-requisite for identifying cognates.Cognates have been employed for a number of bitext-related tasks, including sentence alignment (Simard etal., 1992), inducing translation lexicons (Mann and Ya-rowsky, 2001), and improving statistical machine trans-lation models (Al-Onaizan et al, 1999).
Cognates areparticularly useful when machine-readable bilingual dic-tionaries are not available.
Al-Onaizan et al (1999) ex-perimented with using bilingual dictionaries and cog-nates in the training of Czech?English translation mod-els.
They found that appending probable cognates to thetraining bitext significantly lowered the perplexity scoreon the test bitext (in some cases more than when using abilingual dictionary), and observed improvement in wordalignments of test sentences.In this paper, we investigate the problem of incorpo-rating the potentially valuable cognate information intothe translation models of Brown et al (1990), which, intheir original formulation, consider lexical items in ab-straction of their form.
For training of the models, weuse the GIZA program (Al-Onaizan et al, 1999).
A listof likely cognate pairs is extracted from the training cor-pus on the basis of orthographic similarity, and appendedto the corpus itself.
The objective is to reinforce the co-ocurrence count between cognates in addition to alreadyexisting co-ocurrences.
The results of experiments con-ducted on a variety of bitexts show that cognate iden-tification can improve word alignments, which leads tobetter translation models, and, consequently, translationsof higher quality.
The improvement is achieved withoutmodifying the statistical training algorithm.2 The methodWe experimented with three word similarity measu-res: Simard?s condition, Dice?s coefficient, and LCSR.Simard et al (1992) proposed a simple condition for de-tecting probable cognates in French?English bitexts: twowords are considered cognates if they are at least fourcharacters long and their first four characters are iden-tical.
Dice?s coefficient is defined as the ratio of thenumber of shared character bigrams to the total num-ber of bigrams in both words.
For example, colour andcouleur share three bigrams (co, ou, and ur), so theirDice?s coefficient is 611' 0:55.
The Longest CommonSubsequence Ratio (LCSR) of two words is computedby dividing the length of their longest common subse-quence by the length of the longer word.
For example,LCSR(colour,couleur) = 57' 0:71, as their longest com-mon subsequence is ?c-o-l-u-r?.In order to identify a set of likely cognates in a tok-enized and sentence-aligned bitext, each aligned segmentis split into words, and all possible word pairings arestored in a file.
Numbers and punctuation are not con-sidered, since we feel that they warrant a more specificapproach.
After sorting and removing duplicates, the filerepresents all possible one-to-one word alignments of thebitext.
Also removed are the pairs that include Englishfunction words, and words shorter than the minimumlength (usually set at four characters).
For each word pair,a similarity measure is computed, and the file is againsorted, this time by the computed similarity value.
If themeasure returns a non-binary similarity value, true cog-nates are very frequent near the top of the list, and be-come less frequent towards the bottom.
The set of likelycognates is obtained by selecting all pairs with similarityabove a certain threshold.
Typically, lowering the thresh-old increases recall while decreasing precision of the set.Finally, one or more copies of the resulting set of likelycognates are concatenated with the training set.3 ExperimentsWe induced translation models using IBM Model 4(Brown et al, 1990) with the GIZA toolkit (Al-Onaizanet al, 1999).
The maximum sentence length in the train-ing data was set at 30 words.
The actual translationswere produced with a greedy decoder (Germann et al,2001).
For the evaluation of translation quality, we usedthe BLEU metric (Papineni et al, 2002), which measuresthe n-gram overlap between the translated output and oneor more reference translations.
In our experiments, weused only one reference translation.3.1 Word alignment qualityIn order to directly measure the influence of the addedcognate information on the word alignment quality, weperformed a single experiment using a set of 500 man-ually aligned sentences from Hansards (Och and Ney,2000).
Giza was first trained on 50,000 sentences fromHansards, and then on the same training set augmentedwith a set of cognates.
The set consisted of two copies ofa list produced by applying the threshold of 0:58 to LCSRlist.
The duplication factor was arbitrarily selected on thebasis of earlier experiments with a different training andtest set taken from Hansards.The incorporation of the cognate information resultedin a 10% reduction of the word alignment error rate,from 17.6% to 15.8%, and a corresponding improvementin both precision and recall.
An examination of ran-domly selected alignments confirms the observation ofAl-Onaizan et al (1999) that the use of cognate informa-tion reduces the tendency of rare words to align to manyco-occurring words.In another experiment, we concentrated on co-oc-curring identical words, which are extremely likely torepresent mutual translations.
In the baseline model,links were induced between 93.6% of identical words.
Inthe cognate-augmented model, the ratio rose to 97.2%.3.2 EuroparlEuroparl is a tokenized and sentence-aligned multilingualcorpus extracted from the Proceedings of the European0.2020.2030.2040.2050.2060.2070.2080 1 2 3 4 5 6BLEUscoreDuplication factor"Simard""DICE""LCSR"Figure 1: BLEU scores as a function of the duplicationfactor for five methods of cognates identification aver-aged over nine language pairs.Parliament (Koehn, 2002).
The eleven official EuropeanUnion languages are represented in the corpus.
We con-sider the variety of languages as important for a valida-tion of the cognate-based approach as general, rather thanlanguage-specific.As the training data, we arbitrarily selected a subset ofthe corpus that consisted the proceedings from October1998.
By pairing English with the remaining languages,we obtained nine bitexts1, each comprising about 20,000aligned sentences (500,000 words).
The test data con-sisted of 1755 unseen sentences varying in length from 5to 15 words from the 2000 proceedings (Koehn, 2002).The English language model was trained separately on alarger set of 700,000 sentences from the 1996 proceed-ings.Figure 1 shows the BLEU scores as a function of theduplication factor for three methods of cognates identi-fication averaged over nine language pairs.
The resultsaveraged over a number of language pairs are more in-formative than results obtained on a single language pair,especially since the BLEU metric is only a rough approx-imation of the translation quality, and exhibits consider-able variance.
Three different similarity measures werecompared: Simard, DICE with a threshold of 0.39, andLCSR with a threshold of 0.58.
In addition, we experi-mented with two different methods of extending the train-ing set with with a list of cognates: one pair as one sen-tence (Simard), and thirty pairs as one sentence (DICEand LCSR).21Greek was excluded because its non-Latin script requires adifferent type of approach to cognate identification.2In the vast majority of the sentences, the alignment links arecorrectly induced between the respective cognates when multi-Threshold Pairs ScoreBaseline 0 0.20270.99 863 0.20160.71 2835 0.20300.58 5339 0.20580.51 7343 0.20730.49 14115 0.2059Table 1: The number of extracted word pairs as a func-tion of the LCSR threshold, and the corresponding BLEUscores, averaged over nine Europarl bitexts.The results show a statistically significant improve-ment3 in the average BLEU score when the duplicationfactor is greater than 1, but no clear trend can be discernedfor larger factors.
There does not seem to be much differ-ence between various methods of cognate identification.Table 1 shows results of augmenting the training setwith different sets of cognates determined using LCSR.A threshold of 0.99 implies that only identical wordpairs are admitted as cognates.
The words pairs withLCSR around 0.5 are more likely than not to be unre-lated.
In each case two copies of the cognate list wereused.
The somewhat surprising result was that addingonly ?high confidence?
cognates is less effective thanadding lots of dubious cognates.
In that particular setof tests, adding only identical word pairs, which almostalways are mutual translations, actually decreased theBLEU score.
Our results are consistent with the resultsof Al-Onaizan et al (1999), who observed perplexity im-provement even when ?extremely low?
thresholds wereused.
It seems that the robust statistical training algo-rithm has the ability of ignoring the unrelated word pairs,while at the same time utilizing the information providedby the true cognates.3.3 A manual evaluationIn order to confirm that the higher BLEU scores reflecthigher translation quality, we performed a manual evalua-tion of a set of a hundred six-token sentences.
The modelswere induced on a 25,000 sentences portion of Hansards.The training set was augmented with two copies of a cog-nate list obtained by thresholding LCSR at 0.56.
Resultsple pairs per sentence are added.3Statistical significance was estimated in the following way.The variance of the BLEU score was approximated by randomlypicking a sample of translated sentences from the test set.
Thesize of the test sample was equal to the size of the test set (1755sentences).
The score was computed in this way 200 times foreach language.
The mean and the variance of the nine-languageaverage was computed by randomly picking one of the 200scores for each language and computing the average.
The meanresult produced was 0.2025, which is very close to the baselineaverage score of 0.2027.
The standard deviation of the averagewas estimated to be 0.0018, which implies that averages above0.2054 are statistically significant at the 0.95 level.Evaluation Baseline CognatesCompletely correct 16 21Syntactically correct 8 7Semantically correct 14 12Wrong 62 60Total 100 100Table 2: A manual evaluation of the translations gener-ated by the baseline and the cognate-augmented models.of a manual evaluation of the entire set of 100 sentencesare shown in Table 2.
Although the overall translationquality is low due to the small size of the training corpusand the lack of parameter tuning, the number of com-pletely acceptable translations is higher when cognatesare added.4 ConclusionOur experimental results show that the incorporation ofcognate information can improve the quality of wordalignments, which in turn result in better translations, Inour experiments, the improvement, although statisticallysignificant, is relatively small, which can be attributed tothe relative crudeness of the approach based on append-ing the cognate pairs directly to the training data.
In thefuture, we plan to develop a method of incorporating thecognate information directly into the training algorithm.We foresee that the performance of such a method willalso depend on using more sophisticated word similaritymeasures.ReferencesY.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,D.
Melamed, F. Och, D. Purdy, N. Smith, and D. Yarowsky.1999.
Statistical machine translation.
Technical report,Johns Hopkins University.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1990.The mathematics of statistical machine translation: Parame-ter estimation.
Computational Linguistics, 19(2):263?311.U.
Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada.2001.
Fast decoding and optimal decoding for machinetranslation.
In Proceedings of ACL-01.P.
Koehn.
2002.
Europarl: A multilingual corpus for evaluationof machine translation.
In preparation.G.
Mann and D. Yarowsky.
Multipath translation lexicon induc-tion via bridge languages.
In Proceedings of NAACL 2001.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proceedings of ACL-00.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.
BLEU: amethod for automatic evaluation of machine translation.
InProceedings of ACL-02.M.
Simard, G. F. Foster, and P. Isabelle.
1992.
Using cognatesto align sentences in bilingual corpora.
In Proceedings ofTMI-92.
