Dependency Tree Based Sentence CompressionKatja Filippova and Michael StrubeEML Research gGmbHSchlo?-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlpAbstractWe present a novel unsupervised method forsentence compression which relies on a de-pendency tree representation and shortens sen-tences by removing subtrees.
An automaticevaluation shows that our method obtains re-sult comparable or superior to the state of theart.
We demonstrate that the choice of theparser affects the performance of the system.We also apply the method to German and re-port the results of an evaluation with humans.1 IntroductionWithin the field of text-to-text generation, the sen-tence compression task can be defined as follows:given a sentence S, consisting of words w1w2...wn,what is a subset of the words of S, such that itis grammatical and preserves essential informationfrom S?
There are many applications which wouldbenefit from a robust compression system, such assubtitle generation, compression for mobile deviceswith a limited screen size, or news digests.
Giventhat to date most text and speech summarizationsystems are extractive, sentence compression tech-niques are a common way to deal with redundancyin their output.In recent years, a number of approaches to sen-tence compression have been developed (Jing, 2001;Knight & Marcu, 2002; Gagnon & Da Sylva, 2005;Turner & Charniak, 2005; Clarke & Lapata, 2008,inter alia).
Many explicitly rely on a languagemodel, usually a trigram model, to produce gram-matical output (Knight & Marcu, 2002; Hori & Fu-rui, 2004; Turner & Charniak, 2005; Galley & McK-eown, 2007).
Testing the grammaticality of the out-put with a language model is justified when work-ing with a language with rigid word order like En-glish, and all but one approach mentioned havebeen applied to English data.
However, compress-ing sentences in languages with less rigid word or-der needs a deeper analysis to test grammaticality.And even for languages with rigid word order thetrigram model ignores the structure of the sentenceand therefore may significantly distort the meaningof the source sentence.
Approaches going beyondthe word level either require a comprehensive lexi-con (Jing, 2001), or manually devised rules (Gagnon& Da Sylva, 2005; Clarke & Lapata, 2008) to de-termine prunable constituents.
A lexicon is not al-ways available, whereas the hand-crafted rules maynot cover all cases and are too general to be univer-sally applicable (e.g.
PPs can be pruned).In this paper we present a novel unsupervised ap-proach to sentence compression which is motivatedby the belief that the grammaticality of the outputcan be better ensured by compressing trees.
In par-ticular, given a dependency tree, we want to prunesubtrees which are neither obligatory syntactic argu-ments, nor contribute important information to thecontent of the sentence.
A tree pruning approachdoes not generate new dependencies and is unlikelyto produce a compression with a totally differentmeaning.
Our approach is unsupervised and adapt-able to other languages, the main requirement be-ing that there are a dependency parser and a corpusavailable for the languages.
We test our approachon English and German data sets and obtain resultscomparable or superior to the state of the art.252 Related WorkMany existing compression systems use a noisy-channel approach and rely on a language modelto test the grammaticality of the output (Knight &Marcu, 2002; Turner & Charniak, 2005; Galley &McKeown, 2007).
Other ways to ensure gram-maticality and to decide whether a constituent isobligatory or may be pruned are to utilize a sub-categorization lexicon (Jing, 2001), or to define aset of generally prunable constituents.
Gagnon &Da Sylva (2005) prune dependency trees by remov-ing prepositional complements of the verb, subordi-nate clauses and noun appositions.
Apparently, thisdoes not guarantee grammaticality in all cases.
Itmay also eliminate important information from thetree.Most approaches are supervised and require train-ing data to learn which words or constituents canbe dropped from a sentence (Riezler et al, 2003;McDonald, 2006).
However, it is difficult to obtaintraining data.
Still, there are few unsupervised meth-ods.
For example, Hori & Furui (2004) introducea scoring function which relies on such informa-tion sources as word significance score and languagemodel.
A compression of a given length whichmaximizes the scoring function is then found withdynamic programming.
Clarke & Lapata (2008)present another unsupervised approach.
They for-mulate the task as an optimization problem and solveit with integer linear programming.
Two scores con-tribute to their objective function ?
a trigram lan-guage model score and a word significance score.Additionally, the grammaticality of the output is en-sured by a handful of linguistic constraints, statinge.g.
which arguments should be preserved.In this paper we suggest an alternative to the pop-ular language model basis for compression systems?
a method which compresses dependency trees andnot strings of words.
We will argue that our formu-lation has the following advantages: firstly, the ap-proach is unsupervised, the only requirement beingthat there is a sufficiently large corpus and a depen-dency parser available.
Secondly, it requires neithera subcategorization lexicon nor hand-crafted rules todecide which arguments are obligatory.
Thirdly, itfinds a globally optimal compression by taking syn-tax and word importance into account.3 Dependency Based CompressionOur method compresses sentences in that it removesdependency edges from the dependency tree of asentence.
The aim is to preserve dependencieswhich are either required for the output to be gram-matical or have an important word as the dependent.The algorithm proceeds in three steps: tree transfor-mation (Section 3.1), tree compression (Section 3.2)and tree linearization (Section 3.3).3.1 Tree TransformationBefore a dependency tree is compressed, i.e.
be-fore some of the dependencies are removed, the treeis modified.
We will demonstrate the effect of thetransformations with the sentence below:(1) He said that he lived in Paris and BerlinThe first transformation (ROOT) inserts an explicitrootnode (Fig.
1(a)).
The result of the second trans-formation (VERB) is that every inflected verb in thetree gets an edge originating from the rootnode (Fig.1(b)).
All edges outgoing from the rootnode bear thes label.
Apart from that we remove auxiliary edgesand memorize such grammatical properties as voice,tense or negation for verbs.The purpose of the remaining transformations isto make relations between open-class words moreexplicit.
We want to decide on pruning an edgejudging from two considerations: (i) how importantfor the head this argument is; (ii) how informativethe dependent word is.
As an example, consider asource sentence given in (2).
Here, we want to de-cide whether one prepositional phrase (or both) canbe pruned without making the resulting sentence un-grammatical.
(2) After some time, he moved to London.It would not be very helpful to check whether an ar-gument attached with the label pp is obligatory forthe verb move.
Looking at a particular preposition(after vs. to) would be more enlightening.
Thismotivates the PREP transformation which removesprepositional nodes and places them as labels on theedge from their head to the respective noun (Fig.1(c)).
We also decompose a chain of conjoined ele-ments (CONJ) and attach each of them to the head ofthe first element in the chain with the label the first26ParisBerlinandhelivesayheinroots(a) The source tree after ROOT.ParisBerlinandhelivesayheinroots s(b) After VERBParisBerlinandhelivesayherootssin(c) After PREPlivesayherootParis Berlinhessinin(d) After CONJFigure 1: The dependency structure of He said that he lived in Paris and Berlin after the transformationselement attaches to its head with (Fig.
1(d)).
Thisway we can retain any of the conjoined elementsin the compression and do not have to preserve thewhole sequence of them if we are interested in onlyone.
This last transformation is not applied to verbs.3.2 Tree CompressionWe formulate the compression task as an optimiza-tion problem which we solve using integer linearprogramming1.
Given a transformed dependencytree (a graph if new edges have been added), we de-cide which dependency edges to remove.
For eachdirected dependency edge from head h to word w wethus introduce a binary variable xlh,w where l standsfor the edge?s label:xlh,w ={1 if the dependency is preserved0 otherwise(1)The goal is to find a subtree which gets the highestscore of the objective function (2) to which both the1In our implementation we use lp solve (http://sourceforge.net/projects/lpsolve).probability of dependencies (P (l|h)) and the impor-tance of dependent words (I(w)) contribute:f(X) =?xxlh,w ?
P (l|h) ?
I(w) (2)Intuitively, the conditional probabilities prevent usfrom removing obligatory dependencies from thetree.
For example, P (subj|work) is higher thanP (with|work), and therefore the subject will bepreserved whereas the prepositional label and thusthe whole PP can be pruned.
This way we donot have to create an additional constraint for everyobligatory argument (e.g.
subject or direct object).Neither do we require a subcategorization lexicon tolook up which arguments are obligatory for a cer-tain verb.
Verb arguments are preserved because thedependency edges, with which they are attached tothe head, get high scores.
Table 1 presents the prob-abilities of a number of labels given that the headis study.
Table 2 presents the probabilities for theirGerman counterparts.Note that if we would not apply the PREP trans-formation we would not be able to distinguish be-27subj dobj in at after with to0.16 0.13 0.05 0.04 0.01 0.01 0.01Table 1: Probabilities of subj, d(irect)obj, in, at, after,with, to given the verb studysubj obja in an nach mit zu0.88 0.74 0.44 0.42 0.09 0.02 0.01Table 2: Probabilities of subj, obja(ccusative), in, at, af-ter, with, to given the verb studierentween different prepositions and could only calcu-late P (pp|studieren) which would not be very in-formative.
The probabilities for English are lowerthan those for German because we calculate the con-ditional probabilities given word lemma.
In English,the part of speech information cannot be inducedfrom the lemma and thus the set of possible labelsof a node is on average larger than in German.There are many ways in which word importance,I(w) can be defined.
Here, we use the formula intro-duced by Clarke & Lapata (2008) which is a modifi-cation of the significance score of Hori et al (2003):I(wi) =lN ?
fi logFAFi(3)wi is the topic word (either noun or verb), fi is thefrequency of wi in the document, Fi is the frequencyof wi in the corpus, and FA is the sum of frequenciesof all topic words in the corpus.
l is the number ofclause nodes above w and N is the maximum levelof embedding of the sentence w belongs to.The objective function is subject to constraints oftwo kinds.
The constraints of the first kind are stuc-tural and ensure that the preserved dependencies re-sult in a tree.
(4) ensures that each word has onehead at most.
(5) ensures connectivity in the tree.
(6) restricts the size of the resulting tree to ?
words.
?w ?
W,?h,lxlh,w ?
1 (4)?w ?
W,?h,lxlh,w ?1|W |?u,lxlw,u ?
0 (5)?xxlh,w ?
?
(6)?
is a function of the length of the source sentencein open-class words.
The function is not linear sincethe degree of compression increases with the lengthof the sentence.
The compression rate of human-generated sentences is about 70% (Clarke & Lapata,2008)2.
To approximate this value, we set the pro-portion of deleted words to be 20% for short sen-tences (5-9 non-stop words), this value increases upto 50% for long sentences (30+ words).The constraints of the second type ensure the syn-tactic validity of the output tree and explicitly statewhich edges should be preserved.
These constraintscan be general as well as conditional.
The formerensure that an edge is preserved if its source nodeis retained in the output.
Conditional syntactic con-straints state that an edge has to be preserved if (andonly if) a certain other edge is preserved.
We haveonly one syntactic constraint which states that a sub-ordinate conjunction (sc) should be preserved if andonly if the clause it belongs to functions as a sub-ordinate clause (sub) in the output.
If it is taken asthe main clause, the conjunction should be dropped.In terms of edges, this can be formulated as follows(7):?xscw,u, xsubh,w ?
xscw,u = 0 (7)Due to the constraint (4), the compressed subtreeis always rooted in the node added as a result of thefirst transformation.
A compression of a sentence toan embedded clause is not possible unless one pre-serves the structure above the embedded clause.
Of-ten, however, main clauses are less important than anembedded clause.
For example, given the sentenceHe said they have to be held in Beirut it is the em-bedded clause which is informative and to which thesource sentence should be compressed.
The purposeof the VERB modification is to amend exactly thisproblem.
Having an edge from the rootnode to ev-ery inflected verb allows us to compress the sourcesentence to any clause.3.3 Tree LinearizationA very simple but reasonable linearization techniqueis to present the words of a compressed sentence inthe order they are found in the source sentence.
Thismethod has been applied before and this is how we2Higher rates correspond to longer compressions.28linearize the trees obtained for the English data.
Un-fortunately, this method cannot be directly applied toGerman because of the constraints on word order inthis language.
One of the rules of German grammarstates that in the main clause the inflected part of theverb occupies the second position, the first positionbeing occupied by exactly one constituent.
There-fore, if the sentence initial position in a source sen-tence is occupied by a constituent which got prunedoff as a result of compression, the verb becomesthe first element of the sentence which results in anundesirable output.
There are linearization meth-ods developed for German which find an optimalword order for a sentence (Ringger et al, 2004;Filippova & Strube, 2007).
We use our recentmethod to linearize compressed trees.4 Corpora and AnnotationWe apply our method to sentences from two corporain English and German.
These are presented below.English Compression Corpus: The English datawe use is a document-based compression cor-pus from the British National Corpus andAmerican News Text Corpus which consists of82 news stories3.
We parsed the corpus withRASP (Briscoe et al, 2006) and with the Stan-ford PCFG parser (Klein & Manning, 2003).The output of the former is a set of dependencyrelations whereas the latter provides an optionfor converting the output into dependency for-mat (de Marneffe et al, 2006) which we use.Tu?Ba-D/Z: The German corpus we use is a col-lection of 1,000 newspaper articles (Telljohannet al, 2003)4.
Sentence boundaries, morphol-ogy, dependency structure and anaphoric rela-tions are manually annotated in this corpus.RASP has been used by Clarke & Lapata (2008)whose state of the art results we compare with ours.We use not only RASP but also the Stanford parserfor several reasons.
Apart from being accurate, thelatter has an elaborated set of dependency relations3The corpus is available from http://homepages.inf.ed.ac.uk/s0460084/data.4The corpus is available from http://www.sfs.uni-tuebingen.de/en_tuebadz.shtml.
(48 vs. 15 of RASP) which is not overly large (com-pared with the 106 grammatical relations of the LinkParser).
This is important for our system whichrelies on syntactic information when making prun-ing decisions.
A comparison between the Stanfordparser and two dependency parsers, MiniPar andLink Parser, showed a decent performance of the for-mer (de Marneffe et al, 2006).
It is also of interest tosee to what extent the choice of the parser influencesthe results.Apart from the corpora listed above, we use theTipster corpus to calculate conditional probabilitiesof syntactic labels given head lemmas as well asword significance scores.
The significance scoreis calculated from the total number of 128 mil-lion nouns and verbs.
Conditional probabilities arecalculated from a much smaller portion of Tipster(about 6 million tokens).
The latter number is com-parable to the size of the data set we use to com-pute the probabilities for German.
There, we usea corpus of about 4,000 articles from the GermanWikipedia to calculate conditional probabilities andsignificance scores.
The corpus is parsed with thehighly accurate CDG parser (Foth & Menzel, 2006)and has the same dependency format as Tu?Ba-D/Z(Versley, 2005).Although all corpora are annotated with depen-dency relations, there are considerable differencesbetween the annotation of the English and Germandata sets.
The phrase to dependency structure con-version done by the Stanford parser makes the se-mantic head of the constituent its syntactic head.
Forexample, in the sentence He is right it is the adjec-tive right which is the root of the tree.
Unlike that,sentences from the German corpora always have averb as the root.
To unify the formats, we write a setof rules to make the verb the root of the tree in allcases.5 EvaluationWe evaluate the results automatically as well as withhuman subjects.
To assess the performance of themethod on the English data, we calculate the F-measure on grammatical relations.
Following Rie-zler et al (2003), we calculate average precision andrecall as the amount of grammatical relations sharedbetween the output of our system and the gold stan-29dard variant divided over the total number of rela-tions in the output and in the human-generated com-pression respectively.
According to Clarke & Lapata(2006), this measure reliably correlates with humanjudgements.
The results of our evaluation as well asthe state of the art results reported by Clarke & Lap-ata (2008) (LM+SIG+CONSTR), whose system useslanguage model scoring (LM), word significancescore (SIG), and linguistic constraints (CONSTR),are presented in Table 3.
The F-measure reportedby Clarke & Lapata (2008) is calculated with RASPwhich their system builds upon.
For our system wepresent the results obtained on the data parsed withRASP as well as with the Stanford parser (SP).
Inboth cases the F-measure is found with RASP in or-der to allow for a fair comparison between the threesystems.
We recalculate the compression rate for thegold standard ignoring punctuation.
On the wholecorpus the compression rate turns out to be slightlyhigher than that reported by Clarke & Lapata (2008)(70.3%).F-measure compr.rateLM+SIG+CONSTR 40.5 72.0%DEP-BASED (RASP) 40.7 49.6%DEP-BASED (SP) 49.3 69.3%GOLD - 72.1%Table 3: Average results on the English corpusAs there are no human-generated compressionsfor German data, we evaluate the performance of themethod in terms of grammaticality and importanceby means of an experiment with native speakers.
Inthe experiment, humans are presented with a sourcesentence and its compression which they are askedto evaluate on two five-point scales.
Higher gradesare given to better sentences.
Importance representsthe amount of relevant information from the sourcesentence retained in the compression.
Since ourmethod does not generate punctuation, the judgesare asked to ignore errors due to missing commas.Five participants took part in the experiment andeach rated the total of 25 sentences originating froma randomly chosen newspaper article.
Their ratingsas well as the ratings reported by Clarke & Lapata(2008) on English corpus are presented in Table 4.grammar importanceLM+SIG+CONSTR 3.76 3.53DEP-BASED (DE) 3.62 3.21Table 4: Average results for the German data6 DiscussionThe results on the English data are comparable withor superior to the state of the art.
These were ob-tained with a single linguistic constraint (7) andwithout any elaborated resources which makes oursystem adaptable to other languages.
This suggeststhat tree compression is a better basis for sentencecompression systems than language model-orientedword deletion.In order to explain why the choice of parser sig-nificantly influences the performance of the method,we calculate the precision P defined as the numberof dependencies shared by a human-generated com-pression (depc) and the source sentence (deps) di-vided over the total number of dependencies foundin the compression:P = |depc ?
deps||depc|(8)The intuition is that if a parser does not reach highprecision on gold standard sentences, i.e.
if it doesnot assign similar dependency structures to a sourcesentence and its compression, then it is hopelessto expect it to produce good compression with ourdependency-based method.
However, the precisiondoes not have to be as high as 100% because of,e.g., changes within a chain of conjoined elementsor appositions.
The precision of the two parsers cal-culated over the compression corpus is presented inTable 5.RASP Stanford parserprecision 79.6% 84.3%Table 5: Precision of the parsersThe precision of the Stanford parser is about 5%higher than that of RASP.
In our opinion, this partlyexplains why the use of the Stanford parser increasesthe F-measure by 9 points.
Another possible reasonfor this improvement is that the Stanford parser iden-tifies three times more dependency relations than30RASP and thus allows for finer distinctions betweenthe arguments of different types.Another point concerns the compression rates.The compressions generated with RASP are consid-erably shorter than those generated with the Stanfordparser.
This is mainly due to the fact that the struc-ture output by RASP is not necessarily a tree or aconnected graph.
In such cases only the first subtreeof the sentence is taken as input and compressed.The results on the German set are not conclu-sive since the number of human judges is relativelysmall.
Still, these preliminary results are compara-ble to those reported for English and thus give ussome evidence that the method can be adapted tolanguages other than English.
Interestingly, the im-portance score depends on the grammaticality of thesentence.
A grammatical sentence can convey unim-portant information but it was never the case that anungrammatical sentence got a high rating on the im-portance scale.
Some of the human judges told usthat they had difficulties assigning the importancescore to ungrammatical sentences.7 ConclusionsWe presented a new compression method whichcompresses dependency trees and does not rely on alanguage model to test grammaticality.
The methodis unsupervised and can be easily adapted to lan-guages other than English.
It does not require asubcategorization lexicon or elaborated hand-craftedrules to decide which arguments can be pruned andfinds a globally optimal compression taking syn-tax and word importance into account.
We demon-strated that the performance of the system dependson the parser and suggested a way to estimate howwell a parser is suited for the compression task.
Theresults indicate that the dependency-based approachis an alternative to the language model-based onewhich is worth pursuing.Acknowledgements: This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by a KTFgrant (09.009.2004).
We would like to thank Yan-nick Versley who helped us convert Tu?Ba-D/Z in theCDG format and Elke Teich and the three anony-mous reviewers for their useful comments.ReferencesBriscoe, Edward, John Carroll & Rebecca Watson(2006).
The second release of the RASP sys-tem.
In Proceedings of the COLING-ACL In-teractive Presentation Session, Sydney, Australia,2006, pp.
77?80.Clarke, James & Mirella Lapata (2006).
Models forsentence compression: A comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics, Sydney, Australia, 17?21July 2006, pp.
377?385.Clarke, James & Mirella Lapata (2008).
Global in-ference for sentence compression: An integer lin-ear programming approach.
Journal of ArtificialIntelligence Research, 31:399?429.de Marneffe, Marie-Catherine, Bill MacCartney &Christopher D. Manning (2006).
Generatingtyped dependency parses from phrase structureparses.
In Proceedings of the 5th InternationalConference on Language Resources and Evalua-tion, Genoa, Italy, 22?28 May 2006, pp.
449?454.Filippova, Katja & Michael Strube (2007).
Generat-ing constituent order in German clauses.
In Pro-ceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics, Prague,Czech Republic, 23?30 June 2007, pp.
320?327.Foth, Kilian & Wolfgang Menzel (2006).
Hybridparsing: Using probabilistic models as predictorsfor a symbolic parser.
In Proceedings of the 21stInternational Conference on Computational Lin-guistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, Sydney, Aus-tralia, 17?21 July 2006, pp.
321?327.Gagnon, Michel & Lyne Da Sylva (2005).
Textsummarization by sentence extraction and syn-tactic pruning.
In Proceedings of ComputationalLinguistics in the North East, Gatineau, Que?bec,Canada, 26 August 2005.Galley, Michel & Kathleen R. McKeown (2007).Lexicalized Markov grammars for sentence com-31pression.
In Proceedings of Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, Rochester, N.Y., 22?27 April2007, pp.
180?187.Hori, Chiori & Sadaoki Furui (2004).
Speech sum-marization: An approach through word extractionand a method for evaluation.
IEEE Transactionson Information and Systems, E87-D(1):15?25.Hori, Chiori, Sadaoki Furui, Rob Malkin, Hua Yu& Alex Waibel (2003).
A statistical approach toautomatic speech summarization.
EURASIP Jour-nal on Applied Signal Processing, 2:128?139.Jing, Hongyan (2001).
Cut-and-Paste Text Summa-rization, (Ph.D. thesis).
Computer Science De-partment, Columbia University, New York, N.Y.Klein, Dan & Christopher D. Manning (2003).
Ac-curate unlexicalized parsing.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics, Sapporo, Japan, 7?12July 2003, pp.
423?430.Knight, Kevin & Daniel Marcu (2002).
Summariza-tion beyond sentence extraction: A probabilisticapproach to sentence compression.
Artificial In-telligence, 139(1):91?107.McDonald, Ryan (2006).
Discriminative sentencecompression with soft syntactic evidence.
InProceedings of the 11th Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics, Trento, Italy, 3?7 April 2006,pp.
297?304.Riezler, Stefan, Tracy H. King, Richard Crouch &Annie Zaenen (2003).
Statistical sentence con-densation using ambiguity packing and stochasticdisambiguation methods for Lexical-FunctionalGrammar.
In Proceedings of the Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association for Computa-tional Linguistics, Edmonton, Alberta, Canada,27 May ?1 June 2003, pp.
118?125.Ringger, Eric, Michael Gamon, Robert C. Moore,David Rojas, Martine Smets & Simon Corston-Oliver (2004).
Linguistically informed statisti-cal models of constituent structure for orderingin sentence realization.
In Proceedings of the20th International Conference on ComputationalLinguistics, Geneva, Switzerland, 23?27 August2004, pp.
673?679.Telljohann, Heike, Erhard W. Hinrichs & SandraKu?bler (2003).
Stylebook for the Tu?bingen tree-bank of written German (Tu?Ba-D/Z).
TechnicalReport: Seminar fu?r Sprachwissenschaft, Univer-sita?t Tu?bingen, Tu?bingen, Germany.Turner, Jenine & Eugene Charniak (2005).
Su-pervised and unsupervised learning for sentencecompression.
In Proceedings of the 43rd An-nual Meeting of the Association for Computa-tional Linguistics, Ann Arbor, Mich., 25?30 June2005, pp.
290?297.Versley, Yannick (2005).
Parser evaluation acrosstext types.
In Proceedings of the 4th Workshopon Treebanks and Linguistic Theories, Barcelona,Spain, 9-10 December 2005.32
