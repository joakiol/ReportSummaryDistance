Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343,Honolulu, October 2008. c?2008 Association for Computational LinguisticsBayesian Unsupervised Topic SegmentationJacob Eisenstein and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology77 Massachusetts Ave., Cambridge MA 02139{jacobe,regina}@csail.mit.eduAbstractThis paper describes a novel Bayesian ap-proach to unsupervised topic segmentation.Unsupervised systems for this task are drivenby lexical cohesion: the tendency of well-formed segments to induce a compact andconsistent lexical distribution.
We show thatlexical cohesion can be placed in a Bayesiancontext by modeling the words in each topicsegment as draws from a multinomial lan-guage model associated with the segment;maximizing the observation likelihood in sucha model yields a lexically-cohesive segmenta-tion.
This contrasts with previous approaches,which relied on hand-crafted cohesion met-rics.
The Bayesian framework provides a prin-cipled way to incorporate additional featuressuch as cue phrases, a powerful indicator ofdiscourse structure that has not been previ-ously used in unsupervised segmentation sys-tems.
Our model yields consistent improve-ments over an array of state-of-the-art systemson both text and speech datasets.
We alsoshow that both an entropy-based analysis anda well-known previous technique can be de-rived as special cases of the Bayesian frame-work.11 IntroductionTopic segmentation is one of the fundamental prob-lems in discourse analysis, where the task is todivide a text into a linear sequence of topically-coherent segments.
Hearst?s TEXTTILING (1994)introduced the idea that unsupervised segmentation1Code and materials for this work are available athttp://groups.csail.mit.edu/rbg/code/bayesseg/.can be driven by lexical cohesion, as high-qualitysegmentations feature homogeneous lexical distri-butions within each topic segment.
Lexical cohesionhas provided the inspiration for several successfulsystems (e.g., Utiyama and Isahara, 2001; Galley etal.2003; Malioutov and Barzilay, 2006), and is cur-rently the dominant approach to unsupervised topicsegmentation.But despite the effectiveness of lexical cohesionfor unsupervised topic segmentation, it is clear thatthere are other important indicators that are ignoredby the current generation of unsupervised systems.For example, consider cue phrases, which are ex-plicit discourse markers such as ?now?
or ?how-ever?
(Grosz and Sidner, 1986; Hirschberg and Lit-man, 1993; Knott, 1996).
Cue phrases have beenshown to be a useful feature for supervised topicsegmentation (Passonneau and Litman, 1993; Gal-ley et al, 2003), but cannot be incorporated bycurrent unsupervised models.
One reason for thisis that existing unsupervised methods use arbitrary,hand-crafted metrics for quantifying lexical cohe-sion, such as weighted cosine similarity (Hearst,1994; Malioutov and Barzilay, 2006).
Without su-pervision, it is not possible to combine such met-rics with additional sources of information.
More-over, such hand-crafted metrics may not general-ize well across multiple datasets, and often includeparameters which must be tuned on developmentsets (Malioutov and Barzilay, 2006; Galley et al,2003).In this paper, we situate lexical cohesion in aBayesian framework, allowing other sources of in-formation to be incorporated without the need forlabeled data.
We formalize lexical cohesion in agenerative model in which the text for each seg-334ment is produced by a distinct lexical distribution.Lexically-consistent segments are favored by thismodel because probability mass is conserved fora narrow subset of words.
Thus, lexical cohesionarises naturally through the generative process, andother sources of information ?
such as cue words?
can easily be incorporated as emissions from thesegment boundaries.More formally, we treat the words in each sen-tence as draws from a language model associatedwith the topic segment.
This is related to topic-modeling methods such as latent Dirichlet alocation(LDA; Blei et al 2003), but here the induced topicsare tied to a linear discourse structure.
This propertyenables a dynamic programming solution to find theexact maximum-likelihood segmentation.
We con-sider two approaches to handling the language mod-els: estimating them explicitly, and integrating themout, using the Dirichlet Compound Multinomial dis-tribution (also known as the multivariate Polya dis-tribution).We model cue phrases as generated from a sep-arate multinomial that is shared across all topicsand documents in the dataset; a high-likelihoodmodel will obtain a compact set of cue phrases.The addition of cue phrases renders our dynamicprogramming-based inference inapplicable, so wedesign a sampling-based inference technique.
Thisalgorithm can learn in a completely unsupervisedfashion, but it also provides a principled mechanismto improve search through the addition of declara-tive linguistic knowledge.
This is achieved by bias-ing the selection of samples towards boundaries withknown cue phrases; this does not change the under-lying probabilistic model, but guides search in thedirection of linguistically-plausible segmentations.We evaluate our algorithm on corpora of spokenand written language, including the benchmark ICSImeeting dataset (Janin et al, 2003) and a new tex-tual corpus constructed from the contents of a med-ical textbook.
In both cases our model achieves per-formance surpassing multiple state-of-the-art base-lines.
Moreover, we demonstrate that the addition ofcue phrases can further improve segmentation per-formance over cohesion-based methods.In addition to the practical advantages demon-strated by these experimental results, our model re-veals interesting theoretical properties.
Other re-searchers have observed relationships between dis-course structure and entropy (e.g., Genzel and Char-niak, 2002).
We show that in a special case ofour model, the segmentation objective is equal toa weighted sum of the negative entropies for eachtopic segment.
This finding demonstrates that a re-lationship between discourse segmentation and en-tropy is a natural consequence of modeling topicstructure in a generative Bayesian framework.
Inaddition, we show that the benchmark segmentationsystem of Utiyama and Isahara (2001) can be viewedas another special case of our Bayesian model.2 Related WorkExisting unsupervised cohesion-based approachescan be characterized in terms of the metric used toquantify cohesion and the search technique.
Galleyet al (2003) characterize cohesion in terms of lexicalchains ?
repetitions of a given lexical item over somefixed-length window of sentences.
In their unsu-pervised model, inference is performed by selectingsegmentation points at the local maxima of the cohe-sion function.
Malioutov and Barzilay (2006) opti-mize a normalized minimum-cut criteria based on avariation of the cosine similarity between sentences.Most similar to our work is the approach of Utiyamaand Isahara (2001), who search for segmentationswith compact language models; as shown in Sec-tion 3.1.1, this can be viewed as a special case of ourmodel.
Both of these last two systems use dynamicprogramming to search the space of segmentations.An alternative Bayesian approach to segmentationwas proposed by Purver et al (2006).
They assume aset of documents that is characterized by some num-ber of hidden topics that are shared across multipledocuments.
They then build a linear segmentationby adding a switching variable to indicate whetherthe topic distribution for each sentence is identicalto that of its predecessor.
Unlike Purver et al, wedo not assume a dataset in which topics are sharedacross multiple documents; indeed, our model canbe applied to single documents individually.
Addi-tionally, the inference procedure of Purver et al re-quires sampling multiple layers of hidden variables.In contrast, our inference procedure leverages thenature of linear segmentation to search only in thespace of segmentation points.335The relationship between discourse structure andcue phrases has been studied extensively; for anearly example of computational work on this topic,see (Grosz, 1977).
Passonneau and Litman (1993)were the first to investigate the relationship betweencue phrases and linear segmentation.
More recently,cue phrases have been applied to topic segmentationin the supervised setting.
In a supervised system thatis distinct from the unsupervised model describedabove, Galley et al (2003) automatically identifycandidate cue phrases by mining labeled data forwords that are especially likely to appear at segmentboundaries; the presence of cue phrases is then usedas a feature in a rule-based classifier for linear topicsegmentation.
Elsner and Charniak (2008) specifya list of cue phrases by hand; the cue phrases areused as a feature in a maximum-entropy classifierfor conversation disentanglement.
Unlike these ap-proaches, we identify candidate cue phrases auto-matically from unlabeled data and incorporate themin the topic segmentation task without supervision.3 Lexical Cohesion in a BayesianFrameworkThe core idea of lexical cohesion is that topically-coherent segments demonstrate compact and con-sistent lexical distributions (Halliday and Hasan,1976).
Lexical cohesion can be placed in a prob-abilistic context by modeling the words in eachtopic segment as draws from a multinomial languagemodel associated with the segment.
Formally, if sen-tence t is in segment j, then the bag of words xtis drawn from the multinomial language model ?j .This is similar in spirit to hidden topic models suchas latent Dirichlet alocation (Blei et al, 2003), butrather than assigning a hidden topic to each word,we constrain the topics to yield a linear segmenta-tion of the document.We will assume that topic breaks occur at sen-tence boundaries, and write zt to indicate the topicassignment for sentence t. The observation likeli-hood is,p(X|z,?)
=T?tp(xt|?zt), (1)where X is the set of all T sentences, z is the vectorof segment assignments for each sentence, and ?
isthe set of all K language models.2 A linear segmen-tation is ensured by the additional constraint that ztmust be equal to either zt?1 (the previous sentence?ssegment) or zt?1 + 1 (the next segment).To obtain a high likelihood, the language mod-els associated with each segment should concentratetheir probability mass on a compact subset of words.Language models that spread their probability massover a broad set of words will induce a lower likeli-hood.
This is consistent with the principle of lexicalcohesion.Thus far, we have described a segmentation interms of two parameters: the segment indices z, andthe set of language models ?.
For the task of seg-menting documents, we are interested only in thesegment indices, and would prefer not to have tosearch in the space of language models as well.
Weconsider two alternatives: taking point estimates ofthe language models (Section 3.1), and analyticallymarginalizing them out (Section 3.2).3.1 Setting the language model to the posteriorexpectationOne way to handle the language models is to choosea single point estimate for each set of segmenta-tion points z.
Suppose that each language modelis drawn from a symmetric Dirichlet prior: ?j ?Dir(?0).
Let nj be a vector in which each element isthe sum of the lexical counts over all the sentencesin segment j: nj,i =?
{t:zt=j}mt,i, where mt,i isthe count of word i in sentence t. Assuming thateach xt ?
?j , then the posterior distribution for ?jis Dirichlet with vector parameter nj+?0 (Bernardoand Smith, 2000).
The expected value of this distri-bution is the multinomial distribution ?
?j , where,?
?j,i =nj,i + ?0?Wi nj,i +W?0.
(2)In this equation,W indicates the number of wordsin the vocabulary.
Having obtained an estimate forthe language model ?
?j , the observed data likelihoodfor segment j is a product over each sentence in thesegment,2Our experiments will assume that the number of topics Kis known.
This is common practice for this task, as the desirednumber of segments may be determined by the user (Malioutovand Barzilay, 2006).336p({xt : zt = j}|?
?j) =?{t:zt=j}?i?xt?
?j,i (3)=?{t:zt=j}W?i?
?mt,ij,i (4)=W?i?
?nj,ij,i .
(5)By viewing the likelihood as a product over allterms in the vocabulary, we observe interesting con-nections with prior work on segmentation and infor-mation theory.3.1.1 Connection to previous workIn this section, we explain how our model gen-eralizes the well-known method of Utiyama andIsahara (2001; hereafter U&I).
As in our work,Utiyama and Isahara propose a probabilistic frame-work based on maximizing the compactness of thelanguage models induced for each segment.
Theirlikelihood equation is identical to our equations 3-5.They then define the language models for each seg-ment as ?
?j,i =nj,i+1W+PWi nj,i, without rigorous justifi-cation.
This form is equivalent to Laplacian smooth-ing (Manning and Schu?tze, 1999), and is a specialcase of our equation 2, with ?0 = 1.
Thus, the lan-guage models in U&I can be viewed as the expec-tation of the posterior distribution p(?j |{xt : zt =j}, ?0), in the special case that ?0 = 1.
Our ap-proach generalizes U&I and provides a Bayesianjustification for the language models that they ap-ply.
The remainder of the paper further extends thiswork by marginalizing out the language model, andby adding cue phrases.
We empirically demonstratethat these extensions substantially improve perfor-mance.3.1.2 Connection to entropyOur model also has a connection to entropy,and situates entropy-based segmentation within aBayesian framework.
Equation 1 defines the objec-tive function as a product across sentences; usingequations 3-5 we can decompose this across seg-ments instead.
Working in logarithms,log p(X|z, ??)
=T?tlog p(xt|??zt)=K?j?
{t:zt=j}log p(xt|?
?j)=K?jW?inj,i log ?
?j,i (6)The last line substitutes in the logarithm of equa-tion 5.
Setting ?0 = 0 and rearranging equation 2,we obtain nj,i = Nj ?
?j,i, with Nj =?Wi nj,i, thetotal number of words in segment j. Substitutingthis into equation 6, we obtainlog p(X|z, ??)
=K?jNj?i?
?j,i log ??j,i=K?jNjH(?
?j),where H(?
?j) is the negative entropy of the multino-mial ?
?j .
Thus, with ?0 = 0, the log conditional prob-ability in equation 6 is optimized by a segmentationthat minimizes the weighted sum of entropies persegment, where the weights are equal to the segmentlengths.
This result suggests intriguing connectionswith prior work on the relationship between entropyand discourse structure (e.g., Genzel and Charniak,2002; Sporleder and Lapata, 2006).3.2 Marginalizing the language modelThe previous subsection uses point estimates ofthe language models to reveal connections to en-tropy and prior work on segmentation.
However,point estimates are theoretically unsatisfying froma Bayesian perspective, and better performance maybe obtained by marginalizing over all possible lan-337guage models:p(X|z, ?0) =K?j?{t:zt=j}p(xt|?0)=K?j?d?j?
{t:zt=j}p(xt|?j)p(?j |?0)=K?jpdcm({xt : zt = j}|?0), (7)where pdcm refers to the Dirichlet compound multi-nomial distribution (DCM), also known as the multi-variate Polya distribution (Johnson et al, 1997).
TheDCM distribution expresses the expectation over allmultinomial language models, when conditioningon the Dirichlet prior ?0.
When ?0 is a symmetricDirichlet prior,pdcm({xt : zt = j}|?0)=?(W?0)?
(Nj +W?0)W?i?
(nj,i +W?0)?
(?0),where nj,i is the count of word i in segment j, andNj =?Wi nj,i, the total number of words in thesegment.
The symbol ?
refers to the Gamma func-tion, an extension of the factorial function to realnumbers.
Using the DCM distribution, we can com-pute the data likelihood for each segment from thelexical counts over the entire segment.
The overallobservation likelihood is a product across the likeli-hoods for each segment.3.3 Objective function and inferenceThe optimal segmentation maximizes the joint prob-ability,p(X, z|?0) = p(X|z, ?0)p(z).We assume that p(z) is a uniform distribution overvalid segmentations, and assigns no probabilitymass to invalid segmentations.
The data likelihoodis defined for point estimate language models inequation 5 and for marginalized language modelsin equation 7.
Note that equation 7 is written as aproduct over segments.
The point estimates for thelanguage models depend only on the counts withineach segment, so the overall likelihood for the point-estimate version also decomposes across segments.Any objective function that can be decomposedinto a product across segments can be maximizedusing dynamic programming.
We define B(t) as thevalue of the objective function for the optimal seg-mentation up to sentence t. The contribution to theobjective function from a single segment betweensentences t?
and t is written,b(t?, t) = p({xt?
.
.
.xt}|zt?...t = j)The maximum value of the objective functionis then given by the recurrence relation, B(t) =maxt?<tB(t?
)b(t?+1, t), with the base caseB(0) =1.
These values can be stored in a table of size T(equal to the number of sentences); this admits a dy-namic program that performs inference in polyno-mial time.3 If the number of segments is specifiedin advance, the dynamic program is slightly morecomplex, with a table of size TK.3.4 PriorsThe Dirichlet compound multinomial integratesover language models, but we must still set theprior ?0.
We can re-estimate this prior based onthe observed data by interleaving gradient-basedsearch in a Viterbi expectation-maximization frame-work (Gauvain and Lee, 1994).
In the E-step, weestimate a segmentation z?
of the dataset, as de-scribed in Section 3.3.
In the M-step, we maxi-mize p(?0|X, z?)
?
p(X|?0, z?)p(?0).
Assuming anon-informative hyperprior p(?0), we maximize thelikelihood in Equation 7 across all documents.
Themaximization is performed using a gradient-basedsearch; the gradients are dervied by Minka (2003).This procedure is iterated until convergence or amaximum of twenty iterations.4 Cue PhrasesOne of the key advantages of a Bayesian frameworkfor topic segmentation is that it permits the prin-cipled combination of multiple data sources, even3This assumes that the objective function for individual seg-ments can also be computed efficiently.
In our case, we needonly keep vectors of counts for each segment, and evaluateprobability density functions over the counts.338without labeled data.
We are especially interestedin cue phrases, which are explicit markers for dis-course structure, such as ?now?
or ?first?
(Groszand Sidner, 1986; Hirschberg and Litman, 1993;Knott, 1996).
Cue phrases have previously beenused in supervised topic segmentation (e.g., Gal-ley et al 2003); we show how they can be used inan unsupervised setting.The previous section modeled lexical cohesion bytreating the bag of words in each sentence as a se-ries of draws from a multinomial language modelindexed by the topic segment.
To incorporate cuephrases, this generative model is modified to reflectthe idea that some of the text will be topic-specific,but other terms will be topic-neutral cue phrasesthat express discourse structure.
This idea is imple-mented by drawing the text at each topic boundaryfrom a special language model ?, which is sharedacross all topics and all documents in the dataset.For sentences that are not at segment bound-aries, the likelihood is as before: p(xt|z,?, ?)
=?i?xt ?zt,i.
For sentences that immediately followsegment boundaries, we draw the first ` words from?
instead.
Writing x(`)t for the ` cue words in xt,and x?t for the remaining words, the likelihood for asegment-initial sentence is,p(xt|zt 6= zt?1,?, ?)
=?i?x(`)t?i?i?x?t?zt,i.We draw ?
from a symmetric Dirichlet prior ?0.
Fol-lowing prior work (Galley et al, 2003; Litman andPassonneau, 1995), we consider only the first wordof each sentence as a potential cue phrase; thus, weset ` = 1 in all experiments.4.1 InferenceTo estimate or marginalize the language models ?and ?, it is necessary to maintain lexical counts foreach segment and for the segment boundaries.
Thecounts for ?
are summed across every segment inthe entire dataset, so shifting a boundary will af-fect the probability of every segment, not only theadjacent segments as before.
Thus, the factoriza-tion that enabled dynamic programming inferencein Section 3.3 is no longer applicable.
Instead, wemust resort to approximate inference.Sampling-based inference is frequently used inrelated Bayesian models.
Such approaches builda stationary Markov chain by repeatedly samplingamong the hidden variables in the model.
The mostcommonly-used sampling-based technique is Gibbssampling, which iteratively samples from the condi-tional distribution of each hidden variable (Bishop,2006).
However, Gibbs sampling is slow to con-verge to a stationary distribution when the hiddenvariables are tightly coupled.
This is the case inlinear topic segmentation, due to the constraint thatzt ?
{zt?1, zt?1 + 1} (see Section 3).For this reason, we apply the more generalMetropolis-Hastings algorithm, which permits sam-pling arbitrary transformations of the latent vari-ables.
In our framework, such transformations cor-respond to moves through the space of possible seg-mentations.
A new segmentation z?
is drawn fromthe previous hypothesized segmentation z based ona proposal distribution q(z?|z).4 The probability ofaccepting a proposed transformation depends on theratio of the joint probabilities and a correction termfor asymmetries in the proposal distribution:paccept(z?
z?)
= min{1,p(X, z?|?0, ?0)p(X, z|?0, ?0)q(z|z?
)q(z?|z)}.The Metropolis-Hastings algorithm guaranteesthat by accepting samples at this ratio, our samplingprocedure will converge to the stationary distribu-tion for the hidden variables z.
When cue phrasesare included, the observation likelihood is written:p(X|z,?, ?)
=?
{t:zt 6=zt?1}?i?x(`)t?i?i?x?t?zt,i??
{t:zt=zt?1}?i?xt?zt,i.As in Section 3.2, we can marginalize over thelanguage models.
We obtain a product of DCM dis-tributions: one for each segment, and one for all cuephrases in the dataset.4.2 Proposal distributionMetropolis-Hastings requires a proposal distributionto sample new configurations.
The proposal distri-4Because the cue phrase language model ?
is used acrossthe entire dataset, transformations affect the likelihood of alldocuments in the corpus.
For clarity, our exposition will focuson the single-document case.339bution does not affect the underlying probabilisticmodel ?
Metropolis-Hastings will converge to thesame underlying distribution for any non-degenerateproposal.
However, a well-chosen proposal distribu-tion can substantially speed convergence.Our basic proposal distribution selects an existingsegmentation point with uniform probability, andconsiders a set of local moves.
The proposal is con-structed so that no probability mass is allocated tomoves that change the order of segment boundaries,or merge two segments; one consequence of this re-striction is that moves cannot add or remove seg-ments.5 We set the proposal distribution to decreaseexponentially with the move distance, thus favoringincremental transformations to the segmentation.More formally, let d(z ?
z?)
> 0 equal the dis-tance that the selected segmentation point is movedwhen we transform the segmentation from z to z?.We can write the proposal distribution q(z?
| z) ?c(z ?
z?
)d(z ?
z?
)?, where ?
< 0 sets the rateof exponential decay and c is an indicator functionenforcing the constraint that the moves do not reachor cross existing segmentation points.6We can also incorporate declarative linguisticknowledge by biasing the proposal distribution infavor of moves that place boundaries near knowncue phrase markers.
We multiply the unnormalizedchance of proposing a move to location z?
z?
by aterm equal to one plus the number of candidate cuephrases in the segment-initial sentences in the newconfiguration z?, written num-cue(z?).
Formally,qling(z?
| z?)
?
(1 + num-cue(z?))q(z?
| z).
Weuse a list of cue phrases identified by Hirschberg andLitman (1993).
We evaluate our model with both thebasic and linguistically-enhanced proposal distribu-tions.4.3 PriorsAs in section 3.4, we set the priors ?0 and ?0 us-ing gradient-based search.
In this case, we performgradient-based optimization after epochs of 10005Permitting moves to change the number of segments wouldsubstantially complicate inference.6We set ?
= ?
1max-move , where max-move is the maximummove-length, set to 5 in our experiments.
These parameters af-fect the rate of convergence but are unrelated to the underly-ing probability model.
In the limit of enough samples, all non-pathological settings will yield the same segmentation results.Metropolis-Hasting steps.
Interleaving sampling-based inference with direct optimization of param-eters can be considered a form of Monte CarloExpectation-Maximization (MCEM; Wei and Tan-ner, 1990).5 Experimental SetupCorpora We evaluate our approach on corporafrom two different domains: transcribed meetingsand written text.For multi-speaker meetings, we use the ICSI cor-pus of meeting transcripts (Janin et al, 2003), whichis becoming a standard for speech segmentation(e.g., Galley et al 2003; Purver et al 2006).
Thisdataset includes transcripts of 75 multi-party meet-ings, of which 25 are annotated for segment bound-aries.For text, we introduce a dataset in which eachdocument is a chapter selected from a medical text-book (Walker et al, 1990).7 The task is to divideeach chapter into the sections indicated by the au-thor.
This dataset contains 227 chapters, with 1136sections (an average of 5.00 per chapter).
Eachchapter contains an average of 140 sentences, giv-ing an average of 28 sentences per segment.Metrics All experiments are evaluated in termsof the commonly-used Pk (Beeferman et al, 1999)and WindowDiff (WD) (Pevzner and Hearst, 2002)scores.
Both metrics pass a window through thedocument, and assess whether the sentences on theedges of the window are properly segmented withrespect to each other.
WindowDiff is stricter inthat it requires that the number of intervening seg-ments between the two sentences be identical inthe hypothesized and the reference segmentations,while Pk only asks whether the two sentences are inthe same segment or not.
Pk and WindowDiff arepenalties, so lower values indicate better segmenta-tions.
We use the evaluation source code providedby Malioutov and Barzilay (2006).System configuration We evaluate our Bayesianapproach both with and without cue phrases.
With-out cue phrases, we use the dynamic programminginference described in section 3.3.
This system isreferred to as BAYESSEG in Table 1.
When adding7The full text of this book is available for free download athttp://onlinebooks.library.upenn.edu.340cue phrases, we use the Metropolis-Hastings modeldescribed in 4.1.
Both basic and linguistically-motivated proposal distributions are evaluated (seeSection 4.2); these are referred to as BAYESSEG-CUE and BAYESSEG-CUE-PROP in the table.For the sampling-based systems, results are av-eraged over five runs.
The initial configuration isobtained from the dynamic programming inference,and then 100,000 sampling iterations are performed.The final segmentation is obtained by annealing thelast 25,000 iterations to a temperature of zero.
Theuse of annealing to obtain a maximum a posteri-ori (MAP) configuration from sampling-based in-ference is common (e.g., Finkel 2005; Goldwater2007).
The total running time of our system is on theorder of three minutes per document.
Due to mem-ory constraints, we divide the textbook dataset intoten parts, and perform inference in each part sepa-rately.
We may achieve better results by performinginference over the entire dataset simultaneously, dueto pooling counts for cue phrases across all docu-ments.Baselines We compare against three com-petitive alternative systems from the literature:U&I (Utiyama and Isahara, 2001); LCSEG (Galleyet al, 2003); MCS (Malioutov and Barzilay, 2006).All three systems are described in the related work(Section 2).
In all cases, we use the publicly avail-able executables provided by the authors.Parameter settings For LCSEG, we use the pa-rameter values specified in the paper (Galley et al,2003).
MCS requires parameter settings to be tunedon a development set.
Our corpora do not includedevelopment sets, so tuning was performed using thelecture transcript corpus described by Malioutov andBarzilay (2006).
Our system does not require pa-rameter tuning; priors are re-estimated as describedin Sections 3.4 and 4.3.
U&I requires no parametertuning, and is used ?out of the box.?
In all exper-iments, we assume that the number of desired seg-ments is provided.Preprocessing Standard preprocessing techniquesare applied to the text for all comparisons.
ThePorter (1980) stemming algorithm is applied togroup equivalent lexical items.
A set of stop-wordsis also removed, using the same list originally em-ployed by several competitive systems (Choi, 2000;Textbook Pk WDU&I .370 .376MCS .368 .382LCSEG .370 .385BAYESSEG .339 .353BAYESSEG-CUE .339 .353BAYESSEG-CUE-PROP .343 .355Meetings Pk WDU&I .297 .347MCS .370 .411LCSEG .309 .322BAYESSEG .264 .319BAYESSEG-CUE .261 .316BAYESSEG-CUE-PROP .258 .312Table 1: Comparison of segmentation algorithms.
Bothmetrics are penalties, so lower scores indicate bet-ter performance.
BAYESSEG is the cohesion-onlyBayesian system with marginalized language mod-els.
BAYESSEG-CUE is the Bayesian system with cuephrases.
BAYESSEG-CUE-PROP adds the linguistically-motivated proposal distribution.Utiyama and Isahara, 2001; Malioutov and Barzilay,2006).6 ResultsTable 1 presents the performance results for threeinstantiations of our Bayesian framework and threecompetitive alternative systems.
As shown in the ta-ble, the Bayesian models achieve the best results onboth metrics for both corpora.
On the medical text-book corpus, the Bayesian systems achieve a rawperformance gain of 2-3% with respect to all base-lines on both metrics.
On the ICSI meeting corpus,the Bayesian systems perform 4-5% better than thebest baseline on the Pk metric, and achieve smallerimprovement on the WindowDiff metric.
The resultson the meeting corpus also compare favorably withthe topic-modeling method of Purver et al (2006),who report a Pk of .289 and a WindowDiff of .329.Another observation from Table 1 is that the con-tribution of cue phrases depends on the dataset.
Cuephrases improve performance on the meeting cor-pus, but not on the textbook corpus.
The effective-ness of cue phrases as a feature depends on whetherthe writer or speaker uses them consistently.
At the341Meetings Textbookokay* 234.4 the 1345.9I 212.6 this 14.3so* 113.4 it 4.1um 91.7 these 4.1and* 67.3 a 2.9yeah 10.5 on 2.1but* 9.4 most 2.0uh 4.8 heart 1.8right 2.4 creating 1.8agenda 1.3 hundred 1.8Table 2: Cue phrases selected by our unsupervisedmodel, sorted by chi-squared.
Boldface indicates that thechi-squared value is significant at the level of p < .01.Asterisks indicate cue phrases that were extracted by thesupervised procedure of Galley et al (2003).same time, the addition of cue phrases prevents theuse of exact inference techniques, which may ex-plain the decline in results for the meetings dataset.To investigate the quality of the cue phrases thatour model extracts, we list its top ten cue phrasesfor each dataset in Table 2.
Cue phrases are rankedby their chi-squared value, which is computed basedon the number of occurrences for each word at thebeginning of a hypothesized segment, as comparedto the expectation.
For cue phrases listed in bold,the chi-squared value is statistically significant atthe level of p < .01, indicating that the frequencywith which the cue phrase appears at the beginningof segments is unlikely to be a chance phenomenon.As shown in the left column of the table, ourmodel has identified several strong cue phrases fromthe meeting dataset which appear to be linguisticallyplausible.
Galley et al (2003) performed a simi-lar chi-squared analysis, but used the true segmentboundaries in the labeled data; this can be thoughtof as a sort of ground truth.
Four of the ten cuephrases identified by our system overlap with theiranalysis; these are indicated with asterisks.
In con-trast to our model?s success at extracting cue phrasesfrom the meeting dataset, only very common wordsare selected for the textbook dataset.
This may helpto explain why cue phrases improve performance formeeting transcripts, but not for the textbook.7 ConclusionsThis paper presents a novel Bayesian approach tounsupervised topic segmentation.
Our algorithm iscapable of incorporating both lexical cohesion andcue phrase features in a principled manner, and out-performs state-of-the-art baselines on text and tran-scribed speech corpora.
We have developed exactand sampling-based inference techniques, both ofwhich search only over the space of segmentationsand marginalize out the associated language mod-els.
Finally, we have shown that our model providesa theoretical framework with connections to infor-mation theory, while also generalizing and justify-ing prior work.
In the future, we hope to explore theuse of similar Bayesian techniques for hierarchicalsegmentation, and to incorporate additional featuressuch as prosody and speaker change information.AcknowledgmentsThe authors acknowledge the support of the Na-tional Science Foundation (CAREER grant IIS-0448168) and the Microsoft Research Faculty Fel-lowship.
Thanks to Aaron Adler, S. R. K. Branavan,Harr Chen, Michael Collins, Randall Davis, DanRoy, David Sontag and the anonymous reviewers forhelpful comments and suggestions.
We also thankMichel Galley, Igor Malioutov, and Masao Utiyamafor making their topic segmentation code publicallyavailable.
Any opinions, findings, and conclusionsor recommendations expressed above are those ofthe authors and do not necessarily reflect the viewsof the NSF.ReferencesDoug Beeferman, Adam Berger, and John D. Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34(1-3):177?210.Jose?
M. Bernardo and Adrian F. M. Smith.
2000.Bayesian Theory.
Wiley.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedings ofNAACL, pages 26?33.342Micha Elsner and Eugene Charniak.
2008.
You Talk-ing to Me?
A Corpus and Algorithm for ConversationDisentanglement.
In Proceedings of ACL.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of ACL, pages 363?370.Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
Proceedings ofACL, pages 562?569.Jean-Luc Gauvain and Chin-Hui Lee.
1994.
Maximuma posteriori estimation for multivariate Gaussian mix-ture observations of Markov chains.
IEEE Transac-tions on Speech and Audio Processing, 2(2):291?298.Dmitriy Genzel and Eugene Charniak.
2002.
Entropyrate constancy in text.
In Proceedings of ACL, pages199?206.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of ACL, pages 744?751.Barbara Grosz and Candace Sidner.
1986.
Attention,intentions, and the structure of discourse.
Computa-tional Linguistics, 12(3):175?204.Barbara Grosz.
1977.
The representation and use of fo-cus in dialogue understanding.
Technical Report 151,Artificial Intelligence Center, SRI International.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman.Marti A. Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of ACL, pages 9?16.Julia Hirschberg and Diane Litman.
1993.
Empiricalstudies on the disambiguation of cue phrases.
Com-putational Linguistics, 19(3):501?530.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, et al 2003.
The ICSI Meeting Corpus.
Acous-tics, Speech, and Signal Processing, 2003.
Proceed-ings.(ICASSP?03).
2003 IEEE International Confer-ence on, 1.Norman L. Johnson, Samuel Kotz, and N. Balakrishnan.1997.
Discrete Multivariate Distributions.
Wiley.Alistair Knott.
1996.
A Data-Driven Methodology forMotivating a Set of Coherence Relations.
Ph.D. thesis,University of Edinburgh.Diane J. Litman and Rebecca J. Passonneau.
1995.
Com-bining multiple knowledge sources for discourse seg-mentation.
In Proceedings of the ACL, pages 108?115.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of ACL, pages 25?32.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press.Thomas P. Minka.
2003.
Estimating a dirichlet distri-bution.
Technical report, Massachusetts Institute ofTechnology.Rebecca Passonneau and Diane Litman.
1993.
Intention-based segmentation: Human reliability and correlationwith linguistic cues.
In Proceedings of ACL, pages148?155.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14:130?137.M.
Purver, T.L.
Griffiths, K.P.
Ko?rding, and J.B. Tenen-baum.
2006.
Unsupervised topic modelling for multi-party spoken discourse.
In Proceedings of ACL, pages17?24.Caroline Sporleder and Mirella Lapata.
2006.
Broadcoverage paragraph segmentation across languagesand domains.
ACM Transactions on Speech and Lan-guage Processing, 3(2):1?35.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InProceedings of ACL, pages 491?498.H.
Kenneth Walker, W. Dallas Hall, and J. Willis Hurst,editors.
1990.
Clinical Methods : The History, Physi-cal, and Laboratory Examinations.
Butterworths.Greg C. G. Wei and Martin A. Tanner.
1990.
Amonte carlo implementation of the EM algorithm andthe poor man?s data augmentation algorithms.
Jour-nal of the American Statistical Association, 85(411),September.343
