Evaluating and Integrating Treebank Parsers on a Biomedical CorpusAndrew B. Clegg and Adrian J. ShepherdSchool of CrystallographyBirkbeck CollegeUniversity of LondonLondon WC1E 7HX, UK{a.clegg,a.shepherd}@mail.cryst.bbk.ac.ukAbstractIt is not clear a priori how well parserstrained on the Penn Treebank will parsesignificantly different corpora withoutretraining.
We carried out a compet-itive evaluation of three leading tree-bank parsers on an annotated corpusfrom the human molecular biology do-main, and on an extract from the PennTreebank for comparison, performing adetailed analysis of the kinds of errorseach parser made, along with a quan-titative comparison of syntax usage be-tween the two corpora.
Our results sug-gest that these tools are becoming some-what over-specialised on their trainingdomain at the expense of portability, butalso indicate that some of the errors en-countered are of doubtful importance forinformation extraction tasks.Furthermore, our inital experimentswith unsupervised parse combinationtechniques showed that integrating theoutput of several parsers can amelioratesome of the performance problems theyencounter on unfamiliar text, providingaccuracy and coverage improvements,and a novel measure of trustworthiness.Supplementary materials are availableat http://textmining.cryst.bbk.ac.uk/acl05/.1 IntroductionThe availability of large-scale syntactically-annotated corpora in general, and the Penn Tree-bank1 (PTB; Marcus et al, 1994) in particular, hasenabled the field of stochastic parsing to advancerapidly over the course of the last 10-15 years.However, the newspaper English which makes upthe bulk of the PTB is only one of many dis-tinct genres of writing in the Anglophone world,and certainly not the only domain where poten-tial natural-language processing (NLP) applica-tions exist that would benefit from robust and re-liable syntactic analysis.
Due to the massive glutof published literature, the biomedical sciences ingeneral, and molecular biology in particular, con-stitute one such domain, and indeed much atten-tion has been focused recently on NLP in this area(Shatkay and Feldman, 2003; Cohen and Hunter,2004).Unfortunately, annotated corpora of a largeenough size to retrain stochastic parsers on do notexist in this domain, and are unlikely to for sometime.
This is partially due to the same differencesof vocabulary and usage that set biomedical En-glish apart from the Wall Street Journal in the firstplace; these differences necessitate the input ofboth biological and linguistic knowledge on bio-logical corpus annotation projects (Kulick et al,2004), and thus require a wider variety of annota-tor skills than general-English projects.
For exam-ple, 5?
(pronounced ?five-prime?)
is an adjective inmolecular biology, but p53 is a noun; amino acid1http://www.cis.upenn.edu/~treebank/is an adjective-noun sequence2 but cadmium chlo-ride is a pair of nouns.
These tagging decisionswould be hard to make correctly without biologi-cal background knowledge, as would the preposi-tional phrase attachment decisions in Figure 1.Although it is intuitively apparent that thereare differences between newspaper English andbiomedical English, and that these differencesare quantifiable enough for biomedical writingto be characterised as a sublanguage of En-glish (Friedman et al, 2002), the performance ofconventionally-trained parsers on data from thisdomain is to a large extent an open question.Nonetheless, papers have begun to appear whichemploy treebank parsers on biomedical text, es-sentially untested (Xiao et al, 2005).
Recently,however, the GENIA project (Kim et al, 2003)and the Mining the Bibliome project (Kulick et al,2004) have begun producing small draft corpora ofbiomedical journal paper abstracts with PTB-stylesyntactic bracketing, as well as named-entity andpart-of-speech (POS) tags.
These are not currentlyon a scale appropriate for retraining parsers (com-pare the ?50,000 words in the GENIA Treebankto the ?1,000,000 in the PTB; but see also Sec-tion 7.2) but can provide a sound basis for empiri-cal performance evaluation and analysis.
A collec-tion of methods for performing such an analysis,along with several interesting results and an inves-tigation into techniques for narrowing the perfor-mance gap, is presented here.1.1 MotivationWe undertook this project with the intention ofaddressing several questions.
Firstly, in order todeploy existing parsing technologies in a bioin-formatics setting, the biomedical NLP commu-nity needs a comprehensive assessment of perfor-mance ?
which parser(s) to choose, what accuracyeach should be expected to achieve etc., along withinformation about the different situations in whicheach parser can be expected to perform well orpoorly.
Secondly, assuming there is a performancedeficit, can any simple steps be taken to mitigateit?
Thirdly, what engineering issues arise from the2According to some annotators at least; others tag aminoas a noun, although one would not speak of *an amino, *someamino or *several aminos.idiosyncracies of biomedical text?The differences discovered in the behaviour ofeach parser, either between domains or betweendifferent software versions on the same domain,will also be of interest to those in the computa-tional linguistics community who are involved inparser design.
These values will give a compara-tive index of the flexibility of each parsing modelon being presented with out-of-domain data, andmay help parser developers to detect signs of over-training or, analogously, ?over-design?
for one nar-row genre of English.
It is hoped that our findingscan assist those better equipped than ourselves inproperly investigating these phenomena, and thatour analysis of the problems encountered can shednew light on the thorny problem of parser evalua-tion.Finally, several questions arise from the use ofmultiple parsers on the same corpus that are ofboth theoretical and practical interest.
Does agree-ment between several parsers indicate that a sen-tence has been parsed correctly, or do they tend tomake the same mistakes?
How best can the outputof an ensemble of parsers be integrated, in orderto boost performance above that of the best sin-gle member?
And what additional information canbe gleaned from comparing the opinions of sev-eral parsers that can help make sense of unfamiliartext?2 Evaluation methodologiesWe initially chose to rate the parsers in our as-sessment by several different means which canbe grouped into two broad classes: constituent-and lineage-based.
While Sampson and Babarczy(2003) showed that there is a limited degree of cor-relation between the per-sentence scores assignedby the two methods, they are independent enoughthat a fuller picture of parser competence can bebuilt up by combining them and thus sidestep-ping the drawbacks of either approach.
However,overall performance scores designed for competi-tively evaluating parsers do not provide much in-sight into the aetiology of errors and anomalies, sowe developed a third approach based on produc-tion rules that enabled us to mine the megabytesof syntactic data for enlightening results more ef-a.
[ This protein ] [ binds the DNA [ by the TATA box [ on its minor groove.
]2 ]1 ]b.
[ This protein ] [ binds the DNA [ by the TATA box ]1 [ at its C-terminal domain.
]2 ]Figure 1: These two sentences are biologically clear but syntactically ambiguous.
Only the knowledgethat the C-terminal domain is part of a protein, whereas the TATA box and minor groove are parts ofDNA, allows a human to interpret them correctly, by attaching the prepositional phrases 1 and 2 at theright level.fectively.
All the Perl scoring routines we wroteare available from our website.2.1 Constituent-based assessmentMost evaluations of parser performance are basedupon three primary measures: labelled constituentprecision and recall, and number of crossingbrackets per sentence.
Calculation of these scoresfor each sentence is straightforward.
Each con-stituent in a candidate parse is treated as a tu-ple ?lbound,LABEL,rbound?, where lbound andrbound are the indices of the first and last wordscovered by the constituent.
Precision is the pro-portion of candidate constituents that are correctand is calculated as follows:P =# true positives# true positives + # false positivesRecall is the proportion of constituents from thegold standard that are in the candidate parse:R =# true positives# true positives + # false negativesThe crossing brackets score is reached by count-ing the number of constituents in the candidateparse that overlap with at least one constituent inthe gold standard, in such a way that one is not asubsequence of the other.Although this scoring system is in wide use, itis not without its drawbacks.
Most obviously, itgives no credit for partial matches, for examplewhen a constituent in one parse covers most ofthe same words as the other but is truncated or ex-tended at one or both ends.
Indeed, one can imag-ine situations where a long constituent is truncatedat one end and extended at the other compared tothe gold standard; this would incur a penalty un-der each of the above metrics even though someor even most of the words in the constituent werecorrectly categorised.
One can of course suggestmodifications for these measures designed to ac-count for particular situations like these, althoughnot without losing some of their elegance.
Thesame is true for label mismatches, where a con-stituent?s boundaries are correct but its category iswrong.More fundamentally, it could be argued that bytaking as it were horizontal slices through the syn-tax tree, these measures lose important informa-tion about the ability of a parser to recreate thegross grammatical structure of a sentence.
Theheight of a given constituent in the tree, and thedetails of its ancestors and descendants, are notdirectly taken into account, and it is surely thecase that these broader phenomena are at least asimportant as the extents of individual constituentsin affecting meaning.
However, constituent-basedmeasures are not without specific advantages too.These include the ease with which they can be bro-ken down into scores per label to give an impres-sion of a parser?s performance on particular kindsof constituent, and the straightforward messagethey deliver about whether a badly-performingparser is tending to over-generate (low precision),under-generate (low recall) or mis-generate (highcrossing brackets).2.2 Lineage-based assessmentIn contrast to this horizontal-slice philosophy,Sampson and Babarczy (2003) advocate a verti-cal view of the syntax tree.
By walking up thetree structure from the immediate parent of a givenword until the top node is reached, and addingeach label encountered to the end of a list, a ?lin-eage?
representing the word?s ancestry can be re-trieved.
Boundary symbols are inserted into thislineage before the highest constituent that beginson the word, and after the highest constituent thatends on the word, if such conditions apply; this al-lows potential ambiguities to be avoided, so thatthe tree as a whole has one and only one corre-sponding set of ?lineage strings?
(see Figure 2).Using dynamic programming, a Levenshteinedit distance can be calculated between eachword?s lineage strings in the candidate parse andthe gold standard, by determining the smallestnumber of symbol insertions, deletions and substi-tutions required to transform one of the strings intothe other.
The leaf-ancestor (LA) metric, a simi-larity score ranging between 0 (total parse failure)and 1 (exact match), is then calculated by takinginto account the lengths of the two lineages:LA = 1?dist(lineage1, lineage2)len(lineage1)+ len(lineage2)The per-word score can then be averaged over asentence or a whole corpus in order to arrive at anoverall performance indicator.
Besides avoidingsome of the limitations of constituent-based eval-uation discussed above, one major advantage ofthis approach is that it can provide a word-by-wordmeasure of parser performance, and thus draw at-tention easily to those regions of a sentence whichhave proved problematic (see Section 6.2 for anexample).
The algorithm can be made more sen-sitive to near-matches between phrasal categoriesby tuning the cost incurred for a substitution be-tween similar labels, e.g.
those for ?singular noun?and ?proper noun?, rather than adhering to the uni-form edit cost dictated by the standard Levenshteinscheme.
In order to avoid over-complicating thisstudy, however, we chose to keep the standardpenalty of 1 for each insertion, deletion or substi-tution.One drawback to leaf-ancestor evaluation is thatalthough it scores each word (sentence, corpus)between 0 and 1, and these scores are presentedhere as percentages for readability, it is mislead-ing to think of them as percentages of correctnessin the same way that one would regard constituentprecision and recall.
Indeed, the very fact thatit results in a single score means that it revealsless at first glance about the broad classes of er-rors that a parser is making than precision, recalland crossing brackets do.
Another possible objec-tion is that since an error high in the tree will af-fect many words, the system implicitly gives mostweight to the correct determination of those fea-tures of a sentence which are furthest from be-ing directly observable.
One might argue, how-ever, that since a high-level attachment error cangrossly perturb the structure of the tree and thusthe interpretation of the sentence, this is a perfectlyvalid approach; it is certainly complementary tothe uniform scoring scheme described in the previ-ous section, where every mistake is weighted iden-tically.2.3 Production-based assessmentIn order to properly characterise the kinds of errorsthat occurred in each parse, and to help elucidatethe differences between multiple corpora and be-tween each parser?s behaviour on each corpus, wedeveloped an additional scoring process based onproduction rules.
A production rule is a syntacticoperation that maps from a parent constituent in asyntax tree to a list of daughter constituents and/orPOS tags, of the general form:LABELp ?
LABEL1 .
.
.LABELnFor example, the rule that maps from the top-most constituent in Figure 2 to its daughters wouldbe S ?
NP VP.
A production is the applicationof a production rule at a particular location in thesentence, and can be expressed as:LABELp(lbound,rbound)?
LABEL1 .
.
.LABELnProduction precision and recall can be calcu-lated as in a normal labelled constituent-based as-sessment, except that a proposed production is atrue positive if and only if there exists a productionin the gold standard with the same parent label andboundaries, and the same daughter labels in thesame order.
(The respective widths of the daughterconstituents, where applicable, are not taken intoaccount, only their labels and order; any errors ofwidth in the daughters are detected when they aretested as parents themselves.
)Furthermore, as an aid to the detection and anal-ysis of systematic errors, we developed a heuristicSNPNNTCF-1NNmRNAVPVBDwasVPVBNexpressedADVPRBuniquelyPPINinNPNNTNNSlymphocytesFigure 2: Skipping the POS tag, the lineage string for uniquely is: [ ADVP ] VP VP S .
The left andright boundary markers record the fact that the ADVP constituent both starts and ends with this word.for finding the closest-matching candidate produc-tions PRODc1 .
.
.PRODcm in a parse, in each casewhere a production PRODg in the gold standard isnot exactly matched in the parse.1.
First, the heuristic looks for productions withcorrect boundaries and parent labels, but in-correct daughters.
The corresponding pro-duction rules are returned.2.
Failing that, it looks for productions withcorrect boundaries and daughters, preservingthe order of the daughters, but with incorrectparent labels.
The corresponding productionrules are returned.3.
Failing that, it looks for productions with cor-rect boundaries but incorrect parent labelsand daughters.
The corresponding produc-tion rules are returned.4.
Failing that, it looks for all extensions andtruncations of the production (boundary mod-ifications such that there is at least one wordfrom PRODg still covered) with correct par-ent and daughter labels and daughter order,keeping only those that are closest in width toPRODg (minimum number of extensions andtruncations).
The meta-rules EXT ALLMATCHand/or TRUNC ALLMATCH as appropriate arereturned.5.
Failing that, it looks for all extensionsand truncations of the production wherethe parent label is correct but the daugh-ters are incorrect, keeping only thosethat are closest in width to PRODg.
Themeta-rules EXT PARENTMATCH and/orTRUNC PARENTMATCH are returned.6.
If no matches are found in any of theseclasses, a null result is returned.Note that in some cases, m production rules ofthe same class may be returned, for example whenthe closest matches in the parse are two produc-tions with the correct parent label, one of which isone word longer than PRODg, and one of whichis one word shorter.
It is also conceivable thatmultiple productions with the same parent or samedaughters could occupy the same location in thesentence without branching, although it seems un-likely that this would occur apart from in patho-logically bad parses.
In any ambiguous cases, noattempt is made to decided which is the ?real?
clos-est match; all m matches are returned, but they aredownweighted so that each counts as 1/m of anerror when error frequencies are calculated.
In nocircumstances are matches from different classesreturned.The design of this procedure reflects our re-quirements for a tool to facilitate the diagnosis andsummarisation of parse errors.
We wanted to beable to answer questions like ?given that parserA has a low recall for NP ?
NN NN productions,what syntactic structures is it generating in theirplace?
Why might this be so?
And what effectmight these errors have on the interpretation of thesentence??
Accordingly, as the heuristic casts thenet further and further to find the closest matchfor a production PRODg, the classes to which itassigns errors become broader and broader.
Anymatch at stages 1?3 is not simply recorded as asubstitution error, but a substitution for a particu-lar incorrect production rule.
However, matches atstages 4 and 5 do not make a distinction betweendifferent magnitudes of truncation and extension,and at stage 5 the information about the daugh-ters of incorrect productions is discarded.
This al-lowed us to identify broad trends in the data evenwhere the correspondences between the gold stan-dard and the parses were weak, yet nonethelessrecover detailed substitution information akin toconfusion matrices where possible.Similar principles guided the decision not toconsider extensions and truncations with differentparent labels as potential loose matches, in order toavoid uninformative matches to productions else-where in the syntax tree.
In practice, the matchesreturned by the heuristic accounted for almost allof the significant systematic errors suffered by theparsers (see Section 6) ?
null matches were in-frequent enough in general that their presence inlarger numbers on certain production rules was it-self useful from an explanatory point of view.2.4 Alternative approachesSeveral other proposed solutions to the evalua-tion problem exist, and it is an ongoing and con-tinually challenging field of research.
Suggestedprotocols based on grammatical or dependencyrelations (Crouch et al, 2002), head projection(Ringger et al, 2004), alternative edit distancemetrics (Roark, 2002) and various other schemeshave been suggested.
Many of these alterna-tive methodologies, however, suffer from one ormore disadvantages, such as specificity to one par-ticular grammatical formalism (e.g.
head-drivenphrase structure grammar) or one class of parser(e.g.
partial parsers), or a requirement for a spe-cific manually-prepared evaluation corpus in anon-treebank format.
In addition, none of themdeliver the richness of information supplied byproduction-based assessment, particularly in com-bination with the other methods outlined above.3 Comparing the corporaThe gold standard data for our experiments wasdrawn from the GENIA Treebank3, a beta-stagecorpus of 200 abstracts drawn randomly fromthe MEDLINE database4 with the search terms?human?, ?blood cell?
and ?transcription factor?.These abstracts have been annotated with POStags, named entity classes and boundaries5, andsyntax trees which broadly follow the conventionsof the PTB.
Some manual editing was requiredto correct annotation errors and remove sentenceswith uncorrectable errors, leaving 1757 sentences(45406 tokens) in the gold standard.
All errorswere reported to the GENIA group.For comparison purposes, we used the standardset-aside test set from the PTB, section 23.
Thisconsists of 56684 words in 2416 sentences.To gain insight into the differences between thetwo corpora, we ran several tests of the grammati-cal composition of each.
For consistency with theparser evaluation results, we stripped the follow-ing punctuation tokens from the corpora beforegathering these statistics: period, comma, semi-colon, colon, and double-quotes (whether theywere expressed as a single double-quotes charac-ter, or pairs of opening or closing single-quotes).We also removed any super-syntactic informationsuch as grammatical function suffixes, pruned anytree branches that did not contain textual termi-nals (e.g.
traces), and deleted any duplicatedconstituents ?
that is, constituents with only onedaughter that has the same label.3.1 Sentence length and complexityHaving performed these pre-processing steps, wecounted the distributions of sentence lengths (in3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/topics/Corpus/GTB.html4http://www.pubmed.org/5The named entity annotations are supplied in a separatefile which was discarded.words) and sentence complexities, using the num-ber of constituents, not counting POS tags, asa simple measure of complexity ?
although ofcourse one can imagine various other ways togauge complexity (mean tree depth, maximumtree depth, constituents per word etc.).
The resultsare shown in Figure 3, and reveal an unexpectedlevel of correlation.
Apart from a few sparse in-stances at the right-hand tails of the two GENIAdistributions, and a single-constituent spike on thePTB complexity distribution (due to one-phraseheadings like STOCK REPORT.
), the two corporahave broadly similar distributions of word countand constituent count.
The PTB has slightly moremass on the short end of the length scale, butGENIA does not have a corresponding number oflonger sentences.
This ran contrary to our initialintuition that newspaper English would tend to becomposed predominantly of shorter and simplersentences than biological English.3.2 Constituent and production rule usageNext, we counted the frequency with which eachconstituent label appears in each corpus.
The re-sults are shown in Figure 4.
The distributions arereasonably similar between the two corpora, withthe most obvious difference being that GENIAuses noun phrases more often, by just over six per-centage points.
This may reflect the fact that muchof the text in GENIA describes interactions be-tween multiple biological entities at the molecularand cellular levels; conjunction phrases are threetimes as frequent in GENIA too, although this isnot obvious from the chart as the numbers are solow in each corpus.One surprising result is revealed by lookingat Table 1 which shows production rule usageacross the corpora.
Although GENIA uses slightlymore productions per sentence on average, it usesmarginally fewer distinct production rules per sen-tence, and considerably fewer overall ?
62% of thenumber of rules used in the PTB, despite being73% of the size in sentences.
These figures, alongwith the significantly different rankings and fre-quencies of the actual rules themselves (Table 2),demonstrate that there are important syntactic dif-ferences between the corpora, despite the simi-larities in length, complexity and constituent us-age.
Such differences are invisible to conventionalconstituent-based analysis.The comparative lack of syntactic diversity inGENIA may seem counter-intuitive, since biolog-ical language seems at first glance dense and dif-ficult.
However, it must be remembered that thetext in GENIA consists only of abstracts, whichare tailored to the purpose of communicating afew salient points in a short passage, and tendto be composed in a somewhat formulaic man-ner.
They are written in a very restricted regis-ter, compared to the range of registers that maybe present in one issue of a newspaper ?
news ar-ticles, lifestyle features, opinion pieces, financialreports and letters will be delivered in very dif-ferent voices.
Also, some of the apparent com-plexity of biomedical texts is illusory, stemmingfrom the unfamiliar vocabulary, and furthermore,a distinction must be made between syntactic andsemantic complexity.
Consider a phrase like iron-sulphur cluster assembly transcription factor, thename of a family of DNA-binding proteins, whichis a semantically-complex concept expressed in asyntactically-simple form ?
essentially just a se-ries of nouns.4 Evaluating the parsersThe parsers chosen for this evaluation were thosedescribed originally in Collins (1999), Charniak(1999) and Bikel (2002).
These were selected be-cause they are up-to-date (having last been up-dated in 2002, 2003 and 2004 respectively), highlyregarded by the computational linguistics com-munity, and importantly, free to use and modifyfor academic research.
Since part of our moti-vation was to detect signs of over-specialisationon the PTB, we assessed the current (0.9.9) andprevious (0.9.8) versions of the Bikel parser in-dividually.
The current version was invokedwith the new bikel.properties settings file,which enables parameter pruning (Bikel, 2004),whereas the previous version used the originalcollins.properties settings which were de-signed to emulate the Collins parser model 2 (seebelow).
The same approach was attempted withthe Charniak parser, but the latest version (re-leased February 2005) suffered from fatal errors                             fffiflffi!
"#$"#%"%&'#$                          fffiflffiffi!ffi!
"#Figure 3: Sentence length and complexity distributions, GENIA vs. PTB.    ff fifl ffi!fi"ffi# $ffi%&ffi'()Figure 4: Constituent usages, GENIA vs. PTB.GENIA PTBNum.
productions used in corpus 78831 (44.87/sent.)
96694 (40.02/sent.)Num.
distinct production rules 1364 (5.47/sentence) 2184 (5.55/sent.
)Table 1: Production and production rule usage in the two corpora.on GENIA which could not be diagnosed in timefor publication.
Earlier versions of the Collinsparser are not available; however, the distributioncomes with three language models of increasingsophistication which were treated initially as dis-tinct parsers.Tweaking of parser options was kept to a min-imum, aside from trivial changes to allow forunexpectedly long words, long or complex sen-tences (e.g.
default memory/time limits), and dif-fering standards of tokenisation and punctuation,although a considerable degree of pre- and post-processing by Perl scripts was also necessary tobring these into line.
More detailed tuning wouldhave massively increased the number of variablesunder consideration, given the number of compile-time constants and run-time parameters availableto the programs; furthermore, it is probably safeto assume that each author distributes his softwarewith an optimal or near-optimal configuration, atleast for in-domain data.4.1 Part-of-speech taggingThe Collins parser requires pre-tagged input, andalthough the Bikel parser can take untagged in-put, the author recommends the use of a dedicatedPOS tagger.
For this reason, we pre-processedGENIA with MedPost (Smith et al, 2004), a spe-cialised biomedical POS tagger that was devel-oped and trained on MEDLINE abstracts.
Thesupplied gold-standard POS tags were discardedas using them would not provide a realistic ap-Top-25 production rules in GENIA (left) and PTB (right)Freq.
Rule Rank Freq.
Rule6.36 PP ?
IN NP 1 4.80 PP ?
IN NP3.32 NP ?
NN 2 2.95 S ?
NP VP3.13 NP ?
NP PP 3 2.26 NP ?
NP PP2.17 S ?
NP VP 4 2.15 TOP ?
S2.03 TOP ?
S 5 1.86 NP ?
DT NN1.23 NP ?
DT NN 6 1.43 S ?
VP0.89 NP ?
NN NN 7 1.08 NP ?
PRP0.85 NP ?
NP CC NP 8 0.92 ADVP ?
RB0.82 S ?
VP 9 0.91 NP ?
NNP0.76 VP ?
VBN PP 10 0.83 NP ?
NNS0.74 ADVP ?
RB 11 0.81 VP ?
TO VP0.70 NP ?
DT JJ NN 12 0.78 NP ?
NN0.66 NP ?
NNS 13 0.74 NP ?
NNP NNP0.58 NP ?
JJ NNS 14 0.63 SBAR ?
IN S0.53 NP ?
JJ NN 15 0.62 NP ?
DT JJ NN0.51 SBAR ?
IN S 16 0.60 NP ?
NP NP0.51 PP ?
TO NP 17 0.57 SBAR ?
S0.49 NP ?
DT NN NN 18 0.50 VP ?
VB NP0.48 NP ?
NP PRN 19 0.48 NP ?
NP SBAR0.48 ADJP ?
JJ 20 0.47 VP ?
MD VP0.47 NP ?
NP PP PP 21 0.46 NP ?
JJ NNS0.47 PRN ?
( NP ) 22 0.41 SBAR ?
WHNP S0.45 NP ?
NN NNS 23 0.40 PP ?
TO NP0.44 NP ?
NP VP 24 0.33 VP ?
VBD SBAR0.40 VP ?
VBD VP 25 0.32 NP ?
NP CC NPTable 2: The most common production rules in the two corpora, in order, with the frequency of occur-rence of each.
Notice that several rules are much more common in one corpus than the other, such as VP?
TO VP, which is the 11th most common rule in the PTB but doesn?t make it into GENIA?s list.proximation of the kinds of scenario where parsingsoftware would be deployed on unseen text.
Med-Post was found to tag GENIA with 93% accuracy.Likewise, although the Charniak parser assignsPOS tags itself and was developed and trainedwithout exposure to a biological vocabulary, it wasallowed to compete on its own terms against theother two parsers each in conjunction with Med-Post.
Although this may seem slightly unfair, to dootherwise would not reflect real-life usage scenar-ios.
The parser tagged GENIA with an accuracyof 85%.The PTB extract used was included pre-processed with the MXPOST tagger (Ratnaparkhi,1996) as part of the Collins parser distribution; thesupplied tagging scored 97% accuracy.
The Char-niak parser re-tagged this corpus with 96% accu-racy.4.2 Initial performance comparisonHaving parsed each corpus with each parser, theoutput was post-processed into a standardisedXML format.
The same pruning operations per-formed on the original corpora (see Section 3)were repeated where necessary.
TOP nodes (S1nodes in the case of the Charniak parser) wereremoved from all files as these remain constantacross every sentence.
NAC and NX labels werereplaced by NP labels in the parses of GENIA asthe GENIA annotators use NP labels where thesewould occur.
We then performed lineage- andRaw scores on GENIA (1757 sentences)Parser LA score Precision Recall F-measure % perfect % failureBikel 0.9.8 91.12 81.33 77.43 79.33 14.29 0.06Bikel 0.9.9 65.30 81.68 55.75 66.27 11.21 25.04Charniak 89.91 77.12 76.05 76.58 12.81 0.00Collins 1 88.74 79.06 73.87 76.38 13.15 0.68Collins 2 87.85 81.30 74.49 77.75 14.00 1.42Collins 3 86.33 81.57 73.28 77.20 14.00 2.28Raw scores on PTB (2416 sentences)Parser LA score Precision Recall F-measure % perfect % failureBikel 0.9.8 94.45 88.09 88.13 88.11 33.44 0.04Bikel 0.9.9 80.11 88.03 74.61 80.76 29.80 12.75Charniak 94.36 88.09 88.28 88.18 35.06 0.00Collins 1 94.17 86.80 86.70 86.75 31.13 0.00Collins 2 94.36 87.29 87.20 87.24 33.49 0.04Collins 3 94.25 87.28 87.10 87.19 33.11 0.08Table 3: Initial performance comparison.constituent-based scoring runs using our own Perlscripts.The results of this experiment are summarisedin Table 3, showing both the scores on bothGENIA and the PTB.
The LA score given is themean of the leaf-ancestor scores for all the wordsin the corpus, and the precision and recall scoresare taken over the entire set of constituents in thecorpus.
Initially, these measures were calculatedper sentence, and then averaged across each cor-pus, but the presence of pathologically short sen-tences such as Energy.
gives an unrepresenta-tive boost to per-sentence averages.
(Interestingly,many published papers do not make clear whetherthe results they present are per-sentence averagesor corpus-wide scores.
)?Mean X?
is simply the average number ofcrossing brackets per sentence.
?F-measure?
(vanRijsbergen, 1979) is the harmonic mean of preci-sion and recall; it is a balanced score that penalisesalgorithms which favour one to the detriment ofthe other, and is calculated as follows:F =2?P?RP+R4.3 Parse failuresSince most of the parsers suffered from a consid-erable number of parse failures in GENIA ?
sen-tences where no parse could be obtained ?
Table 4shows recalculated scores based on evaluation ofsuccessfully-parsed sentences only.
Conflating theperformance drops caused by poorly parsed sen-tences with those caused by total failures gives aninaccurate picture of parser behaviour.
In order todetermine if there was any pattern to these fail-ures, we plotted the number of parse failures foreach parser against sentence length and sentencecomplexity (see Figure 5).
These charts revealedsome interesting trends.There is a known problem with the Collins mod-els 2 and 3 failing on two sentences in the PTBsection 23 due to complexity, but this problemis exacerbated in GENIA, with even the simplermodel 1 failing on a number of sentences, one ofwhich was only 24 words long plus punctuation.Overall, however, the failures do tend to clusteraround the right-hand tails of the sentence lengthand constituent count distributions.
Discountingsuch sentences, the three models do show a con-sistent monotonic increase in precision, recall andLA score from the simplest to the most complex,accompanied by a decrease in the number of cross-ing brackets per sentence.
Interestingly, theseintervals are much more pronounced on GENIAthan on the PTB, where the performance seems tolevel off between models 2 and 3.
Difficult sen-tences aside, then, it appears that the advanced fea-                                    fffiffflffi!"#$%&$%'$'()%&                                fffiffflffiffi!ffi!
"#Figure 5: Parse failures on GENIA vs. sentence length and complexity for each parser.Scores on GENIA, successfully-parsed sentences onlyParser LA score Precision Recall F-measure Mean X # parsedBikel 0.9.8 91.15 81.33 77.46 79.35 2.06 1756Bikel 0.9.9 91.17 81.68 77.04 79.29 1.89 1317Charniak 89.91 77.12 76.05 76.58 2.42 1757Collins 1 90.53 79.06 75.35 77.16 2.29 1745Collins 2 91.21 81.30 77.24 79.22 2.01 1732Collins 3 91.32 81.57 77.42 79.44 1.95 1717Scores on PTB, successfully-parsed sentences onlyParser LA score Precision Recall F-measure Mean X # parsedBikel 0.9.8 94.53 88.09 88.20 88.15 1.07 2415Bikel 0.9.9 94.52 88.03 88.07 88.05 1.04 2108Charniak 94.36 88.09 88.28 88.18 1.08 2416Collins 1 94.17 86.80 86.70 86.75 1.23 2416Collins 2 94.45 87.29 87.28 87.28 1.19 2415Collins 3 94.44 87.28 87.27 87.28 1.18 2414Table 4: Performance scores, discounting all parse failures.
Scores for the Charniak parser, and Collinsmodel 1 on the PTB, are shown again for comparison, although they did not fail on any sentences.tures of models 2 and 3 are actually more valuableon this unfamiliar corpus than on the original de-velopment domain ?
provided that they do not tripthe parser up completely.While Bikel 0.9.8?s failures are relatively fewand tend to occur more often in longer and morecomplex sentences, like those of the Collins mod-els, the distributions in Figure 5 for Bikel 0.9.9follow the shapes of the distributions remarkablyaccurately.
In other words, the length or complex-ity of a sentence does not seem to be a major in-fluence on the ability of Bikel 0.9.9 to parse it.Undoubtedly, there is something more subtle inthe composition of these sentences that confusesBikel?s updated algorithm, although we could notdiscern any pattern by eye.
Perhaps this problemcould be diagnosed by monitoring the parser ina Java debugger or modifying it to produce moreverbose output, but such an examination is beyondthe scope of this work.Although version 0.9.9 fails on far fewer sen-tences in the PTB than in GENIA, it still suffersfrom two orders of magnitude more failures thanany other parser on the same corpus.
These re-sults suggest that the author?s claim that parame-ter pruning results in ?no loss of accuracy?
(Bikel,2004) can only be taken seriously when the test sethas been cleaned of all unparseable sentences; thisimpression is reinforced by the fact that the preci-sion and recall scores reported by the author agreequite closely with our results on the PTB once theparse failures have been removed.5 Combining the parsersGiven the poorer results of these parsers onGENIA than on the PTB, and the comparativelack of annotated data in this domain, it is im-portant to consider ways in which performancecan be enhanced without recourse to supervisedtraining methods.
Various experimental tech-niques exist for reducing or eliminating the needfor labelled training data, particularly in the pres-ence of several diverse parsers (or more generally,classifiers).
These include active learning (Os-borne and Baldridge, 2004), bagging and boost-ing (Henderson, 1999) and co-training (Steedmanet al, 2003).
In addition to these ?knowledge-poor?
techniques, one can easily imagine domain-specific ?knowledge-rich?
techniques that employexisting biological data sources and NLP meth-ods in order to select, modify, or constrain parses(see Section 7.2).
For this preliminary investi-gation, however, we concentrated on knowledge-poor methods originating in work on parsing thePTB which could exploit the availability of multi-ple parsers whilst requiring no time-consuming re-training processes or integration with external re-sources.
Perl implementations of the algorithmsdiscussed below can be downloaded from ourwebsite.5.1 Fallback cascadesIn the Collins parser instructions, the author sug-gests stacking the three models in decreasing orderof sophistication (3 ?
2 ?
1), and for each sen-tence, falling back to the next less sophisticatedmodel each time a more sophisticated one fails toobtain a parse.
The principle behind this is that themore complex a model is, the more often it willfail, but the better the results will be when it doesreturn a parse.
We implemented this system forthe Collins models, and also for the Bikel parser,starting with version 0.9.9 and falling back to 0.9.8on failure.
Since the Charniak parser did not sufferany failures, we added it to each of these cascadesas a last-resort level, to fill any remaining gaps.As expected, the results for each cascade(Table 5) were comparable to their componentparsers?
scores on successfully-parsed sentences(Table 4), except with 100% coverage of the cor-pus.
In each of the following parser integrationmethods, we used these fallback cascades to rep-resent the Bikel and Collins parsers, rather thanany of their individual parser models.
The Bikelcascade was used as a baseline against which totest the results of each method for statistical sig-nificance, using a two-tailed dependent t-test overpaired scores.5.2 Constituent votingHenderson (1999) reports good results when us-ing a simple parse integration method called con-stituent voting, where a hybrid parse is producedby taking votes from all the parsers in an ensem-ble.
Essentially, all the constituents proposed bythe parsers are pooled, and each one is added tothe hybrid parse if more than half of the parsersin the ensemble agree on it.
The assumption be-hind this concept is that the mistakes made by theparsers are reasonably independently distributed ?if different kinds of errors beset the parsers, andat different times, then a majority vote will tend toconverge on the correct set of constituents.We implemented a three-way majority vote en-semble between the Collins and Bikel cascadesand the Charniak parser; the results are shown inTable 6.
The most notable gain was in precision,as one would hope from an algorithm designedto screen out minority parsing decisions, but thescores also illustrate an interesting phenomenon.Although the ensemble took the lead on all theconstituent-based performance indicators, it per-formed poorly on LA score.
This demonstratesan important point about parser scoring metrics ?that an algorithm designed to boost one measure ofquality can do so without necessarily raising per-formance according to a different yardstick.Part of the reason for this discrepancy may bea quirk of the constituent voting algorithm thatconstituent-based precision and recall scores glossover.
The trees it produces are not guaranteedto be well-formed under the grammars of any ofthe members of the ensemble; if, for example,the parsers cannot reach consensus about the exactboundaries of a verb phrase, a sentence without aVP constituent will be produced, leading to someunusual attachments at a higher level.
Unlike theconstituent-based approach, LA scoring tends tofavour parses that are accurate at the upper lev-els of the tree, so an increase in precision and re-call without a corresponding increase in LA scorewould be consistent with this kind of oddity.5.3 Parse selectionAn alternative approach to integrating the out-puts of a parser ensemble is whole parse selectionon a per-sentence basis, which has the potentialadded advantage over hybridisation methods likeconstituent voting that the gaps in trees describedabove cannot occur.
The most obvious way toguess the best candidate parse for a sentence is toassume that the true parse lies close to the centroidof all the candidates in parse space, and then, usingsome similarity or distance measure between thecandidates, pick the candidate that is most similarto (or least distant from) all the other parses.We implemented three parse switchers, twobased on constituent overlap, and one based on lin-eage similarity between pairs of parses.
Similarityand distance switching (Henderson, 1999) take theEnsemble scores on GENIA, all sentences parsed successfullyEnsemble LA score Precision Recall F-measure Mean X % perfectCollins-Charniak fallback 90.74 80.51 76.44 78.42 2.16 14.00Bikel-Charniak fallback 91.08 81.31 76.96 79.08 2.05 14.11Table 5: Ensemble scores on GENIA for Collins(3, 2, 1)?Charniak and Bikel(0.9.9, 0.9.8)?Charniakfallback cascades.Ensemble scores on GENIA, all sentences parsed successfullyAlgorithm LA score Precision Recall F-measure Mean X % perfectMajority vote ensemble 90.21 83.41 77.50 80.35 1.71 14.68Table 6: Ensemble scores on GENIA for parse combination by majority constituent voting.number of constituents that each parse has in com-mon, and the number that are proposed by eitherone but not both parsers, as measures of similar-ity and distance respectively.
Levenshtein switch-ing, a novel method, uses the sentence-mean LAscores between parses as the similarity measure.In all methods, the parse with the maximum totalpairwise similarity (minimum total pairwise dis-tance) to the set of rival parses for a sentence ischosen.
In no case were POS tags taken into ac-count when calculating similarity, as they wouldhave made the Collins and Bikel parsers artificiallysimilar.The results of these experiments are shown inTable 7.
All three methods achieved compara-ble improvements overall, with the similarity anddistance switching routines favouring recall andprecision respectively (both differences significantat p < 0.0001).
Note however that the winningLA score for Levenshtein switching is not a sta-tistically significant improvement over the otherswitching methods.6 Error analysisAll of the parser integration methods discussedabove make the assumption that the parsersin an ensemble will suffer from independently-distributed errors, to a greater or lesser extent.Simple fallback cascades rely on their individualmembers failing on different sentences, but themore sophisticated methods in Section 5.2 andSection 5.3 are all ultimately based on the princi-ple that agreement between parsers indicates con-vergence on the true parse.
Although the perfor-mance gains we achieved with such methods arestatistically significant, they are nonetheless some-what unimpressive compared to the 30% reductionof recall errors and 6% reduction of precision er-rors reported for the best ensemble techniques inHenderson and Brill (1999) on the PTB.This led us to suspect that the parsers in the en-sembles were making similar kinds of errors onGENIA, perhaps not across the board, but cer-tainly often enough that consensus methods pickincorrect constituents, and centroid methods con-verge on incorrect parses, with a significant fre-quency.
To investigate this phenomenon, andmore generally to tease apart the reasons for eachparser?s performance drop on GENIA, we mea-sured the precision and recall for each parser oneach production rule over GENIA and the PTB.We then gathered the 25 most common productionrules in GENIA and compared the scores achievedby each parser on each rule to the same rule inPTB, thus drawing attention to parser-specific is-sues and more widespread systematic errors.
Wealso collected closest-match data on each missedproduction in GENIA, for each parser, and calcu-lated substitution frequencies for each productionrule.
This enabled us to identify both the sourcesof performance problems, and to a certain extenttheir causes and connotations.
These data tableshave been omitted for space reasons, since the dis-cussion below covers the important lessons learntfrom them, but they are available as supplemen-tary materials on our website.Ensemble scores on GENIA, all sentences parsed successfullyAlgorithm LA score Precision Recall F-measure Mean X % perfectSimilarity switching 91.34 81.73 78.01 79.83 1.97 14.85Distance switching 91.35 82.10 77.72 79.85 1.92 15.08Levenshtein switching 91.39 81.83 77.51 79.61 1.95 14.74Table 7: Ensemble scores on GENIA for parse selection by three centroid-distance algorithms6.1 Bikel parser errorsDespite the similar overall LA score and F-measure for the two versions on parseable sen-tences only, there are signs that the differencesbetween them run deeper than failure rates.
Thenewer version?s higher precision and lower cross-ing brackets per sentences, along with lower re-call, indicates that it is generating slightly moreconservatively than the older version, on GENIAat least; these scores are much closer on the PTB.Also, the production rule scores show one unex-pected phenomenon ?
the older version is actu-ally considerably better at labelling noun phrasesof the form ( NP ) as parenthetical expressionsin GENIA (F = 81.07) than in PTB (F = 61.29),as are the Collins and Charniak parsers, while thenewer version is much worse at this task in GENIA(F = 42.36).
On closer inspection, however, 84%of the occurrences of PRN ?
( NP ) mislabelledby the newer version are instead marked as PRN?
( NN ) productions of the same width ?
inother words, an intermediate NP constituent cov-ering just a single noun has in these cases beenremoved, and the noun ?promoted?
to a directdaughter of the PRN constituent.
Although thisdemonstrates a difference in the modelling of nounphrases between the two versions, it is unlikelythat such a difference would alter the meaning ofa sentence.
Furthermore, it must be noted that PRN?
( NP ) is much more common in GENIAthan in PTB, so the improvements achieved by theother parsers may be partially accidental; even ifthey simply assumed that every phrase in paren-theses is a noun phrase, they would do better onthis production in GENIA as a result.6.2 Charniak parser errorsThe Charniak parser goes from state-of-the-art onPTB to comparatively poor on GENIA.
It rankslowest in both LA score and F-measure when onlysuccessfully parsed sentences are taken into ac-count, and still only achieves mediocre perfor-mance when the other parsers?
scores cover failedsentences too, despite not failing on any sentencesitself.
This discrepancy can be explained by a lackof biomedical vocabulary available to its built-inPOS tagger.
Although it tags GENIA with an ac-curacy of 85% across all word classes, it achievesonly 63% on the NN (singular/mass noun) class.This is the most numerous single class in GENIA,and that which many domain-specific single-wordterms and components of multi-word phrases be-long to.The knock-on syntactic effects of this disabilitycan be traced in the leaf-ancestor metrics, wherethe parser scores an impressive mean of 91.50 forcorrectly-tagged words, compared to just 80.71for incorrectly-tagged words.
A similar effect canbe seen in the statistics for productions with NNtags on the right hand side.
NP ?
NN and NP?
NN NN are identified with respective recallsof only 33% and 19% in GENIA, for example,as opposed to 90% and 82% in the PTB.
Morethan 40% of mislabelled NP ?
NN productionsin GENIA were identified instead as NP ?
NNP(proper noun) or NP ?
NNS (plural noun) pro-ductions by the parser, and the implications ofthese mistakes for information extraction tasks donot seem great, especially since the majority ofsingle-word noun phrases of particular interest inthis domain are likely to be genes, proteins etc.that can be tagged independently by a dedicatednamed-entity recognizer.
The story is differentfor mislabelled NP ?
NN NN productions, where29% are mistaken for NP ?
JJ NN productions,a substitution that one can imagine causing greatersemantic confusion.
On the other hand, the Char-niak parser goes from being the worst at identify-ing ADVP ?
RB productions (single-word adverbphrases) on the PTB (F = 87.68) to being the bestat this task on GENIA (F = 87.06).Accuracy issues notwithstanding, Charniak?s isstill the most robust of all the parsers, failingon none of the supplied sentences in either cor-pus.
This may reflect a strategy of ?making-do?when an exact parse cannot be derived; it de-ployed more general-purposes FRAG (fragment)and X (unknown/unparseable) constituents com-bined than any other parser, and even a handfulof INTJ (interjection) phrases that no other parserused in GENIA.
Of course, such productions arenot always correct ?
there are actually no inter-jections in GENIA ?
but from an information ex-traction point of view, a rough parse may be betterthan no parse at all, especially if the inclusion ofsuch inexact labels can be reflected in a reducedlevel of confidence or trustworthiness for the sen-tence.6.3 Collins parser errorsWe noted in Section 4.2 that the Collins mod-els achieved successively better performance onGENIA once parse failures were discounted.
Con-sidering individual production rules, however, thetrends are not so clear-cut.
There are rules that dofollow this overall pattern, such as S ?
NP VP,which model 3 actually assigns more effectivelyon GENIA (F = 88.98) than it does on the PTB(F = 88.79).
However, there are several commonproductions where model 3?s accuracy degradesmore than model 2?s, most of which begin with NP?
....
Most of these are found in the PTB moreeffectively by model 3 than model 2, which sug-gests over-fitting; these specific increases in per-formance have apparently come at the expense ofportability.
Note, however, the caveat regardingnoun phrases below.6.4 Common trendsIn addition to these parser-specific observations,there are various phenomena that are common toall or most of the parsers.
Due to slight differ-ences in the annotation of co-ordinated structuresbetween the GENIA and PTB guidelines, the cor-rect generation of noun-phrase conjunctions (NP?
NP CC NP) proved much harder on GENIA,with all parsers having problems with identifica-tion of the boundaries of the conjunction in thetext, and often with correct labelling of the con-stituents involved too.
Cases where each NP isa single word were handled relatively well, withthe essentially equivalent NN CC NN constructionoften being proposed instead, but more complexcases caused widespread difficulty.More surprisingly, the labelling of single-wordnoun and adjective phrases (NP ?
NN and AJDP?
JJ), both of which are significantly more fre-quent in GENIA, seemed challenging across theboard.
The most commonly-occurring error in-volved subsumption by a wider constituent withthe same label, apart from the errors of vocabularyfor the Charniak parser as described above.
How-ever, for correctly-tagged adjectives and nouns,there are many situations where this will not makeany difference to the sense of a sentence.
For ex-ample, in a production like NP ?
DT JJ NN, theadjective still has its modificatory effect on thenoun without needing to be placed within an ADJPphrase of its own, and the noun is entirely capableof acting as the head of the phrase without beingnested within an NP sub-phrase.Similar effects occurred frequently with longerphrases containing nouns, such as NP ?
DT NNor NP ?
NN NN, where the most common errorswere also subsumptions by wider noun phrases.Although the GENIA annotators warn that ?whennoun phrase consists with sequence of nouns [sic],the internal structure is not neccessarily shown,?6which must account for some of the noun handlingproblems, such subsumptions suggest that the op-posite may be occurring too ?
that there are caseswhere the parsers are failing to generate internalstructure within noun phrases.Prepositions were involved in many of the prob-lematic cases, both on the left-hand and right-handsides of productions, and understandably so.
Thedisambiguation of prepositional attachment is acontinuing problem in parser design, and meth-ods that take into account lexical dependencies be-tween head words will be less effective when thewords in question are out-of-domain and thus un-6http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/manual-for-bracketing.htmlseen in training.
The production NP ?
NP PPPP is a good example.
The most common errorfor all parsers was to produce NP ?
NP PP at thesame span of words in the sentence, indicating thatone of the prepositional phrases is frequently at-tached at the wrong level.
The second most com-mon error was to substitute a shorter NP, suggest-ing that one or both of the PPs were excluded.Such errors are potentially of more serious seman-tic importance than differences of opinion abouthow to mark up the internal structure of noun-onlyphrases.7 DiscussionAlthough the performance gains achieved by ourparser integration methods are statistically signif-icant, and illustrative of some important pointsabout parser behaviour and syntactic evaluationmethodologies, it is doubtful that the results aregood enough to justify deploying these techniqueson large amounts of text, at least in their currentform.
The small increases in accuracy are prob-ably outweighed by the additional computationalcosts.
The fallback cascades provided the sameprotection from parse failure, with better perfor-mance than the widest-coverage parser alone, andin a production system most sentences would onlyneed to be parsed by the first parser in the cascade.However, the parser integration idea as a wholeis not without its merits; we determined that an or-acle picking the best sentences on GENIA wouldachieve an LA score of 93.56, so there is still roomfor improvement if the algorithms can be madesmarter.
Although our analysis of the parsers?mistakes on GENIA indicated that the ideal ofindependently-distributed errors which underpinsthese integration methods does not hold true, thevery fact that we can analyse their behaviour pat-terns in such detail suggests that a sufficientlywell-designed ensemble could in principle learnthe circumstances under which each parser couldbe trusted on a given corpus.Furthermore, there are additional ways in whichan ensemble might assist with practical NLP is-sues.
While analysing the data from the parse se-lection algorithms, we discovered that the centroiddistances for the winning parses, scaled by sen-tence size where necessary, correlate fairly well(|r| ?
0.5) with those parses?
true LA scores and F-measures (see Section 7.2).
We developed a sim-ilar measurement for constituent voting based onthe level of consensus among the ensemble mem-bers ?
the number of constituents winning a ma-jority vote divided by the number of distinct can-didate constituents ?
which showed a comparabledegree of correlation.
This provides an answerto our initial question about the extent to whichparser agreement signals parse correctness.
Pre-sumably the major limiting factor on these corre-lations is the presence of widespread systematicerrors like those described in Section 6.4.7.1 Engineering issuesAs noted previously, the Charniak parser takes rawtext and performs tokenisation and POS tagginginternally.
While this may seem like an advanta-geous convenience, in practice it is the source ofconsiderable extra work, besides being the causeof avoidable parse errors.
The tokenisation stan-dards encoded by Charniak did not match those as-sumed by either the GENIA corpus, or indeed thePTB extract, although problems were much morewidespread in the GENIA.
Words containing em-bedded punctuation were frequently split into mul-tiple tokens, so these word-internal symbols hadto be converted into textual placeholders beforeparsing and converted back afterwards.
The some-what idiosyncratic conventions of the GENIA cor-pus did not help (differentiation/activation beingtagged as one token for example) but the fact thatsimilar issues occurred on the newspaper corpus(e.g.
with US$ or 81-year-old ) suggests that mak-ing assumptions about the ?correct?
way to to-kenise text is a bad policy in any domain.Even when working on in-domain data, it seemslike a bad design decision to assume the parser willbe able to match the performance of a state-of-the-art POS tagger on unseen text.
The Bikel parsercan operate in either mode, which is a much moreflexible policy.
In all fairness, however, it wouldprobably be fairly trivial for an interested C++ de-veloper to bypass the Charniak parser?s tokeniserand tagger and recompile it.A different kind of engineering issue is that ofcomputation time.
Parsing is a slow process in anycase, and ensemble methods compound this prob-lem.
However, parsing is a canonically easy taskto perform in parallel, since (at this level of under-standing at least) each sentence has no dependen-cies on the previous, so even the parse integrationstep can be split across multiple pipelines.
We in-tend to run pilot studies on the scalability of paral-lel distributed parsing techniques, both on an IBMBlade Center cluster running Beowulf Linux, anda heterogeneous network of Windows PCs in theirspare hours, in order to determine the feasibilityand comparative attractiveness of each approach.7.2 Future workWe mentioned above that there is a significant cor-relation between the distance of a winning parsefrom the centroid and its accuracy compared tothe gold standard.
In an IE or other text min-ing scenario, one could use these values as esti-mators of trustworthiness for each parse ?
empir-ical measures indicating how reliable to considerit.
We would like to explore this idea further, as itcan provide an extra level of understanding whichis missing from many IE techniques, and couldbe easily employed in the ranking of extracted?facts?
or the resolution of contradictions.
SinceLA scores can be calculated per word or per anyarbitrary region of the sentence, the potential evenfor rating the trustworthiness of different clausesor relationships individually cannot be ignored.We have been experimenting with training aneural network to pick the best parse for a sentencebased only on the pairwise Levenshtein distancesbetween the candidate parses, in the hope that itcan learn what decision to make based on patternsof agreement between the parsers, rather than justpicking the parse which is most similar to all theothers as the current methods do.
So far, however,it has been unable to exceed the performance ofthe unsupervised methods, which suggests that ad-ditional features may need to be considered.A complementary approach to parse integra-tion would be to merge the PTB with an anno-tated biomedical corpus and retrain a parser fromscratch.
This proposition appears more attractivein the light of the production rule frequency differ-ences between GENIA and the PTB, and will be-come more effective as the GENIA treebank growsand the Mining the Bibliome project begins post-ing official releases.
In the meantime, we have be-gun investigating the potential for using biologicalnamed-entity and ontological-class information tohelp rule out unlikely parses, for example in caseswhere an entity name is bisected by a constituentboundary.AcknowledgementsThis work was funded by the Biotechnologyand Biological Sciences Research Council andAstraZeneca PLC.
We would like to thank thereviewers for their insights and suggestions, andMark McLauchlan for his help with prepositions.ReferencesDaniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
InProceedings of the Human Language Technol-ogy Conference 2002 (HLT2002).
San Diego.Daniel M. Bikel.
2004.
A distributional analysisof a lexicalized statistical parsing model.
InProceedings of the 2004 Conference on Empir-ical Methods in Natural Language Processing(EMNLP 2004).
ACL, Barcelona.Eugene Charniak.
1999.
A maximum-entropy-inspired parser.
Technical report, Brown Uni-versity.Kevin B. Cohen and Lawrence Hunter.
2004.
Nat-ural language processing and systems biology.In Werner Dubitzky and Francisco Azuaje, ed-itors, Artificial Intelligence Methods and Toolsfor Systems Biology.
Springer Verlag, Heidel-berg.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
PhD,University of Pennsylvania.Richard Crouch, Ronald Kaplan, Tracy King, andStefan Riezler.
2002.
A comparison of evalu-ation metrics for a broad coverage parser.
InProceedings of the LREC-2002 workshop ?Be-yond PARSEVAL: Towards Improved Evalua-tion Measures for Parsing Systems?.
Las Pal-mas, Spain.Carol Friedman, Pauline Kra, and Andrey Rzhet-sky.
2002.
Two biomedical sublanguages: a de-scription based on the theories of Zellig Harris.Journal of Biomedical Informatics, 35(4):222?235.John C. Henderson.
1999.
Exploiting Diversity forNatural Language Parsing.
PhD, Johns Hop-kins University.John C. Henderson and Eric Brill.
1999.
Ex-ploiting diversity in natural language process-ing: Combining parsers.
In Proceedings ofthe Fourth Conference on Empirical Methodsin Natural Language Processing (EMNLP99).University of Maryland.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, andJun?ichi Tsujii.
2003.
GENIA corpus ?
a se-mantically annotated corpus for bio-textmining.Bioinformatics, 19(Suppl.
1):i180?i182.Seth Kulick, Ann Bies, Mark Liberman, MarkMandel, Ryan McDonald, Martha Palmer, An-drew Schein, Lyle Ungar, Scott Winters, andPete White.
2004.
Integrated annotation forbiomedical information extraction.
In LynetteHirschman and James Pustejovsky, editors,HLT-NAACL 2004 Workshop: BioLINK 2004,Linking Biological Literature, Ontologies andDatabases, pages 61?68.
Association for Com-putational Linguistics, Boston, Massachusetts,USA.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1994.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Miles Osborne and Jason Baldridge.
2004.Ensemble-based active learning for parse selec-tion.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Pro-ceedings, pages 89?96.
Association for Com-putational Linguistics, Boston, Massachusetts,USA.Adwait Ratnaparkhi.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of theEmpirical Methods in Natural Language Pro-cessing Conference.
University of Pennsylva-nia.Eric Ringger, Robert C. Moore, Eugene Char-niak, Lucy Vanderwende, and Hisami Suzuki.2004.
Using the Penn Treebank to evaluate non-treebank parsers.
In Proceedings of 4th Interna-tional Conference on Language Resource andEvaluation (LREC2004), volume IV.
ELDA.Brian Roark.
2002.
Evaluating parser accuracy us-ing edit distance.
In Proceedings of the LREC-2002 workshop ?Beyond PARSEVAL: TowardsImproved Evaluation Measures for Parsing Sys-tems?.
Las Palmas, Spain.Geoffrey Sampson and Anna Babarczy.
2003.
Atest of the leaf-ancestor metric for parse accu-racy.
Journal of Natural Language Engineer-ing, 9(4):365?380.Hagit Shatkay and Ronen Feldman.
2003.
Miningthe biomedical literature in the genomic era: Anoverview.
Journal of Computational Biology,10(6):821?856.Larry H. Smith, Thomas Rindflesch, and W. JohnWilbur.
2004.
MedPost: a part-of-speechtagger for biomedical text.
Bioinformatics,20(14):2320?2321.Mark Steedman, Steven Baker, Stephen Clark, JayCrim, Julia Hockenmaier, Rebecca Hwa, MilesOsborne, Paul Ruhlen, and Anoop Sarkar.
2003.CLSP WS-02 final report: Semi-supervisedtraining for statistical parsing.
Technical report,Johns Hopkins University.Cornelis J. van Rijsbergen.
1979.
Information Re-trieval, 2nd edition.
Butterworths, London.Juan Xiao, Jian Su, GuoDong Zhou, andChewLim Tan.
2005.
Protein-protein interac-tion: A supervised learning approach.
In Pro-ceedings of the First International Symposiumon Semantic Mining in Biomedicine.
Hinxton,UK.
