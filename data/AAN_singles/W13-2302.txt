Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 11?18,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsPOS Tagging for Historical Texts with Sparse Training DataMarcel BollmannDepartment of Linguistics, Ruhr University Bochumbollmann@linguistics.rub.deAbstractThis paper presents a method for part-of-speech tagging of historical data and eval-uates it on texts from different corporaof historical German (15th?18th century).Spelling normalization is used to prepro-cess the texts before applying a POS tag-ger trained on modern German corpora.Using only 250 manually normalized to-kens as training data, the tagging accuracyof a manuscript from the 15th century canbe raised from 28.65% to 74.89%.1 Introduction1Part-of-speech (POS) tagging of modern languagedata is a well-explored field, commonly achiev-ing accuracies around 97% (Brants, 2000; Schmidand Laws, 2008).
For historical language varieties,the situation is worse, as specialized taggers aretypically not available.
As an example, a studyby Scheible et al(2011a) reports an average tag-ging accuracy of 69.6% for Early Modern Germantexts.
However, with projects to create historicalcorpora being on the rise (Sa?nchez-Marco et al2010; Scheible et al 2011b, are recent examples),the need for more accurate tagging methods onthese types of data increases.A common approach for historical texts is touse spelling normalization to map historical word-forms to modern ones (Baron and Rayson, 2008;Jurish, 2010).
Manually normalized data wasfound to improve POS tagging accuracy for avariety of languages such as German, English,and Portuguese, with accuracies between 79%and 91% (Scheible et al 2011a; Rayson et al2007; Hendrickx and Marquilhas, 2011).1I would like to thank the anonymous reviewers for theirhelpful comments.
The research reported here was sup-ported by Deutsche Forschungsgemeinschaft (DFG), GrantsDI 1558/4-1 and DI 1558/5-1.This paper presents results for POS tagging ofhistorical German from 1400 to 1770, classifiedhere as Early New High German (ENHG), us-ing automatic spelling normalization to prepro-cess the data for a POS tagger trained on mod-ern German corpora.
To train the normalizationtool, short fragments of a few hundred tokens areused for each text.
This approach allows for abetter adaptation to the individual spelling char-acteristics of each text while requiring only smallamounts of training data.
Additionally, differentways to deal with typical obstacles for processinghistorical texts (e.g., inconsistent use of punctua-tion) are compared.The structure of this paper is as follows.
Sec.
2presents the historical texts used for the evalua-tion.
Sec.
3 describes the approach to normal-ization, while Sec.
4 discusses problems and re-sults of POS tagging on normalized data.
Sec.
5presents related work, and Sec.
6 concludes.2 CorporaThis study considers texts from two corpora of his-torical German: the Anselm corpus (Dipper andSchultz-Balluff, 2013) and the GerManC-GS cor-pus (Scheible et al 2011b).The Anselm corpus consists of more than50 different versions of a medieval religious trea-tise written up in various German dialects.
As thecreation of gold-standard annotations for the cor-pus is still in progress, only two texts are usedhere: a manuscript in an Eastern Upper Germandialect kept in Melk, Austria; and an EasternCentral German manuscript kept in Berlin.
Bothmanuscripts are dated to the 15th century.The GerManC-GS corpus aims to be a repre-sentative subcorpus of GerManC with additionalgold-standard annotations.
It contains texts fromEarly Modern German categorized by genre, re-gion, and time period.
For this study, the threetexts of the genre ?sermon?
are used.
They are11Corpus Date Name TokensAnselm15c Berlin 5,39915c Melk 4,783GerManC-GS1677 LeichSermon 2,5851730 JubelFeste 2,5231770 Gottesdienst 2,292Table 1: Texts used for the evaluationdated from 1677 to 1770, which makes them con-siderably newer than the Anselm texts.
Table 1gives an overview of all texts used here.All texts are manually annotated with normal-izations and POS tags.
In the normalization layer,tokens are mapped to modern German equivalents.The normalization schemes are not identical, butroughly comparable for both GerManC-GS andAnselm (see Scheible et al(2011b) and Bollmannet al(2012) for details).
In both corpora, POS tag-ging follows the STTS tagset (Schiller et al 1999)without morphological information, though someadditional tags were introduced in GerManC-GS.For our evaluation, they are mapped back to stan-dard STTS tags; this mapping only affects 80 to-kens from all three texts.Additionally, both corpora are annotated withmodern punctuation and sentence boundaries;however, while modern punctuation is a sep-arate annotation layer in Anselm, there is al-ways a 1:1 correspondence between historical andmodern (i.e., normalized) punctuation marks inGerManC-GS.Finally, both corpora preserve many spellingcharacteristics of the original manuscripts, e.g.,superposition of characters such as u?, or abbrevi-ation marks such as the nasal bar (as in v?).
Be-fore any further processing, all wordforms are sim-plified to plain alphabetic characters; e.g., u?
ismapped to uo.
For some abbreviation marks inthe Anselm corpus, there is no clear ?best?
sim-plification: the nasal bar is a prime example here,which should be simplified most appropriately toe, (e)n, (e)m, or nothing, either before or after theletter on which it is placed, depending on context.In these cases, manually defined heuristics wereused to guess the most appropriate mapping.
Ascapitalization is not used consistently in the texts,all letters were additionally lowercased.3 NormalizationSpelling normalization is performed using theNorma tool (Bollmann, 2012).
It implements achain of normalization methods?to the effect thatmethods further down the chain are only called ifprevious ones failed to produce a result?in thefollowing order: (1) wordlist mapping; (2) rule-based normalization; and (3) weighted Leven-shtein distance.Wordlist mapping considers simple 1:1 map-pings of historical wordforms to modern ones(e.g., vnd?
und ?and?
), while rule-based normal-ization applies context-sensitive character rewriterules (e.g., transform v to u between a wordboundary and n) to an input string from left toright.
Weighted Levenshtein distance assigns in-dividual weights to character replacements (e.g.,v ?
u), and performs normalization by retrievingthe wordform from a modern lexicon which canbe derived from the historical input wordformwiththe lowest cost, i.e., using a sequence of edit oper-ations with the lowest sum of weights.3.1 Normalization procedureAll normalization algorithms described above re-quire some kind of parametrization to work (i.e.,a wordlist; rewrite rules; Levenshtein weights).These parametrizations are neither hard-coded normanually defined, but are derived automatically bythe Norma tool from a set of manually normal-ized training data.
For this purpose, short samplesfrom the text to be normalized are used; i.e., train-ing set and evaluation set are always disjoint partsof the same text.
The reasons for choosing thisapproach lie in the individual spelling character-istics of the texts?the following examples showexcerpts from Berlin, Melk, and LeichSermon, re-spectively, along with their (gold-standard) nor-malizations:(1) dyndeinlybesliebeskyntkind?your dear child?
(2) meinmeinliebsliebeschindkind?my dear child?
(3) einseinsihrerihrerandernanderenkinderkinder?one of their other children?12Text Baseline Normalizations100 250 500 1,000Berlin 23.05% 68.99% 75.02% 79.14% 81.83%Melk 39.32% 69.10% 74.39% 75.74% 77.98%LeichSermon 72.71% 77.96% 80.51% 82.85% 87.23%JubelFeste 79.47% 88.50% 89.98% 91.87% 93.13%Gottesdienst 83.41% 93.77% 95.24% 95.27% 95.56%Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of10 random training and evaluation sets), compared to the ?baseline?
score of the full text without anynormalizationThe first two examples, while both dated to the15th century, show quite different spellings of themodern Kind ?child?
: Ex.
(1) shows the frequentuse of y for modern ei or i(e), while Ex.
(2) demon-strates the frequent spelling ch for k. These dif-ferences are likely a cause of the different dialec-tal regions from which the manuscripts originate,but could also be attributed, at least in parts, to in-dividual preferences by the manuscripts?
writers.The LeichSermon text from 1677 in Ex.
(3), onthe other hand, already has the modern Germanspelling Kind.Given this range of spelling variations, it seemsimplausible to achieve good normalization resultsusing the same parametrization for each of thetexts.
Furthermore, for the older manuscriptsshowing more variation such as in Ex.
(1), it isunclear what other training data could be used.The full GerManC-GS consists of texts from 1650to 1800, while Jurish (2010) uses a corpus ofGerman texts from 1780 to 1880; these texts areall considerably newer, consequently having lessspelling variation than the Anselm texts.
This lackof appropriate training data applies similarly to allkinds of less-resourced language varieties.Therefore, while this approach requires slightlymore effort for manually normalizing parts of thetexts beforehand, it does not depend on the avail-ability of a large training corpus or a specializedtool for the language variety to be processed.3.2 EvaluationNormalization is evaluated separately for eachtext, using a part of that text for training and eval-uating on a different part of the same text.
Toaddress the question of how much training datais needed, evaluation is performed with differentsizes of the training set in a range between 100 and1,000 tokens.
The evaluation set is kept at a fixedsize of 1,000 tokens.
Normalization accuracy iscalculated by taking the average of 10 trials withrandomly drawn training and evaluation sets.
Theresults of this evaluation are shown in Table 2.The baseline score for a text is defined as thepercentage of matching tokens between the un-modified, historical text and its gold-standard nor-malization.
There is a clear difference between theAnselm texts, with scores of 23% and 39%, andthe GerManC-GS texts, which range from 72%to 83%.
This shows that spelling variation affectssignificantly more wordforms in the Anselm texts.The age of a text is likely to be the main factorfor this, as even within the group of GerManC-GS texts, a clear tendency for newer texts to havehigher baseline scores can be observed.Spelling normalization with the Norma toolshows rather positive results even for small train-ing samples: with only 100 tokens used for train-ing, it achieves a normalization accuracy of 69%for the Anselm texts, and raises the score for theGerManC-GS texts by 5?10 percentage points.Using 250 tokens results in another noticeable in-crease in accuracy, although the relative gain fromincreasing the training size even further attenuatesafter this point.4 Part-of-speech taggingWhile spelling normalization can be useful in it-self (e.g., for search queries in the corpus), ourmain focus is on its usefulness for further pro-cessing of the data such as part-of-speech tagging.The results presented here were achieved using theRFTagger (Schmid and Laws, 2008) with an in-creased context size of 10, which we found to per-form best on average on our data.13Text OrigP ModP NoPBerlin 85.78% 87.29% 87.07%Melk 85.21% 87.76% 87.74%LeichSermon 81.22% 80.59% 81.04%JubelFeste 90.41% 90.41% 90.03%Gottesdienst 93.24% 93.24% 92.27%Table 3: Tagging accuracy on the gold-standardnormalizations (OrigP = original punctuation,ModP = modern punctuation, NoP = no punctu-ation)4.1 Impact of punctuationNormalization tries to handle the problem ofspelling inconsistencies found in historical lan-guage data.
However, this is not the only challengefor processing the data with modern POS taggers.There is often no consistent capitalization, whichcan normally be used as a clue to detect nouns inmodern German.
This has already led to all word-forms being lowercased for the normalization pro-cess.
Additionally, punctuation marks are also of-ten used inconsistently or are missing completely:e.g., the Melk manuscript mostly uses virgules (vi-sually resembling a modern slash ?/?)
where mod-ern German would use a full stop, but this is farfrom a definite rule, and large parts of the Anselmtexts feature no punctuation marks at all.
Thisraises the question whether punctuation should beused for POS tagging at all for these texts.In order to test the impact of punctuation ontagging performance, three scenarios are consid-ered: tagging with original, modern, and no punc-tuation marks.
In order to provide a fairer com-parison, instead of using the supplied parameterfile for German, we retrain RFTagger on a pre-pared set of data.
For this purpose, the TIGERcorpus (Brants et al 2002) and version 6 of Tu?ba-D/Z (Telljohann et al 2004) are used.
First, thetwo corpora are combined?with minor modifica-tions to the POS tags to make them uniform?andlowercased.
The combined corpus has a size ofmore than 1.6 million tokens.
Additionally, for theevaluation without punctuation, a separate taggermodel is trained on a version of the TIGER/Tu?bacorpus where all punctuation marks and sentenceboundaries have been removed.Using these tagger models, tagging perfor-Original 96.85%Lowercased 96.50%No punctuation and SB 96.22%Lowercased + no punctuation and SB 95.74%Table 4: Tagging accuracy on the combinedTIGER/Tu?ba corpus, using 10-fold CV, evaluatedwith and without capitalization, punctuation, andsentence boundaries (SB)mance is evaluated on the gold-standard normal-izations with different levels of punctuation.
Theresults are shown in Table 3.
For better compara-bility, accuracy was evaluated excluding punctua-tion marks in all scenarios.Tagging with modern punctuation or no punc-tuation is shown to be best in all cases, with thedifference between these two scenarios never be-ing statistically significant (p > 0.05).
For theAnselm texts, using the original punctuation isworse than using none at all.
This is not truefor GerManC-GS, though the differences are mi-nor; also, original and modern punctuation areidentical for the JubelFeste and Gottesdienst texts,showing that they already follow modern Germanconventions in this regard.The results show that removing all punctua-tion marks does not lead to significant losses inPOS tagging accuracy.
Indeed, for texts with in-frequent and/or inconsistent use of punctuationmarks, discarding punctuation is shown to bepreferable.
For these reasons, the tagging ap-proach without punctuation is used for all follow-ing experiments.4.2 Tagging ?with handicaps?So far, the preprocessing of the historical data in-cludes removing all capitalization and punctua-tion.
Consequently, information about sentenceboundaries should also be removed, as it cannoteasily be derived from texts without (consistent)punctuation.
However, POS tagging with these?handicaps?
potentially increases the difficulty ofthe task in general.To gauge the extent of this effect, an evalua-tion on modern data was performed using 10-foldcross-validation on the combined TIGER/Tu?bacorpus, both with and without these artificialmodifications.
Table 4 shows the results of14Text Tokens Original Automatically normalized Gold100 250 500 1,000Berlin 4,719 28.65% 58.68% 74.89% 75.95% 78.03% 87.07%Melk 4,550 44.70% 69.63% 74.02% 76.24% 78.66% 87.74%LeichSermon 2,215 67.95% 72.87% 74.63% 75.85% 78.01% 81.04%JubelFeste 2,137 82.26% 82.64% 83.62% 86.52% 87.74% 90.03%Gottesdienst 1,953 88.07% 88.84% 90.27% 91.30% 91.65% 92.27%Table 5: POS tagging accuracy on texts without punctuation and capitalization, for tagging on the originaldata, the gold-standard normalization, and automatic normalizations using the first n tokens as trainingdatathis experiment; tagging accuracy drops from96.85% to 95.74% when removing capitalizationand punctuation.
While this change is significant(p < 0.01) considering the corpus size, with re-gard to the effort involved in manually annotatingwhole texts with modern capitalization and punc-tuation marks, it seems small enough to make tag-ging without this information a viable approachfor historical data.4.3 Tagging historical dataPOS tagging on the historical texts is evaluated inthree different scenarios: first, tagging on the sim-plified, but otherwise unmodified, original texts;second, tagging on the gold-standard normaliza-tions; and third, tagging on texts which have beennormalized automatically as described in Sec.
3.For automatic normalization, the first n tokensof a text were used for training the Norma tool,with different values for n (cf.
Sec.
3.2).
Only theremainder of the text has then been automaticallyprocessed by Norma.
This means that, e.g., for atext with 500 tokens used for training, POS tag-ging is performed on a version of the text con-sisting of 500 gold-standard normalizations plusautomatically generated normalizations for the re-mainder of the text.
This evaluation method mod-els a typical application scenario, where a tradeoffis made between no manual effort (= tagging onthe original) and full manual preprocessing (= tag-ging on the gold-standard).Full evaluation results are shown in Table 5.Tagging accuracy roughly correlates with nor-malization accuracy (cf.
Table 2); it tends to beslightly above the normalization score for Anselmand a few points below that score for GerManC-GS.
Tagging on the original, historical data isparticularly inaccurate for the Anselm texts, withthe Berlin text only achieving an accuracy of28.7%.
This again highlights the need for special-ized tagging methods on such types of data.
TheGerManC-GS texts from the 18th century performmuch better without normalization, with accura-cies up to 88% for the Gottesdienst text.
Theseresults mainly confirm the observations that theAnselm texts show much more variety in spellingthan the newer texts from GerManC-GS.Similar to the results for normalization, us-ing only 100 tokens for training is enough to in-crease tagging accuracy for the Melk text from45% to 70%.
For Berlin, this method results inan even higher relative increase, more than dou-bling the number of correct POS tags.
Resultsfor these texts can be improved further to about74% when using 250 tokens for training; after thisfigure, POS tagging seems to profit less from in-creasing the size of the training set, with accura-cies around 78% for a training set of 1,000 tokens.The GerManC-GS texts, particularly JubelFesteand Gottesdienst, do not benefit as much from asmall number of training tokens.
With 100 tokens,POS tagging accuracy only increases by 0.38?0.77percentage points.
However, these texts alreadyhave a comparatively high baseline to start with(82?88%).
As they are already much closer tomodern German spelling, fewer wordforms havespelling variations at all; consequently, more train-ing data is required to capture a similar amount ofvariant wordforms as in the Anselm texts.
Indeed,when increasing the training portion to 1,000 to-kens, the benefit of spelling normalization be-comes more pronounced.Curiously, for the LeichSermon text, eventhe gold-standard normalization only achieves81% accuracy, which is significantly lower thanfor any other text in the evaluation.
This is un-15expected, considering that the text is much morerecent than Berlin and Melk.
The reason for thisdiscrepancy is the frequent use of bible verse num-bers in LeichSermon, which are written as numer-als followed by a dot and annotated as CARD (car-dinal number) in the gold-standard data.
In theTIGER corpus and Tu?ba-D/Z, such numerals aretreated as ordinal numbers and tagged as ADJA,leading to a high number of mismatching tags.4.4 Error analysisPOS tagging results for the historical texts are stillconsiderably worse than those for modern data,even when tagging on gold-standard normaliza-tions (81?92% vs. 95.74%).
There are several fac-tors responsible for this.It is important to observe that even perfectlynormalized historical data has different character-istics than modern data, as normalization only af-fects the spelling of wordforms.
One potentialsource of errors are semantic changes, as shownin Ex.
(4) from the LeichSermon text: the word-form so is an adverb in modern German, but isfrequently used as a relative pronoun (PRELS2)in ENHG, which nevers occurs in the training dataof the TIGER/Tu?ba corpus.
(4) diedieARTfaellefa?lleNNsosoPRELSausausAPPRschwacheitschwachheitNNgeschehengeschehenVVPP?the cases which occur out of weakness?Extinct wordforms are a major problem for thenormalization approach.
They cannot usually benormalized to a modern wordform by applyingspelling changes, but would have to be mapped ona word-by-word basis.
However, both GerManC-GS and the normalization layer of Anselm3 mapextinct wordforms to artificial lemmas, which arestill useful to identify spelling variants, but im-practical for this POS tagging approach.
A com-mon example in Melk is czuhant ?immediately?,2Actually, GerManC-GS annotates so in this examplewith the new tag PTKREL, which is mapped back to PRELSfor reasons of compatibility.
As PTKREL is not found inTIGER or Tu?ba-D/Z, keeping this tag would not solve theproblem here, though.3The Anselm corpus provides an additional ?moderniza-tion?
layer which maps extinct forms to actual modern words,but a first evaluation showed that using this layer has a nega-tive impact on overall normalization accuracy.which is mapped to the artificial lemma zehant,but would rather be expressed as sofort in modernGerman:(5) czuhantzehantADVchustku?sstVVFINiudasjudasNEmeinmeinPPOSATchintkindNN?Immediately, Judas kisses my child?Finally, a significant number of errors ap-pears to result from limitations of the modernTIGER/Tu?ba corpus used to train the POS tag-ger.
This corpus is created from newspaper texts,which are typically written in a rather formal style.The Anselm texts, on the other hand, consist ofquestion/answer sets which contain a lot of directspeech.
Similarly, the Gottesdienst text is a reli-gious speech which addresses its audience rightfrom the beginning.
Ex.
(6) shows a phrase thatoccurs frequently in the Berlin text:(6) siehVVIMPanselmNE?Look, Anselm?The imperative form sieh ?look?
is used24 times in the Berlin text, but typically mistaggedas a proper noun (NE) despite being correctly nor-malized.
A look at the TIGER/Tu?ba training datareveals the cause for this: the wordform sieh doesnot occur there at all; only the standard form siehewas learned.
Imperative verb forms in generalare very uncommon in TIGER/Tu?ba, only mak-ing up 397 tokens (0.02%).
In comparison, thegold-standard POS annotation of Berlin alreadycontains 43 imperative verb forms (0.91%).Similarly, the religious texts in Anselm andGerManC-GS often use vocabulary that is rarelyused in newspaper text.
Ex.
(7) shows the fi-nite verb form verschma?hten ?despised/spurned?,which has only one occurrence in the TIGER/Tu?bacorpus where it was used as an adjective instead,inevitably leading to a tagging error.
(7) vndundKONvorsmetenverschma?htenVVFINynihnPPER?and [they] despised him?These examples show that even if spelling nor-malization was done perfectly on historical texts,semantic/syntactic variation and domain adapta-tion of the POS tagger provide further obstaclesfor achieving higher tagging accuracies.165 Related workFor automatic spelling normalization, VARD 2(Baron and Rayson, 2008) is another tool that hasbeen developed for Early Modern English.
It hasbeen successfully adapted to other languages, e.g.Portuguese (Hendrickx and Marquilhas, 2011),though previous experiments found it to performworse than Norma on the Anselm data (Bollmann,2012).
Jurish (2010) presents a normalizationmethod that includes token context, which seemsto be the logical next step to further improve nor-malization results.POS tagging on normalized data has been triedfor the GerManC-GS corpus before with an aver-age accuracy of 79.7% (Scheible et al 2011a),however, only manual normalization was consid-ered.
For English, Rayson et al(2007) reportan accuracy of 89?91% on gold standard normal-izations and 85?89% on automatically normal-ized texts.
Hendrickx and Marquilhas (2011) per-form a similar evaluation for Portuguese, achiev-ing 86.6% and 83.4% on gold standard and auto-matic normalizations, respectively.There are some notable differences, however,between the aforementioned studies and the ap-proach outlined here.
Firstly, those studies usingautomatic normalization methods typically utilizeeither a much higher amount of training data orsome kind of manually crafted resource.
VARD,for instance, uses a manually compiled list ofspelling variants totalling more than 45,000 en-tries (Rayson et al 2005), while Hendrickx andMarquilhas (2011) use a training set of more than37,000 tokens.
While I certainly expect to improvethe results in the future by using full texts from theAnselm and/or GerManC-GS corpora as basis fortraining, this approach might not always be feasi-ble.
The approach presented here, requiring only afew hundred tokens for training, seems especiallysuited for languages where projects to create his-torical corpora have only been started, and there-fore do not have large amounts of previously an-notated training material to fall back to.Secondly, the Anselm texts evaluated here showa much lower baseline than the texts evaluated inother studies.
Without normalization, POS tag-ging accuracy is 82?88% in Rayson et al(2007),76.9% in Hendrickx and Marquilhas (2011), and69.6% for the German data in Scheible et al(2011a).
The texts from Berlin and Melk, on theother hand, perform much worse without the nor-malization step (28.7% and 44.7%, respectively).This suggests a higher amount of variance in theAnselm data compared to the types of text used inprevious studies, making their automatic process-ing a potentially more challenging problem.
Also,annotated data from these studies is less likely tobe useful as training data for these texts.6 ConclusionI presented an approach to part-of-speech taggingfor historical texts that uses spelling normalizationas a preprocessing step.
Evaluation on texts fromEarly New High German showed that by manuallynormalizing 250 tokens of a text and using them astraining data, automatic normalization of the re-maining text performs well enough to result in anotable increase in POS tagging accuracy.
Textswith more spelling variation were shown to ben-efit more from this approach than texts which arealready closer to the modern target language.For one German manuscript from the 15th cen-tury, this method increased tagging accuracy from28.65% to 74.89%.
While this is still far fromthe accuracy scores reported for modern languagedata, and also quite a bit worse than taggingon the gold-standard normalization (87.07% forthis text), it offers a way to facilitate the (semi-automatic) POS annotation of historical texts withrelatively minor effort.
Furthermore, as it does notrequire a sizeable amount of training data, this ap-proach is potentially interesting for less-resourcedlanguage varieties in general, assuming some levelof graphematic similarity to a well-resourced tar-get language.Future work should likely consider inclusion oftoken context for the normalization as proposed byJurish (2010).
Analysis of the POS tagging errorsalso highlighted some of the problems that remain.Domain-specific differences can negatively impacttagging performance even on perfectly normalizeddata.
Furthermore, spelling normalization cannotaccount for semantic and syntactic peculiarities ofhistorical language.
For a corpus of Old Spanish,this led Sa?nchez-Marco et al(2010) to abandonthe normalization approach and use a customizedPOS tagger instead.
On the other hand, a study byDipper (2010) showed that normalization is stillbeneficial even when retraining a tagger on a cor-pus of historical data.
Future research could tryto combine a normalization step with a modifiedPOS tagger to improve the results further.17ReferencesAlistair Baron and Paul Rayson.
2008.
VARD 2: Atool for dealing with spelling variation in historicalcorpora.
In Proceedings of the Postgraduate Con-ference in Corpus Linguistics.Marcel Bollmann, Stefanie Dipper, Julia Krasselt, andFlorian Petran.
2012.
Manual and semi-automaticnormalization of historical spelling ?
Case studiesfrom Early New High German.
In Proceedingsof KONVENS 2012 (LThist 2012 workshop), pages342?350, Vienna, Austria.Marcel Bollmann.
2012.
(Semi-)automatic normaliza-tion of historical texts using distance measures andthe Norma tool.
In Proceedings of the Second Work-shop on Annotation of Corpora for Research in theHumanities (ACRH-2), Lisbon, Portugal.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Erhard Hinrichs and Kiril Simov, ed-itors, Proceedings of the First Workshop on Tree-banks and Linguistic Theories (TLT 2002), Sozopol,Bulgaria.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of ANLP 2000, pages224?231, Seattle, USA.Stefanie Dipper and Simone Schultz-Balluff.
2013.The Anselm corpus: Methods and perspectives ofa parallel aligned corpus.
In Proceedings of theNODALIDA Workshop on Computational HistoricalLinguistics.Stefanie Dipper.
2010.
POS-tagging of historical lan-guage data: First experiments.
In Proceedings ofKONVENS 2010, pages 117?121, Saarbru?cken, Ger-many.Iris Hendrickx and Rita Marquilhas.
2011.
From oldtexts to modern spellings: an experiment in auto-matic normalisation.
JLCL, 26(2):65?76.Bryan Jurish.
2010.
More than words: using to-ken context to improve canonicalization of histori-cal German.
Journal for Language Technology andComputational Linguistics, 25(1):23?39.Paul Rayson, Dawn Archer, and Nicholas Smith.
2005.VARD versus Word.
A comparison of the UCRELvariant detector and modern spell checkers on en-glish historical corpora.
In Proceedings of CorpusLinguistics 2005, Birmingham, UK.Paul Rayson, Dawn Archer, Alistair Baron, JonathanCulpeper, and Nicholas Smith.
2007.
Tagging thebard: Evaluating the accuracy of a modern POS tag-ger on Early Modern English corpora.
In Proceed-ings of Corpus Linguistics 2007, University of Birm-ingham, UK.Cristina Sa?nchez-Marco, Gemma Boleda, Josep MariaFontana, and Judith Domingo.
2010.
Annotationand representation of a diachronic corpus of Span-ish.
In Proceedings of the Seventh Conference onInternational Language Resources and Evaluation,pages 2713?2718.Silke Scheible, Richard J. Whitt, Martin Durrell, andPaul Bennett.
2011a.
Evaluating an ?off-the-shelf?POS-tagger on Early Modern German text.
In Pro-ceedings of the ACL-HLT 2011 Workshop on Lan-guage Technology for Cultural Heritage, Social Sci-ences, and Humanities (LaTeCH 2011), pages 19?23, Portland, Oregon, USA.Silke Scheible, Richard J. Whitt, Martin Durrell, andPaul Bennett.
2011b.
A gold standard corpus ofEarly Modern German.
In Proceedings of the ACL-HLT 2011 Linguistic Annotation Workshop (LAW V),pages 124?128, Portland, Oregon, USA.Anne Schiller, Simone Teufel, Christine Sto?ckert, andChristine Thielen.
1999.
Guidelines fu?r das Taggingdeutscher Textcorpora mit STTS.
Technical report.Helmut Schmid and Florian Laws.
2008.
Estima-tion of conditional probabilities with decision treesand an application to fine-grained POS tagging.
InProceedings of COLING ?08, Manchester, GreatBritain.Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.2004.
The Tu?ba-D/Z Treebank: Annotating Ger-man with a Context-Free Backbone.
In Proceed-ings of the Fourth International Conference onLanguage Resources and Evaluation (LREC 2004),pages 2229?2235, Lisbon, Portugal.18
