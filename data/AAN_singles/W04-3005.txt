Automatic Call Routing with Multiple Language ModelsQiang Huang and Stephen CoxSchool of Computing Sciences,University of East Anglia,Norwich NR4 7TJ, U.K.(h.qiang|sjc}@cmp.uea.ac.ukAbstractOur motivation is to perform call routing of utteranceswithout recourse to transcriptions of the training data,which are very expensive to obtain.
We therefore usephonetic recognition of utterances and search forsalient phonetic sequences within the decodings.
Animportant issue in phonetic recognition is the languagemodel.
It has been demonstrated [1] that the use of aniterative language model gives benefits in speechrecognition performance that are translated toimprovements in utterance classification.
However, anall-purpose language model sometimes producesdecodings that are ambiguous, in that they apparentlycontain key phonetic sequences from several differentroutes, or non-informative, in that they apparentlycontain no useful phonetic sequences.
This paperdescribes a method that uses multiple language modelsto detect useful information in such utterances.
Theoutputs from recognizers that use these multiple modelsare examined by post-processing HMMs that decidewhether putative sequences are present or not.
It isfound that using multiple language models increasesperformance significantly by classifying utterances thata single language model is unable to discriminate.1.
IntroductionCall routing refers to the technique of automaticallyrelaying a customer's telephone enquiry to one ofseveral appropriate destinations, using computationalspeech and language processing techniques.Transcribing calls for training purposes for a particularapplication requires considerable human effort, and itwould be preferable for the system to learn routeswithout transcriptions being provided [2].In this study, we assume that we are provided with a setof training utterances that have been labelled with theirdestination by an expert, but not transcribed into wordsor phonemes.
We also assume (perhaps over-pessimistically) that we have no prior knowledge of thevocabulary or syntax of our application.In this situation, one possible course of action is to usephone recognition and attempt to identify phoneticsequences that are salient to particular routes.Unfortunately, the speech signals are often of very poorquality, being subject to the usual distortion, bandwidthrestriction and noise associated with telephone signals,and often compounded by the fact that callers usuallyspeak casually and spontaneously, and sometimes with astrong accent.Some approaches to the problem of extracting salientphonetic strings from these utterances are:?
Improve phone accuracy by using a variable lengthlanguage model and building models for insertionand substitution; [3,4]?
Identify subword units (e.g.
phonemes, phonemestrings, syllables and morphemes) from therecognised phonetic sequences by using clusteringand segmentation methods;  [5,6,7]?
Use matrix-based methods for classification, such asLSA, LDA, ICA, SVM, etc.
[8,9,10]Work at AT&T [1] showed that call routing performanceusing this phone-string utterance classification can besurprisingly close to what can be achieved byconventional methods involving word-trigram languagemodels that require manual transcription.
The methoddescribed in [1] combines automatic training ofapplication-specific phonotactic language modelstogether with token sequence classifiers.Our own experiments, using data different from that usedby AT&T, showed that this technique gave only a smallbenefit in phone recognition accuracy, but was useful forfinding salient phoneme strings.
However, we foundthat, in some cases, it was impossible to obtain salientphoneme sequences from the recognised utterances evenwhen it was known that they occurred within theutterance.
The reason may be that when building asingle language model with the collected utterances fromall call routes, the salience of a particular sequence for aparticular route is lost in the ?noise?
from mis-recognisedsequences of phonemes from the other routes.
Hence wesought a way of making the language model moresensitive to the keywords occurring in the utterances.
Inour system, an independent corpus is used to build ann-gram phonotactic language  model that enables aninitial recogniser to be built to decode all the trainingutterances.
This model is refined iteratively using theoutput from the recogniser as the basis for the nextlanguage model.
A specific language model for eachcall route is then built using the utterances from thiscall route.
These are much more sensitive to keysalient phoneme sequences in the utterance.The structure of the paper is as follows: in section 2,the data corpus used is introduced.
Section 3 describesin detail the language modelling techniques, section 4presents experiments and analysis of results, and weend with a Discussion in section 5.2.
DatabaseThe application studied here was the enquiry-point forthe store card for a large retail store.
Customers wereinvited to call up the system and to make the kind ofenquiry they would normally make when talking to anoperator.
Their calls were routed to 61 differentdestinations, although some destinations were usedvery infrequently.
15 000 utterances were available,and a subset of 4511 utterances was used for trainingand 3518 for testing, in which 18 different call typeswere represented.
Some of these call types are quiteeasily confused e.g.
PaymentDue and PaymentDate,PaymentAddress and Changeaddress.
Phonemerecognition of the input speech queries was performedusing an HMM recogniser whose acoustic models hadbeen trained on a large corpus of telephone speech andwhich had separate models for males and females.
Theaverage length of an utterance is 8.36 words.
Inaddition, transcriptions of the prompts from the WallStreet Journal (WSJ) database were used to generatephoneme-level statistical language models for initialtraining.
These models were generated using a schemefor backing off to probability estimates for shorter n-grams.The size of the vocabulary is 1208 words.
To get a feelfor the difficulty of the task, the mutual information(MI) between each word and the classes wascalculated.
By setting a threshold on this figure, weobserved that there were about 51 keywords occurringin 4328 utterances which were capable on their own ofclassifying a call with high accuracy (some utteranceshad no keywords).3.
Modelling3.1.
Model StructureFigure 1 shows the method used to produce an initiallanguage model.The algorithm follows that described in [1]:1.
Build an n-gram language model (LM) using thedictionary transcriptions of the WSJ corpus (we usedn=6).
Make this the current LM.2.
Use the current LM in the recognizer to produce aset of phone strings.3.
Build a new LM based on the recognizer phonestrings:4.
If niterations <=threshold, goto 2 elsefinish and produce a single language model forall routes.Phonotactic languagemodel (WSJ)Figure.
1 The Iterative training procedureThe phone strings are now segmented and clustered sothat salient phone sequences for each route can beidentified.
This is done as follows:FOR EACH ROUTE1.
Segment each recognized phone string in the routeinto all possible sequences of 3,4, ?
, 9 phones.2.
Estimate the MI for each sequence, and identify thesalient sequences as the sequences with the highestMI [11].3.
Cluster the salient sequences within the route.
Thisis done by calculating and combining two measuresof distance (using dynamic programmingtechniques) for each pair of sequences:?
The Levensthein distance between the phonesymbols representing the sequences.?
The acoustic distance in ?MFCC space?between the two waveform segmentsrepresenting the sequences.4.
Use a simple lexicon pruning scheme thateliminates long agglomerations of short primitives[12].At this point, we have generated a set of clusteredphone sequences for each route.
Each phone sequencecorresponds to a sequence of frames, and the framesequences within a cluster are used to build an HMMThese HMMs are used later to estimate the class of asegment output by the recognizer (see section 3.2).Finally, we build a language model for each route, asfollows by collecting together the recognised phoneticsequences of utterances from each route and usingthem to construct a language model.After iterating the LM, detection of key phoneticsequences improves.
However, many utterances do notproduce any sequences  or produce several sequencesfrom different routes.
For recognition, we use a?divide and conquer?
approach.
Utterances that yieldone or more sequences from the same route areclassified immediately as that route, and utteranceswhose output is ambiguous, in that they yield nosequences, or sequences from several routes, or whoserecognition confidence is too low to trust, are subject toa more detailed recognition pass in which separate LMsfor each route are used.
This has the advantage ofonly applying the extra computational effort required touse multiple LMs  for those utterances that need this.In practice, if lattices are used, the additionalcomputational effort is not too great.
The confidencemeasure used was the measure available from theNuance speech recognizer v8.0.Hence recognition proceeds as follows.1.
A single language model is used in the recognizer toproduce an output phone string.2.
Any phonetic sequences in the output string that alsooccur within any of the clusters of key phoneticsequences in any of the routes are found.3.
IF the number of key phonetic sequences found isone or more AND the sequences all belong to thesame route:the utterance is classified as belonging to this route.ELSEIF  the number of key phonetic sequences iszero OR there are one or more sequences fromdifferent routes OR the confidence measure of thewhole utterance is lower than some threshold:the utterance is re-recognized using all 18 languagemodels.4.
Recognition using multiple language models works asfollows.
18 recognized phonetic sequences areoutput, one from each recognizer (as shown in Figure2), and key phonetic sequences are detected in eachoutput.IF there are one or more sequences from differentroutes:Putative sections of the speech that contain keywordsare identified by comparing the symbolic output of arecognizer using a certain LM with the sequences thatwere used to form the HMMs of the clustered keyphonetic sequences for this LM.
These HMMs arethen used to determine the likelihood of eachsequence given the output string, and the utterance isassigned to the route of the highest likelihood.ELSEIF  the number of key phonetic sequences iszeroThe utterance is not classified (rejected).TestutterancesTotal utts = 3515# classified correct by 1 LM = 2553# classified correct by 18 LMs = 487Figure 2: The Recognition ProcessNo sequencesdetected(3515 utts) (2553 utts)(962 utts)(368 utts) 487 utts correct2553 utts correct0 utts incorrect107 utts incorrectSequences froma routesingle1.
No sequences detected2.
Sequences from routes detected3.
Conf-measure too lowseveralKey phoneticsequences detected?Recogniserwith one LMRecogniser with18 LMsHMMs of keyphonetic sequencesDetection of keyphonetic sequencesREJECTCall-routeclassifyCall-routeclassifyCall type classification is done using a vector-basedapproach as described in [8].
It is perhaps surprisingthat this classifier gets 100% accuracy (2553/2553) onutterances in which all the sequences are apparentlyfrom the same route?we attribute this to the fact thatthe 18 call-types were used were highly independent intheir use of keywords.Figure 2 gives an overview of the whole process,together the number of utterances that were involved ineach stage.3.2.
Key Phonetic Sequence DetectionKey phonetic sequences can be incorrectly matched toincorrect segments of the utterance, causing falsealarms.
To combat this problem, we use matching inthe acoustic domain as well as the symbolic domain.HMMs for 41 key phonetic sequences whose numberof occurrences was larger than a threshold (we used 30)were built.
Each key phonetic sequence was modelledby a five-state left-to-right HMM with no skips andeach state is characterised by a mixture Gaussian stateobservation density.
A maximum of 3 mixturecomponents per state is used.
The Baum-Welchalgorithm is then used to estimate the parameters of theGaussian densities for all states of subword HMM?s.We use key phrase detection as described in [13][14].By using the phonetic output from the recogniser, theposition in the utterance waveform of putative stringscan be identified, and this section of the waveform isinput into the phonetic sequence HMMs.
Detection ofphrases is achieved by monitoring the forwardprobability of the data given the model at any time andsearching for peaks in the probability.
If full-likelihoodrecognition is used, we estimate the score ),( twS f :?=swf tstetwS),(),(),( ??
(1)In equation (1), ),( twS f  is the forward probability ofword w at time t [13].
In practice, we used the Viterbiequivalent of equation (1) to determine the likelihood.4.
Experiments4.1.
Phone accuracy based on one LMFigure 3 illustrates the effects of(a) using the recogniser output strings to construct anew language model as described in section 3.1;(b)  using 18 different LMs as well as a single LM.010203040506070801 2 3 4 5 6 7Iteration numberPhone error rate(%)1 LM Rec-Phone 18 LMs Rec-Phone 1 LM Trans-PhoneFig 3.
Phone error rate using 1 LM and 18 LMsRec-Phone: Build language model using recognisedphonetic sequences of utterances from training set;Trans-Phone: Build language model using phonemetranscriptions of words of utterances from training set1 LM:   Recognition using one language model;18LMs: Recognition using 18 language models.Figure 3 shows that the phone error rate is very muchhigher when recognised phone sequences (Rec-Phone)rather than dictionary transcriptions (Trans-Phone) areused to build an LM.
However, an interesting point isthat iterative performance decreases when thetranscriptions are used, but increases when therecognised strings are used.
This is probably because,when the recognised strings are used, the initial LM,which is trained on WSJ, does not reflect the distributionof n-grams in the data, and so performance is poor.However, the vocabulary in the data is quite small, sothat after even a single recognition pass, although theerror-rate is high, the new LM is a better reflection of then-grams in the data.
This has the effect of improving thephone recognition performance, and this improvementcontinues with each iteration.When we use an initial language model built usingdictionary phoneme transcriptions, the performance isinitially much better than using an LM trained on anindependent corpus, as would be expected.
However,because of the small vocabulary size and the relativelyhigh number of occurrences of a few phonetic sequences,any errors in recognition of these sequences dominate,and this leads to an increasing overall error-rate.These results are not as good as those obtained by Hiyan[1] using an iterative language model.
This may bebecause of the difference in the speech recognisers, or,more likely, in the average length of the phrases in thedifferent vocabularies, which are much shorter than thephrases used here.4.2.
Classification AccuracyIterationNo.1 2 3 4 5Phoneaccuracy25.7 27.1 30.0 30.6 31.0Classif-icationaccuracyRec-Phone(%)44.3 60.4 69.4 72.1 72.6Table 1.
Phone recognition accuracy andcall routing accuracyTable 1 shows the call-routing classificationperformance when a single LM is used and the LM isiterated.
What is interesting here is that an apparentlysmall increase in phone accuracy on iteration gives riseto a huge increase in call-routing accuracy.
This isbecause although the overall phone error-rate improvesonly slightly, the error rate on the key phoneticsequences is greatly improved, leading to improvedclassification performance.
Note that performance onthis dataset when the dictionary translations of thetranscriptions of the utterances are used is 93.7%.Name Trans-Phone1LM1 LM +Multiple LMsCorrectclassificationrate (%)93.7 72.6 86.5Table 2.
Comparison of correct classification rateTrans-Phone: language model built with dictionaryphoneme transcriptions of the utterances;1 LM: iterative language model built using  recognitionoutput;1 LM + Multiple LMs: Using the two-pass approachdescribed in section 3.1.Table 2 compares the call-routing classificationaccuracies.
The accuracy achieved using the two passsystem with multiple LMs (86.5%) is much better thanthat using a single iterated LM, but not quite as good asthat obtained by using the dictionary transcriptions.It could be argued that it is not possible to say whetherthe improvement shown in column 4 of Table 2compared with column 3 is due to the use of multipleLMs or to the use of the HMM post-processor.However, when a single LM is used, the situation iseither that there are one or more fairly unambiguousoutput sequences from a single call type, or there aremany noisy and ambiguous sequences whose positionsare not well-defined.
It is very difficult to process theseputative sequences with all the HMMs of key phoneticsequences.
Using multiple LMs has the effect ofproducing relatively unambiguous sequences from only asmall subset set of call-types, whose position in thewaveform is quite well-defined.
This reduces thenumber of HMM sequences that need to used and hencealso the difficulty of application.5.
DiscussionIn this paper, we have presented a method for automaticcall routing in which we do not require transcriptions ofthe training utterances, only the route of each utterance.The technique is based on phonetic recognition ofutterances, and we have focused on the design of thelanguage model in this recognition process.
Ourconclusions are that iterating a single phone languagemodel (as described in [1]) is highly beneficial toperformance, but performance can be further increasedby using multiple language models for recognition forutterances whose content is ambiguous when a singlelanguage model is used.
Using multiple LMs inevitablygives rise to identification of false keywords, but thisdifficulty is resolved by the use of post-processingHMMs which estimate the likelihood of the putativekeyword phonetic sequence being present in thewaveform.
Future work will concentrate on use ofconfidence measures and classification of ambiguousutterances.
We will also investigate the use of ?lightlysupervised?
adaptation, in which a small proportion ofthe utterances available have been transcribed [15].6.
AcknowledgmentWe are grateful to Nuance Communications forproviding the data for this study.7.
References[1] Hiyan Alshawi, ?Effective Utterance Classificationwith Unsupervised Phonotactic Models?, in Proc.HLT-NAACL 2003, pp.
1-7, Edmonton.
[2] Qiang Huang, Stephen Cox, ?Automatic Call-routingwithout Transcriptions?, in Proc.
EuroSpeech,Geneva, 2003.
[3] Deligne S.,  Bimbot F., ?Inference of Variable-lengthLinguistic and Acoustic Units by Multigrams.
?Speech Communication 23, pp.
223-241, 1997.
[4] Thomas Hain, Philip C. Woodland, ?ModellingSub-Phone Insertions and Deletions in ContinuousSpeech?, in Proc.
International Conference onSpoken Language Processing 2000, Beijing, China[5] K. Ng, V.W.
Zue, ?Subword Unit representationsfor spoken document retrieval?, in Proc.Eurospeech 1997.
[6] T. Nagarajan, Hema Murthy, ?Segmentation ofSpeech into Syllable-like units?, in Proc.EuroSpeech 2003, Geneva.
[7] D. Petrovska-Delacretaz, A. L. Gorin, J. H. Wright,and G. Riccardi.
?Detecting Acoustic Morphemesin Lattices for Spoken Language Understanding?,in Proc.
International Conference on SpokenLanguage Processing 2000, Beijing, China.
[8] S. Cox.
, ?Discriminative Techniques in CallRouting?, Proc.
IEEE International Conference onAcoustics, Speech, and Signal Processing, 2003,HongKong, China[9] Leopold, E.and J. Kindermann, ?TextCategorization with Support Vector Machines.How to Represent Texts in Input Space??
MachineLearning, 2002.
46: pp.423-444.
[10] Lee, T.-W., Lewicki, M.S., and Sejnowski, T.J.,?ICA Mixture Models for UnsupervisedClassification and Automatic Context Switching.?Proc.
International Workshop on IndependentComponent Analysis , 1999.
[11] S. Cox and B. Shahshahani, ?A Comparison ofsome Different Techniques for Vector Based Call-Routing?
Proc.
Workshop on Innovation in SpeechProcessing, Stratford, April 2001.
[12] Fuchun Peng, Dale Schuurmans, ?A HierarchicalEM Approach to Word Segmentation?, inProceeding of the Sixth Natural LanguageLanguage Processing Pacific Rim Symposium.Nov.
2001, Tokyo, Japan[13] J. R. Rohlicek, P. Jeanrenaud, K. Ng, H. Gish, B.Musicus M. Siu, ?Phonetic Training and LanguageModeling for Word Spotting?, in Proc.
of IEEEInternational Conference on Acoustic, Speech, andSignal Processing, 1993.
[14] Tatsuya Kawahara, Chin-Hui Lee, Bijing-HwangJuang, ?Flexible Speech Understanding Based onCombined Key-Phrase Detection and Verification?,IEEE Transactions on Speech and Audio Processing,Vol.6, No.
6, November 1998.
[15] D. Giuliani and M. Federico, "UnsupervisedLanguage and Acoustic Model Adaptation for CrossDomain Portability", in Proc.
ISCA ITR Workshop,Sophia-Antipolis, France, 2001.
