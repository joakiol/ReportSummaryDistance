Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 108?115,Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational LinguisticsRefining the Design of a Contracting Finite-State Dependency ParserAnssi Yli-Jyra?
and Jussi Piitulainen and Atro VoutilainenThe Department of Modern LanguagesPO Box 300014 University of Helsinki{anssi.yli-jyra,jussi.piitulainen,atro.voutilainen}@helsinki.fiAbstractThis work complements a parallel paper ofa new finite-state dependency parser archi-tecture (Yli-Jyra?, 2012) by a proposal fora linguistically elaborated morphology-syntaxinterface and its finite-state implementation.The proposed interface extends Gaifman?s(1965) classical dependency rule formalismby separating lexical word forms and morpho-logical categories from syntactic categories.The separation lets the linguist take advantageof the morphological features in order to re-duce the number of dependency rules and tomake them lexically selective.
In addition,the relative functional specificity of parse treesgives rise to a measure of parse quality.
By fil-tering worse parses out from the parse forestusing finite-state techniques, the best parsesare saved.
Finally, we present a synthesis ofstrict grammar parsing and robust text pars-ing by connecting fragmental parses into treeswith additional linear successor links.1 IntroductionFinite-state dependency parsing aims to combine de-pendency syntax and finite-state automata into a sin-gle elegant system.
Deterministic systems such as(Elworthy, 2000) are fast but susceptible to garden-path type errors although some ambiguity is encodedin the output.
Some other systems such as (Oflazer,2003; Yli-Jyra?, 2005) carry out full projective de-pendency parsing while being much slower, espe-cially if the syntactic ambiguity is high.
In the worstcase, the size of the minimal finite-state automa-ton storing the forest is exponentially larger thanthe sentence: an 80-word sentence has potentially1.1?1062 unrooted unlabeled dependency trees thatare stored ?compactly?
into a finite-state lattice thatrequires at least 2.4?1024 states, see Table 4 in Yli-Jyra?
(2012).A truly compact representation of the parse forestis provided by an interesting new extended finite-state parsing architecture (Yli-Jyra?, 2012) that firstrecognizes the grammatical sentences in quadratictime and space if the nested dependencies are lim-ited by a constant (in cubic time if the length of thesentence limits the nesting).
The new system (Yli-Jyra?, 2012) replaces the additive (Oflazer, 2003) andthe intersecting (Yli-Jyra?, 2005) validation of depen-dency links with reductive validation that graduallycontracts the dependencies until the whole tree hasbeen reduced into a trivial one.
The idea of the con-tractions is illustrated in Example 1.
In practice, ourparser operates on bracketed trees (i.e., strings), butthe effect will be similar.
(1) a. time flies like an arrowSUBJ ADVLNOBJDETb.
time flies like an arrowNOBJc.
time flies like an arrow108Despite being non-deterministic and efficient,there are two important requirements that are not ful-filled by the core of the new architecture (Yli-Jyra?,2012):1.
A mature finite-state dependency parser mustbe robust.
The outputs should not be restrictedto complete grammatical parses.
For exam-ple, Oflazer (2003) builds fragmental parses butlater drops those fragmental parses for whichthere are alternative parses with fewer frag-ments.
However, his approach handles onlygap-free bottom-up fragments and optimizesthe number of fragments by a counting methodwhose capacity is limited.2.
Besides robustness, a wide-coverage parsershould be able to assign reasonably well-motivated syntactic categories to every word inthe input.
This amounts to having a morpho-logical guesser and an adequate morphology-syntax interface.
Most prior work trivializesthe complexity of the interface, being compara-ble to Gaifman?s (1965) legacy formalism thatis mathematically elegant but based on word-form lists.
A good interface formalism is pro-vided, e.g., by Constraint Grammar parsers(Karlsson et al, 1995) where syntactic rulescan refer to morphological features.
Oflazer(2003) tests morphological features in compli-cated regular expressions.
The state complexityof the combination of such expressions is, how-ever, a potential problem if many more ruleswould be added to the system.This paper makes two main contributions:1.
It adapts Gaifman?s elegant formalism to the re-quirements of morphologically rich languages.With the adapted formalism, grammar writingbecomes easier.
However, efficient implemen-tation of the rule lookup becomes inherentlytrickier because testing several morphologicalconditions in parallel increases the size of thefinite-state automata.
Fortunately, the new for-malism comes with an efficient implementationthat keeps the finite-state representation of therule set as elegant as possible.2.
The paper introduces a linguistically motivatedranking for complete trees.
According to it, atree is better than another tree if a larger propor-tion of its dependency links is motivated by thelinguistic rules.
In contrast to Oflazer (2003),our method counts the number of links neededto connect the fragments into a spanning tree.Moreover, since such additional links are in-deed included in the parses, the ranking methodturns a grammar parser into a robust text parser.The paper is structured as follows.
The next sectionwill give an overview of the new parser architecture.After it, we present the new morphology-syntax in-terface in Section 3 and the parse ranking method inSection 4.
The paper ends with theoretical evalua-tion and discussion about the proposed formalism inSection 5.2 The General Design2.1 The Internal Linguistic RepresentationWe need to define a string-based representation forthe structures that are processed by the parser.
Forthis purpose, we encode the dependency trees andthen augment the representation with morphologicalfeatures.Dependency brackets encode dependency linksbetween pairs of tokens that are separated by an (im-plicit) token boundary.
The four-token string abcdhas 12 distinct undirected unlabeled dependencybracketings a((()b)c)d, a((b())c)d, a(()b()c)d,a(()bc())d, a(()b)c()d, a(b(()c))d, a(b(c()))d,a(b()c())d, a(b())c()d, a()b(()c)d, a()b(c())d,a()b()c()d.1The basic dependency brackets extend with labelssuch as in (LBL LBL) and directions such as in <LBLLBL\ and in /LBL LBL>.
Directed dependency linksdesignate one of the linked words as the head andanother as the dependent.
The extended brackets letus encode a full dependency tree in a string formatas indicated in (2).2 The dependent word of each1Dependency bracketing differs clearly from binary phrase-structure bracketings that put brackets around phrases: thestring abcd has only five distinct bracketings ((ab)(cd)),(((ab)c)d), ((a(bc))d), (a((bc)d)), and (a(b(cd))).2The syntactic labels used in this paper are: AG=Agent,by=Preposition ?by?
as a phrasal verb complement,D=Determiner, EN=Past Participle, FP=Final Punctuation,P=adjunctive preposition, PC=Preposition Complement,S=Subject, sgS=Singular Subject.109link is indicated in trees with an arrowhead but inbracketing with an angle bracket.
(2) it<SwasS\ /FP/ENinspiredEN> /AGbyAG> /PCthe<DwritingsD\PC> FP>.S ENGPCDFPIn Table 1, the dependency bracketing is com-bined with a common textual format for morpho-logical analyses.
In this format, the base forms aredefined over the alphabet of orthographical symbols?
whereas the morphological symbols and syntacticcategories are multi-character symbols that belong,respectively, to the alphabets ?
and ?.
In addition,there is a token boundary symbol #.Table 1: One morpho-syntactic analysis of a sentence1 i t PRON NOM SG3 <S #2 b e V PAST SG13 S\ /FP /EN #3 i n s p i r e EN EN> /AG #4 b y PREP AG> /PC #5 t h e DET SG/PL <D #6 w r i t i n g N NOM PL D\ PC> #7 .
PUNCT FP> #Depending on the type of the languages, one or-thographical word can be split into several parts suchas the inflectional groups in Turkish (Oflazer, 2003).In this case, a separate word-initial token boundarycan be used to separate such parts into lines of theirown.The current dependency bracketing captures pro-jective and weakly non-projective (1-planar) treesonly, but an extended encoding for 2-planar andmulti-planar dependency trees seems feasible (Yli-Jyra?, 2012):2.2 The Valid TreesWe are now going to define precisely the semanticsof the syntactic grammar component using finite-state relations.The finite-state languages will be defined over afinite alphabet ?
and they include all finite subsets ofthe universal language ??.
The (binary) finite-staterelations are defined over ??
and include all finitesubsets of ??
?
??.
In addition, they are closed un-der the operations over finite-state languages L andM and finite-state relations R and S according toTable 2.
The language relation Id(L) restricts theidentity relation to a language L. The compositionof language relations corresponds to the intersectionof their languages.Table 2: The relevant closure propertieslanguage relation meaningLM RS concatenationL?
R?
(Kleene) starL+ R+ (Kleene) plusL ?M R ?
S unionId(L) language relationId?1(R) L for R = Id(L)L?M Id(L)?Id(M) set differenceL?M cross productR|L input restrictionR ?
S compositionR?1 inverseProj1(R) Id(the input side of R)Proj2(R) Id(the output side of R)For notational convenience, the empty string isdenoted by ?.
A string x is identified with the sin-gleton set {x}.The syntactic component of the grammar definesa set of parse strings where the bracketing is a validdependency tree.
In these parses, there is no mor-phological information.
One way to express the setis to intersect a set of constraints as in (Yli-Jyra?,2005).
However, the contracting dependency parserexpresses the Id relation of the set through a compo-sition of simple finite-state relations:Synt = Proj1(Abst ?R ?
... ?R?
??
?t?Root), (1)Root = Id(#).
(2)In (1), Abst is a relation that removes all non-syntactic information from the strings,Abst = (Id(?)
?
Id(#) ?
Delete)?, (3)Delete = {(x, ?)
| x ?
?
??
}, (4)110and R is a relation that performs one layer of con-tractions in dependency bracketing.R = (Id(?)
?
Id(#) ?
Left ?
Right)?, (5)Left = {(<?
# ?\, ?)
| <?, ?\ ?
?
}, (6)Right = {(/?
# ?>, ?)
| /?, ?> ?
?}.
(7)The parameter t determines the maximum numberof layers of dependency links in the validated brack-etings.
The limit of Synt as t approaches ?
is notnecessarily a finite-state language, but it remainscontext-free because only projective trees are as-signed to the sentences.2.3 The Big PictureWe are now ready to embed the contraction basedgrammar into the bigger picture.Let x ?
??
be an orthographical string to beparsed.
Assume that it is segmented into n tokens.The string x is parsed by composition of four rela-tions: the relation {(x, x)}, the lexical transducer(Morph), the morphology-syntax interface (Iface),and the syntactic validator Synn?1.Parses(x) = Id(x) ?Morph ?
Iface ?
Synn?1.
(8)The language relation Proj2(Parses(x)) encodesthe parse forest of the input x.In practice, the syntactic validator Synn?1 cannotbe compiled into a finite-state transducer due to itslarge state complexity.
However, when each copy ofthe contracting transducer R in (1) is restricted byits admissible input-side language, a compact rep-resentation for the input-side restriction (Synn?1)|Xwhere X = Proj2(Id(x)?Morph?Iface) is computedefficiently as described in (Yli-Jyra?, 2012).3 The Grammar FormalismIn the parser, the linguistic knowledge is organizedinto Morph (the morphology) and Iface (the lexical-ized morphology-syntax interface), while Syn hasmainly a technical role as a tree validator.
Imple-menting the morphology-syntax interface is far froman easy task since it is actually the place that lexical-izes the whole syntax.3.1 Gaifman?s Dependency RulesGaifman?s legacy notation (Gaifman, 1965; Hays,1964) for dependency grammars assigns word formsto a finite number of potential morpho-syntactic cat-egories that relate word forms to their syntactic func-tions.
The words of particular categories are thenrelated by dependency rules:X0(Xp, .
.
.
, X?1,*, X1, .
.
.
, Xm).
(9)The rule (9) states that a word in category X0 is thehead of dependent words in categories Xp, .
.
.
, X?1before it and words in categories X1, .
.
.
, Xm afterit, in the given order.
The rule expresses, in a cer-tain sense, the frame or the argument structure of theword.
Rule X(*) indicates that the word in categoryX can occur without dependents.In addition, there is a root rule *(X) that statesthat a word in category X can occur independently,that is, as the root of the sentence.In the legacy notation, the distinction betweencomplements and adjuncts is not made explicit, asboth need to be listed as dependents.
To compact thenotation, we introduce optional dependents that willbe indicated by categories Xp?, .
.
.
, X?1?
and cat-egories X1?, .
.
.
, Xm?.
This extension potentiallysaves a large number of rules in cases where sev-eral dependents are actually adjuncts, some kinds ofmodifiers.33.2 The Decomposed CategoriesIn practice, atomic morpho-syntactic categories areoften too coarse for morphological description buttoo refined for convenient description of syntacticframes.
A practical description requires a more ex-pressive and flexible formalism.In our new rule formalism, each morpho-syntacticcategory X is viewed as a combination of a morpho-logical category M (including the information onthe lexical form of the word) and a syntactic cate-gory S. The morphological category M is a stringof orthographical and morphological feature labelswhile S is an atomic category label.The morphological category M0 and the syntacticcategory S0 are specified for the head of each de-pendency rule.
Together, they specify the morpho-syntactic category (M0, S0).
In contrast, the rulespecifies only the syntactic categories Sp, .
.
.
, S?1,3Optional dependents may be a worthwhile extension evenin descriptions that treat the modified word as a complement ofa modifier.111and S1, .
.
.
, Sm of the dependent words and thusdelegates the selection of the morphological cate-gories to the respective rules of the dependent words.The categories Sp, .
.
.
, S?1, and S1, .
.
.
, Sm mayagain be marked optional with the question mark.The rules are separated according to the directionof the head dependency.
Rules (10), (11) and (12)attach the head to the right, to the left, and in any di-rection, respectively.
In addition, the syntactic cate-gory of the root is specified with a rule of the form(13).?
S0(Sp, .
.
.
, S?1,*[M0], S1, .
.
.
, Sm), (10)?
S0(Sp, .
.
.
, S?1,*[M0], S1, .
.
.
, Sm), (11)S0(Sp, .
.
.
, S?1,*[M0], S1, .
.
.
, Sm), (12)*(S0).
(13)The interpretations of rules (10) - (12) are similar torule (9), but the rules are lexicalized and directed.The feature string M0 ?
(??%??
?
??)??
definesthe relevant head word forms using the features pro-vided by Morph.
The percent symbol (%) stands forthe unspecified part of the lexical base form.The use of the extended rule formalism is illus-trated in Table 3.
According to the rules in the table,a phrase headed by preposition by has three uses:an adjunctive preposition (P), the complement of aphrasal verb (by), or the agent of a passive verb (AG).Note that the last two uses correspond to a fully lexi-calized rule where the morphological category spec-ifies the lexeme.
The fourth rule illustrates how mor-phological features are combined in N NOM SG andthen partly propagated to the atomic name of thesyntactic category.Table 3: Extended Gaifman rules1 P (*[% PREP], PC) % prepos.2 by (*[b y PREP], PC) % phrasal3 AG (*[b y PREP], PC) % agent4 sgS (D?, M?, *[% N NOM SG], M?)
% noun3.3 Making a Gaifman Grammar RobustDependency syntax describes complete trees whereeach node is described by one of the dependencyrules.
Sometimes, however, no complete tree for aninput is induced by the linguistically motivated de-pendency rules.
In these cases, only tree fragmentscan be motivated by the linguistic knowledge.
Toglue the fragments together, we interpret the rootsof fragments as linear successors ?
thus dependents?
for the word that immediately precedes the frag-ment.The link to a linear successor is indicated with aspecial category ++ having a default rule ++(*).
Sinceany word can act as a root of a fragment, every wordis provided with this potential category.
In addi-tion, there is, for every rule (12), an automatic rule++(Sp, .
.
.
, S?1,*[M ], S1, .
.
.
, Sm) that allows theroots of the fragments to have the corresponding de-pendents.
Similar automatic rules are defined for thedirected rules.The category ++ is used to indicate dependentwords that do not have any linguistically motivatedsyntactic function.
The root rule *(++) states thatthis special category can act as the root of the wholedependency tree.
In addition to the root function ex-pressed by that rule, an optional dependent ++?
isappended to the end of every dependency rule.
Thisconnects fragments to their left contexts.With the above extensions, all sentences will haveat least one complete tree as a parse.
A parse withsome dependents of the type ++ are linguistically in-ferior to parses that do not have such dependents orhave fewer of them.
Removing such inferior analy-ses from the output of the parser is proposed in Sec-tion 4.3.4 The Formal Semantics of the InterfaceLet there be r dependency rules.
For each rule i,i ?
{1, ..., r} of type (10), letFi = M0, (14)Gi = S?1\ .
.
.
Sp\ S0> /Sm.../S1, (15)where S?1\, .
.
.
, Sp\, S0>, /Sm, .
.
.
, /S1 ?
?.
Foreach rule of type (11), S0> in (15) is replaced with<S0.
Rules with optional dependents are expandedinto subrules, and every undirected rule (12) splitsinto two directed subrules.In (16), Iface is a finite-state relation that injectsdependency brackets to the parses according to the112dependency rules.Iface = Intro ?
Chk, (16)Intro = (Id(????)(????
)Id(#))?, (17)Chk = Proj1(Match ?
Rules), (18)Rules = Id (?ri=1FiGi#)?
.
(19)Match = (Id(??)
Mid Id(??)
Tag?
Id(#))?
(20)Mid = Id(?)
?
(??
?%), (21)Tag = Id(?)
?
(???).
(22)Iface is the composition of relations Intro and Chk.Relation Intro inserts dependency brackets betweenthe morphological analysis of each token and the fol-lowing token boundary.
Relation Chk verifies thatthe inserted brackets are supported by dependencyrules that are represented by relation Rules.In order to allow generalizations in the specifi-cation of morphological categories, the relation In-tro does not match dependency rules directly, butthrough a filter.
This filter, Match, optionally re-places the middle part of each lexeme with % and ar-bitrary morphological feature labels with the emptystring.In addition to the dependency rules, we need todefine the semantics of the root rules.
Let H be theset of the categories having a root rule.
The categoryof the root word will be indicated in the dependencybracketing as an unmatched bracket.
It is checked byrelation Root = Id(H#) that replaces Root = Id(#)in the composition formulas (1) .3.5 An Efficient ImplementationThe definition of Iface gives rise to a naive parserimplementation that is based on the formulaParses(x) = MIx ?
Chk ?
Synn?1, (23)MIx = Id(x) ?Morph ?
Intro.
(24)The naive implementation is inefficient in practice.The main efficiency problem is that the state com-plexity of relation Chk can be exponential to thenumber of rules.
To avoid this, we replace it withChkx, a restriction of Chk.
This restriction is com-puted lazily when the input is known.Parses(x) = MIx ?
Chkx ?
Synn?1, (25)Chkx = Proj1(Matchx?Rules) (26)Matchx = Proj2(MIx) ?Match.
(27)In this improved method, the application of Iface de-mands only linear space according to the number ofrules.
This method is also fast to apply to the input,as far as the morphology-syntax interface is con-cerned.
Meanwhile, one efficient implementation ofSynn?1 is already provided in (Yli-Jyra?, 2012).4 The Most Specific ParseThe parsing method of (Yli-Jyra?, 2012) builds theparse forest efficiently using several transducers,but there is no guarantee that the whole set ofparses could be extracted efficiently from the com-pact representation constructed during the recogni-tion phase.
We will now assume, however, thatthe number of parses is, in practice, substantiallysmaller than in the theoretically possible worst case.Moreover, it is even more important to assume thatthe set of parses is compactly packed into a finite au-tomaton.
These two assumptions let us proceed byrefining the parse forest without using weights suchas in (Yli-Jyra?, 2012).In the following, we restrict the parse forest tothose parses that have the smallest number of ?linearsuccessor?
dependencies (++).
The number of suchdependencies is compared with a finite-state relationCp ?
(??{#})??(??{#})?
constructed as follows:??
= ??
{++>}, (28)Cp = Mapi?(Id(++>?)(?
?++>)+)?Map?1i , (29)Mapi = (Id(++>) ?
(????))?.
(30)In practice, the reduction of the parse forest is pos-sible only if the parse forest Proj2(Parses(x)) is rec-ognized by a sufficiently small finite-state automa-ton that can then be operated in Formula (33).
Theparses that minimize the number of ?linear succes-sor?
dependencies are obtained as the output of therelation Parses?(x).Parses?
(x) = MIx ?
Chkx ?
Tx,1, (31)Tx,0 = Proj2(Parses(x)), (32)Tx,1 = Tx,0 ?
Proj2(Tx,0 ?
Cp ?
Tx,0).
(33)This restriction technique could be repeatedly ap-plied to further levels of specificity.
For example,lexically motivated complements could be preferredover adjuncts and other grammatically possible de-pendents.1135 Evaluation and Discussion5.1 EleganceWe have retained most of the elegancy in thecontracting finite-state dependency parser (Yli-Jyra?,2012).
The changes introduced in this paper aremodular and implementable with standard opera-tions on finite-state transducers.Our refined design for a parser can be imple-mented largely in similar lines as the general ap-proach (Yli-Jyra?, 2012) up to the point when theparses are extracted from the compact parse forest.Parsing by arc contractions is closely relatedto the idea of reductions with restarting automata(Pla?tek et al, 2003).5.2 CoverageThe representation of the parses can be extended tohandle word-internal token boundaries, which facil-itates the adequate treatment of agglutinative lan-guages, cf.
(Oflazer, 2003).The limit for nested brackets is based on the psy-cholinguistic reality (Miller, 1956; Kornai and Tuza,1992) and the observed tendency for short depen-dencies (Lin, 1995; Eisner and Smith, 2005) in nat-ural language.The same general design can be used to producenon-projective dependency analyses as required bymany European languages.
The crossing dependen-cies can be assigned to two or more planes as sug-gested in (Yli-Jyra?, 2012).
2-planar bracketing al-ready achieves very high recall in practice (Go?mez-Rodr?
?guez and Nivre, 2010).5.3 Ambiguity ManagementOflazer (2003) uses the lenient composition opera-tion to compute the number of bottom-up fragmentsin incomplete parses.
The current solution improvesabove this by supporting gapped fragments and un-restricted counting of the graph components.Like in another extended finite-state approach(Oflazer, 2003), the ambiguity in the output of ourparsing method can be reduced by removing parseswith high total link length and by applying filtersthat enforce barrier constraints to the dependencylinks.5.4 Computational ComplexityThanks to dynamically applied finite-state opera-tions and the representation of feature combinationsas strings rather than regular languages, the depen-dency rules can be compiled quickly into the trans-ducers used by the parser.
For example, the actualspecifications of dependency rules are now com-piled into a linear-size finite-state transducer, Chk.The proposed implementation for the morphology-syntax interface is, thus, a significant improvementin comparison to the common approach that com-piles and combines replacement rules into a singletransducer where the morphological conditions ofthe rules are potentially mixed in a combinatorialmanner.Although we have started to write an experimentalgrammar, we do not exactly know how many rulesa mature grammar will contain.
Lexicalization ofthe rules will increase the number of rules signifi-cantly.
The number of syntactic categories will in-crease even more if complements are lexicalized.5.5 RobustnessIn case the grammar does not fully disambiguate orbuild a complete dependency structure, the parsershould be able to build and produce a partial anal-ysis.
(In interactive treebanking, it would be usefulif an additional knowledge source, e.g.
a human, canbe used to provide additional information to help theparser carry on the analysis to a complete structure.
)The current grammar system indeed assumes thatit can build complete trees for all input sentences.This assumption is typical for all generative gram-mars, but seems to contradict the requirement of ro-bustness.
To support robust parsing, we have nowproposed a simple technique where partial analysesare connected into a tree with the ?linear succes-sor?
links.
The designed parser tries its best to avoidthese underspecific links, but uses the smallest pos-sible number of them to connect the partial analysesinto a tree if more grammatical parses are not avail-able.5.6 Future WorkAlthough Oflazer (2003) does not report significantproblems with long sentences, it may be difficult toconstruct a single automaton for the parse forest of a114sentence that contains many words.
In the future,a more efficient method for finding the most spe-cific parse from the forest can be worked out us-ing weighted finite-state automata.
Such a methodwould combine the approaches of the companion pa-per (Yli-Jyra?, 2012) and the current paper.It seems interesting to study further how the speci-ficity reasoning and statistically learned weightscould complement each other in order to find thebest analyses.
Moreover, the parser can be modifiedin such a way that debugging information is pro-duced.
This could be very useful, especially whenlearning contractions that handle the crossing depen-dencies of non-projective trees.A dependency parser should enable the buildingof multiple types of analyses, e.g.
to account forsyntactic and semantic dependencies.
Also addingmore structure to the syntactic categories could beuseful.6 ConclusionsThe current theoretical work paves the way for a fullparser implementation.
The parser should be able tocope with large grammars to enable efficient devel-opment, testing and application cycles.The current work has sketched an expressive andcompact formalism and its efficient implementationfor the morphology-syntax interface of the contract-ing dependency parser.
In addition, the work haselaborated strategies that help to make the grammarmore robust without sacrificing the optimal speci-ficity of the analysis.AcknowledgmentsThe research has received funding from theAcademy of Finland under the grant agreement #128536 and the FIN-CLARIN project, and from theEuropean Commission?s 7th Framework Programunder the grant agreement # 238405 (CLARA).ReferencesJason Eisner and Noah A. Smith.
2005.
Parsing withsoft and hard constraints on dependency length.
InProceedings of the International Workshop on ParsingTechnologies (IWPT), pages 30?41, Vancouver, Octo-ber.David Elworthy.
2000.
A finite state parser with de-pendency structure output.
In Proceedings of Sixth In-ternational Workshop on Parsing Technologies (IWPT2000, Trento, Italy, February 23-25.
Institute for Sci-entific and Technological Research.Haim Gaifman.
1965.
Dependency systems andphrase-structure systems.
Information and Control,8:304?37.Carlos Go?mez-Rodr?
?guez and Joakim Nivre.
2010.
Atransition-based parser for 2-planar dependency struc-tures.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1492?-1501, Uppsala, Sweden, 11-16 July.David G. Hays.
1964.
Dependency theory: A formalismand some observations.
Language, 40:511?525.Fred Karlsson, Atro Voutilainen, Juha Heikkia?, andArto Anttila, editors.
1995.
Constraint Grammar:a Language-Independent System for Parsing Unre-stricted Text, volume 4 of Natural Language Process-ing.
Mouton de Gruyter, Berlin and New York.Andra?s Kornai and Zsolt Tuza.
1992.
Narrowness, path-width, and their application in natural language pro-cessing.
Discrete Applied Mathematics, 36:87?92.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof the Fourteenth International Joint Conference onArtificial Intelligence, IJCAI 95, Montre?al Que?bec,Canada, August 20-25, 1995, volume 2, pages 1420?1425.George A. Miller.
1956.
The magical number seven,plus or minus two: Some limits on our capacityfor processing information.
Psychological Review,63(2):343?355.Kemal Oflazer.
2003.
Dependency parsing with an ex-tended finite-state approach.
Computational Linguis-tics, 29(4):515?544.Martin Pla?tek, Marke?ta Lopatkova?, and Karel Oliva.2003.
Restarting automata: motivations and applica-tions.
In M. Holzer, editor, Workshop ?Petrinetze?
and13.
Theorietag ?Formale Sprachen und Automaten?,pages 90?96, Institut fu?r Informatik, Technische Uni-versita?t Mu?nchen.Anssi Yli-Jyra?.
2005.
Approximating dependencygrammars through intersection of star-free regular lan-guages.
International Journal of Foundations of Com-puter Science, 16(3).Anssi Yli-Jyra?.
2012.
On dependency analysis via con-tractions and weighted FSTs.
In Diana Santos, KristerLinde?n, and Wanjiku Ng?ang?a, editors, Shall we Playthe Festschrift Game?
Essays on the Occasion of LauriCarlson?s 60th Birthday.
Springer-Verlag, Berlin.115
