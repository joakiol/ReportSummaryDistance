Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645?648,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Simple Approach for HPSG Supertagging Using Dependency InformationYao-zhong Zhang ?
Takuya Matsuzaki ??
Department of Computer Science, University of Tokyo?
School of Computer Science, University of Manchester?National Centre for Text Mining, UK{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jpJun?ichi Tsujii??
?AbstractIn a supertagging task, sequence labelingmodels are commonly used.
But their lim-ited ability to model long-distance informa-tion presents a bottleneck to make further im-provements.
In this paper, we modeled thislong-distance information in dependency for-malism and integrated it into the process ofHPSG supertagging.
The experiments showedthat the dependency information is very in-formative for supertag disambiguation.
Wealso evaluated the improved supertagger in theHPSG parser.1 IntroductionSupertagging is a widely used speed-up techniquefor lexicalized grammar parsing.
It was firstproposed for lexicalized tree adjoining grammar(LTAG) (Bangalore and Joshi, 1999), then extendedto combinatory categorial grammar (CCG) (Clark,2002) and head-driven phrase structure grammar(HPSG) (Ninomiya et al, 2006).
For deep parsing,supertagging is an important preprocessor: an ac-curate supertagger greatly reduces search space ofa parser.
Not limited to parsing, supertags can beused for NP chunking (Shen and Joshi, 2003), se-mantic role labeling (Chen and Rambow, 2003) andmachine translation (Birch et al, 2007; Hassan etal., 2007) to explore rich syntactic information con-tained in them.Generally speaking, supertags are lexical tem-plates extracted from a grammar.
These templatesencode possible syntactic behavior of a word.
Al-though the number of supertags is far larger than the45 POS tags defined in Penn Treebank, sequence la-beling techniques are still effective for supertagging.Previous research (Clark, 2002) showed that a POSsequence is very informative for supertagging, andsome extent of local syntactic information can becaptured by the context of surrounding words andPOS tags.
However, since the context windowlength is limited for the computational cost reasons,there are still long-range dependencies which are noteasily captured in sequential models (Zhang et al,2009).
In practice, the multi-tagging technique pro-posed by Clark (2002) assigned more than one su-pertag to each word and let the ambiguous supertagsbe selected by the parser.
As for other NLP applica-tions which use supertags, resolving more supertagambiguities in supertagging stage is preferred.
Withthis consideration, we focus on supertagging andaim to make it as accurate as possible.In this paper, we incorporated long-distance in-formation into supertagging.
First, we used depen-dency parser formalism to model long-distance re-lationships between the input words, which is hardto model in sequence labeling models.
Then, wecombined the dependency information with localcontext in a simple point-wise model.
The experi-ments showed that dependency information is veryinformative for supertagging and we got a compet-itive 93.70% on supertagging accuracy (fed goldenPOS).
In addition, we also evaluated the improvedsupertagger in the HPSG parser.2 HPSG Supertagging and Dependency2.1 HPSG SupertagsHPSG (Pollard and Sag, 1994) is a lexicalist gram-mar framework.
In HPSG, a large number oflexical entries is used to describe word-specificsyntactic characteristics, while only a small num-ber of schemas is used to explain general con-struction rules.
These lexical entries are called?HPSG supertags?.
For example, one possi-ble supertag for the word ?like?
is written like?
[NP.nom<V.bse>NP.acc] lxm?, which indicates645the head syntactic category of ?like?
is verb in baseform.
It has a NP subject and a NP complement.With such fine-grained grammatical type distinc-tions, the number of supertags is much larger thanthe number of tags used in other sequence labelingtasks.
The HPSG grammar used in our experimentincludes 2,308 supertags.
This increases computa-tional cost of sequence labeling models.2.2 Why Use Dependency in SupertaggingBy analyzing the internal structure of the supertags,we found that subject and complements are two im-portant syntactic properties for each supertag.
Ifwe could predict subject and complements of theword well, supertagging would be an easier job todo.
However, current widely used sequence labelingmodels have the limited ability to catch these long-distance syntactic relations.
In supertagging stage,tree structures are still not constructed.
Dependencyformalism is an alternative way to describe these twosyntactic properties.
Based on this observation, wethink dependency information could assist supertagprediction.Figure 1: Model structure of incorporating dependencyinformation into the supertagging stage.
Dotted arrowsdescribe the augmented long distance dependency infor-mation provided for supertag prediction.3 Our Method3.1 Modeling Dependency for SupertagsFirst of all, we need to characterize the dependencybetween words for supertagging.
Since exact de-pendency locations are not encoded in supertags, tomake use of state-of-the-art dependency parser, werecover HPSG supertag dependencies with the aidof HPSG treebanks.
The dependencies are extractedfrom each branch in the HPSG trees by regardingthe non-head daughter as the modifier of the head-daughter.
HPSG schemas are expressed in depen-dency arcs.To model the dependency, we follow mainstreamdependency parsing formalism.
Two representa-tive methods for dependency parsing are transition-based model like MaltParser (Nivre, 2003) andgraph-based model like MSTParser1 (McDonald etal., 2005).
Previous research (Nivre and McDon-ald, 2008) showed that MSTParser is more accuratethan MaltParser for long dependencies.
Since ourmotivation is to capture long-distance dependencyas a complement for local supertagging models, weuse the projective MSTParser formalism to modeldependencies.
{(pi ?
pj)&sj |(j, i) ?
E}MOD-IN {(pi ?
wj)&sj|(j, i) ?
E}{(wi ?
pj)&sj|(j, i) ?
E}{(wi ?
wj)&sj |(j, i) ?
E}{(pi ?
pj)&si|(i, j) ?
E}MOD-OUT {(pi ?
wj)&si|(i, j) ?
E}{(wi ?
pj)&si|(i, j) ?
E}{(wi ?
wj)&si|(i, j) ?
E}Table 1: Non-local feature templates used for super-tagging.
Here, p, w and s represent POS, wordand schema respectively.
Direction (Left/Right) fromMODIN/MODOUTword to the current word is also con-sidered in the feature templates.3.2 Integrating Dependency into SupertaggingThere are several ways to combine long-distancedependency into supertagging.
Integrating depen-dency information into training process would bemore intuitive.
Here, we use feature-based integra-tion.
The base model is a point-wise averaged per-ceptron (PW-AP) which has been shown very ef-fective (Zhang et al, 2009).
The improved modelstructure is described in Figure 1.
The long-distanceinformation is formalized as first-order dependency.For the word being predicted, we extract its modi-fiers (MODIN) and its head (MODOUT) (Table 1)based on first-order dependency arcs.
Then MODINand MODOUT relations are combined as featureswith local context for supertag prediction.
To com-pare with previous work, the basic local context fea-tures are the same as in Matsuzaki et al (2007).1http://sourceforge.net/projects/mstparser/6464 ExperimentsWe evaluated dependency-informed supertagger(PW-DEP) both by supertag accuracy 2 and by aHPSG parser.
The experiments were conducted onWSJ-HPSG treebank (Miyao, 2006).
Sections 02-21 were used to train the dependency parser, thedependency-informed supertagger and the HPSGparser.
Section 23 was used as the testing set.
Theevaluation metric for HPSG parser is the accuracyof predicate-argument relations in the parser?s out-put, as in previous work (Sagae et al, 2007).Model Dep Acc%?
Acc%PW-AP / 91.14PW-DEP 90.98 92.18PW-AP (gold POS) / 92.48PW-DEP (gold POS) 92.05 93.70100 97.43Table 2: Supertagging accuracy on section 23.
(?
)Dependencies are given by MSTParser evaluated withlabeled accuracy.
PW-AP is the baseline point-wiseaveraged perceptron model.
PW-DEP is point-wisedependency-informed model.
The automatically taggedPOS tags were given by a maximum entropy tagger with97.39% accuracy.4.1 Results on SupertaggingWe first evaluated the upper-bound of dependency-informed supertagging model, given gold standardfirst-order dependencies.
As shown in Table 2,with such long-distance information supertaggingaccuracy can reach 97.43%.
Comparing to point-wise model (PW-AP) which only used local con-text (92.48%), this absolute 4.95% gain indicatedthat dependency information is really informativefor supertagging.
When automatically predicted de-pendency relations were given, there still were ab-solute 1.04% (auto POS) and 1.22% (gold POS) im-provements from baseline PW-AP model.We also compared supertagging results with pre-vious works (reported on section 22).
Here wemainly compared the dependency-informed point-wise models with perceptron-based Bayes point ma-chine (BPM) plus CFG-filter (Zhang et al, 2009).To the best of our knowledge, these are the state-of-the-art results on the same dataset with gold POS2?UNK?
supertags are ignored in evaluation as previous.Figure 2: HPSG Parser F-score on section 23, given au-tomatically tagged POS.tags.
CFG-filtering can be considered as an al-ternative way of incorporating long-distance con-straints on supertagging results.
Although our base-line system was slightly behind (PW-AP: 92.16%vs.
BPM:92.53%), the final accuracies of grammati-cally constrained models were very close (PW-DEP:93.53% vs. BPM-CFG: 93.60%); They were not sta-tistically significantly different (P-value is 0.26).
Asthe result of oracle PW-DEP indicated, supertaggingaccuracy can be further improved with better depen-dency modeling (e.g., with a semi-supervised de-pendency parser), which makes it more extensibleand attractive than using CFG-filter after the super-tagging process.4.2 HPSG parsing resultsWe also evaluated the dependency-informed su-pertagger in a HPSG parser.
Considering the effi-ciency, we use the HPSG parser3 described by Ma-tsuzaki et al (2007).In practice, several supertag candidates are re-served for each word to avoid parsing failure.
Toevaluate the quality of the two supertaggers, we re-stricted the number of each word?s supertag candi-dates fed to the HPSG parser.
As shown in Figure 2,for the case when only one supertag was predictedfor each word, F-score of the HPSG parser usingdependency-informed supertagger is 5.06% higherthan the parser using the baseline supertagger mod-ule.
As the candidate number increased, the gap nar-rowed: when all candidates were given, the gainsgradually came down to 0.2%.
This indicated that3Enju v2.3.1, http://www-tsujii.is.s.u-tokyo.ac.jp/enju.647improved supertagger can optimize the search spaceof the deep parser, which may contribute to more ac-curate and fast deep parsing.
From another aspect,supertagging can be viewed as an interface to com-bine different types of parsers.As for the overall parsing time, we didn?t opti-mize for speed in current setting.
The parsing time4saved by using the improved supertagger (around6.0 ms/sen, 21.5% time reduction) can not compen-sate for the extra cost of MSTParser (around 73.8ms/sen) now.
But there is much room to improve thefinal speed (e.g., optimizing the dependency parserfor speed or reusing acquired dependencies for ef-fective pruning).
In addition, small beam-size can be?safely?
used with improved supertagger for speed.Using shallow dependencies in deep HPSG pars-ing has been previously explored by Sagae et al(2007), who used dependency constraints in schemaapplication stage to guide HPSG tree construction(F-score was improved from 87.2% to 87.9% witha single shift-reduce dependency parser).
Since thebaseline parser is different, we didn?t make a directcomparison here.
However, it would be interestingto compare these two different ways of incorporat-ing the dependency parser into HPSG parsing.
Weleft it as further work.5 ConclusionsIn this paper, focusing on improving the accu-racy of supertagging, we proposed a simple buteffective way to incorporate long-distance depen-dency relations into supertagging.
The experimentsmainly showed that these long-distance dependen-cies, which are not easy to model in traditional se-quence labeling models, are very informative for su-pertag predictions.
Although these were preliminaryresults, the method shows its potential strength forrelated applications.
Not limited to HPSG, it can beextended to other lexicalized grammar supertaggers.AcknowledgmentsThanks to the anonymous reviewers for valuablecomments.
We also thank Goran Topic for his self-less help.
The first author was supported by TheUniversity of Tokyo Fellowship (UT-Fellowship).4Tested on section 23 (2291 sentences) using an AMDOpteron 2.4GHz server, given all supertag candidates.This work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan).ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Super-tagging: An approach to almost parsing.
Computa-tional Linguistics, 25:237?265.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation.John Chen and Owen Rambow.
2003.
Use of deep lin-guistic features for the recognition and labeling of se-mantic arguments.
In Proceedings of EMNLP-2003.Stephen Clark.
2002.
Supertagging for combinatory cat-egorial grammar.
In Proceedings of the 6th Interna-tional Workshop on Tree Adjoining Grammars and Re-lated Frameworks (TAG+ 6), pages 19?24.Hany Hassan, Mary Hearne, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.In Proceedings of ACL 2007, pages 288?295.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Efficient hpsg parsing with supertagging andcfg-filtering.
In Proceedings of IJCAI-07.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL-05.Yusuke Miyao.
2006.
From Linguistic Theory to Syn-tactic Analysis: Corpus-Oriented Grammar Develop-ment and Feature Forest Model.
Ph.D. Dissertation,The University of Tokyo.Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsu-zaki, and Yusuke Miyao.
2006.
Extremely lexicalizedmodels for accurate and fast hpsg parsing.
In Proceed-ings of EMNLP-2006, pages 155?163.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-08: HLT.J.
Nivre.
2003.
An efficient algorithm for projective de-pendency parsing.
In Proceedings of IWPT-03, pages149?160.
Citeseer.Carl Pollard and Ivan A.
Sag.
1994.
Head-driven PhraseStructure Grammar.
University of Chicago / CSLI.Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii.
2007.Hpsg parsing with shallow dependency constraints.
InProceedings of ACL-07.Libin Shen and Aravind K. Joshi.
2003.
A snow basedsupertagger with application to np chunking.
In Pro-ceedings of ACL 2003, pages 505?512.Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi Tsu-jii.
2009.
Hpsg supertagging: A sequence labelingview.
In Proceedings of IWPT-09, Paris, France.648
