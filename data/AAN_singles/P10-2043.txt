Proceedings of the ACL 2010 Conference Short Papers, pages 231?235,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsOnline Generation of Locality Sensitive Hash SignaturesBenjamin Van DurmeHLTCOEJohns Hopkins UniversityBaltimore, MD 21211 USAAshwin LallCollege of ComputingGeorgia Institute of TechnologyAtlanta, GA 30332 USAAbstractMotivated by the recent interest in stream-ing algorithms for processing large textcollections, we revisit the work ofRavichandran et al (2005) on using theLocality Sensitive Hash (LSH) method ofCharikar (2002) to enable fast, approxi-mate comparisons of vector cosine simi-larity.
For the common case of featureupdates being additive over a data stream,we show that LSH signatures can be main-tained online, without additional approxi-mation error, and with lower memory re-quirements than when using the standardoffline technique.1 IntroductionThere has been a surge of interest in adapting re-sults from the streaming algorithms community toproblems in processing large text collections.
Theterm streaming refers to a model where data ismade available sequentially, and it is assumed thatresource limitations preclude storing the entiretyof the data for offline (batch) processing.
Statis-tics of interest are approximated via online, ran-domized algorithms.
Examples of text applica-tions include: collecting approximate counts (Tal-bot, 2009; Van Durme and Lall, 2009a), findingtop-n elements (Goyal et al, 2009), estimatingterm co-occurrence (Li et al, 2008), adaptive lan-guage modeling (Levenberg and Osborne, 2009),and building top-k ranklists based on pointwisemutual information (Van Durme and Lall, 2009b).Here we revisit the work of Ravichandran et al(2005) on building word similarity measures fromlarge text collections by using the Locality Sensi-tive Hash (LSH) method of Charikar (2002).
Forthe common case of feature updates being addi-tive over a data stream (such as when trackinglexical co-occurrence), we show that LSH signa-tures can be maintained online, without additionalapproximation error, and with lower memory re-quirements than when using the standard offlinetechnique.We envision this method being used in conjunc-tion with dynamic clustering algorithms, for a va-riety of applications.
For example, Petrovic et al(2010) made use of LSH signatures generated overindividual tweets, for the purpose of first story de-tection.
Streaming LSH should allow for the clus-tering of Twitter authors, based on the tweets theygenerate, with signatures continually updated overthe Twitter stream.2 Locality Sensitive HashingWe are concerned with computing the cosine sim-ilarity of feature vectors, defined for a pair of vec-tors ~u and ~v as the dot product normalized by theirlengths:cosine?similarity(~u,~v) =~u ?
~v|~u||~v|.This similarity is the cosine of the angle be-tween these high-dimensional vectors and attainsa value of one (i.e., cos (0)) when the vectors areparallel and zero (i.e., cos (pi/2)) when orthogo-nal.Building on the seminal work of Indyk andMotwani (1998) on locality sensitive hashing(LSH), Charikar (2002) presented an LSH thatmaps high-dimensional vectors to a much smallerdimensional space while still preserving (cosine)similarity between vectors in the original space.The LSH algorithm computes a succinct signatureof the feature set of the words in a corpus by com-puting d independent dot products of each featurevector ~v with a random unit vector ~r, i.e.,?i viri,and retaining the sign of the d resulting products.Each entry of ~r is drawn from the distributionN(0, 1), the normal distribution with zero meanand unit variance.
Charikar?s algorithm makes useof the fact (proved by Goemans and Williamson231(1995) for an unrelated application) that the an-gle between any two vectors summarized in thisfashion is proportional to the expected Hammingdistance of their signature vectors.
Hence, we canretain length d bit-signatures in the place of highdimensional feature vectors, while preserving theability to (quickly) approximate cosine similarityin the original space.Ravichandran et al (2005) made use of this al-gorithm to reduce the computation in searchingfor similar nouns by first computing signatures foreach noun and then computing similarity over thesignatures rather than the original feature space.3 Streaming AlgorithmIn this work, we focus on features that can bemaintained additively, such as raw frequencies.1Our streaming algorithm for this problem makesuse of the simple fact that the dot product of thefeature vector with random vectors is a linear op-eration.
This permits us to replace the vi ?
ri op-eration by vi individual additions of ri, once foreach time the feature is encountered in the stream(where vi is the frequency of a feature and ri is therandomly chosen Gaussian-distributed value asso-ciated with this feature).
The result of the finalcomputation is identical to the dot products com-puted by the algorithm of Charikar (2002), butthe processing can now be done online.
A simi-lar technique, for stable random projections, wasindependently discussed by Li et al (2008).Since each feature may appear multiple timesin the stream, we need a consistent way to retrievethe random values drawn from N(0, 1) associatedwith it.
To avoid the expense of computing andstoring these values explicitly, as is the norm, wepropose the use of a precomputed pool of ran-dom values drawn from this distribution that wecan then hash into.
Hashing into a fixed pool en-sures that the same feature will consistently be as-sociated with the same value drawn from N(0, 1).This introduces some weak dependence in the ran-dom vectors, but we will give some analysis show-ing that this should have very limited impact onthe cosine similarity computation, which we fur-ther support with experimental evidence (see Ta-ble 3).Our algorithm traverses a stream of words and1Note that Ravichandran et al (2005) used pointwise mu-tual information features, which are not additive since theyrequire a global statistic to compute.Algorithm 1 STREAMING LSH ALGORITHMParameters:m : size of poold : number of bits (size of resultant signature)s : a random seedh1, ..., hd : hash functions mapping ?s, fi?
to {0, .
.
.
,m?1}INITIALIZATION:1: Initialize floating point array P [0, .
.
.
,m?
1]2: Initialize H , a hashtable mapping words to floating pointarrays of size d3: for i := 0 .
.
.m?
1 do4: P [i] := random sample from N(0, 1), using s as seedONLINE:1: for each word w in the stream do2: for each feature fi associated with w do3: for j := 1 .
.
.
d do4: H[w][j] := H[w][j] + P [hj(s, fi)]SIGNATURECOMPUTATION:1: for each w ?
H do2: for i := 1 .
.
.
d do3: if H[w][i] > 0 then4: S[w][i] := 15: else6: S[w][i] := 0maintains some state for each possible word thatit encounters (cf.
Algorithm 1).
In particular, thestate maintained for each word is a vector of float-ing point numbers of length d. Each element of thevector holds the (partial) dot product of the featurevector of the word with a random unit vector.
Up-dating the state for a feature seen in the stream fora given word simply involves incrementing eachposition in the word?s vector by the random valueassociated with the feature, accessed by hash func-tions h1 through hd.
At any point in the stream,the vector for each word can be processed (in timeO(d)) to create a signature computed by checkingthe sign of each component of its vector.3.1 AnalysisThe update cost of the streaming algorithm, perword in the stream, is O(df), where d is the targetsignature size and f is the number of features asso-ciated with each word in the stream.2 This resultsin an overall cost of O(ndf) for the streaming al-gorithm, where n is the length of the stream.
Thememory footprint of our algorithm isO(n0d+m),where n0 is the number of distinct words in thestream and m is the size of the pool of normallydistributed values.
In comparison, the originalLSH algorithm computes signatures at a cost ofO(nf + n0dF ) updates and O(n0F + dF + n0d)memory, where F is the (large) number of unique2For the bigram features used in ?
4, f = 2.232features.
Our algorithm is superior in terms ofmemory (because of the pooling trick), and has thebenefit of supporting similarity queries online.3.2 Pooling Normally-distributed ValuesWe now discuss why it is possible to use afixed pool of random values instead of generatingunique ones for each feature.
Let g be the c.d.f.of the distribution N(0, 1).
It is easy to see thatpicking x ?
(0, 1) uniformly results in g?1(x) be-ing chosen with distribution N(0, 1).
Now, if weselect for our pool the valuesg?1(1/m), g?1(2/m), .
.
.
, g?1(1?
1/m),for some sufficiently large m, then this is identicalto sampling from N(0, 1) with the caveat that theaccuracy of the sample is limited.
More precisely,the deviation from sampling from this pool is offfrom the actual value by at mostmaxi=1,...,m?2{g?1((i+ 1)/m)?
g?1(i/m)}.By choosing m to be sufficiently large, we canbound the error of the approximate sample froma true sample (i.e., the loss in precision expressedabove) to be a small fraction (e.g., 1%) of the ac-tual value.
This would result in the same relativeerror in the computation of the dot product (i.e.,1%), which would almost never affect the sign ofthe final value.
Hence, pooling as above shouldgive results almost identical to the case where allthe random values were chosen independently.
Fi-nally, we make the observation that, for large m,randomly choosing m values from N(0, 1) resultsin a set of values that are distributed very similarlyto the pool described above.
An interesting avenuefor future work is making this analysis more math-ematically precise.3.3 ExtensionsDecay The algorithm can be extended to supporttemporal decay in the stream, where recent obser-vations are given higher relative weight, by mul-tiplying the current sums by a decay value (e.g.,0.9) on a regular interval (e.g., once an hour, oncea day, once a week, etc.
).Distributed The algorithm can be easily dis-tributed across multiple machines in order to pro-cess different parts of a stream, or multiple differ-ent streams, in parallel, such as in the context ofthe MapReduce framework (Dean and Ghemawat,(a)(b)Figure 1: Predicted versus actual cosine values for 50,000pairs, using LSH signatures generated online, with d = 32 inFig.
1(a) and d = 256 in Fig.
1(b).2004).
The underlying operation is a linear op-erator that is easily composed (i.e., via addition),and the randomness between machines can be tiedbased on a shared seed s. At any point in process-ing the stream(s), current results can be aggregatedby summing the d-dimensional vectors for eachword, from each machine.4 ExperimentsSimilar to the experiments of Ravichandran etal.
(2005), we evaluated the fidelity of signaturegeneration in the context of calculating distribu-tional similarity between words across a largetext collection: in our case, articles taken fromthe NYTimes portion of the Gigaword corpus(Graff, 2003).
The collection was processed as astream, sentence by sentence, using bigram fea-233d 16 32 64 128 256SLSH 0.2885 0.2112 0.1486 0.1081 0.0769LSH 0.2892 0.2095 0.1506 0.1083 0.0755Table 1: Mean absolute error when using signatures gener-ated online (StreamingLSH), compared to offline (LSH).tures.
This gave a stream of 773,185,086 tokens,with 1,138,467 unique types.
Given the numberof types, this led to a (sparse) feature space withdimension on the order of 2.5 million.After compiling signatures, fifty-thousand?x, y?
pairs of types were randomly sampledby selecting x and y each independently, withreplacement, from those types with at least 10 to-kens in the stream (where 310,327 types satisfiedthis constraint).
The true cosine values betweeneach such x and y was computed based on offlinecalculation, and compared to the cosine similaritypredicted by the Hamming distance between thesignatures for x and y.
Unless otherwise specified,the random pool size was fixed at m = 10, 000.Figure 1 visually reaffirms the trade-off in LSHbetween the number of bits and the accuracy ofcosine prediction across the range of cosine val-ues.
As the underlying vectors are strictly posi-tive, the true cosine is restricted to [0, 1].
Figure 2shows the absolute error between truth and predic-tion for a similar sample, measured using signa-tures of a variety of bit lengths.
Here we see hori-zontal bands arising from truly orthogonal vectorsleading to step-wise absolute error values trackedto Hamming distance.Table 1 compares the online and batch LSH al-gorithms, giving the mean absolute error betweenpredicted and actual cosine values, computed forthe fifty-thousand element sample, using signa-tures of various lengths.
These results confirm thatwe achieve the same level of accuracy with onlineupdates as compared to the standard method.Figure 3 shows how a pool size as low as m =100 gives reasonable variation in random values,and that m = 10, 000 is sufficient.
When using astandard 32 bit floating point representation, thisis just 40 KBytes of memory, as compared to, e.g.,the 2.5 GBytes required to store 256 random vec-tors each containing 2.5 million elements.Table 2 is based on taking an example for eachof three part-of-speech categories, and reportingthe resultant top-5 words as according to approx-imated cosine similarity.
Depending on the in-tended application, these results indicate a rangeFigure 2: Absolute error between predicted and true co-sine for a sample of pairs, when using signatures of lengthlog2(d) ?
{4, 5, 6, 7, 8}, drawn with added jitter to avoidoverplotting.Pool SizeMeanAbsolute Error0.20.40.60.8 lll l l l l101 102 103 104 105Figure 3: Error versus pool size, when using d = 256.of potentially sufficient signature lengths.5 ConclusionsWe have shown that when updates to a feature vec-tor are additive, it is possible to convert the offlineLSH signature generation method into a stream-ing algorithm.
In addition to allowing for on-line querying of signatures, our approach leads tospace efficiencies, as it does not require the ex-plicit representation of either the feature vectors,nor the random matrix.
Possibilities for futurework include the pairing of this method with algo-rithms for dynamic clustering, as well as exploringalgorithms for different distances (e.g., L2) and es-timators (e.g., asymmetric estimators (Dong et al,2009)).234LondonMilan.97, Madrid.96, Stockholm.96, Manila.95, Moscow.95ASHER0, Champaign0, MANS0, NOBLE0, come0Prague1, Vienna1, suburban1, synchronism1, Copenhagen2Frankfurt4, Prague4, Taszar5, Brussels6, Copenhagen6Prague12, Stockholm12, Frankfurt14, Madrid14, Manila14Stockholm20, Milan22, Madrid24, Taipei24, Frankfurt25induring.99, on.98, beneath.98, from.98, onto.97Across0, Addressing0, Addy0, Against0, Allmon0aboard0, mishandled0, overlooking0, Addressing1, Rejecting1Rejecting2, beneath2, during2, from3, hamstringing3during4, beneath5, of6, on7, overlooking7during10, on13, beneath15, of17, overlooking17solddeployed.84, presented.83, sacrificed.82, held.82, installed.82Bustin0, Diors0, Draining0, Kosses0, UNA0delivered2, held2, marks2, seared2, Ranked3delivered5, rendered5, presented6, displayed7, exhibited7held18, rendered18, presented19, deployed20, displayed20presented41, rendered42, held47, leased47, reopened47Table 2: Top-5 items based on true cosine (bold), then usingminimal Hamming distance, given in top-down order whenusing signatures of length log2(d) ?
{4, 5, 6, 7, 8}.
Ties bro-ken lexicographically.
Values given as subscripts.AcknowledgmentsThanks to Deepak Ravichandran, Miles Osborne,Sasa Petrovic, Ken Church, Glen Coppersmith,and the anonymous reviewers for their feedback.This work began while the first author was at theUniversity of Rochester, funded by NSF grant IIS-1016735.
The second author was supported inpart by NSF grant CNS-0905169, funded underthe American Recovery and Reinvestment Act of2009.ReferencesMoses Charikar.
2002.
Similarity estimation tech-niques from rounding algorithms.
In Proceedingsof STOC.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapRe-duce: Simplified Data Processing on Large Clusters.In Proceedings of OSDI.Wei Dong, Moses Charikar, and Kai Li.
2009.
Asym-metric distance estimation with sketches for similar-ity search in high-dimensional spaces.
In Proceed-ings of SIGIR.Michel X. Goemans and David P. Williamson.
1995.Improved approximation algorithms for maximumcut and satisfiability problems using semidefiniteprogramming.
JACM, 42:1115?1145.Amit Goyal, Hal Daume?
III, and Suresh Venkatasub-ramanian.
2009.
Streaming for large scale NLP:Language Modeling.
In Proceedings of NAACL.David Graff.
2003.
English Gigaword.
LinguisticData Consortium, Philadelphia.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In Proceedings of STOC.Abby Levenberg and Miles Osborne.
2009.
Stream-based Randomised Language Models for SMT.
InProceedings of EMNLP.Ping Li, Kenneth W. Church, and Trevor J. Hastie.2008.
One Sketch For All: Theory and Applicationof Conditional Random Sampling.
In Advances inNeural Information Processing Systems 21.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.2010.
Streaming First Story Detection with appli-cation to Twitter.
In Proceedings of NAACL.Deepak Ravichandran, Patrick Pantel, and EduardHovy.
2005.
Randomized Algorithms and NLP:Using Locality Sensitive Hash Functions for HighSpeed Noun Clustering.
In Proceedings of ACL.David Talbot.
2009.
Succinct approximate counting ofskewed data.
In Proceedings of IJCAI.Benjamin Van Durme and Ashwin Lall.
2009a.
Proba-bilistic Counting with Randomized Storage.
In Pro-ceedings of IJCAI.Benjamin Van Durme and Ashwin Lall.
2009b.Streaming Pointwise Mutual Information.
In Ad-vances in Neural Information Processing Systems22.235
