Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 1?9,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsModeling Syntactic Context Improves Morphological SegmentationYoong Keok Lee Aria Haghighi Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{yklee, aria42, regina}@csail.mit.eduAbstractThe connection between part-of-speech (POS)categories and morphological properties iswell-documented in linguistics but underuti-lized in text processing systems.
This pa-per proposes a novel model for morphologi-cal segmentation that is driven by this connec-tion.
Our model learns that words with com-mon affixes are likely to be in the same syn-tactic category and uses learned syntactic cat-egories to refine the segmentation boundariesof words.
Our results demonstrate that incor-porating POS categorization yields substantialperformance gains on morphological segmen-tation of Arabic.
11 IntroductionA tight connection between morphology and syntaxis well-documented in linguistic literature.
In manylanguages, morphology plays a central role in mark-ing syntactic structure, while syntactic relationshelp to reduce morphological ambiguity (Harley andPhillips, 1994).
Therefore, in an unsupervised lin-guistic setting which is rife with ambiguity, model-ing this connection can be particularly beneficial.However, existing unsupervised morphologicalanalyzers take little advantage of this linguisticproperty.
In fact, most of them operate at the vo-cabulary level, completely ignoring sentence con-text.
This design is not surprising: a typical mor-phological analyzer does not have access to syntac-1The source code for the work presented in this paper isavailable at http://groups.csail.mit.edu/rbg/code/morphsyn/.tic information, because morphological segmenta-tion precedes other forms of sentence analysis.In this paper, we demonstrate that morphologicalanalysis can utilize this connection without assum-ing access to full-fledged syntactic information.
Inparticular, we focus on two aspects of the morpho-syntactic connection:?
Morphological consistency within POS cat-egories.
Words within the same syntactic cat-egory tend to select similar affixes.
This lin-guistic property significantly reduces the spaceof possible morphological analyses, ruling outassignments that are incompatible with a syn-tactic category.?
Morphological realization of grammaticalagreement.
In many morphologically rich lan-guages, agreement between syntactic depen-dents is expressed via correlated morphologicalmarkers.
For instance, in Semitic languages,gender and number agreement between nounsand adjectives is expressed using matching suf-fixes.
Enforcing mutually consistent segmen-tations can greatly reduce ambiguity of word-level analysis.In both cases, we do not assume that the relevantsyntactic information is provided, but instead jointlyinduce it as part of morphological analysis.We capture morpho-syntactic relations in aBayesian model that grounds intra-word decisionsin sentence-level context.
Like traditional unsuper-vised models, we generate morphological structurefrom a latent lexicon of prefixes, stems, and suffixes.1In addition, morphological analysis is guided by alatent variable that clusters together words with sim-ilar affixes, acting as a proxy for POS tags.
More-over, a sequence-level component further refines theanalysis by correlating segmentation decisions be-tween adjacent words that exhibit morphologicalagreement.
We encourage this behavior by encodinga transition distribution over adjacent words, usingstring match cues as a proxy for grammatical agree-ment.We evaluate our model on the standard Arabictreebank.
Our full model yields 86.2% accuracy,outperforming the best published results (Poon etal., 2009) by 8.5%.
We also found that modelingmorphological agreement between adjacent wordsyields greater improvement than modeling syntac-tic categories.
Overall, our results demonstrate thatincorporating syntactic information is a promisingdirection for improving morphological analysis.2 Related WorkResearch in unsupervised morphological segmenta-tion has gained momentum in recent years bring-ing about significant developments to the area.These advances include novel Bayesian formula-tions (Goldwater et al, 2006; Creutz and Lagus,2007; Johnson, 2008), methods for incorporat-ing rich features in unsupervised log-linear models(Poon et al, 2009) and the development of multilin-gual morphological segmenters (Snyder and Barzi-lay, 2008a).Our work most closely relates to approaches thataim to incorporate syntactic information into mor-phological analysis.
Surprisingly, the research inthis area is relatively sparse, despite multiple resultsthat demonstrate the connection between morphol-ogy and syntax in the context of part-of-speech tag-ging (Toutanova and Johnson, 2008; Habash andRambow, 2005; Dasgupta and Ng, 2007; Adlerand Elhadad, 2006).
Toutanova and Cherry (2009)were the first to systematically study how to in-corporate part-of-speech information into lemmati-zation and empirically demonstrate the benefits ofthis combination.
While our high-level goal is simi-lar, our respective problem formulations are distinct.Toutanova and Cherry (2009) have considered asemi-supervised setting where an initial morpholog-ical dictionary and tagging lexicon are provided butthe model also has access to unlabeled data.
Since alemmatizer and tagger trained in isolation may pro-duce mutually inconsistent assignments, and theirmethod employs a log-linear reranker to reconcilethese decisions.
This reranking method is not suit-able for the unsupervised scenario considered in ourpaper.Our work is most closely related to the approachof Can and Manandhar (2009).
Their method alsoincorporates POS-based clustering into morpholog-ical analysis.
These clusters, however, are learnedas a separate preprocessing step using distributionalsimilarity.
For each of the clusters, the model se-lects a set of affixes, driven by the frequency of theiroccurrences in the cluster.
In contrast, we modelmorpho-syntactic decisions jointly, thereby enablingtighter integration between the two.
This designalso enables us to capture additional linguistic phe-nomena such as agreement.
While this techniqueyields performance improvement in the context oftheir system, the final results does not exceed state-of-the-art systems that do not exploit this informa-tion (for e.g., (Creutz and Lagus, 2007)).3 ModelGiven a corpus of unannotated and unsegmentedsentences, our goal is to infer the segmentationboundaries of all words.
We represent segmen-tations and syntactic categories as latent variableswith a directed graphical model, and we performBayesian inference to recover the latent variables ofinterest.
Apart from learning a compact morphemelexicon that explains the corpus well, we also modelmorpho-syntactic relations both within each wordand between adjacent words to improve segmenta-tion performance.
In the remaining section, we firstprovide the key linguistic intuitions on which ourmodel is based before describing the complete gen-erative process.3.1 Linguistic IntuitionWhile morpho-syntactic interface spans a range oflinguistic phenomena, we focus on two facets of thisconnection.
Both of them provide powerful con-straints on morphological analysis and can be mod-eled without explicit access to syntactic annotations.2Morphological consistency within syntactic cate-gory.
Words that belong to the same syntactic cat-egory tend to select similar affixes.
In fact, the powerof affix-related features has been empirically shownin the task of POS tag prediction (Habash and Ram-bow, 2005).
We hypothesize that this regularity canalso benefit morphological analyzers by eliminat-ing assignments with incompatible prefixes and suf-fixes.
For instance, a state-of-the-art segmenter er-roneously divides the word ?Al{ntxAbAt?
into fourmorphemes ?Al-{ntxAb-A-t?
instead of three ?Al-{ntxAb-At?
(translated as ?the-election-s?.)
The af-fix assignment here is clearly incompatible ?
de-terminer ?Al?
is commonly associated with nouns,while suffix ?A?
mostly occurs with verbs.Since POS information is not available to themodel, we introduce a latent variable that encodesaffix-based clustering.
In addition, we consider avariant of the model that captures dependencies be-tween latent variables of adjacent words (analogousto POS transitions).Morphological realization of grammatical agree-ment.
In morphologically rich languages, agree-ment is commonly realized using matching suffices.In many cases, members of a dependent pair suchas adjective and noun have the exact same suf-fix.
A common example in the Arabic Treebankis the bigram ?Al-Df-p Al-grby-p?
(which is trans-lated word-for-word as ?the-bank the-west?)
wherethe last morpheme ?p?
is a feminine singular nounsuffix.Fully incorporating agreement constraints in themodel is difficult, since we do not have access tosyntactic dependencies.
Therefore, we limit our at-tention to adjacent words which end with similarstrings ?
for e.g., ?p?
in the example above.
Themodel encourages consistent segmentation of suchpairs.
While our string-based cue is a simple proxyfor agreement relation, it turns to be highly effectivein practice.
On the Penn Arabic treebank corpus, ourcue has a precision of around 94% at the token-level.3.2 Generative ProcessThe high-level generative process proceeds in fourphases:(a) Lexicon Model: We begin by generating mor-pheme lexicons L using parameters ?.
This setof lexicons consists of separate lexicons for pre-fixes, stems, and suffixes generated in a hierar-chical fashion.
(b) Segmentation Model: Conditioned on L, wedraw word types, their segmentations, and alsotheir syntactic categories (W ,S,T ).
(c) Token-POS Model: Next, we generate the un-segmented tokens in the corpus and their syn-tactic classes (w, t) from a standard first-orderHMM which has dependencies between adja-cent syntactic categories.
(d) Token-Seg Model: Lastly, we generate tokensegmentations s from a first-order Markov chainthat has dependencies between adjacent seg-mentations.The complete generative story can be summarizedby the following equation:P (w,s, t,W ,S,T ,L,?,?|?,?,?)
=P (L|?)
(a)P (W ,S,T ,?|L,?,?)
(b)Ppos(w, t,?|W ,S,T ,L,?)
(c)Pseg(s|W ,S,T ,L,?,?)
(d)where ?,?,?,?,?
are hyperparameters and pa-rameters whose roles we shall detail shortly.Our lexicon model captures the desirability ofcompact lexicon representation proposed by priorwork by using parameters ?
that favors small lexi-cons.
Furthermore, if we set the number of syntac-tic categories in the segmentation model to one andexclude the token-based models, we recover a seg-menter that is very similar to the unigram DirichletProcess model (Goldwater et al, 2006; Snyder andBarzilay, 2008a; Snyder and Barzilay, 2008b).
Weshall elaborate on this point in Section 4.The segmentation model captures morphologicalconsistency within syntactic categories (POS tag),whereas the Token-POS model captures POS tagdependencies between adjacent tokens.
Lastly, theToken-Seg model encourages consistent segmenta-tions between adjacent tokens that exhibit morpho-logical agreement.3Lexicon Model The design goal is to encouragemorpheme types to be short and the set of affixes(i.e.
prefixes and suffixes) to be much smaller thanthe set of stems.
To achieve this, we first draw eachmorpheme ?
in the master lexicon L?
according to ageometric distribution which assigns monotonicallysmaller probability to longer morpheme lengths:|?| ?
Geometric(?l)The parameter ?l for the geometric distribution isfixed and specified beforehand.
We then draw theprefix, the stem, and suffix lexicons (denoted byL?, L0, L+ respectively) from morphemes in L?.Generating the lexicons in such a hierarchical fash-ion allows morphemes to be shared among thelower-level lexicons.
For instance, once determiner?Al?
is generated in the master lexicon, it can beused to generate prefixes or stems later on.
To fa-vor compact lexicons, we again make use of a ge-ometric distribution that assigns smaller probabilityto lexicons that contain more morphemes:prefix: |L?| ?
Geometric(??
)stem: |L0| ?
Geometric(?0)suffix: |L+| ?
Geometric(?+)By separating morphemes into affixes and stems, wecan control the relative sizes of their lexicons withdifferent parameters.Segmentation Model The model independentlygenerates each word type using only morphemes inthe affix and stem lexicons, such that each wordhas exactly one stem and is encouraged to have fewmorphemes.
We fix the number of syntactic cate-gories (tags) to K and begin the process by generat-ing multinomial distribution parameters for the POStag prior from a Dirichlet prior:?T ?
Dirichlet(?T , {1, .
.
.
,K})Next, for each possible value of the tag T ?
{1, .
.
.
,K}, we generate parameters for a multino-mial distribution (again from a Dirichlet prior) foreach of the prefix and the suffix lexicons:?
?|T ?
Dirichlet(?
?, L?
)?0 ?
Dirichlet(?0, L0)?+|T ?
Dirichlet(?+, L+)By generating parameters in this manner, we allowthe multinomial distributions to generate only mor-phemes that are present in the lexicon.
Also, at infer-ence time, only morphemes in the lexicons receivepseudo-counts.
Note that the affixes are generatedconditioned on the tag; But the stem are not.2Now, we are ready to generate each word typeW , its segmentation S, and its syntactic category T .First, we draw the number of morpheme segments|S| from a geometric distribution truncated to gener-ate at most five morphemes:|S| ?
Truncated-Geometric(?|S|)Next, we pick one of the morphemes to be the stemuniformly at random, and thus determine the numberof prefixes and suffixes.
Then, we draw the syntacticcategory T for the word.
(Note that T is a latentvariable which we recover during inference.
)T ?
Multinomial(?T )After that, we generate each stem ?0, prefix ?
?, andsuffix ?+ independently:?0 ?
Multinomial(?0)?
?|T ?
Multinomial(?
?|T )?+|T ?
Multinomial(?+|T )Token-POS Model This model captures the de-pendencies between the syntactic categories of ad-jacent tokens with a first-order HMM.
Conditionedon the type-level assignments, we generate (unseg-mented) tokens w and their POS tags t:Ppos(w, t|W ,T ,?
)=?wi,tiP (ti?1|ti, ?t|t)P (wi|ti, ?w|t)where the parameters of the multinomial distribu-tions are generated by Dirichlet priors:?t|t ?
Dirichlet(?t|t, {1, .
.
.
,K})?w|t ?
Dirichlet(?w|t,W t)2We design the model as such since the dependencies be-tween affixes and the POS tag are much stronger than those be-tween the stems and tags.
In our preliminary experiments, whenstems are also generated conditioned on the tag, spurious stemsare easily created and associated with garbage-collecting tags.4Here, W t refers to the set of word types that aregenerated by tag t. In other words, conditioned ontag t, we can only generate word w from the set ofword types inW t which is generated earlier (Lee etal., 2010).Token-Seg Model The model captures the mor-phological agreement between adjacent segmenta-tions using a first-order Markov chain.
The proba-bility of drawing a sequence of segmentations s isgiven byPseg(s|W ,S,T ,L,?,?)
=?
(si?1,si)p(si|si?1)For each pair of segmentations si?1 and si, we de-termine: (1) if they should exhibit morpho-syntacticagreement, and (2) if their morphological segmenta-tions are consistent.
To answer the first question, wefirst obtain the final suffix for each of them.
Next,we obtain n, the length of the longer suffix.
Foreach segmentation, we define the ending to be thelast n characters of the word.
We then use matchingendings as a proxy for morpho-syntactic agreementbetween the two words.
To answer the second ques-tion, we use matching final suffixes as a cue for con-sistent morphological segmentations.
To encode thelinguistic intuition that words that exhibit morpho-syntactic agreement are likely to be morphologicalconsistent, we define the above probability distribu-tion to be:p(si|si?1)=???
?1 if same endings and same final suffix?2 if same endings but different final suffixes?3 otherwise (e.g.
no suffix)where ?1 + ?2 + ?3 = 1, with ?1 > ?3 > ?2.
Bysetting ?1 to a high value, we encourage adjacenttokens that are likely to exhibit morpho-syntacticagreement to have the same final suffix.
And by set-ting ?3 > ?2, we also discourage adjacent tokenswith the same endings to be segmented differently.
34 InferenceGiven a corpus of unsegmented and unannotatedword tokens w, the objective is to recover values of3Although p sums to one, it makes the model deficient since,conditioned everything already generated, it places some prob-ability mass on invalid segmentation sequences.all latent variables, including the segmentations s.P (s, t,S,T ,L|w,W ,?,?,?)?
?P (w, s, t,W ,S,T ,L,?,?|?,?,?
)d?d?We want to sample from the above distribution us-ing collapsed Gibbs sampling (?
and ?
integratedout.)
In each iteration, we loop over each word typeWi and sample the following latent variables: its tagTi, its segmentation Si, the segmentations and tagsfor all of its token occurrences (si, ti), and also themorpheme lexicons L:P (L, Ti, Si, si, ti|s?i, t?i,S?i,T?i,w?i,W?i,?,?,?)
(1)such that the type and token-level assignments areconsistent, i.e.
for all t ?
ti we have t = Ti, and forall s ?
si we have s = Si.4.1 Approximate InferenceNaively sampling the lexicons L is computationallyinfeasible since their sizes are unbounded.
There-fore, we employ an approximation which turns issimilar to performing inference with a Dirichlet Pro-cess segmentation model.
In our approximationscheme, for each possible segmentation and tag hy-pothesis (Ti, Si, si, ti), we only consider one possi-ble value for L, which we denote the minimal lexi-cons.
Hence, the total number of hypothesis that wehave to consider is only as large as the number ofpossibilities for (Ti, Si, si, ti).Specifically, we recover the minimal lexicons asfollows: for each segmentation and tag hypothesis,we determine the set of distinct affix and stem typesin the whole corpus, including the morphemes intro-duced by segmentation hypothesis under considera-tion.
This set of lexicons, which we call the minimallexicons, is the most compact ones that are neededto generate all morphemes proposed by the currenthypothesis.Furthermore, we set the number of possible POStags K = 5.
4 For each possible value of the tag,we consider all possible segmentations with at mostfive segments.
We further restrict the stem to have no4We find that increasing K to 10 does not yield improve-ment.5more than two prefixes or suffixes and also that thestem cannot be shorter than the affixes.
This furtherrestricts the space of segmentation and tag hypothe-ses, and hence makes the inference tractable.4.2 Sampling equationsSuppose we are considering the hypothesis with seg-mentation S and POS tag T for word type Wi.
LetL = (L?, L?, L0, L+) be the minimal lexicons forthis hypothesis (S, T ).
We sample the hypothesis(S, T, s = S, t = T,L) proportional to the productof the following four equations.Lexicon Model???L??l(1?
?l)|?| ???(1?
??
)|L?| ??0(1?
?0)|L0| ??+(1?
?+)|L+| (2)This is a product of geometric distributions involv-ing the length of each morpheme ?
and the sizeof each of the prefix, the stem, and the suffix lexi-cons (denoted as |L?|, |L0|, |L+| respectively.)
Sup-pose, a new morpheme type ?0 is introduced as astem.
Relative to a hypothesis that introduces none,this one incurs an additional cost of (1 ?
?0) and?l(1 ?
?l)|?0|.
In other words, the hypothesis is pe-nalized for increasing the stem lexicon size and gen-erating a new morpheme of length |?0|.
In this way,the first and second terms play a role similar to theconcentration parameter and base distribution in aDP-based model.Segmentation Model?|S|(1?
?|S|)|S|?5j=0 ?|S|(1?
?|S|)j?n?iT + ?N?i + ?K?n?i?0 + ?0N?i0 + ?0|L0|?n?i?
?|T + ?
?N?i?|T + ?
?|L?|?n?i?+|T + ?+N?i+|T + ?+|L+|(3)The first factor is the truncated geometric distribu-tion of the number of segmentations |S|, and thesecond factor is the probability of generate the tagT .
The rest are the probabilities of generating thestem ?0, the prefix ?
?, and the suffix ?+ (where theparameters of the multinomial distribution collapsedout).
n?1T is the number of word types with tag Tand N?i is the total number of word types.
n?i?
?|Trefers to the number of times prefix ??
is seen in allword types that are tagged with T , and N?i?|T is thetotal number of prefixes in all word types that has tagT .
All counts exclude the word type Wi whose seg-mentation we are sampling.
If there is another pre-fix, N?i?|T is incremented (and also n?i?
?|Tif the sec-ond prefix is the same as the first one.)
Integratingout the parameters introduces dependencies betweenprefixes.
The rest of the notations read analogously.Token-POS Model?w|t(mi)(M?it + ?w|t|W t|)(mi)?K?t=1K?t?=1(m?it?|t + ?t|t)(mit?|t)(M?it + ?t|t)(mit?|t)(4)The two terms are the token-level emission and tran-sition probabilities with parameters integrated out.The integration induces dependences between alltoken occurrences of word type W which resultsin ascending factorials defined as ?
(m) = ?(?
+1) ?
?
?
(?
+ m ?
1) (Liang et al, 2010).
M?it isthe number of tokens that have POS tag t, mi is thenumber of tokens wi, and m?it?|t is the number of to-kens t-to-t?
transitions.
(Both exclude counts con-tributed by tokens belong to word type Wi.)
|W t| isthe number of word types with tag t.Token-Seg Model?mi?11 ?mi?22 ?mi?33 (5)Here,mi?1 refers to the number of transitions involv-ing token occurrences of word type Wi that exhibitmorphological agreement.
This does not result inascending factorials since the parameters of transi-tion probabilities are fixed and not generated fromDirichlet priors, and so are not integrated out.64.3 Staged TrainingAlthough the Gibbs sampler mixes regardless of theinitial state in theory, good initialization heuristicsoften speed up convergence in practice.
We there-fore train a series of models of increasing complex-ity (see section 6 for more details), each with 50 iter-ations of Gibbs sampling, and use the output of thepreceding model to initialize the subsequent model.The initial model is initialized such that all words arenot segmented.
When POS tags are first introduced,they are initialized uniformly at random.5 Experimental SetupPerformance metrics To enable comparison withprevious approaches, we adopt the evaluation set-upof Poon et al (2009).
They evaluate segmentationaccuracy on a per token basis, using recall, precisionand F1-score computed on segmentation points.
Wealso follow a transductive testing scenario where thesame (unlabeled) data is used for both training andtesting the model.Data set We evaluate segmentation performanceon the Penn Arabic Treebank (ATB).5 It consists ofabout 4,500 sentences of modern Arabic obtainedfrom newswire articles.
Following the preprocessingprocedures of Poon et al (2009) that exclude certainword types (such as abbreviations and digits), weobtain a corpus of 120,000 tokens and 20,000 wordtypes.
Since our full model operates over sentences,we train the model on the entire ATB, but evaluateon the exact portion used by Poon et al (2009).Pre-defined tunable parameters and testingregime In all our experiments, we set ?l = 12 (forlength of morpheme types) and ?|S| =12 (for num-ber of morpheme segments of each word.)
To en-courage a small set of affix types relative to stemtypes, we set ??
= ?+ = 11.1 (for sizes of the af-fix lexicons) and ?0 = 110,000 (for size of the stemlexicon.)
We employ a sparse Dirichlet prior for thetype-level models (for morphemes and POS tag) bysetting ?
= 0.1.
For the token-level models, we sethyperparameters for Dirichlet priors ?w|t = 10?55Our evaluation does not include the Hebrew and ArabicBible datasets (Snyder and Barzilay, 2008a; Poon et al, 2009)since these corpora consists of short phrases that omit sentencecontext.Model R P F1 t-testPCT 09 69.2 88.5 77.7 -Morfessor 72.6 77.4 74.9 -BASIC 71.4 86.7 78.3 (2.9) -+POS 75.4 87.4 81.0 (1.5) ++TOKEN-POS 75.7 88.5 81.6 (0.7) ?+TOKEN-SEG 82.1 90.8 86.2 (0.4) ++Table 1: Results on the Arabic Treebank (ATB) dataset: We compare our models against Poon et al (2009)(PCT09) and the Morfessor system (Morfessor-CAT).For our full model (+TOKEN-SEG) and its simplifica-tions (BASIC, +POS, +TOKEN-POS), we perform fiverandom restarts and show the mean scores.
The samplestandard deviations are shown in brackets.
The last col-umn shows results of a paired t-test against the precedingmodel: ++ (significant at 1%), + (significant at 5%), ?
(not significant), - (test not applicable).
(for unsegmented tokens) and ?t|t = 1.0 (for POStags transition.)
To encourage adjacent words thatexhibit morphological agreement to have the samefinal suffix, we set ?1 = 0.6, ?2 = 0.1, ?1 = 0.3.In all the experiments, we perform five runs us-ing different random seeds and report the mean scoreand the standard deviation.Baselines Our primary comparison is against themorphological segmenter of Poon et al (2009)which yields the best published results on the ATBcorpus.
In addition, we compare against the Mor-fessor Categories-MAP system (Creutz and Lagus,2007).
Similar to our model, their system uses latentvariables to induce clustering over morphemes.
Thedifference is in the nature of the clustering: the Mor-fessor algorithm associates a latent variable for eachmorpheme, grouping morphemes into four broadcategories (prefix, stem, suffix, and non-morpheme)but not introducing dependencies between affixes di-rectly.
For both systems, we quote their performancereported by Poon et al (2009).6 ResultsComparison with the baselines Table 1 shows thatour full model (denoted +TOKEN-SEG) yields amean F1-score of 86.2, compared to 77.7 and 74.9obtained by the baselines.
This performance gapcorresponds to an error reduction of 38.1% over thebest published results.7Ablation Analysis To assess relative impact ofvarious components, we consider several simplifiedvariants of the model:?
BASIC is the type-based segmentation modelthat is solely driven by the lexicon.6?
+POS adds latent variables but does not cap-ture transitions and agreement constraints.?
+TOKEN-POS is equivalent to the full model,without agreement constraints.Our results in Table 1 clearly demonstrate thatmodeling morpho-syntactic constraints greatly im-proves the accuracy of morphological segmentation.We further examine the performance gains arisingfrom improvements due to (1) encouraging morpho-logical consistency within syntactic categories, and(2) morphological realization of grammatical agree-ment.We evaluate our models on a subset of words thatexhibit morphological consistency.
Table 2 showsthe accuracies for words that begin with the prefix?Al?
(determiner) and end with a suffix ?At?
(pluralnoun suffix.)
An example is the word ?Al-{ntxAb-At?
which is translated as ?the-election-s?.
Suchwords make up about 1% of tokens used for eval-uation, and the two affix boundaries constitute about3% of the all gold segmentation points.
By intro-ducing a latent variable to capture dependencies be-tween affixes, +POS is able to improve segmenta-tion performance over BASIC.
When dependenciesbetween latent variables are introduced, +TOKEN-POS yields additional improvements.We also examine the performance gains due tomorphological realization of grammatical agree-ment.
We select the set of tokens that share thesame final suffix as the preceding token, such asthe bigram ?Al-Df-p Al-grby-p?
(which is translatedword-for-word as ?the-bank the-west?)
where thelast morpheme ?p?
is a feminine singular noun suf-fix.
This subset makes up about 4% of the evaluationset, and the boundaries of the final suffixes take upabout 5% of the total gold segmentation boundaries.6The resulting model is similar in spirit to the unigram DP-based segmenter (Goldwater et al, 2006; Snyder and Barzilay,2008a; Snyder and Barzilay, 2008b).ModelToken TypeF1 Acc.
F1 Acc.BASIC 68.3 13.9 73.8 24.3+POS 75.4 26.4 78.5 38.0+TOKEN-POS 76.5 34.9 82.0 49.6+TOKEN-SEG 84.0 49.5 85.4 57.7Table 2: Segmentation performance on words that beginwith prefix ?Al?
(determiner) and end with suffix ?At?
(plural noun suffix).
The mean F1 scores are computedusing all boundaries of words in this set.
For each word,we also determine if both affixes are recovered while ig-noring any other boundaries between them.
The othertwo columns report this accuracy at both the type-leveland the token-level.ModelToken TypeF1 Acc.
F1 Acc.BASIC 85.6 70.6 79.5 58.6+POS 87.6 76.4 82.3 66.3+TOKEN-POS 87.5 75.2 82.2 65.3+TOKEN-SEG 92.8 91.1 88.9 84.4Table 3: Segmentation performance on words that havethe same final suffix as their preceding words.
The F1scores are computed based on all boundaries within thewords, but the accuracies are obtained using only the finalsuffixes.Table 3 reveals this category of errors persisted un-til the final component (+TOKEN-SEG) was intro-duced.7 ConclusionAlthough the connection between syntactic (POS)categories and morphological structure is well-known, this relation is rarely exploited to improvemorphological segmentation performance.
The per-formance gains motivate further investigation intomorpho-syntactic models for unsupervised languageanalysis.AcknowledgementsThis material is based upon work supported bythe U.S. Army Research Laboratory and theU.S.
Army Research Office under contract/grantnumber W911NF-10-1-0533.
Thanks to the MITNLP group and the reviewers for their comments.8ReferencesMeni Adler and Michael Elhadad.
2006.
An un-supervised morpheme-based hmm for hebrew mor-phological disambiguation.
In Proceedings of theACL/CONLL, pages 665?672.Burcu.
Can and Suresh Manandhar.
2009.
Unsupervisedlearning of morphology by using syntactic categories.In Working Notes, CLEF 2009 Workshop.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing, 4(1).Sajib Dasgupta and Vincent Ng.
2007.
Unsuper-vised part-of-speech acquisition for resource-scarcelanguages.
In Proceedings of the EMNLP-CoNLL,pages 218?227.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of the ACL, pages673?680.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Heidi Harley and Colin Phillips, editors.
1994.
TheMorphology-Syntax Connection.
Number 22 in MITWorking Papers in Linguistics.
MIT Press.Mark Johnson.
2008.
Unsupervised word segmentationfor Sesotho using adaptor grammars.
In Proceedingsof the Tenth Meeting of ACL Special Interest Groupon Computational Morphology and Phonology, pages20?27, Columbus, Ohio, June.
Association for Com-putational Linguistics.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised POS tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 853?861, Cambridge, MA, October.
Association for Com-putational Linguistics.Percy Liang, Michael I. Jordan, and Dan Klein.
2010.Type-based mcmc.
In Human Language Technolo-gies: The 2010 Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 573?581, Los Angeles, California,June.
Association for Computational Linguistics.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proceedings of HLT-NAACL2009, pages 209?217, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Benjamin Snyder and Regina Barzilay.
2008a.
Crosslin-gual propagation for morphological analysis.
In Pro-ceedings of the AAAI, pages 848?854.Benjamin Snyder and Regina Barzilay.
2008b.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proceedings of ACL-08: HLT, pages 737?745, Columbus, Ohio, June.
Association for Computa-tional Linguistics.Kristina Toutanova and Colin Cherry.
2009.
A globalmodel for joint lemmatization and part-of-speech pre-diction.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 486?494, Suntec, Singapore,August.
Association for Computational Linguistics.Kristina Toutanova and Mark Johnson.
2008.
A bayesianlda-based model for semi-supervised part-of-speechtagging.
In J.C. Platt, D. Koller, Y.
Singer, andS.
Roweis, editors, Advances in Neural InformationProcessing Systems 20, pages 1521?1528.
MIT Press,Cambridge, MA.9
