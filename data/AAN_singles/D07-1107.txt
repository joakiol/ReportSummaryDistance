Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
1005?1014, Prague, June 2007. c?2007 Association for Computational LinguisticsLearning to Merge Word SensesRion Snow Sushant PrakashComputer Science DepartmentStanford UniversityStanford, CA 94305 USA{rion,sprakash}@cs.stanford.eduDaniel JurafskyLinguistics DepartmentStanford UniversityStanford, CA 94305 USAjurafsky@stanford.eduAndrew Y. NgComputer Science DepartmentStanford UniversityStanford, CA 94305 USAang@cs.stanford.eduAbstractIt has been widely observed that different NLP appli-cations require different sense granularities in order tobest exploit word sense distinctions, and that for manyapplications WordNet senses are too fine-grained.
Incontrast to previously proposed automatic methods forsense clustering, we formulate sense merging as a su-pervised learning problem, exploiting human-labeledsense clusterings as training data.
We train a discrimi-native classifier over a wide variety of features derivedfrom WordNet structure, corpus-based evidence, andevidence from other lexical resources.
Our learnedsimilarity measure outperforms previously proposedautomatic methods for sense clustering on the task ofpredicting human sense merging judgments, yieldingan absolute F-score improvement of 4.1% on nouns,13.6% on verbs, and 4.0% on adjectives.
Finally, wepropose a model for clustering sense taxonomies us-ing the outputs of our classifier, and we make avail-able several automatically sense-clustered WordNetsof various sense granularities.1 IntroductionDefining a discrete inventory of senses for a word isextremely difficult (Kilgarriff, 1997; Hanks, 2000;Palmer et al, 2005).
Perhaps the greatest obstacle isthe dynamic nature of sense definition: the correctgranularity for word senses depends on the appli-cation.
For language learners, a fine-grained set ofword senses may help in learning subtle distinctions,while coarsely-defined senses are probably moreuseful in NLP tasks like information retrieval (Gon-zalo et al, 1998), query expansion (Moldovan andMihalcea, 2000), and WSD (Resnik and Yarowsky,1999; Palmer et al, 2005).Lexical resources such as WordNet (Fellbaum,1998) use extremely fine-grained notions of wordsense, which carefully capture even minor distinc-tions between different possible word senses (e.g.,the 8 noun senses of bass shown in Figure 1).
Pro-ducing sense-clustered inventories of arbitrary sensegranularity is thus crucial for tasks which depend onlexical resources like WordNet, and is also impor-tant for the task of automatically constructing newWordNet-like taxonomies.
A solution to this prob-lem must also deal with the constraints of the Word-Net taxonomy itself; for example when clusteringtwo senses, we need to consider the transitive effectsof merging synsets.The state of the art in sense clustering is insuffi-cient to meet these needs.
Current sense clusteringalgorithms are generally unsupervised, each relyingon a different set of useful features or hand-builtrules.
But hand-written rules have little flexibilityto produce clusterings of different granularities, andpreviously proposed methods offer little in the di-rection of intelligently combining and weighting themany proposed features.In response to these challenges, we propose anew algorithm for clustering large-scale sense hier-archies like WordNet.
Our algorithm is based on asupervised classifier that learns to make graduatedjudgments corresponding to the estimated probabil-ity that each particular sense pair should be merged.This classifier is trained on gold standard sense clus-tering judgments using a diverse feature space.
Weare able to use the outputs of our classifier to producea ranked list of sense merge judgments by mergeprobability, and from this create sense-clustered in-ventories of arbitrary sense granularity.1In Section 2 we discuss past work in sense cluster-1We have made sense-clustered Wordnets using the al-gorithms discussed in this paper available for download athttp://ai.stanford.edu/?rion/swn.1005INSTRUMENT 7: ...the lowest range of a family of musical instrumentsFISH4: the lean flesh of a saltwater fish of the family Serranidae5: any of various North American freshwater fish with lean flesh8: nontechnical name for any of numerous... fishesSINGER3: an adult male singer with the lowest voice6: the lowest adult male singing voicePITCH1: the lowest part of the musical range2: the lowest part in polyphonic musicFigure 1: Sense clusters for the noun bass; the eightWordNet senses as clustered into four groups in theSENSEVAL-2 coarse-grained evaluation dataing, and the gold standard datasets that we use in ourwork.
In Section 3 we introduce our battery of fea-tures; in Section 4 we show how to extend our sense-merging model to cluster full taxonomies like Word-Net.
In Section 5 we evaluate our classifier againstthirteen previously proposed methods.2 BackgroundA wide number of manual and automatic techniqueshave been proposed for clustering sense inventoriesand mapping between sense inventories of differentgranularities.
Much work has gone into methods formeasuring synset similarity; early work in this direc-tion includes (Dolan, 1994), which attempted to dis-cover sense similarities between dictionary senses.A variety of synset similarity measures based onproperties of WordNet itself have been proposed;nine such measures are discussed in (Pedersen et al,2004), including gloss-based heuristics (Lesk, 1986;Banerjee and Pedersen, 2003), information-contentbased measures (Resnik, 1995; Lin, 1998; Jiang andConrath, 1997), and others.
Other approaches haveused specific cues from WordNet structure to informthe construction of semantic rules; for example, (Pe-ters et al, 1998) suggest clustering two senses basedon a wide variety of structural cues from Word-Net, including if they are twins (if two synsets sharemore than one word in their synonym list) or ifthey represent an example of autohyponymy (if onesense is the direct descendant of the other).
(Mihal-cea and Moldovan, 2001) implements six semanticrules, using twin and autohyponym features, in addi-tion to other WordNet-structure-based rules such aswhether two synsets share a pertainym, antonym, orare clustered together in the same verb group.A large body of work has attempted to capturecorpus-based estimates of word similarity (Pereiraet al, 1993; Lin, 1998); however, the lack oflarge sense-tagged corpora prevent most such tech-niques from being used effectively to compare dif-ferent senses of the same word.
Some corpus-basedattempts that are capable of estimating similaritybetween word senses include the topic signaturesmethod; here, (Agirre and Lopez, 2003) collect con-texts for a polysemous word based either on sense-tagged corpora or by using a weighted agglomera-tion of contexts of a polysemous word?s monose-mous relatives (i.e., single-sense synsets related byhypernym, hyponym, or other relations) from somelarge untagged corpus.
Other corpus-based tech-niques developed specifically for sense clusteringinclude (McCarthy, 2006), which uses a combina-tion of word-to-word distributional similarity com-bined with the JCN WordNet-based similarity mea-sure, and work by (Chugur et al, 2002) in find-ing co-occurrences of senses within documents insense-tagged corpora.
Other attempts have exploiteddisagreements between WSD systems (Agirre andLopez, 2003) or between human labelers (Chklovskiand Mihalcea, 2003) to create synset similaritymeasures; while promising, these techniques areseverely limited by the performance of the WSDsystems or the amount of available labeled data.Some approaches for clustering have made use ofregular patterns of polysemy among words.
(Pe-ters et al, 1998) uses the COUSIN relation definedin WordNet 1.5 to cluster hyponyms of categoricallyrelated noun synsets, e.g., ?container/quantity?
(e.g.,for clustering senses of ?cup?
or ?barrel?)
or ?or-ganization/construction?
(e.g., for the building andinstitution senses of ?hospital?
or ?school?
); otherapproaches based on systematic polysemy includethe hand-constructed CORELEX database (Buite-laar, 1998), and automatic attempts to extract pat-terns of systematic polysemy based on minimal de-scription length principles (Tomuro, 2001).Another family of approaches has been touse either manually-annotated or automatically-constructed mappings to coarser-grained sense in-ventories; an attempt at providing coarse-grainedsense distinctions for the SENSEVAL-1 exercise in-cluded a mapping between WordNet and the Hec-tor lexicon (Palmer et al, 2005).
Other attempts in1006this vein include mappings between WordNet andPropBank (Palmer et al, 2004) and mappings toLevin classes (Levin, 1993; Palmer et al, 2005).
(Navigli, 2006) presents an automatic approach formapping between sense inventories; here similari-ties in gloss definition and structured relations be-tween the two sense inventories are exploited in or-der to map between WordNet senses and distinc-tions made within the coarser-grained Oxford En-glish Dictionary.
Other work has attempted to ex-ploit translational equivalences of WordNet sensesin other languages, for example using foreign lan-guage WordNet interlingual indexes (Gonzalo et al,1998; Chugur et al, 2002).2.1 Gold standard sense clustering dataOur approach for learning how to merge sensesrelies upon the availability of labeled judgmentsof sense relatedness.
In this work we focus ontwo datasets of hand-labeled sense groupings forWordNet: first, a dataset of sense groupings overnouns, verbs, and adjectives provided as part ofthe SENSEVAL-2 English lexical sample WSD task(Kilgarriff, 2001), and second, a corpus-driven map-ping of nouns and verbs in WordNet 2.1 to theOmega Ontology (Philpot et al, 2005), produced aspart of the ONTONOTES project (Hovy et al, 2006).A wide variety of semantic and syntactic criteriawere used to produce the SENSEVAL-2 groupings(Palmer et al, 2004; Palmer et al, 2005); this datacovers all senses of 411 nouns, 519 verbs, and 257adjectives, and has been used as gold standard senseclustering data in previous work (Agirre and Lopez,2003; McCarthy, 2006)2.
The number of judgmentswithin this data (after mapping to WordNet 2.1) isdisplayed in Table 1.Due to a lack of interannotator agreement data forthis dataset, (McCarthy, 2006) performed an anno-tation study using three labelers on a 20-noun sub-set of the SENSEVAL-2 groupings; the three label-ers were given the task of deciding whether the 351potentially-related sense pairs were ?Related?, ?Un-related?, or ?Don?t Know?.3 In this task the pair-2In order to facilitate future work in this area, wehave made cleaned versions of these groupings available athttp://ai.stanford.edu/?rion/swn along with a ?diff?
with theoriginal files.3McCarthy?s gold standard data is available atSENSEVAL-2POS Total Pairs Merged Pairs ProportionNouns 16403 2593 0.1581Verbs 30688 3373 0.1099Adjectives 8368 2209 0.2640ONTONOTESPOS Total Pairs Merged Pairs ProportionNouns 3552 347 0.0977Verbs 4663 1225 0.2627Table 1: Gold standard datasets for sense merging;only sense pairs that share a word in common areincluded; proportion refers to the fraction of synsetssharing a word that have been mergedPOS Overlap ON-True ON-False F-ScoreS-T S-F S-T S-FNouns 2116 121 55 181 1759 0.5063Verbs 3297 351 503 179 2264 0.5072Table 2: Agreement data for gold standard datasetswise interannotator F-scores were (0.4874, 0.5454,0.7926), for an average F-score of 0.6084.The ONTONOTES dataset4 covers a smaller setof nouns and verbs, but it has been created with amore rigorous corpus-based iterative annotation pro-cess.
For each of the nouns and verbs in question, a50-sentence sample of instances is annotated usinga preliminary set of sense distinctions; if the wordsense interannotator agreement for the sample is lessthan 90%, then the sense distinctions are revised andthe sample is re-annotated, and so forth, until an in-terannotator agreement of at least 90% is reached.We construct a combined gold standard set fromthese SENSEVAL-2 and ONTONOTES groupings,removing disagreements.
The overlap and agree-ment/disagreement data between the two groupingsis given in Table 2; here, for example, the columnwith ON-True and S-F indicates the count of sensesthat ONTONOTES judged as positive examples ofsense merging, but that SENSEVAL-2 data did notmerge.
We also calculate the F-score achieved byconsidering only one of the datasets as a gold stan-dard, and computing precision and recall for theother.
Since the two datasets were created indepen-dently, with different annotation guidelines, we can-ftp://ftp.informatics.susx.ac.uk/pub/users/dianam/relateGS/.4The OntoNotes groupings will be available through theLDC at http://www.ldc.upenn.edu.1007not consider this as a valid estimate of interannota-tor agreement; nonetheless the F-score for the twodatasets on the overlapping set of sense judgments(50.6% for nouns and 50.7% for verbs) is roughlyin the same range as those observed in (McCarthy,2006).3 Learning to merge word senses3.1 WordNet-based featuresHere we describe the feature space we construct forclassifying whether or not a pair of synsets should bemerged; first, we employ a wide variety of linguisticfeatures based on information derived from Word-Net.
We use eight similarity measures implementedwithin the WordNet::Similarity package5, describedin (Pedersen et al, 2004); these include three mea-sures derived from the paths between the synsetsin WordNet: HSO (Hirst and St-Onge, 1998), LCH(Leacock and Chodorow, 1998), and WUP (Wu andPalmer, 1994); three measures based on informationcontent: RES (Resnik, 1995), LIN (Lin, 1998), andJCN (Jiang and Conrath, 1997); the gloss-based Ex-tended Lesk Measure LESK, (Banerjee and Peder-sen, 2003), and finally the gloss vector similaritymeasure VECTOR (Patwardan, 2003).
We imple-ment the TWIN feature (Peters et al, 1998), whichcounts the number of shared synonyms betweenthe two synsets.
Additionally we produce pair-wise features indicating whether two senses share anANTONYM, PERTAINYM, or derivationally-relatedforms (DERIV).
We also create the verb-specificfeatures of whether two verb synsets are linked ina VERBGROUP (indicating semantic similarity) orshare a VERBFRAME, indicating syntactic similar-ity.
Also, we encode a generalized notion of sib-linghood in the MN features, recording the distanceof the synset pair?s nearest least common subsumer(i.e., closest shared hypernym) from the two synsets,and, separately, the maximum of those distances (inthe MAXMN feature.Previous attempts at categorizing systematic pol-ysemy patterns within WordNet has resulted in theCOUSIN feature6; we create binary features which5We choose not to use the PATH measure due to its negligibledifference from the LCH measure.6This data is included in the WordNet 1.6 distribution as the?cousin.tops?
file.indicate whether a synset pair belong to hypernymancestries indicated by one or more of these COUSINfeatures, and the specific cousin pair(s) involved.Finally we create sense-specific features, includingSENSECOUNT, the total number of senses associ-ated with the shared word between the two synsetswith the highest number of senses, and SENSENUM,the specific pairing of senses for the shared wordwith the highest number of senses (which might al-low us to learn whether the most frequent sense of aword has a higher chance of having similar deriva-tive senses with lower frequency).3.2 Features derived from corpora and otherlexical resourcesIn addition to WordNet-based features, we usea number of features derived from corpora andother lexical resources.
We use the publicly avail-able topic signature data7 described in (Agirre andLopez, 2004), yielding representative contexts forall nominal synsets from WordNet 1.6.
These topicsignatures were obtained by weighting the contextsof monosemous relatives of each noun synset (i.e.,single-sense synsets related by hypernym, hyponym,or other relations); the text for these contexts wereextracted from snippets using the Google search en-gine.
We then create a sense similarity feature bytaking a thresholded cosine similarity between pairsof topic signatures for these noun synsets.Additionally, we use the WordNet domain datasetdescribed in (Magnini and Cavaglia, 2000; Ben-tivogli et al, 2004).
This dataset contains one ormore labels indicating of 164 hierarchically orga-nized ?domains?
or ?subject fields?
for each noun,verb, and adjective synset in WordNet; we derive aset of binary features from this data, with a singlefeature indicating whether or not two synsets sharea domain, and one indicator feature per pair of do-mains indicating respective membership of the sensepair within those domains.Finally, we use as a feature the mappings pro-duced in (Navigli, 2006) of WordNet senses to Ox-ford English Dictionary senses.
This OED datasetwas used as the coarse-grained sense inventory in theCoarse-grained English all-words task of SemEval-7The topic signature data is available for download athttp://ixa.si.ehu.es/Ixa/resources/sensecorpus.100820078; we specify a single binary feature for eachpair of synsets from this data; this feature is true ifthe words are clustered in the OED mapping, andfalse otherwise.3.3 Classifier, training, and feature selectionFor each part of speech, we split the merged goldstandard data into a part-of-speech-specific train-ing set (70%) and a held-out test set (30%).
Forevery synset pair we use the binary ?merged?
or?not-merged?
labels to train a support vector ma-chine classifier9 (Joachims, 2002) for each POS-specific training set.
We perform feature selectionand regularization parameter optimization using 10-fold cross-validation.4 Clustering Senses in WordNetThe previous section describes a classifier whichpredicts whether two synsets should be merged; wewould like to use the pairwise judgments of thisclassifier to cluster the senses within a sense hierar-chy.
In this section we present the challenge implicitin applying sense merging to full taxonomies, andpresent our model for clustering within a taxonomy.4.1 Challenges of clustering a sense taxonomyThe task of clustering a sense taxonomy presentscertain challenges not present in the problem of clus-tering the senses of a word; in order to create aconsistent clustering of a sense hierarchy an algo-rithm must consider the transitive effects of mergingsynsets.
This problem is compounded in sense tax-onomies like WordNet, where each synset may haveadditional structured relations, e.g., hypernym (IS-A) or holonym (is-part-of) links.
In order to consis-tently merge two noun senses with different hyper-nym ancestries within WordNet, for example, an al-gorithm must decide whether to have the new senseinherit both hypernym ancestries, or whether to in-herit only one, and if so it must decide which ances-try is more relevant for the merged sense.Without strict checking, human labelers willlikely find it difficult to label a sense inventory with8http://lcl.di.uniroma1.it/coarse-grained-aw/index.html9We use the SV Mperf package, freely available for non-commercial use from http://svmlight.joachims.org; we use thedefault settings in v2.00, except for the regularization parameter(set in 10-fold cross-validation).Clusering based on ??need?
?Clustering based on ??require?
?need#v#1require#v#1 require as useful, just, or properneed#v#2require#v#4 have need ofneed#v#3 have or feel a need forrequire#v#1need#v#1 require as useful, just, or properrequire#v#4need#v#2 have need ofrequire#v#2 consider obligatory; request and expectrequire#v#3 make someone do somethingFigure 2: Inconsistent sense clusters for the verbsrequire and need from SENSEVAL-2 judgmentstransitively-consistent judgments.
As an example,consider the SENSEVAL-2 clusterings of the verbsrequire and need, as shown in Figure 2.
In WN 2.1require has four verb senses, of which the first hassynonyms {necessitate, ask, postulate, need, take,involve, call for, demand}, and gloss ?require as use-ful, just, or proper?
; and the fourth has synonyms{want, need}, and gloss ?have need of.
?Within the word require, the SENSEVAL-2 datasetclusters senses 1 and 4, leaving the rest unclustered.In order to make a consistent clustering with respectto the sense inventory, however, we must enforcethe transitive closure by merging the synset corre-sponding to the first sense (necessitate, ask, needetc.
), with the senses of want and need in the fourthsense.
In particular, these two senses correspondto WordNet 2.1 senses need#v#1 and need#v#2, re-spectively, which are not clustered according tothe SENSEVAL-2 word-specific labeling for need ?need#v#1 is listed as a singleton (i.e., unclustered)sense, though need#v#2 is clustered with need#v#3,?have or feel a need for.
?While one might hope that such disagreementsbetween sense clusterings are rare, we found178 such transitive closure disagreements in theSENSEVAL-2 data.
The ONTONOTES data is muchcleaner in this respect, most likely due to thestricter annotation standard (Hovy et al, 2006);we found only one transitive closure disagreement1009in the OntoNotes data, specifically WordNet 2.1synsets (head#n#2, lead#n#7: ?be in charge of?)
and(head#n#3, lead#v#4: ?travel in front of?)
are clus-tered under head but not under lead.4.2 Sense clustering within a taxonomyAs a solution to the previously mentioned chal-lenges, in order to produce taxonomies of differentsense granularities with consistent sense distinctionswe propose to apply agglomerative clustering overall synsets in WordNet 2.1.
While one might con-sider recalculating synset similarity features aftereach synset merge operation, depending on the fea-ture set this could be prohibitively expensive; for ourpurposes we use average-link agglomerative cluster-ing, in effect approximating the the pairwise similar-ity score between a given synset and a merged senseas the average of the similarity scores between thegiven synset and the clustered sense?s componentsynsets.
Further, for the purpose of sense cluster-ing we assume a zero sense similarity score betweensynsets with no intersecting words.Without exploiting additional hypernym orcoordinate-term evidence, our algorithm doesnot distinguish between judgments about whichhypernym ancestry or other structured relationshipsto keep or remove upon merging two synsets.
Inlieu of additional evidence, for our experimentswe choose to retain only the hypernym ancestry ofthe sense with the highest frequency in SEMCOR,breaking frequency ties by choosing the first-listedsense in WordNet.
We add every other relationship(meronyms, entailments, etc.)
to the new mergedsense (except in the rare case where adding arelation would cause a cycle in acyclic relations likehypernymy or holonymy, in which case we omitit).
Using this clustering method we have producedseveral sense-clustered WordNets of varying sensegranularity, which we evaluate in Section 5.3.5 EvaluationWe evaluate our classifier in a comparison with thir-teen previously proposed similarity measures andautomatic methods for sense clustering.
We conducta feature ablation study to explore the relevance ofthe different features in our system.
Finally, we eval-uate the sense-clustered taxonomies we create onthe problem of providing improved coarse-grainedsense distinctions for WSD evaluation.5.1 Evaluation of automatic sense mergingWe evaluate our classifier on two held-out testsets; first, a 30% sample of the sense judgmentsfrom the merged gold standard dataset consistingof both the SENSEVAL-2 and ONTONOTES sensejudgments; and, second, a test set consisting of onlythe ONTONOTES subset of our first held-out test set.For comparison we implement thirteen of the meth-ods discussed in Section 2.
First, we evaluate eachof the eight WordNet::Similarity measures individu-ally.
Next, we implement cosine similarity of topicsignatures (TOPSIG) built from monosemous rela-tives (Agirre and Lopez, 2003), which provides areal-valued similarity score for noun synset pairs.Additionally, we implement the two methodsproposed in (Peters et al, 1998), namely usingmetonymy clusters (MetClust) and generalizationclusters (GenClust) based on the COUSIN relation-ship in WordNet.
While (Peters et al, 1998) onlyconsiders four cousin pairs, we re-implement theirmethod for general purpose sense clustering by us-ing all 226 cousin pairs defined in WordNet 1.6,mapped to WordNet 2.1 synsets.
These methodseach provide a single clustering of noun synsets.Next, we implement the set of semantic rules de-scribed in (Mihalcea and Moldovan, 2001) (MIMO);this algorithm for merging senses is based on 6 se-mantic rules, in effect using a subset of the TWIN,MAXMN, PERTAINYM, ANTONYM, and VERB-GROUP features; in our implementation we set theparameter for when to cluster based on number oftwins to K = 2; this results in a single clusteringfor each of nouns, verbs, and adjectives.
Finally, wecompare against the mapping from WordNet to theOxford English Dictionary constructed in (Navigli,2006), equivalent to clustering based solely on theOED feature.Considering merging senses as a binary classifi-cation task, Table 3 gives the F-score performanceof our classifier vs. the thirteen other classifiers andan uninformed ?merge all synsets?
baseline on ourheld-out gold standard test set.
This table shows thatour SVM classifier outperforms all implementedmethods on the basis of F-score on both datasets1010SENSEVAL-2 + ONTONOTESONTONOTESMethod Nouns Verbs Adj Nouns VerbsSVM 0.4228 0.4319 0.4727 0.3698 0.4545RES 0.3817 0.2703 ?
0.2807 0.3156WUP 0.3763 0.2782 ?
0.3036 0.3451LCH 0.3700 0.2440 ?
0.2857 0.3396OED 0.3310 0.2878 0.3712 0.2183 0.3962LESK 0.3174 0.2956 0.4323 0.2914 0.3774HSO 0.3090 0.2784 0.4312 0.3025 0.3156TOPSIG 0.3072 ?
?
0.2581 ?VEC 0.2960 0.2315 0.4321 0.2454 0.3420JCN 0.2818 0.2292 ?
0.2222 0.3156LIN 0.2759 0.2464 ?
0.2056 0.3471Baseline 0.2587 0.2072 0.4312 0.1488 0.3156MIMO 0.0989 0.2142 0.0759 0.1833 0.2157GenClust 0.0973 ?
?
0.0264 ?MetClust 0.0876 ?
?
0.0377 ?Table 3: F-score sense merging evaluation on hand-labeled testsetsfor all parts of speech.
In Figure 3 we give a pre-cision/recall plot for noun sense merge judgmentsfor the SENSEVAL-2 + ONTONOTES dataset.
Forsake of simplicity we plot only the two best mea-sures (RES and WUP) of the eight WordNet-basedsimilarity measures; we see that our classifier, RES,and WUP each have higher precision all levels ofrecall compared to the other tested measures.Of the methods we compare against, only theWordNet-based similarity measures, (Mihalcea andMoldovan, 2001), and (Navigli, 2006) provide amethod for predicting verb similarities; our learnedmeasure widely outperforms these methods, achiev-ing a 13.6% F-score improvement over the LESKsimilarity measure.
In Figure 4 we give a pre-cision/recall plot for verb sense merge judgments,plotting the performance of the three best WordNet-based similarity measures; here we see that our clas-sifier has significantly higher precision than all othertested measures at nearly every level of recall.Only the measures provided by LESK, HSO,VEC, (Mihalcea and Moldovan, 2001), and (Nav-igli, 2006) provide a method for predicting adjectivesimilarities; of these, only LESK and VEC outper-form the uninformed baseline on adjectives, whileour learned measure achieves a 4.0% improvementover the LESK measure on adjectives.5.2 Feature analysisNext we analyze our feature space.
Table 4 gives theablation analysis for all features used in our system0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91RecallPrecisionPrecision/Recall for Merging NounsSVM ClassifierResnik MeasureWu & Palmer MeasureTopic SignaturesOED MappingGeneralization ClustersMetonym ClustersSemantic RulesFigure 3: Precision/Recall plot for noun sense mergejudgmentsas evaluated on our held-out test set; here the quan-tity listed in the table is the F-score loss obtained byremoving that single feature from our feature space,and retraining and retesting our classifiers, keepingeverything else the same.
Here negative scores cor-respond to an improvement in classifier performancewith the removal of the feature.For noun classification, the three features thatyield the highest gain in testset F-score are thetopic signature, OED, and derivational link features,yielding a 4.0%, 3.6%, and 3.5% gain, respectively.For verb classification, we find that three featuresyield more than a 5% F-score gain; by far the largestsingle-feature performance gain for verb classifica-tion found in our ablation study was the DERIV fea-ture, i.e., the count of shared derivational links be-tween the two synsets; this single feature improvesour maximum F-score by 9.8% on the testset.
Thisis a particularly interesting discovery, as none of thereferenced automatic techniques for sense clusteringpresently make use of this very useful feature.
Wealso achieve large gains with the LIN and LESK sim-ilarity features, with F-score improvement of 7.4%and 5.4% gain respectively.For adjective classification again the DERIV fea-ture proved very helpful, with a 3.5% gain on thetestset.
Interestingly, only the DERIV feature andthe SENSECNT features helped across all parts ofspeech; in many cases a feature which proved to be10110 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.9RecallPrecisionPrecision/Recall for Merging VerbsSVM ClassifierLesk MeasureHirst & St?OngeWu & PalmerOED MappingSemantic RulesFigure 4: Precision/Recall plot for verb sense mergejudgmentsvery helpful for one part of speech actually hurt per-formance on another part of speech (e.g., LIN onnouns and OED on adjectives).5.3 Evaluation of sense-clustered WordnetsOur goal in clustering a sense taxonomy is to pro-duce fully sense-clustered WordNets, and to be ableto produce coarse-grained Wordnets at many differ-ent levels of resolution.
In order to evaluate the en-tire sense-clustered taxonomy, we have employed anevaluation method inspired by Word Sense Disam-biguation (this is similar to an evaluation used inNavigli, 2006, however we do not remove monose-mous clusters).
Given past system responses in theSENSEVAL-3 English all-words task, we can eval-uate past systems on the same corpus, but usingthe coarse-grained sense hierarchy provided by oursense-clustered taxonomy.
We may then comparethe scores of each system on the coarse-grained taskagainst their scores given a random clustering at thesame resolution.
Our expectation is that, if our senseclustering is much better than a random sense clus-tering (and, of course, that the WSD algorithms per-form better than random guessing), we will see amarked improvement in the performance of WSDalgorithms using our coarse-grained sense hierarchy.We consider the outputs of the top 3 all-words WSD systems that participated in Senseval-3:Gambl (Decadt et al, 2004), SenseLearner (Mihal-cea and Faruque, 2004), and KOC University (Yuret,Nouns Verbs AdjectivesF-SCORE 0.4228 0.4319 0.4727Feature F-Score Ablation DifferenceTOPSIG 0.0403 ?
?OED 0.0355 0.0126 -0.0124DERIV 0.0351 0.0977 0.0352RES 0.0287 0.0147 ?TWIN 0.0285 0.0109 -0.0130MN 0.0188 0.0358 ?LESK 0.0183 0.0541 -0.0250SENSENUM 0.0155 0.0146 -0.0147SENSECNT 0.0121 0.0160 0.0168DOMAIN 0.0119 0.0082 -0.0265LCH 0.0099 0.0068 ?WUP 0.0036 0.0168 ?JCN 0.0025 0.0190 ?ANTONYM 0.0000 0.0295 0.0000MAXMN -0.0013 0.0179 ?VEC -0.0024 0.0371 -0.0062HSO -0.0073 0.0112 -0.0246LIN -0.0086 0.0742 ?COUSIN -0.0094 ?
?VERBGRP ?
0.0327 ?VERBFRM ?
0.0102 ?PERTAINYM ?
?
-0.0029Table 4: Feature ablation study; F-score differenceobtained by removal of the single feature2004).
A guess by a system is given full credit if itwas either the correct answer or if it was in the samecluster as the correct answer.Clearly any amount of clustering will only in-crease WSD performance.
Therefore, to account forthis natural improvement and consider only the ef-fect of our particular clustering, we also calculatethe expected score for a random clustering of thesame granularity, as follows: Let C represent the setof clusters over the possible N synsets containing agiven word; we then calculate the expectation that anincorrectly-chosen sense and the actual correct sensewould be clustered together in the random clusteringasPc?C |c|(|c|?1)N(N?1) .Our sense clustering algorithm provides little im-provement over random clustering when too few ortoo many clusters are chosen; however, with an ap-propriate threshold for average-link clustering wefind a maximum of 3.55% F-score improvement inWSD over random clustering (averaged over the de-cisions of the top 3 WSD algorithms).
Table 5 showsthe improvement of the three top WSD algorithmsgiven a sense clustering created by our algorithm vs.a random clustering at the same granularity.10120 0.5 1 1.5 2 2.5 3 3.5x 1040.650.70.750.8Sense Merge IterationsWSDF?ScoreGroup Average Agglomerative ClusteringRandom ClusteringFigure 5: WSD Improvement with coarse-grainedsense hierarchiesSystem F-score Avg-link Random Impr.Gambl 0.6516 0.7702 0.7346 0.0356SenseLearner 0.6458 0.7536 0.7195 0.0341KOC Univ.
0.6414 0.7521 0.7153 0.0368Table 5: Improvement in SENSEVAL-3 WSD perfor-mance using our average-link agglomerative cluster-ing vs. random clustering at the same granularity6 ConclusionWe have presented a classifier for automatic sensemerging that significantly outperforms previouslyproposed automatic methods.
In addition to its noveluse of supervised learning and the integration ofmany previously proposed features, it is interest-ing that one of our new features, the DERIV countof shared derivational links between two synsets,proved an extraordinarily useful new cue for sense-merging, particularly for verbs.We also show how to integrate this sense-mergingalgorithm into a model for sense clustering full sensetaxonomies like WordNet, incorporating taxonomicconstraints such as the transitive effects of mergingsynsets.
Using this model, we have produced severalWordNet taxonomies of various sense granularities;we hope these new lexical resources will be usefulfor NLP applications that require a coarser-grainedsense hierarchy than that already found in WordNet.AcknowledgmentsThanks to Marie-Catherine de Marneffe, MonaDiab, Christiane Fellbaum, Thad Hughes, and Ben-jamin Packer for useful discussions.
Rion Snow issupported by an NSF Fellowship.
This work wassupported in part by the Disruptive Technology Of-fice (DTO)?s Advanced Question Answering for In-telligence (AQUAINT) Phase III Program.ReferencesEneko Agirre and Oier Lopez de Lacalle.
2003.
Cluster-ing WordNet word senses.
In Proceedings of RANLP2003.Eneko Agirre and Oier Lopez de Lacalle.
2004.
Pub-licly available topic signatures for all WordNet nomi-nal senses.
In Proceedings of LREC 2004.Satanjeev Banerjee and Ted Pedersen.
2003.
ExtendedGloss Overlaps as a Measure of Semantic Relatedness.In Proceedings of IJCAI 2003.Lisa Bentivogli, Pamela Forner, Bernardo Magnini, andEmanuele Pianta.
2004.
Revising the WordNet Do-mains Hierarchy: Semantics, Coverage, and Balanc-ing.
In Proceedings of COLING Workshop on Multi-lingual Linguistic Resources, 2004.Timothy Chklovski and Rada Mihalcea.
2003.
Exploit-ing Agreement and Disagreement of Human Annota-tors for Word Sense Disambiguation.
In Proceedingsof RANLP 2003.Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and Sense Proximity in the Senseval-2 TestSuite.
In Proceedings of ACL 2002 WSD Workshop.Bart Decadt, Veronique Hoste, Walter Daelemans, andAntal van den Bosch.
2004.
Gamble, genetic algo-rithm optimization of memory-based wsd.
In Proceed-ings of ACL/SIGLEX Senseval-3.William Dolan.
1994.
Word Sense Ambiguation: Clus-tering Related Senses.
In Proceedings of ACL 1994.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Cambridge, MA: MIT Press.Julio Gonzalo, Felia Verdejo, Irina Chugur, and JuanCigarran.
1998.
Indexing with WordNet synsets canimprove text retrieval.
In Proceedings of COLING-ACL 1998 Workshop on WordNet in NLP Systems.Patrick Hanks.
2000.
Do word meanings exist?
Com-puters and the Humanities, 34(1-2): 171-177.1013Graeme Hirst and David St-Onge.
1998.
Lexical chainsas representations of context for the detection and cor-rection of malapropisms.
In WordNet: An ElectronicLexical Database.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% Solution.
Proceedings of HLT-NAACL 2006.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference on Re-search in Computational Linguistics, 19-33.Thorsten Joachims.
2002.
Learning to Classify Text Us-ing Support Vector Machines.
Dissertation, Kluwer,2002.Adam Kilgariff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31(1-2): 1-13.Adam Kilgarriff.
2001.
English lexical sample task de-scription.
In Proceedings of the SENSEVAL-2 work-shop, 17-20.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity for wordsense identification.
In WordNet: An Electronic Lexi-cal Database.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of SIG-DOC 1986.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago, IL.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML 1998.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL 1998.Bernardo Magnini and Gabriela Cavaglia.
2000.
Inte-grating Subject Field Codes into WordNet.
In Pro-ceedings of LREC 2000.Diana McCarthy.
2006.
Relating WordNet Senses forWord Sense Disambiguation.
In Proceedings of ACLWorkshop on Making Sense of Sense, 2006.Rada Mihalcea and Dan I. Moldovan.
2001.
AutomaticGeneration of a Coarse Grained WordNet.
In Proceed-ings of NAACL Workshop on WordNet and Other Lex-ical Resources.Rada Mihalcea and Ehsanul Faruque.
2004.
Sense-learner: Minimally supervised word sense disam-biguation for all words in open text.
In Proceedingsof ACL/SIGLEX Senseval-3.Dan I. Moldovan and Rada Mihalcea.
2000.
UsingWordNet and lexical operators to improve Internetsearches.
IEEE Internet Computing, 4(1):34-43.Roberto Navigli.
2006.
Meaningful Clustering ofSenses Helps Boost Word Sense Disambiguation Per-formance.
In Proceedings of COLING-ACL 2006.Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang.2004.
Different Sense Granularities for Different Ap-plications.
In Proceedings of Workshop on ScalableNatural Language Understanding.Martha Palmer, Hoa Trang Dang, and Christiane Fell-baum.
2005.
Making fine-grained and coarse-grainedsense distinctions.
Journal of Natural Language Engi-neering.Siddharth Patwardhan.
2003.
Incorporating dictionaryand corpus information into a context vector measureof semantic relatedness.
Master?s thesis, Univ.
of Min-nesota, Duluth.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity - Measuring theRelatedness of Concepts.
In Proceedings of NAACL2004.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional Clustering of English Words.
In Pro-ceedings of ACL 1993.Wim Peters, Ivonne Peters, and Piek Vossen.
1998.
Au-tomatic Sense Clustering in EuroWordNet.
In Pro-ceedings of LREC 1998.Andrew Philpot, Eduard Hovy, and Patrick Pantel.
2005.The Omega Ontology.
In Proceedings of the ON-TOLEX Workshop at IJCNLP 2005.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceedingsof the IJCAI 1995, 448-453.Philip Resnik and David Yarowsky.
1999.
Distinguish-ing systems and distinguishing senses: new evaluationmethods for word sense disambiguation.
Natural Lan-guage Engineering, 5(2):113-134.Noriko Tomuro.
2001.
Tree-cut and A Lexicon basedon Systematic Polysemy.
In Proceedings of NAACL2001.Zhibiao Wu and Martha Palmer.
1994.
Verb Semanticsand Lexical Selection.
In Proceedings of ACL 1994.Deniz Yuret.
2004.
Some experiments with a naivebayes wsd system.
In Proceedings of ACL/SIGLEXSenseval-3.1014
