Producing Biographical Summaries: Combining LinguisticKnowledge with Corpus Statistics1Barry SchiffmanColumbia University1214 Amsterdam AvenueNew York, NY 10027, USABschiff@cs.columbia.eduInderjeet Mani2The MITRE Corporation11493 Sunset Hills RoadReston, VA 20190, USAimani@mitre.orgKristian J. ConcepcionThe MITRE Corporation11493 Sunset Hills RoadReston, VA 20190, USAkjc9@mitre.org1This work has been funded by DARPA?s Translingual Information Detection, Extraction, and Summarization (TIDES)research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049.2Also at the Department of Linguistics, Georgetown University, Washington, D. C. 20037.AbstractWe describe a biographical multi-document summarizer that summarizesinformation about people described inthe news.
The summarizer uses corpusstatistics along with linguisticknowledge to select and mergedescriptions of people from a documentcollection, removing redundantdescriptions.
The summarizationcomponents have been extensivelyevaluated for coherence, accuracy, andnon-redundancy of the descriptionsproduced.1 IntroductionThe explosion of the World Wide Web hasbrought with it a vast hoard of information, mostof it relatively unstructured.
This has created ademand for new ways of managing this oftenunwieldy body of dynamically changinginformation.
The goal of automatic textsummarization is to take a partially-structuredsource text, extract information content from it,and present the most important content in acondensed form in a manner sensitive to theneeds of the user and task (Mani and Maybury1999).
Summaries can be ?generic?, i.e., aimedat a broad audience, or topic-focused, i.e.,tailored to the requirements of a particular useror group of users.
Multi-DocumentSummarization (MDS) is, by definition, theextension of single-document summarization tocollections of related documents.
MDS canpotentially help the user to see at a glance what acollection is about, or to examine similaritiesand differences in the information content in thecollection.Specialized multi-documentsummarization systems can be constructed forvarious applications; here we discuss abiographical summarizer.
Biographies can, ofcourse, be long, as in book-length biographies,or short, as in an author?s description on a bookjacket.
The nature of descriptions in thebiography can vary, from physicalcharacteristics (e.g., for criminal suspects) toscientific or other achievements (e.g., aspeaker?s biography).
The crucial point here isthat facts about a person?s life are selected,organized, and presented so as to meet thecompression and task requirements.While book-quality biographies are outof reach of computers, many other kinds can besynthesized by sifting through large quantities ofon-line information, a task that is tedious forhumans to carry out.
We report here on thedevelopment of a biographical MDS summarizerthat summarizes information about peopledescribed in the news.
Such a summarizer is ofinterest, for example, to analysts who want toautomatically construct a dossier about a personover time.Rather than determining in advancewhat sort of information should go into abiography, our approach is more data-driven,relying on discovering how people are actuallydescribed in news reports in a collection.
We usecorpus statistics from a background corpus alongwith linguistic knowledge to select and mergedescriptions from a document collection,removing redundant descriptions.
The focus hereis on synthesizing succinct descriptions.
Theproblem of assembling these descriptions into acoherent narrative is not a focus of our paper;the system currently uses canned text methods toproduce output text containing thesedescriptions.
Obviously, the merging ofdescriptions should take temporal informationinto account; this very challenging issue is alsonot addressed here.To give a clearer idea of the system?s output,here are some examples of biographies producedby our system (the descriptions themselves areunderlined, the rest is canned text).
Thebiographies contain descriptions of the salientattributes and activities of people in the corpus,along with lists of their associates.
These shortsummaries illustrate the extent of compressionprovided.
The first two summaries are of acollection of 1300 wire service news documentson the Clinton impeachment proceedings(707,000 words in all, called the ?Clinton?corpus).
In this corpus, there are 607 sentencesmentioning Vernon Jordan by name, from whichthe system extracted 82 descriptions expressedas appositives (78) and relative clauses (4),along with 65 descriptions consisting ofsentences whose deep subject is Jordan.
The 4relative clauses are duplicates of one another:?who helped Lewinsky find a job?.
The 78appositives fall into just 2 groups: ?friend?
(orequivalent descriptions, such as ?confidant?),?adviser?
(or equivalent such as ?lawyer?).
Thesentential descriptions are filtered in part basedon the presence of verbs like ?testify, ?plead?, or?greet?
that are strongly associated with thehead noun of the appositive, namely ?friend?.The target length can be varied to producelonger summaries.Vernon Jordan is a presidential friend and aClinton adviser.
He is 63 years old.
He helpedMs.
Lewinsky find a job.
He testified  that Ms.Monica Lewinsky said  that she hadconversations  with the president,  that shetalked  to the president.
He has numerousacquaintances, including Susan Collins, BettyCurrie, Pete Domenici, Bob Graham,  JamesJeffords and Linda Tripp.1,300 docs, 707,000 words (Clinton corpus) 607Jordan sentences, 78 extracted appositives, 2groups: friend, adviser.Henry Hyde is a Republican chairman of HouseJudiciary Committee and a prosecutor in Senateimpeachment trial.
He will lead the JudiciaryCommittee's impeachment review.
Hyde urgedhis colleagues  to heed  their consciences ,  ?thevoice  that whispers  in our ear ,  ?duty,  duty,duty.?
?Clinton corpus, 503 Hyde sentences, 108extracted appositives, 2 groups: chairman,impeachment prosecutor.Victor Polay  is the Tupac Amaru rebels' topleader,  founder and the organization'scommander-and-chief.
He was arrested  againin  1992  and is serving  a life sentence.
Hisassociates include  Alberto Fujimori, TupacAmaru Revolutionary, and Nestor Cerpa.73 docs, 38,000 words, 24 Polay sentences, 10extracted appositives, 3 groups: leader, founderand commander-in-chief.2 Producing biographical descriptions2.1 PreprocessingEach document in the collection to besummarized is processed by a sentencetokenizer, the Alembic part-of-speech tagger(Aberdeen et al 1995), the Nametag namedentity tagger  (Krupka 1995) restricted to peoplenames, and the CASS parser (Abney 1996).
Thetagged sentences are further analyzed by acascade of finite state machines leveragingpatterns with lexical and syntactic information,to identify constructions such as pre- and post-modifying appositive phrases, e.g., ?Presidentialcandidate George Bush?, ?Bush, the presidentialcandidate?, and relative clauses, e.g., ?Senator..., who is running for re-election this Fall,?.These appositive phrases and relative clausescapture descriptive information which cancorrespond variously to a person?s age,occupation, or some role a person played in anincident.
In addition, we also extract sententialdescriptions in the form of sentences whose(deep) subjects are person names.2.2 Cross-document coreferenceThe classes of person names identified withineach document are then merged acrossdocuments in the collection using a cross-document coreference program from theAutomatic Content Extraction (ACE) researchprogram (ACE 2000), which compares namesacross documents based on similarity of awindow of words surrounding each name, aswell as specific rules having to do with differentways of abbreviating a person?s name (Mani andMacMillan 1995).
The end result of this processis that for each distinct person, the set ofdescriptions found for that person in thecollection are grouped together.2.3 Appositives2.3.1 IntroductionThe appositive phrases usually providedescriptions of attributes of a person.
However,the preprocessing component described inSection 2.1 does produce errors in appositiveextraction, which are filtered out by syntacticand semantic tests.
The system also filters outredundant descriptions, both duplicatedescriptions as well as similar ones.
Thesefiltering methods are discussed next.2.3.2 Pruning Erroneous  and DuplicateAppositivesThe appositive descriptions are first pruned torecord only one instance of an appositive phrasewhich has multiple repetitions, and descriptionswhose head does not appear to refer to a person.The latter test relies on a person typing programwhich uses semantic information from WordNet1.6 (Miller 1995) to test whether the head of thedescription is a person.
A given string is judgedas a person if a threshold percentage ?1  (set to35% in our work) of senses of the string aredescended from the synset for Person inWordNet.
For example, this picks out ?counsel?as a person, but ?accessory?
as a non-person.2.3.3 Merging Similar AppositivesThe pruning of erroneous and duplicatedescriptions still leaves a large number ofredundant appositive descriptions acrossdocuments.
The system compares each pair ofappositive descriptions of a person, mergingthem based on corpus frequencies of thedescription head stem, syntactic information,and semantic information based on therelationship between the heads in WordNet.
Thedescriptions are merged if they have the samehead stem, or if both heads have a commonparent below Person in WordNet (in the lattercase the head which is more frequent in thecorpus is chosen as the merged head), or if onehead subsumes the other under Person inWordNet (in which case the more general headis chosen).When the heads of descriptions aremerged, the most frequent modifying phrase thatappears in the corpus with the selected head isused.
When a person ends up with more thanone description, the modifiers are checked forduplication, with distinct modifiers beingconjoined together, so that ?Wisconsinlawmaker?
and ?Wisconsin democrat?
yields?Wisconsin lawmaker and Democrat?.Prepositional phrase variants of descriptions arealso merged here, so that ?chairman of theBudget Committee?
and ?Budget CommitteeChairman?
are merged.
Modifiers are droppedbut their original order is preserved for the sakeof fluency.2.3.4 Appositive Description WeightingThe system then weights the appositives forinclusion in a summary.
A person?s appositivesare grouped into equivalence classes, with asingle head noun being chosen for eachequivalence class, with a weight for that classbased on the corpus frequency of the head noun.The system then picks descriptions in decreasingorder of class weight until either thecompression rate is achieved or the head noun isno longer in the top ?2 % most frequentdescriptions (?2 is set to 90% in our work).
Notethat the summarizer refrains from choosing asubsuming term from WordNet that is notpresent in the descriptions, preferring to not riskinventing new descriptions, instead confiningitself to cutting and pasting of actual words usedin the document.2.4 Relative Clause WeightingOnce the relative clauses have been pruned forduplicates, the system weights the appositiveclauses for inclusion in a summary.
Theweighting is based on how often the relativeclause?s main verb is strongly associated with a(deep) subject in a large corpus, compared to itstotal number of appearances in the corpus.
Theidea here is to weed out ?promiscuous?
verbsthat are weakly associated with lots of subjects.The corpus statistics are derived from theReuters portion of the North American NewsText Corpus (called ?Reuters?
in this paper) --nearly three years of wire service news reportscontaining 105.5 million words.Examples of verbs in the Reuters corpuswhich show up as promiscuous include ?get?,?like?, ?give?, ?intend?, ?add?, ?want?, ?be?,?do?, ?hope?, ?think?, ?make?, ?dream?,?have?, ?say?, ?see?, ?tell?, ?try?.
In a test,detailed below in Section 4.2, this feature fired40 times in 184 trials.To compute strong associations, weproceed as follows.
First, all subject-verb pairsare extracted from the Reuters corpus with aspecially developed finite state grammar and theCASS parser.
The head nouns and main verbsare reduced to their base forms by changingplural endings and tense markers for the verbs.Also included are ?gapped?
subjects, such as thesubject of ?run?
in ?the student promised to runthe experiment?
; in this example, both pairs?student-promise?
and ?student-run?
arerecorded.
Passive constructions are alsorecognized and the object of the by-PPfollowing the verb is taken as the deep subject.Strength of association between subject i andverb j is measured using mutual information(Church and Hanks 1990):)ln(),(jiijtftftfNjiMI?
?= .Here tfij is the maximum frequency ofsubject-verb pair ij in the Reuters corpus, tfi isthe frequency of subject head noun i in thecorpus, tfj is the frequency of verb j in thecorpus, and N is the number of terms in thecorpus.
The associations are only scored for tfcounts greater than 4, and a threshold ?3  (set tolog score > -21 in our work) is used for a strongassociation.The relative clauses are thus filteredinitially (Filter 1) by excluding those whosemain verbs are highly promiscuous.
Next, theyare filtered (Filter 2) based on various syntacticfeatures, as well as the number of proper namesand pronouns.
Finally, the relative clauses arescored conventionally (Filter 3) by summing thewithin-document relative term frequency ofcontent terms in the clause (i.e., relative to thenumber of terms in the document), with anadjustment for sentence length (achieved bydividing by the total number of content terms inthe clause).3 Sentential DescriptionsThese descriptions are the relatively large set ofsentences which have a person name as a (deep)subject.
We filter them based on whether theirmain verb is strongly associated with either ofthe head nouns of the appositive descriptionsfound for that person name (Filter 4).
Theintuition here is that particular occupationalroles will be strongly associated with particularverbs.
For example, politicians vote and elect,executives resign and appoint, police arrest andshoot; so, a summary of information about apoliceman may include an arresting andshooting event he was involved with.
(The verb-occupation association isn?t manifest in relativeclauses because the latter are too few innumber).A portion of the results of doing this isshown in Table 1.
The results for ?executive?are somewhat loose, whereas for ?politician?and ?police?, the associations seem tighter, withthe associated verbs meeting our intuitions.All sentences which survive Filter 4 areextracted and then scored, just as relative clausesare, using Filter 1 and Filter 3.
Filter 4 aloneprovides a high degree of compression; forexample, it reduces a total of 16,000 words inthe combined sentences that include VernonJordan' s name in the Clinton corpus to 578words in 12 sentences; sentences up to the targetlength can be selected from these based onscores from Filter 1 and then Filter 3.However, there are several difficulties withthese sentences.
First, we are missing a lot ofthem due to the fact that we do not as yet handlepronominal subjects which are coreferential withthe proper name.
Second, these sentencescontain lots of dangling anaphors, which willneed to be resolved.
Third, there may beredundancy between the sentential descriptions,on one hand, and the appositive and relativeclause descriptions, on the other.
Finally, theentire sentence is extracted, including anysubordinate clauses, although we are working onrefinements involving sentence compaction.
Asa result, we believe that more work is requiredbefore the sentential descriptions can be fullyintegrated into the biographies.executive police politicianreprimand16.36 shoot 17.37 clamor 16.94conceal 17.46 raid 17.65 jockey 17.53bank 18.27 arrest 17.96 wrangle 17.59foresee 18.85 detain 18.04 woo 18.92conspire 18.91 disperse 18.14 exploit 19.57convene 19.69 interrogate18.36 brand 19.65plead 19.83 swoop 18.44 behave 19.72sue 19.85 evict 18.46 dare 19.73answer 20.02 bundle 18.50 sway 19.77commit 20.04 manhandle18.59 criticize 19.78worry 20.04 search 18.60 flank 19.87accompany20.11confiscate18.63proclaim19.91own 20.22 apprehend18.71 annul 19.91witness 20.28 round 18.78 favor 19.92testify 20.40 corner 18.80 denounce20.09shift 20.42 pounce 18.81 condemn20.10target 20.56 hustle 18.83 prefer 20.14lie 20.58 nab 18.83 wonder 20.18expand 20.65 storm 18.90 dispute 20.18learn 20.73 tear 19.00 interfere 20.37shut 20.80 overpower19.09 voice 20.38Table 1.
Verbs strongly associated withparticular classes of people in the Reuterscorpus (negative log scores).4 Evaluation4.1 OverviewMethods for evaluating text summarization canbe broadly classified into two categories(Sparck-Jones and Galliers 1996).
The first, anextrinsic evaluation, tests the summarizationbased on how it affects the completion of someother task, such as comprehension, e.g., (Morriset al 1992), or relevance assessment (Brandowet al 1995) (Jing et al 1998) (Tombros andSanderson 1998) (Mani et al 1998).
An intrinsicevaluation, on the other hand, can involveassessing the coherence of the summary(Brandow et al 1995) (Saggion and Lapalme2000).Another intrinsic approach involvesassessing the informativeness of the summary,based on to what extent key information fromthe source is preserved in the system summary atdifferent levels of compression (Paice and Jones1993), (Brandow et al 1995).
Informativenesscan also be assessed in terms of how muchinformation in an ideal (or ?reference?)
summaryis preserved in the system summary, where thesummaries being compared are at similar levelsof compression  (Edmundson 1969).We have carried out a number of intrinsicevaluations of the accuracy of componentsinvolved in the summarization process, as wellas the succinctness, coherence andinformativeness of the descriptions.
As this is aMDS system, we also evaluate the non-redundancy of the descriptions, since similarinformation may be repeated across documents.4.2 Person Typing EvaluationThe component evaluation tests how accuratelythe tagger can identify whether a head noun in adescription is appropriate as a person descriptionThe evaluation uses the WordNet 1.6 SEMCORsemantic concordance, which has files from theBrown corpus whose words have semantic tags(created by WordNet' s creators) indicatingWordNet sense numbers.
Evaluation on 6,000sentences with almost 42,000 nouns comparespeople tags generated by the program withSEMCOR tags, and provided the followingresults: right = 41,555, wrong = 1,298, missing= 0, yielding Precision, Recall, and F-Measureof 0.97.4.3 Relative Clause Extraction EvaluationThis component evaluation tests the well-formedness of the extracted relative clauses.
Forthis evaluation, we used the Clinton corpus.
Therelative clause is judged correct if it has the rightextent, and the correct coreference indexindicating which person the relative clausedescription pertains to.
The judgments are basedon 36 instances of relative clauses from 22documents.
The results show 28 correct relativeclauses found, plus 4 spurious finds, yieldingPrecision of 0.87, Recall of 0.78, and F-measureof .82.
Although the sample is small, the resultsare very promising.4.4 Appositive Merging EvaluationThis component evaluation tests the system?sability to accurately merge appositivedescriptions.
The score is based on an automaticcomparison of the system?s merge of system-generated appositive descriptions against ahuman merge of them.
We took all the namesthat were identified in the Clinton corpus andran the system on each document in the corpus.We took the raw descriptions that the systemproduced before merging, and wrote a briefdescription by hand for each person who hadtwo or more raw descriptions.
The hand-writtendescriptions were not done with any reference tothe automatically merged descriptions nor withany reference to the underlying source material.The hand-written descriptions were thencompared with the final output of the system(i.e., the result after merging).
The comparisonwas automatic, measuring similarity amongvectors of content words (i.e., stop words suchas articles and prepositions were removed).Here is an example to further clarify thestrict standard of the automatic evaluation(words scored correct are underlined):System: E. Lawrence Barcella is a Washingtonlawyer, Washington white-collar defense lawyer,former federal prosecutorSystem Merge: Washington white-collar defenselawyerHuman Merge: a Washington lawyer and formerfederal prosecutorAutomatic Score: Correct=2; Extra-Words=2;Missed-Words=3Thus, although ?lawyer?
and?prosecutor?
are synonymous in WordNet, theautomatic scorer doesn?t know that, and so?prosecutor?
is penalized as an extra word.The evaluation was carried out over theentire Clinton corpus, with descriptionscompared for 226 people who had more thanone description.
65 out of the 226 descriptionswere Correct (28%), with a further 32 casesbeing semantically correct ?obviously similar?substitutions which the automatic scorer missed(giving an adjusted accuracy of 42%).
As abaseline, a merging program which performedjust a string match scored 21% accuracy.
Themajor problem areas were errors in coreference(e.g., Clinton family members being put in thesame coreference class), lack of gooddescriptions for famous people (news articlestend not to introduce such people), and parsinglimitations (e.g., ?Senator Clinton?
being parsederroneously as an NP in ?The Senator Clintondisappointed??).
Ultimately, of course,domain-independent systems like ours arelimited semantically in merging by the lack ofworld knowledge, e.g., knowing that Starr' schief lieutenant can be a prosecutor.4.5 Description Coherence andInformativeness EvaluationTo assess the coherence and informativeness ofthe relative clause descriptions3, we asked  4subjects who were unaware of our research tojudge descriptions generated by our system fromthe Clinton corpus.
For each relative clausedescription, the subject was given thedescription, a person name to whom thatdescription pertained, and a capsule descriptionconsisting of merged appositives created by thesystem.
The subject was asked to assess (a) thecoherence of the relative clause description interms of its succinctness (was it a good length?
)and its comprehensibility (was it andunderstandable by itself or in conjunction withthe capsule?
), and (b) its informativeness interms of whether it was an accurate description(does it conflict with the capsule or with whatyou know?)
and whether it was non-redundant(is it distinct or does it repeat what is in thecapsule?
).The subjects marked 87% of thedescriptions as accurate, 96% as non-redundant,and 65% as coherent.
A separate 3-subject inter-3Appositives are not assessed in this way as few errors ofcoherence or informativeness were noticed in theappositive extraction.annotator agreement study, where all subjectsjudged the same 46 decisions, showed that allthree subjects agreed on 82% of the accuracydecisions, 85% of the non-redundancy decisionsand 82% of the coherence decisions.5 Learning  to Produce CoherentDescriptions5.1 OverviewTo learn rules for coherence for extractingsentential descriptions, we used the examplesand judgments we obtained for coherence in theevaluation of relative clause descriptions inSection 4.5.
Our focus was on features thatmight relate to content and specificity: low verbpromiscuity scores, presence of proper names,pronouns, definite and indefinite clauses.
Theentire list is as follows:badend:boolean.
is there an impossibleend, indicating a bad extraction (... Mr.)?bestverb:continuous.
use the verbpromiscuity threshhold ?3 tofind the score of the most non-promiscuous verb in the clauseclasses(label):boolean.
accept the clause,reject the clausecountpronouns:continuous.
number of personalpronounscountproper:continuous.
number of nounstagged as NPhasobject: continuous.
how many np'sfollow the verb?haspeople: continuous.
how many "name"constituents are found?haspossessive:continuous.
how manypossessive pronouns are there?hasquote: boolean.
is there a quotation?hassubc: boolean.
is there a subordinateclause?isdefinite: continuous.
how many definiteNP's are there?repeater: boolean.
is the subject's namerepeated, or is there no subject?timeref: boolean.
is there a timereference?withquit: is there a ?quit?
or ?resign?verb?withsay: boolean.
is there a ?say?
verb inthe clause?5.2 Accuracy of Learnt DescriptionsTable 2 provides information on differentlearning methods.
The results are for a ten-foldcross-validation on 165 training vectors and 19test vectors, measured in terms of PredictiveAccuracy (percentage test vectors correctlyclassified).Tool AccuracyBarry?s Rules .69MC4 Decision Tree .69C4.5Rules .67Ripper .62Naive Bayes .62Majority Class (coherent) .60Table 2.
Accuracy of Different DescriptionLearners on Clinton corpusThe best learning methods are comparablewith rules created by hand by one of the authors(Barry?s rules).
In the learners, the bestverbfeature is used heavily in tests for the negativeclass, whereas in Barry?s Rules it occurs in testsfor the positive class.6 Related WorkOur work on measuring subject-verbassociations has a different focus from theprevious work.
(Lee and Pereira 1999), forexample, examined verb-object pairs.
Theirfocus was on a method that would improvetechniques for gathering statistics where thereare a multitude of sparse examples.
We arefocusing on the use of the verbs for the specificpurpose of finding associations that we havepreviously observed to be strong, with a viewtowards selecting a clause or sentence, ratherthan just to measure similarity.
We also try tostrengthen the numbers by dealing with ?gapped?constructions.While there has been plenty of work onextracting named entities and relations betweenthem, e.g., (MUC-7 1998), the main previousbody of work on biographical summarization isthat of (Radev and McKeown 1998).
Thefundamental differences in our work are asfollows: (1) We extract not only appositivephrases, but also clauses at large based oncorpus statistics; (2) We make heavy use ofcoreference, whereas they don?t use coreferenceat all; (3) We focus on generating succinctdescriptions by removing redundancy andmerging, whereas they categorize descriptionsusing WordNet, without a focus on succinctness.7 ConclusionThis research has described and evaluatedtechniques for producing a novel kind ofsummary called biographical summaries.
Thetechniques use syntactic analysis and semantictype-checking (from WordNet), in combinationwith a variety of corpus statistics.
Futuredirections could include improved sententialdescriptions as well as further intrinsic andextrinsic evaluations of the summarizer as awhole (i.e., including canned text).ReferencesJ.
Aberdeen, J. Burger, D. Day, L. Hirschman,P.
Robinson, and M. Vilain.
1995.
?MITRE:Description of the Alembic System system as usedfor MUC-6?.
In Proceedings of the Sixth MessageUnderstanding Conference (MUC-6), Columbia,Maryland.S.
Abney.
1996.
?Partial parsing Via Finite-StateCascades?.
Proceedings of the ESSLLI '96 RobustParsing Workshop.Automatic Context Extraction Program.http://www.nist.gov/speech/tests/ace/index.htmR.
Brandow, K. Mitze, and L. Rau.
1995.
?Automaticcondensation of electronic publications bysentence selection.?
Information Processing andManagement 31(5): 675-685.
Reprinted inAdvances in Automatic Text Summarization, I.Mani and M.T.
Maybury (eds.
), 293-303.Cambridge, Massachusetts: MIT Press.K.
W. Church and P. Hanks.
1990.
?Word associationnorms, mutual information, and lexicography?.Computational Linguistics 16(1): 22-29.H.
P. Edmundson.
1969.
?New methods in automaticabstracting?.
Journal of the Association forComputing Machinery 16 (2): 264-285.
Reprintedin Advances in Automatic Text Summarization, I.Mani and M.T.
Maybury (eds.
), 21-42.Cambridge, Massachusetts: MIT Press.G.
Krupka.
1995.
?SRA: Description of the SRAsystem as used for MUC-6?.
In Proceedings of theSixth Message Understanding Conference (MUC-6), Columbia, Maryland.L.
Lee and F. Pereira.
1999.
?DistributionalSimilarity Models: Clustering vs. NearestNeighbors?.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics, 33-40.I.
Mani and T. MacMillan.
1995.
?IdentifyingUnknown Proper Names in Newswire Text?.
InCorpus Processing for Lexical Acquisition, B.Boguraev and J. Pustejovsky (eds.
), 41-73.Cambridge, Massachusetts: MIT Press.I.
Mani and M. T. Maybury.
(eds.).
1999.
Advancesin Automatic Text Summarization.
Cambridge,Massachusetts: MIT Press.G.
Miller.
1995.
?WordNet: A Lexical Database forEnglish?.
Communications of the Association ForComputing Machinery (CACM) 38(11): 39-41.A.
Morris, G. Kasper, and D. Adams.
1992.
?TheEffects and Limitations of Automatic TextCondensing on Reading ComprehensionPerformance?.
Information Systems Research 3(1):17-35.
Reprinted in Advances in Automatic TextSummarization, I. Mani and M.T.
Maybury (eds.),305-323.
Cambridge, Massachusetts: MIT Press.MUC-7.
1998.
Proceedings of the Seventh MessageUnderstanding Conference, DARPA.C.
D. Paice and P. A. Jones.
1993.
?TheIdentification of Important Concepts in HighlyStructured Technical Papers.?
In Proceedings ofthe 16th International Conference on Researchand Development in Information Retrieval(SIGIR'93), 69-78.D.
R. Radev and K. McKeown.
1998.
?GeneratingNatural Language Summaries from Multiple On-Line Sources?.
Computational Linguistics 24(3):469-500.H.
Saggion and G. Lapalme.
2000.
?ConceptIdentification and Presentation in the Context ofTechnical Text Summarization?.
In Proceedings ofthe Workshop on Automatic Summarization, 1-10.K.
Sparck-Jones and J. Galliers.
1996.
EvaluatingNatural Language Processing Systems: AnAnalysis and Review.
Lecture Notes in ArtificialIntelligence 1083.
Berlin: Springer.A.
Tombros and M. Sanderson.
1998.?Advantages ofquery biased summaries in information retrieval?.In Proceedings of the 21st InternationalConference on Research and Development inInformation Retrieval (SIGIR'98), 2-10.
