Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 33?40,Sydney, July 2006. c?2006 Association for Computational LinguisticsExploring Semantic Constraints for Document RetrievalHua Cheng, Yan Qu, Jesse Montgomery, David A. EvansClairvoyance Corporation5001 Baum Blvd., Suite 700, Pittsburgh, PA 15213, U.S.A.{H.Cheng, Y.Qu, J.Montgomery, dae}@clairvoyancecorp.comAbstractIn this paper, we explore the use of struc-tured content as semantic constraints forenhancing the performance of traditionalterm-based document retrieval in specialdomains.
First, we describe a method forautomatic extraction of semantic contentin the form of attribute-value (AV) pairsfrom natural language texts based ondomain models constructed from a semi-structured web resource.
Then, we ex-plore the effect of combining a state-of-the-art term-based IR system and a sim-ple constraint-based search system thatuses the extracted AV pairs.
Our evalua-tion results have shown that such combi-nation produces some improvement in IRperformance over the term-based IR sys-tem on our test collection.1 IntroductionThe questions of where and how sophisticatednatural language processing techniques can im-prove traditional term-based information re-trieval have been explored for more than a dec-ade.
A considerable amount of work has beencarried out that seeks to leverage semantic in-formation for improving traditional IR.
EarlyTREC systems such as INQUERY handled bothnatural language and semi-structured queries andtried to search for constraint expressions forcountry and time etc.
in queries (Croft et al,1994).
Later work, as discussed in (Strzalkowskiet al, 1996), has focused on exploiting semanticinformation at the word level, including variousattempts at word-sense disambiguation, e.g.,(Voorhees, 1998), or the use of special-purposeterms; other approaches have looked at phrase-level indexing or full-text query expansion.
Noapproaches to date, however, have sought to em-ploy semantic information beyond the wordlevel, such as that expressed by attribute-value(AV) pairs, to improve term-based IR.Attribute-value pairs offer an abstraction forinstances of many application domains.
For ex-ample, a person can be represented by a set ofattributes such as name, date-of-birth, job title,and home address, and their associated values; ahouse has a different set of attributes such as ad-dress, size, age and material; many productspecifications can be mapped directly to AVpairs.
AV pairs represent domain specific seman-tic information for domain instances.Using AV pairs as semantic constraints for re-trieval is related to some recent developments inareas such as Semantic Web retrieval, XMLdocument retrieval, and the integration of IR anddatabases.
In these areas, structured informationis generally assumed.
However, there is abundantand rich information that exists in unstructuredtext only.
The goal of this work includes first toexplore a method for automatically extractingstructured information in the form of AV pairsfrom text, and then to utilize the AV pairs as se-mantic constraints for enhancing traditionalterm-based IR systems.The paper is organized as follows.
Section 2describes our method of adding AV annotationsto text documents that utilizes a domain modelautomatically extracted from the Web.
Section 3presents two IR systems using a vector spacemodel and semantic constraints respectively, aswell as a system that combines the two.
Section 4describes the data set and topic set for evaluatingthe IR systems.
In Section 5, we compare theperformance of the three IR systems, and drawinitial conclusions on how NLP techniques canimprove traditional IR in specific domains.2 Domain-Driven AV ExtractionThis section describes a method that automati-cally discovers attribute-value structures fromunstructured texts, the result of which is repre-sented as texts annotated with semantic tags.33We chose the digital camera domain to illus-trate and evaluate the methodology described inthis paper.
We expect this method to be applica-ble to all domains whose main features can berepresented as a set of specifications.2.1 Construction of Domain ModelA domain model (DM) specifies a terminologyof concepts, attributes and values for describingobjects in a domain.
The relationships betweenthe concepts in such a model can be heterogene-ous (e.g., the link between two concepts canmean inheritance or containment).
In this work,a domain model is used for establishing a vo-cabulary as well as for establishing the attribute-value relationship between phrases.For the digital camera domain, we automati-cally constructed a domain model from existingWeb resources.
Web sites such as epinions.comand dpreview.com generally present informationabout cameras in HTML tables generated frominternal databases.
By querying these databasesand extracting table content from the dynamicweb pages, we can automatically reconstruct thedatabases as domain models that could be usedfor NLP purposes.
These models can optionallybe organized hierarchically.
Although domainmodels generated from different websites of thesame domain are not exactly the same, they oftenshare many common features.From the epinions.com product specificationsfor 1157 cameras, we extracted a nearly compre-hensive domain model for digital cameras, con-sisting of a set of attributes (or features) and theirpossible values.
A portion of the model is repre-sented as follows:{Digital Camera}<Brand> <Price> <Lens>{Brand}(57) Canon(33) Nikon{Price} $(136) 100 - 200(100) >= 400{Lens}<Optical Zoom> <Focus Range>{Optical Zoom} x(17) 4(3) 2.5{Focus Range} in., ?
(2) 3.9 - infinity(1) 12 - infinityIn this example, attributes are shown in curlybrackets and sub-attributes in angle brackets.Attributes are followed by possible units for theirnumerical values.
Values come below the attrib-utes, headed by their frequencies in all specifica-tions.
The frequency information (in parentheses)is used to calculate term weights of attributes andvalues.Specifications in HTML tables generally donot specify explicitly the type restrictions on val-ues (even though the types are typically definedin the underlying databases).
As type restrictionscontain important domain information that isuseful for value extraction, we recover the typerestrictions by identifying patterns in values.
Forexample, attributes such as price or dimensionusually have numerical values, which can be ei-ther a single number (?$300?
), a range (?$100 -$200?
), or a multi-dimensional value (?4 in.
x 3in.
x 2 in.?
), often accompanied by a unit, e.g., $or inches, whereas attributes such as brand andaccessory usually have string values, e.g.,?Canon?
or ?battery charger?.We manually compile a list of units for identi-fying numerical values, which is partially do-main general.
We identify range and multi-dimensional values using such patterns as ?A ?B?, ?A to B?, ?less than A?, and ?A x B?, etc.Numerical values are then normalized to a uni-form format.2.2 Identification of AV PairsBased on the constructed domain model, we canidentify domain values in unstructured texts andassign attribute names and domains to them.
Wefocus on extracting values of a domain attribute.Attribute names appearing by themselves are notof interest here because attribute names alonecannot establish attribute-value relations.
How-ever, identification of attribute names is neces-sary for disambiguation.The AV extraction procedure contains the fol-lowing steps:1.
Use MINIPAR (Lin, 1998) to generatedependency parses of texts.2.
For all noun phrase chunks in parses, it-eratively match sub-phrases of eachchunk with the domain model to find allpossible matches of attribute names andvalues above a threshold:?
A chunk contains all words up tothe noun head (inclusive);?
Post-head NP components (e.g.,PP and clauses) are treated asseparate chunks.3.
Disambiguate values with multiple at-tribute assignments using the sentencecontext, with a preference toward closercontext based on dependency.344.
Mark up the documents with XML tagsthat represent AV pairs.Steps 2 and 3 are the center of the AV extrac-tion process, where different strategies are em-ployed to handle values of different types andwhere ambiguous values are disambiguated.
Wedescribe these strategies in detail below.Numerical ValueNumerical values are identified based on the unitlist and the range and multi-dimensional numberpatterns described earlier in Section 2.1.
Thepredefined mappings between units and attrib-utes suggest attribute assignment.
It is possiblethat one unit can be mapped to multiple attrib-utes.
For example, ?x?
can be mapped to eitheroptical zoom or digital zoom, both of which arekept as possible candidates for future disam-biguation.
For range and multi-dimensionalnumbers, we find all attributes in the domainmodel that have at least one matched range ormulti-dimensional value, and keep attributesidentified by either a unit or a pattern as candi-dates.
Numbers without a unit can only bematched exactly against an existing value in thedomain model.String ValueHuman users often refer to a domain entity indifferent ways in text.
For example, a cameracalled ?Canon PowerShot G2 Black DigitalCamera?
in our domain model is seldom men-tioned exactly this way in ads or reviews, butrather as ?Canon PowerShot G2?, ?Canon G2?,etc.
However, a domain model generally onlyrecords full name forms rather than their all pos-sible variations.
This makes the identification ofdomain values difficult and invalidates the use ofa trained classifier that needs training samplesconsisting of a large variety of name references.An added difficulty is that web texts oftencontain grammatical errors and incomplete sen-tences as well as large numbers of out-of-vocabulary words and, therefore, make the de-pendency parses very noisy.
As a result, effec-tiveness of extraction algorithms based on certaindependency patterns can be adversely affected.Our approach makes use of the more accurateparser functionalities of part-of-speech taggingand phrase boundary detection, while reducingthe reliance on low level dependency structures.For noun phrase chunks extracted from parsetrees, we iteratively match all sub-phrases ofeach chunk with the domain model to findmatching attributes and values above a threshold.It is often possible to find multiple AV pairs in asingle NP chunk.Assigning domain attributes to an NP is essen-tially a classification problem.
In our domainmodel, each attribute can be seen as a target classand its values as the training set.
For a newphrase, the idea is to find the value in the domainmodel that is most similar and then assign theattribute of this nearest neighbor to the phrase.This motivates us to adopt K Nearest Neighbor(KNN) (Fix and Hodges, 1951) classification forhandling NP values.
The core of KNN is a simi-larity metric.
In our case, we use word editingdistance (Wagner and Fischer, 1974) that takesinto account the cost of word insertions, dele-tions, and substitutions.
We compute word edit-ing distance using dynamic programming tech-niques.Intuitively, words do not carry equal weightsin a domain.
In the earlier example, words suchas ?PowerShot?
and ?G2?
are more importantthan ?digital?
and ?camera?, so editing costs forsuch words should be higher.
This draws ananalogy to the metric of Inverse Document Fre-quency (IDF) in the IR community, used tomeasure the discriminative capability of a termin a document collection.
If we regard each valuestring as a document, we can use IDF to measurethe weight of each term in a value string to em-phasize important domain terms and de-emphasize more general ones.
The normalizedcost is computed as:)log(/)/log( TNNTNwhere TN is the total number of values for anattribute, and N is the number of values where aterm occurs.
This equation assigns higher cost tomore discriminative terms and lower cost tomore general terms.
It is also used to computecosts of terms in attribute names.
For words notappearing in a class the cost is 1, the maximumcost.The distance between a new phrase and a DMphrase is then calculated using word editing costbased on the costs of substitution, insertion, anddeletion, whereCostsub = (CostDM + Costnew) / 2Costins = CostnewCostdel = CostDMCostedit = min(Costsub, Costins, Costdel)where CostDM is the cost of a word in a domainvalue (i.e., its normalized IDF score), and Costnew35is that of a word in the new phrase.
The cost isalso normalized by the larger of the weightedlengths of the two phrases.
We use a thresholdof 0.6 to cut off phrases with higher cost.For a phrase that returns only a couple ofmatches, the similarity, i.e., the matching prob-ability, is computed as 1 - Costedit; otherwise, thesimilarity is the maximum likelihood of an at-tribute based on the number of returned valuesbelonging to this attribute.Disambiguation by Sentence ContextThe AV identification process often returns mul-tiple attribute candidates for a phrase that needsto be further disambiguated.
The words close tothe phrase usually provide good indications ofthe correct attribute names.
Motivated by thisobservation, we design the disambiguation pro-cedure as follows.
First we examine the siblingnodes of the target phrase node in the depend-ency structure for a mention of an attribute namethat overlaps with a candidate.
Next, we recur-sively traverse upwards along the dependencytree until we find an overlap or reach the top ofthe tree.
If an overlap is found, that attribute be-comes the final assignment; otherwise, the at-tribute with the highest probability is assigned.This method gives priority to the context closest(in terms of dependency) to the target phrase.
Forexample, in the sentence ?The 4x stepless digitalzoom lets you capture intricate details?
(parsetree shown below), ?4x?
can be mapped to bothoptical zoom and digital zoom, but the sentencecontext points to the second candidate.3 Document Retrieval SystemsThis section introduces three document retrievalsystems: the first one retrieves unstructured textsbased on vector space models, the second onetakes advantage of semantic structures con-structed by the methods in Section 2, and the lastone combines the first two systems.3.1 Term-Based Retrieval (S1)Our system for term-based retrieval from un-structured text is based on the CLARIT system,implementing a vector space retrieval model (Ev-ans and Lefferts, 1995; Qu et al, 2005).
TheCLARIT system identifies terms in documentsand constructs its index based on NLP-determined linguistic constituents (NPs, sub-phrases and words).
The index is built upon fulldocuments or variable-length subdocuments.
Weused subdocuments in the range of 8 to 12 sen-tences as the basis for indexing and scoringdocuments in our experiments.Various similarity measures are supported inthe model.
For the experiments described in thepaper, we used the dot product function for com-puting similarities between a query and a docu-ment:where WQ(t) is the weight associated with thequery term t and WD(t) is the weight associatedwith the term t in the document D. The twoweights were computed as follows:where IDF and TF are standard inverse docu-ment frequency and term frequency statistics,respectively.
IDF(t) was computed with the tar-get corpus for retrieval.
The coefficient C(t) is an?importance coefficient?, which can be modifiedeither manually by the user or automatically bythe system (e.g., updated during feedback).For term-based document retrieval, we havealso experimented with pseudo relevance feed-back (PRF) with various numbers of retrieveddocuments and various numbers of terms fromsuch documents for query expansion.
While PRFdid result in improvement in performance, it wasnot significant.
This is probably due to the factthat in this restricted domain, there is not muchvocabulary variation and thus the advantage ofusing query expansion is not fully realized.3.2 Constraint-Based Retrieval (S2)The constraint-based retrieval approach searchesthrough the AV-annotated document collectionbased on the constraints extracted from queries.Given a query q, our constraint-based systemscores each document in the collection by com-paring the extracted AV pairs with the con-straints in q.
Suppose q has a constraint c(a, v)that restricts the value of the attribute a to v,where v can be either a concrete value (e.g., 5megapixels) or a range (e.g., less than $400).
If a)()()()( tIDFtTFtCtW QQ ??=).
()()( tIDFtTFtW DD ?=).
()(),( tWtWDQsim DDQtQ ?= ??
?36is present in a document d with a value v?
thatsatisfies v, that is, v?= v if v is a concrete value orv?
falls in the range defined by v, d is given apositive score w. However, if v?
does not satisfyv, then d is given a negative score -w. No men-tion of a does not change the score of d, exceptthat, when c is a string constraint, we use a back-off model that awards d a positive score w if itcontains v as a substring.
The final score of dgiven q is the sum of all scores for each con-straint in q, normalized by the maximum scorefor q: ?=niiiwc1, where ci is one of the n con-straints specified in q and wi its score.We rank the documents by their scores.
Thisscoring schema facilitates a sensible cutoff point,so that a constraint-based retrieval system canreturn 0 or fewer than top N documents when aquery has no or very few relevant documents.3.3 Combined Retrieval (S3)Lee (1997) analyzed multiple post-search datafusion methods using TREC3 ad hoc retrievaldata and explained the combination of differentsearch results on the grounds that different runsretrieve similar sets of relevant documents, butdifferent sets of non-relevant documents.
Thecombination methods therefore boost the ranksof the relevant documents.
One method studiedwas the summation of individual similarities,which bears no significant difference from thebest approach (i.e., further multiply the summa-tion with the number of nonzero similarities).Our system therefore adopts the summationmethod for its simplicity.
Because the scoresfrom term-based and constraint-based retrievalare normalized, we simply add them together foreach document retrieved by both approaches andre-rank the documents based on their new scores.More sophisticated combination methods can beexplored here, such as deciding which score toemphasize based on the characterizations of thequeries, e.g., whether a query has more numeri-cal values or string values.4 Experimental StudyIn this section, we describe the experiments weperformed to investigate combining terms andsemantic constraints for document retrieval.4.1 Data SetsTo construct a domain corpus, we used searchresults from craigslist.org.
We chose the ?forsale ?
electronics?
section for the ?San FranciscoBay Area?.
We then submitted the search term?digital camera?
in order to retrieve advertise-ments.
After manually removing duplicates andexpired ads, our corpus consisted of 437 adsposted between 2005-10-28 and 2005-11-07.
Atypical ad is illustrated below, with a small set ofXML tags specifying the fields of the title of thead (title), date of posting (date), ad body (text),ad id (docno), and document (doc).
The lengthof the documents varies considerably, from 5 or6 sentences to over 70 (with specifications cop-ied from other websites).
The ads have an aver-age length of 230 words.<doc><docno>docid519</docno><title>brand new 12 mega pixel digital cam-era</title><date>2005-11-07,  8:27AM PST</date><text>BRAND NEW 12 mega pixel digital cam-era..............only $400,-12 Mega pixels (4000x3000) Max Resolution-2.0 Color LCD Display-8x Digital Zoom-16MB Built-In (internal) Memory-SD or MMC card (external) Memory-jpeg picture formatALSO COMES WITH SOFTWARE & CABLES</text></doc>The test queries were constructed based onhuman written questions from the Digital Pho-tography Review website (www.dpreview.com)Q&A forums, which contain discussions fromreal users about all aspects of digital photogra-phy.
Often, users ask for suggestions on purchas-ing digital cameras and formulate their needs as aset of constraints.
These queries form the base ofour topic collection.The following is an example of such a topicmanually annotated with the semantic constraintsof interest to the user:<topic><id>1</id><query>I wanted to know what kind of Digital SLR cam-era I should buy.
I plan to spend nothing higherthan $1500.
I was told to check out the NikonD70.</query><constraint><hard: type = ?SLR?
/><hard: price le $1500 /><soft: product_name = ?Nikon D70?
/></constraint></topic>37In this example, the user query text is in thequery field and the manually extracted AV con-straints based on the domain model are in theconstraint field.
Two types of constraints aredistinguished: hard and soft.
The hard constraintsmust be satisfied while the soft constraints can berelaxed.
Manual determination of hard vs. softconstraints is based on the linguistic features inthe text.
Automatic constraint extraction goesone step beyond AV extraction for the need toidentify relations between attributes and values,for example, ?nothing higher than?
indicates a?<=?
relationship.
Such constraints can be ex-tracted automatically from natural text using apattern-based method.
However, we have yet toproduce a rich set of patterns addressing con-straints.
In addition, such query capability can besimulated with a form-based parametric searchinterface.In order to make a fair comparison betweensystems, we use only phrases in the manuallyextracted constraints as queries to system S1.
Forthe example topic, S1 extracted the NP terms?SLR?, ?1500?
and ?Nikon D70?.
During re-trieval, a term is further decomposed into its sub-terms for similarity matching.
For instance, theterm ?Nikon D70?
is decomposed into subterms?Nikon?
and ?D70?
and thus documents thatmention the individual subterms can be retrieved.For this topic, the system S2 produced annota-tions as those shown in the constraint field.Table 1 gives a summary of the distributionstatistics of terms and constraints for 30 topicsselected from the Digital Photography Reviewwebsite.Average Min MaxNo.
of terms 13.2 2 31No.
of constraints 3.2 1 7No.
of hard constraints 2.4 1 6No.
of soft constraints 0.8 0 3No.
of string constraints 1.4 0 5No.
of numerical constraints 1.8 0 4Table 1: Summary of the distribution statistics ofterms and constraints in the test topics4.2 Relevance JudgmentsInstead of using human subjects to give rele-vance judgments for each document and querycombination, we use a human annotator to markup all AV pairs in each document, using theGATE annotation tool (Cunningham et al 2002).The attribute set contains the 40 most importantattributes for digital cameras based on automati-cally computed term distributions in our data set.The inter-annotator agreement (without annotatortraining) as measured by Kappa is 0.72, whichsuggests satisfactory agreement.Annotating AV pairs in all documents gives usthe capability of making relevance judgmentsautomatically, based on the number of matchesbetween the AV pairs in a document and theconstraints in a topic.
This automatic approach isreasonable because unlike TREC queries whichare short and ambiguous, the queries in our ap-plication represent very specific informationneeds and are therefore much longer.
The lack ofambiguity makes our problem closer to booleansearch with structured queries like SQL than tra-ditional IR search.
In this case, a human assessorshould give the same relevance judgments as ourautomatic system if they follow the same instruc-tions closely.
An example instruction could be ?adocument is relevant if it describes a digital cam-era whose specifications satisfy at least one con-straint in the query, otherwise it is not relevant?
(similar to the narrative field of a TREC topic).We specify two levels of relevance: strict andrelaxed.
Strict means that all hard constraints of atopic have to be satisfied for a document to berelevant to the topic, whereas relaxed means thatat least half of the hard constraints have to besatisfied.
Soft constraints play no role in a rele-vance judgment.
The advantage of the automaticapproach is that when the levels of relevance aremodified for different application purposes, therelevance judgment can be recomputed easily,whereas in the manual approach, the human as-sessor has to examine all documents again.0204060801001201401601 3 5 7 9 11 13 15 17 19 21 23 25 27 29Topic NumberNumber of Relevant Docsstrict_judgments relaxed_judgmentsFigure 1: Distribution of relevant documentsacross topics for relaxed and strict judgmentsFigure 1 shows the distributions of the rele-vant documents for the test topic set.
With strictjudgments, only 20 out of the 30 topics haverelevant documents, and among them 6 topics38have fewer than 10 relevant documents.
The top-ics with many constraints are likely to result inlow numbers of relevant documents.
The averagenumbers of relevant documents for the set are57.3 for relaxed judgments, and 18 for strictjudgments.5  Results and DiscussionOur goal is to explore whether using semanticinformation would improve document retrieval,taking into account the errors introduced by se-mantic processing.
We therefore evaluate twoaspects of our system: the accuracy of AV ex-traction and the precision of document retrieval.5.1 Evaluate AV ExtractionWe tested the AV extraction system on a portionof the annotated documents, which contains 253AV pairs.
Of these pairs, 151 have string values,and the rest have numerical values.The result shows a prediction accuracy of50.6%, false negatives (missing AV pairs)35.2%, false positives 11%, and wrong predica-tions 3%.
Some attributes such as brand andresolution have higher extraction accuracy thanother attributes such as shooting mode and di-mension.
An analysis of the missing pairs revealsthree main sources of error: 1) an incompletedomain model, which misses such camera Con-dition phrases as ?minor surface scratching?
; 2) anoisy domain model, due to the automatic natureof its construction; 3) parsing errors caused byfree-form human written texts.
Considering thatthe predication accuracy is calculated over 40attributes and that no human labor is involved inconstructing the domain model, we consider ourapproach a satisfactory first step toward explor-ing the AV extraction problem.5.2 Evaluate AV-based Document RetrievalThe three retrieval systems (S1, S2, and S3) eachreturn top 200 documents for evaluation.
Figure2 summarizes the precision they achieved againstboth the relaxed and strict judgments, measuredby the standard TREC metrics (PN ?
Precision atN, MAP ?
Mean Average Precision, RP ?
R-Precision)1.
For both judgments, the combined1 Precision at N is the precision at N document cutoff point;Average Precision is the average of the precision value ob-tained after each relevant document is retrieved, and MeanAverage Precision is the average of AP over all topics; R-Precision is the precision after R documents have been re-trieved, where R is the number of relevant documents forthe topic.system S3 achieved higher precision and recallthan S1 and S2 by all metrics.
In the case of re-call, the absolute scores improve at least ninepercent.
Table 2 shows a pairwise comparison ofthe systems on three of the most meaningfulTREC metrics, using paired T-Test; statisticallysignificant results are highlighted.
The tableshows that the improvement of S3 over S1 andS2 is significant (or very nearly) by all metricsfor the relaxed judgment.
However, for the strictjudgment, none of the improvements are signifi-cant.
The reason might be that one third of thetopics have no relevant documents in our data set.This reduces the actual number of topics forevaluation.
In general, the performance of allthree systems for the strict judgment is worsethan that for the relaxed, likely due to the lowernumber of relevant documents for this category(averaged at 18 per topic), which makes it aharder IR task.00.10.20.30.40.50.60.7P10 MAP RP RecallS1_strict S2_strict S3_strict S1_relaxed S2_relaxed S3_relaxedFigure 2: System performance as measured byTREC metrics, averaged over all topics with non-zero relevant documentsPaired T-Test (p) P10 AP RP(S1,S2) .22 .37 .65(S2,S3) 1 .004 .10strict(S1,S3) .17 .48 .45(S1,S2) .62 .07 .56(S2,S3) .056 <.0001 .0007relaxed(S1,S3) .04 .02 .03Table 2: Paired T-Test (with two-tailed distribu-tion) between systems over all topicsThe constraint-based system S2 produceshigher initial precision than S1 as measured byP10.
However, semantic constraints contributeless and less as more documents are retrieved.The performance of S2 is slightly worse than S1as measured by AP and RP, which is likely dueto errors from AV extraction.
None of the met-rics is statistically significant.39Topic-by-topic analysis gives us a more de-tailed view of the behavior of the three systems.Figure 3 shows the performance of the systemsmeasured by P10, sorted by that of S3.
In gen-eral, the performance of S1 and S2 deviates sig-nificantly for individual topics.
However, thecombined system, S3, seems to be able to boostthe good results from both systems for most top-ics.
We are currently exploring the factors thatcontribute to the performance boost.00.10.20.30.40.50.60.70.80.9116 8 15 18 4 26 30 11 24 28 7 10 2 5 13 20 21 27 3 25 1 12 17 29 19 6 23 14 22Topic NumbersPrecision@10S1_relaxed S2_relaxed S3_relaxedFigure 3: Precision@10 for relaxed judgmentA closer look at topics where S3 improvessignificantly over S1 and S2 at P10 reveals thatthe combined lists are biased toward the docu-ments returned by S2, probably due to the higherscores assigned to documents by S2 than thoseby S1.
This suggests the need for better scorenormalization methods that take into account theadvantage of each system.In conclusion, our results show that using se-mantic information can improve IR results forspecial domains where the information need canbe specified as a set of semantic constraints.
Theconstraint-based system itself is not robustenough to be a standalone IR system, and has tobe combined with a term-based system toachieve satisfactory results.
The IR results fromthe combined system seem to be able to toleratesignificant errors in semantic annotation, consid-ering that the accuracy of AV-extraction is about50%.
It remains to be seen whether similar im-provement in retrieval can be achieved in generaldomains such as news articles.6 SummaryThis paper describes our exploratory study ofapplying semantic constraints derived from at-tribute-value pair annotations to traditional term-based document retrieval.
It shows promisingresults in our test domain where users have spe-cific information needs.
In our ongoing work, weare expanding the test topic set for the strictjudgment as well as the data set, improving AVextraction accuracy, analyzing how the combinedsystem improves upon individual systems, andexploring alternative ways of combining seman-tic constraints and terms for better retrieval.ReferencesHamish Cunningham, Diana Maynard, KalinaBontcheva and Valentin Tablan.
2002.
GATE: AFramework and Graphical Development Environ-ment for Robust NLP Tools and Applications.
Pro-ceedings of the 40th Anniversary Meeting of theAssociation for Computational Linguistics(ACL'02).
Philadelphia.Bruce Croft, James Callan and John Broglio.
1994.TREC-2 Routing and Ad-Hoc Retrieval EvaluationUsing the INQUERY System.
In Proceedings ofthe 2nd Text Retrieval Conference, NIST SpecialPutlication 500-215.David A. Evans and Robert Lefferts.
1995.
CLARIT-TREC experiments.
Information Processing andManagement, 31(3), 385-395.E.
Fix and J. Hodges.
1951.
Discriminatory Analysis,Nonparametric Discrimination: Consistency Prop-erties.
Technical Report, USAF School of AviationMedicine, Texas.Joon Ho Lee.
1997.
Analyses of Multiple EvidenceCombination.
Proceedings of the 20th Annual In-ternational ACM-SIGIR Conference on Researchand Development in Information Retrieval.
Phila-delphia, pp.
267-276.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
Workshop on the Evaluation of ParsingSystems, Spain.Yan Qu, David A.
Hull, Gregory Grefenstette, DavidA.
Evans, et al 2005.
Towards Effective Strategiesfor Monolingual and Bilingual Information Re-trieval: Lessons Learned from NTCIR-4.
ACMTransactions on Asian Language InformationProcessing, 4(2): 78-110.Robert Wagner and Michael Fischer.
1974.
TheString-to-string Correction Problem.
Journal of theAssociation for Computing Machinery, 21(1):168-173.Tomek Strzalkowski, Louise Guthrie, Jussi Karigren,Jim Leistensnider, et al 1996.
Natural language in-formation retrieval, TREC-5 report.
In Proceedingsof the 5th Text Retrieval Conference (TREC-5), pp.291-314, Gaithersburg, Maryland.Ellen Voorhees.
1998.
Using WordNet for text re-trieval.
In Wordnet, an Electronic Lexical Data-base, pp 285-303.
The MIT Press.40
