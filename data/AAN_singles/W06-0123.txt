Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 146?149,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese Word Segmentation and Named Entity Recognition byCharacter TaggingKun Yu1      Sadao Kurohashi2     Hao Liu1     Toshiaki Nakazawa1Graduate School of Information Science and Technology, The University ofTokyo, Tokyo, Japan, 113-86561Graduate School of Informatics, Kyoto University, Kyoto, Japan, 606-85012{kunyu, liuhao, nakazawa}@kc.t.u-tokyo.ac.jp1kuro@i.kyoto-u.ac.jp2AbstractThis paper describes our word segmenta-tion system and named entity recognition(NER) system for participating in thethird SIGHAN Bakeoff.
Both of them arebased on character tagging, but use dif-ferent tag sets and different features.Evaluation results show that our wordsegmentation system achieved 93.3% and94.7% F-score in UPUC and MSRA opentests, and our NER system got 70.84%and 81.32% F-score in LDC and MSRAopen tests.1 IntroductionDealing with word segmentation as charactertagging showed good results in last SIGHANBakeoff (J.K.Low et al,2005).
It is good at un-known word identification, but only using char-acter-level features sometimes makes mistakeswhen identifying known words (T.Nakagawa,2004).
Researchers use word-level features(J.K.Low et al,2005) to solve this problem.Based on this idea, we develop a word segmenta-tion system based on character-tagging, whichalso combine character-level and word-level fea-tures.
In addition, a character-based NER moduleand a rule-based factoid identification moduleare developed for post-processing.Named entity recognition based on character-tagging has shown better accuracy than word-based methods (H.Jing et al,2003).
But the smallwindow of text makes it difficult to recognize thenamed entities with many characters, such asorganization names (H.Jing et al,2003).
Consid-ering about this, we developed a NER systembased on character-tagging, which combinesword-level and character-level features together.In addition, in-NE probability is defined in thissystem to remove incorrect named entities andcreate new named entities as post-processing.2 Character Tagging for WordSegmentation and NER2.1 Basic ModelWe look both word segmentation and NER ascharacter tagging, which is to find the tag se-quence T* with the highest probability given asequence of characters S=c1c2?cn.
)|(maxarg* STPTT=  (1)Then we assume that the tagging of one char-acter is independent of each other, and modifyformula 1 as?=====niiitttTnntttTctPccctttPTnn1...2121...*)|(maxarg)...|...(maxarg2121(2)Beam search (n=3) (Ratnaparkhi,1996) is ap-plied for tag sequence searching, but we onlysearch the valid sequences to ensure the validityof searching result.
SVM is selected as the basicclassification model for tagging because of itsrobustness to over-fitting and high performance(Sebastiani, 2002).
To simplify the calculation,the output of SVM is regarded as P(ti|ci).2.2 Tag DefinitionFour tags ?B, I, E, S?
are defined for the wordsegmentation system, in which ?B?
means thecharacter is the beginning of one word, ?I?
meansthe character is inside one word, ?E?
means thecharacter is at the end of one word and ?S?
meansthe character is one word by itself.For the NER system, different tag sets are de-fined for different corpuses.
Table 1 shows the146tag set defined for MSRA corpus.
It is the prod-uct of Segment-Tag set and NE-Tag set, becausenot only named entities but also words are seg-mented in this corpus.
Here NE-Tag ?O?
meansthe character does not belong to any named enti-ties.
For LDC corpus, because there is no seg-mentation information, we delete NE-Tag ?O?but add tag ?NONE?
to indicate the characterdoes not belong to any named entities (Table 2).Table 1 Tags of NER for MSRA corpusSegment-Tag NE-TagB, I, E, S ?
PER, LOC, ORG, OTable 2 Tags of NER for LDC corpusSegment Tag NE TagB, I, E, S ?
PER, LOC, ORG, GPE + NONE2.3 Feature DefinitionFirst, some features based on characters aredefined for the two tasks, which are:(a) Cn (n=-2,-1,0,1,2)(b) Pu(C0)Feature Cn (n=-2,-1,0,1,2) mean the Chinesecharacters appearing in different positions (thecurrent character and two characters to its leftand right), and they are binary features.
A char-acter list, which contains all the characters in thelexicon introduced later, is used to identify them.Besides of that, feature Pu(C0) means whether C0is in a punctuation character list.
It is also binaryfeature and all the punctuations in the punctua-tion character list come from Penn Chinese Tree-bank 5.1 (N.Xue et al,2002).In addition, we define some word-level fea-tures based on a lexicon to enlarge the windowsize of text in the two tasks, which are:(c) Wn (n=-1,0,1)Feature Wn (n=-1,0,1) mean the lexicon wordsin different positions (the word containing C0and one word to its left and right) and they arealso binary features.
We select all the possiblewords in the lexicon that satisfy the requirements,not like only selecting the longest one in(J.K.Low et al,2005).
To create the lexicon, weuse following steps.
First, a lexicon from NICT(National Institute of Information and Communi-cations Technology, Japan) is used as the basiclexicon, which is extracted from Peking Univer-sity Corpus of the second SIGHAN Bakeoff(T.Emerson, 2005), Penn Chinese Treebank 4.0(N.Xue et al,2002), a Chinese-to-English Word-list1  and part of NICT corpus (K.Uchimoto etal.,2004; Y.J.Zhang et al,2005).
Then, all thewords containing digits and letters are removed1http://projects.ldc.upenn.edu/Chinese/from this lexicon.
At last, all the punctuations inPenn Chinese Treebank 5.1 (N.Xue et al,2002)and all the words in the training data of UPUCand MSRA corpuses are added into the lexicon.Besides of above features, some extra featuresare defined only for NER task.First, we add some character-based features toimprove the accuracy of person name recognition,which are CNn (n=-2,-1,0,1,2).
They meanwhether Cn (n=-2,-1,0,1,2) belong to a Chinesesurname list.
All of them are binary features.
TheChinese surname list contains the most famous100 Chinese surnames, such as ?, ?, ?, ?
(Zhao, Qian, Sun, Li).Then, we add some word-based features tohelp identify the organization name, which areWORGn (n=-1,0,1).
They mean whether W n (n=-1,0,1) belong to an organization suffix list.
Allof them are also binary features.
The organiza-tion suffix list is created by extracting the lastword from all the organization names in thetraining data of both MSRA and LDC corpuses.3 Post-processingBesides of the basic model, a NER moduleand a factoid identification module are developedin our word segmentation system for post-processing.
In addition, we define in-NE prob-ability to delete the incorrect named entities andidentify new named entities in the post-processing phrase of our NER system.3.1 Named Entity Recognition for WordSegmentationIn this module, if two or more segments in theoutputs of basic model are recognized as onenamed entity, we combine them as one segment.This module uses the same basic NER modelas what we introduced in the previous section.But it only identifies person and location names,because organization names often contain morethan one word.
In addition, to keep the high ac-curacy of person name recognition, the featuresabout organization suffixes are not used here.3.2 Factoid Identification for Word Seg-mentationRules are used to identify the following fac-toids among the segments from the basic wordsegmentation model:NUMBER: Integer, decimal, Chinese numberPERCENT: Percentage and fractionDATE: DateFOREIGN: English words147Table 3 shows some rules defined here.Table 3 Some Rules for Factoid IdentificationFactoid RuleNUMBER If previous segment ends with DIGIT and currentsegment starts with DIGIT, then combine them.PERCENT If previous segment is composed of DIGIT andcurrent segment equals ?%?, then combine them.DATEIf previous segment is composed of DIGIT andcurrent segment is in the list of ?
?, ?, ?, ?
(Year, Month, Day, Day)?, then combine them.FOREIGN Combine the consequent letters as one segment.
(DIGIT means both Arabic and Chinese numerals)3.3 NER Deletion and CreationIn-word probability has been used in unknownword identification successfully (H.Q.Li et al,2004).
Accordingly, we define in-NE probabilityto help delete and create named entities (NE).Formula 3 shows the definition of in-NE prob-ability for character sequence cici+1?ci+n.
Here ?#of cici+1?ci+n as NE?
is defined as TimeInNE andthe occurrence of cici+1?ci+n in different type ofNE is treated differently.data in testing ... of #NE as ... of #)...(111niiiniiiniiiInNEcccccccccP++++++ =(3)Then, we use some criteria to delete the incor-rect NE and create new possible NE, in whichdifferent thresholds are set for different tasks.Criterion 1: If PInNE(cici+1?ci+n) of one NEtype is lower than TDel, and TimeInNE(cici+1?ci+n)of the same NE type is also lower than TTime, thendelete this type of NE composed of cici+1?ci+n.Criterion 2: If PInNE(cici+1?ci+n) of one NEtype is higher than TCre, and in other places thecharacter sequence cici+1?ci+n does not belong toany NE, then create a new NE containingcici+1?ci+n with this NE type.4 Evaluation Results and Discussion4.1 Evaluation SettingSVMlight (T.Joachims, 1999) was used asSVM tool.
In addition, we used the MSRA train-ing corpus of NER task in this Bakeoff to trainour NER post-processing module.4.2 Results of Word SegmentationWe attended the open track of word segmenta-tion task for two corpuses: UPUC and MSRA.Table 4 shows the evaluation results.Table 4 Results of Word Segmentation Task (in percentage %)Corpus Pre.
Rec.
F-score Roov RivUPUC 94.4 92.2 93.3 68.0 97.0MSRA 94.0 95.3 94.7 50.3 96.9The F-score of our word segmentation systemin UPUC corpus ranked 4th (same as that of the3rd group) among all the 8 participants.
And itwas only 1.1% lower than the highest one and0.2% lower than the second one.
It showed thatour character-tagging approach was feasible.
Butthe F-score of MSRA corpus was only higherthan one participant in all the 10 groups (thehighest one was 97.9%).
Error analysis showsthat there are two main reasons.First, in MSRA corpus, they tend to segmentone organization name as one word, such as ??????
(China Chamber of Commerce inUSA).
But our basic segmentation model seg-mented such word into several words, e.g.
??/??/??
(USA/China/Chamber of Commerce),and our post-processing NER module does notconsider about organization names.Second, our factoid identification rule did notcombine the consequent DATE factoids into oneword, but they are combined in MSRA corpus.For example, our system segmented the word??
9??
(9 o?clock in the evening) into threeparts ?
?/9 ?/?
(Evening/9 o?clock/Exact).This error can be solved by revising the rules forfactoid identification.Besides of that, we also found although ourlarge lexicon helped identify the known wordsuccessfully, it also decreased the recall of OOVwords (our Riv of UPUC corpus ranked 2nd, withonly 0.6% decrease than the highest one, butRoov ranked 4th, with 8.8% decrease than thehighest one).
The large size of this lexicon islooked as the main reason.Our lexicon contains 221,407 words, in which6,400 words are single-character words.
It madeour system easy to segment one word into sev-eral words, for example word ???
(EconomyGroup) in UPUC corpus was segmented into??
(Economy) and ?
(Group).
Moreover, thelarge size of this lexicon also brought errors ofcombining two words into one word if the wordwas in the lexicon.
For example, words ?
(Only)and ?
(Have) in MSRA corpus were identifiedas one word because there existed the word??
(Only) in our lexicon.
We will reduce our lexi-con to a reasonable size to solve these problems.4.3 Results of NERWe also attended the open track of NER taskfor both LDC corpus and MSRA corpus.
Table 5and Table 6 give the evaluation results.There were only 3 participants in the opentrack of LDC corpus and our group got the bestF-score.
In addition, among all the 11 partici-pants for MSRA corpus, our system ranked 6th148by F-score.
It showed the validity of our charac-ter-tagging method for NER.
But for locationname (LOC) in LDC corpus, both the precisionand recall of our NER system were very low.
Itwas because there were too few location namesin the training data (there were only 476 LOC inthe training data, but 5648 PER, 5190 ORG and9545 GPE in the same data set).Table 5 Results of NER Task for LDC corpus (in percentage %)PER LOC ORG GPE OverallPre.
83.29 58.52 61.48 78.66 76.16Rec.
66.93 18.87 45.19 79.94 66.21F-score 74.22 28.57 52.09 79.30 70.84Table 6 Results of NER Task for MSRA corpus (in percentage %)PER LOC ORG OverallPre.
90.76 85.62 73.90 84.68Rec.
76.13 85.41 65.74 78.22F-score 82.80 85.52 69.58 81.32Besides of that, error analysis shows there arefour types of main errors in the NER results.First, some organization names were very longand can be divided into several words, in whichparts of them can also be looked as named enti-ties.
In such case, our system only recognized thesmall parts as named entities.
For example,  ?????????????
(Fei ZhengqingEastern Asia Research Center of Harvard Univ.
)was an organization name.
But our system rec-ognized it as????
(Harvard Univ.)/ORG+??
?
(Fei Zheng Qing)/PER+ ?
?
(EasternAsia)/LOC+ ????
(Research Center)/ORG.Adding more context features may be useful toresolve this issue.In addition, our system was not good at recog-nizing foreign person names, such as ???
(Riordan), and abbreviations, such as ??
(LosAngeles), if they seldom or never appeared intraining corpus.
It is because the use of the largelexicon decreased the unknown word identifica-tion ability of our NER system simultaneously.Third, the in-NE probability used in post-processing is helpful to identify named entitieswhich cannot be recognized by the basic model.But it also recognized some words which canonly be regarded as named entities in the localcontext incorrectly.
For example, our systemrecognized??
(Najing) as GPE in ??????
(Send to Najing for remedy) in LDC corpus.We will consider about adding the in-NE prob-ability as one feature into the basic model tosolve this problem.At last, in LDC corpus, they combine the at-tributive of one named entity (especially personand organization names) with the named entitytogether.
But our system only recognized thenamed entity by itself.
For example, our systemonly recognized ???
(Liu Gui Fang) as PERin the reference person name ????????
(Liu Gui Fang who does not know the inside).5 Conclusion and Future WorkThrough the participation of the thirdSIGHAN Bakeoff, we found that tagging charac-ters with both character-level and word-level fea-tures was effective for both word segmentationand NER.
While, this work is only ourpreliminary attempt and there are still manyworks needed to do in the future, such as thecontrol of lexicon size, the use of extraknowledge (e.g.
pos-tag), the feature definition,and so on.
In addition, our word segmentationsystem only combined the NER module as post-processing, which resulted in that lots of infor-mation from NER module cannot be used by thebasic model.
We will consider about combiningthe NER and factoid identification modules intothe basic word segmentation model by definingnew tag sets in our future work.AcknowledgementWe would like to thank Dr. Kiyotaka Uchi-moto for providing the NICT lexicon.ReferenceT.Emerson.
2005.
The Second International Chinese Word Seg-mentation Bakeoff.
In the 4th SIGHAN Workshop.
pp.
123-133.H.Jing et al 2003.
HowtogetaChineseName(Entity): Segmentationand Combination Issues.
In EMNLP 2003. pp.
200-207.T.Joachims.
1999.
Making large-scale SVM learning practical.Advances in Kernel Methods - Support Vector Learning.
MIT-Press.H.Q.Li et al 2004.
The Use of SVM for Chinese New Word Identi-fication.
In IJCNLP 2004. pp.
723-732.J.K.Low et al 2005.
A Maximum Entropy Approach to ChineseWord Segmentation.
In the 4th SIGHAN Workshop.
pp.
161-164.T.Nakagawa.
2004.
Chinese and Japanese Word SegmentationUsing Word-level and Character-level Information.
In COLING2004.
pp.
466-472.A.Ratnaparkhi.
1996.
A Maximum Entropy Model for Part-of-Speech Tagging.
In EMNLP 1996.F.Sebastiani.
2002.
Machine learning in automated text categoriza-tion.
ACM Computing Surveys.
34(1): 1-47.K.Uchimoto et al 2004.
Multilingual Aligned Parallel TreebankCorpus Reflecting Contextual Information and its Applications.In Proceedings of the MLR 2004. pp.
63-70.N.Xue et al 2002.
Building a Large-Scale Annotated Chinese Cor-pus.
In COLING 2002.Y.J.Zhang et al 2005.
Building an Annotated Japanese-ChineseParallel Corpus ?
A part of NICT Multilingual Corpora.
In Pro-ceedings of the MT SummitX.
pp.
71-78.149
