Proceedings of ACL-08: HLT, pages 1030?1038,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExtraction of Entailed Semantic Relations ThroughSyntax-based Comma ResolutionVivek Srikumar 1 Roi Reichart2 Mark Sammons1 Ari Rappoport2 Dan Roth11University of Illinois at Urbana-Champaign{vsrikum2|mssammon|danr}@uiuc.edu2Institute of Computer Science, Hebrew University of Jerusalem{roiri|arir}@cs.huji.ac.ilAbstractThis paper studies textual inference by inves-tigating comma structures, which are highlyfrequent elements whose major role in the ex-traction of semantic relations has not beenhitherto recognized.
We introduce the prob-lem of comma resolution, defined as under-standing the role of commas and extracting therelations they imply.
We show the importanceof the problem using examples from Tex-tual Entailment tasks, and present A SentenceTransformation Rule Learner (ASTRL), a ma-chine learning algorithm that uses a syntac-tic analysis of the sentence to learn sentencetransformation rules that can then be used toextract relations.
We have manually annotateda corpus identifying comma structures and re-lations they entail and experimented with bothgold standard parses and parses created by aleading statistical parser, obtaining F-scores of80.2% and 70.4% respectively.1 IntroductionRecognizing relations expressed in text sentences isa major topic in NLP, fundamental in applicationssuch as Textual Entailment (or Inference), QuestionAnswering and Text Mining.
In this paper we ad-dress this issue from a novel perspective, that of un-derstanding the role of the commas in a sentence,which we argue is a key component in sentencecomprehension.
Consider for example the followingthree sentences:1.
Authorities have arrested John Smith, a retiredpolice officer.2.
Authorities have arrested John Smith, his friendand his brother.3.
Authorities have arrested John Smith, a retiredpolice officer announced this morning.Sentence (1) states that John Smith is a retiredpolice officer.
The comma and surrounding sen-tence structure represent the relation ?IsA?.
In (2),the comma and surrounding structure signifies a list,so the sentence states that three people were ar-rested: (i) John Smith, (ii) his friend, and (iii) hisbrother.
In (3), a retired police officer announcedthat John Smith has been arrested.
Here, the commaand surrounding sentence structure indicate clauseboundaries.In all three sentences, the comma and the sur-rounding sentence structure signify relations essen-tial to comprehending the meaning of the sentence,in a way that is not easily captured using lexical-or even shallow parse-level information.
As a hu-man reader, we understand them easily, but auto-mated systems for Information Retrieval, QuestionAnswering, and Textual Entailment are likely to en-counter problems when comparing structures likethese, which are lexically similar, but whose mean-ings are so different.In this paper we present an algorithm for commaresolution, a task that we define to consist of (1) dis-ambiguating comma type and (2) determining therelations entailed from the sentence given the com-mas?
interpretation.
Specifically, in (1) we assigneach comma to one of five possible types, and in(2) we generate a set of natural language sentencesthat express the relations, if any, signified by eachcomma structure.
The algorithm uses informationextracted from parse trees.
This work, in addition tohaving immediate significance for natural languageprocessing systems that use semantic content, haspotential applications in improving a range of auto-1030mated analysis by decomposing complex sentencesinto a set of simpler sentences that capture the samemeaning.
Although there are many other widely-used structures that express relations in a similarway, commas are one of the most commonly usedsymbols1.
By addressing comma resolution, we of-fer a promising first step toward resolving relationsin sentences.To evaluate the algorithm, we have developed an-notation guidelines, and manually annotated sen-tences from the WSJ PennTreebank corpus.
Wepresent a range of experiments showing the goodperformance of the system, using gold-standard andparser-generated parse trees.In Section 2 we motivate comma resolutionthrough Textual Entailment examples.
Section 3 de-scribes related work.
Sections 4 and 5 present ourcorpus annotation and learning algorithm.
Resultsare given in Section 6.2 Motivating Comma Resolution ThroughTextual EntailmentComma resolution involves not only comma dis-ambiguation but also inference of the arguments(and argument boundaries) of the relationship repre-sented by the comma structure, and the relationshipsholding between these arguments and the sentenceas a whole.
To our knowledge, this is the first pa-per that deals with this problem, so in this sectionwe motivate it in depth by showing its importanceto the semantic inference task of Textual Entailment(TE) (Dagan et al, 2006), which is increasingly rec-ognized as a crucial direction for improving a rangeof NLP tasks such as information extraction, ques-tion answering and summarization.TE is the task of deciding whether the meaningof a text T (usually a short snippet) can be inferredfrom the meaning of another text S. If this is thecase, we say that S entails T .
For example2, we saythat sentence (1) entails sentence (2):1.
S: Parviz Davudi was representing Iran at ameeting of the Shanghai Co-operation Orga-nization (SCO), the fledgling association that1For example, the WSJ corpus has 49K sentences, amongwhich 32K with one comma or more, 17K with two or more,and 7K with three or more.2The examples of this section are variations of pairs takenfrom the Pascal RTE3 (Dagan et al, 2006) dataset.binds two former Soviet republics of centralAsia, Russia and China to fight terrorism.2.
T: SCO is the fledgling association that bindsseveral countries.To see that (1) entails (2), one must understandthat the first comma structure in sentence (1) is anapposition structure, and does not indicate the begin-ning of a list.
The second comma marks a boundarybetween entities in a list.
To make the correct infer-ence one must determine that the second comma is alist separator, not an apposition marker.
Misclassify-ing the second comma in (1) as an apposition leadsto the conclusion that (1) entails (3):3.
T: Russia and China are two former Soviet re-publics of central Asia .Note that even to an educated native speaker ofEnglish, sentence 1 may be initially confusing; dur-ing the first reading, one might interpret the firstcomma as indicating a list, and that ?the ShanghaiCo-operation Organization?
and ?the fledgling asso-ciation that binds...?
are two separate entities that aremeeting, rather than two representations of the sameentity.From these examples we draw the following con-clusions: 1.
Comma resolution is essential in com-prehending natural language text.
2.
Explicitly rep-resenting relations derived from comma structurescan assist a wide range of NLP tasks; this can bedone by directly augmenting the lexical-level rep-resentation, e.g., by bringing surface forms of twotext fragments with the same meaning closer to-gether.
3.
Comma structures might be highly am-biguous, nested and overlapping, and consequentlytheir interpretation is a difficult task.
The argumentboundaries of the corresponding extracted relationsare also not easy to detect.The output of our system could be used to aug-ment sentences with an explicit representation of en-tailed relations that hold in them.
In Textual Entail-ment systems this can increase the likelihood of cor-rect identification of entailed sentences, and in otherNLP systems it can help understanding the shallowlexical/syntactic content of a sentence.
A similar ap-proach has been taken in (Bar-Haim et al, 2007; deSalvo Braz et al, 2005), which augment the sourcesentence with entailed relations.10313 Related WorkSince we focus on extracting the relations repre-sented by commas, there are two main strands ofresearch with similar goals: 1) systems that directlyanalyze commas, whether labeling them with syn-tactic information or correcting inappropriate use intext; and 2) systems that extract relations from text,typically by trying to identify paraphrases.The significance of interpreting the role of com-mas in sentences has already been identified by (vanDelden and Gomez, 2002; Bayraktar et al, 1998)and others.
A review of the first line of research isgiven in (Say and Akman, 1997).In (Bayraktar et al, 1998) the WSJ PennTreebankcorpus (Marcus et al, 1993) is analyzed and a verydetailed list of syntactic patterns that correspond todifferent roles of commas is created.
However, theydo not study the extraction of entailed relations asa function of the comma?s interpretation.
Further-more, the syntactic patterns they identify are unlexi-calized and would not support the level of semanticrelations that we show in this paper.
Finally, theirsis a manual process completely dependent on syn-tactic patterns.
While our comma resolution systemuses syntactic parse information as its main sourceof features, the approach we have developed focuseson the entailed relations, and does not limit imple-mentations to using only syntactic information.The most directly comparable prior work is thatof (van Delden and Gomez, 2002), who use fi-nite state automata and a greedy algorithm to learncomma syntactic roles.
However, their approach dif-fers from ours in a number of critical ways.
First,their comma annotation scheme does not identifyarguments of predicates, and therefore cannot beused to extract complete relations.
Second, for eachcomma type they identify, a new Finite State Au-tomaton must be hand-encoded; the learning com-ponent of their work simply constrains which FSAsthat accept a given, comma containing, text spanmay co-occur.
Third, their corpus is preprocessed byhand to identify specialized phrase types needed bytheir FSAs; once our system has been trained, it canbe applied directly to raw text.
Fourth, they excludefrom their analysis and evaluation any comma theydeem to have been incorrectly used in the sourcetext.
We include all commas that are present in thetext in our annotation and evaluation.There is a large body of NLP literature on punctu-ation.
Most of it, however, is concerned with aidingsyntactic analysis of sentences and with developingcomma checkers, much based on (Nunberg, 1990).Pattern-based relation extraction methods (e.g.,(Davidov and Rappoport, 2008; Davidov et al,2007; Banko et al, 2007; Pasca et al, 2006; Sekine,2006)) could in theory be used to extract relationsrepresented by commas.
However, the types ofpatterns used in web-scale lexical approaches cur-rently constrain discovered patterns to relativelyshort spans of text, so will most likely fail onstructures whose arguments cover large spans (forexample, appositional clauses containing relativeclauses).
Relation extraction approaches such as(Roth and Yih, 2004; Roth and Yih, 2007; Hiranoet al, 2007; Culotta and Sorenson, 2004; Zelenko etal., 2003) focus on relations between Named Enti-ties; such approaches miss the more general apposi-tion and list relations we recognize in this work, asthe arguments in these relations are not confined toNamed Entities.Paraphrase Acquisition work such as that by (Linand Pantel, 2001; Pantel and Pennacchiotti, 2006;Szpektor et al, 2004) is not constrained to namedentities, and by using dependency trees, avoids thelocality problems of lexical methods.
However,these approaches have so far achieved limited accu-racy, and are therefore hard to use to augment exist-ing NLP systems.4 Corpus AnnotationFor our corpus, we selected 1,000 sentences con-taining at least one comma from the Penn Treebank(Marcus et al, 1993) WSJ section 00, and manu-ally annotated them with comma information3.
Thisannotated corpus served as both training and testdatasets (using cross-validation).By studying a number of sentences from WSJ (notamong the 1,000 selected), we identified four signif-icant types of relations expressed through commas:SUBSTITUTE, ATTRIBUTE, LOCATION, and LIST.Each of these types can in principle be expressed us-ing more than a single comma.
We define the notion3The guidelines and annotations are available at http://L2R.cs.uiuc.edu/?cogcomp/data.php.1032of a comma structure as a set of one or more commasthat all relate to the same relation in the sentence.SUBSTITUTE indicates an IS-A relation.
An ex-ample is ?John Smith, a Renaissance artist, was fa-mous?.
By removing the relation expressed by thecommas, we can derive three sentences: ?John Smithis a Renaissance artist?, ?John Smith was famous?,and ?a Renaissance artist was famous?.
Note that intheory, the third relation will not be valid: one exam-ple is ?The brothers, all honest men, testified at thetrial?, which does not entail ?all honest men testifiedat the trial?.
However, we encountered no examplesof this kind in the corpus, and leave this refinementto future work.ATTRIBUTE indicates a relation where one argu-ment describes an attribute of the other.
For ex-ample, from ?John, who loved chocolate, ate withgusto?, we can derive ?John loved chocolate?
and?John ate with gusto?.LOCATION indicates a LOCATED-IN relation.
Forexample, from ?Chicago, Illinois saw some heavysnow today?
we can derive ?Chicago is located inIllinois?
and ?Chicago saw some heavy snow today?.LIST indicates that some predicate or propertyis applied to multiple entities.
In our annotation,the list does not generate explicit relations; instead,the boundaries of the units comprising the list aremarked so that they can be treated as a single unit,and are considered to be related by the single rela-tion ?GROUP?.
For example, the derivation of ?John,James and Kelly all left last week?
is written as?
[John, James, and Kelly] [all left last week]?.Any commas not fitting one of the descriptionsabove are designated as OTHER.
This does not in-dicate that the comma signifies no relations, onlythat it does not signify a relation of interest in thiswork (future work will address relations currentlysubsumed by this category).
Analysis of 120 OTHERcommas show that approximately half signify clauseboundaries, which may occur when sentence con-stituents are reordered for emphasis, but may alsoencode implicit temporal, conditional, and other re-lation types (for example, ?Opening the drawer, hefound the gun.?).
The remainder comprises mainlycoordination structures (for example, ?Although hewon, he was sad?)
and discourse markers indicatinginter-sentence relations (such as ?However, he sooncheered up.?).
While we plan to develop an anno-Rel.
Type Avg.
Agreement # of Commas # of Rel.sSUBSTITUTE 0.808 243 729ATTRIBUTE 0.687 193 386LOCATION 0.929 71 140LIST 0.803 230 230OTHER 0.949 909 0Combined 0.869 1646 1485Table 1: Average inter-annotator agreement for identify-ing relations.tation scheme for such relations, this is beyond thescope of the present work.Four annotators annotated the same 10% of theWSJ sentences in order to evaluate inter-annotatoragreement.
The remaining sentences were dividedamong the four annotators.
The resulting corpus waschecked by two judges and the annotation correctedwhere appropriate; if the two judges disagreed, athird judge was consulted and consensus reached.Our annotators were asked to identify comma struc-tures, and for each structure to write its relation type,its arguments, and all possible simplified version(s)of the original sentence in which the relation impliedby the comma has been removed.
Arguments mustbe contiguous units of the sentence and will be re-ferred to as chunks hereafter.
Agreement statisticsand the number of commas and relations of eachtype are shown in Table 4.
The Accuracy closely ap-proximates Kappa score in this case, since the base-line probability of chance agreement is close to zero.5 A Sentence Tranformation Rule Learner(ASTRL)In this section, we describe a new machine learningsystem that learns Sentence Transformation Rules(STRs) for comma resolution.
We first define thehypothesis space (i.e., STRs) and two operations ?substitution and introduction.
We then define thefeature space, motivating the use of Syntactic Parseannotation to learn STRs.
Finally, we describe theASTRL algorithm.5.1 Sentence Transformation RulesA Sentence Transformation Rule (STR) takes aparse tree as input and generates new sentences.
Weformalize an STR as the pair l ?
r, where l is atree fragment that can consist of non-terminals, POStags and lexical items.
r is a set {ri}, each ele-ment of which is a template that consists of the non-1033terminals of l and, possibly, some new tokens.
Thistemplate is used to generate a new sentence, called arelation.The process of applying an STR l ?
r to a parsetree T of a sentence s begins with finding a match forl in T .
A match is said to be found if l is a subtreeof T .
If matched, the non-terminals of each ri areinstantiated with the terminals that they cover in T .Instantiation is followed by generation of the outputrelations in one of two ways: introduction or sub-stitution, which is specified by the corresponding ri.If an ri is marked as an introductory one, then therelation is the terminal sequence obtained by replac-ing the non-terminals in ri with their instantiations.For substitution, firstly, the non-terminals of the riare replaced by their instantiations.
The instantiatedri replaces all the terminals in s that are covered bythe l-match.
The notions of introduction and substi-tution were motivated by ideas introduced in (Bar-Haim et al, 2007).Figure 1 shows an example of an STR and Figure2 shows the application of this STR to a sentence.
Inthe first relation, NP1 and NP2 are instantiated withthe corresponding terminals in the parse tree.
In thesecond and third relations, the terminals of NP1 andNP2 replace the terminals covered by NPp.LHS: NPpNP1 , NP2 ,RHS:1.
NP1 be NP2 (introduction)2.
NP1 (substitution)3.
NP2 (substitution)Figure 1: Example of a Sentence Transformation Rule.
Ifthe LHS matches a part of a given parse tree, then theRHS will generate three relations.5.2 The Feature SpaceIn Section 2, we discussed the example where therecould be an ambiguity between a list and an apposi-tion structure in the fragment two former Soviet re-publics, Russia and China.
In addition, simple sur-face examination of the sentence could also identifythe noun phrases ?Shanghai Co-operation Organi-zation (SCO)?, ?the fledgling association that bindsSNPpNP1John Smith, NP2a renaissanceartist,V PwasfamousRELATIONS:1 [John Smith]/NP1 be [a renaissance artist]/NP22 [John Smith] /NP1 [was famous]3 [a renaissance artist]/NP2 [was famous]Figure 2: Example of application of the STR in Figure 1.In the first relation, an introduction, we use the verb ?be?,without dealing with its inflections.
NP1 and NP2 areboth substitutions, each replacing NPp to generate thelast two relations.two former Soviet Republics?, ?Russia?
and ?China?as the four members of a list.
To resolve such ambi-guities, we need a nested representation of the sen-tence.
This motivates the use of syntactic parse treesas a logical choice of feature space.
(Note, however,that semantic and pragmatic ambiguities might stillremain.
)5.3 Algorithm OverviewIn our corpus annotation, the relations and their ar-gument boundaries (chunks) are explicitly marked.For each training example, our learning algorithmfirst finds the smallest valid STR ?
the STR with thesmallest LHS in terms of depth.
Then it refines theLHS by specializing it using statistics taken fromthe entire data set.5.4 Generating the Smallest Valid STRTo transform an example into the smallest validSTR, we utilize the augmented parse tree of thesentence.
For each chunk in the sentence, we findthe lowest node in the parse tree that covers thechunk and does not cover other chunks (even par-tially).
It may, however, cover words that do notbelong to any chunk.
We refer to such a node asa chunk root.
We then find the lowest node that cov-ers all the chunk roots, referring to it as the pat-tern root.
The initial LHS consists of the sub-tree of the parse tree rooted at the pattern root andwhose leaf nodes are all either chunk roots or nodesthat do not belong to any chunk.
All the nodes arelabeled with the corresponding labels in the aug-1034mented parse tree.
For example, if we consider theparse tree and relations shown in Figure 2, then do-ing the above procedure gives us the initial LHSas S (NPp(NP1, NP2, ) V P ).
The three relationsgives us the RHS with three elements ?NP1 beNP2?, ?NP1 V P ?
and ?NP1 V P ?, all three beingintroduction.This initial LHS need not be the smallest one thatexplains the example.
So, we proceed by finding thelowest node in the initial LHS such that the sub-tree of the LHS at that node can form a new STRthat covers the example using both introduction andsubstitution.
In our example, the initial LHS has asubtree, NPp(NP1, NP2, ) that can cover all the re-lations with the RHS consisting of ?NP1 be NP2?,NP1 and NP2.
The first RHS is an introduction,while the second and the third are both substitutions.Since no subtree of this LHS can generate all threerelations even with substitution, this is the requiredSTR.
The final step ensures that we have the small-est valid STR at this stage.5.5 Statistical RefinementThe STR generated using the procedure outlinedabove explains the relations generated by a singleexample.
In addition to covering the relations gen-erated by the example, we wish to ensure that it doesnot cover erroneous relations by matching any of theother comma types in the annotated data.Algorithm 1 ASTRL: A Sentence TransformationRule Learning.1: for all t: Comma type do2: Initialize STRList[t] = ?3: p = Set of annotated examples of type t4: n = Annotated examples of all other types5: for all x ?
p do6: r = Smallest Valid STR that covers x7: Get fringe of r.LHS using the parse tree8: S = Score(r,p,n)9: Sprev = ?
?10: while S 6= Sprev do11: if adding some fringe node to r.LHS causes a signifi-cant change in score then12: Set r = New rule that includes that fringe node13: Sprev = S14: S = Score(r,p,n)15: Recompute new fringe nodes16: end if17: end while18: Add r to STRList[t]19: Remove all examples from p that are covered by r20: end for21: end forFor this purpose, we specialize the LHS so that itcovers as few examples from the other comma typesas possible, while covering as many examples fromthe current comma type as possible.
Given the mostgeneral STR, we generate a set of additional, moredetailed, candidate rules.
Each of these is obtainedfrom the original rule by adding a single node tothe tree pattern in the rule?s LHS, and updating therule?s RHS accordingly.
We then score each of thecandidates (including the original rule).
If there isa clear winner, we continue with it using the sameprocedure (i.e., specialize it).
If there isn?t a clearwinner, we stop and use the current winner.
Afterfinishing with a rule (line 18), we remove from theset of positive examples of its comma type all exam-ples that are covered by it (line 19).To generate the additional candidate rules that weadd, we define the fringe of a rule as the siblingsand children of the nodes in its LHS in the originalparse tree.
Each fringe node defines an additionalcandidate rule, whose LHS is obtained by addingthe fringe node to the rule?s LHS tree.
We refer tothe set of these candidate rules, plus the original one,as the rule?s fringe rules.
We define the score of anSTR asScore(Rule,p,n) = Rp|p| ?Rn|n|where p and n are the set of positive and negativeexamples for this comma type, and Rp and Rn arethe number of positive and negative examples thatare covered by the STR.
For each example, all exam-ples annotated with the same comma type are pos-itive while all examples of all other comma typesare negative.
The score is used to select the win-ner among the fringe rules.
The complete algorithmwe have used is listed in Algorithm 1.
For conve-nience, the algorithm?s main loop is given in termsof comma types, although this is not strictly nec-essary.
The stopping criterion in line 11 checkswhether any fringe rule has a significantly betterscore than the rule it was derived from, and exits thespecialization loop if there is none.Since we start with the smallest STR, we onlyneed to add nodes to it to refine it and never haveto delete any nodes from the tree.
Also note that thealgorithm is essentially a greedy algorithm that per-forms a single pass over the examples; other, more1035complex, search strategies could also be used.6 Evaluation6.1 Experimental SetupTo evaluate ASTRL, we used the WSJ derived cor-pus.
We experimented with three scenarios; in twoof them we trained using the gold standard treesand then tested on gold standard parse trees (Gold-Gold), and text annotated using a state-of-the-art sta-tistical parser (Charniak and Johnson, 2005) (Gold-Charniak), respectively.
In the third, we trained andtested on the Charniak Parser (Charniak-Charniak).In gold standard parse trees the syntactic cate-gories are annotated with functional tags.
Since cur-rent statistical parsers do not annotate sentences withsuch tags, we augment the syntactic trees with theoutput of a Named Entity tagger.
For the NamedEntity information, we used a publicly available NERecognizer capable of recognizing a range of cat-egories including Person, Location and Organiza-tion.
On the CoNLL-03 shared task, its f-score isabout 90%4.
We evaluate our system from differentpoints of view, as described below.
For all the eval-uation methods, we performed five-fold cross vali-dation and report the average precision, recall andf-scores.6.2 Relation Extraction PerformanceFirstly, we present the evaluation of the performanceof ASTRL from the point of view of relation ex-traction.
After learning the STRs for the differentcomma types using the gold standard parses, wegenerated relations by applying the STRs on the testset once.
Table 2 shows the precision, recall andf-score of the relations, without accounting for thecomma type of the STR that was used to generatethem.
This metric, called the Relation metric in fur-ther discussion, is the most relevant one from thepoint of view of the TE task.
Since a list does notgenerate any relations in our annotation scheme, weuse the commas to identify the list elements.
Treat-ing each list in a sentence as a single relation, wescore the list with the fraction of its correctly identi-fied elements.In addition to the Gold-Gold and Gold-Charniak4A web demo of the NER is at http://L2R.cs.uiuc.edu/?cogcomp/demos.php.settings described above, for this metric, we alsopresent the results of the Charniak-Charniak setting,where both the train and test sets were annotatedwith the output of the Charniak parser.
The improve-ment in recall in this setting over the Gold-Charniakcase indicates that the parser makes systematic er-rors with respect to the phenomena considered.Setting P R FGold-Gold 86.1 75.4 80.2Gold-Charniak 77.3 60.1 68.1Charniak-Charniak 77.2 64.8 70.4Table 2: ASTRL performance (precision, recall and f-score) for relation extraction.
The comma types wereused only to learn the rules.
During evaluation, only therelations were scored.6.3 Comma Resolution PerformanceWe present a detailed analysis of the performance ofthe algorithm for comma resolution.
Since this paperis the first one that deals with the task, we could notcompare our results to previous work.
Also, thereis no clear baseline to use.
We tried a variant ofthe most frequent baseline common in other disam-biguation tasks, in which we labeled all commas asOTHER (the most frequent type) except when thereare list indicators like and, or and but in adjacentchunks (which are obtained using a shallow parser),in which case the commas are labeled LIST.
Thisgives an average precision 0.85 and an average recallof 0.36 for identifying the comma type.
However,this baseline does not help in identifying relations.We use the following approach to evaluate thecomma type resolution and relation extraction per-formance ?
a relation extracted by the system is con-sidered correct only if both the relation and the typeof the comma structure that generated it are correctlyidentified.
We call this metric the Relation-Typemetric.
Another way of measuring the performanceof comma resolution is to measure the correctness ofthe relations per comma type.
In both cases, lists arescored as in the Relation metric.
The performance ofour system with respect to these two metrics are pre-sented in Table 3.
In this table, we also compare theperformance of the STRs learned by ASTRL withthe smallest valid STRs without further specializa-tion (i.e., using just the procedure outlined in Sec-tion 5.4).1036Type Gold-Gold Setting Gold-Charniak SettingRelation-Type metricSmallest Valid STRs ASTRL Smallest Valid STRs ASTRLP R F P R F P R F P R FTotal 66.2 76.1 70.7 81.8 73.9 77.6 61.0 58.4 59.5 72.2 59.5 65.1Relations Metric, Per Comma TypeATTRIBUTE 40.4 68.2 50.4 70.6 59.4 64.1 35.5 39.7 36.2 56.6 37.7 44.9SUBSTITUTE 80.0 84.3 81.9 87.9 84.8 86.1 75.8 72.9 74.3 78.0 76.1 76.9LIST 70.9 58.1 63.5 76.2 57.8 65.5 58.7 53.4 55.6 65.2 53.3 58.5LOCATION 93.8 86.4 89.1 93.8 86.4 89.1 70.3 37.2 47.2 70.3 37.2 47.2Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types andgenerating relations.There is an important difference between the Re-lation metric (Table 2) and the Relation-type met-ric (top part of Table 3) that depends on the seman-tic interpretation of the comma types.
For example,consider the sentence ?John Smith, 59, went home.
?If the system labels the commas in this as both AT-TRIBUTE and SUBSTITUTE, then, both will gener-ate the relation ?John Smith is 59.?
According tothe Relation metric, there is no difference betweenthem.
However, there is a semantic difference be-tween the two sentences ?
the ATTRIBUTE relationsays that being 59 is an attribute of John Smith whilethe SUBSTITUTE relation says that John Smith is thenumber 59.
This difference is accounted for by theRelation-Type metric.From this standpoint, we can see that the special-ization step performed in the full ASTRL algorithmgreatly helps in disambiguating between the AT-TRIBUTE and SUBSTITUTE types and consequently,the Relation-Type metric shows an error reductionof 23.5% and 13.8% in the Gold-Gold and Gold-Charniak settings respectively.
In the Gold-Goldscenario the performance of ASTRL is much betterthan in the Gold-Charniak scenario.
This reflects thenon-perfect performance of the parser in annotatingthese sentences (parser F-score of 90%).Another key evaluation question is the per-formance of the method in identification of theOTHER category.
A comma is judged to be asOTHER if no STR in the system applies to it.The performance of ASTRL in this aspect is pre-sented in Table 4.
The categorization of this cate-gory is important if we wish to further classify theOTHER commas into finer categories.Setting P R FGold-Gold 78.9 92.8 85.2Gold-Charniak 72.5 92.2 81.2Table 4: ASTRL performance (precision, recall and f-score) for OTHER identification.7 ConclusionsWe defined the task of comma resolution, and devel-oped a novel machine learning algorithm that learnsSentence Transformation Rules to perform this task.We experimented with both gold standard and parserannotated sentences, and established a performancelevel that seems good for a task of this complexity,and which will provide a useful measure of futuresystems developed for this task.
When given au-tomatically parsed sentences, performance degradesbut is still much higher than random, in both sce-narios.
We designed a comma annotation scheme,where each comma unit is assigned one of four typesand an inference rule mapping the patterns of theunit with the entailed relations.
We created anno-tated datasets which will be made available over theweb to facilitate further research.Future work will investigate four main directions:(i) studying the effects of inclusion of our approachon the performance of Textual Entailment systems;(ii) using features other than those derivable fromsyntactic parse and named entity annotation of theinput sentence; (iii) recognizing a wider range of im-plicit relations, represented by commas and in otherways; (iv) adaptation to other domains.AcknowledgementThe UIUC authors were supported by NSF grantITR IIS-0428472, DARPA funding under the Boot-strap Learning Program and a grant from Boeing.1037ReferencesM.
Banko, M. Cafarella, M. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the web.
In Proc.
of IJCAI, pages 2670?2676.R.
Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.2007.
Semantic inference at the lexical-syntactic level.In Proc.
of AAAI, pages 871?876.M.
Bayraktar, B.
Say, and V. Akman.
1998.
An analysisof english punctuation: The special case of comma.International Journal of Corpus Linguistics, 3(1):33?57.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and maxent discriminative reranking.
In Proc.of the Annual Meeting of the ACL, pages 173?180.A.
Culotta and J. Sorenson.
2004.
Dependency tree ker-nels for relation extraction.
In Proc.
of the AnnualMeeting of the ACL, pages 423?429.I.
Dagan, O. Glickman, and B. Magnini, editors.
2006.The PASCAL Recognising Textual Entailment Chal-lenge., volume 3944.
Springer-Verlag, Berlin.D.
Davidov and A. Rappoport.
2008.
Unsupervised dis-covery of generic relationships using pattern clustersand its evaluation by automatically generated sat anal-ogy questions.
In Proc.
of the Annual Meeting of theACL.D.
Davidov, A. Rappoport, and M. Koppel.
2007.
Fullyunsupervised discovery of concept-specific relation-ships by web mining.
In Proc.
of the Annual Meetingof the ACL, pages 232?239.R.
de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, andM.
Sammons.
2005.
An inference model for seman-tic entailment in natural language.
In Proc.
of AAAI,pages 1678?1679.T.
Hirano, Y. Matsuo, and G. Kikui.
2007.
Detectingsemantic relations between named entities in text usingcontextual features.
In Proc.
of the Annual Meeting ofthe ACL, pages 157?160.D.
Lin and P. Pantel.
2001.
DIRT: discovery of inferencerules from text.
In Proc.
of ACM SIGKDD Confer-ence on Knowledge Discovery and Data Mining 2001,pages 323?328.M.
P. Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.G.
Nunberg.
1990.
CSLI Lecture Notes 18: The Lin-guistics of Punctuation.
CSLI Publications, Stanford,CA.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging generic patterns for automatically harvesting se-mantic relations.
In Proc.
of the Annual Meeting of theACL, pages 113?120.M.
Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Names and similarities on the web: Fact extrac-tion in the fast lane.
In Proc.
of the Annual Meeting ofthe ACL, pages 809?816.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InHwee Tou Ng and Ellen Riloff, editors, Proc.
of theAnnual Conference on Computational Natural Lan-guage Learning (CoNLL), pages 1?8.
Association forComputational Linguistics.D.
Roth and W. Yih.
2007.
Global inference for en-tity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.B.
Say and V. Akman.
1997.
Current approaches topunctuation in computational linguistics.
Computersand the Humanities, 30(6):457?469.S.
Sekine.
2006.
On-demand information extraction.
InProc.
of the Annual Meeting of the ACL, pages 731?738.I.
Szpektor, H. Tanev, I. Dagan, and B. Coppola.
2004.Scaling web-based of entailment relations.
In Proc.
ofEMNLP, pages 49?56.S.
van Delden and F. Gomez.
2002.
Combining finitestate automata and a greedy learning algorithm to de-termine the syntactic roles of commas.
In Proc.
of IC-TAI, pages 293?300.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
Journal of MachineLearning Research, 3:1083?1106.1038
