Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 2?12,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLearning from a Neighbor: Adapting a Japanese Parser for Koreanthrough Feature Transfer LearningHiroshi KanayamaIBM Research - TokyoKoto-ku, Tokyo, Japanhkana@jp.ibm.comYoungja ParkIBM Research - T.J.Watson Research CenterYorktown Heights, NY, USAyoung park@us.ibm.comYuta TsuboiIBM Research - TokyoKoto-ku, Tokyo, Japanyutat@jp.ibm.comDongmook YiKorea Software Solutions Laboratory, IBM KoreaGangnam-gu, Seoul, Koreadmyi@kr.ibm.comAbstractWe present a new dependency parsingmethod for Korean applying cross-lingualtransfer learning and domain adaptationtechniques.
Unlike existing transfer learn-ing methods relying on aligned corpora orbilingual lexicons, we propose a featuretransfer learning method with minimal su-pervision, which adapts an existing parserto the target language by transferring thefeatures for the source language to the tar-get language.
Specifically, we utilize theTriplet/Quadruplet Model, a hybrid pars-ing algorithm for Japanese, and apply adelexicalized feature transfer for Korean.Experiments with Penn Korean Treebankshow that even using only the transferredfeatures from Japanese achieves a highaccuracy (81.6%) for Korean dependencyparsing.
Further improvements were ob-tained when a small annotated Korean cor-pus was combined with the Japanese train-ing corpus, confirming that efficient cross-lingual transfer learning can be achievedwithout expensive linguistic resources.1 IntroductionMotivated by increasing demands for advancednatural language processing (NLP) applicationssuch as sentiment analysis (Pang et al., 2002;Nasukawa and Yi, 2003) and question answer-ing (Kwok et al., 2001; Ferrucci et al., 2010), thereis a growing need for accurate syntactic parsingand semantic analysis of languages, especially fornon-English languages with limited linguistic re-sources.
In this paper, we propose a new depen-dency parsing method for Korean which requiresminimal human supervision.
Dependency parsingcan handle long-distance relationships and coor-dination phenomena very well, and has proven tobe very effective for parsing free-order languagessuch as Korean and Japanese (K?ubler et al., 2009).Most statistical parsing methods rely on anno-tated corpora labeled with phrase structures ordependency relationships, but it is very expen-sive to create a large number of consistent anno-tations.
Recently, treebanks have become avail-able for many languages such as English, Ger-man, Arabic, and Chinese.
However, the pars-ing results on these treebanks vary a lot depend-ing on the size of annotated sentences and the typeof annotations (Levy and Manning, 2003; Mc-Donald et al., 2013).
Further, many languageslack annotated corpus, or the size of the anno-tated corpus is too small to develop a reliable sta-tistical method.
To address these problems, therehave been several attempts at unsupervised pars-ing (Seginer, 2007; Spitkovsky et al., 2011), gram-mar induction (Klein and Manning, 2004; Naseemet al., 2010), and cross-lingual transfer learningusing annotated corpora of other languages (Mc-Donald et al., 2011).
However, the accuracies ofunsupervised methods are unacceptably low, andresults from cross-lingual transfer learning showdifferent outcomes for different pairs of languages,but, in most cases, the parsing accuracy is still lowfor practical purposes.
A recent study by McDon-ald et al.
(2013) concludes that cross-lingual trans-fer learning is beneficial when the source and tar-get languages were similar.
In particular, it reportsthat Korean is an outlier with the lowest scores(42% or less in UAS) when a model was trainedfrom European languages.In this paper, we present a new cross-lingual2transfer learning method that learns a new modelfor the target language by transferring the fea-tures for the source language.
Unlike other ap-proaches which rely on aligned corpora or abilingual lexicon, we learn a parsing model forKorean by reusing the features and annotateddata used in the Japanese dependency parsing,the Triplet/Quadruplet Model (Kanayama et al.,2000), which is a hybrid approach utilizing bothgrammatical knowledge and statistics.We exploit many similarities between the twolanguages, such as the head-final structure, thenoun to verb modification via case and topic mark-ers, and the similar word-order constraints.
It wasreported that the mapping of the grammar formal-ism in the language pair was relatively easy (Kimet al., 2003b; Kim et al., 2003a).
However, as thetwo languages are classified into independent lan-guage families (Gordon and Grimes, 2005), thereare many significant differences in their morphol-ogy and grammar (especially in the writing sys-tems), so it is not trivial to handle the two lan-guages in a uniform way.We show the Triplet/Quadruplet Model is suit-able for bilingual transfer learning, because thegrammar rules and heuristics reduce the numberof modification candidates and can mitigate thedifferences between two languages efficiently.
Inaddition, this model can handle the relationshipsamong the candidates as a richer feature space,making the model less dependent upon the lexi-cal features of the content words that are difficultto align between the two languages.
Similarly tothe delexicalized parsing model in (McDonald etal., 2011), we transfer only part-of-speech infor-mation of the features for the content words.
Wecreate new mapping rules to extract syntactic fea-tures for Korean parsing from the Japanese anno-tated corpus and refine the grammar rules to getcloser modification distributions in two languages.Our experiments with Penn Korean Tree-bank (Han et al., 2002) confirm that theTriplet/Quadruplet Model adapted for Korean out-performs a distance-based dependency parsingmethod, achieving 81.6% accuracy when no an-notated Korean corpus was used.
Further perfor-mance improvements were obtained when a smallsize of annotated Korean corpus was added, con-firming that our algorithm can be applied with-out more expensive linguistic resources such as analigned corpora or bilingual lexicons.
Moreover,the delexicalized feature transfer method enablesthe algorithm applicable to any two languages thathave similar syntactic structures.2 Related Work2.1 Parsing for KoreanSince Korean is a morphologically-rich language,many efforts for Korean parsing have focusedon automatically extracting rich lexical informa-tion such as the use of case frame patterns forthe verbs (Lee et al., 2007), the acquisition ofcase frames and nominal phrases from raw cor-pora (Park et al., 2013), and effective featuresfrom phrases and their neighboring contexts (Choiand Palmer, 2011).
Recently, Choi et al.
(2012)discussed the transformation of eojeol-based Ko-rean treebank to entity-based treebank to effec-tively train probabilistic CFG parsers.
We applysimilar techniques as in (Choi et al., 2012) to miti-gate the differences between Korean and Japanesesyntactic structures.Chung and Rim (2003) applied theTriplet/Quadruplet Model for Korean parsingas done in our work.
They reported that the modelperformed well for long-distance dependencies,but, in their experiments, the number of modi-fication candidates was not effectively reduced(only 91.5% of phrases were in one of the threepositions, while it was 98.6% in Kanayama?swork for Japanese).
In this paper, we introducemore sophisticated grammatical knowledge andheuristics to have similar dependency distribu-tions in the two languages.
Smith and Smith(2004) attempted a bilingual parsing for Englishand Korean by combining statistical dependencyparsers, probabilistic context-free grammars, andword translation models into a unified frameworkthat jointly searches for the best English parse,Korean parse and word alignment.
However, weutilize an existing parser and align the featuresfrom the source language to the features forthe target language, and, thus, our method isapplicable to situations where there is no alignedcorpora or word translation models.2.2 Transfer learning and domain adaptationRecently, transfer learning has attracted much at-tention, as it can overcome the lack of trainingdata for new languages or new domains for bothclassification and regression tasks (Pan and Yang,2010).
Transfer learning has also been applied to3?????/NNC?/PCA??/VV?/EAN???????/NPR?/PAN????/NNC?????/NNC?/PCA??????/NNC??/PAD?????/VV?/ECS???/VX?/EFN?./SFN?wife-NOM??buy-PAST??France-GEN?
?travel??bag-ACC??friend-DAT?
?show?
?want?
?.?e1e2e3e4e5e6e7e8e9?
???
??
?
?Figure 1: An example of dependency structures of a Korean sentence ??????????????
????
???
??.?
(?
(I) want to show the French travel bag which (my) wife bought to (my)friend?).
Each box corresponds to a Korean phrasal unit eojeol.???/n?/pc????/v?/aux?????????/np?/pn????????/n???/n?/pc?????/n?/pc???????/v??/aux?/?wife-NOM??buy-PAST??France-GEN?
?travel bag-ACC??friend-DAT?
?want to show?b1b2b3b4b5b6?
??
?
?Figure 2: A dependency structure of a Japanese sentence which corresponds to the Korean sentence inFigure 1, ??????????????????????????.
Each box corresponds to a Japanesephrasal unit bunsetsu.syntactic parsing, where a parsing model for a tar-get language is learned from linguistic resourcesin one or more different languages (Hwa et al.,2005; Zeman and Resnik, 2008; McDonald et al.,2011; Durrett et al., 2012; Georgi et al., 2012;Naseem et al., 2012).
McDonald et al.
(2011)proposed a delexicalized parsing model for cross-lingual dependency parsing and demonstrated thata high accuracy parsing was achieved for Indo-European languages where significant amount ofparallel texts exist.
However, in more recent work,McDonald et al.
(2013) showed that, unlike trans-fer learning within close language families, build-ing a Korean parser from European languages wasnot successful with a very low accuracy.
Durrettet al.
(2012) and Georgi et al.
(2012) show thattransfer parsing can be improved when additionalbilingual resources are available, such as bilingualdictionaries and parallel corpora of glossed textsrespectively.Our method does not require such resources anddoes not have restrictions on the sentence type thatcan be parsed.
Instead, we use a mixture of asmall corpus in a target language (i.e., Korean) anda larger corpus of a source language (Japanese).This task is similar to domain adaptation, and ourobjective is to outperform the training model builton each language separately.
To avoid the loss ofaccuracy due to the differences between two do-mains, we apply the domain adaptation techniqueproposed by Daum?e III (2007) which duplicatesthe feature space into three categories with eachof the features trained by source, by target, and bycombined domains.3 Dependency Structures of Korean andJapaneseA dependency structure in Korean is typically an-alyzed in terms of eojeol units, a basic phrasethat consists of a content word agglutinated withoptional functional morphemes such as postposi-tional particles or endings for verbs.
Figure 1shows an example Korean sentence with the de-pendencies between the eojeols indicated by ar-rows.
Figure 2 illustrates the dependency struc-ture between the bunsetsus in Japanese that cor-responds to the Korean structure in Figure 1.
Asthese figures show, the syntactic structures arequite similar in these languages: All of the de-pendencies are directed from left to right, and thepostpositional particles determine if the contentword modifies a verb (???
in e1and ???
in b1)or a noun (???
in e3and ???
in b3).
The eojeolsin Korean roughly correspond to the bunsetsus inJapanese.
In the remainder of this paper, we de-note both an eojeol or a bunsetsu as a ?PU?
(phrasalunit) when distinction is not needed.While Korean and Japanese have similar syn-tactic structures, the two languages have many dif-ferences.
The eojeols in Korean are separated bywhite space, while the bunsetsus in Japanese arenot.
Further, the statistics show several differencesin the two languages.
Table 1 compares a Koreancorpus, Penn Korean Treebank (henceforth KTB)4Table 1: Statistics of Korean and Japanese corpora.KTB (Korean) EDR (Japanese)Average number of characters (except for whitespace) per sentence 73.7 28.0Average number of PUs per sentence 25.5 8.53Average number of morphemes per PU 1.83 2.86Ratio of modification to the next PU 70.0% 61.8%Table 2: Simplified examples of Japanese grammar rules.Rightmost morpheme of the modifier PU Conditions for the modified PUspostpositional ???
wo (accusative) verb, adjectivepostpositional ???
no (genitive, nominative) noun, verb, adjectivepostpositional ???
to (conjunctive) noun, verb, adjective, adverb ?????
isshoni (?together?
)adverb verb, adjective, adverb, copula(Han et al., 2002), and a Japanese corpus, EDRCorpus (EDR, 1996).
Both corpora consist ofword-level bracketed constituents, so they are con-verted into PU-level dependency structures usingthe method described in Choi and Palmer (2011).Though both corpora consist mainly of newspaperor magazine articles, the sentences are not alignedwith each other, so the statistics show the compar-isons of the two corpora, rather than the theoret-ical comparisons of the two languages.
However,we can see that Korean sentences tend to be longerthan Japanese sentences both in terms of the num-ber of characters and PUs.
More eojeols modify anadjacent eojeol in Korean than in Japanese.
For in-stance, e1, e4, e6, e7, and e8modify the next eojeolin Figure 1, but only b1, b3, and b5modify the nextbunsetsu in Figure 2.
Those differences suggestsome of the difficulties in applying the Japanesedependency model to Korean.
The Japanese pars-ing method that will be described in the next sec-tion exploits these characteristics, which we applyto Korean parsing.4 Triplet/Quadruplet ModelThis section describes the Triplet/QuadrupletModel (Kanayama et al., 2000) which was origi-nally designed for Japanese parsing.
First, we re-view the two main ideas of the model ?
restrictionof modification candidates and feature selectionfor probability calculation.
Then, we describe howwe apply the Triplet/Quadruplet Model to Koreanparsing in Section 4.3.4.1 Restriction of modification candidatesThe Triplet/Quadruplet Model utilizes a smallnumber (about 50) of hand-crafted grammar rulesthat determine whether a PU can modify each PUto its right in a sentence.
The main goal of thegrammar rules is to maximize the coverage, andthe rules are simple describing high-level syntac-tic dependencies, and, thus, the rules can be eas-ily created without worrying about the precisionor contradictory rules.
The statistical informationis later used to select the right rules for a givensentence to produce an accurate parsing result.
Ta-ble 2 shows several grammar rules for Japanese, inwhich the modified PUs are determined depend-ing on the conditions of the rightmost morphemein the modifier PU.An analysis of the EDR corpus shows that98.6% of the correct dependencies are either thenearest PU, the second nearest PU, or the farthestPU from the modifier (more details in Table 4(a)).Therefore, the model can be simplified by restrict-ing the candidates to these three candidates andby ignoring the other PUs with a small sacrifice(1.4%) of parsing accuracy.4.2 Calculation of modification probabilitiesLet u be a modifier PU in question, cunthe u?s n-th modification candidate PU, ?uand ?cunthe at-tributes of u and cun, respectively.
Then the prob-ability that u modifies its n-th candidate is calcu-lated by the triplet equation (1) or the quadrupletequation (2) when u has two or three candidates,respectively1.P (u ?
cun) = P (n | ?u,?cu1,?cu2) (1)P (u ?
cun) = P (n | ?u,?cu1,?cu2,?cu3) (2)1It is trivial to show that P (u ?
cu1) = 1, when u hasonly one candidate.5Table 3: Simplified examples of Korean grammar rules.Rightmost morpheme of the modifier PU Conditions for the modified PUsPCA,PCJ,PAD,PAU (postpositional particles) V* (verb, adjective or auxiliary), CO (copula)EAN (nominal verb ending e.g.
???
eun) N* (noun)ADV (adverb), ADC (conjunction) N* (noun), V* (verb, adjective or auxiliary), ADV (adverb), ADC (conjunction)postpositional ???
gwa (conjunctive) N* (noun), V* (verb, adjective or aux), adverb ????
hamkke (?together?
)N* (noun) N* (noun), V* (verb, adjective or auxiliary)Table 4: Distribution (percentage) of the position of the correct modified PU among the candidate PUsselected by the initial grammar rules.
The column ?Sum?
shows the coverage of the 1st, 2nd and lastPUs.
The EDR corpus was used for Japanese, and the KTB was used for Korean in this analysis.
(a) Japanese# of Candidates Ratio 1st 2nd Last Sum1 32.7 100.0 ?
?
100.02 28.1 74.3 26.7 ?
100.03 17.5 70.6 12.6 16.8 100.04 9.9 70.4 11.1 13.8 95.3?5 11.8 70.2 11.1 10.5 91.9Total 100 ?
?
?
98.6(b) Korean (with the initial grammar)# of Candidates Ratio 1st 2nd Last Sum1 10.5 100.0 ?
?
100.02 11.4 85.9 14.1 ?
100.03 10.4 76.2 13.4 10.4 100.04 9.3 74.7 11.3 8.0 93.9?5 58.4 75.5 10.0 4.9 90.5Total 100 ?
?
?
93.9These probabilities are estimated by the maxi-mum entropy method with a feature set to express?
and ?.
Assuming the independence of thosemodifications, the probability of the dependencytree for an entire sentence P (T ) is calculated asthe product of the probabilities of all of the depen-dencies in the sentence using beam search to max-imize P (T ) under the constraints of the projectedstructure.P (T ) ?
?uP (u ?
cun) (3)In comparison, a traditional statistical parser(Collins, 1997) uses Equation (4) to calculate theprobability of u modifying t.P (u ?
t) = P (True | ?u,?t,?u,t) (4)We call the model based on Equation (4) the Dis-tance Model, since ?u,t(the distance between uand t) is typically used as the key feature.
Thoughother contextual information, in addition to the at-tributes of u and t, can be added, the model calcu-lates the probabilities of the dependencies betweenu and t independently and thus often fails to incor-porate appropriate contextual information.Equations (1) and (2) have two major advan-tages over the Distance Model: First, all the at-tributes of the modifier and its candidates can behandled simultaneously.
The combination of thoseattributes helps the model to express the context ofthe modifications.
Second, the probability of eachmodification is calculated based on the relative po-sitions of the candidates, instead of the distancefrom the modifier PU in the surface sentence, and,thus, the model is more robust.4.3 Korean dependency parsing with theTriplet/Quadruplet ModelWe design the Korean parser by adapting theTriplet/Quadruplet Model based on the analogouscharacteristics of Japanese and Korean.
First, wecreated the Korean grammar rules for generatingcandidate modified PUs by modifying the rulesfor Japanese shown in Table 2 for Korean.
Therule set, containing fewer than 50 rules, is sim-ple enough to be created manually, because therules simply describe possible dependencies, andJapanese phenomena are good hints for Koreanphenomena.
Table 3 shows some examples of therules for Korean based on the POS schema used inthe KTB corpus.
We did not automatically extractthe rules from the annotated corpora so that therules are general and independent of the trainingcorpus.
Nonetheless, 96.6% of the dependenciesin KTB are covered by the grammar rules.
The re-maining dependencies (3.4%) not covered by therule set are mainly due to rare modifications andmay indicate inconsistencies in the annotations,so we do not seek any grammar rules to achievenearly 100%.6???
(?wife-NOM?)
?
(?buy?)
???
(?show?)
??
(?want?)e1e2e7e8?
?
??1?
NNC (common noun)?2?
PCA (postpositional)?3????
(?-NOM?)?5?
VV (verb)?7?
EAN (adnominal ending)?8????
(past adnominal)?5?
VV (verb)?7?
ECS (ending)?8????
(conjunctive)?5?
VX (auxiliary)?7?
EFN (final ending)?8????
(predicative)Figure 3: The features used to select the modified PU of e1among its three candidates.
The full sentenceof this example is shown in Figure 1.
The numbers in brackets correspond to the feature IDs in Table 5.Table 5: The features to express attributes of a modifier and modification candidates.Feature set ID Description?1?
PoS of the head morpheme of the modifier?2?
PoS of the last morpheme of the modifier?3?
Lex of the postpositional or endings of the modifier?4?
Lex of the adverb of the modifier?5?
PoS of the head morpheme of the modification candidate?6?
Lex of the head morpheme of the modification candidate?7?
PoS of the last morpheme of the modification candidate?8?
Lex of the postpositional or endings of the modification candidate?9?
Existence of a quotation expression ????
dago or ????
rago?10?
Number of ???
eun (TOPIC marker) between the modifier and modification candidate?11?
Number of commas between the modifier and modification candidatecombination ?1?
?
?5?
/ ?2?
?
?5?
/ ?2?
?
?7?
/ ?3?
?
?5?
/ ?3?
?
?8?Table 4(a) and (b) show the distribution of thenumbers of candidate PUs and the position of thecorrect modified PUs obtained from the analysisof the EDR corpus and the KTB corpus respec-tively.
As we can see, the first candidate is pre-ferred in both languages, but the preference of thenearer candidate is stronger in Korean.
For in-stance, when there are more than one candidates,the probability that the first candidate is the cor-rect one is 78% for Korean but 71% for Japanese.Further, when there are more than 2 candidates,Japanese prefers the last candidate, while Koreanprefers the second candidate.
Based on the analy-sis results, the number of modification candidatesis restricted to at most three (the first, second andlast candidates) for Korean as well.The next step is to design ?uand ?cun, whichare required in Equations (1) and (2) to choose thecorrect modified PU.
We converted the feature setfrom the Japanese study to get the Korean featuresas listed in Table 5.
For example, to find the mod-ified PU of e1?????
anae-ga (?wife-NOM?)
inthe sentence shown in Figure 1, the attributes ofe1and the attributes of the three candidates, e2,e7, and e8, are extracted as shown in Figure 3, andtheir attributes are used to estimate the probabilityof each candidate in Equation (2).5 Adaptation for Bilingual TransferLearningIn Section 4.3, we explained how theTriplet/Quadruplet Model can be used forKorean.
In this section, we describe the featureadaption techniques in more detail and investigateif the new model with transferred features workswell when a small amount of annotated corpus forthe target language is provided.
Further, we studyif we can leverage the annotated corpus for thesource language in addition to the parsing modeland train a model for the target language using thetraining data for the source language.5.1 Feature TransferWith the assumption that Korean and Japanesehave similar syntactic dependencies, we adoptthe delexicalized parsing model presented in Mc-Donald et al.
(2011).
We transfer the part-of-speech (POS) in the Japanese features to the POSscheme in the KTB corpus, and translate Japanesefunctional words to the corresponding functionalwords in Korean.
This transfer process is manda-tory because we use the language specific POSsystems to capture language-specific dependencyphenomena, unlike other works using languageuniversal but coarser POS systems.We do not transfer lexical knowledge on con-7Table 6: Example of mapping rules for parts-of-speech and functional words.Japanese PoS Korean PoScommon noun NNCverb VVadjective VJnominal suffix XSFcase particle???,???,???,????
PADothers PCAJapanese particle Korean particle???
wo (?-ACC?)
???
eul????
yori (?from?)
????
buteo???
ha (?-TOPIC?)
???
eun???
mo (?too?)
???
do???
ga case particle (?-NOM?)
???
iconjunctive particle (?but?)
????
jimantent words and exceptional cases, so feature sets?4?
and ?6?
are not transferred.
Table 6 showssome examples of the feature transfer which han-dle POS tags and functional words.
We note thatthe Korean features shown in Figure 3 are directlyextracted from Japanese corpus using those rules.5.2 Adaptation of parsing rulesWhile Japanese and Korean are similar in terms ofsyntactic dependencies, there are significant dif-ferences between the two languages in the distri-bution of modification as shown in the Table 4(a)and (b): In Korean, more than half of modifiershave 5 or more candidates, while only 12% ofJapanese modifiers do.
In Japanese, 98.6% of cor-rect modified PUs are located in one of the threepositions (1st, 2nd or last), but, in Korean, the ratiofalls to 93.9% as shown in Table 4.
Another ma-jor difference of the two languages is the differentaverage numbers of PUs per sentence as shown inTable 1.
Korean has 25.5 PUs per sentence, whilethe number is only 8.5 in Japanese.
This is mainlycaused by the difference between eojeol in Koreanand bunsetsu in Japanese.
In Japanese, compoundnouns and verb phrases with an auxiliary verb arelikely to form a single bunsetsu, while, in Korean,they are split into multiple eojeols with a whites-pace in-between.These differences significantly reduce the ef-fect of transfer learning.
To address these prob-lems, we further refine the grammar rules as inthe following.
We added heuristic rules for theKorean model to effectively reduce the number ofcandidates in compound nouns which consist of anoun sequence in multiple eojeols, and verbs oradjectives followed by auxiliary verbs.
Figure 4shows an algorithm to reduce the number of mod-ified PUs considering the structure of compoundnouns.
In this example, both PUs e4(?travel?)
ande5(?bag-ACC?)
can be candidate PUs for eojeole3.
However, based on the rule in Figure 4, e4ande5are considered as a compound noun (line 1),and e4is determined to modify e5(line 3).
Sub-sequently, e3?s modifiability to e4is rejected (line5), and, thus, the correct modified PU of e3is de-termined as e5.
After refining the rules for com-pound nouns and auxiliary verbs, the probabilityof the correct modified PU being the 1st, 2nd orlast candidate PU increases from 93.9% to 96.3%as shown in Table 7, and the distribution of thecandidate?s positions for Korean became closer tothe Japanese distribution shown in Table 4(a).5.3 Learning from heterogeneous bilingualcorporaThe feature transfer and rule adaptation methodsdescribed in previous sections generate a very ac-curate Korean parser using only a Japanese cor-pus as shown in the first row in Table 8.
The nextquestion is if we can leverage bilingual corpora tofurther improve the accuracy, when annotated cor-pus for the target language (Korean) is available.We note that the two corpora do not need to bealigned and can come from different domains.
Tomitigate the side effects of merging heterogeneoustraining data in different languages, we apply thedomain adaptation method proposed by Daum?e III(2007) and augment the feature set to a sourcelanguage-specific version, a target-specific versionand a general version.
Specifically, a feature set xin Table 5 is expanded as follows:xK=< x,0,x > (5)xJ=< 0,x,x > (6)where xKand xJdenote the feature sets extractedfrom the Korean corpus and the Japanese corpusrespectively.
Then, the features specific to Koreanand Japanese get higher weights for the first partor the second part respectively, and the character-istics existing in both languages influence the lastpart.8if PoS of ui?s last morpheme is N* and PoS of ui+1?s first morpheme is N*thenuimust modify ui+1if ui?1?s last morpheme is not ???
then ui?1cannot modify ui+1else ui?1cannot modify uiu1to ui?2cannot modify ui???????/NPR?/PAN????/NNC?????/NNC?/PCA?France-GEN?
?travel??bag-ACC?e3e4e5??
?Figure 4: Heuristic rules to reduce the number of modification candidates surrounding compound nounsin Korean.
The example in the right figure shows that candidates in the dotted lines are removed by theheuristics.Table 7: Distribution of the position of correct modified PU for Korean after the refinement of the Koreangrammar rules.# of candidates Ratio 1st 2nd Last Sum1 46.4% 100.0% ?
?
100.0%2 9.8% 79.0% 21.0% ?
100.0%3 9.2% 75.5% 12.7% 11.8% 100.0%4 8.0% 71.0% 11.8% 9.6% 92.4%?
5 26.6% 70.4% 10.1% 7.8% 88.3%Total 100% ?
?
?
96.3%6 ExperimentsIn this section, we validate the effectiveness oflearning a Korean parser using the feature transferlearning from the Japanese parser and compare theKorean model with other baseline cases.
We alsocompare the parsing results when various sizes ofbilingual corpora were used to train the Koreanmodel.6.1 Korean parsing using theTriplet/Quadruplet ModelFirst, to validate the effectiveness of theTriplet/Quadruplet Model for parsing Korean, webuilt eight Korean dependency parsing models us-ing different numbers of training sentences for Ko-rean.
The KTB corpus Version 2.0 (Han et al.,2002) containing 5,010 annotated sentences wasused in this study.
We first divide the corpus into 5subsets by putting each sentence into its (sentenceID mod 5)-th group.
We use sentences from thefirst subgroup for estimating the parameters, sen-tences from the second subgroup for testing, anduse the remaining three subgroups for training.
Webuilt 8 models in total, using from 0 sentence upto 3,006 sentences selected from the training set.The number of training sentences in each modelis shown in the first column in Table 8.
The pa-rameters were estimated by the maximum entropymethod, and the most preferable tree is selectedusing each dependency probability and the beamsearch.
The test data set contains 1,043 sentences.We compare the Triplet/Quadruplet Model-based models with the Distance Model.
For theDistance Model, we used the same feature set asin Table 5, and added the distance feature (?u,t)by grouping the distance between two PUs into 3categories (1, 2 to 5, and 6 or more).
The perfor-mances are measured by UAS (unlabeled attach-ment score), and the results of the two methodsare shown in the second column, where JapaneseCorpus Size=0, in Table 8 (a) and (b) respectively.The top leftmost cells (80.61% and 71.63%) showthe parsing accuracies without any training cor-pora.
In these cases the nearest candidate PU isselected as the modified PU.
The difference be-tween two models suggests the effect of restrictionof modification candidates by the grammar rules.We note that the Triplet/Quadruplet Model pro-duces more accurate results and outperforms theDistance Model by more than 2 percentage pointsin all cases.
The results confirm that the methodfor Japanese parsing is suitable for Korean pars-ing.6.2 Results of bilingual transfer learningNext, we evaluate the transfer learning when anno-tated sentences for Japanese were also added.
Ta-ble 8(a) shows the accuracies of our model whenvarious numbers of training sentences from Ko-rean and Japanese are used.
The first row showsthe accuracies of Korean parsing when the modelswere trained only with the Japanese corpus, and9Table 8: The accuracy of Korean dependency parsing with various numbers of annotated sentences in thetwo languages.
?denotes that the mixture of bilingual corpora significantly outperformed (p < .05)the parser trained with only the Korean corpus without Japanese corpus.
(a) Triplet/Quadruplet ModelJapanese Corpus Size0 2,500 5,000 10,000KoreanCorpusSize0 80.61% 80.78% 81.23%?81.58%?50 82.21% 82.32% 82.40%?82.43%?98 82.36% 82.66%?82.69%?82.70%?197 83.13% 83.18% 83.30%?83.28%383 83.62% 83.92%?83.94%?83.91%?750 84.03% 84.00% 84.06% 84.06%1,502 84.41% 84.34% 84.32% 84.28%3,006 84.77% 84.64% 84.64% 84.65%(b) Distance ModelJapanese Corpus Size0 2,500 5,000KoreanCorpusSize0 71.63% 62.42% 54.92%50 79.31% 79.55%?79.54%?98 80.53% 80.63%?80.72%?197 80.91% 80.84% 80.85%383 81.86% 81.75% 81.76%750 82.10% 81.92% 81.94%1,502 82.50% 82.48% 82.50%3,006 82.66% 82.57% 82.54%other rows show the results when the Korean andJapanese corpora were mixed using the methoddescribed in Section 5.3.As we can see from the results, the bene-fit of transfer learning is larger when the sizeof the annotated corpus for Korean (i.e., targetlanguage) is smaller.
In our experiments withTriplet/Quadruplet Model, positive results wereobtained by the mixture of the two languages whenthe Korean corpus is less than 500 sentences, thatis, the annotations in the source language success-fully compensated the small corpus of the targetlanguage.
When the size of the Korean corpus isrelatively large (?
1, 500 sentences), adding theJapanese corpus decreased the accuracy slightly,due to syntactic differences between the two lan-guages.
Also the effect of the corpus from thesource language tends to saturate as the size ofthe source corpus, when the target corpus is larger.This is mainly because our mapping rules ignorelexical features, so few new features found in thelarger corpus were incorrectly processed.When merging the corpus in two languages,if we simply concatenate the transferred featuresfrom the source language and the features fromthe target language (instead of using the dupli-cated features shown in Equations (5) and (6)), theaccuracy dropped from 82.70% to 82.26% whenthe Korean corpus size was 98 and Japanese cor-pus size was 10,000, and from 83.91% to 83.40%when Korean=383.
These results support thatthere are significant differences in the dependen-cies between two languages even if we have im-proved the feature mapping, and our approachwith the domain adaptation technique (Daum?e III,2007) successfully solved the difficulty.Table 8(b) shows the results of the DistanceModel.
As we can see from the first row, usingonly the Japanese corpus did not help the Dis-tance Model at all in this case.
The DistanceModel was not able to mitigate the differences be-tween the two languages, because it does not useany grammatical rules to control the modifiability.This demonstrates that the hybrid parsing methodwith the grammar rules makes the transfer learn-ing more effective.
On the other hand, the domainadaptation method described in (5) and (6) suc-cessfully counteracted the contradictory phenom-ena in the two languages and increased the accu-racy when the size of the Korean corpus was small(size=50 and 98).
This is because the interactionsamong multiple candidates which cannot be cap-tured from the small Korean corpus were providedby the Japanese corpus.Some of previous work reported the parsingaccuracy with the same KTB corpus; 81% withtrained grammar (Chung et al., 2010) and 83%with Stanford parser after corpus transformation(Choi et al., 2012), but as Choi et al.
(2012) notedit is difficult to directly compare the accuracies.6.3 DiscussionThe analysis of e2?s dependency in Figure 1is a good example to illustrate how theTriplet/Quadruplet Model and the Japanesecorpus help Korean parsing.
Eojeol e2has threemodification candidates, e3, e5, and e6.
In the10Distance Model, e3is chosen because the distancebetween the two eojeols (?e2,e3) was 1, whichis a very strong clue for dependency.
Also, inthe Triplet/Quadruplet Model trained only witha small Korean corpus, e3received a higherprobability than e5and e6.
However, when alarger Japanese corpus was combined with theKorean corpus, e5was correctly selected as theJapanese corpus provided more samples of thedependency relation of ?verb-PAST?
(e2) and?common noun-ACC (?)?
(e5) than that of?verb-PAST?
and ?proper noun-GEN (?)?
(e3).As we can notice, larger contextual informationis required to make the right decision for this case,which may not exist sufficiently in a small cor-pus due to data sparseness.
The grammar rules inthe Triplet/Quadruplet Model can effectively cap-ture such contextual knowledge even from a rel-atively small corpus.
Further, since the grammarrules are based only on part-of-speech tags and asmall number of functional words, they are sim-ilar to the delexicalized parser (McDonald et al.,2011).
These delexicalized rules are more robustto linguistic idiosyncrasies, and, thus, are more ef-fective for transfer learning.7 ConclusionWe presented a new dependency parsing algo-rithm for Korean by applying transfer learningfrom an existing parser for Japanese.
Unlike othertransfer learning methods relying on aligned cor-pora or bilingual lexical resources, we proposed afeature transfer method utilizing a small numberof hand-crafted grammar rules that exploit syn-tactic similarities of the source and target lan-guages.
Experimental results confirm that the fea-tures learned from the Japanese training corpuswere successfully applied for parsing Korean sen-tences and mitigated the data sparseness problem.The grammar rules are mostly delexicalized com-prising only POS tags and a few functional words(e.g., case markers), and some techniques to re-duce the syntactic difference between two lan-guages makes the transfer learning more effective.This methodology is expected to be applied to anytwo languages that have similar syntactic struc-tures, and it is especially useful when the targetlanguage is a low-resource language.ReferencesJinho D. Choi and Martha Palmer.
2011.
Statisticaldependency parsing in Korean: From corpus gener-ation to automatic parsing.
In Proceedings of theSecond Workshop on Statistical Parsing of Morpho-logically Rich Languages, pages 1?11.DongHyun Choi, Jungyeul Park, and Key-Sun Choi.2012.
Korean treebank transformation for parsertraining.
In Proceedings of the ACL 2012 JointWorkshop on Statistical Parsing and Semantic Pro-cessing of Morphologically Rich Languages, pages78?88.Hoojung Chung and Heechang Rim.
2003.
Anew probabilistic dependency parsing model forhead-final, free word order languages.
IEICETRANSACTIONS on Information and Systems, E86-1(11):2490?2493.Tagyoung Chung, Matt Post, and Daniel Gildea.
2010.Factors affecting the accuracy of korean parsing.
InProceedings of the NAACL HLT 2010 First Work-shop on Statistical Parsing of Morphologically-RichLanguages, pages 49?57.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics and Eighth Conference of theEuropean Chapter of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263.Greg Durrett, Adam Pauls, and Dan Klein.
2012.
Syn-tactic transfer using a bilingual lexicon.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1?11.EDR.
1996.
EDR (Japan Electronic Dictionary Re-search Institute, Ltd.) electronic dictionary version1.5 technical guide.David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyan-pur, Adam Lally, J. William Murdock, Eric Nyberg,John M. Prager, Nico Schlaefer, and Christopher A.Welty.
2010.
Building Watson: An overview of theDeepQA project.
AI Magazine, 31(3):59?79.Ryan Georgi, Fei Xia, and William D Lewis.
2012.Improving dependency parsing with interlinearglossed text and syntactic projection.
In Proceed-ings of COLING 2012, pages 371?380.Raymond GGordon and Barbara F Grimes.
2005.
Eth-nologue: Languages of the world, volume 15.
SILinternational Dallas, TX.11Chung-hye Han, Na-Rae Han, Eon-Suk Ko, HeejongYi, and Martha Palmer.
2002.
Penn Korean tree-bank: Development and evaluation.
In Proc.
PacificAsian Conf.
Language and Comp.Rebecca Hwa, Philip Resnik, and Amy Weinberg.2005.
Breaking the resource bottleneck for multi-lingual parsing.
Technical report, DTIC Document.Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsu-ishi, and Jun?ichi Tsujii.
2000.
A hybrid Japaneseparser with hand-crafted grammar and statistics.
InProceedings of the 18th International Conference onComputational Linguistics, pages 411?417.Roger Kim, Mary Dalrymple, Ronald M Kaplan, andTracy Holloway King.
2003a.
Porting grammarsbetween typologically similar languages: Japaneseto korean.
In Proceedings of the 17th Pacific AsiaConference on Language, Information.Roger Kim, Mary Dalrymple, Ronald M Kaplan,Tracy Holloway King, Hiroshi Masuichi, andTomoko Ohkuma.
2003b.
Multilingual grammardevelopment via grammar porting.
In ESSLLI 2003Workshop on Ideas and Strategies for MultilingualGrammar Development, pages 49?56.Dan Klein and Christopher D Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, pages 478?487.Sandra K?ubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
Synthesis Lectures onHuman Language Technologies, 1(1):1?127.Cody Kwok, Oren Etzioni, and Daniel S Weld.
2001.Scaling question answering to the web.
ACM Trans-actions on Information Systems (TOIS), 19(3):242?262.Hyeon-Yeong Lee, Yi-Gyu Hwang, and Yong-SeokLee.
2007.
Parsing of Korean based on CFG usingsentence pattern information.
International Journalof Computer Science and Network Security, 7(7).Roger Levy and Christopher Manning.
2003.
Is itharder to parse chinese, or the chinese treebank?
InProceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, ACL, pages439?446.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 62?72.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, KuzmanGanchev, Keith Hall, Slav Petrov, Hao Zhang, OscarT?ackstr?om, et al.
2013.
Universal dependency an-notation for multilingual parsing.
In Proceedings ofACL 2013.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1234?1244.Tahira Naseem, Regina Barzilay, and Amir Globerson.2012.
Selective sharing for multilingual dependencyparsing.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 629?637.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Senti-ment analysis: Capturing favorability using naturallanguage processing.
In Proceedings of the SecondInternational Conferences on Knowledge Capture,pages 70?77.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
Knowledge and Data Engineer-ing, IEEE Transactions on, 22(10):1345?1359.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in nat-ural language processing (EMNLP), pages 79?86,Philadelphia, Pennsylvania.Jungyeul Park, Daisuke Kawahara, Sadao Kurohashi,and Key-Sun Choi.
2013.
Towards fully lexicalizeddependency parsing for Korean.
In Proceedings ofThe 13th International Conference on Parsing Tech-nologies.Yoav Seginer.
2007.
Fast unsupervised incrementalparsing.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 384?391.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using English toparse Korean.
In Proceedings of the 2004 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 49?56.Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,and Daniel Jurafsky.
2011.
Unsupervised depen-dency parsing without gold part-of-speech tags.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 1281?1290.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In IJCNLP, pages 35?42.12
