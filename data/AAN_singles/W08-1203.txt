Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 8?16Manchester, August 2008Exploiting ?Subjective?
AnnotationsDennis ReidsmaHuman Media InteractionUniversity of Twente, PO Box 217NL-7500 AE, Enschede, The Netherlandsdennisr@ewi.utwente.nlRieks op den AkkerHuman Media InteractionUniversity of Twente, PO Box 217NL-7500 AE, Enschede, The Netherlandsinfrieks@ewi.utwente.nlAbstractMany interesting phenomena in conversa-tion can only be annotated as a subjec-tive task, requiring interpretative judge-ments from annotators.
This leads todata which is annotated with lower lev-els of agreement not only due to errors inthe annotation, but also due to the differ-ences in how annotators interpret conver-sations.
This paper constitutes an attemptto find out how subjective annotations witha low level of agreement can profitablybe used for machine learning purposes.We analyse the (dis)agreements betweenannotators for two different cases in amultimodal annotated corpus and explic-itly relate the results to the way machine-learning algorithms perform on the anno-tated data.
Finally we present two newconcepts, namely ?subjective entity?
clas-sifiers resp.
?consensus objective?
classi-fiers, and give recommendations for usingsubjective data in machine-learning appli-cations.1 IntroductionResearch that makes use of multimodal annotatedcorpora is always presented with something of adilemma.
One would prefer to have results whichare reproducible and independent of the particularannotators that produced the corpus.
One needsdata which is annotated with as few disagreementsbetween annotators as possible.
But labeling a cor-pus is a task which involves a judgement by the an-?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.notator and is therefore, in a sense, always a sub-jective task.
Of course, for some phenomena thosejudgements can be expected to come out mostlythe same for different annotators.
For other phe-nomena the judgements can be more dependent onthe annotator interpreting the behavior being anno-tated, leading to annotations which are more sub-jective in nature.
The amount of overlap or agree-ment between annotations is then also influencedby the amount of intersubjectivity in the judge-ments of annotators.This relates to the spectrum of content typesdiscussed extensively by Potter and Levine-Donnerstein (1999).
One of the major distinctionsthat they make is a distinction in annotation ofmanifest content (directly observable events), pat-tern latent content (events that need to be inferredindirectly from the observations), and projectivelatent content (loosely said, events that require asubjective interpretation from the annotator).Manifest content is what is directly observable.Some examples are annotation of instances wheresomebody raises his hand or raises an eyebrow,annotation of the words being said and indicatingwhether there is a person in view of the camera.Annotating manifest content can be a relativelyeasy task.
Although the annotation task involvesa judgement by the annotator, those judgementsshould not diverge a lot for different annotators.At the other end of the spectrum we find pro-jective latent content.
This is a type of contentfor which the annotation schema does not spec-ify in extreme detail the rules and surface formsthat determine the applicability of classes, but inwhich the coding relies on the annotators?
exist-ing mental conception1 of the classes.
Such an ap-1Potter and Levine-Donnerstein use the word ?mentalscheme?
for this.
We will use ?mental conceptions?
in this8proach is useful for everyday concepts that mostpeople understand and to a certain extent share acommon meaning for, but for which it is almostimpossible to provide adequately complete defini-tions.
Potter and Levine-Donnerstein use the ex-ample ?chair?
for everyday concepts that are dif-ficult to define exhaustively.
But this concept isalso especially relevant in an application contextthat requires the end user of the data to agree withthe distinctions being made.
This is very importantwhen machine learning classifiers are developedto be used in everyday applications.
For exam-ple, one can make a highly circumscribed, etholog-ically founded definition of the class ?dominant?
toguide annotation.
This is good for, e.g., researchinto social processes in multiparty conversations.However, in a scenario where an automatic classi-fier, trained to recognize this class, is to be usedin an application that gives a participant in a meet-ing a quiet warning when he is being too dominant(Rienks, 2007) one would instead prefer the classrather to fit the mental conceptions of dominancethat a ?naive?
user may have.
When one designsan annotation scheme for projective latent content,the focus of the annotation guidelines is on instruc-tions that trigger the appropriate existing mentalconceptions of the annotators rather than on writ-ing exhaustive descriptions of how classes can bedistinguished from each other (Potter and Levine-Donnerstein, 1999).Interannotator agreement takes on differentroles for the two ends of the spectrum.
For mani-fest content the level of agreement tells you some-thing about how accurate the measurement in-strument (schema plus coders) is.
Bakeman andGottman, in their text book observing interaction:introduction to sequential analysis (1986, p 57),say about this type of reliability measurement thatit is a matter of ?calibrating your observers?.
Forprojective content, we have additional problems;the level of agreement may be influenced by thelevel of intersubjectivity, too.
Where Krippen-dorff (1980) describes that annotators should beinterchangeable, annotations of projective latentcontent can sometimes say as much about the men-tal conceptions of the particular annotator as aboutthe person whose interactions are being annotated.The personal interpretations of the data by the an-notator should not necessarily be seen as ?errors?,though, even if those interpretations lead to low in-paper to avoid confusion with the term ?annotation scheme?.terannotator agreement: they may simply be an un-avoidable aspect of the interesting type of data oneworks with.Many different sources of low agreement levels,and many different solutions, are discussed in theliterature.
It is important to note that some types ofdisagreement are more systematic and other typesare more noise like.
For projective latent con-tent one would expect more consistent structure inthe disagreements between annotators as they arecaused by the differences in the personal ways ofinterpreting multimodal interaction.
Such system-atic disagreements are particularly problematic forsubsequent use of the data, more so than noise-like disagreements.
Therefore, an analysis of thequality of an annotated corpus should not stop atpresenting the value of a reliability metric; insteadone should investigate the patterns in the disagree-ments and discuss the possible impact they have onthe envisioned uses of the data (Reidsma and Car-letta, 2008).
Some sources of disagreements arethe following.
(1) ?Clerical errors?
caused by a limited viewof the interactions being annotated (low qualityvideo, no audio, occlusions, etc) or by slipshodwork of the annotator or the annotator misunder-standing the instructions.
Some solutions are toprovide better instructions and training, using onlygood annotators, and using high quality recordingsof the interaction being annotated.
(2) ?Invalid or imprecise annotation schemas?that contain classes that are not relevant or do notcontain classes that are relevant, or force the anno-tator to make choices that are not appropriate to thedata (e.g.
to choose one label for a unit where morelabels are applicable).
Solutions concern redesign-ing the annotation schema, for example by merg-ing difference classes, allowing annotators to usemultiple labels, removing classes, or adding newclasses.
(3) ?Genuinely ambiguous expressions?
as de-scribed by Poesio and Artstein (2005).
They dis-cuss that disagreements caused by ambiguity arenot so easily solved.
(4) ?A low level of intersubjectivity?
for the in-terpretative judgements of the annotators, causedby the fact that there is less than perfect overlapbetween the mental conceptions of the annotators.The solutions mentioned above for issue (2) partlyalso apply here.
However, in this article we focuson an additional, entirely different, way of coping9with disagreements resulting from a low level ofintersubjectivity that actively exploits the system-atic differences in the annotations caused by this.1.1 Useful results from data with lowagreementData with a low interannotator agreement may bedifficult to use, but there are other fields wherepartial solutions have been found to the problem,such as the information retrieval evaluation confer-ences (TREC).
Relevance judgements in TREC as-sessments (and document relevance in general) arequite subjective and it is well known that agree-ment for relevance judgements is not very high(Voorhees and Harman report 70% three-way per-cent agreement on 15,000 documents for threeassessors (1997)).
Quite early in the history ofthe TREC, Voorhees investigated what the conse-quences of this low level of agreement are for theusefulness of results obtained on the TREC collec-tion.
It turns out that specifying a few constraints2is enough to be able to use the TREC assessmentsto obtain meaningful evaluation results (Voorhees,2000).
Inspired by this we try to find ways of look-ing at subjective data that tells us what constraintsand restrictions on the use of it follow from the pat-terns in the disagreements between annotators, asalso advised by Reidsma and Carletta (2008).1.2 Related WorkIn corpus research there is much work with anno-tations that need subjective judgements of a moresubjective nature from an annotator about the be-havior being annotated.
This holds for HumanComputer Interaction topics such as affective com-puting or the development of Embodied Conversa-tional Agents with a personality, but also for workin computational linguistics on topics such as emo-tion (Craggs and McGee Wood, 2005), subjectivity(Wiebe et al, 1999; Wilson, 2008) and agreementand disagreement (Galley et al, 2004).If we want to interpret the results of classifiers interms of the patterns of (dis)agreement found be-tween annotators, we need to subject the classifierswith respect to each other and to the ?ground truthdata?
to the same analyses used to evaluate andcompare annotators to each other.
Vieira (2002)and Steidl et al (2005) similarily remark that it2Only discuss relative performance differences on differ-ent (variations of) algorithms/systems run on exactly the sameset of assessments using the same set of topics.is not ?fair?
to penalize machine learning perfor-mance for errors made in situations where humanswould not agree either.
Vieira however only looksat the amount of disagreement and does not explic-itly relate the classes where the system and codersdisagree to the classes where the coders disagreewith each other.
Steidl et al?s approach is gearedto data which is multiply coded for the whole cor-pus (very expensive) and for annotations that canbe seen as ?additive?, i.e., where judgements arenot mutually exclusive.Passonneau et al (2008) present an extensiveanalysis of the relation between per-class machinelearning performance and interannotator agree-ment obtained on the task of labelling text frag-ments with their function in the larger text.
Theyshow that overall high agreement can indicate ahigh learnability of a class in a multiply annotatedcorpus, but that the interannotator agreement is notnecessarily predictive of the learnability of a la-bel from a single annotator?s data, especially in thecontext of what we call projective latent content.1.3 This PaperThis paper constitutes an attempt to find out howsubjective annotations, annotated with a low levelof agreement, can profitably be used for machinelearning purposes.
First we present the relevantparts of the corpus.
Subsequently, we analyse the(dis)agreements between annotators, on more as-pects than just the value of a reliability metric, andexplicitly relate the results to the way machine-learning algorithms perform on the annotated data.Finally we present two new concepts that can beused to explain and exploit this relation (?subjec-tive entity?
classifiers resp.
?consensus objective?classifiers) and give some recommendations forusing subjective data in machine-learning applica-tions.2 From Agreement to Machine LearningPerformanceWe used the hand annotated face-to-face conversa-tions from the 100 hour AMI meeting corpus (Car-letta, 2007).
In the scenario-based AMI meetings,design project groups of four players have the taskto design a new remote TV control.
Group mem-bers have roles: project manager (PM), industrialdesigner (ID), user interface design (UD), and mar-keting expert (ME).
Every group has four meetings(20-40 min.
each), dedicated to a subtask.
Most of10the time the participants sit at a square table.The meetings were recorded in a meeting roomstuffed with audio and video recording devices,so that close facial views and overview video, aswell as high quality audio is available.
Speechwas transcribed manually, and words were timealigned.
The corpus has several layers of anno-tation for several modalities, such as dialogue acts,topics, hand gestures, head gestures, subjectivity,visual focus of attention (FOA), decision points,and summaries, and is easily extendible with newlayers.
The dialogue act (DA) layer segmentsspeaker turns into dialogue act segments, on top ofthe word layer, and they are labeled with one of 15dialogue act type labels, following an annotationprocedure.In this section we will inspect (dis)agreementsand machine learning performance for two cor-pus annotation layers: the addressing annotations(Jovanovic?
et al, 2006) and for a particular typeof utterances in the corpus, the ?Yeah-utterances?
(Heylen and op den Akker, 2007).2.1 Contextual AddressingA part of the AMI corpus is also annotated with ad-dressee information.
Real dialogue acts (i.e.
all di-alogue acts but backchannels, stalls and fragments)were assigned a label indicating who the speakeraddresses his speech to (is talking to).
In thesetype of meetings most of the time the speaker ad-dresses the whole group, but sometimes his dia-logue act is particularly addressed to some indi-vidual (about 2743 of the 6590 annotated real dia-logue acts); for example because he wants to knowthat individual?s opinion.
The basis of the con-cept of addressing underlying the addressee an-notation in the AMI corpus originates from Goff-man (Goffman, 1981).
The addressee is the par-ticipant ?oriented to by the speaker in a mannerto suggest that his words are particularly for them,and that some answer is therefore anticipated fromthem, more so than from the other ratified partic-ipants?.
Sub-group addressing hardly occurs andwas not annotated.
Thus, DAs are either addressedto the group (G-addressed) or to an individual (I-addressed) (see Jovanovic et al (2006)).Another layer of the corpus contains focus of at-tention information derived from head, body andgaze observations (Ba and Odobez, 2006), so thatfor any moment it is known whether a person islooking at the table, white board, or some otherparticipant.
Gaze and focus of attention are impor-tant elements of addressing behavior, and thereforeFOA is a strong cue for the annotator who needs todetermine the addressee of an utterance.
However,FOA is not the only cue.
Other relevant cues are,for example, proper names and the use of address-ing terms such as ?you?.
Even when the gaze isdrawn to a projection screen, or the meeting is heldas a telephone conference without visuals, peopleare able to make the addressee of their utterancesclear.From an extensive (dis)agreement analysis ofthe addressing and FOA layers the following con-clusions can be summarized: the visual focus ofattention was annotated with a very high level ofagreement (Jovanovic?, 2007); in the addressee an-notation there is a large confusion between DAsbeing G-addressed or I-addressed; if the annota-tors agree on an utterance being I-addressed theytypically also agree on the particular individual be-ing addressed; ?elicit?
DAs were easier to annotatewith addressee than other types of dialog act; andreliability of addressee annotation is dependent onthe FOA context (Reidsma et al, 2008).
When thespeaker?s FOA is not directed to any participant theannotators must rely on other cues to determine theaddressee and will disagree a lot more than whenthey are helped by FOA related cues.
Some ofthese disagreements can be due to systematic sub-jective differences, e.g.
an annotator being biasedtowards the ?Group?
label for utterances that areanswers to some question.
Other disagreementsmay be caused by the annotator being forced tochoose an addressee label for utterances that werenot be clearly addressed in the first place.In this section we will not so much focus onthe subjectivity of the addressee annotation as onthe multimodal context in which annotators agreemore.
Specifically, we will look further at the waythe level of agreement with which addressee hasbeen annotated is dependent on the FOA contextof a set of utterances.
We expect this will be re-flected directly by the machine learning perfor-mance in these two contexts: the low agreementmight indicate a context where addressee is in-herently difficult to determine and furthermore thecontext with high agreement will result in annota-tions containing more consistent information thatmachine learning can model.To verify this assumption we experimented withautomatic detection of the addressee of an utter-11ance based on lexical and multimodal features.Compared to Jovanovic?
(2007), we use a limitedset of features that does not contain local contextfeatures such as ?previous addressee?
or ?previousdialogue act type?.
Besides several lexical fea-tures we also used features for focus of attentionof the speaker and listeners during the utterance.Below we describe two experiments with this task.Roughly 1 out of every 3 utterances is performedin a context where the speaker?s FOA is not di-rected at any other participant.
This gives us threecontexts to train and to test on: all utterances, allutterances where the speaker?s FOA is not directedat any other participant (1/3 of the data) and allutterances during which the speaker?s FOA is di-rected at least once at another participant (2/3 ofthe data).First Experiment For the first experiment wetrained a Bayesian Network adapted from Jo-vanovic?
(2007) on a mix of utterances from allcontexts, and tested its performance on utterancesfrom the three different contexts: (1) all data, (2)all data in the context ?at least some person inspeaker?s FOA?
and (3) all data in the context ?noperson in speaker?s FOA during utterance?.
As wasto be expected, the performance in the second con-text showed a clear gain compared to the first con-text, and the performance in the third context wasclearly worse.
The performance differences, fordifferent train/test splits, tend to be about five per-cent.Second Experiment Because the second con-text showed such a better performance, we ran asecond experiment where we trained the networkon only data from the second context, to see ifwe could improve the performance in that contexteven more.
In different train/test splits this gave usanother small performance increase.Conclusions for Contextual Addressing Theperformance increases can mostly be attributedto the distinction between different individual ad-dressees for I-addressed utterances.
Precision andrecall for the G-addressed utterances does notchange so much for the different contexts.
Thisresult is reminiscent of the fact that when the an-notators agreed on an utterance being I-addressedthey typically also agreed on the particular individ-ual being addressed.These results are particularly interesting in thelight of the high accuracy with which FOA was an-notated.
If this accuracy points at the possibility toalso achieve a high automatic recognition rate forFOA we can exploit these results in a practical ap-plication context by defining a addressee detectionmodule which only assigns an addressee to an ut-terance in the second FOA context (FOA at someparticipants), and in all other cases labels an utter-ance as ?addressee cannot be determined?.
Sucha detection module achieves a much higher preci-sion than a module that tries to assign an addresseelabel regardless; of course this happens at the costof recall.2.2 Interannotator Training and TestingClassifiers behave as they are trained.
When twoannotators differ in the way they annotate, i.e.
havedifferent ?mental conceptions?
of the phenomenonbeing annotated, we can expect that a classifiertrained on the data annotated by one annotatorbehaves different from a classifier trained on theother annotator?s data.
As Rienks describes, thisproperty allows us to use all data in the corpus, in-stead of just the multiply annotated part of it, foranalyzing differences between annotators (Rienks,2007, page 105).
We can expect that a classifier Atrained on data annotated by A will perform bet-ter when tested on data annotated by A, than whentested on data annotated by B.
In other words, clas-sifier A is geared towards modelling the ?mentalconception?
of annotator A.
In this section we willtry to find out whether it is possible to explicitlytease apart the overlap and the differences in themental conceptions of the annotators as mirroredin the behavior of classifiers, on a subjective anno-tation task.
Suppose that we build a Voting Clas-sifier, based on the votes of a number of classifierseach trained on a different annotator?s data.
TheVoting Classifier only makes a decision when allvoters agree on the class label.
How good willthe Voting Classifier perform?
Is there any rela-tion between the (dis)agreement of the voters, andthe (dis)agreement of the annotators?
Will the re-sulting Voting Classifier in some way embody theoverlap between the ?mental conceptions?
of thedifferent annotators?As an illustration and a test case for such aVoting Classifier, we consider the human annota-tions and automatic classification of a particulartype of utterances in the AMI corpus, the ?Yeah-utterances?, utterances that start with the word?yeah?.12class train-tot test-tot DH-train/test S9-train/test VK-train/testbc 3043 1347 1393/747 670/241 980/359as 3724 1859 1536/1104 689/189 1499/566in 782 377 340/229 207/60 235/88ot 1289 596 316/209 187/38 786/349Table 1: Sizes of train and test data sets used and the distribution of class labels over these data sets forthe different annotators.The Data Response tokens like ?yeah?, ?okay?,?right?
and ?no?
have the interest of linguists be-cause they may give a clue about the stance that thelistener takes towards what is said by the speaker(Gardner, 2004).
Jefferson described the differ-ence between ?yeah?
and other backchannels interms of speaker recipiency, the willingness ofthe speaker to take the floor (Jefferson, 1984).Yeah utterances make up a substantial part of thedialogue acts in the AMI meeting conversations(about eight percent).
?Yeah?
is the most ambigu-ous utterance that occurs in discussion segments inAMI meetings.
In order to get information aboutthe stance that participants take with respect to-wards the issue discussed it is important to be ableto tell utterances of ?Yeah?
as a mere backchannel,from Yeah utterances that express agreement withthe opinion of the speaker (see the work of Heylenand Op den Akker (2007)).The class variables for dialogue act types ofYeah utterances that are distinguished are: Assess(as), Backchannel (bc), Inform (in), and Other (ot).Table 1 gives a distribution of the labels in ourtrain and test data sets.
Note that for each annota-tor, a disjunct train and test set have been defined.The inter-annotator agreement on the Yeah utter-ances is low.
The pairwise alpha values for meet-ing IS1003d, which was annotated by all three an-notators, are (in brackets the number of agreed DAsegments that start with ?Yeah?
): alpha(VK,DH)= 0.36 (111), alpha (VK,S9) = 0.36 (132), al-pha(DH,S9) = 0.45 (160).Testing for Systematic Differences When onesuspects the annotations to have originated fromdifferent mental conceptions of annotators, the firststep is to test whether these differences are system-atic.
Table 2 presents the intra and inter annota-tor classification accuracy.
There is a clear perfor-mance drop between using the test data from thesame annotator from which the training data wastaken and using the test data of other annotatorsor the mixed test data of all annotators.
This sug-gest that some of the disagreements in the annota-tion stem from systematic differences in the mentalconceptions of the annotators.TESTTRAIN DH S9 VK MixedDH 69 64 52 63S9 59 68 48 57VK 63 57 66 63Table 2: Performance of classifiers (in terms of ac-curacy values ?
i.e.
percentage correct predictions)trained and tested on various data sets.
Resultswere obtained with a decision tree classifier, J48in the Weka toolkit.Building the Voting Classifier Given the threeclassifiers DH, S9 and VK, each trained on thetrain data taken from one single annotator, we havebuild a Voting Classifier that outputs a class labelwhen all three ?voters?
(the classifiers DH, S9 andVK) give the same label, and the label ?unknown?otherwise.
As was to be expected, the accuracyfor this Voting Classifier is much lower than theaccuracy of each of the single voters and than theaccuracy of a classifier trained on a mix of datafrom all annotators (see Table 3), due to the manytimes the Voting Classifier assigns the label ?un-known?
which is not present in the test data and isalways false.
The precision of the Voting Classi-fier however is higher than that of any of the otherclassifiers, for each of the classes (see Table 4).Conclusions for the Voting Classifier For thedata that we used in this experiment, building aVoting Classifier as described above gave us a highprecision classifier.
Based on our starting point,this would relate to the classifier in some way em-bodying the overlap in the mental conceptions ofeach of the annotators.
If that were true, the casesin which the Voting Classifier returns an unani-mous vote would be mostly those cases in whichthe different annotators would also have agreed.13TRAIN Accuracytrain MIX(8838) 67DH(3585) 63S9(1753) 57VK(3500) 63VotingClassifier(8838) 43Table 3: Performance of the MaxEnt classifiers (interms of accuracy values ?
i.e.
percentage cor-rect predictions) tested on the whole test set, a mixof three annotators data (4179 ?Yeah?
utterances).The first column between brackets the size of thetrain sets.ClassifierClass Voting DH S9 VK train MIXBC 71 65 63 71 69AS 73 62 64 61 66IN 60 58 34 52 50OT 86 59 32 57 80Table 4: Precision values per class label for theclassifiers.This can be tested quite simply using multiply an-notated data.
Note that not all data needs to beannotated by more annotators: just enough to testthis hypothesis.
Otherwise, it will suffice to haveenough data for each single annotator, be it over-lapping or not.
This is especially advantageouswhen the corpus is really large, such as the 100hAMI corpus.
Another way to test the hypothesisthat the voting behavior relates to intersubjectivityis to look at the type and context of the agreementsbetween annotators, found in the reliability analy-sis, and see if that relates to the type and contextof the cases where the Voting Classifier renders anunanimous judgement.
That would be strong cir-cumstantial evidence in support of the hypothesis.Note that the gain in precision is obtained atthe cost of recall, because the Voting Classifier ap-proach explicitly restricts judgements to the caseswhere annotators would have agreed and, presum-ably, therefore to the cases in which users of thedata are able to agree to the judgements as well.
Itis possible that you ?lose?
a class label in the clas-sifier by having a high precision but a recall of lessthan five percent, which in our example happenedfor the ?other?
class.3 The Classifier as Subjective Entity vsthe Classifier as Embodiment ofConsensus ObjectivityMany annotation tasks are subjective to a larger de-gree.
When this is simply taken as a given, and thesystematic disagreements resulting from the differ-ent mental conceptions of the annotators are nottaken into account while training a machine classi-fier on the resulting data, there is no simple reasonto assume that the resulting classifier is any lesssubjective in the judgements it makes.
Without ad-ditional analyses one cannot suppose the classifierdid not pick up idiosyncrasies from the annotators.We have seen that machine classifiers can indeedconsidered to be subjective in their judgements, aproperty they have inherited from the annotationsthey have been trained on.
A judgement made bysuch a classifier should be approached in a simi-lar manner as a judgement made by another per-son3.
We will call the resulting classifier thereforea ?subjective entity?
classifier.A careful analysis of the interannotator agree-ments and disagreements might make it possibleto build classifiers that partly embody the intersub-jective overlap between the mental conceptions ofthe annotators.
Because the classifier only tries togive a judgement in situations where one can ex-pect annotators or users to agree, one can approachthe judgements made by the classifier as a ?com-mon sense?
of judgements that people can agreeon, despite the subjective quality of the annotationtask.
We will call the resulting classifier a ?consen-sus objective?
classifier.4 DiscussionIn the Introduction we distinguished several usesof data annotation using human annotators.
Theanalyses and research in this paper mainly con-cerns the use of annotated data for the trainingand development of automatic machine classifiers.Ideally the annotation schema and the class labelsthat are distinguished reflect the use that is madeof the output of the machine classifiers in someparticular application in which the classifier op-erates as a module.
Imagine for example a sys-tem that detects when meeting participants are toodominant and signals the chairman of the meet-3On a side note, letting the machine classifiers judgmentsbe presented through an embodied conversational agent canbe a way to present this human-like subjectivity for the user(Reidsma et al, 2007).14ing to prevent some participants being dissatisfiedwith the decision making processes.
Or, a clas-sifier for addressee detection that signals remoteparticipants that they are addressed by the speaker.The way that users of the system interpret the sig-nals output by the classifier should correspond tothe meanings that were used by the annotators andthat were implemented in the classifier.When there is a lot of disagreement in the an-notations this should be taken into account formachine learning if one does not want to obtaina ?subjective entity?
classifier, the judgements ofwhich the user will often disagree with.
In Sec-tion 2 we presented two ways to exploit such datafor building machine classifiers.
Here we elabo-rate a bit on a difference between the two cases re-lating to the different causes of the inter-annotatordisagreement.For the addressing annotations, the annotatorssometimes had problems with choosing betweenG-addressed and I-addressed.
The participantsin the conversation usually did not seem to haveany problem with that.
There are only a few in-stances in the data where the participants explic-itly requested clarification.
It is reasonable to ex-pect that in cases where it really matters ?
for theconversational partners ?
who is being addressed,outside observers will not have a problem to iden-tify this.
Thus, in those cases where the annotatorshad problems to decide upon the type of address-ing there maybe was no reason for the participantsin the conversation to make that clear because itsimply was not an issue.
The annotators were thentripped by the fact that they were forced by the an-notation guidelines to choose one addressee label.In the dialogue act classification task somethingadditional is going on.
Here we see that annota-tors also have problems because many utterancesthemselves are ambiguous or poly-interpretable.Some annotator may prefer to call this act an as-sess where an other prefers to call it an inform, andboth may have good reason to back up their choice.A similar situation occurs in the case of the clas-sification of Yeah utterances.
The disagreementsthen seem to be caused more explicitly by differ-ing judgements of a conversational situation.5 ConclusionsWe have argued that dis-agreements between dif-ferent observers of ?subjective content?
is unavoid-able and an intrinsic quality of the interpretationand classification process of such type of content.Any subdivision of these type of phenomena into apredefined set of disjunct classes suffers from be-ing arbitrary.
There are always cases that can be-long to this but also to that class.
Analysis of an-notations of the same data by different annotatorsmay reveal that there are differences in the deci-sions they make, such as some personal preferencefor one class over another.Instead of throwing away the data as not beingvaluable at all for machine learning purposes, wehave shown two ways to exploit such data, bothleading to high precision / low recall classifiersthat in some cases refuse to give a judgement.
Thefirst way was based on the identification of subsetsof the data that show higher inter-annotator agree-ment.
When the events in these subsets can beidentified computationally the way is open to useclassifiers trained on these subsets.
We have illus-trated this with several subsets of addressing eventsin the AMI meeting corpus and we have shown thatthis leads to an improvement in the accuracy of theclassifiers.
Precision is raised in case the classi-fier refrains from making a decision in those situa-tion that fall outside the subsets.
The second wayis to train a number of classifiers, one for each ofthe annotators data part of the corpus, and builda Voting Classifier that only makes a decision incase all classifiers agree on the class label.
Thisapproach was illustrated by the problem of classi-fication of the dialogue act type of Yeah-utterancesin the AMI corpus.
The results show that the ap-proach indeed leads to the expected improvementin precision, at the cost of a lower recall, becauseof the cases in which the classifier doesn?t make adecision.AcknowledgementsThe authors are in debt to many people for manyfruitful discussions, most prominently Jean Car-letta, Ron Artstein, Arthur van Bunningen, Hen-ning Rode and Dirk Heylen.
This work is sup-ported by the European IST Programme ProjectFP6-033812 (AMIDA, publication 136).
This ar-ticle only reflects the authors?
views and fundingagencies are not liable for any use that may bemade of the information contained herein.ReferencesBa, S. O. and J.-M. Odobez.
2006.
A study on visualfocus of attention recognition from head pose in a15meeting room.
In Renals, S. and S. Bengio, editors,Proc.
of the MLMI 2006, volume 4299 of LectureNotes in Computer Science, pages 75?87.
Springer.Bakeman, R. and J. M. Gottman.
1986.
ObservingInteraction: An Introduction to Sequential Analysis.Cambridge University Press.Carletta, J. C. 2007.
Unleashing the killer corpus:experiences in creating the multi-everything AMImeeting corpus.
Language Resources and Evalua-tion, 41(2):181?190.Craggs, R. and M. McGee Wood.
2005.
Evaluatingdiscourse and dialogue coding schemes.
Computa-tional Linguistics, 31(3):289?296.Galley, M., K. McKeown, J. Hirschberg, andE.
Shriberg.
2004.
Identifying agreement anddisagreement in conversational speech: Use ofBayesian networks to model pragmatic dependen-cies.
In Proc.
of the 42nd Meeting of the ACL, pages669?676.
ACL.Gardner, R. 2004.
Acknowledging strong ties betweenutterances in talk: Connections through right as a re-sponse token.
In Proceedings of the 2004 Confer-ence of the Australian Linguistic Society, pages 1?12.Goffman, E. 1981.
Footing.
In Forms of Talk, pages124?159.
Philadelphia: University of PennsylvaniaPress.Heylen, D. and H. op den Akker.
2007.
Comput-ing backchannel distributions in multi-party conver-sations.
In Cassell, J. and D. Heylen, editors, Proc.of the ACL Workshop on Embodied Language Pro-cessing, Prague, pages 17?24.
ACL.Jefferson, G. 1984.
Notes on a systematic deploy-ment of the acknowledgement tokens ?yeah?
and?mm hm?.
Papers in Linguistics, 17:197?206.Jovanovic?, N., H. op den Akker, and A. Nijholt.
2006.A corpus for studying addressing behaviour in multi-party dialogues.
Language Resources and Evalua-tion, 40(1):5?23.Jovanovic?, N. 2007.
To Whom It May Concern -Addressee Identification in Face-to-Face Meetings.Phd thesis, University of Twente.Krippendorff, K. 1980.
Content Analysis: An Intro-duction to its Methodology, volume 5 of The SageCommText Series.
Sage Publications, Beverly Hills,London.Passonneau, R. J., T. Yano, T. Lippincott, and J. Kla-vans.
2008.
Relation between agreement mea-sures on human labeling and machine learning per-formance: Results from an art history image index-ing domain.
In Proc.
of the LREC 2008.Poesio, M. and R. Artstein.
2005.
The reliability ofanaphoric annotation, reconsidered: Taking ambigu-ity into account.
In Proc.
of the Workshop on Fron-tiers in Corpus Annotations II: Pie in the Sky, pages76?83, Ann Arbor, Michigan.
ACL.Potter, J. W. and D. Levine-Donnerstein.
1999.
Re-thinking validity and reliability in content analy-sis.
Journal of applied communication research,27(3):258?284.Reidsma, D. and J. C. Carletta.
2008.
Reliability mea-surement without limits.
Computational Linguistics,34(3).Reidsma, D., Z. M. Ruttkay, and A. Nijholt, 2007.Challenges for Virtual Humans in Human Comput-ing, chapter 16, pages 316?338.
Number 4451 inLNAI: State of the Art Surveys.
Springer Verlag,Berlin/Heidelberg.Reidsma, D., D. Heylen, and H. op den Akker.
2008.On the contextual analysis of agreement scores.
InProc.
of the LREC Workshop on Multimodal Cor-pora.Rienks, R. J.
2007.
Meetings in Smart Environments:Implications of progressing technology.
Phd thesis,SIKS Graduate School / University of Twente, En-schede, NL.Steidl, S., M. Levit, A. Batliner, E. No?th, and H. Nie-mann.
2005.
?of all things the measure is man?
auto-matic classification of emotion and intra labeler con-sistency.
In ICASSP 2005, International Conferenceon Acoustics, Speech, and Signal Processing.Vieira, R. 2002.
How to evaluate systems against hu-man judgment on the presense of disagreement?
InProc.
workshop on joint evaluation of computationalprocessing of Portugese at PorTAL 2002.Voorhees, E. M. and D. Harman.
1997.
Overview ofthe trec-5.
In Proc.
of the Fifth Text REtrieval Con-ference (TREC-5), pages 1?28.
NIST.Voorhees, E. M. 2000.
Variations in relevancejudgments and the measurement of retrieval effec-tiveness.
Information Processing & Management,36(5):697?716.Wiebe, J. M., R. F. Bruce, and T. P. O?Hara.
1999.Development and use of a gold-standard data set forsubjectivity classifications.
In Proc.
of the 37th An-nual Meeting of the ACL, pages 246?253.
ACL.Wilson, T. 2008.
Annotating subjective content inmeetings.
In Proc.
of the Language Resources andEvaluation Conference (LREC-2008).16
