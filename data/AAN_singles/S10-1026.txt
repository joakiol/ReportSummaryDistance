Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 129?133,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsCOLEUR and COLSLM: A WSD approach to Multilingual LexicalSubstitution, Tasks 2 and 3 SemEval 2010Weiwei Guo and Mona DiabCenter for Computational Learning SystemsColumbia University{weiwei,mdiab}@ccls.columbia.eduAbstractIn this paper, we present a word sensedisambiguation (WSD) based system formultilingual lexical substitution.
Ourmethod depends on having a WSD systemfor English and an automatic word align-ment method.
Crucially the approach re-lies on having parallel corpora.
For Task2 (Sinha et al, 2009) we apply a super-vised WSD system to derive the Englishword senses.
For Task 3 (Lefever & Hoste,2009), we apply an unsupervised approachto the training and test data.
Both of oursystems that participated in Task 2 achievea decent ranking among the participatingsystems.
For Task 3 we achieve the highestranking on several of the language pairs:French, German and Italian.1 IntroductionIn this paper, we present our system that was ap-plied to the cross lingual substitution for two tasksin SEMEVAL 2010, Tasks 2 and 3.
We adoptthe same approach for both tasks with some dif-ferences in the basic set-up.
Our basic approachrelies on applying a word sense disambiguation(WSD) system to the English data that comes froma parallel corpus for English and a language ofrelevance to the task, language 2 (l2).
Then weautomatically induce the English word sense cor-respondences to l2.
Accordingly, for a given testtarget word, we return its equivalent l2 words as-suming that we are able to disambiguate the targetword in context.2 Our Detailed ApproachWe approach the problem of multilingual lexicalsubstitution from a WSD perspective.
We adoptthe hypothesis that the different word senses ofambiguous words in one language probably trans-late to different lexical items in another language.Hence, our approach relies on two crucial compo-nents: a WSD module for the source language (ourtarget test words, in our case these are the Englishtarget test words) and an automatic word align-ment module to discover the target word sense cor-respondences with the foreign words in a secondlanguage.
Our approach to both tasks is unsuper-vised since we don?t have real training data anno-tated with the target words and their correspondingtranslations into l2 at the onset of the problem.Accordingly, at training time, we rely on auto-matically tagging large amounts of English data(target word instances) with their relevant sensesand finding their l2 correspondences based on au-tomatically induced word alignments.
Each ofthese English sense and l2 correspondence pairshas an associated translation probability value de-pending on frequency of co-occurrence.
This in-formation is aggregated in a look-up table overthe entire training set.
An entry in the tablewould have a target word sense type paired with allthe observed translation correspondences l2 wordtypes.
Each of the l2 word types has a probabil-ity of translation that is calculated as a normal-ized weighted average of all the instances of thisl2 word type with the English sense aggregatedacross the whole parallel corpus.
This process re-sults in an English word sense translation table(WSTT).
The word senses are derived from Word-Net (Fellbaum, 1998).
We expand the Englishword sense entry correspondences by adding thetranslations of the members of target word sensesynonym set as listed in WordNet.For alignment, we specifically use the GIZA++software for inducing word alignments across theparallel corpora (Och & Ney, 2003).
We applyGIZA++ to the parallel corpus in both directionsEnglish to l2 and l2 to English then take only theintersection of the two alignment sets, hence fo-129cusing more on precision of alignment rather thanrecall.For each language in Task 3 and Task 2, weuse TreeTagger1to do the preprocessing for alllanguages.
The preprocessing includes segmenta-tion, POS tagging and lemmatization.
Since Tree-Tagger is independent of languages, our systemdoes not rely on anything that is language spe-cific; our system can be easily applied to otherlanguages.
We run GIZA++ on the parallel cor-pus, and obtain the intersection of the alignmentsin both directions.
Meanwhile, every time a targetEnglish word appears in a sentence, we apply ourWSD system on it, using the sentence as context.From this information, we build a WSST fromthe English sense(s) to their corresponding foreignwords.
Moreover, we use WordNet as a means ofaugmenting the translation correspondences.
Weexpand the word sense to its synset from WordNetadding the l2 words that corresponded to all themember senses in the synset yielding more trans-lation variability.At test time, given a test data target word, weapply the same WSD system that is applied to thetraining corpus to create the WSTT.
Once the tar-get word instance is disambiguated in context, welook up the corresponding entry in the WSTT andreturn the ranked list of l2 correspondences.
Wepresent results for best and for oot which vary onlyin the cut off threshold.
In the BEST condition wereturn the highest ranked candidate, in the oot con-dition we return the top 10 (where available).2Given the above mentioned pipeline, Tasks 2and 3 are very similar.
Their main difference liesin the underlying WSD system applied.3 Task 23.1 System DetailsWe use a relatively simple monolingual supervisedWSD system to create the sense tags on the En-glish data.
We use the SemCor word sense anno-tated corpus.
SemCor is a subset of the BrownCorpus.
For each of our target English wordsfound disambiguated in the SemCor corpus, wecreate a sense profile for each of its senses.
Asense profile is a vector of all the content wordsthat occur in the context of this sense in the Sem-Cor corpus.
The dimensions of the vector are word1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/2Some of the target word senses had less than 10 l2 wordcorrespondences.Corpus best ootP R P RT2-COLSLM 27.59 25.99 46.61 43.91T2-COLEUR 19.47 18.15 44.77 41.72Table 1: Precision and Recall results per corpus onTask 2 test settypes, as in a bag of words model, and the vec-tor entries are the co-occurrence frequency of theword sense and the word type.
At test time, givena a target English word, we create a bag of wordtypes contextual vector for each instance of theword using the surrounding context.
We comparethe created test vector to the SemCor vectors andchoose the highest most similar sense and use thatfor sense disambiguation.
In case of ties, we returnmore than one sense tag.3.2 DataWe use both naturally occurring parallel data andmachine translation data.
The data for our firstTask 2 submission, T2-COLEUR, comprises nat-urally occurring parallel data, namely, the Span-ish English portion of the EuroParl data providedby Task 3 organizers.
For the machine transla-tion data, we use translations of the source En-glish data pertaining to the following corpora:the Brown corpus, WSJ, SensEval1, SensEval2datasets as translated by two machine translationsystems: Global Link (GL), Systran (SYS) (Guo& Diab, 2010).
We refer to the translated corpusas the SALAAM corpus.
The intuition for creatingSALAAM (an artificial parallel corpus) is to createa balanced translation corpus that is less domainand genre skewed than the EuroParl data.
This lat-ter corpus results in our 2nd system for this taskT2-COLSLM.3.3 ResultsTable 1 presents our overall results as evaluated bythe organizers.It is clear that the T2-COLSLM outperformsT2-COLEUR.4 Task 34.1 System DetailsContrary to Task 2, we apply a context based un-supervised WSD module to the English side of theparallel data.
Our unsupervised WSD method, asdescribed in (Guo & Diab, 2009), is a graph based130unsupervised WSD method.
Given a sequence ofwords W = {w1, w2...wn}, each word wiwithseveral senses {si1, si2...sim}.
A graph G = (V,E)is defined such that there exists a vertex v for eachsense.
Two senses of two different words may beconnected by an edge e, depending on their dis-tance.
That two senses are connected suggeststhey should have influence on each other, accord-ingly a maximum allowable distance is set.
Theyexplore 4 different graph based algorithms.We fo-cus on the In-Degree graph based algorithm.The In-Degree algorithm presents the problemas a weighted graph with senses as nodes and sim-ilarity between senses as weights on edges.
TheIn-Degree of a vertex refers to the number ofedges incident on that vertex.
In the weightedgraph, the In-Degree for each vertex is calcu-lated by summing the weights on the edges that areincident on it.
After all the In-Degree valuesfor each sense are computed, the sense with max-imum value is chosen as the final sense for thatword.
In our implementation of the In-Degreealgorithm, we use the JCN similarity measure forboth Noun-Noun and Verb-Verb similarity calcu-lation.4.2 DataWe use the training data from EuroParl providedby the task organizers for the 5 different languagepairs.
We participate in all the language competi-tions.
We refer to our system as T3-COLEUR.4.3 ResultsTable 2 shows our system results on Task 3, spec-ified by languages.4.4 Error Analysis and DiscussionAs shown in Table 2, our system T3-COLEURranks the highest for the French, German and Ital-ian language tasks on both best and oot.
Howeverthe overall F-measures are very low.
Our systemranks last for Dutch among 3 systems and it ismiddle of the pack for the Spanish language task.In general we note that the results for oot are nat-urally higher than for BEST since by design it is amore relaxed measure.5 Related worksOur work mainly investigates the influence ofWSD on providing machine translation candi-dates.
Carpuat & Wu (2007) and Chan et al(2007)show WSD improves MT.
However, in (Carpuat& Wu, 2007) classical WSD is missing by ignor-ing predefined senses.
They treat translation can-didates as sense labels, then find linguistic fea-tures in the English side, and cast the disambigua-tion process as a classification problem.
Of rele-vance also to our work is that related to the taskof English monolingual lexical substitution.
Forexample some of the approaches that participatedin the SemEval 2007 excercise include the follow-ing.
Yuret (2007) used a statistical language modelbased on a large corpus to assign likelihoods toeach candidate substitutes for a target word in asentence.
Martinez et al (2007) uses WordNet tofind candidate substitutes, produce word sequenceincluding substitutes.
They rank the substitutes byranking the word sequence including that substi-tutes using web queries.
In (Giuliano C. et al,2007), they extract synonyms from dictionaries.They have 2 ways of ranking of the synonyms:by similarity metric based on LSA and by occur-rence in a large 5-gram web corpus.
Dahl et al(2007) also extract synonyms from dictionaries.They present two systems.
The first one scoressubstitutes based on how frequently the local con-text match the target word.
The second one in-corporates cosine similarity.
Finally, Hassan et al(2007) extract candidates from several linguisticresources, and combine many techniques and ev-idences to compute the scores such as machinetranslation, most common sense, language modeland so on to pick the most suitable lexical substi-tution candidates.6 Conclusions and Future DirectionsIn this paper we presented a word sense disam-biguation based system for multilingual lexicalsubstitution.
The approach relies on having aWSD system for English and an automatic wordalignment method.
Crucially the approach relieson having parallel corpora.
For Task 2 we applya supervised WSD system to derive the Englishword senses.
For Task 3, we apply an unsuper-vised approach to the training and test data.
Bothof our systems that participated in Task 2 achievea decent ranking among the participating systems.For Task 3 we achieve the highest ranking on sev-eral of the language pairs: French, German andItalian.In the future, we would like to investigate theusage of the Spanish and Italian WordNets for the131Language best ootP R rank P R rankDutch 10.71 10.56 3/3 21.47 21.27 3/3Spanish 19.78 19.59 3/7 35.84 35.46 5/7French 21.96 21.73 1/7 49.44 48.96 1/5German 13.79 13.63 1/3 33.21 32.82 1/3Italian 15.55 15.4 1/3 40.7 40.34 1/3Table 2: Results of T3-COLEUR per language on Task 3 Test settask.
We would like to also expand our exami-nation to other sources of bilingual data such ascomparable corpora.
Finally, we would like to in-vestigate using unsupervised clustering of senses(Word Sense Induction) methods in lieu of theWSD approaches that rely on WordNet.ReferencesCARPUAT M. & WU D. (2007).
Improving statis-tical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), p. 61?72, Prague,Czech Republic: Association for ComputationalLinguistics.CHAN Y. S., NG H. T. & CHIANG D. (2007).
Wordsense disambiguation improves statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,p.
33?40, Prague, Czech Republic: Association forComputational Linguistics.DAHL G., FRASSICA A.
& WICENTOWSKI R. (2007).SW-AG: Local Context Matching for English Lexi-cal Substitution.
In Proceedings of the 4th workshopon Semantic Evaluations (SemEval-2007), Prague,Czech Republic.FELLBAUM C. (1998).
?wordnet: An electronic lexicaldatabase?.
MIT Press.GIULIANO C., GLIOZZO A.
& STRAPPARAVA C(2007).
FBK-irst: Lexical Substitution Task Ex-ploiting Domain and Syntagmatic Coherence.
InProceedings of the 4th workshop on Semantic Eval-uations (SemEval-2007), Prague, Czech Republic.GUO W. & DIAB M. (2009).
?Improvements to mono-lingual English word sense disambiguation?.
InACL Workshop on Semantics Evaluations.GUO W. & DIAB M. (2010).
?Combining orthogonalmonolingual and multilingual sources of evidencefor All Words WSD?.
In ACL 2010.HASSAN S., CSOMAI A., BANEA C., SINHA R. &MIHALCEA R. (2007).
UNT: SubFinder: Combin-ing Knowledge Sources for Automatic Lexical Sub-stitution.
In Proceedings of the 4th workshop on Se-mantic Evaluations (SemEval-2007), Prague, CzechRepublic.IDE N. & V RONIS J.
(1998).
Word sense disambigua-tion: The state of the art.
In Computational Linguis-tics, p. 1?40.JIANG J.
& CONRATH.
D. (1997).
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proceedings of the International Conference onResearch in Computational Linguistics, Taiwan.LEACOCK C. & CHODOROW M. (1998).
Combininglocal context and wordnet sense similarity for wordsense identification.
In WordNet, An Electronic Lex-ical Database: The MIT Press.LEFEVER C. & HOSTE V. (2009).
SemEval-2010Task 3: Cross-lingual Word Sense Disambiguation.In Proceedings of the NAACL HLT Workshop on Se-mantic Evaluations: Recent Achievements and Fu-ture Directions, Boulder, Colorado.LESK M. (1986).
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In In Proceedings ofthe SIGDOC Conference, Toronto.MARTINEZ D., KIM S. & BALDWIN T. (2007).MELB-MKB: Lexical Substitution system basedon Relatives in Context In Proceedings of the4th workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic.M.
PALMER, C. FELLBAUM S. C. L. D. & DANGH.
(2001).
English tasks: all-words and verb lex-ical sample.
In In Proceedings of ACL/SIGLEXSenseval-2, Toulouse, France.MIHALCEA R. (2005).
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedingsof Human Language Technology Conference andConference on Empirical Methods in Natural Lan-guage Processing, p. 411?418, Vancouver, BritishColumbia, Canada: Association for ComputationalLinguistics.MILLER G. A.
(1990).
Wordnet: a lexical database forenglish.
In Communications of the ACM, p. 39?41.132NAVIGLI R. (2009).
Word sense disambiguation: asurvey.
In ACM Computing Surveys, p. 1?69: ACMPress.OCH F. J.
& NEY H. (2003).
A systematic compari-son of various statistical alignment models.
Compu-tational Linguistics, 29(1), 19?51.PEDERSEN B.
& PATWARDHAN (2005).
Maximizingsemantic relatedness to perform word sense disam-biguation.
In University of Minnesota Supercomput-ing Institute Research Report UMSI 2005/25, Min-nesotta.PRADHAN S., LOPER E., DLIGACH D. & PALMERM.
(2007).
Semeval-2007 task-17: English lexi-cal sample, srl and all words.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), p. 87?92, Prague, Czech Re-public: Association for Computational Linguistics.SINHA R. & MIHALCEA R. (2007).
Unsupervisedgraph-based word sense disambiguation using mea-sures of word semantic similarity.
In Proceedingsof the IEEE International Conference on SemanticComputing (ICSC 2007), Irvine, CA.SINHA R., MCCARTHY D. & MIHALCEA R. (2009).SemEval-2010 Task 2: Cross-Lingual Lexical Sub-stitution.
In Proceedings of the NAACL HLT Work-shop on Semantic Evaluations: Recent Achieve-ments and Future Directions, Irvine, CA.SNYDER B.
& PALMER M. (2004).
The english all-words task.
In R. MIHALCEA & P.
EDMONDS,Eds., Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysisof Text, p. 41?43, Barcelona, Spain: Association forComputational Linguistics.YURET D. (2007).
KU: Word sense disambiguationby substitution.
In Proceedings of the 4th workshopon Semantic Evaluations (SemEval-2007), Prague,Czech Republic.133
