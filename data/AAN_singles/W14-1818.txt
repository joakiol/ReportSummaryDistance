Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 149?154,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsSentence-level Rewriting DetectionFan ZhangUniversity of PittsburghPittsburgh, PA, 15260zhangfan@cs.pitt.eduDiane LitmanUniversity of PittsburghPittsburgh, PA, 15260litman@cs.pitt.eduAbstractWriters usually need iterations of revisionsand edits during their writings.
To bet-ter understand the process of rewriting,we need to know what has changed be-tween the revisions.
Prior work mainly fo-cuses on detecting corrections within sen-tences, which is at the level of wordsor phrases.
This paper proposes to de-tect revision changes at the sentence level.Looking at revisions at a higher level al-lows us to have a different understandingof the revision process.
This paper alsoproposes an approach to automatically de-tect sentence revision changes.
The pro-posed approach shows high accuracy in anevaluation using first and final draft essaysfrom an undergraduate writing class.1 IntroductionRewriting is considered to be an important processduring writing.
However, conducting successfulrewriting is not an easy task, especially for novicewriters.
Instructors work hard on providing sug-gestions for rewriting (Wells et al., 2013), but usu-ally such advice is quite general.
We need to un-derstand the changes between revisions better toprovide more specific and helpful advice.There has already been work on detecting cor-rections in sentence revisions (Xue and Hwa,2014; Swanson and Yamangil, 2012; Heilmanand Smith, 2010; Rozovskaya and Roth, 2010).However, these works mainly focus on detectingchanges at the level of words or phrases.
Ac-cording to Faigley?s definition of revision change(Faigley and Witte, 1981), these works could helpthe identification of Surface Changes (changesthat do not add or remove information to the orig-inal text).
However, Text Changes (changes thatadd or remove information) will be more difficultto identify if we only look at revisions within sen-tences.
According to Hashemi and Schunn (2014),when instructors were presented a comparison ofdifferences between papers derived from words,they felt the information regarding changes be-tween revisions was overwhelming.This paper proposes to look at the changes be-tween revisions at the level of sentences.
Com-paring to detecting changes at the word level, de-tecting changes at the sentence level contains lessinformation, but still keeps enough informationto understand the authors?
intention behind theirmodifications to the text.
The sentence level editscould then be grouped and classified into differ-ent types of changes.
The long-term goal of thisproject is to allow us to be able to identify bothText Changes and Surface Changes automatically.Students, teachers, and researchers could then per-form analysis on the different types of changes andhave a better understanding of the rewriting pro-cess.
As a preliminary work, this paper exploressteps toward this goal: First, automatically gener-ate the description of changes based on four prim-itives: Add, Delete, Modify, Keep; Second, mergethe primitives that come from the same purpose.2 Related workHashemi and Schunn (2014) presented a toolto help professors summarize students?
changesacross papers before and after peer review.
Theyfirst split the original documents into sentencesand then built on the output of Compare Suite(CompareSuite, 2014) to count and highlightchanges in different colors.
Figure 1 shows ascreenshot of their work.
As we can see, the mod-ifications to the text are misinterpreted.
Line 66in the final draft should correspond to line 55 andline 56 in the first draft, while line 67 and line 68should be a split of line 57 in the first draft.
How-ever, line 67 is aligned to line 56 wrongly in theirwork.
This wrong alignment caused many mis-149recognized modifications.
According to Hashemi,the instructors who use the system think that theoverwhelming information of changes make thesystem less useful.
We hypothesize that since theirwork is based on analysis at the word level, al-though their approach might work for identifyingdifferences within one sentence, it makes mistakeswhen sentence analysis is the primary concern.Our work avoids the above problem by detect-ing differences at the sentence level.
Sentencealignment is the first step of our method; fur-ther inferences about revision changes are thenbased on the alignments generated.
We borrowideas from the research on sentence alignment formonolingual corpora.
Existing research usuallyfocuses on the alignment from the text to its sum-marization or its simplification (Jing, 2002; Barzi-lay and Elhadad, 2003; Bott and Saggion, 2011).Barzilay and Elhadad (2003) treat sentence align-ment as a classification task.
The paragraphs areclustered into groups, and a binary classifier istrained to decide whether two sentences should bealigned or not.
Nelken (2006) further improvesthe performance by using TF*IDF score instead ofword overlap and also utilizing global optimiza-tion to take sentence order information into con-sideration.
We argue that summarization couldbe considered as a special form of revision andadapted Nelken?s approach to our approach.Edit sequences are then inferred based on theresults of sentence alignment.
Fragments of ed-its that come from the same purpose will then bemerged.
Related work to our method is sentenceclustering (Shen et al., 2011; Wang et al., 2009).While sentence clustering is trying to find andcluster sentences similar to each other, our workis to find a cluster of sentences in one documentthat is similar to one sentence in the other docu-ment after merging.3 Sentence-level changes across revisions3.1 Primitives for sentence-level changesPrevious work in educational revision analysis(Faigley and Witte, 1981; Connor and Asenav-age, 1994) categorized revision changes to be ei-ther surface changes or text-based changes.
Withboth categories, six kinds of changes were definedas shown in Table 1.Different from Faigley?s definition, we defineonly 4 primitives for our first step of edit sequencegeneration: Add, Delete, Modify and Keep.
ThisCode ExplanationAddition Adding a word or phraseDeletion Omitting a word or phraseSubstitutions exchange words with synonymsPermutation rearrange of words or phrasesDistribution one segment divided into twoConsolidation combine two segments into oneTable 1: Code Definition by L.Faigley and S.Wittedefinition is similar to Bronner?s work (Bronnerand Monz, 2012).
We choose this definition be-cause these 4 primitives only correspond to onesentence at a time.
Add, Delete, Modify indicatesthat the writer has added/deleted/modified a sen-tence.
Keep means the original sentence is notmodified.
We believe Permutation, Distributionand Consolidation as defined by Faigley could bedescribed with these four primitives, which couldbe recognized in the later merge step.3.2 Data and annotationThe corpus we choose consists of paired first andfinal drafts of short papers written by undergradu-ates in a course ?Social Implications of Comput-ing Technology?.
Students are required to writepapers on one topic and then revise their own pa-pers.
The revisions are guided by other students?feedback based on a grading rubric, using a web-based peer review system.
Students first submittedtheir original paper into the system, and then wererandomly assigned to review and comment others?work according to the writing rubric.
The authorswould receive the others?
anonymous comments,and then could choose to revise their work basedon others?
comments as well as their own insightsobtained by reviewing other papers.The papers in the corpus contain two topics.In the first topic, the students discussed the rolethat Big Data played in Obama?s presidential cam-paign.
This topic contains 11 pairs of first and finaldrafts of short papers.
We name this C1.
The othertopic, named C2, talks about intellectual propertyand contains 10 pairs of paper drafts.
The studentsinvolved in these two topics are from the sameclass.
Students make more modifications to theirpapers in C2.
More details can be seen in Table 2.Our revision change detection approach con-tains three steps: sentence alignment, edit se-quence generation and merge of edit sequences.Thus we annotated for these three steps.150(a) first draft (b) final draft(c) Revision detection using Hashemi?s approachFigure 1: Fragments of a paper in corpus C2 discussing intellectual property, (c) is Hashemi?s work,green for recognized modifications, blue for insertions and red for deletionFor sentence alignment, each sentence in the fi-nal draft is assigned the index of its aligned sen-tence in the original draft.
If a sentence is newlyadded, it will be annotated as ADD.
Sentencealignment is not necessarily one-to-one.
It canalso be one-to-many (Consolidation) and many-to-one (Distribution).
Table 3 shows a fragmentof the annotation for the text shown in Figure 1.For edit sequences, the annotators do the anno-tation based on the initial draft.
For the same frag-ment in Table 3, the annotated sequence is: Keep,Modify, Delete, Modify, Add1.For edit sequence merging, we further annotateConsolidation and Distribution based on the editsequences.
In our example, 66 consolidates 55 and56, while 57 distributes to 67 and 68.pairs #D1 #D2 Avg1 Avg2C1 11 761 791 22.5 22.7C2 10 645 733 24.7 24.5Table 2: Detailed information of corpora.
#D1 and#D2 are the number of sentences in the first andfinal draft, Avg1 and Avg2 are the average numberof words in one sentence in the first and final draftAs a preliminary work, we only have one anno-tator doing all the annotations.
But for the anno-tation of sentence alignments, we have two anno-166 consolidates 55, 56; while 57 distributes to 67, 68.Notice that Consolidation is illustrated as Modify, Delete andDistribution is illustrated as Modify, Add.
As the annotatorsannotate based on the first draft, Modify always appears be-fore Add or Deletetators annotating on one pair of papers.
The papercontains 76 sentences, and the annotators only dis-agree in one sentence.
The kappa is 0.7942, whichsuggests that the annotation is reliable based onour annotation scheme.4 Automatic detection of revisionchangesThe detection of revision changes contains threeparts: sentence alignment, edit sequence genera-tion and edit sequence merging.
The first two partsgenerate edit sequences detected at the sentencelevel, while the third part groups edit sequencesand classifies them into different types of changes.Currently the third step only covers the identifica-tion of Consolidation and Distribution.Sentence Index (Final) 65 66 67 68Sentence Index (First) 54 55,56 57 57Table 3: An example of alignment annotationSentence alignment We adapted Nelken?s ap-proach to our problem.Alignment based on sentence similarityThe alignment task goes through three stages.1.
Data preparation: for each sentence in the an-notated final draft, if it is not a new sentence, cre-ate a sentence pair with its aligned sentence in the2We calculate the Kappa value following Macken?s idea(Macken, 2010), where the aligned sentences are categorizedas direct-link, while new added sentences are categorized asnull-link (ADD).151first draft.
The pair is considered to be an alignedpair.
Also, randomly select another sentence fromthe first draft to make a negative sentence pair.Thus we ensure there are nearly equal numbers ofpositive and negative cases in the training data.2.
Training: according to the similarity met-ric defined, calculate the similarity of the sentencepairs.
A logistic regression classifier predictingwhether a sentence pair is aligned or not is trainedwith the similarity score as the feature.
In addi-tion to classification, the classifier is also used toprovide a similarity score for global alignment.3.
Alignment: for each pair of paper drafts, con-struct sentence pairs using the Cartesian productof sentences in the first draft and sentences in thefinal.
Logistic regression classifier is used to deter-mine whether the sentence pair is aligned or not.We added Levenshtein distance (LD) (Leven-shtein, 1966) as another similarity metric in ad-dition to Nelken?s metrics.
Together three similar-ity metrics were compared: Levenshtein Distance,Word Overlap(WO), and TF*IDF.Global alignmentSentences are likely to preserve the same or-der between rewritings.
Thus, sentence or-dering should be an important feature in sen-tence alignment.
Nelken?s work modifies theNeedleman-Wunsch alignment (Needleman andWunsch, 1970) to find the sentence alignments andgoes in the following steps.Step1: The logistic regression classifier previ-ously trained assigns a probability value from 0 to1 for each sentence pair s(i, j).
Use this value asthe similarity score of sentence pair: sim(i, j).Step2: Starting from the first pair of sen-tences, find the best path to maximize the likeli-hood between sentences according to the formulas(i, j) = max{s(i ?
1, j ?
1) + sim(i, j), s(i ?1, j) + sim(i, j) , s(i, j ?
1) + sim(i, j)}Step3: Infer the sentence alignments by backtracing the matrix s(i, j).We found out that changing bolded parts in theformula to s(i, j) = max{s(i ?
1, j ?
1) +sim(i, j), s(i ?
1, j) + insertcost , s(i, j ?
1) +deletecost} shows better performance in our prob-lem.
According to our experiment with C1, insert-cost and deletecost are both set to 0.1 as they arefound to be the most effective during practice.Edit sequence generation This step is an inter-mediate step, which tries to generate the edit se-quence based on the sentence alignment resultsfrom the previous step.
The edit sequences gen-erated would later be grouped together and clas-sified into different types.
In our current work, arule-based method is proposed for this step.Step1: The index of original document i and theindex of the modified document j both start from0.
If sentence i in the original document is alignedto sentence j in the modified one, go to step 2, ifnot go to step 3.Step2: If the two sentences are exactly the same,add Keep to the edit sequence, if not, add Modify.Increase i and j by 1, go to step 1.Step3: Check the predicted alignment index ofsentence j, if the predicted index is larger than sen-tence i in the original document, add Delete andincrease i by 1, otherwise, mark as Add and in-crease j by 1, go to step 1.Edit sequence merging Distribution meanssplitting one sentence into two or more sentences,while Consolidation means merging two or moresentences into one sentence.
These two operationscan be derived with primitives Modify, Add andDelete.
They follow the following patterns:Consolidation: Modify-Delete-Delete-...Distribution: Modify-Add-Add-...These sequences both start with Modify fol-lowed with a repetitive number of Delete or Add.A group of edit sequences can be merged if theycan be merged to a sentence close to the sentencein the other draft.
We applied a rule-based ap-proach based on our observations.We first scan through the sequence generatedabove.
Sequences with Modify-Add-... or Mod-ify-Delete-... are extracted.
For each sequence ex-tracted, if there are n consecutive Add or Deletefollowing Modify, create n groups, Groupi(i ?n) contains sentences from the modified sentenceto the next consecutive i sentences.
For eachgroup, merge all the sentences, and use the clas-sifier trained above to get the similarity scoreSimgroupibetween the merged sentence and theoriginal one.
If there are multiple groups classi-fied as aligned, choose group i that has the largestSimgroupi, merge the basic edit operations intoConsolidation or Distribution.
If none of thegroups are classified as aligned, do not merge.5 EvaluationSentence alignment We use accuracy as theevaluation metric.
For each pair of drafts, wecount the number of sentences in the final draft152N1.
For each sentence in the final draft, we countthe number of sentences that get the correct align-ment as N2.
The accuracy of the sentence align-ment isN2N1.3We use Hashemi?s approach as the baseline.Compare Suite colors the differences out, asshown in Figure 1.
We treat the green sentencesas Modify and aligned to the original sentence.For our method, we tried four groups of set-tings.
Group 1 and group 2 perform leave-one-outcross validation on C1 and C2 (test on one pair ofpaper drafts and train on the others).
Group 3 andgroup 4 train on one corpus and test on the other.Group LD WO TF*IDF Baseline1 0.9811 0.9863 0.9931 0.94272 0.9649 0.9593 0.9667 0.90113 0.9727 0.9700 0.9727 0.90454 0.9860 0.9886 0.9798 0.9589Table 4: Accuracy of our approach vs. baselineTable 4 shows that all our methods beat thebaseline4.
Among the three similarity metrics,TF*IDF is the most predictive.Edit sequence generation We use WER (WordError Rate) from speech recognition for evaluat-ing the generated sequence by comparing the gen-erated sequence to the gold standard.WER is calculated based on edit distances be-tween sequences.
The ratio is calculated as:WER =S+D+IN, where S means the number ofmodifications, D means the number of deletes, Imeans the number of inserts.We apply our method on the gold standard ofsentence alignment.
The generated edit sequenceis then compared with the gold standard edit se-quence to calculate WER.
Hashemi?s approach ischosen as the baseline.
The WER of our method is0.035 on C1 and 0.017 on C2, comparing to 0.091on C1 and 0.153 on C2 for the baseline, whichshows that our rule-based method has promise.3Notice that we have the case that one sentence is alignedto two sentences (i.e.
Consolidation, as sentence 66 in Table3).
In our evaluation, an alignment is considered to be correctonly if the alignment covers all the sentences that should becovered.
For example, if Sentence 66 in Table 3 is aligned toSentence 55 in the first draft, it is counted as an error.4For Groups 1 and 2, we calculate the accuracy ofHashemi?s approach under a leave-one-out setting, each timeremove one pair of document and calculate the accuracy.
Asignificance test is also conducted, the worst metric LD inGroup 1 and WO in Group 2 both beat the baseline signifi-cantly ( p1= 0.025,p2= 0.017) in two-tailed T-test.Applying our method on the predicted alignmenton the first step gets 0.067 on C1 and 0.025 on C2,which although degraded still beats the baseline.Edit sequence merging There are only a limitednumber of Consolidation and Distribution exam-ples in our corpus.
Together there are 9 Consolida-tion and 5 Distribution operations.
In our currentdata, the number of sentences involved in theseoperations is always 2.
Our rule-based methodachieved 100% accuracy in the identification ofthese operations.
It needs further work to see ifthis method would perform equally well in morecomplicated corpora.6 ConclusionThis paper presents a preliminary work in the ef-fort of describing changes across revisions at ahigher level than words, motivated by a long termgoal to build educational applications to supportrevision analysis for writing.
Comparing to revi-sion analysis based on words or phrases, our ap-proach is able to capture higher level revision op-erations.
We also propose algorithms to detect re-vision changes automatically.
Experiments showthat our method has a reliable performance.Currently we are investigating applying se-quence merging on the automatic generated editsequences based on edit distances directly.
Ournext plan is to develop a tool for comparing drafts,and conduct user studies to have extrinsic evalua-tions on whether our method would provide moreuseful information to the user.
We are also plan-ning to do further analysis based on the revisionsdetected, and ultimately be able to distinguish be-tween surface changes and text-based changes.AcknowledgmentsWe would like to thank W. Wang, W. Luo, H. Xue,and the ITSPOKE group for their helpful feedbackand all the anonymous reviewers for their sugges-tions.This research is supported by the Institute ofEducation Sciences, U.S. Department of Educa-tion, through Grant R305A120370 to the Univer-sity of Pittsburgh.
The opinions expressed arethose of the authors and do not necessarily repre-sent the views of the Institute or the U.S. Depart-ment of Education.153ReferencesRegina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProceedings of the 2003 conference on Empiricalmethods in natural language processing, pages 25?32.
Association for Computational Linguistics.Stefan Bott and Horacio Saggion.
2011.
An un-supervised alignment algorithm for text simplifica-tion corpus construction.
In Proceedings of theWorkshop on Monolingual Text-To-Text Generation,pages 20?26.
Association for Computational Lin-guistics.Amit Bronner and Christof Monz.
2012.
User editsclassification using document revision histories.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 356?366.
Association for Computa-tional Linguistics.CompareSuite.
2014.
Compare suite, feature-richfile and folder compare tool.
http://www.comparesuite.com.Ulla Connor and Karen Asenavage.
1994.
Peer re-sponse groups in esl writing classes: How much im-pact on revision?
Journal of Second Language Writ-ing, 3(3):257?276.Lester Faigley and Stephen Witte.
1981.
Analyzingrevision.
College composition and communication,pages 400?414.Homa B. Hashemi and Christian D. Schunn.
2014.A tool for summarizing students?
shanges acrossdrafts.
In International Conference on IntelligentTutoring Systems(ITS).Michael Heilman and Noah A Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 1011?1019.Association for Computational Linguistics.Hongyan Jing.
2002.
Using hidden markov modelingto decompose human-written summaries.
Computa-tional linguistics, 28(4):527?543.Vladimir I Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
InSoviet physics doklady, volume 10, page 707.Lieve Macken.
2010.
An annotation scheme andgold standard for dutch-english word alignment.In 7th conference on International Language Re-sources and Evaluation (LREC 2010), pages 3369?3374.
European Language Resources Association(ELRA).Saul B Needleman and Christian D Wunsch.
1970.A general method applicable to the search for simi-larities in the amino acid sequence of two proteins.Journal of molecular biology, 48(3):443?453.Rani Nelken and Stuart M Shieber.
2006.
Towards ro-bust context-sensitive sentence alignment for mono-lingual corpora.
In EACL.Alla Rozovskaya and Dan Roth.
2010.
Annotatingesl errors: Challenges and rewards.
In Proceedingsof the NAACL HLT 2010 fifth workshop on innova-tive use of NLP for building educational applica-tions, pages 28?36.
Association for ComputationalLinguistics.Chao Shen, Tao Li, and Chris HQ Ding.
2011.
Inte-grating clustering and multi-document summariza-tion by bi-mixture probabilistic latent semantic anal-ysis (plsa) with sentence bases.
In AAAI.Ben Swanson and Elif Yamangil.
2012.
Correctiondetection and error type selection as an esl educa-tional aid.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 357?361.
Association for Computa-tional Linguistics.Dingding Wang, Shenghuo Zhu, Tao Li, and YihongGong.
2009.
Multi-document summarization us-ing sentence-based topic models.
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,pages 297?300.
Association for Computational Lin-guistics.Jaclyn M. Wells, Morgan Sousa, Mia Martini, andAllen Brizee.
2013.
Steps for revising your pa-per.
http://owl.english.purdue.edu/owl/resource/561/05.Huichao Xue and Rebecca Hwa.
2014.
Improved cor-rection detection in revised esl sentences.
In Pro-ceedings of The 52nd Annual Meeting of the Associ-ation for Computational Linguistics(ACL).154
