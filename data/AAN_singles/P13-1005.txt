Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?52,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSmoothed marginal distribution constraints for language modelingBrian Roark?
?, Cyril Allauzen?
and Michael Riley?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New Yorkroarkbr@gmail.com, {allauzen,riley}@google.comAbstractWe present an algorithm for re-estimatingparameters of backoff n-gram languagemodels so as to preserve given marginaldistributions, along the lines of well-known Kneser-Ney (1995) smoothing.Unlike Kneser-Ney, our approach is de-signed to be applied to any given smoothedbackoff model, including models that havealready been heavily pruned.
As a result,the algorithm avoids issues observed whenpruning Kneser-Ney models (Siivola et al,2007; Chelba et al, 2010), while retain-ing the benefits of such marginal distribu-tion constraints.
We present experimen-tal results for heavily pruned backoff n-gram models, and demonstrate perplexityand word error rate reductions when usedwith various baseline smoothing methods.An open-source version of the algorithmhas been released as part of the OpenGrmngram library.11 IntroductionSmoothed n-gram language models are the de-facto standard statistical models of language fora wide range of natural language applications, in-cluding speech recognition and machine transla-tion.
Such models are trained on large text cor-pora, by counting the frequency of n-gram col-locations, then normalizing and smoothing (reg-ularizing) the resulting multinomial distributions.Standard techniques store the observed n-gramsand derive probabilities of unobserved n-grams viatheir longest observed suffix and ?backoff?
costsassociated with the prefix histories of the unob-served suffixes.
Hence the size of the model growswith the number of observed n-grams, which isvery large for typical training corpora.1www.opengrm.orgNatural language applications, however, arecommonly used in scenarios requiring relativelysmall footprint models.
For example, applica-tions running on mobile devices or in low latencystreaming scenarios may be required to limit thecomplexity of models and algorithms to achievethe desired operating profile.
As a result, statisti-cal language models ?
an important component ofmany such applications ?
are often trained on verylarge corpora, then modified to fit within somepre-specified size bound.
One method to achievesignificant space reduction is through random-ized data structures, such as Bloom (Talbot andOsborne, 2007) or Bloomier (Talbot and Brants,2008) filters.
These data structures permit effi-cient querying for specific n-grams in a modelthat has been stored in a fraction of the spacerequired to store the full, exact model, thoughwith some probability of false positives.
Anothercommon approach ?
which we pursue in this pa-per ?
is model pruning, whereby some number ofthe n-grams are removed from explicit storage inthe model, so that their probability must be as-signed via backoff smoothing.
One simple prun-ing method is count thresholding, i.e., discardingn-grams that occur less than k times in the corpus.Beyond count thresholding, the most widely usedpruning methods (Seymore and Rosenfeld, 1996;Stolcke, 1998) employ greedy algorithms to re-duce the number of stored n-grams by comparingthe stored probabilities to those that would be as-signed via the backoff smoothing mechanism, andremoving those with the least impact according tosome criterion.While these greedy pruning methods are highlyeffective for models estimated with most com-mon smoothing approaches, they have been shownto be far less effective with Kneser-Ney trainedlanguage models (Siivola et al, 2007; Chelba etal., 2010), leading to severe degradation in modelquality relative to other standard smoothing meth-434-gram models Backoff InterpolatedPerplexity n-grams Perplexity n-gramsSmoothing method full pruned (?1000) full pruned (?1000)Absolute Discounting (Ney et al, 1994) 120.5 197.3 383.4 119.8 198.1 386.2Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4Ristad (1995) 126.4 203.6 395.6 ?
?- N/A ?
?-Katz (1987) 119.8 198.1 386.2 ?
?- N/A ?
?-Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7Mod.
Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1Table 1: Reformatted version of Table 3 in Chelba et al (2010), demonstrating perplexity degradation of Kneser-Neysmoothed models in contrast to other common smoothing methods.
Data: English Broadcast News, 128M words training;692K words test; 143K word vocabulary.
4-gram language models, pruned using Stolcke (1998) relative entropy pruning toapproximately 1.3% of the original size of 31,095,260 n-grams.ods.
Thus, while Kneser-Ney may be the preferredsmoothing method for large, unpruned models?
where it can achieve real improvements overother smoothing methods ?
when relatively sparse,pruned models are required, it has severely dimin-ished utility.Table 1 presents a slightly reformatted versionof Table 3 from Chelba et al (2010).
In theirexperiments (see Table 1 caption for specifics ontraining/test setup), they trained 4-gram Broad-cast News language models using a variety ofboth backoff and interpolated smoothing methodsand measured perplexity before and after Stol-cke (1998) relative entropy based pruning.
Withthis size training data, the perplexity of all ofthe smoothing methods other than Kneser-Neydegrades from around 120 with the full modelto around 200 with the heavily pruned model.Kneser-Ney smoothed models have lower perplex-ity with the full model than the other methods byabout 5 points, but degrade with pruning to farhigher perplexity, between 270-285.The cause of this degradation is Kneser-Ney?sunique method for estimating smoothed languagemodels, which will be presented in more detail inSection 3.
Briefly, the smoothing method reesti-mates lower-order n-gram parameters in order toavoid over-estimating the likelihood of n-gramsthat already have ample probability mass allocatedas part of higher-order n-grams.
This is done viaa marginal distribution constraint which requiresthe expected frequency of the lower-order n-gramsto match their observed frequency in the trainingdata, much as is commonly done for maximumentropy model training.
Goodman (2001) provedthat, under certain assumptions, such constraintscan only improve language models.
Lower-ordern-gram parameters resulting from Kneser-Ney arenot relative frequency estimates, as with othersmoothing methods; rather they are parametersestimated specifically for use within the largersmoothed model.There are (at least) a couple of reasons why suchparameters do not play well with model pruning.First, the pruning methods commonly use lowerorder n-gram probabilities to derive an estimateof state marginals, and, since these parameters areno longer smoothed relative frequency estimates,they do not serve that purpose well.
For this rea-son, the widely-used SRILM toolkit recently pro-vided switches to modify their pruning algorithmto use another model for state marginal estimates(Stolcke et al, 2011).
Second, and perhaps moreimportantly, the marginal constraints that were ap-plied prior to smoothing will not in general be con-sistent with the much smaller pruned model.
Forexample, if a bigram parameter is modified dueto the presence of some set of trigrams, and thensome or all of those trigrams are pruned from themodel, the bigram associated with the modifiedparameter will be unlikely to have an overall ex-pected frequency equal to its observed frequencyanymore.
As a result, the resulting model degradesdramatically with pruning.In this paper, we present an algorithm thatimposes marginal distribution constraints of thesort used in Kneser-Ney modeling on arbitrarysmoothed backoff n-gram language models.
Ourapproach makes use of the same sort of deriva-tion as the original Kneser-Ney modeling, but,among other differences, relies on smoothed es-timates of the empirical relative frequency ratherthan the unsmoothed observed frequency.
The al-gorithm can be applied after the smoothed modelhas been pruned, hence avoiding the pitfalls asso-ciated with Kneser-Ney modeling.
Furthermore,while Kneser-Ney is conventionally defined as avariant of absolute discounting, our method canbe applied to models smoothed with any backoffsmoothing, including mixtures of models, widely44used for domain adaptation.We next establish formal preliminaries andour smoothed marginal distribution constraintsmethod.2 PreliminariesN-gram language models are typically presentedmathematically in terms of words w, the strings(histories) h that precede them, and the suffixesof the histories (backoffs) h?
that are used in thesmoothing recursion.
Let V be a vocabulary (al-phabet), and V ?
a string of zero or more symbolsdrawn from V .
Let V k denote the set of stringsw ?
V ?
of length k, i.e., |w| = k. We will usevariables u, v, w, x, y, z ?
V to denote single sym-bols from the vocabulary; h, g ?
V ?
to denote his-tory sequences preceding the specific word; andh?, g?
?
V ?
the respective backoff histories of hand g as typically defined (see below).
For a stringw = w1 .
.
.
w|w| we can calculate the smoothedconditional probability of each word wi in the se-quence given the k words that preceded it, de-pending on the order of the Markov model.
Lethki = wi?k .
.
.
wi?1 be the previous k words inthe sequence.
Then the smoothed model is definedrecursively as follows:P(wi | hki ) ={P(wi | hki ) if c(hkiwi) > 0?
(hki ) P(wi | hk?1i ) otherwisewhere c(hkiwi) is the count of the n-gram sequencewi?k .
.
.
wi in the training corpus; P is a regular-ized probability estimate that provides some prob-ability mass for unobserved n-grams; and ?
(hki )is a factor that ensures normalization.
Note thatfor h = hki , the typically defined backoff historyh?
= hk?1i , i.e., the longest suffix of h that is not hitself.
When we use h?
and g?
(for notational con-venience) in future equations, it is this definitionthat we are using.There are many ways to estimate P, includ-ing absolute discounting (Ney et al, 1994), Katz(1987) and Witten and Bell (1991).
Interpolatedmodels are special cases of this form, where the Pis determined using model mixing, and the ?
pa-rameter is exactly the mixing factor value for thelower order model.N-gram language models allow for a sparse rep-resentation, so that only a subset of the possible n-grams must be explicitly stored.
Probabilities forthe rest of the n-grams are calculated through the?otherwise?
semantics in the equation above.
Foran n-gram language model G, we will say that ann-gram hw ?
G if it is explicitly represented inthe model; otherwise hw 6?
G. In the standard n-gram formulation above, the assumption is that ifc(hkiwi) > 0 then the n-gram has a parameter; yetwith pruning, we remove many observed n-gramsfrom the model, hence this is no longer the ap-propriate criterion.
We reformulate the standardequation as follows:P(wi|hki ) ={?
(hkiwi) if hkiwi ?
G?
(hki , hk?1i ) P(wi|hk?1i ) otherwise(1)where ?
(hkiwi) is the parameter associated withthe n-gram hkiwi and ?
(hki , hk?1i ) is the backoffcost associated with going from state hki to statehk?1i .
We assume that, if hw ?
G then all prefixesand suffixes of hw are also in G.Figure 1 presents a schema of an automaton rep-resentation of an n-gram model, of the sort used inthe OpenGrm library (Roark et al, 2012).
Statesrepresent histories h, and the words w, whoseprobabilities are conditioned on h, label the arcs,leading to the history state for the subsequentword.
State labels are provided in Figure 1 asa convenience, to show the (implicit) history en-coded by the state, e.g., ?xyz?
indicates that thestate represents a history with the previous threesymbols being x, y and z.
Failure arcs, labeledwith a ?
in Figure 1, encode an ?otherwise?
se-mantics and have as destination the origin state?sbackoff history.
Many higher order states willback off to the same lower order state, specificallythose that share the same suffix.Note that, in general, the recursive definition ofbackoff may require the traversal of several back-yzzxyzu/?(xyzu)w/?(yzw)w/?(zw)?/?(xyz,yz)?/?(yz,z)zwyyz?/?
(yyz,yz)yzw?yzu yzvv/?(yyzv)w/?(yyzw)?/?(z,?)?/?(yzw,zw)z/?
(z)Figure 1: N-gram weighted automaton schema.
State labelsare presented for convenience, to specify the history implic-itly encoded by the state.45off arcs before emitting a word, e.g., the highestorder states in Figure 1 needing to traverse a cou-ple of ?
arcs to reach state ?z?.
We can definethe backoff cost between a state hki and any of itssuffix states as follows.
Let ?
(h, h) = 1 and form > 1,?
(hki , hk?mi ) =m?j=1?
(hk?j+1i , hk?ji ).If hkiw 6?
G then the probability of that n-gramwill be defined in terms of backoff to its longestsuffix hk?mi w ?
G. Let hwG denote the longestsuffix of h such that hwGw ?
G. Note that thisis not necessarily a proper suffix, since hwG couldbe h itself or it could be .
ThenP(w | h) = ?
(h, hwG) ?
(hwGw) (2)which is equivalent to equation 1.3 Marginal distribution constraintsMarginal distribution constraints attempt to matchthe expected frequency of an n-gram with its ob-served frequency.
In other words, if we use themodel to randomly generate a very large corpus,the n-grams should occur with the same rela-tive frequency in both the generated and original(training) corpus.
Standard smoothing methodsovergenerate lower-order n-grams.
Using standardn-gram notation (where g?
is the backoff historyfor g), this constraint is stated in Kneser and Ney(1995) asP?
(w | h?)
=?g:g?=h?P(g, w | h?)
(3)where P?
is the empirical relative frequency esti-mate.
Taking this approach, certain base smooth-ing methods end up with very nice, easy to cal-culate solutions based on counts.
Absolute dis-counting (Ney et al, 1994) in particular, using theabove approach, leads to the well-known Kneser-Ney smoothing approach (Kneser and Ney, 1995;Chen and Goodman, 1998).
We will follow thissame approach, with a couple of changes.
First,we will make use of regularized estimates of rela-tive frequency P rather than raw relative frequencyP?.
Second, rather than just looking at observedhistories h that back off to h?, we will look atall histories (observed or not) of the length ofthe longest history in the model.
For notationalsimplicity, suppose we have an n+1-gram model,hence the longest history in the model is of lengthn.
Assume the length of the particular backoff his-tory |h?| = k. Let V n?kh?
be the set of stringsh ?
V n with h?
as a suffix.
Then we can restatethe marginal distribution constraint in equation 3asP(w | h?)
=?h?V n?kh?P(h,w | h?)
(4)Next we solve for ?
(h?w) parameters used inequation 1.
Note that h?
is a suffix of any h ?V n?kh?, so conditioning probabilities on h and h?is the same as conditioning on just h. Each ofthe following derivation steps simply relies on thechain rule or definition of conditional probability,as well as pulling terms out of the summation.P(w | h?)
=?h?V n?kh?P(h,w | h?
)=?h?V n?kh?P(w | h, h?)
P(h | h?
)=?h?V n?kh?P(w | h) P(h)?g?V n?kh?P(g)= 1?g?V n?kh?P(g)?h?V n?kh?P(w | h) P(h) (5)Then, multiplying both sides by the normaliz-ing denominator on the right-hand side and usingequation 2 to substitute ?
(h, hwG) ?
(hwGw) forP(w | h):P(w | h?
)?g?V n?kh?P(g) =?h?V n?kh?P(w | h) P(h)=?h?V n?kh??
(h, hwG) ?
(hwGw) P(h) (6)Note that we are only interested in h?w ?
G,hence there are two disjoint subsets of historiesh ?
V n?kh?
that are being summed over: thosesuch that hwG = h?
and those such that |hwG| >|h?|.
We next separate these sums in the next stepof the derivation:P(w | h?
)?g?V n?kh?P(g) =?h?V n?kh?:|hwG|>|h?|?
(h, hwG) ?
(hwGw) P(h) +?h?V n?kh?:hwG=h??
(h, h?)
?
(h?w) P(h) (7)Finally, we solve for ?
(h?w) in the second sumon the right-hand side of equation 7, yielding theformula in equation 8.
Note that this equation isthe correlate of equation (6) in Kneser and Ney46?
(h?w) =P(w | h?
)?g?V n?kh?P(g) ?
?h?V n?kh?:|hwG|>|h?|?
(h, hwG) ?
(hwGw) P(h)?h?V n?kh?:hwG=h??
(h, h?)
P(h)(8)(1995), modulo the two differences noted earlier:use of smoothed probability P rather than raw rel-ative frequency; and summing over all history sub-strings in V n?kh?
rather than just those with countgreater than zero, which is also a change due tosmoothing.
Keep in mind, P is the target expectedfrequency from a given smoothed model.
Kneser-Ney models are not useful input models, sincetheir P n-gram parameters are not relative fre-quency estimates.
This means that we cannot sim-ply ?repair?
pruned Kneser-Ney models, but mustuse other smoothing methods where the smoothedvalues are based on relative frequency estimation.There are, in addition, two other important dif-ferences in our approach from that in Kneser andNey (1995), which would remain as differenceseven if our target expected frequency were theunsmoothed relative frequency P?
instead of thesmoothed estimate P. First, the sum in the nu-merator is over histories of length n, the highestorder in the n-gram model, whereas in the Kneser-Ney approach the sum is over histories that imme-diately back off to h?, i.e., from the next highestorder in the n-gram model.
Thus the unigram dis-tribution is with respect to the bigram model, thebigram model is with respect to the trigram model,and so forth.
In our optimization, we sum in-stead over all possible history sequences of lengthn.
Second, an early assumption made in Kneserand Ney (1995) is that the denominator term intheir equation (6) (our Eq.
8) is constant across allwords for a given history, which is clearly false.We do not make this assumption.
Of course, theprobabilities must be normalized, hence the finalvalues of ?
(h?w) will be proportional to the val-ues in Eq.
8.We briefly note that, like Kneser-Ney, if thebaseline smoothing method is consistent, then theamount of smoothing in the limit will go to zeroand our resulting model will also be consistent.The smoothed relative frequency estimate P andhigher order ?
values on the right-hand side of Eq.8 are given values (from the input smoothed modeland previous stages in the algorithm, respectively),implying an algorithm that estimates highest or-ders of the model first.
In addition, steady statehistory probabilities P(h) must be calculated.
Weturn to the estimation algorithm next.4 Model constraint algorithmOur algorithm takes a smoothed backoff n-gramlanguage model in an automaton format (see Fig-ure 1) and returns a smoothed backoff n-gram lan-guage model with the same topology.
For all n-grams in the model that are suffixes of other n-grams in the model ?
i.e., that are backed-off to?
we calculate the weight provided by equation 8and assign it (after normalization) to the appropri-ate n-gram arc in the automaton.
There are severalimportant considerations for this algorithm, whichwe address in this section.
First, we must providea probability for every state in the model.
Second,we must memoize summed values that are usedrepeatedly.
Finally, we must iterate the calcula-tion of certain values that depend on the n-gramweights being re-estimated.4.1 Steady state probability calculationThe steady state probability P(h) is taken to be theprobability of observing h after a long word se-quence, i.e., the state?s relative frequency in a longsequence of randomly-generated sentences fromthe model:P(h) = limm???w1...wmP?
(w1 .
.
.
wmh) (9)where P?
is the corpus probability derived as fol-lows: The smoothed n-gram probability modelP(w | h) is naturally extended to a sentences = w0 .
.
.
wl, where w0 = <s> and wl = </s>are the sentence initial and final words, by P(s) =?li=1 P(wi | hni ).
The corpus probability s1 .
.
.
sris taken as:P?
(s1 .
.
.
sr) = (1?
?
)?r?1r?i=1P(si) (10)where ?
parameterizes the corpus length distri-bution.2 Assuming the n-gram language modelautomaton G has a single final state </s> into2P?
models words in a corpus rather than a single sen-tence since Equation 9 tends to zero as m ?
?
otherwise.In Markov chain terms, the corpus distribution is made irre-ducible to allow a non-trivial stationary distribution.47which all </s> arcs enter, adding a ?
weighted arc from the </s> state to the initial state andhaving a final weight 1 ?
?
in order to leave theautomaton at the </s> state will model this cor-pus distribution.
According to Eq.
9, P (h) is thenthe stationary distribution of the finite irreducibleMarkov Chain defined by this altered automaton.There are many methods for computing such a sta-tionary distribution; we use the well-known powermethod (Stewart, 1999).One difficulty remains to be resolved.
Thebackoff arcs have a special interpretation in theautomaton: they are traversed only if a word failsto match at the higher order.
These failure arcsmust be properly handled before applying stan-dard stationary distribution calculations.
A simpleapproach would be for each word w?
and state hsuch that hw?
/?
G, but h?w?
?
G, add a w?
arcfrom state h to h?w?
with weight ?
(h, h?)?(h?w?
)and then remove all failure arcs (see Figure 2a).This however results in an automaton with |V | arcsleaving every state, which is unwieldy with largervocabularies and n-gram orders.
Instead, for eachword w and state h such that hw ?
G, add a w arcfrom state h to h?w with weight ??
(h, h?)?
(h?w)and then replace all failure labels with  labels (seeFigure 2b).
In this case, the added negatively-weighted arcs compensate for the excess probabil-ity mass allowed by the epsilon arcs3.
The numberof added arcs is no more than found in the originalmodel.4.2 Accumulation of higher order valuesWe are summing over all possible histories oflength n in equation 8, and the steady state prob-ability calculation outlined in the previous sectionincludes the probability mass for histories h 6?
G.The probability mass of states not inG ends up be-ing allocated to the state representing their longestsuffix that is explicitly in G. That is the state thatwould be active when these histories are encoun-tered.
Hence, once we have calculated the steadystate probabilities for each state in the smoothedmodel, we only need to sum over states explicitlyin the model.As stated earlier, the use of ?
(hwGw) in thenumerator of equation 8 for hwG that are largerthan h?
implies that the longer n-grams must be3Since each negatively-weighted arc leaving a stateexactly cancels an epsilon arc followed by a matchingpositively-weighted arc in each iteration of the powermethod, convergence is assured.
(a) (b)KKw/?(Kw)w/?(Kw)?/?(K,K)KwKww/?(K,K)?(Kw)KKw/?(Kw)w/?(Kw)?/?(K,K)KwKww/?(K,K)?
(Kw)Figure 2: Schemata showing failure arc handling: (a) ?removal: add w?
arc (red), delete ?
arc; (b) ?
replacement:add w arc (red), replace ?
by  (red)re-estimated first.
Thus we process each historylength in descending order, finishing with the un-igram state.
Since we assume that, for every n-gram hw ?
G, every prefix and suffix is alsoin G, we know that if h?w 6?
G then there isno history h such that h?
is a suffix of h andhw ?
G. This allows us to recursively accumu-late the ?
(h, h?)
P(h) in the denominator of Eq.
8.For every n-gram, we can accumulate values re-quired to calculate the three terms in equation 8,and pass them along to calculate lower order n-gram values.
Note, however, that a naive imple-mentation of an algorithm to assign these values isO(|V |n).
This is due to the fact that the denom-inator factor must be accumulated for all higherorder states that do not have the given n-gram.Hence, for every state h directly backing off toh?
(order |V |), and for every n-gram arc leavingstate h?
(order |V |), some value must be accumu-lated.
This can be particularly clearly seen at theunigram state, which has an arc for every unigram(the size of the vocabulary): for every bigram state(also order of the vocabulary), in the naive algo-rithm we must look for every possible arc.
Sincethere are O(|V |n?2) lower order histories in themodel in the worst case, we have overall complex-ity O(|V |n).
However, we know that the numberof stored n-grams is very sparse relative to the pos-sible number of n-grams, so the typical case com-plexity is far lower.
Importantly, the denominatoris calculated by first assuming that all higher orderstates back off to the current n-gram, then subtractout the mass associated with those that are alreadyobserved at the higher order.
In such a way, weneed only perform work for higher order n-gramshw that are explicitly in the model.
This opti-mization achieves orders-of-magnitude speedups,so that models take seconds to process.Because smoothing is not necessarily con-48strained across n-gram orders, it is possible thathigher-order n-grams could be smoothed less thanlower order n-grams, so that the numerator ofequation 8 can be less than zero, which is not valid.A value less than zero means that the higher or-der n-grams will already produce the n-gram morefrequently than its smoothed expected frequency.We set a minimum value  for the numerator, andany n-gram numerator value less than  is replacedwith  (for the current study,  = 0.001).
Wefind this to be relatively infrequent, about 1% ofn-grams for most models.4.3 IterationRecall that P and ?
terms on the right-hand side ofequation 8 are given and do not change.
But thereare two other terms in the equation that change aswe update the n-gram parameters.
The ?
(h, h?
)backoff weights in the denominator ensure nor-malization at the higher order states, and changeas the n-gram parameters at the current state aremodified.
Further, the steady state probabilitieswill change as the model changes.
Hence, at eachstate, we must iterate the calculation of the denom-inator term: first adjust n-gram weights and nor-malize; then recalculate backoff weights at higherorder states and iterate.
Since this only involvesthe denominator term, each n-gram weight can beupdated by multiplying by the ratio of the old termand the new term.After the entire model has been re-estimated,the steady state probability calculation presentedin Section 4.1 is run again and model estimationhappens again.
As we shall see in the experimen-tal results, this typically converges after just a fewiterations.
At this time, we have no convergenceproofs for either of these iterative components tothe algorithm, but expect that something can besaid about this, which will be a priority in futurework.5 Experimental resultsAll results presented here are for English Broad-cast News.
We received scripts for replicating theChelba et al (2010) results from the authors, andwe report statistics on our replication of their pa-per?s results in Table 2.
The scripts are distributedin such a way that the user supplies the data fromLDC98T31 (1996 CSR HUB4 Language Modelcorpus) and the script breaks the collection intotraining and testing sets, normalizes the text, andSmoothing Perplexity n-grams (?1000)method full pruned model diffAbs.Disc.
120.4 197.1 382.3 -1.1Witten-Bell 118.7 196.1 379.3 -1.1Ristad 126.2 203.4 394.6 -1.1Katz 119.7 197.9 385.1 -1.1Kneser-Ney?
114.4 234.1 375.4 -12.7Table 2: Replication of Chelba et al (2010) using providedscript.
Using the script, the size of the unpruned model is31,091,219 ngrams, 4,041 fewer than Chelba et al (2010).?
Kneser-Ney model pruned using -prune-history-lmswitch in SRILM.trains and prunes the language models using theSRILM toolkit (Stolcke et al, 2011).
Presumablydue to minor differences in text normalization, re-sulting in very slightly fewer n-grams in all con-ditions, we achieve negligibly lower perplexities(one or two tenths of a point) in all conditions, ascan be seen when comparing with Table 1.
Allof the same trends result, thus that paper?s resultis successfully replicated here.
Note that we ranour Kneser-Ney pruning (noted with a ?
in the ta-ble), using the new -prune-history-lm switch inSRILM ?
created in response to the Chelba et al(2010) paper ?
which allows the use of anothermodel to calculate the state marginals for pruning.This fixes part of the problem ?
perplexity does notdegrade as much as the Kneser-Ney pruned modelin Table 1 ?
but, as argued earlier in this paper, thisis not the sole reason for the degradation and theperplexity remains extremely inflated.We follow Chelba et al (2010) in training andtest set definition, vocabulary size, and parame-ters for reporting perplexity.
Note that unigramsin the models are never pruned, hence all modelsassign probabilities over an identical vocabularyand perplexity is comparable across models.
Forall results reported here, we use the SRILM toolkitfor baseline model training and pruning, then con-vert from the resulting ARPA format model toan OpenFst format (Allauzen et al, 2007), asused in the OpenGrm n-gram library (Roark et al,2012).
We then apply the marginal distributionconstraints, and convert the result back to ARPAformat for perplexity evaluation with the SRILMtoolkit.
All models are subjected to full normaliza-tion sanity checks, as with typical model functionsin the OpenGrm library.Recall that our algorithm assumes that, for ev-ery n-gram in the model, all prefix and suffix n-grams are also in the model.
For pruned mod-els, the SRILM toolkit does not impose such arequirement, hence explicit arcs are added to the49Perplexity n-gramsSmoothing Pruned Pruned (?1000)Method Model +MDC ?
in WFSTAbs.Disc.
197.1 187.4 9.7 389.2Witten-Bell 196.1 185.7 10.4 385.0Ristad 203.4 190.3 13.1 395.9Katz 197.9 187.5 10.4 390.8AD,WB,KatzMixture 196.6 186.3 10.3 388.7Table 3: Perplexity reductions achieved with marginal dis-tribution constraints (MDC) on the heavily pruned modelsfrom Chelba et al (2010), and a mixture model.
WFST n-gram counts are slightly higher than ARPA format in Table 2due to adding prefix and suffix n-grams.model during conversion, with probability equal towhat they would receive in the the original model.The resulting model is equivalent, but with a smallnumber of additional arcs in the explicit repre-sentation (around 1% for the most heavily prunedmodels).Table 3 presents perplexity results for modelsthat result from applying our marginal distributionconstraints to the four heavily pruned models fromTable 2.
In all four cases, we get perplexity reduc-tions of around 10 points.
We present the num-ber of n-grams represented explicitly in the WFST,which is a slight increase from those presented inTable 2 due to the reintroduction of prefix and suf-fix n-grams.In addition to the four models reported inChelba et al (2010), we produced a mixture modelby interpolating (with equal weight) smoothed n-gram probabilities from the full (unpruned) ab-solute discounting, Witten-Bell and Katz models,which share the same set of n-grams.
After renor-malizing and pruning to approximately the samesize as the other models, we get commensurategains using this model as with the other models.Figure 3 demonstrates the importance of iterat-ing the steady state history calculation.
All of themethods achieve perplexity reductions with sub-sequent iterations.
Katz and absolute discountingachieve very little reduction in the first iteration,but catch back up in the second iteration.The other iterative part of the algorithm, dis-cussed in Section 4.3, is the denominator of equa-tion 8, which changes due to adjustments in thebackoff weights required by the revised n-gramprobabilities.
If we do not iteratively update thebackoff weights when reestimating the weights,the ?Pruned+MDC?
perplexities in Table 3 in-crease by between 0.2?0.4 points.
Hence, iterat-ing the steady state probability calculation is quiteimportant, as illustrated by Figure 3; iterating the0 1 2 3 4 5 6180185190195200205Iterations of estimation (recalculating steady state probs)PerplexityWitten?BellRistadKatzAbsolute DiscountingWB,AD,Katz mixtureFigure 3: Models resulting from different numbers of pa-rameter re-estimation iterations.
Iteration 0 is the baselinepruned model.denominator calculation much less so, at least forthese models.
We noted in Section 3 that a key dif-ference between our approach and Kneser and Ney(1995) is that their approach treated the denomina-tor as a constant.
If we do this, the ?Pruned+MDC?perplexities increase by between 4.5?5.6 points,i.e., about half of the perplexity reduction is lostfor each method.
Thus, while iteration of denomi-nator calculation may not be critical, it should notbe treated as a constant.We now look at the impacts on system perfor-mance we can achieve with these new models4,and whether the perplexity differences that we ob-serve translate to real error rate reductions.For automatic speech recognition experiments,we used as test set the 1997 Hub4 evaluation setconsisting of 32,689 words.
The acoustic modelis a tied-state triphone GMM-based HMM whoseinput features are 9-frame stacked 13-dimensionalPLP-cepstral coefficients projected down to 39 di-mensions using LDA.
The model was trained onthe 1996 and 1997 Hub4 acoustic model train-ing sets (about 150 hours of data) using semi-tiedcovariance modeling and CMLLR-based speakeradaptive training and 4 iterations of boosted MMI.We used a multi-pass decoding strategy: twoquick passes for adaptation supervision, CMLLRand MLLR estimation; then a slower full decodingpass running about 3 times slower than real time.Table 4 presents recognition results for theheavily pruned models that we have been con-sidering, both for first pass decoding and rescor-ing of the resulting lattices using failure transi-tions rather than epsilon backoff approximations.4For space purposes, we exclude the Ristad method fromthis point forward since it is not competitive with the others.50Word error rate (WER)First pass RescoringSmoothing Pruned Pruned Pruned PrunedMethod Model +MDC Model +MDCAbs.Disc.
20.5 19.7 20.2 19.6Witten-Bell 20.5 19.9 20.1 19.6Katz 20.5 19.7 20.2 19.7Mixture 20.5 19.6 20.2 19.6Kneser-Neya 22.1 22.2Kneser-Neyb 20.5 20.6Table 4: WER reductions achieved with marginal dis-tribution constraints (MDC) on the heavily pruned modelsfrom Chelba et al (2010), and a mixture model.
Kneser-Ney results are shown for: a) original pruning; and b) with-prune-history-lm switch.The perplexity reductions that were achieved forthese models do translate to real word error ratereductions at both stages of between 0.5 and 0.9percent absolute.
All of these gains are sta-tistically significant at p < 0.0001 using thestratified shuffling test (Yeh, 2000).
For prunedKneser-Ney models, fixing the state marginalswith the -prune-history-lm switch reduces theWER versus the original pruned model, but no re-ductions were achieved vs. baseline methods.Table 5 presents perplexity and WER resultsfor less heavily pruned models, where the prun-ing thresholds were set to yield approximately1.5 million n-grams (4 times more than the pre-vious models); and another set at around 5 mil-lion n-grams, as well as the full, unpruned mod-els.
While the robust gains we?ve observed up tonow persist with the 1.5M n-gram models (WERreductions significant, Witten-Bell at p < 0.02,others at p < 0.0001), the larger models yielddiminishing gains, with no real WER improve-ments.
Performance of Witten-Bell models withthe marginal distribution constraints degrade badlyfor the larger models, indicating that this methodof regularization, unmodified by aggressive prun-ing, does not provide a well suited distribution forthis sort of optimization.
We speculate that thisis due to underregularization, having noted somefloating point precision issues when allowing thebackoff recalculation to run indefinitely.6 Summary and Future DirectionsThe presented method reestimates lower ordern-gram model parameters for a given smoothedbackoff model, achieving perplexity and WER re-ductions for many smoothed models.
There re-main a number of open questions to investigatein the future.
Recall that the numerator in Eq.8 can be less than zero, meaning that no param-eterization would lead to a model with the targetfrequency of the lower order n-gram, presumablydue to over- or under-regularization.
We antic-ipate a pre-constraint on the baseline smoothingmethod, that would recognize this problem and ad-just the smoothing to ensure that a solution doesexist.
Additionally, it is clear that different reg-ularization methods yield different behaviors, no-tably that large, relatively lightly pruned Witten-Bell models yield poor results.
We will look toidentify the issues with such models and providegeneral guidelines for prepping models prior toprocessing.
Finally, we would like to perform ex-tensive controlled experimentation to examine therelative contribution of the various aspects of ourapproach.AcknowledgmentsThanks to Ciprian Chelba and colleagues for thescripts to replicate their results.
This work wassupported in part by a Google Faculty ResearchAward and NSF grant #IIS-0964102.
Any opin-ions, findings, conclusions or recommendationsexpressed in this publication are those of the au-thors and do not necessarily reflect the views ofthe NSF.M Less heavily pruned model Moderately pruned model Full modelSmoothing D ngrams WER ngrams WER ngrams WERMethod C (?106) PPL FP RS (?106) PPL FP RS (?106) PPL FP RSAbs.
N 1.53 146.6 18.1 17.9 5.19 129.1 17.0 16.6 31.1 120.4 16.2 16.1Disc.
Y 141.2 17.2 17.2 126.3 16.6 16.6 31.1 117.0 16.0 16.0Witten- N 1.54 145.8 18.1 17.6 5.08 129.4 17.3 16.8 31.1 118.7 16.3 16.1Bell Y 139.7 17.9 17.4 126.4 18.4 17.3 31.1 118.4 18.1 17.6Katz N 1.57 146.6 17.8 17.7 5.10 128.9 16.8 16.6 31.1 119.7 16.2 16.1Y 141.1 17.3 17.3 125.7 16.6 16.6 31.1 114.7 16.2 16.1Mixture N 1.55 145.5 18.1 17.7 5.11 128.2 16.9 16.6 31.1 118.5 16.3 16.1Y 139.2 17.3 17.2 123.6 16.6 16.4 31.1 114.6 17.3 16.4Kneser-Ney backoff model, unpruned: 31.1 114.4 15.8 15.9Table 5: Perplexity (PPL) and both first pass (FP) and rescoring (RS) WER reductions for less heavily pruned models usingmarginal distribution constraints (MDC).51ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the Twelfth InternationalConference on Implementation and Application ofAutomata (CIAA 2007), Lecture Notes in ComputerScience, volume 4793, pages 11?23.Ciprian Chelba, Thorsten Brants, Will Neveitt, andPeng Xu.
2010.
Study on interaction between en-tropy pruning and Kneser-Ney smoothing.
In Pro-ceedings of Interspeech, page 24222425.Stanley Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical Report, TR-10-98, HarvardUniversity.Joshua Goodman.
2001.
A bit of progress in lan-guage modeling.
Computer Speech and Language,15(4):403?434.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recogniser.
IEEE Transactions on Acoustic,Speech, and Signal Processing, 35(3):400?401.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing (ICASSP), pages181?184.Hermann Ney, Ute Essen, and Reinhard Kneser.
1994.On structuring probabilistic dependences in stochas-tic language modeling.
Computer Speech and Lan-guage, 8:1?38.Eric S. Ristad.
1995.
A natural law of succession.Technical Report, CS-TR-495-95, Princeton Univer-sity.Brian Roark, Richard Sproat, Cyril Allauzen, MichaelRiley, Jeffrey Sorensen, and Terry Tai.
2012.
TheOpenGrm open-source finite-state grammar soft-ware libraries.
In Proceedings of the ACL 2012 Sys-tem Demonstrations, pages 61?66.Kristie Seymore and Ronald Rosenfeld.
1996.
Scal-able backoff language models.
In Proceedings ofthe International Conference on Spoken LanguageProcessing (ICSLP).Vesa Siivola, Teemu Hirsimaki, and Sami Virpioja.2007.
On growing and pruning kneserney smoothedn-gram models.
IEEE Transactions on Audio,Speech, and Language Processing, 15(5):1617?1624.William J Stewart.
1999.
Numerical methods for com-puting stationary distributions of finite irreduciblemarkov chains.
Computational Probability, pages81?111.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
Srilm at sixteen: Update and out-look.
In Proceedings of the IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU).Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proc.
DARPA Broad-cast News Transcription and Understanding Work-shop, pages 270?274.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
In Pro-ceedings of ACL-08: HLT, pages 505?513.David Talbot and Miles Osborne.
2007.
SmoothedBloom filter language models: Tera-scale LMs onthe cheap.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 468?476.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
IEEETransactions on Information Theory, 37(4):1085?1094.A.
Yeh.
2000.
More accurate tests for the statisticalsignificance of result differences.
In Proceedings ofthe 18th International COLING, pages 947?953.52
