Clustering Words with the MDL PrincipleHang L i  and  Naoki AbeTheory  NEC Laboratory ,  RWCP*c /o  C&C Research  Labora.
tor ies ,  NEC4-1-1 Miya~zaki M iyamae-ku ,  Ke~wasa.ki, 216 Japan{ l ihang ,abe} (@sbl.cl.nee.co.jpAbstractWe address the probhml of automaticMlyconstructing a thesaurus by clusteringwords based on corpus data.
We viewthis problem as that of estimating a jointdistribution over the (:artesian productof a partition of a set of nouns and apartition of a set of verbs, and proposea learning a.lgorithm based on the Min-inmm Description Length (MDL) Prin-ciple for such estimation.
We empiri-cally compared the performance of ourmethod based on the MDL Principleagainst the Maximum Likelihood Esti-mator in word clustering, and found thatthe former outperforms the latter.
~?Vealso evaluated the method by conduct-ing pp-attachment disambiguation ex-periments using an automaticMly con-structed thesaurus.
Our experimentalresults indicate that such a thesaurus canbe used to improve accuracy in disam-biguation.1 I n t roduct ionRecently various methods for automatically con-structing a thesaurus (hierarchically clusteringwords) based on corpus data.
have been proposed(Hindle, 1990; Brown et al, 1992; Pereira et al,1993; Tokunaga et al, 1995).
The realizationof such an automatic onstruction method wouldmake it possible to a) save the cost of constructinga thesaurus by hand, b) do away with subjectivityinherent in a hand made thesaurus, and c) makeit easier to adapt a natural language processingsystem to a new domain.
In this paper, we pro-pose a new method for automatic onstruction ofthesauri.
Specifically, we view the problem of au-tomatically clustering words as that of estimatinga joint distributiofl over the Cartesian product ofa partition of a set of nouns (in general, any setof words) and a partition of a set of w:rbs (in gen-eral, any set of words), and propose an est.imation*Real World Computing Partershipalgorithm using simulated annealing with an en-ergy function based on the blinimum DescriptionLength (MDL) Principle.
The MDL Principle isa well-motivated and theoretically sound principlefor data compression and estimation in informa-tion theory and statistics.
As a method of sta-tisticM estimation MDL is guaranteed to be nearoptimal.We empiricMly evMuated the effectiveness ofour method.
In particular, we compared the per-formance of an MDL-based sinm\]ated anuealilagMgorithm in hierarchical word clustering against.that of one based on the Maximum LikelihoodEstimator (MLE, for short).
We found thatthe MDL-based method performs better thanthe MLE-based method.
We also evaluatedour method by conducting pp-attachment disam-biguation experiments using a thesaurus automat-ically constructed by it and found that disam-biguation results can be improved.Since some words never occur in a corpus, andthus cannot be reliably classified by a methodsolely based on corpus data, we propose to com-bine the use of an automatically constructed the-saurus and a hand made thesaurus in disambigua-tion.
We conducted some experiments in order totest the effectiveness of this strategy.
Our exper-imental results indicate that combining an auto-matically constructed thesaurus and a hand madethesaurus widens the coverage 1 of our disambigua-tion method, while maintaining high accuracy e.2 The Problem SettingA method of constructing a thesaurus based oncorpus data usually consists of the following threesteps: (i) Extract co-occurrence data (e.g.
caseframe data, adjacency data) fl'om a corpus, (ii)Starting from a single class (or each word compos-ing its own class), divide (or merge) word classes1 ~Cover~tge' refers to the proportion (in percentage)of test data for which the disambiguat.ion method canmake a decision.2'Accuracy' refers to the success rate, given thatthe disambiguation method makes a decision.based Oll the co-occurrence data using 8Ollle Sill>ilarity (distance) measure.
(The former apl)roachis called 'divisive', the latter 'agglomerative'.)
(iii)Repeat step (ii) until some stopping condition ismet, to construct a thesaurus (tree).
The methodwe propose here consists of the same three st.eps.Suppose available to us are frequency data (co-occurrence data.)
between verbs and their case slot.values extracted from a corpus (step (i)).
We thenview the problem of clustering words as that ofestimating a probabilistic model (representing a.probability distribution) tllat generates uch dataWe assume that the target model can be de-fined in the following way.
First, we define a nounpartition "PA. ~ over a given set of nouns ..'V" and averb partioll "Pv over a given set.
of verbs 12.
Anoun partit ion is any set T'-~ satisfying "P,~ C 2 H,Wc~e'&v('i = A/ and VCi, (..) E 7)A.
', Ci 0 (/j = O.A verb partition 7)v is defined analogously.
Inthis paper, we call a member of a noun partition'a, llOUll cluster', and a nlenlbe, r of a verb parti-tion a ~verb cluster'.
We refer to a member of theCartesian product of a noun partition and a verbpartition ( C "P:v x "Pv ) simply as 'a cluster'.
Wethen define a probabilistic model (a joint distribu-tion), written I ' (C, ,  (:v), where random variableC,, assumes a value fl'om a fizcd nouu partition~PX, and C~.
a va.lue from a fixed verb partition7)v. Within a given cluster, we assume thai eachelement is generated with equal probability, i.e.,P(c,,,c~,)v. ,  E c,,,v,,, E c,,, P(,,,,,,) - IC .
x <,1  (t)In this paper, we assume that the observed ataare generaied by a model belonging to the class ofmodels just de.scribed, and select a model whichbest explains the data.. As a result of this, we ob-tain both noun clusters and verb clusters.
Thisproblem setting is based on the intuit.lye assump-tion that similar words occur in the sa.me contextwith roughly equal likelihood, as is made explicitin equation (l).
Thus selecting a model which bestexplains the given data is equivalent o finding themost appropriate classification of words base(t ontheir co-occurrence.3 C luster ing  w i th  MDLWe now turn to the question of what.
strategy(or criterion) we should employ for estimatingthe best model.
Our choice is the MDL (Min-imum Description I,ength) principle (tlissanen,1989), a well-known principle of data compres-sion and statistical estimation from inforlnationtheory.
MDI, stipulates that the best probabil-ity model for given data is that model which re-quires the least cod(: length \['or encoding of themodel itself, as well as the giwql data relative toit a.
We refer to the code length for the modelaWe refer /.he interested reader to eli aml Abe,1!195) for explana.tion of ra.tionals behind using theas 'the model description h'ngth' and that for tiledata 'the data description length.
"We apply MDI, to the problem of estimatinga model consisting of a pair of partitions as de-scribed above.
In this context, a model with lessclusters tends to be simpler (in t.erms of the num-ber of parameters), but also tends to have a poorerfit.
to the data.
In contrast, a model with moreclusters is more complex, but tends to have a bet-ter fit to the data.
Thus, there is a trade-off rela-tionship between the simplicity of a model and thegoodness of fit to the data.
The model descriptionlength quantifies the simplicity (complexity) of amodel, and the data description length quantifiesthe tlt.
to the data.
According to MDL, the modelwhich minimizes the sum total of the two types ofdescription lengths should be selected.In what follows, we will describe in detail howthe description length is to be calculated in ourcurrent context, as well as our silnulated annealingalgorithm based on MI)L.3.1 Ca lcu la t ing  Descr ip t ion  LengthWe will now describe how the description lengthfor a model is calculated, lh'call that each modelis specified by the Cartesian product of a partitionof nouns and a partition of verbs, and a numberof parameters for them.
Here we let /,', denote thesize of the noun partition, and /q, the size of theverb partition.
Tiien, there are k , .
k~,-  1 freeparameters in a model.Given a model M and data k', its total de-scription length L( J / )  4 is COlnputed as the suniof the model description length L .... d('lt), the de-scription length of its parameters I;~,,,,.
(M), anddata description length Ld,~t(M).
(We often referto Lm.od(.
'l.\]) q- Lpar (:'~l) as the model descriptionlength).
Namely,L(:~'I) = L,,~o(~(:~I) + L>.,,.
(:~I) + L~, (M)  (2)We employ the %inary noun clustering method',in which k,, is fixed at IVt and we are to dechlewhether k,~ -- 1 or k,,.
= 2, which is then to beapplied recursiw~ly to the clusters thus obtained.This is as if we view the noutls as entities a.nd theverbs as features and cluster the entities based ontheir feat.ures.
Since there are 2Pv'I subsets of theset of llottns .~, and for each 'binary'  noun parti-tion we have two different subsets (a special caseof which is when one subset is A 'r and the other theempty set 0), the number of possible binary nomlpartitions is 2tAq/2 = 21~'l-J.
Thus for each I)i-nary noun partition we need log 21a"l-t = i3j- I _ 1bit.s 5 to describe it.
6 Ilenee L ..... a(M) is calculatedMI)L principle in natural anguage processing.~L(M) depends on .
';, but we will leave ,5' implicit.5Throughout the paper 'log' denotes the logarit.hntto the base 2.6 For further explanation, see (Quinlan and Rivest,1989).as  7L,,~o<+~,s) = I~r l -  1 (3)Lpar(k~/), often referred to as the parallleter de-scription length, is calculated by,L,,~,.
(M) = 2 .
log I,~'t (4)where ISl denotes the input data size, and/?,.
\]c,,-1 is the nnnlber of (free) parauleters ill tlle nlodel.It is known that using log ~ = ~ bits to de-scribe each of the parameters will (approximately)minimize the description length (1Rissanen, 1.989).FinMly, Ld,t(M) is calculated byLdat(M)=- E f (n ,v ) .
logP(n ,v )  (5)(n,v)ESwhere f(n,,v) denotes the observed frequency ofthe noun verb pair (n,v), and P(n,v) the esti-mated probability of (n, v), which is calculated asfollows.v,,.
c c,,,w, c Cv P( ,~, , ,~ , )  - f'((::,,,c'~,) (s)' IC,, x c,.IP(C,,, C,, ) - f(C,,, C,, ) (r)Is1where f(C',~, C,,) denotes the obserw.d frequencyof the noun verb pairs belonging to cluster(c,~, <;'~ ).With tile description length of a model de-fined in the above manner, we wish to select amodel having the minimum description length andoutput it as the result of clustering.
Since themodel description length Lmod is the same for eachmodel, in practice we only need to calculate andcompare L'(M) = L,,<,,.
(M) + \];d<~,(M).3.2 A Sinl l l lated Annealing-basedAlgor i thmWe could ill principle calculate the descriptionlength for each model and select, a model withthe nfininmm description length, if COlnputationtime were of no concern.
However, since the num-ber of probal)ilistic models under consideration issuper exponential, this is not feasible in practice.We employ the 'simulated a.m~ealing technique' todeal with this problem.
Figure 1 shows our (divi-sive) clustering algorithm s .4 Advantages of Our MethodIn this section, we elaborate on the merits of ourmethod.In.
statistical natural language processing, usu-ally the number of parameters in a probabilistic7The exact formulation of L,~od(M) is subjective,and it depends on the exact coding scheme used forthe description of the models.SAs we noted earlier, an Mternative would be toemploy an agglomerative Mgorithm.model to be estimated is very large, and thereforesuch a model is difficult to estimate with a reason-able data size that is available in practice.
(Thisproblem is usually referred to as the 'data sparse-ness problem'.)
We could smooth the estimatedprobabilities using an existing smoothing tech-nique (e.g., (Dagan el, al., 1992; Gale and Church,1990)), then calculate some similarity measure us-ing the smoothed probabilities, and then clusterwords according to it.
There is no guarantee,however, that the employed smoothing method isin any way consistent with the clustering methodused subsequently.
Our method based on MDL re-solves this issue in a unified fashion.
By employingmodels that embody the assumption that wordsbelonging to a same class occur in the same con-text with equal likelihood, our method achievesthe smoothing effect as a side effect of the clus-tering process, where the domains of smoothingcoincide with the classes obtained by clustering.Thus, the coarseness or fineness of clustering alsodetermines the degree of smoothing.
All of theseeffects fall out naturally as a corollary of the im-peratiw?
of 'best possible estimation', the originalmotivation behind the MDL principle.in our simulated annealing algorithm, we couldalternatively employ the Maxinmm Likelihood Es-timator (MLE) as criterion for the best prob-abilistic model, instead of MDL.
MLE, as itsname suggests, selects a model which maxi-mizes the likelihood of the data, that is, /5 =a.rg maxp I-\[~?s P(x).
This is equivalent o min-infizing the 'data description length' as definedin Section 3, i.e.
i 5 = arg minp ~,~-~s - log P(x).We can see easily that MDL genet:al\[zes MLE, inthat it also takes into account the complexity ofthe model itself.
In the presence of models withvarying complexity, MLE tends to overfit the data,and output; a model that is too complex and tai-lored to fit the specifics of the input data.
If weemploy MLE as criterion in our simulated anneal-ing algorithm, it.
will result in selecting a very finemodel with many small clusters, most of whichwill have probabilities estimated as zero.
Thus, incontrast o employing MDL, it will not have theeffect of smoothing a.t all.Purely as a method of estimation as well, thesuperiority of MI)L over MLE is supported byconvincing theoretical findings (c.f.
(Barton andCover, 1991; Yamanishi, 1992)).
For instance, thespeed of convergence of the models selected byMDL to the true model is known to be near op-tiinal.
(The models selected by MDL converge tothe true model approximately at the rate of 1/swhere s is the nmnber of parameters in the truemodel, whereas for MLE the rate is l / t ,  where t isthe size of the domain, or in our context, the totalnumber of elements of N" x V.) 'Consistency' isanother desirable property of MDL, which is notshared by MLE.
That is, the number of parame-Algor i thm:  C lus ter ing1.
Divide the noun set N into two subs0ts.
I)efine a probabilistic model consisting of the l)artitionof nouns si)ecified by the two sul)sets and th(" entire set.
of verbs.2.
do{2.1 Randomly select, one noun, rcmow> it from t.h~; subset it.
belongs to and add it.
to the other.2.2 C.alcuh~tc the description length for the two models (before and after the mow~') as L1 andLe, respectively.2.3 Viewing the description length as the energy flmction for annealing, let AL  = Le - L:.If AL  < 0, fix the mow~, otherwise ascertain the mowe with probability P = eXl ) ( -AL /T) .}
whi le  (the description length has decreased uring the past 10.
INI trials.
)Itere T is the a.nnealing t.enq.
)crat.urc whose initial value, is 1 and updated to be 0.97' after10.
\]NI trials.3.
If one of the obtained subset is elul)t,y, t\]ll?ll return the I lOl l -Ol l lpty subset, otherwise recursiw,lyapply C lus ter ing  on both of the two subsets.Figure 1: Simulated annealing algorithm for word clusteringters in l;he models selected by MDI~ ('otivorg~" tothat of the true model (Rissanen, 1989).
Both ofthese prol>erties of MI)I, ar~ Oml>irically w'ri/ied inour present (;Ollt(?x\[,, as  will be show,: in t.ho t:(,xlsection.
In particular, we haw~ compared l,h(' p(u'-forn:a.nc0 of employing an M1)L-based simula.tedannealing against that of one 1)ascd on M\[,I", illhierarchical woM clust.c'ring.5 Exper imenta l  Resu l ts- - i t .
he con:party they we ithe t:rue model and the estimated model.
('l'hc al-gorithm used for MI,E was lhe same as that showJtin Figure 1, except the 'data description length'replaces the (total) description length' in Sl.ep 2.
)Figure 3(a) plots the number of obtained IIOlllIclusters (leaf nodes in the obtained thesaurus trc~,)w?rsus the input data size, aw;raged ow;r 10 trials.
(The number of noun clusters in the true modelis 4.)
Figure 3(b) plots the KI, distance versusthe data size, also averaged over l:he san> 10 tri-als.
The results indicalc that MI)L conw,rges tothe true Inode\] fasl.er i.\]ian M I,E.
Also, MI,I'; tendsto select a mo(h'l overfittil:g the data, while Ml)l,t.cnds to seh>ct a. model which is simple and yettits the data reasonably well.- -  salel K ~  stock sha,'~'t billion millionl,'iguro 2: An example thesaurusWe desert bc our experimental rcsull s ill th is sec-tion.5.1 Exper iment 1: MDL v.s.
MLEWe COml)ared the performance of elnploying M1)\],as a criterion in our silnulatcd annealing algo-rithm, against that of employing M IA~; by sim-ulation experiments.
We artificially constructeda true model of word co-occurrence, and thengenerated ata according to its distributiou.
Wethen used the data.
to estimale a model (clusteringwords), and measured the I(L distancd ~between?
'l'he K\], distance (relative Clt|,l:Opy), which iswidely used in information theory and sta, tist, ics, isa, n leasur,2 of 'dista,  n<:c' l>~\[,wcen two  distributions5.2 Experiment 2: Qualitative EvaluationWe extracted roughly 180,000 case fl:anles fromthe bracketed WSJ (Wall Street Journal) corpusof the Penn Tree Bank (Marcus et al, 1993) asco-occurrence data.
We then eonstrucl.ed a num-ber of thesauri based on these data, using ourmethod.
Figure 2 shows all example thesaurusfor the 20 most frequently occurred nouns in thedata, constructed based on their appearances assubject and object of roughly 2000 verbs.
Theobtained thesaurus seems to agree with humanintuition to set t le  degr(~e.
For example, 'million'and 'billion' are classilied in one IIOll\[I ch l s te r ,  alld'stock' and 'share' arc classified together.
Not allof tile IlOUII C\]ltsters, however, seem to be mean-ingful in the useflll sense.
This is probably be-cause the.
data size we had was not large enough.Pragmatical ly speaking, however, whethcl: the ob-tained thesaurus agrees with our intuition in itselfis only of secondary concern, since the main lmr  -pose is to use the constructed t.hcsaurus to helpi~uprow~ on a disaml)igual.ion I,ask.
(('.over and Tl,omas, 1991).
\]t is Mways non-negativea.nd is zero iff the two distributions arc identical.7"MDL""MLE" -~ ' ",t t' "'"',..,+.._ __ ...__ _.~ .......,~'"-'~",, ,,,.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
I , .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~0 100 1000 10000 100000I .
.
.
.
.
.
, .
.
.
.
.
.
.
.
, .
.
.
.
, .
.
.
.
.
.
.
,',, "MOL"',., ,, "MLE" -~-.o.e0.60.4?i I I~  100000Figure 3: (a) Number of clusters versus data size and (b) KL distance wersus data size5.3 Experiment 3: DisambiguationWe also evaluated our method by using a con-structed thesaurus in a pp-attachment disan>bigua.tion experiment.We used as training data the same 180,000 casefl'ames in Experiment 1.
We also extracted asour test data 172 (verb, no~nll,prep,'noune) pat-terns Dora the data in the same corpus, which isnot used in the training data.
For the 150 wordsthat appear in the position of ,oun.e in the testdata, we constructed a thesaurus based on theco-occurrences between heads and slot.
values ofthe fl'ames in the training data.
This is becausein our disambiguation test we only need a. the-saurus consisting of these 150 words.
We thenapplied the learning method proposed in (Li andAbe, 1995) to learn case fl'ame patterns with theconstructed thesaurus as input using the sametraining data.
That is, we used it to learn theconditional distributions P( Classlll,erb, prep),P(Classe \[n, ounl, prep), where Class1 and Classevary over the internal nodes in a certain 'cut'in the thesaurus tree l0 We then compareTable 1: PP-attachment disaml)iguation resultsBase LineWord-BasedMI) L-ThesaurusMLE-ThesaurusWordNetCowerage(%,) Accuracy(%)100 70.:219.7 95.133.1 93.033.7 89.749.4 88.2which are estimated based on the case fl'amepatterns, to determine the a.ttachment site of(prep, not*he).
More specifically, if the former islarger than the latter, we attach it.
to verb, and ifthe latter is larger tha.n the former, we attach it.to n.o'unl, and otherwise (including when both are1?Each 'cut.'
in a t.hesa.urus tree defines a differentnoun paxt.ition.
See (Li and Abe, 1995) for details.0), we conclude that we cannot make a decision.Table 1 shows the results of our pp-attachmentdisambiguation experiment in terms of 'coverage'and 'accuracy.'
tlere 'coverage' refers to the pro-portion (in percentage) of the test patterns onwhich the disambiguation method could make adecision.
'Base Line' refers to tile method of al-ways ~ttaching (prep, noun.~.)
to noun1.
'Word-Based', 'MLE-Thesaurus', and 'MDL-Thesaurus'respectively stand tbr using word-based estimates,using a thesaurus constructed by employing MLE,and using a thesaurus constructed by our method.Note that the coverage of ~MDL-Thesaurus' signif-iea.ntly outperformed that of 'Word-Based', whilebasically maintaining high accuracy (though itdrops somewhat), indicating that using an auto-matically constructed thesaurus can improve dis-ambiguation results in terms of coverage.We also tested the method proposed in (Li andAbe, 1995) of learning case frames patterns usingall existing thesaurus.
In particular, we used thismethod with WordNet (Miller et al, 1993) andusing the same training data., and then conductedpp-attachment disambiguation experiment usingthe obtained case frame patterns.
We show theresult of this experiment as 'WordNet'  in Table 1.We can see that in terms of 'coverage', ~WordNet'outperforms 'MDL-Thesaurus', but in terms of"accuracy', MDL-Thesaurus' outperforms 'Word-Net.'.
These results can be interpreted as follows.An automa.tically constructed thesaurus is moredomaiu dependent and captures the domain de-pendent features better, and thus using it achieveshigh accuracy.
On the other hand, since trainingdata.
we had available is insufficient, its coverageis smaller than that of a hand made thesaurus.In practice, it makes sense to combine both typesof thesauri.
More specifically, an atttomaticallyconstructed thesaurus can be used within its cov-erage, and outside its coverage, a hand made the-saurus can be used.
Given the current state ofthe word clustering technique (namely, it requiresdata size that is usually not available, and it tendsto be computationally demanding), this strategyis practical.
We show the result of this combinedT~bte 2: I)l '-attachinent, disambiguation resultsM1)l,-'I'h,~saurus + Word NetMI)L-Thesaltrus + \VordNct: + I,A + I)efaull;Coverage(%) Accuracy(~)54,1 8 7. l100 85.5method a.s 'M l ) l / l 'hesaur l ts  + WordNot.'
it/ Ta-Me 2.
Our exlmritnenl,al resnlt, shows lltal: eln-ph)yhag t;he cotn\]>ined nlet.hod does itwrease t.hecow:rage of disainbiguation.
We also tested +M1)I,Thesaurus + WordNel.-t- I,A -t- l)('fatllt.
', whichsl:ands for using l.hc' learm~d thesaurus altd \Vord-Net first+, t.heu t.he lexical associal.iotl valtm l>rO -posed by (lIindle a.nd F/.ooth, 1991), and finallytile defa.ull; (i.e.
always atl.aching \])/'el), *~ottl+2l;o no+tn~).
Our hest disaml)iguatioll rcsull, ob-tained using t, his last; combined niet.tiod sontewhatimproves t, he accuracy rei>orl.ed itt (Li and At><',1.995) (84.3%).6 Conchtd ing  RemarksWe have proposed a tnethod of" hierarchical <'his-feting of words hased on laxge corpus data.
\Voconclude wit, h the following remarl,:s,\[.
()lip ll/et ho(\[ (>\[" chtst:('ritlg w(wds has(,d cqt th('MI)L t',ritlciph" is ~h<,reficalty sc.+,rtd.
Ourexperimental  t'esult.s how t.hal: il.
is I+ot .er toenq)loy MI)I, than M 1,1!
; as estimation <'riW-rion in hierarchical word chtstering.2.
\[lsing a tlwsaltrus consl.rt~cl,cd l>y ottr met hodcan inq>rov(; pp-;fl.t, ach)nent, disaml)igtmtionresults.3.
At, t.he Clm:ent, st, a, te of the art.
itt st.al, isticalna.t;ttral languag(~ I)rocessing, it.
is b('st.
I.o use acotnbination of an a.ut.ontat.ically const.rucl(>dthesa.urus and a hand made l;hesattrus \['or dis-atnbigua.l.ion purpose.
"Fhe disaulhiglmtionaccttra.cy obtained this way wets 85+5(/c,.\[u the fut.ttre, hopefillly wit, h target training dat.asize, we plao_ to construct, larger thesauri as wellas to test other clustering algorit.hms.Re ferencesAndrew 1~.
Barren and Thomas M. (:ow;r.t991.
Min imum comph'xit.y densil.y estima-tion.
l.l';YE 7'ra,saclion.
o, lnformatio~ The-or:q, 37(4):1034 t054.Peter F. Browu, Vincent 3.
I)ella Piet.ra, Pe-t, er V. deSouza, Jenifer (',.
I, ai, and l(ohert L.Mercer.
t992+ (:lass-hased ~t-gratn models ofnatura.l language.
Computational Li,.quistics,18(4):283 298.Tholicta.s M. (:over and .loy A. Thonias.
1991.
EI-emen, ls of \[nformalion.
7'heor!l.
,lohu Wiley &Sons Inc.\[do I)agan, Shaul Marcus, and Shaul Makovit, ch.\[9!/2.
(:ontextua.1 word similarit, y a.nd estitna.-tion fi:om sparse data.
Proceedings oflhc ,701hA t/L, pages 1{:;,'1 tVl.\ViHiams A.
(;ale and Kenth W. (:hutch.
1990.Poor esl, itnales of conl;cxt are worse {.\[tan ltOlte.l>,oceedings of Ihc I)A I~PA Speech and Nalu~'alLa,:luage Workshop, pages 283 287.Donald llindle and Mal;s 1-1ooth.
1991.
St, ructuralambiguity and lexicd relations.
Proceedi,.
:lS ofthe 291h A CL, pages 229 236.Donald tIindle.
1990.
Noun classification frontpredicat, e-argument st, ructures.
Proceedings ofthe 281h ACL, \[>ages 268 275.\[\[aug \[,i aud Naoki Abe.
1995.
Getwralizing caseframes using a. thesaurus and the MDL princi-ple.
Proceedings of Rrce,t A d~,a~ccs in Nal~trolLangua:lc Proces.sing, pages 239 2,18.Mitchell P. Marcus, Beatrice Sant.oriui, andMary Ann Marcinkiewicz.
1993.
Bttildhiga.
large annotated corpus of english: Thepeu.n t, reebank.
Computational Linguistics,19(1):313 330.
(;eorgc A. Milh'r, I~.ichar<l Beckwilh, (!hirst.ian<~I:ellbaunL Derek ClrOSS, and Kat,herine Miller.1!)!)3.
Introducl;ion to WordNet: An on-lira, le?ical database.
.,tT~o~ymous I"7'P: clar-ily.l>rt, c~lo~:, cdu.l:ernando Pereira, Naft.ali Tishhy, and LiIlia.n l,ee.\[993. l)ist, ributional clustering of maglish words.t)rocccdings of lke .
'7tsl A (TL, pages 183 190.a.
lT.oss Quinlan aitd I?onahl 1,.
Rives/.. 1989.
In-ferring decision trees using t.he mininiutn de-scription \[engt, h principle, lnformalion andC'omputation, 80:227- 248., lorma lTissanen.
1989. ,qlor'haslic Uomple<+'it 9 in5'talislical \[nquiey.
Worhl Scientific Publishing( io.Takenobu 'Fokunaga, Makot:o hva.yama, andltozttmi Tanaka.
1995.
Aut, omat.ic thesauruscot~struct, ion based-on grannnat, ica/ relations.Proceedings of 1.1CA \['95.Kenji Yatnanishi.
1992.
A learning criterion \['orstochast, ic rules.
Machine Lcarnin.fl , 9:165 203.
