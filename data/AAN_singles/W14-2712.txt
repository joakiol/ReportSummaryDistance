Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88?93,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsTowards Tracking Political Sentiment through Microblog DataYu WangEmory Universityyu.wang@emory.eduTom ClarkEmory Universitytclark7@emory.eduEugene AgichteinEmory Universityeugene@mathcs.emory.eduJeffrey StatonEmory Universityjkstato@emory.eduAbstractPeople express and amplify political opin-ions in Microblogs such as Twitter, espe-cially when major political decisions aremade.
Twitter provides a useful vehicle forcapturing and tracking popular opinion onburning issues of the day.
In this paper,we focus on tracking the changes in polit-ical sentiment related to the U.S. SupremeCourt (SCOTUS) and its decisions, fo-cusing on the key dimensions on support,emotional intensity, and polarity.
Mea-suring changes in these sentiment dimen-sions could be useful for social and politi-cal scientists, policy makers, and the pub-lic.
This preliminary work adapts existingsentiment analysis techniques to these newdimensions and the specifics of the cor-pus (Twitter).
We illustrate the promiseof our work with an important case studyof tracking sentiment change building upto, and immediately following one recentlandmark Supreme Court decision.
Thisexample illustrates how our work couldhelp answer fundamental research ques-tions in political science about the natureof Supreme Court power and its capacityto influence public discourse.1 Background and MotivationPolitical opinions are a popular topic in Mi-croblogs.
On June 26th, 2013, when the U.S.Supreme Court announced the decision on the un-constitutionality of the ?Defense of Marriage Act?
(DOMA), there were millions of Tweets about theusers?
opinions of the decision.
In their Tweets,people not only voice their opinions about the is-sues at stake, expressing different dimensions ofsentiment, such as support or opposition to the de-cision, or anger or happiness.
Thus, simply ap-plying traditional sentiment analysis scales suchas ?positive?
vs. ?negative?
classification wouldnot be sufficient to understand the public reactionto political decisions.Research on mass opinion and the SupremeCourt is valuable as it could shed light on the fun-damental and related normative concerns about therole of constitutional review in American gover-nance, which emerge in a political system possess-ing democratic institutions at cross-purposes.
Oneline of thought, beginning with Dahl (Dahl, 1957),suggests that the Supreme Court of the UnitedStates has a unique capacity among major institu-tions of American government to leverage its legit-imacy in order to change mass opinion regardingsalient policies.
If the Dahl?s hypothesis is correct,then the Supreme Court?s same-sex marriage deci-sions should have resulted in a measurable changein opinion.
A primary finding about implication ofDahl?s hypothesis is that the Court is polarizing,creating more supportive opinions of the policiesit reviews among those who supported the pol-icy before the decision and more negative opin-ions among those who opposed the policy prior tothe decision (Franklin and Kosaki, 1989) (Johnsonand Martin, 1998).We consider Twitter as important example ofsocial expression of opinion.
Recent studies ofcontent on Twitter have revealed that 85% of Twit-ter content is related to spreading and commentingon headline news (Kwak et al., 2010); when userstalk about commercial brands in their Tweets,about 20% of them have personal sentiment in-volved (Jansen et al., 2009).
These statistical evi-dences imply that Twitter has became a portal forpublic to express opinions.
In the context of pol-itics, Twitter content, together with Twitter users?88information, such as user?s profile and social net-work, have shown reasonable power of detectinguser?s political leaning (Conover et al., 2011) andpredicting elections (Tumasjan et al., 2010).
Al-though promising, the effectiveness of using Twit-ter content to measure public political opinions re-mains unclear.
Several studies show limited corre-lation between sentiment on Twitter and politicalpolls in elections (Mejova et al., 2013) (O?Connoret al., 2010).
Our study mainly focuses on inves-tigating sentiment on Twitter about U.S. SupremeCourt decisions.We propose more fine-grained dimensions forpolitical sentiment analysis, such as supportive-ness, emotional intensity and polarity, allowingpolitical science researchers, policy makers, andthe public to better comprehend the public reactionto major political issues of the day.
As we describebelow, these different dimensions of discourse onTwitter allows examination of the multiple ways inwhich discourse changes when the Supreme Courtmakes a decision on a given issue of public policy.Our dimensions also open the door to new avenuesof theorizing about the nature of public discourseon policy debates.Although general sentiment analysis has madesignificant advances over the last decade (Pang etal., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wil-son et al., 2009), and with the focus on certainaspects, such as intensity (Wilson et al., 2004),irony detection (Carvalho et al., 2009) and sar-casm detection (Davidov et al., 2010), analyzingMicroblog content such as Twitter remains a chal-lenging research topic (Reyes et al., 2012) (Vaninet al., 2013) (Agarwal et al., 2011).
Unlike previ-ous work, we introduce and focus on sentiment di-mensions particularly important for political anal-ysis of Microblog text, and extend and adapt clas-sification techniques accordingly.
To make thedata and sentiment analysis results accessible forresearchers in other domain, we build a website tovisualize the sentiment dynamics over time and letusers download the data.
Users could also definetheir own topics of interest and perform deeperanalysis with keyword filtering and geolocationfiltering.We present a case study in which our resultsmight be used to answer core questions in polit-ical science about the nature of Supreme Courtinfluence on public opinion.
Political scientistshave long been concerned with whether and howSupreme Court decisions affect public opinion anddiscourse about political topics (Hoekstra, 2003)(Johnson and Martin, 1998) (Gibson et al., 2003).Survey research on the subject has been limited intwo ways.
Survey analysis, including panel de-signs, rely on estimates near but never on the dateof particular decisions.
In addition, all survey-based research relies on estimates derived from aninstrument designed to elicit sentiment ?
surveyresponses, useful as they are, do not reflect wellhow public opinion is naturally expressed.
Ouranalysis allows for the examination of public opin-ion as it is naturally expressed and in a way that isprecisely connected to the timing of decisions.Next, we state the problem more formally, andoutline our approach and implementation.2 Problem Statement and Approach2.1 Political Sentiment ClassificationWe propose three refinements to sentiment analy-sis to quantify political opinions.
Specifically, wepose the following dimensions as particularly im-portant for politics:?
Support: Whether a Tweet is Opposed, Neu-tral, or Supportive regarding the topic.?
Emotional Intensity: Whether a Tweet isemotionally Intense or Dispassionate.?
Sentiment Polarity: Whether a Tweet?s toneis Angry, Neutral, or Pleased.2.2 ApproachIn this work, each of the proposed measures istreated as a supervised classification problem.
Weuse multi-class classification algorithms to modelSupport and Sentiment Polarity, and binary classi-fication for Emotional Intensity and Sarcasm.
Sec-tion 3.2 describes the labels used to train the super-vised classification models.
Notice some classesare more interesting than the others.
For exam-ple, the trends or ratio of opposed vs. supportiveMicroblogs are more informative than the factualones.
Particularly, we pay more attention to theclasses of opposed, supportive, intense, angry, andpleased.2.3 Classifier Feature GroupsTo classify the Microblog message into the classesof interest, we develop 6 groups of features:Popularity: Number of times the message has been89posted or favored by users.
As for a Tweet, thisfeature means number of Retweets and favorites.Capitalization and Punctuation.N-gram of text: Unigram, bigram, and trigram ofthe message text.Sentiment score: The maximum, minimum, aver-age and sum of sentiment score of terms and eachPart-of-Speech tags in the message text.Counter factuality and temporal compression dic-tionary: This feature counts the number of timessuch words appear in the message text.Political dictionary: Number of times a political-related word appears in the message text.We compute sentiment scores based on Senti-WordNet1, a sentiment dictionary constructed onWordNet.2Political dictionary is built uponpolitical-related words in WordNet.
As in this pa-per, we construct a political dictionary with 56words and phrases, such as ?liberal?, ?conserva-tive?, and ?freedom?
etc.3 Case Study: DOMAOur goal is to build and test classifiers that can dis-tinguish political content between classes of inter-est.
Particularly, we focus on classifying Tweetsrelated to one of the most popular political topics,?Defence of Marriage Act?
or DOMA, as the tar-get.
The techniques can be easily generalized toother political issues in Twitter.3.1 DatasetIn order to obtain relevant Tweets, we use Twit-ter streaming API to track representative key-words which include ?DOMA?, ?gay marriage?,?Prop8?, etc.
We track all matched Tweets gen-erated from June 16th to June 29th, immedi-ately prior and subsequent to the DOMA decision,which results in more than 40 thousand Tweets perday on average.3.2 Human JudgmentsWith more than 0.5 million potential DOMA rele-vant Tweets collected, we randomly sampled 100Tweets per day from June 16th to June 29th, and1,400 Tweets were selected in total.
Three re-search assistants were trained and they showedhigh agreement on assigning labels of relevance,support, emotional intensity, and sentiment polar-ity after training.
Each Tweet in our samples was1http://sentiwordnet.isti.cnr.it/2http://wordnet.princeton.edu/labeled by all three annotators.
After the label-ing, we first removed ?irrelevant?
Tweets (if theTweet was assigned ?irrelevant?
label by at leastone annotator), and then the tweets with no majoragreement among annotators on any of the senti-ment dimensions were removed.
As a result, 1,151tweets with what we consider to be reliable labelsremained in our dataset (which we expect to sharewith the research community).3.2.1 Annotator AgreementThe Fleiss?
Kappa agreement for each scale is re-ported in Table 1 and shows that labelers have analmost perfect agreement on relevance.
Support,emotional intensity, and sentiment polarity, showeither moderate or almost perfect agreement.Measure Fleiss?
KappaRelevance 0.93Support 0.84Intensity 0.54Polarity 0.49Table 1: Agreement (Fleiss?
Kappa) of Human Labels.3.3 Classification Performance ResultsWe reproduce the same feature types as previouswork and develop the political dictionary featurefor this particular task.
We experimented with avariety of automated classification algorithms, andfor this preliminary experiment report the perfor-mance of Na?
?ve Bayes algorithm (simple, fast, andshown to be surprisingly robust to classificationtasks with sparse and noisy training data).
10-foldcross validation are performed to test the general-izability of the classifiers.
Table 2 reports the aver-age precision, recall and accuracy for all measures.Sarcasm is challenging to detect in part due to thelack of positive instances.
One goal in this studyis to build a model that captures trends among thedifferent classes.
In Section 3.4, we will show thatthe trends of different measures estimated by thetrained classifier align with the human annotatedones over time.3.4 Visualizing Sentiment Before and AfterDOMAOne natural application of the automated politi-cal sentiment analysis proposed in this paper istracking public sentiment around landmark U.S.Supreme Court decisions.
To provide a more re-liable estimate, we apply our trained classifier onall relevant Tweets in our collection.
More than90Value Prec.
(%) Rec.
(%) Accuracy(%)Supportive (48%) 73 74Neutral (45%) 76 67 68Opposed (7%) 17 30Intense (31%) 56 6073Dispassionate (69%) 81 79Pleased (10%) 48 31Neutral (79%) 84 78 69Angry (11%) 24 45Table 2: Performance of Classifiers on Each Class.2.5 million Tweets are estimated in four proposedmeasures.
Figure 1 shows the distribution of on-topic Tweet count over time.
The Supreme Courtdecision triggered a huge wave of Tweets, and thevolume went down quickly since then.0100,000200,000300,000400,00016-Jun 19-Jun 22-Jun 25-Jun 28-JunNumber of TweetsDateFigure 1: Number of ?Gay Marriage?
Tweets Over Time.Figures 2 and 3 visualize both the human la-beled trends and the ones obtained by the classi-fier for the classes ?Supportive?
and ?Intense?.
Inboth figures, the peaks in the predicted labels gen-erally align with the human-judged ones.
We cansee the supportiveness and intensity are both rela-tively high before the decision, and then they de-cline gradually after the Supreme Court decision.Figure 3 shows the volume of intensive Tweetsdetected by our trained model has a burst on June22rd, which is not captured by human labeleddata.
To investigate this, we manually checked allTweets estimated as ?intensive?
on June 22rd.
Itturns out most of the Tweets are indeed intensive.The reason of the burst is that one Tweet was heav-ily retweeted on that day.
We do not disclose theactual tweet due to its offensive content.00.10.20.30.40.50.60.70.80.916-Jun 19-Jun 22-Jun 25-Jun 28-JunHuman LabeledEstimatedFigure 2: Percentage of ?Supportive?
Tweets Over Time.Figure 4 plots the trends of ?supportive?
and00.10.20.30.40.50.60.716-Jun 19-Jun 22-Jun 25-Jun 28-JunHuman LabeledEstimatedFigure 3: Percentage of ?Intense?
Tweets Over Time.00.0020.0040.0060.0080.010.0120.0140.01600.10.20.30.40.50.60.70.80.916-Jun 19-Jun 22-Jun 25-Jun 28-Jun% ofOpposed Tweets% of SupportiveTweetsSupportive OpposedFigure 4: Comparison between ?Supportive?
and ?Op-posed?
Trends.?opposed?
Tweets in different scales.
Accordingto the Supreme Court decision, the ?supportive?group wins the debate.
Interestingly, instead ofresponding immediately, the ?loser?
group reactand start Tweeting 2 days after the decision.
Thesetrends indicate that ?winner?
and ?loser?
in the de-bate react differently in time and intensity dimen-sions.We believe that our estimates of sentiment canbe used in various ways by political scientists.The ?positivity bias?
(Gibson and Caldeira, 2009)model of Supreme Court opinion suggests thatthe Court can move public opinion in the direc-tion of its decisions.
Our results possibly indicatethe opposite, the ?polarizing?
model suggested by(Franklin and Kosaki, 1989) and (Johnson andMartin, 1998), where more negative opinions areobserved after the decision (in Figure 4), at leastfor a short period.
By learning and visualize polit-ical sentiments, we could crystalize the nature ofthe decision that influences the degree to which theSupreme Court can move opinion in the directionof its decisions.4 An Open Platform for Sharing andAnalyzing Political SentimentsFigure 5 shows a website3that visualizes politi-cal sentiments over time.
The website shows sev-eral popular U.S. Supreme Court cases, such as?gay marriage?, ?voting right act?, ?tax cases?,3http://www.courtometer.com91etc., and general topics, such as ?Supreme Court?and ?Justices?.
Each of the topics is representedby a list of keywords developed by political sci-ence experts.
The keywords are also used to trackrelevant Tweets through Twitter streaming API.
Tolet users go deeper in analyzing public opinions,the website provides two types of real-time filter-ing: keywords and location of Tweet authors.
Af-ter applying filters, a subset of matched Tweets aregenerated as subtopics and their sentiments are vi-sualized.
The example filtering in Figure 5 showsthe process of creating subtopic ?voting right act?out of a general topic ?Supreme Court?
by usingkeyword ?VRA?.
We can see that the volume ofnegative Tweets of ?voting right act?
is higher thanthe positive ones, compared to the overall senti-ment of the general Supreme Court topic.
Once aninteresting subtopic is found, users can downloadthe corresponding data and share with other users.Topic ?Supreme Court?NumberofTweetsNumberofTweetsSubtopic ?Voting Right Act?Filtered by keyword: ?VRA?Figure 5: We build a website that visualizes political sen-timents over time and let users create ?subtopics?
by usingkeyword and location filters.5 ConclusionsIn this paper we considered the problem of polit-ical sentiment analysis.
We refined the notion ofsentiment, as applicable to the political domain,and explored the features needed to perform auto-mated classification to these dimensions, on a realcorpus of tweets about one U.S. Supreme Courtcase.
We showed that our existing classifier canalready be useful for exploratory political analy-sis, by comparing the predicted sentiment trends tothose derived from manual human judgments, andthen applying the classifier on a large sample oftweets ?
with the results providing additional ev-idence for an important model of Supreme Courtopinion formation from political science.This work provides an important step towardsrobust sentiment analysis in the political domain,and the data collected in our study is expected toserve as a stepping stone for subsequent explo-ration.
In the future, we plan to refine and im-prove the classification performance by exploringadditional features, in particular in the latent topicspace, and experimenting with other political sci-ence topics.ACKNOWLEDGMENTS The work of Yu Wangand Eugene Agichtein was supported in part byDARPA grants N11AP20012 and D11AP00269,and by the Google Research Award.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
SentimentAnalysis of Twitter Data.
In Proceedings of theWorkshop on Language in Social Media (LSM).Paula Carvalho, Lu?
?s Sarmento, M?ario J. Silva, andEug?enio de Oliveira.
2009.
Clues for detectingirony in user-generated contents: oh...!!
it?s ?soeasy?
;-).
In Proceedings of the 1st internationalCIKM workshop on Topic-sentiment analysis formass opinion.M.D.
Conover, B. Goncalves, J. Ratkiewicz, A. Flam-mini, and F. Menczer.
2011.
Predicting the PoliticalAlignment of Twitter Users In Proceedings of IEEEthird international conference on social computingRobert Dahl.
1957.
Decision-Making in a Democracy:The Supreme Court as National Policy-Maker.
Jour-nal of Public Law.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised Recognition of Sarcastic Sentencesin Twitter and Amazon.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning (CoNLL).Charles H. Franklin, and Liane C. Kosaki.
1989.
Re-publican Schoolmaster: The U.S. Supreme Court,Public Opinion, and Abortion.
The American Po-litical Science Review.James L Gibson, and Gregory A Caldeira.
2009.
Cit-izens, courts, and confirmations: Positivity theoryand the judgments of the American people.
Prince-ton University Press.James L Gibson, Gregory A Caldeira, and Lester Keny-atta Spence.
2003.
Measuring Attitudes toward the92United States Supreme Court.
American Journal ofPolitical Science.Valerie Hoekstra.
2003.
Public Reaction to SupremeCourt Decisions.
Cambridge University Press.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Micro-blogging As Online Wordof Mouth Branding.
in CHI ?09 Extended Abstractson Human Factors in Computing Systems.Timothy R. Johnson, and Andrew D. Martin.
1998.The Public?s Conditional Response to SupremeCourt Decisions.
American Political Science Re-view 92(2):299-309.Haewoon Kwak, Changhyun Lee, Hosung Park, andSue Moon.
2010.
What is Twitter, a Social Networkor a News Media?.
in Proceedings of the 19th Inter-national Conference on World Wide Web (WWW).Yu-Ru Lin, Drew Margolin, Brian Keegan, and DavidLazer.
2013.
Voices of Victory: A ComputationalFocus Group Framework for Tracking Opinion Shiftin Real Time.
In Proceedings of International WorldWide Web Conference (WWW).Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies.Yelena Mejova, Padmini Srinivasan, and Bob Boynton.2013.
GOP Primary Season on Twitter: ?Popular?Political Sentiment in Social Media.
In Proceedingsof the Sixth ACM International Conference on WebSearch and Data Mining (WSDM).B.
O?Connor, R. Balasubramanyan, B. R. Routledge,and N. A. Smith.
2010.
From tweets to polls: Link-ing text sentiment to public opinion time series.
InProceedings of International AAAI Conference onWeblogs and Social Media (ICWSM).Bo Pang, and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Antonio Reyes, Paolo Rosso, and Tony Veale.
2012.A multidimensional approach for detecting irony inTwitter.
Language Resources and Evaluation.Swapna Somasundaran, Galileo Namata, Lise Getoor,and Janyce Wiebe.
2009.
Opinion Graphs for Po-larity and Discourse Classification.
TextGraphs-4: Graph-based Methods for Natural Language Pro-cessing.Aline A. Vanin, Larissa A. Freitas, Re-nata Vieira, andMarco Bochernitsan.
2013.
Some clues on ironydetection in tweets.
In Proceedings of InternationalWorld Wide Web Conference (WWW).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing Contextual Polarity: an explo-ration of features for phrase-level sentiment analy-sis.
Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.2004.
Just how mad are you?
Finding strong andweak opinion clauses.
In Proceedings of Conferenceon Artificial Intelligence (AAAI).Andranik Tumasjan, Timm O. Sprenger, Philipp G.Sandner, and Isabell M. Welpe.
2010.
PredictingElections with Twitter: What 140 Characters Re-veal about Political Sentiment.
In Proceedings ofthe Fourth International AAAI Conference on We-blogs and Social Media (ICWSM).93
