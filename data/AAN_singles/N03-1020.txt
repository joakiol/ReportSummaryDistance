Automatic Evaluation of Summaries Using N-gramCo-Occurrence StatisticsChin-Yew Lin and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292{cyl,hovy}@isi.eduAbstractFollowing the recent adoption by the machinetranslation community of automatic evalua-tion using the BLEU/NIST scoring process,we conduct an in-depth study of a similar ideafor evaluating summaries.
The results showthat automatic evaluation using unigram co-occurrences between summary pairs correlatessurprising well with human evaluations, basedon various statistical metrics; while direct ap-plication of the BLEU evaluation proceduredoes not always give good results.1 IntroductionAutomated text summarization has drawn a lot of inter-est in the natural language processing and informationretrieval communities in the recent years.
A series ofworkshops on automatic text summarization (WAS2000, 2001, 2002), special topic sessions in ACL,COLING, and SIGIR, and government sponsoredevaluation efforts in the United States (DUC 2002) andJapan (Fukusima and Okumura 2001) have advancedthe technology and produced a couple of experimentalonline systems (Radev et al 2001, McKeown et al2002).
Despite these efforts, however, there are nocommon, convenient, and repeatable evaluation meth-ods that can be easily applied to support system devel-opment and just-in-time comparison among differentsummarization methods.The Document Understanding Conference (DUC 2002)run by the National Institute of Standards and Technol-ogy (NIST) sets out to address this problem by provid-ing annual large scale common evaluations in textsummarization.
However, these evaluations involvehuman judges and hence are subject to variability (Rathet al 1961).
For example, Lin and Hovy (2002) pointedout that 18% of the data contained multiple judgmentsin the DUC 2001 single document evaluation1.To further progress in automatic summarization, in thispaper we conduct an in-depth study of automaticevaluation methods based on n-gram co-occurrence inthe context of DUC.
Due to the setup in DUC, theevaluations we discussed here are intrinsic evaluations(Sp?rck Jones and Galliers 1996).
Section 2 gives anoverview of the evaluation procedure used in DUC.Section 3 discusses the IBM BLEU (Papineni et al2001) and NIST (2002) n-gram co-occurrence scoringprocedures and the application of a similar idea inevaluating summaries.
Section 4 compares n-gram co-occurrence scoring procedures in terms of their correla-tion to human results and on the recall and precision ofstatistical significance prediction.
Section 5 concludesthis paper and discusses future directions.2 Document Understanding ConferenceThe 2002 Document Understanding Conference2 in-cluded the follow two main tasks:?
Fully automatic single-document summarization:given a document, participants were required tocreate a generic 100-word summary.
The trainingset comprised 30 sets of approximately 10 docu-ments each, together with their 100-word humanwritten summaries.
The test set comprised 30 un-seen documents.?
Fully automatic multi-document summarization:given a set of documents about a single subject,participants were required to create 4 generic sum-maries of the entire set, containing 50, 100, 200,and 400 words respectively.
The document setswere of four types: a single natural disaster event; a1 Multiple judgments occur when more than one performancescore is given to the same system (or human) and human sum-mary pairs by the same human judge.2 DUC 2001 and DUC 2002 have similar tasks, but summariesof 10, 50, 100, and 200 words are requested in the multi-document task in DUC 2002.Edmonton, May-June 2003Main Papers , pp.
71-78Proceedings of HLT-NAACL 2003single event; multiple instances of a type of event;and information about an individual.
The trainingset comprised 30 sets of approximately 10 docu-ments, each provided with their 50, 100, 200, and400-word human written summaries.
The test setcomprised 30 unseen sets.A total of 11 systems participated in the single-document summarization task and 12 systems partici-pated in the multi-document task.2.1 Evaluation MaterialsFor each document or document set, one human sum-mary was created as the ?ideal?
model summary at eachspecified length.
Two other human summaries werealso created at each length.
In addition, baseline sum-maries were created automatically for each length asreference points.
For the multi-document summariza-tion task, one baseline, lead baseline, took the first 50,100, 200, and 400 words in the last document in thecollection.
A second baseline, coverage baseline, tookthe first sentence in the first document, the first sentencein the second document and so on until it had a sum-mary of 50, 100, 200, or 400 words.
Only one baseline(baseline1) was created for the single document summa-rization task.2.2 Summary Evaluation EnvironmentTo evaluate system performance NIST assessors whocreated the ?ideal?
written summaries did pairwise com-parisons of their summaries to the system-generatedsummaries, other assessors?
summaries, and baselinesummaries.
They used the Summary Evaluation Envi-ronment (SEE) 2.0 developed by (Lin 2001) to supportthe process.
Using SEE, the assessors compared thesystem?s text (the peer text) to the ideal (the modeltext).
As shown in Figure 1, each text was decomposedinto a list of units and displayed in separate windows.SEE 2.0 provides interfaces for assessors to judge boththe content and the quality of summaries.
To measurecontent, assessors step through each model unit, markall system units sharing content with the current modelunit (green/dark gray highlight in the model summarywindow), and specify that the marked system units ex-press all, most, some, or hardly any of the content of theFigure 1.
SEE in an evaluation session.current model unit.
To measure quality, assessors rategrammaticality3, cohesion4, and coherence5 at five dif-ferent levels: all, most, some, hardly any, or none6.
Forexample, as shown in Figure 1, an assessor marked sys-tem units 1.1 and 10.4 (red/dark underlines in the leftpane) as sharing some content with the current modelunit 2.2 (highlighted green/dark gray in the right).2.3 Evaluation MetricsRecall at different compression ratios has been used insummarization research to measure how well an auto-matic system retains important content of originaldocuments (Mani et al 1998).
However, the simple sen-tence recall measure cannot differentiate system per-formance appropriately, as is pointed out by Donawayet al (2000).
Therefore, instead of pure sentence recallscore, we use coverage score C. We define it as fol-lows7:)1(summary model in the MUs ofnumber  Totalmarked) MUs of(Number EC ?=E, the ratio of completeness, ranges from 1 to 0: 1 forall, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0for none.
If we ignore E (set it to 1), we obtain simplesentence recall score.
We use average coverage scoresderived from human judgments as the references toevaluate various automatic scoring methods in the fol-lowing sections.3 BLEU and N-gram Co-OccurrenceTo automatically evaluate machine translations the ma-chine translation community recently adopted an n-gramco-occurrence scoring procedure BLEU (Papineni et al2001).
The NIST (NIST 2002) scoring metric is basedon BLEU.
The main idea of BLEU is to measure thetranslation closeness between a candidate translationand a set of reference translations with a numerical met-ric.
To achieve this goal, they used a weighted averageof variable length n-gram matches between systemtranslations and a set of human reference translationsand showed that a weighted average metric, i.e.
BLEU,correlating highly with human assessments.Similarly, following the BLEU idea, we assume that thecloser an automatic summary to a professional human3 Does the summary observe English grammatical rules inde-pendent of its content?4 Do sentences in the summary fit in with their surroundingsentences?5 Is the content of the summary expressed and organized in aneffective way?6 These category labels are changed to numerical values of100%, 80%, 60%, 40%, 20%, and 0% in DUC 2002.7 DUC 2002 uses a length adjusted version of coverage metricC?, where C?
= ?
*C + (1-?)*B.
B is the brevity and ?
is a pa-rameter reflecting relative importance (DUC 2002).summary, the better it is.
The question is: ?Can we ap-ply BLEU directly without any modifications to evalu-ate summaries as well??.
We first ran IBM?s BLEUevaluation script unmodified over the DUC 2001 modeland peer summary set.
The resulting Spearman rankorder correlation coefficient (?)
between BLEU and thehuman assessment for the single document task is 0.66using one reference summary and 0.82 using three ref-erence summaries; while Spearman ?
for the multi-document task is 0.67 using one reference and 0.70 us-ing three.
These numbers indicate that they positivelycorrelate at ?
= 0.018.
Therefore, BLEU seems a prom-ising automatic scoring metric for summary evaluation.According to Papineni et al (2001), BLEU is essentiallya precision metric.
It measures how well a machinetranslation overlaps with multiple human translationsusing n-gram co-occurrence statistics.
N-gram precisionin BLEU is computed as follows:?
??
??
???
???
?=}{}{)()(CandidatesC CgramnCandidatesC Cgramnclipn gramnCountgramnCountp   (2)Where Countclip(n-gram) is the maximum number of n-grams co-occurring in a candidate translation and a ref-erence translation, and Count(n-gram) is the number ofn-grams in the candidate translation.
To prevent veryshort translations that try to maximize their precisionscores, BLEU adds a brevity penalty, BP, to the for-mula:)3(1|)|/||1( ???????>=?
rcifercifBP crWhere |c| is the length of the candidate translation and|r| is the length of the reference translation.
The BLEUformula is then written as follows:)4(logexp1??????
?= ?=Nnnn pwBPBLEUN is set at 4 and wn, the weighting factor, is set at 1/N.For summaries by analogy, we can express equation (1)in terms of n-gram matches following equation (2):)5()()(}{}{ ?
??
??
???
???
?=UnitsModelC CgramnUnitsModelC Cgramnmatchn gramnCountgramnCountCWhere Countmatch(n-gram) is the maximum number ofn-grams co-occurring in a peer summary and a modelunit and Count(n-gram) is the number of n-grams in themodel unit.
Notice that the average n-gram coveragescore, Cn, as shown in equation 5 is a recall metric8 The number of instances is 14 (11 systems, 2 humans, and 1baseline) for the single document task and is 16 (12 systems, 2humans, and 2 baselines) for the multi-document task.instead of a precision one as pn.
Since the denominatorof equation 5 is the total sum of the number of n-gramsoccurring at the model summary side instead of the peerside and only one model summary is used for eachevaluation; while there could be multiple referencesused in BLEU and Countclip(n-gram) could come frommatching different reference translations.
Furthermore,instead of a brevity penalty that punishes overly shorttranslations, a brevity bonus, BB, should be awarded toshorter summaries that contain equivalent content.
Infact, a length adjusted average coverage score was usedas an alternative performance metric in DUC 2002.However, we set the brevity bonus (or penalty) to 1 forall our experiments in this paper.
In summary, the n-gram co-occurrence statistics we use in the followingsections are based on the following formula:)6(logexp),( ????????
?= ?=jinnn CwBBjiNgramWhere j ?
i, i and j range from 1 to 4, and wn is 1/(j-i+1).
Ngram(1, 4) is a weighted variable length n-grammatch score similar to the IBM BLEU score; whileNgram(k, k), i.e.
i = j = k, is simply the average k-gramcoverage score Ck.With these formulas, we describe how to evaluate themin the next section.4 Evaluations of N-gram Co-OccurrenceMetricsIn order to evaluate the effectiveness of automaticevaluation metrics, we propose two criteria:1.
Automatic evaluations should correlate highly,positively, and consistently with human assess-ments.2.
The statistical significance of automatic evaluationsshould be a good predictor of the statistical signifi-cance of human assessments with high reliability.The first criterion ensures whenever a human recognizesa good summary/translation/system, an automaticevaluation will do the same with high probability.
Thisenables us to use an automatic evaluation procedure inplace of human assessments to compare system per-formance, as in the NIST MT evaluations (NIST 2002).The second criterion is critical in interpreting the sig-nificance of automatic evaluation results.
For example,if an automatic evaluation shows there is a significantdifference between run A and run B at ?
= 0.05 usingthe z-test (t-test or bootstrap resampling), how does thistranslate to ?real?
significance, i.e.
the statistical signifi-cance in a human assessment of run A and run B?
Ide-ally, we would like there to be a positive correlationbetween them.
If this can be asserted with strong reli-ability (high recall and precision), then we can use theautomatic evaluation to assist system development andto be reasonably sure that we have made progress.4.1 Correlation with Human AssessmentsAs stated in Section 3, direct application of BLEU onthe DUC 2001 data showed promising results.
However,BLEU is a precision-based metric while the humanevaluation protocol in DUC is essentially recall-based.We therefore prefer the metric given by equation 6 anduse it in all our experiments.
Using DUC 2001 data, wecompute average Ngram(1,4) scores for each  peer sys-tem at different summary sizes and rank systems ac-cording to their scores.
We then compare theNgram(1,4) ranking with the human ranking.
Figure 2shows the result of DUC 2001 multi-document data.Stopwords are ignored during the computation ofNgram(1,4) scores and words are stemmed using a Por-ter stemmer (Porter 1980).
The x-axis is the humanranking and the y-axis gives the correspondingNgram(1,4) rankings for summaries of difference sizes.The straight line marked by AvgC is the ranking givenby human assessment.
For example, a system at (5,8)Table 1.
Spearman rank order correlation coeffi-cients of different DUC 2001 data betweenNgram(1, 4)n rankings and human rankings includ-ing (S) and excluding (SX) stopwords.
SD-100 isfor single document summaries of 100 words andMD-50, 100, 200, and 400 are for multi-documentsummaries of 50, 100, 200, and 400 words.
MD-Allaverages results from summaries of all sizes.1234567891011121314151 2 3 4 5 6 7 8 9 10 11 12 13 14 15Human RankingNgram(1, 4)nRankingAvgCNgram(1, 4)50Ngram(1, 4)100Ngram(1, 4)200Ngram(1, 4)400Ngram(1, 4)allFigure 2.
Scatter plot of Ngram(1,4)n score rank-ings versus human ranking for the multi-document task data from DUC 2001.
The samesystem is at each vertical line with ranking givenby different Ngram(1,4)n scores.
The straight line(AvgC) is the human ranking and n marks sum-maries of different sizes.
Ngram(1,4)all combinesresults from all sizes.SD-100 MD-All MD-50 MD-100 MD-200 MD-400SX 0.604 0.875 0.546 0.575 0.775 0.861S 0.615 0.832 0.646 0.529 0.814 0.843means that human ranks its performance at the 5th rankwhile Ngram(1,4)400 ranks it at the 8th.
If an automaticranking fully matches the human ranking, its plot willcoincide with the heavy diagonal.
A line with less de-viation from the heavy diagonal line indicates bettercorrelation with the human assessment.To quantify the correlation, we compute the Spearmanrank order correlation coefficient (?)
for each N-gram(1,4)n run at different summary sizes (n).
We alsotest the effect of inclusion or exclusion of stopwords.The results are summarized in Table 1.Although these results are statistically significant (?
=0.025) and are comparable to IBM BLEU?s correlationfigures shown in Section 3, they are not consistentacross summary sizes and tasks.
For example, the corre-lations of the single document task are at the 60% level;while they range from 50% to 80% for the multi-document task.
The inclusion or exclusion of stopwordsalso shows mixed results.
In order to meet the require-ment of the first criterion stated in Section 3, we needbetter results.The Ngram(1,4)n score is a weighted average of variablelength n-gram matches.
By taking a log sum of the n-gram matches, the Ngram(1,4)n favors match of longern-grams.
For example, if ?United States of America?occurs in a reference summary, while one peer sum-mary, A, uses ?United States?
and another summary, B,uses the full phrase ?United States of America?, sum-mary B gets more contribution to its overall score sim-ply due to the longer version of the name.
However,intuitively one should prefer a short version of the namein summarization.
Therefore, we need to change theweighting scheme to not penalize or even reward shorterequivalents.
We conduct experiments to understand theeffect of individual n-gram co-occurrence scores in ap-proximating human assessments.
Tables 2 and 3 showthe results of these runs without and with stopwordsrespectively.For each set of DUC 2001 data, single document 100-word summarization task, multi-document 50, 100, 200,and 400 -word summarization tasks, we compute 4 dif-ferent correlation statistics: Spearman rank order corre-lation coefficient (Spearman ?
),  linear regression t-test(LRt, 11 degree of freedom for single document task and13 degree of freedom for multi-document task), Pearsonproduct moment coefficient of correlation (Pearson ?
),and coefficient of determination (CD) for eachNgram(i,j) evaluation metric.
Among them Spearman ?is a nonparametric test, a higher number indicateshigher correlation; while the other three tests are para-metric tests.
Higher LRt, Pearson ?, and CD also sug-gests higher linear correlation.Analyzing all runs according to Tables 2 and 3, wemake the following observations:(1) Simple unigram, Ngram(1,1), and bi-gram,Ngram(2,2), co-occurrence statistics consistentlyoutperform (0.99 ?
Spearman ?
?
0.75) theweighted average of n-gram of variable lengthNgram(1, 4) (0.88 ?
Spearman ?
?
0.55) in singleand multiple document tasks when stopwords areignored.
Importantly, unigram performs especiallywell with Spearman ?
ranging from 0.88 to 0.99that is better than the best case in which weightedaverage of variable length n-gram matches is usedand is consistent across different data sets.
(2) The performance of weighted average n-gramscores is in the range between bi-gram and tri-gramco-occurrence scores.
This might suggest somesummaries are over-penalized by the weighted av-erage metric due to the lack of longer n-grammatches.
For example, given a model string?United States, Japan, and Taiwan?, a candidateTable 3.
Various Ngram(i, j) rank/score correlationsfor 4 different statistics (with stopwords).Table 2.
Various Ngram(i,j) rank/score correlationsfor 4 different statistics (without stopwords): Spear-man rank order coefficient correlation (Spearman ?
),linear regression t-test (LRt), Pearson product mo-ment coefficient of correlation (Pearson ?
), and co-efficient of determination (CD).Ngram (1,4) Ngram (1,1) Ngram (2,2) Ngram (3,3) Ngram (4,4)Single Doc Spearman ?
0.604 0.989 0.868 0.527 0.505100 LRt 1.025 7.130 2.444 0.704 0.053Pearson ?
0.295 0.907 0.593 0.208 0.016CD 0.087 0.822 0.352 0.043 0.000Multi-Doc Spearman ?
0.875 0.993 0.950 0.782 0.736All LRt 3.910 13.230 5.830 3.356 2.480Pearson ?
0.735 0.965 0.851 0.681 0.567CD 0.540 0.931 0.723 0.464 0.321Multi-Doc Spearman ?
0.546 0.879 0.746 0.496 0.34350 LRt 2.142 5.681 3.350 2.846 2.664Pearson ?
0.511 0.844 0.681 0.620 0.594CD 0.261 0.713 0.463 0.384 0.353Multi-Doc Spearman ?
0.575 0.896 0.761 0.543 0.468100 LRt 2.369 7.873 3.641 1.828 1.385Pearson ?
0.549 0.909 0.711 0.452 0.359CD 0.301 0.827 0.505 0.204 0.129Multi-Doc Spearman ?
0.775 0.979 0.904 0.782 0.754200 LRt 3.243 15.648 4.929 2.772 2.126Pearson ?
0.669 0.974 0.807 0.609 0.508CD 0.447 0.950 0.651 0.371 0.258Multi-Doc Spearman ?
0.861 0.982 0.961 0.854 0.661400 LRt 4.390 10.569 6.409 3.907 2.755Pearson ?
0.773 0.946 0.872 0.735 0.607CD 0.597 0.896 0.760 0.540 0.369Ngram (1,4) Ngram (1,1) Ngram (2,2) Ngram (3,3) Ngram (4,4)Single Doc Spearman ?
0.615 0.951 0.863 0.615 0.533100 LRt 1.076 4.873 2.228 0.942 0.246Pearson ?
0.309 0.827 0.558 0.273 0.074CD 0.095 0.683 0.311 0.075 0.005Multi-Doc Spearman ?
0.832 0.918 0.936 0.832 0.732All LRt 3.752 6.489 5.451 3.745 2.640Pearson ?
0.721 0.874 0.834 0.720 0.591CD 0.520 0.764 0.696 0.519 0.349Multi-Doc Spearman ?
0.646 0.586 0.650 0.589 0.60050 LRt 2.611 2.527 2.805 2.314 1.691Pearson ?
0.587 0.574 0.614 0.540 0.425CD 0.344 0.329 0.377 0.292 0.180Multi-Doc Spearman ?
0.529 0.636 0.625 0.571 0.468100 LRt 2.015 3.338 2.890 2.039 1.310Pearson ?
0.488 0.679 0.625 0.492 0.342CD 0.238 0.462 0.391 0.242 0.117Multi-Doc Spearman ?
0.814 0.964 0.879 0.814 0.746200 LRt 3.204 10.134 4.926 3.328 2.173Pearson ?
0.664 0.942 0.807 0.678 0.516CD 0.441 0.888 0.651 0.460 0.266Multi-Doc Spearman ?
0.843 0.914 0.946 0.857 0.721400 LRt 4.344 5.358 6.344 4.328 3.066Pearson ?
0.769 0.830 0.869 0.768 0.648CD 0.592 0.688 0.756 0.590 0.420string ?United States, Taiwan, and Japan?
has aunigram score of 1, bi-gram score of 0.5, and tri-gram and 4-gram scores of 0 when the stopword?and?
is ignored.
The weighted average n-gramscore for the candidate string is 0.
(3) Excluding stopwords in computing n-gram co-occurrence statistics generally achieves better cor-relation than including stopwords.4.2 Statistical Significance of N-gram Co-Occurrence Scores versus Human As-sessmentsWe have shown that simple unigram, Ngram(1,1), or bi-gram, Ngram(2,2), co-occurrence statistics based onequation 6 outperform the weighted average of n-grammatches, Ngram(1,4), in the previous section.
To exam-ine how well the statistical significance in the automaticNgram(i,j) metrics translates to real significance whenhuman assessments are involved, we set up the follow-ing test procedures:(1) Compute pairwise statistical significance test suchas z-test or t-test for a system pair (X,Y) at certain ?level, for example ?
= 0.05, using automatic met-rics and human assigned scores.
(2) Count the number of cases a z-test indicates there isa significant difference between X and Y based onthe automatic metric.
Call this number NAs.
(3) Count the number of cases a z-test indicates there isa significant difference between X and Y based onthe human assessment.
Call this number NHs.
(4) Count the cases when an automatic metric predictsa significant difference and the human assessmentalso does.
Call this Nhit.
For example, if a z-test in-dicates system X is significantly different from Ywith ?
= 0.05 based on the automatic metric scoresand the corresponding z-test also suggests the samebased on the human agreement, then we have a hit.
(5) Compute the recall and precision using the follow-ing formulas:recall =HshitNNprecision =AshitNNA good automatic metric should have high recall andprecision.
This implies that if a statistical test indicates asignificant difference between two runs using the auto-matic metric then very probably there is also a signifi-cant difference in the manual evaluation.
This would bevery useful during the system development cycle togauge if an improvement is really significant or not.Figure 3 shows the recall and precision curves for theDUC 2001 single document task at different ?
levelsand Figure 4 is for the multi-document task with differ-ent summary sizes.
Both of them exclude stopwords.We use z-test in all the significance tests with ?
level at0.10, 0.05, 0.25, 0.01, and 0.005.From Figures 3 and 4, we can see Ngram(1,1) andNgram(2,2) reside on the upper right corner of the recalland precision graphs.
Ngram(1,1) has the best overallbehavior.
These graphs confirm Ngram(1,1) (simple0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91RecallPrecisionSignificance Predication Recall and Precision CurveNgram(1,4)Ngram(1,1)Ngram(2,2)Ngram(3,3)Ngram(4,4)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91RecallPrecisionSignificance Predication Recall and Precision CurveNgram(1,4)Ngram(1,1)Ngram(2,2)Ngram(3,3)Ngram(4,4)Figure 3.
Recall and precision curves of N-gram co-occurrence statistics versus humanassessment for DUC 2001 single documenttask.
The 5 points on each curve represent val-ues for the 5 ?
levels.Figure 4.
Recall and precision curves of N-gramco-occurrence statistics versus human assessmentfor DUC 2001 multi-document task.
Dark (black)solid lines are for average of all summary sizes,light (red) solid lines are for 50-word summaries,dashed (green) lines are for 100-word summaries,dash-dot lines (blue) are for 200-word summaries,and dotted (magenta) lines are for 400-wordsummaries.unigram) is a good automatic scoring metric with goodstatistical significance prediction power.5 ConclusionsIn this paper, we gave a brief introduction of the manualsummary evaluation protocol used in the DocumentUnderstanding Conference.
We then discussed the IBMBLEU MT evaluation metric, its application to sum-mary evaluation, and the difference between precision-based BLEU translation evaluation and recall-basedDUC summary evaluation.
The discrepancy led us toexamine the effectiveness of individual n-gram co-occurrence statistics as a substitute for expensive anderror-prone manual evaluation of summaries.
To evalu-ate the performance of automatic scoring metrics, weproposed two test criteria.
One was to make sure systemrankings produced by automatic scoring metrics weresimilar to human rankings.
This was quantified bySpearman?s rank order correlation coefficient and threeother parametric correlation coefficients.
Another wasto compare the statistical significance test results be-tween automatic scoring metrics and human assess-ments.
We used recall and precision of the agreementbetween the test statistics results to identify good auto-matic scoring metrics.According to our experiments, we found that unigramco-occurrence statistics is a good automatic scoringmetric.
It consistently correlated highly with humanassessments and had high recall and precision in signifi-cance test with manual evaluation results.
In contrast,the weighted average of variable length n-gram matchesderived from IBM BLEU did not always give good cor-relation and high recall and precision.
We surmise that areason for the difference between summarization andmachine translation might be that extraction-basedsummaries do not really suffer from grammar problems,while translations do.
Longer n-grams tend to score forgrammaticality rather than content.It is encouraging to know that the simple unigram co-occurrence metric works in the DUC 2001 setup.
Thereason for this might be that most of the systems par-ticipating in DUC generate summaries by sentence ex-traction.
We plan to run similar experiments on DUC2002 data to see if unigram does as well.
If it does, wewill make available our code available via a website tothe summarization community.Although this study shows that unigram co-occurrencestatistics exhibit some good properties in summaryevaluation, it still does not correlate to human assess-ment 100% of the time.
There is more to be desired inthe recall and precision of significance test agreementwith manual evaluation.
We are starting to explore vari-ous metrics suggested in Donaway et al (2000).
Forexample, weight n-gram matches differently accordingto their information content measured by tf, tfidf, orSVD.
In fact, NIST MT automatic scoring metric (NIST2002) already integrates such modifications.One future direction includes using an automatic ques-tion answer test as demonstrated in the pilot study inSUMMAC (Mani et al 1998).
In that study, an auto-matic scoring script developed by Chris Buckleyshowed high correlation with human evaluations, al-though the experiment was only tested on a small set of3 topics.According to Over (2003), NIST spent about 3,000 manhours each in DUC 2001 and 2002 for topic and docu-ment selection, summary creation, and manual evalua-tion.
Therefore, it would be wise to use these valuableresources, i.e.
manual summaries and evaluation results,not only in the formal evaluation every year but also indeveloping systems and designing automatic evaluationmetrics.
We would like to propose an annual automaticevaluation track in DUC that encourages participants toinvent new automated evaluation metrics.
Each year thehuman evaluation results can be used to evaluate theeffectiveness of the various automatic evaluation met-rics.
The best automatic metric will be posted at theDUC website and used as an alternative in-house andrepeatable evaluation mechanism during the next year.In this way the evaluation technologies can advance atthe same pace as the summarization technologies im-prove.ReferencesDonaway, R.L., Drummey, K.W., and Mather, L.A.2000.
A Comparison of Rankings Produced bySummarization Evaluation Measures.
In Proceedingof the Workshop on Automatic Summarization, post-conference workshop of ANLP-NAACL-2000, pp.69-78, Seattle, WA, 2000.DUC.
2002.
The Document Understanding Conference.http://duc.nist.gov.Fukusima, T. and Okumura, M. 2001.
Text Summariza-tion Challenge: Text Summarization Evaluation atNTCIR Workshop2.
In Proceedings of the SecondNTCIR Workshop on Research in Chinese & Japa-nese Text Retrieval and Text Summarization, NII,Tokyo, Japan, 2001.Lin, C.-Y.
2001.
Summary Evaluation Environment.http://www.isi.edu/~cyl/SEE.Lin, C.-Y.
and E. Hovy.
2002.
Manual and AutomaticEvaluations of Summaries.
In Proceedings of theWorkshop on Automatic Summarization, post-conference workshop of ACL-2002, pp.
45-51, Phila-delphia, PA, 2002.McKeown, K., R. Barzilay, D. Evans, V. Hatzivassi-loglou, J. L. Klavans, A. Nenkova, C. Sable, B.Schiffman, S. Sigelman.
Tracking and SummarizingNews on a Daily Basis with Columbia?s Newsblaster.In Proceedings of Human Language TechnologyConference 2002 (HLT 2002).
San Diego, CA, 2002.Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T.Firmin, M. Chrzanowski, and B. Sundheim.
1998.The TIPSTER SUMMAC Text SummarizationEvaluation: Final Report.
MITRE Corp. Tech.
Re-port.NIST.
2002.
Automatic Evaluation of Machine Transla-tion Quality using N-gram Co-Occurrence Statistics.Over, P. 2003.
Personal Communication.Papineni, K., S. Roukos, T. Ward, W.-J.
Zhu.
2001.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
IBM Research Report RC22176(W0109-022).Porter, M. F. 1980.
An Algorithm for Suffix Stripping.Program, 14, pp.
130-137.Radev, D. R., S. Blair-Goldensohn, Z. Zhang, and R.S.
Raghavan.
Newsinessence: A System for Domain-Independent, Real-Time News Clustering and Multi-Document Summarization.
In Proceedings of humanLanguage Technology Conference (HLT 2001), SanDiego, CA, 2001.Sp?rck Jones, K. and J. R. Galliers.
1996.
EvaluatingNatural Language Processing Systems: An Analysisand Review.
New York: Springer.Rath, G.J., Resnick, A., and Savage, T.R.
1961.
TheFormation of Abstracts by the Selection of Sen-tences.
American Documentation, 12(2), pp.
139-143.
Reprinted in Mani, I., and Maybury, M., eds,Advances in Automatic Text Summarization, MITPress, pp.
287-292.WAS.
2000.
Workshop on Automatic Summarization,post-conference workshop of ANLP-NAACL-2000,Seattle, WA, 2000.WAS.
2001.
Workshop on Automatic Summarization,pre-conference workshop of NAACL-2001, Pitts-burgh, PA, 2001.WAS.
2002.
Workshop on Automatic Summarization,post-conference workshop of ACL-2002, Philadel-phia, PA, 2002.
