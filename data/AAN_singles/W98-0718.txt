Usage of WordNet in Natural Language GenerationHongyan J ingDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAhjing~cs.columbia.eduAbstractWordNet has rarely been applied to natural lan-guage generation, despite of its wide applica-tion in other fields.
In this paper, we addressthree issues in the usage of WordNet in gener-ation: adapting a general lexicon like WordNetto a specific application domain, how the infor-mation in WordNet can be used in generation,and augmenting WordNet with other types ofknowledge that are helpful for generation.
Wepropose a three step procedure to tailor Word-Net to a specific domain, and carried out ex-periments on a basketball corpus (1,015 gamereports, 1.TMB).1 IntroductionWordNet (Miller et al, 1990) has been success-fully applied in many human language relatedapplications, such as word sense disambigua-tion, information retrieval, and text categoriza-tion; yet generation is among the fields in whichthe application of WordNet has rarely been ex-plored.
We demonstrate in this paper that, as arich semantic net, WordNet is indeed a valuableresource for generation.
We propose a corpusbased technique to adapt WordNet to a specificdomain and present experiments in the basket-ball domain.
We also discuss possible ways touse WordNet knowledge in the generation taskand to augment WordNet with other types ofknowledge.In Section 2, we answer the question whyWordNet is useful for generation.
In Section3, we discuss problems to be solved to success-fully apply WordNet to generation.
In Section4, we present techniques to solve the problems.Finally, we present future work and conclude.2 Why a valuable resource  forgenerat ion?WordNet is a potentially valuable resource forgeneration for four reasons.
First, Synonymsets in WordNet (synsets) can possibly providelarge amount of lexical paraphrases.
One ma-jor shortcoming ofcurrent generation systems isits poor expressive capability.
Usually none orvery limited paraphrases are provided by a gen-eration system due to the cost of hand-coding inthe lexicon.
Synsets, however, provide the pos-sibility to generate l xical paraphrases withouttedious hand-coding in individual systems.
Forexample, for the output sentence "Jordan hit ajumper", we can generate the paraphrase "Jor-dan hit a jump shot" simply by replacing theword jumper in the sentence with its synonymjump shot listed in WordNet synset.
Whereas,such replacements are not always appropriate.For example, tally and rack up are listed as syn-onyms of the word score, although the sentencelike "Jordan scored 22 points" are common innewspaper sport reports, sentences like "Jor-dan tallied 22 points" or "Jordan racked up 22points" seldomly occur.
To successfully applyWordNet for paraphrasing, we need to developtechniques which can correctly identify inter-changeability of synonyms in a certain context.Secondly, as a semantic net linked by lexi-cal relations, WordNet can be used for lexical-ization in generation.
Lexicalization maps thesemantic oncepts to be conveyed to appropri-ate words.
Usually it is achieved by step-wiserefinements based on syntactic, semantic, andpragmatic onstraints while traversing a seman-tic net (Danlos, 1987).
Currently most genera-tion systems acquire their semantic net for lexi-calization by building their own, while WordNetprovides the possibility to acquire such knowl-edge automatically from an existing resource.128!!!!!,!!!aI!!!
!iliIIIIINext, WordNet ontology can be used forbuilding domain ontology.
Most current genera-tion systems manually build their domain ontol-ogy from scratch.
The process is time and laborintensive, and introduction of errors is likely.WordNet ontology has a wide coverage, so canpossibly be used as a basis for building domainontology.
The problem to be solved is how toadapt it to a specific domain.Finally, WordNet is indexed by conceptsrather than merely by words maims it especiallydesirable for the generation task.
Unlike lan-guage interpretation, generation has as inputsthe semantic oncepts to be conveyed and mapsthem to appropriate words.
Thus an ideal gen-eration lexicon should be indexed by semanticconcepts rather than words.
Most available lin-guistic resources are not suitable to use in gen-eration directly due to their lack of mapping be-tween concepts and words.
WordNet is by farthe richest and largest database among all re-sources that are indexed by concepts.
Other rel-atively large and concept=based resources suchas PENMAN ontology (Bateman et al, 1990)usually include only hyponymy relations com-pared to the rich types of lexical relations pre-sented in WordNet.Once WordNet is tailored to the domain, themain problem is how to use its knowledge in thegeneration process.
As we mentioned in section2, WordNet can potentially benefit generationin three aspects: producing large amount of lex-icai paraphrases, providing the semantic net forlexicalization, and providing a basis for buildingdomain ontology.
A number of problems to besolved at this stage, including: (a)while usingsynset for producing paraphrases, how to de-termine whether two synonyms are interchange-able in a particular context?
(b)while WordNetcan provide the semantic net for lexicalization,the constraints to choose a particular node dur-ing lexical choice still need to be established.
(c) How to use the WordNet ontology?The last problem is relevant o augmentingWordNet with other types of information.
Al-though WordNet is a rich lexical database, itcan not contain all types of information thatare needed for generation, for example, syntac-tic information in WordNet is weak.
It is thenworthwhile to investigate the possibility to com-bine it with other resources.In the following section, we address the aboveissues in order and present our experiment re-sults in the basketball domain.3 Prob lems to be solvedDespite the above advantages, there are someproblems to be solved for the application ofWordNet in a generation system to be success-ful.The first problem is how to adapt WordNetto a particular domain.
With 121,962 uniquewords, 99,642 synsets, and 173,941 senses ofwords as of version 1.6, WordNet represents thelargest publically available lexical resource todate.
The wide coverage on one hand is benefi-cial, since as a general resource, wide coverageallows it to provide information for different ap-plications.
On  the other hand, this can also bequite problematic since it is very difficult foran application to efficiently handle such a largedatabase.
Therefore, the first step towards uti-lizing WordNet in generation is to prune unre-lated information in the general database so asto tailor it to the domain.
On  the other hand,domain specific knowledge that is not coveredby the general database needs to be added tothe database.4 So lu t ions4.1 Adapt ing  WordNet  to a domainWe propose a corpus based method to automat-ically adapt a general resource like WordNet toa domain.
Most generation systems till usehand-coded lexicons and ontologies, however,corpus based automatic techniques are in de-mand as natural anguage generation is used inmore ambitious applications and large corporain various domains are becoming available.
Theproposed method involves three steps of pro-cessing.Step 1: P rune  unused words andsynsetsWe first prune words and synsets that arelisted in WordNet but not used in the domain.This is accomplished by tagging the domain cor-pus with part of speech information, then foreach word in WordNet, if it appears in the do-main corpus and its part of speech is the sameas that in the corpus, the word is kept in the re-sult, otherwise it is eliminated; for each synset129IIIIIIIiI!IIIin WordNet, if none of the words in the synsetappears in the domain corpus, the synset as awhole is deleted.
The only exception is that ifa synset is the closest common ancestor of twosyrmets in the domain corpus, the synset is al-ways kept in the result.
The reason to keep thiskind of synsets is to generalize the semantic at-egory of verb arg~lments, aswe illustrate in step2.
The frequency of words in such synsets willbe marked zero so that they will not be usedin output.
Figure 1 shows two example prun-ing operations: (A) is a general case, and (B)is the case involving ancestor syuset.
In thisstep, words are not yet disambiguated, so all thesenses of a word remain in the result; the prun-ing of unlikely senses is achieved in step 2, whenverb argument clusters are utilized.
Words thatare in the corpus but not covered by WordNetare also identified in this stage, and later at step3, we guess the meanings of these known wordsand place them into domain ontology.A total of 1,015 news reports on basketballgames (1.TMB, Clarinet news, 1990-1991) werecollected.
The frequency count reported totally1,414 unique nouns (proper names excluded)and 993 unique verbs in the corpus.
Comparedto 94,473 nouns and 10,318 verbs in WordNet1.6, only 1.5% of nouns and 9.6% of verbs areused in the domain.
As we can see, this firstpruning operation results in a significant reduc-tion of entries.
For the words in the domaincorpus, while some words appear much more of-ten (such as the verb score, which appear 3,141times in 1,015 reports, average 3.1 times perarticle), some appear rarely (for example, theverb atone only occur once in all reports).
Inpractical applications, low frequency words areusually not handled by a generation system, sothe reduction rate should be even higher.47 (3.3%) nouns and 22 (2.2%) verbs in thecorpus are not covered by WordNet.
Theseare domain specific words such as layup andlayin.
The small portion of these words showsthat WordNet is an appropriate general resourceto use as a basis for building domain lexiconsand ontologies since it will probably cover moatwords in a specific domain.
But the situationmight be different if the domain is very specific,for example, astronomy, in which case specifictechnical terms which are heavily used in thedomain might not be included in WordNet.
(A}/ \{S} .
.
./ \{C}{F}/ \{D}{E}{A}/ \{D} .
.
.before after(A) Synset A and D appear in the corpus,while B, C, E, and F do not .
(A}/ \ ?A){BY(C} ===> / \/ \  ~B}~D~{ D }~ E}before after(B) Synset B and D appear in the corpus,A, C, and E do not .
Note Synset A i s  notremoved s ince i t ' s  the c losest  ancestorof B and D.Figure 1: Examples fo r  corpus based prun ingStep 2.
P run ing  unrelevant senses us-ing verb argument  c lustersOur study in the basketball domain showsthat a word is typically used uniformly in aspecific domain, that is, it often has one or afew predominant senses in the domain, and fora verb, its arguments tend to be semanticallyclose to each other and belong to a single ora few more general semantic ategory.
In thefollowing, we show by an example how the uni-form usage of words in a domain can help toidentify predominant senses and obtain seman-tic constraints of verb arguments.In our basketball corpus, the verb add takesthe following set of words as objects: (rebound,assist, throw, shot, basket, points).
Based onthe assumption that a verb typically take argu-ments that belong to the same semantic ate-gory, we identify the senses of each word thatwill keep it connected to the largest number ofwords in the set.
For example, for the word re-bound, only one out of its three senses are linked130to other words in the set, so it is marked as thepredominant sense of the word in the domain.The algorithm we used to identify the predom-inant senses is similar to the algorithm we in-troduced in (Jing et al, 1997), which identi-ties predominant senses of words using domain-dependent semantic lassifications and Word-Net.
In this case, the set of arg~,ments for averb is considered as a semantic luster.
Thealgorithm can be briefly summarized as follows:Construct he set of arT,ments for a verbTraverse the WordNet hierarchy and lo-cate all the possible finks between sensesof words in the set.The predominant sense of a word is thesense which has the most n-tuber of finksto other words in the set.In this example, the words (rebound, assist,throw, shot, basket) will be disambiguated intothe sense that will make all of them fall into thesame semantic subtree in WordNet hierarchy, asshown in Figure 2.
The word points, however,does not belong to the same category and isnot disambiguated.
As we can see, the result ismuch further pruned compared to result fromstep 1, with 5 out of 6 words are now disam-biguated into a single sense.
At the mean while,we have also obtained semantic onstraints onverb arguments.
For this example, the object ofthe verb add can be classified into two semanticcategories: either points or the semantic ate-gory (accomplishment, achievement).
The clos-est common ancestor (accomplishment, achieve-ment) is used to generalize the semantic ate-gory of the arguments for a verb, even thoughthe word accomplishment and achievement arenot used in the domain.
This explains why instep I pruning, synsets that are the closest com-mon ancestor of two synsets in the domain arealways kept in the result.A simple parser is developed to extract sub-ject, object, and the main verb of a sentence.We then ran the algorithm described aboveand obtained selectional constraints for frequentverbs in the domain.
The results show that,for most of frequent verbs, majority of its argu-ments can be categorized into one or a few se-mantic categories, with only a small number of131exceptions.
Table 1 shows some frequent verbsin the domain and their selectional constraints.
{action}I{accomplishment, achievement)/ I I \(rebound} {assist} {throw) {basket}I{shot~Figure 2: Argument clusterfo r  the verb ' ' add ' '\?
?
oWORD FREQ SUBJ 0BJscore 789 player points (771)(789) basket (18)add 329 player points(accomplishment)l - reboundsI throwsI shotsI assists- basketshit 237 player (accomplishment)I -jumperI throwsI shots- basketsoutscore 45 team teambeat 11 team teamTable 1: Selectional Constraintsin Basketball DomainNote, the existing of predominant senses fora word in a domain does not mean every occur-rence of the word must have the predominantsense.
For example, although the verb hit isused mainly in the sense as in hitting a jumper,hitting a free throw in basketball domain, sen-tences like "The player fell and hit the floor"iIIII1IIdo appear in the corpus, although rarely.
Suchusage is not represented in our generalized se-lectional constraints on the verb arg~lments dueto its low frequency.Step 3.
Guessing unknown words andmerg ing with domain specific ontologies.The grouping of verb arguments can also helpus to guess the meaning of unknown words.For example, the word layup is often used asthe object of the verb hit, but is not listed inWordNet.
According to selectional constraintsfrom step 2, the object of the verb hit is typi-cally in the semantic category (accomplishment,achievement).
Therefore, we can guess that theword layup is probably in the semantic ategorytoo, though we do not know exactly where inthe semantic hierarchy of Figure 2 to place theword.We discussed above how to prune WordNet,whereas the other part of work in adaptingWordNet to a domain is to integrate domain-specific ontologies with pruned WordNet ontol-ogy.
There are a few possible operations to dothis: (1) Insertion.
For e~ample, in basketballdomain, if we have an ontology adapted fromWordNet by following step 1 and 2, and wealso have a specific hierarchy of basketball teamnames, a good way to combine them is to placethe hierarchy of team name under an appropri-ate node in WordNet hierarchy, such as the node(basketball team).
(2) Replacement.
For exam-ple, in medical domain, we need an ontology ofmedical disorders.
WordNet includes ome in-formation under the node "Medical disorder",but it might not be enough to satisfy the ap-plication's need.
If such information, however,can be obtained from a medical dictionary, wecan then substitute the subtree on "medical dis-order" in WordNet with the more complete andreliable hierarchy from a medical dictionary.
(3)Merging.
If WordNet and domain ontology con-tain information on the same topic, but knowl-edge from either side is incomplete, to get abetter ontology, we need to combine the two.We studied ontologies in five generation systemsin medical domain, telephone network planning,web log, basketball, and business domain.
Gen-erally, domain specific ontology can be easilymerged with WordNet by either insertion or re-placement operation.1324.2 Using the result for generat ionThe result we obtained after applying step 1 tostep 3 of the above method is a reduced Word-Net hierarchy, integrated with domain specificontology.
In addition, it is augmented with se-lection constraints and word frequency informa-tion acquired from corpus.
Now we discuss theusage of the result for generation.
* Lexical Paraphrases.
As we mentioned inSection 1, synsets can provide lexical para-phrases, the problem to be solved is deter-mining which words are interchangeable in aparticular context.
In our result, the wordsthat appear in a synset but axe not used inthe domain are eliminated by corpus analy-sis, so the words left in the synsets are basi-cally all applicable to the domain.
They can,however, be further distinguished by the se-lectional constraints.
For example, if A and Bare in the same synset but they have differentconstraints on their arguments, they are notinterchangeable.
Frequency can also be takeninto account.
A low frequency word should beavoided if there are other choices.
Words leftafter these restrictions can be considered asinterchangeable synonyms and used for para-phrasing.?
D iscr iminat ion net for lexical ization.The reduced WordNet hierarchy togetherwith selectional and frequency constraintsmade up a discrimination et for lexicaliza-tion.
The selection can be based on the gen-erality of the words, for example, a jumper isa kind of throw.
If a user wants the outputto be as detailed as possible, we can say "Hehit a jumper", otherwise we can say" "He hita throw.
"Selectional constraints can also be used inselecting words.
For example, both theword w/n and score can convey the mean-ing of obtaining advantages, gaining pointsetc, and w/n is a hypernym of score.
Inthe basketball domain, w/n is mainly used aswin(team, game), while score is mainly usedas score(player, points), so depending on thecategories of input arguments, we can choosebetween score and udn.Frequency can also be used in a way similar tothe above.
Although selectional constraintsand frequency are useful criteria for lexical se-lection, there are many other constraints thatcan be used in a generation system for select-ing words, for example, syntactic constraints,discourse, and focus etc.
These constraintsare usually coded in individual systems, notobtained from WordNet.Domain ontology.. From step 3, we canacquire a unified ontology by integrating thepruned WordNet hierarchy with domain spe-cific ontologies.
The unified ontology can thenbe used by planning and lexicalization com-ponents.
How different modules use the on-tology is a generation issue, which we will notaddress in the paper.4.3 Combining other types ofknowledge for generationAlthough WordNet contains rich lexical knowl-edge, its information on verb arg~lment s ruc-tures is relatively weak.
Also, while Word-Net is able to provide lexical paraphrases byits synsets, it can not provide syntactic para-phrases for generation.
Other resources suchas COMLEX syntax dictionary (Grishman etal., 1994) and English Verb Classes and Al-ternations(EVCA) (Levin, 1993) can provideverb subcategorization information and syntac-tic paraphrases, but they are indexed by wordsthus not suitable to use in generation directly.To augment WordNet with syntactic infor-mation, we combined three other resourceswith WordNet: COMLEX,  EVCA,  and TaggedBrown Corpus.
The resulting database containsnot only rich lexical knowledge, but also sub-stantial syntactic knowledge and language us-age information.
The combined database can beadapted to a specific domain using similar tech-niques as we introduced in this paper.
We ap-plied the combined lexicon to PLanDOC (McK-eown et al, 1994), apractical generation systemfor telephone network plaunlng.
Together witha flexible architecture we designed, the lexiconis able to effectively improve the system para-phrasing power, minimize the chance of gram-matical errors, and simplify the developmentprocess ubstantially.
The detailed escriptionof the combining process and the application ofthe lexicon is presented in (Jing and McKeown,199S).1335 Future  work and conclusionIn this paper, we demonstrate that WordNet isa valuable resource for generation: it can pro-duce large amount of paraphrases, provide se-mantic net for lexicalization, and can be usedfor building domain ontologies.The main problem we discussed is adaptingWordNet o a specific domain.
We propose athree step procedure based on corpus analysis tosolve the problem.
First, The general WordNetontology is pruned based on a domain corpus,then verb argument clusters are used to furtherprune the result, and finally, the pruned Word-Net hierarchy is integrated with domain specificontology to build a ,ni6ed ontology.
The otherproblems we discussed are how WordNet knowl-edge can be used in generation and how to aug-ment WordNet with other types of knowledge.In the future, we would like to test our tech-niques in other domains beside basketball, andapply such techniques to practical generationsystems.AcknowledgmentThis material is based upon work supported bythe National Science Foundation under GrantNo.
IRI 96-19124, IRI 96-18797 and by a grantfrom Columbia University's Strategic InitiativeFund.
Any opinions, findings, and conclusionsor recommendations expressed in this materialare those of the authors and do not necessarilyreflect the views of the National Science Foun-dation.ReferencesJ.A.
Bateman, R.T. Kasper, J.D.
Moore, andR.A.
Whitney.
1990.
A general organizationof knowledge for natural language processing:the penman upper-model.
Technical report,ISI, Marina del Rey, CA.Laurence Danlos.
1987.
The Linguistic Basisof Text Generation.
Cambridge UniversityPress.Ralph Grishman, Catherine Macleod, andAdam Meyers.
1994.
COMLEX syntax:Building a computational lexicon.
In Proceed-ings of COLING-94, Kyoto, Japan.Hongyan Jing and Kathleen McKeown.
1998.Combining multiple, large-scale resources ina reusable lexicon for natural anguage gen-eration.
In To appear in the Proceedingsof COLING-A CL'98, University of Montreal,Montreal, Canada, August.Hongyan Jing, Vasileio8 Hatzivassiloglou, Re-becca Passonnesu, and Kathleen McKeown.1997.
Investigating complementary methodsfor verb sense pruning.
In Proceedings ofANLP'97 ~ Semantics Workshop, ages58-65, Washint, l~n, D.C., April.Beth Levin.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago, Illinois.Kathleen McKeown, Karen Kuldch, and JamesShaw.
1994.
Practical issues in automaticdocumentation generation.
In Proceedingsof the Applied Natural Language ProcessingConference 9~, pages 7-14, Stuttgart, Ger-many, October.George A. Miller, R/chard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine J.Miller.
1990.
Introduction to WordNet: Anon-line lexical database.
International Jour-nal of Lezic.ogmphy (special issue), 3(4):235-312.134
