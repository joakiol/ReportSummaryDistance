Probabilistic Text Structuring: Experiments with Sentence OrderingMirella LapataDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 Portobello StreetSheffield S1 4DP, UKmlap@dcs.shef.ac.ukAbstractOrdering information is a critical task fornatural language generation applications.In this paper we propose an approach toinformation ordering that is particularlysuited for text-to-text generation.
We de-scribe a model that learns constraints onsentence order from a corpus of domain-specific texts and an algorithm that yieldsthe most likely order among several al-ternatives.
We evaluate the automaticallygenerated orderings against authored textsfrom our corpus and against human sub-jects that are asked to mimic the model?stask.
We also assess the appropriateness ofsuch a model for multidocument summa-rization.1 IntroductionStructuring a set of facts into a coherent text is anon-trivial task which has received much attentionin the area of concept-to-text generation (see Reiterand Dale 2000 for an overview).
The structured textis typically assumed to be a tree (i.e., to have a hier-archical structure) whose leaves express the contentbeing communicated and whose nodes specify howthis content is grouped via rhetorical or discourse re-lations (e.g., contrast, sequence, elaboration).For domains with large numbers of facts andrhetorical relations, there can be more than one pos-sible tree representing the intended content.
Thesedifferent trees will be realized as texts with differentsentence orders or even paragraph orders and differ-ent levels of coherence.
Finding the tree that yieldsthe best possible text is effectively a search prob-lem.
One way to address it is by narrowing downthe search space either exhaustively or heuristically.Marcu (1997) argues that global coherence can beachieved if constraints on local coherence are sat-isfied.
The latter are operationalized as weights onthe ordering and adjacency of facts and are derivedfrom a corpus of naturally occurring texts.
A con-straint satisfaction algorithm is used to find the treewith maximal weights from the space of all possi-ble trees.
Mellish et al (1998) advocate stochasticsearch as an alternative to exhaustively examiningthe search space.
Rather than requiring a global op-timum to be found, they use a genetic algorithm toselect a tree that is coherent enough for people tounderstand (local optimum).The problem of finding an acceptable order-ing does not arise solely in concept-to-text gener-ation but also in the emerging field of text-to-textgeneration (Barzilay, 2003).
Examples of applica-tions that require some form of text structuring, aresingle- and multidocument summarization as well asquestion answering.
Note that these applications donot typically assume rich semantic knowledge orga-nized in tree-like structures or communicative goalsas is often the case in concept-to-text generation.
Al-though in single document summarization the posi-tion of a sentence in a document can provide cueswith respect to its ordering in the summary, this isnot the case in multidocument summarization wheresentences are selected from different documents andmust be somehow ordered so as to produce a coher-ent summary (Barzilay et al, 2002).
Answering aquestion may also involve the extraction, potentiallysummarization, and ordering of information acrossmultiple information sources.Barzilay et al (2002) address the problem ofinformation ordering in multidocument summariza-tion and show that naive ordering algorithms suchas majority ordering (selects most frequent ordersacross input documents) and chronological ordering(orders facts according to publication date) do notalways yield coherent summaries although the latterproduces good results when the information is event-based.
Barzilay et al further conduct a study wheresubjects are asked to produce a coherent text fromthe output of a multidocument summarizer.
Their re-sults reveal that although the generated orders differfrom subject to subject, topically related sentencesalways appear together.
Based on the human studythey propose an algorithm that first identifies top-ically related groups of sentences and then ordersthem according to chronological information.In this paper we introduce an unsupervisedprobabilistic model for text structuring that learnsordering constraints from a large corpus.
The modeloperates on sentences rather than facts in a knowl-edge base and is potentially useful for text-to-textgeneration applications.
For example, it can be usedto order the sentences obtained from a multidocu-ment summarizer or a question answering system.Sentences are represented by a set of informativefeatures (e.g., a verb and its subject, a noun and itsmodifier) that can be automatically extracted fromthe corpus without recourse to manual annotation.The model learns which sequences of featuresare likely to co-occur and makes predictions con-cerning preferred orderings.
Local coherence is thusoperationalized by sentence proximity in the train-ing corpus.
Global coherence is obtained by greedilysearching through the space of possible orders.
As inthe case of Mellish et al (1998) we construct an ac-ceptable ordering rather than the best possible one.We propose an automatic method of evaluating theorders generated by our model by measuring close-ness or distance from the gold standard, a collectionof orders produced by humans.The remainder of this paper is organized as fol-lows.
Section 2 introduces our model and an algo-rithm for producing a possible order.
Section 3 de-scribes our corpus and the estimation of the modelparameters.
Our experiments are detailed in Sec-tion 4.
We conclude with a discussion in Section 5.2 Learning to OrderGiven a collection of texts from a particular domain,our task is to learn constraints on the ordering oftheir sentences.
In the training phase our model willlearn these constraints from adjacent sentences rep-resented by a set of informative features.
In the test-ing phase, given a set of unseen sentences, we willrely on our prior experience of how sentences areusually ordered for choosing the most likely order-ing.2.1 The ModelWe express the probability of a text made up of sen-tences S1 .
.
.Sn as shown in (1).
According to (1), thetask of predicting the next sentence is dependent onits n?
i previous sentences.P(T ) = P(S1 .
.
.Sn)= P(S1)P(S2jS1)P(S3jS1,S2) .
.
.P(SnjS1 ... Sn?1)=n?i=1P(SnjS1 .
.
.Sn?i)(1)We will simplify (1) by assuming that the prob-ability of any given sentence is determined only byits previous sentence:P(T ) = P(S1)P(S2jS1)P(S3jS2) .
.
.P(SnjSn?1)=n?i=1P(SijSi?1)(2)This is a somewhat simplistic attempt at cap-turing Marcu?s (1997) local coherence constraints aswell as Barzilay et al?s (2002) observations abouttopical relatedness.
While this is clearly a naive viewof text coherence, our model has some notion of thetypes of sentences that typically go together, eventhough it is agnostic about the specific rhetorical re-lations that glue sentences into a coherent text.
Alsonote that the simplification in (2) will make the es-timation of the probabilities P(SijSi?1) more reli-able in the face of sparse data.
Of course estimat-ing P(SijSi?1) would be impossible if Si and Si?1were actual sentences.
It is unlikely to find the ex-act same sentence repeated several times in a corpus.What we can find and count is the number of timesa given structure or word appears in the corpus.
Wewill therefore estimate P(SijSi?1) from features thatexpress its structure and content (these features aredescribed in detail in Section 3):P(SijSi?1) =P(hahi,1i,ahi,2i .
.
.ahi,niijhahi?1,1i,ahi?1,2i .
.
.ahi?1,mii)(3)where hahi,1i,ahi,2i .
.
.ahi,nii are features relevant forsentence Si and hahi?1,1i,ahi?1,2i .
.
.ahi?1,mii for sen-tence Si?1.
We will assume that these features areindependent and that P(SijSi?1) can be estimatedfrom the pairs in the Cartesian product definedover the features expressing sentences Si and Si?1:(ahi, ji,ahi?1,ki) 2 Si Si?1.
Under these assumptionsP(SijSi?1) can be written as follows:P(SijSi?1) = P(ahi,1ijahi?1,1i) .
.
.P(ahi,nijahi?1,mi)= ?
(ahi, ji,ahi?1,ki)2SiSi?1P(ahi, jijahi?1,ki)(4)Assuming that the features are independentagain makes parameter estimation easier.
The Carte-sian product over the features in Si and Si?1 is an at-tempt to capture inter-sentential dependencies.
SinceS1 : a b c dS2 : e f gS3 : h iFigure 1: Example of probability estimationwe don?t know a priori what the important featurecombinations are, we are considering all possiblecombinations over two sentences.
This will admit-tedly introduce some noise, given that some depen-dencies will be spurious, but the model can be easilyretrained for different domains for which differentfeature combinations will be important.
The proba-bility P(ahi, jijahi?1,ki) is estimated as:P(ahi, jijahi?1,ki) =f (ahi, ji,ahi?1,ki)?ahi, jif (ahi, ji,ahi?1,ki)(5)where f (ahi, ji,ahi?1,ki) is the number of times fea-ture ahi, ji is preceded by feature ahi?1,ki in thecorpus.
The denominator expresses the number oftimes ahi?1,ki is attested in the corpus (precededby any feature).
The probabilities P(ahi, jijahi?1,ki)will be unreliable when the frequency estimates forf (ahi, ji,ahi?1,ki) are small, and undefined in caseswhere the feature combinations are unattested in thecorpus.
We therefore smooth the observed frequen-cies using back-off smoothing (Katz, 1987).To illustrate with an example consider the textin Figure 1 which has three sentences S1, S2, S3,each represented by their respective features denotedby letters.
The probability P(S3jS2) will be calcu-lated by taking the product of P(hje), P(hj f ), P(hjg),P(ije), P(ij f ), and P(ijg).
To obtain P(hje), we needf (h,e) and f (e) which can be estimated in Figure 1by counting the number of edges connecting e andh and the number of edges starting from e, respec-tively.
So, P(hje) will be 0.16 given that f (h,e) isone and f (e) is six (see the normalization in (5)).2.2 Determining an OrderOnce we have collected the counts for our featureswe can determine the order for a new text thatwe haven?t encountered before, since some of thefeatures representing its sentences will be familiar.Given a text with N sentences there are N!
possi-ble orders.
The set of orders can be represented as acomplete graph, where the set of vertices V is equalto the set of sentences S and each edge u !
v hasa weight, the probability P(ujv).
Cohen et al (1999)STARTHHHHHHS1(0.2)HS2S3S3S2S2(0.3)HHS1(0.006)S3S3(0.02)S1S3(0.05)HS2S1S1S2Figure 2: Finding an order for a three sentence textshow that the problem of finding an optimal orderingthrough a directed weighted graph is NP-complete.Fortunately, they propose a simple greedy algorithmthat provides an approximate solution which can beeasily modified for our task (see also Barzilay et al2002).The algorithm starts by assigning each vertexv 2 V a probability.
Recall that in our case verticesare sentences and their probabilities can be calcu-lated by taking the product of the probabilities oftheir features.
The greedy algorithm then picks thenode with the highest probability and orders it aheadof the other nodes.
The selected node and its incidentedges are deleted from the graph.
Each remainingnode is now assigned the conditional probability ofseeing this node given the previously selected node(see (4)).
The node which yields the highest condi-tional probability is selected and ordered ahead.
Theprocess is repeated until the graph is empty.As an example consider again a three sentencetext.
We illustrate the search for a path through thegraph in Figure 2.
First we calculate which of thethree sentences S1, S2, and S3 is most likely to startthe text (during training we record which sentencesappear in the beginning of each text).
Assuming thatP(S2jSTART) is the highest, we will order S2 first,and ignore the nodes headed by S1 and S3.
We nextcompare the probabilities P(S1jS2) and P(S3jS2).Since P(S3jS2) is more likely than P(S1jS2), we or-der S3 after S2 and stop, returning the order S2, S3,and S1.
As can be seen in Figure 2 for each vertexwe keep track of the most probable edge that ends inthat vertex, thus setting th beam search width to one.Note, that equation (4) would assign lower andlower probabilities to sentences with large numbersof features.
Since we need to compare sentence pairswith varied numbers of features, we will normalizethe conditional probabilities P(SijSi?1) by the num-ber feature of pairs that form the Cartesian productover Si and Si?1.1.
Laidlaw Transportation Ltd. said shareholders will be asked at its Dec. 7 annual meeting to approve a change of name toLaidlaw Inc.2.
The company said its existing name hasn?t represented its businesses since the 1984 sale of its trucking operations.3.
Laidlaw is a waste management and school-bus operator, in which Canadian Pacific Ltd. has a 47% voting interest.Figure 3: A text from the BLLIP corpus3 Parameter EstimationThe model in Section 2.1 was trained on the BLLIPcorpus (30 M words), a collection of texts from theWall Street Journal (years 1987-89).
The corpus con-tains 98,732 stories.
The average story length is 19.2sentences.
71.30% of the texts in the corpus are lessthan 50 sentences long.
An example of the texts inthis newswire corpus is shown in Figure 3.The corpus is distributed in a Treebank-style machine-parsed version which was producedwith Charniak?s (2000) parser.
The parser is a?maximum-entropy inspired?
probabilistic gener-ative model.
It achieves 90.1% average preci-sion/recall for sentences with maximum length 40and 89.5% for sentences with maximum length 100when trained and tested on the standard sectionsof the Wall Street Journal Treebank (Marcus et al,1993).We also obtained a dependency-style versionof the corpus using MINIPAR (Lin, 1998) a broadcoverage parser for English which employs a manu-ally constructed grammar and a lexicon derived fromWordNet with an additional dictionary of propernames (130,000 entries in total).
The grammar isrepresented as a network of 35 nodes (i.e., grammat-ical categories) and 59 edges (i.e., types of syntactic(dependency) relations).
The output of MINIPAR is adependency graph which represents the dependencyrelations between words in a sentence (see Table 1for an example).
Lin (1998) evaluated the parser onthe SUSANNE corpus (Sampson, 1996), a domain in-dependent corpus of British English, and achieved arecall of 79% and precision of 89% on the depen-dency relations.From the two different parsed versions of theBLLIP corpus the following features were extracted:Verbs.
Investigations into the interpretation of nar-rative discourse (Asher and Lascarides, 2003) haveshown that specific lexical information (e.g., verbs,adjectives) plays an important role in determiningthe discourse relations between propositions.
Al-though we don?t have an explicit model of rhetoricalrelations and their effects on sentence ordering, wecapture the lexical inter-dependencies between sen-tences by focusing on verbs and their precedence re-lationships in the corpus.From the Treebank parses we extracted theverbs contained in each sentence.
We obtainedtwo versions of this feature: (a) a lemmatized ver-sion where verbs were reduced to their base formsand (b) a non-lemmatized version which preservedtense-related information; more specifically, verbalcomplexes (e.g., I will have been going) were iden-tified from the parse trees heuristically by devis-ing a set of 30 patterns that search for sequencesof modals, auxiliaries and verbs.
This is an attemptat capturing temporal coherence by encoding se-quences of events and their morphology which in-directly indicates their tense.To give an example consider the text in Fig-ure 3.
For the lemmatized version, sentence (1) willbe represented by say, will, be, ask, and approve; forthe tensed version, the relevant features will be said,will be asked, and to approve.Nouns.
Centering Theory (CT, Grosz et al 1995)is an entity-based theory of local coherence, whichclaims that certain entities mentioned in an utteranceare more central than others and that this propertyconstrains a speaker?s use of certain referring ex-pressions.
The principles underlying CT (e.g., conti-nuity, salience) are of interest to concept-to-text gen-eration as they offer an entity-based model of textand sentence planning which is particularly suitedfor descriptional genres (Kibble and Power, 2000).We operationalize entity-based coherence fortext-to-text generation by simply keeping track ofthe nouns attested in a sentence without howevertaking personal pronouns into account.
This simpli-fication is reasonable if one has text-to-text genera-tion mind.
In multidocument summarization for ex-ample, sentences are extracted from different docu-ments; the referents of the pronouns attested in thesesentences are typically not known and in some casesidentical pronouns may refer to different entities.
Somaking use of noun-pronoun or pronoun-pronounco-occurrences will be uninformative or in fact mis-leading.We extracted nouns from a lemmatized versionof the Treebank-style parsed corpus.
In cases of nouncompounds, only the compound head (i.e., rightmostnoun) was taken into account.
A small set of ruleswas used to identify organizations (e.g., United Lab-oratories Inc.), person names (e.g., Jose Y. Cam-pos), and locations (e.g., New England) spanningmore than one word.
These were grouped togetherand were also given the general categories person,organization, and location.
The model backs offto these categories when unknown person names, lo-cations, and organizations are encountered.
Dates,years, months and numbers were substituted by thecategories date, year, month, and number.In sentence (1) (see Figure 3) we identifythe nouns Laidlaw Transportation Ltd., shareholder,Dec 7, meeting, change, name and Laidlaw Inc. Insentence (2) the relevant nouns are company, name,business, 1984, sale, and operation.Dependencies.
Note that the noun and verb fea-tures do not capture the structure of the sentencesto be ordered.
This is important for our domain, astexts seem to be rather formulaic and similar syn-tactic structures are often used (e.g., direct and in-direct speech, restrictive relative clauses, predicativestructures).
In this domain companies typically saythings, and texts often begin with a statement of whata company or an individual has said (see sentence (1)in Figure 3).
Furthermore, companies and individu-als are described with certain attributes (persons canbe presidents or governors, companies are bankruptor manufacturers, etc.)
that can give clues for infer-ring coherence.The dependencies were obtained from the out-put of MINIPAR.
Some of the dependencies for sen-tence (2) from Figure 3 are shown in Table 1.
Thedependencies capture structural as well lexical infor-mation.
They are represented as triples, consisting ofa head (leftmost element, e.g., say, name), a modi-fier (rightmost element, e.g., company, its) and a re-lation (e.g., subject (V:subj:N), object (V:obj:N),modifier (N:mod:A)).For efficiency reasons we focused on tripleswhose dependency relations (e.g., V:subj:N) wereattested in the corpus with frequency larger thanone per million.
We further looked at how individ-ual types of relations contribute to the ordering task.More specifically we experimented with dependen-cies relating to verbs (49 types), nouns (52 types),verbs and nouns (101 types) (see Table 1 for exam-ples).
We also ran a version of our model with alltypes of relations, including adjectives, adverbs andVerb Nounsay V:subj:N company name N:gen:N itsrepresent V:subj:N name name N:mod:A existingrepresent V:have:have have business N:gen:N itsrepresent V:obj:N business business N:mod:Prep sincecompany N:det:Det theTable 1: Dependencies for sentence (2) in Figure 3A B C D E F G H I JModel 1 1 2 3 4 5 6 7 8 9 10Model 2 2 1 5 3 4 6 7 9 8 10Model 3 10 2 3 4 5 6 7 8 9 1Table 2: Example of rankings for a 10 sentence textprepositions (147 types in total).4 ExperimentsIn this section we describe our experiments with themodel and the features introduced in the previoussections.
We first evaluate the model by attemptingto reproduce the structure of unseen texts from theBLLIP corpus, i.e., the corpus on which the modelis trained on.
We next obtain an upper bound for thetask by conducting a sentence ordering experimentwith humans and comparing the model against thehuman data.
Finally, we assess whether this modelcan be used for multi-document summarization us-ing data from Barzilay et al (2002).
But before weoutline the details of our experiments we discuss ourchoice of metric for comparing different orders.4.1 Evaluation MetricOur task is to produce an ordering for the sentencesof a given text.
We can think of the sentences asobjects for which a ranking must be produced.
Ta-ble 2 gives an example of a text containing 10 sen-tences (A?J) and the orders (i.e., rankings) producedby three hypothetical models.A number of metrics can be used to measurethe distance between two rankings such as Spear-man?s correlation coefficient for ranked data, Cayleydistance, or Kendall?s ?
(see Lebanon and Lafferty2002 for details).
Kendall?s ?
is based on the numberof inversions in the rankings and is defined in (6):(6) ?
= 1?
2(number of inversions)N(N ?1)/2where N is the number of objects (i.e., sentences)being ranked and inversions are the number of in-terchanges of consecutive elements necessary to ar-range them in their natural order.
If we think in termsof permutations, then ?
can be interpreted as the min-imum number of adjacent transpositions needed tobring one order to the other.
In Table 2 the numberof inversions can be calculated by counting the num-ber of intersections of the lines.
The metric rangesfrom ?1 (inverse ranks) to 1 (identical ranks).
The ?for Model 1 and Model 2 in Table 2 is .822.Kendall?s ?
seems particularly appropriate forthe tasks considered in this paper.
The metric is sen-sitive to the fact that some sentences may be alwaysordered next to each other even though their absoluteorders might differ.
It also penalizes inverse rank-ings.
Comparison between Model 1 and Model 3would give a ?
of 0.244 even though the orders be-tween the two models are identical modulo the be-ginning and the end.
This seems appropriate giventhat flipping the introduction in a document with theconclusions seriously disrupts coherence.4.2 Experiment 1: Ordering Newswire TextsThe model from Section 2.1 was trained on theBLLIP corpus and tested on 20 held-out randomlyselected unseen texts (average length 15.3).
We alsoused 20 randomly chosen texts (disjoint from thetest data) for development purposes (average length16.2).
All our results are reported on the test set.The input to the the greedy algorithm (see Sec-tion 2.2) was a text with a randomized sentence or-dering.
The ordered output was compared againstthe original authored text using ?.
Table 3 gives theaverage ?
(T ) for all 20 test texts when the fol-lowing features are used: lemmatized verbs (VL),tensed verbs (VT ), lemmatized nouns (NL), lem-matized verbs and nouns (VLNL), tensed verbs andlemmatized nouns (VT NL), verb-related dependen-cies (VD), noun-related dependencies (ND), verb andnoun dependencies (VDND), and all available de-pendencies (AD).
For comparison we also report thenaive baseline of generating a random oder (BR).
Ascan be seen from Table 3 the best performing fea-tures are NL and VDND.
This is not surprising giventhat NL encapsulates notions of entity-based coher-ence, which is relatively important for our domain.
Alot of texts are about a particular entity (company orindividual) and their properties.
The feature VDNDsubsumes several other features and does expectedlybetter: it captures entity-based coherence, the inter-relations among verbs, the structure of sentences andalso preserves information about argument structure(who is doing what to whom).
The distance betweenthe orders produced by the model and the originaltexts increases when all types of dependencies areFeature T StdDev Min MaxBR .35 .09 .17 .47VL .44 .24 .17 .93VT .46 .21 .17 .80NL .54 .16 .18 .76VLNL .46 .12 .18 .61VT NL .49 .17 .21 .86VD .51 .17 .10 .83ND .45 .17 .10 .67VDND .57 .12 .62 .83AD .48 .17 .10 .83Table 3: Comparison between original BLLIP textsand model generated variantstaken into account.
The feature space becomes toobig, there are too many spurious feature pairs, andthe model can?t distinguish informative from non-informative features.We carried out a one-way Analysis of Vari-ance (ANOVA) to examine the effect of different fea-ture types.
The ANOVA revealed a reliable effectof feature type (F(9,171) = 3.31; p < 0.01).
Weperformed Post-hoc Tukey tests to further examinewhether there are any significant differences amongthe different features and between our model andthe baseline.
We found out that NL, VT NL, VD, andVDND are significantly better than BR (?
= 0.01),whereas NL and VDND are not significantly differ-ent from each other.
However, they are significantlybetter than all other features (?
= 0.05).4.3 Experiment 2: Human EvaluationIn this experiment we compare our model?s perfor-mance against human judges.
Twelve texts were ran-domly selected from the 20 texts in our test data.
Thetexts were presented to subjects with the order oftheir sentences scrambled.
Participants were askedto reorder the sentences so as to produce a coherenttext.
Each participant saw three texts randomly cho-sen from the pool of 12 texts.
A random order of sen-tences was generated for every text the participantssaw.
Sentences were presented verbatim, pronounsand connectives were retained in order to make or-dering feasible.
Notice that this information is absentfrom the features the model takes into account.The study was conducted remotely over the In-ternet using a variant of Barzilay et al?s (2002) soft-ware.
Subjects first saw a set of instructions that ex-plained the task, and had to fill in a short question-naire including basic demographic information.
Theexperiment was completed by 137 volunteers (ap-proximately 33 per text), all native speakers of En-glish.
Subjects were recruited via postings to localFeature T StdDev Min MaxVL .45 .16 .10 .90VT .46 .18 .10 .90NL .51 .14 .10 .90VLNL .44 .14 .18 .61VT NL .49 .18 .21 .86VD .47 .14 .10 .93ND .46 .15 .10 .86VDND .55 .15 .10 .90AD .48 .16 .10 .83HH .58 .08 .26 .75Table 4: Comparison between orderings produced byhumans and the model on BLLIP textsFeatures T StdDev Min MaxBR .43 .13 .19 .97NL .48 .16 .21 .86VDND .56 .13 .32 .86HH .60 .17 ?1 .98Table 5: Comparison between orderings produced byhumans and the model on multidocument summariesUsenet newsgroups.Table 4 reports pairwise ?
averaged over12 texts for all participants (HH) and the average ?between the model and each of the subjects for allfeatures used in Experiment 1.
The average distancein the orderings produced by our subjects is .58.
Thedistance between the humans and the best featuresis .51 for NL and .55 for VDND.
An ANOVA yieldeda significant effect of feature type (F(9,99) = 5.213;p < 0.01).
Post-hoc Tukey tests revealed that VL,VT , VD, ND, AD, VLNL, and VT NL perform sig-nificantly worse than HH (?
= 0.01), whereas NLand VDND are not significantly different from HH(?
= 0.01).
This is in agreement with Experiment 1and points to the importance of lexical and structuralinformation for the ordering task.4.4 Experiment 3: SummarizationBarzilay et al (2002) collected a corpus of multipleorderings in order to study what makes an order co-hesive.
Their goal was to improve the ordering strat-egy of MULTIGEN (McKeown et al, 1999) a mul-tidocument summarization system that operates onnews articles describing the same event.
MULTIGENidentifies text units that convey similar informationacross documents and clusters them into themes.Each theme is next syntactically analysed into pred-icate argument structures; the structures that are re-peated often enough are chosen to be included intothe summary.
A language generation system outputsa sentence (per theme) from the selected predicateargument structures.Barzilay et al (2002) collected ten sets of arti-cles each consisting of two to three articles reportingthe same event and simulated MULTIGEN by man-ually selecting the sentences to be included in thefinal summary.
This way they ensured that order-ings were not influenced by mistakes their systemcould have made.
Explicit references and connec-tives were removed from the sentences so as not toreveal clues about the sentence ordering.
Ten sub-jects provided orders for each summary which hadan average length of 8.8.We simulated the participants?
task by using themodel from Section 2.1 to produce an order for eachcandidate summary1.
We then compared the differ-ences in the orderings generated by the model andparticipants using the best performing features fromExperiment 2 (i.e., NL and VDND).
Note that themodel was trained on the BLLIP corpus, whereas thesentences to be ordered were taken from news arti-cles describing the same event.
Not only were thenews articles unseen but also their syntactic struc-ture was unfamiliar to the model.
The results areshown in table 5, again average pairwise ?
is re-ported.
We also give the naive baseline of choosinga random order (BR).
The average distance in theorderings produced by Barzilay et al?s (2002) par-ticipants is .60.
The distance between the humansand NL is .48 whereas the average distance betweenVDND and the humans is .56.
An ANOVA yielded asignificant effect of feature type (F(3,27) = 15.25;p < 0.01).
Post-hoc Tukey tests showed that VDNDwas significantly better than BR, but NL wasn?t.
Thedifference between VDND and HH was not signifi-cant.Although NL performed adequately in Experi-ments 1 and 2, it failed to outperform the baseline inthe summarization task.
This may be due to the factthat entity-based coherence is not as important astemporal coherence for the news articles summaries.Recall that the summaries describe events acrossdocuments.
This information is captured more ad-equately by VDND and not by NL that only keeps arecord of the entities in the sentence.5 DiscussionIn this paper we proposed a data intensive approachto text coherence where constraints on sentence or-dering are learned from a corpus of domain-specific1The summaries as well as the human data are available fromhttp://www.cs.columbia.edu/?noemie/ordering/.texts.
We experimented with different feature encod-ings and showed that lexical and syntactic informa-tion is important for the ordering task.
Our resultsindicate that the model can successfully generate or-ders for texts taken from the corpus on which it istrained.
The model also compares favorably with hu-man performance on a single- and multiple docu-ment ordering task.Our model operates on the surface level ratherthan the logical form and is therefore suitable fortext-to-text generation systems; it acquires orderingconstraints automatically, and can be easily ported todifferent domains and text genres.
The model is par-ticularly relevant for multidocument summarizationsince it could provide an alternative to chronolog-ical ordering especially for documents where pub-lication date information is unavailable or uninfor-mative (e.g., all documents have the same date).
Weproposed Kendall?s ?
as an automated method forevaluating the generated orders.There are a number of issues that must be ad-dressed in future work.
So far our evaluation metricmeasures order similarities or dissimilarities.
Thisenables us to assess the importance of particularfeature combinations automatically and to evaluatewhether the model and the search algorithm gener-ate potentially acceptable orders without having torun comprehension experiments each time.
Such ex-periments however are crucial for determining howcoherent the generated texts are and whether theyconvey the same semantic content as the originallyauthored texts.
For multidocument summarizationcomparisons between our model and alternative or-dering strategies are important if we want to pursuethis approach further.Several improvements can take place with re-spect to the model.
An obvious question is whethera trigram model performs better than the modelpresented here.
The greedy algorithm implementsa search procedure with a beam of width one.
Inthe future we plan to experiment with larger widths(e.g., two or three) and also take into account fea-tures that express semantic similarities across docu-ments either by relying on WordNet or on automaticclustering methods.AcknowledgmentsThe author was supported by EPSRC grant number R40036.
Weare grateful to Regina Barzilay and Noemie Elhadad for makingavailable their software and for providing valuable commentson this work.
Thanks also to Stephen Clark, Nikiforos Kara-manis, Frank Keller, Alex Lascarides, Katja Markert, and MilesOsborne for helpful comments and suggestions.ReferencesAsher, Nicholas and Alex Lascarides.
2003.
Logics of Conver-sation.
Cambridge University Press.Barzilay, Regina.
2003.
Information Fusion for Multi-Document Summarization: Praphrasing and Generation.Ph.D.
thesis, Columbia University.Barzilay, Regina, Noemie Elhadad, and Kathleen R. McKeown.2002.
Inferring strategies for sentence ordering in multidoc-ument news summarization.
Journal of Artificial IntelligenceResearch 17:35?55.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Conference of the North AmericanChapter of the Association for Computational Linguistics.Seattle, WA, pages 132?139.Cohen, William W., Robert E. Schapire, and Yoram Singer.1999.
Learning to order things.
Journal of Artificial Intelli-gence Research 10:243?270.Grosz, Barbara, Aravind Joshi, , and Scott Weinstein.
1995.Centering: A framework for modeling the local coherenceof discourse.
Computational Linguistics 21(2):203?225.Katz, Slava M. 1987.
Estimation of probabilities from sparsedata for the language model component of a speech recog-nizer.
IEEE Transactions on Acoustics Speech and SignalProcessing 33(3):400?401.Kibble, Rodger and Richard Power.
2000.
An integrated frame-work for text planning and pronominalisation.
In In Pro-ceedings of the 1st International Conference on Natural Lan-guage Generation.
Mitzpe Ramon, Israel, pages 77?84.Lebanon, Guy and John Lafferty.
2002.
Combining rankingsusing conditional probability models on permutations.
InC. Sammut and A. Hoffmann, editors, In Proceedings of the19th International Conference on Machine Learning.
Mor-gan Kaufmann Publishers, San Francisco, CA.Lin, Dekang.
1998.
Dependency-based evaluation of MINIPAR.In In Proceedings on of the LREC Workshop on the Evalua-tion of Parsing Systems.
Granada, pages 48?56.Marcu, Daniel.
1997.
From local to global coherence: Abottom-up approach to text planning.
In In Proceedings ofthe 14th National Conference on Artificial Intelligence.
Prov-idence, Rhode Island, pages 629?635.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof english: The penn treebank.
Computational Linguistics19(2):313?330.McKeown, Kathleen R., Judith L. Klavans, Vasileios Hatzivas-siloglou, Regina Barzilay, and Eleazar Eskin.
1999.
Towardsmultidocument summarization by reformulation: Progressand prospects.
In Proceedings of the 16th National Confer-ence on Artificial Intelligence.
Orlando, FL, pages 453?459.Mellish, Chris, Alistair Knott, Jon Oberlander, and Mick O?Donnell.
1998.
Experiments using stochastic search for textplanning.
In In Proceedings of the 9th International Work-shop on Natural Language Generation.
Ontario, Canada,pages 98?107.Reiter, Ehud and Robert Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge University Press,Cambridge.Sampson, Geoffrey.
1996.
English for the Computer.
OxfordUniversity Press.
