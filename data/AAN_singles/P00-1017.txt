Using existing systems to supplement small amounts ofannotated grammatical relations training data?Alexander YehMitre Corp.202 Burlington Rd.Bedford, MA 01730USAasy@mitre.orgAbstractGrammatical relationships (GRs)form an important level of natu-ral language processing, but dier-ent sets of GRs are useful for dier-ent purposes.
Therefore, one may of-ten only have time to obtain a smalltraining corpus with the desired GRannotations.
To boost the perfor-mance from using such a small train-ing corpus on a transformation rulelearner, we use existing systems thatnd related types of annotations.1 IntroductionGrammatical relationships (GRs), which in-clude arguments (e.g., subject and object) andmodiers, form an important level of naturallanguage processing.
Examples of GRs in thesentenceToday, my dog pushed the ball on the oor.are pushed having the subject my dog, theobject the ball and the time modier To-day, and the ball having the location modieron (the oor).
The resulting annotation ismy dog ?subj?
pushedon ?mod-loc?
the ball?This paper reports on work performed at theMITRE Corporation under the support of the MITRESponsored Research Program.
Marc Vilain providedthe motivation to nd GRs.
Warren Grei suggestedusing randomization-type techniques to determine sta-tistical signicance.
Sabine Buchholz and John Car-roll ran their GR nding systems over our data for theexperiments.
Jun Wu provided some helpful explana-tions.
Christine Doran and John Henderson providedhelpful editing.
Three anonymous reviewers providedhelpful suggestions.etc.
GRs are the objects of study in rela-tional grammar (Perlmutter, 1983).
In theSPARKLE project (Carroll et al, 1997), GRsform the top layer of a three layer syntaxscheme.
Many systems (e.g., the KERNELsystem (Palmer et al, 1993)) use GRs as anintermediate form when determining the se-mantics of syntactically parsed text.
GRs areoften stored in structures similar to the F-structures of lexical-functional grammar (Ka-plan, 1994).A complication is that dierent sets of GRsare useful for dierent purposes.
For exam-ple, Ferro et al (1999) is interested in seman-tic interpretation, and needs to dierentiatebetween time, location and other modiers.The SPARKLE project (Carroll et al, 1997),on the other hand, does not dierentiate be-tween these types of modiers.
As has beenmentioned by John Carroll (personal commu-nication), combining modier types togetheris ne for information retrieval.
Also, havingless dierentiation of the modiers can makeit easier to nd them (Ferro et al, 1999).Furthermore, unless the desired set of GRsmatches the set aleady annotated in somelarge training corpus,1one will have to eithermanually write rules to nd the GRs, as donein A?t-Mokhtar and Chanod (1997), or anno-tate a new training corpus for the desired set.Manually writing rules is expensive, as is an-notating a large corpus.Often, one may only have the resources toproduce a small annotated training set, andmany of the less common features of the set's1One example is a memory-based GR nder (Buch-holz et al, 1999) that uses the GRs annotated in thePenn Treebank (Marcus et al, 1993).domain may not appear at all in that set.In contrast are existing systems that performwell (probably due to a large annotated train-ing set or a set of carefully hand-crafted rules)on related (but dierent) annotation stan-dards.
Such systems will cover many moredomain features, but because the annotationstandards are slightly dierent, some of thosefeatures will be annotated in a dierent waythan in the small training and test set.A way to try to combine the dierent advan-tages of these small training data sets and ex-isting systems which produce related annota-tions is to use a sequence of two systems.
Werst use an existing annotation system whichcan handle many of the less common features,i.e., those which do not appear in the smalltraining set.
We then train a second systemwith that same small training set to take theoutput of the rst system and correct for thedierences in annotations.
This approach wasused by Palmer (1997) for word segmentation.Hwa (1999) describes a somewhat similar ap-proach for nding parse brackets which com-bines a fully annotated related training dataset and a large but incompletely annotated -nal training data set.
Both these works dealwith just one (word boundary) or two (startand end parse bracket) annotation label typesand the same label types are used in both theexisting annotation system/training set andthe nal (small) training set.
In compari-son, our work handles many annotation la-bel types, and the translation from the typesused in the existing annotation system to thetypes in the small training set tends to be bothmore complicated and most easily determinedby empirical means.
Also, the type of baselinescore being improved upon is dierent.
Ourwork adds an existing system to improve therules learned, while Palmer (1997) adds rulesto improve an existing system's performance.We use this related system/small trainingset combination to improve the performanceof the transformation-based error-drivenlearner described in Ferro et al (1999).
Sofar, this learner has started with a blankinitial labeling of the GRs.
This paperdescribes experiments where we replace thisblank initial labeling with the output froman existing GR nder that is good at asomewhat dierent set of GR annotations.With each of the two existing GR nders thatwe use, we obtained improved results, withthe improvement being more noticeable whenthe training set is smaller.We also nd that the existing GR ndersare quite uneven on how they improve the re-sults.
They each tend to concentrate on im-proving the recovery of a few kinds of rela-tions, leaving most of the other kinds alone.We use this tendency to further boost thelearner's performance by using a merger ofthese existing GR nders' output as the initiallabeling.2 The ExperimentWe now improve the performance of theFerro et al (1999) transformation rulelearner on a small annotated training set byusing an existing system to provide initialGR annotations.
This experiment is repeatedon two dierent existing systems, whichare reported in Buchholz et al (1999) andCarroll et al (1999), respectively.Both of these systems nd a somewhatdierent set of GR annotations than theone learned by the Ferro et al (1999) sys-tem.
For example, the Buchholz et al (1999)system ignores verb complements of verbsand is designed to look for relationshipsto verbs and not GRs that exist betweennouns, etc.
This system also handlesrelative clauses dierently.
For example,in Miller, who organized ..., this system istrained to indicate that who is the subjectof organized, while the Ferro et al (1999)system is trained to indicate that Milleris the subject of organized.
As for theCarroll et al (1999) system, among otherthings, it does not distinguish between sub-types of modiers such as time, location andpossessive.
Also, both systems handle copu-las (usually using the verb to be) dierentlythan in Ferro et al (1999).2.1 Experiment Set-UpAs described in Ferro et al (1999), the trans-formation rule learner starts with a p-o-stagged corpus that has been chunked intonoun chunks, etc.
The starting state also in-cludes imperfect estimates of pp-attachmentsand a blank set of initial GR annotations.In these experiments, this blank initial setis changed to be a translated version of theannotations produced by an existing system.This is how the existing system transmitswhat it found to the rule learner.
The set-up for this experiment is shown in gure 1.The four components with + signs are takenout when one wants the transformation rulelearner to start with a blank set of initial GRannotations.The two arcs in that gure with a * indicatewhere the translations occur.
These transla-tions of the annotations produced by the ex-isting system are basically just an attempt tomap each type of annotation that it producesto the most likely type of corresponding an-notation used in the Ferro et al (1999) sys-tem.
For example, in our experiments, theBuchholz et al (1999) system uses the anno-tation np-sbj to indicate a subject, while theFerro et al (1999) system uses the annota-tion subj.
We create the mapping by ex-amining the training set to be given to theFerro et al (1999) system.
For each type ofrelation eioutput by the existing system whengiven the training set text, we look at whatrelation types (which tk's) co-occur with eiinthe training set.
We look at the tk's with thehighest number of co-occurrences with thatei.
If that tkis unique (no ties for the highestnumber of co-occurrences) and translating eito that tkgenerates at least as many correctannotations in the training set as false alarms,then make that translation.
Otherwise, trans-late eito no relation.
This latter translationis not uncommon.
For example, in one run ofour experiments, 9% of the relation instancesin the training set were so translated, in an-other run, 46% of the instances were so trans-lated.Some relations in the Carroll et al (1999)system are between three or four elements.These relations are each rst translated intoa set of two element sub-relations before theexamination process above is performed.Even before applying the rules, the trans-lations nd many of the desired annotations.However, the rules can considerably improvewhat is found.
For example, in two of ourearly experiments, the translations by them-selves produced F-scores (explained below)of about 40% to 50%.
After the learnedrules were applied, those F-scores increasedto about 70%.An alternative to performing translations isto use the untranslated initial annotations asan additional type of input to the rule sys-tem.
This alternative, which we have yetto try, has the advantage of tting into thetransformation-based error-driven paradigm(Brill and Resnik, 1994) more cleanly thanhaving a translation stage.
However, this ad-ditional type of input will also further slow-down an already slow rule-learning module.2.2 Overall ResultsFor our experiment, we use the same1151 word (748 GR) test set used inFerro et al (1999), but for a training set, weuse only a subset of the 3299 word training setused in Ferro et al (1999).
This subset con-tains 1391 (71%) of the 1963 GR instances inthe original training set.
The overall resultsfor the test set areSmaller Training Set, Overall ResultsR P F ERIaC 478 (63.9%) 77.2% 69.9% 7.7%IaB 466 (62.3%) 78.1% 69.3% 5.8%NI 448 (59.9%) 77.1% 67.4%where row IaB is the result of using the ruleslearned when the Buchholz et al (1999) sys-tem's translated GR annotations are usedas the Initial Annotations, row IaC is thesimilar result with the Carroll et al (1999)system, and row NI is the result of usingthe rules learned when No Initial GR an-notations are used (the rule learner as runin Ferro et al (1999)).
R(ecall) is the num-ber (and percentage) of the keys that arerecalled.
P(recision) is the number of cor-??
?existing system+??
?existing system+test set???????????????????????????????????????????
?small training setrule learnerkey GR annotations for small training set**rules+GR annotationsinitial test+initial trainingGR annotationsnal testGR annotationsrule interpreterFigure 1: Set-up to use an existing system to improve performancerectly recalled keys divided by the num-ber of GRs the system claims to exist.F(-score) is the harmonic mean of recall (r)and precision (p) percentages.
It equals2pr/(p + r).
ER stands for Error Reduc-tion.
It indicates how much adding the ini-tial annotations reduced the missing F-score,where the missing F-score is 100%?F.
ER=100%?
(FIA?FNI)/(100%?FNI), where FNIis the F-score for the NI row, and FIAis theF-score for using the Initial Annotations ofinterest.
Here, the dierences in recall and F-score between NI and either IaB or IaC (butnot between IaB and IaC) are statistically sig-nicant.
The dierences in precision is not.2In these results, most of the modest F-scoregain came from increasing recall.One may note that the error reductions hereare smaller than Palmer (1997)'s error reduc-tions.
Besides being for dierent tasks (wordsegmentation versus GRs), the reductions arealso computed using a dierent type of base-line.
In Palmer (1997), the baseline is howwell an existing system performs before therules are run.
In this paper, the baseline isthe performance of the rules learned without2When comparing dierences in this paper, thestatistical signicance of the higher score being bet-ter than the lower score is tested with a one-sidedtest.
Dierences deemed statistically signicant aresignicant at the 5% level.
Dierences deemed non-statistically signicant are not signicant at the 10%level.
For recall, we use a sign test for matched-pairs(Harnett, 1982, Sec.
15.5).
For precision and F-score,a matched-pairs randomization test (Cohen, 1995,Sec.
5.3) is used.rst using an existing system.
If we were touse the same baseline as Palmer (1997), ourbaseline would be an F of 37.5% for IaB and52.6% for IaC.
This would result in a muchhigher ER of 51% and 36%, respectively.We now repeat our experiment with thefull 1963 GR instance training set.
These re-sults indicate that as a small training set getslarger, the overall results get better and theinitial annotations help less in improving theoverall results.
So the initial annotations aremore helpful with smaller training sets.
Theoverall results on the test set areFull Training Set, Overall ResultsR P F ERIaC 487 (65.1%) 79.7% 71.7% 6.3%IaB 486 (65.0%) 76.5% 70.3% 1.7%NI 476 (63.6%) 77.3% 69.8%The dierences in recall, etc.
between IaB andNI are now small enough to be not statisti-cally signicant.
The dierences between IaCand NI are statistically signicant,3but thedierence in both the absolute F-score (1.9%versus 2.5% with the smaller training set) andER (6.3% versus 7.7%) has decreased.2.3 Results by RelationThe overall result of using an existing systemis a modest increase in F-score.
However, thisincrease is quite unevenly distributed, with a3The recall dierence is semi-signicant, being sig-nicant at the 10% level.few relation(s) having a large increase, andmost relations not having much of a change.Dierent existing systems seem to have dier-ent relations where most of the increase oc-curs.As an example, take the results of usingthe Buchholz et al (1999) system on the 1391GR instance training set.
Many GRs, like pos-sessive modier, are not aected by the addedinitial annotations.
Some GRs, like locationmodier, do slightly better (as measured bythe F-score) with the added initial annota-tions, but some, like subject, do better with-out.
With GRs like subject, some dierencesbetween the initial and desired annotationsmay be too subtle for the Ferro et al (1999)system to adjust for.
Or those dierences maybe just due to chance, as the result dierencesin those GRs are not statistically signicant.The GRs with statistically signicant resultdierences are the time and other4modiers,where adding the initial annotations helps.The time modier5results are quite dierent:Smaller Training Set, Time ModiersR P F ERIaB 29 (64.4%) 80.6% 71.6% 53%NI 14 (31.1%) 56.0% 40.0%The dierence in the number recalled (15) forthis GR accounts for nearly the entire dier-ence in the overall recall results (18).
The re-call, precision and F-score dierences are allstatistically signicant.Similarly, when using theCarroll et al (1999) system on this trainingset, most GRs are not aected, while othersdo slightly better.
The only GR with a sta-tistically signicant result dierence is object,where again adding the initial annotationshelps:Smaller Training Set, Object RelationsR P F ERIaC 198 (79.5%) 79.5% 79.5% 17%NI 179 (71.9%) 78.9% 75.2%The dierence in the number recalled (19) forthis GR again accounts for most of the dif-4Modiers that do not fall into any of the subtypesused, such as time, location, possessive, etc.
Examplesof unused subtypes are purpose and modality.5There are 45 instances in the test set key.ference in the overall recall results (30).
Therecall and F-score dierences are statisticallysignicant.
The precision dierence is not.As one changes from the smaller 1391 GRinstance training set to the larger 1963 GRinstance training set, these F-score improve-ments become smaller.
When using theBuchholz et al (1999) system, the improve-ment in the other modier is now no longerstatistically signicant.
However, the timemodier F-score improvement stays statisti-cally signicant:Full Training Set, Time ModiersR P F ERIaB 29 (64.4%) 74.4% 69.0% 46%NI 15 (33.3%) 57.7% 42.3%When using the Carroll et al (1999) system,the object F-score improvement stays statisti-cally signicant:Full Training Set, Object RelationsR P F ERIaC 194 (77.9%) 85.1% 81.3% 16%NI 188 (75.5%) 80.3% 77.8%2.4 Combining Sets of InitialAnnotationsSo the initial annotations from dierent ex-isting systems tend to each concentrate onimproving the performance of dierent GRtypes.
From this observation, one may wonderabout combining the annotations from thesedierent systems in order to increase the per-formance on all the GR types aected by thosedierent existing systems.Various works (van Halteren et al, 1998;Henderson and Brill, 1999; Wilkes andStevenson, 1998) on combining dierent sys-tems exist.
These works use one or both oftwo types of schemes.
One is to have thedierent systems simply vote.
However, thisdoes not really make use of the fact that dif-ferent systems are better at handling dier-ent GR types.
The other approach uses acombiner that takes the systems' output asinput and may perform such actions as de-termining which system to use under whichcircumstance.
Unfortunately, this approachneeds extra training data to train such a com-biner.
Such data may be more useful whenused instead as additional training data forthe individual methods that one is consider-ing to combine, especially when the systemsbeing combined were originally given a smallamount of training data.To avoid the disadvantages of these existingschemes, we came up with a third method.We combine the existing related systems bytaking a union of their translated annota-tions as the new initial GR annotation forour system.
We rerun rule learning on thesmaller (1391 GR instance) training set witha Union of the Buchholz et al (1999) andCarroll et al (1999) systems' translated GRannotations.
The overall results for the testset are (shown in row IaU)Smaller Training Set, Overall ResultsR P F ERIaU 496 (66.3%) 76.4% 71.0% 11%IaC 478 (63.9%) 77.2% 69.9% 7.7%IaB 466 (62.3%) 78.1% 69.3% 5.8%NI 448 (59.9%) 77.1% 67.4%where the other rows are as shown in Sec-tion 2.2.
Compared to the F-score withusing Carroll et al (1999) (IaC), the IaUF-score is borderline statistically signi-cantly better (11% signicance level).
TheIaU F-score is statistically signicantly bet-ter than the F-scores with either usingBuchholz et al (1999) (IaB) or not using anyinitial annotations (NI).As expected, most (42 of 48) of the overallincrease in recall going from NI to IaU comesfrom increasing the recall of the object, timemodier and other modier relations, the re-lations that IaC and IaB concentrate on.
TheER for object is 11% and for time modier is56%.When this combining approach is repeatedthe full 1963 GR instance training set, theoverall results for the test set areFull Training Set, Overall ResultsR P F ERIaU 502 (67.1%) 77.7% 72.0% 7.3%IaC 487 (65.1%) 79.7% 71.7% 6.3%IaB 486 (65.0%) 76.5% 70.3% 1.7%NI 476 (63.6%) 77.3% 69.8%Compared to the smaller training set results,the dierence between IaU and IaC here issmaller for both the absolute F-score (0.3%versus 1.1%) and ER (1.0% versus 3.3%).
Infact, the F-score dierence is small enough tonot be statistically signicant.
Given the pre-vious results for IaC and IaB as a small train-ing set gets larger, this is not surprising.3 DiscussionGRs are important, but dierent sets of GRsare useful for dierent purposes and dierentsystems are better at nding certain types ofGRs.
Here, we have been looking at ways ofimproving automatic GR nders when one hasonly a small amount of data with the desiredGR annotations.
In this paper, we improvethe performance of the Ferro et al (1999) GRtransformation rule learner by using existingsystems to nd related sets of GRs.
The out-put of these systems is used to supply ini-tial sets of annotations for the rule learner.We achieve modest gains with the existingsystems tried.
When one examines the re-sults, one notices that the gains tend to beuneven, with a few GR types having largegains, and the rest not being aected much.The dierent systems concentrate on improv-ing dierent GR types.
We leverage this ten-dency to make a further modest improvementin the overall results by providing the rulelearner with the merged output of these ex-isting systems.
We have yet to try other waysof combining the output of existing systemsthat do not require extra training data.
Onepossibility is the example-based combiner inBrill and Wu (1998, Sec.
3.2).6Furthermore,nding additional existing systems to add tothe combination may further improve the re-sults.ReferencesS.
A?t-Mokhtar and J.-P. Chanod.
1997.
Subjectand object dependency extraction using nite-state transducers.
In Proc.
ACL workshop onautomatic information extraction and building6Based on the paper, we were unsure if extra train-ing data is needed for this combiner.
One of the au-thors, Wu, has told us that extra data is not needed.of lexical semantic resources for NLP applica-tions, Madrid.E.
Brill and P. Resnik.
1994.
A rule-based ap-proach to prepositional phrase attachment dis-ambiguation.
In 15th International Conf.
onComputational Linguistics (COLING).E.
Brill and J. Wu.
1998.
Classier combina-tion for improved lexical disambiguation.
InCOLING-ACL'98, pages 191195, Montr?al,Canada.S.
Buchholz, J. Veenstra, and W. Daelemans.1999.
Cascaded grammatical relation assign-ment.
In Joint SIGDAT Conference on Empir-ical Methods in NLP and Very Large Corpora(EMNLP/VLC'99).
cs.CL/9906004.J.
Carroll, T. Briscoe, N. Calzolari, S. Fed-erici, S. Montemagni, V. Pirrelli, G. Grefen-stette, A. Sanlippo, G. Carroll, and M. Rooth.1997.
Sparkle work package 1, spec-ication of phrasal parsing, nal report.Available at http://www.ilc.pi.cnr.it/-sparkle/sparkle.htm, November.J.
Carroll, G. Minnen, and T. Briscoe.
1999.Corpus annotation for parser evaluation.
InEACL99 workshop on Linguistically InterpretedCorpora (LINC'99).
cs.CL/9907013.P.
Cohen.
1995.
Empirical Methods for ArticialIntelligence.
MIT Press, Cambridge, MA, USA.L.
Ferro, M. Vilain, and A. Yeh.
1999.
Learn-ing transformation rules to nd grammaticalrelations.
In Computational natural languagelearning (CoNLL-99), pages 4352.
EACL'99workshop, cs.CL/9906015.D.
Harnett.
1982.
Statistical Methods.
Addison-Wesley Publishing Co., Reading, MA, USA,third edition.J.
Henderson and E. Brill.
1999.
Exploiting diver-sity in natural language processing: combiningparsers.
In Joint SIGDAT Conference on Em-pirical Methods in NLP and Very Large Cor-pora (EMNLP/VLC'99).R.
Hwa.
1999.
Supervised grammar inductionusing training data with limited constituent in-formation.
In ACL'99.
cs.CL/9905001.R.
Kaplan.
1994.
The formal architecture oflexical-functional grammar.
In M. Dalrymple,R.
Kaplan, J. Maxwell III, and A. Zaenen, ed-itors, Formal issues in lexical-functional gram-mar.
Stanford University.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: the penn treebank.
Computational Lin-guistics, 19(2).M.
Palmer, R. Passonneau, C. Weir, and T. Finin.1993.
The kernel text understanding system.Articial Intelligence, 63:1768.D.
Palmer.
1997.
A trainable rule-based algo-rithm for word segmentation.
In Proceedings ofACL/EACL97.D.
Perlmutter.
1983.
Studies in Relational Gram-mar 1.
U. Chicago Press.H.
van Halteren, J. Zavrel, and W. Daelemans.1998.
Improving data driven wordclass taggingby system combination.
In COLING-ACL'98,pages 491497, Montr?al, Canada.Y.
Wilkes and M. Stevenson.
1998.
Word sensedisambiguation using optimized combinationsof knowledge sources.
In COLING-ACL'98,pages 13981402, Montr?al, Canada.
