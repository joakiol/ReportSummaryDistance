Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 51?56,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsSketch Techniques for Scaling Distributional Similarity to the WebAmit Goyal, Jagadeesh Jagarlamudi, Hal Daume?
III, and Suresh VenkatasubramanianSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{amitg,jags,hal,suresh}@cs.utah.eduAbstractIn this paper, we propose a memory, space,and time efficient framework to scale dis-tributional similarity to the web.
Weexploit sketch techniques, especially theCount-Min sketch, which approximatesthe frequency of an item in the corpuswithout explicitly storing the item itself.These methods use hashing to deal withmassive amounts of the streaming text.
Westore all item counts computed from 90GB of web data in just 2 billion coun-ters (8 GB main memory) of CM sketch.Our method returns semantic similaritybetween word pairs in O(K) time andcan compute similarity between any wordpairs that are stored in the sketch.
In ourexperiments, we show that our frameworkis as effective as using the exact counts.1 IntroductionIn many NLP problems, researchers (Brants et al,2007; Turney, 2008) have shown that having largeamounts of data is beneficial.
It has also beenshown that (Agirre et al, 2009; Pantel et al, 2009;Ravichandran et al, 2005) having large amountsof data helps capturing the semantic similarity be-tween pairs of words.
However, computing distri-butional similarity (Sec.
2.1) between word pairsfrom large text collections is a computationally ex-pensive task.
In this work, we consider scaling dis-tributional similarity methods for computing se-mantic similarity between words to Web-scale.The major difficulty in computing pairwise sim-ilarities stems from the rapid increase in the num-ber of unique word-context pairs with the size oftext corpus (number of tokens).
Fig.
1 shows that5 10 15 20 255101520Log2 of # of wordsLog 2of #of uniqueItemsword?context pairswordsFigure 1: Token Type Curvethe number of unique word-context pairs increaserapidly compared to the number words when plot-ted against the number of tokens1.
For example,a 57 million word corpus2 generates 224 thousandunique words and 15 million unique word-contextpairs.
As a result, it is computationally hard tocompute counts of all word-context pairs with a gi-ant corpora using conventional machines (say withmain memory of 8 GB).
To overcome this, Agirreet al (2009) used MapReduce infrastructure (with2, 000 cores) to compute pairwise similarities ofwords on a corpus of roughly 1.6 Terawords.In a different direction, our earlier work (Goyalet al, 2010) developed techniques to make thecomputations feasible on a conventional machinesby willing to accept some error in the counts.
Sim-ilar to that work, this work exploits the idea ofCount-Min (CM) sketch (Cormode and Muthukr-ishnan, 2004) to approximate the frequency ofword pairs in the corpus without explicitly stor-ing the word pairs themselves.
In their, we stored1Note that the plot is in log-log scale.2?Subset?
column of Table 1 in Section 5.151counts of all words/word pairs in fixed amount ofmain memory.
We used conservative update withCM sketch (referred as CU sketch) and showedthat it reduces the average relative error of its ap-proximate counts by a factor of two.
The approx-imate counts returned by CU Sketch were usedto compute approximate PMI between word pairs.We found their that the approximate PMI valuesare as useful as the exact PMI values for com-puting semantic orientation (Turney and Littman,2002) of words.
In addition, our intrinsic evalua-tions in their showed that the quality of approxi-mate counts and approximate PMI is good.In this work, we use CU-sketch to store countsof items (words, contexts, and word-context pairs)using fixed amount of memory of 8 GB by usingonly 2B counters.
These approximate counts re-turned by CU Sketch are converted into approx-imate PMI between word-context pairs.
The topK contexts (based on PMI score) for each wordare used to construct distributional profile (DP) foreach word.
The similarity between a pair of wordsis computed based on the cosine similarity of theirrespective DPs.The above framework of using CU sketch tocompute semantic similarity between words hasfive good properties.
First, this framework can re-turn semantic similarity between any word pairsthat are stored in the CU sketch.
Second, it canreturn the similarity between word pairs in timeO(K).
Third, because we do not store items ex-plicitly, the overall space required is significantlysmaller.
Fourth, the additive property of CUsketch (Sec.
3.2) enables us to parallelize mostof the steps in the algorithm.
Thus it can be easilyextended to very large amounts of text data.
Fifth,this easily generalizes to any kind of associationmeasure and semantic similarity measure.2 Background2.1 Distributional SimilarityDistributional Similarity is based on the distribu-tional hypothesis (Firth, 1968; Harris, 1985) thatwords occur in similar contexts tend to be sim-ilar.
The context of a word is represented bythe distributional profile (DP), which contains thestrength of association between the word and eachof the lexical, syntactic, semantic, and/or depen-dency units that co-occur with it3.
The association3In this work, we only consider lexical units as context.is commonly measured using conditional proba-bility, pointwise mutual information (PMI) or loglikelihood ratios.
Then the semantic similarity be-tween two words, given their DPs, is calculatedusing similarity measures such as Cosine, ?-skewdivergence, and Jensen-Shannon divergence.
Inour work, we use PMI as association measure andcosine similarity to compute pairwise similarities.2.2 Large Scale NLP problemsPantel et al (2009) computed similarity between500 million word pairs using the MapReduceframework from a 200 billion word corpus using200 quad-core nodes.
The inaccessibility of clus-ters for every one has attracted NLP community touse streaming, and randomized algorithms to han-dle large amounts of data.Ravichandran et al (2005) used locality sensi-tive hash functions for computing word-pair simi-larities from large text collections.
Their approachstores a enormous matrix of all unique words andtheir contexts in main memory which makes ithard for larger data sets.
In our work, we storeall unique word-context pairs in CU sketch with apre-defined size4.Recently, the streaming algorithm paradigm hasbeen used to provide memory and time-efficientplatform to deal with terabytes of data.
Forexample, we (Goyal et al, 2009); Levenbergand Osborne (2009) build approximate languagemodels and show their effectiveness in SMT.
In(Van Durme and Lall, 2009b), a TOMB Counter(Van Durme and Lall, 2009a) was used to find thetop-K verbs ?y?
with the highest PMI for a givenverb ?x?.
The idea of TOMB is similar to CUSketch.
However, we use CU Sketch because ofits simplicity and attractive properties (see Sec.
3).In this work, we go one step further, and computesemantic similarity between word-pairs using ap-proximate PMI scores from CU sketch.2.3 Sketch TechniquesSketch techniques use a sketch vector as a datastructure to store the streaming data compactly ina small-memory footprint.
These techniques usehashing to map items in the streaming data onto asmall sketch vector that can be easily updated andqueried.
These techniques generally process theinput stream in one direction, say from left to right,4We use only 2 billion counters which takes up to 8 GBof main memory.52without re-processing previous input.
The mainadvantage of using these techniques is that theyrequire a storage which is significantly smallerthan the input stream length.
A survey by (Rusuand Dobra, 2007; Cormode and Hadjieleftheriou,2008) comprehensively reviews the literature.3 Count-Min SketchThe Count-Min Sketch (Cormode and Muthukr-ishnan, 2004) is a compact summary data struc-ture used to store the frequencies of all items inthe input stream.Given an input stream of items of length Nand user chosen parameters ?
and ?, the algorithmstores the frequencies of all the items with the fol-lowing guarantees:?
All reported frequencies are within ?N oftrue frequencies with probability of atleast ?.?
Space used by the algorithm is O(1?
log 1?
).?
Constant time of O(log(1? ))
per each updateand query operation.3.1 CM Data StructureA Count-Min Sketch with parameters (?,?)
is rep-resented by a two-dimensional array with width wand depth d :??
?sketch[1, 1] ?
?
?
sketch[1, w].........sketch[d, 1] ?
?
?
sketch[d,w]??
?Among the user chosen parameters, ?
controls theamount of tolerable error in the returned count and?
controls the probability with which the returnedcount is not within this acceptable error.
Thesevalues of ?
and ?
determine the width and depthof the two-dimensional array respectively.
Toachieve the guarantees mentioned in the previoussection, we set w=2?
and d=log(1?
).
The depth ddenotes the number of pairwise-independent hashfunctions employed by the algorithm and thereexists an one-to-one correspondence between therows and the set of hash functions.
Each of thesehash functions hk:{x1 .
.
.
xN} ?
{1 .
.
.
w}, 1 ?k ?
d takes an item from the input stream andmaps it into a counter indexed by the correspond-ing hash function.
For example, h2(x) = 10 indi-cates that the item ?x?
is mapped to the 10th posi-tion in the second row of the sketch array.
Thesed hash functions are chosen uniformly at randomfrom a pairwise-independent family.Initialize the entire sketch array with zeros.Update Procedure: When a new item ?x?
withcount c arrives5, one counter in each row, as de-cided by its corresponding hash function, is up-dated by c. Formally, ?1 ?
k ?
dsketch[k,hk(x)]?
sketch[k,hk(x)] + cQuery Procedure: Since multiple items can behashed to the same counter, the frequency storedby each counter is an overestimate of the truecount.
Thus, to answer the point query, we con-sider all the positions indexed by the hash func-tions for the given item and return the minimumof all these values.
The answer to Query(x) is:c?
= mink sketch[k, hk(x)].Both update and query procedures involve eval-uating d hash functions.
Hence, both these proce-dures are linear in the number of hash functions.
Inour experiments (see Section5), we use d=3 simi-lar to our earlier work (Goyal et al, 2010).
Hence,the update and query operations take only constanttime.3.2 PropertiesApart from the advantages of being space efficientand having constant update and querying time, theCM sketch has other advantages that makes it at-tractive for scaling distributional similarity to theweb:1.
Linearity: given two sketches s1 and s2 com-puted (using the same parameters w and d)over different input streams, the sketch of thecombined data stream can be easily obtainedby adding the individual sketches.2.
The linearity allows the individual sketchesto be computed independent of each other.This means that it is easy to implement it indistributed setting, where each machine com-putes the sketch over a subset of the corpus.3.3 Conservative UpdateEstan and Varghese introduce the idea of conserva-tive update (Estan and Varghese, 2002) in the con-text of networking.
This can easily be used withCM Sketch (CU Sketch) to further improve the es-timate of a point query.
To update an item, w withfrequency c, we first compute the frequency c?
of5In our setting, c is always 1.53this item from the existing data structure and thecounts are updated according to: ?1 ?
k ?
dsketch[k,hk(x)]?
max{sketch[k,hk(x)], c?
+ c}The intuition is that, since the point query returnsthe minimum of all the d values, we will update acounter only if it is necessary as indicated by theabove equation.
This heuristic avoids the unneces-sary updating of counter values and thus reducesthe error.4 Efficient Distributional SimilarityTo compute distributional similarity efficiently, westore counts in CU sketch.
Our algorithm has threemain steps:1.
Store approximate counts of all words, con-texts, and word-context pairs in CU-sketchusing fixed amount of counters.2.
Convert these counts into approximate PMIscores between word-context pairs.
Use thesePMI scores to store top K contexts for a wordon the disk.
Store these top K context vectorsfor every word stored in the sketch.3.
Use cosine similarity to compute the similar-ity between word pairs using these approxi-mate top K context vectors constructed usingCU sketch.5 Word pair Ranking EvaluationsAs discussed earlier, the DPs of words are used tocompute similarity between a pair of words.
Weused the following four test sets and their corre-sponding human judgements to evaluate the wordpair rankings.1.
WS-353: WordSimilarity-3536 (Finkelsteinet al, 2002) is a set of 353 word pairs.2.
WS-203: A subset of WS-353 containing 203word pairs marked according to similarity7(Agirre et al, 2009).3.
RG-65: (Rubenstein and Goodenough, 1965)is set of 65 word pairs.4.
MC-30: A smaller subset of the RG-65dataset containing 30 word pairs (Miller andCharles, 1991).6http://www.cs.technion.ac.il/ gabr/resources/data/word-sim353/wordsim353.html7http://alfonseca.org/pubs/ws353simrel.tar.gzEach of these data sets come with human rankingof the word pairs.
We rank the word pairs basedon the similarity computed using DPs and evalu-ate this ranking against the human ranking.
Wereport the spearman?s rank correlation coefficient(?)
between these two rankings.5.1 Corpus StatisticsThe Gigaword corpus (Graff, 2003) and a copy ofthe web crawled by (Ravichandran et al, 2005)are used to compute counts of all items (Table.
1).For both the corpora, we split the text into sen-tences, tokenize, convert into lower-case, removepunctuations, and collapse each digit to a sym-bol ?0?
(e.g.
?1996?
gets collapsed to ?0000?
).We store the counts of all words (excluding num-bers, and stop words), their contexts, and countsof word-context pairs in the CU sketch.
We de-fine the context for a given word ?x?
as the sur-rounding words appearing in a window of 2 wordsto the left and 2 words to the right.
The contextwords are concatenated along with their positions-2, -1, +1, and +2.
We evaluate ranking of wordpairs on three different sized corpora: Gigaword(GW), GigaWord + 50% of web data (GW-WB1),and GigaWord + 100% of web data (GW-WB2).Corpus Sub GW GW- GW-set WB1 WB2Size.32 9.8 49 90(GB)# of sentences 2.00 56.78 462.60 866.02(Million)Stream Size.25 7.65 37.93 69.41(Billion)Table 1: Corpus Description5.2 ResultsWe compare our system with two baselines: Ex-act and Exact1000 which use exact counts.
Sincecomputing the exact counts of all word-contextpairs on these corpora is not possible using mainmemory of only 8 GB , we generate context vec-tors for only those words which appear in the testset.
The former baseline uses all possible contextswhich appear with a test word, while the latterbaseline uses only the top 1000 contexts (based onPMI value) for each word.
In each case, we usea cutoff (of 10, 60 and 120) on the frequency ofword-context pairs.
These cut-offs were selectedbased on the intuition that, with more data, youget more noise, and not considering word-contextpairs with frequency less than 120 might be a bet-54Data GW GW-WB1 GW-WB2Model Frequency cutoff Frequency cutoff Frequency cutoff10 60 120 10 60 120 10 60 120?
?
?WS-353Exact .25 .25 .22 .29 .28 .28 .30 .28 .28Exact1000 .36 .28 .22 .46 .43 .37 .47 .44 .41Our Model .39 .28 .22 -0.09 .48 .40 -0.03 .04 .47WS-203Exact .35 .36 .33 .38 .38 .37 .40 .38 .38Exact1000 .49 .40 .35 .57 .55 .47 .56 .56 .52Our Model .49 .39 .35 -0.08 .58 .47 -0.06 .03 .55RG-65Exact .21 .12 .08 .42 .28 .22 .39 .31 .23Exact1000 .14 .09 .08 .45 .16 .13 .47 .26 .12Our Model .13 .10 .09 -0.06 .32 .18 -0.05 .08 .31MC-30Exact .26 .23 .21 .45 .33 .31 .46 .39 .29Exact1000 .27 .18 .21 .63 .42 .32 .59 .47 .36Our Model .36 .20 .21 -0.08 .52 .39 -0.27 -0.29 .52Table 2: Evaluating word pairs ranking with Exact and CU counts.
Scores are evaluated using ?
metric.ter choice than a cutoff of 10.
The results areshown in Table 2From the above baseline results, first we learnthat using more data helps in better capturingthe semantic similarity between words.
Second,it shows that using top (K) 1000 contexts foreach target word captures better semantic similar-ity than using all possible contexts for that word.Third, using a cutoff of 10 is optimal for all differ-ent sized corpora on all test-sets.We use approximate counts from CU sketchwith depth=3 and 2 billion (2B) counters (?OurModel?)8.
Based on previous observation, we re-strict the number of contexts for a target word to1000.
Table 2 shows that using CU counts makesthe algorithm sensitive to frequency cutoff.
How-ever, with appropriate frequency cutoff for eachcorpus, approximate counts are nearly as effectiveas exact counts.
For GW, GW-WB1, and GW-WB2, the frequency cutoffs of 10, 60, and 120 re-spectively performed the best.
The reason for de-pendence on frequency cutoffs is due to the over-estimation of low-frequent items.
This is morepronounced with bigger corpus (GW-WB2) as thesize of CU sketch is fixed to 2B counters andstream size is much bigger (69.41 billion) com-pared to GW where the stream size is 7.65 billion.The advantages of using our model is that thesketch contains counts for all words, contexts, andword-context pairs stored in fixed memory of 8GB by using only 2B counters.
Note that it is not8Our goal is not to build the best distributional similaritymethod.
It is to show that our framework scales easily to largecorpus and it is as effective as exact method.feasible to keep track of exact counts of all word-context pairs since their number increases rapidlywith increase in data (see Fig.
1).
We can use ourmodel to create context vectors of size K for allpossible words stored in the Sketch and computessemantic similarity between two words in O(K)time.
In addition, the linearity of sketch allowsus to include new incoming data into the sketchwithout building the sketch from scratch.
Also,it allows for parallelization using the MapReduceframework.
We can generalize our framework toany kind of association and similarity measure.6 ConclusionWe proposed a framework which uses CU Sketchto scale distributional similarity to the web.
It cancompute similarity between any word pairs thatare stored in the sketch and returns similarity be-tween them in O(K) time.
In our experiments, weshow that our framework is as effective as usingthe exact counts, however it is sensitive to the fre-quency cutoffs.
In future, we will explore ways tomake this framework robust to the frequency cut-offs.
In addition, we are interested in exploringthis framework for entity set expansion problem.AcknowledgmentsWe thank the anonymous reviewers for helpfulcomments.
This work is partially funded by NSFgrant IIS-0712764 and Google Research GrantGrant for Large-Data NLP.55ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distri-butional and wordnet-based approaches.
In NAACL?09: Proceedings of HLT-NAACL.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofthe 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).Graham Cormode and Marios Hadjieleftheriou.
2008.Finding frequent items in data streams.
In VLDB.Graham Cormode and S. Muthukrishnan.
2004.
Animproved data stream summary: The count-minsketch and its applications.
J. Algorithms.Cristian Estan and George Varghese.
2002.
New direc-tions in traffic measurement and accounting.
SIG-COMM Comput.
Commun.
Rev., 32(4).L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2002.
Plac-ing search in context: The concept revisited.
InACM Transactions on Information Systems.J.
Firth.
1968.
A synopsis of linguistic theory 1930-1955.
In F. Palmer, editor, Selected Papers of J. R.Firth.
Longman.Amit Goyal, Hal Daume?
III, and Suresh Venkatasub-ramanian.
2009.
Streaming for large scale NLP:Language modeling.
In North American Chap-ter of the Association for Computational Linguistics(NAACL).Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume?
III,and Suresh Venkatasubramanian.
2010.
Sketchingtechniques for Large Scale NLP.
In 6th Web as Cor-pus Workshop in conjunction with NAACL-HLT.D.
Graff.
2003.
English Gigaword.
Linguistic DataConsortium, Philadelphia, PA, January.Z.
Harris.
1985.
Distributional structure.
In J. J. Katz,editor, The Philosophy of Linguistics, pages 26?47.Oxford University Press, New York.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for SMT.
InProceedings of EMNLP, August.G.A.
Miller and W.G.
Charles.
1991.
Contextual cor-relates of semantic similarity.
Language and Cogni-tive Processes, 6(1):1?28.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP.Deepak Ravichandran, Patrick Pantel, and EduardHovy.
2005.
Randomized algorithms and nlp: usinglocality sensitive hash function for high speed nounclustering.
In ACL ?05: Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics.H.
Rubenstein and J.B. Goodenough.
1965.
Contex-tual correlates of synonymy.
Computational Lin-guistics, 8:627?633.Florin Rusu and Alin Dobra.
2007.
Statistical analysisof sketch estimators.
In SIGMOD ?07.
ACM.P.D.
Turney and M.L.
Littman.
2002.
Unsupervisedlearning of semantic orientation from a hundred-billion-word corpus.Peter D. Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
In Pro-ceedings of COLING 2008.Benjamin Van Durme and Ashwin Lall.
2009a.
Prob-abilistic counting with randomized storage.
In IJ-CAI?09: Proceedings of the 21st international jontconference on Artifical intelligence.Benjamin Van Durme and Ashwin Lall.
2009b.Streaming pointwise mutual information.
In Ad-vances in Neural Information Processing Systems22.56
