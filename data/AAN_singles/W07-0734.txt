Proceedings of the Second Workshop on Statistical Machine Translation, pages 228?231,Prague, June 2007. c?2007 Association for Computational LinguisticsMeteor: An Automatic Metric for MT Evaluation with HighLevels of Correlation with Human JudgmentsAlon Lavie and Abhaya AgarwalLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{alavie,abhayaa}@cs.cmu.eduAbstractMeteor is an automatic metric for Ma-chine Translation evaluation which has beendemonstrated to have high levels of corre-lation with human judgments of translationquality, significantly outperforming the morecommonly used Bleu metric.
It is one ofseveral automatic metrics used in this year?sshared task within the ACL WMT-07 work-shop.
This paper recaps the technical de-tails underlying the metric and describes re-cent improvements in the metric.
The latestrelease includes improved metric parametersand extends the metric to support evalua-tion of MT output in Spanish, French andGerman, in addition to English.1 IntroductionAutomatic Metrics for MT evaluation have been re-ceiving significant attention in recent years.
Evalu-ating an MT system using such automatic metrics ismuch faster, easier and cheaper compared to humanevaluations, which require trained bilingual evalua-tors.
Automatic metrics are useful for comparingthe performance of different systems on a commontranslation task, and can be applied on a frequentand ongoing basis during MT system development.The most commonly used MT evaluation metric inrecent years has been IBM?s Bleu metric (Papineniet al, 2002).
Bleu is fast and easy to run, and itcan be used as a target function in parameter op-timization training procedures that are commonlyused in state-of-the-art statistical MT systems (Och,2003).
Various researchers have noted, however, var-ious weaknesses in the metric.
Most notably, Bleudoes not produce very reliable sentence-level scores.Meteor , as well as several other proposed metricssuch as GTM (Melamed et al, 2003), TER (Snoveret al, 2006) and CDER (Leusch et al, 2006) aim toaddress some of these weaknesses.Meteor , initially proposed and released in 2004(Lavie et al, 2004) was explicitly designed to im-prove correlation with human judgments of MT qual-ity at the segment level.
Previous publications onMeteor (Lavie et al, 2004; Banerjee and Lavie,2005) have described the details underlying the met-ric and have extensively compared its performancewith Bleu and several other MT evaluation met-rics.
This paper recaps the technical details underly-ing Meteor and describes recent improvements inthe metric.
The latest release extends Meteor tosupport evaluation of MT output in Spanish, Frenchand German, in addition to English.
Furthermore,several parameters within the metric have been opti-mized on language-specific training data.
We presentexperimental results that demonstrate the improve-ments in correlations with human judgments that re-sult from these parameter tunings.2 The Meteor MetricMeteor evaluates a translation by computing ascore based on explicit word-to-word matches be-tween the translation and a given reference trans-lation.
If more than one reference translation isavailable, the translation is scored against each refer-ence independently, and the best scoring pair is used.Given a pair of strings to be compared, Meteor cre-ates a word alignment between the two strings.
Analignment is mapping between words, such that ev-ery word in each string maps to at most one wordin the other string.
This alignment is incrementallyproduced by a sequence of word-mapping modules.The ?exact?
module maps two words if they are ex-actly the same.
The ?porter stem?
module maps twowords if they are the same after they are stemmed us-ing the Porter stemmer.
The ?WN synonymy?
mod-ule maps two words if they are considered synonyms,based on the fact that they both belong to the same?synset?
in WordNet.The word-mapping modules initially identify all228possible word matches between the pair of strings.We then identify the largest subset of these wordmappings such that the resulting set constitutes analignment as defined above.
If more than one maxi-mal cardinality alignment is found, Meteor selectsthe alignment for which the word order in the twostrings is most similar (the mapping that has theleast number of ?crossing?
unigram mappings).
Theorder in which the modules are run reflects word-matching preferences.
The default ordering is tofirst apply the ?exact?
mapping module, followed by?porter stemming?
and then ?WN synonymy?.Once a final alignment has been produced betweenthe system translation and the reference translation,the Meteor score for this pairing is computed asfollows.
Based on the number of mapped unigramsfound between the two strings (m), the total num-ber of unigrams in the translation (t) and the totalnumber of unigrams in the reference (r), we calcu-late unigram precision P = m/t and unigram recallR = m/r.
We then compute a parameterized har-monic mean of P and R (van Rijsbergen, 1979):Fmean =P ?
R?
?
P + (1?
?)
?
RPrecision, recall and Fmean are based on single-word matches.
To take into account the extent towhich the matched unigrams in the two strings arein the same word order, Meteor computes a penaltyfor a given alignment as follows.
First, the sequenceof matched unigrams between the two strings is di-vided into the fewest possible number of ?chunks?such that the matched unigrams in each chunk areadjacent (in both strings) and in identical word or-der.
The number of chunks (ch) and the number ofmatches (m) is then used to calculate a fragmenta-tion fraction: frag = ch/m.
The penalty is thencomputed as:Pen = ?
?
frag?The value of ?
determines the maximum penalty(0 ?
?
?
1).
The value of ?
determines thefunctional relation between fragmentation and thepenalty.
Finally, the Meteor score for the align-ment between the two strings is calculated as:score = (1?
Pen) ?
FmeanIn all previous versions of Meteor , the values ofthe three parameters mentioned above were set to be:?
= 0.9, ?
= 3.0 and ?
= 0.5, based on experimen-tation performed in early 2004.
In the latest release,we tuned these parameters to optimize correlationwith human judgments based on more extensive ex-perimentation, as reported in section 4.3 Meteor Implementations forSpanish, French and GermanWe have recently expanded the implementation ofMeteor to support evaluation of translations inSpanish, French and German, in addition to English.Two main language-specific issues required adapta-tion within the metric: (1) language-specific word-matching modules; and (2) language-specific param-eter tuning.
The word-matching component withinthe English version of Meteor uses stemming andsynonymy modules in constructing a word-to-wordalignment between translation and reference.
The re-sources used for stemming and synonymy detectionfor English are the Porter Stemmer (Porter, 2001)and English WordNet (Miller and Fellbaum, 2007).In order to construct instances of Meteor for Span-ish, French and German, we created new language-specific ?stemming?
modules.
We use the freelyavailable Perl implementation packages for Porterstemmers for the three languages (Humphrey, 2007).Unfortunately, we have so far been unable to obtainfreely available WordNet resources for these threelanguages.
Meteor versions for Spanish, Frenchand German therefore currently include only ?exact?and ?stemming?
matching modules.
We are investi-gating the possibility of developing new synonymymodules for the various languages based on alterna-tive methods, which could then be used in place ofWordNet.
The second main language-specific issuewhich required adaptation is the tuning of the threeparameters within Meteor , described in section 4.4 Optimizing Metric ParametersThe original version of Meteor (Banerjee andLavie, 2005) has instantiated values for three pa-rameters in the metric: one for controlling the rela-tive weight of precision and recall in computing theFmean score (?
); one governing the shape of thepenalty as a function of fragmentation (?)
and onefor the relative weight assigned to the fragmenta-tion penalty (?).
In all versions of Meteor to date,these parameters were instantiated with the values?
= 0.9, ?
= 3.0 and ?
= 0.5, based on early data ex-perimentation.
We recently conducted a more thor-ough investigation aimed at tuning these parametersbased on several available data sets, with the goal offinding parameter settings that maximize correlationwith human judgments.
Human judgments come inthe form of ?adequacy?
and ?fluency?
quantitativescores.
In our experiments, we looked at optimizingparameters for each of these human judgment typesseparately, as well as optimizing parameters for thesum of adequacy and fluency.
Parameter adapta-229Corpus Judgments SystemsNIST 2003 Ara-to-Eng 3978 6NIST 2004 Ara-to-Eng 347 5WMT-06 Eng-to-Fre 729 4WMT-06 Eng-to-Ger 756 5WMT-06 Eng-to-Spa 1201 7Table 1: Corpus Statistics for Various Languagestion is also an issue in the newly created Meteorinstances for other languages.
We suspected thatparameters that were optimized to maximize corre-lation with human judgments for English would notnecessarily be optimal for other languages.4.1 DataFor English, we used the NIST 2003 Arabic-to-English MT evaluation data for training and theNIST 2004 Arabic-to-English evaluation data fortesting.
For Spanish, German and French we usedthe evaluation data provided by the shared task atlast year?s WMT workshop.
Sizes of various corporaare shown in Table 1.
Some, but not all, of these datasets have multiple human judgments per translationhypothesis.
To partially address human bias issues,we normalize the human judgments, which trans-forms the raw judgment scores so that they have sim-ilar distributions.
We use the normalization methoddescribed in (Blatz et al, 2003).
Multiple judgmentsare combined into a single number by taking theiraverage.4.2 MethodologyWe performed a ?hill climbing?
search to find theparameters that achieve maximum correlation withhuman judgments on the training set.
We use Pear-son?s correlation coefficient as our measure of corre-lation.
We followed a ?leave one out?
training proce-dure in order to avoid over-fitting.
When n systemswere available for a particular language, we train theparameters n times, leaving one system out in eachtraining, and pooling the segments from all othersystems.
The final parameter values are calculatedas the mean of the n sets of trained parameters thatwere obtained.
When evaluating a set of parameterson test data, we compute segment-level correlationwith human judgments for each of the systems in thetest set and then report the mean over all systems.4.3 Results4.3.1 Optimizing for Adequacy and FluencyWe trained parameters to obtain maximum cor-relation with normalized adequacy and fluency judg-Adequacy Fluency Sum?
0.82 0.78 0.81?
1.0 0.75 0.83?
0.21 0.38 0.28Table 2: Optimal Values of Tuned Parameters forDifferent Criteria for EnglishAdequacy Fluency SumOriginal 0.6123 0.4355 0.5704Adequacy 0.6171 0.4354 0.5729Fluency 0.6191 0.4502 0.5818Sum 0.6191 0.4425 0.5778Table 3: Pearson Correlation with Human Judg-ments on Test Data for Englishments separately and also trained for maximal corre-lation with the sum of the two.
The resulting optimalparameter values on the training corpus are shown inTable 2.
Pearson correlations with human judgmentson the test set are shown in Table 3.The optimal parameter values found are somewhatdifferent than our previous metric parameters (lowervalues for all three parameters).
The new parame-ters result in moderate but noticeable improvementsin correlation with human judgments on both train-ing and testing data.
Tests for statistical significanceusing bootstrap sampling indicate that the differ-ences in correlation levels are all significant at the95% level.
Another interesting observation is thatprecision receives slightly more ?weight?
when op-timizing correlation with fluency judgments (versuswhen optimizing correlation with adequacy).
Recall,however, is still given more weight than precision.Another interesting observation is that the value of?
is higher for fluency optimization.
Since the frag-mentation penalty reflects word-ordering, which isclosely related to fluency, these results are consistentwith our expectations.
When optimizing correlationwith the sum of adequacy and fluency, optimal val-ues fall in between the values found for adequacy andfluency.4.3.2 Parameters for Other LanguagesSimilar to English, we trained parameters forSpanish, French and German on the available WMT-06 training data.
We optimized for maximum corre-lation with human judgments of adequacy, fluencyand for the sum of the two.
Resulting parametersare shown in Table 4.3.2.
For all three languages, theparameters that were found to be optimal were quitedifferent than those that were found for English, andusing the language-specific optimal parameters re-230Adequacy Fluency SumFrench:?
0.86 0.74 0.76?
0.5 0.5 0.5?
1.0 1.0 1.0German:?
0.95 0.95 0.95?
0.5 0.5 0.5?
0.6 0.8 0.75Spanish:?
0.95 0.62 0.95?
1.0 1.0 1.0?
0.9 1.0 0.98Table 4: Tuned Parameters for Different Languagessults in significant gains in Pearson correlation levelswith human judgments on the training data (com-pared with those obtained using the English opti-mal parameters)1.
Note that the training sets usedfor these optimizations are comparatively very small,and that we currently do not have unseen test datato evaluate the parameters for these three languages.Further validation will need to be performed once ad-ditional data becomes available.5 ConclusionsIn this paper we described newly developedlanguage-specific instances of the Meteor metricand the process of optimizing metric parameters fordifferent human measures of translation quality andfor different languages.
Our evaluations demonstratethat parameter tuning improves correlation with hu-man judgments.
The stability of the optimized pa-rameters on different data sets remains to be inves-tigated for languages other than English.
We arecurrently exploring broadening the set of featuresused in Meteor to include syntax-based featuresand alternative notions of synonymy.
The latest re-lease of Meteor is freely available on our websiteat: http://www.cs.cmu.edu/~alavie/METEOR/AcknowledgementsThe work reported in this paper was supported byNSF Grant IIS-0534932.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
ME-TEOR: An Automatic Metric for MT Evalua-tion with Improved Correlation with Human Judg-ments.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measures1Detailed tables are not included for lack of space.for Machine Translation and/or Summarization,pages 65?72, Ann Arbor, Michigan, June.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidence Es-timation for Machine Translation.
Technical Re-port Natural Language Engineering Workshop Fi-nal Report, Johns Hopkins University.Marvin Humphrey.
2007.
Perl In-terface to Snowball Stemmers.http://search.cpan.org/ creamyg/Lingua-Stem-Snowball-0.941/lib/Lingua/Stem/Snowball.pm.Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-man.
2004.
The Significance of Recall in Auto-matic Metrics for MT Evaluation.
In Proceedingsof the 6th Conference of the Association for Ma-chine Translation in the Americas (AMTA-2004),pages 134?143, Washington, DC, September.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2006.
CDER: Efficient MT Evaluation UsingBlock Movements.
In Proceedings of the Thir-teenth Conference of the European Chapter of theAssociation for Computational Linguistics.I.
Dan Melamed, Ryan Green, and Joseph Turian.2003.
Precision and Recall of Machine Transla-tion.
In Proceedings of the HLT-NAACL 2003Conference: Short Papers, pages 61?63, Edmon-ton, Alberta.George Miller and Christiane Fellbaum.
2007.
Word-Net.
http://wordnet.princeton.edu/.Franz Josef Och.
2003.
Minimum Error Rate Train-ing for Statistical Machine Translation.
In Pro-ceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
In Pro-ceedings of 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 311?318, Philadelphia, PA, July.Martin Porter.
2001.
The Porter Stem-ming Algorithm.
http://www.tartarus.org/ mar-tin/PorterStemmer/index.html.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
AStudy of Translation Edit Rate with Targeted Hu-man Annotation.
In Proceedings of the 7th Confer-ence of the Association for Machine Translation inthe Americas (AMTA-2006), pages 223?231, Cam-bridge, MA, August.C.
van Rijsbergen, 1979.
Information Retrieval.Butterworths, London, UK, 2nd edition.231
