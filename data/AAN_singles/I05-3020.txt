Report to BMM-based Chinese Word Segmentor with Context-basedUnknown Word Identifier for the Second International Chinese WordSegmentation BakeoffJia-Lin TsaiTung Nan Institute of Technology, Department of Information ManagementTaipei 222, Taiwan, R.O.C.tsaijl@mail.tnit.edu.twAbstractThis paper describes a Chinese wordsegmentor (CWS) based on backwardmaximum matching (BMM) techniquefor the 2nd Chinese Word Segmenta-tion Bakeoff in the Microsoft Research(MSR) closed testing track.
Our CWScomprises of a context-based Chineseunknown word identifier (UWI).
Allthe context-based knowledge for theUWI is fully automatically generatedby the MSR training corpus.
Accord-ing to the scored results of the MSRclosed testing track and our analysis, itshows that our BMM-based CWS withthe context-based UWI is a simple andeffective system to achieve high Chi-nese word segmentation performanceof more than 95.5% F-measure.1 IntroductionIn the research fields of Chinese natural lan-guage processing (NLP), a high-performanceChinese word segmentor (CWS) is a useful pre-processing stage to produce an intermediate re-sult for later processes, such as search engines,text mining and speech recognition, etc.
Thebottleneck of developing a high-performanceCWS is to comprise of a high-performance Chi-nese UWI (Lin et al 1993; Tsai et al 2003).
It isbecause Chinese is written without any separa-tion between words and meanwhile more than50% words of the Chinese texts in web corpusare out-of-vocabulary (Tsai et al 2003).Conventionally, there are four approaches todevelop a CWS: (1) Dictionary-based approach(Cheng et al 1999), especial forward and back-ward maximum matching (Wong  and Chan,1996); (2) Linguistic approach based on syntax-semantic knowledge (Chen et al 2002); (3) Sta-tistical approach based on statistical languagemodel (SLM) (Sproat and Shih, 1990; Teahan etal.
2000; Gao et al 2003); and (4) Hybrid ap-proach trying to combine the benefits of diction-ary-based, linguistic and statistical approaches(Tsai et al 2003; Ma and Chen, 2003).
In prac-tice, statistical approaches are most widely usedbecause their effective and reasonable perform-ance.
For a CWS, there are two types of wordsegmentation ambiguities while there are no un-known words in them: (1) Overlap ambiguity(OA), take a character string ABC as an exam-ple.
If its segmentation can be either AB/C orA/BC depending on different context, the ABCis called an overlap ambiguity string (OAS),such as ???
(a general)/?(use)?
and ??(toget)/??
(for military use)?
(the symbol ?/?
in-dicates a word boundary); (2) Combinationambiguity (CA), take a character string AB asan example.
If its segmentation can be eitherA/B or AB depending on different context, theAB is called a combination ambiguity string(CAS), such as ??(just)/?(can)?
and ???(ability).?
Meantime, there are two types of errorsegmentation caused by unknown word problem:(1) Lack of unknown word (LUW), it meansthe error segmentation occurred by lack of anunknown word in the system dictionary, such as??/?/???
; (2) Error identified word (EIW),it means the error segmentation occurred by anerror identified unknown words, such as ???142?.?
To sum up, for a CWS in most case theUWI is a pre-processing stage to detect un-known words for the optimization of LUW-EIWtradeoff, and then to disambiguate those auto-detected OAS and CAS problems from the seg-mentation results.The goal of this paper is to illustrate and re-port the effectiveness and the scored results ofour BMM-based CWS for the second Interna-tional Chinese Word Segmentation Bakeoff inthe MSR closed (MSR_C) track.
For this Bake-off, our CWS is mainly addressed on optimizingthe LUW-EIW tradeoff.The remainder of this paper is arranged asfollows.
In Section 2, we present the details ofour BMM-based CWS comprised of a context-based UWI.
In Section 3, we present the scoredresults of the CWS in the MSR_C track and giveour analysis.
Finally, in Section 4, we give ourconclusions and suggest some future researchdirections.2 Development of BMM-based CWSAs per (Tsai et al 2004), the Chinese word seg-mentation performance of BMM technique isabout 1% greater than that of FMM technique.Thus, we adopt BMM technique as base to de-velop our CWS.
The descriptions of symbolsused in our CWS are given as below:<BOS>: begin of sentence;<EOS>: end of sentence;<BOW>: begin of word;<EOW>: end of word;/: word boundary;+: inner word boundaries of the segmentation ofa system word segmented by BMM tech-nique with the system dictionary exclusiveof this system word;SWS (stop word string): for a system word(such as ??(of)?
), if the ratio (non-SWSprobability) of total frequency of the othersystem words including it (such as ???(beautiful)?)
and its character string fre-quency is less than or equal to 1%, it is aSWS;SWBS (stop word bigram string): for a wordbigram (such as ??(just)/?(can)?
), if theratio (non-SWBS probability) of its charac-ter string (such as ???
(ability)?
fre-quency and its character string frequency isless than or equal to 1%, it is a SWBS;BMM-ASM (BMM ambiguity string mappingtable: the BMM-ASM table lists all thepairs of correct SS (given in training corpus)and the error BMM SS (generated by BMMwith the training system dictionary).
Takethe Chinese sentence ??????
as an ex-ample.
As per its MSR-standard segmenta-tion ???(effect)/?(really)/?(good)?
andits BMM segmentation ??
(follow)/??(indeed)/?(good),?
the pair  ???/??-??/???
is a BMM-ASM;TCT (triple context template): a TCT comprisedof three items from left to right are: the leftword, the segmented system word and theright word, where the system word is not amono-syllabic Chinese word.
Take the Chi-nese sentence ???/?/??
as an example.The two generated TCT are:?<BOS>/?+1-char-word/???<BOS>/1-char-word+?/??
; andWCT (word context template): a WCT com-prised of three items from left to right are:?<BOW>?, the segmented system word and?<EOW>?, where the system word is not amono-syllabic word.
Take the system word????(lamasery)?
as an example.
Its twoWCT are:?<BOW>/??+1-char-word/<EOW>??<BOW>/2-char-word+?/<EOW>.
?The algorithm of our BMM-based CWS com-prised of a context-based UWI is as below:Step 1.
Generate BMM segmentation for theinput Chinese sentence with system dictionary,firstly.
The system dictionary comprised of allword types found in the training corpus.
Then,use BMM-ASM table to revise the matchedBMM ambiguity string.Step 2.
Use UWI to identify unknown wordsfrom the segmentation of Step 1 by the TCTknowledge, firstly.
For the matched TCT, thecharacters between the left word and the rightword will be combined as an UWI-identifiedword.
If the UWI-identified word includes aSWS or a SWBS, it will be not an UWI-identified word.
Then, use the system diction-ary of Step 1 inclusive of the UWI-identifiedwords of this step to repeat Step 1 process.Step 3.
Add tags ?<BOW>?
and ?<EOW>?
at143the left-side and right-side of the continue 1-char character segmentations of Step 2, firstly.Then, use UWI to identify unknown words bythe WCT knowledge.
If the number of charac-ters between ?<BOW>?
and ?EOW>?
is samewith that of the matched WCT, these 1-charcharacters will be combined as an UWI-identified word.
If the UWI-identified wordincludes a SWS or a SWBS, it will be not anUWI-identified word.
Finally, use the systemdictionary of Step 2 inclusive of those UWI-identified words of this step to repeat Step 1process.Step 4.
Use UWI to combine a word bigraminto a word by the following two conditions:(1) if the non-SWS probability of the rightfirst character of the left-side word is greateror equal to 99% and (2) if the non-SWS prob-ability of the left first character of the right-side word is greater or equal to 99%.
Take theword bigram ????
/??
as an example.Since the non-SWS probability of the rightfirst character ???
of the left-side word ?????
is 99.95%, ??????
is identified asan UWI-identified word.
If the UWI-identifiedword includes a SWS or a SWBS, it will benot an UWI-identified word.
Finally, use thesystem dictionary of Step 3 inclusive of thoseUWI-identified words of this step to repeat theStep 1 process.Step 5.
Repeat the Step 2 process.Step 6.
Repeat the Step 3 process.Step 7.
Repeat the Step 4 process.Step 8.
Stop.In the above algorithm, Steps 2, 3 and 4 re-peated at Steps 5, 6 and 7, respectively, are de-signed to show the recursive effect of our CWS.3 The Scored Results and AnalysisIn the 2nd Chinese Word Segmentation Bakeoff,there are four training corpus: AS (AcademiaSinica) and CU (City University of Hong Kong)are traditional Chinese corpus, PU (Peking Uni-versity) and Microsoft Research (MSR) are sim-plified Chinese corpus.
Meanwhile, there aretwo testing tracks of this bakeoff: closed andopen.
We attend MSR_C track.
The non-SWSand the non-SWBS probabilities of our CWS forthis bakeoff are all set to 1%.
And, the segmen-tation results of each step of our CWS are col-lected and scored, respectively.3.1 The Scored ResultsTable 1 shows the details of MSR training andtesting corpus.
Note that, in Table 1, the detailsof MSR testing corpus were computed by usaccording to the MSR gold testing corpus.
FromTable 1, it indicates that the MSR testing trackseems to be a 25-folds experiment design.Table 1.
The details of MSR_C corpusTraining  TestingSentences 86,924  3,985Word types 88,119  12,924Words  2,368,391 109,002Character types 5,167  2,839Characters 4,050,469 184,356Table 2 shows the scored results of our CWSin MSR_C track.
The performance of ?Step1(P)?
in Table 2 was computed by us and theothers were from the scored results.
It shows avery high performance of 99.1% F-measure canbe achieved while the BMM-based CWS by us-ing a system dictionary comprised of word typesfound in the MSR training and testing corpus atStep 1 (?P?
means ?Perfect?
).Table 2.
The performance of each step of our CWSin the MSR-C track (OOV is 0.026)Step R P F ROOV RIV1(P) 0.993 0.989 0.991 - -1 0.963  0.924  0.943  0.025  0.9892 0.964  0.924  0.944 0.025 0.9893 0.968  0.938  0.953  0.205  0.9894 0.958  0.949  0.954  0.465  0.9725 0.958 0.951 0.954 0.493 0.9716 0.958  0.952  0.955  0.503  0.9707 0.958  0.952  0.955  0.504  0.9703.2 The AnalysisTable 3 (see next page) shows the differences ofF-measure and ROOV between each near-by stepof our CWS.
From Table 3, it indicates that themost contribution for increasing the overall per-formance (F-measure) of our CWS is at Step 3,which uses WCT knowledge.Table 4 (see next page) shows the distribu-tions of four segmentation error types (OAS,CAS, LUW and EIW) for each step of our CWS.From Table 4, it shows that our context-basedUWI with the knowledge of TCT and WCT can144effectively to optimize the LUW-EIW tradeoff.Moreover, from Table 4, it also shows that theknowledge of SWS, SWBS and BMM-ASM caneffectively to resolve the CAS errors.Table 3.
The differences of F-measure and ROOVbetween near-by steps of our CWSStep F F(d) ROOV ROOV(d)1 0.943  - 0.025  -2 0.944 0.001 0.025 03 0.953  0.011 0.205  0.184 0.954  0.001 0.465  0.265 0.954 0 0.493 0.0286 0.955  0.001 0.503  0.017 0.955 0 0.504 0.001Table 4.
The number of OAS (types), CAS (types),LUW (types) and EIW (types) for each step of ourCWSOAS            CAS          LUW              EIW1   210(194)     233(80)     2702(1930)    157(96)2   184(173)     233(80)     2698(1927)    157(96)3   185(174)     232(80)     2169(1473)    187(126)4   250(226)     226(77)     1373(1090)    946(609)5   250(226)     226(77)     1283(1018)    991(658)6   251(227)     224(77)     1255(1001)    1005(669)7   262(216)     224(76)     1260(1005)    1007(668)4 Conclusions and Future DirectionsIn this paper, we have applied a BMM-basedCWS comprised of a context-based UWI to theChinese word segmentation and obtained a highperformance of 95.5% F-measure in the MSRclosed track.
To sum up the results of this study,we have following conclusions and future direc-tions:(1)Since the F-measure of Step 1 of our CWS is94.3%, it indicates that the BMM with BMM-ASM knowledge is a simple but probably ef-fective technique as a good base in developinga high performance CWS;(2)Since 82% of segmentation errors of ourCWS caused by LUW problem, this resultsupports that a high performance CWS is re-lied on a high performance Chinese UWI.
(3)For a CWS, there are two critical and proba-bly independent tasks: the optimization ofLUW-EIW tradeoff and the detection and dis-ambiguation of OAS and CAS error segmen-tation.
We believe the former task is morecritical than the later one.
(4)We will continue to expand our CWS withother linguistic knowledge (such as part-of-speech information and morphology) andBTM model (Tsai 2005) to improve ourBMM-based CWS for attending the third In-ternational Chinese Word SegmentationBakeoff in both closed and open testing tracks.ReferencesChen, Keh-Jiann and Wei-Yun, Ma.
2002.
UnknownWord Extraction for Chinese Documents, Pro-ceedings of 19th COLING 2002, Taipei, 169-175.Cheng, Kowk-Shing, Gilbert H. Yong and Kam-FaiWong.. 1999.
A study on word-based and in-tegral-bit Chinese text compression algorithms.JASIS, 50(3): 218-228.Gao, Jianfeng, Mu Li and Chang-Ning uang.
2003.Improved Source-Channel Models for ChineseWord Segmentation.
Proceedings of the 41stAnnual Meeting of the Association for Compu-tational Linguistics, 272-279.Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yi Su.1993.
A preliminary study on unknown wordproblem in Chinese word segmentation.ROCLING 6, 119-141.Ma, Wei-Yun and Keh-Jiann Chen, 2003, "Introduc-tion to CKIP Chinese Word SegmentationSystem for the First International ChineseWord Segmentation Bakeoff", Proceedings ofACL, Second SIGHAN Workshop on ChineseLanguage Processing, pp168-171.Sproat, R. and C., Shih.
1990.
A Statistical Methodfor Finding Word Boundaries in Chinese Text.Computer proceeding of Chinese and OrientalLanguage, 4(4):336 349.Teahan, W. J., Yingying Wen, Rodger McNad andIan Witten.
2000.
A compression-based algo-rithm for Chinese word segmentation.
Compu-tational Linguistics, 26(3): 375-393.Tsai, Jia-Lin, C.L., Sung and W.L., Hsu.
2003.
Chi-nese Word Auto-Confirmation Agent, Pro-ceedings of ROCLING XV, Taiwan, 175-192.Tsai, Jia-Lin, G., Hsieh and W.L., Hsu.
2004.
Auto-Generation of NVEF knowledge in Chinese,Computational Linguistics and Chinese Lan-guage Processing, 9(1):41-64.Tsai, Jia-Lin.
2005.
A Study of Applying BTMModel on the Chinese Chunk Bracketing.
Pro-ceedings of IJCNLP, 6th International Work-shop on Linguistically Interpreted Corpora,Jeju Island.Wong, Pak-Kwong and Chorkin ChanWong.
1996.Chinese Word Segmentation.
based on Maxi-mum Matching and Word Binding Force.
Pro-ceedings of the 16th International conferenceon Computational linguistic, 1:200-203.145
