Extracting Noun Phrases from Large-Scale Texts:A Hybrid Approach and Its Automatic EvaluationKuang-hua Chen and Hsin-Hsi ChenDepar tment  o f  Computer  Science and In format ion Engineer ingNat ional  Ta iwan Univers i tyTaipei, Taiwan, R.O.C.Internet: hh_chen@csie,  ntu.
edu.
twAbstractTo acquire noun phrases from running texts is useful formany applications, uch as word grouping, terminologyindexing, etc.
The reported literatures adopt pureprobabilistic approach, or pure rule-based noun phrasesgrammar to tackle this problem.
In this paper, we applya probabilistic chunker to deciding the implicitboundaries of constituents and utilize the linguisticknowledge to extract he noun phrases by a finite statemechanism.
The test texts are SUSANNE Corpus andthe results are evaluated by comparing the parse field ofSUSANNE Corpus automatically.
The results of thispreliminary experiment are encouraging.1.
IntroductionFrom the cognitive point of view, human being mustrecognize, learn and understand the entities or concepts(concrete or abstract) in the texts for natural languagecomprehension.
These entities or concepts are usuallydescribed by noun phrases.
The evidences from thelanguage learning of children also show the belief (Snowand Ferguson, 1977).
Therefore, if we can grasp thenoun phases of the texts, we will understand the texts tosome extent.
This consideration is also captured bytheories of discourse analysis, such as DiscourseRepresentation Theory (Kamp, 1981).Traditionally, to make out the noun phrases in a textmeans to parse the text and to resolve the attachmentrelations among the constituents.
However, parsing thetext completely is very difficult, since variousambiguities cannot be resolved solely by syntactic orsemantic information.
Do we really need to fully parsethe texts in every application?
Some researchers applyshallow or partial parsers (Smadja, 1991; Hindle, 1990)to acquiring specific patterns from texts.
These tell usthat it is not necessary to completely parse the texts forsome applications.This paper will propose a probabilistic partial parserand incorporate linguistic knowledge to extract nounphrases.
The partial parser is motivated by an intuition(Abney, 1991):(1) When we read a sentence, we read it chunk bychunk.Abney uses two level grammar ules to implement theparser through pure LR parsing technique.
The firstlevel grammar ule takes care of the chunking process.The second level grammar ule tackles the attachmentproblems among chunks.
Historically, our statistics-based partial parser is called chunker.
The chunkerreceives tagged texts and outputs a linear chunksequences.
We assign a syntactic head and a semantichead to each chunk.
Then, we extract the plausiblemaximal noun phrases according to the information ofsyntactic head and semantic head, and a finite statemechanism with only 8 states.Section 2 will give a brief review of the works for theacquisition of noun phrases.
Section 3 will describe thelanguage model for chunker.
Section 4 will specify howto apply linguistic knowledge to assigning heads to eachchunk.
Section 5 will list the experimental results ofchunker.
Following Section 5, Section 6 will give theperformance of our work on the retrieval of noun phrases.The possible extensions of the proposed work will bediscussed in Section 7.
Section 8 will conclude theremarks.2.
Previous WorksChurch (1988) proposes a part of speech tagger and asimple noun phrase extractor.
His noun phrase extractorbrackets the noun phrases of input tagged texts accordingto two probability matrices: one is starting noun phrasematrix; the other is ending noun phrase matrix.
Themethodology is a simple version of Garside and Leech'sprobabilistic parser (1985).
Church lists a sample text inthe Appendix of his paper to show the performance of hiswork.
It demonstrates only 5 out of 248 noun phrases areomitted.
Because the tested text is too small to assess theresults, the experiment for large volume of texts is needed.234Bourigault (1992) reports a tool, LEXTER,  forextracting terminologies from texts.
LEXTER triggerstwo-stage processing: 1) ana lys i s  (by identification offrontiers), which extracts the maximal-length nounphrase: 2) pars ing  (the maximal-length noun phrases),which, furthermore, acquires the terminology embeddedin the noun phrases.
Bourigault declares the LEXTERextracts 95?/'0 maximal-length noun phrases, that is,43500 out of 46000 from test corpus.
The result isvalidated by an expert.
However, the precision is notreported in the Boruigault's paper.Voutilainen (1993) announces NPtoo l  for acquisitionof maximal-length noun phrases.
NPtool applies twofinite state mechanisms (one is NP-hostile; the other isNP-friendly) to the task.
The two mechanisms producetwo NP sets and any NP candidate with at least oneoccurrence in both sets will be labeled as the "ok" NP.The reported recall is 98.5-100% and the precision is 95-98% validated manually by some 20000 words.
But fromthe sample text listed in Appendix of his paper, the recallis about 85%, and we can find some inconsistenciesamong these extracted noun phrases.3.
Language ModelParsing can be viewed as optimizing.
Suppose an n-word sentencc, w j, w 2 .
.
.
.
.
w (including punctuationmarks), the parsing task is to find a parsing tree T, suchthat P(7\]w l, w e .
.
.
.
.
w n) has the maximal probability.
Wedefine T here to be a sequence of chunks, cp  c 2 .
.
.
.
.
c m,and each c (0 < i <_ m)  contains one or more words wj(0 < j _< n).
For example, the sentence "parsing can beviewed as optimization."
consists of 7 words.
Its onepossible parsing result under our demand is:(2) \[parsing\] [can be viewed\] \[as optimization\] \[.\]C 1 C2 C3 C4Now, the parsing task is to find the best chunk sequence,('*.
such that(3) C*=argmaxP((, Iw,)Tile ('i is one possible chunk sequence, c\], C 2 .
.
.
.
.
Cmi ,where m i is the number of chunks of the possible chunksequence.
To chunk raw text without other informationis ve.ry difficult, since the word patterns are manymillions.
Therefore, we apply a tagger to preprocessingthe raw texts and give each word a unique part of speech.That is.
for an n-word sentence, w 1, w 2 .
.
.
.
.
w n (includingpunctuation marks), we assign part of speeches t l, t 2 .
.
.
.
.t n to the respective words.
Now the real working modelis:(4) C* = argmaxP(C~lt,")Using bi-gram language model, we then reduce P(C i l t  1,t 2 .
.
.
.
.
tn) as (5),(5) n ~ n P(C, It, ) = P,(c, It, )r~C n _~ l - I  P,(c, lc,_,,t~)?
t\],( ,it, )k=l-~ l - I  P,(c.ic._,) ?
P,(c.)k=lwhere Pi(  " ) denotes the probability for the i'th chunksequence and c o denotes the beginning mark of asentence.
Following (5), formula (4) becomes(6) argmaxP(C~lt~')= argmaxl- I P (c, Ic,_, ) x P (c,)k=l= argmax~l log(P  ~ (c, Ic,_, )) + log(P~ (c , ) ) lk=lIn order to make the expression (6) match the intuition ofhuman being, namely, 1) the scoring metrics are allpositive, 2) large value means high score, and 3) thescores are between 0 and 1, we define a score functionS(P(  ? ))
shown as (7).
(7) S(P(  ? ))
= 0 when P( ? )
= 0;S(P( .
))= 1.0/(1.0+ABS(Iog(P(. ))))
o/w.We then rewrite (6) as (8).
(8) C* = argmaxP(C, It,")n~-= argmaxI-  P,(q \[c._,) x P, (c.)f=l= argmax Z \[log(P~ (c, Ic,_, )) + log(P~ (c,))lk=lr~= argmaxE 18(P ~ (c. Ic._, )) + S(P, (c.)) lk=lThe final language model is to find a chunk sequence C*,which satisfies the expression (8).Dynamic programming shown in (9) is used to findthe best chunk sequence.
The score \ [ i \ ]  denotes the scoreof position i.
The words between position pre \ [ i \ ]  andposition i form the best chunk from the viewpoint ofposition i.
The dscore(cO is the score of the probability235P(ci) and the cscore(ci\[ci-l) is the score of the probabilityP(cilci-l).
These scores are collected from the trainingcorpus, SUSANNE corpus (Sampson, 1993; Sampson,1994).
The details will be touched on in Section 5.
(9) Algorithminput : word sequence wl, w2 ..... wn, andthe corresponding POS sequence t~, t2 ..... tnoutput : a sequence of chunks c~, c2, ..., Cm1.
score\[0\] = 0;prel0l = 0,2. for (i = 1: i<n+l; i++) do 3 and 4;3. j*= maxarg (score\[prelJ\]l+dscore(cj)+cscore(cjlcj-1));0~_j<iwhere cj = tj+~ ..... ti;Cj-1 = tpre\[j\]+l .
.
.
.
.
tj;4. score\[il=scorelpreiJ*ll+dscore(cj*)+cscore(cj*lq*-0;prelil = j*:5. for (i=n; i>0; i=preli\]) dooutput he word Wpre\[i\]+l ..... wi to form a chunk;4.
L ingu is t ic  KnowledgeIn order to assign a head to each chunk, we first definepriorities of POSes.
X'-theory (Sells, 1985) has definedthe X'-equivalences shown as Table 1.Table 1.
X'-EquivalencesR t, ~X"NPV V' VPA A' APp p' ppINFL S (I') S' (IP)Table 1 defines five different phrasal structures and thehierarchical structures.
The heads of these phrasalstructures are the first level of X'-Equivalences, that is, X.The other grammatical constituents function as thespecifiers or modifiers, that is, they are accompanyingwords not core words.
Following this line, we define theprimary priority of POS listed in Table 1.
(10) Primary POS priority 1 : V > N > A > PIn order to extract the exact head, we further defineSecondary POS priority among the 134 POSes defined inLOB corpus (Johansson, 1986).
(11) Secondary POS priority is a linearprecedence relationship within the primarypriorities for coarse POSesI We do not consider the INFL.
since our model will not touch on thisstructure.For example, LOB corpus defines four kinds of verbialwords under the coarse POS V: VB*, DO*, BE* andHV* 2.
The secondary priority within the coarse POS Vis:(12) VB* > I-iV* > DO* > BE*Furthermore, we define the semantic head and thesyntactic head (Abney, 1991).
(13) Semantic head is the head of a phraseaccording to the semantic usage; butsyntactic head is the head based on thegrammatical relations.Both the syntactic head and the semantic head are usefulin extracting noun phrases.
For example, if the semantichead of a chunk is the noun and the syntactic one is thepreposition, it would be a prepositional phrase.Therefore, it can be connected to the previous nounchunk to form a new noun phrase.
In some case, we willfind some chunks contain only one word, called one-word chunks.
They maybe contain a conjunction, e.g.,that.
Therefore.
the syntactic head and the semantichead of one-word chunks are the word itself.Following these definitions, we extract the nounphrases by procedure (14):(14) (a)Co)(c)(d)Tag the input sentences.Partition the tagged sentences intochunks by using a probabilistic partialparser.Decide the syntactic head and thesemantic head of each chunk.According to the syntactic and thesemantic heads, extract noun phrasefrom these chunks and connect asmany noun phrases as possible by afinite state mechanism.raw tagged chunked(TAo- PER) NPso,Figure 1.
The Noun Phrases Extraction ProcedureFigure 1 shows the procedure.
The input raw texts willbe assigned POSes to each word and then pipelined into2 Asterisk * denotes wildcard.
Therefore, VB* represents VB (verb,base form), VBD (verb, preterite), VBG (present participle), VBN (pastparticiple) and VBZ (3rd singular form of verb).236a chunker.
The tag sets of LOB and SUSANNE aredifferent.
Since the tag set of SUSANNE corpus issubsumed by the tag set of LOB corpus, a TAG-MAPPER is used to map tags of SUSANNE corpus tothose of LOB corpus.
The chunker will output asequence of chunks.
Finally, a finite state NP-TRACTOR will extract NPs.
Figure 2 shows the finitestate mechanism used in our work.CD** J .
"~  ~ ' .
r , f f~*  VBN orP'l  _,..,N~w-w,~ "~ '~ VBN o~ i~--,,w~ k~Figure 2.
The Finite State Machine for Noun PhrasesThe symbols in Figure 2 are tags of LOB corpus.
N*denotes nous: P* denotes pronouns; J* denotes adjectives;A* denotes quantifiers, qualifiers and determiners; INdenotes prepositions: CD* denotes cardinals; OD*denotes ordinals, and NR* denotes adverbial nouns.Asterisk * denotes a wildcard.
For convenience, someconstraints, such as syntactic and semantic headchecking, are not shown in Figure 2.5.
First Stage of  ExperimentsFollowing the procedures depicted in Figure 1, weshould train a chunker firstly.
This is done by using theSUSANNE Corpus (Sampson, 1993; Sampson, 1994) asthe training texts.
The SUSANNE Corpus is a modifiedand condensed version of Brown Corpus (Francis andKucera, 1979).
It only contains the 1/10 of BrownCorpus, but involves more information than BrownCorpus.
The Corpus consists of four kinds of texts: 1) A:press reportage; 2) G: belles letters, biography, memoirs;3) J: learned writing; and 4) N: adventure and Westernfiction.
The Categories of A, G, J and N are named fromrespective categories of the Brown Corpus.
EachCategory consists of 16 files and each file contains about2000 words.The following shows a snapshot of SUSANNE Corpus.G01:00\ ]0a  - YB ~minbrk> \[Oh.
Oh\]G0\ ] :O0\ ]0b  - J J  NORTHERN nor thern  \ [O\ [S\ [Np:s .G01:0010c  NN2 l ibera l s  l ibera l  .Np:s\]G0 \ ] :0010d - VBR are be \[Vab.
Vab\]G0\ ] :0010e  AT the the  \[Np:e.G0 l :0010f  JB ch ie f  ch ie fG0\]:f l010g - NN2 suppor ters  suppor terG01:0010h - IO of  of  \[Po.G01:0010 i  - J J  c iv i l  c iv i \ ]  \[Np.G01:0010 j  - NN2 r ights  r ight  .Np\]G01:0020a  - CC and and !Po~.G01:0020b - IO of ofG01:0020c  NNIu  in tegrat ion  in tegrat ion  .Po+\ ]Po \ ]Np:e I5 \ ]G01:0020d - YF +.Table 2 lists basic statistics of SUSANNE Corpus.Table 2.
The Overview of SUSANNE CorpusC~e~ofies \[ Files \[ Paragraphs I Sentences \[ WordsA 16 767 1445 37'180G 16 280 1554 37583J 16 197 1353 36554N 16 723 2568 38736To~l I 64 I 1967 I 6920 I 150053In order to avoid the errors introduced by tagger, theSUSANNE corpus is used as the training and testingtexts.
Note the tags of SUSANNE corpus are mapped toLOB corpus.
The 3/4 of texts of each categories ofSUSANNE Corpus are both for training the chunker andtesting the chunker (inside test).
The rest texts are onlyfor testing (outside test).
Every tree structure containedin the parse field is extracted to form a potential chunkgrammar and the adjacent tree structures are alsoextracted to form a potential context chunk grammar.After the training process, total 10937 chunk grammarrules associated with different scores and 37198 contextchunk grammar rules are extracted.
These chunkgrammar rules are used in the chunking process.Table 3 lists the time taken for processing SUSANNEcorpus.
This experiment is executed on the Sun Sparc10, model 30 workstation, T denotes time, W word, Cchunk, and S sentence.
Therefore, T/W means the timetaken to process a word on average.\[,AGJNAv.
IITable 3.
The Processing TimeT/W T/C T/S0.00295 0.0071 0.07580.00283 0.0069 0.06850.00275 0.0073 0.07430.00309 0.0066 0.04670.00291 1 0.0()70 \] 0.0663According to Table 3, to process a word needs 0.00291seconds on average.
To process all SUSANNE corpusneeds about 436 seconds, or 7.27 minutes.In order to evaluate the performance of our chunker,we compare the results of our chunker with thedenotation made by the SUSANNE Corpus.
Thiscomparison is based on the following criterion:(15) The content of each chunk should bedominated by one non-terminal node inSUSANNE parse field.237This criterion is based on an observation that each non-terminal node has a chance to dominate a chunk.Table 4 is the experimental results of testing theSUSANNE Corpus according to the specified criterion.As usual, the symbol C denotes chunk and S denotessentence.Table 4.
Experimental Results[t Cat.
C" [ -S --# of correct 4866 380 10480 1022A # of incorrect 40 14 84 29total# 4906 394 10564 1051correct rate 0.99 0.96 0.99 0.97# o f  cor rec t  4748 355 10293 1130G # of incorrect 153 32 133 37total# 4901 387 10426 1167correct rate 0.97 0.92 0.99 0,97# of correct 4335 283 9193 1032J # of incorrect 170 15 88 23total# 4505 298 9281 1055correct rate 0.96 0.95 0.99 0,98# of correct 5163 536 12717 1906N # of incorrect 79 42 172 84total# 5242 578 12889 1990correct rate 0,98 0.93 0.99 0.96# of correct 19112 1554 42683 5090Av.
# of incorrect 442 103 477 173total# 19554 1657 43160 5263correct rate 0.98 0.94 0.99 0.97Table 4 shows the chunker has more than 98% chunkcorrect rate and 94% sentence correct rate in outside test,and 99% chunk correct rate and 97% sentence correctrate in inside test.
Note that once a chunk is mischopped,the sentence is also mischopped.
Therefore, sentencecorrect rate is always less than chunk correct rate.Figure 3 gives a direct view of the correct rate of thischunker.10.940 9209II g8 .
.
.
.Chunk Sentence Chunk SetltenceOuts ide Test  Inside TestFigure 3.
The Correct Rate of Experiments6.
Acquis i t ion of  Noun PhrasesWe employ the SUSANNE Corpus as test corpus.
Sincethe SUSANNE Corpus is a parsed corpus, we may use itas criteria for evaluation.
The volume of test texts isaround 150,000 words including punctuation marks.The time needed from inputting texts of SUSANNECorpus to outputting the extracted noun phrases is listedin Table 5.
Comparing with Table 3, the time ofcombining chunks to form the candidate noun phrases isnot significant.Table 5.
Time for Acquisition of Noun PhrasesIIAGJNTotal IIWords Time (see.)
Time/Word37180 112.32 0.0030237583 108.80 0.0028936554 103.04 0.0028238736 122.72 0.00317150053 I 446.88 I 0.00298The evaluation is based on two metrics: precision andrecall.
Precision means the correct rate of what thesystem gets.
Recall indicates the extent o which the realnoun phrases retrieved from texts against he real nounphrases contained in the texts.
Table 6 describes how tocalculate these metrics.Table 6.
Contingency Table for Evaluation1 SUSANNENP ] non-NP]l NP syst?m ,l .on NP }} a I bThe rows of "System" indicate our NP-TRACTOR thinksthe candidate as an NP or not an NP: the columns of"SUSANNE" indicate SUSANNE Corpus takes thecandidate as an NP or not an NP.
Following Table 6, wewill calculate precision and recall shown as (16).
(16) Precision = a/(a+b) * 100%Recall = a/(a+c) * 100%To calculate the precision and the recall based on theparse field of SUSANNE Corpus is not sostraightforward at the first glance.
For example, (17) 3itself is a noun phrse but it contains four noun phrases.A tool for extracting noun phrases should output whatkind of and how many noun phrases, when it processesthe texts like (17).
Three kinds of noun phrases(maximal noun phrases, minimal noun phrases andordinary noun phrases) are defined first.
Maximal nounphrases are those noun phrases which are not containedin other noun phrases.
In contrast, minimal nounphrases do not contain any other noun phrases.3 This example is taken from N06:0280d-N06:0290d, Susanne Corpus(N06 means file N06, 0280 and 0290 are the original line numbers inBrown Corpus.
Recall that the Susanne Corpus is a modified and reducedversion of Brown Corpus).238Apparently, a noun phrase may be both a maximal nounphrase and a minimal noun phrase.
Ordinary nounphrases are noun phrases with no restrictions.
Take (17)as an example.
It has three minimal noun phrases, onemaximal noun phrases and five ordinary noun phrases.In general, a noun-phrase extractor forms the front endof other applications, e.g., acquisition of verbsubcategorization frames.
Under this consideration, it isnot appropriate to taking (17) as a whole to form a nounphrase.
Our system will extract wo noun phrases from(17).
"a black badge of frayed respectability" and "hisneck".
(17) ilia black badge\] of lfrayed respectabilityllthat ought never to have left \[his neck\]\]We calculate the numbers of maximal noun phrases,minimal noun phrases and ordinary noun phrasesdenoted in SUSANNE Corpus, respectively and comparethese numbers with the number of noun phrasesextracted by our system.Table 7 lists the number of ordinary noun phrases(NP), maximal noun phrases (MNP), minimal nounphrases (mNP) in SUSANNE Corpus.
MmNP denotesthe maximal noun phrases which are also the minimalnoun phrases.
On average, a maximal noun phrasesubsumes 1.61 ordinary noun phrases and 1.09 minimalnoun phrases.Table 7.
The Number of Noun Phrases in CorpusAGJNTotaljNP\[ MNPI mNPIMmNPI NP I mNP  MNP10063 5614 6503 3207 1.79 1.169221 5451 6143 3226 1.69 1.138696 4568 5200 2241 1.90 1.149851 7895 7908 5993 1.25 1.0037831 23528 25754 14667 1.61 1.09To calculate the precision, we examine the extractednoun phrases (ENP) and judge the correctness by theSUSANNE Corpus.
The CNP denotes the correctordinary noun phrases, CMNP the correct maximal nounphrases.
CmNP correct minimal noun phrases andCMmNP the correct maximal noun phrases which arealso the minimal noun phrases.
The results are itemizedin Table 8.
The average precision is 95%.Table 8.
Precision of Our SystemU ENp I I CMNP I CmNP I C nNP I  eci ionA 8011 7660 3709 4348 3047 0.96G 7431 6943 3626 4366 3028 0.93J 6457 5958 2701 3134 2005 0.92N 8861 8559 6319 6637 5808 0.97To~l 30760 29120 16355 18485 13888 0.95Here, the computation of recall is ambiguous to someextent.
Comparing columns CMNP and CmNP in Table8 with columns MNP and mNP in Table 7, 70% of MNPand 72% of mNP in SUSANNE Corpus are extracted, Inaddition, 95% of MmNP is extracted by our system.
Itmeans the recall for extracting noun phrases that existindependently in SUSANNE Corpus is 95%.
What typesof noun phrases are extracted are heavily dependent onwhat applications we will follow.
We will discuss thispoint in Section 7.
Therefore, the real number of theapplicable noun phrases in the Corpus is not known.The number should be between the number of NPs andthat of MNPs.
In the original design for NP-TRACTO1La maximal noun phrase which contains clauses orprepositional phrases with prepositions other than "of' isnot considered as an extracted unit.
As the result, thenumber of such kinds of applicable noun phrases (ANPs)form the basis to calculate recall.
These numbers arelisted in Table 9 and the corresponding recalls are alsoshown.Table 9.
The limitation of Values for RecallAGJNAv,1 ANP CNP7873 76607199 69436278 59588793 855930143 29120I Recall0.970.960.950.970.96The automatic validation of the experimental resultsgives us an estimated recall.
Appendix provides asample text and the extracted noun phrases.
Interestedreaders could examine the sample text and calculaterecall and precision for a comparison.7.
Appl icat ionsIdentification of noun phrases in texts is useful for manyapplications.
Anaphora resolution (Hirst, 1981) is toresolve the relationship of the noun phrases, namely,what the antecedent of a noun phrase is.
The extractednoun phrases can form the set of possible candidates (oruniversal in the terminology of discourse representationtheory).
For acquisition of verb subcategorization frames,to bracket he noun phrases in the texts is indispensable.It can help us to find the boundary of the subject, theobject and the prepositional phrase.
We would use theacquired noun phrases for an application of adjectivegrouping.
The extracted noun phrases may containadjectives which pre-modify the head noun.
We thenutilize the similarity of head nouns to group the adjectives.In addition, we may give the head noun a semantic tag,such as Roget's Thesaurus provides, and then analyze theadjectives.
To automatically produce the index of a book,239we would extract he noun phrases contained in the book,calculate the inverse document frequency (IDF) and theirterm frequency (TF) (Salton, 1991), and screen out theimplausible terms.These applications also have impacts on identifyingnoun phrases.
For applications like anaphora resolutionand acquisition of verb subcategorization frames, themaximal noun phrases are not suitable.
For applicationslike grouping adjectives and automatic book indexing,some kinds of maximal noun phrases, such as nounphrases postmodified by "of" prepositional phrases, aresuitable: but some are not, e.g., noun phrases modified byrelative clauses.8.
Concluding RemarksThe difficulty of this work is how to extract the realmaximal noun phrases.
If we cannot decide theprepositional phrase "over a husband eyes" is licensed bythe verb "pull", we will not know "the wool" and "ahusband eyes" are two noun phrases or form a nounpharse combined by the preposition "over".
(18) to pull the wool over a husband eyesto sell the books of my uncleIn contrast, the noun phrase "the books of my uncle" isso called maximal noun phrase in current context.
Asthe result, we conclude that if we do not resolve PP-attachment problem (Hindle and Rooth, 1993), to theexpected extent, we will not extract he maximal nounphrases.
In our work, the probabilistic hunker decidesthe implicit boundaries between words and the NP-TRACTOR connects the adjacent noun chunks.
When anoun chunk is followed by a preposition chunk, we donot connect the two chunks except he preposition chunkis led by "of' preposition.Comparing with other works, our results areevaluated by a parsed corpus automatically and show thehigh precision.
Although we do not point out the exactrecall, we provide estimated values.
The testing scale islarge enough (about 150,000 words).
In contrast,Church (1988) tests a text and extracts the simple nounphrases only.
Bourigault's work (1992) is evaluatedmanually, and dose not report the precision.
Hence, thereal performance is not known.
The work executed byVoutilainen (1993) is more complex than our work.
Theinput text first is morphologizied, then parsed byconstraint grammar, analyzed by two different nounphrases grammar and finally extracted by theoccurrences.
Like other works, Voutilainen's work isalso evaluated manually.In this paper, we propose a language model to chunktexts.
The simple but effective chunker could be seen asa linear structure parser, and could be applied to manyapplications.
A method is presented to extract he nounphrases.
Most importantly, the relations of maximalnoun phrases, minimal noun phrases, ordinary nounphrases and applicable noun phrases are distinguished inthis work.
Their impacts on the subsequent applicationsare also addressed.
In addition, automatic evaluationprovides a fair basis and does not involve human costs.The experimental results how that this parser is a usefultool for further esearch on large volume of real texts.AcknowledgementsWe are grateful to Dr. Geoffrey Sampson for his kindlyproviding SUSANNE Corpus and the details of tag set toUS.ReferencesAbney, Steven (1991), "Parsing by Chunks," inPrinciple-Based Parsing, Berwick, Abney andTenny (Eds.
), Khiwer Academic Publishers, pp.257-278.Bourigault, Didier (1992), "Surface GrammaticalAnalysis for the Extraction of Terminological NounPhrases," Proceedings of the 15th InternationalConference on Computational Linguistics,COLING-92, Vol.
III, Nantes, France, pp.
977-98 I.Church, Kenneth (1988), "A Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text,"Proceedings of ,Second Conference on AppliedNatural Language Processing, pp.
136-143.Francis, N. and Kucera, H. (1979), Manual ofInformation to Accompany a Standard Sample ofPresentday Edited American English, for Use withDigital Computers, Department of Linguistics,Brown University, Providence, R. I., U.S.A.,original ed.
1964, revised 1971, revised andaugmented 1979.Garside, Roger and Leech, Geoffrey (1985), "AProbabilistic Parser," Proceedings of SecondConference of the European Chapter of the A CL.pp.
166-170.Hindle, Donald (1990), "Noun Classification fromPredicate-Argument S ructures," Proceedings of28th Annual Meeting of ACL, pp.
268-275.Hindle, Donald and Rooth, Mats (1993), "StructuralAmbiguity and Lexical Relations," ComputationalLinguistics, 19(1), pp.
103-120.Hirst, G. (1981), Anaphora in Natural LanguageUnderstanding: a ,Survey, Lecture Notes 119.Springer-Verlag.Johansson, Stig (1986), The Tagged LOB Corpus:Users' Manual, Bergen: Norwegian ComputingCentre for the Humanities.240Kamp, H. (1981), "A Theory of Truth and SemanticRepresentation," Formal Methods in the Study ofLanguage, Vol.
1, (J. Groenendijk, T. Janssen, andM.
Stokhof Eds.
), Mathema-tische Centrum.Salton, G. (1991), "Developments in Automatic TextRetrieval," Science, Vol.
253, pp.
974-979.Sampson, Geoffrey (1993), "The SUSANNE Corpus,"l('AME.lournal, No.
17, pp.
125-127.Sampson, Geoffrey (1994), English for the Computer,Oxford University Press.Sells, Peter (1985), Lectures on Contemporary 5~vntacticTheories, Lecture Notes, No.
3, CSLI.Smadja, Frank (1991), Extracting Collocations fromText.
An Application: Language Generation, Ph.D.Dissertation.
Columbia University, 1991.Snow.
C.E.
and Ferguson, C.A.
(Eds.)
(1977), Talkingto ('hildren: Language lnput and Acquisition,Cambridge, Cambridge University Press.Voutilalnen, Atro (1993), "NPtool, a Detector ofEnglish Noun Phrases."
Proceedings of theWorkshop on l/ery Large Corpora: Academic andIndustrial Perspectives, Ohio State University,Columbus, Ohio, USA, pp.
48-57.Append ixFor demonstration, we list a sample text quoted fromN18:0010a-N18:0250e, SUSANNE Corpus.
Theextracted noun phrases are bracketed.
We could computethe precision and the recall from the text as a referenceand compare the gap with the experimental resultsitemized in Section 6.
In actual, the result shows that thesystem has high precision and recall for the text.I Too_QL many AP people_NNS \] think VB that CS \[ the ATIprimary_JJ purpose_.NN of_IN a AT higher_J JR education_NN \]is -BEZto TO help_ VB I you_PP2 1 mal<e_VB \[ a_AT living NN \] +;_; ~ DTis BEZ not XNOT so RB +,_, for_CS \[ education_NN \] offers ~'BZ\[ all ABN kinds_NN-S of IN dividends_NNS \] +,_, including INhow WRB toTO pull_VB \[ the ATI wool NN \] over_IN \[ a AThusband NN eyes NNS \] while_CS-\[ you_PP2- l are BER having I~VGI an AT-affair NN I with_IN \[ his_PP$ wife_NN \] ~_.
If CS \[ it_PP3 lwere_ BED not_X'NOT for IN \[ an AT old JJ professor NPT\]who WPR made VBD \[ me_PPIO \] rea-d VB \[ the_ATl classics_NN \]\[ I..PPIA \] would_MD have_HV been_BEN stymied_VBN on INwhat WDT to_TO do DO +,_, and CC now RN \[ I_PP1A\]understand VB why_WRl3 \[ they PP3AS \] are_BER \[-classics_NN \] +; ;those DTS who WPR wrote VBD I them PP3OS \] knew VBD\[ people NNS \] and CC what WDT made VBD \[ people-NNS\]tick VB .
.
\[ I_PP1A-\] worked ~'BD for IN \[ my_PP$ Uncle_NPT \](_( \[ +an_AT Uncle NPT by_ll~ marriage_NN \] so_RB \[ you_PP2 \]will MD not XNOT-think VB this DT has HVZ \[ a AT mild JJundercurrent ~\[N of IN incest NN- \] +) ~- who WP-R ran VBDI one_CDl of IN those DTS antique_JJ shops_NNS \] in_IN \[ New JJOrleans NP \] Vieux_&F-W Carre_&FW +,_, \[ the_ATl old JJ French-JJQuarter_NPL \] ._.
\[ The_ATI arrangement NN \] \[ I_PPI,~ \] had HVDwith IN \[ him PP30 \] was_BEDZ to_TO work VB \[ four_CDhours NRS \] I a_AT day_NR 1 ._- \[ The ATI rest N-N of IN the ATItime NR I \[ I_PPIA 1 devoted_VBD to_I/~ painting~VBG or CC to INthose DTS \[ other JJB activities_NNS I \[ a_AT young_J-J and CChealtl~y_JJ man_NN-\] just_RB out IN of_IN \[ college_NN \] finds VCBZinteresting_JJ .
.
\[ I_PP1A \] had HVD \[ a AT one-room JJ studio NN Iwhich WDTR overlooked VBD I an_AT ancient JJ courtyard_NN Ifilled_-VBN with IN l mowers NNS and_CC piants_NNS \] ~..blooming_VBG everlastingly_Rl3 in IN I the ATI southern JJsun_NN \] ._.
I I_PPIA \] had_HVD-come_VBN to IN \[ New JJOrleans_NP \] \[ two CD years_NRS \] earlier_RBR after IN\[ graduating_VBG college_NN \] +,_, partly_RB because CS \[ 1 PPI A IIoved_VBD I the ATI city_NPL \] and_CC partly RB because CSthere_EX was_BEDZ quite_QL \[ a AT noted JJ art NN colony NN Ithere RN .
.
When_CS \[ my_PP$ Uncle NPT \]- offered VBD\[ me_-PPlO \] l aAT  part-time JJ job_NN \] which_WDTR would MDtake VB I care NN \] of_IN I my_PP$ normal_JJ expenses I~NSand_-CC give_Vl3 \[ me_PP10 \] I time_NR \] to_TO paint_VB \[ I_PPIAaccepted_VBD ._.
\[ The_ATI arrangement_NN \] turned VBD out_RPto TO be BE excellent JJ .
.
\[ I_PP1A \] loved VB-D \[ the ATIcity_NPL \] and_CC \[ I_PP1A \] particularly_RB loved VBD \[ the_ATlgaiety_NN and CC spirit_NN \] of_IN \[ Mardi NR-Gras NR \] ._I I_PP1A l hadSlVD seen_VBN I two_CD of IN them PP3OS-\] and_CC\[ we_PPIAS \] would MD soon RB be_BE in_IN-another DT city-wide_JJ +,_, \[ joyous_JJ celebration_NN with IN romance_N-N \] in IN\[ the_ATI air_NN \] +;_; and_CC +,_, when C-S \[ you_PP2 l took V-BD\[ a_AT walk NPL \] l you_PP2 \] never RB knew_VBD what WDT\[ adventure ~IN or CC pair_NN of i-N sparkling_JJ eyes_NNS\]were_BED waiting_Vl3G around_IN \[ the_-ATI next_OD corner_NPL \] ._.241
