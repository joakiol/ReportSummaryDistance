Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 16?23,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsExperiments Using OSTIA for a Language Production TaskDana Angluin and Leonor Becerra-BonacheDepartment of Computer Science, Yale UniversityP.O.Box 208285, New Haven, CT, USA{dana.angluin, leonor.becerra-bonache}@yale.eduAbstractThe phenomenon of meaning-preservingcorrections given by an adult to a childinvolves several aspects: (1) the childproduces an incorrect utterance, whichthe adult nevertheless understands, (2) theadult produces a correct utterance with thesame meaning and (3) the child recognizesthe adult utterance as having the samemeaning as its previous utterance, andtakes that as a signal that its previous ut-terance is not correct according to the adultgrammar.
An adequate model of this phe-nomenon must incorporate utterances andmeanings, account for how the child andadult can understand each other?s mean-ings, and model how meaning-preservingcorrections interact with the child?s in-creasing mastery of language production.In this paper we are concerned with howa learner who has learned to comprehendutterances might go about learning to pro-duce them.We consider a model of language com-prehension and production based on finitesequential and subsequential transducers.Utterances are modeled as finite sequencesof words and meanings as finite sequencesof predicates.
Comprehension is inter-preted as a mapping of utterances to mean-ings and production as a mapping of mean-ings to utterances.
Previous work (Castel-lanos et al, 1993; Pieraccini et al, 1993)has applied subsequential transducers andthe OSTIA algorithm to the problem oflearning to comprehend language; here weapply them to the problem of learning toproduce language.
For ten natural lan-guages and a limited domain of geomet-ric shapes and their properties and rela-tions we define sequential transducers toproduce pairs consisting of an utterancein that language and its meaning.
Usingthis data we empirically explore the prop-erties of the OSTIA and DD-OSTIA al-gorithms for the tasks of learning compre-hension and production in this domain, toassess whether they may provide a basisfor a model of meaning-preserving correc-tions.1 IntroductionThe role of corrections in language learning hasrecently received substantial attention in Gram-matical Inference.
The kinds of corrections con-sidered are mainly syntactic corrections based onproximity between strings.
For example, a cor-rection of a string may be given by using editdistance (Becerra-Bonache et al, 2007; Kinber,2008) or based on the shortest extension of thequeried string (Becerra-Bonache et al, 2006),among others.
In these approaches semantic in-formation is not used.However, in natural situations, a child?s er-roneous utterances are corrected by her parentsbased on the meaning that the child intends to ex-press; typically, the adult?s corrections preservethe intended meaning of the child.
Adults use cor-rections in part as a way of making sure they haveunderstood the child?s intentions, in order to keepthe conversation ?on track?.
Thus the child?s ut-terance and the adult?s correction have the samemeaning, but the form is different.
As Chouinardand Clark point out (2003), because children at-tend to contrasts in form, any change in form thatdoes not mark a different meaning will signal tochildren that they may have produced somethingthat is not acceptable in the target language.
Re-sults in (Chouinard and Clark, 2003) show thatadults reformulate erroneous child utterances of-ten enough to help learning.
Moreover, these re-16sults show that children can not only detect differ-ences between their own utterance and the adultreformulation, but that they do make use of thatinformation.Thus in some natural situations, correctionshave a semantic component that has not been takeninto account in previous Grammatical Inferencestudies.
Some interesting questions arise: Whatare the effects of corrections on learning syntax?Can corrections facilitate the language learningprocess?
One of our long-term goals is to find aformal model that gives an account of this kindof correction and in which we can address thesequestions.
Moreover, such a model might allow usto show that semantic information can simplify theproblem of learning formal languages.A simple computational model of semantics andcontext for language learning incorporating se-mantics was proposed in (Angluin and Becerra-Bonache, 2008).
This model accommodates twodifferent tasks: comprehension and production.That paper focused only on the comprehensiontask and formulated the learning problem as fol-lows.
The teacher provides to the learner severalexample pairs consisting of a situation and an ut-terance denoting something in the situation; thegoal of the learner is to learn the meaning func-tion, allowing the learner to comprehend novel ut-terances.
The results in that paper show that undercertain assumptions, a simple algorithm can learnto comprehend an adult?s utterance in the sense ofproducing the same sequence of predicates, evenwithout mastering the adult?s grammar.
For exam-ple, receiving the utterance the blue square abovethe circle, the learner would be able to produce thesequence of predicates (bl, sq, ab, ci).In this paper we focus on the production task,using sequential and subsequential transducers tomodel both comprehension and production.
Adultproduction can be modeled as converting a se-quence of predicates into an utterance, which canbe done with access to the meaning transducer forthe adult?s language.However, we do not assume that the child ini-tially has access to the meaning transducer forthe adult?s language; instead we assume that thechild?s production progresses through differentstages.
Initially, child production is modeled asconsisting of two different tasks: finding a correctsequence of predicates, and inverting the meaningfunction to produce a kind of ?telegraphic speech?.For example, from (gr, tr, le, sq) the child mayproduce green triangle left square.
Our goal is tomodel how the learner might move from this tele-graphic speech to speech that is grammatical in theadult?s sense.
Moreover, we would like to find aformal framework in which corrections (in form ofexpansions, for example, the green triangle to theleft of the square) can be given to the child dur-ing the intermediate stages (before the learner isable to produce grammatically correct utterances)to study their effect on language learning.We thus propose to model the problem ofchild language production as a machine trans-lation problem, that is, as the task of translat-ing a sequence of predicate symbols (representingthe meaning of an utterance) into a correspond-ing utterance in a natural language.
In this pa-per we explore the possibility of applying existingautomata-theoretic approaches to machine transla-tion to model language production.
In Section 2,we describe the use of subsequential transducersfor machine translation tasks and review the OS-TIA algorithm to learn them (Oncina, 1991).
InSection 3, we present our model of how the learnercan move from telegraphic to adult speech.
In Sec-tion 4, we present the results of experiments in themodel made using OSTIA.
Discussion of these re-sults is presented in Section 5 and ideas for futurework are in Section 6.2 Learning Subsequential TransducersSubsequential transducers (SSTs) are a formalmodel of translation widely studied in the liter-ature.
SSTs are deterministic finite state mod-els that allow input-output mappings between lan-guages.
Each edge of an SST has an associatedinput symbol and output string.
When an in-put string is accepted, an SST produces an out-put string that consists of concatenating the out-put substrings associated with sequence of edgestraversed, together with the substring associatedwith the last state reached by the input string.
Sev-eral phenomena in natural languages can be eas-ily represented by means of SSTs, for example,the different orders of noun and adjective in Span-ish and English (e.g., un cuadrado rojo - a redsquare).
Formal and detailed definitions can befound in (Berstel, 1979).For any SST it is always possible to find anequivalent SST that has the output strings assignedto the edges and states so that they are as close to17the initial state as they can be.
This is called anOnward Subsequential Transducer (OST).It has been proved that SSTs are learnable inthe limit from a positive presentation of sentencepairs by an efficient algorithm called OSTIA (On-ward Subsequential Transducer Inference Algo-rithm) (Oncina, 1991).
OSTIA takes as input a fi-nite training set of input-output pairs of sentences,and produces as output an OST that generalizesthe training pairs.
The algorithm proceeds as fol-lows (this description is based on (Oncina, 1998)):?
A prefix tree representation of all the inputsentences of the training set is built.
Emptystrings are assigned as output strings to boththe internal nodes and the edges of this tree,and every output sentence of the training setis assigned to the corresponding leaf of thetree.
The result is called a tree subsequentialtransducer.?
An onward tree subsequential transducerequivalent to the tree subsequential trans-ducer is constructed by moving the longestcommon prefixes of the output strings, levelby level, from the leaves of the tree towardsthe root.?
Starting from the root, all pairs of states ofthe onward tree subsequential transducer areconsidered in order, level by level, and aremerged if possible (i.e., if the resulting trans-ducer is subsequential and does not contra-dict any pair in the training set).SSTs and OSTIA have been successfully ap-plied to different translation tasks: Roman numer-als to their decimal representations, numbers writ-ten in English to their Spanish spelling (Oncina,1991) and Spanish sentences describing simplevisual scenes to corresponding English and Ger-man sentences (Castellanos et al, 1994).
Theyhave also been applied to language understandingtasks (Castellanos et al, 1993; Pieraccini et al,1993).Moreover, several extensions of OSTIA havebeen introduced.
For example, OSTIA-DR incor-porates domain (input) and range (output) mod-els in the learning process, allowing the algorithmto learn SSTs that accept only sentences compat-ible with the input model and produce only sen-tences compatible with the output model (Oncinaand Varo, 1996).
Experiments with a language un-derstanding task gave better results with OSTIA-DR than with OSTIA (Castellanos et al, 1993).Another extension is DD-OSTIA (Oncina, 1998),which instead of considering a lexicographic orderto merge states, uses a heuristic order based on ameasure of the equivalence of the states.
Experi-ments in (Oncina, 1998) show that better resultscan be obtained by using DD-OSTIA in certaintranslation tasks from Spanish to English.3 From telegraphic to adult speechTo model how the learner can move from tele-graphic speech to adult speech, we reduce thisproblem to a translation problem, in which thelearner has to learn a mapping from sequences ofpredicates to utterances.
As we have seen in theprevious section, SSTs are an interesting approachto machine translation.
Therefore, we explore thepossibility of modeling language production usingSSTs and OSTIA, to see whether they may pro-vide a good framework to model corrections.As described in (Angluin and Becerra-Bonache,2008), after learning the meaning function thelearner is able to assign correct meanings to ut-terances, and therefore, given a situation and anutterance that denotes something in the situation,the learner is able to point correctly to the objectdenoted by the utterance.
To simplify the taskwe consider, we make two assumptions about thelearner at the start of the production phase: (1)the learner?s lexicon represents a correct meaningfunction and (2) the learner can generate correctsequences of predicates.Therefore, in the initial stage of the productionphase, the learner is able to produce a kind of?telegraphic speech?
by inverting the lexicon con-structed during the comprehension stage.
For ex-ample, if the sequence of predicates is (bl, sq, ler,ci), and in the lexicon blue is mapped to bl, squareto sq, right to ler and circle to ci, then by invert-ing this mapping, the learner would produce bluesquare right circle.In order to explore the capability of SSTs andOSTIA to model the next stage of language pro-duction (from telegraphic to adult speech), we takethe training set to be input-output pairs each ofwhich contains as input a sequence of predicates(e.g., (bl, sq, ler, ci)) and as output the correspond-ing utterance in a natural language (e.g., the bluesquare to the right of the circle).
In this example,18the learner must learn to include appropriate func-tion words.
In other languages, the learner mayhave to learn a correct choice of words determinedby gender, case or other factors.
(Note that we arenot yet in a position to consider corrections.
)4 ExperimentsOur experiments were made for a limited domainof geometric shapes and their properties and re-lations.
This domain is a simplification of theMiniature Language Acquisition task proposed byFeldman et al (Feldman et al, 1990).
Previousapplications of OSTIA to language understandingand machine translation have also used adapta-tions and extensions of the Feldman task.In our experiments, we have predicates for threedifferent shapes (circle (ci), square (sq) and tri-angle (tr)), three different colors (blue (bl), green(gr) and red (re)) and three different relations (tothe left of (le), to the right of (ler), and above (ab)).We consider ten different natural languages: Ara-bic, English, Greek, Hebrew, Hindi, Hungarian,Mandarin, Russian, Spanish and Turkish.We created a data sequence of input-outputpairs, each consisting of a predicate sequence anda natural language utterance.
For example, onepair for Spanish is ((ci, re, ler, tr), el circulo rojoa la derecha del triangulo).
We ran OSTIA on ini-tial segments of the sequence of pairs, of lengths10, 20, 30, .
.
., to produce a sequence of subse-quential transducers.
The whole data sequencewas used to test the correctness of the transducersgenerated during the process.
An error is countedwhenever given a data pair (x, y), the subsequen-tial transducer translates x to y?, and y?
6= y. Wesay that OSTIA has converged to a correct trans-ducer if all the transducers produced afterwardshave the same number of states and edges, and 0errors on the whole data sequence.To generate the sequences of input-output pairs,for each language we constructed a meaning trans-ducer capable of producing the 444 different pos-sible meanings involving one or two objects.
Werandomly generated 400 unique (non-repeated)input-output pairs for each language.
This processwas repeated 10 times.
In addition, to investigatethe effect of the order of presentation of the input-output pairs, we repeated the data generation pro-cess for each language, sorting the pairs accordingto a length-lex ordering of the utterances.We give some examples to illustrate the trans-ducers produced.
Figure 1 shows an example ofa transducer produced by OSTIA after just tenpairs of input-output examples for Spanish.
Thistransducer correctly translates the ten predicate se-quences used to construct it, but the data is notsufficient for OSTIA to generalize correctly in allcases, and many other correct meanings are stillincorrectly translated.
For example, the sequence(ci, bl) is translated as el circulo a la izquierda delcirculo verde azul instead of el circulo azul.The transducers produced after convergence byOSTIA and DD-OSTIA correctly translate all 444possible correct meanings.
Examples for Spanishare shown in Figure 2 (OSTIA) and Figure 3 (DD-OSTIA).
Note that although they correctly trans-late all 444 correct meanings, the behavior of thesetwo transducers on other (incorrect) predicate se-quences is different, for example on (tr, tr).1bl/ azulsq/ el cuadradoci/el circulo a laizquierda delcirculo verde2tr/ el triangulole/ler/re/ rojo a la derechadel cuadradosq/ci/bl/ azulgr/ler/ a la derechadel cuadrado3ab/tr/ encima del trianguloci/ verde encima delcirculo azulbl/re/ rojoFigure 1: Production task, OSTIA.
A transducerproduced using 10 random unique input-outputpairs (predicate sequence, utterance) for Spanish.1bl/ azulsq/ el cuadrado2le/ a la izquierda deller/ a la derecha delab/ encima delre/ rojogr/ verdeci/ el circulotr/ el triangulole/ a la izquierda deller/ a la derecha delab/ encima delbl/ azulre/ rojogr/ verdesq/ cuadradoci/ circulotr/ trianguloFigure 2: Production task, OSTIA.
A transducerproduced (after convergence) by using randomunique input-output pairs (predicate sequence, ut-terance) for Spanish.Different languages required very differentnumbers of data pairs to converge.
Statistics onthe number of pairs needed until convergence forOSTIA for all ten languages for both randomunique and random unique sorted data sequencesare shown in Table 1.
Because better results werereported using DD-OSTIA in machine translation191bl/ azulre/ rojogr/ verdesq/ el cuadradoci/ el circulotr/ el triangulo2le/ a la izquierda deller/ a la derecha delab/ encima delsq/ cuadradoci/ circulotr/ trianguloFigure 3: Production task, DD-OSTIA.
A trans-ducer produced (after convergence) using randomunique input-output pairs (predicate-sequence, ut-terance) for Spanish.Language # Pairs # Sorted PairsArabic 150 200English 200 235Greek 375 400Hebrew 195 30Hindi 380 350Hungarian 365 395Mandarin 45 150Russian 270 210Spanish 190 150Turkish 185 80Table 1: Production task, OSTIA.
The entries givethe median number of input-output pairs until con-vergence in 10 runs.
For Greek, Hindi and Hun-garian, the median for the unsorted case is calcu-lated using all 444 random unique pairs, instead of400.tasks (Oncina, 1998), we also tried using DD-OSTIA for learning to translate a sequence ofpredicates to an utterance.
We used the same se-quences of input-output pairs as in the previousexperiment.
The results obtained are shown in Ta-ble 2.We also report the sizes of the transducerslearned by OSTIA and DD-OSTIA.
Table 3 andTable 4 show the numbers of states and edgesof the transducers after convergence for each lan-guage.
In case of disagreements, the number re-ported is the mode.To answer the question of whether productionis harder than comprehension in this setting, wealso considered the comprehension task, that is,to translate an utterance in a natural languageinto the corresponding sequence of predicates.Language # Pairs # Sorted PairsArabic 80 140English 85 180Greek 350 400Hebrew 65 80Hindi 175 120Hungarian 245 140Mandarin 40 150Russian 185 210Spanish 80 150Turkish 50 40Table 2: Production task, DD-OSTIA.
The entriesgive the median number of input-output pairs un-til convergence in 10 runs.
For Greek, Hindi andHungarian, the median for the unsorted case is cal-culated using all 444 random unique pairs, insteadof 400.Languages #states #edgesArabic 2 20English 2 20Greek 9 65Hebrew 2 20Hindi 7 58Hungarian 3 20Mandarin 1 10Russian 3 30Spanish 2 20Turkish 4 31Table 3: Production task, OSTIA.
Sizes of trans-ducers at convergence.The comprehension task was studied by Oncinaet al (Castellanos et al, 1993).
They used En-glish sentences, with a more complex version ofthe Feldman task domain and more complex se-mantic representations than we use.
Our resultsare presented in Table 5.
The number of statesand edges of the transducers after convergence isshown in Table 6.5 DiscussionIt should be noted that because the transducersoutput by OSTIA and DD-OSTIA correctly repro-duce all the pairs used to construct them, once ei-ther algorithm has seen all 444 possible data pairsin either the production or the comprehension task,the resulting transducers will correctly translate allcorrect inputs.
However, state-merging in the al-20Languages #states #edgesArabic 2 17English 2 16Greek 9 45Hebrew 2 13Hindi 7 40Hungarian 3 20Mandarin 1 10Russian 3 23Spanish 2 13Turkish 3 18Table 4: Production task, DD-OSTIA.
Sizes oftransducers at convergence.Languages OSTIA DD-OSTIAArabic 65 65English 60 20Greek 325 60Hebrew 90 45Hindi 60 35Hungarian 40 45Mandarin 60 40Russian 280 55Spanish 45 30Turkish 60 35Table 5: Comprehension task, OSTIA and DD-OSTIA.
Median number (in 10 runs) of input-output pairs until convergence using a sequence of400 random unique pairs of (utterance, predicatesequence).gorithms induces compression and generalization,and the interesting questions are how much datais required to achieve correct generalization, andhow that quantity scales with the complexity ofthe task.
This are very difficult questions to ap-proach analytically, but empirical results can offervaluable insights.Considering the comprehension task (Tables 5and 6), we see that OSTIA generalizes correctlyfrom at most 15% of all 444 possible pairs exceptin the cases of Greek, Hebrew and Russian.
DD-OSTIA improves the OSTIA results, in some casesdramatically, for all languages except Hungarian.DD-OSTIA achieves correct generalization fromat most 15% of all possible pairs for all ten lan-guages.
Because the meaning function for all tenlanguage transducers is independent of the state,in each case there is a 1-state sequential trans-Languages #states #edgesArabic 1 15English 1 13Greek 2 25Hebrew 1 13Hindi 1 13Hungarian 1 14Mandarin 1 17Russian 1 24Spanish 1 14Turkish 1 13Table 6: Comprehension task, OSTIA and DD-OSTIA.
Sizes of transducers at convergence using400 random unique input-output pairs (utterance,predicate sequence).
In cases of disagreement, thenumber reported is the mode.ducer that achieves correct translation of correctutterances into predicate sequences.
OSTIA andDD-OSTIA converged to 1-state transducers forall languages except Greek, for which they con-verged to 2-state transducers.
Examining one suchtransducer for Greek, we found that the require-ment that the transducer be ?onward?
necessitatedtwo states.
These results are broadly compatiblewith the results obtained by Oncina et al (Castel-lanos et al, 1993) on language understanding; themore complex tasks they consider also give evi-dence that this approach may scale well for thecomprehension task.Turning to the production task (Tables 1, 2, 3and 4), we see that providing the random sampleswith a length-lex ordering of utterances has incon-sistent effects for both OSTIA and DD-OSTIA,sometimes dramatically increasing or decreasingthe number of samples required.
We do not fur-ther consider the sorted samples.Comparing the production task with the com-prehension task for OSTIA, the production taskgenerally requires substantially more randomunique samples than the comprehension task forthe same language.
The exceptions are Mandarin(production: 45 and comprehension: 60) and Rus-sian (production: 270 and comprehension: 280).For DD-OSTIA the results are similar, with thesole exception of Mandarin (production: 40 andcomprehension: 40).
For the production task DD-OSTIA requires fewer (sometimes dramaticallyfewer) samples to converge than OSTIA.
How-ever, even with DD-OSTIA the number of sam-21ples is in several cases (Greek, Hindi, Hungarianand Russian) a rather large fraction (40% or more)of all 444 possible pairs.
Further experimentationand analysis is required to determine how these re-sults will scale.Looking at the sizes of the transducers learnedby OSTIA and DD-OSTIA in the production task,we see that the numbers of states agree for all lan-guages except Turkish.
(Recall from our discus-sion in Section 4 that there may be differences inthe behavior of the transducers learned by OSTIAand DD-OSTIA at convergence.)
For the produc-tion task, Mandarin gives the smallest transducer;for this fragment of the language, the translationof correct predicate sequences into utterances canbe achieved with a 1-state transducer.
In contrast,English and Spanish both require 2 states to handlearticles correctly.
For example, in the transducerin Figure 3, the predicate for a circle (ci) is trans-lated as el circulo if it occurs as the first object (instate 1) and as circulo if it occurs as second ob-ject (in state 2) because del has been supplied bythe translation of the intervening binary relation(le, ler, or ab.)
Greek gives the largest transducerfor the production task, with 9 states, and requiresthe largest number of samples for DD-OSTIA toachieve convergence, and one of the largest num-bers of samples for OSTIA.
Despite the evidenceof the extremes of Mandarin and Greek, the rela-tion between the size of the transducer for a lan-guage and the number of samples required to con-verge to it is not monotonic.In our model, one reason that learning the pro-duction task may in general be more difficult thanlearning the comprehension task is that while themapping of a word to a predicate does not dependon context, the mapping of a predicate to a wordor words does (except in the case of our Mandarintransducer.)
As an example, in the comprehensiontask the Russian words triugolnik, triugolnika andtriugonikom are each mapped to the predicate tr,but the reverse mapping must be sensitive to thecontext of the occurrence of tr.These results suggest that OSTIA or DD-OSTIA may be an effective method to learn totranslate sequences of predicates into natural lan-guage utterances in our domain.
However, some ofour objectives seem incompatible with the proper-ties of OSTIA.
In particular, it is not clear howto incorporate the learner?s initial knowledge ofthe lexicon and ability to produce ?telegraphicspeech?
by inverting the lexicon.
Also, the in-termediate results of the learning process do notseem to have the properties we expect of a learnerwho is progressing towards mastery of produc-tion.
That is, the intermediate transducers per-fectly translate the predicate sequences used toconstruct them, but typically produce other trans-lations that the learner (using the lexicon) wouldknow to be incorrect.
For example, the intermedi-ate transducer from Figure 1 translates the predi-cate sequence (ci) as el circulo a la izquierda delcirculo verde, which the learner?s lexicon indicatesshould be translated as (ci, le, ci, gr).6 Future workFurther experiments and analysis are required tounderstand how these results will scale with largerdomains and languages.
In this connection, it maybe interesting to try the experiments of (Castel-lanos et al, 1993) in the reverse (production) di-rection.
Finding a way to incorporate the learner?sinitial lexicon seems important.
Perhaps by incor-porating the learner?s knowledge of the input do-main (the legal sequences of predicates) and usingthe domain-aware version, OSTIA-D, the interme-diate results in the learning process would be morecompatible with our modeling objectives.
Copingwith errors will be necessary; perhaps an explic-itly statistical framework for machine translationshould be considered.If we can find an appropriate model of howthe learner?s language production process mightevolve, then we will be in a position to modelmeaning-preserving corrections.
That is, thelearner chooses a sequence of predicates and mapsit to a (flawed) utterance.
Despite its flaws, thelearner?s utterance is understood by the teacher(i.e., the teacher is able to map it to the sequenceof predicates chosen by the learner) and respondswith a correction, that is, a correct utterance forthat meaning.
The learner, recognizing that theteacher?s utterance has the same meaning but adifferent form, then uses the correct utterance (aswell as the meaning and the incorrect utterance) toimprove the mapping of sequences of predicates toutterances.It is clear that in this model, corrections are notnecessary to the process of learning comprehen-sion and production; once the learner has a correctlexicon, the utterances of the teacher can be trans-lated into sequences of predicates, and the pairs22of (predicate sequence, utterance) can be used tolearn (via an appropriate variant of OSTIA) a per-fect production mapping.
However, it seems verylikely that corrections can make the process oflearning a production mapping easier or faster, andfinding a model in which such phenomena can bestudied remains an important goal of this work.7 AcknowledgmentsThe authors sincerely thank Prof. Jose Oncinafor the use of his programs for OSTIA and DD-OSTIA, as well as his helpful and generous ad-vice.
The research of Leonor Becerra-Bonachewas supported by a Marie Curie InternationalFellowship within the 6th European CommunityFramework Programme.ReferencesDana Angluin and Leonor Becerra-Bonache.
2008.Learning Meaning Before Syntax.
ICGI, 281?292.Leonor Becerra-Bonache, Colin de la Higuera, J.C.Janodet, and Frederic Tantini.
2007.
Learning Ballsof Strings with Correction Queries.
ECML, 18?29.Leonor Becerra-Bonache, Adrian H. Dediu, andCristina Tirnauca.
2006.
Learning DFA from Cor-rection and Equivalence Queries.
ICGI, 281?292.Jean Berstel.
1979.
Transductions and Context-FreeLanguages.
PhD Thesis, Teubner, Stuttgart, 1979.Antonio Castellanos, Enrique Vidal, and Jose Oncina.1993.
Language Understanding and SubsequentialTransducers.
ICGI, 11/1?11/10.Antonio Castellanos, Ismael Galiano, and Enrique Vi-dal.
1994.
Applications of OSTIA to machine trans-lation tasks.
ICGI, 93?105.Michelle M. Chouinard and Eve V. Clark.
2003.
AdultReformulations of Child Errors as Negative Evi-dence.
Journal of Child Language, 30:637?669.Jerome A. Feldman, George Lakoff, Andreas Stolcke,and Susan Hollback Weber.
1990.
Miniature Lan-guage Acquisition: A touchstone for cognitive sci-ence.
Technical Report, TR-90-009.
InternationalComputer Science Institute, Berkeley, California.April, 1990.Efim Kinber.
2008.
On Learning Regular Expres-sions and Patterns Via Membership and CorrectionQueries.
ICGI, 125?138.Jose Oncina.
1991.
Aprendizaje de lenguajes regu-lares y transducciones subsecuenciales.
PhD Thesis,Universitat Politecnica de Valencia, Valencia, Spain,1998.Jose Oncina.
1998.
The data driven approach appliedto the OSTIA algorithm.
ICGI, 50?56.Jose Oncina and Miguel Angel Varo.
1996.
Using do-main information during the learing of a subsequen-tial transducer.
ICGI, 301?312.Roberto Pieraccini, Esther Levin, and Enrique Vidal.1993.
Learning how to understand language.
Eu-roSpeech?93, 448?458.23
