Proceedings of the Eighteenth Conference on Computational Language Learning, pages 21?29,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsReconstructing Native Language Typology from Foreign Language UsageYevgeni BerzakCSAIL MITberzak@mit.eduRoi ReichartTechnion IITroiri@ie.technion.ac.ilBoris KatzCSAIL MITboris@mit.eduAbstractLinguists and psychologists have longbeen studying cross-linguistic transfer, theinfluence of native language properties onlinguistic performance in a foreign lan-guage.
In this work we provide empiricalevidence for this process in the form of astrong correlation between language simi-larities derived from structural features inEnglish as Second Language (ESL) textsand equivalent similarities obtained fromthe typological features of the native lan-guages.
We leverage this finding to re-cover native language typological similar-ity structure directly from ESL text, andperform prediction of typological featuresin an unsupervised fashion with respect tothe target languages.
Our method achieves72.2% accuracy on the typology predic-tion task, a result that is highly competi-tive with equivalent methods that rely ontypological resources.1 IntroductionCross-linguistic transfer can be broadly describedas the application of linguistic structure of aspeaker?s native language in the context of anew, foreign language.
Transfer effects may beexpressed on various levels of linguistic perfor-mance, including pronunciation, word order, lex-ical borrowing and others (Jarvis and Pavlenko,2007).
Such traces are prevalent in non-nativeEnglish, and in some cases are even cele-brated in anecdotal hybrid dialect names such as?Frenglish?
and ?Denglish?.Although cross-linguistic transfer was exten-sively studied in Psychology, Second LanguageAcquisition (SLA) and Linguistics, the conditionsunder which it occurs, its linguistic characteristicsas well as its scope remain largely under debate(Jarvis and Pavlenko, 2007; Gass and Selinker,1992; Odlin, 1989).In NLP, the topic of linguistic transfer wasmainly addressed in relation to the Native Lan-guage Identification (NLI) task, which requires topredict the native language of an ESL text?s au-thor.
The overall high performance on this classi-fication task is considered to be a central piece ofevidence for the existence of cross-linguistic trans-fer (Jarvis and Crossley, 2012).
While the successon the NLI task confirms the ability to extract na-tive language signal from second language text, itoffers little insight into the linguistic mechanismsthat play a role in this process.In this work, we examine the hypothesis thatcross-linguistic structure transfer is governed bythe typological properties of the native language.We provide empirical evidence for this hypothe-sis by showing that language similarities derivedfrom structural patterns of ESL usage are stronglycorrelated with similarities obtained directly fromthe typological features of the native languages.This correlation has broad implications on theability to perform inference from native languagestructure to second language performance and viceversa.
In particular, it paves the way for a noveland powerful framework for comparing nativelanguages through second language performance.This framework overcomes many of the inher-ent difficulties of direct comparison between lan-guages, and the lack of sufficient typological doc-umentation for the vast majority of the world?s lan-guages.Further on, we utilize this transfer enabledframework for the task of reconstructing typolog-ical features.
Automated prediction of languagetypology is extremely valuable for both linguisticstudies and NLP applications which rely on suchinformation (Naseem et al., 2012; T?ackstr?om etal., 2013).
Furthermore, this task provides an ob-jective external testbed for the quality of our native21language similarity estimates derived from ESLtexts.Treating native language similarities obtainedfrom ESL as an approximation for typologicalsimilarities, we use them to predict typologicalfeatures without relying on typological annotationfor the target languages.
Our ESL based methodyields 71.4% ?
72.2% accuracy on the typology re-construction task, as compared to 69.1% ?
74.2%achieved by typology based methods which de-pend on pre-existing typological resources for thetarget languages.To summarize, this paper offers two main con-tributions.
First, we provide an empirical resultthat validates the systematic existence of linguistictransfer, tying the typological characteristics of thenative language with the structural patterns of for-eign language usage.
Secondly, we show that ESLbased similarities can be directly used for predic-tion of native language typology.
As opposed toprevious approaches, our method achieves strongresults without access to any a-priori knowledgeabout the target language typology.The remainder of the paper is structured as fol-lows.
Section 2 surveys the literature and positionsour study in relation to previous research on cross-linguistic transfer and language typology.
Section3 describes the ESL corpus and the database oftypological features.
In section 4, we delineateour method for deriving native language similar-ities and hierarchical similarity trees from struc-tural features in ESL.
In section 5 we use typolog-ical features to construct another set of languagesimilarity estimates and trees, which serve as abenchmark for the typological validity of the ESLbased similarities.
Section 6 provides a correla-tion analysis between the ESL based and typologybased similarities.
Finally, in section 7 we reportour results on typology reconstruction, a task thatalso provides an evaluation framework for the sim-ilarity structures derived in sections 4 and 5.2 Related WorkOur work integrates two areas of research, cross-linguistic transfer and linguistic typology.2.1 Cross-linguistic TransferThe study of cross-linguistic transfer has thus farevolved in two complementary strands, the lin-guistic comparative approach, and the computa-tional detection based approach.
While the com-parative approach focuses on case study basedqualitative analysis of native language influenceon second language performance, the detectionbased approach revolves mainly around the NLItask.Following the work of Koppel et al.
(2005), NLIhas been gaining increasing interest in NLP, cul-minating in a recent shared task with 29 partici-pating systems (Tetreault et al., 2013).
Much ofthe NLI efforts thus far have been focused on ex-ploring various feature sets for optimizing classifi-cation performance.
While many of these featuresare linguistically motivated, some of the discrimi-native power of these approaches stems from cul-tural and domain artifacts.
For example, our pre-liminary experiments with a typical NLI featureset, show that the strongest features for predictingChinese are strings such as China and in China.Similar features dominate the weights of other lan-guages as well.
Such content features boost clas-sification performance, but are hardly relevant formodeling linguistic phenomena, thus weakeningthe argument that NLI classification performanceis indicative of cross-linguistic transfer.Our work incorporates an NLI component, butdeparts from the performance optimization orien-tation towards leveraging computational analysisfor better understanding of the relations betweennative language typology and ESL usage.
In par-ticular, our choice of NLI features is driven bytheir relevance to linguistic typology rather thantheir contribution to classification performance.
Inthis sense, our work aims to take a first step to-wards closing the gap between the detection andcomparative approaches to cross-linguistic trans-fer.2.2 Language TypologyThe second area of research, language typology,deals with the documentation and comparativestudy of language structures (Song, 2011).
Muchof the descriptive work in the field is summa-rized in the World Atlas of Language Structures(WALS)1(Dryer and Haspelmath, 2013) in theform of structural features.
We use the WALS fea-tures as our source of typological information.Several previous studies have used WALS fea-tures for hierarchical clustering of languages andtypological feature prediction.
Most notably, Tehet al.
(2007) and subsequently Daum?e III (2009)1http://wals.info/22predicted typological features from language treesconstructed with a Bayesian hierarchical cluster-ing model.
In Georgi et al.
(2010) additional clus-tering approaches were compared using the samefeatures and evaluation method.
In addition to thefeature prediction task, these studies also evalu-ated their clustering results by comparing them togenetic language clusters.Our approach differs from this line of workin several aspects.
First, similarly to our WALSbased baselines, the clustering methods presentedin these studies are affected by the sparsity ofavailable typological data.
Furthermore, thesemethods rely on existing typological documen-tation for the target languages.
Both issues areobviated in our English based framework whichdoes not depend on any typological informationto construct the native language similarity struc-tures, and does not require any knowledge aboutthe target languages except from the ESL essays ofa sample of their speakers.
Finally, we do not com-pare our clustering results to genetic groupings,as to our knowledge, there is no firm theoreticalground for expecting typologically based cluster-ing to reproduce language phylogenies.
The em-pirical results in Georgi et al.
(2010), which showthat typology based clustering differs substantiallyfrom genetic groupings, support this assumption.3 Datasets3.1 Cambridge FCEWe use the Cambridge First Certificate in English(FCE) dataset (Yannakoudakis et al., 2011) as oursource of ESL data.
This corpus is a subset ofthe Cambridge Learner Corpus (CLC)2.
It con-tains English essays written by upper-intermediatelevel learners of English for the FCE examination.The essay authors represent 16 native lan-guages.
We discarded Dutch and Swedish speak-ers due to the small number of documents avail-able for these languages (16 documents in total).The remaining documents are associated with thefollowing 14 native languages: Catalan, Chinese,French, German, Greek, Italian, Japanese, Korean,Polish, Portuguese, Russian, Spanish, Thai andTurkish.
Overall, our corpus comprises 1228 doc-uments, corresponding to an average of 87.7 doc-uments per native language.2http://www.cambridge.org/gb/elt/catalogue/subject/custom/item36466033.2 World Atlas of Language StructuresWe collect typological information for the FCEnative languages from WALS.
Currently, thedatabase contains information about 2,679 ofthe world?s 7,105 documented living languages(Lewis, 2014).
The typological feature list has 188features, 175 of which are present in our dataset.The features are associated with 9 linguistic cat-egories: Phonology, Morphology, Nominal Cate-gories, Nominal Syntax, Verbal Categories, WordOrder, Simple Clauses, Complex Sentences andLexicon.
Table 1 presents several examples forWALS features and their range of values.One of the challenging characteristics of WALSis its low coverage, stemming from lack of avail-able linguistic documentation.
It was previouslyestimated that about 84% of the language-featurepairs in WALS are unknown (Daum?e III, 2009;Georgi et al., 2010).
Even well studied languages,like the ones used in our work, are lacking valuesfor many features.
For example, only 32 of theWALS features have known values for all the 14languages of the FCE corpus.
Despite the preva-lence of this issue, it is important to bear in mindthat some features do not apply to all languages bydefinition.
For instance, feature 81B Languageswith two Dominant Orders of Subject, Object, andVerb is relevant only to 189 languages (and hasdocumented values for 67 of them).We perform basic preprocessing, discarding 5features that have values for only one language.Further on, we omit 19 features belonging to thecategory Phonology as comparable phonologicalfeatures are challenging to extract from the ESLtextual data.
After this filtering, we remain with151 features, 114.1 features with a known valueper language, 10.6 languages with a known valueper feature and 2.5 distinct values per feature.Following previous work, we binarize all theWALS features, expressing each feature in termsof k binary features, where k is the number ofvalues the original feature can take.
Note thatbeyond the well known issues with feature bi-narization, this strategy is not optimal for someof the features.
For example, the feature 111ANon-periphrastic Causative Constructions whosepossible values are presented in table 1 wouldhave been better encoded with two binary featuresrather than four.
The question of optimal encodingfor the WALS feature set requires expert analysisand will be addressed in future research.23ID Type Feature Name Values26A Morphology Prefixing vs. Suffixing in Little affixation, Strongly suffixing, WeaklyInflectional Morphology suffixing, Equal prefixing and suffixing,Weakly prefixing, Strong prefixing.30A Nominal Number of Genders None, Two, Three, Four, Five or more.Categories83A Word Order Order of Object and Verb OV, VO, No dominant order.111A Simple Clauses Non-periphrastic Causative Neither, Morphological but no compound,Constructions Compound but no morphological, Both.Table 1: Examples of WALS features.
As illustrated in the table examples, WALS features can takedifferent types of values and may be challenging to encode.4 Inferring Language Similarities fromESLOur first goal is to derive a notion of similarity be-tween languages with respect to their native speak-ers?
distinctive structural usage patterns of ESL.
Asimple way to obtain such similarities is to traina probabilistic NLI model on ESL texts, and in-terpret the uncertainty of this classifier in distin-guishing between a pair of native languages as ameasure of their similarity.4.1 NLI ModelThe log-linear NLI model is defined as follows:p(y|x; ?)
=exp(?
?
f(x, y))?y??Yexp(?
?
f(x, y?
))(1)where y is the native language, x is the observedEnglish document and ?
are the model parame-ters.
The parameters are learned by maximizingthe L2 regularized log-likelihood of the trainingdata D = {(x1, y1), ..., (xn, yn)}.L(?)
=n?i=1log p(yi|xi; ?)?
???
?2(2)The model is trained using gradient ascent with L-BFGS-B (Byrd et al., 1995).
We use 70% of theFCE data for training and the remaining 30% fordevelopment and testing.As our objective is to relate native language andtarget language structures, we seek to control forbiases related to the content of the essays.
As pre-viously mentioned, such biases may arise from theessay prompts as well as from various cultural fac-tors.
We therefore define the model using only un-lexicalized morpho-syntactic features, which cap-ture structural properties of English usage.Our feature set, summarized in table 2, containsfeatures which are strongly related to many of thestructural features in WALS.
In particular, we usefeatures derived from labeled dependency parses.These features encode properties such as the typesof dependency relations, ordering and distance be-tween the head and the dependent.
Additionalsyntactic information is obtained using POS n-grams.
Finally, we consider derivational and in-flectional morphological affixation.
The annota-tions required for our syntactic features are ob-tained from the Stanford POS tagger (Toutanovaet al., 2003) and the Stanford parser (de Marneffeet al., 2006).
The morphological features are ex-tracted heuristically.4.2 ESL Based Native Language SimilarityEstimatesGiven a document x and its author?s native lan-guage y, the conditional probability p(y?|x; ?)
canbe viewed as a measure of confusion between lan-guages y and y?, arising from their similarity withrespect to the document features.
Under this in-terpretation, we derive a language similarity ma-trix S?ESLwhose entries are obtained by averagingthese conditional probabilities on the training setdocuments with the true label y, which we denoteas Dy= {(xi, y) ?
D}.S?ESLy,y?=???1|Dy|?
(x,y)?Dyp(y?|x; ?)
if y?6= y1 otherwise(3)For each pair of languages y and y?, the matrixS?ESLcontains an entry S?ESLy,y?which capturesthe average probability of mistaking y for y?, andan entry S?ESLy?,y, which represents the opposite24Feature Type ExamplesUnlexicalized labeled dependencies Relation = prep Head = VBN Dependent = INOrdering of head and dependent Ordering = right Head = NNS Dependent = JJDistance between head and dependent Distance = 2 Head = VBG Dependent = PRPPOS sequence between head and dependent Relation = det POS-between = JJPOS n-grams (up to 4-grams) POS bigram = NN VBZInflectional morphology Suffix = ingDerivational morphology Suffix = ityTable 2: Examples of syntactic and morphological features of the NLI model.
The feature values are setto the number of occurrences of the feature in the document.
The syntactic features are derived from theoutput of the Stanford parser.
A comprehensive description of the Stanford parser dependency annotationscheme can be found in the Stanford dependencies manual (de Marneffe and Manning, 2008).confusion.
We average the two confusion scores toreceive the matrix of pairwise language similarityestimates SESL.SESLy,y?= SESLy?,y=12(S?ESLy,y?+ S?ESLy?,y)(4)Note that comparable similarity estimates canbe obtained from the confusion matrix of the clas-sifier, which records the number of misclassifica-tions corresponding to each pair of class labels.The advantage of our probabilistic setup over thismethod is its robustness with respect to the actualclassification performance of the model.4.3 Language Similarity TreeA particularly informative way of representinglanguage similarities is in the form of hierarchi-cal trees.
This representation is easier to inspectthan a similarity matrix, and as such, it can bemore instrumental in supporting linguistic inquiryon language relatedness.
Additionally, as we showin section 7, hierarchical similarity trees can out-perform raw similarities when used for typologyreconstruction.We perform hierarchical clustering using theWard algorithm (Ward Jr, 1963).
Ward is abottom-up clustering algorithm.
Starting with aseparate cluster for each language, it successivelymerges clusters and returns the tree of clustermerges.
The objective of the Ward algorithm isto minimize the total within-cluster variance.
Tothis end, at each step it merges the cluster pairthat yields the minimum increase in the overallwithin-cluster variance.
The initial distance ma-trix required for the clustering algorithm is de-fined as 1 ?
SESL.
We use the Scipy implemen-tation3of Ward, in which the distance between anewly formed cluster a ?
b and another cluster cis computed with the Lance-Williams distance up-date formula (Lance and Williams, 1967).5 WALS Based Language SimilaritiesIn order to determine the extent to which ESLbased language similarities reflect the typologicalsimilarity between the native languages, we com-pare them to similarities obtained directly from thetypological features in WALS.The WALS based similarity estimates betweenlanguages y and y?are computed by measuring thecosine similarity between the binarized typologi-cal feature vectors.SWALSy,y?=vy?
vy??vy??vy??
(5)As mentioned in section 3.2, many of the WALSfeatures do not have values for all the FCE lan-guages.
To address this issue, we experiment withtwo different strategies for choosing the WALSfeatures to be used for language similarity compu-tations.
The first approach, called shared-all, takesinto account only the 32 features that have knownvalues in all the 14 languages of our dataset.
Inthe second approach, called shared-pairwise, thesimilarity estimate for a pair of languages is deter-mined based on the features shared between thesetwo languages.As in the ESL setup, we use the two matricesof similarity estimates to construct WALS basedhierarchical similarity trees.
Analogously to theESL case, a WALS based tree is generated by the3http://docs.scipy.org/.../scipy.cluster.hierarchy.linkage.html25Figure 1: shared-pairwise WALS based versusESL based language similarity scores.
Each pointrepresents a language pair, with the vertical axiscorresponding to the ESL based similarity andthe horizontal axis standing for the typologicalshared-pairwise WALS based similarity.
Thescores correlate strongly with a Pearson?s coeffi-cient of 0.59 for the shared-pairwise constructionand 0.50 for the shared-all feature-set.Ward algorithm with the input distance matrix 1?SWALS.6 Comparison ResultsAfter independently deriving native language sim-ilarity matrices from ESL texts and from typo-logical features in WALS, we compare them toone another.
Figure 1 presents a scatter plotof the language similarities obtained using ESLdata, against the equivalent WALS based similar-ities.
The scores are strongly correlated, with aPearson Correlation Coefficient of 0.59 using theshared-pairwise WALS distances and 0.50 usingthe shared-all WALS distances.This correlation provides appealing evidencefor the hypothesis that distinctive structural pat-terns of English usage arise via cross-linguistictransfer, and to a large extent reflect the typologi-cal similarities between the respective native lan-guages.
The practical consequence of this result isthe ability to use one of these similarity structuresto approximate the other.
Here, we use the ESLbased similarities as a proxy for the typologicalsimilarities between languages, allowing us to re-construct typological information without relyingon a-priori knowledge about the target languagetypology.In figure 2 we present, for illustration purposes,the hierarchical similarity trees obtained with theWard algorithm based on WALS and ESL similar-ities.
The trees bear strong resemblances to oneother.
For example, at the top level of the hier-archy, the Indo-European languages are discernedfrom the non Indo-European languages.
Furtherdown, within the Indo-European cluster, the Ro-mance languages are separated from other Indo-European subgroups.
Further points of similaritycan be observed at the bottom of the hierarchy,where the pairs Russian and Polish, Japanese andKorean, and Chinese and Thai merge in both trees.In the next section we evaluate the quality ofthese trees, as well as the similarity matrices usedfor constructing them with respect to their abilityto support accurate nearest neighbors based recon-struction of native language typology.7 Typology PredictionAlthough pairwise language similarities derivedfrom structural features in ESL texts are highlycorrelated with similarities obtained directly fromnative language typology, evaluating the absolutequality of such similarity matrices and trees ischallenging.We therefore turn to typology prediction basedevaluation, in which we assess the quality ofthe induced language similarity estimates by theirability to support accurate prediction of unseen ty-pological features.
In this evaluation mode weproject unknown WALS features to a target lan-guage from the languages that are closest to it inthe similarity structure.
The underlying assump-tion of this setup is that better similarity structureswill lead to better accuracies in the feature predic-tion task.Typological feature prediction not only pro-vides an objective measure for the quality of thesimilarity structures, but also has an intrinsic valueas a stand-alone task.
The ability to infer typolog-ical structure automatically can be used to createlinguistic databases for low-resource languages,and is valuable to NLP applications that exploitsuch resources, most notably multilingual parsing(Naseem et al., 2012; T?ackstr?om et al., 2013).Prediction of typological features for a targetlanguage using the language similarity matrix isperformed by taking a majority vote for the valueof each feature among the K nearest languages ofthe target language.
In case none of the K nearestlanguages have a value for a feature, or given a tie26(a) Hierarchical clustering using WALS based shared-pairwise distances.
(b) Hierarchical clustering using ESL based distances.Figure 2: Language Similarity Trees.
Both treesare constructed with the Ward agglomerative hi-erarchical clustering algorithm.
Tree (a) uses theWALS based shared-pairwise language distances.Tree (b) uses the ESL derived distances.between several values, we iteratively expand thegroup of nearest languages until neither of thesecases applies.To predict features using a hierarchical clustertree, we set the value of each target language fea-ture to its majority value among the members ofthe parent cluster of the target language, excludingthe target language itself.
For example, using thetree in figure 2(a), the feature values for the targetlanguage French will be obtained by taking ma-jority votes between Portuguese, Italian and Span-ish.
Similarly to the matrix based prediction, miss-ing values and ties are handled by backing-off to alarger set of languages, in this case by proceedingto subsequent levels of the cluster hierarchy.
Forthe French example in figure 2(a), the first fall-back option will be the Romance cluster.Following the evaluation setups in Daum?e III(2009) and Georgi et al.
(2010), we evaluate theWALS based similarity estimates and trees by con-structing them using 90% of the WALS features.We report the average accuracy over 100 randomfolds of the data.
In the shared-all regime, we pro-vide predictions not only for the remaining 10%of features shared by all languages, but also for allthe other features that have values in the target lan-guage and are not used for the tree construction.Importantly, as opposed to the WALS basedprediction, our ESL based method does not re-quire any typological features for inferring lan-guage similarities and constructing the similaritytree.
In particular, no typological information isrequired for the target languages.
Typological fea-tures are needed only for the neighbors of the tar-get language, from which the features are pro-jected.
This difference is a key advantage of ourapproach over the WALS based methods, whichpresuppose substantial typological documentationfor all the languages involved.Table 3 summarizes the feature reconstructionresults.
The ESL approach is highly competitivewith the WALS based results, yielding comparableaccuracies for the shared-all prediction, and lag-ging only 1.7% ?
3.4% behind the shared-pairwiseconstruction.
Also note that for both WALS basedand ESL based predictions, the highest results areachieved using the hierarchical tree predictions,confirming the suitability of this representation foraccurately capturing language similarity structure.Figure 3 presents the performance of thestrongest WALS based typological feature com-pletion method, WALS shared-pairwise tree, as afunction of the percentage of features used for ob-taining the language similarity estimates.
The fig-ure also presents the strongest result of the ESLmethod, using the ESL tree, which does not re-quire any such typological training data for ob-taining the language similarities.
As can be seen,the WALS based approach would require access toalmost 40% of the currently documented WALSfeatures to match the performance of the ESLmethod.The competitive performance of our ESLmethod on the typology prediction task underlines27Method NN 3NN TreeWALS shared-all 71.6 71.4 69.1WALS shared-pairwise 73.1 74.1 74.2ESL 71.4 70.7 72.2Table 3: Typology reconstruction results.
Threetypes of predictions are compared, nearest neigh-bor (NN), 3 nearest neighbors (3NN) and near-est tree neighbors (Tree).
WALS shared-all areWALS based predictions, where only the 32 fea-tures that have known values in all 14 languagesare used for computing language similarities.
Inthe WALS shared-pairwise predictions the lan-guage similarities are computed using the WALSfeatures shared by each language pair.
ESL re-sults are obtained by projection of WALS featuresfrom the closest languages according to the ESLlanguage similarities.its ability to extract strong typologically drivensignal, while being robust to the partial nature ofexisting typological annotation which hinders theperformance of the baselines.
Given the smallamount of ESL data at hand, these results arehighly encouraging with regard to the prospectsof our approach to support typological inference,even in the absence of any typological documen-tation for the target languages.8 Conclusion and OutlookWe present a novel framework for utilizing cross-linguistic transfer to infer language similaritiesfrom morpho-syntactic features of ESL text.
Trad-ing laborious expert annotation of typological fea-tures for a modest amount of ESL texts, weare able to reproduce language similarities thatstrongly correlate with the equivalent typologybased similarities, and perform competitively ona typology reconstruction task.Our study leaves multiple questions for futureresearch.
For example, while the current work ex-amines structure transfer, additional investigationis required to better understand lexical and phono-logical transfer effects.Furthermore, we currently focuse on native lan-guage typology, and assume English as the foreignlanguage.
This limits our ability to study the con-straints imposed on cross-linguistic transfer by theforeign language.
An intriguing research directionwould be to explore other foreign languages andcompare the outcomes to our results on English.Figure 3: Comparison of the typological fea-ture completion performance obtained using theWALS tree with shared-pairwise similarities andthe ESL tree based typological feature comple-tion performance.
The dotted line represents theWALS based prediction accuracy, while the hor-izontal line is the ESL based accuracy.
Thehorizontal axis corresponds to the percentage ofWALS features used for constructing the WALSbased language similarity estimates.Finally, we plan to formulate explicit modelsfor the relations between specific typological fea-tures and ESL usage patterns, and extend our ty-pology induction mechanisms to support NLP ap-plications in the domain of multilingual process-ing.AcknowledgmentsWe would like to thank Yoong Keok Lee, JesseHarris and the anonymous reviewers for valuablecomments on this paper.
This material is basedupon work supported by the Center for Brains,Minds, and Machines (CBMM), funded by NSFSTC award CCF-1231216, and by Google FacultyResearch Award.ReferencesRichard H Byrd, Peihuang Lu, Jorge Nocedal, andCiyou Zhu.
1995.
A limited memory algorithm forbound constrained optimization.
SIAM Journal onScientific Computing, 16(5):1190?1208.Hal Daum?e III.
2009.
Non-parametric Bayesian areallinguistics.
In Proceedings of human language tech-nologies: The 2009 annual conference of the northamerican chapter of the association for computa-tional linguistics, pages 593?601.
Association forComputational Linguistics.28Marie-Catherine de Marneffe and Christopher D Man-ning.
2008.
Stanford typed dependencies manual.URL http://nlp.
stanford.
edu/software/dependenciesmanual.
pdf.Marie-Catherine de Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generatingtyped dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Matthew S. Dryer and Martin Haspelmath, editors.2013.
WALS Online.
Max Planck Institute for Evo-lutionary Anthropology, Leipzig.Susan M Gass and Larry Selinker.
1992.
LanguageTransfer in Language Learning: Revised edition,volume 5.
John Benjamins Publishing.Ryan Georgi, Fei Xia, and William Lewis.
2010.Comparing language similarity across genetic andtypologically-based groupings.
In Proceedings ofthe 23rd International Conference on Computa-tional Linguistics, pages 385?393.
Association forComputational Linguistics.Scott Jarvis and Scott A Crossley.
2012.
Approachinglanguage transfer through text classification: Explo-rations in the detection-based approach, volume 64.Multilingual Matters.Scott Jarvis and Aneta Pavlenko.
2007.
Crosslinguis-tic influence in language and cognition.
Routledge.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.2005.
Determining an author?s native language bymining a text for errors.
In Proceedings of theeleventh ACM SIGKDD international conference onKnowledge discovery in data mining, pages 624?628.
ACM.Godfrey N Lance and William Thomas Williams.1967.
A general theory of classificatory sortingstrategies ii.
clustering systems.
The computer jour-nal, 10(3):271?277.M.
Paul Lewis.
2014.
Ethnologue: Languages of theworld.
www.ethnologue.com.Tahira Naseem, Regina Barzilay, and Amir Globerson.2012.
Selective sharing for multilingual dependencyparsing.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 629?637.
Asso-ciation for Computational Linguistics.Terence Odlin.
1989.
Language transfer: Cross-linguistic influence in language learning.
Cam-bridge University Press.J.J.
Song.
2011.
The Oxford Handbook of LinguisticTypology.
Oxford Handbooks in Linguistics.
OUPOxford.Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.2013.
Target language adaptation of discriminativetransfer parsers.
Proceedings of NAACL-HLT.Yee Whye Teh, Hal Daum?e III, and Daniel M Roy.2007.
Bayesian agglomerative clustering with co-alescents.
In NIPS.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.2013.
A report on the first native language identi-fication shared task.
NAACL/HLT 2013, page 48.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Joe H Ward Jr. 1963.
Hierarchical grouping to opti-mize an objective function.
Journal of the Americanstatistical association, 58(301):236?244.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In ACL, pages 180?189.29
