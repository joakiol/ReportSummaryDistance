Memory-Based Named Entity Recognition using Unannotated DataFien De Meulder and Walter DaelemansCNTS - Language Technology GroupUniversity of Antwerp{Fien.DeMeulder,Walter.Daelemans}@ua.ac.beAbstractWe used the memory-based learner Timbl(Daelemans et al, 2002) to find names in En-glish and German newspaper text.
A first sys-tem used only the training data, and a numberof gazetteers.
The results show that gazetteersare not beneficial in the English case, whilethey are for the German data.
Type-token gen-eralization was applied, but also reduced per-formance.
The second system used gazetteersderived from the unannotated corpus, as well asthe ratio of capitalized versus uncapitalized useof each word.
These strategies gave an increasein performance.1 IntroductionThis paper describes a memory-based approach to learn-ing names in English and German newspaper text.The first system used no unannotated data - only theprovided training material, and a number of gazetteers.It was shown that the gazetteers made for a better per-formance in the German task, but not in the English task.Type-token generalization was helpful for neither Englishnor German.The second system used unannotated data, but only forthe English task.
The extra data were used in two ways:first, more gazetteers were derived from the corpus byexploiting conjunctions: if in a conjunction of capitalizedstrings one string is recognized as being a certain typeof name, the other strings are assumed to be of the sametype and stored in a new gazetteer.
This list was then usedto construct an additional feature for training the machinelearning algorithm.
The second approach counts how of-ten each word form in the additional corpus is capitalized,and how often it is not.
This is used as another feature forthe learning algorithm.2 Memory Based LearningWe used Timbl (Daelemans et al, 2002), a memory-based learner.
When presented with training instances,the learner stores them all, and then classifies new dataon the basis of its k nearest neighbours in the training set.Before classification, the learner assigns weights to eachof the features, marking their importance for the learningtask.
Features with higher weights are treated as moreimportant in classification as those with lower weights.Timbl has some parameters which can be adjusted inorder to improve learning.
For the NER system describedin this paper, we varied the parameters k and m. k is thenumber of nearest neighbours Timbl looks at.
m deter-mines the feature metrics, i.e.
the importance weightsgiven to each feature, and the way similarity betweenvalues of the same feature is computed.
This parame-ter can be adjusted separately for each feature.
The twometrics used were weighted overlap and modified valuedifference.3 System 1: Description3.1 FeaturesFor the basic English system, 37 features were used.
Thefirst seven features were the lowercase versions of the fo-cus word, and a context of three words to the left and theright.
The next seven features were the part-of-speechtags of the same seven words.
Then followed seven fea-tures indicating for each of the seven words if they werecapitalized or not.
The next six features represented thefirst and last three letters of the word to be classified.These features were included in order to make it possiblefor the memory-based learner to use word-internal infor-mation.
Frequent prefixes and suffixes can thus be usedto learn names.
Finally, ten features indicated if the focusword appears in any of the gazetteers used for this task.These gazetteers are discussed in more detail in the nextsection.For the German system, the same features were used,with an additional seven features: for each word in theseven-word window, the stem of the word was also in-cluded.3.2 GazetteersTen gazetteers were used to provide features.
Thesegazetteers listed names of all four kinds, as well as wordswhich often appear inside names (such as International(for organization names) and de (for person names)).3.3 Type ?
token generalizationA module was created to generalize NE tags from typesto tokens.
It is a simple program which assumes that iftwo capitalized words have the same form, they will alsohave the same NE tag.
This is potentially problematic,because many words can be used either as part of a nameor not, and in this case it indeed proved to be unhelpful.4 System 2: DescriptionFor the extended English system, four more features wereadded to each instance: the first four indicated if the focusword was part of a named entity found in a list of namedentities derived from the unannotated data.
The secondnew feature indicated if the focusword is capitalized oruncapitalized most often in the unannotated data.4.1 Gazetteers extracted from conjunctionsFirst, potential names were identified in the unannotateddata.
This was done using the gazetteers which wereused for the first system, and a simple grammar of names.Then we looked for all conjunctions of capitalized stringsin the unannotated data.
If one of the strings was taggedin its entirety as being of one NE type, and no otherstrings in the conjunction had another NE tag, it was hy-pothesized that all strings in this conjunction were of thesame type.
All strings would then be stored in a gazetteerof NEs of that type.The next step was to add four more features to thetraining and test sets of the NE system.
In the training andtest texts, strings of capitalized words were matched withthe strings in the newly made gazetteers.
All instanceswere enlarged by four binary features, one for each typeof NE (L, M, O, P).
These features are on when the fo-cus word (and its context in the case of a longer name)matches a string in the associated gazetteer, and off whenit does not.4.2 Ratio of capitalized to non-capitalizedoccurrence of tokensA last feature added to all instances indicated if the focusword (the word to be classified) appears more often cap-italized or uncapitalized in the unannotated corpus.
Thisapproach has been used earlier by (Collins, 2002).
Inorder to make this feature, a list was made of all word-forms, converted to lowercase, in the corpus, and the ra-tio of capitalized to uncapitalized occurrences.
The extrafeature was binary: on if a word appears more often cap-italized than not, and off otherwise.5 System 1: Discussion of results5.1 Role of gazetteersTwo experiments were run to assess the importance ofthe gazetteers in this experiment: the first used only theword to be classified and its context, the second used bi-nary features indicating inclusion in gazetteers, as wellas the features used in the first experiment.
Perhaps sur-prisingly, the English system did worse when gazetteerinformation was used.
This was true using the default pa-rameter settings, and also after (limited) separate param-eter optimization.
The German system did slightly betteron the development data when gazetteers were used.The difference between the English and German sys-tems is very surprising, as the lists were not adjusted toinclude extra German names.
They contain mainly En-glish and Dutch names, as a result of previous work onDutch and English.
In order to find an explanation, welooked at the performance (not optimized) of the lists ontheir own, not using any context or word-internal infor-mation at all.
The result did not make things at all clearer:the precision of the lists on the German data was striking,even more so than on the English data.English devel.
Precision Recall F?=1No gazetteers 84.09% 85.20% 84.64With gazetteers 78.27% 78.11% 78.19Only gazetteers 49.20% 33.82% 40.08German devel.
Precision Recall F?=1No gazetteers 60.63% 48.36% 53.80With gazetteers 61.35% 49.87% 55.02Only gazetteers 29.53% 5.75% 9.62Table 1: Role of gazetteers5.2 Type ?
token generalizationType-token generalization was attempted only on the En-glish data.
The intuition behind this approach is that amemory-based learner may recognize a name due to itscontext, but it will not generalize the classification toother tokens of the same type.
However, a concern isthat mistakes will be introduced by generalizing ambigu-ous words to the wrong type, and by repeating mistakeswhich would otherwise occur only sporadically.
In theend, introducing generalization did not make much ofa difference.
While precision declines marginally (twomore phrases were incorrectly tagged as names), recall isunaffected.The results in Table 2 were derived using Timbl withdefault parameters.
The lack of optimization explains thelow result even without generalization.5.3 Parameter optimization and feature selectionParameter optimization was used both for system 1 andfor system 2.
This was combined with limited featureselection.
The difference feature selection can make, isalready obvious from the results above, and will be shownEnglish devel.
Precision Recall F?=1No generalization 75.90% 82.88% 79.23With generalization 75.87% 82.88% 79.22Table 2: Role of type ?
token generalizationin the rest of the paper also.
Parameter optimization canhave a major effect on performance of machine learningsystems in general, and Timbl in particular, as can be seenin Table 3.As was shown by Daelemans and Hoste (2002), param-eter optimization and feature selection heavily interact inmachine learning: separate optimization leads to inferiorresults to interleaved optimization.
Different parametersettings might be best for different feature selections, andvice versa.
It would therefore be best to optimize bothat the same time, treating feature selection and parameteroptimization together as one search space.
This was doneto a very limited extent for this problem, but because ofthe time needed for each experiment, a full search of thesolution space was impossible.Another restriction is the fact that not all parameters ofthe learner were optimized, again due to time constraints.The two that were found to have a great effect were usedonly.
These are k, the number of nearest neighbours takeninto account when classifying a new instance, and m, thefeature metric.
m was toggled between weighted overlapand modified value difference.The results shown in Table 3 are those on the consis-tently best featureset found, i.e.
the one using all infor-mation minus gazetteers.On the German data, parameter optimization and fea-ture selection were also found to be beneficial, but opti-mization had to be cut short due to time constraints.English devel.
Precision Recall F?=1k=1, overlap 75.88% 82.88% 79.22k=1, mvdm 82.28% 84.69% 83.47k=3, overlap 74.04% 80.51% 77.14k=3, mvdm 84.09% 85.20% 84.64k=5, overlap 72.67% 79.21% 75.80k=5, mvdm 83.94% 84.77% 84.35Table 3: Role of parameter optimization6 System 2: Discussion of resultsIn this system, extra information is added to the train-ing set in the following way: the number of the instancesin the training set remains the same, but the number offeatures for each instance is increased.
The informationfor the extra instances is found in the unannotated data,so this should bring the benefit of using this extra infor-mation source.
At the same time, only the hand-taggedtraining set is used, which means that no extra noise isintroduced into the training set.6.1 Gazetteers extracted from conjunctionsIn this step, four new features were added to each instancein the training and test sets, one for each type of NE.Even though gazetteers were already in use, we ex-tracted new gazetteers from the unannotated data.
Thehope was that these gazetteers would be more useful forthis particular task, as they would be corpus-specific.
Thegazetteers which were used originally, and which did notimprove performance, were mainly taken off the inter-net, and partially hand-crafted.
This means that they aregeneral-purpose gazetteers.
Also, they were a mixtureof Dutch and English names.
The new gazetteers wereonly English, and only included those names which werefound in the Reuters corpus.Once the gazetteers were extracted, their entries werematched against the text in the training data.
When astring of words in the training data matched a name, thiswould be reflected in the new features.
For example, ifNew York was found both in the locations gazetteer and inthe training set, then both New and York would receive afeature value Ltag (for location tag) for the newly addedlocation feature.
The results in Table 4 show that thisstrategy was successful.The results were found using Timbl with default set-tings.English devel.
Precision Recall F?=1Only context 75.88% 82.88% 79.22With old lists 70.40% 75.73% 72.97With new lists 77.58% 83.81% 80.58Table 4: Effect of corpus-specific gazetteers6.2 Ratio of capitalized to non-capitalizedoccurrence of tokensNext, another feature was added to the training and testinstances.
This feature is another binary feature, and it in-dicates if the focus word of the instance is found more of-ten in its capitalized form, or in its non-capitalized form.This feature can help the process of NER in differentways.
One of them is the identification of sentence-initialwords.
They are always capitalized in English, but if theytend to appear uncapitalized more often, they are proba-bly not a name.
Another way they can help is in findingwords which are sometimes names, and sometimes ordi-nary words (e.g.
Apple).
They should not be tagged as aname if the uncapitalized version occurs more frequently.This approach was also successful.
Results shown inTable 5 were once again obtained by using Timbl withdefault settings.English devel.
Precision Recall F?=1No cap.
info 75.88% 82.88% 79.22With cap.
info 77.18% 84.20% 80.54Table 5: Effect of capitalization/non-capitalization ratio6.3 Combination of conjunction lists andcapitalization informationFinally, all features were combined, and a number of op-timization and (limited) feature selection runs were exe-cuted.
The best run found used all five of the extra fea-tures derived from the unannotated data.
This is goodnews, because it means that using unannotated data canhelp to improve NER of English.Both results shown in Table 6 are those of the best runsafter optimization.English devel.
Precision Recall F?=1No extra data 84.09% 85.20% 84.64With extra data 84.75% 87.06% 85.89Table 6: Effect of using unannotated data and optimiza-tion runs7 ConclusionIn the plain learning problem (i.e.
using only annotateddata), our system used only context and word-internal in-formation.
Type ?
token generalization was never benefi-cial, and gazetteers helped only for the German task.When using unannotated data, performance was im-proved in two ways: extra gazetteers were constructed byexploiting conjunctions, and words which appear mostlyin capitalized form were set apart from those that do not.8 AcknowledgementsThe authors would like to thank Erik Tjong Kim Sang forhis help with data preprocessing, as well as for helpfulhints in the construction of the system.ReferencesMichael Collins.
2002.
Ranking Algorithms for Named-Entity Extraction: Boosting and the Voted Perceptron.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages489?496, Philadelphia.English devel.
Precision Recall F?=1LOC 87.59% 91.02% 89.27MISC 84.97% 81.56% 83.23ORG 74.54% 79.27% 76.83PER 89.49% 91.53% 90.50overall 84.75% 87.06% 85.89English test Precision Recall F?=1LOC 77.25% 87.35% 81.99MISC 71.67% 73.50% 72.57ORG 70.36% 68.03% 69.18PER 81.52% 81.01% 81.27overall 75.84% 78.13% 76.97German devel.
Precision Recall F?=1LOC 56.75% 69.43% 62.45MISC 74.82% 41.78% 53.62ORG 52.49% 36.58% 43.11PER 67.74% 50.96% 58.17overall 61.35% 49.87% 55.02German test Precision Recall F?=1LOC 59.68% 62.22% 60.93MISC 66.49% 37.01% 47.56ORG 48.77% 35.83% 41.31PER 76.67% 61.59% 68.31overall 63.93% 51.86% 57.27Table 7: Best results obtained for English using the unan-notated data, and for German using only the training dataand gazetteersWalter Daelemans and Ve?ronique Hoste.
2002.
Evalu-ation of Machine Learning Methods for Natural Lan-guage Processing Tasks.
In Proceedings of the ThirdInternational Conference on Language Resources andEvaluation (LREC 2002), pages 755?760, Las Palmas,Gran Canaria.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2002.
TiMBL: TilburgMemory Based Learner, version 4.3, Reference Guide.ILK Technical Report 02-10, ILK.
Available fromhttp://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.Fien De Meulder, Walter Daelemans, and Ve?roniqueHoste.
2002.
A Named Entity Recognition Systemfor Dutch.
In M. Theune, A. Nijholt, and H. Hondrop,editors, Computational Linguistics in the Netherlands2001.
Selected Papers from the Twelfth CLIN Meeting,pages 77?88, Amsterdam ?
New York.
Rodopi.
