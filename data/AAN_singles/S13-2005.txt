Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 25?33, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemeval-2013 Task 8:Cross-lingual Textual Entailment for Content SynchronizationMatteo NegriFBK-irstTrento, Italynegri@fbk.euAlessandro MarchettiCELCTTrento, Italyamarchetti@celct.itYashar MehdadUBCVancouver, Canadamehdad@cs.ubc.caLuisa BentivogliFBK-irstTrento, Italybentivo@fbk.euDanilo GiampiccoloCELCTTrento, Italygiampiccolo@celct.itAbstractThis paper presents the second round of thetask on Cross-lingual Textual Entailment forContent Synchronization, organized withinSemEval-2013.
The task was designed to pro-mote research on semantic inference over textswritten in different languages, targeting at thesame time a real application scenario.
Par-ticipants were presented with datasets for dif-ferent language pairs, where multi-directionalentailment relations (?forward?, ?backward?,?bidirectional?, ?no entailment?)
had to beidentified.
We report on the training and testdata used for evaluation, the process of theircreation, the participating systems (six teams,61 runs), the approaches adopted and the re-sults achieved.1 IntroductionThe cross-lingual textual entailment task (Mehdad etal., 2010) addresses textual entailment (TE) recog-nition (Dagan and Glickman, 2004) under the newdimension of cross-linguality, and within the newchallenging application scenario of content synchro-nization.
Given two texts in different languages, thecross-lingual textual entailment (CLTE) task con-sists of deciding if the meaning of one text can beinferred from the meaning of the other text.
Cross-linguality represents an interesting direction for re-search on recognizing textual entailment (RTE), es-pecially due to its possible application in a vari-ety of tasks.
Among others (e.g.
question answer-ing, information retrieval, information extraction,and document summarization), multilingual contentsynchronization represents a challenging applicationscenario to evaluate CLTE recognition componentsgeared to the identification of sentence-level seman-tic relations.Given two documents about the same topic writ-ten in different languages (e.g.
Wikipedia pages),the content synchronization task consists of au-tomatically detecting and resolving differences inthe information they provide, in order to producealigned, mutually enriched versions of the two docu-ments (Monz et al 2011; Bronner et al 2012).
To-wards this objective, a crucial requirement is to iden-tify the information in one page that is either equiv-alent or novel (more informative) with respect to thecontent of the other.
The task can be naturally castas an entailment recognition problem, where bidi-rectional and unidirectional entailment judgementsfor two text fragments are respectively mapped intojudgements about semantic equivalence and novelty.The task can also be seen as a machine translationevaluation problem, where judgements about se-mantic equivalence and novelty depend on the pos-sibility to fully or partially translate a text fragmentinto the other.The recent advances on monolingual TE on theone hand, and the methodologies used in Statisti-cal Machine Translation (SMT) on the other, offerpromising solutions to approach the CLTE task.
Inline with a number of systems that model the RTEtask as a similarity problem (i.e.
handling similar-ity scores between T and H as features contributingto the entailment decision), the standard sentenceand word alignment programs used in SMT offera strong baseline for CLTE (Mehdad et al 2011;25Figure 1: Example of SP-EN CLTE pairs.Mehdad et al 2012).
However, although repre-senting a solid starting point to approach the prob-lem, similarity-based techniques are just approx-imations, open to significant improvements com-ing from semantic inference at the multilinguallevel (e.g.
cross-lingual entailment rules such as?perro???animal?).
Taken in isolation, similarity-based techniques clearly fall short of providing aneffective solution to the problem of assigning direc-tions to the entailment relations (especially in thecomplex CLTE scenario, where entailment relationsare multi-directional).
Thanks to the contiguity be-tween CLTE, TE and SMT, the proposed task pro-vides an interesting scenario to approach the issuesoutlined above from different perspectives, and of-fers large room for mutual improvement.Building on the success of the first CLTE evalua-tion organized within SemEval-2012 (Negri et al2012a), the remainder of this paper describes thesecond evaluation round organized within SemEval-2013.
The following sections provide an overviewof the datasets used, the participating systems, theapproaches adopted, the achieved results, and thelessons learned.2 The taskGiven a pair of topically related text fragments (T1and T2) in different languages, the CLTE task con-sists of automatically annotating it with one of thefollowing entailment judgements (see Figure 1 forSpanish/English examples of each judgement):?
bidirectional (T1?T2 & T1?T2): the twofragments entail each other (semantic equiva-lence);?
forward (T1?T2 & T16?T2): unidirectionalentailment from T1 to T2;?
backward (T16?T2 & T1?T2): unidirectionalentailment from T2 to T1;?
no entailment (T16?T2 & T16?T2): there isno entailment between T1 and T2 in either di-rection;In this task, both T1 and T2 are assumed to betrue statements.
Although contradiction is relevantfrom an application-oriented perspective, contradic-tory pairs are not present in the dataset.3 Dataset descriptionThe CLTE-2013 dataset is composed of four CLTEcorpora created for the following language combi-nations: Spanish/English (SP-EN), Italian/English(IT-EN), French/English (FR-EN), German/English(DE-EN).
Each corpus consists of 1,500 sentencepairs (1,000 for training and 500 for test), balancedacross the four entailment judgements.In this year?s evaluation, as training set we usedthe CLTE-2012 corpus1 that was created for theSemEval-2012 evaluation exercise2 (including bothtraining and test sets).
The CLTE-2013 test set wascreated from scratch, following the methodology de-scribed in the next section.3.1 Data collection and annotationTo collect the entailment pairs for the 2013 test setwe adopted a slightly modified version of the crowd-sourcing methodology followed to create the CLTE-2012 corpus (Negri et al 2011).
The main differ-ence with last year?s procedure is that we did nottake advantage of crowdsourcing for the whole datacollection process, but only for part of it.As for CLTE-2012, the collection and annotationprocess consists of the following steps:1.
First, English sentences were manually ex-tracted from Wikipedia and Wikinews.
The se-lected sentences represent one of the elements(T1) of each entailment pair;1http://www.celct.it/resources.php?id page=CLTE2http://www.cs.york.ac.uk/semeval-2012/task8/262.
Next, each T1 was modified in various waysin order to obtain a corresponding T2.
Whilein the CLTE-2012 dataset the whole T2 cre-ation process was carried out through crowd-sourcing, for the CLTE-2013 test set we crowd-sourced only the first phase of T1 modification,namely the creation of paraphrases.
Focusingon the creation of high quality paraphrases, wefollowed the crowdsourcing methodology ex-perimented in Negri et al(2012b), in whicha paraphrase is obtained through an itera-tive modification process of an original sen-tence, by asking workers to introduce meaning-preserving lexical and syntactic changes.
Ateach round of the iteration, new workers arepresented with the output of the previous iter-ation in order to increase divergence from theoriginal sentence.
At the end of the process,only the more divergent paraphrases accordingto the Lesk score (Lesk, 1986) are selected.
Asfor the second phase of T2 creation process,this year it was carried out by expert annota-tors, who followed the same criteria used lastyear for the crowdsourced tasks, i.e.
i) removeinformation from the input (paraphrased) sen-tence and ii) add information from sentencessurrounding T1 in the source article;3.
Each T2 was then paired to the original T1, andthe resulting pairs were annotated with one ofthe four entailment judgements.
In order to re-duce the correlation between the difference insentences?
length and entailment judgements,only the pairs where the difference between thenumber of words in T1 and T2 (length diff ) wasbelow a fixed threshold (10 words) were re-tained.3 The final result is a monolingual En-glish dataset annotated with multi-directionalentailment judgements, which are well dis-tributed over length diff values ranging from 0to 9;4.
In order to create the cross-lingual datasets,each English T1 was manually translated into3Such constraint has been applied in order to focus as muchas possible on semantic aspects of the problem, by reduc-ing the applicability of simple association rules such as IFlength(T1)>length(T2) THEN T1?T2.four different languages (i.e.
Spanish, German,Italian and French) by expert translators;5.
By pairing the translated T1 with the cor-responding T2 in English, four cross-lingualdatasets were obtained.To ensure the good quality of the datasets, all thecollected pairs were cross-annotated and filtered toretain only those pairs with full agreement in theentailment judgement between two expert annota-tors.
The final result is a multilingual parallel en-tailment corpus, where T1s are in 5 different lan-guages (i.e.
English, Spanish, German, Italian, andFrench), and T2s are in English.
It is worth men-tioning that the monolingual English corpus, a by-product of our data collection methodology, will bepublicly released as a further contribution to the re-search community.3.2 Dataset statisticsAs described in section 3.1, the methodology fol-lowed to create the training and test sets was thesame except for the crowdsourced tasks.
This al-lowed us to obtain two datasets with the same bal-ance across the entailment judgements, and to keepunder control the distribution of the pairs for differ-ent length diff values in each language combination.Training Set.
The training set is composed of1,000 CLTE pairs for each language combina-tion, balanced across the four entailment judge-ments (bidirectional, forward, backward, andno entailment).
As shown in Table 1, our data col-lection procedure led to a dataset where the major-ity of the pairs falls in the +5 -5 length diff rangefor each language pair (67.2% on average across thefour language pairs).
This characteristic is partic-ularly relevant as our assumption is that such datadistribution makes entailment judgements based onmere surface features such as sentence length inef-fective, thus encouraging the development of alter-native, deeper processing strategies.Test Set.
The test set is composed of 500 entail-ment pairs for each language combination, balancedacross the four entailment judgements.
As shownin Table 2, also in this dataset the majority of thecollected entailment pairs is uniformly distributed27(a) SP-EN (b) IT-EN(c) FR-EN (d) DE-ENFigure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).SP-EN IT-EN FR-EN DE-ENForward 104 132 121 179Backward 202 182 191 123No entailment 163 173 169 174Bidirectional 175 199 193 209ALL 644 686 674 685% (out of 1,000) 64.4 68.6 67.4 68.5Table 1: Training set pair distribution within the -5/+5length diff range.in the [-5,+5] length diff range (68.1% on averageacross the four language pairs).However, comparing training and test set foreach language pair, it can be seen that while theSpanish-English and Italian-English datasets are ho-mogeneous with respect to the length diff feature,the French-English and German-English datasetspresent noticeable differences between training andtest set.
These figures show that, despite the consid-erable effort spent to produce comparable trainingSP-EN IT-EN FR-EN DE-ENbackward 82 89 82 102bidirectional 89 92 90 106forward 69 78 76 98no entailment 71 80 59 100ALL 311 339 307 406% (out of 500) 62.2 67.8 61.4 81.2Table 2: Test set pair distribution within the -5/+5length diff range.and test sets, the ideal objective of a full homogene-ity between the datasets for these two languages wasdifficult to reach.Complete details about the distribution of thepairs in terms of length diff for the four cross-lingual corpora in the test set are provided in Figure2.
Vertical bars represent, for each length diff value,the proportion of pairs belonging to the four entail-ment classes.284 Evaluation metrics and baselinesEvaluation results have been automatically com-puted by comparing the entailment judgements re-turned by each system with those manually assignedby human annotators in the gold standard.
The met-rics used for systems?
ranking is accuracy over thewhole test set, i.e.
the number of correct judge-ments out of the total number of judgements in thetest set.
Additionally, we calculated precision, re-call, and F1 measures for each of the four entail-ment judgement categories taken separately.
Thesescores aim at giving participants the possibility togain clearer insights into their system?s behaviour onthe entailment phenomena relevant to the task.To allow comparison with the CLTE-2012 re-sults, the same three baselines were calculated on theCLTE-2013 test set for each language combination.The first one is the 0.25 accuracy score obtained byassigning each test pair in the balanced dataset toone of the four classes.
The other two baselines con-sider the length difference between T1 and T2:?
Composition of binary judgements (Bi-nary).
To calculate this baseline an SVMclassifier is trained to take binary en-tailment decisions (?YES?, ?NO?).
Theclassifier uses length(T1)/length(T2) andlength(T2)/length(T1) as features respectivelyto check for entailment from T1 to T2 and vice-versa.
For each test pair, the unidirectionaljudgements returned by the two classifiers arecomposed into a single multi-directional judge-ment (?YES-YES?=?bidirectional?, ?YES-NO?=?forward?, ?NO-YES?=?backward?,?NO-NO?=?no entailment?);?
Multi-class classification (Multi-class).
Asingle SVM classifier is trained with the samefeatures to directly assign to each pair one ofthe four entailment judgements.Both the baselines have been calculated with theLIBSVM package (Chang and Lin, 2011), using de-fault parameters.
Baseline results are reported in Ta-ble 3.Although the four CLTE datasets are derived fromthe same monolingual EN-EN corpus, baseline re-sults present slight differences due to the effect oftranslation into different languages.
With respect tolast year?s evaluation, we can observe a slight dropin the binary classification baseline results.
Thismight be due to the fact that the length distributionof examples is slightly different this year.
How-ever, there are no significant differences between themulti-class baseline results of this year in compar-ison with the previous round results.
This mightsuggest that multi-class classification is a more ro-bust approach for recognizing multi-directional en-tailment relations.
Moreover, both baselines failedin capturing the ?no-entailment?
examples in alldatasets (F1no?entailment = 0).SP-EN IT-EN FR-EN DE-EN1-class 0.25 0.25 0.25 0.25Binary 0.35 0.39 0.37 0.39Multi-class 0.43 0.44 0.42 0.42Table 3: Baseline accuracy results.5 Submitted runs and resultsLike in the 2012 round of the CLTE task, partici-pants were allowed to submit up to five runs for eachlanguage combination.
A total of twelve teams reg-istered for participation and downloaded the train-ing set.
Out of them, six4 submitted valid runs.Five teams produced submissions for all the fourlanguage combinations, while one team participatedonly in the DE-EN task.
In total, 61 runs have beensubmitted and evaluated (16 for DE-EN, and 15 foreach of the other language pairs).Accuracy results are reported in Table 4.
As canbe seen from the table, the performance of the bestsystems is quite similar across the four languagecombinations, with the best submissions achievingresults in the 43.4-45.8% accuracy interval.
Simi-larly, also average and median results are close toeach other, with a small drop on DE-EN.
This dropmight be explained by the difference between thetraining and test set with respect to the length difffeature.
Moreover, the performance of DE-EN auto-matic translation might affect approaches based on?pivoting?, (i.e.
addressing CLTE by automaticallytranslating T1 in the same language of T2, as de-scribed in Section 6).4Including the task organizers.29System name SP-EN IT-EN FR-EN DE-ENaltn run1* 0.428 0.432 0.420 0.388BUAP run1 0.364 0.358 0.368 0.322BUAP run2 0.374 0.358 0.364 0.318BUAP run3 0.380 0.358 0.362 0.316BUAP run4 0.364 0.388 0.392 0.350BUAP run5 0.386 0.360 0.372 0.318celi run1 0.340 0.324 0.334 0.342celi run2 0.342 0.324 0.340 0.342ECNUCS run1 0.428 0.426 0.438 0.422ECNUCS run2 0.404 0.420 0.450 0.436ECNUCS run3 0.408 0.426 0.458 0.432ECNUCS run4 0.422 0.416 0.436 0.452ECNUCS run5 0.392 0.402 0.442 0.426SoftCard run1 0.434 0.454 0.416 0.414SoftCard run2 0.432 0.448 0.426 0.402umelb run1 ?
?
?
0.324Highest 0.434 0.454 0.458 0.452Average 0.404 0.404 0.401 0.378Median 0.428 0.426 0.420 0.369Lowest 0.342 0.324 0.340 0.324Table 4: CLTE-2013 accuracy results (61 runs) over the4 language combinations.
Highest, average, median andlowest scores are calculated considering only the best runfor each team (*task organizers?
system).Compared to the results achieved last year, shownin Table 5, a sensible decrease in the highest scorescan be observed.
While in CLTE-2012 the top sys-tems achieved an accuracy well above 0.5 (with amaximum of 0.632 in SP-EN), the results for thisyear are far below such level (the peak is now at45,8% for FR-EN).
A slight decrease with respectto 2012 can also be noted for average performances.However, it?s worth remarking the general increaseof the lowest and median scores, which are less sen-sitive to isolate outstanding results achieved by sin-gle teams.
This indicates that a progress in CLTEresearch has been made building on the lessonslearned after the first round of the initiative.To better understand the behaviour of each sys-tem, Table 6 provides separate precision, recall, andF1 scores for each entailment judgement, calculatedover the best runs of each participating team.
Incontrast to CLTE-2012, where the ?bidirectional?and ?no entailment?
categories consistently provedto be more problematic than ?forward?
and ?back-ward?
judgements, this year?s results are more ho-mogeneous across the different classes.
Neverthe-less, on average, the classification of ?bidirectional?pairs is still worse for three language pairs (SP-EN,IT-EN and FR-EN), and results for ?no entailment?are lower for two of them (SP-EN and DE-EN).SP-EN IT-EN FR-EN DE-ENHighest 0.632 0.566 0.570 0.558Average 0.440 0.411 0.408 0.408Median 0.407 0.350 0.365 0.363Lowest 0.274 0.326 0.296 0.296Table 5: CLTE-2012 accuracy results.
Highest, average,median and lowest scores are calculated considering onlythe best run for each team.As regards the comparison with the baselines,this year?s results confirm that the length diff -basedbaselines are hard to beat.
More specifically, mostof the systems are slightly above the binary classi-fication baseline (with the exception of the DE-ENdataset where only two systems out of six outper-formed it), whereas for all the language combina-tions the multi-class baseline was beaten only by thebest participating system.This shows that, despite the effort in keeping thedistribution of the entailment classes uniform acrossdifferent length diff values, eliminating the correla-tion between sentence length and correct entailmentdecisions is difficult.
As a consequence, althoughdisregarding semantic aspects of the problem, fea-tures considering length information are quite ef-fective in terms of overall accuracy.
Such features,however, perform rather poorly when dealing withchallenging cases (e.g.
?no-entailment?
), which arebetter handled by participating systems.6 ApproachesA rough classification of the approaches adopted byparticipants can be made along two orthogonal di-mensions, namely:?
Pivoting vs. Cross-lingual.
Pivoting meth-ods rely on the automatic translation of one ofthe two texts (either single words or the en-tire sentence) into the language of the other(typically English) in order perform monolin-gual TE recognition.
Cross-lingual methodsassign entailment judgements without prelim-inary translation.?
Composition of binary judgements vs.Multi-class classification.
Compositional ap-proaches map unidirectional (?YES?/?NO?
)30SP-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374AVG.
0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376IT-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388AVG.
0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376FR-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386AVG.
0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386DE-ENForward Backward No entailment BidirectionalSystem name P R F1 P R F1 P R F1 P R F1altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378AVG.
0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389Table 6: Precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.entailment decisions taken separately into sin-gle judgements (similar to the Binary baselinein Section 4).
Methods based on multi-classclassification directly assign one of the four en-tailment judgements to each test pair (similar toour Multi-class baseline).In contrast with CLTE-2012, where the combina-tion of pivoting and compositional methods was theoption adopted by the majority of the approaches,this year?s solutions do not show a clear trend.
Con-cerning the former dimension, participating systemsare equally distributed in cross-lingual and pivotingmethods relying on external automatic translationtools.
Regarding the latter dimension, in additionto compositional and multi-class strategies, also al-ternative solutions that leverage more sophisticatedmeta-classification strategies have been proposed.Besides the recourse to MT tools (e.g.
GoogleTranslate), other tools and resources used by partic-ipants include: WordNet, word alignment tools (e.g.Giza++), part-of-speech taggers (e.g.
Stanford POSTagger), stemmers (e.g.
Snowball), machine learn-ing libraries (e.g.
Weka, SVMlight), parallel corpora(e.g.
Europarl), and stopword lists.
More in detail:ALTN [cross-lingual, compositional] (Turchiand Negri, 2013) adopts a supervised learningmethod based on features that consider word align-ments between the two sentences obtained withGIZA++ (Och et al 2003).
Binary entailmentjudgements are taken separately, and combined intofinal CLTE decisions.BUAP [pivoting, multi-class and meta-classifier] (Vilarin?o et al 2013) adopts a pivotingmethod based on translating T1 into the language of31T2 and vice versa (using Google Translate5).
Sim-ilarity measures (e.g.
Jaccard index) and featuresbased on n-gram overlap, computed at the level ofwords and part of speech categories, are used (eitheralone or in combination) by different classificationstrategies including: multi-class, a meta-classifier(i.e.
combining the output of 2/3/4-class classifiers),and majority voting.CELI [cross-lingual, meta-classifier](Kouylekov, 2013) uses dictionaries for wordmatching, and a multilingual corpus extractedfrom Wikipedia for term weighting.
A variety ofdistance measures implemented in the RTE systemEDITS (Kouylekov and Negri, 2010; Negri etal., 2009) are used to extract features to train ameta-classifier.
Such classifier combines binarydecisions (?YES?/?NO?)
taken separately for eachof the four CLTE judgements.ECNUCS [pivoting, multi-class] (Jiang andMan, 2013) uses Google Translate to obtain the En-glish translation of each T1.
After a pre-processingstep aimed at maximizing the commonalities be-tween the two sentences (e.g.
abbreviation replace-ment), a number of features is extracted to traina multi-class SVM classifier.
Such features con-sider information about sentence length, text sim-ilarity/difference measures, and syntactic informa-tion.SoftCard [pivoting, multi-class] (Jimenez et al2013) after automatic translation with Google Trans-late, uses SVMs to learn entailment decisions basedon information about the cardinality of: T1, T2, theirintersection and their union.
Cardinalities are com-puted in different ways, considering tokens in T1 andT2, their IDF, and their similarity.Umelb [cross-lingual, pivoting, compositional](Graham et al 2013) adopts both pivoting andcross-lingual approaches.
For the latter, GIZA++was used to compute word alignments between theinput sentences.
Word alignment features are usedto train binary SVM classifiers whose decisions areeventually composed into CLTE judgements.7 ConclusionFollowing the success of the first round of the Cross-lingual Textual Entailment for Content Synchroniza-5http://translate.google.com/tion task organized within SemEval-2012, a secondevaluation task has been organized within SemEval-2013.
Despite the decrease in the number of partic-ipants (six teams - four less than in the first round- submitted a total of 61 runs) the new experienceis still positive.
In terms of data, a new test sethas been released, extending the old one with 500new CLTE pairs.
The resulting 1,500 cross-lingualpairs, aligned over four language combinations (inaddition to the monolingual English version), andannotated with multiple entailment relations, repre-sent a significant contribution to the research com-munity and a solid starting point for further develop-ments.6 In terms of results, in spite of a significantdecrease of the top scores, the increase of both me-dian and lower results demonstrates some encour-aging progress in CLTE research.
Such progress isalso demonstrated by the variety of the approachesproposed.
While in the first round most of theteams adopted more intuitive and ?simpler?
solu-tions based on pivoting (i.e.
translation of T1 andT2 in the same language) and compositional entail-ment decision strategies, this year new ideas andmore complex solutions have emerged.
Pivoting andcross-lingual approaches are equally distributed, andnew classification methods have been proposed.
Ourhope is that the large room for improvement, the in-crease of available data, and the potential of CLTEas a way to address complex NLP tasks and applica-tions will motivate further research on the proposedproblem.AcknowledgmentsThis work has been partially supported by the EC-funded project CoSyne (FP7-ICT-4-248531).
Theauthors would also like to acknowledge PamelaForner and Giovanni Moretti from CELCT, and thevolunteer translators that contributed to the creationof the dataset: Giusi Calo, Victoria D?
?az, BiancaJeremias, Anne Kauffman, Laura Lo?pez Ortiz, JulieMailfait, Laura Mora?n Iglesias, Andreas Schwab.6Together with the datasets derived from translation of theRTE data (Negri and Mehdad, 2010), this is the only materialcurrently available to train and evaluate CLTE systems.32ReferencesAmit Bronner, Matteo Negri, Yashar Mehdad, AngelaFahrni, and Christof Monz.
2012.
Cosyne: Synchro-nizing multilingual wiki content.
In Proceedings ofWikiSym 2012.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:a library for support vector machines.
ACM Trans-actions on Intelligent Systems and Technology (TIST),2(3):27.Ido Dagan and Oren Glickman.
2004.
Probabilistic Tex-tual Entailment: Generic Applied Modeling of Lan-guage Variability.
In Proceedings of the PASCALWorkshop of Learning Methods for Text Understand-ing and Mining.Yvette Graham, Bahar Salehi, and Tim Baldwin.
2013.Unimelb: Cross-lingual Textual Entailment with WordAlignment and String Similarity Features.
In Proceed-ings of the 7th International Workshop on SemanticEvaluation (SemEval 2013).Zhao Jiang and Lan Man.
2013.
ECNUCS: RecognizingCross-lingual Textual Entailment Using Multiple Fea-ture Types.
.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013).Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2013.
Soft Cardinality-CLTE: Learning to Iden-tify Directional Cross-Lingual Entailmens from Car-dinalities and SMT.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013).Milen Kouylekov and Matteo Negri.
2010.
An open-source package for recognizing textual entailment.
InProceedings of the ACL 2010 System Demonstrations.Milen Kouylekov.
2013.
Celi: EDITS and Generic TextPair Classification.
In Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013).Michael Lesk.
1986.
Automated Sense DisambiguationUsing Machine-readable Dictionaries: How to Tell aPine Cone from an Ice Cream Cone.
In Proceedingsof the 5th annual international conference on Systemsdocumentation (SIGDOC86).Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards Cross-Lingual Textual Entailment.
InProceedings of the 11th Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL HLT 2010).Yashar Mehdad, Matteo Negri, and Marcello Federico.2011.
Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies(ACL HLT 2011).Yashar Mehdad, Matteo Negri, and Marcello Federico.2012.
Detecting Semantic Equivalence and Informa-tion Disparity in Cross-lingual Documents.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics (ACL 2012).Christof Monz, Vivi Nastase, Matteo Negri, AngelaFahrni, Yashar Mehdad, and Michael Strube.
2011.Cosyne: a framework for multilingual content syn-chronization of wikis.
In Proceedings of WikiSym2011.Matteo Negri and Yashar Mehdad.
2010.
Creating a Bi-lingual Entailment Corpus through Translations withMechanical Turk: $100 for a 10-day Rush.
In Pro-ceedings of the NAACL HLT 2010 Workshop on Cre-ating Speech and Language Data with Amazons?
Me-chanical Turk.Matteo Negri, Milen Kouylekov, Bernardo Magnini,Yashar Mehdad, and Elena Cabrio.
2009.
Towards ex-tensible textual entailment engines: the edits package.In AI* IA 2009: Emergent Perspectives in Artificial In-telligence, pages 314?323.
Springer.Matto Negri, Luisa Bentivogli, Yashar Mehdad, DaniloGiampiccolo, and Alessandro Marchetti.
2011.
Di-vide and Conquer: Crowdsourcing the Creation ofCross-Lingual Textual Entailment Corpora.
Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing (EMNLP 2011).Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2012a.Semeval-2012 Task 8: Cross-lingual Textual Entail-ment for Content Synchronization.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tion (SemEval 2012).Matteo Negri, Yashar Mehdad, Alessandro Marchetti,Danilo Giampiccolo, and Luisa Bentivogli.
2012b.Chinese Whispers: Cooperative Paraphrase Acqui-sition.
In Proceedings of the Eight InternationalConference on Language Resources and Evaluation(LREC12), volume 2, pages 2659?2665.F.
Och, H. Ney, F. Josef, and O. H. Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics.Marco Turchi and Matteo Negri.
2013.
ALTN: WordAlignment Features for Cross-Lingual Textual Entail-ment.
In Proceedings of the 7th International Work-shop on Semantic Evaluation (SemEval 2013).Darnes Vilarin?o, David Pinto, Saul Leo?n, YuridianaAlema?n, and Helena Go?mez-Adorno.
2013.
BUAP:N -gram based Feature Evaluation for the Cross-Lingual Textual Entailment Task.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion (SemEval 2013).33
