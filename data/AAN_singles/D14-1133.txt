Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261?1272,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsConfidence-based Rewriting of Machine Translation OutputBenjamin MarieLIMSI-CNRS, Orsay, FranceLingua et Machina, Le Chesnay, Francebenjamin.marie@limsi.frAur?elien MaxLIMSI-CNRS, Orsay, FranceUniv.
Paris Sud, Orsay, Franceaurelien.max@limsi.frAbstractNumerous works in Statistical MachineTranslation (SMT) have attempted to iden-tify better translation hypotheses obtainedby an initial decoding using an improved,but more costly scoring function.
In thiswork, we introduce an approach that takesthe hypotheses produced by a state-of-the-art, reranked phrase-based SMT sys-tem, and explores new parts of the searchspace by applying rewriting rules se-lected on the basis of posterior phrase-level confidence.
In the medical do-main, we obtain a 1.9 BLEU improve-ment over a reranked baseline exploitingthe same scoring function, correspondingto a 5.4 BLEU improvement over the orig-inal Moses baseline.
We show that if anindication of which phrases require rewrit-ing is provided, our automatic rewritingprocedure yields an additional improve-ment of 1.5 BLEU.
Various analyses, in-cluding a manual error analysis, further il-lustrate the good performance and poten-tial for improvement of our approach inspite of its simplicity.1 IntroductionThe standard configuration of modern phrase-based Statistical Machine Translation (SMT)(Koehn et al., 2003) systems can produce very ac-ceptable results on some tasks.
However, earlyintegration of better features to guide the searchfor the best hypothesis can result in significant im-provements, an expression of the complexity ofmodeling translation quality.
For instance, im-provements have been obtained by integrating fea-tures into decoding that better model semantic co-herence at the sentence level (Hasan and Ney,2009) or syntactic well-formedness (Schwartz etal., 2011).
However, early use of such complexfeatures typically comes at a high computationalcost.
Moreover, some informative features requireor are better computed when complete translationhypotheses are available.
This is addressed in nu-merous works on reranking of the highest scoredsub-space of hypotheses, on so-called n-best lists(Och et al., 2004; Zhang et al., 2006; Carter andMonz, 2011) or output lattices (Schwenk et al.,2006; Blackwood et al., 2010), where many worksspecifically target the inclusion of better languagemodelling capabilities, a well-known weakness ofcurrent automatic generation approaches (Knight,2007).Another way to improve translation a posteriorican be done by rewriting initial hypotheses, for in-stance in a greedy fashion by including new mod-els (Langlais et al., 2007; Hardmeier et al., 2012),or by specifically modeling a task of automaticpost-editing targeting a specific system (Simard etal., 2007; Dugast et al., 2007).
While such auto-matic post-editing may seem to be too limited, no-tably because of the limited initial diversity con-sidered and the fact that it may be in some in-stances agnostic to the internals of the initial sys-tem, it has been shown to potentially improve ac-curacy of the new translation hypotheses (Partonet al., 2012) and to offer very high oracle perfor-mance (Marie and Max, 2013).However, an important issue for such ap-proaches is their capacity to only rewrite incor-rect parts of the translation hypotheses and to useappropriate replacement candidates.
Many workshave tackled the issue of word to n-gram confi-dence estimation in SMT output (Zens and Ney,2006; Ueffing and Ney, 2007; Bach et al., 2011;de Gispert et al., 2013), and some attempts havebeen made to exploit confidence estimates for lat-tice rescoring (Blackwood et al., 2010) or n-bestreranking (Bach et al., 2011; Luong et al., 2014b).In this work, we present an approach in which1261new complete hypotheses are produced by rewrit-ing existing hypotheses, and are scored using com-plex models that could not be used during the ini-tial decoding.
We will use as competitive baselinessystems that rerank the output of an initial decoderusing the complete set of available features, andwill show that we manage to improve their trans-lation.
The difference between our approach andthe reranking baseline lies in the manner in whichwe expand our training data, as well as in our useof high-confidence rewritings to obtain new trans-lation hypotheses.
Importantly, this work will onlyexploit simple confidence estimates correspondingto phrase-based posteriors, which do not requirethat large sets of human-annotated data be avail-able as in other works (Bach et al., 2011; Luong etal., 2014b).The remainder of this paper is organized as fol-lows.
Section 2 is devoted to the description ofour approach, with details on our rewriting ap-proach (2.1), additional features (2.2), rewritingphrase table (2.3), and training examples (2.4).Section 3 presents experiments.
We first describeour experimental setup (3.1) and our baseline sys-tems (3.2).
We then report results when naiverewriting is performed and then with confidence-based rewriting (3.3).
We next devote a significantpart of the paper in section 4 to report further re-sults and analyses: an analysis of the performanceof our system depending on the quality of initialhypotheses (4.1); a semi-oracle experiment wherecorrect phrases are known (4.2); an oracle exper-iment where only correct rewriting decisions aremade (4.3); a manual error analysis of the mainconfigurations studied in this work (4.4); and, fi-nally, a study of the performance of our approachon a more difficult translation task (4.5).
Relatedwork is discussed in section 5 and we concludeand introduce our future work in section 6.2 Description of the approach2.1 Rewriting of translation hypothesesLanglais et al (2007) proposed a greedy searchprocedure to improve translations by reusing thesame translation table and scoring function thatwere used during an initial phrase-based decoding.In our approach, we rewrite hypotheses by usingthe same greedy search algorithm, adding morecomplex models and using the most-confident bi-phrases according to the initial decoder?s searchspace.
To select the hypothesis to rewrite foreach sentence, we produce a n-best list of the ini-tial decoder and rerank this list with a new, bet-ter informed scoring function (see section 2.2).The one-best hypothesis obtained after rerank-ing is then rewritten by our system (denoted asrewriter).
In this way, we ensure that the hy-pothesis that was rewritten had been so far thebest one according to the initial decoding best sub-space and the new models used.At each iteration, new hypotheses are obtainedfrom a current hypothesis by applying one rewrit-ing operation on bi-phrases.
The set of all new hy-potheses is called the neighborhood of the currenthypothesis.
Focusing in this work on local rewrit-ing, we used the following set of operations (N de-notes the number of bi-phrases, T the maximumnumber of entries per source phrase in a rewritingphrase table (see 2.3), and S the average numberof tokens per source phrase)1:1. replace (O(N.T )): replaces the transla-tion of a source phrase with another transla-tion from the rewriting phrase table;2. split (O(N.S.T2)): splits a source phraseinto all possible sets of two (contiguous)phrases, and uses replace on each of theresulting phrases;3. merge (O(T.N)): merges two contiguoussource phrases and uses replace on the re-sulting new phrase.This rewriting algorithm is described in pseudo-code in Algorithm 1.Algorithm 1 rewriter AlgorithmRequire: source a sentence to translatenbestList?
TRANSLATE(source)oneBest?
RERANK(nbestList)sCurrent?
GET SCORE(oneBest)loophypothesesSet?
NEIGHBORHOOD(oneBest)newOneBest?
RANK(hypothesesSet)s?
GET SCORE(newOneBest)if s ?
sCurrent thenreturn oneBestelseoneBest?
newOneBestsCurrent?
send ifend loop1Complexity is expressed in terms of the maximum num-ber of hypotheses that will be considered given some hypoth-esis to rewrite.1262The produced hypotheses are then ranked ac-cording to a new, better informed scoring function(see 2.2).
At the next iteration, the hypothesis nowranked at the top of the list is rewritten, and searchterminates when no better hypothesis is found.Such a greedy search has several obvious lim-itations, in particular it can only perform a lim-ited exploration of the search space, a situationthat can be improved by using a beam (see Sec-tion 3.3).
However, associated with a small andprecise rewriting phrase table, this approach onlyvisits small numbers of more-confident hypothe-ses, which is a critical property given the cost ofcomputing the new scoring function used.2.2 Reranking and featuresThe rerankings of the hypotheses sets de-scribe in this work are all performed withkb-mira (Cherry and Foster, 2012) using the ini-tial features set of the decoder in conjunction withthe following additional features:2?
SOUL models: SOUL models are structuredoutput layer neural network language mod-els (LMs) which have been shown to be use-ful in reranking tasks, for instance for WMTevaluations (Allauzen et al., 2013; P?echeux etal., 2014).
SOUL scoring being too costly tobe integrated during decoding, it fits perfectlythe reranker scenario, which furthermoreenables to use larger contexts for n-grams.We used both monolingual (Le et al., 2011)and bilingual (Le et al., 2012) SOUL 10-grammodels, which were trained on the WMT?12data.?
POS language model: part-of-speech (POS)LMs have been shown to yield improvementsin n-best list reranking (Carter and Monz,2011).
In this work, we trained a 6-gram POSLM using Witten-Bell smoothing.?
IBM1 : the IBM1 scores (p(e|f) and p(f |e))of the complete hypothesis (Och et al., 2004).?
phrase-based confidence score : bi-phrasesare associated to a posterior probability, in-spired from n-gram posterior probability esti-mation as defined in (de Gispert et al., 2013).Let E be the set of all hypotheses in thespace of translation hypotheses defined by2Note that we did not try to explore the independant con-tribution of each feature in this work.the n-best list used for source sentence f , andE?be the subset of E such that word align-ments in sentence pairs (e?, f), ?e??
E?,allow us to extract bi-phrase ?.
Let alsoH(e, f) be the score assigned by a base-line decoder (denoted as 1-pass Moseshenceforth) to sentence pair (e, f).
We usethe following posterior probability for ?
:P (?|F ) =?e?
?E?exp(H(e?, f))?e???Eexp(H(e?
?, f))(1)Then, the logarithms of each phrase?s con-fidence score are summed to use as a confi-dence score for the complete hypothesis.2.3 Rewriting phrase tableTaking the whole translation table of the decoderas a rewriting phrase table to perform the greedysearch produces very large neighborhoods thatrewriter cannot handle due to the cost of themodels that have to be computed.
We tried twodifferent approaches to extract a rewriting phrasetable from the translation table of the system.We first tried a naive approach where the rewrit-ing phrase table of rewriter for the test setuses the phrase table of 1-pass Moses, filteredto keep the k best entries according to the directtranslation model.
We denote such a configurationrptkpef.Our second approach consists in extracting therewriting phrase table containing bi-phrases thatwere the most probable according to the set of allmodels used in 1-pass Moses.
Selection of bi-phrases for each sentence is done in a binary fash-ion, depending on their presence in k-best lists of1-pass Moses for a given value of k. This con-figuration will be denoted confk.2.4 Training examplesWe tried several sets of examples to train theranker of rewriter.
We used the 1,000-bestlist of the development set produced by 1-passMoses during its tuning.
In other configurationswe mixed a) the neighborhood of the rerankern-best hypotheses computed by our system on thedevelopment set using a rewriting phrase tablecontaining the bi-phrases found in the k-best listproduced by 1-pass Moses; and b) the neigh-borhood of the one-best hypotheses of rerankerusing a rewriting phrase table containing the 10-best translations from the 1-pass Moses trans-lation table according to the direct translation1263model.
Both neighborhoods are produced by asingle iteration of rewriter.
We denote re-spectively these sets of hypotheses n-bestNeighand 10PefNeigh.
Our intuition behind the consti-tution of these training sets is that the ranker ofrewriter needs, in order to perform well, train-ing examples that will be similar to hypothesesthat it actually generates.3 Experiments3.1 Experimental setupWe used two datasets from two different domains:the data provided for the WMT?14 medical trans-lation task3(Medical) and a smaller task usingthe TED talks4(TED Talks) data of the IWSLTevaluation campaigns.
For the Medical task weused only the English to French translation di-rection, and both translation directions, Englishto French and French to English, for the TEDTalks task.
In this work, the main part of our ex-periments uses Medical, and TED Talks willbe used at a later stage to study a lower-qualitysituation (cf.
4.5).
For the Medical task, initialdecodings were produced using a LM trained onall WMT?14 monolingual and bilingual medicaldata, while for the TED Talks task we used amuch larger LM trained on all the data providedfor WMT?135.
Both are 4-gram LMs estimatedwith Kneser-Ney smoothing (Chen and Goodman,1998).
For the 6-gram POS LMs used (see 2.2),we used the same data as used for the token-basedLM for Medical, and the concatenation of theNews Commentaries and Europarl sub-parts of theWMT?13 data for TED Talks.
Table 1 providesrelevant statistics about the data used.Tasks Corpus Sentences Tokens (en-fr)Medicaltrain 4.9M 78M - 91Mdev 500 10k - 12ktest 1,000 21k - 26kLM - 146MTED Talkstrain 107 758 2M - 2.2Mdev 934 20k - 20ktest 1,664 31k - 34kLM 6B - 2.5BTable 1: Corpora used in this work.3http://www.statmt.org/wmt14/medical-task/4https://wit3.fbk.eu/mt.php?release=2013-015http://www.statmt.org/wmt13We first built a state-of-the-art phrase-basedSMT system using Moses (Koehn et al., 2003)with standard settings.
We tuned its parameters to-wards BLEU (Papineni et al., 2002) on the tuningdataset using the kb-mira implementation avail-able in Moses with default parameters.Our results will be compared using BLEUand TER (Snover et al., 2006) to a) the initialbest translation produced by the Moses decoder(1-pass Moses) and b) the best translation ob-tained by reranking the 1,000-best list of 1-passMoses (reranker).
Since reranker imple-ments a well-documented approach and uses typesof features commonly used in reranking tasks wewill consider it as our main baseline.
It was trainedusing kb-mira on the 1,000-best of the develop-ment data decoded by 1-pass Moses.In our experiments, rewriter rewrites theone-best hypothesis6produced by rerankerusing the operators Replace, Split andMerge as described in section 2.1.3.2 Baseline resultsTable 2 gives the results of the 1-pass Mosesdecoding for the Medical task and the rerank-ing results of reranker applied to the 1-passMoses 1,000-best list.1-pass Moses obtains a score of 38.2 BLEUon the test set, which can be considered asa good baseline system.7reranker outper-forms 1-pass Moses by 3.5 BLEU, indicatinga strong performance of the features used on thistask.
In particular, SOUL is known to be a use-ful feature for reranking n-best lists on highly-inflected languages such as French.
Note also thatthe SOUL models we used were trained on theWMT?12 monolingual and bilingual data and sowere better informed than the models used dur-ing the 1-pass Moses decoding.8Moreover,as can be seen on Figure 1, the 1,000-best ora-cle reveals a large potential for improvement overthe one-best (+12.4 BLEU).
We further observethat the reranked list of reranker shows a muchfaster potential for translation improvement.6Note that we will also provide results where a beam ofk-best hypotheses are rewritten.7Distribution of error types on a sub-part of the test setwill be provided in section 4.4.8However, SOUL considers only a small sample of thetraining data for training.
For instance, the training of theFrench monolingual model used roughly only 1% (895K sen-tences) of all the WMT?12 data.1264Figure 1: n-best list oracle for 1-pass Mosesand reranker3.3 rewriter resultsResults for the different rewriting phrase tablesand training examples are given in Table 2.
First,concerning the rewriting phrase table, for thek=5 (rpt5pef) and k=10 (rpt10pef) con-figurations9a decrease of 0.7-0.8 BLEU overreranker is obtained.
This illustrates that naiverewritings applied on the test set cannot be usedwith our training regime to improve translationquality.In the next experiments, we used a confkrewriting table.
Table 210shows the results ofrewriter when rewriting the one-best hypothe-sis from reranker for various values of k to de-fine the k-best list from which the rewriting tableis built.
Various training sets are also consideredin the table.The 1-pass Moses 1,000-best configurationreused the same set of hypotheses used to trainreranker.
For this configuration, rewriterloses 2.6 BLEU over reranker on the test setwith conf10k.
Of course, this training data setis of a quite different nature compared to the hy-potheses built by rewriter.In the 10pefNeigh training, the ranker is trainedwith the neighborhoods produced by the first itera-tion of rewriter on the development set with arewriting phrase table containing only the k-besttranslations for each source phrase according tothe direct translation model.
This configuration9We did not experiment with higher values of k because ofthe computationnal cost of the features used by reranker.Indeed, adding more phrase translations increases the size ofthe neighborhoods corresponding to many additional n-gramsto score by SOUL, the most expensive model.10In Table 2 the number of unique bi-phrases for therpt rewriting phrase tables is computed by considering onlysource phrases appearing in the test set, for the n-best Neigh-borhood configurations we merged the phrase tables of eachsentence into one and count just as one unique entry bi-phrases appearing several times.improves over the previous one by 1.7 BLEU, butis still 0.9 BLEU below reranker.
Adding theneighborhoods of the reranker n-best hypothe-ses produced with a conf10k rewriting phrasetable to the training data does not improve overthe previous situation for n = 10, but increasing nto 30 and then 50 produces strong improvementson the test set (resp.
+1.4 and +1.6 BLEU).
Con-sidering a larger neighborhood obtained by rewrit-ing the best n = 90 hypotheses does not yieldfurther gains.
We denote from now on optiour best configuration thus far, considering theperformance on the development set and havingthe largest confidence-based rewriting phrase ta-ble considered.Letting rewriter perform a beam search onthe 10-best hypotheses of the test set, further gainsare obtained, corresponding now to an improve-ment of +1.9 BLEU over our reranker base-line, or +5.4 BLEU over 1-pass Moses.11Fur-thermore, although taking the bi-phrases from the10,000-best is our best configuration, it is inter-esting to note that taking bi-phrases from the 10-best only already yields a moderate improvementof +0.6 BLEU over reranker.
Figure 2a showsthat up to k = 10, 000 higher value of k to ex-tract the rewriting phrase table increase the BLEUscore on the test set.12We did not experiment withhigher values of k, but plan to use the output lat-tice produced by 1-pass Moses to compute ef-ficiently posteriors for larger sets of bi-phrases (deGispert et al., 2013).As illustrated on Figure 2b, rewriter mostlyimproves the BLEU score during the three firstiterations and then converges at the ninth iteration.However, it is important to note that not allsentences are actually improved by our system.As illustrated on Figure 3a, opti improves40.8% of the sentences of the test set but degrades29.2% of them according to sentence-BLEU (Linand Och, 2004).
It is certainly the case thatmore informative confidence features may helpidenfity more precisely which fragments of thetranslations should really undergo rewriting.
Wewill investigate the exploitation of an oraclephrase-based confidence measure in Section 4.2.11Using a beam becomes quickly prohibitive: using12 threads, 25 mn vs. 3h were needed for the test set forthe configurations of size 1 and 10, respectively.12Note that even for k = 10, 000 the computed neighbor-hoods are still quite small with an average of 116 hypothesesfor each hypothesis to rewrite per iteration, against an averageof 788 hypotheses for the rpt10pef configuration.1265(a) Results of rewriter with rpt5pef, rpt10pef and dif-ferent values of k for confk(b) Iterations of rewriter on test with opti and two beamsizes : 1 and 10.Figure 2: Performance of rewriter depending on the type of the rewriting phrase table and the numberof iterations and beam sizes.baselinedev testBLEU BLEU TER GOS BLEU1-pass Moses 40.9 38.3 44.6reranker 44.1 41.8 41.6training datarewriting unique beamphrase table bi-phrases size1-pass Moses 1 000-best conf10k 38 455 1 44.1 39.2(?2.6)43.8(+2.2)58.710pefNeigh conf10k 38 455 1 43.9 40.9(?0.9)41.2(?0.4)58.710-bestNeigh + 10pefNeigh conf10k 38 455 1 43.8 40.9(?0.9)41.2(?0.4)58.730-bestNeigh + 10pefNeigh conf10k 38 455 1 44.2 43.2(+1.4)40.6(?1.0)58.750-bestNeigh + 10pefNeigh rpt5pef 85 530 1 44.5 41.0(?0.8)42.0(+0.4)50.6= rpt10pef 149 887 1 44.5 41.1(?0.7)42.1(+0.5)54.5= conf10 21 398 1 44.5 42.4(+0.6)41.0(?0.6)45.9= conf100 28 730 1 44.5 42.9(+1.1)40.8(?0.8)50.2= conf1k 33 929 1 44.5 43.0(+1.2)40.6(?1.0)53.3= (opti) conf10k 38 455 1 44.5 43.4(+1.6)40.4(?1.2)58.7= conf10k 38 455 10 44.5 43.7(+1.9)40.1(?1.5)59.690-bestNeigh + 10pefNeigh conf10k 38 455 1 44.4 43.4(+1.6)40.4(?1.2)58.7Table 2: Results on Medical for different training configurations, rewriting phrase tables and beamsizes.
opti denotes our optimal configuration for rewriter.4 Analysis of confidence-based rewriting4.1 Performance of rewriter depending onthe quality of initial hypothesesThe first question we address in our analysis ofrewriter is whether its performance dependson the difficulty of each individual sentence.
Asa proxy of sentence difficulty we used sentence-BLEU of 1-pass Moses, and used it to di-vide the sentences of the test set into quartiles.Figure 4 shows that reranker improves moreover 1-pass Moses and that at the same timerewriter improves more over reranker asthe sentences are more difficult.
In particular,rewriter obtains a 8.6 BLEU improvementover 1-pass Moses on the more difficult quar-tile, but only a 1.3 BLEU improvement on the leastdifficult quartile.
We hypothesize that better per-formance may be achieved if adapting the trainingand rewriting of rewriter to sentences of vary-ing quality, which may, for instance, be estimatedwith off-the-shelf estimators (Specia et al., 2013).4.2 Semi-oracle experiments: rewriting onlyincorrect fragmentsWe observed in section 3.3 that our opti con-figuration, which obtains strong improvements intranslation quality (as given by corpus-BLEU),in fact degrades (as given by sentence-BLEU)a significant proportion of sentences.
To fur-ther analyze these results, we simulate a situa-tion where oracle confidence information is avail-able at the phrase-level: in particular, rewriteris prevented from rewriting bi-phrases whose tar-get phrase appears exactly in the reference transla-1266(a) automatic(b) semi-oracleFigure 3: sBLEU delta, for each sentence, between the reranker one-best to rewrite and its auto-matic (3a) or semi-oracle (3b) rewriting computed by rewriter with the opti configuration.Figure 4: Source sentences were divided intoquartiles according to sBLEU of the 1-passMoses system.
For each quartile we reported theperformance of 1-pass Moses, reranker,rewriter, GOS.tion.13Furthermore, this ?freezing?
of bi-phrasescan be repeated after each iteration of rewriter.Thus, we now have an oracle situation forchoosing which source phrases may be rewrit-ten, but the rest of the rewriting procedure isstill fully automatic.
Moreover, we purposefullydid not adapt the training procedure to this newconfiguration, and reused opti as is.
Results,reported in Table 3, indicate that an additional1.5 BLEU is obtained from opti, or 3.1 BLEUfrom reranker and 6.6 BLEU from 1-passMoses.
The use of a larger beam of size 10did not improve those results any further.
At13This is obviously not an optimal solution.the first iteration, rewriter ?froze?
approx-imatively 65.6% of the bi-phrases, and 70.5%at the last iteration, demonstrating the ability ofrewriter to find good rewritings that match thereference translation.
Looking at Figure 3b, wenow find that, as expected, only a limited num-ber of sentences are now degraded by rewriter.The large improvements obtained clearly under-lines the important role that better confidence esti-mates could play in our framework.System testBLEU TERreranker 41.8 41.6opti 43.4 40.4semi-oracle, beam 1 44.9(+1.5)39.2(?1.2)semi-oracle, beam 10 44.9(+1.5)39.0(?1.4)Table 3: Results for the semi-oracle using opti.4.3 Oracle experiments: making only thecorrect decisionsWe now turn to the situation where only rewrit-ings that actually improve translation performancewould be made.
In practice, we use a sim-ple solution: we resort to greedy oracle search(GOS) (Marie and Max, 2013), where sentence-BLEU is maximized using rewritings from theopti phrase table.
At each iteration the rewrit-ing in the neighborhood that maximizes sentence-BLEU is selected until convergence.Results for this greedy search oracle appear inthe last column of Table 2 and allow us to putin perspective the individual potential of the var-1267ious tested configurations.
We can first noticethat the rpt5pef phrase table allows the ora-cle to reach 50.6 BLEU, 8.1 BLEU below theoracle value obtained with conf10k, althoughrpt5pef contains twice as many bi-phrases.
Thesame conclusion can be made about rpt10pef,which is 3.9 BLEU higher than rpt5pef but con-tains nearly twice as many bi-phrases.
Finally, al-though conf10k contains approximatively fourtimes fewer bi-phrases than rpt10pef, its ora-cle value is 4.2 BLEU higher.
This points out thefact that conf10k is a lot more precise rewrit-ing phrase table for the translations to rewrite, aswell as the fact that rpt5pef and rpt10pefare much noisier and consequently difficult to useefficiently by our automatic rewriting procedure.4.4 Manual error analysisIn the previous sections, we have shown that ourautomatic rewriting procedure can improve trans-lation quality over both an initial Moses baseline,and a reranked baseline using the same featuresas our procedure.
We have further shown in sec-tion 4.3 that much larger improvements could beobtained by using an oracle procedure.We now focus on the four following con-figurations: 1-pass Moses, reranker,rewriter and GOS.
Although this four configu-rations are well separated both in terms of BLEUand TER scores, it is informative to look moreprecisely into what makes their results different.We performed a small-scale manual error analysisof these four configurations.
A French nativespeaker annotated 70 translation hypotheses usingan error typology adapted from (Vilar et al.,2006).Results of the manual error analysis are re-ported in Table 4.
The most significant resultsare for the disamb(iguation) and form error types,the former being more related to translation accu-racy, and the later to fluency.
In both cases, wefirst observe a strong reduction of errors between1-pass Moses and reranker, which demon-strates the positive impact of the features usedon these levels.
Then, another, similar reductionis obtained between reranker and rewriter,demonstrating that our reranking procedure man-ages to identify more precise and fluent hypothe-ses.
Finally, a further reduction is found betweenrewriter and GOS, indicating that our proposedlocal, greedy rewriting can still be improved, no-tably by using more informative features and bet-ter confidence estimates.The other types of error categories are less in-formative.
We find no clear differences in er-ror types attributable to style issues, which seemto be irrecoverable even for GOS.
rerankerand rewriter both improve on order-related er-rors over 1-pass Moses, but our local rewrit-ing unsurprisingly did not fix any of these errors.Finally, reranker and rewriter decreasedslightly the number of extra words from 1-passMoses, while GOS sometimes artificially intro-duces extra words.4.5 Lower-quality SMT experimentsWe now turn to the question of how our rewrit-ing system fares on a more difficult task, and usedTED Talks, 6 BLEU below Medical for theEnglish to French direction, for this purpose.
Inthe same way as we did for Medical, we firsttried to find the best training configuration for theranker of the rewriting system.
For this task, mix-ing the n-best neighborhood and 10pefNeigh withn=10 seemed to be sufficient to have no more im-provement on the development set by increasing nfor both language directions, so we used this train-ing configuration.
As for the rewriting phrase tableused on the test set, we simply selected conf10kas in the Medical task.
Results are reportedin Table 5 for French to English and English toFrench.We first observe that reranker performedsimilarly for the two translation directions, byimproving 1-pass Moses by 0.5 BLEU.
Thesmaller improvements may be partly attributed tothe better LM used in 1-pass Moses, implyinga better early modeling of grammaticality, but alsoby the fact that models such as SOUL and POSLMs rely on accurate contexts and are thereforemore apt to help in choosing translations amonggenerally better candidates.Finally, rewriter obtains smaller but consis-tent improvements over reranker: +0.4 BLEUfor translation into English, and +0.9 BLEU fortranslation into French.
The smaller improvementin the former situation may be attributed to the na-ture of the target language which has a simpleragreement system.
Consequently, the form-relatederrors discussed in Section 4.4 are possibly lesssubject to improvement here.1268extra missing incorrect unknownword word disamb form style order word all1-pass Moses 11 1 57 91 13 31 10 214reranker 5 3 47 73 11 19 10 168rewriter 4 4 40 55 12 19 10 144rewriter oracle 19 2 26 44 14 22 10 137Table 4: Results for manual error analysis for the first 70 test sentences.System fr-en en-frBLEU TER BLEU TER1-pass Moses 32.5 47.7 32.3 49.9reranker 33.0 47.3 32.8 49.4rewriter 33.4(+0.4)47.4(+0.1)33.7(+0.9)49.3(?0.1)semi-oracle 34.1(+1.1)46.6(?0.7)34.2(+1.4)48.6(?0.8)Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.5 Related workReranking of translation hypotheses n-bestlist reranking was extensively studied in (Och etal., 2004), using features not used in the initialdecoder such as IBM1 scores (which also proveduseful for word-level confidence estimation (Blatzet al., 2004)) and generative syntactic models.While the experiments in (Och et al., 2004) didnot show any clear contribution of syntactic in-formation used in this manner, the later work byCarter and Monz (2011) managed to successfullyexploit syntactic features using discriminative lan-guage modeling for n-best reranking.
Gimpel etal.
(2013) outperformed n-best reranking by gen-erating, with an expensive but simple method, di-verse hypotheses used as training data.
Recently,Luong et al.
(2014b) reranked n-best lists usingconfidence scores at the hypothesis level com-puted from word-level confidence measures learntfrom roughly 10,000 SMT system outputs anno-tated by humans.Rewriting of translation hypotheses Langlaiset al.
(2007) described a greedy search decoder,first introduced in (Germann et al., 2001), able toimprove translations produced by a dynamic pro-gramming decoder using the same scoring func-tion and translation table.
However, the more re-cent work by Arun et al.
(2010) using a Gibbssampler for approximating maximum translationdecoding showed the adequacy of the approxima-tions made by state-of-the-art decoders for findingthe best translation in their search space.
Otherworks were more directly targeted at automaticpost-editing of SMT output, and approached theproblem as one of second-pass translation be-tween automatic predictions and correct transla-tions (Simard et al., 2007; Dugast et al., 2007).The recent work of Zhu et al.
(2013) attempts torepair translations by exploiting confidence esti-mates for examples derived from the similaritybetween source words in the input text and intraining examples.
Luong et al.
(2014a) obtainedimprovements by computing word confidence es-timation, trained on human annotated data, andlarge sets of lexical, syntactic and semantic fea-tures, for the words in the n-best list producedduring a first-pass decoding, and performing asecond-pass decoding exploiting these new scores.Confidence estimation of Machine TranslationThe Word Posterior Probability (WPP) proposedby Ueffing and Ney (2007), derived from informa-tion from the n-best list produced by a decoder,proved to be useful for estimating word-level con-fidence.
Bach et al.
(2011) worked on the issueof predicting sentence-level and word-level MTerrors by using WPP and other features derivedfrom the source context, the source-target align-ment, and dependency structures, but relied on asignificantly large manually annotated corpus ofMT errors.
De Gispert et al.
(2013) calculate k-1269gram posterior probabilities from n-best lists orword lattices, and demonstrated that they were rea-sonably accurate indications of whether specific k-grams would be found or not in human referencetranslations.
Finally, the work of Blackwood etal.
(2010) proposed to segment translation latticesaccording to confidence measures over the maxi-mum likelihood translation hypothesis to focus onregions with potential translation errors.
Hypothe-sis space constraints based on monolingual cover-age are then applied to the low confidence regionsto improve translation fluency.6 Conclusions and perspectivesIn this paper, we have described an approachthat improves translations a posteriori by applyingsimple local rewritings.
We have shown that thequality of phrase-level confidence estimates hasa direct impact of the amplitude of the improve-ments that can be obtained, as well as the initialquality of the rewritten hypotheses.
We have useda very simple definition for confidence estimatesunder the form of phrase posteriors estimated fromn-best lists from an initial decoder, which obtainedgood empirical performance, in spite of not requir-ing large human-annotated datasets as in other ap-proaches (Bach et al., 2011; Luong et al., 2014b).Our work could be extended in several direc-tions.
First, we could use a larger set of rewrit-ing operations (Langlais et al., 2007), includingthe rewrite (sic) operation introduced in (Marieand Max, 2013) that paraphrases source phrasesand then translates them.We could also possibly consider any phrase seg-mentation compatible with a specific word align-ment rather than rely on specific phrase segmenta-tions.
This would allow us to attain faster somerewritings that could otherwise require severalrewriting iterations and may never be attained bythe greedy procedure.More features could also be used, for instanceto model more fine-grained syntax (Post, 2011)or document-level lexical coherence (Hardmeieret al., 2012).
However, anticipating that somefeatures might be very expensive to compute, wecould adapt our procedure to work in severalpasses: initial passes would tend to restrict thesearch space more and more using an initial setof features, before a more expensive pass wouldconcentrate on a limited number of hypotheses.Figure 1 indeed already showed a much faster or-acle improvement between 1-pass Moses andreranker for n-best list of small sizes.Another avenue for improvement lies in the pos-sibility to perform the training of our rewriterby providing it with more reference translations.As these are typically not readily available, wecould resort to targeted paraphrasing (Madnaniand Dorr, 2013) to rewrite reference translationsinto acceptable paraphrases that reuse n-gramsfrom the best hypotheses of the system so far.Contrarily to (Madnani and Dorr, 2013), we couldbias the paraphrasing table so that it only con-tains paraphrases that correspond to target phrasesof high confidence values, which would add newn-grams likely of being produced by rewriter.It is furthermore worth noticing that our workproposes a potential answer to an original ques-tion: contrarily to typical works on sub-sentencialMT confidence estimation, which predict whethera word or phrase is correct or not, our rewritersystem could be used to determine automaticallywhether a rewriting system could (if asked to) at-tempt to improve locally a translation, or whethera human post-editor should already tackle work-ing on improving it.
As we showed in our manualerror analysis in section 4.4, there are in fact manyinstances of errors that could not be recovered byour approach, be it because of its local rewritingstrategy or of the bilingual resources or modelsused, so that some knowledge would have to beprovided as hard constraints by a human transla-tor, as hinted in (Crego et al., 2010).
We couldthen finally have our rewriter system work in aturn-based fashion in collaboration with a humantranslator, fixing errors or making improvementsthat are being made possible by the last edits fromthe translator.AcknowledgmentsThe authors would like to thank the anonymous re-viewers and Guillaume Wisniewski for their use-ful remarks.
Additional thanks go to Hai Son Lefor ?anticipating?
the need for a large and effi-cient cache in his SOUL implementation, QuocKhanh Do for his assistance on using SOUL, andLi Gong and Nicolas P?echeux for providing theauthors with data used in the experiments.
Thework of the first author is supported by a CIFREgrant from French ANRT.1270ReferencesAlexandre Allauzen, Nicolas P?echeux, Quoc KhanhDo, Marco Dinarelli, Thomas Lavergne, Aur?elienMax, Hai-son Le, and Franc?ois Yvon.
2013.LIMSI @ WMT13.
In Proceedings of WMT, Sofia,Bulgaria.Abhishek Arun, Phil Blunsom, Chris Dyer, AdamLopez, Barry Haddow, and Philipp Koehn.
2010.Monte Carlo inference and maximization for phrase-based translation.
In Proceedings of CoNLL, Boul-der, USA.Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.2011.
Goodness: A Method for Measuring Ma-chine Translation Confidence.
In Proceedings ofACL, Portland, USA.Graeme Blackwood, Adri`a de Gispert, and WilliamByrne.
2010.
Fluency Constraints for MinimumBayes-Risk Decoding of Statistical Machine Trans-lation Lattices.
In Proceedings of COLING, Beijing,China.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence Es-timation for Machine Translation.
In Proceedings ofCOLING, Geneva, Switzerland.Simon Carter and Christof Monz.
2011.
SyntacticDiscriminative Language Model Rerankers for Sta-tistical Machine Translation.
Machine Translation,25(4):317?339.Stanley F. Chen and Joshua T. Goodman.
1998.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Technical Report TR-10-98, Com-puter Science Group, Harvard University.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of NAACL, Montr?eal, Canada.Josep M. Crego, Aur?elien Max, and Franc?ois Yvon.2010.
Local lexical adaptation in Machine Trans-lation through triangulation: SMT helping SMT.
InProceedings of COLING, Beijing, China.Adri`a de Gispert, Graeme Blackwood, Gonzalo Igle-sias, and William Byrne.
2013.
N-gram poste-rior probability confidence measures for statisticalmachine translation: an empirical study.
MachineTranslation, 27(2):85?114.Lo?
?c Dugast, Jean Senellart, and Philipp Koehn.
2007.Statistical Post-Editing on SYSTRANs Rule-BasedTranslation System.
In Proceedings of WMT,Prague, Czech Republic.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast Decodingand Optimal Decoding for Machine Translation.
InProceedings of ACL, Toulouse, France.Kevin Gimpel, Dhruv Batra, Chris Dyer, GregoryShakhnarovich, and Virginia Tech.
2013.
A Sys-tematic Exploration of Diversity in Machine Trans-lation.
In Proceedings of EMNLP, Seatlle, USA.Christian Hardmeier, Joakim Nivre, and Jorg Tiede-man.
2012.
Document-Wide Decoding for Phrase-Based Statistical Machine Translation.
In Proceed-ings of EMNLP, Jeju Island, Korea.Sa?sa Hasan and Hermann Ney.
2009.
Comparison ofExtended Lexicon Models in Search and Rescoringfor SMT.
In Proceedings of NAACL, short papers,Boulder, USA.Kevin Knight.
2007.
Automatic Language TranslationGeneration Help Needs Badly.
In MT Summit (in-vited talk), Copenhagen, Denmark.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of NAACL, Edmonton, Canada.Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.2007.
A Greedy Decoder for Phrase-Based Statisti-cal Machine Translation.
In Proceedings of Confer-ence on Theoretical and Methodological Issues inMachine Translation (TMI), Skovde, Sweden.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
StructuredOutput Layer Neural Network Language Model.
InProceedings of ICASSP, Prague, Czech Republic.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous Space Translation Models withNeural Networks.
In Proceedings of NAACL,Montr?eal, Canada.Chin Y. Lin and Franz J. Och.
2004.
ORANGE: amethod for evaluating automatic evaluation metricsfor machine translation.
In Proceedings of COL-ING, Geneva, Switzerland.Ngoc-Quang Luong, Laurent Besacier, and BenjaminLecouteux.
2014a.
An Efficient Two-Pass Decoderfor SMT Using Word Confidence Estimation.
InProceedings of EAMT, Dubrovnik, Croatia.Ngoc-Quang Luong, Laurent Besacier, and BenjaminLecouteux.
2014b.
Word Confidence Estimationfor SMT N -best List Re-ranking.
In Proceedingsof the Workshop on Humans and Computer-assistedTranslation (HaCaT), Gothenburg, Sweden.Nitin Madnani and Bonnie J. Dorr.
2013.
Generat-ing Targeted Paraphrases for Improved Translation.ACM Transactions on Intelligent Systems and Tech-nology, special issue on Paraphrasing, 4(3).Benjamin Marie and Aur?elien Max.
2013.
A Studyin Greedy Oracle Improvement of Translation Hy-potheses.
In Proceedings of IWSLT, Heidelberg,Germany.1271Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.A Smorgasbord of Features for Statistical MachineTranslation.
In Proceedings of NAACL, Boston,USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof ACL, Philadelphia, USA.Kristen Parton, Nizar Habash, Kathleen R. McKeown,Gonzalo Iglesias, and Adri`a de Gispert.
2012.
CanAutomatic Post-editing Make MT more Meaning-ful?
In Proceedings of EAMT, Trento, Italy.Nicolas P?echeux, Li Gong, Quoc Khanh Do, Ben-jamin Marie, Yulia Ivanishcheva, Alexander Al-lauzen, Thomas Lavergne, Jan Niehues, Aur?elienMax, and Franc?ois Yvon.
2014.
LIMSI @ WMT?14Medical Translation Task.
In Proceedings of WMT,Baltimore, USA.Matt Post.
2011.
Judging Grammaticality with TreeSubstitution Grammar Derivations.
In Proceedingsof ACL, short papers, Portland, USA.Lane Schwartz, Chris Callison-Burch, WilliamSchuler, and Stephen Wu.
2011.
IncrementalSyntactic Language Models for Phrase-basedTranslation.
In Proceedings of ACL, Portland, USA.Holger Schwenk, Daniel D?echelotte, and Jean-LucGauvain.
2006.
Continuous Space Language Mod-els for Statistical Machine Translation.
In Proceed-ings of COLING-ACL, Sydney, Australia.Michel Simard, Cyril Goutte, and Pierre Isabelle.2007.
Statistical Phrase-based Post-editing.
In Pro-ceedings of NAACL, Rochester, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, , and John Makhoul.
2006.
AStudy of Translation Edit Rate with Targeted HumanAnnotation.
In Proceedings of AMTA, Cambridge,USA.Lucia Specia, Kashif Shah, Jose G.C.
de Souza, andTrevor Cohn.
2013.
QuEst - A Translation Qual-ity Estimation Framework.
In Proceedings of ACL,System Demonstrations, Sofia, Bulgaria.Nicola Ueffing and Hermann Ney.
2007.
Word-Level Confidence Estimation for Machine Transla-tion.
Computational Linguistics.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Statistical Ma-chine Translation Output.
In Proceedings of LREC,Genoa, Italy.Richard Zens and Hermann Ney.
2006.
N -Gram Pos-terior Probabilities for Statistical Machine Transla-tion.
In Proceedings of WMT, New York, USA.Ying Zhang, Almut Silja Hildebrand, and Stephan Vo-gel.
2006.
Distributed Language Modeling for N-best List Re-ranking.
In Proceedings of EMNLP,Sydney, Australia.Junguo Zhu, Muyun Yang, Sheng Li, and Tiejun Zhao.2013.
Repairing Incorrect Translation with Exam-ples.
In Proceedings of IJCNLP, Nagoya, Japan.1272
