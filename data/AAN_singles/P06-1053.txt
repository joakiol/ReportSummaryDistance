Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 417?424,Sydney, July 2006. c?2006 Association for Computational LinguisticsIntegrating Syntactic Priming into an Incremental Probabilistic Parser,with an Application to Psycholinguistic ModelingAmit Dubey and Frank Keller and Patrick SturtHuman Communication Research Centre, University of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, UK{amit.dubey,patrick.sturt,frank.keller}@ed.ac.ukAbstractThe psycholinguistic literature providesevidence for syntactic priming, i.e., thetendency to repeat structures.
This pa-per describes a method for incorporatingpriming into an incremental probabilis-tic parser.
Three models are compared,which involve priming of rules betweensentences, within sentences, and withincoordinate structures.
These models sim-ulate the reading time advantage for par-allel structures found in human data, andalso yield a small increase in overall pars-ing accuracy.1 IntroductionOver the last two decades, the psycholinguisticliterature has provided a wealth of experimentalevidence for syntactic priming, i.e., the tendencyto repeat syntactic structures (e.g., Bock, 1986).Most work on syntactic priming has been con-cerned with sentence production; however, recentstudies also demonstrate a preference for struc-tural repetition in human parsing.
This includesthe so-called parallelism effect demonstrated byFrazier et al (2000): speakers processes coordi-nated structures more quickly when the secondconjunct repeats the syntactic structure of the firstconjunct.Two alternative accounts of the parallelism ef-fect have been proposed.
Dubey et al (2005) ar-gue that the effect is simply an instance of a perva-sive syntactic priming mechanism in human pars-ing.
They provide evidence from a series of cor-pus studies which show that parallelism is not lim-ited to co-ordination, but occurs in a wide rangeof syntactic structures, both within and betweensentences, as predicted if a general priming mech-anism is assumed.
(They also show this effect isstronger in coordinate structures, which could ex-plain Frazier et al?s (2000) results.
)Frazier and Clifton (2001) propose an alterna-tive account of the parallelism effect in terms of acopying mechanism.
Unlike priming, this mecha-nism is highly specialized and only applies to co-ordinate structures: if the second conjunct is en-countered, then instead of building new structure,the language processor simply copies the structureof the first conjunct; this explains why a speed-up is observed if the two conjuncts are parallel.
Ifthe copying account is correct, then we would ex-pect parallelism effects to be restricted to coordi-nate structures and not to apply in other contexts.This paper presents a parsing model which im-plements both the priming mechanism and thecopying mechanism, making it possible to com-pare their predictions on human reading time data.Our model also simulates other important aspectsof human parsing: (i) it is broad-coverage, i.e.,it yields accurate parses for unrestricted input,and (ii) it processes sentences incrementally, i.e.,on a word-by-word basis.
This general modelingframework builds on probabilistic accounts of hu-man parsing as proposed by Jurafsky (1996) andCrocker and Brants (2000).A priming-based parser is also interesting froman engineering point of view.
To avoid sparsedata problems, probabilistic parsing models makestrong independence assumptions; in particular,they generally assume that sentences are indepen-dent of each other, in spite of corpus evidence forstructural repetition between sentences.
We there-fore expect a parsing model that includes struc-tural repetition to provide a better fit with real cor-pus data, resulting in better parsing performance.A simple and principled approach to handlingstructure re-use would be to use adaptation prob-abilities for probabilistic grammar rules (Church,2000), analogous to cache probabilities used incaching language models (Kuhn and de Mori,1990).
This is the approach we will pursue in thispaper.Dubey et al (2005) present a corpus study thatdemonstrates the existence of parallelism in cor-pus data.
This is an important precondition for un-derstanding the parallelism effect; however, they417do not develop a parsing model that accounts forthe effect, which means they are unable to evaluatetheir claims against experimental data.
The presentpaper overcomes this limitation.
In Section 2, wepresent a formalization of the priming and copy-ing models of parallelism and integrate them intoan incremental probabilistic parser.
In Section 3,we evaluate this parser against reading time datataken from Frazier et al?s (2000) parallelism ex-periments.
In Section 4, we test the engineeringaspects of our model by demonstrating that a smallincrease in parsing accuracy can be obtained witha parallelism-based model.
Section 5 provides ananalysis of the performance of our model, focus-ing on the role of the distance between prime andtarget.2 Priming ModelsWe propose three models designed to capture thedifferent theories of structural repetition discussedabove.
To keep our model as simple as possi-ble, each formulation is based on an unlexicalizedprobabilistic context free grammar (PCFG).
In thissection, we introduce the models and discuss thenovel techniques used to model structural similar-ity.
We also discuss the design of the probabilisticparser used to evaluate the models.2.1 Baseline ModelThe unmodified PCFG model serves as the Base-line.
A PCFG assigns trees probabilities by treat-ing each rule expansion as conditionally indepen-dent given the parent node.
The probability of arule LHS ?
RHS is estimated as:P(RHS|LHS) = c(LHS ?
RHS)c(LHS)2.2 Copy ModelThe first model we introduce is a probabilisticvariant of Frazier and Clifton?s (2001) copyingmechanism: it models parallelism in coordinationand nothing else.
This is achieved by assumingthat the default operation upon observing a coordi-nator (assumed to be anything with a CC tag, e.g.,?and?)
is to copy the full subtree of the preced-ing coordinate sister.
Copying impacts on how theparser works (see Section 2.5), and in a probabilis-tic setting, it also changes the probability of treeswith parallel coordinated structures.
If coordina-tion is present, the structure of the second item iseither identical to the first, or it is not.1 Let us call1The model only considers two-item coordination or thelast two sisters of multiple-item coordination.the probability of having a copied tree as pident.This value may be estimated directly from a cor-pus using the formulap?ident =cidentctotalHere, cident is the number of coordinate structuresin which the two conjuncts have the same internalstructure and ctotal is the total number of coordi-nate structures.
Note we assume there is only oneparameter pident applicable everywhere (i.e., it hasthe same value for all rules).How is this used in a PCFG parser?
Let t1 and t2represent, respectively, the first and second coor-dinate sisters and let PPCFG(t) be the PCFG prob-ability of an arbitrary subtree t.Because of the independence assumptions ofthe PCFG, we know that pident ?
PPCFG(t).
Oneway to proceed would be to assign a probabilityof pident when structures match, and (1?
pident) ?PPCFG(t2) when structures do not match.
However,some probability mass is lost this way: there isa nonzero PCFG probability (namely, PPCFG(t1))that the structures match.In other words, we may have identical subtreesin two different ways: either due to a copy oper-ation, or due to a PCFG derivation.
If pcopy is theprobability of a copy operation, we can write thisfact more formally as: pident = PPCFG(t1)+ pcopy.Thus, if the structures do match, we assign thesecond sister a probability of:pcopy +PPCFG(t1)If they do not match, we assign the second con-junct the following probability:1?PPCFG(t1)?
pcopy1?PPCFG(t1) ?PPCFG(t2)This accounts for both a copy mismatch and aPCFG derivation mismatch, and assures the prob-abilities still sum to one.
These probabilities forparallel and non-parallel coordinate sisters, there-fore, gives us the basis of the Copy model.This leaves us with the problem of finding anestimate for pcopy.
This value is approximated as:p?copy = p?ident ?1|T2| ?t?T2 PPCFG(t)In this equation, T2 is the set of all second con-juncts.2.3 Between ModelWhile the Copy model limits itself to parallelismin coordination, the next two models simulatestructural priming in general.
Both are similar indesign, and are based on a simple insight: we may418condition a PCFG rule expansion on whether therule occurred in some previous context.
If Prime isa binary-valued random variable denoting if a ruleoccurred in the context, then we define:P(RHS|LHS,Prime) = c(LHS ?
RHS,Prime)c(LHS,Prime)This is essentially an instantiation of Church?s(2000) adaptation probability, albeit with PCFGrules instead of words.
For our first model, thiscontext is the previous sentence.
Thus, the modelcan be said to capture the degree to which rule useis primed between sentences.
We henceforth referto this as the Between model.
Following the con-vention in the psycholinguistic literature, we referto a rule use in the previous sentence as a ?prime?,and a rule use in the current sentence as the ?tar-get?.
Each rule acts once as a target (i.e., the eventof interest) and once as a prime.
We may classifysuch adapted probabilities into ?positive adapta-tion?, i.e., the probability of a rule given the ruleoccurred in the preceding sentence, and ?negativeadaptation?, i.e., the probability of a rule given thatthe rule did not occur in the preceding sentence.2.4 Within ModelJust as the Between model conditions on rulesfrom the previous sentence, the Within sentencemodel conditions on rules from earlier in the cur-rent sentence.
Each rule acts once as a target, andpossibly several times as a prime (for each subse-quent rule in the sentence).
A rule is considered?used?
once the parser passes the word on the left-most corner of the rule.
Because the Within modelis finer grained than the Between model, it can beused to capture the parallelism effect in coordina-tion.
In other words, this model could explain par-allelism in coordination as an instance of a moregeneral priming effect.2.5 ParserAs our main purpose is to build a psycholinguisticmodel of structure repetition, the most importantfeature of the parsing model is to build structuresincrementally.2Reading time experiments, including the paral-lelism studies of Frazier et al (2000), make word-by-word measurements of the time taken to read2In addition to incremental parsing, a characteristic someof psycholinguistic models of sentence comprehension is toparse deterministically.
While we can compute the best in-cremental analysis at any point, ours models do not parse de-terministically.
However, following the principles of rationalanalysis (Anderson, 1991), our goal is not to mimic the hu-man parsing mechanism, but rather to create a model of hu-man parsing behavior.a novel and a bookwrote0 3Terry4 5 61 2 7NP NPNPa novel and a bookTerry wrote0 31 4 5 62 7NPNP NPFigure 1: Upon encountering a coordinator, thecopy model copies the most likely first conjunct.sentences.
Slower reading times are known to becorrelated with processing difficulty, and fasterreading times (as is the case with parallel struc-tures) are correlated with processing ease.
A prob-abilistic parser may be considered to be a sen-tence processing model via a ?linking hypothesis?,which links the parser?s word-by-word behavior tohuman reading behavior.
We discuss this topic inmore detail in Section 3.
At this point, it sufficesto say that we require a parser which has the pre-fix property, i.e., which parses incrementally, fromleft to right.Therefore, we use an Earley-style probabilis-tic parser, which outputs Viterbi parses (Stolcke,1995).
We have two versions of the parser: onewhich parses exhaustively, and a second whichuses a variable width beam, pruning any edgeswhose merit is 12000 of the best edge.
The meritof an edge is its inside probability times a priorP(LHS) times a lookahead probability (Roark andJohnson, 1999).
To speed up parsing time, we rightbinarize the grammar,3 remove empty nodes, coin-dexation and grammatical functions.
As our goalis to create the simplest possible model which cannonetheless model experimental data, we do notmake any tree modification designed to improveaccuracy (as, e.g., Klein and Manning 2003).The approach used to implement the Copymodel is to have the parser copy the subtree of thefirst conjunct whenever it comes across a CC tag.Before copying, though, the parser looks ahead tocheck if the part-of-speech tags after the CC areequivalent to those inside the first conjunct.
Thecopying model is visualized in Figure 1: the toppanel depicts a partially completed edge upon see-ing a CC tag, and the second panel shows the com-pleted copying operation.
It should be clear that3We found that using an unbinarized grammar did not al-ter the results, at least in the exhaustive parsing case.419the copy operation gives the most probable sub-tree in a given span.
To illustrate this, consider Fig-ure 1.
If the most likely NP between spans 2 and 7does not involve copying (i.e.
only standard PCFGrule derivations), the parser will find it using nor-mal rule derivations.
If it does involve copying, forthis particular rule, it must involve the most likelyNP subtree from spans 2 to 3.
As we parse in-crementally, we are guaranteed to have found thisedge, and can use it to construct the copied con-junct over spans 5 to 7 and therefore the wholeco-ordinated NP from spans 2 to 7.To simplify the implementation of the copyingoperation, we turn off right binarization so that theconstituent before and after a coordinator are partof the same rule, and therefore accessible from thesame edge.
This makes it simple to calculate thenew probability: construct the copied subtree, anddecide where to place the resulting edge on thechart.The Between and Within models require a cacheof recently used rules.
This raises two dilem-mas.
First, in the Within model, keeping track offull contextual history is incompatible with chartparsing.
Second, whenever a parsing error occurs,the accuracy of the contextual history is compro-mised.
As we are using a simple unlexicalizedparser, such parsing errors are probably quite fre-quent.We handle the first problem by using one sin-gle parse as an approximation of the history.
Themore realistic choice for this single parse is thebest parse so far according to the parser.
Indeed,this is the approach we use for our main results inSection 3.
However, because of the second prob-lem noted above, in Section 4, we simulated thecontext by filling the cache with rules from thecorrect tree.
In the Between model, these are therules of the correct parse of the previous tree; inthe Within model, these are the rules used in thecorrect parse at points up to (but not including) thecurrent word.3 Human Reading Time ExperimentIn this section, we test our models by applyingthem to experimental reading time data.
Frazieret al (2000) reported a series of experiments thatexamined the parallelism preference in reading.
Inone of their experiments, they monitored subjects?eye-movements while they read sentences like (1):(1) a. Hilda noticed a strange man and a tallwoman when she entered the house.b.
Hilda noticed a man and a tall womanwhen she entered the house.They found that total reading times were faster onthe phrase tall woman in (1a), where the coordi-nated noun phrases are parallel in structure, com-pared with in (1b), where they are not.There are various approaches to modeling pro-cessing difficulty using a probabilistic approach.One possibility is to use an incremental parserwith a beam search or an n-best approach.
Pro-cessing difficulty is predicted at points in the inputstring where the current best parse is replaced byan alternative derivation (Jurafsky, 1996; Crockerand Brants, 2000).
An alternative is to keep trackof all derivations, and predict difficulty at pointswhere there is a large change in the shape ofthe probability distribution across adjacent pars-ing states (Hale, 2001).
A third approach is tocalculate the forward probability (Stolcke, 1995)of the sentence using a PCFG.
Low probabilitiesare then predicted to correspond to high process-ing difficulty.
A variant of this third approach isto assume that processing difficulty is correlatedwith the (log) probability of the best parse (Keller,2003).
This final formulation is the one used forthe experiments presented in this paper.3.1 MethodThe item set was adapted from that of Frazier et al(2000).
The original two relevant conditions oftheir experiment (1a,b) differ in terms of length.This results in a confound in the PCFG frame-work, because longer sentences tend to result inlower probabilities (as the parses tend to involvemore rules).
To control for such length differences,we adapted the materials by adding two extra con-ditions in which the relation between syntacticparallelism and length was reversed.
This resultedin the following four conditions:(2) a. DT JJ NN and DT JJ NN (parallel)Hilda noticed a tall man and a strangewoman when she entered the house.b.
DT NN and DT JJ NN (non-parallel)Hilda noticed a man and a strangewoman when she entered the house.c.
DT JJ NN and DT NN (non-parallel)Hilda noticed a tall man and a womanwhen she entered the house.d.
DT NN and DT NN (parallel)Hilda noticed a man and a woman whenshe entered the house.420In order to account for Frazier et al?s paral-lelism effect a probabilistic model should pre-dict a greater difference in probability be-tween (2a) and (2b) than between (2c) and (2d)(i.e., (2a)?
(2b) > (2c)?(2d)).
This effect will notbe confounded with length, because the relationbetween length and parallelism is reversed be-tween (2a,b) and (2c,d).
We added 8 items to theoriginal Frazier et al materials, resulting in a newset of 24 items similar to (2).We tested three of our PCFG-based models onall 24 sets of 4 conditions.
The models were theBaseline, the Within and the Copy models, trainedexactly as described above.
The Between modelwas not tested as the experimental stimuli werepresented without context.
Each experimental sen-tence was input as a sequence of correct POS tags,and the log probability estimate of the best parsewas recorded.3.2 Results and DiscussionTable 1 shows the mean log probabilities estimatedby the models for the four conditions, along withthe relevant differences between parallel and non-parallel conditions.Both the Within and the Copy models show aparallelism advantage, with this effect being muchmore pronounced for the Copy model than theWithin model.
To evaluate statistical significance,the two differences for each item were comparedusing a Wilcoxon signed ranks test.
Significantresults were obtained both for the Within model(N = 24, Z = 1.67, p < .05, one-tailed) and forthe Copy model (N = 24, Z = 4.27, p < .001, one-tailed).
However, the effect was much larger forthe Copy model, a conclusion which is confirmedby comparing the differences of differences be-tween the two models (N = 24, Z = 4.27, p < .001,one-tailed).
The Baseline model was not evalu-ated statistically, because by definition it predicts aconstant value for (2a)?
(2b) and (2c)?
(2d) acrossall items.
This is simply a consequence of thePCFG independence assumption, coupled with thefact that the four conditions of each experimen-tal item differ only in the occurrences of two NPrules.The results show that the approach taken herecan be successfully applied to the modeling ofexperimental data.
In particular, both the Withinand the Copy models show statistically reliableparallelism effects.
It is not surprising that thecopy model shows a large parallelism effect forthe Frazier et al (2000) items, as it was explicitlydesigned to prefer structurally parallel conjuncts.The more interesting result is the parallelism ef-fect found for the Within model, which shows thatsuch an effect can arise from a more general prob-abilistic priming mechanism.4 Parsing ExperimentIn the previous section, we were able to show thatthe Copy and Within models are able to accountfor human reading-time performance for parallelcoordinate structures.
While this result alone issufficient to claim success as a psycholinguisticmodel, it has been argued that more realistic psy-cholinguistic models ought to also exhibit high ac-curacy and broad-coverage, both crucial propertiesof the human parsing mechanism (e.g., Crockerand Brants, 2000).This should not be difficult: our starting pointwas a PCFG, which already has broad coveragebehavior (albeit with only moderate accuracy).However, in this section we explore what effectsour modifications have to overall coverage, and,perhaps more interestingly, to parsing accuracy.4.1 MethodThe models used here were the ones introducedin Section 2 (which also contains a detailed de-scription of the parser that we used to apply themodels).
The corpus used for both training andevaluation is the Wall Street Journal part of thePenn Treebank.
We use sections 1?22 for train-ing, section 0 for development and section 23 fortesting.
Because the Copy model posits coordi-nated structures whenever POS tags match, pars-ing efficiency decreases if POS tags are not pre-determined.
Therefore, we assume POS tags as in-put, using the gold-standard tags from the treebank(following, e.g., Roark and Johnson 1999).4.2 Results and DiscussionTable 2 lists the results in terms of F-score onthe test set.4 Using exhaustive search, the base-line model achieves an F-score of 73.3, which iscomparable to results reported for unlexicalizedincremental parsers in the literature (e.g.
the RB1model of Roark and Johnson, 1999).
All modelsexhibit a small decline in performance when beamsearch is used.
For the Within model we observe aslight improvement in performance over the base-line, both for the exhaustive search and the beam4Based on a ?2 test on precision and recall, all results arestatistically different from each other.
The Copy model actu-ally performs slightly better than the Baseline in the exhaus-tive case.421Model para: (2a) non-para: (2b) non-para: (2c) para: (2d) (2a)?
(2b) (2c)?
(2d)Baseline ?33.47 ?32.37 ?32.37 ?31.27 ?1.10 ?1.10Within ?33.28 ?31.67 ?31.70 ?29.92 ?1.61 ?1.78Copy ?16.18 ?27.22 ?26.91 ?15.87 11.04 ?11.04Table 1: Mean log probability estimates for Frazier et al(2000) itemsExhaustive Search Beam Search Beam + Coord Fixed CoverageModel F-score Coverage F-score Coverage F-score Coverage F-score CoverageBaseline 73.3 100 73.0 98.0 73.1 98.1 73.0 97.5Within 73.6 100 73.4 98.4 73.0 98.5 73.4 97.5Between 71.6 100 71.7 98.7 71.5 99.0 71.8 97.5Copy 73.3 100 ?
?
73.0 98.1 73.1 97.5Table 2: Parsing results for the Within, Between, and Copy model compared to a PCFG baseline.search conditions.
The Between model, however,resulted in a decrease in performance.We also find that the Copy model performs atthe baseline level.
Recall that in order to simplifythe implementation of the copying, we had to dis-able binarization for coordinate constituents.
Thismeans that quaternary rules were used for coordi-nation (X ?
X1 CC X2 X ?
), while normal binaryrules (X ?
Y X ?)
were used everywhere else.
Itis conceivable that this difference in binarizationexplains the difference in performance betweenthe Between and Within models and the Copymodel when beam search was used.
We there-fore also state the performance for Between andWithin models with binarization limited to non-coordinate structures in the column labeled ?Beam+ Coord?
in Table 2.
The pattern of results, how-ever, remains the same.The fact that coverage differs between modelsposes a problem in that it makes it difficult tocompare the F-scores directly.
We therefore com-pute separate F-scores for just those sentences thatwere covered by all four models.
The results arereported in the ?Fixed Coverage?
column of Ta-ble 2.
Again, we observe that the copy model per-forms at baseline level, while the Within modelslightly outperforms the baseline, and the Betweenmodel performs worse than the baseline.
In Sec-tion 5 below we will present an error analysis thattries to investigate why the adaptation models donot perform as well as expected.Overall, we find that the modifications we intro-duced to model the parallelism effect in humanshave a positive, but small, effect on parsing ac-curacy.
Nonetheless, the results also indicate thesuccess of both the Copy and Within approachesto parallelism as psycholinguistic models: a mod-ification primarily useful for modeling human be-havior has no negative effects on computationalmeasures of coverage or accuracy.5 Distance Between Rule UsesAlthough both the Within and Copy models suc-ceed at the main task of modeling the paral-lelism effect, the parsing experiments in Section 4showed mixed results with respect to F-scores:a slight increase in F-score was observed for theWithin model, but the Between model performedbelow the baseline.
We therefore turn to an erroranalysis, focusing on these two models.Recall that the Within and Between models es-timate two probabilities for a rule, which we havebeen calling the positive adaptation (the probabil-ity of a rule when the rule is also in the history),and the negative adaptation (the probability of arule when the rule is not in the history).
Whilethe effect is not always strong, we expect positiveadaptation to be higher than negative adaptation(Dubey et al, 2005).
However, this is not alwaysthe case.In the Within model, for example, the ruleNP ?
DT JJ NN has a higher negative than posi-tive adaptation (we will refer to such rules as ?neg-atively adapted?).
The more common rule NP ?DT NN has a higher positive adaptation (?pos-itively adapted?).
Since the latter is three timesmore common, this raises a concern: what if adap-tation is an artifact of frequency?
This ?frequency?hypothesis posits that a rule recurring in a sentenceis simply an artifact of the its higher frequency.The frequency hypothesis could explain an inter-esting fact: while the majority of rules tokens havepositive adaptation, the majority of rule types havenegative adaptation.
An important corollary of thefrequency hypothesis is that we would not expectto find a bias towards local rule re-uses.422Iterate through the treebankRemember how many words each constituent spansIterate through the treebankIterate through each treeUpon finding a constituent spanning 1-4 wordsSwap it with a randomly chosen constituentof 1-4 wordsUpdate the remembered size of the swappedconstituents and their subtreesIterate through the treebank 4 more timesSwap constituents of size 5-9, 10-19, 20-35and 35+ words, respectivelyFigure 2: The treebank randomization algorithmNevertheless, the NP ?
DT JJ NN rule isan exception: most negatively adapted rules havevery low frequencies.
This raises the possibilitythat sparse data is the cause of the negativelyadapted rules.
This makes intuitive sense: we needmany rule occurrences to accurately estimate pos-itive or negative adaptation.We measure the distribution of rule use to ex-plore if negatively adapted rules owe more to fre-quency effects or to sparse data.
This distributionalanalysis also serves to measure ?decay?
effects instructural repetition.
The decay effect in priminghas been observed elsewhere (Szmrecsanyi, 2005),and suggests that positive adaptation is higher thecloser together two rules are.5.1 MethodWe investigate the dispersion of rules by plot-ting histograms of the distance between subse-quent rule uses.
The basic premise is to look forevidence of an early peak or skew, which sug-gests rule re-use.
To ensure that the histogram it-self is not sensitive to sparse data problems, wegroup all rules into two categories: those which arepositively adapted, and those which are negativelyadapted.If adaptation is not due to frequency alone, wewould expect the histograms for both positivelyand negatively adapted rules to be skewed towardslocal rule repetition.
Detecting a skew requires abaseline without repetition.
We propose the con-cept of ?randomizing?
the treebank to create sucha baseline.
The randomization algorithm is de-scribed in Figure 2.
The algorithm entails swap-ping subtrees, taking care that small subtrees areswapped first (otherwise large chunks would beswapped at once, preserving a great deal of con-text).
This removes local effects, giving a distribu-tion due frequency alone.After applying the randomization algorithm tothe treebank, we may construct the distance his-0 5 10Logarithm of Word Distance00.0050.010.0150.02NormalizedFrequencyofRule Occurance+ Adapt, Untouched Corpus+ Adapt, Randomized Corpus- Adapt, Untouched Corpus- Adapt, Randomized CorpusFigure 3: Log of number of words between ruleinvocationstogram for both the non-randomized and random-ized treebanks.
The distance between two occur-rences of a rule is calculated as the number ofwords between the first word on the left corner ofeach rule.
A special case occurs if a rule expansioninvokes another use of the same rule.
When thishappens, we do not count the distance between thefirst and second expansion.
However, the secondexpansion is still remembered as the most recent.We group rules into those that have a higherpositive adaptation and those that have a highernegative adaptation.
We then plot a histogram ofrule re-occurrence distance for both groups, inboth the non-randomized and randomized corpora.5.2 Results and DiscussionThe resulting plot for the Within model is shownin Figure 3.
For both the positive and negativelyadapted rules, we find that randomization resultsin a lower, less skewed peak, and a longer tail.We conclude that rules tend to be repeated closeto one another more than we expect by chance,even for negatively adapted rules.
This is evidenceagainst the frequency hypothesis, and in favor ofthe sparse data hypothesis.
This means that thesmall size of the increase in F-score we found inSection 4 is not due to the fact that the adaptionis just an artifact of rule frequency.
Rather, it canprobably be attributed to data sparseness.Note also that the shape of the histogram pro-vides a decay curve.
Speculatively, we suggest thatthis shape could be used to parameterize the decayeffect and therefore provide an estimate for adap-tation which is more robust to sparse data.
How-ever, we leave the development of such a smooth-ing function to future research.4236 Conclusions and Future WorkThe main contribution of this paper has been toshow that an incremental parser can simulate syn-tactic priming effects in human parsing by incor-porating probability models that take account ofprevious rule use.
Frazier et al (2000) argued thatthe best account of their observed parallelism ad-vantage was a model in which structure is copiedfrom one coordinate sister to another.
Here, we ex-plored a probabilistic variant of the copy mecha-nism, along with two more general models basedon within- and between-sentence priming.
Al-though the copy mechanism provided the strongestparallelism effect in simulating the human readingtime data, the effect was also successfully simu-lated by a general within-sentence priming model.On the basis of simplicity, we therefore argue thatit is preferable to assume a simpler and more gen-eral mechanism, and that the copy mechanism isnot needed.
This conclusion is strengthened whenwe turn to consider the performance of the parseron the standard Penn Treebank test set: the Withinmodel showed a small increase in F-score over thePCFG baseline, while the copy model showed nosuch advantage.5All the models we proposed offer a broad-coverage account of human parsing, not just a lim-ited model on a hand-selected set of examples,such as the models proposed by Jurafsky (1996)and Hale (2001) (but see Crocker and Brants2000).A further contribution of the present paper hasbeen to develop a methodology for analyzing the(re-)use of syntactic rules over time in a corpus.
Inparticular, we have defined an algorithm for ran-domizing the constituents of a treebank, yieldinga baseline estimate of chance repetition.In the research reported in this paper, we haveadopted a very simple model based on an unlex-icalized PCFG.
In the future, we intend to ex-plore the consequences of introducing lexicaliza-tion into the parser.
This is particularly interest-ing from the point of view of psycholinguisticmodeling, because there are well known inter-actions between lexical repetition and syntacticpriming, which require lexicalization for a propertreatment.
Future work will also involve the useof smoothing to increase the benefit of primingfor parsing accuracy.
The investigations reported5The broad-coverage parsing experiment speaks againsta ?facilitation?
hypothesis, i.e., that the copying and prim-ing mechanisms work together.
However, a full test of this(e.g., by combining the two models) is left to future research.in Section 5 provide a basis for estimating thesmoothing parameters.ReferencesAnderson, John.
1991.
Cognitive architectures in a ratio-nal analysis.
In K. VanLehn, editor, Architectures for In-telligence, Lawrence Erlbaum Associates, Hillsdale, N.J.,pages 1?24.Bock, J. Kathryn.
1986.
Syntactic persistence in languageproduction.
Cognitive Psychology 18:355?387.Church, Kenneth W. 2000.
Empirical estimates of adapta-tion: the chance of two Noriegas is closer to p/2 than p2.In Proceedings of the 17th Conference on ComputationalLinguistics.
Saarbru?cken, Germany, pages 180?186.Crocker, Matthew W. and Thorsten Brants.
2000.
Wide-coverage probabilistic sentence processing.
Journal ofPsycholinguistic Research 29(6):647?669.Dubey, Amit, Patrick Sturt, and Frank Keller.
2005.
Paral-lelism in coordination as an instance of syntactic priming:Evidence from corpus-based modeling.
In Proceedingsof the Human Language Technology Conference and theConference on Empirical Methods in Natural LanguageProcessing.
Vancouver, pages 827?834.Frazier, Lyn, Alan Munn, and Chuck Clifton.
2000.
Process-ing coordinate structures.
Journal of Psycholinguistic Re-search 29(4):343?370.Frazier, Lynn and Charles Clifton.
2001.
Parsing coordinatesand ellipsis: Copy ?.
Syntax 4(1):1?22.Hale, John.
2001.
A probabilistic Earley parser as a psy-cholinguistic model.
In Proceedings of the 2nd Confer-ence of the North American Chapter of the Associationfor Computational Linguistics.
Pittsburgh, PA.Jurafsky, Daniel.
1996.
A probabilistic model of lexical andsyntactic access and disambiguation.
Cognitive Science20(2):137?194.Keller, Frank.
2003.
A probabilistic parser as a model ofglobal processing difficulty.
In R. Alterman and D. Kirsh,editors, Proceedings of the 25th Annual Conference of theCognitive Science Society.
Boston, pages 646?651.Klein, Dan and Christopher D. Manning.
2003.
Accurate Un-lexicalized Parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics.Sapporo, Japan, pages 423?430.Kuhn, Roland and Renate de Mori.
1990.
A cache-based nat-ural language model for speech recognition.
IEEE Tran-sanctions on Pattern Analysis and Machine Intelligence12(6):570?583.Roark, Brian and Mark Johnson.
1999.
Efficient probabilistictop-down and left-corner parsing.
In Proceedings of the37th Annual Meeting of the Association for ComputationalLinguistics.
pages 421?428.Stolcke, Andreas.
1995.
An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.Computational Linguistics 21(2):165?201.Szmrecsanyi, Benedikt.
2005.
Creatures of habit: A corpus-linguistic analysis of persistence in spoken English.
Cor-pus Linguistics and Linguistic Theory 1(1):113?149.424
