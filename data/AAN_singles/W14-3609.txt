Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 73?77,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsCombining strategies for tagging and parsing ArabicMaytham AlabbasDepartment of Computer ScienceUniversity of BasrahBasrah, Iraqmaytham.alabbas@gmail.comAllan RamsaySchool of Computer ScienceUniversity of ManchesterManchester M13 9PL, UKAllan.Ramsay@manchester.ac.ukWe describe a simple method for com-bining taggers which produces substan-tially better performance than any of thecontributing tools.
The method is verysimple, but it leads to considerable im-provements in performance: given threetaggers for Arabic whose individual ac-curacies range from 0.956 to 0.967, thecombined tagger scores 0.995?a seven-fold reduction in the error rate whencompared to the best of the contributingtools.Given the effectiveness of this approachto combining taggers, we have investi-gated its applicability to parsing.
Forparsing, it seems better to take pairs ofsimilar parsers and back off to a third ifthey disagree.1 IntroductionIf you have several systems that perform the sametask, it seems reasonable to suppose that you canobtain better performance by using some judiciouscombination of them than can be obtained by anyof them in isolation.
A large number of combin-ing strategies have been proposed, with majorityvoting being particularly popular (Stefano et al.,2002).
We have investigated a range of such strate-gies for combining taggers and parsers for Ara-bic: the best strategy we have found for tagginginvolves asking each of the contributing taggershow confident it is, and accepting the answer givenby the most confident one.
We hypothesise thatthe reason for the effectiveness of this strategy fortagging arises from the fact that the contributingtaggers work in essentially different ways (differ-ent training data, different underlying algorithms),and hence if they make systematic mistakes thesewill tend to be different.
This means, in turn, thatthe places where they don?t make mistakes will bedifferent.This strategy is less effective for parsing.
Wehave tried combining two members of the MALT-Parser family (Nivre et al., 2006; Nivre et al.,2007; Nivre et al., 2010) with MSTParser (Mc-Donald et al., 2006a; McDonald et al., 2006b).The best strategy here seems to be to accept theoutput of the two versions of MALTParser whenthey agree, but to switch to MSTParser if theMALTParser versions disagree.
It may be that thisis because the MALTParser versions are very sim-ilar, so that when they disagree this suggests thatthere is something anomalous about the input text,and that neither of them can be trusted at this point.2 TaggingWe present a very simple strategy for combin-ing part-of-speech (POS) taggers which leads tosubstantial improvements in accuracy.
A num-ber of combination strategies have been proposedin the literature (Zeman and ?Zabokrtsky`, 2005).In experiments with combining three Arabic tag-gers (AMIRA (Diab, 2009), MADA (Habash etal., 2009) and a simple affix-based maximum-likelihood Arabic tagger (MXL) (Ramsay andSabtan, 2009)) the current strategy significantlyoutperformed voting-based strategies.We used the Penn Arabic Treebank (PATB) Part1 v3.0 as a resource for our experiments.
Thewords in the PATB are already tagged, which thusprovides us with a widely-accepted Gold standard.Even PATB tagging is not guaranteed to be 100%accurate, but it nonetheless provides as good a ref-erence set as can be found.1The PATB uses the tags provided by the Buck-walter morphological analyser (Buckwalter, 2004;Buckwalter, 2007), which carry a great deal1The PATB is the largest easily available tagged Arabiccorpus, with about 165K words in the section we are us-ing.
Thus for each fold of our 10-fold testing regime we aretraining on 150K words and testing on 15K, which should beenough to provide robust results.73of syntactically relevant information (particularlycase-marking).
This tagset contains 305 tags, withfor instance 47 tags for different kinds of verband 44 for different kinds of noun.
The very finedistinctions between different kinds of nouns andverbs (e.g.
between subject and object case nouns)in the absence of visible markers make this an ex-tremely difficult tagset to work with.
It is in gen-eral virtually impossible to decide the case of anArabic noun until its overall syntactic role is de-termined, and it is similarly difficult to decide theform of a verb until the overall syntactic structureof the sentence is determined.
For this reason tag-gers often work with a coarser set of tags, of whichthe ?Bies tagset?
(Maamouri and Bies, 2004) iswidely used (see for instance the Stanford Arabicparser (Green and Manning, 2010)).
We carriedout our experiments with a variant of the origi-nal fine-grained tagset, and also with a variant ofthe coarser-grained Bies set obtained by deletingdetails such as case- and agreement-markers.
Wecarried out two sets of experiments, with a coarse-grained set of tags (a superset of the Bies tagsetwith 39 tags, shown in Figure 1) and the originalfine-grained one with 305 tags.ABBREVADJADVCONJCVCVSUFF DODEM PRONDETDET+ADJDET+NOUNDET+NOUN PROPDET+NUMEMPH PARTEXCEPT PARTFOCUS PARTFUT+IVINTERJINTERROG PARTIVIVSUFF DOLATINNEG PARTNOUNNOUN PROPNO FUNCNUMPARTPOSS PRONPREPPRONPUNCPVPVSUFF DORC PARTREL ADVREL PRONSUBSUB CONJVERB PARTTable 1: Coarse-grained tagsetThe accuracy of a tagger clearly depends onthe granularity of the tagset: the contributing tag-gers produced scores from 0.955 to 0.967 on thecoarse-grained tagset, and from 0.888 to 0.936 onthe fine-grained one.
We applied transformation-based retagging (TBR) (Brill, 1995; Lager, 1999)to the output of the basic taggers, which produceda small improvement in the results for MADAand MXL and a more substantial improvementfor AMIRA.
Table 2 shows the performance ofthe three taggers using the two tagsets with andwithout TBR.
The improvement obtained by usingPOS TBR AMIRA MXL MADACoarse ?
0.896 0.952 0.941?
0.953 0.956 0.967Fine ?
0.843 0.897 0.917?
0.888 0.912 0.936Table 2: Tagger accuracies in isolation, with andwithout TBRTBR for AMIRA arises largely from the fact thatin some cases AMIRA uses tags similar to thoseused in the English Penn Treebank rather than theones in the the tags in the PATB, e.g.
JJ foradjectives where the PATB uses ADJ.
TBR pro-vides a simple and reliable mechanism for discov-ering and patching systematic renamings of thiskind, and hence is extremely useful when workingwith different tagsets.
A significant component ofthe remaining errors produced by AMIRA arisebecause AMIRA has a much coarser classifica-tion of particles than the classification provided bythe Buckwalter tagset.
Since AMIRA assigns thesame tag to a variety of different particles, TBRcannot easily recover the correct fine-grained tags,and hence AMIRA makes a substantial number oferrors on these items.The key to the proposed combining strategy isthat each of the contributing taggers is likely tomake systematic mistakes; and that if they arebased on different principles they are likely tomake different systematic mistakes.
If we clas-sify the mistakes that a tagger makes, we should beable to avoid believing it in cases where it is likelyto be wrong.
So long as the taggers are basedon sufficiently different principles, they should bewrong in different places.We therefore collected confusion matrices foreach of the individual taggers showing how likelythey were to be right for each category of item?how likely, for instance, was MADA to be rightwhen it proposed to tag some item as a noun (verylikely?accuracy of MADA when it proposes NNis 0.98), how likely was AMIRA to be right whenit proposed the tag RP (very unlikely?accuracy of0.08 in this case)?
Given these tables, we simplytook the tagger whose prediction was most likelyto be right.2Table 3 shows an excerpt from the output of the2All the tagging results reported below were obtained byusing 10-fold cross validation, i.e.
carrying out 10 experi-ments each of which involved removing 10% of the data fortesting and training on the remaining 90%.74Word Gold standard MADA MXL AMIRA TAG.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.gyr NEG PART NOUN (0.979) NEG PART (0.982) RP (0.081) NEG PART<lA EXCEPT PART EXCEPT PART (1.00) SUB CONJ (0.965) RP (0.790) EXCEPT PART.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Table 3: Confidence levels for individual tagsthree individual taggers looking at a string con-taining the two words gyr and <lA, with the tagsannotated with the accuracy of each tagger on thegiven tag, e.g.
in this sequence MADA has taggedgyr as a noun, and MXL has tagged it as a neg-ative particle and AMIRA has tagged it as RP;and when MADA suggests NOUN as the tag it isright 97.9% of the time, whereas when MXL sug-gests NEG PART it is right 98.2% of the time andAMIRA is right just 8.1% of the time when it sug-gests RP.
It is important to note that the tags areassigned to words in context, but the confidencelevels are calculated across the entire training data.The fact that MADA is right 97.9% of the timewhen it assigns the tag NOUN is not restricted tothe word gyr, and certainly not to this occurrenceof this word.We compared the results of this simple strategy,which is similar to a strategy proposed for imageclassification by Woods at el.
(1997), with a strat-egy proposed by (2005), in which you accept themajority view if at least two of the taggers agree,and you back off to one of them if they all dis-agree, and with a variation on that where you ac-cept the majority view if two agree and back off tothe most confident if they all disagree.
The resultsare given in Table 4.All four strategies produce an improvementover the individual taggers.
The fact that ma-jority voting works better when backing off toMXL than to MADA, despite the fact that MADAworks better in isolation, is thought-provoking.
Itseems likely to be that this arises from the fact thatMADA and AMIRA are based on similar princi-ples, and hence are likely to agree even when theyare wrong.
This hypothesis suggested that lookingat the likely accuracy of each tagger on each casemight be a good backoff strategy.
It turns out thatit is not just a good backoff strategy, as shown inthe third column of Table 4: it is even better whenused as the main strategy (column 5).
The differ-ences between columns 4 and 5 are not huge,3 butthat should not be too surprising, since these twostrategies will agree in every case where all threeof the contributing taggers agree, so the only placewhere these two will disagree is when one of thetaggers disagrees with the others and the isolatedtagger is more confident than either of the others.The idea reported here is very simple, but it isalso very effective.
We have reduced the error intagging with fairly coarse-grained tags to 0.05%,and we have also produced a substantial improve-ment for the fine grained tags, from 0.936 for thebest of the individual taggers to 0.96 for the com-bination.3 ParsingGiven the success of the approach outlined abovefor tagging, it seemed worth investigating whetherthe same idea could be applied to parsing.
Wetherefore tried using it with a combination of de-pendency parsers, for which we used MSTParser(McDonald et al., 2006a; McDonald et al., 2006b)and two variants from the MALTParser family(Nivre et al., 2006; Nivre et al., 2007; Nivre etal., 2010), namely Nivre arc-eager, which we willrefer to as MALTParser1, and stack-eager, whichwe will refer to as MALTParser2.
The results inTable 5 include (i) the three parsers in isolation;(ii) a strategy in which we select a pair and trusttheir proposals wherever they agree, and back-off3In terms of error rate the difference looks more substan-tial, since the error rate, 0.005, for column 5 for the fine-grained set is 62.5% of that for column 4, 0.008; and for thecoarse-grained set the error rate for column 5, 0.04, is 73%of that for column 4, 0.055TagsetMajority voting Majority voting Majority voting Majority voting Just most(back off to MXL) (back off to MADA) (back off to AMIRA) (most confident) confidentCoarse-grained 0.982 0.979 0.975 0.992 0.995Fine-grained 0.918 0.915 0.906 0.945 0.96Table 4: Modified majority voting vs proposed strategy75Parser LA(i)MSTParser 0.816MALTParser10.797MALTParser20.796(ii)Use MSTParser & MALTParser1if they agree, backoff to MALTParser20.838Use MSTParser & MALTParser2if they agree, backoff to MALTParser20.837Use MALTParser1& MALTParser2if they agree, backoff to MSTParser 0.848(iii)Use MSTParse & MALTParser1if they agree, backoff to most confident 0.801Use MSTParser & MALTParser2if they agree, backoff to most confident 0.799Use MALTParser1& MALTParser2if they agree, backoff to most confident 0.814(iv)If at least two agree use their proposal, backoff to most confident 0.819If all three agree use their proposal, backoff to most confident 0.797Most confident parser only 0.789Table 5: Labelled accuracy (LA) for various combinations of MSTParser, MALTParser1andMALTParser2five fold cross-validation with 4000 training sentences and 1000 testingto the other one when they do not; (iii) a strategyin which we select a pair and trust them wheneverthey agree and backoff to the parser which is mostconfident (which may be one of these or may bethe other one) when they do not; (iv) strategieswhere we either just use the most confident one,or where we take either a unanimous vote or a ma-jority vote and backoff to the most confident oneif this is inconclusive.
All these experiments werecarried using fivefold cross-validation over a set of5000 sentences from the PATB (i.e.
each fold has4000 sentences for training and 1000 for testing).These results indicate that for parsing, simplyrelying on the parser which is most likely to beright when choosing the head for a specific depen-dent in isolation does not produce the best over-all result, and indeed does not even surpass theindividual parsers in isolation.
For these exper-iments, the best results were obtained by askinga predefined pair of parsers whether they agreeon the head for a given item, and backing off tothe other one when they do not.
This fits withHenderson and Brill (2000)?s observations abouta similar strategy for dependency parsing for En-glish.
It seems likely that the problem with rely-ing on the most confident parser for each individ-ual daughter-head relation is that this will tend toignore the big picture, so that a collection of rela-tions that are individually plausible, but which donot add up to a coherent overall analysis, will bepicked.4 ConclusionsIt seems that the success of the proposed methodfor tagging depends crucially on having taggersthat exploit different principles, since under thosecircumstances the systematic errors that the dif-ferent taggers make will be different; and on thefact that POS tags can be assigned largely inde-pendently (though of course each of the individualtaggers makes use of information about the localcontext, and in particular about the tags that havebeen assigned to neighbouring items).
The rea-son why simply taking the most likely proposalsin isolation is ineffective when parsing may be thatglobal constraints such as Henderson and Brill?s?no crossing brackets?
requirement are likely tobe violated.
Interestingly, the most effective ofour strategies for combining parsers takes two thatuse the same learning algorithm and same featuresets but different parsing strategies (MALTParser1and MALTParser2), and relies on them when theyagree; and backs off to MSTParser, which ex-ploits fundamentally different machinery, whenthese two disagree.
In other words, it makes useof two parsers that depend on very similar under-lying principles, and hence are likely to make thesame systematic errors, and backs off to one thatexploits different principles when they disagree.We have not carried out a parallel set of exper-iments on taggers for languages other than Arabicbecause we do not have access to taggers wherewe have reason to believe that the underlying prin-ciples are different for anything other than Ara-bic.
In situations where three (or more) distinctapproaches to a problem of this kind are available,76it seems at least worthwhile investigating whetherthe proposed method of combination will work.AcknowledgementsMaytham Alabbas owes his deepest gratitude toIraqi Ministry of Higher Education and ScientificResearch for financial support in his PhD study.Allan Ramsay?s contribution to this work was par-tially supported by the Qatar National ResearchFund (grant NPRP 09-046-6-001).ReferencesE Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a casestudy in part of speech tagging.
Computational Lin-guistics, 23(4):543?565.T Buckwalter.
2004.
Buckwalter Arabic morpholog-ical analyzer version 2.0.
Linguistic Data Consor-tium.T Buckwalter.
2007.
Issues in Arabic morphologicalanalysis.
ARabic computational morphology, pages23?41.M.
Diab.
2009.
Second Generation Tools (AMIRA2.0): Fast and Robust Tokenization, POS Tagging,and Base Phrase Chunking.
In Proceedings of theSecond International Conference on Arabic Lan-guage Resources and Tools, pages 285?288, Cairo,Eygpt, April.
The MEDAR Consortium.S Green and C D Manning.
2010.
Better arabicparsing: Baselines, evaluations, and analysis.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages394?402, Stroudsburg, PA, USA.
Association forComputational Linguistics.N.
Habash, O. Rambow, and R. Roth.
2009.MADA+TOKAN: A Toolkit for Arabic Tokeniza-tion, Diacritization, Morphological Disambiguation,POS Tagging, Stemming and Lemmatization.
InProceedings of the Second International Conferenceon Arabic Language Resources and Tools, Cairo.The MEDAR Consortium.J C Henderson and E Brill.
2000.
Exploiting diversityin natural language processing: Combining parsers.CoRR, cs.CL/0006003.T Lager.
1999.
?-tbl lite: a small, extendibletransformation-based learner.
In Proceedings of the9th European Conference on Computational Lin-guistics (EACL-99), pages 279?280, Bergen.
Asso-ciation for Computational Linguistics.M Maamouri and A Bies.
2004.
Developing an Ara-bic treebank: methods, guidelines, procedures, andtools.
In Proceedings of the Workshop on Com-putational Approaches to Arabic Script-based Lan-guages, pages 2?9, Geneva.R McDonald, K Lerman, and F Pereira.
2006a.
Mul-tilingual dependency parsing with a two-stage dis-criminative parser.
In Tenth Conference on Com-putational Natural Language Learning (CoNLL-X),New York.R McDonald, K Lerman, and F Pereira.
2006b.
Mul-tilingual dependency parsing with a two-stage dis-criminative parser.
In Tenth Conference on Com-putational Natural Language Learning (CoNLL-X),New York.J.
Nivre, J.
Hall, and J. Nilsson.
2006.
MaltParser:A data-driven parser-generator for dependency pars-ing.
In Proceedings of the International Conferenceon Language Resources and Evaluation (LREC),volume 6, pages 2216?2219.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(02):95?135.J Nivre, L Rimell, R McDonald, and C Go?mez-Rodr??guez.
2010.
Evaluation of dependency parserson unbounded dependencies.
In Proceedings of the23rd International Conference on ComputationalLinguistics, COLING ?10, pages 833?841, Beijing.A.
Ramsay and Y. Sabtan.
2009.
Bootstrapping alexicon-free tagger for Arabic.
In Proceedings ofthe 9th Conference on Language Engineering, pages202?215, Cairo, Egypt, December.Claudio De Stefano, Antonio Della Cioppa, and An-gelo Marcelli.
2002.
An adaptive weighted major-ity vote rule for combining multiple classifiers.
InICPR (2), pages 192?195.Kevin Woods, W. Philip Kegelmeyer, Jr., and KevinBowyer.
1997.
Combination of multiple classifiersusing local accuracy estimates.
IEEE Trans.
PatternAnal.
Mach.
Intell., 19(4):405?410, April.D.
Zeman and Z.
?Zabokrtsky`.
2005.
Improvingparsing accuracy by combining diverse dependencyparsers.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 171?178.Association for Computational Linguistics.77
