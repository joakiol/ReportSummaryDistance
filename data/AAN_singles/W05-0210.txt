Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 61?68, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Measuring Non-native Speakers?
Proficiency of English by Using a Testwith Automatically-Generated Fill-in-the-Blank QuestionsEiichiro SUMITASpoken Language CommunicationResearch LaboratoriesATRKyoto 619-0288 Japaneiichiro.sumita@atr.jpFumiaki SUGAYAText Information Processing Labo-ratoryKDDI R&D Laboratories Inc.Saitama 356-8502 Japanfsugaya@kddilabs.jpSeiichi YamamotoDepartment of Information SystemsDesignDoshisha UniversityKyoto 610-0321 Japanseyamamo@mail.doshisha.ac.jp&Spoken Language CommunicationResearch LaboratoriesATRAbstractThis paper proposes the automatic generationof Fill-in-the-Blank Questions (FBQs) togetherwith testing based on Item Response Theory(IRT) to measure English proficiency.
First, theproposal generates an FBQ from a given sen-tence in English.
The position of a blank in thesentence is determined, and the word at thatposition is considered as the correct choice.The candidates for incorrect choices for theblank are hypothesized through a thesaurus.Then, each of the candidates is verified by us-ing the Web.
Finally, the blanked sentence, thecorrect choice and the incorrect choices surviv-ing the verification are together laid out toform the FBQ.
Second, the proficiency of non-native speakers who took the test consisting ofsuch FBQs is estimated through IRT.Our experimental results suggest that:(1) the generated questions plus IRT estimatethe non-native speakers?
English proficiency;(2) while on the other hand, the test can becompleted almost perfectly by English nativespeakers; and (3) the number of questions canbe reduced by using item information in IRT.The proposed method provides teach-ers and testers with a tool that reduces timeand expenditure for testing English profi-ciency.1 IntroductionEnglish has spread so widely that 1,500 millionpeople, about a quarter of the world?s population,speak it, though at most about 400 million speak itas their native language (Crystal, 2003).
Thus,English education for non-native speakers bothnow and in the near future is of great importance.The progress of computer technology is ad-vancing an electronic tool for language learningcalled Computer-Assisted Language Learning(CALL) and for language testing called Computer-Based Testing (CBT) or Computer-Adaptive Test-ing (CAT).
However, no computerized support forproducing a test, a collection of questions forevaluating language proficiency, has emerged todate.
*Fill-in-the-Blank Questions (FBQs) are widelyused from the classroom level to far larger scalesto measure peoples?
proficiency at English as asecond language.
Examples of such tests includeTOEFL (Test Of English as a Foreign Language,http://www.ets.org/toefl/) and TOEIC (Test OfEnglish for International Communication,http://www.ets.org/toeic/).A test comprising FBQs has merits in that (1) itis easy for test-takers to input answers, (2) com-puters can mark them, thus marking is invariableand objective, and (3) they are suitable for themodern testing theory, Item Response Theory(IRT).Because it is regarded that writing incorrectchoices that distract only the non-proficient test-taker is a highly skilled business (Alderson, 1996),FBQs have been written by human experts.
Thus,test construction is time-consuming and expensive.As a result, utilizing up-to-date texts for questionwriting is not practical, nor is tuning in to individ-ual students.
* See the detailed discussion in Section 6.61To solve the problems of time and expenditure,this paper proposes a method for generating FBQsusing a corpus, a thesaurus, and the Web.
Experi-ments have shown that the proficiency estimatedthrough IRT with generated FBQs highly corre-lates with non-native speakers?
real proficiency.This system not only provides us with a quick andinexpensive testing method, but it also features thefollowing advantages:(I) It provides ?anyone?
individually withup-to-date and interesting questions forself-teaching.
We have implemented aprogram that downloads any Web pagesuch as a news site and generates ques-tions from it.
(II) It also enables on-demand testing at?anytime and anyplace.?
We have im-plemented a system that operates on amobile phone.
Questions are generatedand pooled in the server, and upon auser?s request, questions aredownloaded.
CAT (Wainer, 2000) isthen conducted on the phone.
The sys-tem for mobile phone is scheduled to bedeployed in May of 2005 in Japan.The remainder of this paper is organized as fol-lows.
Section 2 introduces a method for makingFBQ, Section 3 explains how to estimate test-takers?
proficiency, and Section 4 presents the ex-periments that demonstrate the effectiveness of theproposal.
Section 5 provides some discussion, andSection 6 explains the differences between ourproposal and related work, followed by concludingremarks.22.1Question Generation MethodWe will review an FBQ, and then explain ourmethod for producing it.Fill-in-the-Blank Question (FBQ)FBQs are the one of the most popular types ofquestions in testing.
Figure 1 shows a typical sam-ple consisting of a partially blanked English sen-tence and four choices for filling the blank.
Thetester ordinarily assumes that exactly one choice iscorrect (in this case, b)) and the other three choicesare incorrect.
The latter are often called distracters,because they fulfill a role to distract the less profi-cient test-takers.Figure 1: A sample Fill-in-the-Blank Question(FBQ)Question 1 (FBQ)I only have to _______ my head above water one moreweek?a) reserve b) keep c) guarantee d) promiseN.B.
the correct choice is b) keep.2.2Flow of generationUsing question 1 above, the outline of generationis presented below (Figure 2).A seed sentence (in this case, ?I only have tokeep my head above water one more week.?)
isinput from the designated source, e.g., a corpus ora Web page such as well-known news site.
*Figure 2: Flow generating Fill-In-The-Blank Ques-tion (FBQ)Seed Sentence CorpusTestingknowledge[a] Determine the blank position[b] Generate distracter candidatesLexicon[c] Verify the incorrectness[d] Form the questionQuestion[a] The seed sentence is a correct English sen-tence that is decomposed into a sentencewith a blank (blanked sentence) and thecorrect choice for the blank.
After the seed*  Selection of the seed sentence (source text) is an importantopen problem because the difficulty of the seed (text) shouldinfluence the difficulty of the generated question.
As for textdifficulty, several measures such as Lexile by MetaMetrics(http://www.Lexile.com) have been proposed.
They are knownas readability and are usually defined as a function of sentencelength and word frequency.In this paper, we used corpora of business and travel con-versations, because TOEIC itself is oriented toward businessand daily conversation.62sentence is analyzed morphologically by acomputer, according to the testing knowl-edge* the blank position of the sentence isdetermined.
In this paper?s experiment, theverb of the seed is selected, and we obtainthe blanked sentence ?I only have to______ my head above water one moreweek.?
and the correct choice ?keep.?
[b] To be a good distracter, the candidates mustmaintain the grammatical characteristics ofthe correct choice, and these should besimilar in meaning?
.
Using a thesaurus?
,words similar to the correct choice arelisted up as candidates, e.g., ?clear,?
?guar-antee,?
?promise,?
?reserve,?
and ?share?for the above ?keep.?
[c] Verify (see Section 2.3 for details) the in-correctness of the sentence restored by eachcandidate, and if it is not incorrect (in thiscase, ?clear?
and ?share?
), the candidate isgiven up.
[d] If a sufficient number (in this paper, three)of candidates remain, form a question byrandomizing the order of all the choices(?keep,?
?guarantee,?
?promise,?
and ?re-serve?
); otherwise, another seed sentence isinput and restart from step [a].2.3 Incorrectness VerificationIn FBQs, by definition, (1) the blanked sentencerestored with the correct choice is correct, and (2)the blanked sentence restored with the distractermust be incorrect.In order to generate an FBQ, the incorrectnessof the sentence restored by each distracter candi-date must be verified and if the combination is notincorrect, the candidate is rejected.Zero-Hit SentenceThe Web includes all manners of language datain vast quantities, which are for everyone easy toaccess through a networked computer.
Recently,exploitation of the Web for various natural lan-guage applications is rising (Grefenstette, 1999;Turney, 2001; Kilgarriff and Grefenstette, 2003;Tonoike et al, 2004).We also propose a Web-based approach.
Wedare to assume that if there is a sentence on theWeb, that sentence is considered correct; other-wise, the sentence is unlikely to be correct in thatthere is no sentence written on the Web despite thevariety and quantity of data on it.
*  Testing knowledge tells us what part of the seed sentenceshould be blanked.
For example, we selected the verb of theseed because it is one of the basic types of blanked words inpopular FBQs such as in TOEIC.Figure 3 illustrates verification based on the re-trieval from the Web.
Here, s (x) is the blankedsentence, s (w) denotes the sentence restored by theword w, and hits (y) represents the number ofdocuments retrieved from the Web for the key y.This can be a word of another POS (Part-Of-Speech).
Forthis, we can use knowledge in the field of second-languageeducation.
Previous studies on errors in English usage byJapanese native speakers such as (Izumi and Isahara, 2004)unveiled patterns of errors specific to Japanese, e.g., (1) articleselection error, which results from the fact there are no articlesin Japanese; (2) preposition selection error, which results fromthe fact some Japanese counterparts have broader meaning; (3)adjective selection error, which results from mismatch ofmeaning between Japanese words and their counterpart.
Suchknowledge may generate questions harder for Japanese whostudy English.?
There are various aspects other than meaning, for example,spelling, pronunciation, and translation and so on.
Dependingon the aspect, lexical information sources other than a thesau-rus should be consulted.
Figure 3: Incorrectness and Hits on the WebBlanked sentence:s (x)= ?I only have to ____ my head above waterone more week?
?Hits of incorrect choice candidates:hits (s (?clear?))
= 176 ; correcthits (s (?guarantee?))
= 0 ; incorrecthits (s (?promise?))
= 0 ; incorrecthits (s (?reserve?))
= 0 ; incorrecthits (s (?share?))
= 3 ; correct?
We used an in-house English thesaurus whose hierarchy isbased on one of the off-the-shelf thesauruses for Japanese,called Ruigo-Shin-Jiten (Ohno and Hamanishi, 1984).
In theabove examples, the original word ?keep?
expresses two dif-ferent concepts: (1) possession-or-disposal, which is shared bythe words ?clear?
and ?share,?
and (2) promise, which isshared by the words ?guarantee,?
?promise,?
and ?reserve.
?Since this depends on the thesaurus used, some may sense aslight discomfort at these concepts.
If a different thesaurus isused, the distracter candidates may differ.If hits (s (w)), is small, then the sentence re-stored with the word w is unlikely, thus the word wshould be a good distracter.
If hits (s (w)), is largethen the sentence restored with the word w is likely,then the word w is unlikely to be a good distracterand is given up.63We used the strongest condition.
If hits (s (w))is zero, then the sentence restored with the word wis unlikely, thus the word w should be a good dis-tracter.
If hits (s (w)), is not zero, then the sentencerestored with the word w is likely, thus the word wis unlikely to be a good distracter and is given up.33.1Estimating ProficiencyItem Response Theory (IRT)Item Response Theory (IRT) is the basis of modernlanguage tests such as TOEIC, and enables Com-puterized Adaptive Testing (CAT).
Here, webriefly introduce IRT.
IRT, in which a question iscalled an item, calculates the test-takers?
profi-ciency based on the answers for items of the giventest (Embretson, 2000).Retrieval NOT By SentenceIt is often the case that retrieval by sentence doesnot work.
Instead of a sentence, a sequence ofwords around a blank position, beginning with acontent word (or sentence head) and ending with acontent word (or sentence tail) is passed to a searchengine automatically.
For the abovementionedsample, the sequence of words passed to the engineis ?I only have to clear my head?
and so on.The basic idea is the item response function,which relates the probability of test-takers answer-ing particular items correctly to their proficiency.The item response functions are modeled as logis-tic curves making an S-shape, which take the form(1) for item i.Web Search))(exp(11)(iii baP ?
?+= ??
(1) We can use any search engine, though we havebeen using Google since February 2004.
At thatpoint in time, Google covered an enormous fourbillion pages.The test-taker parameter, ?, shows the profi-ciency of the test-taker, with higher values indicat-ing higher performance.
The ?correct?
hits may come from non-nativespeakers?
websites and contain invalid languageusage.
To increase reliability, we could restrictGoogle searches to Websites with URLs based inEnglish-speaking countries, although we have notdone so yet.
There is another concern: even ifsentence fragments cannot be located on the Web,it does not necessarily mean they are illegitimate.Thus, the proposed verification based on the Webis not perfect; the point, however, is that with suchlimitations, the generated questions are useful forestimating proficiency as demonstrated in a latersection.Each of the item parameters, ai and bi, controlsthe shape of the item response function.
The a pa-rameter, called discrimination, indexes howsteeply the item response function rises.
The b pa-rameter is called difficulty.
Difficult items featurelarger b values and the item response functions areshifted to the right.
These item parameters are usu-ally estimated by a maximal likelihood method.For computations including the estimation, thereare many commercial programs such as BILOG(http://www.assess.com/) available.3.2 Reducing test size by selection of effectiveitemsSetting aside the convenience provided by theoff-the-shelf search engine, another search special-ized for this application is possible, although thecurrent implementation is fast enough to automategeneration of FBQs, and the demand to acceleratethe search is not strong.
Rather, the problem oftime needed for test construction has been reducedby our proposal.It is important to estimate the proficiency of thetest-taker by using as few items as possible.
Forthis, we have proposed a method based on iteminformation.Expression (2) is the item information of item iat ?j, the proficiency of the test-taker j, which indi-cates how much measurement discrimination anitem provides.The throughput depends on the text from whicha seed sentence comes and the network trafficwhen the Web is accessed.
Empirically, one FBQis obtained in 20 seconds on average and the totalnumber of FBQs in a day adds up to over 4,000 ona single computer.The procedure is as follows.1.
Initialize I by the set of all generated FBQs.642.
According to Equation (3), we select the itemwhose contribution to test information ismaximal.3.
We eliminate the selected item from I accord-ing to Equation (4).4.
If I is empty, we obtain the ordered list of ef-fective items; otherwise, go back to step 2.
))(1)(()( 2 jijiiji PPaI ???
?=  (2)( )???????
?= ??
?j IijiiIi ?maxarg?
(3)iII ?
?=  (4)44.1ExperimentThe FBQs for the experiment were generated inFebruary of 2004.
Seed sentences were obtainedfrom ATR?s corpus (Kikui et al, 2003) of thebusiness and travel domains.
The vocabulary of thecorpus comprises about 30,000 words.
Sentencesare relatively short, with the average length being6.47 words.
For each domain 5,000 questions weregenerated automatically and each question consistsof an English sentence with one blank and fourchoices.Experiment with non-native speakersWe used the TOEIC score as the experiment?s pro-ficiency measure, and collected 100 Japanese sub-jects whose TOEIC scores were scattered from 400to less than 900.
The actual range for TOEICscores is 10 to 990.
Our subjects covered thedominant portion* of test-takers for TOEIC in Ja-pan, excluding the highest and lowest extremes.
?We had the subjects answer 320 randomly se-lected questions from the 10,000 mentioned above.The raw marks were as follows: the average?
markwas 235.2 (73.5%); the highest mark was 290(90.6%); and the lowest was 158 (49.4%)?Thissuggests that our FBQs are sensitive to test-takers?proficiency.
In Figure 4, the y-axis represents es-timated proficiency according to IRT (Section 3.1)and generated questions, while the x-axis is thereal TOEIC score of each subject.As the graph illustrates, the IRT-estimated pro-ficiency (?)
and real TOEIC scores of subjects cor-relate highly with a co-efficiency of about 80%.For comparison we refer to CASEC(http://casec.evidus.com/), an off-the-shelf testconsisting of human-made questions and IRT.
Itsco-efficiency with real TOEIC scores is reported tobe 86%.This means the proposed automatically gener-ated questions are promising for measuring Englishproficiency, achieving a nearly competitive levelwith human-made questions but with a few reser-vations: (1) whether the difference of 6% is largedepends on the standpoint of possible users; (2) asfor the number of questions to be answered, ourproposal uses 320 questions in the experiments,while TOEIC uses 200 questions and CASEC usesonly about 60 questions; (3) the proposed methoduses FBQs only whereas CASEC and TOEIC usevarious types of questions.Figure 4: IRT-Estimated Proficiency (?)
vs. RealTOEIC Score?TOEIC Score900807006005032.521.510.50-0.5-14004.2Experiment with a native speakerTo examine the quality of the generated questions,we asked a single subject?
who is a native speakerof English to answer 4,000 questions (Table 1).
* Over 70% of all test-takers are covered(http://www.toeic.or.jp/toeic/data/data02.html).
The native speaker largely agreed with our gen-eration, determining correct choices (type I).
The?
We have covered only the range of TOEIC scores from 400to 900 due to expense of the experiment.
In this restrictedexperiment, we do not claim that our proficiency estimationmethod covers the full range of TOEIC scores.?
Please note that the analysis is based on a single native-speaker, thus we need further analysis by multiple subjects.
?
The standard deviation was 29.8 (9.3%).65rate was 93.50%, better than 90.6%, the highestmark among the non-native speakers.We present the problematic cases here.z Type II is caused by the seed sentence beingincorrect for the native speaker, and a distracter isbad because it is correct.
Or like type III, it con-sists of ambiguous choices?z Type III is caused by some generated distractersbeing correct; therefore, the choices are ambiguous.
Figure 5 Correlation coefficient and Test sizeRTest Size in Items3503002502001501005000.90.80.70.60.50.40.30.20.10z Type IV is caused by the seed sentence beingincorrect and the generated distracters also beingincorrect; therefore, the question cannot be an-swered.z Type V is caused by the seed sentence beingnonsense to the native speaker; the question, there-fore, cannot be answered.Table 1 Responses of a Native speakerType Explanation Count %I Match 3,740 93.50IISingleSelection No match 55 1.38III Ambiguous Choices 70 1.75IV No Correct Choice 45 1.13VNoSelectionNonsense 90 2.25Cases with bad seed sentences (portions of II,IV, and V) require cleaning of the corpus by a na-tive speaker, and cases with bad distracters (por-tions of II and III) require refinement of theproposed generation algorithm.Since the questions produced by this methodcan be flawed in ways which make them unan-swerable even by native speakers (about 6.5% ofthe time) due to the above-mentioned reasons, it isdifficult to use this method for high-stakes testingapplications although it is useful for estimatingproficiency as explained in the previous section.4.35 Discussion5.15.2This section explains the on-demand generation ofFBQs according to individual preference, an im-mediate extension and a limitation of our proposedmethod, and finally touches on free-format Q&A.Effects of Automatic FBQ ConstructionThe method provides teachers and testers with atool that reduces time and expenditure.
Further-more, the method can deal with any text.
For ex-ample, up-to-date and interesting materials such asnews articles of the day can be a source of seedsentences (Figure 6 is a sample generated from anarticle (http://www.japantimes.co.jp/) on an earth-quake that occurred in Japan), which enables reali-zation of a personalized learning environment.Figure 6: On-demand construction ?
a samplequestion from a Web news article in The JapanTimes on ?an earthquake?N.B.
The correct answer is c) originated.Question 2 (FBQ)The second quake            10 km below the seabed some130 km east of Cape Shiono.a) put  b) came  c) originated d) openedWe have generated questions from over 100 docu-ments on various genres such as novels, speeches,academic papers and so on found in the enormouscollection of e-Books provided by Project Guten-berg (http://www.gutenberg.org/).Proficiency ?
estimated with the reducedtest and its relation to TOEIC ScoresFigure 5 shows the relationship between reductionof the test size according to the method explainedin Section 3.2 and the estimated proficiency basedon the reduced test.
The x-axis represents the sizeof the reduced test in number of items, while the y-axis represents the correlation coefficient (R) be-tween estimated proficiency and real TOEIC score.A Variation of Fill-in-the-Blank Ques-tions for Grammar CheckingIn Section 2.2, we mentioned a constraint that agood distracter should maintain the grammaticalcharacteristics of the correct choice originating in66the seed sentence.
The question checks not thegrammaticality but the semantic/pragmatic cor-rectness.We can generate another type of FBQ byslightly modifying step [b] of the procedure in Sec-tion 2.2 to retain the stem of the original word wand vary the surface form of the word w. Thismodified procedure generates a question thatchecks the grammatical ability of the test takers.Figure 7 shows a sample of this kind of questiontaken from a TOEIC-test textbook (EducationalTesting Service, 2002).Figure 7: A variation on fill-in-the-blank questions5.35.46Limitation of the Addressed FBQsThe questions dealt with in this paper concern test-ing reading ability, but these questions are not suit-able for testing listening ability because they arepresented visually and cannot be pronounced.
Totest listening ability, like in TOIEC, other types ofquestions should be used, and automated genera-tion of them is yet to be developed.Free-Format Q&ABesides measuring one?s ability to receive infor-mation in a foreign language, which has been ad-dressed so far in this paper, it is important tomeasure a person?s ability to transmit informationin a foreign language.
For that purpose, tests fortranslating, writing, or speaking in a free formathave been actively studied by many researchers(Shermis, 2003; Yasuda, 2004).Related Work*Here, we explain other studies on the generation ofmultiple-choice questions for language learning.There are a few previous studies on computer-based generation such as Mitkov (2003) and Wil-son (1997).6.16.27 ConclusionCloze TestA computer can generate questions by deletingwords or parts of words randomly or at every N-thword from text.
Test-takers are requested to restorethe word that has been deleted.
This is called a?cloze test.?
The effectiveness of a ?cloze test?
orits derivatives is a matter of controversy amongresearchers of language testing such as Brown(1993) and Alderson (1996).N.B.
The correct answer is c) care.Question 3 (FBQ)Because the equipment is very delicate, it must be han-dled with ______?a) caring b) careful  c) care  d) carefullyTests on FactsMitkov (2003) proposed a computer-aided proce-dure for generating multiple-choice questions fromtextbooks.
The differences from our proposal arethat (1) Mitkov?s method generates questions notabout language usage but about facts explicitlystated in a text?
; (2) Mitkov uses techniques suchas term extraction, parsing, transformation of trees,which are different from our proposal; and (3) Mit-kov does not use IRT while we use it.This paper proposed the automatic construction ofFill-in-the-Blank Questions (FBQs).
The proposedmethod generates FBQs using a corpus, a thesaurus,and the Web.
The generated questions and ItemResponse Theory (IRT) then estimate second-language proficiency.Experiments have shown that the proposedmethod is effective in that the estimated profi-ciency highly correlates with non-native speakers?real proficiency as represented by TOEIC scores;native-speakers can achieve higher scores thannon-native speakers.
It is possible to reduce thesize of the test by removing non-discriminativequestions with item information in IRT.?
Based on a fact stated in a textbook like, ?A prepositionalphrase at the beginning of a sentence constitutes an introduc-tory modifier,?
Mitkov generates a question such as, ?Whatdoes a prepositional phrase at the beginning of a sentenceconstitute?
i. a modifier that accompanies a noun; ii.
an asso-ciated modifier; iii.
an introductory modifier; iv.
a misplacedmodifier.?
* There are many works on item generation theory (ITG) suchas Irvine and Kyllonen (2002), although we do not go anyfurther into the area.
We focus only on multiple-choice ques-tions for language learning in this paper.67The method provides teachers, testers, and testtakers with novel merits that enable low-cost test-ing of second-language proficiency and provideslearners with up-to-date and interesting materialssuitable for individuals.Further research should be done on (1) large-scale evaluation of the proposal, (2) application todifferent languages such as Chinese and Korean,and (3) generation of different types of questions.AcknowledgementsThe authors?
heartfelt thanks go to anonymous review-ers for providing valuable suggestions and Kadokawa-Shoten for providing the thesaurus named Ruigo-Shin-Jiten.
The research reported here was supported in partby a contract with the NiCT entitled, ?A study of speechdialogue translation technology based on a large cor-pus.?
It was also supported in part by the Grants-in-Aidfor Scientific Research (KAKENHI), contract withMEXT numbered 16300048.
The study was conductedin part as a cooperative research project by KDDI andATR.ReferencesAlderson, Charles.
1996.
Do corpora have a role inlanguage assessment?
Using Corpora for LanguageResearch, eds.
Thomas, J. and Short, M., Longman:248?259.Brown, J. D. 1993.
What are the characteristics of natu-ral cloze tests?
Language Testing 10: 93?116.Crystal, David.
2003.
English as a Global Language,(Second Edition).
Cambridge University Press: 212.Educational Testing Service 2002.
TOEIC koushikigaido & mondaishu.
IIBC: 249.Embretson, Susan et al 2000.
Item Response Theory forPsychologists.
LEA: 371.Grefenstette, G. 1999.
The WWW as a resource for ex-ample-based MT tasks.
ASLIB ?Translating and theComputer?
conference.Irvine, H. S., and Kyllonen, P. C. (2002).
Item genera-tion for test development.
LEA: 412.Izumi, E., and Isahara, H. (2004).
Investigation intolanguage learners' acquisition order based on the er-ror analysis of the learner corpus.
In Proceedings ofPacific-Asia Conference on Language, Informationand Computation (PACLIC) 18 Satellite Workshopon E-Learning, Japan.
(in printing)Kikui, G., Sumita, E., Takaezawa, T. and Yamamoto, S.,?Creating Corpora for Speech-to-Speech Transla-tion,?
Special Session ?Multilingual Speech-to-Speech Translation?
of EuroSpeech, 2003.Kilgarriff, A. and Grefenstette, G. 2003.
Special Issueon the WEB as Corpus.
Computational Linguistics 29(3): 333?502.Mitkov, Ruslan and Ha, Le An.
2003.
Computer-AidedGeneration of Multiple-Choice Tests.
HLT-NAACL2003 Workshop: Building Educational ApplicationsUsing Natural Language Processing: 17?22.Ohno, S. and Hamanishi, M. 1984.
Ruigo-Shin-Jiten,Kadokawa, Tokyo (in Japanese)Shermis, M. D. and Burstein.
J. C. 2003.
AutomatedEssay Scoring.
LEA: 238.Tonoike, M., Sato, S., and Utsuro, T. 2004.
AnswerValidation by Keyword Association.
IPSJ, SIGNL,161: 53?60, (in Japanese).Turney, P.D.
2001.
Mining the Web for synonyms: PMI-IR vs. LSA on TOEFL.
ECML 2001: 491?502.Wainer, Howard et al 2000.
Conputerized AdaptiveTesting: A Primer, (Second Edition).
LEA: 335.Wilson, E. 1997.
The Automatic Generation of CALLexercises from general corpora, in eds.
Wichmann,A., Fligelstone, S., McEnery, T., Knowles, G.,Teaching and Language Corpora, Harlow: Long-man:116-130.Yasuda, K., Sugaya, F., Sumita, E., Takezawa, T., Kikui,G.
and Yamamoto, S. 2004.
Automatic Measuring ofEnglish Language Proficiency using MT EvaluationTechnology, COLING 2004 eLearning for Computa-tional Linguistics and Computational Linguistics foreLearning: 53-60.68
