Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 129?132,New York, June 2006. c?2006 Association for Computational LinguisticsParser Combination by ReparsingKenji Sagae and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213{sagae,alavie@cs.cmu.edu}AbstractWe present a novel parser combinationscheme that works by reparsing input sen-tences once they have already been parsedby several different parsers.
We apply thisidea to dependency and constituent parsing,generating results that surpass state-of-the-art accuracy levels for individual parsers.1 IntroductionOver the past decade, remarkable progress hasbeen made in data-driven parsing.
Much of thiswork has been fueled by the availability of largecorpora annotated with syntactic structures, espe-cially the Penn Treebank (Marcus et al, 1993).
Infact, years of extensive research on training andtesting parsers on the Wall Street Journal (WSJ)corpus of the Penn Treebank have resulted in theavailability of several high-accuracy parsers.We present a framework for combining the out-put of several different accurate parsers to produceresults that are superior to those of each of the in-dividual parsers.
This is done in a two stage proc-ess of reparsing.
In the first stage, m differentparsers analyze an input sentence, each producinga syntactic structure.
In the second stage, a parsingalgorithm is applied to the original sentence, takinginto account the analyses produced by each parserin the first stage.
Our approach produces resultswith accuracy above those of the best individualparsers on both dependency and constituent pars-ing of the standard WSJ test set.2 Dependency ReparsingIn dependency reparsing we focus on unlabeleddependencies, as described by Eisner (1996).
Inthis scheme, the syntactic structure for a sentencewith n words is a dependency tree representinghead-dependent relations between pairs of words.When m parsers each output a set ofdependencies (forming m dependency structures)for a given sentence containing n words,  thedependencies can be combined in a simple word-by-word voting scheme, where each parser votesfor the head of each of the n words in the sentence,and the head with most votes is assigned to eachword.
This very simple scheme guarantees that thefinal set of dependencies will have as many votesas possible, but it does not guarantee that the finalvoted set of dependencies will be a well-formeddependency tree.
In fact, the resulting graph maynot even be connected.
Zeman & ?abokrtsk?
(2005) apply this dependency voting scheme toCzech with very strong results.
However, whenthe constraint that structures must be well-formedis enforced, the accuracy of their results dropssharply.Instead, if we reparse the sentence based on theoutput of the m parsers, we can maximize thenumber of votes for a well-formed dependencystructure.
Once we have obtained the m initialdependency structures to be combined, the firststep is to build a graph where each word in thesentence is a node.
We then create weighteddirected edges between the nodes corresponding towords for which dependencies are obtained fromeach of the initial structures.1  In cases where morethan one dependency structure indicates that anedge should be created, the corresponding weightsare simply added.
As long as at least one of the minitial structures is a well-formed dependencystructure, the directed graph created this way willbe connected.1Determining the weights is discussed in section 4.1.129Once this graph is created, we reparse thesentence using a dependency parsing algorithmsuch as, for example, one of the algorithmsdescribed by McDonald et al (2005).
Finding theoptimal dependency structure given the set ofweighted dependencies is simply a matter offinding the maximum spanning tree (MST) for thedirected weighted graph, which can be done usingthe Chu-Liu/Edmonds directed MST algorithm(Chu & Liu, 1965; Edmonds, 1967).
Themaximum spanning tree maximizes the votes fordependencies given the constraint that the resultingstructure must be a tree.
If projectivity (nocrossing branches) is desired, Eisner?s (1996)dynamic programming algorithm  (similar to CYK)for dependency parsing can be used instead.3 Constituent ReparsingIn constituent reparsing we deal with labeled con-stituent trees, or phrase structure trees, such asthose in the Penn Treebank (after removing traces,empty nodes and function tags).
The general ideais the same as with dependencies.
First, m parserseach produce one parse tree for an input sentence.We then use these m initial parse trees to guide theapplication of a parse algorithm to the input.Instead of building a graph out of words (nodes)and dependencies (edges), in constituent reparsingwe use the m initial trees to build a weighted parsechart.
We start by decomposing each tree into itsconstituents, with each constituent being a 4-tuple[label, begin, end, weight], where label is thephrase structure type, such as NP or VP, begin isthe index of the word where the constituent starts,end is the index of the word where the constituentends plus one, and weight is the weight of the con-stituent.
As with dependencies, in the simplestcase the weight of each constituent is simply 1.0,but different weighting schemes can be used.Once the initial trees have been broken down intoconstituents, we put all the constituents from all ofthe m trees into a single list.
We then look for eachpair of constituents A and B where the label, begin,and end are identical, and merge A and B into asingle constituent with the same label, begin, andend, and with weight equal to the weight of A plusthe weight of B.
Once no more constituent mergersare possible, the resulting constituents are placedon a standard parse chart, but where the constitu-ents in the chart do not contain back-pointers indi-cating what smaller constituents they contain.Building the final tree amounts to determiningthese back-pointers.
This can be done by running abottom-up chart parsing algorithm (Allen, 1995)for a weighted grammar, but instead of using agrammar to determine what constituents can bebuilt and what their weights are, we simply con-strain the building of constituents to what is al-ready in the chart (adding the weights of constitu-ents when they are combined).
This way, we per-form an exhaustive search for the tree that repre-sents the heaviest combination of constituents thatspans the entire sentence as a well-formed tree.A problem with simply considering all constitu-ents and picking the heaviest tree is that this favorsrecall over precision.
Balancing precision and re-call is accomplished by discarding every constitu-ent with weight below a threshold t before thesearch for the final parse tree starts.
In the simplecase where each constituent starts out with weight1.0 (before any merging), this means that a con-stituent is only considered for inclusion in the finalparse tree if it appears in at least t of the m initialparse trees.
Intuitively, this should increase preci-sion, since we expect that a constituent that ap-pears in the output of more parsers to be morelikely to be correct.
By changing the threshold twe can control the precision/recall tradeoff.Henderson and Brill (1999) proposed two parsercombination schemes, one that picks an entire treefrom one of the parsers, and one that, like ours,builds a new tree from constituents from the initialtrees.
The latter scheme performed better, produc-ing remarkable results despite its simplicity.
Thecombination is done with a simple majority vote ofwhether or not constituents should appear in thecombined tree.
In other words, if a constituent ap-pears at least (m + 1)/2 times in the output of the mparsers, the constituent is added to the final tree.This simple vote resulted in trees with f-score sig-nificantly higher than the one of the best parser inthe combination.
However, the scheme heavilyfavors precision over recall.
Their results on WSJsection 23 were 92.1 precision and 89.2 recall(90.61 f-score), well above the most accurateparser in their experiments (88.6 f-score).4 ExperimentsIn our dependency parsing experiments we usedunlabeled dependencies extracted from the Penn130Treebank using the same head-table as Yamadaand Matsumoto (2003), using sections 02-21 astraining data and section 23 as test data, following(McDonald et al, 2005; Nivre & Scholz, 2004;Yamada & Matsumoto, 2003).
Dependencies ex-tracted from section 00 were used as held-out data,and section 22 was used as additional developmentdata.
For constituent parsing, we used the sectionsplits of the Penn Treebank as described above, ashas become standard in statistical parsing research.4.1 Dependency Reparsing ExperimentsSix dependency parsers were used in our combina-tion experiments, as described below.The deterministic shift-reduce parsing algorithmof (Nivre & Scholz, 2004) was used to create twoparsers2, one that processes the input sentence fromleft-to-right (LR), and one that goes from right-to-left (RL).
Because this deterministic algorithmmakes a single pass over the input string with noback-tracking, making decisions based on the pars-er?s state and history, the order in which input to-kens are considered affects the result.
Therefore,we achieve additional parser diversity with thesame algorithm, simply by varying the direction ofparsing.
We refer to the two parsers as LR and RL.The deterministic parser of Yamada and Ma-tsumoto (2003) uses an algorithm similar to Nivreand Scholz?s, but it makes several successive left-to-right passes over the input instead of keeping astack.
To increase parser diversity, we used a ver-sion of Yamada and Matsumoto?s algorithm wherethe direction of each of the consecutive passes overthe input string alternates from left-to-right andright-to-left.
We refer to this parser as LRRL.The large-margin parser described in(McDonald et al, 2005) was used with no altera-tions.
Unlike the deterministic parsers above, thisparser uses a dynamic programming algorithm(Eisner, 1996) to determine the best tree, so thereis no difference between presenting the input fromleft-to-right or right-to-left.Three different weight configurations were con-sidered: (1) giving all dependencies the sameweight; (2) giving dependencies different weights,depending only on which parser generated the de-pendency; and (3) giving dependencies different2Nivre and Scholz use memory based learning in theirexperiments.
Our implementation of their parser usessupport vector machines, with improved results.weights, depending on which parser generated thedependency, and the part-of-speech of the depend-ent word.
Option 2 takes into consideration thatparsers may have different levels of accuracy, anddependencies proposed by more accurate parsersshould be counted more heavily.
Option 3 goes astep further, attempting to capitalize on the specificstrengths of the different parsers.The weights in option 2 are determined by com-puting the accuracy of each parser on the held-outset (WSJ section 00).
The weights are simply thecorresponding parser?s accuracy (number of cor-rect dependencies divided by the total number ofdependencies).
The weights in option 3 are deter-mined in a similar manner, but different accuracyfigures are computed for each part-of-speech.Table 1 shows the dependency accuracy androot accuracy (number of times the root of the de-pendency tree was identified correctly divided bythe number of sentences) for each of the parsers,and for each of the different weight settings in thereparsing experiments (numbered according totheir descriptions above).System Accuracy Root Acc.LR 91.0 92.6RL 90.1 86.3LRRL 89.6 89.1McDonald 90.9 94.2Reparse dep 1 91.8 96.0Reparse dep 2 92.1 95.9Reparse dep 3 92.7 96.6Table 1: Dependency accuracy and root accuracy ofindividual dependency parsers and their combinationunder three different weighted reparsing settings.4.2 Constituent Reparsing ExperimentsThe parsers that were used in the constituentreparsing experiments are: (1) Charniak and John-son?s (2005) reranking parser; (2) Henderson?s(2004) synchronous neural network parser; (3)Bikel?s (2002) implementation of the Collins(1999) model 2 parser; and (4) two versions of Sa-gae and Lavie?s (2005) shift-reduce parser, oneusing a maximum entropy classifier, and one usingsupport vector machines.Henderson and Brill?s voting scheme mentionedin section 3 can be emulated by our reparsing ap-proach by setting all weights to 1.0 and t to (m +1)/2, but better results can be obtained by settingappropriate weights and adjusting the preci-sion/recall tradeoff.
Weights for different types of131constituents from each parser can be set in a simi-lar way to configuration 3 in the dependency ex-periments.
However, instead of measuring accu-racy for each part-of-speech tag of dependents, wemeasure precision for each non-terminal label.The parameter t is set using held-out data (fromWSJ section 22) and a simple hill-climbing proce-dure.
First we set t to (m + 1)/2 (which heavilyfavors precision).
We then repeatedly evaluate thecombination of parsers, each time decreasing thevalue of t (by 0.01, say).
We record the values of tfor which precision and recall were closest, and forwhich f-score was highest.Table 2 shows the accuracy of each individualparser and for three reparsing settings.
Setting 1 isthe emulation of Henderson and Brill?s voting.
Insetting 2, t is set for balancing precision and recall.In setting 3, t is set for highest f-score.System Precision Recall F-scoreCharniak/Johnson 91.3 90.6 91.0Henderson 90.2 89.1 89.6Bikel (Collins) 88.3 88.1 88.2Sagae/Lavie (a) 86.9 86.6 86.7Sagae/Lavie (b) 88.0 87.8 87.9Reparse 1 95.1 88.5 91.6Reparse 2 91.8 91.9 91.8Reparse 3 93.2 91.0 92.1Table 2: Precision, recall and f-score of each constituentparser and their combination under three differentreparsing settings.5 DiscussionWe have presented a reparsing scheme that pro-duces results with accuracy higher than the bestindividual parsers available by combining theirresults.
We have shown that in the case of de-pendencies, the reparsing approach successfullyaddresses the issue of constructing high-accuracywell-formed structures from the output of severalparsers.
In constituent reparsing, held-out data canbe used for setting a parameter that allows for bal-ancing precision and recall, or increasing f-score.By combining several parsers with f-scores rangingfrom 91.0% to 86.7%, we obtain reparsed resultswith a 92.1% f-score.ReferencesAllen, J.
(1995).
Natural Language Understanding (2nded.).
Redwood City, CA: The Benjamin/CummingsPublishing Company, Inc.Bikel, D. (2002).
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedingsof HLT2002.
San Diego, CA.Charniak, E., & Johnson, M. (2005).
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
InProceedings of the 43rd meeting of the Associationfor Computational Linguistics.
Ann Arbor, MI.Chu, Y. J., & Liu, T. H. (1965).
On the shortest arbores-cence of a directed graph.
Science Sinica(14), 1396-1400.Edmonds, J.
(1967).
Optimum branchings.
Journal ofResearch of the National Bureau of Standards(71B),233-240.Eisner, J.
(1996).
Three new probabilistic models fordependency parsing: An exploration.
In Proceedingsof the International Conference on ComputationalLinguistics (COLING'96).
Copenhagen, Denmark.Henderson, J.
(2004).
Discriminative training of a neu-ral network statistical parser.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics.
Barcelona, Spain.Henderson, J., & Brill, E. (1999).
Exploiting diversity innatural language processing: combining parsers.
InProceedings of the Fourth Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Marcus, M. P., Santorini, B., & Marcinkiewics, M.
A.(1993).
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics, 19.McDonald, R., Pereira, F., Ribarov, K., & Hajic, J.(2005).
Non-Projective Dependency Parsing usingSpanning Tree Algorithms.
In Proceedings of theConference on Human Language Technolo-gies/Empirical Methods in Natural Language Proc-essing (HLT-EMNLP).
Vancouver, Canada.Nivre, J., & Scholz, M. (2004).
Deterministic depend-ency parsing of English text.
In Proceedings of the20th International Conference on Computational Lin-guistics (pp.
64-70).
Geneva, Switzerland.Sagae, K., & Lavie, A.
(2005).
A classifier-based parserwith linear run-time complexity.
In Proceedings ofthe Ninth International Workshop on Parsing Tech-nologies.
Vancouver, Canada.Yamada, H., & Matsumoto, Y.
(2003).
Statistical de-pendency analysis using support vector machines.
InProceedings of the Eighth International Workshop onParsing Technologies.
Nancy, France.Zeman, D., & ?abokrtsk?, Z.
(2005).
Improving ParsingAccuracy by Combining Diverse Dependency Pars-ers.
In Proceedings of the International Workshop onParsing Technologies.
Vancouver, Canada.132
