Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 965?972,Honolulu, October 2008. c?2008 Association for Computational LinguisticsRelative Rank Statistics for Dialog AnalysisJuan M. HuertaIBM T.J. Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598huerta@us.ibm.comAbstractWe introduce the relative rank differential sta-tistic which is a non-parametric approach todocument and dialog analysis based on wordfrequency rank-statistics.
We also present asimple method to establish semantic saliency indialog, documents, and dialog segments usingthese word frequency rank statistics.
Applica-tions of our technique include the dynamictracking of topic and semantic evolution in adialog, topic detection, automatic generation ofdocument tags, and new story or event detec-tion in conversational speech and text.
Our ap-proach benefits from the robustness, simplicityand efficiency of non-parametric and rankbased approaches and consistently outper-formed term-frequency and TF-IDF cosine dis-tance approaches in several experiments con-ducted.1 BackgroundExisting research in dialog analysis has focused onseveral specific problems including dialog act de-tection (e.g., Byron and Heeman 1998), segmenta-tion and chunking (e.g., Hearst 1993), topic detec-tion (e.g., Zimmerman et al2005), distillation andsummarization (e.g., Mishne et al2005) etc.
Thebreath of this research reflects the increasing im-portance that dialog analysis has for multiple do-mains and applications.
While historically, dialoganalysis research has initially leveraged the corre-sponding techniques originally intended for textualdocument analysis, techniques tailored specificallyfor dialog processing eventually should be able toaddress the sparseness, noise, and time considera-tions intrinsic to dialog and conversations.The approach proposed in this paper focuses on therelative change of rank ordering of words occur-ring in a conversation according to their frequen-cies.
Our approach emphasizes relatively improb-able terms by focusing on terms that are relativelyunlikely to appear frequently and thus weightingtheir change in rank more once they are observed.Our technique achieves this in a non-parametricfashion without explicitly computing probabilities,without the assumption of an underlying distribu-tion, and without the computation of likelihoods.In general, non-parametric approaches to dataanalysis are well known and present several attrac-tive characteristics (as a general reference see Hol-lander and Wolfe 1999).
Non-parametric ap-proaches require few assumptions about the dataanalyzed and can present computational advan-tages over parametric approaches especially whenthe underlying distributions of the data are notnormal.
In specific, our approach uses rank orderstatistics of word-feature frequencies to compute arelative rank-differential statistic.This paper is organized as follows: in Section 2 weintroduce and describe our basic approach (therelative rank differential RRD function and itssorted list).
In Section 3 we address the temporalnature of dialogs and describe considerations todynamically update the RRD statistics in an on-line fashion especially for the case of shifting tem-poral windows of analysis.
In Section 4 we relatethe RRD approach to relevant existing and previ-ous dialog and text analysis approaches.
In Section5 we illustrate the usefulness of our metric by ana-lyzing a set of conversations in various ways usingthe RRD.
Specifically, in that section we will em-pirically demonstrate its robustness to noise anddata sparseness compared to the popular term fre-quency and TF-IDF cosine distance approaches in965a dialog classification task.
And finally, in Section6, we present some concluding remarks and futuredirections2 The Relative Rank DifferentialLet ...},,{ 321 uuuu dddd =  denote the rankeddictionary of a language (i.e., the ordered list ofwords sorted in decreasing order of frequency).The superscript u denotes that this ranked list isbased on the universal language.
Specifically, theword uid  is thethi  entry in ud  if its frequency ofoccurrence in the language denoted by )( uidf  islarger than )( ujdf  for every j  where ji <  (fornotational simplicity we assume that no two wordsshare the same frequency).
In the case where wantto relax this assumption we simply allow ji <when )()( ujui dfdf =  as long as uid  precedesujd  lexicographically, or under any other desiredprecedence criteria.
For ud  we assume that0)( >uidf  for every entry (i.e., each word hasbeen observed at least once in the language).Similarly, let now ...},,{ 321 SSSS dddd =  de-note the corresponding ranked dictionary for a dia-log, or dialog segment,  S    (ordered, as in thecase of the language dictionary, in decreasing orderof frequency)1.
The superscript S denotes that thisranked list is based on the dialog S. The word Sidis the thi  entry in sd  if its frequency of occurrencein the conversation segment S denoted by )( Sidfis larger than )( Sjdf  for every j  where ji < .In this case we allow  0)( ?Sidf  for every i  sothat the cardinality of ud  is the same as  sd .Let )(wrd  denote the rank of word w  in theranked dictionary d  so that, for example,idr uiud =)( .1We only consider at this point the case in which both speak-ers?
parts in a dialog interaction are considered jointly (i.e.,single channel), however, our method can be easily extendedto separate conversation channels.
Also, for simplicity weconsider at this point only words (or phrases) as features.Based now on a dialog segment and a universallanguage, any given word w   will be associatedwith a rank in ud  (the universal ranked dictionary)and a rank in sd , the dialog segment ranked dic-tionary.Let us define now for every word the relativerank differential (RRD) function or statistic2 givenby:( )?
)()()()(,wrwrwrwcusuusddddd?=The relative rank-differential is the ratio of theabsolute difference (or change) in rank between theword?s original position in the universal dictionaryand the segment s. The exponent ?
in the de-nominator allows us to emphasize or deemphasizechanges in terms according to their position or rankin the language (or universal) dictionary.
Typicallywe will want to increase the denominator?s value(i.e., deemphasize) for terms that have very lowfrequency (and their rank value in the universaldictionary is large) so that only relatively bigchanges in rank will result in substantial values ofthis function.When alpha is zero, the RRD focuses on everyword identically as we consider only the absolutechange in rank.
For alpha equal to 1.0 the relativechange in rank gets scaled down linearly accordingto its rank, while for alphas larger than 1.0 the nu-merator will scale down or reduce to a larger extentthe value of relative rank differential for words thathave large rank value.Based on each word?s relative rank differentialwe can compute the ranked list of words sorted indecreasing order by their corresponding value ofrelative rank differential.
Let this sorted list ofwords be denoted by  ,...},{),( 21 wwddR Su = .
Sothat  )( iwc  is larger  than )( jwc 3  for every jwhere ji < .We now provide some intuition on the rankedRRD lists and the RRD function.
The rankeddictionary of a language contains information2For brevity, we refer to the Relative Rank Differential of aword given two utterances as a statistic.
It is not, strictlyspeaking, a metric or a distance, but rather a function.3For simplicity, c is written without subscripts when these areapparent from the context.966about the frequency of all words in a language (i.e.,across the universe of conversations) while thesegment counterpart pertains a single conversationor segment thereof.
The relative rank differentialtells us how different a word is ranked in aconversation segment from the universal language,but this difference is normalized by the universalrank of the word.
Intuitively, and especially whenalpha equals 1.0, the RRD denotes some sort ofpercent change in rank.
This also means that thisfunction is less sensitive to small changes infrequency in the case of frequent words and tosmall changes in rank in case of infrequent words.Finally, the sorted list ),( Su ddR  contains in orderof importance the most relatively salient terms of adialog segment, as measured by relative changes ordifferences in rank.3 Collecting Rank StatisticsWe now discuss how to extend the metrics de-scribed in the previous section to consider finite-time sliding windows of analysis, that is, we de-scribe how to update rank statistics, specifically theranked lists and relative rank differential informa-tion for every feature in an on-line fashion.
This isuseful when tracking the evolution of single dia-logs, when focusing the analysis to span shorterregions, as well as to supporting dynamic real-timeanalytics of large number of dialogs.To approach this, we decompose the wordevents (words as they occur in time) into arrivingand departing events.
An arriving event at time t isa word that is covered by the analysis window atits specific time as the finite length window slidesin time, and a departing word at time t is a featurethat stops falling within the window of analysis.For simplicity, and without loss of generality, wenow assume that we are performing the analysis inreal time and that the sliding window of analysisspans from current time t back to (t-T), where T isthe length of the window of analysis.An arriving word at current time t falls into ourcurrent window of analysis and thus needs to beprocessed.
To account for these events efficiently,we need a new structure: the temporal event FIFOlist (i.e., a queue where events get registered) thatkeeps track of events as they arrive in time.
As anevent (word tw ) arrives it is registered and proc-essed as follows:1.
Find the corresponding identifier of tw  inthe universal ranked dictionary and add itasuid  at the end of the temporal event listtogether with its time stamp.2.
The corresponding entry in sd , the rankedsegment dictionary, is located through anindex list that maps skui dd ?
and thesegment frequency associated is incre-mented 1)()( += sksk dfdf3.
Verify if the rank of the feature needs to beupdated in the segment rank list.
In otherwords evaluate whether )()( 1 sksk dfdf >?still holds true after the update.
If this isnot true then shift feature up in the rank list(to a higher rank) and shift down thepredecessor feature in the rank list.
In thissingle shift-up-down operation, update theindex list and the value of k.4.
For every feature shifted down in 3 downre-compute the relative rank differentialRRD function and verify if its positionneeds to be modified in ),( Su ddR  (a sec-ond index list is needed to compute this ef-ficiently).5.
Repeat step 3 iteratively until feature is notable to push up any further in the rankedlist.The process for dealing with departing events isquite similar to the arriving process just described.Of course, as the analysis window slides in time, itis necessary to keep track of the temporal eventFIFO list to make sure that the events at the top areremoved as soon as they fall out of the analysiswindow.
The process is then:1.
The departing event is identified and itscorresponding identifier in the universalranked dictionary uid  is removed from thetop of the temporal event list.2.
Its location in sd the ranked segment dic-tionary is located through the index list.The corresponding segment frequency as-sociated is decreased as follows:1)()( ?= sksk dfdf .3.
Verify if the rank of the feature needs to beupdated in the segment rank list.
In other967words evaluate if )()( 1 sksk dfdf <+  stillholds true after the update.
If not shift fea-ture down in rank (to a lower rank, denot-ing less frequent occurrence) and shift thesuccessor feature up in the rank list.
In thissingle shift up-down operation, update theindex list and the value of k.4.
For every feature shifted up in step 3 re-compute the relative rank differential andverify if its location needs to be modifiedin ),( Su ddR5.
Repeat step 3 until the feature is not able toshift down any further in the ranked list.The procedures just described are efficientlyimplementable as they simply identify entries inrank lists through index lists, update values by in-crementing and decrementing variables, and per-formed some localized and limited re-sorting.
Ad-ditionally, simple operations like adding data at theend and removing data at the beginning of theFIFO list are needed making it altogether computa-tionally inexpensive.4 Related TechniquesOur work relates to several existing techniques asfollows.
Many techniques of text and dialog analy-sis utilize a word frequency vector based approach(e.g., Chu-Carroll et al1999) in which lexical fea-tures counts (term frequencies) are used to popu-late the vector.
Sometimes the term frequency isnormalized by document size and weighted by theinverse document frequency (TF-IDF).
The TF-IDF and TF metrics are the base of other ap-proaches like discriminative classification (Kuoand Lee 2003; and Li and Huerta 2004), Text Till-ing or topic chains (Hearst 1993; Zechner 2001),and latent semantic indexing (Landauer et al1998).Ultimately, these types of approaches are the foun-dation of complex classification and document un-derstanding systems which use these features to-gether with possibly more sophisticated classifica-tion algorithms (e.g., D?Avanzo et al2007).When using TF and TF-IDF approaches, it is im-portant to notice that by normalizing the term fre-quency by the document length, TF-based ap-proaches are effectively equivalent to estimation ofa multinomial distribution.
The variance of the es-timate will be larger as the number of observationsdecreases.
Recently, approaches that explicitly es-tablish this parametric assumption and performparameter inference have been presented in (Blei etal 2003).
This work is an example of the potentialcomplexity associated when performing parameterinference.The area of adaptation of frequency parametersfor ASR, specifically the work of (Church 2000), isrelevant to our work in the sense that both ap-proaches emphasize the importance of and presenta method to update the lexical or semantic featurestatistics on-line.In the area of non-parametric processing of dialogand text, the work of (Huffaker et al2006), is veryclose to the work in this paper as it deals with non-parametric statistics of the word frequencies (rankof occurrences) and uses the Spearman?s Correla-tion Coefficient.
Our work differs from this ap-proach in two ways: first, the Relative Rank Dif-ferential tells us about the relative change in rank(while SCC focuses in the absolute change) andsecondly, from the ranked RDD list, we can iden-tify the saliency of each term (as opposed to sim-ply computing the overall similarity between twopassages).5 ExperimentsIn order to illustrate the application of the RRDstatistic, we conducted two sets of experimentsbased on conversations recorded in a large cus-tomer contact center for an American car manufac-turer.
In the first group of experiments we took acorpus of 258 hand transcribed dialogs and con-ducted classification experiments using the basicRRD statistic as feature.
We compared its per-formance against term frequency and TF-IDFbased cosine distance approaches.
The second setof experiments is based on ASR transcribed speechand for this we used a second corpus consisting ofa set of 44 conversations spanning over 3 hours ofconversational speech.In the first set of experiments we intend to illus-trate two things: first the usefulness of RRD as afeature in terms of representational accuracy andsecond, its robustness to noise and data sparsenesscompared to other popular features.
In the secondset of experiments we illustrate the versatility andpotential of our technique to be applied in dialog-oriented analysis.9685.1 RRD for Dialog matchingFor this set of experiments we used a corpus of 258hand transcribed conversations.
Each dialog wastreated like a single document.
Using the set ofdialogs we constructed different query vectors andaffected these queries using various noise condi-tions, and then we utilized these vectors to performa simple document query classification experiment.We measured the cosine distance between thenoisy query vector and the document vector ofeach document in this corpus.
A noisy query isconstructing by adding zero mean additive gaus-sian noise to the query vector with amplitude pro-portional to the value of a parameter N and withfloor value of zero to avoid negative valued fea-tures.
We allow, in these experiments, for counts tohave non-integer values; as the dialog becomeslarger, the Gaussian assumption holds true due tothe Central Limit Theorem, independently of theactual underlying distribution of the noise source.This distortion is intended to mimic the variationbetween two similar dialogs (or utterances) that areessentially similar, except for a additive zero meanrandom changes.
A good statistic should be able toshow robustness to these types of distortions.
Acorrect match is counted when the closest matchfor each query is the generating document.N=0.0 N=.05 N=0.1 N=0.2 N=0.4TF-cosine99.6 98.0 84.9 60.0 32.5TF-IDFcosine99.6 99.6 97.3 88.0 67.4RRD-dot99.6 99.6 97.6 91.8 70.9Table 1.
Query match accuracy for 3 features un-der several query noise conditions.Table 1 shows the percent correct matches for theTF, TF-IDF and Relative Rank Differential fea-tures, under various levels of query noise.
As wecan see, in clean conditions the accuracy of the 3features is quite high but as the noise conditionsincrease the accuracy of the 3 techniques decreasessubstantially.
However, the TF feature is muchmore sensitive to noise than the other two tech-niques.
We can see that our technique is better thanboth TF and TF-IDF in noisy conditions.We also conducted experiments to test the com-parative robustness or the RRD feature to querydata sparseness.
To measure this, we evaluated theaccuracy in query-document match when using arandom subset of the document as query.
Figure 1show the results of this experiment using the RRDfeature, the Term Frequency, and the TF-IDF fea-ture vectors.
We can see that with as little as 5% ofthe document size as query, the RRD achievesclose to 90% accuracy while the TF-IDF featureneeds up to 20% to achieve the same performance,and the TF counts only need close to 70%.These results empirically demonstrate that RRDstatistics are more robust to noise and to term cov-erage sparseness than TF and TF-IDF.Figure1.
Query match accuracy for 3 feature typesunder various query data sparseness conditions5.2 ASR Based experimentsFor the experiments of this section we used 44 dia-logs.
Manual transcriptions for these 44 conversa-tions were obtained in order to evaluate the speechrecognition accuracy.
While we could have usedthe manual transcripts to perform the analysis, theresults reported here are based on the recognitionoutput.
The reason for using ASR transcripts asopposed to human transcription is that we wantedto evaluate how useful our approach would be in areal ASR based solution dealing with largeamounts of noisy data at this level of ASR error.Each dialog was recorded in two separate channels(one for the agent and one for the customer) andautomatically transcribed separately using a largevocabulary two-stage automatic speech recognizersystem.
In the first stage, a speaker independentrecognition pass is performed after which the re-sulting hypothesis is used to compensate and adaptfeature and models.
Using the adapted feature andmodels the second stage recognition is performed.After recognition, the single best hypothesis with969time stamps for the agent and customer are weavedback together.The overall Word Error Rate is about 24% and var-ies significantly between the set of agents and theset of customers (the set of agents being more ac-curate).The universal dictionary we used consists exclu-sively of the words occurring in the corpus whichtotal 2046 unique words.
Call length ranged fromjust less than 1 minute to more than 20 minuteswith most of the calls lasting between 2 and 3 min-utes.
The corpus consists of close to 30k tokensand does not distinguish between agent channeland customer channel.
A universal dictionary ofranked words is built from the set of dialogs andeach dialog is treated as a segment.Dialog Tagging and Topic SaliencyIn this analysis we look at complete dialogs.
A use-ful application of the methods we describe in thiswork is to identify and separate calls that are inter-esting from non-interesting calls4, furthermore, onecould also be interested in singling out which spe-cific terms make this dialog salient.
An applicationof this approach is the automatic generation of tags(e.g., social-network style of document tagging).
Inour approach, we will identify calls whose top en-tries in their sorted relative rank differential listsare above a certain threshold and deem these callsas semantically salient.We now describe in detail how an interesting callcan be distinguished from a non-interesting callusing the relative rank differential statistic.Figure 2 below shows the ranked dictionary...},,{ 321 SSSS dddd =  (i.e., the universal rank id?sas a function of their observed ranks) and Figure 3shows the plot of the sorted relative rank differen-tial list ),( Su ddR  for when the segment corre-sponds to an interesting call (as defined above).The chosen call, specifically shows as topic AIR-BAG deployment in the context of a car accident.Specifically, Figure 2 shows the correspondingrank in the universal ranked dictionary versus therank in the dialog or segment.
We can see that the4For the purpose of this work, we simply define as an inter-esting call a call that deals with an infrequent or rare topicwhich influences the distribution of keywords and key-phrases.Examples of calls in our domain meeting this criterion arecalls dealing with accidents and airbags.right-most part of the plot is largely monotonic,meaning that most entries of lesser frequency occurin the same ranked order both in the universal andthe specific dialog (including zero times for thesegment), while a subset across the whole range ofthe universal dictionary were substantially relo-cated up in the rank (i.e., occurred more frequentlyin the dialog than in the language).
If the plot was asingle straight line each word would have the samerank both in the language and in the dialog.We argue that while the terms of interest lie in thatsubset of interest in the graph (the terms whoserank increased substantially), not all of those wordsare equally interesting or important and rather thansimply looking at absolute changes in rank we fo-cus on the relative-rank differential RRD metric.Thus, Figure 3 shows the sorted values of the rela-tive rank differential list (with 3.1=?
).
The topentries and their rank in the universal dictionary (inparentheses) are: AIRBAGS (253), AS (55),FRONT (321), DEPLOY (369),  SIDE (279), AC-CIDENT (687).
As we can see, the top entries aredistributed across a broad range of ranks in theuniversal dictionary and relate to the topic of theconversation, which from the top ranked entries areevidently the deployment of front and side airbagsduring an accident, and thus, for this call were ableto identify its semantic saliency from the corpus ofconversations.Other interesting or salient calls also showed asimilar this profile in the RRD curve.The question now is what the behavior of our ap-proach for uninteresting calls is.
We repeated theprocedure above for a call which we deemed se-mantically un-interesting (i.e., dealing with acommon topic like call transfer and other routineprocedures).
Figure 4 shows the sorted relativerank differential values and, especially when com-pared with Figure 2, we see a large monotoniccomponent on the higher ranked terms and not somarked discontinuities in the low and mid-rangepart of the curve.We computed the relative rank differential RRDmetric for each feature similarly as with the inter-esting call, and ranked the words based on thesevalues.
The distribution of the ranked values isshown in Figure 5.
The resulting words with topvalues are CLEAR (1113), INFORMATION (122)BUYING (1941), and CLEARLY (1910).
Fromthese words we cannot really tell what is the spe-cific topic of the conversation is as easily as with970the interesting call.
More importantly, we can nowcompare Figures 3 and 5 and see that the highestrelative rank differential value of the top entry inFigure 3 (larger than 10) is significantly larger thanthe largest relative rank differential value in Figure5 (just above 7) reflecting the fact that the relativerank differential metric could be a useful parameterin evaluating semantic saliency of a segment usinga static threshold.
As an interesting point, con-ceivably the highly ranked features based on RRDcould reflect language utilization idiosyncrasies.Figure 2.
Ranked dictionary entry vs UniversalRank for a salient callFigure 3.
Sorted relative rank differential valuesof ),(Su ddRfor a semantically salient call.Figure 4.
Ranked dictionary entry vs UniversalRank for a non-salient callFigure 5.
Sorted relative rank differential valuesof ),(Su ddRfor a non-interesting (semanticallynon-salient) call.6 ConclusionsIn this paper we presented a novel non parametricrank-statistics based method for the semanticanalysis of conversations and dialogs.
Our methodis implementable in segment-based or dialog-basedmodalities, as well as in batch form and in on-lineor dynamic form.
Applications of our method in-clude topic detection, event tracking, story/topicmonitoring, new-event detection, summarization,information filtering, etc.
Because our approach isbased on non-parametric statistics it has favorableintrinsic benefits, like making no assumptionsabout the underlying data, which makes it suitablefor the use of both lexical semantic features as wellas classifier-based semantic features.
Furthermore,our approach could, in the future, benefit from971classical non-parametric approaches like block-treatment analysis etc.We demonstrated that our approach is as effectivein query classification as TF and TF-IDF in lownoise and no noise (i.e., distortion) conditions, andconsistently better than those techniques in noisyconditions.
We also found RRD to be more robustto query data sparseness than TF and TF-IDF.These results provide a motivation to combine ourstatistic with other techniques like topic chains,textilling, latent semantic indexing, and discrimi-nant classification approaches; specifically RRDcould replace TF and TF-IDF based features.Future work could focus on applying ranking sta-tistics to techniques for mining and tracking tem-poral and time-changing parameters in conjunctionwith techniques like (Agrawal and Srikant 1995;Pratt 2001;  Last et al2001).Another area of possible future work is the detec-tion and separation of multiple underlying trends indialogs.
Our approach is also suited for the analy-sis of large streams of real time conversations, andthis is a very important area of focus as presentlymore and more conversational data gets  generatedthrough channels like chat, mobile telephony, VoIPetc.ReferencesAgrawal R. and Srikant  R. 1995.
Mining SequentialPatterns.
In Proc.
of the 11th Int'l Conference onData Engineering, Taipei, Taiwan.Berger, A. L., Pietra, V. J., and Pietra, S. A.
1996.
Amaximum entropy approach to natural languageprocessing.
Comp.
Linguist.
22, 1Blei D., Ng A., and Jordan M. 2003.
Latent Dirichletallocation.
J. of Machine Learning ResearchByron, D. K. and Heeman, P. A.
1998.
DiscourseMarker Use in Task-Oriented Spoken Dialog.TR664, University of Rochester.Chu-Carroll, J, and Carpenter R. 1999.
Vector-BasedNatural Language Call Routing.
Journal of Computa-tional Linguistics, 25(30), pp.
361-388Church, K. 2000.
Empirical estimates of adaptation:The chance of two Noriega 's is closer to p/2 than p2.In ColingD'Avanzo E., Elia A., Kuflik T., Vietri S. 2007.
Univer-sity of Salerno, LAKE System at DUC 2007, Proc.Document Understanding ConferenceHearst, M. 1993.
TextTiling: A Quantitative Approachto Discourse Segmentation, Technical ReportUCB:S2K-93-24, Berkeley, CAHollander & Wolfe 1999.
Nonparametric StatisticalMethods, Second Edition, John Wiley and SonsHuffaker, D., Jorgensen, J., Iacobelli, F., Tepper, P., &Cassell, J.
2006.
Computational Measures for Lan-guage Similarity across Time in Online Communities.Workshop on ACTS at HLT-NAACL, New YorkCity, NY.Klinkenberg R. and Renz I.
1998.
Adaptive informationfiltering: Learning in the presence of concept drifts.In Learning for Text Categorization, Menlo ParkKuo H.-K.J.
and Lee C. H. Discriminative training ofnatural language call routers, IEEE Transactions onSpeech and Audio Processing, Volume 11, Issue 1,Jan 2003 Page(s): 24 - 35.Landauer T., Foltz P. W., and  Laham D. Introduction toLatent Semantic Analysis Discourse Processes 25,1998.Last M., Klein Y., and  Kandel A., Knowledge Discov-ery in Time Series Databases IEEE Trans.
on Sys-tems, Man, and Cybernetics 31B(2001).Li X. and  Huerta J.M., Discriminative Training ofCompound word based Multinomial Classifiers forSpeech Routing Proc.
ICSLP 2004Mishne, G., Carmel, D., Hoory, R., Roytman, A., andSoffer, A.
2005.
Automatic analysis of call-centerconversations.
In Proc.
of the 14th ACM interna-tional Conference on information and Knowledge.Pratt K. B.
Locating patterns in discrete time series.Master's thesis, Computer Science and Engineering,University of South Florida, 2001.Stanley K. O.
Learning concept drift with a committeeof decision trees.
Comp.
Science Dept., University ofTexas-Austin.
TR  AI-03-302, 2003.Zechner K. Automatic Summarization of Spoken Dia-logues in Unrestricted Domains.
PhD thesis, LTI,CMU, 2001Zimmermann M.,  Liu Y., E. Shriberg, and A. Stolcke,Toward Joint Segmentation and Classification ofDialog Acts in Multiparty Meetings MLMI work-shop, 2005972
