Context-Sensitive Convolution Tree Kernelfor Pronoun ResolutionZHOU GuoDong    KONG Fang    ZHU QiaomingJiangSu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and TechnologySoochow Univ.
Suzhou, China 215006Email: {gdzhou, kongfang, qmzhu}@suda.edu.cnAbstractThis paper proposes a context-sensitive convo-lution tree kernel for pronoun resolution.
It re-solves two critical problems in previousresearches in two ways.
First, given a parsetree and a pair of an anaphor and an antecedentcandidate, it implements a dynamic-expansionscheme to automatically determine a propertree span for pronoun resolution by takingpredicate- and antecedent competitor-relatedinformation into consideration.
Second, it ap-plies a context-sensitive convolution tree ker-nel, which enumerates both context-free andcontext-sensitive sub-trees by considering theirancestor node paths as their contexts.
Evalua-tion on the ACE 2003 corpus shows that ourdynamic-expansion tree span scheme can wellcover necessary structured information in theparse tree for pronoun resolution and the con-text-sensitive tree kernel much outperformsprevious tree kernels.1 IntroductionIt is well known that syntactic structured informa-tion plays a critical role in many critical NLP ap-plications, such as parsing, semantic role labeling,semantic relation extraction and co-reference reso-lution.
However, it is still an open question onwhat kinds of syntactic structured information areeffective and how to well incorporate such struc-tured information in these applications.Much research work has been done in this direc-tion.
Prior researches apply feature-based methodsto select and define a set of flat features, which canbe mined from the parse trees, to represent particu-lar structured information in the parse tree, such asthe grammatical role (e.g.
subject or object), ac-cording to the particular application.
Indeed, suchfeature-based methods have been widely applied inparsing (Collins 1999; Charniak 2001), semanticrole labeling (Pradhan et al2005), semantic rela-tion extraction (Zhou et al2005) and co-referenceresolution  (Lapin and Leass 1994; Aone and Ben-nett 1995; Mitkov 1998; Yang et al2004; Luo andZitouni 2005; Bergsma and Lin 2006).
The majorproblem with feature-based methods on exploringstructured information is that they may fail to wellcapture complex structured information, which iscritical for further performance improvement.The current trend is to explore kernel-basedmethods (Haussler, 1999) which can implicitlyexplore features in a high dimensional space byemploying a kernel to calculate the similarity be-tween two objects directly.
In particular, the ker-nel-based methods could be very effective atreducing the burden of feature engineering forstructured objects in NLP, e.g.
the parse tree struc-ture in coreference resolution.
During recent years,various tree kernels, such as the convolution treekernel (Collins and Duffy 2001), the shallow parsetree kernel (Zelenko et al2003) and the depend-ency tree kernel (Culota and Sorensen 2004), havebeen proposed in the literature.
Among previoustree kernels, the convolution tree kernel representsthe state-of-the-art and have been successfully ap-plied by Collins and Duffy (2002) on parsing, Mo-schitti (2004) on semantic role labeling, Zhang etal (2006) on semantic relation extraction  and Yanget al(2006) on pronoun resolution.However, there exist two problems in Collinsand Duffy?s kernel.
The first is that the sub-treesenumerated in the tree kernel are context-free.
Thatis, each sub-tree enumerated in the tree kernel doesnot consider the context information outside thesub-tree.
The second is how to decide a proper treespan in the tree kernel computation according tothe particular application.
To resolve above twoproblems, this paper proposes a new tree spanscheme and applies a new tree kernel and to bettercapture syntactic structured information in pronoun25resolution, whose task is to find the correspondingantecedent for a given pronominal anaphor in text.The rest of this paper is organized as follows.
InSection 2, we review related work on exploringsyntactic structured information in pronoun resolu-tion and their comparison with our method.
Section3 first presents a dynamic-expansion tree spanscheme by automatically expanding the shortestpath to include necessary structured information,such as predicate- and antecedent competitor-related information.
Then it presents a context-sensitive convolution tree kernel, which not onlyenumerates context-free sub-trees but also context-sensitive sub-trees by considering their ancestornode paths as their contexts.
Section 4 shows theexperimental results.
Finally, we conclude ourwork in Section 5.2 Related WorkRelated work on exploring syntactic structuredinformation in pronoun resolution can be typicallyclassified into three categories: parse tree-basedsearch algorithms (Hobbs 1978), feature-based(Lappin and Leass 1994; Bergsma and Lin 2006)and tree kernel-based methods (Yang et al2006).As a representative for parse tree-based searchalgorithms, Hobbs (1978) found the antecedent fora given pronoun by searching the parse trees ofcurrent text.
It processes one sentence at a timefrom current sentence to the first sentence in textuntil an antecedent is found.
For each sentence, itsearches the corresponding parse tree in a left-to-right breadth-first way.
The first antecedent candi-date, which satisfies hard constraints (such as gen-der and number agreement), would be returned asthe antecedent.
Since the search is completely doneon the parse trees, one problem with the parse tree-based search algorithms is that the performancewould heavily rely on the accuracy of the parsetrees.
Another problem is that such algorithms arenot good enough to capture necessary structuredinformation for pronoun resolution.
There is still abig performance gap even on correct parse trees.Similar to other NLP applications, feature-based methods have been widely applied in pro-noun resolution to explore syntactic structured in-formation from the parse trees.
Lappin and Leass(1994) derived a set of salience measures (e.g.
sub-ject, object or accusative emphasis) with manuallyassigned weights from the syntactic structure out-put by McCord?s Slot Grammar parser.
The candi-date with the highest salience score would beselected as the antecedent.
Bergsma and Lin (2006)presented an approach to pronoun resolution basedon syntactic paths.
Through a simple bootstrappingprocedure, highly co-reference paths can belearned reliably to handle previously challenginginstances and robustly address traditional syntacticco-reference constraints.
Although feature-basedmethods dominate on exploring syntactic struc-tured information in the literature of pronoun reso-lution, there still exist two problems with them.One problem is that the structured features have tobe selected and defined manually, usually by lin-guistic intuition.
Another problem is that they mayfail to effectively capture complex structured parsetree information.As for tree kernel-based methods, Yang et al(2006) captured syntactic structured informationfor pronoun resolution by using the convolutiontree kernel (Collins and Duffy 2001) to measurethe common sub-trees enumerated from the parsetrees and achieved quite success on the ACE 2003corpus.
They also explored different tree spanschemes and found that the simple-expansionscheme performed best.
One problem with theirmethod is that the sub-trees enumerated in Collinsand Duffy?s kernel computation are context-free,that is, they do not consider the information out-side the sub-trees.
As a result, their ability of ex-ploring syntactic structured information is muchlimited.
Another problem is that, among the threeexplored schemes, there exists no obvious over-whelming one, which can well cover syntacticstructured information.The above discussion suggests that structuredinformation in the parse trees may not be well util-ized in the previous researches, regardless of fea-ture-based or tree kernel-based methods.
Thispaper follows tree kernel-based methods.
Com-pared with Collins and Duffy?s kernel and its ap-plication in pronoun resolution (Yang et al2006),the context-sensitive convolution tree kernel enu-merates not only context-free sub-trees but alsocontext-sensitive sub-trees by taking their ancestornode paths into consideration.
Moreover, this paperalso implements a dynamic-expansion tree spanscheme by taking predicate- and antecedent com-petitor-related information into consideration.263 Context Sensitive Convolution TreeKernel for Pronoun ResolutionIn this section, we first propose an algorithm todynamically determine a proper tree span for pro-noun resolution and then present a context-sensitive convolution tree kernel to compute simi-larity between two tree spans.
In this paper, all thetexts are parsed using the Charniak parser(Charniak 2001) based on which the tree span isdetermined.3.1 Dynamic-Expansion Tree Span SchemeNormally, parsing is done on the sentence level.
Todeal with the cases that an anaphor and an antece-dent candidate do not occur in the same sentence,we construct a pseudo parse tree for an entire textby attaching the parse trees of all its sentences toan upper ?S?
node, similar to Yang et al(2006).Given the parse tree of a text, the problem ishow to choose a proper tree span to well cover syn-tactic structured information in the tree kernelcomputation.
Generally, the more a tree span in-cludes, the more syntactic structured informationwould be provided, at the expense of more noisyinformation.
Figure 2 shows the three tree spanschemes explored in Yang et al(2006): Min-Expansion (only including the shortest path con-necting the anaphor and the antecedent candidate),Simple-Expansion (containing not only all thenodes in Min-Expansion but also the first levelchildren of these nodes) and Full-Expansion (cov-ering the sub-tree between the anaphor and thecandidate), such as the sub-trees inside the dashcircles of Figures 2(a), 2(b) and 2(c) respectively.It is found (Yang et al2006) that the simple-expansion tree span scheme performed best on theACE 2003 corpus in pronoun resolution.
This sug-gests that inclusion of more structured informationin the tree span may not help in pronoun resolution.To better capture structured information in theparse tree, this paper presents a dynamic-expansionscheme by trying to include necessary structuredinformation in a parse tree.
The intuition behindour scheme is that predicate- and antecedent com-petitor- (all the other compatible1 antecedent can-didates between the anaphor and the consideredantecedent candidate) related information plays acritical role in pronoun resolution.
Given an ana-1 With matched number, person and gender agreements.phor and an antecedent candidate, e.g.
?Mary?
and?her?
as shown in Figure 1, this is done by:1) Determining the min-expansion tree span viathe shortest path, as shown in Figure 1(a).2) Attaching all the antecedent competitors alongthe corresponding paths to the shortest path.
Asshown in Figure 1(b), ?the woman?
is attachedwhile ?the room?
is not attached since the for-mer is compatible with the anaphor and the lat-ter is not compatible with the anaphor.
In thisway, the competition between the consideredcandidate and other compatible candidates canbe included in the tree span.
In some sense, thisis a natural extension of the twin-candidatelearning approach proposed in Yang et al(2003), which explicitly models the competitionbetween two antecedent candidates.3) For each node in the tree span, attaching thepath from the node to the predicate terminalnode if it is a predicate-headed node.
As shownin Figure 1(c), ?said?
and ?bit?
are attached.4) Pruning those nodes (except POS nodes) withthe single in-arc and the single out-arc and withits syntactic phrase type same as its child node.As shown in Figure 1(d), the left child of the?SBAR?
node, the ?NP?
node, is removed andthe sub-tree (NP the/DT woman/NN) is at-tached to the ?SBAR?
node directly.To show the difference among min-, simple-,full- and dynamic-expansion schemes, Figure 2compares them for three different sentences, giventhe anaphor ?her/herself?
and the antecedent can-didate ?Mary?.
It shows that:?
Min-, simple- and full-expansion schemes havethe same tree spans (except the word nodes) forthe three sentences regardless of the differenceamong the sentences while the dynamic-expansion scheme can adapt to difference ones.?
Normally, the min-expansion scheme is toosimple to cover necessary information (e.g.
?thewoman?
in the 1st sentence is missing).?
The full-expansion scheme can cover all theinformation at the expense of much noise (e.g.
?the man in that room?
in the 2nd sentence).?
The simple-expansion scheme can cover somenecessary predicate-related information (e.g.?said?
and ?bit?
in the sentences).
However, itmay introduce some noise (e.g.
the left child of27the ?SBAR?
node, the ?NP?
node, may not benecessary in the 2nd sentence) and ignore neces-sary antecedent competitor-related information(e.g.
?the woman?
in the 1st sentence).?
The dynamic-expansion scheme normallyworks well.
It can not only cover predicate-related information but also structured informa-tion related with the competitors of the consid-ered antecedent candidate.
In this way, thecompetition between the considered antecedentcandidate and other compatible candidates canbe included in the dynamic-expansion scheme.Figure 1: Dynamic-Expansion Tree Span SchemeFigure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples283.2 Context-Sensitive Convolution Tree KernelGiven any tree span scheme, e.g.
the dynamic-expansion scheme in the last subsection, we nowstudy how to measure the similarity between twotree spans using a convolution tree kernel.A convolution kernel (Haussler D., 1999) aimsto capture structured information in terms of sub-structures.
As a specialized convolution kernel, theconvolution tree kernel, proposed in Collins andDuffy (2001), counts the number of common sub-trees (sub-structures) as the syntactic structuresimilarity between two parse trees.
This convolu-tion tree kernel has been successfully applied byYang et al(2006) in pronoun resolution.
However,there is one problem with this tree kernel: the sub-trees involved in the tree kernel computation arecontext-free (That is, they do not consider the in-formation outside the sub-trees.).
This is contrastto the tree kernel proposed in Culota and Sorensen(2004) which is context-sensitive, that is, it consid-ers the path from the tree root node to the sub-treeroot node.
In order to integrate the advantages ofboth tree kernels and resolve the problem inCollins and Duffy?s kernel, this paper applies thesame context-sensitive convolution tree kernel,proposed by Zhou et al(2007) on relation extrac-tion.
It works by taking ancestral information (i.e.the root node path) of sub-trees into consideration:?
?=?
?D=miNnNniiCiiiinnTTK1]2[]2[]1[]1[111111])2[],1[(])2[],1[(  (1)where ][1 jNi is the set of root node paths withlength i in tree T[j] while the maximal length of aroot node path is defined by m; and])2[],1[( 11ii nnD  counts the common context-sensitive sub-trees rooted at root node paths ]1[1inand ]2[1in .
In the tree kernel, a sub-tree becomescontext-sensitive via the ?root node path?
movingalong the sub-tree root.
For more details, pleaserefer to Zhou et al(2007).4 ExperimentationThis paper focuses on the third-person pronounresolution and, in all our experiments, uses theACE 2003 corpus for evaluation.
This ACE corpuscontains ~3.9k pronouns in the training data and~1.0k pronouns in the test data.Similar to Soon et al(2001), an input raw text isfirst preprocessed automatically by a pipeline ofNLP components, including sentence boundarydetection, POS tagging, named entity recognitionand phrase chunking, and then a training or testinstance is formed by a pronoun and one of its an-tecedent candidates.
During training, for each ana-phor encountered, a positive instance is created bypairing the anaphor and its closest antecedentwhile a set of negative instances is formed by pair-ing the anaphor with each of the non-coreferentialcandidates.
Based on the training instances, a bi-nary classifier is generated using a particular learn-ing algorithm.
In this paper, we use SVMLightdeleveloped by Joachims (1998).
During resolution,an anaphor is first paired in turn with each preced-ing antecedent candidate to form a test instance,which is presented to a classifier.
The classifierthen returns a confidence value indicating the like-lihood that the candidate is the antecedent.
Finally,the candidate with the highest confidence value isselected as the antecedent.
In this paper, the NPsoccurring within the current and previous two sen-tences are taken as the initial antecedent candidates,and those with mismatched number, person andgender agreements are filtered out.
On average, ananaphor has ~7 antecedent candidates.
The per-formance is evaluated using F-measure instead ofaccuracy since evaluation is done on all the pro-nouns occurring in the data.Scheme/m 1 2 3 4Min 78.5 79.8 80.8 80.8Simple 79.8 81.0 81.7 81.6Full 78.3 80.1 81.0 81.1Dynamic 80.8 82.3 83.0 82.9Table 1: Comparison of different context-sensitiveconvolution tree kernels and tree span schemes(with entity type info attached at both the anaphorand the antecedent candidate nodes by default)In this paper, the m parameter in our context-sensitive convolution tree kernel as shown inEquation (1) indicates the maximal length of rootnode paths and is optimized to 3 using 5-fold crossvalidation on the training data.
Table 1 systemati-cally evaluates the impact of different m in ourcontext-sensitive convolution tree kernel and com-pares our dynamic-expansion tree span schemewith the existing three tree span schemes, min-,29simple- and full-expansions as described in Yanget al(2006).
It also shows that that our tree kernelachieves best performance with m = 3 on the testdata, which outperforms the one with m = 1 by~2.2 in F-measure.
This suggests that the parentand grandparent nodes of a sub-tree  contain muchinformation for pronoun resolution whileconsidering more ancestral nodes doesnot furtherimprove the performance.
This may be due to that,although our experimentation on the training dataindicates that  more than 90% (on average) ofsubtrees has a root node path longer than 3 (sincemost of the subtrees are deep from the root nodeand more than 90% of the parsed trees are deeperthan 6 levels in the ACE 2003 corpus), including aroot node path longer than 3 may be vulnerable tothe full parsing errors and have negative impact.
Italso shows that our dynamic-expansion tree spanscheme outperforms min-expansion, simple-expansion and full-expansion schemes by ~2.4,~1.2 and ~2.1 in F-measure respectively.
Thissuggests the usefulness of dynamically expandingtree spans to cover necessary structuredinformation in pronoun resolution.
In all thefollowing experiments, we will apply our treekernel with m=3 and the dynamic-expansion treespan scheme by default, unless specified.We also evaluate the contributions of antecedentcompetitor-related information, predicate-relatedinformation and pruning in our dynamic-expansiontree span scheme by excluding one of them fromthe dynamic-expansion scheme.
Table 2 shows that1) antecedent competitor-related information con-tributes much to our scheme; 2) predicate-relatedinformation contributes moderately; 3) pruningonly has slight contribution.
This suggests the im-portance of including the competition in the treespan and the effect of predicate-argument struc-tures in pronoun resolution.
This also suggests thatour scheme can well make use of such predicate-and antecedent competitor-related information.Dynamic Expansion Effect- Competitors-related Info 81.1(-1.9)- Predicates-related Info 82.2 (-0.8)- Pruning 82.8(-0.2)All 83.0Table 2: Contributions of different factors in ourdynamic-expansion tree span schemeTable 3 compares the performance of differenttree span schemes for pronouns with antecedents indifferent sentences apart.
It shows that our dy-namic-expansion scheme is much more robust thanother schemes with the increase of sentences apart.Scheme /#Sentences Apart0 1 2Min 86.3 76.7 39.6Simple 86.8 77.9 43.8Full 86.6 77.4 35.4Dynamic 87.6 78.8 54.2Table 3: Comparison of tree span schemes withantecedents in different sentences apart5 ConclusionSyntactic structured information holds great poten-tial in many NLP applications.
The purpose of thispaper is to well capture syntactic structured infor-mation in pronoun resolution.
In this paper, weproposes a context-sensitive convolution tree ker-nel to resolve two critical problems in previousresearches in pronoun resolution by first automati-cally determining a dynamic-expansion tree span,which effectively covers structured information inthe parse trees by taking predicate- and antecedentcompetitor-related information into consideration,and then applying a context-sensitive convolutiontree kernel, which enumerates both context-freesub-trees and context-sensitive sub-trees.
Evalua-tion on the ACE 2003 corpus shows that our dy-namic-expansion tree span scheme can bettercapture necessary structured information than theexisting tree span schemes and our tree kernel canbetter model structured information than the state-of-the-art Collins and Duffy?s kernel.For the future work, we will focus on improvingthe context-sensitive convolution tree kernel bybetter modeling context-sensitive information andexploring new tree span schemes by better incor-porating useful structured information.
In themeanwhile, a more detailed quantitative evaluationand thorough qualitative error analysis will be per-formed to gain more insights.AcknowledgementThis research is supported by Project 60673041under the National Natural Science Foundation ofChina and Project 2006AA01Z147 under the ?863?National High-Tech Research and Development ofChina.30ReferencesAone C and Bennett W.W. (1995).
Evaluating auto-mated and manual acquisition of anaphora resolu-tion strategies.
ACL?1995:122-129.Bergsma S. and Lin D.K.(2006).
Bootstrapping path-based pronoun resolution.
COLING-ACL?2006: 33-40.Charniak E. (2001).
Immediate-head Parsing for Lan-guage Models.
ACL?2001: 129-137.
Toulouse,FranceCollins M. (1999) Head-driven statistical models fornatural language parsing.
Ph.D. Thesis.
Universityof Pennsylvania.Collins M. and Duffy N. (2001).
Convolution Ker-nels for Natural Language.
NIPS?2001: 625-632.Cambridge, MACulotta A. and Sorensen J.
(2004).
Dependency treekernels for relation extraction.
ACL?2004.
423-429.21-26 July 2004.
Barcelona, Spain.Haussler D. (1999).
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10, Uni-versity of California, Santa Cruz.Hobbs J.
(1978).
Resolving pronoun references.
Lin-gua.
44:339-352.Joachims T. (1998).
Text Categorization with Sup-port Vector Machine: learning with many relevantfeatures.
ECML-1998: 137-142.
Chemnitz, Ger-manyLappin S. and Leass H. (1994).
An algorithm for pro-nominal anaphora resolution.
Computational Lin-guistics.
20(4):526-561.Mitkov R. (1998).
Robust pronoun resolution withlimited knowledge.
COLING-ACL?1998:869-875.Montreal, Canada.Moschitti A.
(2004).
A study on convolution kernelsfor shallow semantic parsing.
ACL?2004:335-342.Pradhan S., Hacioglu K., Krugler V., Ward W., Mar-tin J.H.
and Jurafsky D. (2005).
Support VectorLearning for Semantic Argument Classification.Machine Learning.
60(1):11-39.Soon W. Ng H.T.and Lim D. (2001).
A machinelearning approach to creference resolution of nounphrases.
Computational Linguistics.
27(4): 521-544.Yang X.F., Zhou G.D., Su J. and Tan C.L., Corefer-ence Resolution Using Competition Learning Ap-proach, ACL?2003):176-183.
Sapporo, Japan, 7-12July 2003.Yang X.F., Su J. and Tan C.L.
(2006).
Kernel-basedpronoun resolution with structured syntactic knowl-edge.
COLING-ACL?2006: 41-48.Zelenko D., Aone C. and Richardella.
(2003).
Kernelmethods for relation extraction.
Journal of MachineLearning Research.
3(Feb):1083-1106.Zhang M., Zhang J., Su J. and Zhou G.D. (2006).
AComposite Kernel to Extract Relations between En-tities with both Flat and Structured Features.COLING-ACL-2006: 825-832.
Sydney, AustraliaZhou G.D., Su J. Zhang J. and Zhang M. (2005).
Ex-ploring various knowledge in relation extraction.ACL?2005.
427-434.
25-30 June, Ann Arbor, Mich-gan, USA.Zhou G.D., Zhang M., Ji D.H. and Zhu Q.M.
(2007).Tree Kernel-based Relation Extraction with Con-text-Sensitive Structured Parse Tree Information.EMNLP-CoNLL?200731
