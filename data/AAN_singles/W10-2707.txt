Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 37?42,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics?How was your day??S.
G. Pulman, J. BoyeUniversity of Oxfordsgp@clg.ox.ac.ukM.
Cavazza, C. SmithTeesside Universitym.o.cavazza@tees.ac.ukR.
S. de la Ca?maraTelefonica I+De.rsai@tid.esAbstractWe describe a ?How was your day??
(HWYD) Companion whose purpose is toestablish a comforting and supportive rela-tionship with a user via a conversation ona variety of work-related topics.
The sys-tem has several fairly novel features aimedat increasing the naturalness of the interac-tion: a rapid ?short loop?
response primedby the results of acoustic emotion anal-ysis, and an ?interruption manager?, en-abling the user to interrupt lengthy or ap-parently inappropriate system responses,prompting a replanning of behaviour onthe part of the system.
The ?long loop?also takes into account the emotional stateof the user, but using more conventionaldialogue management and planning tech-niques.
We describe the architecture andcomponents of the implemented prototypeHWYD system.1 IntroductionAs the existence of this workshop shows, there is agood deal of interest in a type of spoken languagedialogue system distinct from the traditional task-based models used for booking airline tickets andthe like.
The purpose of these ?social agent?
sys-tems is to be found in the relationship they canestablish with human users, rather than on the as-sistance the agent can provide in giving informa-tion or solving a problem.
Designing such agentsprovides many significant technical challenges, re-quiring progress in the integration of linguisticcommunication and non-verbal behaviour for af-fective dialogue (Andre?
et al 2004).
In this pa-per, we present the implementation of a Compan-ion Embodied Conversational Agent (ECA) whichintegrates emotion and sentiment detection withmore traditional dialogue components.2 From Dialogue to ConversationMost spoken language dialogue systems are ?task-based?
: they aim at getting from the user values fora fixed number of slots in some template.
Whenenough values have been found, the filled tem-plate is sent off to some back-end system so thatthe task in question - ordering a pizza, booking aticket etc.
- can be carried out.
However, a so-cial Companion agent assumes a kind of conver-sation not necessarily connected to any immediatetask, and which may not follow the conventionsassociated with task-driven dialogues, for exam-ple, the relatively strict turn-taking of task-baseddialogue.
In everyday life, many interhuman con-versations see one of the participants producinglengthy descriptions of events, without this corre-sponding to any specific request or overall con-versational purpose.
Our objective was to sup-port such free conversation, whilst still obtainingmeaningful answers from the agent, in the form ofadvice appropriate both to the affective and infor-mational content of the conversation.
In order tobalance the constraints of free conversation withthose of tractability, we have deliberately optedfor a single-domain conversation, in contrast withboth small talk (Bickmore and Cassell, 1999) and?chatterbot?
approaches.
Our HWYD domain in-volves typical events and topics of conversation inthe workplace, ranging from the relatively mun-dane - meeting colleagues, getting delayed by traf-fic, project deadlines - to rather more important -promotions, firings, arguments, office politics - de-signed to evoke stronger emotions and hence moreaffective dialogues.However, our HWYD Companions retainssome features of a typical task based system, inthat each of these subtopics can be thought of as atask or information extraction template.
Unfilledslots will drive the dialogue manager to questionthe user for possible values.
When enough slots37are filled, the initiative will be passed to an ?affec-tive strategy?
module, which will generate a longerresponse designed to empathise appropriately withthe user over that particular topic.3 System Overview and ArchitectureThe HWYD Companion integrates 15 differentsoftware components, covering at least to somedegree all the necessary aspects of multimodal af-fective input and output: including speech recog-nition (ASR, using Dragon Naturally Speaking),emotional speech recognition (AA: the EmoVoicesystem (Vogt et al 2008)), turn detection (ATT),Dialogue Act segmentation and tagging (DAT),Emotional modelling (EM), Sentiment Analy-sis (SA) (Moilanen et al 2007), Natural Lan-guage Understanding (NLU), Dialogue Manage-ment (DM), user modelling and a knowledgebase (KB/UM), an ?Affective Strategy Module?
(ASM) generating complex system replies, Natu-ral Language Generation (NLG), Speech Synthe-sis (TTS), an avatar (ECA), and Multimodal con-trol of the ECA persona (MFM): gesture and fa-cial expression, supported by the Haptek anima-tion toolkit.
Clearly the use of Naturally Speakingimposes on us speaker dependence, since the sys-tem needs training: in the scenario we have chosenthis is in fact not too unrealistic an assumption, butthis is merely a practical decision - we are not do-ing research on speech recognition as such in thisproject and so want to get as good a recognitionrate as possible.The software architecture of the prototype re-lies on the Inamode Framework developed byTelefnica I+D.
Communication between modulesfollows a blackboard-like paradigm, in which cen-tral hubs broadcast any incoming message fromany module to all of the other modules that areconnected to it.
Figure 1 below shows the systemarchitecture, and Figure 2 shows one version ofwhat is on the screen when the system is running.4 Emotional Feedback LoopsRecognising and responding appropriately to dif-ferent emotions is an important aspect of a socialagent.
In our HWYD Companion, emotion andsentiment are used in two ways: firstly, to pro-vide immediate feedback to a user utterance (giventhat there will inevitably be some delay in the re-sponse from natural language and dialogue pro-cessing modules) and secondly to inform the moreextended responses given by the system when ithas learned enough about the current sub-topic.There are two feedback loops: the ?short loop?
(response time < 700 ms) provides an immedi-ate backchannel, and its main purpose is to main-tain contact and keep the communication alive andrealistic.
This is achieved by matching the non-verbal response (gesture, facial expression) of theavatar to the emotional speech parameters detectedby EmoVoice prior to affective fusion (where theemotion detected from speech and the sentimentvalue detected from the corresponding text aremerged: see below), and occasionally includingan appropriate verbal acknowledgement, on a ran-dom basis to avoid acknowledging all user utter-ances.
The short loop essentially aligns the ECAresponse to the user?s attitude, thus showing empa-thy.
(We should also use SA for this, but currentlyprocessing speed is not fast enough).The ?major loop?
(response time < 3000 ms) in-volves the ECA?s full response to the user utter-ance in terms of both verbal and non-verbal be-haviour.
There are effectively two sources of sys-tem output: the dialogue manager engages withthe user to find out what happened during theirwork day, and will ask questions, or drop intoclarificatory sub-dialogues, gradually building upa complex event description along with an assess-ment of the prevailing emotions of the speaker.When sufficient information has been gathered,control is passed to an ?affective strategy module?which will produce a longer output, typically ad-vice or warning in response to the user?s recollec-tion of his daily events.The system also includes an interruption man-ager which detects interruption and barge-in by theuser, resulting in the immediate suspension of thecurrent system utterance, triggering the process-ing of any content specific to the interrupting ut-terance, and consequent replanning on the part ofother modules to produce an appropriate response.Such an interruption is illustrated in Figure 1.
Thedesign of such an interruption manager in a sys-tem with so many separate modules is quite chal-lenging, in fact: the system is described further inCrook et al (2010).The ECA listens sympathetically to the user?saccount of work difficulties, whilst also reacting toapparent discrepancies between perceived moodand the affective content of the recognised events.In the following example from a real conversation,38Figure 1: Overview of the System Architecture illustrating the information flow between modules.Figure 2: The HWYD Companion in operation: the central portion of the screen traces the dialogue, whilst the right-handone displays which current software module is activated.39the user announces a company restructuring, but ina rather cheerful tone.
Notice that some of the En-glish is a little strange: the generator produces amixture of genuinely generated output along withsome canned text.User: I have just learnt that my department is going to bemerged with the Barcelona one (EMOTION FUSION: POS-ITIVE ACTIVE)System: You seem a bit too ecstatic!
I?m doubtful that yourworkload could be maintained.Every time the Companion has gathered suffi-cient information it will try to respond more glob-ally to the situation at hand.
In the following ex-ample (see also Figure 2), after the user has ex-pressed different sorts of concerns, the Companionpicks on the latest (positive) information to sum-marise the few positive elements it picked from theconversation:User: yes but one good thing anyway we had a newcoffee machine arrived today (EMOTION FUSION: POSI-TIVE ACTIVE)System: it?s always reassuring to hear good news!
It?s rightto feel good about such a good situation.
I am quite fond ofgetting new stuff.
I?m happy that your mood improves.
It?sworth remembering you should take time to enjoy the goodthings when they happen.5 Emotional Modelling and FusionTwo different modules analyse the emotional con-tent of user utterances: an emotional speech recog-nition system (EmoVoice (Vogt et al 2008) whichreturns information indicating both the arousal andvalence of the acoustic properties of the user?sspeech as negative passive, negative-active, neu-tral, positive-active or positive-passive, and a text-based Sentiment Analysis module which operateson the utterance transcript after its recognition bythe ASR module.
The SA module operates ina compositional way and is able to classify lin-guistic units of any syntactic type: noun phrases,clauses, sentences etc.
It is also able to assigna ?strength?
of the sentiment expressed.
In thecurrent implementation it simply classifies clausesas either negative, neutral or positive.
These twoemotional inputs are then merged by a fusion pro-cedure, whose purpose is to provide an aggregateemotional category to be attached to the event de-scription template produced by the NLU and DMmodule.
Essentially, the mechanism for affectivefusion consists in overriding the valence categoryof EmoVoice with the one obtained by SA everytime the confidence score attached to EmoVoiceis below a preset value (depending on the com-peting valence categories).
Fusion is currently anunderdeveloped module: for example, detectingmismatches between speech and language emo-tion and sentiment values could lead to the recog-nition of irony, sarcasm etc.
(Tepperman et al2006).
Saying an intrinsically negative thing in apositive and cheerful way, or the other way round,suggests that the speaker is trying for some specialeffect.6 Natural Language Understanding andDialogue ManagementThe task of the NLU module is to recognise a spe-cific set of events reported by the user within ut-terances which can be of significant length (> 50words) and which can be difficult to parse due tospeech recognition errors.
This led us to follow anInformation Extraction (IE) approach to dialogueanalysis (see Jo?nsson et al 2004), using shallowsyntactic and semantic processing to find instan-tiations of event templates.
The NLU componentof the HWYD Companion demonstration systemtakes the 1-best output from the speech recogniser(currently: work in progress will take n-best),which has already been segmented into dialogue-act sized utterances (by the DAT module which si-multaneously segments and labels the recogniseroutput: see Figure 1).
So, for example, a sequencelike ?It was okay there are not many projects at themoment so it is very quiet would be segmentedinto three separate dialogue acts.
The utterancesare then part-of-speech tagged and chunked intoNoun Phrase (NP) and Verb Group (VG) units.VGs consist of a main verb and any auxiliary verbsor semantically important adverbs.
Both of thesestages are carried out by a Hidden Markov Modeltrained on the Penn Treebank, although some cus-tomisation has been carried out for this applica-tion: relevant vocabulary added and some proba-bilities re-estimated to reflect properties of the ap-plication.
NP and VG chunks are then classifiedinto ?Named Entity?
classes, some of which arethe usual person, organisation, time etc.
but oth-ers of which are specific to the scenario, as is tradi-tional in IE: e.g.
salient work events, expressionsof emotion, organisational structure etc.
NamedEntity classification, in the absence of domain spe-cific training data, is carried out via hand-writtenpattern matching rules and gazetteers.
Each chunk40is further annotated with features encoding thehead word, stem form, polarity, agreement fea-tures, relevant modifiers, etc.
for later syntac-tic and semantic processing.
The NPs and VGsare represented as unification grammar categoriescontaining information about the internal structureof the constituents.The next stage applies unification based syn-tax rules which combine NP and VG chunks intolarger constituents.
These rules are of two types:most are syntactically motivated and are attempt-ing to build a parse tree from which main gram-matical relations (subject, object, etc.)
can berecognised.
These have coverage of the main syn-tactic constructs of English.
But within the sameformalism we add domain specific InformationExtraction type patterns, looking out for particularconstellations of entities and events relevant to theHWYD scenario, for example ?argument at workbetween X and Y?, or ?meeting with X about Y?.Processing is non-deterministic and so sentenceswill get many analyses.
We use a ?shortest paththrough the chart heuristic to select an interpre-tation.
This is far from perfect, and we are cur-rently working on a separate more motivated dis-ambiguation module.The final stage of processing before the Dia-logue Manager takes over is to perform referenceresolution for pronouns and definite NPs.
Thismodule is based partly on the system describedby Kennedy and Boguraev 1996, with the variousweighting factors based on theirs, but designed sothat the weights can be trained given appropriatedata.
Currently we are collecting such data andthe present set of weights are taken from Kennedyand Boguraev but with additional salience givento the domain-specific named entity classes.
Eachreferring NP gives rise to a discourse referent, andthese are grouped into coreference classes basedon grammatical, semantic, and salience properties.The DMmaintains an information state contain-ing all objects mentioned during the conversation,and uses this information to decide whether theobjects referred to in the utterance are salient ornot.
The DM also uses type information to inter-pret elliptical answers to questions (System: ?Whowas at the meeting??
User: ?Nigel.?).
After theuser?s utterance has been interpreted in its dia-logue context and the information state has beenupdated, the dialogue manager decides on the ap-propriate response.
If a new object has been intro-duced by the user, the DM adds a goal to its agendato talk about that object.
For instance, if a new per-son is mentioned, the DM will ask questions aboutthe user?s relation to that person, etc.For each turn of the dialogue, the DM chooseswhich topic to pursue next by considering all thecurrently un-satisfied goals on the agenda andheuristically rating them for importance.
Theheuristics employed use factors such as recency inthe dialogue history, general importance, and emo-tional value associated with the goal.
We are cur-rently exploring the use of reinforcement learningwith a reward function based on the results of SAon the users input to choose goals in a more naturalway.
The DM also has the option of invoking theASM (described below) to generate an appropri-ate answer, in the cases where the user says some-thing highly emotive.
Again, this is a decision thatcould involve reinforcement learning, and we areexploring this in our current work.The joint operation of the NLU and the DMhence supports a kind of IE or task-specifictemplate-filling: the content of the user?s utter-ances, prompted by questions from the DM, pro-vides the information necessary to fill a templateto the point where the ASM can take over.
Thenumber of templates for domain events is signifi-cantly higher than in traditional IE or task-baseddialogue systems, however, since the HWYDCompanion currently instantiates more than 30templates, and will eventually cover around 50.7 Affective Dialogue StrategiesOnce the NLU and DM have a sufficiently in-stantiated template, which also records emotionalvalue, it is passed to the ASM.
This controls thegeneration of longer ECA narrative replies whichaim at influencing the user by providing advice orreassurance.
Our overall framework for influenceis inspired by the work of Bremond 1973.
Thenarrative is constituted by a set of argumentativestatements which can be based on emotional op-erators (e.g.
show-empathy) or specific commu-nicative operators.
The ASM is based on a Hier-archical Task Network (HTN) planner (Nau et al2004), which works through recursive decompo-sition of a high level task into sub-tasks until wereach a plan of sub-tasks that can be directly ex-ecuted.
The operators constituting the plan gen-erated by the HTN implement Bremond?s the-ory of influence by emphasising the determinants41of the event reported by the user.
For instance,various operators can emphasise or play downthe event consequences (emphasise-outcome-importance, emphasise-outcome-justification,emphasise-outcome-warning) or comment onadditional factors that may affect the courseof events (commend-enabler, reassure-helper).The planner uses a set of 25 operators, each ofwhich can be in addition instantiated to incorpo-rate specific elements of the event.
Overall thissupports the generation of hundreds of signifi-cantly different influencing strategies.8 Results and ConclusionsWe have described an initial, fully-implementedprototype of a Companion ECA supporting freeconversation, including affective aspects, over avariety of everyday work-related topics.
The sys-tem has been demonstrated extensively outside ofits development group and was regularly able tosustain consistent dialogues with an average du-ration exceeding 20 minutes.
The CompanionECA recently won the best demonstration prizeat AAMAS 2010,the 9th International Conferenceon Autonomous Agents and Multiagent Systems,Toronto, which is some subjective indication atleast that its behaviour is of some interest outsideof the project which developed it.However, we have not yet systematically evalu-ated the ECA, although this task has begun (Webbet al 2010).
The question of evaluation for sys-tems like this is in fact a rather difficult one, sinceunlike task-based systems there is no simple mea-sure of success.
In our current work we aim toconduct extensive trials with real users and viainterview and questionnaires to get some usefulmeasure of how natural and ?companionable?
thesystem is perceived to be.In other current work we are, as mentionedabove, experimenting with reinforcement learningwhere the reward function is based on the emo-tion and sentiment detected in the user?s input.
Weare collecting data via Amazon?s Mechanical Turkand hope to be able to show how the ECA can de-velop different ?personalities?
depending on howthis reward function is defined.
For example, wecould imagine using simulated dialogues to pro-duce a Companion that was relentlessly cheerful,producing positive outputs whatever the input.
Al-ternatively, we could produce a ?mirror?
Compan-ion which simply reflected the mood of the user.We could even produce a ?misery loves company?Companion which, instead of trying to cheer theuser up when recognising negative sentiment oremotion, could reply in an equally negative man-ner.AcknowledgementsThis work was funded by the Companions project(http://www.companions-project.org) sponsored by the Euro-pean Commission as part of the Information Society Tech-nologies (IST) programme under EC grant number IST-FP6-034434.
The EmoVoice system has been used courtesy ofthe Multimedia Concepts and Applications Group of the Uni-versity of Augsburg.
Other contributors to the prototype de-scribed in this paper are Karo Moilanen, and from the COM-PANIONS consortium: David Benyon, Jay Bradley, DanielCharlton, WeiWei Cheng, Morena Danieli, Simon Dobnik,Carlos Sanchez Fernandez, Debora Field, Mari Carmen Ro-driguez Gancedo, Jose Relano Gil, Ramon Granell, JaakkoHakulinen, Preben Hansen, Sue Harding, Topi Hurtig, OliMival, Roger Moore, Olov Stahl, Markku Turunen, EnricoZovato.ReferencesAndre?, E., Dybkjr, L., Minker, W., and Heisterkamp, P.(Eds.
), 2004, Affective Dialogue Systems Lecture Notes inComputer Science 3068, Springer.Bickmore, T., and Cassell, J., 1999.
Small Talk and Con-versational Storytelling in Embodied Interface Agents.
Pro-ceedings of the AAAI Fall Symposium on Narrative Intelli-gence, pp.
87-92.
November 5-7, Cape Cod, MA.Bremond, C., 1973, Logique du Re?cit, Paris: Editions duSeuil.Cavazza, M., Pizzi, D., Charles, F., Vogt, T. And Andre?,E.
2009, Emotional input for character-based interactive sto-rytelling.
International Joint Conference on AutonomousAgents and Multi-Agents Systems 2009, pp.
313-320.Nigel Crook, Cameron Smith, Marc Cavazza, StephenPulman, Roger Moore, Johan Boye, 2010, Handling User In-terruptions in an Embodied Conversational Agent Proceed-ings of International Workshop on Interacting with ECAs asVirtual Characters, AAMAS 2010.Jo?nsson, A., Ande?n, F., Degerstedt, L., Flycht-Eriksson,A., Merkel, M., and Norberg, S., 2004, Experiences fromcombining dialogue system development with information ex-traction techniques, in: Mark T. Maybury (Ed), New Direc-tions in Question Answering, AAAI/MIT Press.Kennedy and B. Boguraev, 1996, Anaphora for everyone:Pronominal anaphora resolution without a parser.
Proceed-ings of the 16th International Conference on ComputationalLinguistics, Copenhagen, ACL, pp 113-118.Moilanen, K. and Pulman, S. G. , 2007, Sentiment Compo-sition, Proceedings of the Recent Advances in Natural Lan-guage Processing International Conference (RANLP-2007),pp 378?382.Nau, D., Ghallab, M., Traverso, P., 2004,Automated Plan-ning: Theory and Practice, Morgan Kaufmann PublishersInc., San Francisco, CA.J Tepperman, D Traum, and S Narayanan, 2006, ?Yeahright?
: Sarcasm recognition for spoken dialogue systems, In-terspeech 2006, Pittsburgh, PA, 2006.Vogt, T., Andre?, E. and Bee, N., 2008 EmoVoice - A frame-work for online recognition of emotions from voice.
In: Pro-ceedings of Workshop on Perception and Interactive Tech-nologies for Speech-Based Systems, Springer, Kloster Irsee,Germany, (June 2008).Webb, N., D. Benyon, P. Hansen and O. Mival, 2010,Evaluating Human-Machine Conversation for Appropriate-ness, in proceedings of the 7th International Conference onLanguage Resources and Evaluation (LREC2010), Valletta,Malta.42
