Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1734?1743,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsVerb Phrase Ellipsis Resolution Using Discriminative and Margin-InfusedAlgorithmsKian Kenyon-Dean Jackie Chi Kit Cheung Doina PrecupSchool of Computer ScienceMcGill Universitykian.kenyon-dean@mail.mcgill.ca, {jcheung,dprecup}@cs.mcgill.caAbstractVerb Phrase Ellipsis (VPE) is an anaphoricconstruction in which a verb phrase has beenelided.
It occurs frequently in dialogue andinformal conversational settings, but despiteits evident impact on event coreference reso-lution and extraction, there has been relativelylittle work on computational methods for iden-tifying and resolving VPE.
Here, we presenta novel approach to detecting and resolvingVPE by using supervised discriminative ma-chine learning techniques trained on featuresextracted from an automatically parsed, pub-licly available dataset.
Our approach yieldsstate-of-the-art results for VPE detection byimproving F1 score by over 11%; additionally,we explore an approach to antecedent identifi-cation that uses the Margin-Infused-Relaxed-Algorithm, which shows promising results.1 IntroductionVerb Phrase Ellipsis (VPE) is an anaphoric construc-tion in which a verbal constituent has been omitted.In English, an instance of VPE consists of two parts:a trigger, typically an auxiliary or modal verb, thatindicates the presence of a VPE; and an antecedent,which is the verb phrase to which the elided elementresolves (Bos and Spenader, 2011; Dalrymple et al,1991).
For example, in the sentence, ?The govern-ment includes money spent on residential renova-tion; Dodge does not?, the trigger ?does?
resolvesto the antecedent ?includes money spent on residen-tial renovation?.The ability to perform VPE resolution is impor-tant for tasks involving event extraction, especiallyin conversational genres such as informal dialoguewhere VPE occurs more frequently (Nielsen, 2005).Most current event extraction systems ignore VPEand derive some structured semantic representationby reading information from a shallow dependencyparse of a sentence.
Such an approach would notonly miss many valid links between an elided verband its arguments, it could also produce nonsensi-cal extractions if applied directly on an auxiliarytrigger.
In the example above, a naive approachmight produce an unhelpful semantic triple such as(Dodge, agent, do).There have been several previous empirical stud-ies of VPE (Hardt, 1997; Nielsen, 2005; Bos andSpenader, 2011; Bos, 2012; Liu et al, 2016).
Manyprevious approaches were restricted to solving spe-cific subclasses of VPE (e.g., VPE triggered by do(Bos, 2012)), or have relied on simple heuristics forsome or all of the steps in VPE resolution, such asby picking the most recent previous clause as the an-tecedent.In this paper, we develop a VPE resolutionpipeline which encompasses a broad class of VPEs(Figure 1), decomposed into the following two steps.In the VPE detection step, the goal is to determinewhether or not a word triggers VPE.
The secondstep, antecedent identification, requires selecting theclause containing the verbal antecedent, as well asdetermining the exact boundaries of the antecedent,which are often difficult to define.Our contribution is to combine the rich linguis-tic analysis of earlier work with modern statisticalapproaches adapted to the structure of the VPE res-olution problem.
First, inspired by earlier work,1734Figure 1: Example of the VPE resolution pipeline on an exam-ple found in WSJ file wsj 0036.our system exploits linguistically informed featuresspecific to VPE in addition to standard featuressuch as lexical features or POS tags.
Second,we adapt the Margin-Infused-Relaxed-Algorithm(MIRA) (Crammer et al, 2006), which has beenpopular in other tasks, such as machine translation(Watanabe et al, 2007) and parsing (McDonald etal., 2005), to antecedent identification.
This algo-rithm admits a partial loss function which allowscandidate solutions to overlap to a large degree.
Thismakes it well suited to antecedent identification, ascandidate antecedents can overlap greatly as well.On VPE detection, we show that our approach sig-nificantly improves upon a deterministic rule-basedbaseline and outperforms the state-of-the-art systemof Liu et al (2016) by 11%, from 69.52% to 80.78%.For antecedent identification we present results thatare competitive with the state-of-the-art (Liu et al,2016).
We also present state-of-the-art results withour end-to-end VPE resolution pipeline.
Finally, weperform feature ablation experiments to analyze theimpact of various categories of features.2 Related WorkVPE has been the subject of much work in the-oretical linguistics (Sag, 1976; Dalrymple et al,1991, inter alia).
VPE resolution could have a sig-nificant impact on related problems such as eventcoreference resolution (Lee et al, 2012; Bejan andHarabagiu, 2010; Liu et al, 2014) and event ex-traction (Ahn, 2006; Kim et al, 2009; Ritter et al,2012).
It has, however, received relatively little at-tention in the computational literature.Hardt (1992) engaged in the first study of compu-tational and algorithmic approaches for VPE detec-tion and antecedent identification by using heuris-tic, linguistically motivated rules.
Hardt (1997) ex-tracted a dataset of 260 examples from the WSJ cor-pus by using an algorithm that exploited null ele-ments in the PTB parse trees.
Nielsen (2005) built adataset that combined sections of the WSJ and BNC;he showed that the more informal settings capturedin the BNC corpora show significantly more fre-quent occurrences of VPE, especially in dialogue ex-cerpts from interviews and plays.
Using this dataset,he created a full VPE pipeline from raw input textto a full resolution by replacing the trigger with theintended antecedent1.Bos and Spenader (2011) annotated the WSJ foroccurrences of VPE.
They found over 480 instancesof VPE, and 67 instances of the similar phenomenonof do-so anaphora.
Bos (2012) studied do-VPE bytesting algorithmic approaches to VPE detection andantecedent identification that utilize Discourse Rep-resentation Theory.Concurrently with the present work, Liu et al(2016) explored various decompositions of VPE res-olution into detection and antecedent identificationsubtasks, and they corrected the BNC annotationscreated by Nielsen (2005), which were difficult touse because they depended on a particular set ofpreprocessing tools.
Our work follows a similarpipelined statistical approach.
However, we explorean expanded set of linguistically motivated featuresand machine learning algorithms adapted for eachsubtask.
Additionally, we consider all forms ofVPE, including to-VPE, whereas Liu et al only con-sider modal or light verbs (be, do, have) as candi-dates for triggering VPE.
This represented about 7%1e.g., the resolution of the example in Figure 1 would be?The government includes money spent on residential renova-tion; Dodge does not [include money spent on residential reno-vation]?.
We did not pursue this final step due to the lack of acomplete dataset that explicitly depicts the correct grammaticalresolution of the VPE.1735Auxiliary Type Example FrequencyDo does, done 214 (39%)Be is, were 108 (19%)Have has, had 44 (8%)Modal will, can 93 (17%)To to 29 (5%)So do so/same3 67 (12%)TOTAL 554Table 1: Auxiliary categories for VPE and their frequencies inall 25 sections of the WSJ.of the dataset that they examined.3 Approach and DataWe divide the problem into two separate tasks: VPEdetection (Section 4), and antecedent identification(Section 5).
Our experiments use the entire datasetpresented in (Bos and Spenader, 2011).
For prepro-cessing, we used CoreNLP (Manning et al, 2014)to automatically parse the raw text of WSJ for fea-ture extraction.
We also ran experiments using gold-standard parses; however, we did not find significantdifferences in our results2.
Thus, we only report re-sults on automatically generated parses.We divide auxiliaries into the six different cate-gories shown in Table 1, which will be relevant forour feature extraction and model training process,as we will describe.
This division is motivated bythe fact that different auxiliaries exhibit different be-haviours (Bos and Spenader, 2011).
The results wepresent on the different auxiliary categories (see Ta-bles 2 and 4) are obtained from training a single clas-sifier over the entire dataset and then testing on aux-iliaries from each category, with the ALL result be-ing the accuracy obtained over all of the test data.2An anonymous reviewer recommended that further exper-iments could be performed by using the more informative NPscreated with NML nodes (Vadas and Curran, 2007) on the gold-standard parsed WSJ.3For example, ?John will go to the store and Mary will dothe same/likewise/the opposite?.
Do X anaphora and modals arenot technically auxiliary verbs, as noted by the annotators of ourdataset (Bos and Spenader, 2011), but for the purposes of thisstudy we generalize them all as auxiliaries while simultaneouslydividing them into their correct lexical categories.4 VPE DetectionThe task of VPE detection is structured as a binaryclassification problem.
Given an auxiliary, a, weextract a feature vector f , which is used to predictwhether or not the auxiliary is a trigger for VPE.
InFigure 1, for example, there is only one auxiliarypresent, ?does?, and it is a trigger for VPE.
In ourexperiments, we used a logistic regression classifier.4.1 Feature ExtractionWe created three different sets of features related tothe auxiliary and its surrounding context.Auxiliary.
Auxiliary features describe the charac-teristics of the specific auxiliary, including the fol-lowing: word identity of the auxiliary lemma of the auxiliary auxiliary type (as shown in Table 1)Lexical.
These features represent: the three words before and after the trigger their part-of-speech (POS) tags their POS bigramsSyntactic.
We devise these features to encode therelationship between the candidate auxiliary and itslocal syntactic context.
These features were deter-mined to be useful through heuristic analysis of VPEinstances in a development set.
The feature set in-cludes the following binary indicator features (a =the auxiliary): a c-commands4 a verb a c-commands a verb that comes after it a verb c-commands a a verb locally5 c-commands a a locally c-commands a verb a is c-commanded by ?than?, ?as?, or ?so? a is preceded by ?than?, ?as?, or ?so? a is next to punctuation the word ?to?
precedes a a verb immediately follows a a is followed by ?too?
or ?the same?4A word A c-commands another word B if A?s nearestbranching ancestor in the parse tree is an ancestor of B, fol-lowing the definition of Carnie (2013).
We use this term purelyto define a syntactic relation between two points in a parse tree.5A word A and word B share a local structure if they havethe same closest S-node ancestor in the parse tree.17364.2 BaselineAs a baseline, we created a rule-based system in-spired by Nielsen?s (2005) approach to solving VPEdetection.
The baseline algorithm required signifi-cant experimental tuning on the development set be-cause different linguistically hand-crafted rules wereneeded for each of the six trigger forms.
For exam-ple, the following rule for modals achieved 80% F1-accuracy (see Table 2): ?assume VPE is occurring ifthe modal does not c-command a verb that followsit?.
The other trigger forms, however, required sev-eral layers of linguistic rules.
The rules for be andhave triggers were the most difficult to formulate.4.3 ExperimentsWe evaluate our models as usual using precision, re-call and F1 metric for binary classification.
The pri-mary results we present in this section are obtainedthrough 5-fold cross validation over all 25 sectionsof the automatically-parsed dataset.
We use crossvalidation because the train-test split suggested byBos and Spenader (2011) could result in highly var-ied results due to the small size of the dataset (seeTable 1).
Because the vast majority of auxiliaries donot trigger VPE, we over-sample the positive casesduring training.
Table 2 shows a comparison be-tween the machine learning technique and a rule-based baseline for the six auxiliary forms.
Table 3shows results obtained from using the same train-test split used by Liu et al (2016) in order to providea direct comparison.Results.
Using a standard logistic regression clas-sifier, we achieve an 11% improvement in accuracyover the baseline approach, as can be seen in Table 2.The rule-based approach was insufficient for be andhave VPE, where logistic regression provides thelargest improvements.
Although we improve uponthe baseline by 29%, the accuracy achieved for be-VPE is still low; this occurs mainly because: (i) beis the most commonly used auxiliary, so the numberof negative examples is high compared to the num-ber of positive examples; and, (ii) the analysis of thesome of the false positives showed that there mayhave been genuine cases of VPE that were missedby the annotators of the dataset (Bos and Spenader,2011).
For example, this sentence (in file wsj 2057)was missed by the annotators (trigger in bold, an-Auxiliary Baseline ML ChangeDo 0.83 0.89 +0.06Be 0.34 0.63 +0.29Have 0.43 0.75 +0.32Modal 0.80 0.86 +0.06To 0.76 0.79 +0.03So 0.67 0.86 +0.19ALL 0.71 0.82 +0.11Table 2: VPE detection results (baseline F1, Machine LearningF1, ML F1 improvement) obtained with 5-fold cross validation.Test Set Results P R F1Liu et al (2016) 0.8022 0.6134 0.6952This work 0.7574 0.8655 0.8078Table 3: Results (precision, recall, F1) for VPE detection usingthe train-test split proposed by Bos and Spenader (2011).tecedent italicized) ?Some people tend to ignore thata 50-point move is less in percentage terms than itwas when the stock market was lower.?
; here it isclear that was is a trigger for VPE.In Table 3, we compare our results to thoseachieved by Liu et al (2016) when using WSJ sets0-14 for training and sets 20-24 for testing.
We im-prove on their overall accuracy by over 11%, dueto the 25% improvement in recall achieved by ourmethod.
Our results show that oversampling the pos-itive examples in the dataset and incorporating lin-guistically motivated syntactic features provide sub-stantial gains for VPE detection.
Additionally, weconsider every instance of the word to as a potentialtrigger, while they do not - this lowers their recall be-cause they miss every gold-standard instance of to-VPE.
Thus, not only do we improve upon the state-of-the-art accuracy, but we also expand the scope ofVPE-detection to include to-VPE without causing asignificant decrease in accuracy.5 Antecedent IdentificationIn this section we assume that we are given a trig-ger, from which we have to determine the correctantecedent; i.e., in the example in Figure 1, our taskwould be to identify ?includes money spent on res-1737idential renovation?
as the correct antecedent.
Ourapproach to this problem begins with generating alist of candidate antecedents.
Next, we build a fea-ture vector for each candidate by extracting featuresfrom the context surrounding the trigger and an-tecedent.
Lastly, we use these features to learn aweight vector by using the Margin-Infused-Relaxed-Algorithm.5.1 Candidate GenerationWe generate a list of candidate antecedents by firstextracting all VPs and ADJPs (and all contiguouscombinations of their constituents) from the currentsentence and the prior one.
We then filter these can-didates by predefining possible POS tags that an an-tecedent can start or end with according to the train-ing set?s gold standard antecedents.
This methodgenerates an average of 55 candidate antecedents pertrigger, where triggers in longer sentences cause thecreation of a larger number of candidate antecedentsdue to the larger number of VPs.
This strategy ac-counts for 92% of the gold antecedents on the val-idation set by head match.
We experimented witha less restrictive generation filter, but performancewas not improved due to the much larger number ofcandidate antecedents.5.2 Feature ExtractionWe construct a feature vector representation for eachcandidate antecedent; in the example in Figure 1, forexample, we would need feature vectors that differ-entiate between the two potential antecedents ?in-cludes money?
and ?includes money spent on resi-dential renovation?.Alignment.
This feature set results from an align-ment algorithm that creates a mapping between theS-clause nearest to the trigger, St, and the S-clausenearest to the potential antecedent, Sa.
The purposeof these features is to represent the parallelism (orlack thereof) between an antecedent?s local vicinitywith that of the trigger.
The creation of this align-ment algorithm was motivated by our intuition thatthe clause surrounding the trigger will have a par-allel structure to that of the antecedent, and that analignment between the two would best capture thisparallelism.
In the example sentence in Figure 2(trigger in bold, antecedent italicized) ?Investors canFigure 2: Alignment algorithm example with simplified depen-dencies.get slightly higher yields on deposits below $50,000than they can on deposits of $90,000 and up?
a sim-ple observation of parallelism is that both the trig-ger and the correct antecedent are followed by thephrase ?on deposits?.Formally, for each S ?
{Sa, St}, we extract thedependencies in S as chunks of tokens, where eachdependency chunk di contains all tokens betweenits governor and dependent (whichever comes first).Next, for each di ?
Sa, if di contains any tokens thatbelong to the antecedent, delete those tokens.
Sim-ilarly, for each di ?
St, delete any token in di thatbelongs to T .
We then perform a bipartite match-ing to align the di ?
St to the dj ?
Sa, whereeach edge?s weight is determined by a scoring func-tion s(di, dj).
The scoring function we use consid-ers the F1-similarity between the lemmas, POS-tags,and words shared between the two chunks, as wellas whether or not the chunks share the same depen-dency name.1738In the example in Figure 2 we can see that thecorrect antecedent, ?get slightly higher yields?, hasa stronger alignment than the incorrect one, ?getslightly higher yields on deposits?.
This occurs be-cause we remove the candidate antecedent from itsS-clause before creating the chunks; this leaves threenodes for the correct antecedent which map to thethree nodes of the trigger?s S-clause.
However, thisprocess only leaves two nodes for the incorrect can-didate antecedent, thus causing one chunk to be un-mapped, thus creating a weaker alignment.We then use this mapping to generate a featurevector for the antecedent, which contains: the mini-mum, maximum, average, and standard deviation ofthe scores between chunks in the mapping; the num-ber and percentage of unmapped chunks; the depen-dencies that have (and have not) been mapped to; thedependency pairs that were mapped together; andthe minimum, maximum, average, and standard de-viation of the cosine-similarity between the averageword embedding of the words in a chunk betweeneach di, dj pair in the mapping.NP Relation.
These features compare the NounPhrase (NP) closest to the antecedent to the NP clos-est to the trigger.
This is motivated by an obser-vation of many instances of VPE where it is oftenthe case that the entity preceding the trigger is eitherrepeated, similar, or corefers to the entity preced-ing the antecedent.
The relationship between eachNP is most significantly represented by features cre-ated with pre-trained word2vec word embeddings(Mikolov et al, 2013).
For each NP, and for eachword in the NP, we extract its pre-trained word em-bedding and then average them all together.
We thenuse the cosine similarity between these two vectorsas a feature.Syntactic.
Syntactic features are based on the re-lationship between the candidate antecedent?s parsetree with that of the trigger.
This feature set includesthe following features, with the last three being in-fluenced by Hardt?s (1997) ?preference factors?
(a =candidate antecedent, t = trigger): if a?s first word is an auxiliary if a?s head (i.e., first main verb) is an auxiliary the POS tag of a?s first and last words the frequency of each POS tag in the antecedent the frequency of each phrase (i.e., NP, VP,ADJP, etc.)
in a?s sentence and t?s sentence if ?than?, ?as?, or ?so?
is between a and t if the word before a has the same POS-tag orlemma as t if a word in a c-commands a word in t if a?s first or last word c-commands the trigger Be-Do Form: if the lemma of the token preced-ing a is be and the t?s lemma is do Recency: distance between a and t and the dis-tance between the t?s nearest VP and a Quotation: if t is between quotation marks andsimilarly for aMatching.
This last feature set was influenced bythe features described by Liu et al (2016).
Weonly use the ?Match?
features described by them;namely: whether the POS-tags, lemmas, or wordsin a two-token window before the start of the an-tecedent exactly match the two before the trigger;and whether the POS-tag, lemma, or word of the ithtoken before the antecedent equals that of the i-1thtoken before the trigger (for i ?
{1, 2, 3}, wherei = 1 considers the trigger itself).5.3 Training Algorithm - MIRASince many potential antecedents share relativelysimilar characteristics, and since we have many fea-tures and few examples, we use the Margin-Infused-Relaxed-Algorithm (MIRA) in order to identify themost likely potential antecedent.
MIRA maximizesthe margin between the best candidate and the restof the potential antecedents according to a lossfunction.
It has been used for tasks with similarcharacteristics, such as statistical machine transla-tion (Watanabe et al, 2007).The training algorithm begins with a random ini-tialization of the weight vector w. The trainingset contains triggers, each trigger?s candidate an-tecedents, and their gold standard antecedents; it isreshuffled after each training epoch.
We find theK highest-scoring potential antecedents, a1, .
.
.
, ak,according to the current weight value.
A learn-ing rate parameter determines how much we retainthe new weight update with respect to the previousweight vector values.MIRA defines the update step of the standard on-line training algorithm: it seeks to learn a weight1739vector that, when multiplied with a feature vector fi,gives the highest score to the antecedent that is mostsimilar to the gold standard antecedent, a?.
This isposed as an optimization problem:minimizewi?wi ?
wi?1?+ CK?k?ksubject to wi ?
a?
?
wi ?
ak + ?k ?
L(a?, ak),k = 1, .
.
.
,K(1)Here, L is the loss function that controls the mar-gin between candidates and the gold standard; it isdefined as the evaluation metric proposed by Bosand Spenader (2011) (described in Section 5.5).The ?
are slack variables and C ?
0 is a hyper-parameter that controls the acceptable margin.
Thisproblem is solved by converting it to its Lagrangedual form6.5.4 Baseline AlgorithmThe baseline we created was motivated by Bos?s(2012) baseline algorithm: given a trigger, return asthe antecedent the nearest VP that does not includethe trigger.
This is a na?
?ve approach to antecedentidentification because it does not consider the re-lationship between the context surrounding the an-tecedent and the context surrounding the trigger.5.5 ExperimentsWe evaluate our results following the proposed met-rics of Bos and Spenader (2011), as do Liu et al(2016).
Accuracy for antecedent identification iscomputed according to n = the number of correctlyidentified tokens between the candidate antecedentand the gold standard antecedent.
Precision is ndivided by the length of the candidate antecedent,recall is n divided by the length of the correct an-tecedent, and accuracy is the harmonic mean of pre-cision and recall.
For MIRA, final results are deter-mined by choosing the weight vector that achievedthe best performance on a validation set that is splitoff from part of the training set, as calculated aftereach update step.6In this study, the dual form was implemented by hand usingGurobi?s python API (Gurobi Optimization Inc., 2015).Auxiliary Baseline MIRA Changedo 0.42 0.71 +0.29be 0.37 0.63 +0.26modal 0.42 0.67 +0.25so 0.15 0.53 +0.38have 0.39 0.61 +0.22to 0.03 0.58 +0.55ALL 0.36 0.65 +0.29Table 4: Results (baseline accuracy, MIRA accuracy, accuracyimprovement) for antecedent identification; obtained with 5-fold cross validation.End-to-end Results P R F1Liu et al (2016) 0.5482 0.4192 0.4751This work 0.4871 0.5567 0.5196Table 5: End-to-end results (precision, recall, F1) using thetrain-test split proposed by Bos and Spenader (2011).MIRA has several hyper-parameters that weretuned through a grid search over the validation set.The most crucial parameters were the learning rate?, and C, while the value of K did not cause signif-icant changes in accuracy.Results.
In Table 4, we see that MIRA improvesupon the baseline with a 29% increase in overall ac-curacy.
MIRA provides significant gains for eachform of VPE, although there is room for improve-ment, especially when identifying the antecedents ofdo-so triggers.Liu et al (2016) achieve an accuracy of 65.20%with their joint resolution model for antecedent iden-tification when using the train-test split proposedby Bos and Spenader (2011); our model achieves62.20% accuracy.
However, their experimental de-sign was slightly different than ours ?
they onlyconsidered antecedents of triggers detected by theiroracle trigger detection method, while we use allgold-standard triggers, meaning our results are notdirectly comparable to theirs.
Our cross validatedresults (65.18% accuracy) paint a better picture ofthe quality of our model because the small size ofthe dataset (554 samples) can cause highly variedresults.1740Excluded P R F1Auxiliary 0.7982 0.7611 0.7781Lexical 0.6937 0.8408 0.7582Syntactic 0.7404 0.7330 0.7343NONE 0.8242 0.8120 0.8170Table 6: Feature ablation results (feature set excluded, preci-sion, recall, F1) on VPE detection; obtained with 5-fold crossvalidation.In Table 5 we present end-to-end results obtainedfrom our system when using the triggers detected byour VPE detection model (see Section 4).
We com-pare these results to the end-to-end results of the bestmodel of Liu et al (2016).
Following Liu et al, weassign partial credit during end-to-end evaluation inthe following way: for each correctly detected (truepositive) trigger, the Bos and Spenader (2011) an-tecedent evaluation score between the trigger?s pre-dicted antecedent and its gold antecedent is used (asopposed to a value of 1).
As can be seen from Table5, we trade about 6 points of precision for 14 pointsof recall, thus improving state-of-the-art end-to-endaccuracy from 47.51% to 51.96%.6 Feature Ablation StudiesWe performed feature ablation experiments in orderto determine the impact that the different feature setshad on performance.Trigger Detection.
In Table 6 we can see that thesyntactic features were essential for obtaining thebest results, as can be seen by the 8.3% improve-ment, from 73.4% to 81.7%, obtained from includ-ing these features.
This shows that notions from the-oretical linguistics can prove to be invaluable whenapproaching the problem of VPE detection and thatextracting these features in related problems mayimprove performance.Antecedent Identification.
Table 7 presents theresults from a feature ablation study on antecedentidentification.
The most striking observation is thatthe alignment features do not add any significant im-provement in the results.
This is either because theresimply is not an inherent parallelism between theFeatures Excluded AccuracyAlignment 0.6511NP Relation 0.6428Syntactic 0.5495Matching 0.6504NONE 0.6518Table 7: Feature ablation results (feature set excluded, preci-sion, recall, F1) on antecedent identification; obtained with 5-fold cross validation.trigger site and the antecedent site, or because theother features represent the parallelism adequatelywithout necessitating the addition of the alignmentfeatures.
The heuristic syntactic features provide alarge (10%) accuracy improvement when included.These results show that a dependency-based align-ment approach to feature extraction does not rep-resent the parallelism between the trigger and an-tecedent as well as features based on the lexical andsyntactic properties of the two.7 Conclusion and Future WorkWe presented an approach for the tasks of VerbPhrase Ellipsis detection and antecedent identifica-tion that leverages features informed both by the-oretical linguistics and NLP, and employs machinelearning methods to build VPE detection and an-tecedent identification tools using these features.Our results show the importance of distinguishingVPE triggers from each other, and highlight the im-portance of using the notion of c-command for bothtasks.For VPE detection, we improve upon the accu-racy of the state-of-the-art system by over 11%, from69.52% to 80.78%.
For antecedent identification,our results significantly improve upon a baseline al-gorithm and we present results that are competitivewith the state-of-the-art, as well as state-of-the-artresults for an end-to-end system.
We also expand thescope of previous state-of-the-art by including thedetection and resolution of to-VPE, thus building asystem that encompasses the entirety of the Bos andSpenader (2011) VPE dataset.In future work, we would like to further inves-1741tigate other margin-based optimizations similar toMIRA, but perhaps even more resilient to over-fitting.
We also seek to improve the antecedent iden-tification approach by extracting stronger features.AcknowledgmentsThis work was funded by McGill University and theNatural Sciences and Engineering Research Councilof Canada via a Summer Undergraduate ResearchProject award granted to the first author.
We thankthe anonymous reviewers for their helpful sugges-tions, and we thank Nielsen, Hector Liu, and EdgarGonza`lez for their clarifying remarks over email.ReferencesDavid Ahn.
2006.
The stages of event extraction.
InProceedings of the Workshop on Annotating and Rea-soning about Time and Events, pages 1?8.
Associationfor Computational Linguistics.Cosmin Adrian Bejan and Sanda Harabagiu.
2010.
Un-supervised event coreference resolution with rich lin-guistic features.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1412?1422.
Association for ComputationalLinguistics.Johan Bos and Jennifer Spenader.
2011.
An annotatedcorpus for the analysis of VP ellipsis.
Language Re-sources and Evaluation, 45(4):463?494.Johan Bos.
2012.
Robust VP ellipsis resolution in DRtheory.
In Staffan Larsson and Lars Borin, editors,From Quantification to Conversation, volume 19 ofTributes, pages 145?159.
College Publications.Andrew Carnie.
2013.
Syntax: A generative introduc-tion.
John Wiley & Sons.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
The Journal of Machine Learn-ing Research, 7:551?585.Mary Dalrymple, Stuart M Shieber, and Fernando CNPereira.
1991.
Ellipsis and higher-order unification.Linguistics and Philosophy, 14(4):399?452.Gurobi Optimization Inc. 2015.
Gurobi optimizer refer-ence manual.Daniel Hardt.
1992.
An algorithm for VP ellipsis.
InProceedings of the 30th Annual Meeting on Associa-tion for Computational Linguistics, pages 9?14.
Asso-ciation for Computational Linguistics.Daniel Hardt.
1997.
An empirical approach to VP ellip-sis.
Computational Linguistics, 23(4):525?541.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP ?09 shared task on event extraction.
InProceedings of the Workshop on Current Trends inBiomedical Natural Language Processing: SharedTask, pages 1?9.
Association for Computational Lin-guistics.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages489?500.
Association for Computational Linguistics.Zhengzhong Liu, Jun Araki, Eduard H Hovy, and TerukoMitamura.
2014.
Supervised within-document eventcoreference using information propagation.
In LREC,pages 4539?4544.Zhengzhong Liu, Edgar Gonzalez, and Dan Gillick.2016.
Exploring the steps of verb phrase ellipsis.
InProceedings of the Workshop on Coreference Resolu-tion Beyond OntoNotes (CORBON 2016), co-locatedwith NAACL 2016, pages 32?40.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Association for Computational Lin-guistics (ACL) System Demonstrations, pages 55?60.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages91?98.
Association for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Advances in neural information processing systems,pages 3111?3119.Leif Arda Nielsen.
2005.
A Corpus-Based Study of VerbPhrase Ellipsis Identification and Resolution.
Ph.D.thesis, King?s College London.Alan Ritter, Oren Etzioni, Sam Clark, et al 2012.
Opendomain event extraction from twitter.
In Proceedingsof the 18th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1104?1112.
ACM.Ivan A Sag.
1976.
Deletion and logical form.
Ph.D.thesis, Massachusetts Institute of Technology.David Vadas and James Curran.
2007.
Adding nounphrase structure to the penn treebank.
In Annual Meet-ing - Association for Computational Linguistics, vol-ume 45, page 240.1742Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773.1743
