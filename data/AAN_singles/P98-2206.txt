Chinese Word Segmentationwithout Using Lexicon and Hand-crafted Training DataSun Maosong, Shen Dayang*, Benjamin K Tsou**State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, Beijing, ChinaEmail: lkc-dcs@mail.tsinghua.edu, cn* Computer Science Institute, Shantou University, Guangdong, China** Language Information Sciences Research Centre, City University ofHong Kong, Hong KongAbstractChinese word segmentation is the first step in anyChinese NLP system.
This paper presents a newalgorithm for segmenting Chinese texts withoutmaking use of any lexicon and hand-craftedlinguistic resource.
The statistical data required bythe algorithm, that is, mutual information and thedifference of t-score between characters, isderived automatically from raw Chinese corpora.The preliminary experiment shows that thesegmentation accuracy of our algorithm isacceptable.
We hope the gaining of this approachwill be beneficial to improving theperfomaance(especially in ability to cope withunknown words and ability to adapt to variousdomains) of the existing segmenters, though thealgorithm itself can also be utilized as a stand-alonesegmenter in some NLP applications.1.
IntroductionAny Chinese word is composed of either singleor multiple characters.
Chinese texts are explicitlyconcatenations of characters, words are notdelimited by spaces as that in English.
Chineseword segmentation is therefore the first step for anyChinese information processing system\[ 1\].Almost all methods for Chinese wordsegmentation developed so far, both statistical andrule-based, exploited two kinds of importantresources, i.e., lexicon and hand-crafted linguisticresources(manually segmented and tagged corpus,knowledge for unknown words, and linguisticThis work was supported in part by the NationalNatural Science Foundation of China under grantNo.
69433010.rules)\[1,2,3,5,6,8,9,10\].
Lexicon is usually used asthe means for finding segmentation candidates forinput sentences, while linguistic resources forsolving segnaentation ambiguities.
Preparation ofthese resources (well-defined lexicon, widelyaccepted tag set, consistent annotated corpus etc.
)is very hard due to particularity of Chinese, andtime consuming.
Furthermore, ven the lexicon islarge enough, and the corpus annotated is balancedand huge in size, the word segmenter will still facethe problem of data incompleteness, sparseness andbias as it is utilized in different domains.An important issue in designing Chinesesegmenters i  thus how to reduce the effort ofhuman supervision as much as possible.Palmer(1997) conducted a Chinese segrnenterwhich merely made use of a manually segmentedcorpus(without referring to any lexicon).
Atransformation-based algorithm was then exploredto learn segmentation rules automatically from thesegmented corpus.
Sproat and Shih(1993) furtherproposed a method using neither lexicon norsegmented corpus: for input texts, simply groupingcharacter pairs with high value of mutualinformation into words.
Although this strategy isvery simple and has many limitations(e.g., it canonly treat bi-character words), the characteristic ofit is that it is fully automatic -- the nmtualinformation between characters can be trained fromraw Chinese corpus directly.Following the line of Sproat and Shih, here wepresent a new algorithm for segmenting Chinesetexts which depends upon neither lexicon nor anyhand-crafted resource.
All data necessary for oursystem is derived from the raw corpus.
The systemmay be viewed as a stand-alone s gmenter in someapplications (preliminary experiments show that its1265accuracy is acceptable); nevertheless, our mainpurpose is to study how and how well the work canbe done by machine at the extreme conditions, say,without any assistance of human.
We believe theperformance of the existing Chinese segmenters,that is, the ability to deal with segmentationambiguities and unknown words as well as theability to adapt to new domains, will be improvedin some degree if the gaining of this approach isincorporated into systems properly.2.
Principle2.1.
Mutual information and difference oft-score between charactersMutual information and t-score, twoimportant concepts in information theory andstatistics, have been exploited to measure thedegree of association between two words in anEnglish corpus\[4\].
We adopt these measuresalmost completely here, with one majormodification: the variables in two relevant formulaeare no longer words but Chinese characters.Definition 1 Given a Chinese character string 'xy',the mutual information between characters x and3,(or equally, the mutual information of thelocation between x and y) is defined as:mi(x:y) = log 2 p(x ,y)p(x)p(y)where p(x,y) is the co-occurrence probability of xand y, and p(x), p(y) are the independentprobabilities of x and y respectively.As claimed by Church(1991), the larger themutual information between x and y, the higher thepossibility of x and y being combined together.
Forexample:?
10ml6420-2- -~-~~o (1)The distribution fmi(x:y) for sentence (I) isillustrated inFig.
l(where "~" denotes x,y shouldbe combined and "m" be separated in terms ofhuman judgment.
This convention will be effectivethroughout the paper).
The correct segmentationfor (1) can be achieved when we decide that everylocation between x and y in the sentence b  treatedas 'combined' or 'separated' accordingly if its mYvalue is greater than or below a threshold(supposethe threshold is 3.0 for this example):economy cooperation will beI  ff?for current world economy trendof an appropriate answer(Economic cooperation will be anappropriate answer to the trend of economicsin current worM.
)It is evident hat x and y are to be stronglycombined together if mY(x.
'y)>>O and to beseparated if mi(x:y)<<O.
But if mi(x.
'y) ~ O, theassociation of x and y becomes uncertain.Observe the mY distribution for sentence (2) inFig.
2:~o  (2)In the region of 2.0 ~< mY < 4.0, there existsome confusions: we have mY(~."
~=mi(~t:.Y~ :) >mi(.T/z.
?
~Yt~), mi(fl~: ~)  > mi(~.
7 ~') > mi(;~?
: t~),and mY(~."
~) > mY(/~: f/:), however, "~J~:~""7~:~'" '~}~:~'" '~:  "should be separated and "~:~ ' " '~:~ ' " '~:  \[\] '"'}~: ~J:" be combined by humanjudgment -- the power of mi is somewhat weak ini;:.
.
.
.
.
.
.
.
.
.
.
.
.
: : ::) i i i=;::~i E '  ~1~ iZ i i i : : .
:.~i~iii!
!ill : i i i::ii.
: .~7;m. !
Ill:":: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
...  : : i g: .
: :  :s : .
================================================================ ~i i  ~ ?
: : ::.:.::.
~ : i : :?
,  , m:, , , , ,  .
.
.
.
.
.
.
.
.
.
.
.
.
.
~:~: : :~ : : : : "  : :: :i:===============================,:,:m: ~:~i::;i m: ' :  I l l  " - : .
: .
: : : : :E;E" E :E : :  " "  " : :E: " : .
"hq  .
.
.
.
.
.
.
.
.
.
.
.  "
.
.
.
.
.
.
.
.
.
.
.Character pairs in sentence Fig.
1 The distribution ofmi(sentence 1)?
connecti break1266mi 8 t :" : : .... ~ : ~ ~ iiiiiiiiiiiiiiiiiiiii}iii}i ii~iiiiii;iiiiii~iiii6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
%: ;22Z2221;21 ;Z : ;Z I ; I I2 ;Z I%2222; IZ ;221; I ;Z I I / IZ I : ; :2 :4 : ~,::!
:: :~:;~:;:~.~/~i~:~ii~!~ii~;~iii:iiiiiiii~i~ii:i~i;i!iii~iiii~i?ii!~:~;i~;~i~i!i~iiiiiiiiiii~i~i~i~!~!~!i~:i:~;~!i:i~ii:i:~: \] .connect\]break, i :  i ?
~; ~;  :" : "  :: :!:!
::':':: "::::'::" : "  :i31~!~i!.i:::ih::i!:i!i}:~!:!:;5}!~::~:?i~:ii:iiilh~!!i!!iii::i!!!:!i!
:'::i:~ \]?
.
.
: : .
: : : .
........ : : - : : : : : :  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  .
.
.
.
.
.
.
.
.
.
.
.
.
:  .
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.0 ~ ~ " :~ ~ : :  : :  :i~ii:i~i:i~i~  ~ '=::iiiiiiii~i,i~:~!~ii,!iii.~i~i~iiii~iii~i~iii~i:i~i ~ ill.:}: .iii~!~}!~} ~:i" :  ....................... ::::~ii~:: ........... " .......
~:iiiiiiiiii~!iiiiiii~!iii~!~i!i!i~.~iiii!iiiii1i1i1i1~1ii!~!i2i!
!i!iiii~i-2  "~:ii?i/~!5~2ii~i2~;~!~:~i;ii~iiiiii5iiiiiiiigig!iii~i~iiiii~!~!~1!~iiiiiiiiiiiiiii~iiiiiiii?i?~s~s~s-4 .
.
.
.
.
.
ii;i~!:~i~i~i~i::ii!!i~i~iiiiiiiiii~iiiiii~iiiii!:!!i~;i!!i~i~i!iii!iiiii!iiiiiiii~iiiii~i!~i~i~i!!!ii!
!ii~iiiiiii~i~iiiFig.2 The distribution ofmi(sentence 2) Characterpmrs m sentencethe 'intermediate' range of its value.
To solve thisproblem, we need to seek other ways additionally.Definition 2 Given a Chinese character string'xvz'.
the t-score of the character y relevant ocharacters x and z is defined as:p(zl y) - p(y\[ x)tSx"(Y) = ~/var(p(zly))  + var(p(y lx) )where p(ylx) is the conditional probability of ygiven x, and p(zly), of z given y, and var(p(ylx)),var(p(zly)) are variances of p(ylx) and of p(zly)respectively.Also as pointed out by Church( 1991), ts~, z (y)indicates the binding tendency of y in the context ofx and z:ifp(zly)> p(ylx), or ts~.z(y) > 0then y tends to be bound with z ratherthan with xif p(ylx)> p(zly), or tsx, (y) < 0then y tends to be bound with x ratherthan with zA distinct feature of ts is that it is context-dependent (a relative measure), along with certaindegree of flexibility to the context, whereas mi iscontext-independent (an absolute measure).
Itsdrawback is it attaches to a character rather than tothe location between two adjacent characters.
Thismay cause some inconvenience if we want to unifyit with mi.
We initially introduce a new measure dtsinstead of ts:Definition 3 Given a Chinese character string'vxyw', the difference oft-score between charactersx and y is defined as:dts(x: y )  = tSv.y (x) - tSx, w. (y)Now dts(x:y)  is allocated to the locationbetween x and y, just like mi(x :y ) .
And thecontext of dts(x:y)  becomes 4 characters, 1character larger than that of tSx, z (y ) .The value of dts(x:y)  reflects thecompetition results among four adjacent charactersv, x, y and w:(1) tsv,y(x) > 0 tsx,w(y ) < 0(x tends to combine with y, and y tends tocombine with x) ==> dts(x:y)  > 0?
?In this case, x and y attract each other.
Thelocation between x and y should be bound.
(2) tSv.y (x) < 0 tSx.
w (y)  > 0(x tends to combine with v, and y tends tocombine with w) ==> dts(x:y)  < 0?< ?
@ >?In this case, x and y repel each other.
Thelocation between x and y should be separated.
(3a) tsv.y (x) > 0 tsx,w (y)  > 0(x tends to combine with y, whereas y tendsto combine with w)(3b) tsv.
e (x) < 0 tsx.
~ (y) < 0(x tends to combine with v, whereas y tendsto combine with x)?< ?< @ ?In cases of (3a) and (3b), the status of thelocation between x and y is determined by thecompetition of ts~, e(x) and tSx, w (Y) :if dts(x:y)  > 0 then it tends to be boundif dts(x:y)  < 0 then it tends to be separated1267dts200Iii!
:: ii iii!i iiiii!iiiiii iii!iiiiiii   ii!
!i!i!iiiii !iii i !!
iii!ii !
i i iiiiiiii i!5o : ..~,... : .... ::~: .~;~;;;~;;;~ii~i~i~i~i~;~;~;~i~i~!~i~ii~;~;~;~;~iii~;~:;;~ ,, break I0 ?
I1~ :- : .
.
.~:~: i .
: :~: : : .
.
:;:.
:i~: :~i~.~ii~::~:~::~:~!
:..i: i::~;~iiii!~i:i~i:i!~i:~i~!
~i :.
: :.... : ........ !
i iiiiii I !
i:-~oo4?
\ [ ~ !
i i l !
: .
.
.
:  " .
:: ' " " ~:~i:ii~!
!~i;!i!i~:!~ :::~:iiiiiiFig.3 The distribution ofdts(sentence 2) Character pairs in sentenceThe general rule governing dts is similar asthat governing mi: the higher the difference of t-score between x and y, the stronger thecombination strength between them, and vice versa.But the role of dts is somewhat different from thatof mi: it is capable of complementing the 'blindarea" of mi on some occasions.Consider sentence (2) again.
The distributionof dis for it is shown in Fig.
3.
Return to thecharacter pairs whose mi values fall into the regionof 2.0 ~< mi < 4.0 in Fig.
2, compare their dtsvalues accordingly: dts( ~:.T/:) > dts(?~Je: ~)  >dts(H. ~7~g), dts(;~."
l~) > dts(y~: ~)  > dts(~."
7~?~),and dts(~: f f )>  dts(~_: E) -- the conclusiondra~ from these comparisons i very close to thehuman judgment.2.2.
Local maximum and local minimumof dtsMost of the character pairs in sentence (2)have got satisfactory explanations by their mi anddts so far.
"~\]~ :  .
.
.
.
~ : ~"  are two of fewexceptions.
We have mi(~.
~)> mi(J\]::~) anddts(?Yj~: ~)> dts(Tf: \]~), however, the humanjudgment is the former should be separated and thelatter be bound.
Aiming at this, we furtherproposed two new concepts, that is, local maximumand local minimum of dts.Definition 4 Given 'vxyw' a Chinese characterstring, dts(x:y) is said to be a local maximum ifdts(x.
'y) > dts(v:x) and dts(x:y) > dts(y:w).
And,the height of the local maximum dts(x:y) is definedas :h(dts(x:y)) = min { dts(x:y)- dts(v:x),dts(x:y) -- dts(y:w) }Definition 5 Given 'vxyw' a Chinese characterstring, dts(x:y) is said to be a local minimum ifdts(x.
'y)< dts(v:x) and dts(x:y) < dts(y:w).
And,the depth of the local minimum dts(x:y) is definedas"d(dts(x:y)) = min { dts(v:x)-- dts(x.y),dts(y:w) -- dts(x:y) }Two basic hypotheses can be easily made asthe consequence of context-dependability ofdts(note: mi has not such property):Hypothesis 1 x and y tends to be bound ifdts(x:y)is a local maximum, regardless of the value ofdts(x:y)(even it is low).Hypothesis 2 x and y tends to be separated ifdts(x:y) is a local minimum, regardless of the valueof dts(x:y) (even it is high).In Fig.
3, dts(fi4-j~: ,~) is a local minimumwhereas dts(H.'j~g) isn't.
At least we can say that"~-\]t:~" is likely to be separated, as suggested bythe hypothesis 2(though we still can say nothingmore about "T\[::~").2.3.
The second local maximum and thesecond local minimum of dtsWe continue to define other four relatedconcepts:Definition 6 Suppose 'vxyzw' is a Chinesecharacter string, and dts(x:y) is a local maximum.Then dts(y:z) is said to be the right second localmaximum of dts(x:y) if dts(y:z)> dts(v:x) anddts(y:z) > dts(z:w).And, the distance between thelocal maximum and the second local maximum isdefined as:dis(locmax, y:z) = dts(x:y)- dts(y:z)Definition 7 Suppose 'vxyzw' is a Chinese1268character string, and dts(x:y) is a local minimum.Then dts(y:z) is said to be the right second localminimum of dts(x:y) if dts(y:z)< dts(v:x) anddts(y:z) < dts(z:w).
And, the distance between thelocal minimum and the second local minimum isdefined as:dis(locmin, y:z) = dts(y:z)- dts(x:y)The left second local maximum and the leftsecond local minimum of dts(x:y) can be definedsimilarly.Refer to Fig.
3.
By definition, dts(fl~.
'yT~) is theleft second local minimum of dts(3~g: 7~'), anddts(y~.
'~) is the right second local maximum ofdts ( '~"y~)  meanwhile the left second localminimum of dts(?~: ~).These four measures are designed to deal withtwo conunon construction types in Chinese wordformation: "2 characters + I character" and"1 character + 2 characters".
We will skip thediscussion about this due to the limited volume ofthe paper.3.
AlgorithmThe basic idea is to try to integrate all of themeasures introduced in section 2 together into analgorithm, making best use of the advantages andbypassing the disadvantages of them underdifferent conditions.Given an input sentence S, let/~,,, : the mean ofmi of all locations in S;o'm,: the standard eviation ofmi of alllocations in S;flat.,.
: the mean ofdts of all locations in S;(in fact, /ta, ~.
----- 0)o-a, s: the standard eviation of dts of alllocations in Swe divide the distribution graphs of mi and dtsof S into several regions(4 regions for each graph)by ~tm~, o',,~, /laL ,.
and O'dt  s "region Aregion Bregion Cregion Dregion aregion bdts(x:y) > cr ats0 < dts(x:y)<~ o'at ~-o'at ~ < dts(x:y)~ 0dts(x:y) <~- o" a,;mi(x:y) > l.t., + o',.
iiU mi < mi(x:y)~ /.t mi + O'miregion c ~t,, i -- o-mi < mi(x:y)<~ lu,,iregion d mi(x:y) <~ lu,.~ -- o-,,,The algorithm scans the input sentence S fromleft to right two times:The first round for SFor any location (x:y) in S, do1.
in cases that <dts(x:y), mi(x:y)> falls into:1.1 Aa or Ba or Ca or Da or Abmark (x:y) 'bound'1.2 Ad or Bd or Cd or Dd or Dcmark (x:y) 'separated'1.3 Ac or Cbifdts(x:y) is local maximum thenif h(dts(x:y)) > 81then mark (x:y) 'bound' else '?
'ifdts(x:y) is local minimum thenif d(dts(x.
'y)) > ~2then mark (x:y) 'separated' else '?
'1.4 Bc or Dbifdts(x:y) is local maximum thenif h(dts(x:y)) > 8 2then mark (x:y) 'bound' else '?
'ifdts(x:y) is local minimum thenif d(dts(x:y)) > ~lthen mark (x:y) 'separated' else '9'1.5 Ccif (dts(x.y) is local maximum) and(h(dts(x:y)) > 6 3 )then mark (x:y) 'bound' else '9'if dts(x.
'y) is local minimumthen mark (x:y) 'separated' else '?
'1.6 Bbifdts(x:y) is local maximumthen mark (x:y) 'bound' else '9'if (dts(x:y) is local minimum) and(a(ats(x:y)) > )then mark (x:y) 'separated' else '?'2.
For (x:y) unmarked so far, mark it as '9'except hat:ifdts(x:y) is the second local maximumthen if dis(locmax, x:y) <0.5 X lrmin(loc, x:y)/* Refer to the notations in definition 6&7.lrmin(loc, x.y) = rain {Idts(x:y)-- dts(v:x)l,Idts(x:y)- dts(z:w)l } *11269then mark (x:y) "--' if(x:y) is the right second local maxor ' - - ' i f(x:y) is the left second local maxifdts(x:y) is the second local minimumthen if dis(locmin, x:y) <0.5 ?
lrmin(loc, x:y)then mark (x:y) "--' if(x:y) is the right second local minor '~ '  if(x:y) is the left second local minThe second round for Sif (x:y) is marked '?
'then if mi(x:y) >~ 0then mark (x:y) 'bound' else 'separated'if (x:y) is marked '---"then the status of (x:y) follows that ofthe adjacent location on the left sideif (x:y) is marked '---"then the status of (x:y) follows that ofthe adjacent location on the right side(The constants 61, 62, 63, ~l, ~2, ~3 aredetermined by experiments, satisfying:G < &_ < G ; G < G < Gand 0=2.5)Generally speaking, the lower the <dts(x:y),mi(x:y)> in distribution graphs, the more restrictivethe constraints.
Take 'bound' operation as example:there is not an 3, additional condition in case 1.1; incase 1.6 however, the existence of a localmaximum is needed; in case 1.3, a requirement forthe height of local maximum is added; in case 1.4,the height required becomes even higher; and incase 1.5, which is the worst case for 'bound'operation, the height must be high enough.Case 2 says if the second local maximum ispretty, near to the local maximum corresponded,then its status ('bound' or 'separated') would belikely to be consistent with that of the localmaximum.
So does the second local minimum.Finally, for locations marked '?'
with whichwe have no more means to cope, simply makedecisions by the value of mi(we set it to 2.5, sameas that in the system of Sproat and Shih(1993)).Recall sentence (2).
The character pair "7~:~E" is regarded as 'separated' successfully byfollowing "~E: W_,"(local minimum) with the rule incase 2 although its mi value is rather high(3.4).
"~:~J~" is marked '?'
in the first round and treatedproperly by 0 in the second round.The algorithm outputssegmentation for sentence (2) at last:the correctFrance tennis competition todayE I I Iin Paris the western suburbsIopen curtain(The Tennis Competition of France opened inthe western suburbs of Paris today.
)Note that there exist wo ambiguous fragments"~T I :~" ( "~ I ~'" or "~")  and " ~~"("~ I ~"  or "~1 ~ I ;~\]~"), aswellas two proper nouns "France" and "Paris" insentence (2).4.
Experimental resultsWe select 100 Chinese sentences, consisting of1588 characters(or 1587 locations betweencharacter pairs) randomly as testing texts.
Thestatistical data required by calculating mi and dts,in fact it is character bigram, is automaticallyderived from a news corpus of about 20M Chinesecharacters.
The testing texts and training corpusare mutually excluded.Out of 1587 locations in the testing texts,1456 are correctly marked by our algorithm.We define the accuracy of segmentation as:# of locations being correctly marked# of locations in textsThen, the accuracy for testing texts is1456/1587 = 91.75%.The distribution of local maximum, localminimum and other types ofdts value(involving thesecond local maximum and the second localminimum) of the testing texts over <dts, mi>regions is summarized in Fig.
4 (Fig.
5 is the samedistribution in percentage representation).
Thiswould be helpful for readers to understand ouralgorithm.Future work includes: (1) enlarging the size of1270experiments; (2) refining the algorithm by studyingthe relationship between mi and dts in depth; and (3)integrating it as a module with the existing Chinesesegmenters so as to improve their performance(especially in ability to cope with unknown wordsand ability to adapt to various domains).
-- it isindeed the ultimate goal of our research ere.5.
AcknowledgmentsThis work benefited a lot from discussionswith Professor Huang Changning of TsinghuaUniversity, Bering, China.
We would also like tothank anonymous COLING-ACL'98 reviewers fortheir helpful comments.25O200150g.
1005OAa Ab Ac Ad Ba Bb Bc Bd Ca Cb Cc Cd Da Db Dc DdFig.4 The distribution fdts types in testing texts Region\[\] Others?
LocMin\[\] LocMaxoo% .... .....
I !
l l20%0%Aa Ab Ac Ad Ba Bb Bc Bd Ca Cb Cc Cd Da Db Dc DdFig.5 The distribution fdts types in testing texts\[\] Others I?
LocMin I\[\] LocMax\[RegionReferences\[1\] Liang N.Y., "CDWS: An Automatic WordSegmentation System for Written Chinese Texts",Journal of Chinese Information Processing, Vol.
1,No.2, 1987 (in Chinese)\[2\] Fan C.K.,Tsai WH., "Automatic WordIdentification in Chinese Sentences by theRelaxation Technique", Computer Processing ofChinese & Oriental Languages, Vol.4, No.
1, 1988\[3\] Yao T.S., Zhang G.P., Wu Y.M., "A Rule-based Chinese Word Segmentation System",Journal of Chinese Information Processing, Vol.4,No.
1, 1990 (in Chinese)\[4\] Church K.W., Hanks P., Hindle D., "UsingStatistics in Lexical Analysis", In LexicalAcquisition: Exploiting On-line Resources toBuild a Lexicon, edited by U. Zernik, Hillsdale,N.J.
:Erlbaum, 1991\[5\] Chan K.J., Liu S.H., "Word Identification forMandarin Chinese Sentences", Proc.
of COL1NG-92, Nantes, 1992\[6\] Sun M.S., Lai B.Y., Lun S., Sun C.F., "SomeIssues on Statistical Approach to Chinese WordIdentification", Proc.
of the 3rd InternationalConference on Chinese Information Processing,Beijing, 1992\[7\] Sproat R., Shih C.L., "A Statistical Methodfor Finding Word Boundaries in Chinese Text",Computer Processing of Chinese and OrientalLanguages, No.4, 1993\[8\] Sproat R. et al "A Stochastic Finite-StateWord Segmentation Algorithm for Chinese", Proc.of the 32nd Annual Meetmg of ACL, New Mexico,1994\[9\] Palmer D.D., "A Trainable Rule-basedAlgorithm for Word Segmentation", Proc.
of the35th Annual Meeting of ACL and 8th Conferenceof the European Chapter of ACL, Madrid, 1997\[10\] Sun M.S., Shen D.Y., Huang C.N.,"CSeg&Tagl.0: A Practical Word Segmenter andPOS Tagger for Chinese Texts", Proc.
of the 6thANLP, Washington D.C., 19971271
