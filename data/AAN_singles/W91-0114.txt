SHARED PREFERENCESJ ames  Barnet t  and Inderjeet ManiMCC3500 West Balcones Center Dr.Austin, TX 78759barnett@mcc.commani@mcc.com!AbstractThis paper at tempts  to develop a theory ofheuristics or preferences that can be shared be-tween understanding and generation systems.
Wefirst develop a formal analysis of preferences andconsider the relation between their uses in gener-ation and understanding.
We then present a bi-directional algorithm for applying them and ex-amine typical heuristics for lexical choice, scopeand anaphora in:, more detail.1 Int roduct ionUnderstanding and generation systems must bothdeal with ambiguity.
In understanding, there areoften a number of possible meanings for a string,while there are usually a number of different waysof expressing a given meaning in generation.
Tocontrol the expl6sion of possibilities, researchershave developed a variety of heuristics or prefer-ences - for example, a preference for low attach-ment of modifiers in understanding or for conci-sion in generation.
This paper investigates thepossibility of sharing such preferences betweenunderstanding and generation as part of a bidirec-tional NL system.
In Section 2 we formalize theconcept of a preference, and Section 3 presentsan algorithm fo !
applying such preferences uni-formly in understanding and generation.
In Sec-tion 4 we consider specific heuristics for lexicalchoice, scope,  and anaphora.
These heuristicshave special properties that permit a more effi-cient implementation than the general algorithmfrom Section 3.
Section 5 discusses ome of theshort-comings of the theory developed here andsuggests directions for future research.2 Preferences in Under-standing and Generat ionNatural language understanding is a mappingfrom utterances to meanings, while generationgoes in the opposite direction.
Given a set Stringof input strings (of a given language) and a set Intof interpretations or meanings, we can representunderstanding as a relation U C String x Int,and generation as G C IntxString.
U and G arerelations, rather than functions, since they allowfor ambiguity: multiple meanings for an utter-ance and multiple ways of expressing a meaning 1.A minimal requirement for a reversible system isthat U and G be inverses of each other.
For alls 6 String and i 6 Int:(s, Oeu  (i,s)ec (1)Intuitively, preferences are ways of controllingthe ambiguity of U and G by ranking some inter-pretations (for U) or strings (for G) more highlythan others.
Formally, then, we can view prefer-ences as total orders on the objects in question(we will capitalize the term when using it in thistechnical sense).
2 Thus, for any s 6 String an un-derstanding Preference Pint will order the pairs{(s, 01(s,=) E U}, while a generation Preference* The definitions of U and G allow for strings with nointerpretations and meanings with no strings.
Since anymeaning can presumably be expressed in any language,we may want to further estrict G so that everything isexpressible:  Y i  6 Int (Bs 6 String \[(s, ) E GI) .2We use total orders rather than partial orders to avoidhaving to deal with incommensurate s ructures.
The re-quirement ofcommensurability is not burdensome in prac-tice, even though many heuristics apparently don't applyto certain structures.
For example, a heuristic favoring lowattachment of post-modifiers doesn't clearly tell us how torank a sentence without post-modifiers, but we can insertsuch sentences into a total order by observing that theyhave all modifiers attached as low as po6sible.109P,,r will rank {(/,s)l(/,, ) 6 G} s. Thus we canview the task of understanding as enumeratingthe interpretations of a string in the order givenby Pint.
Similarly, generation will produce stringsin the order given by Po,,.
Using Up,., and Gp.,.to denote the result of combining U and G withthese preferences, we have, for all s G String andi 6 Int:Up,., (s) =d,l (ix .
.
.
.
, in) (2)where U(s) = {i l , .
.
.
,  in} and\[j < k\] ~ \[(s, it) <p,., (s, ik)\]GPo,.
(i) =d,!
(s, .
.
.so 0 (3)where G(0 = {sx,.. .
,  sin} andIJ < --.
<p .
, .
(i, sk))\]Alternatively, we note that any Preference Pinduces an equivalence r lation ='p which groupstogether the objects that are equal under p.4We can therefore view the task of Generationand Understanding as being the enumeration ofP's equivalence lasses in order, without worryingabout order within classes (note that Formulae 2and 3 specify the order only of pairs where onemember is less than the other under P.)The question ow arises of what the relation be-tween understanding Preferences and generationPreferences should be.
Understanding heuristicsare intended to find the meaning that the speakeris most likely to have intended for an utterance,and generation heuristics hould select he stringthat is most likely to communicate a given mean-ing to the hearer.
We would expect hese Prefer-ences to be inverses of each other: if s is the bestway to express meaning i, then i should be themost likely interpretation f s. If we don't acceptthis condition, we will generate sentences thatwe expect the listener to misinterpret.
There-fore we define class(Preference, pair) to be theequivalence class that pair is assigned to underPreference's ordering, 5 and link the the first3Note that  this  def init ion allows Preferences to work'across der ivat ions. '
For example ,  it allows Pint to rankpairs (s, ,}, (s', i9 where 8 # s ' .
It permi ts  a Preference tosay that  i is a bet te r  in terpretat ion  for s than  i '  is for s:.It is not  clear if th is  sort  of  power is necessary,  and  thealgor i thtns below require only that  Preferences be able torank  different in terpretat ions  ( tr ings) for a given str ing( interpretat ion) .4Any  order P on a set of  ob jects  D part i t ions  D into aset of  equivalence c lasses by ass igning each x E D to theset {ulv _<P x :z x _<p u}.Selass(Preference, pair) is def ined as the number  ofclasses conta in ing  i tems that  rank  more  h igh ly  than  pairunder  Preference.
(most highly ranked) classes under P/., and P.,ras follows:elass(eo,r, (/, 8)) = 0 (4)- .
crass(P,.,, (s, O) = 0It is also reasonable to require that opposingsets of preferences in understanding be reflectedin generation.
If string s, has two interpretationsit and i2, with it being preferred to is, and stringss has the same two interpretations with the pref-erences reversed, then s, should be a better wayof expressing i, than i2, and vice-versa for ss:\[ (sl, il) <p,., @1, is)&(as, i2) <e,., (as, il) \]\ [ ( i l ,sd <p.,.
(i ,82)&(is,as) <p.,.
62,.x) \](5)Formula 4 provides a tight coupling of heuris-tics for understanding and generating the mostpreferred structures, but it doesn't provide anyway to share Preferences for secondary readings.Formula 5 offers a way to share heuristics for sec-ondary interpretations, but it is quite weak andwould be highly inefficient to use.
To employ itduring generation to choose between sl and ss asways of expressing il, we would have to run theunderstanding system on both sl and ss to see ifwe could find another interpretation i2 that bothstrings share but with opposite rankings relativeto il.If we want to share Preferences for secondaryreadings, we will need to make stronger assump-tions.
The question of ranking secondary in-terpretations brings us onto treacherous groundsince most common heuristics (e.g., preferringlow attachment) specify only the best readingand don't help choose between secondary andtertiary readings.
Furthermore, native speakersdon't seem to have clear intuitions about the rel-ative ranking of lesser readings.
Finally, there issome question about why we should care aboutnon-primary readings, since the best interpreta-tion or string is normally what we want.
However,it is important to deal with secondary preferences,in part for systematic completeness, but mostlybecause secondary readings are vital in any at,-tempt to deal with figurative language- humor,irony, and metaphor - which depends on the in-terplay between primary and secondary readings.i i 0iTo begin to develop a theory of secondary Pref-erences, we will simply stipulate that the heuris-tics in question are shared 'across the board' be-Itween understanding and generatmn.~ The sim-plest way to do this is to extend Formula 4 into abiconditional, a~d require it to hold of all classes(we will reconsider this stipulation in Section 5).For all s6Str in~l  and i 6 In t ,  we have:et.ss(P,.,, (,,,)) = el.ss(P.,., (i, 8)) (6)Since Preferences now work in either direction,we can simplify our notation and represent themas total orderings of a set T of trees, where eachnode of each tre4 is annotated with syntactic andsemantic information, and, for any t 6 T, str(t)?
k returns the string in Str ing that t dominates (i.e.,spans), and sere(t) returns the interpretation iInt for the root node of t. For apreferenee P onT and trees t l ,  th, we stipulate:t, <p  t2Up(str ( t l ) )tl <p t2=&Gp(sem(t l ) )  =s t r ( t l )=st r ( t2 )  (7)~ .
.
sem(t l  ) .
.
.
sem(t2 ) .
.
.
)sere(q)  = sere(t2) (8)6. .
s t r ( t , ) .
.
.
s t r ( t2 ) .
.
.
)We close this Section by noting a property ofPreferences thatwi l l  be important in Section 4:an ordered list Of Preferences can be combinedinto a new Preference by using each item in thelist to refine the bordering specified by the previ-ous ones.
That is, the second Preference orderspairs that are equal under the first Preference,and the third Preference applies to those that arestill equal under the second Preference, etc.
IfP1-- .
P ,  are Preferences, we define a new Com-plex Preference P<,...,> as follows:tl <Pc-, ...,.)
t2 (9)~-;B l< j< n \[Q <pj t2\]& - ,3 i< j  \[t2 <p, tl\]3 An  A lgor i thm for Shar ingPre ferencesIf we consider ways of sharing Preferences be-tween understanding and generation, the simplestone is to simply produce all possible interpreta-tions(strings), and then sort them using the Pref-erence.
This is, of course, inefficient in caseswhere we are interested in only the more highlyranked possibilities.
We can do better if we arewilling to make few assumptions about the struc-ture of Preferences and the understanding andgeneration routines.
The crucial requirement onPreferences i that they be 'upwardly monotonic'in the following sense: if t,  is preferred to t2, thenit is also preferred to any tree containing tz as asubtree.
Using subtree(t, ,t2) to mean that tx isa subtree of t2, we stipulate\[tl <p t2 ~ subtree(t2,t3)\] (10)--~ t l  <P gSWithout such a requirement, there is no way tocut off unpromising paths, since we can't predictthe ranking of a complete structure from that ofits constituents?FinaLly, we assume that both understandingand generation are agenda-driven procedures thatwork by creating, combining, and elaboratingtrees.
6 Under these assumptions, the followinghigh-level algorithm can be wrapped around theunderlying parsing and generation routines tocause the output to be enumerated in the ordergiven by a Preference P. In the pseudo-code be-low, mode specifies the direction of processing andinput is a string (if mode is understanding) or asemantic representation (if mode is generation).execute_item removes an item from the agendaand executes it, returning 0 or more new trees.generate_items takes a newly formed tree, a set ofpreviously existing trees, and the mode, and addsa set of new actions to the agenda.
(The un-derlying understanding or generation algorithmis hidden inside generate_items.)
The variable ac-tive holds the set of trees that are currently be-ing used to generate new items, while frozen holdsthose that won't be used until later, complete_treeis a termination test that returns True if a tree iscomplete for the mode in question (i.e., if it hasa full semantic interpretation for understanding,or dominates a complete string for generation).The global variable classes holds a list of equiva-lence classes used by equiv_class (defined below),while level holds the number of the equivalenceclass currently being enumerated.
Thaw~restartis called each time level is incremented to gener-ate new agenda items for trees that may belongto that class.ALGORITHM 1e A wide variety of NLP algorithms can be implementedin this manner, particularly such recent reversible gen-eration algorithms as \[Shieber, van Noord, Moore, andPereira, 1989\] and \[Calder, Reape, and Zeevat, 1989\].i i iclasses := Nil; solutions := Nil;new-trees := Nil; agenda := Nil;level := 1;frozen := initialize.agenda(input, mode);{end of global declarations}while frozen dobeginsolutions := get_complete_trees(frozen, level, mode);agenda := thaw&restart(frozen, level, agenda, mode);while agenda dobeginnew_trees := execute_item(agenda);while new_trees dobeginnew_tree := pop(new_trees);if equiv_class (P, new_tree), > levelthen push(new_tree, frozen);else if complete_tree(new_tree,mode)then push(newAree, solutions);else generate, items(new_tree, active,agenda, mode);end;end; {agenda exhausted for this level}{solutions may need partitioning}while solutions dobegincomplete_tree := pop(solutions);if equiv_class(P, complete_tree)> levelthen push(complete_tree, frozen);else output(complete_tree, l vel) ;end{increment level to output next class}level := level + 1;end;The function equiv_class keeps track of theequivalence classes induced by the Preferences.Given an input tree, it returns the number ofthe equivalence class that the tree belongs to.Since it must construct the equivalence classes asit goes,along, it may return different values ondifferent calls with the same argument (for ex-ample, it will always return 1 the first time it iscalled, even though the tree in question may endup ha a lower class.)
However, successive callsto equiv_class will always return a non-decreasingseries of values, so that a given tree is guaran-teed to be ranked no more highly than the valuereturned (it is this property of eqaiv_class thatforces the extra pass over the completed trees inthe algorithm above: a tree that was assigned toclass n when it was added to solutions may havebeen demoted to a lower class in the interim asmore trees were examined).
Less_than and eqeaitake a Preference and a pair of trees and returnTrue if the first tree is less than (equal to) thesecond under the Preference.
Create_class takes atree and creates a new class whose only memberis that tree, while insert adds a class to classesin the indicated position (shifting other classesdown, if necessary), and select_member r turns anarbitrary member of a class.funct ion equiv_class (P: Preference, T: Tree)beginclass_num := 1;for class in classes dobeginif less_than(P, T, select_member(class))thenbegininsert(new_class(T),classes, class_num);return(classmum);end;else if equal(P, T, select_member(class))thenbeginadd_member(T, class);return(class_hum);end;else class_num := class_num + 1;end ;{T < all classes}insert(new_class(T),classes, class_num);return(class_num);end {equiv_elass}To see that the algorithm enumerates trees inthe order given by <p, note that the first itera-tion outputs trees which are minimal under <p.Now consider any tree t ,  which is output on asubseqent i ertion N. For all other t,, output onthat iteration, t ,  =p t,,.
Furthermore, t con-tains a subtree t,ub which was frozen for all levelsup to N. Using T(J) to denote the set of treesoutput on iteration J, we have: VI_< I<  NIV ti 6 T(I) ti <p t,ub\]\], whence, by stipulation10, t,  <p ti.
Thus t ,  is greater than or equal to112all trees which were enumerated before it.
To cal-culate the time complexity of the algorithm, notethat it calls equiv_class once for each tree createdby the underlying understanding or generation al-gorithm (and once for each complete interpreta-tion).
Equiv_class, in turn, must potentially com-pare its argument with each existing equivalenceclass.
Assuming that the comparison takes con-stant time, the '.complexity of the algorithm de-pends on the number k of equivalence classes <pinduces: if the Underlying algorithm is O(f(n)),the overall comp~lexity is O(f(n)) x k. Dependingon the Preference, k could be a small constant,or itself proportional to f (n) ,  in which case thecomplexity woul~ be O(f(n)~).4 Opt imizat ion  o f  Prefer-encesAs we make more restrictive assumptions aboutPreferences, more efficient algorithms becomepossible.
Initialily , we assumed only that Pref-erences pecified!
total orders on trees, i.e., thatwould take two I trees as input and determineif one was less than, greater than, or equal tothe other ~.
Given such an unrestricted viewof Preferences, ~ve can do no better than pro-ducing all interp~-etations(strings) andthen sort-ing them.
This simple approach is fine if wewant all possibilities, especially if we assumethat there won't, be a large number of them, sothat standard n ,2 or n logn sorting algorithms(see \[Aho, Hopcroft, and Ullman, 1983\]) won't bemuch of an addit~ional burden.
However, this ap-proach is inefficient if we are interested in onlysome of the possibilities.
Adding the monotonic-ity restriction 10 permits Algorithm 1, which ismore efficient in.
that it postpones the creationof (successors of) lower ranked trees.
However,we are still opera'ting with a very general view ofwhat Preferencesl are, and further improvementsare possible when we look at individual Prefer-ences in detail, in this section, we will considerheuristics for lexical selection, scope, and anaphorresolution.
We do not make any claims for theusefullness of these heuristics as such, but takethem as concrete 'examples that show the impor-tance of considering the computational propertiesof Preferences.Note that Algorithm 1 is stated in terms of asingle Preference.
It is possible to combine multi-ple Preferences into a single one using Formula 9,rWe also assume \[hat his test takes constant time.and we are currently investigating other methodsof combination.
Since the algorithms below arehighly specialized, they cannot be combined withother Preferences using Formula 9.
The ultimategoal of this research, however, is to integrate suchspecialized algorithms with a more sophisticatedversion of Algorithm 1.4 .1  Lex ica l  Cho iceOne simple preferencing scheme involves assign-ing integer weights to lexical items and syntacticrules.
Items or rules with higher weights are lesscommon and are considered only if lower rankeditems fail.
When combined with restriction 10,this weighting scheme yields a Preference <wtthat ranks trees according to their lexical and ruleweights.
Using maz_wt(T) to denote the mostheavily weighted lexical item or rule used in theconstruction of T, we have:tl <tot 7t2 ('~del maz-wt ( t l )  < maz_wt(t2)(11)The significant property here is that the equiva-lence classes under <wt can be computed withoutdirectly comparing trees.
Given a lexical itemwith weight n, we know that any tree contain-ing it must be in class n or lower.
Noting thatour algorithm works by generate-and-test (treesare created and then ranked by equiv_class), wecan achieve a modest improvement in efficiencyby not creating trees with level n lexical items orrules until it is time to enumerate that equivalenceclass.
We can implement his change for bothgeneration and understanding by adding level asa parameter to both initialize_agenda and gener-ate_items, and changing the functions they callto consider only rules and lexical items at or be-low level.
How much of an improvement thisyields will depend on how many classes we want toenumerate and how many lexical items and rulesthere are below the last class enumerated.4 .2  ScopeScope is another place where we can improveon the basic algorithm.
We start by consider-ing scoping during Understanding.
Given a sen-tence s with operators (quantifiers) o l .
.
.o , ,  as-signing a scope amounts to determining a totalorder on ol .
.
.o ,  s. If a scope Preference can doSNote that this ordering is not a Preference.
A Prefer-ence will be a total ordering of trees, each of which containssuch a scope ordering, i.e., a scope Preference will be anordering of orderings of operators.113no more than compare and rank pairs of scopings,then the simple generate-and-test algorithm willrequire O(n!)
steps to find the best scoping sinceit will potentially have to examine very possibleordering.
However, the standard heuristics for as-signing scope (e.g., give "strong" quantifiers widescope, respect left-to-right order in the sentence)can be used to directly assign the preferred or-dering of ox.
.
.
ON.
If we assume that secondaryreadings are ranked by how closely they matchthe preferred scoping, we have a Preference <,ccan be defined.
In the following (ol, oj) 6 Sc(s)means that oi preceeds oj in scoping Sc of sen-tence s, and Scb,,t(s) is the preferred ordering ofthe operators in s given by the heuristics:Sc,(s) <,~ Se2(s) ~d, !
(12)Vo ,oi \[(o ,o9 eSc (s) --.
(o,,o9 sc,(.
)\] \]Given such a Preference, we can generate thescopings of a sentence more efficiently by first pro-ducing the preferred reading (the first equivalenceclass), then all scopes that have one pair of oper-ators switched (the second class), then all thosewith two pairs out of order, etc.
In the followingalgorithm, ops is the set of operators in the sen-tence, and sort is any sorting routine, switched?is a predicate returning True if its two argumentshave already been switched (i.e., if its first argwas to the right of its second in Scbe,t(s)), whileswitch(o,, o2, ord) is a function that returns newordering which is the same as ord except that o~precedes o, in it.
{the best scoping}root_set := sort(operators, SCbe?~(s));level := 1;output(root_set, level);new_set := Nil;old_set := add_item(root_set, Nil);{loop will execute n!
- 1 times }whi le  old_set dobeginfor ordering in old_set dobeg infor op in ordering dobeg in{consider adjacent pairs of operators}next := right_neighbor(op, ordering);{switch any pair that hasn't already been}i f  next and  not(switched?
(op, next))then  dobeg innew_scope := switch(op, next, ordering);add_item(new.scope, n w_set);output(new_scope, l vel) ;endendendold_set := new_set;new_set := Nil;endWhile the Algorithm 1 would require O(n!
)steps to generate the first scoping, this algo-rithm will output the best scoping in the n 2or n log n steps that it takes to do the sort (cf\[Aho, Hopcroft, and Ullman, 1983\]), while eachadditional scoping is produced in constant time.
9The algorithm is profligate in that it generatesall possible orderings of quantifiers, many ofwhich do not correspond to legal scopings (see\[Hobbs and Shieber, 1987\]).
It can be tightenedup by adding a legality test before scope is out-put.When we move from Understanding to Gener-ation, following Formula 6, we see that the taskis to take an input semantics with scoping Scand enumerate first all strings that have Sc astheir best scoping, then all those with Sc as thesecond best scoping, etc.
Equivalently, we enu-merate first strings whose scopings exactly matchSc, then those that match Sc except for one pairof operators, then those matching except for twopairs, etc.
We can use the Algorithm 1 to imple-ment this efficiently if we replace each of the twoconditional calls to equiv_class.
Instead of firstcomputing the equivalence class and then testingwhether it is less than level, we call the followingfunction class_less_than:{True iff candidate ranked at level or below}{ Target is the desired scoping}funct ion  classAess_than( candidate, target, level)beginswitchAimit := level; {global variable}switches := O; {global variable}re turn  test_order(candidate, target, target);end  {class_less_than }funct ion  test_order( eand, targ_rest, targ)begini f  null(cand)re turn  True;else9switched.?
can be implemented in constant time ifwe record the position of each operator in the originalscoping SCbest.
Then switched.?
(Ol, 02) returns True iffposiaon(o2) < p0siao,(ol).114!begintarg_tail := member(first(cand), targ_rest);i f  targ_tailreturn test_order(rest(cand), targ_tail, targ);elsebeginswitches := switches + 1;i f  >(switches, switch.limit)return FalSe;endelseii f  (simple_test(rest(cand), targ_rest)re turn  tesLorder(cand, targ, targ);else re turn  False;endend {test_order} ifunct ion simple~test( cand_rest, arg_rest)beg infor  cand in cand_rest dobegini f  not(member(cand, targ_rest))beginswitches := switches + l;i f  >(switches, switch_limit)re turn  falseiendendre turn  true;end  {simple_test}To estimate the complexity of class_less_than,note that if no switches are encountered,test_orderwill make one pass through targ_rest (=targ) in O(n) steps, where n is the length of targ.Each switch encoUntered results in a call'to sita- rpie_test, O(n) steps, plus a call to test_arg on thefull list targ for another O(n) steps.
The overallcomplexity is thus O((j+ 1) x n), where level = jis the number switches permitted.
Note thatclass_less_than tests a candidate string's scopingonly against the target scope, without having toinspect other possible strings or other possiblescopings for the string.
We therefore do not needto consider all strings that can have Sc as a scop-ing in order to fifid the most highly ranked onesthat do.
Furthermore, class_less_than will workon partial constituents (it doesn't require thatcand have the same number of operators as targ),so unpromising pi ths can be pruned early.4 .3 '  Anapho i .aNext we consider the problem of anaphoric ref-erence.
From the standpoint of Understanding,resolving an anaphoric reference can be viewedas a matter of finding a Preference ordering ofall the possible antecedents of the pronoun.
Al-gorithm 1 would have to produce a separate in-terpretation for each object that had been men-tioned in the discourse and then rank them all.This would clearly be extremely inefficient inany discourse more than a couple of sentenceslong.
Instead, we will take the anaphora reso-lution algorithm from \[Rich and Luperfoy, 1988\],\[Luperfoy and Rich, 1991\] and show how it can beviewed as an implementation f a Complex Pref-erence, allowing for a more efficient implementa-tion.Under this algorithm, anaphora resolution isentrusted to Experts of three kinds: a Proposerfinds likely candidate antecendents, Filters pro-vide a quick way of rejecting many candidates,and Rankers perform more expensive tests tochoose among the rest.
Recency is a good ex-ample of a Proposer; antecedents are often foundin the last couple of sentences, o we should startwith the most recent sentences and work back.Gender is a typical Filter; given a use of "he", wecan remove from consideration all non-male ob-jects that the Proposers have offered.
Semanticplausibility or Syntactic parallelism are Rankers;they are more expensive than the Filters andassign a rational-valued score to each candidaterather than giving a yes/no answer.When we translate these experts into ourframework, we see that Proposers are Prefer-ences that can efficiently generate their equiva-lence classes in rank order, rather than having tosort a pre-existing set of candidates.
This is whereour gain in efficiency will come: we can work backthrough the Proposer's candidates in order, confi-dent that any candidates we haven't seen must beranked lower than those we have seen.
Filters rep-resent a special class of Preference that partitioncandidates into only two classes: those that passand those that are rejected.
Furthermore, we areinterested only in candidates that aiifilters assignto the first class.
If we simply combine n Filtersinto a Complex Preference using Formula 9, theresult is not a Filter since it partitions the inputinto 2" classes.
We therefore define a new sim-ple Filter F(I ,..J.)
that assigns its input to class1 iff F1...Fn all do.
Finally, Rankers are Pref-erences of the kind we've been discussing so far.When we observe that the effect of running a Pro-poser and then removing all candidates that theFilters reject is equivalent to first running the Fil-ter and then using the Proposer to refine its first115class 1?, we see that the algorithm above, when runwith Proposer Pr, Filters F1... Fn and RankersRt .
.
.
Rj, implements the Complex PreferenceP(Ftl ' I.
),pr,at...a~), defined in accordance withFormu'la 9.
We thus have the following algorithm,where nezt_class takes a Proposer and a pronounas input and returns its next equivalence class ofcandidate antecedents for the pronoun.class := 1; {global variable}cand := next_class(Proposer, p onoun);filtered_cand := cand;whi le  (cand) dobeg infor  eand in cands dobeg infor  filter in Filters dobeg ini f  not(Filter(cand))then  remove(cand, filtered_cand);endend{filtered_cand now contains class n under}{P(F(,,...l.),pr ).
Rankers R1- .
.
R j}{may split it into several classes}refine&output(filtered_cand, Rankers);cand := next_class(Proposer);endfunct ion  Refine&Output(cands, Rankers)beg inrefined_order := sort(cands, Rankers);i f  rest(Rankers)then  refine&output(refined_order,rest(Rankers));e lsebeg inloc_class := 1; for  cand in refined_order doi f  >(equiv_classfirst(Rankers), cand),loc_class)thenbeg inloc_class := loe_elass + 1;class := class + ioc_class;endoutput(cand, class);endend {Refine&Output}Moving to Generation, we use this Preference*0 In both cases, the result is: pl n f l  , .
- -  P .
,  l' lfl, wherePl .
.
.
p .
are the equivalence classes induced by the Pro-poser, and f l  is the Filter's first equivalence class.to decide when to use a pronoun.
Following For-mula 6, we want to use a pronoun to refer to ob-ject z at level n iff that pronoun would be inter-preted as referring to z in class n during Under-standing.
First we need a test occursf(Proposer,z) that will return True iff Proposer will even-tually output z in some equivalence class.
Forexample, a Recency Proposer will never suggest acandidate that hasn't occurred in the antecedentdiscourse, so there is no point in considering apronoun to refer to such an object.
Next, wenote that the candidates that the Proposer re-turns are really pairs consisting of a pronoun andan antecedent, and that Filters work by compar-ing the features of the pronoun (gender, number,etc.)
with those of the antecedent.
We can im-plement Filters to work by unifying the (syntac-tic) features of the pronoun with the (syntacticand semantic) features of the antecedent, return-ing either a more fully-specified set of features forthe pronoun, or .L if unification fails.
We can nowtake a syntactically underspecified pronoun and zand use the Filter to choose the appropriate set offeatures.
We are now assured that the Proposerwill suggest z at some point, and that z will passall the filters.Having established that z is a reasonable can-didate for pronominal reference, we need to de-termine what claxs z will be assigned to as an an-tecedent.
Rankers such as Syntactic Parallelismmust look at the full syntactic structure**, so wemust generate complete sentences before doingthe final ranking.
Given a sentence s contaningpronoun p with antecedent z, we can determinethe equivalence class of (p, z) by running the Pro-poser until it (p, z) appears, then running the Fil-ters on all other candidates, and passing all thesurvivors and (p,x) to refine~ontpnt, and thenseeing what class (p, z) is returned in.
Alterna-tively, if we only want to check whether (p, z) isin a certain class n or not, we can run the reso-lution algorithm given above until n classes havebeen enumerated, quitting if (p,x) is not in it.
(See the next section for a discussion of this algo-rithm's obvious weaknesses.
)nThe definitions we've given so far do not specify howPreferences should rank "unfinished" structures, i.e., thosethat don't contain all the information the Preference re-quires.
One obvious olution is to assign incomplete struc-tures to the first equivalence class; M the structures be-come complete, they can be moved own into lower daasesif necessary.
Under such a strategy, Preferences such asSyntactic Parallelism will return high scores on the in-complete constituents, but these scores will be meaning-less, since many of the resulting complete structures willbe placed into lower classes.1165 D iscuss ionRelated Work: There is an enormous amountof work on preferences for understanding, e.g.,\[Whittemore, Ferrara, and Brunner, 1990\],\[Jensen and Binot, 1988\], \[Grosz, Appelt, Mar-tin, and Pereira, 1987\] for a few recent examples.In work on generation preferences (in the sense ofrankings of structures) are less clearly identifiablesince such rankings tend to be contained implic-itly in strategies for the larger problem of decidingwhat to say (but see \[Mann and Moore, 1981\] and\[Reiter, 1990\].
)i Algorithm 1 is similar in spiritto the "all possibilities plus constraints" strategythat is common in principle-based approaches ( ee\[Epstein, 1988\])i, but it differs from them in that itimposes a preference ordering on interpretations,rather than rest'ricting the set O f legal interpreta-tions to begin With.IStrzalkowski \[Strzalkowski, 1990\] contrasts twostrategies for r~versibility: those with a singlegrammar and two intepreters versus those witha single interpreter and two grammars.
Althoughthe top-level algorithm presented here works forboth understanding and generation, the under-lying generatio~ and understanding algorithmscan belong to either of Strzalkowski's categories.However, the more specific algorithms discussedin Section 4 belong to the former category.
Thereis also a clear "directionality" in both the scopeand the anaphora Preferences; both are basicallyunderstanding h~euristics that have been reformu-lated to work b~i-directionally.
For this reason,they are both considerably weaker as generationheuristics.
In particular, the anaphora Prefer-ence is clearly insufficient as a method of choosingwhen to use a pronoun.
At best, it can serve tovalidate the choices made by a more substantialplanning compoOent.The Two Directions: In general, it is not clear Jwhat the relation between understanding andgeneration heuristics hould be.
Formulae 4 and5 are reasonable requirements, but they are tooweak to provide ithe close linkage between under-standing and generation that we would like tohave in a bi-directional system.
On the otherhand, Formula 6 is probably too strong since itrequires the equlivalence classes to be the sameacross the boardl In particular, it entails the con-verse of Formula.4, and this has counter-intuitiveresults.
For example, consider any highly convo-luted, but grammatical , sentence: it has a bestinterpretation, and by Formula 6 it is thereforeone of the best ways of expressing that meaning.But if it is sufficently opaque, it is not a goodway of saying anything.
Similarly, a speaker maysuddenly use a pronoun to refer to an object ina distant part of the discourse.
If the anaphoraPreference is sophisticated enough, it may resolvethe pronoun correctly, but we would not want thegeneration system to conclude that it should use apronoun in that situation.
One way to tackle thisproblem is to observe that understanding systemstend to be too loose (they accept a lot of thingsthat you don't want to generate), while genera-tion systems are too strict (they cover only a sub-set of the language.)
We can therefore view gener-ation Preferences as restrictions of understandingPreferences.
On this view, one may construct ageneration Preference from one for understandingby adding extra clauses, with the result that itsordering is a refinement of that induced by theunderstanding Preference.Internal Structure: Further research is neces-sary into the internal structure of Preferences.We chose a very general definition of Preferencesto start with, and found that further restrictionsallowed for improvements in efficiency.
Prefer-ences that partition input into a fixed set ofequivalence classes that can be determined in ad-vance (e.g., the Preference for lexical choice dis-cussed in Section 4) are particularly desireablesince they allow structures to be categorized inisolation, without comparing them to other al-ternatives.
Other Preferences, uch as the scopeheuristic, allow us to create the desired struc-tures directly, again without need for compari-son with other trees.
On the other hand, theanaphora Preference is based on an algorithmthat assigns rational-valued scores to candidateantecedents.
Thus there can be arbitrarily manyequivalence classes, and we can't determine whichone a given candidate belongs to without look-ing at all higher-ranked candidates.
This is nota problem during understanding, since the Pro-poser can provide those candidates efficiently, butthe algorithm for generation is quite awkward,amounting to little more than "make a guess, thenrun understanding and see what happens.
"The focus of our future research will be a for-mal analysis of various Preferences to determinethe characteristic properties of good understand-ing and generation heuristics and to investigatemethods other than Formula 9 of combining mul-tiple Preferences.
Given such an analysis, Algo-rithm 1 will be modified to handle multiple Pref-erences and to treat the different ypes of Pref-erences differently, thus reducing the need for117the kind of heuristic-specific algorithms een inSection 4.
We also plan an implementation fthese Preferences as part of the KBNL system\[Barnett, Mani, Knight, and Rich, 1990\].References\[Aho, Hopcroft, and Ullman, 1983\] Alfred Aho,John Hopcroft, and Jeffrey Ullman.
DataStructures and Algorithms.
Addison-Wesley,1983.\[Barnett, Mani, Knight, and Rich, 1990\]Jim Barnett, lnderjeet Mani, Kevin Knight,and Elaine Rich.
Knowledge and natural lan-guage processing.
CACM, August 1990.\[Calder, Reape, and Zeevat, 1989\] J. Calder, M.Reape, and H. Zeevat.
An algorithm for gen-eration in unification categorial grammar.
InProceedings of the ~th conference of the Eu-ropean Chapter of the ACL, 1989.\[Epstein, 1988\] Samuel Epstein.
Principle-basedinterpretation of natural language quanti-tiers.
In Proceedings of AAAI 88, 1988.\[Grosz, Appelt, Martin, and Pereira, 1987\]Barbara Grosz, Douglas Appelt, Paul Mar-tin, and Fernando Pereira.
Team: an exper-iment in the design of portable natural an-guage interfaces.
Artificial Intelligence, 1987.\[Hobbs and Shieber, 1987\] Jerry Hobbs and Stu-art Shieber.
An algorithm for generatingquantifier scopings.
Computational Linguis-tics, 13(1-2):47-63, 1987.\[Jensen and Binot, 1988\]Karen Jensen and Jean-Louis Binot.
Dic-tionary text entries as a source of knowledgefor syntactic and other disambiguations.
InSecond Conference on Applied Natural Lan-guage Processing, Austin, Texas, 9-12 Febru-ary, 1988.\[Luperfoy and Rich, 1991\] Susan Luperfoy andElaine Rich.
Anaphora resolution.
Compu-tational Linguistics, to appear.\[Mann and Moore, 1981\] William~Mann and James Moore.
Computer gener-ation of multiparagraph english text.
Amer-ican Journal of Computational Linguistics,7(1):17-29, 1981.\[Reiter, 1990\] Ehud Reiter.
The computationalcomplexity of avoiding conversational impli-catures.
In Proceedings of the ACL, Pitts-burgh, 6.9 June, 1990.\[Rich and Luperfoy, 1988\] Elaine Rich and SusanLuperfoy.
An architecture for anaphora res-olution.
In Second Conference on AppliedNatural Language Processing, Austin, Texas,9-12 February, 1988.\[Shieber, van Noord, Moore and Pereira, 1989\]S. Shieber, G. van Noord, R. Moore, and F.Pereira.
A semantic head-driven generationalgorithm for unification-based formalisms.In Proceedings of the ACL, Vancouver, 26-29 June, 1989.\[Strzalkowski, 1990\] Tomek Strzalkowski.
Re-versible logic grammars for parsing and gen-eration.
Computational Intelligence, 6(3),1990.\[Whittemore, Ferrara, and Brunner, 1990\] GregWhittemore, Kathleen Ferrara, and HansBrunner.
Post-modifier prepositional phraseambiguity in written interactive dialogues.In Proceedings of the A CL, Pittsburgh, 6-9June, 1990.118
