Workshop on Computational Linguistics for Literature, pages 59?63,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsLiterary authorship attribution with phrase-structure fragmentsAndreas van CranenburghHuygens INGRoyal Netherlands Academy of Arts and SciencesP.O.
box 90754, 2509 LT The Hague, the Netherlandsandreas.van.cranenburgh@huygens.knaw.nlAbstractWe present a method of authorship attributionand stylometry that exploits hierarchical infor-mation in phrase-structures.
Contrary to muchprevious work in stylometry, we focus on con-tent words rather than function words.
Textsare parsed to obtain phrase-structures, and com-pared with texts to be analyzed.
An efficienttree kernel method identifies common tree frag-ments among data of known authors and un-known texts.
These fragments are then used toidentify authors and characterize their styles.Our experiments show that the structural infor-mation from fragments provides complemen-tary information to the baseline trigram model.1 IntroductionThe task of authorship attribution (for an overviewcf.
Stamatatos, 2009) is typically performed with su-perficial features of texts such as sentence length,word frequencies, and use of punctuation & vocabu-lary.
While such methods attain high accuracies (e.g.,Grieve, 2007), the models make purely statistical de-cisions that are difficult to interpret.
To overcomethis we could turn to higher-level patterns of texts,such as their syntactic structure.Syntactic stylometry was first attempted byBaayen et al (1996), who looked at the distribution offrequencies of grammar productions.1 More recently,Raghavan et al (2010) identified authors by derivinga probabilistic grammar for each author and pickingthe author grammar that can parse the unidentified1A grammar production is a rewrite rule that generates aconstituent.SSSNP VP NPVPADJP ADJPADJPPPJJ NNS VBP RB RB : DT JJ NN VBZ JJ IN PRP$NPJJ NNHappy families are all alike ; every unhappy family is unhappy in its own wayFigure 1: A phrase-structure tree produced by the Stanfordparser.text with the highest probability.
There is also workthat looks at syntax on a more shallow level, suchas Hirst and Feiguina (2007), who work with par-tial parses; Wiersma et al (2011) looked at n-gramsof part-of-speech (POS) tags, and Menon and Choi(2011) focussed on particular word frequencies suchas those of ?stop words,?
attaining accuracies wellabove 90% even in cross-domain tasks.In this work we also aim to perform syntactic sty-lometry, but we analyze syntactic parse trees directly,instead of summarizing the data as a set of grammarproductions or a probability measure.
The unit ofcomparison is tree fragments.
Our hypothesis is thatthe use of fragments can provide a more interpretablemodel compared to one that uses fine-grained surfacefeatures such as word tokens.2 MethodWe investigate a corpus consisting of a selection ofnovels from a handful of authors.
The corpus wasselected to contain works from different time periods59SNP VPADJPVBP RB RBare all alikeFigure 2: A phrase-structure fragment from the tree infigure 1.from authors with a putatively distinctive style.
Inorder to analyze the syntactic structure of the corpuswe use hierarchical phrase-structures, which dividesentences into a series of constituents that are repre-sented in a tree-structure; cf.
figure 1 for an example.We analyze phrase-structures using the notion of treefragments (referred to as subset trees by Collins andDuffy, 2002).
This notion is taken from the frame-work of Data-Oriented Parsing (Scha, 1990), whichhypothesizes that language production and compre-hension exploits an inventory of fragments from pre-vious language experience that are used as buildingblocks for novel sentences.
In our case we can sur-mise that literary authors might make use of a specificinventory in writing their works, which characterizestheir style.
Fragments can be characterized as fol-lows:Definition.
A fragment f of a tree T is a connectedsubset of nodes from T , with |f | ?
2, such that eachnode of f has either all or none of the children of thecorresponding node in T .When a node of a fragment has no children, it iscalled a frontier node; in a parsing algorithm suchnodes function as substitution sites where the frag-ment can be combined with other fragments.
Cf.
fig-ure 2 for an example of a fragment.
An importantconsideration is that fragments can be of arbitrarysize.
The notion of fragments captures anything froma single context-free production such as(1) S ?
NP VP.
.
.
to complete stock phrases such as(2) Come with me if you want to live.In other words, instead of making assumptions aboutgrain size, we let the data decide.
This is in contrastto n-gram models where n is an a priori definedsliding window size, which must be kept low becauseAuthor Works(sentences) (year of first publication)Conrad,Joseph(25,889)Heart of Darkness (1899), Lord Jim(1900), Nostromo (1904),The Secret Agent (1907)Hemingway,Ernest(40,818)A Farewell To Arms (1929),For Whom the Bell Tolls (1940),The Garden of Eden (1986),The Sun Also Rises (1926)Huxley,Aldous(23,954)Ape and Essence (1948), BraveNew World (1932), Brave NewWorld Revisited (1958), CromeYellow (1921), Island (1962),The Doors of Perception (1954),The Gioconda Smile (1922)Salinger,J.D.
(26,006)Franny & Zooey (1961), NineStories (1953), The Catcher in theRye (1951), Short stories(1940?1965)Tolstoy,Leo(66,237)Anna Karenina (1877); transl.Constance Garnett, Resurrection(1899); transl.
Louise Maude, TheKreutzer Sonata and Other Stories(1889); transl.
Benjamin R. Tucker,War and Peace (1869); transl.Aylmer Maude & Louise MaudeTable 1: Works in the corpus.
Note that the works byTolstoy are English translations from project Gutenberg;the translations are contemporaneous with the works ofConrad.of data-sparsity considerations.To obtain phrase-structures of the corpus we em-ploy the Stanford parser (Klein and Manning, 2003),which is a treebank parser trained on the Wall Streetjournal (WSJ) section of the Penn treebank (Marcuset al, 1993).
This unlexicalized parser attains an ac-curacy of 85.7 % on the WSJ benchmark (|w| ?
100).Performance is probably much worse when parsingtext from a different domain, such as literature; forexample dialogue and questions are not well repre-sented in the news domain on which the parser istrained.
Despite these issues we expect that usefulinformation can be extracted from the latent hierar-chical structure that is revealed in parse trees, specif-ically in how patterns in this structure recur acrossdifferent texts.60We pre-process all texts manually to strip awaydedications, epigraphs, prefaces, tables of contents,and other such material.
We also verified that no oc-currences of the author names remained.2 Sentenceand word-level tokenization is done by the Stanfordparser.
Finally, the parser assigns the most likelyparse tree for each sentence in the corpus.
No fur-ther training is performed; as our method is memory-based, all computation is done during classification.In the testing phase the author texts from the train-ing sections are compared with the parse trees of textsto be identified.
To do this we modified the fragmentextraction algorithm of Sangati et al (2010) to iden-tify the common fragments among two different setsof parse trees.3 This is a tree kernel method (Collinsand Duffy, 2002) which uses dynamic programmingto efficiently extract the maximal fragments that twotrees have in common.
We use the variant reported byMoschitti (2006) which runs in average linear timein the number of nodes in the trees.To identify the author of an unknown text we col-lect the fragments which it has in common with eachknown author.
In order to avoid biases due to dif-ferent sizes of each author corpus, we use the first15,000 sentences from each training section.
Fromthese results all fragments which were found in morethan one author corpus are removed.
The remainingfragments which are unique to each author are usedto compute a similarity score.We have explored different variations of similarityscores, such as the number of nodes, the average num-ber of nodes, or the fragment frequencies.
A simplemethod which appears to work well is to count thetotal number of content words.4 Given the parse treesof a known author A and those of an unknown authorB, with their unique common fragments denoted asA uB, the resulting similarity is defined as:f(A,B) =?x?AuBcontent words(x)However, while the number of sentences in the train-2Exception: War and Peace contains a character with thesame name as its author.
However, since this occurs in only oneof the works, it cannot affect the results.3The code used in the experiments is available at http://github.com/andreasvc/authident.4Content words consist of nouns, verbs, adjectives, and ad-verbs.
They are identified by the part-of-speech tags that are partof the parse trees.ing sets has been fixed, they still diverge in the aver-age number of words per sentence, which is reflectedin the number of nodes per tree as well.
This causesa bias because statistically, there is a higher chancethat some fragment in a larger tree will match withanother.
Therefore we also normalize for the averagenumber of nodes.
The author can now be guessed as:argmaxA?Authorsf(A,B)1/|A|?t?A |t|Note that working with content words does not meanthat the model reduces to an n-gram model, becausefragments can be discontiguous; e.g., ?he said X butY .?
Furthermore the fragments contain hierarchicalstructure while n-grams do not.
To verify this con-tention, we also evaluate our model with trigramsinstead of fragments.
For this we use trigrams ofword & part-of-speech pairs, with words stemmedusing Porter?s algorithm.
With trigrams we simplycount the number of trigrams that one text shares withanother.
Raghavan et al (2010) have observed thatthe lexical information in n-grams and the structuralinformation from a PCFG perform a complementaryrole, achieving the highest performance when bothare combined.
We therefore also evaluate with acombination of the two.3 Evaluation & DiscussionOur data consist of a collection of novels from fiveauthors.
See table 1 for a specification.
We performcross-validation on 4 works per author.
We evaluateon two different test sizes: 20 and 100 sentences.
Wetest with a total of 500 sentences per work, whichgives 25 and 5 datapoints per work given these sizes.As training sets only the works that are not tested onare presented to the model.
The training sets consistof 15,000 sentences taken from the remaining works.Evaluating the model on these test sets took abouthalf an hour on a machine with 16 cores, employingless than 100 MB of memory per process.
The simi-larity functions were explored on a development set,the results reported here are from a separate test set.The authorship attribution results are in table 2.
Itis interesting to note that even with three differenttranslators, the work of Tolstoy can be successfullyidentified; i.e., the style of the author is modelled, notthe translator?s.6120 sentences trigrams fragments combined 100 sentences trigrams fragments combinedConrad 83.00 87.00 94.00 Conrad 100.00 100.00 100.00Hemingway 77.00 52.00 81.00 Hemingway 100.00 100.00 100.00Huxley 86.32 75.79 86.32 Huxley 89.47 78.95 89.47Salinger 93.00 86.00 94.00 Salinger 100.00 100.00 100.00Tolstoy 77.00 80.00 90.00 Tolstoy 95.00 100.00 100.00average: 83.23 76.16 89.09 average: 96.97 95.96 97.98Table 2: Accuracy in % for authorship attribution with test texts of 20 or 100 sentences.ConradHemingwayHuxleySalingerTolstoyConrad 94 1 2 3Hemingway 3 81 11 5Huxley 5 2 82 1 5Salinger 1 2 3 94Tolstoy 8 2 90Table 3: Confusion matrix when looking at 20 sentenceswith trigrams and fragments combined.
The rows are thetrue authors, the columns the predictions of the model.Gamon (2004) also classifies chunks of 20 sen-tences, but note that in his methodology data fortraining and testing includes sentences from the samework.
Recognizing the same work is easier becauseof recurring topics and character names.Grieve (2007) uses opinion columns of 500?2,000words, which amounts to 25?100 sentences, as-suming an average sentence length of 20 words.Most of the individual algorithms in Grieve (2007)score much lower than our method, when classify-ing among 5 possible authors like we do, while theaccuracies are similar when many algorithms arecombined into an ensemble.
Although the corpusof Grieve is carefully controlled to contain compa-rable texts written for the same audience, our taskis not necessarily easier, because large differenceswithin the works of an author can make classifyingthat author more challenging.Table 3 shows a confusion matrix when workingwith 20 sentences.
It is striking that the errors arerelatively asymmetric: if A is often confused withB, it does not imply that B is often confused withA.
This appears to indicate that the similarity metrichas a bias towards certain categories which could beremoved with a more principled model.Here are some examples of sentence-level and pro-ductive fragments that were found:(3) Conrad: [PP [IN ] [NP [NP [DT ] [NN sort ] ][PP [IN of ] [NP [JJ ] [NN ] ] ] ] ](4) Hemingway: [VP [VB have ] [NP [DT a ] [NNdrink ] ] ](5) Salinger: [NP [DT a ] [NN ] [CC or ] [NN some-thing ] ](6) Salinger: [ROOT [S [NP [PRP I ] ] [VP [VBPmean ] [SBAR ] ] [.
. ]
] ](7) Tolstoy: [ROOT [SINV [?
? ]
[S ] [, , ] [?
?
][VP [VBD said ] ] [NP ] [, , ] [S [VP [VBGshrugging ] [NP [PRP$ his ] [NNS shoulders ]] ] ] [.
. ]
] ]It is likely that more sophisticated statistics, for exam-ple methods used for collocation detection, or generalmachine learning methods to select features such assupport vector machines would allow to select onlythe most characteristic fragments.4 ConclusionWe have presented a method of syntactic stylome-try that is conceptually simple?we do not resortto sophisticated statistical inference or an ensembleof algorithms?and takes sentence-level hierarchicalphenomena into account.
Contrary to much previouswork in stylometry, we worked with content wordsrather than just function words.
We have demon-strated the feasibility of analyzing literary syntaxthrough fragments; the next step will be to use thesetechniques to address other literary questions.62ReferencesHarold Baayen, H. Van Halteren, and F. Tweedie.1996.
Outside the cave of shadows: Using syn-tactic annotation to enhance authorship attribution.Literary and Linguistic Computing, pages 121?132.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
InProceedings of ACL.Michael Gamon.
2004.
Linguistic correlates of style:authorship classification with deep linguistic anal-ysis features.
In Proceedings of COLING.Jack Grieve.
2007.
Quantitative authorship at-tribution: An evaluation of techniques.
Lit-erary and Linguistic Computing, 22(3):251?270.
URL http://llc.oxfordjournals.org/content/22/3/251.abstract.Graeme Hirst and Olga Feiguina.
2007.
Bi-grams of syntactic labels for authorshipdiscrimination of short texts.
Literary andLinguistic Computing, 22(4):405?417.
URLhttp://llc.oxfordjournals.org/content/22/4/405.abstract.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proc.
of ACL,volume 1, pages 423?430.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational linguistics, 19(2):313?330.Rohith K Menon and Yejin Choi.
2011.
Domainindependent authorship attribution without domainadaptation.
In Proceedings of Recent Advances inNatural Language Processing, pages 309?315.Alessandro Moschitti.
2006.
Making tree kernelspractical for natural language learning.
In Pro-ceedings of EACL, pages 113?120.
URL http://acl.ldc.upenn.edu/E/E06/E06-1015.pdf.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using prob-abilistic context-free grammars.
In Proceedings ofACL, pages 38?42.Federico Sangati, Willem Zuidema, and Rens Bod.2010.
Efficiently extract recurring tree fragmentsfrom large treebanks.
In Proceedings of LREC,pages 219?226.
URL http://dare.uva.nl/record/371504.Remko Scha.
1990.
Language theory and languagetechnology; competence and performance.
InQ.A.M.
de Kort and G.L.J.
Leerdam, editors, Com-putertoepassingen in de Neerlandistiek, pages7?22.
LVVN, Almere, the Netherlands.
Orig-inal title: Taaltheorie en taaltechnologie; com-petence en performance.
Translation available athttp://iaaa.nl/rs/LeerdamE.html.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Amer-ican Society for Information Science and Technol-ogy, 60(3):538?556.
URL http://dx.doi.org/10.1002/asi.21001.Wybo Wiersma, John Nerbonne, and Timo Laut-tamus.
2011.
Automatically extracting typi-cal syntactic differences from corpora.
Lit-erary and Linguistic Computing, 26(1):107?124.
URL http://llc.oxfordjournals.org/content/26/1/107.abstract.63
