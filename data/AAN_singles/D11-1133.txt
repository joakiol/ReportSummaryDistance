Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1437?1446,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsExtreme Extraction -- Machine Reading in a WeekMarjorie Freedman, Lance Ramshaw, Elizabeth Boschee, Ryan Gabbard,Gary Kratkiewicz, Nicolas Ward, Ralph WeischedelRaytheon BBN Technologies10 Moulton St.Cambridge, MA 02138mfreedma,lramshaw,eboschee,rgabbard,kratkiewicz,nward,weischedel@bbn.comThe views expressed are those of the author and do not reflect the official policy or position of the Depart-ment of Defense or the U.S. Government.
This is in accordance with DoDI 5230.29, January 8, 2009.AbstractWe report on empirical results in extremeextraction.
It is extreme in that (1) from re-ceipt of the ontology specifying the targetconcepts and relations, development is li-mited to one week and that (2) relativelylittle training data is assumed.
We are ableto surpass human recall and achieve an F1of 0.51 on a question-answering task withless than 50 hours of effort using a hybridapproach that mixes active learning, boot-strapping, and limited (5 hours) manualrule writing.
We compare the performanceof three systems: extraction with handwrit-ten rules, bootstrapped extraction, and acombination.
We show that while the recallof the handwritten rules surpasses that ofthe learned system, the learned system isable to improve the overall recall and F1.1 IntroductionThroughout the Automatic Content Extraction 1(ACE) evaluations and the Message UnderstandingConferences2 (MUC), teams typically had a year ormore from release of the target to submitting sys-tem results.
One exception was MUC-6 (Grishman& Sundheim, 1996), in which scenario templatesfor changing positions were extracted given onlyone month.
Our goal was to confine developmentto a calendar week, in fact, <50 person hours.
This1 http://www.nist.gov/speech/tests/ace/2 http://www-nlpir.nist.gov/related_projects/muc/is significant in two ways: the less effort it takes tobring up a new domain, (1) the more broadly ap-plicable the technology is and (2) the less effortrequired to run a diagnostic research experiment.Our second goal concerned minimizing trainingdata.
Rather than approximately 250k words ofentity and relation annotation as in ACE, only ~20example pairs per relation-type were provided astraining.
Reducing the training requirements hasthe same two desirable outcomes: demonstratingthat the technology can be broadly applicable andreducing the overhead for running experiments.The system achieved recall of 0.49 and precisionof 0.53 (for an F1 of 0.51) on a blind test set of 60queries of the form Ri(arg1, arg2), where Ri is oneof the 5 new relations and exactly one of arg1 orarg2 is a free variable for each query.Key to this achievement was a hybrid of:?
a variant of (Miller, et al, 2004) to learn twonew classes of entities via automatically inducedword classes and active learning (6 hours)?
bootstrap relation learning (Freedman et al2010) to learn 5 new relation classes (2.5 hours),?
handwritten patterns over predicate-argumentstructure (5 hours), and?
coreference (20 hours)Our bootstrap learner is initialized with relationtuples (not annotated text) and uses LDC?s Giga-word and Wikipedia as a background corpus tolearn patterns for relation detection that are basedon normalized predicate argument structure as wellas surface strings.These early empirical results suggest the follow-ing: (1) It is possible to specify a domain, adaptour system, and complete manual scoring, includ-1437ing human performance, within a month.
Experi-ments in machine reading (and in extraction) canbe performed much more quickly and cheaply thanever before.
(2) Through machine learning andlimited human pattern writing (6 hours), weadapted a machine reading system within a week(using less than 50 person hours), achieving ques-tion answering performance with an F1 of 0.5 andwith recall 11% higher (relative) to a human read-er.
(3) Unfortunately, machine learning, thoughachieving 80% precision,3 significantly lags behinda gifted human pattern writer in recall.
Thus, boot-strap learning with much higher recall at minimalsacrifice in precision is highly desirable.2 Related WorkThis effort is evaluated extrinsically via formalquestions expressed as a binary relation with onefree variable.
This contrasts with TREC QuestionAnswering, 4  where the questions are in naturallanguage, and not restricted to a single binary rela-tion.
Like the ?list?
queries of TREC QA, the re-quirement is to find all answers, not just one.Though question interpretation is not required inour work, interpretation of the text corpus is.The goal of rapid adaptation has been tested inother contexts.
In 2003, a series of experiments inadapting to a new language in less than monthtested system performance on Cebuano and Hindi.The primary goal was to adapt to a new language,rather than a new domain.
The extraction partici-pants focused on named-entity recognition, notrelation extraction (May, et al 2003; Sekine &Grishman, 2003; Li & McCallum, 2003; Maynardet al 2003).
The scenario templates of MUC-6(Grishman & Sundheim, 1996) are more similar toour relation extraction task, although the domain isquite different.
Our experiment allowed for 1 weekof development time, while MUC-6 allowed amonth.
The core entities in the MUC-6 task(people and organizations) had been worked onpreviously.
In contrast all of our relations includedat least one novel class.
While MUC-6 systemstended to use finite-state patterns, they did not in-corporate bootstrapping or patterns based on theoutput of a statistical parser.3 Handwritten patterns achieved 52% precision.4 http://trec.nist.gov/data/qamain.htmlFor learning entity classes, we follow Miller, etal., (2004), using word clustering and active learn-ing to train a perceptron model, but unlike thatwork we apply the technique not just to names butalso to descriptions.
An alternative approach tolearning classes, applying structural patterns tobootstrap description recognition without activelearning, is seen in Riloff (1996) and Kozareva etal., (2008)Much research (e.g.
Ramshaw 2001) has fo-cused on learning relation extractors using largeamounts of supervised training, as in ACE.
Theobvious weakness of such approaches is the result-ing reliance on manually annotated examples,which are expensive and time-consuming to create.Others have explored bootstrap relation learn-ing from seed examples.
Agichtein & Gravano(2000) and Ravichandran & Hovy (2002) reportedresults for generating surface patterns for relationidentification; others have explored similar ap-proaches (e.g.
Pantel & Pennacchiotti, 2006).
Mit-chell et al (2009) showed that for macro-reading,precision and recall can be improved by learning alarge set of interconnected relations and conceptssimultaneously.
None use coreference to find train-ing examples; all use surface (word) patterns.Freedman et.
al (2010) report improved perfor-mance from using predicate structure for boot-strappped relation learning.Most approaches to automatic pattern genera-tion have focused on precision, e.g., Ravichandranand Hovy (2002) report results in TREC QA,where extracting one instance of a relation can besufficient, rather than detecting all instances.
Mit-chell et al (2009), while demonstrating high preci-sion, do not measure recall.By contrast, our work emphasizes recall, notjust precision.
Our question answering task askslist-like questions that require multiple answers.We also include the results of a secondary, extrac-tion evaluation which requires that the systemidentify every mention of the relations in a smallset of documents.
This evaluation is loosely basedon the relation mention detection task in ACE.3 Task Set-Up and EvaluationOur effort was divided into four phases.
During thefirst phase, a third party produced an ontology andthe resources, which included: brief (~1 paragraph)guidelines for each relation and class in the ontolo-1438gy; ~20 examples for each relation in the ontology;2K documents that are rich in domain relations.Table 1 lists the 5 new relations and number of ex-amples provided for each.
Arguments in italicswere known by the system prior to the evaluation.Relation Ex.possibleTreatment(Substance, Condition) 23expectedDateOnMarket(Substance, Date) 11responsibleForTreatment(Substance, Agent) 19studiesDisease(Agent, Condition) 16hasSideEffect(Substance, Condition) 27Table 1: New Relations and Number of ExamplesIn phase two, we spent one week extending ourextraction system for the new ontology.
During thethird phase, we ran our system over 10K docu-ments to extract all instances of domain relationsfrom those documents.
In the fourth phase, ourquestion answering system used the extracted in-formation to answer queries.4 Approach to Domain SpecializationOur approach to extracting domain relations inte-grated novel relation and class detectors into anexisting extraction system, designed primarilyaround the ACE tasks.
The existing system uses adiscriminatively trained classifier to detect the enti-ty and value types of ACE.
It also produces a syn-tactic parse for each sentence; normalizes theseparses to find logical predicate argument structure;and detects and coreferences pronominal, nominal,and name mentions for each of the 7 ACE entitytypes (Person, Organization, Geopolitical Entity,Location, Facility, Weapon, and Vehicle).5The extraction system has three components thatallow for rapid adaptation to a new domain:?
Class detectors trained using word classes de-rived from unsupervised clustering and sentence-selected training data.?
A bootstrap relation learner which given a fewseed examples learns patterns that indicate thepresence of relations.?
An expressive pattern language which allows adeveloper to express rules for relation extractionin a simple, but fast manner.Component Approach EffortClass Recognizer Active Learning 6 hrs5 The extraction system detects relations and events in theACE ontology, but these were not used in the current work.Class Recognizer Web-Mined List 1 hrsRelation RecognizerSemi-supervisedBootstrapping8.5 hrsRelation Recognizer Manual Patterns 5 hrsCoreference Heuristics 20 hrsTable 2: Effort and Approach for New Domain4.1 Class ExtractionEach of the relations in the new domain included atleast one argument that was new.
While questionanswering requires the system to identify theclasses only when they appear in a relation, know-ledge of when a class is present provides importantinformation for relation extraction.
For example inour ontology, Y is a treatment for X only if Y is asubstance.
Thus, ?Group counseling sessions areeffective treatments for depression?
does not con-tain an instance of possibleTreatment(), while?SSRIs are effective treatments for depression?does.
The bootstrap learner allows constraintsbased on argument type.
To use this capability, wetrained the recognizer at the beginning of the weekof domain adaptation and used the predictedclasses during learning.We annotated 1064 sentences (~31K words) us-ing active learning combined with unsupervisedword clusters (Miller,et al, 2004) for the followingclasses: Substance-Name, Substance-Description,Condition-Name, and Condition-Description.
Ge-neric noun-phrases like new drugs, the illness, etcwere labeled as descriptors.
Because of the timeframe, we did not develop extensive guidelines normeasure inter-annotator agreement.
Annotationtook 6 hours.
We supplemented our annotationwith lists of substances and treatments from theweb, which took 1 hour.4.2 CoreferenceProviding a name reference is generally preferableto a non-specific string (e.g.
the drugs), but notalways feasible; for instance, reports of new re-search may appear without a name for the drug.Our existing system?s coreference algorithms op-erate only on mentions of ACE entity types (per-sons, organizations, GPEs, other locations,facilities, vehicles, and weapons).
During the weekof domain adaption we developed new heuristicsfor coreference over non-ACE types.
Most of ourheuristics are domain independent (e.g.
linking theparts of an appositive).
Our decision to annotatenames and descriptions separately was driven par-1439tially by the need to select the best reference (i.e.name) for co-referent clusters.
Adding coreferenceheuristics for the two new entity types was the sin-gle most time-consuming activity, taking 20 of thetotal 43 hours.4.3 Relation ExtractionFor relation extraction, we used both patternlearning and handwritten patterns.
We initializedour bootstrap relation learner with the exampleinstances provided with the domain ontology; Ta-ble 3 includes examples of the instances providedto the system as training.
Our bootstrap relationlearner finds instances of the relation argumentpairs in text and then proposes both predicate-argument structure and word-based connectionsbetween the arguments as possible new patterns forthe relation.
The learner automatically prunes po-tential patterns using information about the numberof known-to-be true and novel instances matchedby a proposed pattern.
By running the pattern ex-tractor over a large corpus, the proposed patternsgenerate new seeds which are in turn are used topropose new patterns.
For this experiment, we in-corporated a small amount of supervision duringthe bootstrapping process (roughly 1 hour total perrelation); we also performed ~30 minutes total inpruning domain patterns at the end of learning.Relation Arg-1 Arg-2possTreatmnt AZT AIDSstudyDisease Dr Henri Joyeux cancerstudyDisease Samir Khleif cancerTable 3: Sample Instances for Initializing LearnerWe also used a small amount of human effortcreating rules for detecting the relations.
The pat-tern writer was given the guidelines, the examples,and a 2K document background corpus and spent 1hour per relation writing rules.The learned patterns use a subset of the full pat-tern language used by the pattern-writer.
The lan-guage operates over surface-strings as well aspredicate-argument structure.
Figure 1 illustrateslearned and handwritten patterns for the possible-TreatmentRelation().
The patterns in rectanglesmatch surface-string patterns; the tree-like patternsmatch normalized predicate argument structure.The ?WORD- token indicates a wild card of 1-3words.
The blue rectangles at the root of the treesin the handwritten patterns are sets of predicatesthat can be matched by the pattern.5 EvaluationOur question answering evaluation was inspiredby the evaluation in DARPA?s machine readingprogram, which requires systems to map the in-formation in text into a formal ontology and an-swer questions based on that ontology.
UnlikeACE, this allows evaluators to measure perfor-mance without exhaustively annotating documents,allows for balance between rare and common rela-tions, and implicitly measures coreference withoutrequiring explicit annotation of answer keys forcoreference.
However because the evaluation onlymeasures performance on the set of queries, manyrelation instances will be unscored.
Furthermore,the system is not rewarded for finding the samerelation multiple times; finding 100 instances ofisPossibleTreatment(Penicillin, Strep Throat) isthe same as finding 1 (or 10) instances.Figure 1: Sample Patterns for possibleTreatment()The evaluation included only queries of the typeFind all instances for which the relation P(X, Z) istrue where one of X or Z is constant.
For example,Find possible treatments for diabetes; or What isexpected date to market for Abilify?
There were 60queries in the evaluation set to be answered from a10K document corpus.
To produce a preliminaryanswer key, annotators were given the queries andcorpus indexed by Google Desktop.
Annotatorswere given 1 hour to find potential answers to eachquery.
If no answers were found after 1 hour, theannotators were given a second hour to look foranswers.
For two queries, both of the form Findtreatments with an expected date to market of MM-YYYY, even after two hours of searching the anno-tators were unable to find any answers.6Annotator answers served as the initial gold-standard.
Given this initial answer key, annotatorsreviewed system answers and aligned them withgold-standard answers.
System output not alignedwith the initial gold standard was assessed as cor-rect or incorrect.
We assume that the final gold-standard constitutes a complete answer key, and6 Evaluators wanted some queries with no answers.1440are thus able to calculate recall for our system andfor humans7.
Because we had only one annotatorfor each query and because we assumed that anyanswer found by an annotator was correct, wecould not estimate human precision on this task.Answers can be specific named concepts (e.g.Penicillin) or generic descriptions (e.g.
drug, ill-ness).
Given the sentence, ACME produces a widerange of drugs including treatments for malariaand athletes foot,?
our reading system would ex-tract the relations responsibleForTreatment(drugs,ACME), possibleTreatment(drugs, malaria), pos-sibleTreatment(drugs, athletes foot).
When a namewas available in the document, annotators markedthe answer as correct, but underspecified.
We cal-culated precision and recall treating underspecifiedanswers as incorrect and separately calculated pre-cision and recall counting underspecified answersas correct.
When treated as correct, there was lessthan a 0.05 absolute increase in both precision andrecall.
Unless otherwise specified, all scores re-ported here use the stricter condition which treatsunderspecified answers as incorrect.We also evaluated extracting all information in asmall document collection (here human search ofthe 10k documents does not play a role in findinganswers).
Individuals were asked to annotate everyinstance of the 5 relations in a set of 102 docu-ments.
Recall, Precision, and F were calculated byaligning system responses to the answer key.
Sys-tem answers that aligned are correct; those that didnot are incorrect; and answers in the key that werenot found by the system are misses.
Unlike thequestion answering evaluation, this evaluationmeasures the ability to find every instance of afact.
If the gold standard includes 100 instances ofisPossibleTreatment(Penicillin, Strep Throat), re-call will decrease for each instance missed.
The?extraction?
evaluation does not penalize systemsfor missing coreference.6 Results6.1 Class Detection7 The answer key may contain some answers that were foundneither by the annotator nor by the systems described here,since the answer key includes answers pooled from other sys-tems not reported in this paper.
The system reported here wasthe highest performing of all those participating in the experi-ment.
Furthermore, if a system answer is marked as correct,but underspecified, the specific  answer is put in the key.The recall, precision, and F1 for class detectionusing 10-fold cross validation of the ~1K anno-tated sentences appear in the 3-5th columns of Table4.
Given the amount of training, our results arelower than in Miller et al(2004) (an F1 of 90 withless than 25K words of training).
Several factorscould explain this: Finding boundaries and typesfor descriptions is more complex than for names inEnglish.
8  Our classes, pharmaceutical substancesand physiological conditions, may have been moredifficult to learn.
Our classes are less common innews reporting; as such, both word-class clustersand active learning may have been less effective.Finally, our evaluation was done on a 10-fold splitof the active-learning selected data; bias in select-ing the data could explain at least a part of ourlower performance.Type# inGSWithout Lists With ListsR P F R P FSubst-D 789 77 85 80.8 78 85 81.3Subst-N 410 70 82 75.5 77 81 78.9Cond-D 427 72 78 74.9 72 77 74.4Cond-N 963 80 87 83.4 84 83 83.5Table 4: Cross Validation:  Condition & SubstanceWe noticed that the system frequently reportedcountry names to be substance-names.
Surprising-ly, we found that our well-trained name findermade the opposite mistake, occasionally reportingdrugs as geo-political entities.We incorporated lists of known substances andconditions to improve recall.
Performance on thesame cross-validation split is shown in the finalthree columns of Table 4.
Incorporating the lists ledto recall gains for both substance-name and condi-tion-name.
Because a false-alarm in class recogni-tion only leads to an incorrect relation extraction ifit appears in a context indicating a domain relation,false alarms of classes may be less important in thequestion answering and extraction evaluations.6.2 Question Answering and ExtractionFigure 2 and Table 6 show system performanceusing only handwritten rules (HW), only learnedpatterns (L), and combining both (C).
Figure 2includes scores calculated with all of the systems?answers (in the dotted boxes), and with just thoseanswers that were deemed useful (discussed be-8 English names are capitalized; person names have a typicalform and are frequently signaled by titles; organization namesfrequently have clear signal words, such as Corp.1441low).
We include annotator recall.
Handwrittenpatterns outperform learned patterns consistentlywith much higher recall.
Encouragingly, however,1.
The combined system?s recall and F-Scoreare noticeably higher for 3 of the relations.2.
The learned patterns generate answers notfound by handwritten patterns.3.
The learned patterns have high precision.9There is variation across the different relations.The two best performing relations possibleTreat-ment() and studiesDisease() have F1 more thantwice as high as the two worst performing rela-tions, expectedDateToMarket() and hasSideEf-fect().
This is primarily due to differences in recall.Figure 2: Overall Q/A Performance: All answers indotted boxes; 'Useful Answers' unboxedThe combined system?s recall (0.49), while low,is higher than that of the annotators (0.44).
Whilehardly surprising that a machine can process in-formation much more quickly than a person, it isencouraging that higher recall is achieved evenwith only one week?s effort.
In the context of ourpooled answer-key, the relatively low recall ofboth the system and the annotator suggests thatthere was little overlap between the answers foundby the annotator and those found by the system.As already described, the system answers caninclude both specific references (e.g.
Prozac) andmore generic references (the drug).
When a morespecific answer is present in the document, genericreferences have been treated as incorrect.
Howev-er, sometimes there is not a more specific refer-ence; for example an article written before a drughas been released may never name the drug.
Scoresreported thus far treat such answers as correct.These answers would be useful when answeringmore complex queries.
For example, given the sen-9 The learned patterns' high precision is to be expected for tworeasons.
First, a few bad patterns were manually removed foreach relation.
More importantly, the learning algorithm strong-ly favors high precision patterns because it needs to maintain aseed set with low noise in order to learn effectively.tence ?ACME spent 5 years developing a pill totreat the flu which it will release next week,?
ex-tracting relations involving ?the pill?
would allowa system to answer questions that use multiple rela-tions in the ontology to for example ask about  or-ganizations developing treatments for the flu, orthe expected date of release for ACME?s drugs.However, in our simple question answeringframework such generic answers never conveynovel information and thus were probably ignoredby human annotators.To measure the impact of treating these genericreferences as correct,10 we did additional annota-tion on the correct answers, marking answers as?useful?
(specific) and ?not-useful?
(generic).
Theunboxed bars in Figure 2 show performance when?not-useful?
answers are removed from the answer-key and the responses.
For the four relations wherethere was a change Table 5 provides the relativechange performance when only ?useful?
answersare considered.
The annotator?s recall increasesnoticeably while the combined system?s drops.This results in the overall recall of annotators sur-passing that of the combined system.RelationRecall PrecisionA C H L C H LpossTreat 12 10 10 14 -10 -11 -3respTreat 9 0 -5 8 -4 -4 -1studyDis 12 -6 -9 13 -11 -13 0hasSidEff 3 4 4 4 0 0 0Total 11 -2 -4 6 -9 -10 -2Table 5: Relative Change in Recall and Precision WhenNon-Useful Answers are RemovedTable 7 shows the total number of answers pro-duced by annotators and by each system, as well asthe percentage of queries with at least one correctanswer for each system.
For one relation expec-tedDateOnMarket(), the learned system did notfind any answers.
This relation had far fewer an-swers found by annotators and occurred far morerarely in the fully annotated extraction set (see Ta-ble 8).
Anecdotally, extracting this relation fre-quently required co-referencing ?it?
(e.g.
?It will bereleased in March 2011?).
Our heuristics for core-ference of the new classes did not account for pro-nouns.
Learning from such examples wouldrequire coreference during bootstrapping.
Mostlikely, the learned system was unable to generateenough novel instances to continue bootstrapping10 Generic answers were treated as correct only if a more spe-cific reference was not available in the document.1442and was thus unable to learn the relation.Relation Type(# Queries; # Correct Ans.
)Recall Precision FA C HW L C HW L C HW LpossTreatment (10;247) 0.27 0.63 0.50 0.34 0.51 0.47 0.83 0.56 0.48 0.48respForTreat (15;134) 0.73 0.33 0.24 0.22 0.66 0.78 0.73 0.44 0.37 0.33expectDateMarkt (11;60) 0.90 0.17 0.17 0.00 0.77 0.83 0.00 0.27 0.28 0studiesDisease (13;292) 0.23 0.67 0.59 0.09 0.51 0.50 0.79 0.58 0.54 0.16hasSideEffect (11;104) 0.80 0.10 0.13 0.02 0.83 0.70 1.00 0.17 0.23 0.04Total (60;837) 0.44 0.49 0.42 0.17 0.53 0.52 0.80 0.51 0.46 0.28Table 6: Question Answering Results by Relation TypeRelation TypeTotal Number of Answers % Queries with At Least 1 Corr.
AnsA C HW L A C HW LpossTreatment  66 303 261 100 100.0% 90.0% 90.0% 90.0%respForTreat  98 67 41 40 100.0% 66.7% 60.0% 60.0%expectDateMarkt  54 13 12 0 72.7% 45.5% 45.5% 0.0%studiesDisease  68 379 347 33 100.0% 61.5% 46.2% 46.2%hasSideEffect  83 12 20 2 72.7% 36.4% 45.5% 18.2%Total  369 774 681 175 90.0% 60.0% 56.7% 43.3%Table 7: Number of Answers and Number of Queries AnsweredOverall, the system did better on relations hav-ing more correct answers.
Bootstrap learning hasan easier time discovering new instances and newpatterns when there are more examples to workwith.
Even a human pattern writer will have moreexamples to generalize from for common relations.While possibleTreatment() and hasSideEffect()have similar F-scores, their performance is verydifferent at the query level.
The system was able tofind at least one correct answer to every possible-Treatment() query; however only 72.7% of the stu-diesDisease() queries were answered.Table 8 presents results from the extractionevaluation where a set of ~100 documents wereannotated for all mentions of the 5 relations.
Be-cause every mention in the document set must befound, the system cannot rely on finding the easiestanswers for common relations.
The results in Table8 are significantly lower than for the question ans-wering tasks; yet some of the same trends arepresent.
Handwritten rules outperform learned pat-terns.
For at least some relations, the combinationof the two improves performance.
The three rela-tions for which the learned system has the lowestperformance on the question-answering task havethe fewest instances annotated in the document set.Fewer instance in the large corpus make bootstrap-ping more difficult?the learner is less able to gen-erate novel instances to expand its pattern set.7 Discussion7.1 Sources of ErrorThe most common source of error is pattern cover-age.
In the following figure, the system identifiedresponsibleForTreatment(Janssen Pharmaceutical,Sporanox), but missed the corresponding relationbetween Novartis and Lamisil.Relation Type # Relations Found Recall Precision FGS C HW L C HW L C HW L C HW LpossibleTreatment 518 225 187 68 0.15 0.10 0.09 0.34 0.28 0.66 0.21 0.15 0.15respForTreatment 387 101 77 36 0.10 0.08 0.05 0.41 0.40 0.50 0.17 0.13 0.08expDateOnMarket 66 13 13 0 0.06 0.06 0.00 0.31 0.31 0.00 0.10 0.10 0.00studiesDisease 136 95 91 4 0.08 0.09 0.00 0.12 0.13 0.00 0.10 0.11 0.00hasSideEffect 256 26 25 2 0.04 0.04 0.00 0.39 0.40 0.50 0.07 0.07 0.01Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the RelationsSporanox is made by Janssen Pharmaceutica Inc.,of Titusville, N.J. Lamisil is a product of NovartisPharmaceuticals of East Hanover, N.J.1443Missed class instances contribute to errors, some-times originating in errors in tokenization (e.g.
notremoving the ?_?
in each drug name in a bulletedlist of the form ?_Trovan, an antibiotic...; etc.
)However, many drug-names are simply missed:The system correctly identifies Rebif and Ariceptas drugs, but misses Pregabalin and Serono.
Inboth misses, the immediately preceding and fol-lowing words provide little evidence that the wordrefers to a drug rather than some other product.Substance detection might be better served with aweb-scale, list-learning approach like the doublyanchored patterns described in (Kozareva et al,2008).
Alternatively, our approach may need to beextended to include a larger context window.7.2 Learned PatternsOne of the ways in which learned patterns supple-ment handwritten ones is learning highly specificsurface-string patterns that are insensitive to errorsin parsing.
Figure 3 illustrates two examples ofwhat appear to be easy cases of possibleTreat-ment().
Because the handwritten patterns are notexhaustive and make extensive use of syntacticstructure, parse errors prevented the system basedon handwritten rules from firing.
Learned surface-string patterns were able to find these relations.Even when the syntactic structure is correct,learned patterns capture expressions not commonenough to have been noticed by the rule writer.
Forexample, while the handwritten patterns included?withdrew?
as a predicate indicating a companywas responsible for a drug, they did not include?pulled.?
By including ?pulled?, learned patternsextracted responsibleForTreatment() from ?Ameri-can Home Products pulled Duract, a painkiller.
?Similarly, the learned patterns include an explicitpattern ?CONDITION drug called SUBSTANCE?,and thus extracted a possibleTreatment() relationfrom ?newly approved narcolepsy drug calledmodafinil?
without relying on the coreferencecomponent to link drug to modafinil.Handwritten PatternsDespite the examples above of successfully learnedpatterns, handwritten patterns perform significantlybetter.
In the active-learning context used for theseexperiments, the handwritten rules also requiredless manual effort.
This comparison is not entirelyfair-- while learned patterns required more hours,supervising the bootstrapping algorithm requiresno training.
The handwritten patterns, in contrast,require a trained expert.Figure 3: Extractions Missed by Handwritten Rules &the Erroneous Parses that Hid themWhile handwritten rules and learned patterns usethe same language, they make use of it differently.The handwritten patterns group similar conceptstogether.
A human pattern writer adds relevantsynonyms, as well as words that are not synonym-ous but in the pattern context can be used inter-changeably.
In Figure 4, the handwritten patternsinclude three word-sets: (patient*, people, partici-pant*); (given, taken, took, using); and (report*,experience*, develop*, suffer*).
The ?*?
serves as awild-card to further generalize a pattern.
The word-sets in Figure 4 illustrate challenges for a learnedsystem: the words are not synonyms, but rather arewords that can be used to imply the relation.A human pattern writer frequently generatesnew classes not in the domain ontology.
In Figure4, the circled patterns form a class of ?people tak-ing a substance.?
The handwritten patterns for stu-diesDisease() include classes targeting scientistsand researchers.
These classes are not necessarilytriggered by nouns.
Such classes allow the patternwriter to include complex patterns as in Figure 4and to write relatively precise, but open-ended pat-terns such as: if there is a single named-drug and anamed, non-side-effect disease in the same sen-tence, the drug is a treatment for the disease.Pfizer also hopes to introduce Pregabalin nextyear for treatment of neuropathic pain, epilepsyand anxiety?Other deals include co-promotingRebif for multiple sclerosis with its discoverer,Serono, and marketing Aricept for Alzheimer'sdisease with its developer, Eisai Co.1444Figure 4: Learned and Handwritten Patterns forhasSideEffect()A final difference between handwritten andlearned patterns is the level of predicate-argumentcomplexity used.
In general, handwritten patternsaccount for larger spans of predicate argumentstructure while learned patterns tend to limit them-selves to the connections between the arguments ofthe relation with minor extensions.8 Conclusions and Lessons LearnedFirst, it is encouraging that the synthesis of learn-ing algorithms and handwritten algorithms canachieve an F1 of 0.51 in a new domain in a week(<50 hours of effort).
Second, it is exciting that solittle training data is required: ~20 relation pairsout of context (~2.5 hours of effort) and ~6 hoursof active learning for the new classes.Third, the effectiveness of learning algorithms isstill not competitive with handwritten patternsbased on predicate-argument structure (~5 hours ofeffort on top of active learning for entities).Though the learned patterns have high precision(0.80 on average), recall is low (0.17) and variedgreatly across the relations.
Though the dominantfactor in missing relations is pattern coverage,missing instances of classes contributed to low re-call.
Comparing learned patterns to manually writ-ten patterns, (1) synonyms or other lexicalalternatives that a human pattern writer would in-clude, (2) the creation of subclasses for argumenttypes, and (3) the scope of patterns11 are each ma-jor sources of the disparity in coverage.
Researchon learning approaches to raise recall without sig-nificant sacrifice in precision seems essential.Fourth, despite the disparity in performance oflearned versus manual patterns, and despite the low11 Learned patterns tend to focus on the structure that appearsbetween the two arguments, rather than structure surroundingthe left and right arguments.recall of learned patterns, the combined system?srecall and F-Score are higher for three of the rela-tions because the learned patterns generated an-swers not found by handwritten patterns.
We foundexamples where highly specific, learned, surface-level patterns (lexical patterns) occasionally foundinformation missed by handwritten patterns due toparsing errors or general low coverage.Fifth, the effort for coreference was the mosttime-consuming, given that every new relationcontained at least one of the new argument types.While we included this in our estimate of domainadaptation, the infrastructure we built is domaingeneric.
Improving generic coreference will reducedomain specific effort in future.Perhaps most significant of all, running a com-plete experiment from definition of the domainthrough creation of training data and measurementof end-to-end performance of the system can becompleted in a month.
The ability to rapidly,cheaply, and empirically measure the impact ofextraction research could prove a significant spurto research across the board.These experiments suggest three possible direc-tions for improving the ability to quickly developinformation extraction technology for a new set ofrelations: (1) reducing the amount of supervisionprovided to the bootstrap-learner; (2) improvingthe bootstrapping approach to reach the level ofrecall achieved by the human pattern writer elimi-nating the need for a trained expert during domainadaptation; and (3) focusing improvements to thebootstrapping approach on techniques that allow itto find more of the instances missed by the patternwriter, thus improving the accuracy of the hybridsystem.AcknowledgmentsThis work was supported, in part, by DARPA un-der AFRL Contract FA8750-09-C-179.
Distribu-tion Statement ?A?
(Approved for Public Release,Distribution Unlimited) Thank you to the review-ers for your insightful comments and to MichelleFranchini for coordinating the assessment effort.ReferencesE.
Agichtein and L. Gravano.
Snowball: extracting rela-tions from large plain-text collections.
In Proceed-ings of the ACM Conference on Digital Libraries, pp.85-94, 2000.1445A.
Blum and T. Mitchell.
Combining Labeled and Un-labeled Data with Co-Training.
In Proceedings of the1998 Conference on Computational LearningTheory, July 1998.E.
Boschee, V. Punyakanok, R. Weischedel.
An Explo-ratory Study Towards ?Machines that Learn to Read?.Proceedings of AAAI BICA Fall Symposium, No-vember 2008.J.
Chen, D. Ji, C. Tan and Z. Niu.
(2006).
Relation ex-traction using label propagation based semi-supervised learning.
COLING-ACL 2006: 129-136.July 2006.M.
Freedman, E. Loper, E. Boschee, and R. Weischedel.Empirical Studies in Learning to Read.
Proceedingsof NAACL 2010 Workshop on Formalisms and Me-thodology for Learning by Reading, pp.
61-69, June2010.W.
Li and A. McCallum.
Rapid development of Hindinamed entity recognition using conditional randomfields and feature induction.
Transactions on AsianLanguage Information Processing (TALIP), Volume2 Issue 3  September, 2003.R Grishman and B. Sundheim.
Message UnderstandingConference-6 : A Brief History", in COLING-96,Proc .
of the Int'l Conj.
on Computational Linguis-tics, 1996.Z.
Kozareva and E. Hovy.
Not All Seeds Are Equal:Measuring the Quality of Text Mining Seeds.
HumanLanguage Technologies: The 2010 Annual Confe-rence of the North American Chapter of the Associa-tion for Computational Linguistics, June, 2010, pp.618-626.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Semanticclass learning from the web with hyponym patternlinkage graphs.
In Proceedings of ACL-08: HLT,pages 1048?1056.J.
May, A. Brunstein, P. Natarajan,  and R. Weischedel.Surprise!
What's in a Cebuano or Hindi Name?Transactions on Asian Language InformationProcessing (TALIP), Volume 2 Issue 3  September,2003.D.
Maynard, V. Tablan, K. Bontcheva, and H. Cun-ningham.
Rapid customization of an information ex-traction system for a surprise language.
Transactionson Asian Language Information Processing (TALIP),Volume 2 Issue 3  September, 2003.S.
Miller, J. Guinness, and A. Zamanian, ?Name Tag-ging with Word Cluster and Discriminative Train-ing?, Proceedings of HLT/NAACL 2004, pp.
337-342, 2004T.
Mitchell, J. Betteridge, A. Carlson, E. Hruschka, andR.
Wang.
?Populating the Semantic Web by Macro-Reading Internet Text.
Invited paper, Proceedings ofthe 8th International Semantic Web Conference(ISWC 2009).NIST, ACE 2007:http://www.itl.nist.gov/iad/mig/tests/ace/2007/software.htmlP.
Pantel and M. Pennacchiotti.
Espresso: LeveragingGeneric Patterns for Automatically Harvesting Se-mantic Relations.
In Proceedings of Conference onComputational Linguistics / Association for Compu-tational Linguistics (COLING/ACL-06).
pp.
113-120.Sydney, Australia, 2006.L.
Ramshaw , E. Boschee, S. Bratus, S. Miller, R.Stone, R. Weischedel, A. Zamanian, ?Experiments inmulti-modal automatic content extraction?, Proceed-ings of Human Technology Conference, March 2001.D.
Ravichandran and E. Hovy.
Learning surface textpatterns for a question answering system.
In Pro-ceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2002),pages 41?47, Philadelphia, PA, 2002.E.
Riloff.
Automatically generating extraction patternsfrom untagged text.
In Proceedings of the ThirteenthNational Conference on Artificial Intelligence, pages1044-1049, 1996.S.
Sekine and R. Grishman.
Hindi-English cross-lingualquestion-answering  system.
Transactions on AsianLanguage Information Processing (TALIP), Volume2 Issue 3  September, 2003.G.
Zhou, J. Li, L. Qian, Q. Zhu.
Semi-SupervisedLearning for Relation Extraction.
Proceedings of theThird International Joint Conference on NaturalLanguage Processing: Volume-I.
2008.1446
