Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82?86,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsFreebase QA: Information Extraction or Semantic Parsing?Xuchen Yao1Jonathan Berant3Benjamin Van Durme1,21Center for Language and Speech Processing2Human Language Technology Center of ExcellenceJohns Hopkins University3Computer Science DepartmentStanford UniversityAbstractWe contrast two seemingly distinct ap-proaches to the task of question answering(QA) using Freebase: one based on infor-mation extraction techniques, the other onsemantic parsing.
Results over the sametest-set were collected from two state-of-the-art, open-source systems, then ana-lyzed in consultation with those systems?creators.
We conclude that the differ-ences between these technologies, bothin task performance, and in how theyget there, is not significant.
This sug-gests that the semantic parsing commu-nity should target answering more com-positional open-domain questions that arebeyond the reach of more direct informa-tion extraction methods.1 IntroductionQuestion Answering (QA) from structured data,such as DBPedia (Auer et al., 2007), Freebase(Bollacker et al., 2008) and Yago2 (Hoffart etal., 2011), has drawn significant interest fromboth knowledge base (KB) and semantic pars-ing (SP) researchers.
The majority of such worktreats the KB as a database, to which standarddatabase queries (SPARQL, MySQL, etc.)
are is-sued to retrieve answers.
Language understand-ing is modeled as the task of converting natu-ral language questions into queries through inter-mediate logical forms, with the popular two ap-proaches including: CCG parsing (Zettlemoyerand Collins, 2005; Zettlemoyer and Collins, 2007;Zettlemoyer and Collins, 2009; Kwiatkowski etal., 2010; Kwiatkowski et al., 2011; Krishna-murthy and Mitchell, 2012; Kwiatkowski et al.,2013; Cai and Yates, 2013a), and dependency-based compositional semantics (Liang et al., 2011;Berant et al., 2013; Berant and Liang, 2014).We characterize semantic parsing as the taskof deriving a representation of meaning from lan-guage, sufficient for a given task.
Traditionalinformation extraction (IE) from text may becoarsely characterized as representing a certainlevel of semantic parsing, where the goal is toderive enough meaning in order to populate adatabase with factoids of a form matching a givenschema.1Given the ease with which reasonablyaccurate, deep syntactic structure can be automat-ically derived over (English) text, it is not surpris-ing that IE researchers would start including such?features?
in their models.Our question is then: what is the difference be-tween an IE system with access to syntax, as com-pared to a semantic parser, when both are targetinga factoid-extraction style task?
While our conclu-sions should hold generally for similar KBs, wewill focus on Freebase, such as explored by Kr-ishnamurthy and Mitchell (2012), and then otherssuch as Cai and Yates (2013a) and Berant et al.(2013).
We compare two open-source, state-of-the-art systems on the task of Freebase QA: thesemantic parsing system SEMPRE (Berant et al.,2013), and the IE system jacana-freebase (Yaoand Van Durme, 2014).We find that these two systems are on par witheach other, with no significant differences in termsof accuracy between them.
A major distinction be-tween the work of Berant et al.
(2013) and Yaoand Van Durme (2014) is the ability of the for-mer to represent, and compose, aggregation oper-ators (such as argmax, or count), as well as in-tegrate disparate pieces of information.
This rep-resentational capability was important in previous,closed-domain tasks such as GeoQuery.
The moveto Freebase by the SP community was meant to1So-called Open Information Extraction (OIE) is simplya further blurring of the distinction between IE and SP, wherethe schema is allowed to grow with the number of verbs, andother predicative elements of the language.82provide richer, open-domain challenges.
Whilethe vocabulary increased, our analysis suggeststhat compositionality and complexity decreased.We therefore conclude that the semantic parsingcommunity should target more challenging open-domain datasets, ones that ?standard IE?
methodsare less capable of attacking.2 IE and SP Systemsjacana-freebase2(Yao and Van Durme, 2014)treats QA from a KB as a binary classificationproblem.
Freebase is a gigantic graph with mil-lions of nodes (topics) and billions of edges (re-lations).
For each question, jacana-freebasefirst selects a ?view?
of Freebase concerning onlyinvolved topics and their close neighbors (this?view?
is called a topic graph).
For instance,for the question ?who is the brother of justinbieber?
?, the topic graph of Justin Bieber, con-taining all related nodes to the topic (think of the?Justin Bieber?
page displayed by the browser), isselected and retrieved by the Freebase Topic API.Usually such a topic graph contains hundreds tothousands of nodes in close relation to the centraltopic.
Then each of the node is judged as answeror not by a logistic regression learner.Features for the logistic regression learner arefirst extracted from both the question and thetopic graph.
An analysis of the dependencyparse of the question characterizes the questionword, topic, verb, and named entities of themain subject as the question features, such asqword=who.
Features on each node include thetypes of relations and properties the node pos-sesses, such as type=person.
Finally featuresfrom both the question and each node are com-bined as the final features used by the learner, suchas qword=who|type=person.
In this way the as-sociation between the question and answer typeis enforced.
Thus during decoding, for instance,if there is a who question, the nodes with a per-son property would be ranked higher as the an-swer candidate.SEMPRE3is an open-source system for trainingsemantic parsers, that has been utilized to train asemantic parser against Freebase by Berant et al.(2013).
SEMPRE maps NL utterances to logicalforms by performing bottom-up parsing.
First, a2https://code.google.com/p/jacana/3http://www-nlp.stanford.edu/software/sempre/lexicon is used to map NL phrases to KB predi-cates, and then predicates are combined to form afull logical form by a context-free grammar.
Sincelogical forms can be derived in multiple ways fromthe grammar, a log-linear model is used to rankpossible derivations.
The parameters of the modelare trained from question-answer pairs.3 Analysis3.1 Evaluation MetricsBoth Berant et al.
(2013) and Yao andVan Durme (2014) tested their systems onthe WEBQUESTIONS dataset, which contains3778 training questions and 2032 test questionscollected from the Google Suggest API.
Eachquestion came with a standard answer fromFreebase annotated by Amazon Mechanical Turk.Berant et al.
(2013) reported a score of 31.4%in terms of accuracy (with partial credit if inexactmatch) on the test set and later in Berant and Liang(2014) revised it to 35.7%.
Berant et al.
focusedon accuracy ?
how many questions were correctlyanswered by the system.
Since their system an-swered almost all questions, accuracy is roughlyidentical to F1.
Yao and Van Durme (2014)?s sys-tem on the other hand only answered 80% of alltest questions.
Thus they report a score of 42%in terms of F1on this dataset.
For the purpose ofcomparing among all test questions, we loweredthe logistic regression prediction threshold (usu-ally 0.5) on jacana-freebase for the other 20%of questions where jacana-freebase had not pro-posed an answer to, and selected the best-possibleprediction with the highest prediction score as theanswer.
In this way jacana-freebase was ableto answer all questions with a lower accuracy of35.4%.
In the following we present analysis re-sults based on the test questions where the twosystems had very similar performance (35.7% vs.35.4%).4The difference is not significant accord-ing to the paired permutation test (Smucker et al.,2007).3.2 Accuracy vs. CoverageFirst, we were interested to see the proportions ofquestions SEMPRE and jacana-freebase jointlyand separately answered correctly.
The answer to4In this setting accuracy equals averaged macro F1: firstthe F1value on each question were computed, then averagedamong all questions, or put it in other words: ?accuracy withpartial credit?.
In this section our usage of the terms ?accu-racy?
and ?F1?
can be exchanged.83jacana (F1= 1) jacana (F1?
0.5)SEMPRE????
?153 (0.08) 383 (0.19) 429 (0.21) 321 (0.16)?
136 (0.07) 1360 (0.67) 366 (0.18) 916 (0.45)Table 1: The absolute and proportion of ques-tions SEMPRE and jacana-freebase answeredcorrectly (?)
and incorrectly (?)
jointly and sep-arately, running a threshold F1of 1 and 0.5.many questions in the dataset is a set of answers,for example what to see near sedona arizona?.Since turkers did not exhaustively pick out all pos-sible answers, evaluation is performed by comput-ing the F1between the set of answers given bythe system and the answers provided by turkers.With a strict threshold of F1= 1 and a permis-sive threshold of F1?
0.5 to judge the correct-ness, we list the pair-wise correctness matrix inTable 1.
Not surprisingly, both systems had mostquestions wrong given that the averaged F1?s wereonly around 35%.
With the threshold F1= 1,SEMPRE answered more questions exactly cor-rectly compared to jacana-freebase, while whenF1?
0.5, it was the other way around.
Thisshows that SEMPRE is more accurate in certainquestions.
The reason behind this is that SEMPREalways fires queries that return exactly one set ofanswers from Freebase, while jacana-freebasecould potentially tag multiple nodes as the answer,which may lower the accuracy.We have shown that both systems can be moreaccurate in certain questions, but when?
Is therea correlation between the system confidence andaccuracy?
Thus we took the logistic decodingscore (between 0 and 1) from jacana-freebaseand the probability from the log-linear model usedby SEMPRE as confidence, and plotted an ?accu-racy vs. coverage?
curve, which shows the accu-racy of a QA engine with respect to its coverageof all questions.
The curve basically answers onequestion: at a fixed accuracy, what is the propor-tion of questions that can be answered?
A bettersystem should be able to answer more questionscorrectly with the same accuracy.The curve was drawn in the following way.
Foreach question, we select the best answer candidatewith the highest confidence score.
Then for thewhole test set, we have a list of (question, highestranked answer, confidence score) tuples.
Running0 10 20 30 40 50 60 70 80 90 100Percent Answered203040506070AccuracyAccuracy vs. Coveragejacana-freebaseSEMPREFigure 1: Precision with respect to proportion ofquestions answereda threshold from 1 to 0, we select those questionswith an answer confidence score above the thresh-old and compute accuracy at this point.
The X-axis indicates the percentage of questions abovethe threshold and the Y-axis the accuracy, shownin Figure 1.The two curves generally follow a similar trend,but while jacana-freebase has higher accuracywhen coverage is low, SEMPRE obtains slightlybetter accuracy when more questions are an-swered.3.3 Accuracy by Question Length and TypeDo accuracies of the two systems differ with re-spect to the complexity of questions?
Since thereis no clear way to measure question complexity,we use question length as a surrogate and reportaccuracies by question length in Figure 2.
Most ofthe questions were 5 to 8 words long and there wasno substantial difference in terms of accuracies.The major difference lies in questions of length 3,12 and 13.
However, the number of such ques-tions was not high enough to show any statisticalsignificance.Figure 3 further shows the accuracies with re-spect to the question types (as reflected by theWH-word).
Again, there is no significant differ-ence between the two systems.3.4 Learned FeaturesWhat did the systems learn during training?
Wecompare them by presenting the top features byweight, as listed in Table 2.
Clearly, the type ofknowledge learned by the systems in these fea-tures is similar: both systems learn to associatecertain phrases with predicates from the KB.840?0.05?0.1?0.15?0.2?0.25?0.3?0.35?0.4?0.45?0.5?3?(9)?
4?(78)?5?(299)?6?(432)?7?(395)?8?(273)?9?(122)?10?(48)?11?(19)?12?(10)?
13?(4)?15?(1)?<=?5(?386)?<=?10?(1270)?<=15?(34)?Jacana-?
?freebase?SEMPRE?Figure 2: Accuracy (Y-axis) by question length.The X-axis specifies the question length in wordsand the total number of questions in parenthesis.0?0.05?0.1?0.15?0.2?0.25?0.3?0.35?0.4?0.45?what?(929)?where?(357)?who?(261)?which?(35)?when?(100)?how??(8)?Jacana-?
?freebase?SEMPRE?Figure 3: Accuracy by question type (and thenumber of questions).We note, however, that SEMPRE also obtains in-formation from the fully constructed logical form.For instance, SEMPRE learns that logical formsthat return an empty set when executed against theKB are usually incorrect (the weight for this fea-ture is -8.88).
In this respect the SP approach ?un-derstands?
more than the IE approach.We did not further compare on other datasetssuch as GeoQuery (Tang and Mooney, 2001) andFREE917 (Cai and Yates, 2013b).
The first oneinvolves geographic inference and multiple con-traints in queries, directly fitting the compositionalnature of semantic parsing.
The second one wasmanually generated by looking at Freebase top-ics.
Both datasets were less realistic than theWEBQUESTIONS dataset.
Both datasets were alsoless challenging (accuracy/F1were between 80%and 90%) compared to WEBQUESTIONS (around40%).4 Discussion and ConclusionOur analysis of two QA approaches, semanticparsing and information extraction, has shown nosignificant difference between them.
Note thefeature weightqfocus=religion|type=Religion 8.60qfocus=money|type=Currency 5.56qverb=die|type=CauseOfDeath 5.35qword=when|type=datetime 5.11qverb=border|rel=location.adjoins 4.56(a) jacana-freebasefeature weightdie from=CauseOfDeath 10.23die of=CauseOfDeath 7.55accept=Currency 7.30bear=PlaceOfBirth 7.11in switzerland=Switzerland 6.86(b) SEMPRETable 2: Learned top features and their weights forjacana-freebase and SEMPRE.similarity between features used in both systemsshown in Table 2: the systems learned the same?knowledge?
from data, with the distinction thatthe IE approach acquired this through a direct as-sociation between dependency parses and answerproperties, while the SP approach acquired thisthrough optimizing on intermediate logic forms.With a direct information extraction technol-ogy easily getting on par with the more sophis-ticated semantic parsing method, it suggests thatSP-based approaches for QA with Freebase hasnot yet shown its power from a ?deeper?
under-standing of the questions, among questions of var-ious lengths.
We suggest that more compositionalopen-domain datasets should be created, and thatSP researchers should focus on utterances in exist-ing datasets that are beyond the reach of direct IEmethods.5 AcknowledgementWe thank the Allen Institute for Artificial Intelli-gence for assistance in funding this work.
Thismaterial is partially based on research sponsoredby the NSF under grant IIS-1249516 and DARPAunder agreements number FA8750-13-2-0017 andFA8750-13-2-0040 (the DEFT program).85ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.2007.
DBPedia: A nucleus for a web of open data.In The semantic web, pages 722?735.
Springer.Jonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
In Proceedings of ACL.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic Parsing on Freebase fromQuestion-Answer Pairs.
In Proceedings of EMNLP.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, pages 1247?1250.
ACM.Qingqing Cai and Alexander Yates.
2013a.
Large-scale semantic parsing via schema matching and lex-icon extension.
In Proceedings of ACL.Qingqing Cai and Alexander Yates.
2013b.
Large-scale semantic parsing via schema matching and lex-icon extension.
In Proceedings of ACL.Johannes Hoffart, Fabian M Suchanek, KlausBerberich, Edwin Lewis-Kelham, Gerard De Melo,and Gerhard Weikum.
2011.
Yago2: exploring andquerying world knowledge in time, space, context,and many languages.
In Proceedings of the 20thinternational conference companion on World WideWeb, pages 229?232.
ACM.Jayant Krishnamurthy and Tom Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of EMNLP.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proceedings of EMNLP, pages1223?1233.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical generaliza-tion in CCG grammar induction for semantic pars-ing.
In Proceedings of EMNLP.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling Semantic Parsers withOn-the-fly Ontology Matching.
In Proceedings ofEMNLP.Percy Liang, Michael I. Jordan, and Dan Klein.2011.
Learning Dependency-Based CompositionalSemantics.
In Proceedings of ACL.M.D.
Smucker, J. Allan, and B. Carterette.
2007.
Acomparison of statistical significance tests for in-formation retrieval evaluation.
In Proceedings ofthe sixteenth ACM conference on Conference on in-formation and knowledge management, pages 623?632.
ACM.Lappoon R Tang and Raymond J Mooney.
2001.
Us-ing multiple clause constructors in inductive logicprogramming for semantic parsing.
In MachineLearning: ECML 2001.
Springer.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In Proceedings of ACL.Luke S Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
Uncertainty in Artificial Intelligence(UAI).Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proceedings of EMNLP-CoNLL.Luke S Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
In Proceedings of ACL-CoNLL.86
