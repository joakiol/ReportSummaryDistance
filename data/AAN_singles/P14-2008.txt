Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 42?48,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsImproving Citation Polarity Classification with Product ReviewsCharles Jochim?IBM Research ?
Irelandcharlesj@ie.ibm.comHinrich Sch?utzeCenter for Information & Language ProcessingUniversity of MunichAbstractRecent work classifying citations in scien-tific literature has shown that it is possi-ble to improve classification results withextensive feature engineering.
While thisresult confirms that citation classificationis feasible, there are two drawbacks tothis approach: (i) it requires a large anno-tated corpus for supervised classification,which in the case of scientific literatureis quite expensive; and (ii) feature engi-neering that is too specific to one area ofscientific literature may not be portable toother domains, even within scientific liter-ature.
In this paper we address these twodrawbacks.
First, we frame citation clas-sification as a domain adaptation task andleverage the abundant labeled data avail-able in other domains.
Then, to avoidover-engineering specific citation featuresfor a particular scientific domain, we ex-plore a deep learning neural network ap-proach that has shown to generalize wellacross domains using unigram and bigramfeatures.
We achieve better citation clas-sification results with this cross-domainapproach than using in-domain classifica-tion.1 IntroductionCitations have been categorized and studied fora half-century (Garfield, 1955) to better under-stand when and how citations are used, andto record and measure how information is ex-changed (e.g., networks of co-cited papers or au-thors (Small and Griffith, 1974)).
Recently, thevalue of this information has been shown in practi-cal applications such as information retrieval (IR)?This work was primarily conducted at the IMS ?
Uni-versity of Stuttgart.
(Ritchie et al, 2008), summarization (Qazvinianand Radev, 2008), and even identifying scientificbreakthroughs (Small and Klavans, 2011).
We ex-pect that by identifying and labeling the functionof citations we can improve the effectiveness ofthese applications.There has been no consensus on what aspectsor functions of a citation should be annotated andhow.
Early citation classification focused more oncitation motivation (Garfield, 1964), while laterclassification considered more the citation func-tion (Chubin and Moitra, 1975).
Recent stud-ies using automatic classification have continuedthis tradition of introducing a new classificationscheme with each new investigation into the useof citations (Nanba and Okumura, 1999; Teufelet al, 2006a; Dong and Sch?afer, 2011; Abu-Jbaraet al, 2013).
One distinction that has been moreconsistently annotated across recent citation clas-sification studies is between positive and negativecitations (Athar, 2011; Athar and Teufel, 2012;Abu-Jbara et al, 2013).1The popularity of thisdistinction likely owes to the prominence of sen-timent analysis in NLP (Liu, 2010).
We followmuch of the recent work on citation classificationand concentrate on citation polarity.2 Domain AdaptationBy concentrating on citation polarity we are ableto compare our classification to previous citationpolarity work.
This choice also allows us to accessthe wealth of existing data containing polarity an-notation and then frame the task as a domain adap-tation problem.
Of course the risk in approachingthe problem as domain adaptation is that the do-mains are so different that the representation ofa positive instance of a movie or product review,for example, will not coincide with that of a posi-1Dong and Sch?afer (2011) also annotate polarity, whichcan be found in their dataset (described later), but this is notdiscussed in their paper.42tive scientific citation.
On the other hand, becausethere is a limited amount of annotated citation dataavailable, by leveraging large amounts of anno-tated polarity data we could potentially even im-prove citation classification.We treat citation polarity classification as a sen-timent analysis domain adaptation task and there-fore must be careful not to define features that aretoo domain specific.
Previous work in citation po-larity classification focuses on finding new cita-tion features to improve classification, borrowinga few from text classification in general (e.g., n-grams), and perhaps others from sentiment analy-sis problems (e.g., the polarity lexicon from Wil-son et al (2005)).
We would like to do as littlefeature engineering as possible to ensure that thefeatures we use are meaningful across domains.However, we do still want features that somehowcapture the inherent positivity or negativity of ourlabeled instances, i.e., citations or Amazon prod-uct reviews.
Currently a popular approach for ac-complishing this is to use deep learning neural net-works (Bengio, 2009), which have been shownto perform well on a variety of NLP tasks us-ing only bag-of-word features (Collobert et al,2011).
More specifically related to our work, deeplearning neural networks have been successfullyemployed for sentiment analysis (Socher et al,2011) and for sentiment domain adaptation (Glo-rot et al, 2011).
In this paper we examine oneof these approaches, marginalized stacked denois-ing autoencoders (mSDA) from Chen et al (2012),which has been successful in classifying the po-larity of Amazon product reviews across productdomains.
Since mSDA achieved state-of-the-artperformance in Amazon product domain adapta-tion, we are hopeful it will also be effective whenswitching to a more distant domain like scientificcitations.3 Experimental Setup3.1 CorporaWe are interested in domain adaptation for citationclassification and therefore need a target dataset ofcitations and a non-citation source dataset.
Thereare two corpora available that contain citationfunction annotation, the DFKI Citation Corpus(Dong and Sch?afer, 2011) and the IMS CitationCorpus (Jochim and Sch?utze, 2012).
Both corporahave only about 2000 instances; unfortunately,there are no larger corpora available with citationannotation and this task would benefit from moreannotated data.
Due to the infrequent use of neg-ative citations, a substantial annotation effort (an-notating over 5 times more data) would be nec-essary to reach 1000 negative citation instances,which is the number of negative instances in a sin-gle domain in the multi-domain corpus describedbelow.The DFKI Citation Corpus2has been used forclassifying citation function (Dong and Sch?afer,2011), but the dataset alo includes polarity an-notation.
The dataset has 1768 citation sentenceswith polarity annotation: 190 are labeled as pos-itive, 57 as negative, and the vast majority, 1521,are left neutral.
The second citation corpus, theIMS Citation Corpus3contains 2008 annotated ci-tations: 1836 are labeled positive and 172 are la-beled negative.
Jochim and Sch?utze (2012) useannotation labels from Moravcsik and Murugesan(1975) where positive instances are labeled confir-mative, negative instances are labeled negational,and there is no neutral class.
Because each ofthe citation corpora is of modest size we combinethem to form one citation dataset, which we willrefer to as CITD.
The two citation corpora com-prising CITD both come from the ACL Anthol-ogy (Bird et al, 2008): the IMS corpus uses theACL proceedings from 2004 and the DFKI corpususes parts of the proceedings from 2007 and 2008.Since mSDA also makes use of large amounts ofunlabeled data, we extend our CITD corpus withcitations from the proceedings of the remainingyears of the ACL, 1979?2003, 2005?2006, and2009.There are a number of non-citation corporaavailable that contain polarity annotation.
Forthese experiments we use the Multi-Domain Senti-ment Dataset4(henceforth MDSD), introduced byBlitzer et al (2007).
We use the version of theMDSD that includes positive and negative labelsfor product reviews taken from Amazon.com inthe following domains: books, dvd, electronics,and kitchen.
For each domain there are 1000 pos-itive reviews and 1000 negative reviews that com-prise the ?labeled?
data, and then roughly 4000more reviews in the ?unlabeled?5data.
Reviews2https://aclbib.opendfki.de/repos/trunk/citation_classification_dataset/3http://www.ims.uni-stuttgart.de/?jochimcs/citation-classification/4http://www.cs.jhu.edu/?mdredze/datasets/sentiment/5It is usually treated as unlabeled data even though it ac-43Corpus Instances Pos.
Neg.
Neut.DFKI 1768 190 57 1521IMS 2008 1836 172 ?MDSD 27,677 13,882 13,795 ?Table 1: Polarity corpora.were preprocessed so that for each review you finda list of unigrams and bigrams with their frequencywithin the review.
Unigrams from a stop list of 55stop words are removed, but stop words in bigramsremain.Table 1 shows the distribution of polarity labelsin the corpora we use for our experiments.
Wecombine the DFKI and IMS corpora into the CITDcorpus.
We omit the citations labeled neutral fromthe DFKI corpus because the IMS corpus does notcontain neutral annotation nor does the MDSD.
Itis the case in many sentiment analysis corpora thatonly positive and negative instances are included,e.g., (Pang et al, 2002).The citation corpora presented above are bothunbalanced and both have a highly skewed distri-bution.
The MDSD on the other hand is evenlybalanced and an effort was even made to keepthe data treated as ?unlabeled?
rather balanced.For this reason, in line with previous work us-ing MDSD, we balance the labeled portion of theCITD corpus.
This is done by taking 179 uniquenegative sentences in the DFKI and IMS corporaand randomly selecting an equal number of posi-tive sentences.
The IMS corpus can have multiplelabeled citations per sentence: there are 122 sen-tences containing the 172 negative citations fromTable 1.
The final CITD corpus comprises thisbalanced corpus of 358 labeled citation sentencesplus another 22,093 unlabeled citation sentences.3.2 FeaturesIn our experiments, we restrict our features to un-igrams and bigrams from the product review orcitation context (i.e., the sentence containing thecitation).
This follows previous studies in do-main adaptation (Blitzer et al, 2007; Glorot et al,2011).
Chen et al (2012) achieve state-of-the-artresults on MDSD by testing the 5000 and 30,000most frequent unigram and bigram features.Previous work in citation classification haslargely focused on identifying new features fortually contains positive and negative labels, which have beenused, e.g., in (Chen et al, 2012).improving classification accuracy.
A significantamount of effort goes into engineering new fea-tures, in particular for identifying cue phrases,e.g., (Teufel et al, 2006b; Dong and Sch?afer,2011).
However, there seems to be little consen-sus on which features help most for this task.
Forexample, Abu-Jbara et al (2013) and Jochim andSch?utze (2012) find the list of polar words fromWilson et al (2005) to be useful, and neither studylists dependency relations as significant features.Athar (2011) on the other hand reported significantimprovement using dependency relation featuresand found that the same list of polar words slightlyhurt classification accuracy.
The classifiers andimplementation of features varies between thesestudies, but the problem remains that there seemsto be no clear set of features for citation polarityclassification.The lack of consensus on the most useful cita-tion polarity features coupled with the recent suc-cess of deep learning neural networks (Collobert etal., 2011) further motivate our choice to limit ourfeatures to the n-grams available in the product re-view or citation context and not rely on externalresources or tools for additional features.3.3 Classification with mSDAFor classification we use marginalized stacked de-noising autoencoders (mSDA) from Chen et al(2012)6plus a linear SVM.
mSDA takes the con-cept of denoising ?
introducing noise to make theautoencoder more robust ?
from Vincent et al(2008), but does the optimization in closed form,thereby avoiding iterating over the input vector tostochastically introduce noise.
The result of thisis faster run times and currently state-of-the-artperformance on MDSD, which makes it a goodchoice for our domain adaptation task.
The mSDAimplementation comes with LIBSVM, which wereplace with LIBLINEAR (Fan et al, 2008) forfaster run times with no decrease in accuracy.
LIB-LINEAR, with default settings, also serves as ourbaseline.3.4 Outline of ExperimentsOur initial experiments simply extend those ofChen et al (2012) (and others who have usedMDSD) by adding another domain, citations.
Wetrain on each of the domains from the MDSD ?6We use their MATLAB implementation available athttp://www.cse.wustl.edu/?mchen/code/mSDA.tar.44books dvd electronics kitchen0.00.10.20.30.40.50.6SVMmSDAIn?domain F1Figure 1: Cross domain macro-F1results train-ing on Multi-Domain Sentiment Dataset and test-ing on citation dataset (CITD).
The horizontal lineindicates macro-F1for in-domain citation classifi-cation.books, dvd, electronics, and kitchen ?
and test onthe citation data.
We split the labeled data 80/20following Blitzer et al (2007) (cf.
Chen et al(2012) train on all ?labeled?
data and test on the?unlabeled?
data).
These experiments should helpanswer two questions: does a larger amount oftraining data, even if out of domain, improve ci-tation classification; and how well do the differ-ent product domains generalize to citations (i.e.,which domains are most similar to citations)?In contrast to previous work using MDSD, a lotof the work in domain adaptation also leverages asmall amount of labeled target data.
In our secondset of experiments, we follow the domain adap-tation approaches described in (Daum?e III, 2007)and train on product review and citation data be-fore testing on citations.4 Results and Discussion4.1 Citation mSDAOur initial results show that using mSDA for do-main adaptation to citations actually outperformsin-domain classification.
In Figure 1 we com-pare citation classification with mSDA to the SVMbaseline.
Each pair of vertical bars representstraining on a domain from MDSD (e.g., books)and testing on CITD.
The dark gray bar indicatesthe F1scores for the SVM baseline using the30,000 features and the lighter gray bar shows themSDA results.
The black horizontal line indicatesthe F1score for in-domain citation classification,which sometimes represents the goal for domainadaptation.
We can see that using a larger dataset,even if out of domain, does improve citation clas-sification.
For books, dvd, and electronics, eventhe SVM baseline improves on in-domain classifi-cation.
mSDA does better than the baseline for alldomains except dvd.
Using a larger training set,along with mSDA, which makes use of the un-labeled data, leads to the best results for citationclassification.In domain adaptation we would expect the do-mains most similar to the target to lead to thehighest results.
Like Dai et al (2007), we mea-sure the Kullback-Leibler divergence between thesource and target domains?
distributions.
Accord-ing to this measure, citations are most similar tothe books domain.
Therefore, it is not surprisingthat training on books performs well on citations,and intuitively, among the domains in the Amazondataset, a book review is most similar to a scien-tific citation.
This makes the good mSDA resultsfor electronics a bit more surprising.4.2 Easy Domain AdaptationThe results in Section 4.1 are for semi-superviseddomain adaptation: the case where we have somelarge annotated corpus (Amazon product reviews)and a large unannotated corpus (citations).
Therehave been a number of other successful attempts atfully supervised domain adaptation, where it is as-sumed that some small amount of data is annotatedin the target domain (Chelba and Acero, 2004;Daum?e III, 2007; Jiang and Zhai, 2007).
To seehow mSDA compares to supervised domain adap-tation we take the various approaches presented byDaum?e III (2007).
The results of this comparisoncan be seen in Table 2.
Briefly, ?All?
trains onsource and target data; ?Weight?
is the same as?All?
except that instances may be weighted dif-ferently based on their domain (weights are chosenon a development set); ?Pred?
trains on the sourcedata, makes predictions on the target data, andthen trains on the target data with the predictions;?LinInt?
linearly interpolates predictions using thesource-only and target-only models (the interpola-tion parameter is chosen on a development set);?Augment?
uses a larger feature set with source-specific and target-specific copies of features; see45Domain Baseline All Weight Pred LinInt Augment mSDAbooks 54.5 54.8 52.0 51.9 53.4 53.4 57.1dvd 53.2 50.9 56.0 53.4 51.9 47.5 51.6electronics 53.4 49.0 50.5 53.4 54.8 51.9 59.2kitchen 47.9 48.8 50.7 53.4 52.6 49.2 50.1citations 51.9 ?
?
?
?
?
54.9Table 2: Macro-F1results on CITD using different domain adaptation approaches.
(Daum?e III, 2007) for further details.We are only interested in citations as the tar-get domain.
Daum?e?s source-only baseline cor-responds to the ?Baseline?
column for domains:books, dvd, electronics, and kitchen; while histarget-only baseline can be seen for citations in thelast row of the ?Baseline?
column in Table 2.The semi-supervised mSDA performs quitewell with respect to the fully supervised ap-proaches, obtaining the best results for books andelectronics, which are also the highest scores over-all.
Weight and Pred have the highest F1scores fordvd and kitchen respectively.
Daum?e III (2007)noted that the ?Augment?
algorithm performedbest when the target-only results were better thanthe source-only results.
When this was not thecase in his experiments, i.e., for the treebankchunking task, both Weight and Pred were amongthe best approaches.
In our experiments, trainingon source-only outperforms target-only, with theexception of the kitchen domain.We have included the line for citations to see theresults training only on the target data (F1= 51.9)and to see the improvement when using all of theunlabeled data with mSDA (F1= 54.9).4.3 DiscussionThese results are very promising.
Although theyare not quite as high as other published resultsfor citation polarity (Abu-Jbara et al, 2013)7, wehave shown that you can improve citation polarityclassification by leveraging large amounts of an-notated data from other domains and using a sim-ple set of features.mSDA and fully supervised approaches can alsobe straightforwardly combined.
We do not presentthose results here due to space constraints.
The7Their work included a CRF model to identify the citationcontext that gave them an increase of 9.2 percent F1over asingle sentence citation context.
Our approach achieves sim-ilar macro-F1on only the citation sentence, but using a dif-ferent corpus.combination led to mixed results: adding mSDAto the supervised approaches tended to improve F1over those approaches but results never exceededthe top mSDA numbers in Table 2.5 Related WorkTeufel et al (2006b) introduced automatic citationfunction classification, with classes that could begrouped as positive, negative, and neutral.
Theyrelied in part on a manually compiled list of cuephrases that cannot easily be transferred to otherclassification schemes or other scientific domains.Athar (2011) followed this and was the first tospecifically target polarity classification on scien-tific citations.
He found that dependency tuplescontributed the most significant improvement inresults.
Abu-Jbara et al (2013) also looks at bothcitation function and citation polarity.
A big con-tribution of this work is that they also train a CRFsequence tagger to find the citation context, whichsignificantly improves results over using only theciting sentence.
Their feature analysis indicatesthat lexicons for negation, speculation, and po-larity were most important for improving polarityclassification.6 ConclusionRobust citation classification has been hindered bythe relative lack of annotated data.
In this pa-per we successfully use a large, out-of-domain,annotated corpus to improve the citation polarityclassification.
Our approach uses a deep learningneural network for domain adaptation with labeledout-of-domain data and unlabeled in-domain data.This semi-supervised domain adaptation approachoutperforms the in-domain citation polarity classi-fication and other fully supervised domain adapta-tion approaches.Acknowledgments.
We thank the DFG forfunding this work (SPP 1335 Scalable Visual An-alytics).46ReferencesAmjad Abu-Jbara, Jefferson Ezra, and DragomirRadev.
2013.
Purpose and polarity of citation: To-wards NLP-based bibliometrics.
In Proceedings ofNAACL-HLT, pages 596?606.Awais Athar and Simone Teufel.
2012.
Context-enhanced citation sentiment detection.
In Proceed-ings of NAACL-HLT, pages 597?601.Awais Athar.
2011.
Sentiment analysis of citations us-ing sentence structure-based features.
In Proceed-ings of ACL Student Session, pages 81?87.Yoshua Bengio.
2009.
Learning deep architectures forAI.
Foundations and Trends in Machine Learning,2(1):1?127.Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,Mark Joseph, Min-Yen Kan, Dongwon Lee, BrettPowley, Dragomir Radev, and Yee Fan Tan.
2008.The ACL anthology reference corpus: A referencedataset for bibliographic research in computationallinguistics.
In Proceedings of LREC, pages 1755?1759.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of ACL, pages 440?447.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
In Proceedings of EMNLP, pages 285?292.Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Wein-berger, and Fei Sha.
2012.
Marginalized denoisingautoencoders for domain adaptation.
In Proceedingsof ICML, pages 767?774.Daryl E. Chubin and Soumyo D. Moitra.
1975.
Con-tent analysis of references: Adjunct or alternative tocitation counting?
Social Studies of Science, 5:423?441.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and YongYu.
2007.
Transferring naive bayes classifiers fortext classification.
In AAAI, pages 540?545.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL, pages 256?263.Cailing Dong and Ulrich Sch?afer.
2011.
Ensemble-style self-training on citation classification.
In Pro-ceedings of IJCNLP, pages 623?631.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Eugene Garfield.
1955.
Citation indexes to science:A new dimension in documentation through associ-ation of ideas.
Science, 122:108?111.Eugene Garfield.
1964.
Can citation indexing be au-tomated?
In Statistical Association Methods forMechanized Documentation, Symposium Proceed-ings, pages 189?192.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Pro-ceedings of ICML, pages 513?520.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In ACL,pages 264?271.Charles Jochim and Hinrich Sch?utze.
2012.
Towardsa generic and flexible citation classifier based ona faceted classification scheme.
In Proceedings ofCOLING, pages 1343?1358.Bing Liu.
2010.
Sentiment analysis and subjectivity.In Nitin Indurkhya and Fred J. Damerau, editors,Handbook of Natural Language Processing, SecondEdition.
CRC Press, Taylor and Francis Group.Michael J. Moravcsik and Poovanalingam Murugesan.1975.
Some results on the function and quality ofcitations.
Social Studies of Science, 5:86?92.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using referenceinformation.
In Proceedings of IJCAI, pages 926?931.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In Proceedings of COLING, pages 689?696.Anna Ritchie, Stephen Robertson, and Simone Teufel.2008.
Comparing citation contexts for informationretrieval.
In Proceedings of CIKM, pages 213?222.Henry G. Small and Belver C. Griffith.
1974.
Thestructure of scientific literatures I: Identifying andgraphing specialties.
Science Studies, 4(1):17?40.Henry Small and Richard Klavans.
2011.
Identifyingscientific breakthroughs by combining co-citationanalysis and citation context.
In Proceedings of In-ternational Society for Scientometrics and Informet-rics.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings ofEMNLP, pages 151?161.47Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006a.
An annotation scheme for citation function.In Proceedings of SIGdial Workshop on Discourseand Dialogue, pages 80?87.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006b.
Automatic classification of citation function.In Proceedings of EMNLP, pages 103?110.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, andPierre-Antoine Manzagol.
2008.
Extracting andcomposing robust features with denoising autoen-coders.
In Proceedings of ICML, pages 1096?1103.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP, pages 347?354.48
