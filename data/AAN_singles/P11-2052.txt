Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 299?304,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsScaling up Automatic Cross-Lingual Semantic Role AnnotationLonneke van der PlasDepartment of LinguisticsUniversity of GenevaGeneva, SwitzerlandPaola MerloDepartment of LinguisticsUniversity of GenevaGeneva, Switzerland{Lonneke.vanderPlas,Paola.Merlo,James.Henderson}@unige.chJames HendersonDepartment of Computer ScienceUniversity of GenevaGeneva, SwitzerlandAbstractBroad-coverage semantic annotations fortraining statistical learners are only availablefor a handful of languages.
Previous ap-proaches to cross-lingual transfer of seman-tic annotations have addressed this problemwith encouraging results on a small scale.
Inthis paper, we scale up previous efforts by us-ing an automatic approach to semantic anno-tation that does not rely on a semantic on-tology for the target language.
Moreover,we improve the quality of the transferred se-mantic annotations by using a joint syntactic-semantic parser that learns the correlations be-tween syntax and semantics of the target lan-guage and smooths out the errors from auto-matic transfer.
We reach a labelled F-measurefor predicates and arguments of only 4% and9% points, respectively, lower than the upperbound from manual annotations.1 IntroductionAs data-driven techniques tackle more and morecomplex natural language processing tasks, it be-comes increasingly unfeasible to use complete, ac-curate, hand-annotated data on a large scale fortraining models in all languages.
One approach toaddressing this problem is to develop methods thatautomatically generate annotated data by transfer-ring annotations in parallel corpora from languagesfor which this information is available to languagesfor which these data are not available (Yarowsky etal., 2001; Fung et al, 2007; Pado?
and Lapata, 2009).Previous work on the cross-lingual transfer of se-mantic annotations (Pado?, 2007; Basili et al, 2009)has produced annotations of good quality for testsets that were carefully selected based on seman-tic ontologies on the source and target side.
It hasbeen suggested that these annotations could be usedto train semantic role labellers (Basili et al, 2009).In this paper, we generate high-quality broad-coverage semantic annotations using an automaticapproach that does not rely on a semantic ontol-ogy for the target language.
Furthermore, to ourknowledge, we report the first results on using jointsyntactic-semantic learning to improve the qualityof the semantic annotations from automatic cross-lingual transfer.
Results on correlations betweensyntax and semantics found in previous work (Merloand van der Plas, 2009; Lang and Lapata, 2010) haveled us to make use of the available syntactic anno-tations on the target language.
We use the seman-tic annotations resulting from cross-lingual transfercombined with syntactic annotations to train a jointsyntactic-semantic parser for the target language,which, in turn, re-annotates the corpus (See Fig-ure 1).
We show that the semantic annotations pro-duced by this parser are of higher quality than thedata on which it was trained.Given our goal of producing broad-coverage an-notations in a setting based on an aligned corpus,our choices of formal representation and of labellingscheme differ from previous work (Pado?, 2007;Basili et al, 2009).
We choose a dependency repre-sentation both for the syntax and semantics becauserelations are expressed as direct arcs between words.This representation allows cross-lingual transfer touse word-based alignments directly, eschewing theneed for complex constituent-alignment algorithms.299Train?a?French?syntacticparser?Transfer?semantic?annotationsfrom?EN?to?FR?using?wordalignmentsEN?syntactic?semanticannotationsEN?FR?word?aligneddataFR?syntacticannotationsFR?semanticannotations evaluationTrain?Frenchjoint?syntactic?semantic?parserevaluationFR?syntacticannotationsFR?semanticannotationsFigure 1: System overviewWe choose the semantic annotation scheme definedby Propbank, because it has broad coverage and in-cludes an annotated corpus, contrary to other avail-able resources such as FrameNet (Fillmore et al,2003) and is the preferred annotation scheme for ajoint syntactic-semantic setting (Merlo and van derPlas, 2009).
Furthermore, Monachesi et al (2007)showed that the PropBank annotation scheme can beused for languages other than English directly.2 Cross-lingual semantic transferData-driven induction of semantic annotation basedon parallel corpora is a well-defined and feasibletask, and it has been argued to be particularly suit-able to semantic role label annotation because cross-lingual parallelism improves as one moves to moreabstract linguistic levels of representation.
WhileHwa et al (2002; 2005) find that direct syntactic de-pendency parallelism between English and Spanishconcerns 37% of dependency links, Pado?
(2007) re-ports an upper-bound mapping correspondence cal-culated on gold data of 88% F-measure for in-dividual semantic roles, and 69% F-measure forwhole scenario-like semantic frames.
Recently, Wuand Fung (2009a; 2009b) also show that semanticroles help in statistical machine translation, capi-talising on a study of the correspondence betweenEnglish and Chinese which indicates that 84% ofroles transfer directly, for PropBank-style annota-tions.
These results indicate high correspondenceacross languages at a shallow semantic level.Based on these results, our transfer of semanticannotations from English sentences to their Frenchtranslations is based on a very strong mapping hy-pothesis, adapted from the Direct CorrespondenceAssumption for syntactic dependency trees by Hwaet al (2005).Direct Semantic Transfer (DST) For anypair of sentences E and F that are transla-tions of each other, we transfer the seman-tic relationship R(xE , yE) to R(xF , yF ) ifand only if there exists a word-alignmentbetween xE and xF and between yE andyF , and we transfer the semantic propertyP (xE) to P (xF ) if and only if there existsa word-alignment between xE and xF .The relationships which we transfer are semanticrole dependencies and the properties are predicatesenses.
We introduce one constraint to the direct se-mantic transfer.
Because the semantic annotations inthe target language are limited to verbal predicates,we only transfer predicates to words the syntacticparser has tagged as a verb.As reported by Hwa et al (2005), the direct cor-respondence assumption is a strong hypothesis thatis useful to trigger a projection process, but will notwork correctly for several cases.We used a filter to remove obviously incompleteannotations.
We know from the annotation guide-lines used to annotate the French gold sentences thatall verbs, except modals and realisations of the verbe?tre, should receive a predicate label.
We define afilter that removes sentences with missing predicatelabels based on PoS-information in the French sen-tence.2.1 Learning joint syntactic-semanticstructuresWe know from previous work that there is a strongcorrelation between syntax and semantics (Merloand van der Plas, 2009), and that this correla-tion has been successfully applied for the unsuper-vised induction of semantic roles (Lang and Lap-ata, 2010).
However, previous work in machinetranslation leads us to believe that transferring thecorrelations between syntax and semantics acrosslanguages would be problematic due to argument-structure divergences (Dorr, 1994).
For example,the English verb like and the French verb plaire donot share correlations between syntax and seman-tics.
The verb like takes an A0 subject and an A1300direct object, whereas the verb plaire licences an A1subject and an A0 indirect object.We therefore transfer semantic roles cross-lingually based only on lexical alignments and addsyntactic information after transfer.
In Figure 1, wesee that cross-lingual transfer takes place at the se-mantic level, a level that is more abstract and knownto port relatively well across languages, while thecorrelations with syntax, that are known to divergecross-lingually, are learnt on the target languageonly.
We train a joint syntactic-semantic parseron the combination of the two linguistic levels thatlearns the correlations between these structures inthe target language and is able to smooth out errorsfrom automatic transfer.3 ExperimentsWe used two statistical parsers in our transfer ofsemantic annotations from English to French, onefor syntactic parsing and one for joint syntactic-semantic parsing.
In addition, we used several cor-pora.3.1 The statistical parsersFor our syntactic-semantic parsing model, we usea freely-available parser (Henderson et al, 2008;Titov et al, 2009).
The probabilistic model is a jointgenerative model of syntactic and semantic depen-dencies that maximises the joint probability of thesyntactic and semantic dependencies, while buildingtwo separate structures.For the French syntactic parser, we used the de-pendency parser described in Titov and Hender-son (2007).
We train the parser on the dependencyversion of the French Paris treebank (Candito et al,2009), achieving 87.2% labelled accuracy on thisdata set.3.2 DataTo transfer semantic annotation from English toFrench, we used the Europarl corpus (Koehn,2003)1.
We word-align the English sentences to theFrench sentences automatically using GIZA++ (Och1As is usual practice in preprocessing for automatic align-ment, the datasets were tokenised and lowercased and only sen-tence pairs corresponding to a one-to-one sentence alignmentwith lengths ranging from one to 40 tokens on both French andEnglish sides were considered.and Ney, 2003) and include only intersective align-ments.
Furthermore, because translation shifts areknown to pose problems for the automatic projectionof semantic roles across languages (Pado?, 2007), weselect only those parallel sentences in Europarl thatare direct translations from English to French, orvice versa.
In the end, we have a word-aligned par-allel corpus of 276-thousand sentence pairs.Syntactic annotation is available for French.
TheFrench Treebank (Abeille?
et al, 2003) is a treebankof 21,564 sentences annotated with constituency an-notation.
We use the automatic dependency conver-sion of the French Treebank into dependency formatprovided to us by Candito and Crabbe?
and describedin Candito et al (2009).The Penn Treebank corpus (Marcus et al, 1993)merged with PropBank labels (Palmer et al, 2005)and NomBank labels (Meyers, 2007) is used to trainthe syntactic-semantic parser described in Subsec-tion 3.1 to annotate the English part of the parallelcorpus.3.3 Test setsFor testing, we used the hand-annotated data de-scribed in (van der Plas et al, 2010).
One-thousandFrench sentences are extracted randomly from ourparallel corpus without any constraints on the se-mantic parallelism of the sentences, unlike muchprevious work.
We randomly split those 1000 sen-tences into test and development set containing 500sentences each.4 ResultsWe evaluate our methods for automatic annotationgeneration twice: once after the transfer step, andonce after joint syntactic-semantic learning.
Thecomparison of these two steps will tell us whetherthe joint syntactic-semantic parser is able to improvesemantic annotations by learning from the syntacticannotations available.
We evaluate the models onunrestricted test sets2 to determine if our methodsscale up.Table 1 shows the results of automatically an-notating French sentences with semantic role an-notation.
The first set of columns of results re-2Due to filtering, the test set for the transfer (filter) model issmaller and not directly comparable to the other three models.301Predicates Arguments (given predicate)Labelled Unlabelled Labelled UnlabelledPrec Rec F Prec Rec F Prec Rec F Prec Rec F1 Transfer (no filter) 50 31 38 91 55 69 61 48 54 72 57 642 Transfer (filter) 51 46 49 92 84 88 65 51 57 76 59 673 Transfer+parsing (no filter) 71 29 42 97 40 57 77 57 65 87 64 744 Transfer+parsing (filter) 61 50 55 95 78 85 71 52 60 83 61 705 Inter-annotator agreement 61 57 59 97 89 93 73 75 74 88 91 89Table 1: Percent recall, precision, and F-measure for predicates and for arguments given the predicate, for the fourautomatic annotation models and the manual annotation.ports labelling and identification of predicates andthe second set of columns reports labelling and iden-tification of arguments, respectively, for the predi-cates that are identified.
The first two rows showthe results when applying direct semantic transfer.Rows three and four show results when using thejoint syntactic-semantic parser to re-annotate thesentences.
For both annotation models we show re-sults when using the filter described in Section 2 andwithout the filter.The most striking result that we can read fromTable 1 is that the joint syntactic-semantic learningstep results in large improvements, especially forargument labelling, where the F-measure increasesfrom 54% to 65% for the unfiltered data.
The parseris able to outperform the quality of the semanticdata on which it was trained by using the infor-mation contained in the syntax.
This result is inaccordance with results reported in Merlo and Vander Plas (2009) and Lang and Lapata (2010), wherethe authors find a high correlation between syntacticfunctions and PropBank semantic roles.Filtering improves the quality of the transferredannotations.
However, when training a parser on theannotations we see that filtering only results in betterrecall scores for predicate labelling.
This is not sur-prising given that the filters apply to completeness inpredicate labelling specifically.
The improvementsfrom joint syntactic-semantic learning for argumentlabelling are largest for the unfiltered setting, be-cause the parser has access to larger amounts of data.The filter removes 61% of the data.As an upper bound we take the inter-annotatoragreement for manual annotation on a random setof 100 sentences (van der Plas et al, 2010), givenin the last row of Table 1.
The parser reaches anF-measure on predicate labelling of 55% when us-ing filtered data, which is very close to the up-per bound (59%).
The upper bound for argumentinter-annotator agreement is an F-measure of 74%.The parser trained on unfiltered data reaches anF-measure of 65%.
These results on unrestrictedtest sets and their comparison to manual annotationshow that we are able to scale up cross-lingual se-mantic role annotation.5 Discussion and error analysisA more detailed analysis of the distribution of im-provements over the types of roles further strength-ens the conclusion that the parser learns the corre-lations between syntax and semantics.
It is a well-known fact that there exists a strong correlation be-tween syntactic function and semantic role for theA0 and A1 arguments: A0s are commonly mappedonto subjects and A1s are often realised as direct ob-jects (Lang and Lapata, 2010).
It is therefore notsurprising that the F-measure on these types of ar-guments increases by 12% and 15%, respectively,after joint-syntactic semantic learning.
Since thesearguments make up 65% of the roles, this introducesa large improvement.
In addition, we find improve-ments of more than 10% on the following adjuncts:AM-CAU, AM-LOC, AM-MNR, and AM-MOD that to-gether comprise 9% of the data.With respect to predicate labelling, comparisonof the output after transfer with the output afterparsing (on the development set) shows how theparser smooths out transfer errors and how inter-lingual divergences can be solved by making useof the variations we find intra-lingually.
An exam-ple is given in Figure 2.
The first line shows thepredicate-argument structure given by the English302EN (source) Postal [A1 services] [AM-MOD must] [CONTINUE.01 continue] [C-A1 to] be public services.FR (transfer) Les [A1services] postaux [AM-MOD doivent] [CONTINUE.01rester] des services publics.FR (parsed) Les [A1 services] postaux [AM-MOD doivent] [REMAIN.01rester] des [A3 services] publics.Figure 2: Differences in predicate-argument labelling after transfer and after parsingsyntactic-semantic parser to the English sentence.The second line shows the French translation andthe predicate-argument structure as it is transferredcross-lingually following the method described inSection 2.
Transfer maps the English predicate la-bel CONTINUE.01 onto the French verb rester, be-cause these two verbs are aligned.
The first oc-currence of services is aligned to the first occur-rence of services in the English sentence and getsthe A1 label.
The second occurrence of servicesgets no argument label, because there is no align-ment between the C-A1 argument to, the head ofthe infinitival clause, and the French word services.The third line shows the analysis resulting from thesyntactic-semantic parser that has been trained on acorpus of French sentences labelled with automat-ically transferred annotations and syntactic annota-tions.
The parser has access to several labelled ex-amples of the predicate-argument structure of rester,which in many other cases is translated with remainand has the same predicate-argument structure asrester.
Consequently, the parser re-labels the verbwith REMAIN.01 and labels the argument with A3.Because the languages and annotation frameworkadopted in previous work are not directly compara-ble to ours, and their methods have been evaluatedon restricted test sets, results are not strictly com-parable.
But for completeness, recall that our bestresult for predicate identification is an F-measureof 55% accompanied with an F-measure of 60%for argument labelling.
Pado?
(2007) reports a 56%F-measure on transferring FrameNet roles, know-ing the predicate, from an automatically parsed andsemantically annotated English corpus.
Pado?
andPitel (2007), transferring semantic annotation toFrench, report a best result of 57% F-measure forargument labelling given the predicate.
Basili etal.
(2009), in an approach based on phrase-basedmachine translation to transfer FrameNet-like anno-tation from English to Italian, report 42% recall inidentifying predicates and an aggregated 73% recallof identifying predicates and roles given these pred-icates.
They do not report an unaggregated numberthat can be compared to our 60% argument labelling.In a recent paper, Annesi and Basili (2010) improvethe results from Basili et al (2009) by 11% usingHidden Markov Models to support the automaticsemantic transfer.
Johansson and Nugues (2006)trained a FrameNet-based semantic role labeller forSwedish on annotations transferred cross-linguallyfrom English parallel data.
They report 55% F-measure for argument labelling given the frame on150 translated example sentences.6 ConclusionsIn this paper, we have scaled up previous efforts ofannotation by using an automatic approach to se-mantic annotation transfer in combination with ajoint syntactic-semantic parsing architecture.
Wepropose a direct transfer method that requires nei-ther manual intervention nor a semantic ontology forthe target language.
This method leads to semanti-cally annotated data of sufficient quality to train asyntactic-semantic parser that further improves thequality of the semantic annotation by joint learningof syntactic-semantic structures on the target lan-guage.
The labelled F-measure of the resulting an-notations for predicates is only 4% point lower thanthe upper bound and the resulting annotations for ar-guments only 9%.AcknowledgementsThe research leading to these results has receivedfunding from the EU FP7 programme (FP7/2007-2013) under grant agreement nr 216594 (CLAS-SIC project: www.classic-project.org), and from theSwiss NSF under grant 122643.ReferencesA.
Abeille?, L. Cle?ment, and F. Toussenel.
2003.
Buildinga treebank for French.
In Treebanks: Building andUsing Parsed Corpora.
Kluwer Academic Publishers.303P.
Annesi and R. Basili.
2010.
Cross-lingual alignmentof FrameNet annotations through Hidden MarkovModels.
In Proceedings of CICLing.R.
Basili, D. De Cao, D. Croce, B. Coppola, and A. Mos-chitti, 2009.
Computational Linguistics and Intelli-gent Text Processing, chapter Cross-Language FrameSemantics Transfer in Bilingual Corpora, pages 332?345.
Springer Berlin / Heidelberg.M.-H. Candito, B.
Crabbe?, P. Denis, and F. Gue?rin.
2009.Analyse syntaxique du franc?ais : des constituantsaux de?pendances.
In Proceedings of la Confe?rencesur le Traitement Automatique des Langues Naturelles(TALN?09), Senlis, France.B.
Dorr.
1994.
Machine translation divergences: A for-mal description and proposed solution.
ComputationalLinguistics, 20(4):597?633.C.
J. Fillmore, R. Johnson, and M.R.L.
Petruck.
2003.Background to FrameNet.
International journal oflexicography, 16.3:235?250.P.
Fung, Z. Wu, Y. Yang, and D. Wu.
2007.
Learn-ing bilingual semantic frames: Shallow semantic pars-ing vs. semantic role projection.
In 11th Conferenceon Theoretical and Methodological Issues in MachineTranslation (TMI 2007).J.
Henderson, P. Merlo, G. Musillo, and I. Titov.
2008.
Alatent variable model of synchronous parsing for syn-tactic and semantic dependencies.
In Proceedings ofCONLL 2008, pages 178?182.R.
Hwa, P. Resnik, A. Weinberg, and O. Kolak.
2002.Evaluating translational correspondence using anno-tation projection.
In Proceedings of the 40th AnnualMeeting of the ACL.R.
Hwa, P. Resnik, A.Weinberg, C. Cabezas, and O. Ko-lak.
2005.
Bootstrapping parsers via syntactic projec-tion accross parallel texts.
Natural language engineer-ing, 11:311?325.R.
Johansson and P. Nugues.
2006.
A FrameNet-basedsemantic role labeler for Swedish.
In Proceedings ofthe annual Meeting of the Association for Computa-tional Linguistics (ACL).P.
Koehn.
2003.
Europarl: A multilingual corpus forevaluation of machine translation.J.
Lang and M. Lapata.
2010.
Unsupervised inductionof semantic roles.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 939?947, Los Angeles, California, June.Association for Computational Linguistics.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of English:the Penn Treebank.
Comp.
Ling., 19:313?330.P.
Merlo and L. van der Plas.
2009.
Abstraction and gen-eralisation in semantic role labels: PropBank, VerbNetor both?
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 288?296, Suntec, Singapore.A.
Meyers.
2007.
Annotation guidelines for NomBank- noun argument structure for PropBank.
Technicalreport, New York University.P.
Monachesi, G. Stevens, and J. Trapman.
2007.
Addingsemantic role annotation to a corpus of written Dutch.In Proceedings of the Linguistic Annotation Workshop(LAW), pages 77?84, Prague, Czech republic.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29:19?51.Sebastian Pado?
and Mirella Lapata.
2009.
Cross-lingualannotation projection of semantic roles.
Journal of Ar-tificial Intelligence Research, 36:307?340.S.
Pado?
and G. Pitel.
2007.
Annotation pre?cise dufranc?ais en se?mantique de ro?les par projection cross-linguistique.
In Proceedings of TALN.S.
Pado?.
2007.
Cross-lingual Annotation ProjectionModels for Role-Semantic Information.
Ph.D. thesis,Saarland University.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31:71?105.I.
Titov and J. Henderson.
2007.
A latent variable modelfor generative dependency parsing.
In Proceedings ofthe International Conference on Parsing Technologies(IWPT-07), pages 144?155, Prague, Czech Republic.I.
Titov, J. Henderson, P. Merlo, and G. Musillo.
2009.Online graph planarisation for synchronous parsing ofsemantic and syntactic dependencies.
In Proceedingsof the twenty-first international joint conference on ar-tificial intelligence (IJCAI-09), Pasadena, California,July.L.
van der Plas, T.
Samardz?ic?, and P. Merlo.
2010.
Cross-lingual validity of PropBank in the manual annotationof French.
In In Proceedings of the 4th Linguistic An-notation Workshop (The LAW IV), Uppsala, Sweden.D.
Wu and P. Fung.
2009a.
Can semantic role labelingimprove SMT?
In Proceedings of the Annual Confer-ence of European Association of Machine Translation.D.
Wu and P. Fung.
2009b.
Semantic roles for SMT:A hybrid two-pass model.
In Proceedings of theJoint Conference of the North American Chapter ofACL/Human Language Technology.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.
In-ducing multilingual text analysis tools via robust pro-jection across aligned corpora.
In Proceedings of theInternational Conference on Human Language Tech-nology (HLT).304
