Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 13?21,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsBootstrapping a historical commodities lexicon with SKOS and DBpediaEwan KleinILCC, School of InformaticsUniversity of EdinburghEH8 9AB, Edinburgh, UKewan@inf.ed.ac.ukBeatrice AlexILCC, School of InformaticsUniversity of EdinburghEH8 9AB, Edinburgh, UKbalex@inf.ed.ac.ukJim CliffordDepartment of HistoryUniversity of SaskatchewanSaskatoon, SK S7N 5A5, Canadajim.clifford@usask.caAbstractNamed entity recognition for novel domainscan be challenging in the absence of suitabletraining materials for machine-learning or lex-icons and gazetteers for term look-up.
We de-scribe an approach that starts from a small,manually created word list of commoditiestraded in the nineteenth century, and then usessemantic web techniques to augment the listby an order of magnitude, drawing on datastored in DBpedia.
This work was conductedduring the Trading Consequences project ontext mining and visualisation of historical doc-uments for the study of global trading in theBritish empire.1 IntroductionThe Trading Consequences project1aims to assist en-vironmental historians in understanding the economicand environmental consequences of commodity trad-ing during the nineteenth century.
We are applying textmining to large quantities of historical text in order toconvert unstructured textual information into structureddata that can be queried and visualised.
While prior his-torical research into commodity flows (Cronon, 1991;Cushman, 2013; Innis and Drache, 1995; McCook,2006; Tully, 2009) has focused on a small numberof widely traded natural resources, the large corporaof digitised documents processed by Trading Conse-quences is giving historians data about a much broaderrange of commodities.
A detailed appraisal of trade inthese resources will yield a significantly more accuratepicture of globalisation and its environmental conse-quences.In this paper we focus on our approach to building alexicon to support the recognition of commodity termsin text.
We provide some background to this work inSection 2.
In Section 3, we describe the process of cre-ating the lexicon; this starts from a manually collectedseed set of commodity terms which is then expandedsemi-automatically using DBpedia.2An evaluation ofthe quality of the commodity lexicon is provided inSection 4.1http://tradingconsequences.blogs.edina.ac.uk/2http://www.dbpedia.org2 BackgroundFigure 1 shows an overview of the architecture of theTrading Consequences system.
Input documents areprocessed by the text mining pipeline, which is basedon the LT-XML23and LT-TTT24toolkits (Grover et al.,2008).
After initial format conversion, the text under-DocumentsText MiningAnnotatedDocumentsXML 2 RDBCommoditiesRDBLexicons& GazetteersQuery InterfaceVisualisationCommoditiesOntologySKOSFigure 1: Architecture of the Trading Consequencesprototype.goes language identification and OCR post-correctionand normalisation.5It is then processed further by shal-low linguistic analysis, lexicon and gazetteer lookup,named entity recognition and grounding, and relationextraction (see Figure 2).In Trading Consequences, we determine which com-modities were mentioned when and in relation to which3LT-XML2 includes APIs for parsing XML documents(both as event streams and as trees), creating them, seri-alising them and navigating them with XPath queries; seehttp://www.ltg.ed.ac.uk/software/ltxml2.4LT-TTT2 is built around the LT-XML2 programs and pro-vides NLP components for a variety of text processing taskssuch as tokenisation and sentence-splitting, chunking andrule-based named entity recognition.
It includes a third partypart-of-speech tagger and lemmatiser; see http://www.ltg.ed.ac.uk/software/lt-ttt2.5For more details on dealing with OCR errors, see (Lo-presti, 2008; Alex et al., 2012).13Figure 2: Architecture of the text mining componentlocations.
We also determine whether locations arementioned as points of origin, transit or destination andwhether vocabulary relating to diseases and disastersappears in the text.
All mined information is addedback into the XML documents as different layers ofstand-off annotation.The annotations are subsequently used to populate arelational database.
This stores not just metadata aboutthe individual document, but also detailed informationthat results from the text mining, such as named enti-ties, relations, and how these are expressed in the rel-evant document in context.
Visualisations and a queryinterface access the database so that users can eithersearch the mined information directly through textualqueries or browse the data in a more exploratory man-ner.
A temporal dimension for the visualisation isprovided by correlating commodity mentions in doc-uments with the publication date of those documents.All information mined from the collections is linkedback to the original documents of the data providers.We analyse textual data from a variety of sources,including the House of Commons Parliamentary Pa-pers (HCPP)6from ProQuest;7the Early Canadiana On-line data archive (ECO) from Canadian.org;8the Di-rectors?
Correspondence Collection from the Archivesat Kew Gardens available at Jstor Global Plants(LETTERS);9Adam Matthew?s Confidential Print col-lections (CPRINT);10and a subpart of the Foreign andCommonwealth Office Collection (FCOC) from Jstor.11Together these sources amount to over 10 million pagesof text and over 7 billion word tokens.
Table 1 providesan overview of the number of documents and OCR scanimages per collection or sub-collection available to theTrading Consequences consortium.We used a variety of techniques for carrying outnamed entity recognition, covering not only commodi-ties, but also places, dates and amounts.
Figure 3 showssome of the entities which we extract from the text,6http://parlipapers.chadwyck.co.uk/home.do7http://www.proquest.co.uk8http://eco.canadiana.ca9http://plants.jstor.org/10http://www.amdigital.co.uk11http://www.jstor.org/Collection # of docs # of imagesHCPP 118,526 6,448,739ECO 83,016 3,938,758LETTERS 14,340 n/aCPRINT 1,315 140,010FCOC 1,000 41,611Table 1: Number of documents and images per collec-tion.
One image usually corresponds to one documentpage, except in the case of CPRINT, where it mostlycorresponds to two document pages.
The LETTERS col-lection does not contain OCRed text but summaries ofhand-written letters.e.g.
the places Padang and America, the year 1871,the commodity cassia bark and the quantity and unit6,127 piculs.
We are also able to identify that Padangis an origin location and America is a destination loca-tion and to ground both locations to geographical co-ordinates.
The commodity-place relations LOC(cassiabark, Padang) and LOC(cassia bark, America), visu-alised by the red arrows in Figure 3, are also identified.In this paper, our focus is on commodity mentions, andwe will discuss these in more detail in the next section.Figure 3: Excerpt from Spices (Ridley, 1912).
Ex-tracted entities are highlighted in colour and relationsare visualised using arrows.3 Lexicon ConstructionIn recent years, the dominant paradigm for NER hasbeen supervised machine learning (Tjong Kim Sangand De Meulder, 2003).
However, to be effective, thisrequires a considerable investment of effort in manu-ally preparing suitable training data.
Since we lackedthe resources to create such data, we decided insteadto provide the system with a look-up list of commodityterms.
While there is substantial continuity over time inthe materials that are globally traded as commodities,it is difficult to work with a modern list of commod-ity terms as they include many things that did not exist,or were not widely traded, in the nineteenth century.There are also a relatively large number of commodi-ties traded in the nineteenth century that are no longerused, including a range of materials for dyes and somenineteenth century drugs.
As a result, we set out to de-velop a new lexicon of commodities traded in the nine-teenth century.Before discussing in detail the methods that we used,it is useful to consider some of our requirements.
First14we wanted to be able to capture the fact that there canbe multiple names for the same commodity; for exam-ple, rubber might be referred to in several ways, includ-ing not just rubber but also India rubber, caoutchoucand caouchouc.
Second, we wanted to include a lim-ited amount of hierarchical structure in order to sup-port querying, both in the database interface and alsoin the visualisation process.
For example, it oughtbe possible to group together limes, apples and or-anges within a common category (or hypernym) suchas Fruit.
Third, we wanted the freedom to add arbi-trary attributes to terms, such as noting that both nutsand whales are a source of oil.These considerations argued in favour of a frame-work that had more structure than a simple list ofterms, but was more like a thesaurus than a dictionaryor linguistically-organised lexicon.12This made SKOS(Simple Knowledge Organization System?Miles andBechhofer (2009)) an obvious choice for organising thethesaurus.
SKOS assumes that the ?hierarchical back-bone?
of the thesaurus is organised around concepts.These are semantic rather than linguistic entities, andserve as the hooks to which lexical labels are attached.SKOS employs the Resource Description Framework(RDF)13as a representation language; in particular,SKOS concepts are identified by URIs.
Every concepthas a unique ?preferred?
(or canonical) lexical label (ex-pressed by the property skos:prefLabel), plus anynumber of alternative lexical labels (expressed by theproperty skos:altLabel).
Both of these RDF prop-erties take string literals (with an optional language tag)as values.The graph in Figure 4 illustrates how SKOS al-lows preferred and alternative lexical labels to be at-tached to a concept such as dbp:Natural_Rubber.Figure 4 illustrates a standard shortening for URIs,dbp:Natural_Rubberskos:Concept"rubber"@en"India rubber"@enrdf:typeskos:prefLabelskos:altLabelskos:altLabel"caoutchouc"@frFigure 4: Preferred and alternative lexical labels inSKOS.where a prefix such as dbp: is an alias for the names-pace http://dbpedia.org/resource/.
Con-sequently dbp:Natural\_Rubber is an abbrevia-tion that expands to the full URI http://dbpedia.12The Lemon lexicon model (McCrae et al., 2010) is basedon SKOS, but its richer structure, while linguistically well mo-tivated, is more complex than we require for our application.13http://www.w3.org/RDF/org/resource/Natural\_Rubber.
In an anal-ogous way, skos: and rdf: are prefixes that rep-resent namespaces for the SKOS and RDF vocabulariesrespectively.While a SKOS thesaurus provides a rich organisa-tional structure for representing knowledge about ourdomain, it is not in itself directly usable by our textmining tools; a further step is required to place theprefLabel and altLabel values from the the-saurus into the XML-based lexicon structure requiredby the LT-XML2 toolkit during named entity recogni-tion.
We will discuss this in more detail in Section 3.2.In the remainder of this section, we first describehow we created a seed set of commodity terms man-ually and then explain how we used it to bootstrap amuch larger commodity lexicon.3.1 Manual Curation from Archival SourcesWe took as our starting point the records of the Boardsof Customs, Excise, and Customs and Excise, and HMRevenue and Customs held at the National Archives.14They include a collation of annual ledger books list-ing all of the major goods, ranging from live animalsto works of art, imported into Great Britain during anygiven year during the nineteenth century.
These con-tain a wealth of material, including a list of the quantityand value of the commodities broken down by country.For the purpose of developing a list of commodities,we focused on the headings at the top of each page,drawing on the four books of the 1866 ledgers, whichwere the most detailed year available.15All together,the 1866 ledgers listed 760 different import categories.This data was manually transferred to a spreadsheet in amanner which closely reflected the original, and a por-tion is illustrated in Figure 5.
In Trading Consequenceswe restricted our analysis to raw materials or lightlyprocessed commodities and thereby discarded all com-modities which did not fit this definition.The two major steps in converting the CustomsLedger records into a SKOS format were (i) selecting astring to serve as the SKOS prefLabel, and (ii) asso-ciating the prefLabel with an appropriate semanticconcept.
Both these steps were carried out manually.16For obvious reasons, we wanted as far as possible touse an existing ontology as a source of concepts.
Weinitially experimented with UMBEL,17an extensive up-per ontology in SKOS format based on OpenCyc (Ma-tuszek et al., 2006).
However UMBEL?s coverage of rel-evant plants and botanical substances was poor, lacking14http://discovery.nationalarchives.gov.uk/SearchUI/details?Uri=C6715The customs ledgers used for creation of the seed set ofcommodities is stored at The National Archives (collectionCUST 5).16Assem et al.
(2006) present a methodology for convert-ing thesauri to SKOS format, but the resources that their casestudies take as a starting point are considerably more exten-sive and richly structured than the data we discuss here.17http://umbel.org15Animals Living - AssesAnimals Living - GoatsAnimals Living - KidsAnimals Living - Oxen and BullsAnimals Living - CowsAnimals Living - CalvesAnimals Living - Horses, Mares, Geldings, Colts and FoalsAnimals Living - MulesAnimals Living - SheepAnimals Living - LambsAnimals Living - Swine and HogsAnimals Living - Pigs (sucking)Animals Living - UnenmumeratedAnnatto - RollAnnatto - FlagAntimony - Ore ofAntimony - CrudeAntimony - RegulusApples - RawApples - DriedAqua Fortis - Nitric AcidFigure 5: Sample spreadsheet entries derived from1866 Customs Ledger.for instance entries for alizarin, bergamot andDammargum, amongst many others.
We eventually decided in-stead to base the ontology component of the lexiconon DBpedia (Bizer et al., 2009; Mendes et al., 2012),a structured knowledge base whose core concepts cor-respond to Wikipedia pages, augmented by Wikipediacategories, page links and infobox fields, all of whichare extracted as RDF triples.Figure 6 illustrates a portion of the converted spread-sheet, with columns corresponding to the DBpedia con-cept (using dbp: as the URI prefix), the prefLabel,and a list of altLabels.
Note that asses has beennormalised to a singular form and that it occurs as analtLabel for the concept dbp:Donkey.
This dataConcept prefLabel altLabeldbp:Cork_(material) corkdbp:Cornmeal cornmeal indian44corn4meal,4corn4mealdbp:Cotton cotton cotton4fiberdbp:Cotton_seed cotton4seeddbp:Cowry cowry cowriedbp:Coypu coypu nutria,4river4ratdbp:Cranberry cranberrydbp:Croton_cascarilla croton4cascarilla cascarilladbp:Croton_oil croton4oildbp:Cubeb cubeb cubib,4Java4pepperdbp:Culm culmdbp:Dammar_gum dammar4gum gum4dammardbp:Deer deerdbp:Dipsacus dipsacus 4teaseldbp:Domestic_sheep domestic4sheepdbp:Donkey donkey assdbp:Dracaena_cinnabari dracaena4cinnabari sanguis4draconis,4gum4dragon's4bloodFigure 6: Customs Ledger data converted to SKOS datatypes.
(in the form of a CSV file)18provides enough informa-18Together with other resources from Trading Conse-quences, the word list is available as base_lexicon.csvfrom the Github repository https://github.com/digtrade/digtrade.tion to build a rudimentary SKOS thesaurus whose rootconcept is tc:Commodity.19The following listingillustrates a portion of the thesaurus for donkey.20dbp:Donkeya skos:Concept ;skos:prefLabel "donkey"@en ;skos:altLabel "ass"@en ;skos:broader tc:Commodity ;prov:hadPrimarySource"customs records 1866" .Translated into plain English, this says: dbp:Donkeyis a skos:Concept, its preferred label is"donkey", its alternative label is "ass", it hasa broader concept tc:Commodity, and the primarysource of this information (i.e., its provenance) are thecustoms records of 1866.
Once we have an RDF modelof the thesaurus, it becomes straightforward to carryout most subsequent processing via query, constructand update operations in SPARQL (Prud?Hommeauxand Seaborne, 2008; Seaborne and Harris, 2013), thestandard language for querying RDF data.3.2 Bootstrapping the LexiconThe process just described allows us to construct asmall ?base?
SKOS thesaurus containing 319 concepts.However it is obviously a very incomplete list of com-modities, and by itself would give us poor recall inidentifying commodity mentions.
Many kinds of prod-uct in the Customs Ledgers included open ended sub-categories (i.e., Oil - Seed Unenumerated or Fruit - Un-enumerated Dried).
Similarly, while the ledgers pro-vided a comprehensive list of various gums, they onlyspecified anchovies, cod, eels, herrings, salmon andturtle as types of fish, grouping all other species underthe ?unenumerated?
subcategory.One approach to augmenting the thesaurus would beto integrate it with a more general purpose SKOS upperontology.
In principle, this should be feasible, sincemerging two RDF graphs is a standard operation.
How-ever, trying this approach with UMBEL threw up severalpractical problems.
First, UMBEL includes features thatgo beyond the standard framework of SKOS and whichmade graph merging harder to control.
Second, thistechnique made it extremely difficult to avoid adding alarge amount of information that was irrelevant to thedomain of nineteenth century commodities.Our second approach also involved graph merging,but tried to minimise manual intervention in determin-ing which subparts of the general ontology to mergeinto.
We have already mentioned that one of our orig-inal motivations for adopting SKOS was the presenceof a concept hierarchy; nevertheless, we had little needfor a multi-layered hierarchy of the kind found in many19The conversion from CSV to RDF was carried out with thehelp of the Python rdflib library (https://rdflib.readthedocs.org).20The prefixes tc: and prov: are aliases for http://vocab.inf.ed.ac.uk/tc/ and http://www.w3.org/ns/prov\# respectively.16upper ontologies.
In addition to a class hierarchy of theusual kind, DBpedia contains a level of category, de-rived from the categories that are used to tag Wikipediapages.
Figure 7 illustrates categories, such as Domes-ticated animals, that occur on the page for donkey.
Webelieve that such Wikipedia categories provide a usefuland (for our purposes) sufficient level of abstraction forgrouping together the ?leaf?
concepts that correspondto lexical items in the SKOS thesaurus (e.g., a conceptlike dbp:Donkey).
Within DBpedia, these categoriesare contained in the namespace http://dbpedia.org/resource/Category: (for which we use thealias dbc:) and are related to concepts via the prop-erty dcterms:subject.
Given that the concepts inFigure 7: Wikipedia categories at the bottom of thepage for Donkey.our base SKOS thesaurus are drawn from DBpedia, it issimple to augment the initial SKOS thesaurus G in thefollowing way: for each leaf concept L in G, augmentG with a new triple of the form ?L skos:broaderC?
(i.e., L has broader concept C) whenever L be-longs to category C in DBpedia.
To illustrate, givenour Donkey example above, we would supplement itwith the following triple:dbp:Donkeyskos:broader dbc:Domesticated_animalWe can retrieve all of the categories associated witheach leaf concept by sending a federated query that ac-cesses both the DBpedia SPARQL endpoint and a localinstance of the Jena Fuseki21server which hosts ourSKOS thesaurus.
Since some of the categories recov-ered in this way were clearly too broad or out of scope,we manually filtered the list down to a set of 355 cate-gories before merging the new triples into the base the-saurus.Our next step also involved querying DBpedia, thistime to retrieve all new concepts C which belonged tothe categories recovered in the first step; we call thissibling acquisition, since it allows us to find siblings ofleaf concepts that are children of the Wikipedia cate-gories already present in the thesaurus.
The key stepsin the procedure are illustrated in Figure 8 (where thetop node is the root concept in the SKOS thesaurus,viz.
tc:Commodity).
To continue our earlier exam-ple, the presence of dbc:Domesticated_animalin the hierarchy triggers the addition of concepts foranimals such as camel, llama and water buffalo.
Givena base thesaurus with 319 concepts, sibling acquisition21http://jena.apache.org/documentation/serving_data/base thesaurus category acquisition sibling acquisitionFigure 8: Sibling acquisition.
A base thesaurus is aug-mented with new categories (indicated as black ovals),and these in turn lead to the addition of new leafconcepts (indicated as black circles) which they arebroader than.expands the thesaurus to a size of 17,387 concepts.22This query-based methodology contrasts with, thoughis potentially complementary to, a machine learningapproach to bootstrapping named entity systems as de-scribed, for example, by Kozareva (2006).We mentioned earlier that in order for LT-TTT2 toidentify commodity mentions in text, it is necessary toconvert our SKOS thesaurus into an XML-based lexi-con structure.
A fragment of such a lexicon is illus-trated in Figure 9.
The preferred and alternative lexicallabels are represented via separate entries in the lex-icon, with their value contained in the word attributefor each entry.
The concept and category information isstored in corresponding attribute values; the pipe sym-bol (|) is used to separate multiple categories.
We havealready seen that alternative lexical labels will includesynonyms and spelling variants (e.g., chinchona ver-sus cinchona).
The set of alternative labels associatedwith each concept was further augmented by a series ofpostprocessing steps such as pluralisation; hyphenationand dehyphenation (cocoa nuts versus cocoa-nuts ver-sus cocoanuts; and the addition of selected head nounsto form compounds (apple > apple tree, groundnut >groundnut oil).
Such variants are also stored in the lexi-con as separate entries.
The resulting lexicon contained20,476 commodity terms.During the recognition step, we perform case-insensitive matching against the lexicon in combinationwith context-dependent rules to decide whether or nota given string is a commodity; the longest match is pre-ferred during lookup.
Linguistic pre-processing is im-portant in this step ?
for example, we exclude wordtokens tagged as verb, preposition, particle or adverbin the part-of-speech tagging.
As each lexicon entryis associated with a DBpedia concept and at least onecategory, both types of information are added to theextracted entity mentions for each successful match,thereby linking the text-mined commodities to the hier-archy present in the Trading Consequences commoditythesaurus.22We accessed DBpedia via the SPARQL endpoint on 16Dec 2013, which corresponds to DBpedia version 3.9.17<lex>...<lex category="Rubber|Nonwoven_fabrics" concept="Natural_rubber" word="caoutchouc"/><lex category="Rubber|Nonwoven_fabrics" concept="Natural_rubber" word="indian rubber"/><lex category="Rubber|Nonwoven_fabrics" concept="Natural_rubber" word="rubber"/>...</lex>Figure 9: Lexicon entries for the example presented in Figure 4.4 Evaluation4.1 MethodologyThe quality of text mining software is often evaluatedintrinsically in terms of the precision, recall and bal-anced F-score of its output compared to a human anno-tated gold standard.
We also use this methodology togain a better understanding of the quality of the com-modity lexicon.
We therefore prepared a gold stan-dard by randomly selecting 25 documents extracts fromeach of the five collections listed in Table 1.
Sincemany of the documents were too long to annotate intheir entirety, we split each file into sub-sections ofequal size (5000 bytes) and randomly selected one sub-section per document containing one or more com-modities and commodity-location relations.
This re-sulted in a set of 125 files which we divided into a pilotset of 25 documents (5 per collection) and a main an-notation set of 100 documents (20 per collection).Annotator 1 was provided with guidelines on mark-ing up entities and relations, and was asked to annotatethe 25 pilot documents using the BRAT annotation tool(Stenetorp et al., 2012).23After an opportunity to clar-ify any issues, Annotator 1 carried out the main anno-tation by correcting the system output and adding anyinformation that was missed by the text mining compo-nent.
We refer to the resulting human-annotated datasetas the gold standard and compare our system outputagainst it.
Table 2 shows that relative to our gold stan-dard annotations, the text mining prototype, which usesthe expanded commodity lexicon described in Section3.2), identified commodity mentions with a precision(P) of 0.59, a recall (R) of 0.56 and an F-score of 0.57.These scores are determined with a strict evaluationwhere each commodity mention identified by the sys-tem has to match the manually annotated mention ex-actly in terms of its boundaries and type to count asa true positive.
As soon as one boundary differs ?for example, if the annotator identified palm and thesystem identified palm trees ?- the mis-match countsas both a false positive and a false negative.
In orderto understand how often the commodity extraction re-sults in a boundary error, we also applied a lax evalua-tion where a true positive is counted if both boundariesmatch exactly; or if the left boundary differs and theright matches; or if the left boundary matches and the23The pilot data is not included in the gold standard that isused for the evaluation.right differs.
The improved scores for the lax evalua-tion listed in Table 2 show that boundary errors signif-icantly impact on system performance, with an equallynegative effect on recall and precision.Table 2 also gives inter-annotator agreement (IAA)scores for 25% of the gold standard.
IAA was calcu-lated by comparing the markup of Annotator 1 witha second annotator (Annotator 2) for the same data.The strict and lax scores show that IAA is not par-ticularly high (F=0.72 and F=0.80) for a task that weexpected to be fairly easy and that boundary errorsare also one of the reasons for the disagreement, al-beit not to such a large extent as in the system evalu-ation.
After having carried out some error analysis ofthe double-annotation, we realised that Annotator 2 hadnot completely understood our definition of commodityand had mistakenly included machinery and tools (e.g.,scissors) as well as general terms related to commodi-ties (e.g., produce).
Annotator 2 also missed severalrelevant commodity mentions which Annotator 1 hadcorrectly identified.
For these reasons, Annotator 2?smarkup was ignored when evaluating the text miningoutput.4.2 Analysis and Lexicon ModificationWhen examining the output of the text mining pro-totype, we found that it had identified a total of31,169,104 commodity mentions (tokens) across allfive collections.
However, these corresponded to only5,841 different commodity terms (types).
Since theTrading Consequences thesaurus contains 20,476 com-modity terms, only 28.5% of the content in the lexiconcorresponds to identifiable commodity mentions in thetext.
The top 1,757 most frequent commodity termsoccur at least 100 times in our data; they make up a to-tal of 31,113,978 commodity mentions in the text andtherefore amount to 99.8% of all commodity mentionsfound.
Figure 10 presents the average frequency dis-tribution of different commodity terms (separated intobins) across all text collections.The difference between the strict and lax bound-ary evaluations described above provide evidence thatsome of the commodity mentions in text were sub-strings of commodity terms in the lexicon (e.g., sealvs.
sealskins) and vice versa.
A detailed error analysisshowed that incorrect and missing entries in the lexiconfurther decrease precision and recall, respectively, andOCR errors occurring in the commodity terms in the18Evaluation TP FP FN P R F-scoreText Mining Strict 616 431 491 0.59 0.56 0.57Prototype Lax boundaries 791 256 316 0.76 0.71 0.73IAA Strict 283 112 109 0.72 0.72 0.72Lax boundaries 314 81 80 0.78 0.80 0.80Table 2: Precision (P), recall (R) and F-score figures for evaluating the performance of the commodity recog-nition prototype, as well as numbers of true positive (TP), false positive (FP) and false negative (FN) mentions.These figures are compared against equivalent inter-annotator agreement (IAA) scores in 25% of the gold standarddocuments.
We provide evaluation scores for strict and lax boundary matching of entity mentions.Evaluation TP FP FN P R F-scoreText Mining Prototype Strict 616 431 491 0.59 0.56 0.57Lax 791 256 316 0.76 0.71 0.73(i) Removal of lexicon errors Strict 603 331 504 0.65 0.54 0.59Lax 765 169 342 0.82 0.69 0.75(ii) Context Rules Strict 664 483 443 0.58 0.60 0.59Lax 777 370 330 0.68 0.70 0.69(iii) Bigram-based additions Strict 673 441 434 0.60 0.61 0.61Lax 855 259 252 0.77 0.77 0.77Modified Lexicon: Strict 652 353 455 0.65 0.59 0.62combination of (i)?
(iii) Lax 792 213 315 0.79 0.72 0.75Table 3: Precision (P), recall (R) and F-score figures for evaluating the performance of the commodity recognitionprototype compared to the same scores for two optimisation steps.
We provide evaluation scores for strict and laxboundary matching of entity mentions.text also considerably reduce recall (Alex and Burns,to appear).
In our gold standard, 9.1% (101 of 1,107)of all manually annotated commodity mentions containone or more OCR errors.
In order to improve the accu-racy of the lexicon, we carried out three modifications,which are described below.Step (i): Removal of errors from lexicon All com-modity terms below that of rank 1,757 (in bin 1,701?1,800 and subsequent bins) have a frequency of lessthan 100.
In Trading Consequences we are particularlyinterested in frequently occurring commodities as weaim to identify trends in trade.
Consequently one ofthe authors of this paper (an environmental historian)manually checked the correctness of the top 1,757 com-modity terms.
84 of them (4.8%) were considered to beerrors (either real errors, OCR errors, commodities out-side our scope, or overly-ambiguous terms) and weretherefore deleted from the lexicon.We then tested the effect this change had on the per-formance for against the gold standard.
The scores inTable 3 show that step (i), deleting incorrect entriesfrom the lexicon, has an expected positive effect onprecision, which increased by 0.06 (to P=0.65).
It alsoresulted in a small decrease in recall since Annotator1 had marked several instances of the word bread ascommodity mentions, which is arguably at the bound-ary of our definition of ?natural resources or lightlyprocessed commodities?.
He had also annotated pa-per and linen as commodity mentions, which are notwithin our definition.
Eliminating incorrect terms fromlexicon does not reduce the number of boundary er-rors made by the prototype, and consequently the laxboundary evaluation still results in an increase of 0.16in F-score compared to the strict evaluation (F=0.59versus F=0.75), the same as is the case for the proto-type.Step (ii): Context rules Having examined theboundary errors made by the prototype, we also appliedrules to extend commodity mentions to the left or rightin certain contexts.
We shift a boundary to the left if arecognised commodity mention is preceded by a nounor proper noun starting with an uppercase letter or ifit is preceded by another commodity mention.
Thisboundary shift is carried out to capture noun phrasesin which the recognised commodity mention is a headnoun which is then specified further by its immediateleft context (e.g., coffee is extended to Liberica coffeeor oil is combined with coconut to yield coconut oil).We shift a boundary to the right in the case where arecognised commodity is followed by the word tree ortrees (e.g., palm trees).
We tested the effect of apply-ing these context rules to the prototype (see step (ii)in Table 3).
While this post-processing step decreasesprecision very slightly, recall increases by 0.4.19Figure 10: Average frequency distribution of differentcommodity terms split into bins of size 100.
The Trad-ing Consequences data contains a total of 5,841 differ-ent commodity terms.
The graph is capped at the mostfrequent 2,000 terms as it would otherwise show a longinvisible tail of very low average frequencies.Step (iii): Bigram-based additions Finally, we con-ducted a frequency-based bigram analysis for a set oftrade-related terms like import, export, farm, planta-tion of the text-mined collections (see an example inFigure 11).
We manually examined frequently occur-ring left and right contexts of such words with the aimof identifying a list of terms for commodities of im-portance in the nineteenth century but which were notalready contained in the lexicon and were thereforemissed by the text mining.
We identified a list of 294commodity terms (including plural forms and spellingvariants) which we added to the lexicon.
Step (iii) inTable 3 shows that this change increases recall by 0.05and precision by 0.01.
When combining steps (i)?
(iii),we obtain the highest overall F-score of 0.62 with thestrict evaluation.5 ConclusionIn many named entity recognition tasks, there is rea-sonable agreement in advance about the ontologicalscope of a given class.
For example, when identify-ing mentions of people, locations, companies or datesin a corpus, we are not in doubt as to what consti-tutes these classes.
By contrast, in the Trading Conse-quences project, our goal was precisely to gain a betterunderstanding of what counted as a traded commod-ity during the nineteenth century.
In other words, wewere not only bootstrapping a lexicon, but were alsotrying to bootstrap the ontological class ?commodity?that was true for a specific time period.
Given a smallnumber of clear cases extracted from customs records,we used the categorial similarity of other entities to ourFigure 11: Most frequent tokens followed by the wordexport or exports found in the text-mined output of theHCPP data.
This list excludes all occurrences where theleft context is already recognised as a commodity.
Thecommodities grain and wine have been marked by anexpert historian as commodities that are missing fromthe lexicon.seed set as means of extrapolating to a much larger setof candidate commodities.
However, it is only whenthese candidates can be found as mentions in our cor-pus that we gain confidence in the belief that we reallyhave identified new commodities.
From the perspectiveof historical inquiry, progressing from around a dozenor so well-studied commodities in nineteenth centurytrade to around 2,000 is a significant step forward.The process of sibling acquisition via SPARQL queryto DBpedia is a novel contribution, as far as we areaware, and we have argued that it can help to gener-ate a lexicon which can be used as part of standardtechniques in natural language processing.
Althoughcomputational linguists are still relatively unfamiliarwith RDF as a data model, we believe that its flexibilitymake it well suited to capturing the combination of lex-ical and encyclopaedic knowledge that is central to thedigital history research described here.
In addition, bybasing our concepts on DBpedia, the ?linking?
aspect ofLinked Data (Heath and Bizer, 2011) gives us the po-tential to connect our commodity thesaurus to a wealthof other sources of knowledge about commodities.AcknowledgmentsThe Trading Consequences project is a Digging IntoData II project (CIINN01) funded by Jisc, AHRC andSSHRC.
It is a collaboration between the School ofInformatics at the University of Edinburgh, EDINA,SACHI at the University of St. Andrews, together withYork University and the University of Saskatchewan inCanada.
We are grateful to Kate Byrne and to the threereviewers for their insightful comments on the paper.20ReferencesBeatrice Alex and John Burns.
to appear.
Estimatingand rating the quality of optically character recog-nised text.
In Proceedings of DATeCH 2014.Bea Alex, Claire Grover, Ewan Klein, and Richard To-bin.
2012.
Digitised historical text: Does it have tobe mediOCRe?
In Proceedings of the LThist 2012workshop at KONVENS 2012, pages 401?409.Mark Assem, V?ronique Malais?, Alistair Miles, andGuus Schreiber.
2006.
A method to convert thesaurito SKOS.
In York Sure and John Domingue, edi-tors, The Semantic Web: Research and Applications,volume 4011 of Lecture Notes in Computer Science,pages 95?109.
Springer Berlin Heidelberg.Christian Bizer, Jens Lehmann, Georgi Kobilarov,S?ren Auer, Christian Becker, Richard Cyganiak,and Sebastian Hellmann.
2009.
Dbpedia ?
a crys-tallization point for the web of data.
Web Semantics,7(3):154?165, September.William Cronon.
1991.
Natures Metropolis: Chicagoand the Great West.
W. W. Norton, New York.Gregory T Cushman.
2013.
Guano and the Openingof the Pacific World: A Global Ecological History.Cambridge University Press, Cambridge.Claire Grover, Sharon Givon, Richard Tobin, and Ju-lian Ball.
2008.
Named entity recognition for digi-tised historical texts.
In Proceedings of the SixthInternational Conference on Language Resourcesand Evaluation (LREC?08), pages 1343?1346, Mar-rakech, Morocco.Tom Heath and Christian Bizer.
2011.
Linked Data:Evolving the Web into a Global Data Space.
Syn-thesis Lectures on the Semantic Web: Theory andTechnology.
Morgan & Claypool.Harold Innis and Daniel Drache.
1995.
Staples, Mar-kets, and Cultural Change Selected Essay.
McGill-Queens University Press, Montreal.Zornitsa Kozareva.
2006.
Bootstrapping named entityrecognition with automatically generated gazetteerlists.
In Proceedings of the Eleventh Conference ofthe European Chapter of the Association for Com-putational Linguistics: Student Research Workshop,EACL ?06, pages 15?21, Stroudsburg, PA, USA.Daniel Lopresti.
2008.
Optical character recognitionerrors and their effects on natural language process-ing.
In Proceedings of the second workshop on Ana-lytics for Noisy Unstructured Text Data, pages 9?16.Cynthia Matuszek, John Cabral, Michael J Witbrock,and John DeOliveira.
2006.
An introduction to thesyntax and content of Cyc.
In AAAI Spring Sym-posium: Formalizing and Compiling BackgroundKnowledge and Its Applications to Knowledge Rep-resentation and Question Answering, pages 44?49.Stuart McCook.
2006.
Global rust belt: Hemileia Vas-tatrix and the ecological integration of world coffeeproduction since 1850.
Journal of Global History,1(2):177?195.John McCrae, Guadalupe Aguado de Cea, Paul Buite-laar, Philipp Cimiano, Thierry Declerck, Asun-ci?n G?mez P?rez, Jorge Gracia, Laura Hollink,Elena Montiel-Ponsoda, Dennis Spohr, and To-bias Wunner, 2010.
The Lemon Cookbook.
TheMonnet Project.
http://lemon-model.net/lemon-cookbook.pdf.P.N.
Mendes, M. Jakob, and C. Bizer.
2012.
DBpe-dia: A multilingual cross-domain knowledge base.In Proceedings of the International Conference onLanguage Resources and Evaluation (LREC 2012).Alistair Miles and Sean Bechhofer.
2009.
SKOSsimple knowledge organization system ref-erence.
W3C recommendation, W3C, Au-gust.
http://www.w3.org/TR/2009/REC-skos-reference-20090818/.E.
Prud?Hommeaux and A. Seaborne.
2008.
Sparqlquery language for rdf.
W3C working draft, 4(Jan-uary).Henry Nicholas Ridley.
1912.
Spices.
London,Macmillan and co. Ltd.Andy Seaborne and Steven Harris.
2013.
SPARQL1.1 query language.
W3C recommendation,W3C, March.
http://www.w3.org/TR/2013/REC-sparql11-query-20130321/.Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012.
BRAT: A web-based tool for NLP-assisted text annotation.
In Proceedings of theDemonstrations at the 13th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, EACL ?12, pages 102?107, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InProceedings of the Seventh Conference on NaturalLanguage Learning, CONLL ?03, pages 142?147,Stroudsburg, PA, USA.John Tully.
2009.
A victorian ecological disaster: Im-perialism, the telegraph, and gutta-percha.
Journalof World History, 20(4):559?579.21
