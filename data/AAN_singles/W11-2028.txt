Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 259?265,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsToward Construction of Spoken Dialogue Systemthat Evokes Users?
Spontaneous BackchannelsTeruhisa Misu, Etsuo Mizukami, Yoshinori Shiga, Shinichi Kawamoto?,Hisashi Kawai and Satoshi NakamuraNational Institute of Information and Communications Technology (NICT), Kyoto, Japan.teruhisa.misu@nict.go.jpAbstractThis paper addresses a first step toward aspoken dialogue system that evokes user?sspontaneous backchannels.
We constructan HMM-based dialogue-style text-to-speech(TTS) system that generates human-like cuesthat evoke users?
backchannels.
A spokendialogue system for information navigationwas implemented and the TTS was evaluatedin terms of evoked user backchannels.
Weconducted user experiments and demonstratedthat the user backchannels evoked by our TTSare more informative for the system in detect-ing users?
feelings than those by conventionalreading-style TTS.1 IntroductionOne of the most enduring problems in spoken di-alogue systems research is realizing a natural dia-logue in a human-human form.
One direction re-searchers have been utilizing spontaneous nonverbaland paralinguistic information.
For example,This paper focuses on backchannels, one of themost common forms of para-linguistic informationin human-human dialogue.
In particular, we focuson users?
verbal feedback, such as ?uh-huh?
(calledAizuchi in Japanese), and non-verbal feedback in theform of nods.
Such backchannels are very com-mon phenomena, and considered to be used to fa-cilitate smooth human-human communications.
Inthis regard, Maynard (Maynard, 1986) indicated thatsuch backchannels are listener?s signals to let thespeaker continue speaking (continuer), to indicatethat the listener understands and consents.
It wasalso hypothesized that humans detect feelings ex-pressed via backchannels, and the correlation be-tween backchannel patterns and user interests wasexamined (Kawahara et al, 2008).
These studies in-dicate that detection of spontaneous user backchan-?
currently with Japan Advanced Institute of Science andTechnology (JAIST)nels can benefit spoken dialogue systems by provid-ing informative cues that reflect the user?s situation.For instance, if a spoken dialogue system can detectuser?s backchannels, it can facilitate smooth turn-taking.
The system can also detect user?s feelingsand judge if it should continue the current topic orchange it.Despite these previous studies and decades ofanalysis on backchannels, few practical dialoguesystems have made use of them.
This is proba-bly due to the fact that users do not react as spon-taneously to dialogue systems as they do to otherhumans.
We presume one of the reasons for thisis the unnatural intonation of synthesized speech.That is, conventional speech synthesizers do not pro-vide users with signs to elicit backchannels; an ap-propriate set of lexical, acoustic and prosodic cues(or backchannel-inviting cues (A. Gravano and J.Hirschberg, 2009)), which tends to precede the lis-tener?s backchannels in human-human communica-tion.
Though recorded human speech can providesuch cues, it is costly to re-record system?s speechevery time system scripts are updated.
In this work,we therefore tackle the challenge of constructingdialogue-style text-to-speech (TTS) system that in-spires users to make spontaneous backchannels un-der the hypothesis of:People will give more spontaneous backchannels toa spoken dialogue system that makes more spontaneousbackchannel-inviting cues than a spoken dialogue systemthat makes less spontaneous ones.which is derived from the Media Equation (Reevesand Nass, 1996).2 Related WorksA number of studies have aimed at improvingthe naturalness of TTS.
Though most of thesehave focused on means of realizing a clear andeasy-to-listen-to reading-style speech, some at-tempts have been made at spontaneous conversa-tional speech.
Andersson (Andersson et al, 2010)and Marge (Marge et al, 2010) focused on lexi-259cal phenomena such as lexical filler and acknowl-edgments in spontaneous speech, and showed thatinserting them improves the naturalness of human-computer dialogues.
In this work, we tackle con-structing a natural dialogue-style TTS system focus-ing on prosodic phenomena such as intonation andphoneme duration.In the field of conversation analysis, many studiesanalyzed backchannels in human-human dialoguefocusing on lexical and non-verbal cues (Koiso etal., 1998; Ward and Tsukahara, 2000; A. Gravanoand J. Hirschberg, 2009).
For instance these cueswere examined in preceding utterances, such as inpart-of-speech tags, length of pause, power contourpattern, and F0contour pattern around the end ofthe Inter-Pausal Units (IPUs).
(A. Gravano and J.Hirschberg, 2009) showed that when several of theabove cues occur simultaneously, the likelihood ofoccurrence of a backchannel will increase.Several studies also utilized the above findingsfor spoken dialogue systems.
Okato (Okato et al,1996) and Fujie (Fujie et al, 2005) trained models topredict backchannels, and implemented spoken di-alogue systems that make backchannels.
Our goaldiffers in that it is to inspire users to give backchan-nels.3 Construction of Spoken Dialogue TTS3.1 Spoken Dialogue Data collection for TTSIn order to make spontaneous dialogue-style TTSthat can evoke backchannels, we construct a spon-taneous dialogue-style speech corpus that containsbackchannel-inviting cues, and then train an HMMacoustic model for synthesis.We collected our training data by dubbing a scriptof our Kyoto Sightseeing Guidance Spoken Dia-logue Corpus (Misu et al, 2009), a set of itinerary-planning dialogues in Japanese.
In the dialoguetask, the expert guide has made recommendations onsightseeing spots and restaurants until has decidedon a plan for the day.
With the guide?s recommen-dations, many users give spontaneous backchannels.We made a set of dialogue scripts from the corpus,and asked voice actors to act them out.When preparing the dialogue script for dubbing,we first removed fillers and backchannels from thetranscripts of the dialogue corpus.
We then anno-tated the guide?s end of the IPUs, where the theuser made backchannels, with #.
A sample dialoguescript is shown in Figure 6.
We asked two profes-sional voice actresses to duplicate the spoken dia-logue of the script, with playing the role of the tourguide, and the other as the tourist, sitting face-to-face.
During the recording, we asked the tour guiderole to read the scenario with intonation so that thetourist role would spontaneously make backchan-nels at the points marked with #.
The tourist wasallowed to make backchannels at will at any pausesegments the guide made.
We recorded 12 dialoguesessions in total.
The speech data was manually la-beled, and 239.3 minutes of tour guide utterances,which are used to train our HMM for the TTS sys-tem, were collected.
The training data is comple-mented by the ATR 503 phonetically balanced sen-tence set (Abe et al, 1990), so as to cover deficien-cies in the phoneme sequence.
The sentence set iscollected from news articles, and data consists of43.1 minutes of reading-style speech.3.2 Analysis of Collected Speech DataBefore training the HMM, we analyzed the collectedspoken dialogue data to confirm if the recorded di-alogue speech data contained backchannel-invitingprosodic cues.
We compared prosodic features ofthe dialogue speech data with those of the reading-style speech data (phonetically balanced sentencesthat we collected).
Following the findings of a pre-vious study (Koiso et al, 1998), we investigated theduration, F0contour pattern and power contour pat-tern of the final phoneme of the IPUs1.In conversation analysis of Japanese, the F0con-tour pattern label of the final phoneme is often used.While the contour pattern is usually manually la-beled, we roughly determined the patterns based onthe following procedure.
We first normalized the logF0scale using all utterances so that it has zero meanand one standard deviation (z-score: z = (x??)/?
).We then divided each final phoneme of the IPU intoformer and latter parts, and calculated the F0slopeof each segment by linear regression.
By combina-tion of following three patterns, we defined nine F0contour patterns for the final phonemes of the IPUs.The pattern of the segment was judged as rise if theslope was larger than a threshold ?.
If the slope wasless than the threshold?
?, the pattern was judged asfall.
Otherwise, it was judged as flat.
Here, ?
wasempirically set to 5.0.
The power contour patternsof the IPUs were estimated by a similar procedure.We analyzed 3,311 IPUs that were not followed1For this study, we define an IPU as a maximal sequenceof words surrounded by silence longer than 200 ms.
This unitusually coincides with one Japanese phrasal unit.260Table 1: Prosodic analysis of final phonemes of IPUs(dialogue script vs. newsarticle script)dialogue newsarticledur.
phoneme [msec] 177.1 (?
83.6) 119.4 (?
31.3)average (?
standard deviation)F0powerpattern dialogue news dialogue newsrise-rise 3.7 % 10.4 % 0.0 % 0.0 %rise-flat 2.6 % 2.1 % 0.0 % 0.0 %rise-fall 18.8 % 3.2 % 0.0 % 0.0 %flat-rise 4.8 % 11.5 % 0.0 % 0.0 %flat-flat 3.5 % 1.8 % 0.0 % 9.2 %flat-fall 12.6 % 2.7 % 13.6 % 0.1 %fall-rise 29.2 % 47.0 % 0.0 % 0.0 %fall-flat 7.7 % 9.0 % 86.0 % 90.7 %fall-fall 17.1 % 12.3 % 0.0 % 0.0 %by a turn-switch in the dialogue-style speech dataand 645 non-sentence-end IPUs in the reading-style speech data.
The prosodic features of finalphonemes of these IPUs are listed in Table 1.According to a study (Koiso et al, 1998), in whichprosodic features of IPUs followed by a turn-holdwith backchannel, without backchannel and turn-switch were compared, a long duration in the finalphoneme is a speaker?s typical sign to keep floor.The same study also reported that the flat-fall andrise-fall pattern of F0and power are more likelyto be followed by a backchannel than a turn-holdwithout a backchannel and turn-switch.
In our col-lected speech corpus, there were actually signifi-cant (p < 0.01) differences in the duration of thefinal phoneme between that in the dialogue-stylespeech and in reading-style speech.
There wasalso significant (p < 0.01) difference in the oc-currence probability of the above two prosodic pat-terns between dialogue-style speech and reading-style speech data.
These figures indicate thatas a whole the collected dialogue-style data con-tains more backchannel-inviting cues than collectedreading-style speech data.We trained HMM for our TTS system Ximerausing the HMM-based Speech Synthesis System(HTS) (Zen et al, 2007).
We adopted mel log spec-trum approximation (MLSA) filter-based vocod-ing (SPTK, 2011), a quint-phone-based phonemeset and five state HMM-based acoustic modeling.All training data including reading-style speech datawere used for model training.4 User Experiment4.1 Dialogue System used for ExperimentTo evaluate our TTS system based on users?
reac-tions, a sightseeing guidance spoken dialogue sys-Figure 1: Screen shot of the dialogue systemtem that assist users in making decision was im-plemented.
The system can explain six sightseeingspots in Kyoto.
The system provides responses touser requests for explanation about a certain spot.Each descriptive text on a sightseeing spot consistsof 500 (?1%) characters, 30 phrases.
The text issynthesized using section 3 TTS2.
We set the speechrate of our TTS as nine phoneme per second.A display is used to present photos of the tar-get sightseeing spot and an animated 3D desktopavatar named Hanna.
Figure 1 shows the GUIthe user sees.
The avatar can express its statusthrough several motions.
For example, when theuser begins speaking, it can express the state oflistening using the listener?s motion, as shown inthe figure.
A sample dialogue with the system isshown in Table 7.
A video (with English subtitles)of an sample dialogue with a user can be seen athttp://mastarpj.nict.go.jp/?xtmisu/video/TTS.wmv.To compare the effectiveness of our TTS inevoking users?
spontaneous backchannels, we con-structed a comparison system that adopts a conven-tional reading-style TTS system.
An HMM modelwas trained using 10-hour reading-style speech byanother professional female narrator.
Other settings,such as the descriptive text and avatar agent, werethe same as those of the base system.4.2 Comparison of Prosodic Features of theSynthesized SpeechPrior to the experiments, we investigated theprosodic features of the final phoneme of IPUs inthe synthesized explanations on six spots to confirmif they contain backchannel-inviting cues.
The re-sults are given in Table 2.Tendencies in the duration of the final phonemeand prosody pattern distribution of the synthesized2The descriptive texts are not included in the training data.261Table 2: Prosodic analysis of final phonemes of IPUs(dialogue-style TTS vs. reading-style TTS)dialogue synth.
reading synth.dur.
phoneme [msec] 172.9 (?
29.6) 126.1 (?
19.1)average (?
standard deviation)F0powerpattern dialogue reading dialogue readingrise-rise 5.4 % 0.0 % 0.0 % 0.0 %rise-flat 2.0 % 0.0 % 1.7 % 0.0 %rise-fall 23.5 % 0.0 % 46.3 % 5.3 %flat-rise 5.0 % 0.0 % 0.0 % 0.0 %flat-flat 1.7 % 0.0 % 4.0 % 9.2 %flat-fall 15.8 % 0.0 % 22.8 % 18.1 %fall-rise 15.8 % 0.0 % 0.7 % 0.0 %fall-flat 3.4 % 0.0 % 7.0 % 0.0 %fall-fall 27.5 % 100.0 % 17.4 % 76.5 %speech by the dialogue-style TTS system were simi-lar to that of recorded dialogue speech, suggests thatthe constructed dialogue-style TTS system can du-plicate the backchannel-inviting cues of the recordedoriginal speech.
The synthesized dialogue-stylespeech also contained much more rise-fall and flat-fall patterns in F0and power than that generated bythe reading-style TTS system.
The average dura-tion of the final phoneme was also longer.
Consider-ing the fact that the speech data was generated fromthe same script, this indicates that the synthesizedspeech by the dialogue-style TTS system containsmore backchannel-inviting features than that by thereading-style TTS system.4.3 Experimental SetupWe evaluated the TTS systems using 30 subjectswho had not previously used spoken dialogue sys-tems.
Subjects were asked to use the dialogue sys-tem in two settings; dialogue-style TTS system andreading-style TTS system.
The experiment was con-ducted in a small (about 2 m2) soundproof roomwith no one else present.We instructed the subjects to speak with the avataragent Hanna (not with the system).
We also toldthem that the avatar agent was listening to theirspeech at all times using the microphone, and wasobserving their reactions using the camera above thedisplay3.
Subjects were given the task of acquiringinformation about three candidate sightseeing spotsin Kyoto shown on the display and then selectingone that they liked.
An example dialogue with thesystem is shown in Table 7.
A video (with Englishsubtitles) showing a real user dialogue can be seenat http://mastarpj.nict.go.jp/?xtmisu/video/exp.avi.3The system did not actually sense the subjects?
reactions.Table 3: Questionnaire items1.
Overall, which speech was better?2.
Which speech had easier-to-understand explanations?3.
For which speech did you feel compelled to givebackchannels?4.
Which speech was more appropriate for this system?5.
Which speech had more human-like explanation?
(a) both(b) dialogue style(c) reading style(d) neither#5#4#3#2#1Figure 2: Questionnaire resultsAfter the subject selected from candidate spots,we changed the TTS system settings and instructedthe user to have another dialogue session selectingone of another three spots.
Considering the effects ofthe order, the subjects were divided into four groups;the first group (Group 1) used the system in the orderof ?Spot list A with dialogue-style speech ?
Spotlist B with reading-style speech,?
the second group(Group 2) worked in reverse order.
Groups 3 and 4used a system alternating the order of the spot sets.5 Experimental Results5.1 Questionnaire ResultsAfter the experiments, subjects were asked to fill ina questionnaire about the system.
Table 3 shows thequestionnaire items.
The subjects selected (a) bothare good, (b) dialogue-style speech was better, (c)reading-style speech was better, or (d) neither weregood.
Figure 2 shows the results.The dialogue-style speech generally earnedhigher ratings, but reading-style was slightly higherin items #2 and #5.
This tendency is likely at-tributable to the fact that the dialogue-style speechhad worse clarity and naturalness than reading-style.The mean opinion score (MOS), which is often usedto measure clarity and naturalness of TTS, of thedialogue-style TTS was in fact 2.79, worse than 3.74for the reading-style.5.2 Analysis of Frequency of BackchannelsWe analyzed the number of backchannels that usersmade during the dialogue session.
We manuallyannotated subjects?
verbal feedbacks, such as ?uh-huh?
and nodding of the head using the recordedvideo.
Out of 30 subjects, 26 gave some form of262Table 4: Percentages and average number of users who made backchannelsTTS % users made BCs # average BCs takenGroup 1: (Dialogue?
Reading) Dialogue-style 100.0% (50.0%, 100.0%) 30.4 (1.8, 28.6)(Spot list A?
Spot list B) Reading-style 100.0% (50.0%, 87.5%) 26.1 (3.1, 23.0)Group 2: (Reading?
Dialogue) Dialogue-style 75.0% (25.0%, 62.5%) 12.7 (0.5, 12.2)(Spot list A?
Spot list B) Reading-style 75.0% (25.0%, 62.5%) 12.9 (1.3, 11.6)Group 3: (Dialogue?
Reading) Dialogue-style 100.0% (28.6%, 100.0%) 14.0 (0.4, 13.6)(Spot list B?
Spot list A) Reading-style 100.0% (0%, 100.0%) 19.3 (0, 19.3)Group 4: (Reading?
Dialogue) Dialogue-style 87.5% (42.9%, 87.5%) 28.2 (4.7, 23,5)(Spot list B?
Spot list A) Reading-style 100.0% (71.4%, 87.5%) 24.8 (6.5, 18.3)All: Dialogue-style 86.7% (36.7%, 86.7%) 21.1 (1.7, 19.4)Reading-style 90.0% (40.0%, 83.3%) 20.6 (2.4, 18.2)Total backchannel (verbal feedback [Aizuchi], nodding)backchannel to the system.
Table 4 shows the per-centages and average number of times subjects gavebackchannels.
Many users made more backchannelsusing the dialogue-style TTS system.
Despite thesignificant difference in questionnaire item #3, therewere no significant differences in the average num-ber of users?
backchannels.5.3 Informativeness of BackchannelsWe then evaluated the TTS in terms of the informa-tiveness of evoked backchannels.
The spontaneousprosodic pattern of the backchannels is expectedto suggest positive/negative feelings on regardingthe recommended candidate.
One promising useof backchannels in our application is for detectingusers?
feelings about the currently focused on spot,and choosing to continue the explanation on the cur-rent topic if the user seems interested, or otherwisechange the topic.
We therefore label backchannelsmade during the systems explanation of the spotthat the user finally selected as ?positive?
and thosemade during the explanations of the other two spotsas ?negative?
and consider distinguishing betweenthem.
In human-human dialogues, it was confirmedthat when a user responds promptly, the majority ofresponses are positive, and more backchannels alsosuggest positive responses (Kawahara et al, 2008).We investigated the informativeness of thebackchannels based on their classification rate, orwhether the system can distinguish positive and neg-ative backchannels, using 10-fold cross-validation.That is, the backchannels evoked by the dialogue-style TTS system were divided into 10 groups andnine were used for training and the other for classi-fication tests.
We trained decision trees using J4.8algorithm using timing, frequency, total frequencythroughout the session and type of backchannel (ver-bal feedback or nod) as the feature set.
The classifi-cation error cost of the positive sample was set to (#negative samples / # positive samples) consideringthe difference in the number of positive and nega-tive samples.
Ten trials were conducted by chang-ing the test set and the average classification ratewas calculated.
The classification rate of backchan-nels evoked by the system with dialogue-style TTSwas 71.4%, The confusion matrix of the classifi-cation is shown below.
We obtained precisions of62.8% in the classification of the positive backchan-nels, and 73.2% in that of the negative backchan-nels.
The rates are significantly higher than chancerates of 33.5% and 66.5%.
This result indicatesthe backchannels evoked by the dialogue-style TTSwere informative for the system.Table 5: Confusion matrix of classification?
classified as positive negative?
labelpositive 76 141negative 45 386The classification rate of the reading-style TTSsystem was calculated in the same way.
The av-erage classification rate of backchannels evoked byreading-style TTS was a significantly lower 47.4%,meaning they were not informative at all.These results suggest that our dialogue-style TTSsystem can evoke more spontaneous and informativebackchannels that reflects users?
intentions than theconventional reading-style one.
This classificationrate is not completely satisfactory, but we expect thatusers?
feeling can be detected after observing severalbackchannels.
We also believe that we can estimateusers?
interest more precisely by combining verbalinformation of dialogue acts (Misu et al, 2010).6 ConclusionsThis paper presented our first steps toward a spokendialogue system that evokes users?
spontaneous lis-tener?s reactions.
We constructed a dialogue-styleTTS and confirmed that by generating human-likebackchannel-inviting cues, the system can evokeuser?s spontaneous backchannels, which are infor-mative for the system.263ReferencesA.
Gravano and J. Hirschberg.
2009.
Backchannel-inviting cues in task-oriented dialogue.
In Proc.
In-terspeech, pages 1019?1022.M.
Abe, Y. Sagisaka, T. Umeda, and H. Kuwabara.
1990.Speech Database User?s Manual.
ATR Technical Re-port TR-I-0166.S.
Andersson, K. Georgila, D. Traum, and R. ClarkM.
Aylett.
2010.
Prediction and Realisation of Con-versational Characteristics by Utilising SpontaneousSpeech for Unit Selection.
In Proc.
Speech Prosody.S.
Fujie, K. Fukushima, and T. Kobayashi.
2005.
Back-channel feedback generation using linguistic and non-linguistic information and its application to spoken di-alogue system.
In Proc.
Interspeech, pages 889?892.T.
Kawahara, M. Toyokura, T. Misu, and C. Hori.
2008.Detection of Feeling Through Back-Channels in Spo-ken Dialogue.
In Proc.
Interspeech, pages 1696?1696.H.
Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, andY.
Den.
1998.
An Analysis of Turn-Taking andBackchannels based on Prosodic and Syntactic Fea-tures in Japanese Map Task Dialogue.
Language andSpeech, 41(3-4):295?322.M.
Marge, J. Miranda, A.
Black, and A. I. Rudnicky.2010.
Towards Improving the Naturalness of SocialConversations with Dialogue Systems.
In Proc.
SIG-DIAL, pages 91?94.S.
Maynard.
1986.
On back-channel behavior injapanese and english casual conversation.
Linguistics,24(6):1079?1108.T.
Misu, K. Ohtake, C. Hori, H. Kashioka, and S. Naka-mura.
2009.
Annotating Communicative Functionand Semantic Content in Dialogue Act for Construc-tion of Consulting Dialogue Systems.
In Proc.
Inter-speech.Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, ChioriHori, Hideki Kashioka, Hisashi Kawai, and SatoshiNakamura.
2010.
Dialogue Strategy Optimizationto Assist User?s Decision for Spoken Consulting Di-alogue Systems.
In Proc.
IEEE-SLT, pages 342?347.Y.
Okato, K. Kato, M. Yamamoto, and S. Itahashi.
1996.Insertion of interjectory response based on prosodicinformation.
In Proc.
of IEEE Workshop Interac-tive Voice Technology for Telecommunication Applica-tions, pages 85?88.B.
Reeves and C. Nass.
1996.
The Media Equation: HowPeople Treat Computers, Television, and New MediaLike Real People and Places.
Cambridge UniversityPress.SPTK, 2011.
Speech Signal Processing Toolkit (SPTK).http://sp-tk.sourceforge.net/.N.
Ward and W. Tsukahara.
2000.
Prosodic fea-tures which cue backchannel responses in English andJapanese.
Journal of Pragmatics, 32(8):1177?1207.H.
Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A.W.Black, and K. Tokuda.
2007.
The HMM-based speechsynthesis system version 2.0.
In Proc.
ISCA SSW6.A Appendix264Table 6: Example dialogue scenario used for recording (translation of Japanese)Guide And Kyomizu-dera.Guide Yeah, it is very famous for.Guide There is this like wooden balcony#, and you can?t see the whole city#, but you can see agood part of the city from there.
And it is a beautiful view.User It is nice.
You have been there?Guide Yes, many a times.Guide Unfortunately, in winter it is not very green#, and there are not any flowers.User So if there is snow will they close the stairs or will they - we are just going to be carefulwalking the steps?Guide You have to be careful going up.Guide Basically, it?s a slope it?s a slope#, so if it?s very icy - if there is, you know, like blackice#, you might want to rethink hiking up that slope.
But usually it is okay# in the winter.I mean, Kyomizu-dera and this whole area, like every tourist eventually makes their wayhere#, whether they are Japanese or foreign.Guide So, I am sure the paths should be clear.Guide Yeah, that is a good spot too.Table 7: Example dialogue with the system (translation of Japanese)System Hello, my name is Hanna, a tour guide for sightseeing in Kyoto.
Nice to meet you.
Pleasesay ?Okay?
when you have decided which spot you would like to know about first.User Okay.System Which spot would you like to know about?User Tell me about Shinnyo-do temple.System Sure, I?ll explain Shinnyo-do temple.System Shinnyo-do is a Buddhist Tendai Temple in Sakyo-ku, Kyoto City and the formal name isShinsho Gokuraku-ji.
You may take the bus directly from Kyoto Station for about thirtyminutes, and the fee is two hundred twenty yen.
The admission fee for the temple is fivehundred yen.
The temple was established in nine hundred eighty-four A.D., when AmidaNyorai-zo; a principle image from Mt.
Hiei was placed in the Imperial villa of this area.System Shall I continue?User Sure.System The temple is famous, especially for its coloring of autumn leaves; Red Maple stand on bothsides of the stone-paved approach, and it appears as though the temple is placed inside amaple forest.
.
.
.. .
.System That?s all about this spot.System Which spot would you like to know about next?User Daitoku-ji please.. .
.265
