Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 111?119,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsExploring Miscommunication and Collaborative Behaviour in Hu-man-Robot InteractionTheodora KoulouriDepartment of Information Systems andComputingBrunel UniversityMiddlesex UB8 3PHtheodora.koulouri@brunel.ac.ukStanislao LauriaDepartment of Information Systemsand ComputingBrunel UniversityMiddlesex UB8 3PHstasha.lauria@brunel.ac.ukAbstractThis paper presents the first step in de-signing a speech-enabled robot that is ca-pable of natural management of mis-communication.
It describes the methodsand results of two WOz studies, in whichdyads of na?ve participants interacted in acollaborative task.
The first WOz studyexplored human miscommunicationmanagement.
The second study investi-gated how shared visual space and moni-toring shape the processes of feedbackand communication in task-oriented inte-ractions.
The results provide insights forthe development of human-inspired androbust natural language interfaces in ro-bots.1 IntroductionRobots are now escaping laboratory and indus-trial environments and moving into our homesand offices.
Research activities have focused onoffering richer and more intuitive interfaces,leading to the development of several practicalsystems with Natural Language Interfaces(NLIs).
However, there are numerous open chal-lenges arising from the nature of the mediumitself as well as the unique characteristics ofHuman-Robot Interaction (HRI).1.1 Miscommunication in Human-RobotInteractionHRI involves embodied interaction, in whichhumans and robots coordinate their actions shar-ing time and space.
As most speech-enabled ro-bots remain in the labs, people are generally un-aware of what robots can understand and do re-sulting in utterances that are out of the functionaland linguistic domain of the robot.
Physical co-presence will lead people to make strong butmisplaced assumptions of mutual knowledge(Clark, 1996), increasing the use of underspeci-fied referents and deictic expressions.
Robotsoperate in and manipulate the same environmentas humans, so failure to prevent and rectify errorshas potentially severe consequences.
Finally,these issues are aggravated by unresolved chal-lenges with automatic speech recognition (ASR)technologies.
In conclusion, miscommunicationin HRI grows in scope, frequency and costs, im-pelling researchers to acknowledge the necessityto integrate miscommunication in the designprocess of speech-enabled robots.1.2 Aims of studyThe goal of this study is two-fold; first, to incor-porate ?natural?
and robust miscommunicationmanagement mechanisms (namely, preventionand repair) into a mobile personal robot, which iscapable of learning by means of natural languageinstruction (Lauria et al, 2001).
Secondly, itaims to offer some insights that are relevant forthe development of NLIs in HRI in general.
Thisresearch is largely motivated by models of hu-man communication.
It is situated within the lan-guage-as-action tradition and its approach is toexplore and build upon how humans managemiscommunication.2 MethodWe designed and performed two rounds of Wiz-ard of Oz (WOz) simulations.
Given that thegeneral aim of the study is to determine how ro-bots should initiate repair and provide feedbackin collaborative tasks, the simulations departedfrom the typical WOz methodology in that thewizards were also naive participants.
The domainof the task is navigation.
In particular, the user111guided the robot to six designated locations in asimulated town.
The user had full access to themap whereas the wizard could only see the sur-rounding area of the robot.
Thus, the wizard re-lied on the user?s instructions on how to reachthe destination.
In this section we outline the aimand approach of each WOz study, the materialsused and the experimental procedure.
Sections 4and 5 focus on each study individually and theirresults.2.1 The first WOz studyThis study is a continuation of previous work bythe authors (Koulouri and Lauria, 2009).
In thatstudy, the communicative resources of the wizardwere incrementally restricted, from ?normal?dialogue capabilities towards the capabilities of adialogue system, in three experimental condi-tions: The wizard simulates a super-intelligentrobot capable of using unconstrained,natural language with the user (henceforth,Unconstrained Condition). The wizard can select from a list of de-fault responses but can also ask for clarifi-cation or provide task-related information(henceforth, Semi-Constrained condition). The wizard is restricted to choose from alimited set of canned responses similar toa typical spoken dialogue system (SDS).The current study investigates the first two con-ditions and presents new findings.2.2 The second WOz studyThe second round of WOz experiments exploredthe effects of monitoring and shared visual in-formation on the dialogue.2.3 Set-upA custom Java-based system was developed andwas designed to simulate the existing prototype(the mobile robot).
The system consisted of twoapplications which sent and received coordinatesand dialogue and were connected using theTCP/IP protocol over a LAN.
The system kept alog of the interaction and the robot?s coordinates.The user?s interface displayed the full map ofthe town (Figure 1).
The dialogue box was belowthe map.
Similar to an instant messaging applica-tion, the user could type his/her messages and seethe robot?s responses appearing on the lower partof the box.
In the first WOz study, the user?s in-terface included a small ?monitor?
on the upperright corner of the screen that displayed the cur-rent surrounding area of the robot, but not therobot itself.
Then, for the purposes of the secondstudy, this feature was removed (see Figure 1 inAppendix A).Figure 1.
The user?s interface.The wizard?s interface was modified accord-ing to the two experimental conditions.
For bothconditions, the wizard could only see a fractionof the map- the area around the robot?s currentposition.
The robot was operated by the wizardusing the arrow keys on the keyboard.
The dialo-gue box of the wizard displayed the most recentmessages of both participants as well as a historyof the user?s messages.
The buttons on the rightside of the screen simulated the actual robot?sability to remember previous routes: the wizardclicked on the button that corresponded to aknown route and the robot automatically ex-ecuted.
In the interface for the Unconstrainedcondition, the wizard could freely type and sendmessages (Figure 2).Figure 2.
The wizard?s interface in the Uncon-strained condition.In the version for the Semi-Constrained condi-tion, the wizard could interact with the user intwo ways: first, they could click on the buttons,situated on the upper part of the dialogue box, toautomatically send the canned responses, ?Hel-112lo?, ?Goodbye?, ?Yes?, ?No?, ?Ok?
and theproblem-signalling responses, ?What?
?, ?I don?tunderstand?
and ?I cannot do that?.
The secondway was to click on the ?Robot Asks Question?and ?Robot Gives Info?
buttons which allowedthe wizard to type his/her own responses (seeFigure 2 in Appendix A).2.4 ProcedureA total of 32 participants were recruited, 16 usersand 16 wizards.
The participants were randomlyassigned to the studies, experimental conditionsand to the roles of wizard or user.
The pairs wereseated in different rooms equipped with a desk-top PC.
The wizards were given a short demon-stration and a trial period to familiarise with theoperation of the system and were also informedabout whether the users would be able to monitorthem.
The users were told that they would inte-ract with a robot via a computer interface; thisrobot was very fluent in understanding spatiallanguage and could give appropriate responses, itcould learn routes but had limited vision.
Theusers were asked to begin each task wheneverthey felt ready by clicking on the links on theircomputer screen, start the interaction with ?Hel-lo?, which opened the wizard?s application, andend it with ?Goodbye?
which closed both appli-cations.
The participants received verbal andwritten descriptions of the experiment.
Theywere not given any specific guidelines on how tointeract or what routes to take.
However, the us-ers were asked not to use directions such as?north?, ?south?, ?up?, ?down?
etc.
The wizardswere also advised not to try to ?sound like a ro-bot?, but respond normally.3 Data analysisThe dialogues collected were annotated on thebasis of dialogue acts (DAs), miscommunication,and task status.
An example of an annotated di-alogue is shown in Table 1.ID (x,y@TIME)MESSAGET.S.
MISC HCRC TAGU1 (899,445@10:44:34)go  straight on.instructR2 (880,286@10:44:59)There is a wall straight on.Rej explain ImpU3 (884,286@10:45:28)ok turn left here then take thethird rightacknowledge/instructR4 (352,216@10:45:58)Before the bridge?Non check ImpU5 (351,68@10:46:17)this is the wrong placeWE Corr explainR6 (351,68@10:46:39)Wrong place.
Should Robot goacknowledge/query-ynback?U7 (351,68@10:46:55)go forward and at the cros-sroads keep going forward andthe tube is at the end of theroadinstructR8 (351,0@10:47:14)Out of bounds.WE  explainR9 (351,608@10:47:47)Where to go?query-wU10 (364,608@10:48:12)the tube is in front of youinstructR11 (402,547@10:48:23)Is it this one?BOT  query-ynU12 (402,547@10:49:7)yes it is.SUC  reply-yTable 1.
Example of an annotated dialogue.
IDdenotes the speaker (User or Robot), T.S.
standsfor task status and MISC for miscommunication.3.1 Annotation of dialogue actsThe DAs in the corpus were annotated followingthe HCRC coding scheme (Carletta et al, 1996).Motivated by Skantze (2005), the last column inTable 1 contains information on the explicitnessof the response.
This feature was only relevantfor repair initiations by the wizards.
For instance,responses like ?What??
and the the ones in Table3 were considered to be explicit (EX) signals ofmiscommunication, whereas lines 2 and 4 in thedialogue above were labelled as implicit (IMP).3.2 Annotation of task execution statusThe coordinates (x,y) of the robot?s position re-corded for every exchanged message were placedon the map of the town (of dimensions 1024x600pixels) allowing the analysts to retrace themovements of the robot.
Wrong executions (WE)were determined by juxtaposing the user?s in-struction with the robot?s execution, as indicatedby the coordinates.
Back-on-Track (BOT) wastagged when the first user instruction after awrong execution was executed correctly.
Finally,task success (SUC) was labelled when the robotreached the destination and it was confirmed bythe user.3.3 Annotation of miscommunicationThe annotation of instances of miscommunica-tion in the dialogues is based on the definitionsgiven by Hirst et al (1994).
Miscommunicationincludes three categories of problems: misunder-standings, non-understandings and misconcep-tions.
First, misunderstandings occur when thehearer obtains an interpretation which is notaligned to what the speaker intended him/her toobtain.
In this study, without attempting to unveilthe intention of the user, misunderstandings were113tagged when the user (who was monitoring theunderstanding) signalled a wrong execution (seeline 5 in Table 1).
These correction tags (Corr)did not always coincide with wrong executiontags, but were used when the user became awareof the error (after receiving visual or verbal in-formation).
Following the same definition, mis-understandings were also tagged as rejections(tag: Rej) when the wizard expressed inability toexecute the instruction (for instance, given therobot?s current location, as shown in line 2 in thedialogue), although he/she was able to interpretit.
Secondly, non-understandings (tag: Non, line4) occurred when the wizards obtained no inter-pretation at all or too many.
Non-understandingsalso included cases in which wizards were uncer-tain about their interpretation (as suggested byGabsdil, 2003).
Lastly, misconceptions happenwhen the beliefs of the interlocutors clash, andare outside the scope of this study.4 First WOz studySkantze (2005) and Williams and Young (2004)performed variations of WOz studies to explorehow humans handle ASR errors, using a real orsimulated speech recogniser.
They discoveredthat even after highly inaccurate recognition out-put, the participants rarely signalled non-understanding explicitly.
Accordingly, the expe-rimental hypothesis of the present study is thatwizards in both conditions will not choose expli-cit responses to signal miscommunication (suchas ?I don?t understand?
or ?What??)
but res-ponses that contribute with information.ASR is a major source of errors in SDS.
But asmiscommunication is ubiquitous in interaction,there are many other sources of ambiguity thatgive rise to problematic understanding.
Thus, forthe current purposes of this work, it was decidedthat ASR would have an overwhelming effect onthe interaction that might prevent the observationof other interesting dialogue phenomena.This section describes further work on the Un-constrained and Semi-Constrained conditions(see Section 2.1).
Twenty participants were re-cruited and randomly allocated to each condition.4.1 ResultsAnalysis of the dialogues of the Unconstrainedcondition reinforced previous findings and con-firmed the experimental hypothesis.
In particular,wizards never used explicit repairs, but preferredto describe their location, request clarificationand further instructions.
Integrating finer classi-fication of clarification requests (CRs) and theoriginal dialogue act tagging, the DAs used bythe wizards to signal non-understandings andrejections were categorised as shown in Table 2.Dialogue Act ExplanationExplain The wizard gives description of robot?s location.E.g., ?I crossed the bridge.
?, ?I am at a cross-road?.Check This category covers CRs.
The corpus containedtwo types of CRs: first, task-level reformulations(as in line 4 in Table 1), which reformulate theutterance on the basis of its effects on the task,showing the wizard?s subjective understanding(Gabsdil, 2003).
Second, alternative CRs whichoccur when the wizard gives two alternativeinterpretations, trying to resolve referentialambiguity.
For instance, ?back to the bridge orto the factory?, to resolve ?go back to last loca-tion?.Query-w The wizard asks for further instructions.
E.g.,?Please give me further instructions.
?Explain+Query-wA combo of actions; the wizard provides infor-mation on location and asks for further instruc-tions.
E.g., ?crossroads, now where?
?Table 2.Wizard DAs after miscommunication.Figure 3 illustrates the distribution of these re-sponses to signal non-understandings and rejec-tions (columns labelled ?Uncons-NON?
and?Uncons-REJ?, respectively).
Evidently, there isa much greater variety of CRs than the two CRtypes reported here, as described in the work ofPurver (2006) and Schlangen (2004).
However,for a navigation task and having excluded ASRerrors, problems occurred mainly in the meaningrecognition level (explained below) and aimedfor reference resolution.Figure 3.
Use of strategies to signal non-understandings or rejections, for either condition.In conclusion, wizards in the Unconstrainedcondition did not directly signal problems in un-derstanding but, instead, they attempted to ad-vance the dialogue by providing task-related in-formation in either the form of CRs or simplestatements.
The study contributes to the findingspresented in Skantze (2005) and Williams andYoung (2004) in that it demonstrates the use ofsimilar strategies to deal with different sources ofproblems.114In the Semi-Constrained condition, a degree ofrestrain and control over the error handling ca-pacity of the wizards was introduced.
In particu-lar, the wizards could explicitly signal communi-cation problems in the utterance, meaning andaction level using three predefined responses.This is inspired by the models of Clark (1996)and Allwood (1995), according to which, mis-communication can occur in any of these levelsand people select repair initiations that point tothe source of the problem.
The model (adaptedfrom Mills and Healey, 2006) and the responsesare schematically shown in Table 3 below.Levels of Communication Wizard ResponsesLevel 1 Securing Attention -Level 2 Utterance Recognition ?What?
?Level 3 Meaning Recognition ?Sorry, I don?t understand.
?Level 4 Action Recognition ?I cannot do that.
?Table 3.
Levels of communication.Moreover, based on the classification of thewizard?s error handling strategies in the Uncon-strained condition (Table 2), we collapsed theobserved strategies in two categories of re-sponses which resulted in adding two more errorhandling buttons; namely, the button denoted as?Robot Asks Question?
corresponded to the?Check?
and ?Query-w?
strategies.
The ?RobotGives Info?
was associated with ?Explain?.
Thisclear labelling of error handling actions pre-sented to the wizards of the Semi-Constrainedcondition aimed to ?coerce?
them to use thestrategies in a more transparent way.
This couldallow us a glimpse to the mechanisms and proc-esses underlying human miscommunicationmanagement.Analysis of the dialogues revealed that in theSemi-Constrained condition wizards employedboth explicit and implicit strategies.
Figure 4shows the distribution of explicit and implicitresponses to signal non-understandings and re-jections.
Figure 3 shows the frequency of eachimplicit strategy to signal non-understandings(Semi-NON) and rejections (Semi-REJ).The initial prediction was that wizards will notuse explicit signals of problems in the dialogue.This was contradicted by the results.
It can beargued that the physical presence of the buttonsand the less effort required account for this phe-nomenon.
On the other hand, it is also plausibleto assume that these strategies matched what thewizards wanted to say.
Finally, there were nosignificant differences between conditions interms of user experience, task success and timeon task (as reported in Koulouri and Lauria,2009).Figure 4.
Occurrence of implicit and explicitmiscommunication signals (Semi-Constrained).4.2 Discussion and future workThe findings of this study could be extrapolatedto HRI.
Classification of the responses of thewizards resulted in a limited set of error signal-ling strategies.
Therefore, in the presence of mis-communication the robot could use the static,explicit strategies.
But these strategies alone areinadequate (as shown by Koulouri and Lauria,2009).
They need to be supplemented, but notentirely replaced, with dynamic error handlingstrategies; namely, posing relevant questions andproviding descriptions of location.
Yet this en-tails several challenges.
Gabsdil (2003) identifiesthe complexity of adding clarification requests tosystems with deep semantic processing.
Withregard to alternative clarifications, systemswould need to generate two alternative interpre-tations for one referent.
Task-level reformula-tions would also require the system to have thecapability to identify the effects of all possibleexecutions of the instruction.
As a next step, wewill focus on issues concerning the implementa-tion of such functionality.Schlangen (2004) suggests that ?general-purpose?
repair initiations, such as ?What?
?,which request repetition of the whole utterance,are more severe for the dialogue compared toreprise fragments (e.g., ?Turn where??)
that ac-cept part of the utterance.
Mills and Healey(2006) also found that ?What?s?
were more dis-ruptive to the dialogue than reprise fragments.Guided by these insights, our current work looksat how each error strategy affects the subsequentunfolding of the dialogue.5 The second WOz studyResearch in human communication has shownthat in task-oriented interactions visual informa-tion has a great impact on dialogue patterns andimproves performance in the task.
In particular,Gergle at al.
(2004), Clark and Krych (2004) and115Brennan (2005) explored different communica-tion tasks and compared a condition, in whichvisual and verbal information was available, witha speech-only condition.
In their experiments, aperson gave instructions to another participant onhow to complete a task.
Their findings seem toresonate.
In terms of time for task completionand number of words per turn, the interactions inthe visual information condition were more effi-cient.
The physical actions of the person follow-ing the instructions functioned as confirmationsand substituted for verbal grounding.
Regardingerrors, no significant differences were observedbetween visual and speech-only conditions.
Mo-tivated by these findings in human-human inte-raction, the second study aims to identify the dif-ferences in the processes of communication de-pending on whether the user can or cannot moni-tor the actions of the robot.5.1 Experimental designThe study followed a between-subjects factorialdesign.
Experiments were performed for fourdifferent conditions, as illustrated in Table 4.
Theconditions ?Monitor, Unconstrained?
and ?Moni-tor, Semi-Constrained?
were the same as in thefirst study.
Five pairs of participants were re-cruited to each of the Monitor Conditions andthree pairs to each of the No Monitor Conditions.Unconstrained Semi-ConstrainedMonitorMonitor, Uncon-strainedMonitor, Semi-ConstrainedNo MonitorNo Monitor, Uncon-strainedNo Monitor, Semi-ConstrainedTable 4.
The design of the 2nd study.5.2 ResultsThe data collection resulted in 96 dialogues, 93of which were used in the analysis.
The datawere analysed using a two-way ANOVA.
Alleffects that were found to be significant wereverified by T-tests.
The efficiency of interactionwas determined using the following measures:time per task, number of turns, words, miscom-munication-tagged turns, wrong executions andtask success.Time per task: The second column of Table 5displays the average completion time per task inthe four conditions.
As expected, a main effect ofthe Monitor factor was found (F=4.879, df=1,11,p<0.05).
Namely, when the user could monitorthe robot?s area the routes were completed faster.The interaction effect between factors was alsomarginally significant (F=4.225, df=1,11, p<0.1);pairs in the No Monitor, Semi-Constrained con-dition could not compensate for the lack of visualinformation and took longer for each task.Number of turns and words: The aforemen-tioned studies correlate task efficiency withnumber of turns and words.
In terms of the meannumber of turns per interaction, no significantdifferences were found across the groups.
Nev-ertheless, we measured the number of wordsused per task and in accordance with previousresearch, we observed that pairs in the No Moni-tor conditions used more words (F=4.602,df=1,11, p=0.05).
However, it was the wizardsunder the No Monitor conditions that had to bemore ?talkative?
and descriptive (F=10.324,df=1,11, p<0.01).
Figure 5 shows the ?word-possession?
rates attributed to wizards in the fourconditions.
Moreover, there seems to be a differ-ence (F=4.397, df=1,11, p=0.05) in the meannumber of words per turn.
In particular, when thewizards?
actions were visible to the users, thewizards required fewer words per turn.
There isalso an interaction effect showing more signifi-cant differences between the Monitor, Semi-Constrained condition and the No Monitor,Semi-Constrained condition (F=5.970, df=1,11,p<0.05); in the former, wizards managed withless than 2 words per utterance, taking full ad-vantage of the luxury of the buttons and the factthat they were supervised.
In the latter, wizardsused more than 6 words per turn.Figure 5.
Words used by wizards over total.Frequency of miscommunication: We meas-ured the number of turns that were tagged as con-taining miscommunication.
Surprisingly, mis-communication rates were much lower in the NoMonitor conditions (F=13.316, df=1,11, p<0.01)and not in the conditions in which the user couldcheck at all times the actions and understandingof the robot.
The same pattern was found for us-er-initiated and robot-initiated miscommunica-tion.
The rates of miscommunication are in-cluded in the third column of Table 5.Wrong executions: Analysis of number ofwrong executions per task reveals a similar ef-fect; wrong executions occurred much less fre-quently when the wizards were not supervised by116the users (F=6.046, df=1,11, p<0.05).
Theymade on average 1 mistake per task, whereas theaverage number of wrong executions for thepairs in the Monitor conditions was 5 (fourthcolumn in Table 5).Task success rates: There were no differencesin the number of interrupted or aborted tasks.ConditionTime perTask(min)MiscommunicationTurns/Total Turns#WrongExecutionsper TaskMon, Uncons 4.57 8.21% 4.2Mon, Semi 4.63 8.82% 5.8No Mon, Uncons 5.67 2.55% 1.0No Mon, Semi 7.41 1.71% 0.7Table 5.
Summary of results (mean values).5.3 Discussion and future workThese results are consistent with previous re-search.
The conditions in which the user couldsee exactly what the robot saw and did resultedin faster task completion and shorter dialogues.However, a finding emerged which was not ex-pected based on the aforementioned studies: inthe conditions in which users could not monitorthe robot?s actions, the wizards were more accu-rate, leading to low occurrence of wrong execu-tions and miscommunication (see column 3 and 4in Table 5).
The ?least collaborative effort?
isbalanced and compromised against the need toensure understanding.
Thus, wizards providedrich and timely feedback to the users in order tocompensate for the lack of visual information.This feedback acted in a proactive way and pre-vented miscommunication and wrong executions.In the Monitor conditions, asymmetries in per-ceived responsibility and knowledge between theparticipants could have encouraged wizards to beless cautious to act.
In other words, as the userhad access to the full map and the location of thewizard, the wizard felt less ?obliged?
to contri-bute to the interaction.
However, due to the com-plex nature of the task, unless the wizard couldsufficiently communicate the relevant position ofthe robot, the directions of the user would morelikely be incorrect.
It could also be assumed thatsince visual feedback is instant, the users werealso more inclined to issue commands in a ?trialand error?
process.
Irrespectively to the underly-ing motives, these findings show that despitehigher costs in time and word count, linguisticresources were adequate for completing complextasks successfully.
The findings also resonatewith the collaborative view of communication.The wizards adapted their behaviour in responseto variations in the knowledge state of their part-ners and made up for the lack of visual informa-tion with rich verbal descriptions of their loca-tions.We are currently performing more experi-ments to balance the data sets of the study andvalidate the initial results.
Moreover, a fine-grained analysis of the dialogues is under wayand focuses on the linguistic content of the inte-ractions.
The aim is identical to the first WOzstudy, that is, to identify the strategies of the wi-zards in the presence and absence of visual in-formation.These results have important implications forHRI.
As in human collaborative interaction, therobot?s communicative actions have direct im-pact on the actions of the users.
In real-worldsettings, there will be situations in which the us-ers cannot monitor the robot?s activities or theirinformation and knowledge are either con-strained or outdated.
Robots that can dynamicallydetermine and provide appropriate feedbackcould help the users avoid serious errors.
Never-theless, this is not a straightforward process; pro-viding excessive, untimely feedback compromis-es the ?naturalness?
and efficiency of the interac-tion.
The amount and placement of feedbackshould be decided upon several knowledgesources, combined in a single criterion that isadaptive within and between interactions.
Theseissues are the object of our future work and im-plementation.6 Concluding remarksOne of the most valuable but complex processesin the design of a NLI for a robot is enacting aHRI scenario to obtain naturally-occurring datawhich is yet generalisable and relevant for thefuture implementation of the system.
The presentstudy recreated a navigation scenario in whichnon-experienced users interacted with and taughta mobile robot.
It also simulated two differentsetups which corresponded to the realistic situa-tions of supervised and unsupervised interaction.The current trend in the fields of linguistics androbotics is the unified investigation of spatiallanguage and dialogue (Coventry et al, 2009).Exploring dialogue-based navigation of a robot,our study aimed to contribute to this body of re-search.
It can be argued that there were limita-tions in the simulation as compared to the expe-rimental testing of a real system and, thus, thestudy was primarily explorative.
However, ityielded natural dialogues given that naive ?con-federates?
and no dialogue script were used.
Thedata analysis was more qualitative than quantita-117tive and followed established methods from pre-vious research.
Finally, the results of the studymatched and extended these findings and pro-vided useful information for the next version ofthe system as well as some insight into theprocesses of conversation and social psychology.The next step in our research is to develop thedialogue manager of the robot to incorporate thefeedback and miscommunication managementstrategies, as observed in the collected data.
Thisholds the promise for a robust NLI that can han-dle uncertainties arising from language and theenvironment.
However, miscommunication inHRI reaches beyond preventing and repairingrecognition errors.
Mills and Healey (2008)demonstrate that miscommunication does notinhibit but, on the contrary, it facilitates semanticcoordination.
Martinovsky and Traum (2003)suggest that through miscommunication, peoplegain awareness of the state and capabilities ofeach other.
Miscommunication, thus, is seen asan opportunity for communication.
Under thislight, natural miscommunication management isnot only the end, but also the means to shape andadvance HRI, so that robots are not tools butpartners that play a positive, practical and long-lasting role in human life.ReferencesBilyana Martinovsky and David Traum.
2003.TheError Is the Clue: Breakdown in Human-MachineInteraction.
In Proceedings of the ISCA Workshopon Error Handling in Dialogue Systems.Dan Bohus and Alexander I. Rudnicky.
2005.
Sorry, IDidn?t Catch That!
?
An Investigation of Non-understanding Errors and Recovery Strategies.
InProceedings of SIGdial2005.
Lisbon, Portugal.Darren Gergle, Robert E. Kraut and Susan E. Fussell.2004.
Language Efficiency and Visual Technolo-gy: Minimizing Collaborative Effort with VisualInformation.
Journal of Language and Social Psy-chology, 23(4):491-517.
Sage Publications, CA.David Schlangen.
2004.
Causes and Strategies forRequesting Clarification in dialogue.
In  Proceed-ings of the 5th Workshop of the ACL SIG on Dis-course and Dialogue (SIGdial04), Boston, USA.Gabriel Skantze.
2005.
Exploring Human Error Re-covery Strategies: Implications for Spoken Dialo-gue Systems.
Speech Communication, 45(3):207-359.Graeme Hirst, Susan McRoy, Peter Heeman, PhilipEdmonds, Diane Horton.1994.
Repairing Conver-sational Misunderstandings and Nonunderstand-ings.
Speech Communication 15:213?230.Gregory Mills and Patrick G. T. Healey.
2008.
Nego-tiation in Dialogue: Mechanisms of Alignment.
InProceedings of the 8th SIGdial workshop on Dis-course and Dialogue, Columbus, OH, USA.Gregory Mills and Patrick G. T. Healey.
2006.
Clari-fying Spatial Descriptions: Local and Global Ef-fects on Semantic Co-ordination.
In Procs.
of the10th Workshop on the Semantics and Pragmatics ofDialogue.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press, Cambridge, UK.Herbert H. Clark and Meredyth A.  Krych.
2004.Speaking While Monitoring Addressees for Under-standing.
Journal of Memory and Language,50:62-81.Jason D. Williams and Steve Young.
2004.
Characte-rizing Task-Oriented Dialog Using a SimulatedASR Channel.
ICSLP.
Jeju, South Korea.Jean Carletta, Amy Isard, Stephen Isard, JacquelineKowtko, Gwyneth Doherty-Sneddon and Anne H.Anderson.1996.
HCRC Dialogue Structure CodingManual(HCRC/TR-82).
Human CommunicationResearch Centre, University of Edinburgh.Jens Allwood.
1995.
An Activity based Approach toPragmatics.
Gothenburg Papers in Theoretical Lin-guistics, 76, G?teborg University, Sweden.Kenny Coventry, Thora Tenbrink and John Bateman,2009.
Spatial Language and Dialogue: Navigatingthe Domain.
In K. Coventry, T. Tenbrink, and J.Bateman (Eds.)
Spatial Language and Dialogue.
1-8.
Oxford University Press.
Oxford, UK.Malte Gabsdil.
2003.
Clarification in Spoken Dialo-gue Systems.
In: Proceedings of 2003 AAAI SpringSymposium on Natural Language Generation inSpoken and Written Dialogue, Stanford, USA.Matthew Purver.
2006.
CLARIE: Handling Clarifica-tion Requests in a Dialogue System.
Research onLanguage and Computation .
4(2-3):259-288.Stanislao Lauria, Guido Bugmann, Theocharis Kyria-cou, Johan  Bos and Ewan Klein.
2001.
TrainingPersonal Robots Using Natural Language Instruc-tion.
IEEE Intelligent Systems.
38?45.Susan E. Brennan .
2005.
How Conversation isShaped by Visual and Spoken Evidence.
In J.Trueswell & M. Tanenhaus (Eds.)
Approaches toStudying World-situated Language Use: Bridgingthe Language-as-product and Language-actionTraditions.
95-129.
MIT Press, Cambridge, MA.Theodora Koulouri and Stanislao Lauria.
2009.
AWOz Framework for Exploring Miscommunicationin HRI, In Procs.
of the AISB Symposium on NewFrontiers in Human-Robot Interaction.
Edinburgh,UK.118Appendix A. Screenshot images of the in-terfaceFigure 1.
The interface of the user without themonitor (as used in the second WOz study).Figure 2.
The interface of the wizard in the Semi-Constrained condition.119
