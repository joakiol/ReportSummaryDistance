Page 1A Preliminary Study on Probabilistic Models for Chinese AbbreviationsJing-Shin ChangDepartment of Computer Science &Information EngineeringNational Chi-Nan UniversityPuli, Nantou, Taiwan, ROC.jshin@csie.ncnu.edu.twYu-Tso LaiDepartment of Computer Science &Information EngineeringNational Chi-Nan UniversityPuli, Nantou, Taiwan, ROC.s0321521@ncnu.edu.twAbstractChinese abbreviations are widely used inthe modern Chinese texts.
They are aspecial form of unknown words, includingmany named entities.
This results indifficulty for correct Chinese processing.In this study, the Chinese abbreviationproblem is regarded as an error recoveryproblem in which the suspect root wordsare the ?errors?
to be recovered from a setof candidates.
Such a problem is mappedto an HMM-based generation model forboth abbreviation identification and rootword recovery, and is integrated as part ofa unified word segmentation model whenthe input extends to a complete sentence.Two major experiments are conducted totest the abbreviation models.
In the firstexperiment, an attempt is made to guessthe abbreviations of the root words.
Anaccuracy rate of 72% is observed.
Incontrast, a second experiment isconducted to guess the root words fromabbreviations.
Some submodels couldachieve as high as 51% accuracy with thesimple HMM-based model.
Somequantitative observations against heuristicabbreviation knowledge about Chineseare also observed.1 IntroductionThe modern Chinese language is a highly25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 1 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsabbreviated one due to the mixed uses of ancientsingle character words as well as modernmulti-character words and compound words.
Theabbreviated form and root form are usedinterchangeably everywhere in the current Chinesearticles.
Some news articles may contain about20% of sentences that have suspect abbreviatedwords in them ?Lai 2003?.
Since abbreviationscannot be enumerated in a dictionary, it forms aspecial class of unknown words, many of whichoriginate from named entities.
Many other openclass words are also abbreviatable.
This particularclass thus introduces complication for Chineselanguage processing, including the fundamentalword segmentation process ?Chiang 1992, Lin1993, Chang 1997?
and many word-basedapplications.
For instance, a keyword-basedinformation retrieval system may requires the twoforms, such as ??
and ????legislators?
?, in order not to miss any relevantdocuments.
The Chinese word segmentationprocess is also significantly degraded by theexistence of unknown words ?Chiang 1992?,including unknown abbreviations.There are many heuristics for Chineseabbreviations.
Such heuristics, however, can easilybreak ?Sproat 2002?.
Currently, only somequantitative approaches ?Huang 1994a, 94b?
areavailable in predicting the presentation of anabbreviation.
Since such formulations regard theword segmentation process and abbreviationidentification as two independent processes, theyprobably cannot optimize the identificationprocess jointly with the word segmentation process,and thus may lose the useful contextualinformation.
Some class-based segmentationmodels ?Sun 2002, Gao 2003?
well integrate theidentification of some regular non-lexicalized units?such as named entities?.
However, theabbreviation process can be applied to almost allword forms ?or classes of words?.
Therefore, thisparticular word formation process may have to behandled as a separate layer in the segmentationprocess.To resolve the Chinese abbreviation problems25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 2 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsand integrate its identification into the wordsegmentation process, this study proposes toregard the abbreviation problem in the wordsegmentation process as an ?error recovery?problem in which the suspect root words are the?errors?
to be recovered from a set of candidatesaccording to some generation probability criteria.This idea implies that an HMM-based model foridentifying Chinese abbreviations could beeffective in either identifying the existence of anabbreviation or the recovery of the root wordsPage 2from an abbreviation.
We therefore start with aunified word segmentation model so that bothprocesses can be handled at the same time, andwhen the input is reduced to a single abbreviatedword, the model can be equally useful forrecovering its root.As a side effect of using HMM-basedformulation, we expect that a large abbreviationdictionary could be derived from a large corpus orfrom web documents through the training processof the unified word segmentation modelautomatically.Section 2 will show our HMM models and thethree abbreviation problems correspond to thethree basic HMM problems.
Section 3 will showthe experiment setup.
Section 4 will examine theexperiments to guess abbreviations from root orvice versa.2 Chinese Abbreviation Models2.1 An Error Recovery ParadigmTo resolve the abbreviation problems, first notethat the most common action one would take whenencountering an abbreviation is to find itscandidate roots ?probably from a largeabbreviation dictionary if available or from anordinary dictionary with some educated guesses?,and then identify the most probable one.
Thisprocess is identical to the operation of manyspelling correction models, which generate thecandidate corrections according to a reversed wordformation process, then justify the best candidate.25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 3 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsSuch an analogy indicates that we may use anHMM model ?Rabiner 1993?, which is good atfinding the best unseen state sequence, for errorrecovery.
There will be a direct map between thetwo paradigms if we regard the observed inputcharacter sequence as our ?observation sequence?,and regard the unseen word candidates as theunderlying ?state sequence?.Given these mappings, we will be able to usemany standard processing approaches for HMMwhen we have to answer some interestingquestions ?including root word recovery?.
Amongall interesting questions for an HMM, we havethree basic questions to ask the model ?Rabiner1993?, namely the output probability of an outputsequence, the best underlying state sequence andthe best parameters given a training corpus.If we can ask the HMM for abbreviation thesame questions, then we will also be able toanswer the question on ?1?
what is the likelihoodthat a string is an abbreviation, ?2?
what are thebest underlying root words for an input characterstring that contains abbreviations, and ?3?
how toestimate the model parameters automatically givena corpus.The first question is related to the problem ofgenerating an appropriate abbreviation from a rootword; the second question is linked to finding thebest underlying roots from an abbreviated string,and the third question have a direct link to theconstruction of an abbreviation dictionaryautomatically from a corpus.
For now we will notexplore this third question, but leave it to aresearch that would be launched in the near future.The most interesting question to ask is, ofcourse, the second question in the Chinesetokenization process.
Therefore, we will start witha unified word segmentation model, which has thecapability to handle abbreviation problem jointlywith the word segmentation process.2.2 HMM-Q2: Unified Word SegmentationModel for Abbreviation RecoveryTo integrate the abbreviation process into the wordsegmentation model, firstly we can regard thesegmentation model as finding the best underlyingwordsm25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 4 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsmwww,,11??
?which include onlybase/root forms?, given the surface string ofcharactersnnccc,,11??
?which may containabbreviated forms of compound words.?
Thesegmentation process is then equivalent to findingthe best word sequence*wsuch that:????
?
???
????=????
?=?=25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 5 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations=iinmmnmmnmmcwmiiiiicwwmmncwwnmcwwwwPwcPwPwcPcwPw,11:125 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 6 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations11:11:||maxarg|maxarg|maxarg*111111111Equation 1.
Unified Word SegmentationModel for Abbreviation Recoverywhereicrefers to the surface form ofiw, whichcould be in an abbreviated or non-abbreviated ?orany transformed?
form ofiw.
The last equalityassumes that the generation of an abbreviation isindependent of context, and the language model isa word-based bigram model.
Such assumptionscan be adapted to different submodels for wordsegmentation ?Chiang 1992?
as appropriate.Furthermore, in many cases, the underlying wordiwwill be a compound word consisting of otherconstituent words25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 7 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsijw?e.g., ???.
And,the probability?
?iiwcP |is not always 1 or 0,since the constituents may be abbreviatedPage 3differently in different context, making themapping of the compound ambiguous.
Forinstance, some people may prefer to abbreviate ??
?Industrial Technology ResearchInstitute; ITRI?
into ??
?IRI?
while othermay prefer an abbreviation of ??
?ITI?.Notice that, this equation is equivalent to theformulation for an HMM ?Hidden Markov Model?
?Rabiner 1993?
to find the best ?state?
sequencegiven the observation symbols.
The parameters??1|?iiwwPand?
?iiwc25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 8 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsP |represent thetransition probability and the ?word-wise?
outputprobability of an HMM, respectively; and, theformulations for?
?mwP1and?
?mnwcP11|arethe respective ?language model?
of the Chineselanguage and the ?generation model?
for theabbreviated words ?i.e., the ?abbreviation model?in the current context?.
The ?state?
sequence in theword segmentation case is characterized by theroot formsmmwww,,11?
?, or the hidden words;and, the ?observation symbols?
are characterizedhere bymnncccc25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 9 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsc,,,,111???
?, where thesurface form?
??
?ieibicc ?
?is a chunk of charactersbeginning at the b?i?-th character and ending at thee?i?-th character.Such an analogy with an HMM enables us toestimate the model parameters using anunsupervised training method that is directlyportedfromtheforward-backwardorBaum-Welch re-estimation formula ?Rabiner 1993?or a generic EM algorithm ?Dempster 1977?.Note also that, while the above formulation isintended for finding root words in a sentence, withthe help of contextual words, we can also apply thesame formulation to a single abbreviated word?likely to have a compound word as its root inmany cases?
to find the most likely constituentwords, without the help of surrounding words, butwith the help of contextual constraints among itsconstituents.2.2.1.
Language ModelThe word transition probability?
?125 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 10 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations|?iiwwPused in the language model is used to providecontextual constraints among root words.
It maynot be reliably estimated when the language has alarge vocabulary and when the training corpus issmall.
To resolve this problem, we can back-offthe bigram word transition probability to aunigram word probability using Katz?s method?Katz 1987?
for rare bigrams.
We can, of course,use other smoothing methods to acquire reliableparameters.
The smoothing issues, however, arenot the main focus of this preliminary study.2.2.2 Generation Model for AbbreviationsIn the perfect case where all words arelexicalized, rendering all surface forms identical totheir ?root?
forms and all words are known to thesystem dictionary, we will have?
?|1iiP c w =,1,im?
=, and Equation 1 is no more than a wordbigram model for word segmentation ?Chiang1992?.
In the presence of unknown words ?e.g.,abbreviations being one of such entities?, however,we can no longer ignore the generation probability?
?iiwcP |.25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 11 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsFor example, ificis ??
theniwcouldbe the compound word ??
?TaiwanUniversity?
or ??
?Taiwan MajorLeague?.
In this case, the parameters in P?|?
x P?
|?
x P?
|?
and P?|?x P?
|?
x P?
|?
will indicate howlikely ??
is an abbreviation, and which of theabove two compounds is the root form of theabbreviation.
Therefore, we need a method forestimating the probabilities between theabbreviations and their root forms ?many of whichare compound words with other constituents?.2.3 Applying Abbreviation ModelsThere are two problems to use the unified modelwhich takes abbreviated words into account.
Firstof all, since the word lattice is constructed from allpossiblemmwww,,11?
?, how can we construct itwithout really knowing the candidate base formsofi25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 12 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationscin advance?
We don?t really want torandomly combine all possible root forms, whichis not affordable in computational cost.
Therefore,we have to make some smarter choices.
Second,howtocomputetheabbreviation?output/generation?
probability?
?iiwcP |oncethe lattice is constructed with candidate rootwords?2.3.1 Candidate Root Word GenerationThe first problem can be resolved if we choosesome highly probable constituentswthat wouldgenerate each individual charactersijcinicindependently, and allow such Top-N candidatesto form part of the complete word lattice.
That is,Page 4for each individual characterijc, we choose itsTop-N candidates according to:???
?25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 13 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations|ijP c w P wi.The probability?
?|ijP c where represents thecharacter-wise generation probability of a singlecharacter from its corresponding root word.
Noticethat, after we apply the word segmentation modelEquation 1 to the word lattice, some of the abovecandidates may be preferred and others bediscarded, by consulting the neighboring wordsand their transition probabilities.
This makes theabbreviation model jointly optimized in the wordsegmentation process, instead of being optimizedindependent of context.2.3.2 Abbreviation ProbabilityThe second problem can be resolved using thefollowing equation ifiwcan be segmented intoiLiiwww,,1?
?, each constituent correspondingto a character in?
??
?ieibic25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 14 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsc ??:???
?????
???
?
?
??
??
?1?
?1, 11,||||1e iiLiib iib ijijiji jjLP c wP cwP cwP w wL e i b iL i+ ?
?==25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 15 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations=?=?+ ??
?Equation 2.
Abbreviation Probability.In other words, we use the transitionprobability between constituent words and thecharacter-wisegenerationprobabilitiesofindividual characters from a constituent word toestimate the global generation probability of theabbreviated form.2.3.3 Simplified Abbreviation ModelsIt is sometimes simply not efficient to save allpairs of root compounds and their respectiveabbreviations in an abbreviation dictionary.Therefore, it is desirable to simplify theabbreviation probability by using some simplerfeatures for Chinese abbreviation words.
Forinstance, it is known that many 4-charactercompound words will be abbreviated as2-character abbreviations ?such as the case for the<,> pair.?
It was also knownheuristically that many such 4-character words areabbreviated by reserving the first and the thirdcharacters, which can be represented by a ?1010?bit pattern, where a ?1?
means to reserve therespective character and a ?0?
means to delete it.Therefore, a reasonable simplification for theabbreviation model is to introduce the length andthe bit pattern for abbreviation operations asadditional features into the abbreviation model.
Ifthis is the case, we will have the followingaugmented abbreviation model.?
?1111|25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 16 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations?
, , | , ??
| ?
?| ??
| ?mnmnP c wP c bit m r nP c rP bit n P m n=???
?11: surface characters.
: root word characters.where: length of surface characters.
: length of root word characters.bit: bit pattern of abbreviationmncrmnEquation 3.
Abbreviation Probability usingAbbreviation Pattern and Length Features.All these three terms can be combined freely toproduce as many as 7 sub-models for theabbreviation model.
Note, the first term?
?nmrc11|Prplays the same role as the oldernotation of25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 17 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations?
?wc |Pr, which could means a pairof <abbreviation, root> or be evaluated as theproduct of the per-character generationprobabilities and the sub-constituent transitionprobabilities as outlined in Equation 2.
This termcan of course be ignored from the aboveaugmented abbreviation model so that only verysimple length and position features are used forabbreviation handling.3 Data and Parameter EstimationAn abbreviation dictionary containing theword-abbreviation pairs is required to test theproposed models.
Unfortunately, a large Chineseabbreviation dictionary is not available.
Therefore,we have to collect some of the genericabbreviations, and make others manually fromsome named entity lists.
Almost half of ourcollection comes from the Ministry of Educationof the ROC.
?http://www.edu.tw/clc/dict/?.
?In afuture plan, a large abbreviation dictionary will bebuilt automatically by using the proposed models.
?Eventually, we got 1547 root-abbreviation pairs.Among them, 1235 pairs are considered simpleand 312 pairs are ?tough?
in the sense that theyviolate some model assumptions.
For instance, werequired that a root in a compound word bemapped to at least one character in its abbreviation?not to a null string?, and we also assume that theword cannot be mapped to a character that is notpart of the word.
?For example, AB can beabbreviated as A or B but not C.?
Some toughwords will actually map substrings to null strings;Page 5others may be recursively abbreviated; and yetothers may change the word order ?as inabbreviating ??
as ?
?25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 18 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsinstead of ??.?.
As a result, the tough pairswill not be handled correctly with current models.To simplify the task, only the 1235 simple pairsare tested for evaluation.
They are further dividedrandomly into a training set of 986 pairs ?80%?and a test set of 249 pairs ?20%?.
Since the corpussize is not large, the compound words are alsomanually segmented into their constituents inorder to know the true alignments between eachcharacter of the abbreviation with its root form inthe compound word.
Admittedly, such anextremely small training set causes serious datasparseness problem during training.
Therefore, theevaluated performance in this preliminary reportwill be highly underestimated.The parameters are estimated in theunsupervised mode using a standard EM algorithmor the re-estimation method as conventional HMMmodels would do ?Rabiner 1993?.
In addition, themanually segmented dictionary also allows us toestimate the model parameters in the supervisedmode.The unsupervised training will automaticallyalign each character in the abbreviations to its rootform in the full words.
It is observed that 65.5% ofthe training set dictionary pairs will be alignedcorrectly.
Other pairs are aligned partially correct.Note that parameters?
?nmP |and?
?nbitP|can be estimated using maximum likelihoodestimation by directly consulting the abbreviationdictionary since they are only related to wordlength and character position.
It is interesting, inthe first place, to check these types of parameters25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 19 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsquantitatively to see if they reveal someabbreviation heuristics recognized by nativeChinese speakers.
The high frequency patterns,which are much more frequent than the onesranked in lower places, are listed in Table 1 andTable 2.P?m|n?
Score ExamplesP?1|2?
1.00 ?
|?, ?
|?P?2|3?
0.67 ?|?, ?|?P?2|4?
0.95 ?|?,?|?P?3|5?
0.73 ?|?,?|?P?3|6?
0.70 ?|?,?|?P?3|7?
0.76 ?|?,?|?Table 1.
High Frequency AbbreviationPatterns [by lengths]P(bit|n)ScoreExamplesP?10|2?0.87?
|25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 20 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations?,?
|?P?101|3?0.44?|?,?|?P?1010|4?0.56?|?,?|?P?10101|5?0.66?|?,?|?P?101001|6?0.51?|?,?|?P?1010001|7?0.55?|?,?|?P?10101010|8?0.21?|?,25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 21 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations?|?Table 2.
High Frequency Abbreviation Patterns[by P?bit|n?
]Table 1 shows how word lengths will changeduring the abbreviation process, and Table 2shows which characters will be deleted from theroot of a particular length.
The tablesquantitatively support some general heuristics fornative Chinese speaker.
For instance, most wordswill be abbreviated by deleting about half thecharacters in the words, as shown in Table 1.
Thedata also shows that the first character in atwo-character word will be retained in most cases,and the first and the third characters in a4-character word will be retained in 56% of thecases.
However, the tables also shows that around50% of the cases cannot be uniquely determinedsimply by consulting the word length for itsabbreviated form.
This does suggest the necessityof an abbreviation model for resolving this kind ofunknown words and named entities.4 Experiments and AnalysisThe unified model can be applied to a wholesentence which contains abbreviations duringword segmentation.
When the input is reduced to asingle abbreviated word ?or compound?, it can alsobe applied to recover the underlying rootconstituent words ?without consulting contextualwords?.
In this paper, we will only focus on theabbreviation word recovery problems.Two major experiments are conducted.
Thefirst experiment is to guess the most likelyabbreviation form for a word using various featurecombinations; the second is to guess the root wordPage 6from an abbreviation.
The following sections willgive more details.4.1 Guessing Abbreviations from RootsThe main task of this experiment is to guess themost probable abbreviation forms for theunabbreviated words in a word list.
The25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 22 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsabbreviation forms of a word can be enumeratedby arbitrarily retaining some characters of this rootword and deleting others.
For example, the word?@?
has six possible abbreviated forms: ?
?,?
?, ?
@?, ?
?, ?
@?
and ?
@?.
In general,if we have a root word of length L, there could be22 ?Lpossible abbreviations for this root word?excluding the word itself and the null string?.The best possible abbreviation form*cfor aninput wordiwcan be determined as the one withthe highest generation probability?
?|iiP c w, i.e.,??
*argmax|iiccP c w=.Thegenerationprobability for a candidateic, in turn, can be25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 23 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsestimated by summing up all probabilities ofalignments between each characteri jcinicand the suspect constituent wordsi jwiniw.
Inother words, we have?????
?all alignments|,A ||iiiiAAiiAP c wP cwP c w?=????where?
?|A25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 24 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsiiP c wis the generation probability for aknown alignment A, which can be estimated as inEquation 2.For simplicity, we assume that each characterinicwill be mapped to a substringi jwiniw.In other words, we assume that the mappingbetween the constituents is 1-1, and no 1-0 or 0-1mapping is possible.
?In future works, such aconstraint could be removed.?
Also, we willassume thati jwshould at least contain thecharacter that is aligned to it.
?This is not alwaystrue for Chinese abbreviations.
For example, ??
can be abbreviated with its ancient locationname ?
?, which does not appear anywhere in itsroot.
?There is also a normalization issue incomputing the probability of a particular alignment.In general, a shorter string may be preferred as thebest abbreviation simply because it multiplies lessprobability factors when estimating the alignmentprobability.
To reduce this effect, we intentionallyscale down, by a normalization factor, thegeneration probabilities for those alignments thatmap a complete word into a single character.
Infact, there are only about 10% of such alignments,and many of which are mapping a two-characterword into a single character ?which can becompensated by the large Pr?1|2?
factor in themodel.
This simple normalization approachactually improves the test set performance greatly.The following table shows the test setperformance for using different features in theabbreviation probability as given in Equation 3.25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 25 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations?The training set performance ranges from 94% to98%, which suggests a good fit to the trainingdata.
?FeatureUnsupervised SupervisedP?c|w?xP?wi|wi-1?
1 1 1 1 1 1 1 1P?bit|n?1 1 0 0 1 1 0 0P?m|n?1 0 1 0 1 0 1 0AccuracyRate?%?68 68 61 60 72 70 61 58Table 3.
Test Set Performance forAbbreviation Generation with CombinedFeatures.Each column shows the test set performance fora submodel, which is identified by the featuresused for estimating the probability.
The label ?1?
?or ?0??
indicates that the feature at the firstcolumn is used ?or unused?
in the submodel.
Forinstance, the submodel of the second column??111??
uses all the features, including the wordtransitionprobability,word-to-abbreviationprobability, probability for mapping n characterword to a particular abbreviation bit patternP?bit|n?, and the probability for mappingn-character words into m-character abbreviations.It is seen that supervised training acquires alittle better performance than its unsupervised ?EM?counterpart.
Although not shown in this table, it isobserved that the word transition probability andword-to-abbreviation probability in general shouldbe used to get better performance.
The table alsoshows that the other two features based oncharacter positions and word lengths provideadditional help.
In particular, P?bit|n?
seems to bemore helpful than P?m|n?
since it contains detailedinformation for retaining characters at particularpositions.The best performance is about 72% whensupervised training is used and all the three typesof features are used for estimating the abbreviationprobability.25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 26 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsPage 74.2 Guessing Roots from AbbreviationsIn this experiment, we are given anabbreviation list; the goal is to guess the best rootwords of the abbreviations in the list.
Theparameters used here are acquired from humantagged alignments in a supervised manner.To find the best root candidates of anabbreviated compound word, we need to find thecandidate root words for each input character first.The candidate root words can be found from thetraining set whose generation probability?
?iiwcP |is non-zero.
The Top-N candidates canthen be picked up as described earlier.For instance, if we want to find the root wordsof the abbreviation ?
?, and the probabilitiesP?
|?
and P?
|?
are non-zero, then wehave the chance to recover the abbreviation ??
back to the correct compound word ??
, which consists of the candidate root words??
and ??
for the input characters ?
?and ?
?
respectively.Unfortunately, the limited abbreviationdictionary we have is highly sparse.
Among the249 abbreviations in the test set, only 144 ?58%?
ofthem have their candidate root words available inthe training set.
The other 105 abbreviations ?42%?cannot be recovered since each of them has at leastone character whose candidate cannot bediscovered from the training set.
For this reason,we will limit ourselves to the performance of the?trainable?
test set consisting of the 144abbreviations, in order to factor out the sparseness25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 27 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviationsproblem pertaining to the training corpus.Under such a restricted environment, we havetested various submodels to see how differentlanguage models and simple smoothing affect theresults of this error recovery process.
The resultsare summarized in the following table:LMSM?
Top-N TR?%?
TS?%?
Best?bigram Noall90.935269.6432146.0451unigram Noall44.244bigram Yesall90.7511269.6511146.045Table 4.
Abbreviation Recovery Performance.Notations: LM: Language Model, SM?
: ApplySmoothing?, Top-N: maximum number of Top-Ncandidate root words for each character, TR:Training Set Accuracy Rate, TS: Test SetAccuracy, Best?
: Best TS Performance among allN?s?
?1 = yes, 2= rank 2?The bigram language model uses?
?25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 28 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations1|?iiwwPin the unified HMM model while the unigrammodel uses?
?iwPinstead.
Both of them usemaximum likelihood estimation over the manuallytagged abbreviation-root pairs when smoothing isnot applied.
When smoothing is applied, thesmoothed bigram probability is acquired bylinearly interpolating the unigram and bigramprobabilities with an equal weight ?0.5?.
The abovetable indicates that using the less complicatedunigram model generally improve the test setperformance significantly ?from 35% to 44%?.
Ifthe model parameters are smoothed, theimprovement is even greater.
Such results can bewell expected in the current environment wherethe training data is very sparse.Overall, the best test set performance is about51% when using a smoothed bigram languagemodel; and this can be achieved by using at most 2Top-N candidate root words while constructing theunderlying word lattice.
This suggests that wedon?t really need to wildly enumerate all possiblecandidate root words for each input character withthis model.5.
Concluding RemarksChinese abbreviations, a special form ofunknown words and named entities, are widelyseen in the modern Chinese texts.
This results indifficulty for correct Chinese processing.
In thispreliminary study, the Chinese abbreviationproblem is modeled as an error recovery problemin which the suspect root words are to berecovered from a set of candidates.
AnHMM-based model is thus used for Chinese ineither abbreviation identification, or in therecovery of the root words from an abbreviation.25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 29 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsBy extending a simple abbreviation string into awhole text involving abbreviations, it can also beapplied to the Chinese word segmentation foridentifying abbreviations in a text, or forbootstrapping an abbreviation dictionary from atext corpus.With the proposed model, the abbreviatedforms can be guessed from root words at about72% correction.
The recovery of the root wordsfrom abbreviations is conducted at about 51%accuracy rate.
Although further improvement ispossible, the preliminary results are encouraging.In the near future, bootstrapping a largeabbreviation dictionary from web text by applyingthe proposed models is planned.
This shouldpartially resolve the data sparseness problems.Such models will also be integrated into a ChinesePage 8word segmentation model to partially resolve theunknown word and named entity identificationproblems in the tokenization process.
It is expectedthat more applications will rely on such models forChinese processing.ReferencesChang, Jing-Shin and Keh-Yih Su, 1997.
?AnUnsupervised Iterative Method for ChineseNew Lexicon Extraction?, International Journalof Computational Linguistics and ChineseLanguage Processing (CLCLP), 2?2?
: 97-148.Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Linand Keh-Yih Su, 1992.
?Statistical Models forWord Segmentation and Unknown WordResolution,?
Proceedings of ROCLING-V,pages 123-146, Taipei, Taiwan, ROC.Dempster, A. P., N. M. Laird, and D. B. Rubin,1977.
?Maximum Likelihood from IncompleteData via the EM Algorithm?, Journal of theRoyal Statistical Society, 39 ?b?
: 1-38.Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003.?Improved Source-Channel Models for ChineseWord Segmentation,?
Proc.
ACL 2003, pages272-279.Huang, Chu-Ren, Kathleen Ahrens, and Keh-Jiann25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 30 -shinA Preliminary Study on Probabilistic Models for Chinese AbbreviationsChen, 1994a.
?A data-driven approach topsychological reality of the mental lexicon: Twostudies on Chinese corpus linguistics.?
InLanguage and its Psychobiological Bases,Taipei.Huang, Chu-Ren, Wei-Mei Hong, and Keh-JiannChen, 1994b.
?Suoxie: An information basedlexical rule of abbreviation.?
In Proceedings ofthe Second Pacific Asia Conference on Formaland Computational Linguistics II, pages 49?52,Japan.Katz, Slava M., 1987.
?Estimation of Probabilitiesfrom Sparse Data for the Language ModelComponent of a Speech Recognizer,?
IEEETrans.
ASSP-35 (3).Lai, Yu-Tso, 2003.
A Probabilistic Model forChinese Abbreviations, Master Thesis, NationalChi-Nan University, ROC.Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su,1993.
?A Preliminary Study on Unknown WordProblem in Chinese Word Segmentation,?Proceedings of ROCLING VI, pages 119-142.Rabiner, L., and B.-H., Juang, 1993.
Fundamentalsof Speech Recognition, Prentice-Hall.Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhouand Chang-Ning Huang, 2002.
?Chinese namedentity identification using class-based languagemodel,?
Proc.
of COLING 2002, Taipei, ROC.Sproat, Richard, 2002.
?Corpus-Based Methods inChineseMorphology?,Pre-conferenceTutorials, COLING-2002, Taipei, Taiwan,ROC.25 Apr 2007  03:09Generated by HTML_ToPDF at rustyparts.com - 31 -shinA Preliminary Study on Probabilistic Models for Chinese Abbreviations
