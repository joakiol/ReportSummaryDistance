Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864?874,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDomain Adaptation for CRF-based Chinese Word Segmentation usingFree AnnotationsYijia Liu ?
?, Yue Zhang ?, Wanxiang Che ?, Ting Liu ?, Fan Wu ?
?Singapore University of Technology and Design?Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China{yjliu,car,tliu}@ir.hit.edu.cn {yue zhang,fan wu}@sutd.edu.sgAbstractSupervised methods have been the domi-nant approach for Chinese word segmen-tation.
The performance can drop signif-icantly when the test domain is differentfrom the training domain.
In this paper,we study the problem of obtaining par-tial annotation from freely available datato help Chinese word segmentation on dif-ferent domains.
Different sources of freeannotations are transformed into a unifiedform of partial annotation and a variantCRF model is used to leverage both fullyand partially annotated data consistently.Experimental results show that the Chi-nese word segmentation model benefitsfrom free partially annotated data.
On theSIGHAN Bakeoff 2010 data, we achieveresults that are competitive to the best re-ported in the literature.1 IntroductionStatistical Chinese word segmentation gains highaccuracies on newswire (Xue and Shen, 2003;Zhang and Clark, 2007; Jiang et al., 2009; Zhaoet al., 2010; Sun and Xu, 2011).
However, man-ually annotated training data mostly come fromthe news domain, and the performance can dropseverely when the test data shift from newswireto blogs, computer forums and Internet literature(Liu and Zhang, 2012).Several methods have been proposed for solv-ing the domain adaptation problem for segmenta-tion, which include the traditional token- and type-supervised methods (Song et al., 2012; Zhang etal., 2014).
While token-supervised methods relyon manually annotated target-domain sentences,type-supervised methods leverage manually as-sembled domain-specific lexicons to improvetarget-domain segmentation accuracies.
Both.
?
?
?
?
?
?
?
?
?b b b b b b b b bm m m m m m m m me e e e e e e e es s s s s s s s sFigure 1: The segmentation problem, illustratedusing the sentence ???
(Pudong) ??
(devel-opment) ?
(and) ??
(legal) ??
(construc-tion)?.
Possible segmentation labels are drawn un-der each character, where b, m, e, s stand for thebeginning, middle, end of a multi-character word,and a single character word, respectively.
The pathshows the correct segmentation by choosing onelabel for each character.methods are competitive given the same amount ofannotation effects (Garrette and Baldridge, 2012;Zhang et al., 2014).
However, obtaining manuallyannotated data can be expensive.On the other hand, there are free data whichcontain limited but useful segmentation informa-tion over the Internet, including large-scale un-labeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia.
In thelast case, word-boundary information is containedin hyperlinks and other markup annotations.
Suchfree data offer a useful alternative for improvingthe segmentation performance, especially on do-mains that are not identical to newswire, and forwhich little annotation is available.In this paper, we investigate techniques foradopting freely available data to help improve theperformance on Chinese word segmentation.
Wepropose a simple but robust method for construct-ing partial segmentation from different sourcesof free data, including unlabeled data and theWikipedia.
There has been work on making useof both unlabeled data (Sun and Xu, 2011; Wanget al., 2011) and Wikipedia (Jiang et al., 2013)864to improve segmentation.
However, no empiri-cal results have been reported on a unified ap-proach to deal with different types of free data.We use a conditional random fields (Lafferty et al.,2001; Tsuboi et al., 2008) variant that can lever-age the partial annotations obtained from differentsources of free annotation.
Training is achieved bya modification to the learning objective, incorpo-rating partial annotation likelihood, so that a singlemodel can be trained consistently with a mixtureof full and partial annotation.Experimental results show that our method ofusing partially annotated data can consistently im-proves cross-domain segmentation performance.We obtain results which are competitive to thebest reported in the literature.
Our segmentoris freely released at https://github.com/ExpResults/partial-crfsuite.2 Obtaining Partially Annotated DataWe model the Chinese word segmentation task asa character sequence tagging problem, which is togive each character in a sentence a word-boundarytag (Xue and Shen, 2003).
We adopt four tags, b,m, e and s, which represent the beginning, middle,end of a multi-character word, and a single char-acter word, respectively.
A manually segmentedsentence can be represented as a tag sequence, asshown in Figure 1.We investigate two major sources of freely-available annotations: lexicons and natural anno-tation, both with the help of unannotated data.To make use of the first source of informa-tion, we incorporate words from a lexicon intounannotated sentences by matching of charactersequences, resulting in partially annotated sen-tences, as shown in Figure 2a.
In this example,the word ????
(the Huqi Mountain)?
in theunannotated sentence matches an item in the lex-icon.
As a result, we obtain a partially-annotatedsentence, in which the segmentation ambiguity ofthe characters ??
(fox)?, ??
(brandy road)?
and??
(mountain)?
are resolved (???
being the be-ginning, ???
being the middle and ???
being theend of the same word).
At the same time, the seg-mentation ambiguity of the surrounding characters??
(at)?
and ??
(save)?
are reduced (???
be-ing either a single-character word or the end ofa multi-character word, and ???
being either asingle-character word or the beginning of a multi-character word).. ?
?
?
?
?
?
?
?
?b b b b b b b b bm m m m m m m m me e e e e e e e es s s s s s s s s(a) ??
(at) ???
(Huqi Mountain) ?
?
(save) ??
(Biyao)?, where ?????
matches a lexicon word.. ?
?
?
?
?
?
?
?
?b b b b b b b b bm m m m m m m m me e e e e e e e es s s s s s s s s(b) ??
(e.g.)????
(lysozyme)?
???
(lactoferrin)?,where ??????
is a hyperlink.Figure 2: Examples of partially annotated data.The paths show possible correct segmentations.Natural annotation, which refers to wordboundaries that can be inferred from URLs, fontsor colors on web pages, also result in partially-annotated sentences.
Taking a web page shownin Figure 2b for example.
It can be inferred fromthe URL tags on ??????
that ???
should beeither the beginning of a multi-character word ora single-character word, and ???
should be eitherthe end a multi-character word or single-characterword.
Similarly, possible tags of the surroundingcharacter ???
and ???
can also be inferred.We turn both lexicons and natural annotationinto the same form of partial annotation withsame unresolved ambiguities, as shown in Figure2, and use them together with available full anno-tation (Figure 1) as the training data for the seg-mentor.
In this section, we describe in detail howto obtain partially annotated sentences from eachresource, respectively.2.1 LexiconsIn this scenario, we assume that there are unla-beled sentences along with a lexicon for the targetdomain.
We obtain partially segmented sentencesby extracting word boundaries from the unlabeledsentences with the help of the lexicon.
Previousmatching methods (Wu and Tseng, 1993; Wongand Chan, 1996) for Chinese word segmentationlargely rely on the lexicons, and are generally con-sidered being weak in ambiguity resolution (Gao865People?sDaily??
(saw)??
(Hainan)???
(tourist industry)??
(full)??
(hope)saw tourist industry in Hainan is full of hopeWikipedia??(mainly)?(is)??
(tourist)?
(industry)?(and)??
(software)??
(industry)mainly is tourist industry and software industry(a) Case of incompatible annotation on ????
(tourist industry)?
between People?s Daily and Wikipedia.Literature?????
(Shuo Wen Jie Zi, a book)?(segmented)?
(annotated)?the segmented and annotated version of Shuo Wen Jie ZiComputer??(each)??(record)?(is)??(splitted)?(into)??
(fields)each record is splitted into several fields(b) Similar subsequence ???(field)?
is segmented differently under different domains in Wikipedia.Table 1: Examples natural annotation from Wikipedia.
Underline marks annotated words.et al., 2005).
But for obtaining the partial labeleddata with lexicon, the matching method can still bea solution.
Since we do not aim to recognize everyword from sentence, we can select a lexicon withsmaller coverage but less ambiguity to achieve rel-atively precise matching result.In this paper, we apply two matching schemesto the same raw sentences to obtain partially an-notated sentences.
The first is a simple forward-maximum matching (FMM) scheme, which isvery close to the forward maximum matching al-gorithm ofWu and Tseng (1993) for Chinese wordsegmentation.
This scheme scans the input sen-tence from left to right.
At each position, it at-tempts to find the longest subsequence of Chi-nese characters that matches a lexicon entry.
Ifsuch an entry is found, the subsequence is taggedwith the corresponding tags, and its surroundingcharacters are also constrained to a smaller set oftags.
If no subsequence is found in the lexicon, thecharacter is left with all the possible tags.
Takingthe sentence in Figure 2a for example.
When thealgorithm scans the second character, ??
?, andfinds the entry ?????
in the lexicon, the sub-sequence of characters is recognized as a word,and tagged with b, m and e, respectively.
At thesame time, the previous character ???
can be in-ferred as only end of a multi-character word (e) ora single-character word (s).
The second matchingscheme is backward maximum matching, whichcan be treated as the application of FMM on thereverse of unlabeled sentences using a lexicon ofreversed words.To mitigate the errors resulting from one singlematching scheme, we combine the two matchingresults by agreement.
The basic idea is that if asubsequence of sentence is recognized as word bymultiple matching results, it can be considered as amore precise annotation.
Our algorithm reads par-tial segmentation by different methods and selectsthe subsequences that are identified as word by allmethods as annotated words.2.2 Natural AnnotationWe use the Chinese Wikipedia for natural anno-tation.
Partially annotated sentences are readilyformed in Wikipedia by markup syntax, such asURLs.
However, some subtle issues exist if thesentences are used directly.
One problem is in-compatibility of segmentation standards betweenthe annotated training data and Wikipedia.
Jianget al.
(2009) discuss this incompatibility problembetween two corpora ?
the CTB and the Peo-ple?s Daily; the problem is even more severe onWikipedia because it can be edited by any user.Table 1a shows a case of incompatible annota-tion between the People?s Daily data and naturalannotation in Wikipedia, where the three charac-ters ?????
are segmented differently.
Both canbe treated as correct, although they have differentsegmentation granularities.Another problem is the intrinsic ambiguity ofsegmentation.
The same character sequence canbe segmented into different words under differ-ent contexts.
If the training and test data containdifferent contexts, the learned model can give in-correct results on the test data.
This is particu-larly true across different domains.
Table 1b givessuch an example, where the character sequence????
is segmented differently in two of our testdomains, but both cases exist in Wikipedia.In summary, Wikipedia introduces both use-ful information for domain adaptation and harm-ful noise with negative effects on the model.
To866achieve better performance of domain adaptationusing Wikipedia, one intuitive approach is to se-lect more domain-related data and less irrelevantdata to minimize the risks that result from incom-patible annotation and domain difference.To this end, we assume that there are some rawsentences on the target domain, which can be usedto evaluate the relevance between Wikipedia andtarget domain test data.
We assume that URL-tagged entries reflect the segmentation standardsof Wikipedia sentence, and use them to matchWikipedia sentences with the raw target domaindata.
If the character sequence of any URL-taggedentry in a Wikipedia sentence matches the targetdomain data, the Wikipedia sentence is selectedfor training.
Another advantage of such data se-lection is that the training time consumption canbe reduced by reducing the size of training data.3 CRF for Word SegmentationWe follow the work of Zhao et al.
(2010) and Sunand Xu (2011), and adopt the Conditional RandomFields (CRF) model (Lafferty et al., 2001) for thesequence labeling problem of word segmentation.Given an input characters sequence, the task is toassign one segmentation label from {b,m, e, s} oneach character.
Let x = (x1, x2, ..., xT) be thesequence of characters in sentence whose lengthis T , and y = (y1, y2, ..., yT) be the correspond-ing label sequence, where yi?
Y .
The linear-chain conditional random field for Chinese wordsegmentation can be formalized asp(y|x) =1ZexpT?t=1?k?kfk(yt, yt?1,x) (1)where ?kare the model parameters, fkare the fea-ture functions and Z is the probability normalizer.Z =?yexpT?t=1?k?kfk(yt, yt?1,x) (2)We follow Sun and Xu (2011) and use the fea-ture templates shown in Table 2 to model the seg-mented task.
For ith character in the sentence, then-gram features represent the surrounding charac-ters of this character; Type categorizes the charac-ter it into digit, punctuation, english and other;Identical indicates whether the input character isthe same with its surrounding characters.
Thisfeature captures repetition patterns such as ???
(try)?
or ???
(stroll)?.Type Templateunigram Cs(i?
3 < s < i + 3)bigram CsCs+1(i?
3 < s < i + 2)CsCs+2(i?
3 < s < i + 1)type Type(Ci)Type(Cs)Type(Cs+1)(i?
1 < s < i + 2)identical Identical(Cs, Cs+1) (i ?
3 <s < i + 1)Identical(Cs, Cs+2) (i ?
3 <s < i)Table 2: Feature templates for the ith character.For fully-annotated training data, the learningproblem of conditional random fields is to maxi-mize the log likelihood over all the training data(Lafferty et al., 2001)L =N?n=1log p(y(n)|x(n))Here N is the number of training sentences.
Boththe likelihood p(y(n)|x(n)) and its gradient can becalculated by performing the forward-backwardalgorithm (Baum and Petrie, 1966) on the se-quence, and several optimization algorithm can beadopted to learn parameters from data, includingL-BFGS (Liu and Nocedal, 1989) and SGD (Bot-tou, 1991).4 Training a CRF with partiallyannotated dataFor word segmentation with partially annotateddata, some characters in a sentence can havea definite segmentation label, while some canhave multiple labels with ambiguities remain-ing.
Taking the partially annotated sentencein Figure 2a for example, the correspondingpotential label sequence for ???????
is{(e, s), (b), (m), (e), (b, s)}, where the characters??
?, ???
and ???
have fixed labels but for ??
?and ??
?, some ambiguities exist.
Note that thefull annotation in Figure 1 can be regarded as aspecial case of partial annotation, where the num-ber of potential labels for each character is one.We follow Tsuboi et al.
(2008) and modelmarginal probabilities over partially annotateddata.
Define the possible labels that correspondto the partial annotation as L = (L1, L2, ..., LT),where each Liis a non-empty subset of Y that cor-responds to the set of possible labels for xi.
Let867YLbe the set of all possible label sequences where?y ?
YL, yi?
Li.
The marginal probability ofYLcan be modeled asp(YL|x) =1Z?y?YLexpT?t=1?k?kfk(yt, yt?1,x) (3)Defining the unnormalized marginal probability asZYL=?y?YLexpT?t=1?k?kfk(yt, yt?1,x),and the normalizer Z being the same as Equation2, the log marginal probability of YLover N par-tially annotated training examples can be formal-ized asLYL=N?n=1log p(YL|x) =N?n=1(logZYL?
logZ)The gradient of the likelihood can be written as?LYL?
?k=N?n=1T?t=1?yYL?Lt,y?YL?Lt?1fk(yYL, y?YL,x)pYL(yYL, y?YL|x)?N?n=1T?t=1?y,y?fk(y, y?,x)p(y, y?|x)Both ZYLand its gradient are similar in form toZ.
By introducing a modification to the forward-backward algorithm, ZYLand LYLcan be calcu-lated.
Define the forward variable for partially an-notated data ?YL,t(j) = pYL(x?1,...,t?, yt= j).
Amodification on the forward algorithm can be for-malized as?YL,t(j) ={0 j /?
Lt?i?Lt?1?t(j, i, xt)?YL,t?1(i) j ?
Ltwhere?t(j, i, x) is a potential function that equals?k?kfk(yt= j, yt?1= i, xt).
Similarly, for thebackward variable ?YL,t,?YL,t(i) ={0 i /?
Lt?j?Lt+1?t(j, i, xt+1)?YL,t+1(j) i ?
LtZYLcan be calculated by ?YL(T ),and pYL(y, y?|x) can be calculated by?YL,t?1(y?
)?t(y, y?, xt)?YL,t(y).Note that if each element in YLis constrainedto one single label, the CRF model in Equation 3degrades into Equation 1.
So we can train a unifiedmodel with both fully and partially annotated data.We implement this CRF model based on a opensource toolkit CRFSuite.1In our experiments, weuse the L-BFGS (Liu and Nocedal, 1989) algo-rithm to learn parameters from both fully and par-tially annotated data.5 ExperimentsWe perform our experiments on the domain adap-tation test data from SIGHANBakeoff 2010 (Zhaoet al., 2010), adapting annotated training sentencesfrom People?s Daily (PD) (Yu et al., 2001) todifferent test domains.
The fully annotated datais selected from the People?s Daily newspaperin January of 1998, and the four test domainsfrom the SIGHAN Bakeoff 2010 include finance,medicine, literature and computer.
Sample seg-mented data in the computer domain from thisbakeoff is used as development set.
Statistics ofthe data are shown in first half of Table 3.
Weuse wikidump201404192for the Wikipedia data.All the traditional Chinese pages in Wikipedia areconverted to simplified Chinese.
After filteringfunctional pages like redirection and removing du-plication, 5.45 million sentences are reserved.For comparison with related work on using alexicon to improve segmentation, another set oftest data is chosen for this setting.
We use the Chi-nese Treebank (CTB) as the source domain data,and Zhuxian (a free Internet novel, also named as?Jade dynasty?, referred to as ZX henceforth) asthe target domain data.3The ZX data are writtenin a different style from newswire, and containsmany out-of-vocabulary words.
This setting hasbeen used by Liu and Zhang (2012) and Zhang etal.
(2014) for domain adaptation of segmentationand POS-tagging.
We use the standard training,development and test split.
Statistics of the testdata annotated by Zhang et al.
(2014) are shownin the second half of Table 3.The data preparation method in Section 2 andthe CRF method in Section 4 are used for allthe experiments.
Both recall of out-of-vocabularywords (Roov) and F-score are used to evaluate the1http://www.chokkan.org/software/crfsuite/2http://dumps.wikimedia.org/zhwiki/20140419/3Annotated target domain test data and lexicon are avail-able from http://ir.hit.edu.cn/?mszhang/eacl14mszhang.zip.868PD?SIGHANData set Train Development TestPD Computer Finance Medicine Literature Computer# sent.
19,056 1,000 560 1,308 670 1,329# words 1,109,734 21,398 33,035 31,499 35,735 35,319OOV 0.1766 0.0874 0.1102 0.0619 0.1522CTB5?ZXData set Train Development Test UnlabeledWikipediaUnlabeledCTB5 ZX# sent.
18,086 788 1,394 32,023 5,456,151# words 493,934 20,393 34,355OOV 0.1377 0.1550Table 3: Statistics of data used in this paper.segmentation performance.
There is a mixture ofChinese characters, English words and numericexpression in the test data from SIGHAN Bakeoff2010.
To test the influence of Wikipedia data onChinese word segmentation alone, we apply reg-ular expressions to detect English words and nu-meric expressions, so that they are marked as notsegmented.
After performing this preprocessingstep, cleaned test input data are fed to the CRFmodel to give a relatively strong baseline.5.1 Free Lexicons5.1.1 Obtaining lexiconsFor domain adaption from CTB to ZX, we usea lexicon released by Zhang et al.
(2014).
Thelexicon is crawled from a online encyclopedia4,and contains the names of 159 characters and ar-tifacts in the Zhuxian novel.
We follow Zhang etal.
(2014) and name it NR for convenience of fur-ther discussion.
The NR lexicon can be treatedas a strongly domain-related, high quality but rel-atively small lexicon.
It?s a typical example offreely available lexicon over the Internet.For domain adaptation from PD to medicine andcomputer, we collect a list of page titles underthe corresponding categories in Wikipedia.
Formedicine, entries under essential medicines, bi-ological system and diseases are collected.
Forcomputer, entries under computer network, Mi-crosoft Windows and software widgets are se-lected.
These lexicons are typical freely availablelexicons that we can access to.5.1.2 Obtaining Unlabeled SentencesFor ZX, partially annotated sentences are obtainedusing the NR lexicon and unlabeled ZX sentencesby applying the matching scheme described in4http://baike.baidu.com/view/18277.htm90.190.290.390.41 2 4 8 16 32# of sentences * 1000F scoreon developmentFigure 3: F-score on the development data whenusing different numbers of unlabeled data.Section 2.
The CTB5 training data and the par-tially annotated data are mixed as the final train-ing data.
Different amounts of unlabeled data areapplied to the development test set, and results areshown in Figure 3.
From this figure we can seethat incorporating 16K sentences gives the high-est accuracy, and adding more partial labeled datadoes not change the accuracy significantly.
So forthe ZX experiments, we choose the 16K sentencesas the unlabeled data.For the medicine and computer experiments, weselected domain-specific sentences by matchingwith the domain-specific lexicons.
About 46K outof the 5.45 million wiki sentences contain subse-quences in the medicine lexicon and 22K in thecase of the computer domain.
We randomly se-lect 16K sentences as the unlabeled data for eachdomain, respectively.5.1.3 Final resultsWe incorporate the partially annotated data ob-tained with the help of lexicon for each of thetest domain.
For adaptation from CTB to ZX, wetrained our baseline model on the CTB5 trainingdata with the feature templates in Table 2.
Foradaptation from PD to medicine and computer, we869Domain ZX Medicine ComputerF Roov F Roov F RoovBaseline 87.50 73.65 91.36 72.95 93.16 84.02Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63Zhang et al.
(2014) 88.34 - - - - -Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computerdomains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.trained our baseline model on the PD training datawith the same feature template setting.Previous research makes use of a lexicon byadding lexicon features directly into a model (Sunand Xu, 2011; Zhang et al., 2014), rather thantransforming them into partially annotated sen-tences.
To make a comparison, we follow Sun andXu (2011) and add three lexicon features to repre-sent whether ciis located at the beginning, middleor the end of a word in the lexicon, respectively.For each test domain, the lexicon for the lexi-con feature model consists of the most frequentwords in the source domain training data (about6.7K for CTB5 and 8K for PD, respectively) andthe domain-specific lexicon we obtained in Sec-tion 5.1.1.The results are shown in Table 4, where the firstrow shows the performance of the baseline mod-els and the second row shows the performanceof the model incorporating lexicon feature.
Thethird row shows our method using partial anno-tation.
On the ZX test set, our method outper-forms the baseline by more than 3 absolute per-centage.
The model with partially annotated dataperforms better than the one with additional lexi-con features.
Similar conclusion is obtained whenadapting from PD to medicine and computer.
Byincorporating the partially annotated data, the seg-mentation of lexicon words, along with the con-text, is learned.We also compare our method with the work ofZhang et al.
(2014), who reported results only onthe ZX test data.
We use the same lexicon settings.Our method gives better result than Zhang et al.
(2014), showing that the combination of a lexiconand unannotated sentence into partially annotateddata can lead to better performance than using adictionary alone in type-supervision.
Given thatwe only explore the use of free resource, combin-ing a lexicon with unannotated sentences is a bet-ter option than using the lexicon directly.
Zhanget al.
?s concern, on the other hand, is to compareMethodCom.
DevF RoovBaseline 93.56 83.75Baseline+PA (Random 160K) 94.29 86.58Baseline+PA (Selected) 95.00 88.28Table 5: The performance of data selection on thedevelopment set of the computer domain.type- and token-annotation.
Our partial annota-tion can thus be treated as a compromise to obtainsome pseudo partial token-annotations when fulltoken annotations are unavailable.
Another thingto note is that the model of Zhang et al.
(2014) isa joint model for segmentation and POS-tagging,which is generally considered stronger than a sin-gle segmentation model.5.2 Free Natural AnnotationWhen extracting word boundaries from Wikipediasentences, we ignore natural annotations on En-glish words and digits because these words are rec-ognized by the preprocessor.
Following Jiang etal.
(2013), we also recognize a naturally annotatedtwo-character subsequence as a word.5.2.1 Effect of data selectionTo make better use of more domain-specific data,and to alleviate noise in partial annotation, we ap-ply the selection method proposed in Section 2to the Wikipedia data.
On the computer domaindevelopment test data, this selection method re-sults in 9.4K computer-related sentences with par-tial annotation.
A model is trained with both thePD training data and the partially annotated com-puter domain Wikipedia data.
For comparison, wealso trained a model with 160K randomly selectedWikipedia sentences.
The experimental result isshown in Table 5.
The model incorporating se-lected data achieves better performance comparedto the model with randomly sampled data, demon-strating that data selection is helpful to improving870MethodFinance Medicine Literature ComputerAvg-FF Roov F Roov F Roov F RoovBaseline 95.20 86.90 91.36 72.90 92.27 73.61 93.16 83.48 93.00Baseline+PA (Ran-dom 160K)95.16 87.60 92.41 78.13 92.17 75.30 93.91 83.48 93.41Baseline+PA(Selected)95.54 88.53 92.47 78.28 92.49 76.84 93.93 87.53 93.61+0.34 +1.11 +0.22 +0.77Jiang et al.
(2013) 93.16 93.34 93.53 91.19 92.80Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.the domain adaption accuracy.5.2.2 Final ResultThe final results on the four test domains areshown in Table 6.
From this table, we can seethat significant improvements are achieved withthe help of the partially annotated Wikipedia data,when compared to the baseline.
The modelstrained with selected partial annotation performbetter than those trained with random partial an-notation.
Our F-scores are competitive to those re-ported by Jiang et al.
(2013).
However, since theirmodel is trained on a different source domain, theresults are not directly comparable.5.2.3 AnalysisIn this section, we study the effect of Wikipedia ondomain adaptation when no data selection is per-formed, in order to analyze the effect of partiallyannotated data.
We randomly sample 10K, 20K,40K, 80K and 160K sentences from the 5.45 mil-lion Wikipedia sentences, and incorporate theminto the training process, respectively.
Five modelsare obtained adding the baseline, and we test theirperformances on the four test domains.
Figure 4shows the results.From the figure we can see that for the medicineand computer domains, where the OOV rate is rel-atively high, the F-score generally increases whenmore data from Wikipedia are used.
The trendsof F-score and OOV recall against the volume ofWikipedia data are almost identical.
However, forthe finance and literature domains, which have lowOOV rates, such a relation between data size andaccuracy is not witnessed.
For the literature do-main, even an opposite trends is shown.We can draw the following conclusions: (1)Natural annotation on Wikipedia data contributesto the recognition of OOV words on domain adap-tation; (2) target domains with more OOV wordsbenefit more from Wikipedia data.
(3) along withMethodMed.
Com.F FBaseline 91.36 93.16Baseline+PA (Lex) 91.68 93.47Baseline+PA (Natural) 92.47 93.93Baseline+PA (Lex+Natural) 92.63 94.07Table 7: Results by combining different sources offree annotation.the positive effect on OOV recognition, Wikipediadata can also introduce noise, and hence data se-lection can be useful.5.3 Combining Lexicon and NaturalAnnotationTo make the most use of free annotation, we com-bine available free lexicon and natural annotationresources by joining the partially annotated sen-tences derived using each resource, training ourCRF model with these partially annotated sen-tences and the fully annotated PD sentences.
Thetests are performed on medicine and computer do-mains.
Table 7 shows the results, where furtherimprovements are made on both domains when thetwo types of resources are combined.6 Related WorkThere has been a line of research on making use ofunlabeled data for word segmentation.
Zhao andKit (2008) improve segmentation performance bymutual information between characters, collectedfrom large unlabeled data; Li and Sun (2009) usepunctuation information in a large raw corpus tolearn a segmentation model, and achieve betterrecognition of OOVwords; Sun and Xu (2011) ex-plore several statistical features derived from un-labeled data to help improve character-based wordsegmentation.
These investigations mainly focuson in-domain accuracies.
Liu and Zhang (2012)8710.95160.95190.95220.95250 50 100 1500.87000.87250.87500.8775(a) Finance0.91500.91750.92000.92250 50 100 1500.730.740.750.760.770.78(b) Medicine0.92160.92190.92220.92250 50 100 150 0.7350.7400.7450.750(c) Literature0.9340.9360.9380 50 100 1500.840.850.860.87(d) ComputerFigure 4: Performance of the model incorporating difference sizes of Wikipedia data.
The solid linerepresents the F-score and dashed line represents the recall of OOV words.study domain adaptation using an unsupervisedself-training method.
In contrast to their work,we make use of not only unlabeled data, but alsoleverage any free annotation to achieve better re-sults for domain adaptation.There has also been work on making use of adictionary and natural annotation for segmenta-tion.
Zhang et al.
(2014) study type-supervised do-main adaptation for Chinese segmentation.
Theycategorize domain difference into two types: dif-ferent vocabulary and different POS distributions.While the first type of difference can be effec-tively resolved by using lexicon for each domain,the second type of difference needs to be resolvedby using annotated sentences.
They found thatgiven the same manual annotation time, a com-bination of the lexicon and sentence is the mosteffective.
Jiang et al.
(2013) use 160K Wikipediasentences to improves segmentation accuracies onseveral domains.
Both Zhang et al.
(2014) andJiang et al.
(2013) work on discriminative mod-els using the structure perceptron (Collins, 2002),although they study two different sources of infor-mation.
In contrast to their work, we unify bothtypes of information under the CRF framework.CRF has been used for Chinese word segmenta-tion (Tseng, 2005; Shi and Wang, 2007; Zhao andKit, 2008; Wang et al., 2011).
However, most pre-vious work train a CRF by using full annotationonly.
In contrast, we study CRF based segmenta-tion by using both full and partial annotation.Several other variants of CRF model has beenproposed in the machine learning literature, suchas the generalized expectation method (Mann andMcCallum, 2008), which introduce knowledge byincorporating a manually annotated feature dis-tribution into the regularizer, and the JESS-CM(Suzuki and Isozaki, 2008), which use a EM-likemethod to iteratively optimize the parameter onboth the annotated data and unlabeled data.
Incontrast, we directly incorporate the likelihood ofpartial annotation into the objective function.
Thework that is the most similar to ours is Tsuboi etal.
(2008), who modify the CRF learning objec-tive for partial data.
They focus on Japanese lexi-cal analysis using manually collected partial data,while we investigate the effect of partial annota-tion from freely available sources for Chinese seg-mentation.7 ConclusionIn this paper, we investigated the problem of do-main adaptation for word segmentation, by trans-ferring various sources of free annotations into aconsistent form of partially annotated data and ap-plying a variant of CRF that can be trained usingfully- and partially-annotated data simultaneously.We performed a large set of experiments to studythe effectness of free data, finding that they areuseful for improving segmentation accuracy.
Ex-periments also show that proper data selection canfurther benefit the model?s performance.872AcknowledgmentsWe thank the anonymous reviewers for their con-structive comments.
This work was supportedby the National Key Basic Research Program ofChina via grant 2014CB340503 and the NationalNatural Science Foundation of China (NSFC) viagrant 61133012 and 61370164, the SingaporeMinistry of Education (MOE) AcRF Tier 2 grantT2MOE201301 and SRG ISTD 2012 038 fromSingapore University of Technology and Design.ReferencesLeonard E Baum and Ted Petrie.
1966.
Statisticalinference for probabilistic functions of finite statemarkov chains.
The annals of mathematical statis-tics, pages 1554?1563.L?eon Bottou.
1991.
Stochastic gradient learning inneural networks.
In Proceedings of Neuro-N?
?mes91, Nimes, France.
EC2.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8.
Associ-ation for Computational Linguistics, July.Jianfeng Gao, Mu Li, Andi Wu, and Chang-NingHuang.
2005.
Chinese word segmentation andnamed entity recognition: A pragmatic approach.Comput.
Linguist., 31(4):531?574, December.Dan Garrette and Jason Baldridge.
2012.
Type-supervised hidden markovmodels for part-of-speechtagging with incomplete tag dictionaries.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 821?831, Jeju Island, Korea, July.
Association for Com-putational Linguistics.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging ?
a case study.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 522?530, Suntec, Singapore,August.
Association for Computational Linguistics.Wenbin Jiang, Meng Sun, Yajuan L?u, Yating Yang, andQun Liu.
2013.
Discriminative learning with natu-ral annotations: Word segmentation as a case study.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 761?769, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for chinese word segmentation.Comput.
Linguist., 35(4):505?512, December.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Math.Program., 45(3):503?528, December.Yang Liu and Yue Zhang.
2012.
Unsupervised domainadaptation for joint segmentation and POS-tagging.In Proceedings of COLING 2012: Posters, pages745?754, Mumbai, India, December.
The COLING2012 Organizing Committee.Gideon S. Mann and Andrew McCallum.
2008.Generalized expectation criteria for semi-supervisedlearning of conditional random fields.
In Proceed-ings of ACL-08: HLT, pages 870?878, Columbus,Ohio, June.
Association for Computational Linguis-tics.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layercrfs based joint decoding method for cascaded seg-mentation and labeling tasks.
In Proceedings of the20th International Joint Conference on Artifical In-telligence, IJCAI?07, pages 1707?1712, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Yan Song, Prescott Klassen, Fei Xia, and Chunyu Kit.2012.
Entropy-based training data selection for do-main adaptation.
In Proceedings of COLING 2012:Posters, pages 1191?1200, Mumbai, India, Decem-ber.
The COLING 2012 Organizing Committee.Weiwei Sun and Jia Xu.
2011.
Enhancing chineseword segmentation using unlabeled data.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 970?979, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proceedings of ACL-08: HLT, pages 665?673, Columbus, Ohio, June.Association for Computational Linguistics.Huihsin Tseng.
2005.
A conditional random field wordsegmenter.
In In Fourth SIGHAN Workshop on Chi-nese Language Processing.Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, HirokiOda, and Yuji Matsumoto.
2008.
Training condi-tional random fields using incomplete annotations.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages897?904, Manchester, UK, August.
Coling 2008 Or-ganizing Committee.873Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Tori-sawa.
2011.
Improving chinese word segmentationand pos tagging with semi-supervised methods usinglarge auto-analyzed data.
In Proceedings of 5th In-ternational Joint Conference on Natural LanguageProcessing, pages 309?317, Chiang Mai, Thailand,November.
Asian Federation of Natural LanguageProcessing.Pak-kwong Wong and Chorkin Chan.
1996.
Chineseword segmentation based on maximum matchingand word binding force.
In Proceedings of the 16thConference on Computational Linguistics - Volume1, COLING ?96, pages 200?203, Stroudsburg, PA,USA.
Association for Computational Linguistics.Zimin Wu and Gwyneth Tseng.
1993.
Chinese textsegmentation for text retrieval: Achievements andproblems.
J.
Am.
Soc.
Inf.
Sci., 44(9):532?542, Oc-tober.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as lmr tagging.
In Proceedings of theSecond SIGHAN Workshop on Chinese LanguageProcessing - Volume 17, SIGHAN ?03, pages 176?179, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Shiwen Yu, Jianming Lu, Xuefeng Zhu, HuimingDuan, Shiyong Kang, Honglin Sun, Hui Wang,Qiang Zhao, and Weidong Zhan.
2001.
Processingnorms of modern chinese corpus.
Technical report.Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-based perceptron algorithm.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 840?847, Prague, Czech Republic, June.
Association forComputational Linguistics.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2014.
Type-supervised domain adaptation forjoint segmentation and pos-tagging.
In Proceed-ings of the 14th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 588?597, Gothenburg, Sweden, April.
Asso-ciation for Computational Linguistics.Hai Zhao and Chunyu Kit.
2008.
An empirical com-parison of goodness measures for unsupervised chi-nese word segmentation with a unified framework.In In: The Third International Joint Conference onNatural Language Processing (IJCNLP-2008.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2010.
A unified character-based tagging frame-work for chinese word segmentation.
9(2):5:1?5:32,June.874
