Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1397?1402,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsOn the Automatic Learning of Sentiment LexiconsAliaksei SeverynDISI, University of Trento38123 Povo (TN), Italyseveryn@disi.unitn.itAlessandro MoschittiQatar Computing Research Institue5825 Doha, Qataramoschitti@qf.org.qaAbstractThis paper describes a simple and princi-pled approach to automatically construct sen-timent lexicons using distant supervision.
Weinduce the sentiment association scores forthe lexicon items from a model trained ona weakly supervised corpora.
Our empiri-cal findings show that features extracted fromsuch a machine-learned lexicon outperformmodels using manual or other automaticallyconstructed sentiment lexicons.
Finally, oursystem achieves the state-of-the-art in TwitterSentiment Analysis tasks from Semeval-2013and ranks 2nd best in Semeval-2014 accordingto the average rank.1 IntroductionOne of the early and rather successful models forsentiment analysis (Pang and Lee, 2004; Pang andLee, 2008) relied on manually constructed lexiconsthat map words to their sentiment, e.g., positive,negative or neutral.
The document-level polarity isthen assigned by performing some form of averag-ing, e.g., majority voting, of individual word polari-ties found in the document.
These systems show anacceptable level of accuracy, they are easy to buildand are highly computationally efficient as the onlyoperation required to assign a polarity label are theword lookups and averaging.
However, the informa-tion about word polarities in a document are best ex-ploited when using machine learning models to traina sentiment classifier.In fact, most successful sentiment classificationsystems rely on supervised learning.
Interestingly,a simple bag of words model using just unigramsand bigrams with an SVM has shown excellent re-sults (Wang and Manning, 2012) performing on paror beating more complicated models, e.g., usingneural networks (Socher et al, 2011).Regarding Twitter sentiment analysis, the topperforming system (Mohammad et al, 2013)from Semeval-2013 Twittter Sentiment Analysistask (Nakov et al, 2013) follows this recipe by train-ing an SVM on various surface form, sentiment andsemantic features.
Perhaps, the most valuable find-ing is that sentiment lexicons appear to be the mostuseful source of features accounting for over 8 pointgains in the F-measure on top of the standard featuresets.Sentiment lexicons are mappings from words toscores capturing the degree of the sentiment ex-pressed by a given word.
While several manuallyconstructed lexicons are made available, e.g., theMPQA (Wilson et al, 2005), the Bing and Liu (Huand Liu, 2004) and NRC Emoticon (Mohammadand Turney, 2013) lexicons, providing high qualityword-sentiment associations compiled by humans,still their main drawback is low recall.For example, the largest NRC Emoticon lexiconcontains only 14k items, whereas tweets with ex-tremely sparse surface forms are known to form verylarge vocabularies.
Hence, using larger lexiconswith better recall has the potential of learning moreaccurate models.
Extracting such lexicons automat-ically is a challenging and interesting problem (Lauet al, 2011; Bro and Ehrig, 2013; Liu et al, 2013;Tai and Kao, 2013; Yang et al, 2014; Huang et al,2014).
However, different from previous work ourgoal is not to extract human-interpretable lexiconsbut to use them as a source of features to improvethe classifier accuracy.1397Following this idea, the authors in (Mohammadet al, 2013) use features derived from the lexi-cons to build a state-of-the-art sentiment classifierfor Twitter.
They construct automatic lexicons us-ing noisy labels automatically inferred from emoti-cons and hashtags present in the tweets.
The word-sentiment association scores are estimated usingpointwise mutual information (PMI) computed be-tween a word and a tweet label.While the idea to model statistical correlationsbetween the words and tweet labels using PMI orany other metric is rather intuitive, we believe thereis a more effective way to exploit noisy labels forestimating the word-sentiment association scores.Our method relies on the idea of distant supervision(Marchetti-Bowick and Chambers, 2012).
We usea large distantly supervised Twitter corpus, whichcontains noisy opinion labels (positive or negative)to learn a supervised polarity classifier.
We encodetweets using words and multi-word expressions asfeatures (which are also entries in our lexicon).
Theweights from the learned model are then used to de-fine which lexicon items to keep, i.e., items that con-stitute a good sentiment lexicon.
The scores for thelexicon items can be then directly used to encodenew tweets or used to derive more advanced fea-tures.
Using machine learning to induce the scoresfor the lexicon items has an advantage of learningthe scores that are directly optimized for the classi-fication task, where lexicon items with higher dis-criminative power tend to receive higher weights.To assess the effectiveness of our approach, we re-implemented the state-of-the-art system ranking 1stin Semeval-2013 Twitter Sentiment Analysis chal-lenge and used it as our baseline.
We show thatadding features from our machine-learned sentimentlexicon yields better results than any of the auto-matic PMI lexicons used in the baseline and all ofthem combined together.
Our system obtains newstate-of-the-art results on the SemEval-2013 mes-sage level task with an F-score of 71.32 ?
a 2% ofabsolute improvement over the previous best sys-tem in SemEval-2013.
We also evaluate the util-ity of the ML lexicon on the five test sets from arecent Semeval-2014 task showing significant im-provement over a strong baseline.
Finally, our sys-tem shows high accuracy among the 42 systems par-ticipating in the Semeval-2014 challenge ranking2nd best according to the average rank across all testsets.2 Our modelWe treat the task of sentiment analysis as a super-vised learning problem, where we are given labeleddata {(xi,yi)}ni=1and the goal is to estimate a de-cision function f(x)?
y that maps input examplesto labels.
In particular, we use a linear SVM modelwith the prediction function of the following form:f = sign(wTx + b), where the model weights ware estimated from the training set.In the following we describe our approach to con-struct sentiment lexicons by learning an SVM modelon the the distant supervised dataset.
Finally, we de-scribe our baseline model.2.1 Distant Supervision for Automatic LexiconConstructionOur sentiment lexicon consists of words and wordsequences (we only use word unigrams and bi-grams).
To select lexicon items from a set of all un-igrams and bigrams, we propose the following pro-cess:1.
Collect a large unlabelled corpus of tweets C.2.
For each tweet ti?
C use cues (hashtags oremoticons) to automatically infer its label (pos-itive or negative): yi?
{?1,+1}.
For example,positive or negative emoticons, such as ?:-)?
or?:(?
are good indicators of the general sentimentexpressed by a tweet.3.
Extract unigram and bigram features to encode atweet tiinto a feature vector xi?
R|L|, where thelexicon L is a set of unigrams and bigrams.5.
Train an SVM model w =?i=1..N?iyixionthe encoded corpus C = {(xi, yi)}Ni=1.
Themodel w ?
R|L|is a dense vector whose com-ponents are obtained from a weighted combina-tion of training examples xi(support vectors) andtheir labels yi(only those instances with ?i> 0contribute to the components of w).6.
Given that the each component wjof the modelw directly corresponds to the lexicon entry lj?L its raw score is used as a sentiment associationscore.Different from manually constructed lexiconscompiled by humans where each item is assigned1398with an interpretable sentiment score, the scores inthe automatic lexicon are learned automatically on aweakly supervised task.
We use the weights from anSVM model whose weights are formed by the sup-port vectors, i.e., the most difficult instances closeto the decision boundary, hence most useful for theclassification task.
Additionally, due to its regulari-sation properties, SVM is known to select only themost robust features, which is important in the caseof noisy labeled data.
Hence, our method is a moreprincipled way grounded in the statistical learningtheory to exploit the noisy labels for estimating theword-sentiment association scores for the lexiconentries.
Moreover, feature engineering with our lex-icon appears to be more helpful (see Sec.
3) on asupervised task.2.2 Baseline modelWe re-implement the state-of-the-art NRC modelfrom (Mohammad et al, 2013), which ranked 1st inthe Semeval-2013, and use it as our baseline.
Thissystem relies on various n-gram, surface form andlexicon features.
Briefly, we engineered the follow-ing feature sets:1?
Word and character grams: we use 1,2,3 n-grams for words and 3,4,5 n-grams for charactersequences;?
Negation: the number of negated contexts ?
aspan of words between a negation word (not,never), and a punctuation mark.?
Lexicons: given a word, we lookup its sentimentpolarity score in the lexicon: score(w).
The fol-lowing aggregate features are produced for thelexicon items found in a tweet: the total count,the total sum, the maximal score and the scoreof the last token.
These features are producedfor unigrams, bigrams, each part-of-speech tag,hashtags and all-caps tokens.?
Other: number of hashtags, capitalized words,elongated words, positive and negative emoti-cons, punctuation.3 ExperimentsIn the following experiments our goal is to assessthe value of our distant supervision method to au-1our baseline system, lexicon and the code to construct it arefreely available at: https://github.com/yyyNegative Positive(disappointing,) (no, problem)(depressing,) (not, bad)(bummer,) (not, sad)(sadly,) (cannot, wait)(passed, away) (no, prob)Table 1: Lexicon items learned from Emoticon140 cor-pus with top negative and positive scores.tomatically extract sentiment lexicons.
We com-pare its performance with other automatically con-structed lexicons extracted from large Twitter cor-pora, e.g., auto lexicons built using the PMI ap-proach from (Mohammad et al, 2013).3.1 Lexicon learningWe extract our lexicon from a freely availableEmoticon140 Twitter corpus (Go et al, 2009),where the sentiment labels are automatically in-ferred from emoticons contained in a tweet2.
Themajor advantage of such corpora is that it is easy tobuild as emoticons serve as fairly good cues for thegeneral sentiment expressed in a tweet, thus they canbe used as noisy labels.
Hence, large datasets can becollected without incurring any annotation costs.Tweets with positive emoticons, like ?
:)?, areassumed to be positive, and tweets with negativeemoticons, like ?
:(?, are labeled as negative.
Thecorpus contains 1.6 million tweets with equal distri-bution between positive and negative tweets.
We usea tokeniser from the CMU Twitter tagger (Gimpel etal., 2011) extracting only unigrams and bigrams3toencode training instances.
To make the extraction ofword-sentiment association weights from the modelstraight-forward, we ignore neutral labels thus con-verting the task to a binary classification task.
Weuse LibLinear (Fan et al, 2008) with L2 regulariza-tion and default parameters to learn a model.
Pre-processing, feature extraction and learning is veryfast taking only a few minutes.
As the number ofunique unigrams and bigrams can be very large andwe would like to keep our sentiment lexicon rea-2unfortunately, the corpus to build the NRC Hashtag lex-icon (Mohammad et al, 2013) is not freely available due toTwitter data distribution policies.3Adding tri-grams yielded a very minor improvement, yetthe size of the dictionary exploded, so to keep the size of thedictionary relatively small we use only uni- and bi-grams.1399Dataset Size Pos Neg.
Neu.Train?13 9,728 38% 15% 47%Dev?13 1,654 35% 21% 45%Twitter?13 3,813 41% 16% 43%SMS?13 2093 24% 19% 58%Twitter?14 1,853 53% 11% 36%Sarcasm?14 86 38% 47% 15%LiveJournal?14 1,142 37% 27% 36%Table 2: Datasets.sonably small, we filter entries with small weights.In particular, we found that selecting items with aweight greater than 1e?
6 did not cause any drop inaccuracy, while the resulting lexicon is reasonablycompact ?
it contains about 3 million entries.Table 1 gives an example of top 10 lexicon en-tries with highest positive and negative scores.
In-terestingly, one would expect to find words such asamazing, cool, etc.
as having the highest positivesentiment score.
However, an SVM model assignshigher scores to bigrams containing negative wordsproblem, bad, worries, to outweigh their negativeimpact.
This helps to handle the inversion of thesentiment due to negations.It is important to note that our goal is differentfrom constructing sentiment lexicons that are inter-pretable by humans, e.g., manually built lexicons,but, similar to (Mohammad et al, 2013), we buildautomatic lexicons to derive highly discriminativefeatures improving the accuracy of our sentimentprediction models.3.2 SetupTask.
We focused on the Twitter Sentiment Analy-sis (Task 2) from Semeval-2013 (Nakov et al, 2013)and its rerun (Task 9) from Semeval-2014 (Rosen-thal et al, 2014).
Both tasks include two subtasks:an expression-level and a message-level subtasks.Being more general, we focus only on predictingthe sentiment of tweets at the message level, wheregiven a tweet, the goal is to classify whether it ex-presses positive, negative, or neutral sentiment.Evaluation.
We used the official scorers from theSemeval 2013 & 2014, which compute the averagebetween F-measures for the positive and negativeclasses.Data.
We evaluated our models on both Semeval-2013 and Semeval-2014 tasks with 44 and 42 par-ticipating systems correspondingly.
The Semeval-2013 task released the training set containing 9,728tweets, dev and two test sets: Twitter?13 andSMS?13.
We train our model on a combined trainand dev sets4.
The Semeval-2014 re-uses the sametraining data and systems are evaluated on 5 test sets:two test sets from Semeval-2013 and three new testsets: LiveJournal?14, Twitter?14 and Sarcasm?14.The datasets are summarized in Table 3.1.n-gramsManual PMI MLTwitter?13M B N hash s140 raw agg?
63.53?
?
64.96 (+1.43)?
?
66.74 (+3.21)?
?
64.21 (+0.68)?
?
?
?
67.44 (+3.91)?
?
?
?
?
68.47 (+4.94)?
?
?
?
?
69.08 (+5.55)?
?
?
?
?
?
70.06 (+6.53)?
?
?
?
?
69.47 (+5.94)?
?
?
?
?
69.89 (+6.36)?
?
?
?
?
?
70.93 (+7.40)?
?
?
?
?
?
?
?
71.32 (+7.79)best Semeval?13 system 69.06Table 3: Results on Semeval-2013 test set.
Used fea-ture sets: n-grams; features from Manual lexicons us-ing MPQA (M), BingLiu (B) and NRCEmoticon (N)lexicons; PMI lexicon extracted from NRC-hashtag andEmoticon140 (s140) datasets; our ML lexicon using rawand aggregate (agg) features.
The numbers in parenthe-sis indicate absolute improvement w.r.t.
baseline n-gramsmodel.3.3 ResultsWe report the results on two runs of the TwitterSentiment Analysis challenge organized by Semevalfrom 2013 and 2014.3.3.1 Semeval-2013The n-grams model includes word and charactern-grams, negation and various surface form featuresas described in Section 2.
We use this feature setas a yardstick to assess the value of adding features4While in the real setting it is also possible to include ad-ditional weakly labeled data, e.g.
Emoticon140, for training amodel, we stick to the constrained setting of the Semeval tasks,where training is allowed only on the train and dev sets.1400from various lexicons.
Firstly, we note that usingthree manual lexicons: MPQA (M), BingLiu (B),and NRC (N) results in almost 4 points of abso-lute improvement.
Notably, among all manual lex-icons the BingLiu lexicon accounts for the largestimprovement.
Next, we explore the value of au-tomatically generated lexicons using PMI scoringextracted from two large Twitter datasets: Emoti-con140 (s140) and hashtag (hash).
Both lexi-cons rely on PMI scoring formula to derive word-sentiment association scores.
Adding features fromthese automatically generated lexicons results in fur-ther improvement over the n-grams feature set andyields F-score: 70.06.Next, we explore the value of features derivedfrom our ML based lexicon.
We use the lexicon intwo modalities: (i) including the raw scores (raw)of each lexicon entry (unigrams and bigrams) foundin the given tweet; (ii) deriving aggregate features(agg) from the raw scores as described in Sec.
2; and(iii) using both.
We note that the features from ourML-based lexicon yield superior performance to anyof the PMI lexicons providing at least 2% gains andis even better when the two PMI lexicons are com-bined.
Finally, adding the ML-based lexicon on topof the models including manual and auto lexiconsprovides the new state-of-the-art result on Semeval-2013 with an improvement of almost 8 points w.r.t.to the basic model.
Our model achieves the score of71.32 vs. 69.06 for the previous best system.3.3.2 Semeval-2014Table 4 shows that adding features from our ML-based vocabulary provides a substantial improve-ment over the previous best NRC system on 4 out of5 test sets.
Interestingly, we observe a strong drop onthe Sarcasm?14 test set.
One possible reason is thatthe labels for Emoticon140 corpus are inferred auto-matically using emoticons, which may strongly biasour model to incorrectly predict sentiment for thosetweets containing sarcasm.
With more than 40 sys-tems participating in Semeval-2014 challenge, wenote that the majority of systems perform well onlyon few test sets at once while failing on the others5.The performance of our system is rather high acrossall the test sets with an average rank of 3.4, which5http://alt.qcri.org/semeval2014/task9/Table 4: Semeval-2014.
Numbers in parenthesis is theabsolute rank of a system on a given test set.
Bold scorescompares using our ML lexicon on top of the NRC sys-tem.
Results marked with?are statistically significant atp > 0.05 (via the paired t-test).System NRC NRC +ML lex.bestscoreLJournal?14 75.28 (1) 76.54?
(1) 74.84SMS?13 66.86 (5) 67.20 (5) 70.28Twitter?13 70.06 (5) 71.32?
(2) 72.12Twitter?14 68.71 (6) 70.51?
(2) 70.96Sarcasm?14 59.20 (1) 55.08 (7) 58.16ave-rank 3.8 3.4 (2) 2.4 (1)is the second best result in Semeval-2014 message-level task (the best system is from the NRC teamwith an ave-rank 2.4, whereas the closest follow upsystem has an ave-rank 6).4 ConclusionsWe demonstrated a simple and principled approachgrounded in machine learning to construct senti-ment lexicons.
We show that using off-the-shelf ma-chine learning tools to automatically extract lexiconsgreatly outperforms other automatically constructedlexicons that use pointwise mutual information toestimate sentiment scores for the lexicon items.We have shown that combining our machine-learned lexicon with the previous best system yieldsstate-of-the-art results in Semeval-2013 gaining over2 points in F-score and ranking our system 2nd ac-cording to the average rank over the five test setsof Semeval-2014.
Finally, our ML-based lexiconshows excellent results when added on top of thecurrent state-of-the-art NRC system.
While our ex-perimental study is focused on Twitter, our methodis general enough to be applied to sentiment classifi-cation tasks on other domains.
In the future, we planto experiment with constructing ML lexicons fromlarger Twitter corpora also using hashtags.Recently, deep convolutional neural networks forsentence modelling (Kalchbrenner et al, 2014; Kim,2014) have shown promising results on several NLPtasks.
In particular, (Tang et al, 2014) showed thatlearning sentiment-specific word embeddings andusing them as features can boost the accuracy of ex-isting sentiment classifiers.
In the future work weplan to explore such approaches.1401ReferencesJrgen Bro and Heiko Ehrig.
2013.
Automatic construc-tion of domain and aspect specific sentiment lexiconsfor customer review mining.
In CIKM.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: annotation, features, and experiments.
InACL.Alex Go, Richa Bhayani, and Lei Huang.
2009.
Twittersentiment classification using distant supervision.
InCS224N Project Report, Stanford.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In KDD.Sheng Huang, Zhendong Niu, and Chongyang Shi.2014.
Automatic construction of domain-specific sen-timent lexicon based on constrained label propagation.Knowl.-Based Syst.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In ACL.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In EMNLP.Raymond Yiu-Keung Lau, Chun Lam Lai, Peter Bruza,and Kam-Fai Wong.
2011.
Leveraging web 2.0data for scalable semi-supervised learning of domain-specific sentiment lexicons.
In CIKM.Lizhen Liu, Mengyun Lei, and Hanshi Wang.
2013.Combining domain-specific sentiment lexicon withhownet for chinese sentiment analysis.Micol Marchetti-Bowick and Nathanael Chambers.2012.
Learning for microblogs with distant supervi-sion: Political forecasting with twitter.
In EACL.Saif Mohammad and Peter Turney.
2013.
Crowdsourc-ing a word-emotion association lexicon.
Computa-tional Intelligence, 39(3):555?590.Saif M. Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-artin sentiment analysis of tweets.
In Semeval.Preslav Nakov, Zornitsa Kozareva, Alan Ritter, SaraRosenthal, Veselin Stoyanov, and Theresa Wilson.2013.
In semeval-2013 task 2: Sentiment analysis intwitter.
In Semeval.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In ACL.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Sara Rosenthal, Alan Ritter, Preslav Nakov, and VeselinStoyanov.
2014.
In semeval-2014 task 9: Sentimentanalysis in twitter.
In Semeval.Richard Socher, Jeffrey Pennington, Eric H Huang, An-drew Y Ng, and Christopher D Manning.
2011.
Semi-supervised recursive autoencoders for predicting sen-timent distributions.
In EMNLP.Yen-Jen Tai and Hung-Yu Kao.
2013.
Automaticdomain-specific sentiment lexicon generation with la-bel propagation.
In iiWAS.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,and Bing Qin.
2014.
Learning sentiment-specificword embedding for twitter sentiment classification.In ACL.Sida Wang and Christopher Manning.
2012.
Baselinesand bigrams: Simple, good sentiment and topic classi-fication.
In ACL.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In EMNLP.Min Yang, Dingju Zhu, Rashed Mustafa, and Kam-PuiChow.
2014.
Learning domain-specific sentiment lex-icon with supervised sentiment-aware lda.
In ECAI.1402
