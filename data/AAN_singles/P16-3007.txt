Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 43?50,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsWon?t somebody please think of the children?Improving Topic Model Clustering of Newspaper Comments forSummarisationClare LlewellynSchool of InformaticsUniversity of EdinburghEdinburgh, UKs1053147@sms.ed.ac.ukClaire GroverSchool of InformaticsUniversity of EdinburghEdinburgh, UKgrover@inf.ed.ac.ukJon OberlanderSchool of InformaticsUniversity of EdinburghEdinburgh, UKjon@inf.ed.ac.ukAbstractOnline newspaper articles can accumulatecomments at volumes that prevent closereading.
Summarisation of the commentsallows interaction at a higher level andcan lead to an understanding of the over-all discussion.
Comment summarisationrequires topic clustering, comment rank-ing and extraction.
Clustering must be ro-bust as the subsequent extraction relies ona good set of clusters.
Comment data, aswith many social media datasets, containsvery short documents and the number ofwords in the documents is a limiting fac-tors on the performance of LDA cluster-ing.
We evaluate whether we can combinecomments to form larger documents to im-prove the quality of clusters.
We find thatcombining comments with comments thatreply to them produce the highest qualityclusters.1 IntroductionNewspaper articles can accumulate many hun-dreds and sometimes thousands of online com-ments.
When studied closely and analysed effec-tively they provide multiple points of view anda wide range of experience and knowledge fromdiverse sources.
However, the number of com-ments produced per article can prohibit close read-ing.
Summarising the content of these commentsallows users to interact with the data at a higherlevel, providing a transparency to the underlyingdata (Greene and Cross, 2015).The current state of the art within the commentsummarisation field is to cluster comments us-ing Latent Dirichlet Allocation (LDA) topic mod-elling (Khabiri et al, 2011; Ma et al, 2012;Llewellyn et al, 2014).
The comments withineach topic cluster are ranked and comments aretypically extracted to construct a summary of thecluster.
In this paper we focus on the clusteringsubtask.
It is important that the clustering is ap-propriate and robust as the subsequent extractionrelies on a good set of clusters.
Research in a re-lated domain has found that topical mistakes werethe largest source of error in summarising blogs ?
(Mithun and Kosseim, 2009) a similar data type.Comment data, as with many social mediadatasets, differs from other content types as each?document?
is very short.
Previous studies haveindicated that the number of documents and thenumber of words in the documents are limitingfactors on the performance of topic modelling(Tang et al, 2014).
Topic models built usinglonger documents and using more documents aremore accurate.
Short documents can be enrichedwith external data.
In our corpus the number ofcomments on each newspaper article is finite andthe topics discussed within each set have evolvedfrom the original article.
We therefore decided notto increase the set with data from external sources.In this work we consider whether we can com-bine comments within a comments dataset to formlarger documents to improve the quality of clus-ters.
Combining comments into larger documentsreduces the total number of comments available tocluster which may decrease the quality of the clus-ters.
The contribution of this work is in showingthat combining comments with their direct replies,their children, increases the quality of the cluster-ing.
This approach can be applied to any othertask which requires clustering of newspaper com-ments and any other data which contains smalldocuments linked using a thread like structure.Combining data in this way to improve the clus-tering reduces the need to import data from exter-nal sources or to adapt the underlying clusteringalgorithm.432 Related Work2.1 SummarisationThe summarisation domain is well developed.
Theearliest focus of the field was single documentsummarisation ?
for a survey paper see (Gupta andLehal, 2010).
This approach was extended into thesummarisation of multiple documents on the sametopic (Goldstein et al, 2000) and to summarisingdiscussions such as email or Twitter conversations(Cselle et al, 2007; Sharifi et al, 2010; Inouye andKalita, 2011).The basic idea behind the summarisation of tex-tual data is the grouping together of similar infor-mation and describing those groups (Rambow etal., 2004).
Once these groups are formed they aredescribed using either an extractive or abstractiveapproach.
Extractive summarisation uses units oftext, generally sentences, from within the data inthe group to represent the group.
Abstractive sum-marisation creates a description of the data in thegroup as a whole, analogous to the approach a hu-man would take.2.1.1 Comment SummarisationAbstractive summarisation is a very complex task,and because comment summarisation is a rela-tively new task, current work mostly focuses onextractive approaches.
The general task involvesclustering the comments into appropriate topicsand then extracting comments, or parts of com-ments to represent those topics (Khabiri et al,2011; Ma et al, 2012).
Ma et al (2012)summarise discussion on news articles from Ya-hoo!News and Khabiri et al(2011) summarisecomments on YouTube videos.
Both studies agreeon the definition of the basic task as: clusteringcomments into topics, ranking to identify com-ments that are key in the clusters, and evaluat-ing the results through a human study.
Both ap-proaches focus on using LDA topic modelling tocluster the data.
Ma et al (2012) explored twotopic models, one where topics are derived fromthe original news article and a second, extendedversion that allows new topics to be formed fromthe comments.
They found that the extended ver-sion was judged superior in a user study.
Khabiriet al (2011) contrasted LDA topic models withk-means and found topic modelling superior.
Astudy by Llewellyn et al (2014) contrasted topicmodelling, k-means, incremental one pass cluster-ing and clustering on common unigrams and bi-grams.
They found that the topic modelling ap-proach was superior.
Aker et al (2016) lookedat a graph based model that included informationfrom DBpedia, finding that this approach out per-formed an un-optimised LDA model.
They thenlabelled the clusters using LDA clustering and ex-tracted keywords.Other work has been conducted in related do-mains such as summarising blogs, microblogs ande-mail.2.1.2 Blog SummarisationComments are similar to blogs in that they aregenerated by multiple individuals who exhibit avast array of writing styles.
Mithum and Koseim(2009) found that whereas news articles have ageneralisable structure that can be used to aid sum-marisation, blogs are more variable.
In particu-lar they found that errors in blog summarisationare much higher than in news text summarisation.They determined that errors were often due to thecandidate summary sentences being off topic andthey suggest that blog summarisation needs to beimproved in terms of topic detection.
When in-vestigating the summarisation of blogs and com-ments on blogs Balahur et al(2009) found that itis very common to change topics between the orig-inal blog post and the comments, and from com-ment to comment.
The research of Mithum andKoseim (2009) and Balahur et al (2009) indicatesthat topic identification is a key area on which toconcentrate efforts in the emerging field of com-ment summarisation.2.1.3 Microblog SummarisationA significant amount of work has been conductedin the area of Twitter summarisation.
Many Twit-ter summarisation techniques exploit that tweetsoften include hashtags which serve as an indica-tion of their topic.
Duan et al(2012) designed asummarisation framework for Twitter by definingtopics and selecting tweets to represent those top-ics.
The topics are defined using hashtags and aresplit when at high volume by specific time slicesand word frequency.
Rosa et al (2011) also usehashtags to cluster tweets into topics, using themas annotated classes for training data.
They fo-cus on supervised machine learning, specificallySVM and K Nearest Neighbour, as they found theresults from unsupervised clustering (LDA and k-means clustering) performed poorly when appliedto Twitter data.
In a further Twitter summarisation44tool, TweetMotif, O?Connor et al (2010) use lan-guage modelling to create summaries.
They formtopic clusters by identifying phrases that could de-fine a topic, looking for those phrases in the corpusand merging sets of topics that are similar.
Re-search on microblog summarisation indicates thatwhen summarising comments it is possible but dif-ficult to use unsupervised clustering and severalrules have been suggested that can be followed toproduce the most suitable clusters for summarisa-tion.2.1.4 E-mail SummarisationE-mail and comments are similar in several re-spects: they both exhibit a thread-like structure,containing multiple participant conversations thatoccur along a variable time line, they may referback to previous parts of the conversation and ex-hibit high variability in writing styles (Carenini etal., 2007).
Topic identification is challenging in e-mail threads.
Wan and Mckeown (2004) noted thatseveral different tasks were conducted in emailconversations: decision making, requests for ac-tion, information seeking and social interaction.Rambow et al (2004) found that e-mail has an in-herent structure and that this structure can be useto extract e-mail specific features for summarisa-tion.
This suggests that comments may have aninherent structure which can be used to assist insummarisation.3 Methods3.1 DataThe work reported here is based on commentsfrom news articles taken from the online, UK ver-sion of the Guardian newspaper.
It is composedof online comments that are created by readerswho have registered and posted under a user-name.The site is moderated and comments can be re-moved.
We harvested the comments once the com-ment section is closed and the data is no longer up-dated.
The comment system allows users to viewcomments either in a temporal fashion, oldest ornewest first, or as threads.
Users are then able toadd their own comments to the set by either post-ing directly or by replying to another user, addingtheir comments to any point in the thread.
This de-velops a conversational style of interaction whereusers interact with each other and comment uponthe comments of others.
The topics discussed cantherefore evolve from the topics of the original ar-ticle.In total we have gathered comments posted inresponse to thirteen articles.
Each week a journal-ist from the Guardian summarises the commentson one particular article and we have selected datafrom these weekly summaries to provide a furtherpoint of comparison.
A typical example is ourcomment set 5 where the initial article was titled?Cutting edge: the life of a former London gangleader?, the journalist had divided the commentsinto sets as follows:?
40% criticised gang culture for creating a de-sire for fame and respect?
33% would like to hear more from victims ofgang violence?
17% found Dagrou?s story depressing?
10% believed he should be praised for turn-ing his life aroundAn example of a comment that fit into the jour-nalist based classification scheme is: ?I?d love tosee an in-depth article about a person whose lifeis made a complete misery by gangs.
You know,maybe a middle-aged lady who lives on her ownin a gang area, something like that.
?An example of a comment that does not fit intothe classification scheme: ?So people who don?thave to turn their lives around are ignored andnot supported.
These are the people who aresometimes homeless cause there is no help if youhaven?t been in prison or don?t have kids?.In this work we refer to all of the comments ona single article as a comment set.
There is data thathas been annotated by humans (the gold standardset) and data that has not.
The gold standard dataset contained three comment sets.
It was producedby human(s) assigning all comments from a com-ment set to topic groups.
For one comment set twohumans assigned groups (Set 1) and for two com-ment sets (Sets 2 and 3) a single human assignedgroups.
No guidance was given as to the numberof topics required, but the annotators were askedto make the topics as broad or as inclusive as theycould.In the set where both humans assigned topicsthe first annotator determined that there were 26topics whereas the second annotator identified 45topics.
This difference in topic number was dueto a variation in numbers of clusters with a single45Table 1: Comment Set Composition - A description of the data setSet 1 2 3 4 5 6 7 8 9 10 11 12 13Comments 160 230 181 51 121 169 176 205 254 328 373 397 661Authors 67 140 112 28 65 105 103 111 120 204 240 246 420Threads 54 100 82 21 53 71 67 80 95 132 198 164 319Groups of siblings 126 186 154 45 108 139 148 160 205 256 314 320 553Time Segment 77 113 68 33 72 110 76 117 142 160 124 119 203Over 50 Words (%) 58 52 29 39 37 36 18 38 49 44 26 26 30Mean number of words 80 81 45 58 53 45 38 69 72 61 40 43 48Human topics 14 21 20 - - - - - - - - -Automatic topics - - - 5 5 7 8 5 5 18 18 16 7member.
Once these were removed both annota-tors had created 14 clusters.
The human-humanF-Score was 0.607 including the single clustersand 0.805 without.
It was felt that agreement atthis level meant that double annotation was not re-quired for the futher two sets.
All annotated setshave the clusters with single members removed.A further 10 sets of comments were collectedwhich were not annotated.
Table I shows the com-position of these comment sets.
We can see thatthe number of comments varies and that numberof authors, threads, groups of siblings (commentsthat reply to the same comment) and time seg-ments tend to increase with size.
The number ofwords in a comment does not.
The sets of com-ments, with annotations where available, can befound at (Llewellyn, 2016).3.2 Data ManipulationWe have investigated methods for combining theindividual comments into larger ?documents?
us-ing metadata features.
The data is combined ac-cording to aspects extracted from the metadata;these are as follows:?
STANDARD: Comments are not combinedin any way.
This is a baseline result to whichthe other results can be compared.?
AUTHOR: Comments are grouped togetherif they have the same author.
A common ap-proach to increase the size of Twitter docu-ments is to group tweets together that comefrom a single author on the assumption thatauthors stick to the same/similar topics.
Herethe same approach is tried with comments.?
TIME: Comments are grouped togetherwithin a ten minute segment.
Comments maybe on the same topics if they are posted at thesame time (if the users are viewing commentsthrough the newest first method).It is hypothesised that there may be topical consis-tency within threads.
The ?threadness?
was identi-fied in several ways:?
FULL THREAD: Comments were groupedtogether to reflect the full thread from theoriginal root post and including all repliesto that post and all subsequent posts in thethread.?
CHILDREN: A comment is grouped with alldirect replies to that comment.?
SIBLINGS: A comment is grouped with itssiblings, all other comments that reply to aspecific comment.All of the groups of related comments are com-bined together, according to the method, to form asingle document for each group.3.2.1 Short DocumentsPrevious work indicates that removing short doc-uments from the data sets prior to topic modellingimproves the quality of the topic models (Tanget al, 2014).
We found, in an experiment intowhether length of comments influenced the qual-ity of clusters, that removing comments that con-tain few than 50 terms increases the ability of atopic model to classify documents that are longerthan 50 terms but it does not increase the abilityto classify all documents, especially shorter doc-uments.
If we deem it useful to have short com-ments in the clusters for the ranking and extractionphase of summarisation, then it is important thatthese shorter documents are retained in the model46building stage, we therefore include them in ourexperiments detailed here.3.3 Topic ModellingThe clustering method used in this work is LatentDirichlet Allocation (LDA) topic modelling (Bleiet al, 2003).
It produces a generative model usedto determine the topics contained in a text docu-ment.
A topic is formed from words that oftenco-occur, therefore the words that co-occur morefrequently across multiple documents most likelybelong to the same topic.
It is also true that eachdocument may contain a variety of topics.
LDAprovides a score for each document for each topic.In this case we assign the document to the topic ortopics for which it has the highest score.This approach was implemented using the Mal-let tool-kit (McCallum, 2002).
The Mallet toolkit topic modelling implementation allows dirich-let hyper-parameter re-estimation.
This means thatalthough the hyper parameters are initially set it ispossible to allow them to be re-estimated to bettersuit the data set being modelled.
In these exper-iments, after previous optimisation tests, we ini-tially set the sum of alpha across all topics as 4,beta as 0.08.
We set the number of iterations at1000, and we allow re-estimation of the dirichlethyper-parameters every 10 iterations.In order to cluster the comment data into topicsan appropriate number of topics must be chosen.In choosing the number of topics we aim to picka number which strikes a balance between produc-ing a small number of broad topics or a large num-ber of overly specific topics.
We aim to echo ahuman like decision as to when something is on-or off-topic.
Too few items in each topic is tobe avoided, as is having a small number of topics(O?Connor et al, 2010).In our data set, we choose the number of clus-ters by two methods.
When data has been anno-Table 2: Combined Data, Annotated, F-score (re-sults that beat the standard baseline are in bold)1 2 3Standard Baseline 0.59 0.36 0.33Author 0.43 0.34 0.32Children 0.70 0.41 0.48Full Thread 0.63 0.38 0.37Siblings 0.59 0.37 0.33Time 0.38 0.31 0.24tated by humans the number of topics identified byhumans was chosen as the cluster number.
Whenthe data had not been annotated by humans thecluster number was identified using an automaticmethod of stability analysis.
This method was pro-posed by Greene, O?
Callaghan, and Cunningham(2014), and it assumes that if there is a ?natural?number of topics within a data set, then this num-ber of topics will give the most stable result eachtime the data is re-clustered.
Stability is calculatedusing a ranked list of most predictive topic words.Each time the data is modelled, the change in themembers and ordering of that list is used to cal-culate a stability score.
Green et.
al (2014) usedthe top twenty features to form their ranked list offeatures.
Here, as the length of the documents isshorter, we use the top ten.The sets of documents as described in the pre-vious sections are then used to build topic mod-els and the comments are assigned to topical clus-ters using these models.
Ten-fold cross-validationis used.
As topic modelling is a generative pro-cess, the topics produced are not identical on eachnew run as discussed in more detail in (Koltcov etal., 2014).
Therefore the process is repeated 100times, so that an average score can be supplied.3.4 MetricsThere are two main metrics that are exploited inthis work: Perplexity and micro-averaged F-score.Perplexity is judged by building a model usingtraining data, and then testing with a held out setto see how well the word counts of the test doc-uments are represented by the word distributionsrepresented in the topics in the model (Wallach etal., 2009).
This score shows how perplexed themodel is by the new data.
Perplexity has beenfound to be consistent with other measures of clus-ter quality such as point-wise mutual information(Tang et al, 2014).
PMI data is also available andcan be supplied if requested.It is difficult to judge when a perplexity score is?good enough?
as perplexity will continue to de-crease as the number of clusters increases.
Topicmodels that represent single comments are theleast perplexed.
Therefore a section of the datasethas been hand annotated and this is used to pro-vide a micro-averaged F-score.
This can be usedto gauge if the perplexity scores are equivalent tohuman judgements.
For more details on this met-ric see Sokolova and Lapalme (2009).47Table 3: Combined Data - Perplexity Score (the best / least perplexed model is in bold)Comment Set 1 2 3 4 5 6 7 8 9 10 11 12 13Standard 253 671 520 343 555 444 531 960 1084 818 659 756 810Author 572 644 525 422 555 582 518 1005 1224 908 669 766 761Children 373 608 405 274 427 406 434 673 1019 637 712 514 657Full Thread 707 764 477 394 496 499 567 1026 1490 991 933 753 875Siblings 613 730 560 401 590 532 607 804 1009 734 759 649 813Time 584 715 459 460 579 433 542 796 1090 965 776 720 716.05Here we present scores in terms of micro-averaged F-score (when a gold standard is avail-able for comparison), and by perplexity.
A higherF-score indicates a more human like model anda lower perplexity score indicates a less perplexedmodel.
Significance is tested using a Student?s twotailed t-test and significant results are quoted whenp<0.01 (Field, 2013).4 Results and DiscussionFirst we will discuss the results from the 3 anno-tated data sets (1, 2 and 3).
Using an F-score met-ric we find that, for all three annotated sets thatgrouping comments using the metadata featuresauthor and time does not improve topic cluster-ing.
Grouping comments using thread based fea-tures was more sucessful.
We found that combin-ing comments with their replies (the children) andcombining comments within the full thread setssignificantly beat the standard baseline (Table 2).The results differ when judged by perplexity(Table 3).
We found for two of the comment sets(2 and 3) the children data set gave models thatwere significantly less perplexed than the standardbaseline but this was not the case for comment set1.
For comment set 1 no models beats the baselineusing the perplexity metric.When we look at all of the data, judged usinga perplexity score (Table 3), we found that thecombined children data sets consistently createdmodels (for 10 out of the 13 sets) that are signifi-cantly less perplexed than a standard baseline.
Forone of the datasets the data combined with otherreplies to the same message, the siblings set, beatsthe baseline.
For two of the sets no combinationmethod beats the baseline.The automated results and human results as in-dicated by perplexity and micro-averaged F-scoreare not in complete agreement, but there are somecommonalities.
Both sets of results indicate thatthe group that combines responses with comments(the children group) has the highest agreementwith the human model, and it consistently pro-duces the least perplexed model.5 ConclusionsIt is worth noting that although we focus here onnewspaper comments, the need for summarisationapplies to any web-based chat forum and the find-ings therefore have a wide applicability.LDA topic modelling perfoms better withlonger documents.
Here we have investigatedmethods for combining newspaper comments intolonger documents in order to improve LDA clus-tering and therefore provide a strong basis for sub-sequent comment summarisation.
We found thatcombining comments using features derived fromthe thread structure of the commenting system wasmore successful than features from the commentsmetadata.
We found that using a combination ofa comment and its children provides ?documents?that produce models that can more accurately clas-sify comments into topics than other documentcombination methods.
It is likely that the methodof grouping comments with their direct replies,their children, is the most successful because com-mentors interact with the other comments throughthe thread system (rather than newest or oldestfirst) and they add topically relevent information tothe threads.
It also indicates that topics in threadsevolve, meaning that grouping the entire thread to-gether into a single document works less well thangrouping the immediate decendants - the children.We found that these results were generally con-sistent, but not identical, across two metrics - per-plexity and F-score.
We therefore confirm that theperplexity measure is a useful metric in this do-main when annotated data is not available.48ReferencesAhmet Aker, Emina Kurtic, AR Balamurali, MonicaParamita, Emma Barker, Mark Hepple, and RobGaizauskas.
2016.
A graph-based approach totopic clustering for online comments to news.
InAdvances in Information Retrieval, pages 15?29.Springer.Alexandra Balahur, Mijail Alexandrov Kabadjov, JosefSteinberger, Ralf Steinberger, and Andres Montoyo.2009.
Summarizing opinions in blog threads.
InPACLIC, pages 606?613.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
3:993?1022.Giuseppe Carenini, Raymond T Ng, and XiaodongZhou.
2007.
Summarizing email conversations withclue words.
In Proceedings of the 16th interna-tional conference on World Wide Web, pages 91?100.
ACM.Gabor Cselle, Keno Albrecht, and Rogert Wattenhofer.2007.
BuzzTrack: topic detection and tracking inemail.
In Proceedings of the 12th InternationalConference on Intelligent User Interfaces, IUI ?07,pages 190?197.
ACM.YaJuan DUAN, CHEN ZhuMin WEIF uRu,ZHOU Ming Heung, and Yeung SHUM.
2012.Twitter topic summarization by ranking tweetsusing social influence and content quality.
InProceedings of the 24th International Conferenceon Computational Linguistics, pages 763?780.Andy Field.
2013.
Discovering statistics using IBMSPSS statistics.
Sage.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andMark Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Proceed-ings of the 2000 NAACL-ANLPWorkshop on Auto-matic Summarization - Volume 4, NAACL-ANLP-AutoSum ?00, pages 40?48.
Association for Com-putational Linguistics.Derek Greene and James P Cross.
2015.
Unveil-ing the political agenda of the european parlia-ment plenary: A topical analysis.
arXiv preprintarXiv:1505.07302.Derek Greene, Derek O?Callaghan, and P?adraig Cun-ningham.
2014.
How many topics?
stability anal-ysis for topic models.
In Machine Learning andKnowledge Discovery in Databases, pages 498?513.Springer.Vishal Gupta and Gurpreet Singh Lehal.
2010.
Asurvey of text summarization extractive techniques.2(3).David Inouye and Jugal K Kalita.
2011.
Com-paring twitter summarization algorithms for mul-tiple post summaries.
In Privacy, Security, Riskand Trust (PASSAT) and 2011 IEEE Third Iner-national Conference on Social Computing (Social-Com), 2011 IEEE Third International Conferenceon, pages 298?306.
IEEE.Elham Khabiri, James Caverlee, and Chiao-Fang Hsu.2011.
Summarizing user-contributed comments.
InICWSM.Sergei Koltcov, Olessia Koltsova, and SergeyNikolenko.
2014.
Latent dirichlet alocation:stability and applications to studies of user-generated content.
In Proceedings of the 2014 ACMconference on Web science, pages 161?165.
ACM.Clare Llewellyn, Claire Grover, Jon Oberlander, andEwan Klein.
2014.
Re-using an argument corpusto aid in the curation of social media collections.
InLREC, pages 462?468.Clare Llewellyn.
2016.
Guardian Commentsdata.
http://homepages.inf.ed.ac.uk/s1053147/data/comments_2016.html.
[Online; accessed 09-June-2016].Zongyang Ma, Aixin Sun, Quan Yuan, and Gao Cong.2012.
Topic-driven reader comments summariza-tion.
In Proceedings of the 21st ACM internationalconference on Information and knowledge manage-ment, pages 265?274.
ACM.AK McCallum.
2002.
MALLET: a machine learningfor language toolkit.Shamima Mithun and Leila Kosseim.
2009.
Summa-rizing blog entries versus news texts.
In Proceedingsof the Workshop on Events in Emerging Text Types,pages 1?8.
Association for Computational Linguis-tics.Brendan O?Connor, Michel Krieger, and David Ahn.2010.
Tweetmotif: Exploratory search and topicsummarization for twitter.
In ICWSM.Owen Rambow, Lokesh Shrestha, John Chen, andChirsty Lauridsen.
2004.
Summarizing emailthreads.
In Proceedings of HLT-NAACL 2004: ShortPapers, pages 105?108.
Association for Computa-tional Linguistics.Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Ger-shman, and Robert Frederking.
2011.
Topical clus-tering of tweets.
Proceedings of the ACM SIGIR:SWSM.Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.2010.
Summarizing microblogs automatically.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages685?688.
Association for Computational Linguis-tics.Marina Sokolova and Guy Lapalme.
2009.
A system-atic analysis of performance measures for classifica-tion tasks.
45(4):427?437.49Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, QiaozhuMei, and Ming Zhang.
2014.
Understanding thelimiting factors of topic modeling via posterior con-traction analysis.
In Proceedings of The 31st In-ternational Conference on Machine Learning, pages190?198.Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In Proceedings of the 26th Annual In-ternational Conference on Machine Learning, pages1105?1112.
ACM.Stephen Wan and Kathy McKeown.
2004.
Generatingoverview summaries of ongoing email thread dis-cussions.
In Proceedings of the 20th internationalconference on Computational Linguistics, page 549.Association for Computational Linguistics.50
