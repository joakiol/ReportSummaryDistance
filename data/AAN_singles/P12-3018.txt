Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103?108,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsINPRO_iSS: A Component for Just-In-Time Incremental Speech SynthesisTimo BaumannUniversity of HamburgDepartment for InformaticsGermanybaumann@informatik.uni-hamburg.deDavid SchlangenUniversity of BielefeldFaculty of Linguistics and Literary StudiesGermanydavid.schlangen@uni-bielefeld.deAbstractWe present a component for incrementalspeech synthesis (iSS) and a set of applicationsthat demonstrate its capabilities.
This compo-nent can be used to increase the responsivityand naturalness of spoken interactive systems.While iSS can show its full strength in systemsthat generate output incrementally, we also dis-cuss how even otherwise unchanged systemsmay profit from its capabilities.1 IntroductionCurrent state of the art in speech synthesis for spokendialogue systems (SDSs) is for the synthesis com-ponent to expect full utterances (in textual form) asinput and to deliver an audio stream verbalising thisfull utterance.
At best, timing information is returnedas well so that a control component can determine incase of an interruption / barge-in by the user wherein the utterance this happened (Edlund, 2008; Mat-suyama et al, 2010).We want to argue here that providing capabilitiesto speech synthesis components for dealing with unitssmaller than full utterances can be beneficial for awhole range of interactive speech-based systems.
Inthe easiest case, incremental synthesis simply reducesthe utterance-initial delay before speech output starts,as output already starts when its beginning has beenproduced.
In an otherwise conventional dialogue sys-tem, the synthesis module could make it possibleto interrupt the output speech stream (e. g., when anoise event is detected that makes it likely that theuser will not be able to hear what is being said), andcontinue production when the interruption is over.
Ifother SDS components are adapted more to take ad-vantage of incremental speech synthesis, even moreflexible behaviours can be realised, such as providingutterances in installments (Clark, 1996) that promptfor backchannel signals, which in turn can promptdifferent utterance continuations, or starting an utter-ance before all information required in the utteranceis available (?so, uhm, there are flights to Seoul on uh.
.
.
?
), signaling that the turn is being held.
Another,less conventional type of speech-based system thatcould profit from iSS is ?babelfish-like?
simultaneousspeech-to-speech translation.Research on architectures, higher-level process-ing modules and lower-level processing modules thatwould enable such behaviour is currently underway(Skantze and Schlangen, 2009; Skantze and Hjal-marsson, 2010; Baumann and Schlangen, 2011), buta synthesis component that would unlock the fullpotential of such strategies is so far missing.
In thispaper, we present such a component, which is capa-ble of(a) starting to speak before utterance processing hasfinished;(b) handling edits made to (as-yet unspoken) parts ofthe utterance even while a prefix is already beingspoken;(c) enabling adaptations of delivery parameters suchas speaking rate or pitch;(d) autonomously making appropriate delivery-related decisions;(e) providing information about progress in delivery;and, last but not least,(f) running in real time.Our iSS component is built on top of an exist-ing non-incremental synthesis component, MaryTTS(Schr?der and Trouvain, 2003), and on an existingarchitecture for incremental processing, INPROTK(Baumann and Schlangen, 2012).103After a discussion of related work (Section 2), wedescribe the basic elements of our iSS component(Section 3) and some demonstrator applications thatwe created which showcase certain abilities.12 Related WorkTypically, in current SDSs utterances are gener-ated (either by lookup/template-based generation, or,less commonly, by concept-to-utterance natural lan-guage generation (NLG)) and then synthesised in full(McTear, 2002).
There is very little work on incre-mental synthesis (i.e., one that would work with unitssmaller than full utterances).
Edlund (2008) outlinessome requirements for incremental speech synthe-sis: to give constant feedback to the dialogue systemabout what has been delivered, to be interruptible(and possibly continue from that position), and to runin real time.
Edlund (2008) also presents a prototypethat meets these requirements, but is limited to di-phone synthesis that is performed non-incrementallybefore utterance delivery starts.
We go beyond thisin processing just-in-time, and also enabling changesduring delivery.Skantze and Hjalmarsson (2010) describe a sys-tem that generates utterances incrementally (albeitin a WOz-enviroment), allowing earlier componentsto incrementally produce and revise their hypothesisabout the user?s utterance.
The system can automati-cally play hesitations if by the time it has the turn itdoes not know what to produce yet.
They show thatusers prefer such a system over a non-incrementalone, even though it produced longer dialogues.
Ourapproach is complementary to this work, as it tar-gets a lower layer, the realisation or synthesis layer.Where their system relies on ?regular?
speech syn-thesis which is called on relatively short utterancefragments (and thus pays for the increase in respon-siveness with a reduction in synthesis quality, esp.regarding prosody), we aim to incrementalize thespeech synthesis component itself.Dutoit et al (2011) have presented an incrementalformulation for HMM-based speech synthesis.
How-ever, their system works offline and is fed by non-incrementally produced phoneme target sequences.1The code of the toolkit and its iSS component and the demoapplications discussed below have been released as open-sourceat http://inprotk.sourceforge.net.We aim for a fully incremental speech synthesis com-ponent that can be integrated into dialogue systems.There is some work on incremental NLG (Kilgerand Finkler, 1995; Finkler, 1997; Guhe, 2007); how-ever, that work does not concern itself with the actualsynthesis of speech and hence describes only whatwould generate the input to our component.3 Incremental Speech Synthesis3.1 Background on Speech SynthesisText-to-speech (TTS) synthesis normally proceeds ina top-down fashion, starting on the utterance level(for stress patterns and sentence-level intonation) anddescending to words and phonemes (for pronunci-ation details), in order to make globally optimiseddecisions (Taylor, 2009).
In that way, target phonemesequences annotated with durations and pitch con-tours are generated, in what is called the linguisticpre-processing step.The then following synthesis step proper can beexecuted in one of several ways, with HMM-basedand unit-selection synthesis currently being seen asproducing the perceptually best results (Taylor, 2009).The former works by first turning the target sequenceinto a sequence of HMM states; a global optimiza-tion then computes a stream of vocoding featuresthat optimize both HMM emission probabilities andcontinuity constraints (Tokuda et al, 2000).
Finally,the parameter frames are fed to a vocoder which gen-erates the speech audio signal.
Unit-selection, incontrast, searches for the best sequence of (variablysized) units of speech in a large, annotated corpusof recordings, aiming to find a sequence that closelymatches the target sequence.As mentioned above, Dutoit et al (2011) have pre-sented an online formulation of the optimization stepin HMM-based synthesis.
Beyond this, two other fac-tors influenced our decision to follow the HMM-basedapproach: (a) HMM-based synthesis nicely separatesthe production of vocoding parameter frames fromthe production of the speech audio signal, whichallows for more fine-grained concurrent processing(see next subsection); (b) parameters are partiallyindependent in the vocoding frames, which makesit possible to manipulate e. g. pitch independently(and outside of the HMM framework) without alteringother parameters or deteriorating speech quality.104Figure 1: Hierarchic structure of incremental units describ-ing an example utterance as it is being produced duringutterance delivery.3.2 System ArchitectureOur component works by reducing the aforemen-tioned top-down requirements.
We found that it isnot necessary to work out all details at one levelof processing before starting to process at the nextlower level.
For example, not all words of the utter-ance need to be known to produce the sentence-levelintonation (which itself however is necessary to de-termine pitch contours) as long as a structural outlineof the utterance is available.
Likewise, post-lexicalphonological processes can be computed as longas a local context of one word is available; vocod-ing parameter computation (which must model co-articulation effects) in turn can be satisfied with justone phoneme of context; vocoding itself does notneed any lookahead at all (aside from audio bufferingconsiderations).Thus, our component generates its data structuresincrementally in a top-down-and-left-to-right fashionwith different amounts of pre-planning, using sev-eral processing modules that work concurrently.
Thisresults in a ?triangular?
structure (illustrated in Fig-ure 1) where only the absolutely required minimumhas to be specified at each level, allowing for lateradaptations with few or no recomputations required.As an aside, we observe that our component?s ar-chitecture happens to correspond rather closely toLevelt?s (1989) model of human speech production.Levelt distinguishes several, partially independentprocessing modules (conceptualization, formulation,articulation, see Figure 1) that function incrementallyand ?in a highly automatic, reflex-like way?
(Levelt,1989, p. 2).3.3 Technical Overview of Our SystemAs a basis, we use MaryTTS (Schr?der and Trou-vain, 2003), but we replace Mary?s internal data struc-tures with structures that support incremental spec-ifications; these we take from an extant incremen-tal spoken dialogue system architecture and toolkit,INPROTK (Schlangen et al, 2010; Baumann andSchlangen, 2012).
In this architecture, incrementalprocessing as the processing of incremental units(IUs), which are the smallest ?chunks?
of informationat a specific level (such as words, or phonemes, ascan be seen in Figure 1).
IUs are interconnected toform a network (e. g. words keep links to their asso-ciated phonemes, and vice-versa) which stores thesystem?s complete information state.The iSS component takes an IU sequence ofchunks of words as input (from an NLG component).Crucially, this sequence can then still be modified,through: (a) continuations, which simply link furtherwords to the end of the sequence; or (b) replacements,where elements in the sequence are ?unlinked?
andother elements are spliced in.
Additionally, a chunkcan be marked as open; this has the effect of linkingto a special hesitation word, which is produced onlyif it is not replaced (by the NLG) in time with othermaterial.Technically, the representation levels below thechunk level are generated in our component byMaryTTS?s linguistic preprocessing and convertingthe output to IU structures.
Our component providesfor two modes of operation: Either using MaryTTS?HMM optimization routines which non-incrementallysolve a large matrix operation and subsequently iter-atively optimize the global variance constraint (Todaand Tokuda, 2007).
Or, using the incremental algo-rithm as proposed by Dutoit et al (2011).
In ourimplementation of this algorithm, HMM emissionsare computed with one phoneme of context in bothdirections; Dutoit et al (2011) have found this set-ting to only slightly degrade synthesis quality.
Whilethe former mode incurs some utterance-initial delay,switching between alternatives and prosodic alter-ation can be performed at virtually no lookahead,while requiring just little lookahead for the trulyincremental mode.
The resulting vocoding framesthen are attached to their corresponding phonemeunits.
Phoneme units then contain all the information105Figure 2: Example application that showcases just-in-timemanipulation of prosodic aspects (tempo and pitch) of theongoing utterance.needed for the final vocoding step, in an accessibleform, which makes possible various manipulationsbefore the final synthesis step.The lowest level module of our component is whatmay be called a crawling vocoder, which activelymoves along the phoneme IU layer, querying eachphoneme for its parameter frames one-by-one andproducing the corresponding audio via vocoding.
Thevocoding algorithm is entirely incremental, makingit possible to vocode ?just-in-time?
: only when audiois needed to keep the sound card buffer full does thevocoder query for a next parameter frame.
This iswhat gives the higher levels the maximal amount oftime for re-planning, i. e., to be incremental.3.4 Quality of ResultsAs these descriptions should have made clear, thereare some elements in the processing steps in our iSScomponent that aren?t yet fully incremental, such asassigning a sentence-level prosody.
The best resultsare thus achieved if a full utterance is presented to thecomponent initially, which is used for computation ofprosody, and of which then elements may be changed(e. g., adjectives are replaced by different ones) on thefly.
It is unavoidable, though, that there can be some?breaks?
at the seams where elements are replaced.Moreover, the way feature frames can be modified(as described below) and the incremental HMM op-timization method may lead to deviations from theglobal optimum.
Finally, our system still relies onMary?s non-incremental HMM state selection tech-nique which uses decision trees with non-incrementalfeatures.However, preliminary evaluation of the compo-nent?s prosody given varying amounts of lookaheadindicate that degradations are reasonably small.
Also,the benefits in naturalness of behaviour enabled byiSS may outweigh the drawback in prosodic quality.4 Interface DemonstrationsWe will describe the features of iSS, their implemen-tation, their programming interface, and correspond-ing demo applications in the following subsections.4.1 Low-Latency Changes to ProsodyPitch and tempo can be adapted on the phonemeIU layer (see Figure 1).
Figure 2 shows a demo in-terface to this functionality.
Pitch is determined bya single parameter in the vocoding frames and canbe adapted independently of other parameters in theHMM approach.
We have implemented capabilities ofadjusting all pitch values in a phoneme by an offset,or to change the values gradually for all frames inthe phoneme.
(The first feature is show-cased in theapplication in Figure 2, the latter is used to cancelutterance-final pitch changes when a continuation isappended to an ongoing utterance.)
Tempo can beadapted by changing the phoneme units?
durationswhich will then repeat (or skip) parameter frames(for lengthened or shortened phonemes, respectively)when passing them to the crawling vocoder.
Adapta-tions are conducted with virtually no lookahead, thatis, they can be executed even on a phoneme that iscurrently being output.4.2 Feedback on DeliveryWe implemented a fine-grained, hierarchical mech-anism to give detailed feedback on delivery.
A newprogress field on IUs marks whether the IU?s produc-tion is upcoming, ongoing, or completed.
Listenersmay subscribe to be notified about such progresschanges using an update interface on IUs.
The appli-cations in Figures 2 and 4 make use of this interfaceto mark the words of the utterance in bold for com-pleted, and in italic for ongoing words (incidentally,the screenshot in Figure 4 was taken exactly at theboundary between ?delete?
and ?the?
).4.3 Low-Latency Switching of AlternativesA major goal of iSS is to change what is being saidwhile the utterance is ongoing.
Forward-pointingsame-level links (SLLs, (Schlangen and Skantze,2009; Baumann and Schlangen, 2012)) as shownin Figure 3 allow to construct alternative utterancepaths beforehand.
Deciding on the actual utterancecontinuation is a simple re-ranking of the forward106Figure 3: Incremental units chained together via forward-pointing same-level links to form an utterance tree.Figure 4: Example application to showcase just-in-timeselection between different paths in a complex utterance.SLLs which can be changed until immediately beforethe word (or phoneme) in question is being uttered.The demo application shown in Figure 4 allows theuser to select the path through a fairly complex utter-ance tree.
The user has already decided on the color,but not on the type of piece to be deleted and hencethe currently selected plan is to play a hesitation (seebelow).4.4 Extension of the Ongoing UtteranceIn the previous subsection we have shown how alter-natives in utterances can be selected with very lowlatency.
Adding continuations (or alternatives) toan ongoing utterance incurs some delay (some hun-dred milliseconds), as we ensure that an appropriatesentence-level prosody for the alternative (or con-tinuation) is produced by re-running the linguisticpre-processing on the complete utterance; we thenintegrate only the new, changed parts into the IUstructure (or, if there still is time, parts just before thechange, to account for co-articulation).Thus, practical applications which use incremen-tal NLG must generate their next steps with somelookahead to avoid stalling the output.
However, ut-terances can be marked as non-final, which results ina special hesitation word being inserted, as explainedbelow.4.5 Autonomously Performing DisfluenciesIn a multi-threaded, real-time system, the crawlingvocoder may reach the end of synthesis before theNLG component (in its own thread) has been ableto add a continuation to the ongoing utterance.
Toavoid this case, special hesitation words can be in-serted at the end of a yet unfinished utterance.
If thecrawling vocoder nears such a word, a hesitation willbe played, unless a continuation is available.
In thatcase, the hesitation is skipped (or aborted if currentlyongoing).24.6 Type-to-SpeechA final demo application show-cases truly incremen-tal HMM synthesis taken to its most extreme: A textinput window is presented, and each word that istyped is treated as a single-word chunk which is im-mediately sent to the incremental synthesizer.
(Forthis demonstration, synthesis is slowed to half theregular speed, to account for slow typing speeds andto highlight the prosodic improvements when moreright context becomes available to iSS.)
A use casewith a similar (but probably lower) level of incre-mentality could be simultaneous speech-to-speechtranslation, or type-to-speech for people with speechdisabilities.5 ConclusionsWe have presented a component for incrementalspeech synthesis (iSS) and demonstrated its capa-bilities with a number of example applications.
Thiscomponent can be used to increase the responsivityand naturalness of spoken interactive systems.
WhileiSS can show its full strengths in systems that alsogenerate output incrementally (a strategy which iscurrently seeing some renewed attention), we dis-cussed how even otherwise unchanged systems mayprofit from its capabilities, e. g., in the presence ofintermittent noise.
We provide this component in thehope that it will help spur research on incrementalnatural language generation and more interactive spo-ken dialogue systems, which so far had to made dowith inadequate ways of realising its output.2Thus, in contrast to (Skantze and Hjalmarsson, 2010), hesi-tations do not take up any additional time.107ReferencesTimo Baumann and David Schlangen.
2011.
Predictingthe Micro-Timing of User Input for an Incremental Spo-ken Dialogue System that Completes a User?s OngoingTurn.
In Proceedings of SigDial 2011, pages 120?129,Portland, USA, June.Timo Baumann and David Schlangen.
2012.
TheINPROTK 2012 release.
In Proceedings of SDCTD.to appear.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press.Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nico-las d?Alessandro, and Benjamin Picart.
2011. pHTSfor Max/MSP: A Streaming Architecture for StatisticalParametric Speech Synthesis.
Technical Report 1, nu-mediart Research Program on Digital Art Technologies,March.Jens Edlund.
2008.
Incremental speech synthesis.
InSecond Swedish Language Technology Conference,pages 53?54, Stockholm, Sweden, November.
SystemDemonstration.Wolfgang Finkler.
1997.
Automatische Selbstkorrek-tur bei der inkrementellen Generierung gesprochenerSprache unter Realzeitbedingungen.
Dissertationen zurK?nstlichen Intelligenz.
infix Verlag.Markus Guhe.
2007.
Incremental Conceptualization forLanguage Production.
Lawrence Erlbaum Asso., Inc.,Mahwah, USA.Anne Kilger and Wolfgang Finkler.
1995.
Incremen-tal Generation for Real-time Applications.
TechnicalReport RR-95-11, DFKI, Saarbr?cken, Germany.William J.M.
Levelt.
1989.
Speaking: From Intention toArticulation.
MIT Press.Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.2010.
Analyzing User Utterances in Barge-in-able Spo-ken Dialogue System for Improving Identification Ac-curacy.
In Proceedings of Interspeech, pages 3050?3053, Makuhari, Japan, September.Michael McTear.
2002.
Spoken Dialogue Technology.Toward the Conversational User-Interface.
Springer,London, UK.David Schlangen and Gabriel Skantze.
2009.
A General,Abstract Model of Incremental Dialogue Processing.In Proceedings of the EACL, Athens, Greece.David Schlangen, Timo Baumann, Hendrik Buschmeier,Okko Bu?, Stefan Kopp, Gabriel Skantze, and RaminYaghoubzadeh.
2010.
Middleware for IncrementalProcessing in Conversational Agents.
In Proceedings ofSigDial 2010, pages 51?54, Tokyo, Japan, September.Marc Schr?der and J?rgen Trouvain.
2003.
The GermanText-to-Speech Synthesis System MARY: A Tool forResearch, Development and Teaching.
InternationalJournal of Speech Technology, 6(3):365?377, October.Gabriel Skantze and Anna Hjalmarsson.
2010.
Towardsincremental speech generation in dialogue systems.
InProceedings of SigDial 2010, pages 1?8, Tokyo, Japan,September.Gabriel Skantze and David Schlangen.
2009.
Incrementaldialogue processing in a micro-domain.
In Proceedingsof EACL 2009, Athens, Greece, April.Paul Taylor.
2009.
Text-to-Speech Synthesis.
CambridgeUniv Press, Cambridge, UK.Tomoki Toda and Keiichi Tokuda.
2007.
A Speech Pa-rameter Generation Algorithm Considering Global Vari-ance for HMM-based Speech Synthesis.
IEICE Trans-actions on Information and Systems, 90(5):816?824.Keiichi Tokuda, Takayoshi Yoshimura, Takashi Ma-suko, Takao Kobayashi, and Tadashi Kitamura.
2000.Speech Parameter Generation Algorithms for HMM-based Speech Synthesis.
In Proceedings of ICASSP2000, pages 1315?1318, Istanbul, Turkey.108
