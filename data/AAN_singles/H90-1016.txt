Toward a Real-TimeSpoken Language SystemUsing Commercial HardwareSteve Austin, Pat Peterson, Paul Placeway, Richard Schwartz, Jeff VandergriftBBN Systems and Technologies Inc.10 Moulton St.Cambridge, MA, 02138AbstractWe describe the methods and hardware that we are usingto produce a real-time demonstration f an integrated Spo-ken Language System.
We describe algorithms that greatlyreduce the computation needed to compute the N-Best sen-tence hypotheses.
To avoid grammar coverage problems weuse a fully-connected first-order statistical class grammar.The speech-search algorithm is implemented ona board witha single Intel i860 chip, which provides afactor of 5 speedupover a SUN 4 for straight C code.
The board plugs directlyinto the VME bus of the SUN4, which controls the systemand contains the natural anguage system and applicationback end.1.
IntroductionOne goal of the Spoken Language System (SLS) projectis to demonstrate a real-time interactive system that inte-grates speech recognition and natural language processing.We believe that, currently, the most practical and efficientway to integrate speech recognition with natural anguageis using the N-Best paradigm, in which we find the mostlikely whole-sentence hypotheses using speech recognitionand a simple language model, and then filter and reorderthese hypotheses with natural language.
Although we claimthat this process is efficient, finding the N-Best sentencesstill requires ignificant amounts of computation.To accelerate he speech recognition process, several siteshave been developing special-purpose hardware, from fully-custom VLSI \[5\] to custom boards based on general-purposeprocessors operating in parallel \[1\].
However, with the rapidchanges in commercial hardware in the past few years wefelt that if we could achieve real time with commerciallyavailable hardware we would realize a number of advan-tages, such as decreased development time, increased relia-bility, and better hardware and software support.
We wouldnot have to divert attention from speech-recognition researchand our results wouM be more easily shared with the commu-nity.
We felt that these hopes were not unrealistic because,for our algorithms in particular, which are research-orientedand seldom have a stable, regular StlUCture, the potentialgains from custom hardware wouM not be that great.Our strategy in developing a real-time spoken languagesystem has been to find appropriate commercial hardwareand then tailor our algorithms to the hardware and applica-tion.
In this context, we are trying to approach the followinggoals:1. real-time processing with a short delay2.
reasonable-cost, commercially-available hardware3.
source code compatibility with our research programs4.
computation of the N-Best sentences for large N5.
use of a robust fully-connected statistical grammar6.
practical memory requirements7.
negligible loss of accuracy8.
ability to handle large vocabularyTwo of the new algorithms used in this effort are describedin a separate paper \[8\].
Specifically, the Word-Dependent N-Best search and the Forward-Backward Search.
In Section2 we describe the class of hardware that we have chosen forthis task.
Section 3 reviews the reasons for using a statisti-cal grammar and presents a new more efficient algorithm fortime-synchronous decoding with a statistical grammar.
InSection 4 we compare the accuracy of the standard Viterbialgorithm with the time-synchronous forward search algo-rithm that we use.
And in Section 5 we give the lateststatus of the speed and accuracy of our system.2.
HardwareIt is already quite straightforward to perform signal pro-cessing in real-time on current boards with signal processorchips.
However, the speech recognition search requires alarge amount of computation together with several MB offast readily accessible memory.
In the past there have notbeen commercially available boards or inexpensive comput-ers that meet these needs.
However this is changing.
TheMotorola 88000 and Intel 860 chips are being put on boardswith substantial mounts of random access memory.
Mostchips now come with C compilers, which means that thebulk of development programs can be transfered directly.
Ifneeded, computationally intensive inner loops can be handcoded.72After considering several choices we have chosen boardsbased on the Intel 860 processor.
The Intel 860 processorcombines a RISC core and a floating-point processor witha peak speed of 80 MFLOPS.
Currently, we have lookedat VME boards made by Sky Computer (the SkyBolt) andMercury (the MC860).
The SkyBolt currently is availablewith 4 MB of static RAM.
It will very shortly be availablewith 16 MB of DRAM and 256 KB of static RAM cache.The Mercury MC860 is currently available with 16 MB ofDRAM.
Most C programs that we have run on both of thesemachines mn about five times faster than on a SUN 4/280.Figure 1 illustrates the hardware configuration that wehave built.
The host will be a SUN 4/330.
The microphoneis connected to an external preamp and A/D converter whichconnects directly to the serial port of the Sky Challenger.The Sky Challenger with dual TMS320C30s will be used forsignal processing and vector quantization (VQ).
The SkyBoltwill be used for the speech recognition N-Best search.
Theboards communicate with the host and each other through theVME bus, making high speed data transfers easy.
Howevercurrently the data transfer ate between the boards is verylow.
The SUN 4 will conlrol the overall system and willalso contain the natural language understanding system andthe application back end.SkyBolt 860Sky Dual C30Control, \[Application,NL Understandingf N-Best answeNForward.Backward Decoder If VQ indlceaI An~ysI* IFigure 1: Real-Time Hardware Configuration.
The SkyChallenger Dual C30 board and the Intel 860 board plugdirectly into the VME bus of the SUN 4.We use all three processors during most of the computa-tion.
When speech as started the C30 board will computethe signal processing and VQ in real-time.
The SUN 4will accumulate he speech for possible long-term storage orplayback.
Meanwhile, the Intel 860 will compute the for-ward pass of the forward-backward search.
When the endof the utterance has been detected, the SUN will give the1-Best answer to the natural language understanding systemfor parsing and interpretation.
Meanwhile the Intel 860 willsearch backwards for the remainder of the N Best sentencehypotheses.
These should be completed in about he sametime that the NL system requires to parse the first answer.Then, the NL system can parse down the list of alternativesentences until an acceptable sentence is found.Currently, the computation required for parsing each sen-tencce hypothesis i about 1/2 second.
The delay for the N-Best search is about half the duration of the sentence.
This isexpected to decrease with further algorithm improvements.2.
Time-Synchronous Statistical Lan-guage Model SearchWe know that any language model that severely limits whatsentences are legal cannot be used in a real SLS becausepeople will almost always violate the constraints of the lan-guage model.
Thus, a Word-Pair type language model willhave a fixed high error rate.
The group at IBM has longbeen an advocate of statistical language models that can re-duce the entropy or perplexity of the language while stillallowing all possible word sequences with some probability.For most SLS domains where there is not a large amount oftraining data available, it is most practical to use a statisti-cal model of word classes rather than individual words.
Wehave circulated a so called Class Grammar for the ResourceManagement Domain \[3\].
The language model was simplyconstructed, having only first-order statistics and not distin-guishing the probability of different words within a class.The measured test set perplexity of this language model isabout 100.
While more powerful "fair" models could beconstructed, we felt that this model would predict he diffi-culty of a somewhat larger task domain.
The word error rateis typically twice that of the Word-Pair (WP) grammar.
Oneproblem with this type of grammar is that the computationis quite a bit larger than for the WP grammar, since all 1000words can follow each word (rather than an average of 60as in the WP grammar).During our work on statistical grammars in 1987 \[6\], wedeveloped a technique that would greatly reduce the compu-tational cost for a time-synchronous search with a statisticalgrarnmar I .
Figure 2 illustrates a fully-connected first-orderstatistical grammar.
If the number of classes is C, then thenumber of null-arcs connecting the nodes is C 2.
However,since the language models are rarely well-estimated, mostof the class pairs are never observed in the gaining data.Therefore, most of these null-arc transition probabilities areestimated indirectly.
Two simple techniques that are com-monly used are padding, or interpolating with a lower ordermodel.
In padding we assume that we have seen every pairof words or classes once before we start training.
Thus weestimate p(c2lel) asN(ct, c2) + 1p(e2lel) - N(el) + 6'1We should note that we have heard that this algorithm was indepen-dently arrived at by Andres Santos from the University of Madrid while onsabbatical tSRI in 1989.72Figure 2: Fully Connected First-Order Statistical Grammar.Requires U 2 null arcs.In interpolation we average the first-order probability withthe zeroth-order p obability with a weight that depends onthe n.mher of occurrences of the first class.~e2lcl) = ~(cx)f~e21el) + \[1 - -  ~(cl)\]p(c2 )whereand~c21c l )  - - -N(cl,e2)IV (c1)N(c2)f~c2) = N(all words)In either case, when the pair of classes has never occurred,the probability can be represented much more simply.
Forthe latter case of interpolated models, when N(el, c2) = 0the expression simplifies to just\[1 - 3~(cl)\]/~(c2)The first term, 1 - A(el), depends only on the first class,while the second term, ~e2), depends only on the secondclass.
We can represent all of these probabilities by addinga zero-order state to the language model.
Figure 3 illustratesthis model.
From each class node we have a null transition tothe zero-order state with a probability given by the first term.Then, from the zero-order state to each of the following classnodes we have the zero-order probability of that class.Now that the probabilities for all of the estimated transi-tions has been taken care of we only need the null transitionsthat have probabilities estimated from actual occurrences ofthe pairs of classes, as shown in Figure 4.
Assuming that,on average, there are B different classes that were observedto follow each class, where B << C, the total number oftransitions is only C(B + 2).
For the 100-class grammarwe find that B = 14.8, so we have 1680 transitions insteadof 10,000.
This savings reduces both the computation andstorage associated with using a statistical grammar.,.
.
.
.Figure 3: Zero-state within first-order statistical grammar.All of the transitions estimated from no data are modeled bytransitions to and from the zero-state.It should be clear that this technique can easily be ex-tended to a higher order language model.
The unobservedsecond-order t ansitions would be removed and replacedwith transitions to a general first-order state for each wordor class.
From these we then have first-order probabilitiesto each of the following words or classes.
As we increasethe order of the language model, the percentage of transi-tions that are estimated only from lower order occurrencesis expected to increase.
Thus, the relative savings by usingthis algorithm will increase.3.
T ime-synchronous  Forward  Search  vsV i terb iThe search algorithm that is most commonly used is theViterbi algorithm.
This algorithm has nice properties in thatit can proceed in real time in a time-synchronous manner,is quite amenable to the beam-search pruning algorithm \[4\],and is also relatively easy to implement on a parallel pro-cessor.
Another advantage is that it only requires comparesand adds (ff we use log probabilities).
Unfortunately, theViterbi algorithm finds the most likely sequence of statesrather than the most likely sequence of words.To correctly compute the probability of any particular se-quence of words requires that we add the probabilities of allpossible state sequences for those words.
This can be donewith the "forward pass" of the forward-backward trainingalgorithm.
The only difference between the Viterbi scoringand the Forward-pass computation is that we add the prob-abilities of different heories coming to a state rather thantaking the maximum.We presented a search algorithm in 1985 \[7\] that em-bodied most of this effect.
Basically, within words we addprobabilities, while between words we take the maximum.
Itwas not proven at that time how much better, if any, this al-gorithm was than the simpler Viterbi algorithm, and whetherit was as good as the strictly correct algorithm that computes741- ~.~CI )~/~ ?/ 71 \ \ \ \ f.... a ? "
" ' ~ l  I.... ""'"t ~'~" c 5- - - - , .
,  kxx.
.___ , ,  ~ ~, -- - -7.
~- -  ~yFigure 4: Sparsely Connected First-Order Statistical Gram-mar with zero-state requires many fewer null arcs.the score of each hypothesis ndependently.When we compared these two algorithms under severalconditions, we found that there was a consistent advantagefor adding the probabilities within the word.
For example,when we use the class grammar, we find that the word errorrate decreases from 8% to 6%.To be sure that the time-synchronous forward search givesus the same performance as the ideal forward score is some-what more complicated.
We must guarantee that we havefound the highest scoring sentence with the true forwardprobability score.
One way to find this is to use the exactN-Best algorithm \[2\].
Since the exact N-Best algorithm sep-arates the computation for any two different hypotheses, thescores that result are, in fact, the correct forward probabili-ties, as long as we set N to a large enough value.
A second,much simpler way to verify the time-synchronous algorithmis to see if it ever gets a wrong answer that scores worsethan the correct answer.
We ran a test in which all incorrectanswers were rescored individually using the forward proba-bility.
We compared these scores to the forward probabilityfor the correct answer.
In no case (out of 300 sentences) didthe time-synchronous forward search ever produce a wronganswer that, in fact, scored worse than the correct answer.The reason that this whole discussion about the Viterbialgorithm is relevant here is that the Viterbi algorithm isfaster than the forward search.
Therefore, we use the inte-ger Viterbi algorithm in the forward-pass of the Forward-Backward Search.
Since the function of the forward-pass iprimarily to say which words are likely, it is not essentialthat we get the best possible answer.
The backward N-Bestsearch is then done using the better-performing al orithmthat adds different state-sequence probabilities for the sameword sequence.4.
Speed and AccuracyWhen we started this effort in January, 1990, our unop-timized time-synchronous forward search algorithm tookabout 30 times real time for recognition with the WP gram-mar and a beamwidth set to avoid pruning errors.
The classgrammar equired 10 times more computation.
The exactN-Best algorithm required about 3,000 times real time tofind the best 20 answers.
When we required the best 100answers, the program required about 10,000 times real time.Since January we have implemented several algoritiams, op-timized the code, and used the Intel 860 board to speed upthe processing.
The N-Best pass now runs in about 1/2 realtime.
Below we give each of these methods along with thefactor of speed gained.Statistical grammar algorithm 5Word-Dependent N-Best 5Forward-Backward Search 40Code Optimization 4Intel 860 Board 5Total reduction in computation 20,000As can be seen, the three algorithmic hanges accountedfor a factor of 1,000, while the code optimization and fasterprocessor accounted for a factor of 20.
We expect any ad-ditional large factors in speed to come from algorithmicchanges.
When the VLSI HMM processor becomes avail-able, the speed of the HMM part of the problem will increaseconsiderably, and the bottleneck will be in the languagemodel processor.
We estimate that the language model com-putation accounts for about one third of the total computa-tion.Our current plan is to increase the speed as necessary andcomplete the integration with the natural anguage under-standing and application backend by September, 1990.AccuracyIt is relatively easy to achieve real time if we relax ourgoals for accuracy.
For example, we could simply reducethe pruning beamwidth in the beam search and we knowthat the program speeds up tremendously.
However, if wereduce the beamwidth too much, we begin to incur searcherrors.
That is, the answer that we find is not, in fact, thehighest scoring answer.
There are also several algorithmsthat we could use that require less computation but increasethe error rate.
While some tradeoffs are reasonable, it isimportant that any discussion of real-time computation beaccompanied by a statement of the accuracy relative to thebest possible conditions.In Table I below we show the recognition accuracy resultsunder several different conditions.
All results use speaker-dependent models and are tested on the 300 sentences in theJune '88 test set.
For each condition we state whether theforward pass would mn in less than real time on the SkyBoltfor more than 80% of the sentences - -  which is basicallya function of the pruning beamwidth.
The backward passcurrently runs in less than 1/2 real time, and we expect itwill get faster.
We don't yet have a good feeling for howmuch delay will be tolerable, but our goal is for the delay incomputing the N Best sentences to be shorter than the timeneeded for natural anguage to process the first sentence, orabout 1/2 second.
The accuracy runs were done on the SUN4/280.
Based on our speed measurements, we assume thatanything that runs in under five times real-time on the SUN4 will run in real-time on the Intel 860 board.
For a similarCG condition whose forward pass ran in under five timesreal-time on the SUN 4, we verified real-time operation ona 4 MB SkyBolt.Grammar RT?
Word 1 20 100Err Best Best BestWP-XW N 1.9WP N 3.9 19.7 2.3 2.0WP Y 3.9 20.0 2.7 2.7CG-XW N 4.7CG N 8.2 38.7 7.0 4.0CG 1.2 8.5 39.3 8.7 5.7CG Y 9.1 40.0 11.7 9.3Table 1: Word and sentence rror rates for the real-time N-Best algorithm compared with the best non-real-lime condi-tions.For each condition we give the word error, I-Best sen-tence error, and N-Best sentence rror for N of 20 and100.
"N-Best sentence rror" Results are given for theWord-Pair (WP) grammar and for the Class (CG) Gram-mar.
The conditions WP-XW and CG-XW were done us-ing cross-word triphone models that span across words andhave been smoothed with the triphone cooccurence smooth-ing.
These conditions were only decoded with the 1-Bestforward-search algorithm, and so produced only word er-ror statistics for reference.
The models that do not usecross-word triphones also do not use triphone cooccurencesmoothing.
Since the forward pass is done using the Viterbialgorithm, this affects the word error rate and the 1-Bestsentence rror rate, which are measured from the forwardpass only.Currently we have not run the cross-word models withthe N-Best algorithm.
These models require more memorythan is available on the board, and the computation requiredin the forward pass is too large.
We intend to solve this byusing the cross-word models only in the backward irection.Another alternative would be to use the cross-word modelsto rescore all of the N-Best hypotheses, which could bedone relatively quickly.
In any case, we decided to makethe system work with cross-word models only after we hadachieved real time with simpler non-cross-word models.As we can see, the results using the WP grammar arequite good.
Even without the cross-word models, we findthe correct sentence 97.6% of the time within the first 20choices and 98% of the time within the first 100 choices.When we use a beamwidth that gives us real time, we seeonly a very slight degradation i accuracy.
However, as westated earlier in this paper, the WP grammar is unrealisticallyeasy, both in terms of recognition accuracy and computation.We show these results only for comparison with other real-time recognition results on the RM corpus.Recognition with the class grammar is much harder due tohigher perplexity and the fact that all words are possible atany time.
The word error with cross-word models is 4.7%.For the N-Best conditions with the CG grammar we note alarger difference between the sentence rrors at 20 and 100choices.
In contrast to the WP grammar in which there area limited number of possibilities that can match well, heremore sequences are plausible.
We give the N-Best resultsfor three different speed conditions.
The first has a veryconservative beamwidth.
The second runs at 1.2 times real-time, and the third runs faster than real time.
We can seethat there is a significant degradation due to pruning errorswhen we force the system to run in real time.There are several approaches that are available to speedup the forward pass considerably.
Since the forward passis used for pruning, it is not essential that we achieve thehighest accuracy.
In those rare cases where the N-Best findsa different op choice sentence than the forward pass, andthis new top choice also is accepted by natural language,we will simply have a delay equal to the time taken for theN-Best backward search.
The most promising method forspeeding up the forward search is to use a phonetic tree inwhich the common word beginnings are shared.
Since mostof the words are pruned out after one or two phonemes,much of the computation is eliminated.ConclusionWe have achieved real-time recognition of the N-Best sen-tences on a commercially available board.
When we use aWP grammar, there is no loss in accuracy due to real-timelimitations.
However, currently, when using a class gram-mar there is a degradation.
We expect his degradation tobe reduced as planned algorithm impruverr~mts are imple-mented.Most of the increase in speed came from algorithm modi-fications rather than from fast hardware or low-level codingenhancements, although the latter improvements were sub-stantial and necessary.
All the code is written in C so thereis no machine dependence.
All told we sped up the N-Bestcomputations by a factor of 20,000 with a combination ofalgorithms, code optimization, and faster hardware.AcknowledgementThis work was supported by the Defense Advanced ResearchProjects Agency and monitored by the Office of Naval Re-search under Contract No.
N00014-89-C-0008.References\[1\] Bisiani, R., "Plans for PLUS hardware".
Proceedings76of the DARPA Speech and Natural Language WorkshopCape Cod, October 1989 (1989).
[2] Chow, Y-L. and Schwartz, R.M., "The N-Best Algo-rithm: An Efficient Procedure for Finding Top N Sen-tence Hypotheses".
Procee&'ngs of the DARPA Speechand Natural Language Workshop Cape Cod, October1989.
[3] Derr, A., and Schwartz, R.M., "A Simple StatisticalClass Grammar for Measuring Speech Recognition Per-formance".
Proceedings ofthe DARPA Speech and Nat-ural Language Workshop Cape Cod, October 1989.
[4] Lowerre, B., "The Harpy Speech Recognition System",Doctoral Thesis CMU 1977.
[5] Murveit, H., "Plans for VLSI I-IMM Accelerator".
Pro-ceedings of the DARPA Speech and Natural LanguageWorkshop Cape Cod, October 1989.
[6] Rohlicek, J.A., Chow, Y-L., and ROUCOS, S., "Statis-tical Language Modeling Using a Slna|l Corpus froman Application Domain".
Proceedings of the DARPASpeech and Natural Language Workshop Cambridge,October 1987.
Also in Proceedings of the ICASSP 88,pp.
267-270, April, 1988.
[7] Schwartz, R.M., Chow, Y., Kimball, O., Roucos, S.,Krasner, M., and Makhoul, J. Context-Dependent Mod-eling for Acoustic-Phonetic Recognition of Continu-ous Speech".
Proceedings of the ICASSP 85, pp.
1205-1208, March, 1985.
[8] Schwartz, R.M., and Austin, S.A., "Efficient, High-Performance Algorithms for N-Best Search".
Proceed-ings of the DARPA Speech and Natural Language Work-shop Hidden Valley, June 1990.77
