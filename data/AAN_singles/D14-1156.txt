Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474?1480,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsLeveraging Effective Query Modeling Techniquesfor Speech Recognition and SummarizationKuan-Yu Chen*?,  Shih-Hung Liu*, Berlin Chen#, Ea-Ee Jan+,Hsin-Min Wang*, Wen-Lian Hsu*, and Hsin-Hsi Chen?
*Institute of Information Science, Academia Sinica, Taiwan?National Taiwan University, Taiwan#National Taiwan Normal University, Taiwan+IBM Thomas J. Watson Research Center, USA{kychen, journey, whm, hsu}@iis.sinica.edu.tw,berlin@ntnu.edu.tw, hhchen@csie.ntu.edu.tw, ejan@us.ibm.comAbstractStatistical language modeling (LM) thatpurports to quantify the acceptability of agiven piece of text has long been an in-teresting yet challenging research area.
Inparticular, language modeling for infor-mation retrieval (IR) has enjoyed re-markable empirical success; one emerg-ing stream of the LM approach for IR isto employ the pseudo-relevance feedbackprocess to enhance the representation ofan input query so as to improve retrievaleffectiveness.
This paper presents a con-tinuation of such a general line of re-search and the main contribution is three-fold.
First, we propose a principledframework which can unify the relation-ships among several widely-used querymodeling formulations.
Second, on top ofthe successfully developed framework,we propose an extended query modelingformulation by incorporating critical que-ry-specific information cues to guide themodel estimation.
Third, we further adoptand formalize such a framework to thespeech recognition and summarizationtasks.
A series of empirical experimentsreveal the feasibility of such an LMframework and the performance merits ofthe deduced models on these two tasks.1 IntroductionAlong with the rapidly growing popularity of theInternet and the ubiquity of social web commu-nications, tremendous volumes of multimediacontents, such as broadcast radio and televisionprograms, digital libraries and so on, are madeavailable to the public.
Research on multimediacontent understanding and organization has wit-nessed a booming interest over the past decade.By virtue of the developed techniques, a varietyof functionalities were created to help distill im-portant content from multimedia collections, orprovide locations of important speech segmentsin a video accompanied with their correspondingtranscripts, for users to listen to or to digest.
Sta-tistical language modeling (LM) (Jelinek, 1999;Jurafsky and Martin, 2008; Zhai, 2008), whichmanages to quantify the acceptability of a givenword sequence in a natural language or capturethe statistical characteristics of a given piece oftext, has been proved to offer both efficient andeffective modeling abilities in many practicalapplications of natural language processing andspeech recognition (Ponte and Croft, 1998; Jelin-ek, 1999; Huang, et al., 2001; Zhai and Lafferty,2001a; Jurafsky and Martin, 2008; Furui et al.,2012; Liu and Hakkani-Tur, 2011).The LM approach was first introduced for theinformation retrieval (IR) problems in the late1990s, indicating very good potential, and wassubsequently extended in a wide array of follow-up studies.
One typical realization of the LM ap-proach for IR is to access the degree of relevancebetween a query and a document by computingthe likelihood of the query generated by the doc-ument (usually referred to as the query-likelihood approach) (Zhai, 2008; Baeza-Yatesand Ribeiro-Neto, 2011).
A document is deemedto be relevant to a given query if the correspond-ing document model is more likely to generatethe query.
On the other hand, the Kullback-Leibler divergence measure (denoted by KLMfor short hereafter), which quantifies the degreeof relevance between a document and a queryfrom a more rigorous information-theoretic per-spective, has been proposed (Lafferty and Zhai,2001; Zhai and Lafferty, 2001b; Baeza-Yates andRibeiro-Neto, 2011).
KLM not only can bethought as a natural generalization of the query-likelihood approach, but also has the additionalmerit of being able to accommodate extra infor-mation cues to improve the performance of doc-ument ranking.
For example, a main challengefacing such a measure is that since a given queryusually consists of few words, the true infor-mation need is hard to be inferred from the sur-face statistics of a query.
As such, one emergingstream of thought for KLM is to employ the1474pseudo-relevance feedback process to constructan enhanced query model (or representation) soas to achieve better retrieval effectiveness (Hi-emstra et al., 2004; Lv and Zhai, 2009; Carpinetoand Romano, 2012; Lee and Croft, 2013).Following this line of research, the major con-tribution of this paper is three-fold: 1) we ana-lyze several widely-used query models and thenpropose a principled framework to unify the rela-tionships among them; 2) on top of the success-fully developed query models, we propose anextended modeling formulation by incorporatingadditional query-specific information cues toguide the model estimation; 3) we explore a nov-el use of these query models by adapting them tothe speech recognition and summarization tasks.As we will see, a series of experiments indeeddemonstrate the effectiveness of the proposedmodels on these two tasks.2 Language Modeling Framework2.1 Kullback-Leibler Divergence MeasureA promising realization of the LM approach toIR is the Kullback-Leibler divergence measure(KLM), which determines the degree of rele-vance between a document and a query from arigorous information-theoretic perspective.
Twodifferent language models are involved in KLM:one for the document and the other for the query.The divergence of the document model with re-spect to the query model is defined by.
)|( )|(log)|()||(KL ?
??
Vw DwP QPQwPDQ(1)KLM not only can be thought as a natural gener-alization of the traditional query-likelihood ap-proach (Yi and Allan, 2009; Baeza-Yates andRibeiro-Neto, 2011), but also has the additionalmerit of being able to accommodate extra infor-mation cues to improve the estimation of itscomponent models in a systematic way for betterdocument ranking (Zhai, 2008).Due to that a query usually consists of only afew words, the true query model P(w|Q)mightnot be accurately estimated by the simple MLestimator (Jelinek, 1991).
There are several stud-ies devoted to estimating a more accurate querymodeling, saying that it can be approached withthe pseudo-relevance feedback process (Lavren-ko and Croft, 2001; Zhai and Lafferty, 2001b).However, the success depends largely on the as-sumption that the set of top-ranked documents,DTop={D1,D2,...,Dr,...}, obtained from an initialround of retrieval, are relevant and can be used toestimate a more accurate query language model.2.2 Relevance ModelingUnder the notion of relevance modeling (RM,often referred to as RM-1), each query Q is as-sumed to be associated with an unknown rele-vance class RQ, and documents that are relevantto the semantic content expressed in query aresamples drawn from the relevance class RQ.Since there is no prior knowledge about RQ, wemay use the top-ranked documents DTop to ap-proximate the relevance class RQ.
The corre-sponding relevance model can be estimated usingthe following equation (Lavrenko and Croft,2001; Lavrenko, 2004):.
)|()()|()|()(  )|(RM ?
?
???
?
????????????
?ToprDToprDQw rrQw rrrDwPDPDwPDwPDPQwPDD(2)2.3 Simple Mixture ModelAnother perspective of estimating an accuratequery model with the top-ranked documents isthe simple mixture model (SMM), which as-sumes that words in DTop are drawn from a two-component mixture model: 1) One component isthe query-specific topic model PSMM(w|Q), and 2)the other is a generic background modelP(w|BG).
By doing so, the SMM modelPSMM(w|Q) can be estimated by maximizing thelikelihood over all the top-ranked documents(Zhai and Lafferty, 2001b; Tao and Zhai, 2006):?
?
,)|()1()|( ),(SMM??
??
?????
Topr rD Vw DwcBGwPQwPL D ??
(3)where ?
is a pre-defined weighting parameterused to control the degree of reliance betweenPSMM(w|Q) and P(w|BG).
This estimation willenable more specific words to receive moreprobability mass, thereby leading to a more dis-criminative query model PSMM(w|Q).Although the SMM modeling aims to extractextra word usage cues for enhanced query mod-eling, it may confront two intrinsic problems.One is the extraction of word usage cues fromDTop is not guided by the original query.
The oth-er is that the mixing coefficient ?
is fixed acrossall top-ranked documents albeit that differentdocuments would potentially contribute differentamounts of word usage cues to the enhancedquery model.
To mitigate these two problems,the regularized simple mixture model has beenproposed and can be estimated by maximizingthe likelihood function (Tao and Zhai, 2006; Dil-lon and Collins-Thompson, 2010)?
?
,)|()1()|()|(),(RSMM)|(RSMM????????????
?ToprrrrD VwDwcDDVwQwPBGwPQwPQwPLD???
(4)where   is a weighting factor indicating the con-fidence on the prior information.3 The Proposed Modeling Framework3.1 FundamentalsIt is obvious that the major difference among the1475representative query models mentioned above ishow to capitalize on the set of top-ranked docu-ments and the original query.
Several subtle rela-tionships can be deduced through the followingin-depth analysis.
First, a direct inspiration of theLM-based query reformulation framework canbe drawn from the celebrated Rocchio?s formula-tion, while the former can be viewed as a proba-bilistic counterpart of the latter (Robertson, 1990;Ponte and Croft, 1998; Baeza-Yates and Ribeiro-Neto, 2011).
Second, after some mathematicalmanipulation, the formulation of the RM model(c.f.
Eq.
(2)) can be rewritten as.
)()|()()|()|(  )|(RM ?
?
????
?????
ToprD ToprD rrrrr DPDQPDPDQPDwPQwP D D(5)It becomes evident that the RM model is com-posed by mixing a set of document modelsP(w|Dr).
As such, the RM model bears a closeresemblance to the Rocchio?s formulation.
Fur-thermore, based on Eq.
(5), we can recast theestimation of the RM model as an optimizationproblem, and the likelihood (or objective) func-tion is formulated as1)|( ..,)|()|(),(???????????????
?ToprToprDrVwQwcDrrQDPtsQDPDwPLDD(6)where the document models P(w|Dr) are knownin advance; the conditional probability P(Dr|Q)of each document Dr is unknown and leave to beestimated.
Finally, a principled framework canbe obtained to unify all of these query models,including RM (c.f.
Eq.
(6)), SMM (c.f.
Eq.
(3))and RSMM (c.f.
Eq.
(4))), by using a generalizedobjective likelihood function:1)( ..,)()|(),(?????????????????
?ME MriirMrVw EEwcMrrMPtsMPMwPL  (7)where E represents a set of observations whichwe want to maximize their likelihood, and Mdenotes a set of mixture components.3.2 Query-specific Mixture ModelingThe SMM model and the RSMM model are in-tended to extract useful word usage cues fromDTop, which are not only relevant to the originalquery Q but also external to those already cap-tured by the generic background model.
Howev-er, we argue in this paper that the ?generic in-formation?
should be carefully crafted for eachquery due mainly to the fact that users?
infor-mation needs may be very diverse from one an-other.
To crystallize the idea, a query-specificbackground model PQ(w|BG) for each query Qcan be derived from DTop directly.
Another con-sideration is that since the original query modelP(w|Q) cannot be accurately estimated, it thusmay not necessarily be the best choice for use indefining a conjugate Dirichlet prior for the en-hanced query model to be estimated.
We proposeto use the RM model as a prior to guide the esti-mation of the enhanced query model.
The en-hanced query model is termed query-specificmixture model (QMM), and its correspondingtraining objective function can be expressed as?
?????????????
?ToprrrrD VwDwcQDDVwQwPBGwPQwPQwPLD.
)|()1()|()|(),(QMM)|(QMM RM???
(8)4 Applications4.1 Speech RecognitionLanguage modeling is a critical and integralcomponent in any large vocabulary continuousspeech recognition (LVCSR) system (Huang etal., 2001; Jurafsky and Martin, 2008; Furui et al.,2012).
More concretely, the role of languagemodeling in LVCSR can be interpreted as calcu-lating the conditional probability P(w|H), inwhich H is a search history, usually expressed asa sequence of words H=h1, h2,?, hL, and w isone of its possible immediately succeedingwords.
Once the various aforementioned querymodeling methods are applied to speech recogni-tion, for a search history H, we can conceptuallyregard it as a query and each of its immediatelysucceeding words w as a (single-word) document.Then, we may leverage an IR procedure thattakes H as a query and poses it to a retrieval sys-tem to obtain a set of top-ranked documents froma contemporaneous (or in-domain) corpus.
Final-ly, the enhanced query model (that is P(w|H) inspeech recognition) can be estimated by RM,SMM, RSMM or QM , and further combinedwith the background n-gram (e.g., trigram) lan-guage model to form an adaptive language modelto guide the speech recognition process.4.2 Speech SummarizationOn the other hand, extractive speech summariza-tion aims at producing a concise summary byselecting salient sentences or paragraphs fromthe original spoken document according to a pre-defined target summarization ratio (Carbonelland Goldstein, 1998; Mani and Maybury, 1999;Nenkova and McKeown, 2011; Liu andHakkani-Tur, 2011).
Intuitively, this task couldbe framed as an ad-hoc IR problem, where thespoken document is treated as an informationneed and each sentence of the document is re-garded as a candidate information unit to be re-trieved according to its relevance to the infor-mation need.
Therefore, KLM can be used toquantify how close the document D and one ofits sentences S are: the closer the sentence modelP(w|S) to the document model P(w|D), the more1476likely the sentence would be part of the summary.Due to that each sentence S of a spoken docu-ment D to be summarized usually consists ofonly a few words, the corresponding sentencemodel P(w|S) might not be appropriately esti-mated by the ML estimation.
To alleviate thedeficiency, we can leverage the merit of theabove query modeling techniques to estimate anaccurate sentence model for each sentence toenhance the summarization performance.5 Experimental SetupThe speech corpus consists of about 196 hours ofMandarin broadcast news collected by the Aca-demia Sinica and the Public Television ServiceFoundation of Taiwan between November 2001and April 2003 (Wang et al., 2005), which ispublicly available and has been segmented intoseparate stories and transcribed manually.
Eachstory contains the speech of one studio anchor, aswell as several field reporters and interviewees.A subset of 25-hour speech data compiled duringNovember 2001 to December 2002 was used tobootstrap the acoustic model training.
The vo-cabulary size is about 72 thousand words.
Thebackground language model was estimated froma background text corpus consisting of 170 mil-lion Chinese characters collected from the Chi-nese Gigaword Corpus released by LDC.The dataset for use in the speech recognitionexperiments is compiled by a subset of 3-hourspeech data from the corpus within 2003 (1.5hours for development and 1.5 hours for test).The contemporaneous (in-domain) text corpusused for training the various LM adaptationmethods was collected between 2001 and 2003from the corpus (excluding the test set), whichconsists of one million Chinese characters of theorthographic broadcast news transcripts.
In thispaper, all the LM adaptation experiments wereperformed in word graph rescoring.
The associ-ated word graphs of the speech data were builtbeforehand with a typical LVCSR system (Ort-manns et al., 1997; Young et al., 2006).In addition, the summarization task also em-ploys the same broadcast news corpus as well.
Asubset of 205 broadcast news documents com-piled between November 2001 and August 2002was reserved for the summarization experiments(185 for development and 20 for test).
A subsetof about 100,000 text news documents, compiledduring the same period as the documents to besummarized, was employed to estimate the relat-ed summarization models compared in this paper.We adopted three variants of the widely-usedROUGE metric (i.e., ROUGE-1, ROGUE-2 andROUGE-L) for the assessment of summarizationperformance (Lin, 2003).
The summarizationratio, defined as the ratio of the number of wordsin the automatic (or manual) summary to that inthe reference transcript of a spoken document,was set to 10% in this research.6 Experimental ResultsIn the first part of experiments, we evaluate theeffectiveness of the various query models appliedto the speech recognition task.
The correspond-ing results with respect to different numbers oftop-ranked documents being used for estimatingtheir component models are shown in Table 1.Also worth mentioning is that the baseline sys-tem with the background trigram language model,which was trained with the SRILM toolkit(Stolcke, 2005) and Good-Turing smoothing(Jelinek, 1999), results in a Chinese charactererror rate (CER) of 20.08% on the test set.
Con-sulting Table 1 we notice two particularities.
Oneis that there is more fluctuation in the CER re-sults of SMM than in those of RM.
The reasonmight be that, for SMM, the extraction of rele-vance information from the top-ranked docu-ments is conducted with no involvement of thetest utterance (i.e., the query; or its correspond-ing search histories), as elaborated earlier in Sec-tion 2.
When too many feedback documents arebeing used, there would be a concern for SMMto be distracted from being able to appropriatemodel the test utterance, which is probablycaused by some dominant distracting (or irrele-vant) feedback documents.
The other interestingobservation is that RSMM only achieves a com-parable (even worse) result when compared toSMM.
A possible reason is that the prior con-straint of the RSMM may contain too muchnoisy information so as to bias the model estima-tion.
Furthermore, it is evident that the proposedQMM is the best-performing method among allthe query models compared in the paper.
Alt-hough the improvements made by QMM are notas pronounced as expected, we believe thatQMM has demonstrated its potential to be ap-plied to other related applications.
On the otherhand, we compare the various query models withtwo well-practiced language models, namely thecache model (Cache) (Kuhn and Mori, 1990;Jelinek et al., 1991) and the latent Dirichlet allo-cation (LDA) (Liu and Liu, 2007; Tam andSchultz, 2005).
The CER results of these twomodels are also shown in Table 1, respectively.For the cache model, bigram cache was usedsince it can yield better results than the unigramand trigram cache models in our experiments.
Itis worthy to notice that the LDA model wastrained with the entire set of contemporaneoustext document collection (c.f.
Section 4), whileall of the query models explored in the paperwere estimated based on a subset of the corpusselected by an initial round of retrieval.
The re-sults reveal that most of these query models canachieve superior performance over the two con-ventional language models.1477In the second part of experiments, we evaluatethe utilities of the various query models as ap-plied to the speech summarization task.
At theoutset, we assess the performance level of thebaseline KLM method by comparison with twowell-practiced unsupervised methods, viz.
thevector space model (VSM) (Gong and Liu, 2001),and its extension, maximal marginal relevance(MMR) (Carbonell and Goldstein, 1998).
Thecorresponding results are shown in Table 2 andcan be aligned with several related literature re-views.
By looking at the results, we find thatKLM outperforms VSM by a large margin, con-firming the applicability of the language model-ing framework for speech summarization.
Fur-thermore, MMR that presents an extension ofVSM performs on par with KLM for the textsummarization task (TD) and exhibits superiorperformance over KLM for the speech summari-zation task (SD).
We now turn to evaluate theeffectiveness of the various query models (viz.RM, SMM, RSMM and QMM) in conjunctionwith the pseudo-relevance feedback process forenhancing the sentence model involved in theKLM method.
The corresponding results are alsoshown in Table 2.
Two noteworthy observationscan be drawn from Table 2.
One is that all thesequery models can considerably improve thesummarization performance of the KLM method,which corroborates the advantage of using themfor enhanced sentence representations.
The otheris that QMM is the best-performing one amongall the formulations studied in this paper for boththe TD and SD cases.Going one step further, we explore to use extraprosodic features that are deemed complemen-tary to the LM cue provided by QMM for speechsummarization.
To this end, a support vector ma-chine (SVM) based summarization model istrained to integrate a set of 28 commonly-usedprosodic features (Liu and Hakkani-Tur, 2011)for representing each spoken sentence, sinceSVM is arguably one of the state-of-the-art su-pervised methods that can make use of a diversi-ty of indicative features for text or speech sum-marization (Xie and Liu, 2010; Chen et al.,2013).
The sentence ranking scores derived byQMM and SVM are in turn integrated through asimple log-linear combination.
The correspond-ing results are shown in Table 2, demonstratingconsistent improvements with respect to all thethree variants of the ROUGE metric as comparedto that using either QMM or SVM in isolation.We also investigate using SVM to additionallyintegrate a richer set of lexical and relevance fea-tures to complement QMM and further enhancethe summarization effectiveness.
However, dueto space limitation, we omit the details here.
As aside note, there is a sizable gap between the TDand SD cases, indicating room for further im-provements.
We may seek remedies, such as ro-bust indexing schemes, to compensate for imper-fect speech recognition.7 Conclusion and OutlookIn this paper, we have presented a systematic andthorough analysis of a few well-practiced querymodels for IR and extended their novel applica-bility to speech recognition and summarization ina principled way.
Furthermore, we have pro-posed an extension of this research line by intro-ducing query-specific mixture modeling; the util-ities of the deduced model have been extensivelycompared with several existing query models.
Asto future work, we would like to investigatejointly integrating proximity and other differentkinds of relevance and lexical/semantic infor-mation cues into the process of feedback docu-ment selection so as to improve the empiricaleffectiveness of such query modeling.AcknowledgementsThis research is supported in part by the ?Aimfor the Top University Project?
of National Tai-wan Normal University (NTNU), sponsored bythe Ministry of Education, Taiwan, and by theMinistry of Science and Technology, Taiwan,under Grants MOST 103-2221-E-003-016-MY2,NSC 101-2221-E-003-024-MY3, NSC 102-2221-E-003-014-, NSC 101-2511-S-003-057-MY3, NSC 101-2511-S-003-047-MY3 and NSC103-2911-I-003-301.Table 1.
The speech recognition results (in CER(%)) achieved by various language models alongwith different numbers of latent topics/pseudo-relevance feedback documents.16 32 64 128Baseline 20.08Cache 19.86LDA 19.29 19.30 19.28 19.15RM 19.26 19.26 19.26 19.26SMM 19.19 19.00 19.14 19.10RSMM 19.18 19.14 19.15 19.19QMM 19.05 18.97 19.00 18.99Table 2.
The summarization results (in F-scores)achieved by various language models along withtext and spoken documents.Text Documents (TD) Spoken Documents (SD)ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-LVSM 0.347 0.228 0.290 0.342 0.189 0.287MMR 0.407 0.294 0.358 0.381 0.226 0.331KLM 0.411 0.298 0.361 0.364 0.210 0.307RM 0.453 0.335 0.403 0.382 0.239 0.331SMM 0.439 0.320 0.388 0.383 0.229 0.327RSMM 0.472 0.365 0.423 0.381 0.235 0.329QMM 0.486 0.382 0.435 0.395 0.256 0.349SVM 0.441 0.334 0.396 0.370 0.222 0.326QMM+SVM0.492 0.395 0.448 0.398 0.261 0.3581478ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto.2011.
Modern information retrieval: the con-cepts and technology behind search, ACMPress.David M. Blei, Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent dirichlet allocation.Journal of Machine Learning Research,pp.993?1022.David M. Blei and John Lafferty.
2009.
Topicmodels.
In A. Srivastava and M.
Sahami,(eds.
), Text Mining: Theory and Applications.Taylor and Francis.Jaime Carbonell and Jade Goldstein.
1998.
Theuse of MMR, diversitybased reranking forreordering documents and producing sum-maries.
In Proc.
SIGIR, pp.
335?336.Claudio Carpineto and Giovanni Romano.
2012.A survey of automatic query expansion in in-formation retrieval.
ACM Computing Surveys,vol.
44, pp.1?56.Stephane Clinchant and Eric Gaussier.
2013.
Atheoretical analysis of pseudo-relevancefeedback models.
In Proc.
ICTIR.Guihong Cao, Jian-Yun Nie, Jianfeng Gao, andStephen Robertson.
2008.
Selecting goodexpansion terms for pseudo-relevance feed-back.
In Proc.
SIGIR, pp.
243?250.Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang,and Jia-Wen Liu.
2013.
Extractive speechsummarization using evaluation metric-related training criteria.
Information Pro-cessing & Management, 49(1), pp.
1cessArthur P. Dempster, Nan M. Laird, and DonaldB.
Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.
Jour-nal of Royal Statistical Society B, 39(1), pp.1?38.Joshua V. Dillon and Kevyn Collins-Thompson.2010.
A unified optimization framework forrobust pseudo-relevance feedback algorithms.In Proc.
CIKM, pp.
1069?1078.Sadaoki Furui, Li Deng, Mark Gales, HermannNey, and Keiichi Tokuda.
2012.
Fundamen-tal technologies in modern speech recogni-tion.
IEEE Signal Processing Magazine,29(6), pp.
16?17.Yihong Gong and Xin Liu.
2001.
Generic textsummarization using relevance measure andlatent semantic analysis.
In Proc.
SIGIR, pp.19?25.Djoerd Hiemstra, Stephen Robertson, and HugoZaragoza.
2004.
Parsimonious languagemodels for information retrieval.
In Proc.SIGIR, pp.
178?185.Thomas Hofmann.
1999.
Probabilistic latent se-mantic indexing.
In Proc.
SIGIR, pp.
50?57.Thomas Hofmann.
2001.
Unsupervised learningby probabilistic latent semantic analysis.Machine Learning, 42, pp.
177?196.Xuedong Huang, Alex Acero, and Hsiao-WuenHon.
2001.
Spoken language processing: aguide to theory, algorithm, and system de-velopment.
Prentice Hall PTR, Upper SaddleRiver, NJ, USA.Frederick Jelinek, Bernard Merialdo, Salim Rou-kos, and M. Strauss.
1991.
A dynamic lan-guage model for speech recognition.
In Proc.the DARPA workshop on speech and naturallanguage, pp.
293?295.Frederick Jelinek.
1999.
Statistical methods forspeech recognition.
MIT Press.Daniel Jurafsky and James H. Martin.
2008.Speech and language processing.
PrenticeHall PTR, Upper Saddle River, NJ, USA.Roland Kuhn and Renato D. Mori.
1990.
Acache-based natural language model forspeech recognition.
IEEE Transactions onPattern Analysis and Machine Intelligence,12(6), pp.
570?583.Solomon Kullback and Richard A. Leibler.
1951.On information and sufficiency.
The Annalsof Mathematical Statistics, 22(1), pp.
79?86.Chin-Yew Lin.
2003.
ROUGE: Recall-orientedUnderstudy for Gisting Evaluation.
Availa-ble: http://haydn.isi.edu/ROUGE/.Feifan Liu and Yang Liu.
2007.
Unsupervisedlanguage model adaptation incorporatingnamed entity information.
In Proc.
ACL, pp.672?769.Yang Liu and Dilek Hakkani-Tur.
2011.
Speechsummarization.
Chapter 13 in Spoken Lan-guage Understanding: Systems for Extract-ing Semantic Information from Speech, G.Tur and R. D. Mori (Eds), New York: Wiley.John Lafferty and Chengxiang Zhai.
2001.
Doc-ument language models, query models, andrisk minimization for information retrieval.In Proc.
SIGIR, pp.
111?119.Victor Lavrenko and W. Bruce Croft.
2001.
Rel-evance-based language models.
In Proc.SIGIR, pp.
120?127.Victor Lavrenko.
2004.
A Generative Theory ofRelevance.
PhD thesis, University of Massa-chusetts, Amherst.1479Shasha Xie and Yang Liu.
2010.
Improving su-pervised learning for meeting summarizationusing sampling and regression.
ComputerSpeech & Language, 24(3), pp.
495?514.Yuanhua Lv and Chengxiang Zhai.
2009.
Acomparative study of methods for estimatingquery language models with pseudo feed-back.
In Proc.
CIKM, pp.
1895?1898.Yuanhua Lv and Chengxiang Zhai.
2010.
Posi-tional relevance model for pseudo-relevancefeedback.
In Proc.
SIGIR, pp.
579?586.Kyung Soon Lee, W. Bruce Croft, and JamesAllan.
2008.
A cluster-based resamplingmethod for pseudo-relevance feedback.
InProc.
SIGIR, pp.
235?242.Kyung Soon Lee and W. Bruce Croft.
2013.
Adeterministic resampling method using over-lapping document clusters for pseudo-relevance feedback.
Inf.
Process.
Manage.49(4), pp.
792?806.Inderjeet Mani and Mark T. Maybury (Eds.).1999.
Advances in automatic text summari-zation.
Cambridge, MA: MIT Press.Ani Nenkova and Kathleen McKeown.
2011.Automatic summarization.
Foundations andTrends in Information Retrieval, 5(2?3), pp.103?233.Stefan Ortmanns, Hermann Ney, and Xavier Au-bert.
1997.
A word graph algorithm for largevocabulary continuous speech recognition.Computer Speech and Language, pp.
43?72.Jay M. Ponte and W. Bruce Croft.
1998.
A lan-guage modeling approach to information re-trieval.
In Proc.
SIGIR, pp.
275?281.Stephen E. Robertson.
1990.
On term selectionfor query expansion.
Journal of Documenta-tion, 46(4), pp.
359?364.Andreas Stolcke.
2005.
SRILM - An extensiblelanguage modeling toolkit.
In Proc.
INTER-SPEECH, pp.901?904.Tao Tao and Chengxiang Zhai.
2006.
Regular-ized estimation of mixture models for robustpseudo-relevance feedback.
In Proc.
SIGIR,pp.
162?169.Yik-Cheung Tam and Tanja Schultz.
2005.
Dy-namic language model adaptation using vari-ational Bayes inference.
In Proc.
INTER-SPEECH, pp.
5?8.Xuanhui Wang, Hui Fang, and Chengxiang Zhai.2008.
A study of methods for negative rele-vance feedback.
In Proc.
SIGIR, pp.
219?226.Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, andShih-Sian Cheng.
2005.
MATBN: A Manda-rin Chinese broadcast news corpus.
Interna-tional Journal of Computational Linguistics& Chinese Language Processing, 10(2), pp.219?236.Xing Yi and James Allan.
2009.
A comparativestudy of utilizing topic models for infor-mation retrieval.
In Proc.
ECIR, pp.
29?41.Steve Young, Dan Kershaw, Julian Odell, DaveOllason, Valtcho Valtchev, and Phil Wood-land.
2006.
The HTK book version 3.4.Cambridge University Press.Chengxiang Zhai and John Lafferty.
2001a.
Astudy of smoothing methods for languagemodels applied to ad hoc information re-trieval.
In Proc.
SIGIR, pp.
334?342.Chengxiang Zhai and John Lafferty.
2001b.Model-based feedback in the language mod-eling approach to information retrieval.
InProc.
CIKM, pp.
403?410.Chengxiang Zhai.
2008.
Statistical languagemodels for information retrieval: a criticalreview.
Foundations and Trends in Infor-mation Retrieval, 2 (3), pp.
137?213.Yi Zhang, Jamie Callan, and Thomas Minka.2002.
Novelty and redundancy detection inadaptive filtering.
In Proc.
SIGIR, pp.
81?88.1480
