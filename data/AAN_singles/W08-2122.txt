CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178?182Manchester, August 2008A Latent Variable Model of Synchronous Parsingfor Syntactic and Semantic DependenciesJames HendersonDept Computer ScienceUniv Genevajames.henderson@cui.unige.chPaola MerloDept LinguisticsUniv Genevamerlo@lettres.unige.chGabriele MusilloDepts Linguisticsand Computer ScienceUniv Genevamusillo@lettres.unige.chIvan Titov?Dept Computer ScienceUniv Illinois at U-Ctitov@uiuc.eduAbstractWe propose a solution to the challengeof the CoNLL 2008 shared task that usesa generative history-based latent variablemodel to predict the most likely derivationof a synchronous dependency parser forboth syntactic and semantic dependencies.The submitted model yields 79.1% macro-average F1 performance, for the joint task,86.9% syntactic dependencies LAS and71.0% semantic dependencies F1.
A largermodel trained after the deadline achieves80.5% macro-average F1, 87.6% syntac-tic dependencies LAS, and 73.1% seman-tic dependencies F1.1 IntroductionSuccesses in syntactic tasks, such as statisticalparsing and tagging, have recently paved the wayto statistical learning techniques for levels of se-mantic representation, such as recovering the log-ical form of a sentence for information extractionand question-answering applications (e.g.
(Wongand Mooney, 2007)) or jointly learning the syntac-tic structure of the sentence and the propositionalargument-structure of its main predicates (Musilloand Merlo, 2006; Merlo and Musillo, 2008).
Inthis vein, the CoNLL 2008 shared task sets thechallenge of learning jointly both syntactic depen-dencies (extracted from the Penn Treebank (Mar-cus et al, 1993) ) and semantic dependencies (ex-tracted both from PropBank (Palmer et al, 2005)?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.0Authors in alphabetical order.and NomBank (Meyers et al, 2004) under a uni-fied representation.We propose a solution that uses a generativehistory-based model to predict the most likelyderivation of a synchronous dependency parser forboth syntactic and semantic dependencies.
Ourprobabilistic model is based on Incremental Sig-moid Belief Networks (ISBNs), a recently pro-posed latent variable model for syntactic struc-ture prediction, which has shown very good be-haviour for both constituency (Titov and Hender-son, 2007a) and dependency parsing (Titov andHenderson, 2007b).
The ability of ISBNs to in-duce their features automatically enables us to ex-tend this architecture to learning a synchronousparse of syntax and semantics without modifica-tion of the main architecture.
By solving theproblem with synchronous parsing, a probabilisticmodel is learnt which maximises the joint proba-bility of the syntactic and semantic dependenciesand thereby guarantees that the output structure isglobally coherent, while at the same time buildingthe two structures separately.
This extension of theISBN architecture is therefore applicable to otherproblems where two independent, but related, lev-els of representation are being learnt, such as sta-tistical machine translation.Currently the largest model we have trainedachieves 80.5% macro-average F1 performance forthe joint task, 87.6% syntactic dependencies LAS,and 73.1% semantic dependencies F1.2 The Probability ModelOur probability model is a joint generative modelof syntactic and semantic dependencies.
Thetwo dependency structures are specified as the se-quence of actions for a synchronous parser, whichrequires each dependency structure to be projec-178tivised separately.2.1 Synchronous derivationsThe derivations for syntactic dependency trees arethe same as specified in (Titov and Henderson,2007b), which are based on the shift-reduce styleparser of (Nivre et al, 2006).
The derivations use astack and an input queue.
There are actions for cre-ating a leftward or rightward arc between the top ofthe stack and the front of the queue, for popping aword from the stack, and for shifting a word fromthe queue to the stack.
The derivations for seman-tic dependency graphs use virtually the same setof actions, but impose fewer constraints on whenthey can be applied, due to the fact that a word ina semantic dependency graph can have more thanone parent.
An additional action predicateswasintroduced to label a predicate with sense s.Let Tdbe a syntactic dependency tree withderivation D1d, ..., Dmdd, and Tsbe a semantic de-pendency graph with derivation D1s, ..., Dmss.
Todefine derivations for the joint structure Td, Ts,we need to specify how the two derivations aresynchronised, and in particular make the impor-tant choice of the granularity of the synchronisa-tion step.
Linguistic intuition would perhaps sug-gest that syntax and semantics are connected at theclause level ?
a big step size ?
while a fully in-tegrated system would synchronise at each pars-ing decision, thereby providing the most commu-nication between these two levels.
We choose tosynchronise the construction of the two structuresat every word ?
an intermediate step size.
Thischoice is simpler, as it is based on the natural to-tal order of the input, and it avoids the problemsof the more linguistically motivated choice, wherechunks corresponding to different semantic propo-sitions would be overlapping.We divide the two derivations into the chunksbetween shifting each word onto the stack,ctd= Dbtdd, ..., Detddand cts= Dbtss, ..., Detss,where Dbtd?1d= Dbts?1s= shiftt?1andDetd+1d= Dets+1s= shiftt.
Then the actions ofthe synchronous derivations consist of quadruplesCt= (ctd, switch, cts, shiftt), where switch meansswitching from syntactic to semantic mode.
Thisgives us the following joint probability model,where n is the number of words in the input.P (Td, Ts) = P (C1, .
.
.
, Cn)=?tP (Ct|C1, .
.
.
, Ct?1)(1)The probability of each synchronous derivationchunk Ctis the product of four factors, related tothe syntactic level, the semantic level and the twosynchronising steps.P (Ct|C1, .
.
.
, Ct?1) =P (ctd|C1, .
.
.
, Ct?1)?P (switch|ctd, C1, .
.
.
, Ct?1)?P (cts|switch, ctd, C1, .
.
.
, Ct?1)?P (shiftt|ctd, cts, C1, .
.
.
, Ct?1)(2)These synchronous derivations C1, .
.
.
, Cnonlyrequire a single input queue, since the shift opera-tions are synchronised, but they require two sepa-rate stacks, one for the syntactic derivation and onefor the semantic derivation.The probability of ctdis decomposed into deriva-tion action Diprobabilities, and likewise for cts.P (ctd|C1, .
.
.
, Ct?1)=?iP (Did|Dbtdd,.
.
., Di?1d, C1,.
.
., Ct?1)(3)The actions are also sometimes split into a se-quence of elementary decisions Di= di1, .
.
.
, din,as discussed in (Titov and Henderson, 2007a).2.2 Projectivisation of dependenciesThese derivations can only specify projectivesyntactic or semantic dependency graphs.
Ex-ploratory data analysis indicates that many in-stances of non-projectivity in the complete graphare due to crossings of the syntactic and seman-tic graphs.
The amount of non-projectivity of thejoint syntactic-semantic graph is approximately7.5% non-projective arcs, while summing the non-projectivity within the two separate graphs resultsin only roughly 3% non-projective arcs.Because our synchronous derivations use twodifferent stacks for the syntactic and semantic de-pendencies, respectively, we only require each in-dividual graph to be projective.
As with many de-pendency parsers (Nivre et al, 2006; Titov andHenderson, 2007b), we handle non-projective (i.e.crossing) arcs by transforming them into non-crossing arcs with augmented labels.1Becauseour syntactic derivations are equivalent to those of(Nivre et al, 2006), we use their HEAD methodsto projectivise the syntactic dependencies.Although our semantic derivations use the sameset of actions as the syntactic derivations, they dif-fer in that the graph of semantic dependencies need1During testing, these projectivised structures are thentransformed back to the original format for evaluation.179not form a tree.
The only constraints we place onthe set of semantic dependencies are imposed bythe use of a stack, which excludes crossing arcs.Given two crossing arcs, we try to uncross themby changing an endpoint of one of the arcs.
Thearc (p, a), where p is a predicate and a is an argu-ment, is changed to (p, h), where h is the syntactichead of argument a.
Its label r is then changed tor/d where d is the syntactic dependency of a onh.
This transformation may need to be repeatedbefore the arcs become uncrossed.
The choice ofwhich arc to transform is done using a greedy al-gorithm and a number of heuristics, without doingany global optimisation across the data.This projectivisation method is similar to theHEAD method of (Nivre et al, 2006), but has twointeresting new characteristics.
First, syntactic de-pendencies are used to projectivise the semanticdependencies.
Because the graph of semantic rolesis disconnected, moving across semantic arcs is of-ten not possible.
This would cause a large numberof roles to be moved to ROOT.
Second, our methodchanges the semantic argument of a given pred-icate, whereas syntactic dependency projectivisa-tion changes the head of a given dependent.
Thisdifference is motivated by a predicate-centred viewof semantic dependencies, as it avoids changing apredicate to a node which is not a predicate.3 The Learning ArchitectureThe synchronous derivations described above aremodelled with an Incremental Sigmoid Belief Net-work (ISBN) (Titov and Henderson, 2007a).
IS-BNs are dynamic Bayesian Networks which incre-mentally specify their model structure based on thepartial structure being built by a derivation.
Theyhave previously been applied to constituency anddependency parsing.
In both cases the derivationswere based on a push-down automaton, but ISBNscan be directly applied to any automaton.
We suc-cessfully apply ISBNs to a two-stack automaton,without changing the machine learning methods.3.1 The Incremental Sigmoid Belief NetworksISBNs use vectors of latent variables to representproperties of parsing history relevant to the nextdecisions.
Latent variables do not need to be anno-tated in the training data, but instead get inducedduring learning.
As illustrated by the vectors Siin figure 1, the latent feature vectors are used toestimate the probabilities of derivation actions Di.sSSDDSi?ci?c i?1i?1iijDi dkiFigure 1: An ISBN for estimatingP (dik|history(i, k)) ?
one of the elementarydecisions.
Variables whose values are given inhistory(i, k) are shaded, and latent and currentdecision variables are unshaded.Latent variable vectors are connected to variablesfrom previous positions via a pattern of edges de-termined by the previous decisions.
Our ISBNmodel distinguishes two types of latent states: syn-tactic states, when syntactic decisions are consid-ered, and semantic states, when semantic decisionare made.
Different patterns of interconnectionsare used for different types of states.
We use theneural network approximation (Titov and Hender-son, 2007a) to perform inference in our model.As also illustrated in figure 1, the induced latentvariables Siat state i are statistically dependent onboth pre-defined features of the derivation historyD1, .
.
.
, Di?1and the latent variables for a finiteset of relevant previous states Si?, i?< i. Choos-ing this set of relevant previous states is one of themain design decisions in building an ISBN model.By connecting to a previous state, we place thatstate in the local context of the current decision.This specification of the domain of locality deter-mines the inductive bias of learning with ISBNs.Thus, we need to choose the set of local (i.e.
con-nected) states in accordance with our prior knowl-edge about which previous decisions are likely tobe particularly relevant to the current decision.3.2 Layers and featuresTo choose previous relevant decisions, we makeuse of the partial syntactic and semantic depen-dency structures which have been decided so farin the parse.
Specifically, the current latent statevector is connected to the most recent previous la-tent state vectors (if they exist) whose configura-tion shares a node with the current configuration,as specified in Table 1.
The nodes are chosen be-cause their properties are thought to be relevant tothe current decision.
Each row of the table indi-cates which nodes need to be identical, while each180Closest Current Syn-Syn Srl-Srl Syn-SrlInput Input + + +Top Top + + +RDT Top + +LDT Top + +HT Top + +LDN Top + +Input Top +Table 1: Latent-to-latent variable connections.
In-put= input queue; Top= top of stack; RDT= right-most right dependent of top; LDT= leftmost leftdependent of top; HT= Head of top; LDN= left-most dependent of next (front of input).column indicates whether the latent state vectorsare for the syntactic or semantic derivations.
Forexample, the first row indicates edges between thecurrent state and a state which had the same in-put as the current state.
The three columns indi-cate that this edge holds within syntactic states,within semantic states, and from syntactic to se-mantic states.
The fourth cell of the third row, forexample, indicates that there is an edge betweenthe current semantic state on top of the stack andthe most recent semantic state where the rightmostdependent of the current top of the semantic stackwas at the top of the semantic stack.Each of these relations has a distinct weight ma-trix for the resulting edges in the ISBN, but thesame weight matrix is used at each position wherethe relation applies.
Training and testing timesscale linearly with the number of relations.The pre-defined features of the parse historywhich also influence the current decision are spec-ified in table 2.
The model distinguishes argumentroles of nominal predicates from argument roles ofverbal predicates.3.3 DecodingGiven a trained ISBN as our probability esti-mator, we search for the most probable jointsyntactic-semantic dependency structure using abeam search.
Most pruning is done just after eachshift operation (when the next word is predicted).Global constraints (such as label uniqueness) arenot enforced by decoding, but can be learnt.For the system whose results we submitted, wethen do a second step to improve on the choiceof syntactic dependency structure.
Because of thelack of edges in the graphical model from seman-tic to syntactic states, it is easy to marginalise outthe semantic structure, giving us the most proba-ble syntactic dependency structure.
This syntacticstructure is then combined with the semantic struc-State Stack Syntactic step featuresLEX POS DEPInput + +Top syn + +Top - 1 syn +HT syn +RDT syn +LDT syn +LDN syn +State Stack Semantic step featuresLEX POS DEP SENSEInput + + +Top sem + + +Top - 1 sem + +HT sem + +RDT sem +LDT sem +LDN sem +A0-A5 of Top sem +A0-A5 of Input sem +Table 2: Pre-defined features.
syn=syntactic stack;sem=semantic stack.
Input= input queue; Top=top of stack; RDT= rightmost dependent of top;LDT= leftmost dependent of Top; HT= Head oftop; LDN= leftmost dependent of next (front ofinput); A0-A5 of Top/Input= arguments of top ofstack / input.ture from the first stage, to get our submitted re-sults.
This second stage does not maximise perfor-mance on the joint syntactic-semantic dependencystructure, but it better fits the evaluation measureused to rank systems.4 Experiments and DiscussionThe experimental set-up common for all the teamsis described in the introduction (Surdeanu et al,2008).
The submitted model has latent variablevectors of 60 units, and a word frequency cut-offof 100, resulting in a small vocabulary of 1083words.
We used a beam of size 15 to prune deriva-tions after each shift operation to obtain the jointstructure, and a beam of size 40 when perform-ing the marginalisation.
Training took approxi-mately 2.5 days on a standard PC with 3.0 GHzPentium4 CPU.
It took approximately 2 hours toparse the entire testing set (2,824 sentences) andan additional 3 hours to perform syntactic parsingwhen marginalising out the semantic structures.2Shortly after the submission deadline, we trained a?large?
model with a latent variable vector of size80, a word frequency cut-off of 20, and additionallatent-to-latent connections from semantics to syn-tax of the same configuration as the last column2A multifold speed-up with a small decrease in accuracycan be achieved by using a small beam.181Syn Semantic OverallLAS P R F1 P R F1SubmittedD 86.1 78.8 64.7 71.1 82.5 75.4 78.8W 87.8 79.6 66.2 72.3 83.7 77.0 80.2B 80.0 66.6 55.3 60.4 73.3 67.6 70.3WB 86.9 78.2 65.0 71.0 82.5 76.0 79.1Joint inferenceD 85.5 78.8 64.7 71.1 82.2 75.1 78.5Large, joint inferenceD 86.5 79.9 67.5 73.2 83.2 77.0 80.0W 88.5 80.4 69.2 74.4 84.4 78.8 81.5B 81.0 68.3 57.7 62.6 74.7 69.4 71.9WB 87.6 79.1 67.9 73.1 83.4 77.8 80.5Table 3: Scores on the development set and thefinal testing sets (percentages).
D= developmentset; W=WSJ; B=Brown; WB=WSJ+Brown;of table 1.
This model took about 50% longer intraining and testing.In table 3, we report results for the marginalisedinference (?submitted?)
and joint inference for thesubmitted model, and the results for joint inferencewith the ?large?
model.
The larger model improveson the submitted results by almost 1.5%, a signifi-cant improvement.
If completed earlier, this modelwould have been fifth overall, second for syntacticLAS, and fifth for semantic F1.To explore the relationship between the twocomponents of the model, we removed the edgesbetween the syntax and the semantics in the sub-mitted model.
This model?s performance drops byabout 3.5% for semantic role labelling, thereby in-dicating that the latent annotation of parsing stateshelps semantic role labelling.
However, it alsoindicates that there is much room for improve-ment in developing useful semantic-specific fea-tures, which was not done for these experimentssimply due to constraints on development time.To test whether joint learning degrades the ac-curacy of the syntactic parsing model, we trained asyntactic parsing model with the same features andthe same pattern of interconnections as used for thesyntactic states in our joint model.
The resultinglabelled attachment score was non-significantlylower (0.2%) than the score for the marginalisedinference with the joint model.
This result sug-gests that, though the latent variables associatedwith syntactic states in the joint model were trainedto be useful in semantic role labelling, this did nothave a negative effect on syntactic parsing accu-racy, and may even have helped.Finally, an analysis of the errors on the develop-ment set for the submitted model paints a coherentpicture.
We find attachment of adjuncts particu-larly hard.
For dependency labels, we make themost mistakes on modification labels, while for se-mantic labels, we find TMP, ADV, LOC, and PRNparticularly hard.
NomBank arcs are not learnt aswell as PropBank arcs: we identify PropBank SRLarguments at F1 70.8% while Nombank argumentsreach 58.1%, and predicates at accuracy 87.9% forPropBank and 74.9% for NomBank.5 ConclusionsWhile still preliminary, these results indicate thatsynchronous parsing is an effective way of build-ing joint models on separate structures.
The gen-erality of the ISBN design used so far suggeststhat ISBN?s latent feature induction extends well toestimating very complex probability models, withlittle need for feature engineering.
Nonetheless,performance could be improved by task-specificfeatures, which we plan for future work.AcknowledgementsThis work was partly funded by European Community FP7grant 216594 (CLASSiC, www.classic-project.org), SwissNSF grant 114044, and Swiss NSF fellowships PBGE2-117146 and PBGE22-119276.
Part of this work was donewhen G. Musillo was visiting MIT/CSAIL, hosted by Prof.Michael Collins.ReferencesMarcus, M., B. Santorini, and M.A.
Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19:313?330.Merlo, P. and G. Musillo.
2008.
Semantic parsing for high-precision semantic role labelling.
In Procs of CoNLL2008, Manchester, UK.Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The nombankproject: An interim report.
In Meyers, A., editor, HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,24?31, Boston, MA.Musillo, G. and P. Merlo.
2006.
Accurate semantic parsingof the Proposition Bank.
In Procs of NAACL 2006, NewYork, NY.Nivre, J., J.
Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006.Pseudo-projective dependency parsing with support vectormachines.
In Proc.
of CoNNL, 221?225, New York, USA.Palmer, M., D. Gildea, and P. Kingsbury.
2005.
The Propo-sition Bank: An annotated corpus of semantic roles.
Com-putational Linguistics, 31:71?105.Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J.Nivre.
2008.
The CoNLL-2008 shared task on joint pars-ing of syntactic and semantic dependencies.
In Procs ofCoNLL-2008, Manchester,UK.Titov, I. and J. Henderson.
2007a.
Constituent parsing withincremental sigmoid belief networks.
In Procs of ACL?07,pages 632?639, Prague, Czech Republic.Titov, I. and J. Henderson.
2007b.
A latent variable modelfor generative dependency parsing.
In Procs of IWPT?07,Prague, Czech Republic.Wong, Y.W.
and R. Mooney.
2007.
Learning synchronousgrammars for semantic parsing with lambda calculus.
InProcs of ACL?07, 960?967, Prague, Czech Republic.182
