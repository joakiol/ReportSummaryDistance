Ambiguity Resolution for Machine Translation of Telegraphic Messages IYoung-Suk LeeLincoln LaboratoryMITLexington, MA 02173USAysl@sst.
II.
mit.
eduClifford WeinsteinLincoln LaboratoryMITLexington, MA 02173USAcj w?sst, ll.
mit.
eduStephanie SeneffSLS, LCSMITCambridge, MA 02139USAseneff~lcs, mit.
eduDinesh TummalaLincoln LaboratoryMITLexington, MA 02173USAtummala?sst.
II.
mit.
eduAbst ractTelegraphic messages with numerous instances ofomis-sion pose a new challenge to parsing in that a sen-tence with omission causes a higher degree of ambi6u-ity than a sentence without omission.
Misparsing re-duced by omissions has a far-reaching consequence inmachine translation.
Namely, a misparse of the inputoften leads to a translation into the target languagewhich has incoherent meaning in the given context.This is more frequently the case if the structures ofthe source and target languages are quite different, asin English and Korean.
Thus, the question of how weparse telegraphic messages accurately and efficientlybecomes a critical issue in machine translation.
In thispaper we describe atechnical solution for the issue, andreSent he performance evaluation ofa machine trans-tion system on telegraphic messages before and afteradopting the proposed solution.
The solution lies ina grammar design in which lexicalized grammar rulesdefined in terms of semantic ategories and syntacticrules defined in terms of part-of-speech are utilized to-ether.
The proposed grammar achieves a higher pars-g coverage without increasing the amount of ambigu-ity/misparsing when compared with a purely lexical-ized semantic grammar, and achieves a lower degreeof.
ambiguity/misparses without, decreasing the pars-mg coverage when compared with a purely syntacticgrammar.1 IntroductionAchieving the goal of producing high quality machine transla-tion output is hindered by lexica\] and syntactic ambiguity of theinput sentences.
Lexical ambiguity may be greatly reduced bylimiting the domain to be translated.
However, the same is notgenerally true for syntactic ambiguity.
In particular, telegraphicmessages, uch as military operations reports, pose a new chal-lenge to parsing in that frequently occurring ellipses in the cor-pus induce a h{gher degree of syntactic ambiguity than for textwritten in "~rammatical" English.
Misparsing triggered by theambiguity ot the input sentence often leads to a mistranslationin a machine translation system.
Therefore, the issue becomeshow to parse tele.graphic messages accurately and efficiently toproduce high quahty translation output.In general the syntactic ambiguity of an input text may begreatly reduced by introducing semantic ategories in the gram-mar to capture the co-occurrence r strictions ofthe input string.In addition, ambiguity introduced by omission can be reducedby lexicalizing rammar rules to delimit he lexical items which1This work was sponsored by the Defense Advanced ResearchProjects Agency.
Opinions, interpretations, conclusions, and rec-ommendations are those of the authors and are not necessarilyendorsed by the United States Air Force.~yrP iCally occur in phrases with omission in the given domain.
A awback of this approach, however, is that the grammar cover-age is quite low.
On the other hand, grammar coverage may bemaximized when we rely on syntactic rules defined in terms ofpart-of-speech at the cost of a high degree of ambiguity.
Thus,the goal of maximizing the parsing coverage while minimizingthe ambiguity may be achieved by adequately combining lexi-calized rules with semantic ategories, and non-lexicalized ruleswith syntactic ategories.
The question is how much semanticand syntactic information is necessary to achieve such a goal.In this paper we propose that an adequate amount of lex-ical information to reduce the ambiguity in general originatesfrom verbs, which provide information on subcategorization, a dprepositions, which are critical for PP-attachment ambiguity res-olution.
For the given domain, lexicalizing domain-specific ex-pressions which typically occur in phrases with omission is ade-quate for ambiguity resolution.
Our experimental results showthat the mix of syntactic and semantic grammar as proposedhere has advantages over either a syntactic grammar or a lexi-calized semantic grammar.
Compared with a syntactic grammar,the proposed grammar achieves a much lower degree of ambigu-ity without decreasing the grammar coverage.
Compared witha lexicalized semantic grammar, the proposed grammar achievesa higher rate of parsing coverage without increasing the ambi-guity.
Furthermore, the generality introduced by the syntacticrules facilitates the porting of the system to other domains aswell as enablin.g the system to handle unknown words efficiently.This paper is organized as follows.
In section 2 we discussthe motivation for lexicalizing rammar rules with semantic at-egories in the context of translating telegraphic messages, andits drawbacks with respect o parsing coverage.
In section 3 wepropose a grammar writing technique which minimizes the ambi-guity of the input and maximizes the parsing coverage.
In section4 we give our experimental results of the technique on the basisof two sets of unseen test data.
In section 5 we discuss ystemengineering issues to accommodate he proposed technique, i.e.,integration of part-of-speech tagger and the adaptation of theunderstanding system.
Finally section 6 provides a summary ofthe paper.2 Translation of Telegraphic MessagesTelegraphic messages contain many instances of phrases withomission, cf.
(Grishman, 1989), as in (1).
This introduces agreater degree of syntactic ambiguities than for texts withoutany omitted element, hereby posing a new challenge to parsing.
(1)TU-95 destroyed 220 nm.
(~ An aircraft TU-95 was destroyedat 220 nautical miles)Syntactic ambiguity and the resultant misparse induced bysuch an omission often leads to a mistranslation i  a machinetranslation system, such as the one described in (Weinstein etai., 1996), which is depicted in Figure 1.The system depicted in Figure 1 has a language understandingmodule TINA, (Seneff, 1992), and a language generation module120LANGUAGEGENERATIONGENESISFigure 1: An Interlingua-Based English-to-Korean MachineTranslation SystemGENESIS, (Glass, Polifroni and SeneR', 1994), at the core.
Thesemantic frame is an intermediate meaning representation whichis directly derived from the parse tree andbecomes .the input tothe generation system.
The hierarchical structure of the parsetree is preserved in the semantic frame, and therefore a misparseof the input sentence leads to a mistranslation.
Suppose thatthe sentence (1) is misparsed as an active rather than a passivesentence due to the omission of the verb was, and that the prepo-sitional phrase 220 nm is misparsed as the direct object of theverb destroy.
These instances of misunderstanding are reflectedin the semantic frame.
Since the semantic frame becomes theinput to the generation system, the generation system producesthe non-sensical Korean translation output, as in (2), as opposedto the sensible one, as in (3).
3(2) TU-95-ka 220 hayli-lul pakoy-haysstaTU-95-NOM 220 nautical mile-OBJ destroyed(3) TU-95-ka 220 hayli-eyse pakoy-toyesstaTU-95-NOM 220 nautical mile-LOC was destroyedGiven that the generation of the semantic frame from the parsetree, and the generation of the translation output from the se-mantic frame, are quite straightforward in such a system, andthat the flexibility of the semantic frame representation is wellsuited for multilingual machine translation, it would be more de-sirable to find a way of reducing the ambiguity of the input textto produce high quality translation output, rather than adjust-ing the translation process.
In the sections below we discuss onesuch method in terms of grammar design and some of its sideeffects.x2.1 Lex ica l i za t ion  o f  Grammar  Ru les  w i thSemant ic  Categor iesIn the domain of naval operational report messages (MUC-IImessages hereafter), 4 (Sundheim, 1989), we find two types ofellipsis.
First, top level categories such as subjects and the copulaverb be are often omitted, as in (4).
(4)Considered hostile act (= This was considered to be a hostileact).Second, many function words like prepositions and articles areomitted.
Instances of preposition omission are given in (5), wherez stands for Greenwich Mean Time (GMT).(5)a.
Haylor hit by a torpedo and put out of action 8 hours (---- for8 hours)b.
All hostile recon aircraft outbound 1300 z (= at 1300 z)If we try to parse sentences containing such omissions with thegrammar where the rules are defined in terms of syntactic at-egories (i.e.
part-of-speech), the syntactic ambiguity multiplies.3In the examples, NOM stands for the nominative casemarker, OBJ the object case marker, and LOC the locativepostposition.4MUC-II stands for the Second Message Understanding Con-ference.
MUC-II messages were originally collected and preparedby NRaD(1989) to support DARPA-sponsored research in mes-sage understanding.To accommodate s ntences like (5)a-b, the grammar needs to al-low all instances of noun phrases (NP hereafter) to be ambiguousbetween an NP and a prepositional phrase (PP hereafter) wherethe preposition is omitted.
Allowing an input where the copulaverb be is omitted in the grammar causes the past tense formof a verb to be interpreted either as the main verb with the ap-propriate form of be omitted, as in (6)a, or as a reduced relativeclause modifying the preceding noun, as in (6)b.
(6)Aircraft launched at 1300 z ...a.
Aircraft were launched at 1300 z ...b.
Aircraft which were launched at 1300 z ...Such instances of ambiguity are usually resolved on the basisof the semantic information.
However, relying on a semanticmodule for ambiguity resolution implies that the parser needsto produce all possible parses of the input text andcarry themalong, thereby requiring a more complex understanding process.One way of reducing the ambiguity at an early stage of pro-cessing without relying on a semantic module is to incorporatedomain/semantic knowledge into the grammar as follows:?
Lexicalize grammar rules to delimit the lexical items whichtypically occur in phrases with omission;?
Introduce semantic ategories to capture the co-occurrencerestrictions of lexical items.Some example grammar ules instantiating these ideas aregiven in (7).
(7)a..locative_PP{at in near off on ...} NPheadless_PPe..np_distancenumeric nautical_milenumeric yarde..time_expression\[at\] numeric gmtb..headless_PP\[all np-distancea np_bearingd..temporal_PP(during after prior_to ...} NPtime_expressionf .
.gmtz(7)a states that a locative prepositional phrase consists of asubset of prepositions and a noun phrase.
In addition, there isa subcategory headless_PP which consists of a subset of nounphrases which typically occur in a locative prepositional phrasewith the preposition omitted.
The head nouns which typicallyoccur in prepositional phrases with the preposition omission arenautical miles and yard.
The rest of the rules can be read in asimilar manner.
And it is clear how such lexicalized rules withthe semantic ategories reduce the syntactic ambiguity of theinput text.2.2 DrawbacksWhereas the language processing is very efficient when a systemrelies on a lexicalized semantic grammar, there are some draw-backs as well.?
Since the grammar is domain and word specific, it is noteasily ported to new constructions and new domains.?
Since the vocabulary items are entered in the grammar aspart of lexicalized grammar rules, if an input sentence con-tains words unknown to the grammar, parsing fails.These drawbacks are reflected in the performance evaluation ofour machine translation system.
After the system was developedon all the training data of  the MUC-II corpus (640 sentences, 12words/sentence average), the system was evaluated on the held-out test set of 111 sentences (hereafter TEST set).
The resultsare shown in Table 1.
The system was also evaluated on thedata which were collected from an in-house xperiment.
For thisexperiment, the subjects were asked to study a number of MUC-II sentences, and create about 20 MUC-II-like sentences.
These121Total No.
of sentences 111No.
of sentences with no 66/111 (59.5%)unknown wordsNo.
of parsed sentences 23/66 (34.8%)No, of misparsed sentences 2/23 (8:7%)Table 1: TEST Data Evaluation Results on the LexicalizedSemantic GrammarTotal .No.
of sentences 281No.
of sentences with no 239/281 (85.1%)unknown wordsNO.
of parsed sentences 103/239 (43.1%)No.
of misparsed sentences 15/103 (14.6%)Table 2: TEST'  Data Evaluation Results on the LexicalizedSemantic GrammarMUC-II-like sentences form data set TEST'.
The results of thesvstem evaluation on the data set TEST' are given in Table 2."
Table 1 shows that the grammar coverage for unseen data isabout 35%, excluding the failures due to unknown words.
Table 2indicates that even for sentences constructed to be similar to thetraining data, the grammar coverage is about 43%, again exclud-ing the parsing failures due to unknown words.
The misparse 5rate with respect o the total parsed sentences ranges between8.7% and 14.6%, which is considered to be highly accurate.3 Incorporat ion  o f  Syntact i c  KnowledgeConsidering the low parsing coverage of a semantic grammarwhich relies on domain specific knowledse, and the fact that thesuccessful parsing of the input sentence ks a prerequisite for pro-ducing translation output, it is critical to improve the parsingcoverage.
Such a goal may be achieved by incorporating syn-tactic rules into the ~ammar while retaining lexical/semanticinformation to minim'ize the ambiguity of the input text.
Thequestion is: how much semantic and syntactic information isnecessary?
We propose a solution, as in (8):(8)(a) Rules involving verbs and prepositions need to be lexicalizedto resolve the prepositional phrase attachment ambiguity, cf.
(Brill and Resnik, 1993).
(b) Rules involving verbs need to be lexicalized to prevent mis-arSing due to an incorrect subcategorization.)
Domain specific expressions (e.g.z.
nm in the MUC-II cor-pus) which frequently occur in phrases with omitted elements.need to be lexicalized.
(d) Otherwise.
relv on svntactic rules defined in terms of part-of-speech. "
"In this section, we discuss typical misparses for the syntac-tic grammar on experiments in the MUC-II corpus.
We thenillustrate how these misparses are corrected by lexicalizing thegrammar ules for verbs, prepositions, and some domain-specificphrases.3.1 Typ ica l  M isparses  Caused  by Syntact i cGrammarThe misparses we find in the MUC-II corpus, when tested on asyntactic grammar, are largely due to the three factors specifiedin (9).5The term misparse in this paper should be interpreted withcare.
A number o f  the sentences we consider to be misparses aret svntacuc mksparses, but "semanucallv anomalous.
Sincewe are interested in getting the accurate interpretation i thegiven context at the parsingstage, we consider parses which aresemantically anomalous to be misparses.
(9) i. Misparsing due to prepositional phrase attachment(hereafter PP-attachment) ambiguityii.
Misparsing due to incorrect verb subcategorizationsiii.
Misparsing due to the omission of a preposition, e.g.i,~10 z instead of at I~10 zExamples of misparses due to an incorrect verb subcatego-rization and a PP-attachment ambiguity are given in Figure 2and Figure 3. respectively.
An example of a misparse due topreposition omission is given in Figure 4.In Figure 2, the verb intercepted incorrectly subcategorizes for afinite complement clause.In Figure 3, the prepositional phrase with 12 rounds is u~ronglvattached to the noun phrase the contact, as opposed to the verbphrase vp_active, to which it properly belongs.Figure 4 shows that the prepositional phrase i,~i0 z with atomitted is misparsed as a part of the noun phrase expressionhostile raid composition.3.2 Cor rect ing  M isparses  by  Lex ica l i z ing  Verbs ,P repos i t ions ,  and  Domain  Spec i f i c  PhrasesProviding the accurate subcategorization frame for the verb in-tercept by lexicalizing the higher level category "vp" ensures thatit never takes a finite clause as its complement, leading to thecorrect parse, as in Figure 5.As for PP-attachment ambiguity, lexicalization of verbs andprepositions helps in identifying the proper attachment site of theprepositional phrase, cf.
(t3rill and Resnik, 1993), as illustratedin Figure 6.Misparses due to omission are easily corrected by deployinglexicalized rules for the vocabulary items which occur in phraseswith omitted elements.
For the misparse illustrated in Figure 3,utilizing the lexicalized rules in (10) prevents I J I0  z from beinganalyzed as part of the subsequent oun phrase, as in Figure 7.
(10) a..time_expression b .
.gmt\[at\] numeric gmt z4 Exper imenta l  Resu l tsIn this section we report two types of experimental results.
Oneis the parsing results on two sets of unseen data TEST andTEST' (discussed in Section 2) using the syntactic grammar de-fined purely in terms of part-of-speech.
Tl~e other is the parsingresults on the same sets of data using the grammar which com-bines lexicalized semantic grammar rules and syntactic grammarrules.
The results are compared with respect o the parsing cov-erage and the misparse rate.
These experimental results are alsocompared with the parsing results with respect o the lexicalizedsemantic grammar discussed in Section 2.4.1 Exper imenta l  Resu l ts  on Data  Set  TEST"-Total .No.
of sentences i i i iI No.
of parsed sentences i 84 / i l i  (75.7%) ',\[.No.
of misparsed sentences 24/84 (29%) iTable 3: TEST Data Evaluation Results on the SyntacticG r am m arI Total .No.
of sentences i iIi iNo.
of parsed sentences i 86/III (77%) !No.
of misparsed sentences 9/86 (i0%)Table 4: TEST  Data Evaluation Results on the MixedGrammarIn terms of parsing coverage, the two grammars perform equallvW - -  - -  * ell (around 76%).
In terms of misparse rate, however, the gram-mar which utilizes only syntactic ategories hows a much higher122'!
Iadver~whent,~- :vverO(:letln te rcepte~henn_headrange o~prepsentence?ull_parsestatementpredicatevp_actlve~Inlte_comp~Inlte_statementsub jec to_npPPq_npclet nn_i~esd ;:pr I prep ._~,p nn_headthe  a l rc ra?
t  :o  enterpr  lsewaslln~_comp complement?L.npcardinal nn_head30 nmFigure 2: Misparse due to incorrect verb subcategorizationsub jec ticl_npnn_headspencersentenceI?u l l _parseIstatementvver~ensasedpreOicate\[vp_activeo_npdet nn_heaa ppprep q_npcardlnal nn_neadthe contact with 12 rounds o?prepPPcLnpnn_head ppprep q_nocardinal nn_headI I5-1rich at 3000 gOsFigure 3: Misparse due to PP-attachment ambiguity123I i !
?L , - :  'sentence\[fu l l _parseIfragmen~Icomplement~.
.nppossessive adject ivez host l leOetIt1410F :~ "nn_heaclra id  compositionPPprep q-nDcar'~ ~ na i nn_hearlI Iof  Ig aLrc ra f tFigure 4: Misparse due to Omission of Prepositionpre_ad Junct3temporal_clauseLwhen_clause detwhen statementlpartiCipLai_~IpassiveIvp_intercept.Iv ln terceptIwhensentenceiPull_parseIstatementsubJectLq_npnn_head opprep q_npbrace det nnhead ppprep q_npien nn_hesdEintercspte~he range Of the a i rc ra f t  to enterpPisewaslin~_comg complementIcomplement_ripquant~?~e~a_distanceI Icardinal nautlcal_mLJ30 nmFigure 5: Parse Tree with Correct Verb Subcategorization124!
!sub jec tIq_npr.~_head dkr_objectIvensaseq_np wlthdet nn_hesdspencer  engs led the  contact  withm msentenceI?ull_parseJstatementpredicateivp_ensaseIwlth_no~.nDcardinal nn_head POpre~ ~_npI ' nn_heaO l12 rounds O~ 5- Inch!ocatlve_ppat o_nocardznal nn_hesdi 1tat 3000 WdsFigure 6: Parse Tree with Correct PP-attachmentpre_adjunctIt ime_express ionIgmt_tLmeInumer~c_tlmecardinal gmtI I14tO zsentencet?uiL_parse I?ragmentComplementIq_npadjective nn_head pphost i le  ra id composi t ionn_o?
q_npcar~ Lna I nn_headI I0?
Ig alrcra?tFigure 7: Corrected Parse Tree125rate of misparse (i.e.
29%) than the grammar which utilizesboth syntactic and semantic ategories (i.e.
10%).
Comparingthe evaluation results on the mixed grammar with those on thelexicalized semantic grammar discussed in Section 2, the parsingcoverage of the mixed grammar is much higher (77%) than thatof the semantic grammar (59.5%).
In terms of misparse rate,both grammars perform equally well, i.e.
around 9%.
64.2 Exper imenta l  Resu l ts  on  Data  Set  TEST 'Total No.
of sentences I 281 INo.
of sentences which parse 215/281 (76.5%)No.
of misparsed sentences 60/215 (28%)Table 5: TEST '  Data Evaluation Results on SyntacticGrammarI Total No.
of sentences I 289No.
of parsed sentences 236/289 /82%)No.
of mlsparsed sentences 23/236 (10%)Table 6: TEST '  Data Evaluation Results on Mixed Gram-marEvaluation results of the two types of grammar on the TEST'data, given in Table 5 and Table 6, are similar to those of thetwo types of ~ammar  on the TEST data discussed above.To summarize, the grammar which combines yntactic rulesand lexicalized semantic rules fares better than the syntacticlgrcal.mm, mar or the semantic grammar.
Compared with a lex-lzed semantic grammar, this grammar achieves a higherparsing coverage without increasing the amount of ambigu-ity/misparsing.
When compared with a syntactic grammar, thisgrammar achieves a lower degree of ambiguity/misparsing with-out decreasing the parsing rate.5 System Eng ineer ingAn input to the parser driven by a grammar which utilizes bothsyntactic and lexicalized semantic rules consists of words (to becovered by lexicalized semantic rules) and parts-of-speech (to becovered by syntactic rules).
To accommodate he part-of-speechinput to the parser, the input sentence has to be part-of-speechtagged before parsing.
To produce an adequate translation out-put from the input containing parts-of-speech, there has to bea mechanism by which parts-of-speech are used for parsing pur-poses, and the corresponding lexical items are used for the se-mantic frame representation.5.1 In tegrat ion  o f  Ru le -Based  Par t -o f -SpeechTaggerTo accommodate he part-of-speech input to the parser, we haveintegrated the rule-based part-of-speech tagger, (Brill, 1992),(Brill, 1995), as a preprocessor to the language understandingsystem TINA, as in Figure 8.
An advantage of integrating apart-of-speech tagger over a lexicon containing part-of-speech in-formation is that only the former can tag words which are newto the system, and provides a way of handling unknown words.While most stochastic taggers require a large amount of train-ing data to achieve high rates of tagging accuracy, the rule-basedeThe parsing coverage of the semantic grammar, i.e.
34.8%,is after discounting the parsing failure due to words unknown tothe ~rammar.
The reason why we do not give the statistics of theparsing failure due to unknown words for the syntactic and themixed grammar is because the part-of-speech tagging process,which will be discussed in detail in Section 5, has the effect ofhandling unknown words, and therefore the problem does notarise.RULE-BASED \] I LANGUAGE I I LANGUAGE IPA RT-OF-SPEECI,-("~ UNDERSTANDiNGI-~ GENERATION I - '~ TEXTTAGGER I I TNA I I GENESIS I IOUTPUTIFigure 8: Integration of the Rule-Based Part-of-Speech Tag-ger as a Preprocessor to the Language Understanding Sys-temtagger achieves performance comparable to or higher than thatof stochastic taggers, even with a training corpus of a modestsize.
Given that the size of our training corpus is fairly small(total 7716 words), a transformation-based tagger is wellsuitedto our needs.The transformation-based part-of-speech tagger operates intwo stages.
Each word in the tagged training corpus has anentry in the lexicon consisting of a partially ordered list of tags,indicating the most likely tag for that word, and all other tagsseen with that word (in no particular order).
Every word is firstassigned its most likely tag in isolation.
Unknown words arefirst assumed to be nouns, and then cues based upon prefixes,suffixes, infixes, and adjacent word co-occurrences are used toupgrade the most likely tag.
Secondly, after the most likely tagfor each word is assigned, contextual transformations are used toimprove the accuracy.We have evaluated the tagger performance on the TEST Databoth before and after training on the MUC-II corpus.
The re-sults are given in Table 7.
Tagging statistics 'before training'are based on the lexicon and rules acquired from the BROWNCORPUS and the WALL STREET JOURNAL CORPUS.
Tag-~ ing statistics 'after training' are divided into two categories, oth of which are based on the rules acquired from training datasets of the MUC-II corpus.
The only difference between the twois that in one case (After Training I) we use a lexicon acquiredfrom the MUC-II corpus, and in the other case (After TrainingII) we use a lexicon acquired from a combination of the BROWNCORPUS, the WALL STREET JOURNAL CORPUS, and theMUC-II database.Training StatusBefore TrainingAfter Tralnin ~ IAfter Trainin ~ IITa~ging Accuracy1125/1287 (87.4%)1249/1287 /97%)1263/1287 (98%)Table 7: Tagger Evaluation on Data  Set TESTTable 7 shows that the tagger achieves a tagging accuracy ofup to 98% after training and using the combined lexicon, withan accuracy for unknown words ranging from 82 to 87%.
Thesehigh rates of tagging accuracy are largely due to two factors:(1) Combination of domain specific ontextual rules obtained bytraining the MUC-II corpus with general contextual rules ob-tained by training the WSJ corpus; And (2) Combination of theMUC-II lexicon with the lexicon for the WSJ corpus.5.2 Adaptat ion  of  the  Unders tand ing  SystemThe understanding system depicted in Figure 1 derives the se-mantic frame representation directly from the parse tree.
Theterminal symbols (i.e.
words in general) in the parse tree arerepresented asvocabulary items in the semantic frame.
Once weallow the parser to take part-of-speech as the input, the parts-of-speech (rather than actual words) will appear as the terminalsymbols in the parse tree, and hence as the vocabulary itemsin the semantic frame representation.
We adapted the system sothat the part-of-speech tags are used for parsing, but are replacedwith the original words in the final semantic frame.
Generationcan then proceed as usual.
Figures 9 and (11) illustrate the parsetree and semantic frame produced by the adapted system for theinput sentence 0819 z unknown contacts replied incorrectly.126I(?
'- TF,:'F'H,9":p re_ad junctitime_expressioni8mtmtlmeInumeric_tlmecaPdlnal gmtI0819 zsentenceiCull_parseistatementsub ject!Iq_npadjective nn,_head)1 l)u~known contactpredicatevp_rep iyvrepiy adverb_phraseIadvreplied ~n?cr rect lgFigure 9: Parse Tree Based on the Mix of Word and Part-of-Speech Sequence(11){c statement:time_expression {p numeric_time:topic {q gmt:name "z" }:pred {p cardinal:topic "0819" } }:topic {q nn_head:name "contact":pred {p --known:global 1 } }:subject 1:pred {p reply_v:mode "past":adverb {p incorrectly } } }6 SummaryIn this paper we have proposed atechnique which maximizes theparsing coverage and minimizes the misparse rate for machinetranslation of telegraphic messages.
The key to the technique isto adequately mix semantic and syntactic rules in the grammar.We have given experimental results of the proposed grammar,and compared them with the experimental results of a syntac-tic grammar and a semantic grammar with respect o parsingcoverage and misparse rate, which are summarized in Table 8and Table 9.
We have also discussed the system adaptation toaccommodate the proposed technique.Grammar Type Parsing Rate Misparse RateSemantic Grammar 34.8% 8.7%Syntactic Grammar 75.7% 29%Mixed Grammar 77% 10%Table 8: TEST Data Evaluation Results on the Three Typesof GrammarGrammar Type Farsin~ Rate Misparse RateSemantic Grammar 43.1% 14.6%Syntactic Grammar 76.5% 28%Mixed Grammar 82% 10%Table 9: TEST' Data Evaluation Results on the ThreeTypes of GrammarReferencesEric Brill.
1992.
A Simple Rule-Based Part of Speech Tagger.Proceedings of the Third Conference on Applied Natural Lan-guage Processing, ACL, Tcento, Italy.Eric Brill.
1995.
Transformation-Based Error-Driven Learningand Natural Language Processing: A Case Study in Part-of-Speech Tagging.
Computational Linguistics, 21-4, pages 543-565.Eric Brill and Philip Resnik.
1993 A Rule-Based Approachto Prepositional Phrase Attachment Disambiguation.
Techni-cal report, Department ofComputer and Information Science,University of Pennsylvania.James Glass, Joseph Polifroni and Stephanie Seneff.
1994.
Mul-tilingual Language Generation Across Multiple Domains.
Pre-sented at the 1994 International Conference on Spoken.
Lan-guage Processing, Yokohama, Japan.Ralph Grishman.
1989.
Analyzing Telegraphic Messages.
Pro-ceedings of Speech and Natural Language Workshop, DARPA.Stephanie Seneff.
1992.
TINA: A Natural Language System forSpoken Language Applications.
Computational Linguistics,18:1, pages 61-88.Beth M. Sundheim.
Navy Tactical Incident Reporting in aHighly Constrained Sublanguage: Examples and Analysis.Technical Document 1477, Naval Ocean Systems Center, SanDiego.Clifford Weinstein, Dinesh Tummala, Young-Suk Lee, StephanieSeneff.
1996.
Automatic Engish-to-Korean Text Translationof Telegraphic Messages ina Limited Domain.
To be presentedat the International Conference on Computational Linguistics'96.127
