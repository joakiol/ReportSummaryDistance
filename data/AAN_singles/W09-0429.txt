Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 160?164,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsEdinburgh?s Submission to all Tracks of the WMT2009 Shared Taskwith Reordering and Speed Improvements to MosesPhilipp Koehn and Barry HaddowSchool of InformaticsUniversity of Edinburghpkoehn@inf.ed.ac.uk bhaddow@inf.ed.ac.ukAbstractEdinburgh University participated in theWMT 2009 shared task using the Mosesphrase-based statistical machine transla-tion decoder, building systems for all lan-guage pairs.
The system configuration wasidentical for all language pairs (with a fewadditional components for the German-English language pairs).
This paper de-scribes the configuration of the systems,plus novel contributions to Moses includ-ing truecasing, more efficient decodingmethods, and a framework to specify re-ordering constraints.1 IntroductionThe commitment of the University of Edinburgh tothe WMT shared tasks is to provide a strong sta-tistical machine translation baseline with our opensource tools for all language pairs.
We are againthe only institution that participated in all tracks.The shared task is also an opportunity to incor-porate novel contributions and test them againstthe best machine translation systems for these lan-guage pairs.
In this paper we describe the speedimprovements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify re-ordering constraints with XML markup, which wetested with punctuation-based constraints.2 System ConfigurationWe trained a default Moses system with the fol-lowing non-default settings:?
maximum sentence length 80?
grow-diag-final-and symmetrization ofGIZA++ alignments?
interpolated Kneser-Ney discounted 5-gramlanguage model?
msd-bidrectional-fe lexicalized reorderingLanguage ep nc news intpl.English 449 486 216 192French 264 311 147 131German 785 821 449 402Spanish 341 392 219 190Czech *:1475 1615 752 690Hungarian hung:2148 815 786Table 1: Perplexity (ppl) of the domain-trained (ep= Europarl (CzEng for Czech), nc = News Com-mentary, news = News) and interpolated languagemodels.2.1 Domain AdaptationIn contrast to last year?s task, where news transla-tion was presented as a true out-of-domain prob-lem, this year large monolingual news corporaand a tuning set (last year?s test set) were pro-vided.
While still no in-domain news parallel cor-pora were made available, the monolingual cor-pora could be exploited for domain adaption.For all language pairs, we built a 5-gram lan-guage model, by first training separate languagemodels for the different training corpora (the par-allel Europarl and News Commentary and newmonolingual news), and then interpolated them byoptimizing perplexity on the provided tuning set.Perplexity numbers are shown in Table 1.2.2 TruecasingOur traditional method to handle case is to low-ercase all training data, and then have a separaterecasing (or recapitalization) step.
Last year, weused truecasing: all words are normalized to theirnatural case, e.g.
the, John, eBay, meaning thatonly sentence-leading words may be changed totheir most frequent form.To refine last year?s approach, we record theseen truecased instances and truecase words in testsentences (even in the middle of sentences) to seenforms, if possible.Truecasing leads to small degradation in case-160language pair baseline w/ news mbr/mp truecased big beam ued?08 best?08French-English uncased 21.2 23.1 23.3 22.7 22.9 19.2 21.9cased 21.7 21.6 21.8English-French uncased 17.8 19.4 19.6 19.6 19.7 18.2 21.4cased 18.1 18.7 18.8Spanish-English uncased 22.5 24.4 24.7 24.5 24.7 20.1 22.9cased 23.0 23.3 23.4English-Spanish uncased 22.4 23.9 24.2 23.8 24.4 20.7 22.7cased 22.1 22.8 23.1Czech-English uncased 16.9 18.9 18.9 18.6 18.6 14.5 14.7cased 17.3 17.4 17.4English-Czech uncased 11.4 13.5 13.6 13.6 13.8 9.6 11.9cased 12.2 13.0 13.2Hungarian-English uncased - 11.3 11.4 10.9 11.0 8.8cased 8.3 10.1 10.2English-Hungarian uncased - 9.0 9.3 9.2 9.5 6.5cased 8.1 8.4 8.7Table 2: Results overview for news-dev2009b sets: We see significant BLEU score increases with theaddition of news data to the language model and using truecasing.
As a comparison our results and thebest systems from last year on the full news-dev2009 set are shown.insensitive BLEU, but to a significant gain in case-sensitive BLEU.
Note that we still do not properlyaddress all-caps portions or headlines with our ap-proach.2.3 ResultsResults on the development sets are summarizedin Table 2.
We see significant gains with the addi-tion of news data to the language model (about 2BLEU points) and using truecasing (about 0.5?1.0BLEU points), and minor if any gains using min-imum Bayes risk decoding (mbr), the monotone-at-punctuation reordering constraint (mp, see Sec-tion 3.2), and bigger beam sizes.2.4 German?EnglishFor German?English, we additionally incorpo-ratedrule-based reordering ?
We parse the input us-ing the Collins parser (Collins, 1997) and ap-ply a set of reordering rules to re-arrange theGerman sentence so that it corresponds moreclosely English word order (Collins et al,2005).compound splitting ?
We split German com-pound words (mostly nouns), based on thefrequency of the words in the potential de-compositions (Koehn and Knight, 2003a).part-of-speech language model ?
We use fac-tored translation models (Koehn and Hoang,2007) to also output part-of-speech tags witheach word in a single phrase mapping and runa second n-gram model over them.
The En-German?English BLEU(ued?08: 17.1, best?08: 19.7) (uncased)baseline 16.6+ interpolated news LM 20.6+ minimum Bayes risk decoding 20.6+ monotone at punctuation 20.9+ truecasing 20.9+ rule-based reordering 21.7+ compound splitting 22.0+ part-of-speech LM 22.1+ big beam 22.3Table 3: Results for German?English with the in-cremental addition of methods beyond a baselinetrained on the parallel corpusEnglish?German BLEU(ued?08: 12.1, best?08: 14.2) (uncased)baseline 13.5+ interpolated news LM 15.2+ minimum Bayes risk decoding 15.2+ monotone at punctuation 15.2+ truecasing 15.2+ morphological LM 15.2+ big beam 15.7Table 4: Results for English?German with the in-cremental addition of methods beyiond a baselinetrained on the parallel corpusglish part-of-speech tags are obtained usingMXPOST (Ratnaparkhi, 1996).2.5 English-GermanFor English?German, we additionally incorpo-rated a morphological language model the sameway we incorporated a part-of-speech languagemodel in the other translation direction.
Themorphological tags were obtained using LoPar(Schmidt and Schulte im Walde, 2000).161&German-English & French-EnglishFigure 1: Early discarding results in speedier but still accurate search, compared to reducing stack size.3 Recent ImprovementsIn this section, we describe recent improvementsto the Moses decoder for the WMT 2009 sharedtask.3.1 Early DiscardingWe implemented in Moses a more efficient beamsearch, following suggestions by Moore and Quirk(2007).
In short, the guiding principle of this workis not to build a hypothesis and not to compute itslanguage model scores, if it is likely to be too badanyway.Before a hypothesis is generated, the followingchecks are employed:1. the minimum allowed score for a hypothesisis the worst score on the stack (if full) or thethreshold for the stack (if higher or stack notfull) plus an early discarding threshold cush-ion2.
if (a) new hypothesis future score, (b) the cur-rent hypothesis actual score, and (c) the fu-ture cost of the translation option are worsethan the allowed score, do not generate thehypothesis3.
if adding all real costs except for the languagemodel costs (i.e., reordering costs) makes thescore worse than the allowed score, do notgenerate the hypothesis.4.
complete generation of the hypothesis andadd it to the stackNote that check 1 and 2 mostly consists ofadding and comparing already computed values.In our implementation, step 3 implies the some-what costly construction of the hypothesis datastructure, while step 4 performs the expensivelanguage model calculation.
Without these opti-mizations, the decoder spends about 60-70% ofthe search time computing language model scores.With these optimization, the vast majority of po-tential hypotheses are not built.See Figure 1 for the time/search-accuracy trade-offs using this early discarding strategy.
Givena stack size, we can vary the threshold cushionmentioned in step 1 above.
A tighter threshold(the factor 1.0 implies no cushion at all), resultsin speedier but worse search.
Note, however, thatthe degradation in quality for a given time pointis less severe than the alternative ?
reducing thestack size (and also tightening the beam thresh-old, not shown in the figure).
To mention just twodata points in the German-English setting: Stacksize of 500 and early discarding threshold of 1.0results in faster search (150ms/word) and betterquality (73.5% search accuracy) than the defaultsearch setting of a stack size 200 and no early dis-carding (252ms/word for 62.5% seach accuracy).Accuracy is measured against the best translationsfound under any setting.Note that this early discarding is related to ideasbehind cube pruning (Huang and Chiang, 2007),which generates the top n most promising hy-potheses, but in our method the decision not togenerate hypotheses is guided by the quality of hy-potheses on the result stack.3.2 Framework to Specify ReorderingConstraintsCommonly in statistical machine translation,punctuation tokens are treated just like words.
Fortokens such as commas, many possible transla-tions are collected and they may be translated intoany of these choices or reordered if the languagemodel sees gains.
In fact, since the comma is one162'&$%Requiring the translation of quoted material as a block:He said <zone> " yes " </zone> .Hard reordering constraint:Number 1 : <wall/> the beginning .Local hard reordering constraint within zone:A new idea <zone> ( <wall/> maybe not new <wall/> ) </zone> has come forward .Nesting:The <zone> " new <zone> ( old ) </zone> " </zone> proposal .Figure 2: Framework to specify reordering constraints with zones and walls.
Words within zones haveto be translated without reordering with outside material.
Walls form hard reordering constraints, overwhich words may not be reordered (limited to zones, if defined within them).the most frequent tokens in a corpus and not veryconsistently translated across languages, it has avery noisy translation table, often with 10,000s ifnot 100,000s of translations.Punctuation has a meaningful role in structur-ing a sentence, and we see some gains exploitingthis in the systems we built last year.
By dis-allowing reordering over commas and sentence-ending punctuation, we avoid mixing words fromdifferent clauses, and typically see gains of 0.1?0.2 BLEU.But also other punctuation tokens imply re-ordering constraints.
Parentheses, brackets, andquotation marks typically define units that shouldbe translated as blocks, meaning that words shouldnot be moved in or out of sequences in quotes andalike.To handle such reordering constraints, we intro-duced a framework that uses what we call zonesand walls.
A zone is a sequence of words thatshould be translated as block.
This does not meanthat the sequence cannot be reordered as a whole,but that once we start to translate words in a zone,we have to finish all its words before moving out-side again.
To put it another way: words may notreordered into or out of zones.A wall is a hard reordering constraint that re-quires that all words preceeding it have to be trans-lated before words after may be translated.
If wespecify walls within zones, then we consider themlocal walls where the before-mentioned constraintonly applies within the zone.Walls and zones may be specified with XMLmarkup to the Moses decoder.
See Figure 2 for afew examples.
We use the extended XML frame-work to1.
limit reordering of clause-ending punctuation(walls)2. define zones for quoted and parentheticalword sequences3.
limit reordering of quotes and parentheses(local walls within zones)4. specify translations for punctuation (notcomma).Only (1) leads to any noticable change in BLEU inthe WMT 2009 shared task, a slight gain 0.1?0.2.Note that this framework may be used in otherways.
For instance, we may want to revisitour work on noun phrase translation (Koehn andKnight, 2003b), and check if enforcing the trans-lation of noun phrases as blocks is beneficialor harmful to overall machine translation perfor-mance.AcknowledgementsThis work was supported by the EuroMatrixproject funded by the European Commission (6thFramework Programme) and made use of the re-sources provided by the Edinburgh Compute andData Facility (http://www.ecdf.ed.ac.uk/).The ECDF is partially supported by the eDIKTinitiative (http://www.edikt.org.uk/).ReferencesCollins, M. (1997).
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings ofthe 35th Annual Meeting of the Association ofComputational Linguistics (ACL).Collins, M., Koehn, P., and Kucerova, I.
(2005).Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual163Meeting of the Association for ComputationalLinguistics (ACL?05), pages 531?540, Ann Ar-bor, Michigan.
Association for ComputationalLinguistics.Huang, L. and Chiang, D. (2007).
Forest rescor-ing: Faster decoding with integrated languagemodels.
In Proceedings of the 45th AnnualMeeting of the Association of ComputationalLinguistics, pages 144?151, Prague, Czech Re-public.
Association for Computational Linguis-tics.Koehn, P. and Hoang, H. (2007).
Factored trans-lation models.
In Proceedings of the 2007Joint Conference on Empirical Methods in Nat-ural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL),pages 868?876.Koehn, P., Hoang, H., Birch, A., Callison-Burch,C., Federico, M., Bertoldi, N., Cowan, B., Shen,W., Moran, C., Zens, R., Dyer, C. J., Bo-jar, O., Constantin, A., and Herbst, E. (2007).Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45thAnnual Meeting of the Association for Com-putational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic.
Asso-ciation for Computational Linguistics.Koehn, P. and Knight, K. (2003a).
Empiricalmethods for compound splitting.
In Proceed-ings of Meeting of the European Chapter ofthe Association of Computational Linguistics(EACL).Koehn, P. and Knight, K. (2003b).
Feature-richtranslation of noun phrases.
In 41st AnnualMeeting of the Association of ComputationalLinguistics (ACL).Moore, R. C. and Quirk, C. (2007).
Faster beam-search decoding for phrasal statistical machinetranslation.
In Proceedings of the MT SummitXI.Ratnaparkhi, A.
(1996).
A maximum entropy part-of-speech tagger.
In Proceedings of the Empir-ical Methods in Natural Language ProcessingConference.Schmidt, H. and Schulte im Walde, S. (2000).
Ro-bust German noun chunking with a probabilisticcontext-free grammar.
In Proceedings of the In-ternational Conference on Computational Lin-guistics (COLING).164
