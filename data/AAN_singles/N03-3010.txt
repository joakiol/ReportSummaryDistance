Cooperative Model Based Language Understanding in DialogueDonghui FengInformation Sciences Institute,University of Southern California4676 Admiralty WayMarina Del Rey, CA 90292-6695donghui@isi.eduAbstractIn this paper, we propose a novelCooperative Model for natural languageunderstanding in a dialogue system.
Webuild this based on both Finite State Model(FSM) and Statistical Learning Model(SLM).
FSM provides two strategies forlanguage understanding and have a highaccuracy but little robustness and flexibility.Statistical approach is much more robustbut less accurate.
Cooperative Modelincorporates all the three strategies togetherand thus can suppress all the shortcomingsof different strategies and has all theadvantages of the three strategies.1 IntroductionIn this paper, we propose a novel languageunderstanding approach, Cooperative Model, for adialogue system.
It combines both Finite State Modeland Statistical Learning Model for sentenceinterpretation.This approach is implemented in the project MRE(Mission Rehearsal Exercise).
The goal of MRE is toprovide an immersive learning environment in whicharmy trainees experience the sights, sounds andcircumstances they will encounter in real-worldscenarios (Swartout et al, 2001).
In the wholeprocedure, language processing part plays the role tosupport the communication between trainees andcomputers.In the language processing pipeline, audio signalsare first transformed into natural language sentences byspeech recognition.
Sentence interpretation part is usedto ?understand?
the sentence and extract an informationcase frame for future processing such as dialoguemanagement and action planning.
We adopt theCooperative Model as the overall frame of sentenceinterpretation, which incorporates two mainly usedlanguage processing approaches: the Finite State Modeland the Statistical Learning Model.
Currently there isrelatively little work on the cooperation of the two kindsof models for language understanding.The Cooperative Model has great advantages.
Itbalances the shortcomings of each separate model.
It iseasy to implement the parsing algorithm and get theexact expected result for finite state model (FSM) butit?s difficult and tedious to design the finite statenetwork by hand.
Also, the finite state model is not toorobust and the failure of matching produces no results.On the other hand, statistical learning model (SLM) candeal with unexpected cases during designing andtraining by giving a set of candidate results withconfidence scores.
It is a must to provide some kind ofrules to select results needed.
However, applying it maynot give a completely satisfactory performance.The rest of this paper is organized as follows:Section 2 describes the case frame as the semanticrepresentation produced by the cooperative model.
Insection 3, we explain our cooperative languageunderstanding model and discuss two differentstrategies of the Finite State Model and the StatisticalLearning Model.
We analyze the experimental results inSection 4.
Section 5 concludes with on-going researchand future work.2 Semantic RepresentationThe goal of automated natural language understandingis to parse natural language string, extract meaningfulinformation and store them for future processing.
Forour application of training environment, it?s impossibleto parse sentences syntactically and we here directlyproduce the nested information frames as output.
Thetopmost level of the information frame is defined asfollows:Figure 1.
Topmost-Level Information FrameIn the definition, <semantic-object> consists of<i-form> := ( ^mood <mood>^sem <semantic-object>)three types: question, action and proposition.
Here,question refers to requests for information, action refersto orders and suggestions except requests, and all therest falls into the category of proposition.Each of these types can also be furtherdecomposed as Figure 2 and 3.Figure 2.
Second-Level Information FrameFigure 3.
Third-Level Information FrameThese information frames can be further extendedand nested as necessary.
In our application, most of theinformation frames obtained contain at most three levels.In Figure 4, we give an example of information framefor the English sentence ?who is not critically hurt?
?.All the target information frames in our domain aresimilar to that format.Figure 4.
Example Nested Information FrameSince the information frames are nested, for thestatistical learning model to be addressed, ideally boththe semantic information and structural informationshould be represented correctly.
Therefore we use prefixstrings to represent the cascading level of eachslot-value pair.
The case frame in Figure 4 can bere-represented as shown in Figure 5.
Here we assumethat the slots in the information frame are independentof each other.
Reversely the set of meaning items can berestored to a normal nested information frame.Figure 5.
Re-representation to handle cascadingWe introduce the cooperative model in thefollowing section to extract meaningful informationframes for all the English sentences in our domain.3 Cooperative ModelThe Cooperative Model (CM) combines twocommonly-used methods in natural language processing,Finite State Model (FSM) and Statistical LearningModel (SLM).
We discuss them in section 3.1 and 3.2respectively.3.1 Finite State ModelThe main idea of finite state model is to put all thepossible input word sequences and their related outputinformation on the arcs.For our application, the input is a string composedof a sequence of words, and the output should be acorrectly structured information frame.
We apply twostrategies of FSM.
The Series Mode refers to build aseries of finite state machine with each corresponding toa single slot.
The Single Model builds only one complexFinite State Machine that incorporates all the sentencepatterns and slot-value pairs.3.1.1 Strategy I: Series Model of Finite StateMachineFor this strategy, we analyze our domain to obtain a listof all possible slots.
From the perspective of linguistics,a slot can be viewed as characterized by some specificwords, say, a set of feature words.
We therefore canmake a separate semantic filter for each slot.
Eachsentence passes through a series of filters and as soon as<question> := (  ^type question^q-slot <prop-slot-name>^prop <proposition>)<action>  := (  ^type action-type^name <event-name>^<prop-slot-name> <val>)<proposition> := <state> | <event> | <relation><state> :=  (  ^type state^object-id ID^polarity <pol>?
)<event>  := (  ^type event-type^name <event-name>^<prop-slot-name> <val>?
)<relation> := (  ^type relation^relation <rel-name>^arg1 <semantic-object>^arg2 <semantic-object>)<i> ^mood interrogative<i> ^sem <t0><i> <t0> ^type question<i> <t0> ^q-slot agent<i> <t0> ^prop <t1><i> <t0> <t1> ^type event-type<i> <t0> <t1> ^time present<i> <t0> <t1> ^polarity negative<i> <t0> <t1> ^degree critical-injuries<i> <t0> <t1> ^attribute health-status<i> <t0> <t1> ^value health-badInput Sentence:  who is not critically hurt?Output Information Frame:(<i>  ^mood interrogative^sem <t0>)(<t0> ^type question^q-slot agent^prop <t1>)(<t1> ^type event-type^time present^polarity negative^degree critical-injuries^attribute health-status^value health-bad)we find the ?feature?
words, we extract theircorresponding slot-value pairs.
All the slot-value pairsextracted produce the final nested case frame.Sentence                   Information FrameFigure 6.
An Example from Series Model of FSMFigure 6 is an example of the way that seriesmodel of finite state machine works.
For example, threeslot-value pairs are extracted from the word ?who?.Practically, we identified 27 contexts and built 27 finitestate machines as semantic filters, with each oneassociated with a set of feature words.
The number ofarcs for each finite state machine ranges from 4 to 70and the size of the feature word set varies from 10 to 50.This strategy extracts semantic information basedon the mapping between words and slots.
It is relativelyeasy to design the finite state machine networks andimplement the parsing algorithm.
For every inputsentence it will provide all possible information usingthe predefined mappings.
Even if the sentence containsno feature words, the system will end gracefully with anempty frame.
However, this method doesn?t take intoaccount the patterns of word sequences.
Single wordmay have different meanings under different situations.In most cases it is also difficult to put one word into onesingle class; sometimes a word can even belong todifferent slots?
feature word sets that can contradict eachother.
On the other hand, the result produced may havesome important slot-value pairs missed and the numberof slots is fixed.3.1.2 Strategy II: Single Model of Finite StateMachineIn this strategy we only build a big finite state network.When a new sentence goes into the big FSM parser, itstarts from ?START?
state and a successful matching ofprespecified patterns or words will move forward toanother state.
Any matching procedure coming to the?END?
state means a successful parsing of the wholesentence.
And all the outputs on the arcs along the pathcompose the final parsing result.
If no patterns or wordsare successfully matched at some point, the parser willdie and return failure.This strategy requires all the patterns to beprocessed with this finite state model available beforedesigning the finite state network.
The target sentenceset includes 65 sentence patterns and 23 classes ofwords and we combine them into a complex finite statenetwork manually.
Figure 7 gives some examples of thecollected sentence patterns and word classes.Figure 7.
Target Sentence PatternsAimed at processing these sentences, we designour finite state network consisting of 128 states.
Thisnetwork covers more than 20k commonly-usedsentences in our domain.
It will return the exact parsingresult without missing any important information.
If allof the input sentences in the application belong to thetarget sentence set of this domain, this approachperfectly produces all of the correct results.
However,the design of the network is done totally by hand, whichis very tedious and time-consuming.
The system is notvery flexible or robust and it?s difficult to add newsentences into the network before a thoroughinvestigation of the whole finite state network.
It is notconvenient and efficient for extension and maintenance.Finite state models can?t process any sentence withnew sentence patterns.
However in reality most systemsrequire more flexibility, robustness, and more powerfulprocessing abilities on unexpected sentences.
Thestatistical machine learning model gives us some lighton that.
We discuss learning models in Section 3.2.3.2 Statistical Learning Model3.2.1 Na?ve Bayes LearningNa?ve Bayes learning has been widely used in naturallanguage processing with good results such as statisticalsyntactic parsing (Collins, 1997; Charniak, 1997),hidden language understanding (Miller et al, 1994).We represent the mappings between words andtheir potential associated meanings (meaning itemsincluding level information and slot-value pairs) withP(M|W).
W refers to words and M refers to meaningitems.
With Bayes?
theorem, we have the formula 3.1.P(W)P(M) * M)|P(Wmaxarg W)|P(Mmaxarg =    (3.1)Here P(W|M) refers to the probability of wordsgiven their meanings.whoisdrivingthecar(<i>^mood interrogative^sem  <t0>)(<t0>^type question^q-slot agent^prop <t1>)(<t1>^type action^event drive^patient car^time present)$phrase1 = what is $agent doing;$phrase2 = [and|how about] (you|me|[the]$vehicle|$agent);?$agent = he|she|$people-name|[the] ($person_civ |$person_mil| $squad);$vehicle = ambulance | car | humvee | helicopter|medevac;?In our domain, we can view P (W) as a constantand transform Formula 3.1 to Formula 3.2 as follows:P(M)*M)|P(WmaxargW)|P(Mmaxargmm=     (3.2)3.2.2 Training Set and Testing SetWe created the training sentences and case frames byrunning full range of variation on Finite State Machinedescribed in Section 3.1.2.
This gives a set of 20, 677sentences.
We remove ungrammatical sentences andhave 16,469 left.
Randomly we take 7/8 of that as thetraining set and 1/8 as the testing set.3.2.3 Meaning ModelThe meaning model P(M) refers to the probability ofmeanings.
In our application, meanings are representedby meaning items.
We assume each meaning item isindependent of each other at this point.
In the meaningmodel, the meaning item not only includes slot-valuepairs but level information.
Let C(mi) be the number oftimes the meaning item mi appears the training set, weobtain P(M) as follows:?== nj 1jii)C(m)C(m)P(m(3.3)This can be easily obtained by counting all themeaning items of all the information frames in thetraining set.3.2.4 Word ModelIn the na?ve Bayes learning approach, P(W|M) standsfor the probability of words appearing under givenmeanings.
And from the linguistic perspective, thepatterns of word sequences can imply stronginformation of meanings.
We introduce a languagemodel based on a Hidden Markov Model (HMM).
Theword model can be described as P (wi | mj, wi-2wi-1), P(wi | mj, wi-1) or P (wi | mj) for trigram model, bigrammodel, and unigram model respectively.
They can becalculated with the following formulas:)w w,m(#)w w,m(#)w w,m|P(w1-i2-ij1-i2-ij1-i2-iji ofwof i=   (3.4)) w,m(#) w,m(#)w,m|P(w1-ij1-ij1-iji ofwof i=       (3.5))m(#) ,m(#)m|P(wjjji ofwof i=            (3.6)3.2.5 Weighted Sum Voting and PruningWe parse each sentence based on the na?ve Bayeslearning Formula 3.2.
Each word in the sentence can beassociated with a set of candidate meaning items.
Thenwe normalize each candidate set of meaning items anduse the voting schema to get the final result set with aprobability for each meaning item.However, this inevitably produces noisy results.Sometimes the meanings obtained even contradict otheruseful meaning items.
We employ two cutoff strategiesto eliminate such noise.
The first is to cut offunsatisfactory meaning items based on a gap inprobability.
The degree of jump can be defined with anarbitrary threshold value.
The second is to group all theslot-value pairs with the same name and take the top oneas the result.3.3 Cooperative MechanismIn the previous two sections, we discussed twoapproaches in our natural language understandingsystem.
However, neither is completely satisfactory.Cooperative Model can combine all threeapproaches from these two models.
The main idea is torun the three parsing models together whenever a newsentence comes into the system.
With the statisticallearning model, we obtain a set of information frames.For the result we get from single model of finite statemachine, if an information frame exists, it means thesentence is stored in the finite state network.
Wetherefore assign a score 1.0.
The result should be noworse than any information frame we get fromstatistical learning model.
Otherwise, it means thissentence is not stored in our finite state work, we canignore this result.
In the end, we combine thisinformation frame with the frame set from statisticallearning model and rank them according to theconfidence scores.
Generally we can consider the onewith the highest confidence score as our parsing result.The cooperative model takes all advantages of thethree methods and combines them together.
Thecooperative mechanism also suppresses thedisadvantages of those methods.
The series model of thefinite state machine has the advantage of mappingbetween word classes and contexts, though it sometimesmay lose some information, and it contains realsemantic knowledge.
The statistical learning model canproduce a set of information frames based on the wordpatterns and its noise can be removed by the result ofthe series model of the finite state machine.
For thesingle finite state machine model, if it can parsesentence successfully, the result will always be the bestone.
Therefore through the cooperation of the threemethods, it can either produce the exact result forsentences stored in the finite state network or return themost probable result through statistical machinelearning method if no sentence matching occurs.
Alsothe noise is reduced by the other finite state machinemodel.
The cooperative model is robust and has theability to learn in our target domain.4 Experimental ResultsThe cooperative model will demonstrate its ability onsentence processing no matter whether the sentence is inthe original sentence set.
However, currently we onlyhave simple preference rule for the cooperation andhaven?t obtained the overall performance.
In this section,we?ll compare the different models?
performance todemonstrate the cooperative model?s potential ability.Based on our target sentence patterns and wordclasses, we built a blind set with 159 completely newsentences.
Although all the words used belong to thisdomain these sentences don?t appear in the training setand the testing set.
In the evaluation of its performance,we compare the results of the three approaches and getTable 1.
As we can see from this table, finite statemethod is better in the relative processing speed and forprocessing existing patterns while statistical model isbetter for processing new sentence patterns, whichmakes the system very robust.Sentencesin DomainSpeed ExistingPatternsNewPatternsSeriesofFSMFixed Fast 100% PartialResultSingleFSMFixed Fast 100% DieStatModelOpen Slow 85%(pre)95%(rec)75%(pre)92%(rec)Table 1.
Results ComparisonOn the other hand, we investigate the performanceof statistical model in more detail on the blind test.Given the whole blind testing set, the statistical learningmodel produced 159 partially correct informationframes.
We manually corrected them one by one.
Thistook us 97 minutes in total.
To measure this efficiency,we also built all the real information frames for theblind test set manually, one by one.
It took 366 minutesto finish all the 159 information frames.
This means it ismuch more efficient to process a completely newsentence set with the statistical learning model.We next investigate the precision and recall of thisstatistical learning model.
Taking the result frames wemanually built as the real answers, we define precision,recall, and F-score to measure the system?sperformance.model learning from pairs value-slot of #pairs value-slotcorrect  of #precsion =answer real from pairs value-slot of #pairs value-slotcorrect  of #recall =recall precision)recall *precision (*2F_Score +=Our testing strategy is to randomly select someportion of the new blind set and add it into the trainingset.
Then we test the system with sentences in the rest ofthe blind set.
As more and more new sentences areadded into the training set (1/4, 1/3, 1/2, etc) we can seethe performance changing accordingly.
We investigatethe three models: P(M|W), P(W|M) and P(M)*P(M|W).All of them are tested with same testing strategy.Portion 0 1/4 1/3 1/2 2/3Prec 0.7131 0.7240 0.7243 0.7311 0.7370Rec 0.8758 0.8909 0.8964 0.9133 0.9254F-Score 0.7815 0.7943 0.7966 0.8073 0.8152Table 2.
Result of P (M|W)Portion 0 1/4 1/3 1/2 2/3Prec 0.7218 0.7416 0.7444 0.7429 0.7540Rec 0.8871 0.9161 0.9276 0.9270 0.9386F-Score 0.7913 0.8147 0.8208 0.8197 0.8304Table 3.
Result of P (W|M)Portion 0 1/4 1/3 1/2 2/3Prec 0.7545 0.7693 0.7704 0.7667 0.7839Rec 0.8018 0.8296 0.8407 0.8372 0.8323F-Score 0.7745 0.7950 0.8021 0.7985 0.8035Table 4.
Result of P (W|M) * P (M)From the three tables, we can see that as newsentences are added into the training set, theperformance improves.
Comparing Tables 2, 3 and 4,the poor performance of P (W|M)* P (M) is partially dueto unbalance in the training set.
The higher occurrencesof some specific meaning items increase P(M) andaffect the result during voting.5 ConclusionsIn this paper we proposed a cooperative modelincorporating finite state model and statistical model forlanguage understanding.
It takes all of their advantagesand suppresses their shortcomings.
The successfulincorporation of the methods can make our system veryrobust and scalable for future use.We notice that the series model of the finite statemachine model actually incorporates some semanticknowledge from human beings.
Ongoing research workincludes finding new ways to integrate semanticknowledge to our system.
For the statistical learningmodel, the quality and the different configurations oftraining set highly affect the performance of modelstrained and thus their abilities to process sentences.
Thebalance of training set is also a big issue.
How to build abalanced training set with single finite state machinemodel will remain our important work in the future.
Forthe learning mechanism, Na?ve Bayesian learningrequires more understanding of different factors?
rolesand their importance.
These problems should beinvestigated in future work.AcknowledgementsThe author would like to thank Deepak Ravichandranfor his invaluable help of the whole work.ReferencesEugene Charniak.
1997.
Statistical Parsing with a Context-freeGrammar and Word Statistics.
Proc.
Of AAAI-97.
pp.598-603.M.
Collins.
1997.
Three Generative, Lexicalised Models forStatistical Parsing.
Proc.
of the 35th ACL.S.
Miller, R. Bobrow, R. Ingria, and R. Schwartz.
1994.Hidden Understanding Models of Natural Language,"Proceedings of the Association of ComputationalLinguistics, pp.
25-32.W.
Swartout, et al 2001.
Toward the Holodeck: IntegratingGraphics, Sound, Character and Story.
Proceedings of 5thInternational Conference on Autonomous Agents
