Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377?1381,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImproving Statistical Machine Translation with Word Class ModelsJoern Wuebker, Stephan Peitz, Felix Rietig and Hermann NeyHuman Language Technology and Pattern Recognition GroupRWTH Aachen UniversityAachen, Germany<surname>@cs.rwth-aachen.deAbstractAutomatically clustering words from a mono-lingual or bilingual training corpus intoclasses is a widely used technique in statisti-cal natural language processing.
We presenta very simple and easy to implement methodfor using these word classes to improve trans-lation quality.
It can be applied across differ-ent machine translation paradigms and witharbitrary types of models.
We show its ef-ficacy on a small German?English and alarger French?German translation task withboth standard phrase-based and hierarchicalphrase-based translation systems for a com-mon set of models.
Our results show that withword class models, the baseline can be im-proved by up to 1.4% BLEU and 1.0% TERon the French?German task and 0.3% BLEUand 1.1% TER on the German?English task.1 IntroductionData sparsity is one of the major problems for statis-tical learning methods in natural language process-ing (NLP) today.
Even with the huge training datasets available in some tasks, for many phenomenathat need to be modeled only few training instancescan be observed.
This is partly due to the large vo-cabularies of natural languages.
One possiblity toreduce the sparsity for model estimation is to re-duce the vocabulary size.
By clustering the vocab-ulary into a fixed number of word classes, it is pos-sible to train models that are less prone to sparsityissues.
This work investigates the performance ofstandard models used in statistical machine transla-tion when they are trained on automatically learnedword classes rather than the actual word identities.In the popular tooklit GIZA++ (Och and Ney,2003), word classes are an essential ingredient tomodel alignment probabilities with the HMM orIBM translation models.
It contains the mkcls tool(Och, 1999), which can automatically cluster the vo-cabulary into classes.Using this tool, we propose to re-parameterize thestandard models used in statistical machine transla-tion (SMT), which are usually conditioned on wordidentities rather than word classes.
The idea is thatthis should lead to a smoother distribution, whichis more reliable due to less sparsity.
Here, we fo-cus on the phrase-based and lexical channel modelsin both directions, simple count models identifyingfrequency thresholds, lexicalized reordering modelsand an n-gram language model.
Although our re-sults show that it is not a good idea to replace theoriginal models, we argue that adding them to thelog-linear feature combination can improve transla-tion quality.
They can easily be computed for dif-ferent translation paradigms and arbitrary models.Training and decoding is possible without or withonly little change to the code base.Our experiments are conducted on a medium-sized French?German task and a smallGerman?English task and with both phrase-based and hierarchical phrase-based translationdecoders.
By using word class models, we canimprove our respective baselines by 1.4% BLEU and1.0% TER on the French?German task and 0.3%BLEU and 1.1% TER on the German?English task.Training an additional language model for trans-1377lation based on word classes has been proposed in(Wuebker et al 2012; Mediani et al 2012; Koehnand Hoang, 2007).
In addition to the reduced spar-sity, an advantage of the smaller vocabulary is thatlonger n-gram context can be modeled efficiently.Mathematically, our idea is equivalent to a specialcase of the Factored Translation Models proposedby Koehn and Hoang (2007).
We will go into moredetail in Section 4.
Also related to our work, Cherry(2013) proposes to parameterize a hierarchical re-ordering model with sparse features that are condi-tioned on word classes trained with mkcls.
How-ever, the features are trained with MIRA rather thanestimated by relative frequencies.2 Word Class Models2.1 Standard ModelsThe translation model of most phrase-based and hi-erarchical phrase-based SMT systems is parameter-ized by two phrasal and two lexical channel models(Koehn et al 2003) which are estimated as relativefrequencies.
Their counts are extracted heuristicallyfrom a word aligned bilingual training corpus.In addition to the four channel models, our base-line contains binary count features that fire, if theextraction count of the corresponding phrase pair isgreater or equal to a given threshold ?
.
We use thethresholds ?
= {2, 3, 4}.Our phrase-based baseline contains the hierarchi-cal reordering model (HRM) described by Galleyand Manning (2008).
Similar to (Cherry et al2012), we apply it in both translation directionswith separate scaling factors for the three orientationclasses, leading to a total of six feature weights.An n-gram language model (LM) is another im-portant feature of our translation systems.
Thebaselines apply 4-gram LMs trained by the SRILMtoolkit (Stolcke, 2002) with interpolated modifiedKneser-Ney smoothing (Chen and Goodman, 1998).The smaller vocabulary size allows us to efficientlymodel larger context, so in addition to the 4-gramLM, we also train a 7-gram LM based on wordclasses.
In contrast to an LM of the same size trainedon word identities, the increase in computational re-sources needed for translation is negligible for the7-gram word class LM (wcLM).2.2 TrainingBy replacing the words on both source and targetside of the training data with their respective wordclasses and keeping the word alignment unchanged,all of the above models can easily be trained con-ditioned on word classes by using the same trainingprocedure as usual.
We end up with two separatemodel files, usually in the form of large tables, onewith word identities and one with classes.
Next, wesort both tables by their word classes.
By walkingthrough both sorted tables simultaneously, we canthen efficiently augment the standard model file withan additonal feature (or additional features) based onword classes.
The word class LM is directly passedon to the decoder.2.3 DecodingThe decoder searches for the best translation givena set of models hm(eI1, sK1 , fJ1 ) by maximizing thelog-linear feature score (Och and Ney, 2004):e?I?1 = argmaxI,eI1{M?m=1?mhm(eI1, sK1 , fJ1 )}, (1)where fJ1 = f1 .
.
.
fJ is the source sentence, eI1 =e1 .
.
.
eI the target sentence and sK1 = s1 .
.
.
sK thehidden alignment or derivation.All the above mentioned models can easily be in-tegrated into this framework as additional featureshm.
The feature weights ?m are tuned with mini-mum error rate training (MERT) (Och, 2003).3 Experiments3.1 DataOur experiments are performed on aFrench?German task.
In addition to someproject-internal data, we train the system on the dataprovided for the WMT 2012 shared task1.
Both thedev and the test set are composed of a mixtureof broadcast news and broadcast conversationscrawled from the web and have two references.Table 1 shows the data statistics.To confirm our results we also run experimentson the German?English task of the IWSLT 2012evaluation campaign2.1http://www.statmt.org/wmt12/2http://hltc.cs.ust.hk/iwslt/1378French Germantrain Sentences 1.9MRunning Words 57M 50Mdev Sentences 1900Running Words 61K 55Ktest Sentences 2037Running Words 60K 54KTable 1: Corpus statistics for the French?German task.The running word counts for the German side of dev andtest are averaged over both references.3.2 SetupIn the French?German task, our baseline is a stan-dard phrase-based system augmented with the hier-archical reordering model (HRM) described in Sec-tion 2.1.
The language model is a 4-gram LMtrained on all German monolingual sources providedfor WMT 2012.
For the class-based models, werun mkcls on the source and target side of thebilingual training data to cluster the vocabulary into100 classes each.
This clustering is used to trainthe models described above for word classes on thesame training data as their counterparts based onword identity.
This also holds for the wcLM, whichis a 4-gram LM trained on the same data as the base-line LM.
Further, the smaller vocabulary allows usto build an additional wcLM with a 7-gram contextlength.
On this task we also run additional experi-ments with 200 and 500 classes.On the German?English task, we evaluate ourmethod for both a standard phrase-based and the hi-erarchical phrase-based baseline.
Again, the phrase-based baseline contains the HRM model.
As bilin-gual training data we use the TED talks, which wecluster into 100 classes on both source and targetside.
The 4-gram LM is trained on the TED, Eu-roparl and news-commentary corpora.
On this dataset, we directly use a 7-gram wcLM.In all setups, the feature weights are optimizedwith MERT.
Results are reported in BLEU (Pap-ineni et al 2002) and TER (Snover et al 2006),confidence level computation is based on (Koehn,2004).
Our experiments are conducted with the opensource toolkit Jane (Wuebker et al 2012; Vilar etal., 2010).dev testBLEU TER BLEU TER[%] [%] [%] [%]-TM +wcTM 21.2 64.2 24.7 59.5-LM +wcLM 22.2 62.9 25.9 58.9-HRM +wcHRM 24.6 61.9 27.5 58.1phrase-based 24.6 61.8 27.8 57.6+ wcTM 24.7 61.4 28.1 57.1+ wcLM 24.9 61.2 28.4 57.1+ wcHRM 25.4?
60.9?
28.9?
56.9?+ wcLM7 25.5?
60.7?
29.2?
56.6?+ wcModels200 25.5?
60.8?
29.3?
56.4?+ wcModels500 25.2?
60.8?
29.0?
56.6?Table 2: BLEU and TER results on the French?Germantask.
Results marked with ?
are statistically significantwith 95% confidence, results marked with ?
with 90%confidence.
-X +wcX denote the systems, where themodel X in the baseline is replaced by its word classcounterpart.
The 7-gram word class LM is denotedas wcLM7.
wcModelsX denotes all word class modelstrained on X classes.3.3 ResultsResults for the French?German task are given inTable 2.
In a first set of experiments we replaced oneof the standard TM, LM and HRM models by thesame model based on word classes.
Unsurprisingly,this degrades performance with different levels ofseverity.
The strongest degradation can be seenwhen replacing the TM, while replacing the HRMonly leads to a small drop in performance.
However,when the word class models are added as additionalfeatures to the baseline, we observe improvements.The wcTM yields 0.3% BLEU and 0.5% TER ontest.
By adding the 4-gram wcLM, we get another0.3% BLEU and the wcHRM shows further improve-ments of 0.5% BLEU and 0.2% TER.
Extending thecontext length of the wcLM to 7-grams gives an ad-ditional boost, reaching a total gain over the baselineof 1.4% BLEU and 1.0% TER.
Using 200 classesinstead of 100 seems to perform slightly better ontest, but with 500 classes, translation quality de-grades again.On the German?English task, the results shownin Table 3 are similar in TER, but less pronouncedin BLEU.
Here we are able to improve over thephrase-based baseline by 0.3% BLEU and 1.1% TER1379dev testBLEU TER BLEU TER[%] [%] [%] [%]phrase-based 30.2 49.6 28.6 51.6+ wcTM 30.2 49.2 28.9 51.3+ wcLM7 30.5 48.3?
29.0 50.6?+ wcHRM 30.8 48.3?
28.9 50.5?hiero 29.6 50.3 27.9 52.5+ wcTM 29.8 50.3 28.1 52.3+ wcLM7 30.0 49.8 28.2 51.7Table 3: BLEU and TER results on the German?Englishtask.
Results marked with ?
are statistically significantwith 95% confidence, results marked with ?
with 90%confidence.by adding the wcTM, the 7-gram wcLM and thewcHRM.
With the hierarchical decoder we gain0.3% BLEU and 0.8% TER by adding the wcTM andthe 7-gram wcLM.4 Equivalence to Factored TranslationKoehn and Hoang (2007) propose to integrate differ-ent levels of annotation (e.g.
morphologial analysis)as factors into the translation process.
Here, the sur-face form of the source word is analyzed to producethe factors, which are then translated and finally thesurface form of the target word is generated from thetarget factors.
Although the translations of the fac-tors operate on the same phrase segmentation, theyare assumed to be independent.
In practice this isdone by phrase expansion, which generates a jointphrase table as the cross product from the phrase ta-bles of the individual factors.In contrast, in this work each word is mapped toa single class, which means that when we have se-lected a translation option for the surface form, thetarget side on the word class level is predetermined.Thus, no phrase expansion or generation steps arenecessary to incorporate the word class information.The phrase table can simply be extended with addi-tional scores, keeping the set of phrases constant.Although the implementation is simpler, our ap-proach is mathematically equivalent to a specialcase of the factored translation framework, which isshown in Figure 1.
The generation step from targetword e to its target class c(e) assigns all probabilityInput Outputword f word eclass c(f) class c(e)analysistranslationtranslationgenerationFigure 1: The factored translation model equivalent toour approach.
The generation step assigns all probabilitymass to a single event: pgen(c(e)|e) = 1.mass to a single event:pgen(c|e) ={1, if c = c(e)0, else(2)5 ConclusionWe have presented a simple and very easy to im-plement method to make use of word clusters forimproving machine translation quality.
It is appli-cable across different paradigms and for arbitrarytypes of models.
Depending on the model type,it requires little or no change to the training anddecoding software.
We have shown the efficacyof this method on two translation tasks and withboth the standard phrase-based and the hierarchi-cal phrase-based translation paradigm.
It was ap-plied to relative frequency translation probabilities,the n-gram language model and a hierarchical re-ordering model.
In our experiments, the baselineis improved by 1.4% BLEU and 1.0% TER on theFrench?German task and by 0.3% BLEU and 1.1%TER on the German?English task.In future work we plan to apply our method to awider range of languages.
Intuitively, it should bemost effective for morphologically rich languages,which naturally have stronger sparsity problems.AcknowledgmentsThis work was partially realized as part of theQuaero Programme, funded by OSEO, French Stateagency for innovation.
The research leading to theseresults has also received funding from the EuropeanUnion Seventh Framework Programme (FP7/2007-2013) under grant agreement no 287658.1380ReferencesStanley F. Chen and Joshuo Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
Technical Report TR-10-98, ComputerScience Group, Harvard University, Cambridge, MA,August.Colin Cherry, Robert C. Moore, and Chris Quirk.
2012.On Hierarchical Re-ordering and Permutation Parsingfor Phrase-based Decoding.
In Proceedings of the 7thWorkshop on Statistical Machine Translation, WMT?12, pages 200?209, Montral, Canada.Colin Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In The 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (NAACL-HLT 2013), pages 22?31, Atlanta, Georgia, USA, June.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 847?855, Honolulu, Hawaii, USA, October.Philipp Koehn and Hieu Hoang.
2007.
Factored Transla-tion Models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 868?876, Prague, Czech Republic, June.P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In Proceedings of the 2003Meeting of the North American chapter of the Associa-tion for Computational Linguistics (NAACL-03), pages127?133, Edmonton, Alberta.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
of the Conf.on Empirical Methods for Natural Language Process-ing (EMNLP), pages 388?395, Barcelona, Spain, July.Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, JanNiehues, Eunah Cho, Teresa Herrmann, and AlexWaibel.
2012.
The kit translation systems for iwslt2012.
In Proceedings of the International Work-shop for Spoken Language Translation (IWSLT 2012),Hong Kong.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449, De-cember.F.
J. Och.
1999.
An efficient method for determiningbilingual word classes.
In Proc.
of the Ninth Conf.of the Europ.
Chapter of the Association of Compu-tational Linguistics, pages 71?76, Bergen, Norway,June.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proc.
of the41th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 160?167, Sapporo,Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, Penn-sylvania, USA, July.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas,pages 223?231, Cambridge, Massachusetts, USA, Au-gust.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Int.
Conf.
onSpeech and Language Processing (ICSLP), volume 2,pages 901?904, Denver, CO, September.David Vilar, Daniel Stein, Matthias Huck, and HermannNey.
2010.
Jane: Open source hierarchical transla-tion, extended with reordering and lexicon models.
InACL 2010 Joint Fifth Workshop on Statistical MachineTranslation and Metrics MATR, pages 262?270, Upp-sala, Sweden, July.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-sour, and Hermann Ney.
2012.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, pages 483?491, Mumbai,India, December.1381
