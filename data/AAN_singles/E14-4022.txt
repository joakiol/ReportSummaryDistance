Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111?116,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsUsing a Random Forest Classifier to Compile Bilingual Dictionaries ofTechnical Terms from Comparable CorporaGeorgios Kontonatsios1,2Ioannis Korkontzelos1,2Jun?ichi Tsujii3Sophia Ananiadou1,2National Centre for Text Mining, University of Manchester, Manchester, UK1School of Computer Science, University of Manchester, Manchester, UK2Microsoft Research Asia, Beijing, China3{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.ukjtsujii@microsoft.comAbstractWe describe a machine learning approach,a Random Forest (RF) classifier, that isused to automatically compile bilingualdictionaries of technical terms from com-parable corpora.
We evaluate the RF clas-sifier against a popular term alignmentmethod, namely context vectors, and wereport an improvement of the translationaccuracy.
As an application, we use theautomatically extracted dictionary in com-bination with a trained Statistical MachineTranslation (SMT) system to more accu-rately translate unknown terms.
The dic-tionary extraction method described in thispaper is freely available1.1 BackgroundBilingual dictionaries of technical terms are im-portant resources for many Natural LanguageProcessing (NLP) tasks including Statistical Ma-chine Translation (SMT) (Och and Ney, 2003) andCross-Language Information Retrieval (Balles-teros and Croft, 1997).
However, manually cre-ating and updating such resources is an expensiveprocess.
In addition to this, new terms are con-stantly emerging.
Especially in the biomedicaldomain, which is the focus of this work, there isa vast number of neologisms, i.e., newly coinedterms, (Pustejovsky et al., 2001).Early work on bilingual lexicon extractionfocused on clean, parallel corpora providingsatisfactory results (Melamed, 1997; Kay andR?oscheisen, 1993).
However, parallel corpora areexpensive to construct and for some domains andlanguage pairs are scarce resources.
For these rea-sons, the focus has shifted to comparable corpora1http://personalpages.manchester.ac.uk/postgrad/georgios.kontonatsios/Software/RF-TermAlign.tar.gzthat are more readily available, more up-to-date,larger and cheaper to construct than parallel data.Comparable corpora are collections of monolin-gual documents in a source and target languagethat share the same topic, domain and/or docu-ments are from the same period, genre and soforth.Existing methods for bilingual lexicon extrac-tion from comparable corpora are mainly basedon the same principle.
They hypothesise that aword and its translation tend to appear in simi-lar lexical context (Fung and Yee, 1998; Rapp,1999; Morin et al., 2007; Chiao and Zweigen-baum, 2002).
Context vector methods are reportedto achieve robust performance on terms that occurfrequently in the corpus.
Chiao and Zweigenbaum(2002) achieved a performance of 94% accuracyon the top 20 candidates when translating high fre-quency, medical terms (frequency of 100 or more).In contrast, Morin and Daille (2010) reported anaccuracy of 21% for multi-word terms occurring20 times or less, noting that translating rare termsis a challenging problem for context vectors.Kontonatsios et al.
(2013) introduced an RFclassifier that is able to automatically learn as-sociation rules of textual units between a sourceand target language.
However, they applied theirmethod only on artificially constructed datasetscontaining an equal number of positive and neg-ative instances.
In the case of comparable cor-pora, the datasets are highly unbalanced (given n,m source and target terms respectively, we need toclassify n?m instances).
In this work, we incor-porate the classification margin into the RF model,to allow the method to cope with the skewed dis-tribution of positive and negative instances that oc-curs in comparable corpora.Our proposed method ranks candidate transla-tions using the classification margin and suggestsas the best translation the candidate with the max-imum margin.
We evaluate our method on an111English-Spanish comparable corpus of Wikipediaarticles that are related to the medical sub-domainof ?breast cancer?.
Furthermore, we show that dic-tionaries extracted from comparable corpora canbe used to dynamically augment an SMT sys-tem in order to better translate Out-of-Vocabulary(OOV) terms.2 MethodologyA pair of terms in a source and target language isrepresented as a feature vector where each dimen-sion corresponds to a unique character n-gram.The value of each dimension is 0 or 1 and desig-nates the occurrence of the corresponding n-gramin the input terms.
The feature vectors that weuse contain 2q dimensions where the first q dimen-sions correspond to the n-gram features extractedfrom the source terms and the last q dimensions tothose from the target terms.
In the reported experi-ments, we use the 600 (300 source and 300 target)most frequently occurring n-grams.The underlying mechanism that allows the RFmethod to learn character gram mappings betweenterms of a source and target language is the de-cision trees.
A node in the decision tree is aunique character n-gram.
The nodes are linkedthrough the branches of the trees and therefore thetwo sub-spaces of q source and q target charac-ter grams are combined.
Each decision tree in theforest is constructed as follows: every node is splitby considering |?| random n-gram features of theinitial feature set ?, and a decision tree is fullygrown.
This process is repeated |?
| times and con-structs |?
| decision trees.
We tuned the RF clas-sifier using 140 random trees where we observeda plateau in the classification performance.
Fur-thermore, we set the number of random featuresusing |?| = log2|?|+ 1 as suggested by Breiman(2001).The classification margin that we use to rankthe candidate translations is calculated by simplysubtracting the average number of trees predictingthat the input terms are not translations from theaverage number of decision trees predicting thatthe terms are mutual translations.
A larger classi-fication margin means that more decision trees inthe forest classify an instance as a translation pair.For training an RF model, we use a bilingualdictionary of technical terms.
When the dictionarylists more than one translation for an English term,we randomly select only one.
Negative instancesare created by randomly matching non-translationpairs of terms.
We used an equal number of posi-tive and negative instances for training the model.Starting from 20, 000 translation pairs we gener-ated a training dataset of 40, 000 positive and neg-ative instances.2.1 Baseline methodThe context projection method was first pro-posed by (Fung and Yee, 1998; Rapp, 1999) andsince then different variations have been suggested(Chiao and Zweigenbaum, 2002; Morin et al.,2007; Andrade et al., 2010; Morin and Prochas-son, 2011).
Our implementation more closelyfollows the context vector method introduced by(Morin and Prochasson, 2011).As a preprocessing step, stop words are re-moved using an online list2and lemmatisationis performed using TreeTagger (Schmid, 1994) onboth the English and Spanish part of the compa-rable corpus.
Afterwards, the method proceedsin three steps.
Firstly, for each source and targetterm of the comparable corpus, i.e., i, we collectall lexical units that: (a) occur within a windowof 3 words around i (a seven-word window) and(b) are listed in the seed bilingual dictionary.
Thelexical units that satisfy the above two conditionsare the dimensions of the context vectors.
Eachdimension has a value that indicates the correla-tion between the context lexical unit and the termi.
In our approach, we use the log-likelihood ra-tio.
In the second step, the seed dictionary is usedto translate the lexical units of the Spanish contextvectors.
In this way the Spanish and English vec-tors become comparable.
When several transla-tions are listed in the seed dictionary, we considerall of them.
In the third step, we compute the con-text similarity, i.e., distance metric, between thevector of an English term to be translated with ev-ery projected, Spanish context vector.
For this weuse the cosine similarity.3 ExperimentsIn this section, we evaluate the two dictionary ex-traction methods, namely context vectors and RF,on a comparable corpus of Wikipedia articles.For the evaluation metric, we use the top-ktranslation accuracy3and the mean reciprocal2http://members.unine.ch/jacques.savoy/clef/index.html3the percentage of English terms whose top k candidatescontain a correct translation112rank (MRR)4as in previous approaches (Chiaoand Zweigenbaum, 2002; Chiao and Zweigen-baum, 2002; Morin and Prochasson, 2011; Morinet al., 2007; Tamura et al., 2012).
As a refer-ence list, we use the UMLS metathesaurus5.
Inaddition to this, considering that in several casesthe dictionary extraction methods retrieved syn-onymous translations that do not appear in the ref-erence list, we manually inspected the answers.Finally, unlike previous approaches (Chiao andZweigenbaum, 2002), we do not restrict the testlist only to those English terms whose Spanishtranslations are known to occur in the target cor-pus.
In such cases, the performance of dictionaryextraction methods have been shown to achieve alower performance (Tamura et al., 2012).3.1 DataWe constructed a comparable corpus of Wikipediaarticles.
For this, we used Wikipedia?s search en-gine6and submitted the queries ?breast cancer?and ?c?ancer de mama?
for English and Spanishrespectively.
From the returned list of Wikipediapages, we used the 1, 000 top articles for both lan-guages.The test list contains 1, 200 English single-wordterms that were extracted by considering all nounsthat occur more than 10 but not more than 200times and are listed in UMLS.
For the Spanish partof the corpus, we considered all nouns as candi-date translations (32, 347 in total).3.2 ResultsTable 1 shows the top-k translation accuracy andthe MRR of RF and context vectors.Acc1Acc10Acc20MRRRF 0.41 0.57 0.59 0.47Cont.Vectors 0.1 0.21 0.26 0.11Table 1: top-k translation accuracy and MRR ofRF and context vectors on 1, 200 English termsWe observe that the proposed RF methodachieves a considerably better top-k translation ac-4MRR =1|Q|?Qi=11rankiwhere |Q| is the number ofEnglish terms for which we are extracting translations andrankiis the position of the first correct translation from re-turned list of candidates5nlm.nih.gov/research/umls6http://en.wikipedia.org/wiki/Help:Searchingcuracy and MRR than the baseline method.
More-over, we segmented the 1, 200 test terms into 7frequency ranges7, from high-frequency to rareterms.
Figure 1 shows the translation accuracy attop 20 candidates for the two methods.
We noteFigure 1: Translation accuracy of top 20 candi-dates on different frequency rangesthat for high frequency terms, i.e.
[100,200] range,the performance achieved by the two methods issimilar (53% and 52% for the RF and context vec-tors respectively).
However, for lower frequencyterms, the translation accuracy of the context vec-tors continuously declines.
This confirms that con-text vectors do not behave robustly for rare terms(Morin and Daille, 2010).
In contrast, the RFslightly fluctuates over different frequency rangesand presents approximately the same translationaccuracy for both frequent and rare terms.4 ApplicationAs an application of our method, we use the pre-viously extracted dictionaries to on-line augmentthe phrase table of an SMT system and observethe translation performance on test sentences thatcontain OOV terms.
For the translation probabil-ities in the phrase table, we use the distance met-ric given by the dictionary extraction methods i.e.,classification margin and cosine similarity of RFand context vectors respectively, normalised bythe uniform probability (if a source term has mcandidate translations, we normalise the distancemetric by dividing by m as in (Wu et al., 2008) .4.1 Data and toolsWe construct a parallel, sentence-aligned corpusfrom the biomedical domain, following the pro-cess described in (Wu et al., 2011; Yepes et al.,2013).
The parallel corpus comprises of article ti-tles indexed by PubMed in both English and Span-ish.
We collect 120K parallel sentences for train-7each frequency range contains 100 randomly sampledterms113ing the SMT and 1K sentences for evaluation.
Thetest sentences contain 1, 200 terms that do not ap-pear in the training parallel corpus.
These termsoccur in the Wikipedia comparable corpus.
Hence,the previously extracted dictionaries list a possibletranslation.
Using the PubMed parallel corpus, wetrain Moses (Koehn et al., 2007), a phrase-basedSMT system.4.2 ResultsWe evaluated the translation performance of theSMT that uses the dictionary extracted by the RFagainst the following baselines: (i) Moses usingonly the training parallel data (Moses), (ii) Mosesusing the dictionary extracted by context vectors(Moses+context vector).
The evaluation metric isBLEU (Papineni et al., 2002).Table 2 shows the BLEU score achieved by theSMT systems when we append the top-k transla-tions to the phrase table.BLEUon top-k translations1 10 20Moses 24.22 24.22 24.22Moses+RF 25.32 24.626 24.42Moses+Context Vectors 23.88 23.69 23.74Table 2: Translation performance when addingtop-k translations to the phrase tableWe observe that the best performance isachieved by the RF when we add the top 1 trans-lation with a total gain of 1.1 BLEU points overthe baseline system.
In contrast, context vec-tors decreased the translation performance of theSMT system.
This indicates that the dictionary ex-tracted by the context vectors is too noisy and asa result the translation performance dropped.
Fur-thermore, it is noted that the augmented SMT sys-tems achieve the highest performance for the top 1translation while for k greater than 1, the transla-tion performance decreases.
This behaviour is ex-pected since the target language model was trainedonly on the training Spanish sentences of the par-allel corpus.
Hence, the target language modeldoes not have a prior knowledge of the OOV trans-lations and as a result it cannot choose the correcttranslation among k candidates.To further investigate the effect of the languagemodel on the translation performance of the aug-mented SMT systems, we conducted an oracle ex-periment.
In this ideal setting, we assume a stronglanguage model, that is trained on both trainingand test Spanish sentences of the parallel corpus,in order to assign a higher probability to a correcttranslation if it exists in the deployed dictionary.As we observe in Table 3, a strong language modelcan more accurately select the correct translationamong top-k candidates.
The dictionary extractedby the RF improved the translation performanceby 2.5 BLEU points for the top-10 candidates andcontext vectors by 0.45 for the top-20 candidates.BLEUon top-k translations1 10 20Moses 28.85 28.85 28.85Moses+RF 30.98 31.35 31.2Moses+Context Vectors 28.18 29.17 29.3Table 3: Translation performance when addingtop-k translations to the phrase table.
SMT sys-tems use a language model trained on training andtest Spanish sentences of the parallel corpus.5 DiscussionIn this paper, we presented an RF classifier thatis used to extract bilingual dictionaries of techni-cal terms from comparable corpora.
We evaluatedour method on a comparable corpus of Wikipediaarticles.
The experimental results showed that ourproposed method performs robustly when translat-ing both frequent and rare terms.As an application, we used the automaticallyextracted dictionary to augment the phrase table ofan SMT system.
The results demonstrated an im-provement of the overall translation performance.As future work, we plan to integrate the RF clas-sifier with context vectors.
Intuitively, the twomethods are complementary considering that theRF exploits the internal structure of terms whilecontext vectors use the surrounding lexical con-text.
Therefore, it will be interesting to investigatehow we can incorporate the two feature spaces ina machine learner.1146 AcknowledgementsThis work was funded by the European Commu-nity?s Seventh Framework Program (FP7/2007-2013) [grant number 318736 (OSSMETER)].ReferencesDaniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-jii.
2010.
Robust measurement and comparison ofcontext similarity for finding translation pairs.
InProceedings of the 23rd International Conference onComputational Linguistics, COLING ?10, pages 19?27, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Lisa Ballesteros and W.Bruce Croft.
1997.
Phrasaltranslation and query expansion techniques forcross-language information retrieval.
In ACM SIGIRForum, volume 31, pages 84?91.
ACM.Leo Breiman.
2001.
Random Forests.
Machine Learn-ing, 45:5?32.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for Candidate Translational Equivalents inSpecialized, Comparable Corpora.
In InternationalConference on Computational Linguistics.Pascale Fung and Lo Yuen Yee.
1998.
An ir approachfor translating new words from nonparallel, compa-rable texts.
In Proceedings of the 17th internationalconference on Computational linguistics-Volume 1,pages 414?420.
Association for Computational Lin-guistics.Martin Kay and Martin R?oscheisen.
1993.
Text-translation alignment.
computational Linguistics,19(1):121?142.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Georgios Kontonatsios, Ioannis Korkontzelos, Jun?ichiTsujii, and Sophia Ananiadou.
2013.
Using ran-dom forest to recognise translation equivalents ofbiomedical terms across languages.
In Proceed-ings of the Sixth Workshop on Building and UsingComparable Corpora, pages 95?104.
Associationfor Computational Linguistics, August.I.
Dan Melamed.
1997.
A portable algorithm for map-ping bitext correspondence.
In Proceedings of the35th Annual Meeting of the Association for Com-putational Linguistics and Eighth Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 305?312.
Association forComputational Linguistics.Emmanuel Morin and B?eatrice Daille.
2010.
Com-positionality and lexical alignment of multi-wordterms.
Language Resources and Evaluation, 44(1-2):79?95.Emmanuel Morin and Emmanuel Prochasson.
2011.Bilingual lexicon extraction from comparable cor-pora enhanced with parallel corpora.
In Proceedingsof the 4th Workshop on Building and Using Compa-rable Corpora: Comparable Corpora and the Web,pages 27?34, Portland, Oregon, June.
Associationfor Computational Linguistics.Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,and Kyo Kageura.
2007.
Bilingual terminologymining - using brain, not brawn comparable corpora.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 664?671, Prague, Czech Republic, June.
Association forComputational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.James Pustejovsky, Jose Castano, Brent Cochran, Ma-ciej Kotecki, and Michael Morrell.
2001.
Au-tomatic extraction of acronym-meaning pairs frommedline databases.
Studies in health technology andinformatics, (1):371?375.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In Proceedings of the 37th annual meetingof the Association for Computational Linguistics onComputational Linguistics, pages 519?526.
Associ-ation for Computational Linguistics.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, volume 12, pages 44?49.
Manch-ester, UK.Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.2012.
Bilingual lexicon extraction from compara-ble corpora using label propagation.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 24?36.
Associa-tion for Computational Linguistics.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine transla-tion with domain dictionary and monolingual cor-pora.
In Proceedings of the 22nd InternationalConference on Computational Linguistics-Volume1, pages 993?1000.
Association for ComputationalLinguistics.115Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti.2011.
Statistical machine translation for biomedicaltext: are we there yet?
In AMIA Annual Sympo-sium Proceedings, volume 2011, page 1290.
Ameri-can Medical Informatics Association.Antonio Jimeno Yepes,?Elise Prieur-Gaston, andAur?elie N?ev?eol.
2013.
Combining medline andpublisher data to create parallel corpora for the auto-matic translation of biomedical text.
BMC bioinfor-matics, 14(1):146.116
