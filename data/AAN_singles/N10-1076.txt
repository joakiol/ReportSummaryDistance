Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 519?527,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutomatic Diacritization for Low-Resource Languages Using a HybridWord and Consonant CMMRobbie A. Haertel, Peter McClanahan, and Eric K. RinggerDepartment of Computer ScienceBrigham Young UniversityProvo, Utah 84602, USArah67@cs.byu.edu, petermcclanahan@gmail.com, ringger@cs.byu.eduAbstractWe are interested in diacritizing Semitic lan-guages, especially Syriac, using only dia-critized texts.
Previous methods have requiredthe use of tools such as part-of-speech taggers,segmenters, morphological analyzers, and lin-guistic rules to produce state-of-the-art results.We present a low-resource, data-driven, andlanguage-independent approach that uses ahybrid word- and consonant-level conditionalMarkov model.
Our approach rivals the bestpreviously published results in Arabic (15%WER with case endings), without the use ofa morphological analyzer.
In Syriac, we re-duce the WER over a strong baseline by 30%to achieve a WER of 10.5%.
We also reportresults for Hebrew and English.1 IntroductionAbjad writing systems omit vowels and other di-acritics.
The ability to restore these diacritics isuseful for personal, industrial, and governmentalpurposes?especially for Semitic languages.
In itsown right, the ability to diacritize can aid languagelearning and is necessary for speech-based assis-tive technologies, including speech recognition andtext-to-speech.
Diacritics are also useful for taskssuch as segmentation, morphological disambigua-tion, and machine translation, making diacritizationimportant to Natural Language Processing (NLP)systems and intelligence gathering.
In alphabeticwriting systems, similar techniques have been usedto restore accents from plain text (Yarowsky, 1999)and could be used to recover missing letters in thecompressed writing styles found in email, text, andinstant messages.We are particularly interested in diacritizing Syr-iac, a low-resource dialect of Aramaic, which pos-sesses properties similar to Arabic and Hebrew.
Thiswork employs conditional Markov models (CMMs)(Klein and Manning, 2002) to diacritize Semitic(and other) languages and requires only diacritizedtexts for training.
Such an approach is useful forlanguages (like Syriac) in which annotated data andlinguistic tools such as part-of-speech (POS) tag-gers, segmenters, and morphological analyzers arenot available.
Our main contributions are as follows:(1) we introduce a hybrid word and consonant CMMthat allows access to the diacritized form of the pre-vious words; (2) we introduce new features avail-able in the proposed model; and (3) we describe anefficient, approximate decoder.
Our models signifi-cantly outperform existing low-resource approachesacross multiple related and unrelated languages andeven achieve near state-of-the-art results when com-pared to resource-rich systems.In the next section, we review previous work rel-evant to our approach.
Section 3 then motivates anddescribes the models and features used in our frame-work, including a description of the decoder.
Wedescribe our data in Section 4 and detail our exper-imental setup in Section 5.
Section 6 presents ourresults.
Finally, Section 7 briefly discusses our con-clusions and offers ideas for future work.2 Previous WorkDiacritization has been receiving increased attentiondue to the rising interest in Semitic languages, cou-519pled with the importance of diacritization to otherNLP-related tasks.
The existing approaches can becategorized based on the amount of resources theyrequire, their basic unit of analysis, and of coursethe language they are targeting.
Probabilistic sys-tems can be further divided into generative and con-ditional approaches.Existing methodologies can be placed along acontinuum based on the quantity of resources theyrequire?a reflection of their cost.
Examples ofresources used include morphological analyzers(Habash and Rambow, 2007; Ananthakrishnan et al,2005; Vergyri and Kirchhoff, 2004; El-Sadany andHashish, 1989), rules for grapheme-to-sound con-version (El-Imam, 2008), transcribed speech (Ver-gyri and Kirchhoff, 2004), POS tags (Zitouni et al,2006; Ananthakrishnan et al, 2005), and a list ofprefixes and suffixes (Nelken and Shieber, 2005).When such resources exist for a particular language,they typically improve performance.
For instance,Habash and Rambow?s (2007) approach reduces theerror rate of Zitouni et al?s (2006) by as much as30% through its use of a morphological analyzer.
Infact, such resources are not always available.
Sev-eral data-driven approaches exist that require onlydiacritized texts (e.g., Ku?bler and Mohamed, 2008;Zitouni et al, 2006; Gal, 2002) which are relativelyinexpensive to obtain: most literate speakers of thetarget language could readily provide them.Apart from the quantity of resources required, di-acritization systems also differ in their basic unit ofanalysis.
A consonant-based approach treats eachconsonant1 in a word as a potential host for oneor more (possibly null) diacritics; the goal is topredict the correct diacritic(s) for each consonant(e.g., Ku?bler and Mohamed, 2008).
Zitouni et al(2006) extend the problem to a sequence labelingtask wherein they seek the best sequence of diacrit-ics for the consonants.
Consequently, their approachhas access to previously chosen diacritics.Alternatively, the basic unit of analysis can be thefull, undiacritized word.
Since morphological ana-lyzers produce analyses of undiacritized words, di-acritization approaches that employ them typicallyfall into this category (e.g., Habash and Rambow,1We refer to all graphemes present in undiacritized texts asconsonants.2007; Vergyri and Kirchoff, 2004).
Word-based,low-resource solutions tend to treat the problem asword-level sequence labeling (e.g., Gal, 2002).Unfortunately, word-based techniques face prob-lems due to data sparsity: not all words in thetest set are seen during training.
In contrast,consonant-based approaches rarely face the anal-ogous problem of previously unseen consonants.Thus, one low-resource solution to data sparsity is touse consonant-based techniques for unknown words(Ananthakrishnan et al, 2005; Nelken and Shieber,2005).Many of the existing systems, especially recentones, are probabilistic or contain probabilistic com-ponents.
Zitouni et al (2006) show the superior-ity of their conditional-based approaches over thebest-performing generative approaches.
However,the instance-based learning approach of Ku?bler andMohamed (2008) slightly outperforms Zitouni etal.
(2006).
In the published literature for Arabic,the latter two have the best low-resource solutions.Habash and Rambow (2007) is the state-of-the-art,high-resource solution for Arabic.
To our knowl-edge, no work has been done in this area for Syriac.3 ModelsIn this work, we are concerned with diacritiza-tion for Syriac for which a POS tagger, segmenter,and other tools are not readily available, but forwhich diacritized text is obtainable.2 Use of a sys-tem dependent on a morphological analyzer such asHabash and Rambow?s (2007) is therefore not cost-effective.
Furthermore, we seek a system that is ap-plicable to a wide variety of languages.
AlthoughKu?bler and Mohamed?s (2008) approach is compet-itive to Zitouni et al?s (2006), instance-based ap-proaches tend to suffer with the addition of new fea-tures (their own experiments demonstrate this).
Wedesire to add linguistically relevant features to im-prove performance and thus choose to use a condi-tional model.
However, unlike Zitouni et al (2006),we use a hybrid word- and consonant-level approachbased on the following observations (statistics takenfrom the Syriac training and development sets ex-plained in Section 4):2Kiraz (2000) describes a morphological analyzer for Syriacthat is not publicly available and is costly to reproduce.5201.
Many undiacritized words are unambiguous:90.8% of the word types and 63.5% of the to-kens have a single diacritized form.2.
Most undiacritized word types have only a fewpossible diacritizations: the average number ofpossible diacritizations is 1.11.3.
Low-frequency words have low ambiguity:Undiacritized types occurring fewer than 5times have an average of 1.05 possible diacriti-zations.4.
Diacritized words not seen in the training dataoccur infrequently at test time: 10.5% of thediacritized test tokens were not seen in training.5.
The diacritics of previous words can provideuseful morphological information such as per-son, number, and gender.Contrary to observations 1 and 2, consonant-levelapproaches dedicate modeling capacity to an expo-nential (in the number of consonants) number ofpossible diacritizations of a word.
In contrast, aword-level approach directly models the (few) dia-critized forms seen in training.
Furthermore, word-based approaches naturally have access to the dia-critics of previous words if used in a sequence la-beler, as per observation 5.
However, without a?backoff?
strategy, word-level models cannot pre-dict a diacritized form not seen in the training data.Also, low-frequency words by definition have lessinformation from which to estimate parameters.
Incontrast, abundant information exists for each dia-critic in a consonant-level system.
To the degreeto which they hold, observations 3 and 4 mitigatethese latter two problems.
Clearly a hybrid approachwould be advantageous.To this end we employ a CMM in which we treatthe problem as an instance of sequence labeling atthe word level with less common words being han-dled by a consonant-level CMM.
Let u be the undi-acriatized words in a sentence.
Applying an order oMarkov assumption, the distribution over sequencesof diacritized words d is:P (d|u) =?d?
?i=1P (di|di?o...i?1,u;?,?, ?)
(1)in which the local conditional distribution of a di-acritized word is an interpolation of a word-levelmodel (?ui) and a consonant-level model (?
):P (di|di?o...i?1,u;?,?, ?)
=?P (di|di?o...i?1,u;?ui) +(1 ?
?
)P (di|di?o...i?1,u;?
)We let the consonant-level model be a standardCMM, similar to Zitouni et al (2006), but with ac-cess to previously diacritized words.
Note that theorder of this ?inner?
CMM need not be the same asthat of the outer CMM.The parameter ?
reflects the degree to which wetrust the word-level model.
In the most general case,?
can be a function of the undiacritized words andthe previous o diacritized words.
Based on our ear-lier enumerated observations, we use a simple deltafunction for ?
: we let ?
be 0 when ui is rare and 1otherwise.
We leave discussion for what constitutesa ?rare?
undiacritized type for Section 5.2.Figure 1b presents a graphical model of a sim-ple example sentence in Syriac.
The diacritiza-tion for non-rare words is predicted for a wholeword, hence the random variable D for each suchword.
These diacritized words Di depend on previ-ous Di?1 as per equation (1) for an order-1 CMM(note that the capitalized A, I, and O are in fact con-sonants in this transliteration).
Because ?NKTA?and ?RGT?
are rare, their diacritization is repre-sented by a consonant-level CMM: one variable foreach possible diacritic in the word.
Importantly,these consonant-level models have access to the pre-viously diacritized word (D4 and D6, respectively).We use log-linear models for all local distribu-tions in our CMMs, i.e., we use maximum entropy(maxent) Markov models (McCallum et al, 2000;Berger et al, 1996).
Due to the phenomenon knownas d-separation (Pearl and Shafer, 1988), it is possi-ble to independently learn parameters for each wordmodel ?ui by training only on those instances forthe corresponding word.
Similarly, the consonantmodel can be learned independent of the word mod-els.
We place a spherical normal prior centered atzero with a standard deviation of 1 over the weightsof all models and use an L-BFGS minimizer to findthe MAP estimate of the weights for all the models(words and consonant).521?C C C C C C CSIA AO5,1 5,2 5,3 5,4 5,1 5,2(a)?
DHBA?CSIA?
AO ??
LAD1CSIAD2AOD3DHBAD4AO NKTAD6LA RGTC5,1 C5,2 C5,3 C5,4 C7,1 C7,2 C7,3(b)Figure 1: Graphical models of Acts 20:33 in Syriac, CSIA AO DHBA AO NKTA LA RGT ?silver or gold orgarment I have not coveted,?
using Kiraz?s (1994) transliteration for (a) the initial portion of a consonant-level-only model and (b) a combined word- and consonant-level model.
For clarity, both models assume aconsonant-level Markov order of 1; (b) shows a word-level Markov order of 1.
For simplicity, the figurefurther assumes that additional features come only from the current (undiacritized) word.Note that Zitouni et al?s (2006) model is a spe-cial case of equation (1) where all words are rare, theword-level Markov order (o) is 0, and the consonant-level Markov order is 2.
A simplified version of Zi-touni?s model is presented in Figure 1a.3.1 FeaturesOur features are based on those found in Zitouni etal.
(2006), although we have added a few of our ownwhich we consider to be one of the contributions ofthis paper.
Unlike their work, our consonant-levelmodel has access to previously diacritized words,allowing us to exploit information noted in obser-vation 5.Each of the word-level models shares the same setof features, defined by the following templates:?
The prefixes and suffixes (up to 4 characters) ofthe previously diacritized words.?
The string of the actual diacritics, including thenull diacritic, from each of the previous o dia-critized words and n-grams of these strings; asimilar set of features is extracted but withoutthe null diacritics.?
Every possible (overlapping) n-gram of allsizes from n = 1 to n = 5 of undiacritizedwords contained within the window defined by2 words to the right and 2 to the left.
Thesetemplates yield 15 features for each token.?
The count of how far away the current tokenis from the beginning/end of the sentence upto the Markov order; also, their binary equiva-lents.The first two templates rely on diacritizations of pre-vious words, in keeping with observation 5.The consonant-level model has the following fea-ture templates:?
The current consonant.?
Previous diacritics (individually, and n-gramsof diacritics ending in the diacritic prior to thecurrent consonant, where n is the consonant-level Markov order).?
Conjunctions of the first two templates.?
Indicators as to whether this is the first or lastconsonant.?
The first three templates independently con-joined with the current consonant.?
Every possible (overlapping) n-gram of allsizes from n = 1 to n = 11 consisting of con-sonants contained within the window definedby 5 words to the right and 5 to the left.?
Same as previous, but available diacritics areincluded in the window.?
Prefixes and suffixes (of up to length 4) of pre-viously diacritized words conjoined with previ-ous diacritics in the current token, both individ-ually and n-grams of such.522This last template is only possible because of ourmodel?s dependency on previous diacritized words.3.2 DecoderGiven a sentence consisting of undiacritized words,we seek the most probable sequence of diacritizedwords, i.e., arg maxd P (d|u...).
In sentences con-taining no rare words, the well-known Viterbi algo-rithm can be used to find the optimum.However, as can be seen in Figure 1b, predictionsin the consonant-level model (e.g., C5,1...4) dependon previously diacritized words (D4), and some dia-critized words (e.g., D6) depend on diacritics in theprevious rare word (C5,1...4).
These dependenciesintroduce an exponential number of states (in thelength of the word) for rare words, making exact de-coding intractable.
Instead, we apply a non-standardbeam during decoding to limit the number of statesfor rare words to the n-best (locally).
This is ac-complished by using an independent ?inner?
n-bestdecoder for the consonant-level CMM to producethe n-best diacritizations for the rare word given theprevious diacritized words and other features.
Thesebecome the only states to and from which transitionsin the ?outer?
word-level decoder can be made.
Wenote this is the same type of decoding that is done inpipeline models that use n-best decoders (Finkel etal., 2006).
Additionally, we use a traditional beam-search of width 5 to further reduce the search spaceboth in the outer and inner CMMs.4 DataAlthough our primary interest is in the Syriac lan-guage, we also experimented with the Penn ArabicTreebank (Maamouri et al, 2004) for the sake ofcomparison with other approaches.
We include He-brew to provide results for yet another Semitic lan-guage.
We also apply the models to English to showthat our method and features work well outside ofthe Semitic languages.
A summary of the datasets,including the number of diacritics, is found in Fig-ure 2.
The number of diacritics shown in the tableis less than the number of possible predictions sincewe treat contiguous diacritics between consonants asa single prediction.For our experiments in Syriac, we use the NewTestament portion of the Peshitta (Kiraz, 1994) andlang diacs train dev testSyriac 9 87,874 10,747 11,021Arabic 8 246,512 42,105 51,664Hebrew 17 239,615 42,133 49,455English 5 1,004,073 80,156 89,537Figure 2: Number of diacritics and size (in tokens)of each datasettreat each verse as if it were a sentence.
The diacrit-ics we predict are the five short vowels, as well asSe?ya?me?, Rukka?kha?, Qus?s?a?ya?, and linea ocultans.For Arabic, we use the training/test split definedby Zitouni et al (2006).
We group all words havingthe same P index value into a sentence.
We build ourown development set by removing the last 15% ofthe sentences of the training set.
Like Zitouni, whenno solution exists in the treebank, we take the firstsolution as the gold tag.
Zitouni et al (2006) reportresults on several different conditions, but we focuson the most challenging of the conditions: we pre-dict the standard three short vowels, three tanween,sukuun, shadda, and all case endings.
(Preliminaryexperiments show that our models perform equallyfavorably in the other scenarios as well.
)For Hebrew, we use the Hebrew Bible (Old Tes-tament) in the Westminster Leningrad Codex (Zefa-nia XML Project, 2009).
As with Syriac, we treateach verse as a sentence and remove the paragraphmarkers (pe and samekh).
There is a large numberof diacritics that could be predicted in Hebrew andno apparent standardization in the literature.
Forthese reasons, we attempt to predict as many dia-critics as possible.
Specifically, we predict the di-acritics whose unicode values are 05B0-B9, 05BB-BD, 05BF, 05C1-C2, and 05C4.
We treat the follow-ing list of punctuation as consonants: maqaf, paseq,sof pasuq, geresh, and gershayim.
The cantillationmarks are removed entirely from the data.Our English data comes from the Penn Treebank(Marcus et al, 1994).
We used sections 0?20 astraining data, 21?22 as development data, and 23?24 as our test set.
Unlike words in the Semitic lan-guages, English words can begin with a vowel, re-quiring us to prepend a prosthetic consonant to everyword; we also convert all English text to lowercase.5235 ExperimentsFor all feature engineering and tuning, we trainedand tested on training and development test sets, re-spectively (as specified above).
Final results are re-ported by folding the development test set into thetraining data and evaluating on the blind test set.
Weretain only those features that occur more than once.For each approach, we report the Word Error Rate(WER) (i.e., the percentage of words that were in-correctly diacritized), along with the Diacritic Er-ror Rate (DER) (i.e., the percentage of diacritics, in-cluding the null diacritic, that were incorrectly pre-dicted).
We also report both WER and DER foronly those words that were not seen during training(UWER and UDER, respectively).
We found thatprecision, recall, and f-score were nearly perfectlycorrelated with DER; hence, we omit this informa-tion for brevity.5.1 Models for EvaluationIn previous work, Ku?bler et al (2008) report thelowest error rates of the low-resource models.
Al-though their results are not directly comparable toZitouni et al (2006), we have independently con-firmed that the former slightly outperforms the latterusing the same diacritics and on the same dataset(see Figure 4), thereby providing the strongest pub-lished baseline for Arabic on a common dataset.
Wedenote this model as ku?bler and use it as a strongbaseline for all datasets.For the Arabic results, we additionally include Zi-touni et al?s (2006) lexical model (zitouni-lex)and their model that uses a segmenter and POStagger (zitouni-all), which are not immediatelyavailable to us for Syriac.
For yet another point ofreference for Arabic, we provide the results from thestate-of-the-art (resource-rich) approach of Habashand Rambow (2007) (habash).
This model is at anextreme advantage, having access to a full morpho-logical analyzer.
Note that for these three modelswe simply report their published results and do notattempt to reproduce them.Since ku?bler is of a different model class thanours, we consider an additional baseline that is aconsonant-level CMM with access to the same in-formation, namely, only those consonants within awindow of 5 to either side (ccmm).
This is equiva-lent to a special case of our hybrid model whereinboth the word-level and the consonant-level Markovorder are 0.
The features that we extract from thiswindow are the windowed n-gram features.In order to assess the utility of previous diacriticsand how effectively our features leverage them, webuild a model based on the methodology from Sec-tion 3 but specify that all words are rare, effectivelycreating a consonant-only model that has access tothe diacritics of previous words.
We call this modelcons-only.
We note that the main difference be-tween this model and zitouni-lex are featuresthat depend on previous diacritized words.Finally, we present results using our full hybridmodel (hybrid).
We use a Markov order of 2 atthe word and consonant level for both hybrid andcons-only.5.2 Consonant-Level Model and Rare WordsThe hybrid nature of hybrid naturally raises thequestion of whether or not the inner consonantmodel should be trained only on rare words or onall of the data.
In other words, is the distributionof diacritics different in rare words?
If so, the con-sonant model should be trained only on rare words.To answer this question, we trained our consonant-level model (cons-only) on words occurring fewerthan n times.
We swept the value of the threshold nand compared the results to the same model trainedon a random selection of words.
As can be seen inFigure 3, the performance on unknown words (bothUWER and UDER) using a model trained on rarewords can be much lower than using a model trainedon the same amount of randomly selected data.
Infact, training on rare words can lead to a lower errorrate on unknown words than training on all tokensin the corpus.
This suggests that the distribution ofdiacritics in rare words is different from the distri-bution of diacritics in general.
This difference maycome from foreign words, especially in the Arabicnews corpus.While this phenomenon is more pronounced insome languages and with some models more thanothers, it appears to hold in the cases we tried.
Wefound the WER for unknown words to be lowest fora threshold of 8, 16, 32, and 32 for Syriac, Arabic,Hebrew, and English, respectively.52400.20.40.60.810  10000  20000  30000  40000  50000  60000  70000  80000  90000errorratetokensUWER (random)UWER (rare)UDER (random)UDER (rare)(a) Syriac00.20.40.60.810  50000  100000  150000  200000  250000errorratetokensUWER (random)UWER (rare)UDER (random)UDER (rare)(b) ArabicFigure 3: Learning curves showing impact on consonant-level models when training on rare tokens forSyriac and Arabic.
Series marked ?rare?
were trained with the least common tokens in the dataset.Approach WER DER UWER UDERSyriacku?bler 15.04 5.23 64.65 18.21ccmm 13.99 4.82 54.54 15.18cons-only 12.31 5.03 55.68 19.09hybrid 10.54 4.29 55.16 18.86Arabiczitouni-lex 25.1 8.2 NA NAku?bler 23.61 7.25 66.69 20.51ccmm 22.63 6.61 57.71 16.10cons-only 15.02 5.15 48.10 15.76hybrid 17.87 5.67 47.85 15.63zitouni-all 18.0 5.5 NA NAhabash 14.9 4.8 NA NAHebrewku?bler 30.60 12.96 89.52 36.86ccmm 29.67 12.05 80.02 29.39cons-only 23.39 10.92 75.70 33.34hybrid 22.18 10.71 74.38 32.40Englishku?bler 10.54 4.38 54.96 16.31ccmm 11.60 4.71 58.55 16.34cons-only 8.71 3.87 58.93 17.85hybrid 5.39 2.38 57.24 16.51Figure 4: Results for all languages and approaches6 Discussion of ResultsSince Syriac is of primary interest to us, we beginby examining the results from this dataset.
Syriacappears to be easier to diacritize than Arabic, con-sidering it has a similar number of diacritics andonly one-third the amount of data.
On this dataset,hybrid has the lowest WER and DER, achievingnearly 30% and 18% reduction in WER and DER,respectively, over ku?bler; it reduces both errorrates over cons-only by more than 14%.
Theseresults attest to the effectiveness of our model in ac-counting for the observations made in Section 3.A similar pattern holds for the Hebrew and En-glish datasets, namely that hybrid reduces theWER over ku?bler by 28% to upwards of 50%;cons-only also consistently and significantly out-performs ku?bler and ccmm.
However, the reduc-tion in error rate for our cons-only and hybridmodels tends to be lower for DER than WER inall languages except for English.
In the case ofhybrid, this is probably because it is inherentlyword-based.
Having access to entire previous dia-critized words may be a contributing factor as well,especially in cons-only.When comparing model classes (ku?bler andccmm), it appears that performance is comparableacross all languages, with the maxent approach en-joying a slight advantage except in English.
Interest-ingly, the maxent solution usually handles unknownwords better, although it does not specifically targetthis case.
Both models outperform zitouni-lexin Arabic, despite the fact that they use a muchsimpler feature set, most notably, the lack of pre-vious diacritics.
In the case of ccmm this may be at-tributable in part to our use of an L-BFGS optimizer,convergence criteria, feature selection, or other po-tential differences not noted in Zitouni et al (2006).We note that the maxent-based approaches are muchmore time and memory intensive.Using the Arabic data, we are able to com-pare our methods to several other published results.525The cons-only model significantly outperformszitouni-all despite the additional resources towhich the latter has access.
This is evidence sup-porting our hypothesis that the diacritics from pre-vious words in fact contain useful information forprediction.
This empirically suggests that the inde-pendence assumptions in consonant-only models aretoo strict.Perhaps even more importantly, our low-resourcemethod approaches the performance of habash.
Wenote that the differences may not be statistically sig-nificant, and also that Habash and Rambow (2007)omit instances in the data that lack solutions.
In fact,cons-only has a lower WER than all but two ofthe seven techniques used by Habash and Rambow(2007), which use a morphological analyzer.Interestingly, hybrid does worse thancons-only on this dataset, although it is stillcompetitive with zitouni-all.
We hypothesizethat the observations from Section 3 do not holdas strongly for this dataset.
For this reason, usinga smooth interpolation function (rather than theabrupt one we employ) may be advantageous and isan interesting avenue for future research.One last observation is that the approaches thatuse diacritics from previous words (i.e., cons-onlyand hybrid) usually have lower sentence error rates(not shown in Figure 4).
This highlights an advan-tage of observation 5: that dependencies on previ-ously diacritized words can help ensure a consistenttagging within a sentence.7 Conclusions and Future WorkIn this paper, we have presented a low-resource so-lution for automatic diacritization.
Our approach ismotivated by empirical observations of the ambigu-ity and frequency of undiacritized and diacritizedwords as well as by the hypothesis that diacrit-ics from previous words provide useful informa-tion.
The main contributions of our work, basedon these observations, are (1) a hybrid word-levelCMM combined with a consonant-level model forrare words, (2) a consonant-level model with depen-dencies on previous diacritized words, (3) new fea-tures that leverage these dependencies, and (4) anefficient, approximate decoder for these models.
Asexpected, the efficacy of our approach varies acrosslanguages, due to differences in the actual ambigu-ity and frequency of words in these languages.
Nev-ertheless, our models consistently reduce WER by15% to nearly 50% over the best performing low-resource models in the literature.
In Arabic, ourmodels approach state-of-the-art despite not using amorphological analyzer.
Arguably, our results havebrought diacritization very close to being useful forpractical application, especially when consideringthat we evaluated our method on the most difficulttask in Arabic, which has been reported to have dou-ble the WER (Zitouni et al, 2006).The success of this low-resource solution natu-rally suggests that where more resources are avail-able (e.g., in Arabic), they could be used to furtherreduce error rates.
For instance, it may be fruitful toincorporate a morphological analyzer or segmenta-tion and part-of-speech tags.In future work, we would like to consider usingCRFs in place of MEMMs.
Also, other approximatedecoders used in pipeline approaches could be ex-plored as alternatives to the one we used (e.g., Finkelet al, 2006).
Additionally, we wish to include ourmodel as a stage in a pipeline that segments, dia-critizes, and labels morphemes.
Since obtaining datafor these tasks is substantially more expensive, wehope to use active learning to obtain more data.Our framework is applicable for any sequence la-beling task that can be done at either a word or asub-word (e.g., character) level.
Segmentation andlemmatization are particularly promising tasks towhich our approach could be applied.Finally, for the sake of completeness, we note thatmore recent work has been done based on our base-line models that has emerged since the preparationof the current work, particularly Zitouni et al (2009)and Mohamed et al (2009).
We wish to address anyimprovements captured by this more recent worksuch as the use of different data sets and addressingproblems with the hamza to decrease error rates.AcknowledgmentsWe thank Imed Zitouni, Nizar Habash, SandraKu?bler, and Emad Mohamed for their assistance inreconstructing datasets, models, and features.526ReferencesS.
Ananthakrishnan, S. Narayanan, and S. Bangalore.2005.
Automatic diacritization of Arabic transcriptsfor automatic speech recognition.
In Proceedings ofthe International Conference on Natural LanguageProcessing.A.
L. Berger, S. Della Pietra, and V. J. Della Pietra.
1996.Amaximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22:39?71.Y.
A. El-Imam.
2008.
Synthesis of the intonation of neu-trally spoken Modern Standard Arabic speech.
SignalProcessing, 88(9):2206?2221.T.
A. El-Sadany and M. A. Hashish.
1989.
An Ara-bic morphological system.
IBM Systems Journal,28(4):600?612.J.
R. Finkel, C. D. Manning, and A. Y. Ng.
2006.
Solv-ing the problem of cascading errors: ApproximateBayesian inference for linguistic annotation pipelines.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 618?626.Y.
Gal.
2002.
An HMM approach to vowel restorationin Arabic and Hebrew.
In Proceedings of the ACL-02 Workshop on Computational Approaches to SemiticLanguages, pages 1?7.N.
Habash and O. Rambow.
2007.
Arabic diacritiza-tion through full morphological tagging.
In HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Companion Volume, Short Pa-pers, pages 53?56.G.
Kiraz.
1994.
Automatic concordance generation ofSyriac texts.
In R. Lavenant, editor, VI SymposiumSyriacum 1992, pages 461?471, Rome, Italy.G.
A. Kiraz.
2000.
Multitiered nonlinear morphologyusing multitape finite automata: a case study on Syr-iac and Arabic.
Computational Linguistics, 26(1):77?105.D.
Klein and C. D. Manning.
2002.
Conditional structureversus conditional estimation in NLP models.
In Pro-ceedings of the 2002 Conference on Empirical Meth-ods in Natural Language Processing, pages 9?16.S.
Ku?bler and E. Mohamed.
2008.
Memory-based vocal-ization of Arabic.
In Proceedings of the LREC Work-shop on HLT and NLP within the Arabic World.M.
Maamouri, A. Bies, T. Buckwalter, and W. Mekki.2004.
The Penn Arabic Treebank: Building a large-scale annotated Arabic corpus.
In Proceedings of theNEMLAR Conference on Arabic Language Resourcesand Tools, pages 102?109.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19:313?330.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy Markov models for information extrac-tion and segmentation.
In Proceedings of the 17th In-ternational Conference on Machine Learning, pages591?598.E.
Mohamed and S. Ku?bler.
2009.
Diacritization forreal-world Arabic texts.
In Proceedings of Recent Ad-vances in Natural Language Processing 2009.R.
Nelken and S. M. Shieber.
2005.
Arabic diacritiza-tion using weighted finite-state transducers.
In Pro-ceedings of the ACL Workshop on Computational Ap-proaches to Semitic Languages, pages 79?86.J.
Pearl and G. Shafer.
1988.
Probabilistic reasoningin intelligent systems: networks of plausible inference.Morgan Kaufman, San Mateo, CA.D.
Vergyri and K. Kirchhoff.
2004.
Automatic diacritiza-tion of Arabic for acoustic modeling in speech recog-nition.
In Proceedings of the COLING 2004Workshopon Computational Approaches to Arabic Script-basedLanguages, pages 66?73.D.
Yarowsky.
1999.
A comparison of corpus-based tech-niques for restoring accents in Spanish and Frenchtext.
Natural language processing using very largecorpora, pages 99?120.Zefania XML Project.
2009.
Zefania XML bible:Leningrad codex.
http://sourceforge.net/projects/zefania-sharp/files/Zefania\%20XML\%20Bibles\%204\%20hebraica/Leningrad\%20Codex/sf_wcl.zip/download.I.
Zitouni and R. Sarikaya.
2009.
Arabic diacriticrestoration approach based on maximum entropymod-els.
Computer Speech & Language, 23(3):257?276.I.
Zitouni, J. S. Sorensen, and R. Sarikaya.
2006.
Max-imum entropy based restoration of Arabic diacritics.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 577?584.527
