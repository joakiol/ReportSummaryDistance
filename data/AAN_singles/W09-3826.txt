Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 172?175,Paris, October 2009. c?2009 Association for Computational LinguisticsGrammar Error Detection with Best Approximated ParseJean-Philippe ProstLIFO, Universite?
d?Orle?ansINRIA Lille - Nord EuropeJean-Philippe.Prost@univ-orleans.frAbstractIn this paper, we propose that grammar er-ror detection be disambiguated in generat-ing the connected parse(s) of optimal meritfor the full input utterance, in overcom-ing the cheapest error.
The detected er-ror(s) are described as violated grammat-ical constraints in a framework for Model-Theoretic Syntax (MTS).
We present aparsing algorithm for MTS, which only re-lies on a grammar of well-formedness, inthat the process does not require any extra-grammatical resources, additional rulesfor constraint relaxation or error handling,or any recovery process.1 IntroductionGrammar error detection is a crucial part ofNLP applications such as Grammar Checking orComputer-Assisted Language Learning (CALL).The problem is made highly ambiguous dependingon which context is used for interpreting, and thuspinpointing, the error.
For example, a phrase maylook perfectly fine when isolated (e.g.
brief inter-view), but is erroneous in a specific context (e.g.in *The judge grants brief interview to this plain-tiff, or in *The judges brief interview this plain-tiff ).
Robust partial parsing is often not enough toprecisely desambiguate those cases.
The solutionwe prescribe is to point out the error(s) as a setof violated (atomic) constraints of minimal cost,along with the structural context used for measur-ing that cost.
Given an ungrammatical input string,the aim is then to provide an approximated rootedparse tree for it, along with a description of all thegrammatical constraints it violates.
For example,Figure 1 illustrates an approximated parse for anill-formed sentence in French, and the error be-ing detected in that context.
Property Grammar(Blache, 2001) provides an elegant framework forthat purpose.S15NP3D1LeTheN2jugejudgeVP9V8octroiegrants*NP7AP6A4brefbriefN5entretieninterviewPP10P11a`toNP12D13cethisN14plaignantplaintiffFigure 1: Approximated parse for an erroneous French sen-tence (the Noun ?entretien?
requires a Determiner).Most of the relevant approaches to robustknowledge-based parsing addresses the problemas a recovery process.
More specifically, weobserve three families of approaches in that re-spect: those relying on grammar mal-rules in or-der to specify how to correctly parse what oughtto be ungrammatical (Bender et al, 2004; Foster,2007); those relying on constraint relaxation ac-cording to specified relaxation rules (Douglas andDale, 1992); and those relying on constraint re-laxation with no relaxation rules, along with a re-covery process based on weighted parsing (Fou-vry, 2003; Foth et al, 2005).
The first two areactually quite similar, in that, through their useof extra-grammatical rules, they both extend thegrammar?s coverage with a set of ought-to-be-ungrammatical utterances.
The main drawbackof those approaches is that when faced with un-expected input at best their outcome remains un-known, at worst the parsing process fails.
Withrobust weighted parsing, on the other hand, thatproblem does not occur.
The recovery processconsists of filtering out structures with respect totheir weights or the weights of the constraints be-ing relaxed.
However, these strategies usuallycan not discriminate between grammatical and un-grammatical sentences.
The reason for that comes172from the fact that grammaticality is disconnectedfrom grammar consistency: since the grammarcontains contradicting (universal) constraints, noconclusion can be drawn with regard to the gram-maticality of a syntactic structure, which violatespart of the constraint system.
The same problemoccurs with Optimality Theory.
In a different fash-ion, Fouvry weighs unification constraints accord-ing to ?how much information it contains?.
How-ever, relaxation only seems possible for those uni-fication constraints: error patterns such as wordorder, co-occurrence, uniqueness, mutual exclu-sion, .
.
.
can not be tackled.
The same restriction isobserved in VanRullen (2005), though to a muchsmaller extent in terms of unrelaxable constraints.What we would like is (i) to detect any typeof errors, and present them as conditions of well-formedness being violated in solely relying on theknowledge of a grammar of well-formedness?asopposed to an error grammar or mal-rules, and(ii) to present, along-side the violated constraints,an approximated parse for the full sentence, whichmay explain which errors have been found andovercome.
We propose here a parsing algorithmwhich meets these requirements.2 Property GrammarThe framework we are using for knowledge rep-resentation is Property Grammar (Blache, 2001)(PG), whose model-theoretical semantics was for-malised by Duchier et al (2009).
Intuitively, aPG grammar decomposes what would be rewritingrules of a generative grammar into atomic syntac-tic properties ?
a property being represented as aboolean constraint.
Take, for instance, the rewrit-ing rule NP ?
D N. That rule implicitely informson different properties (for French): (1) NP has aD child; (2) the D child is unique; (3) NP has anN child; (4) the N child is unique; (5) the D childprecedes the N child; (6) the N child requires theD child.
PG defines a set of axioms, each axiomcorresponding to a constraint type.
The proper-ties above are then specified in the grammar as thefollowing constraints: (1) NP :M D; (2) NP : D!
;(3) NP :M N; (4) NP : N!
; (5) NP : D ?
N; (6)NP : N ?
D. These constraints can be indepen-dently violated.
A PG grammar is traditionallypresented as a collection of Categories (or Con-structions), each of them being specified by a setof constraints.
Table 1 shows an example of acategory.
The class of models we are workingNP (Noun Phrase)Features Property Type : Properties[AVM]obligation : NP:M(N ?
PRO)uniqueness : NP: D!
: NP: N!
: NP: PP!
: NP: PRO!linearity : NP: D ?
N: NP: D ?
PRO: NP: D ?
AP: NP: N ?
PPrequirement : NP: N ?
D: NP: AP ?
Nexclusion : NP: N < PROdependency : NP: N?GEND 1NUM 2?
D?GEND 1NUM 2?Table 1: NP specification in Property Grammarwith is made up of trees labelled with categories,whose surface realisations are the sentences ?
oflanguage.
A syntax tree of the realisation of thewell-formed sentence ?
is a strong model of thePG grammar G iff it satisfies every constraint in G.The loose semantics also allows for constraints tobe relaxed.
Informally, a syntax tree of the realisa-tion of the ill-formed sentence ?
is a loose modelof G iff it maximises the proportion of satisfiedconstraints in G with respect to the total numberof evaluated ones for a given category.
The set ofviolated constraints provides a description of thedetected error(s).3 Parsing AlgorithmThe class of models is further restricted to con-stituent tree structures with no pairwise intersect-ing constituents, satisfying at least one constraint.Since the solution parse must have a single root,should a category not be found for a node a wild-card (called Star) is used instead.
The Star cate-gory is not specified by any constraint in the gram-mar.We introduce an algorithm for Loose Satisfac-tion Chart Parsing (LSCP), presented as Algo-rithm 1.
We have named our implementation of itNumbat.
LSCP is based on the probabilistic CKY,augmented with a process of loose constraint sat-isfaction.
However, LSCP differs from CKY invarious respects.
While CKY requires a grammarin Chomsky Normal Form (CNF), LSCP takes anordinary PG grammar, since no equivalent of theCNF exists for PG.
Consequently, LSCP gener-ates n-ary structures.
LSCP also uses scores ofmerit instead of probabilities for the constituents.That score can be optimised, since it only factorsthrough the influence of the constituent?s immedi-ate descendants.Steps 1 and 2 enumerate all the possible and173Algorithm 1 Loose Satisfaction Chart Parsing/?
Initialisation ?/Create and clear the chart pi: every score in pi is set to 0/?
Base case: populate pi with POS-tags for each word ?/for i?
1 to num wordsfor (each POS-category T of wi)if merit(T ) ?
pi[i, 1, T ] thenCreate constituent wTi , whose category is Tpi[i, 1, T ]?
{wTi , merit(wTi )}/?
Recursive case ?//?
Step 1: SELECTION of the current reference span ?/for span?
1 to num wordsfor offset ?
1 to num words?
span + 1end ?
offset + span?
1K ?
?/?
Step 2: ENUMERATION of all the configurations ?/for (every set partition P in [offset, .
.
.
, end])KP ?
buildConfigurations(P)K ?
K ?KP/?
Step 3: CHARACTERISATION of the constraint system from the grammar ?/for (every configurationA ?
KP )?A ?
characterisation(A)/?
Step 4: PROJECTION into categories ?//?
CA is a set of candidate constituents ?/CA ?
projection(?A )checkpoint(CA)/?
Step 5: MEMOISATION of the optimal candidate constituent ?/for (every candidate constituent x ?
CA, of construction C)if merit(x) ?
pi[offset, span, C] thenpi[offset, span, C]?
{x, merit(x)}if pi[offset, span] = ?
thenpi[offset, span]?
preferred forest inKlegal configurations of optimal sub-structures al-ready stored in the chart for a given span and off-set.
At this stage, a configuration is a tree withan unlabelled root.
Note that Step 2 actually doesnot calculate all the set partitions, but only the le-gal ones, i.e.
those which are made up of sub-sets of contiguous elements.
Step 3 evaluates theconstraint system, using a configuration as an as-signment.
The characterisation process is imple-mented with Algorithm 2.
Step 4 consists of mak-Algorithm 2 Characterisation Functionfunction characterisation(A = ?c1, .
.
.
, cn?
: assignment,G: grammar)returns the set of evaluated properties relevant toA,and the set of projected categories forA./?
For storing the result characterisation: ?/create and clear ?A [property]: table of boolean, indexed by property/?
For storing the result projected categories: ?/create and clear CA: set of category/?
For temporarily storing the properties to be evaluated: ?/create and clear S: set of propertyfor (mask ?
[1 .
.
.
2n ?
1])key?
applyBinaryMask(A,mask)if (key is in the set of indexes for G) then/?
Properties are retrieved from the grammar, then evaluated ?/S ?
G[key].getProperties()?A ?
evaluate(S)/?
Projection Step: fetch the categories to be projected ?/CA ?
G[key].getDominantCategories()return ?A , CAThe key is a hash-code of a combination of constructions, used for fetching theconstraints this combination is concerned with.ing a category judgement for a configuration, onthe basis of which constraints are satisfied and vi-olated, in order to label its root.
The process is asimple table lookup, the grammar being indexedby properties.
Step 5 then memoises the optimalsub-structures for every possible category.
Notethat the uniqueness of the solution is not guaran-teed, and there may well be many different parseswith exact same merit for a given input utterance.Should the current cell in the chart not beingpopulated with any constituents, a preferred for-est of partial parses (= Star category) is used in-stead.
The preferred forest is constructed on thefly (as part of buildConfigurations); a pointeris maintained to the preferred configuration dur-ing enumeration.
The preference goes to: (i) theconstituents with the widest span; (ii) the leastoverall number of constituents.
This translatesheuristically into a preference score pF computedas follows (where F is the forest, and Ci its con-stituents): pF = span ?
(merit(Ci) + span).
Inthat way, LSCP always delivers a parse for anyinput.
The technique is somehow similar to theone of Riezler et al (2002), where fragment parsesare allowed for achieving increased robustness, al-though their solution requires the standard gram-mar to be augmented with a fragment grammar.4 EvaluationIn order to measure Numbat?s ability to (i) detecterrors in an ungrammatical sentence, and (ii) buildthe best approximated parse for it, Numbat should,ideally, be evaluated on a corpus of both well-formed and ill-formed utterances annotated withspannnig phrase structures.
Unfortunately, sucha Gold Standard is not available to us.
The de-velopment of adequate resources is central to fu-ture works.
In order to (partially) overcome thatproblem we have carried out two distinct evalua-tions: one aims to measure Numbat?s performanceon grammatical sentences, and the other one onungrammatical sentences.
Evaluation 1, whose re-sults are reported in Table 2, follows the proto-col devised for the EASY evaluation campaign ofparsers of French (Paroubek et al, 2003), with asubset of the campaign?s corpus.
For comparison,Table 3 reports the performance measured underthe same circumstances for two other parsers: ashallow one (VanRullen, 2005) also based on PG,and a stochastic one (VanRullen et al, 2006).
Thegrammar used for that evaluation was developedby VanRullen (2005).
Evaluation 2 was run on174Precision Recall FTotal 0.7835 0.7057 0.7416general lemonde 0.8187 0.7515 0.7837general mlcc 0.7175 0.6366 0.6746general senat 0.8647 0.7069 0.7779litteraire 0.8124 0.7651 0.788mail 0.7193 0.6951 0.707medical 0.8573 0.678 0.757oral delic 0.6817 0.621 0.649questions amaryllis 0.8081 0.7432 0.7743questions trec 0.8208 0.7069 0.7596Table 2: EASY scores of Numbat (Eval.
1)Precision Recall Fshallow parser 0.7846 0.8376 0.8102stochastic parser 0.9013 0.8978 0.8995Table 3: Comparative EASY scoresa corpus of unannotated ungrammatical sentences(Blache et al, 2006), where each of the ungram-matical sentences (amounting to 94% of the cor-pus) matches a controlled error pattern.
Five ex-pert annotators were asked whether the solutiontrees were possible and acceptable syntactic parsesfor their corresponding sentence.
Specific instruc-tions were given to make sure that the judgementdoes not hold on the grammatical acceptability ofthe surface sentence as such, but actually on theparse associated with it.
For that evaluation Van-Rullen?s grammar was completed with nested cat-egories (since the EASY annotation scheme onlyhas chunks).
Given the nature of the material tobe assessed here, the Precision and Recall mea-surements had to be modified.
The total numberof input sentences is interpreted as the number ofpredictions; the number of COMPLETE structuresis interpreted as the number of observations; andthe number of structures evaluated as CORRECTby human judges is interpreted as the number ofcorrect solutions.
Hence the following formula-tions and scores: Precision=CORRECT/COMPLETE=0.74;Recall=CORRECT/Total=0.68; F=0.71.
92% of the cor-pus is analysed with a complete structure; 74% ofthese complete parses were judged as syntacticallycorrect.
The Recall score indicates that the correctparses represent 68% of the corpus.
In spite of alack of a real baseline, these scores compare withthose of grammatical parsers.5 ConclusionIn this paper, we have proposed to address theproblem of grammar error detection in providinga set of violated syntactic properties for an ill-formed sentence, along with the best structuralcontext in the form of a connected syntax tree.
Wehave introduced an algorithm for Loose Satisfac-tion Chart Parsing (LSCP) which meets those re-quirements, and presented performance measuresfor it.
Future work includes optimisation of LSCPand validation on more appropriate corpora.AcknowledgementPartly funded by ANR-07-MDCO-03 (CRoTAL).ReferencesE.
M. Bender, D. Flickinger, S. Oepen, A. Walsh, andT.
Baldwin.
2004.
Arboretum: Using a precisiongrammar for grammar checking in CALL.
In Proc.of InSTIL/ICALL2004, volume 17, page 19.P.
Blache, B. Hemforth, and S. Rauzy.
2006.
Ac-ceptability Prediction by Means of GrammaticalityQuantification.
In Proc.
of CoLing/ACL, pages 57?64.
ACL.P.
Blache.
2001.
Les Grammaires de Proprie?te?s :des contraintes pour le traitement automatique deslangues naturelles.
Herme`s Sciences.S.
Douglas and R. Dale.
1992.
Towards Robust PATR.In Proc.
of CoLing, volume 2, pages 468?474.
ACL.D.
Duchier, J-P. Prost, and T-B-H. Dao.
2009.A Model-Theoretic Framework for GrammaticalityJudgements.
In To appear in Proc.
of FG?09, vol-ume 5591 of LNCS.
FOLLI, Springer.J.
Foster.
2007.
Real bad grammar: Realistic grammat-ical description with grammaticality.
Corpus Lin-guistics and Lingustic Theory, 3(1):73?86.K.
Foth, W. Menzel, and I. Schro?der.
2005.
RobustParsing with Weighted Constraints.
Natural Lan-guage Engineering, 11(1):1?25.F.
Fouvry.
2003.
Constraint relaxation with weightedfeature structures.
pages 103?114.P.
Paroubek, I. Robba, and A. Vilnat.
2003.
EASY:An Evaluation Protocol for Syntactic Parsers.www.limsi.fr/RS2005/chm/lir/lir11/ (08/2008).S.
Riezler, T. H. King, R. M. Kaplan, R. Crouch,J.
T. III Maxwell, and M. Johnson.
2002.Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative EstimationTechniques.
In Proc.
of ACL, pages 271?278.
ACL.T.
VanRullen, P. Blache, and J-M. Balfourier.
2006.Constraint-Based Parsing as an Efficient Solution:Results from the Parsing Evaluation CampaignEASy.
In Proc.
of LREC, pages 165?170.T.
VanRullen.
2005.
Vers une analyse syntaxique a`granularite?
variable.
The`se de doctorat.175
