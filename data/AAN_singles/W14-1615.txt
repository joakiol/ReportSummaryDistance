Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141?150,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsWeakly-Supervised Bayesian Learning of a CCG SupertaggerDan Garrette?Chris Dyer?Jason Baldridge?Noah A.
Smith?
?Department of Computer Science, The University of Texas at Austin?School of Computer Science, Carnegie Mellon University?Department of Linguistics, The University of Texas at Austin?Corresponding author: dhg@cs.utexas.eduAbstractWe present a Bayesian formulation forweakly-supervised learning of a Combina-tory Categorial Grammar (CCG) supertag-ger with an HMM.
We assume supervi-sion in the form of a tag dictionary, andour prior encourages the use of cross-linguistically common category structuresas well as transitions between tags thatcan combine locally according to CCG?scombinators.
Our prior is theoretically ap-pealing since it is motivated by language-independent, universal properties of theCCG formalism.
Empirically, we showthat it yields substantial improvementsover previous work that used similar bi-ases to initialize an EM-based learner.
Ad-ditional gains are obtained by further shap-ing the prior with corpus-specific informa-tion that is extracted automatically fromraw text and a tag dictionary.1 IntroductionUnsupervised part-of-speech (POS) induction is aclassic problem in NLP.
Many proposed solutionsare based on Hidden Markov models (HMMs), withvarious improvements obtainable through: induc-tive bias in the form of tag dictionaries (Kupiec,1992; Merialdo, 1994), sparsity constraints (Leeet al., 2010), careful initialization of parameters(Goldberg et al., 2008), feature based represen-tations (Berg-Kirkpatrick et al., 2010; Smith andEisner, 2005), and priors on model parameters(Johnson, 2007; Goldwater and Griffiths, 2007;Blunsom and Cohn, 2011, inter alia).When tag dictionaries are available, a situa-tion we will call type-supervision, POS induc-tion from unlabeled corpora can be relatively suc-cessful; however, as the number of possible tagsincreases, performance drops (Ravi and Knight,2009).
In such cases, there are a large numberof possible labels for each token, so picking theright one simply by chance is unlikely; the pa-rameter space tends to be large; and devising goodinitial parameters is difficult.
Therefore, it is un-surprising that the unsupervised (or even weakly-supervised) learning of a Combinatory CategorialGrammar (CCG) supertagger, which labels eachword with one of a large (possibly unbounded)number of structured categories called supertags,is a considerable challenge.Despite the apparent complexity of the task, su-pertag sequences have regularities due to univer-sal properties of the CCG formalism (?2) that canbe used to reduce the complexity of the problem;previous work showed promising results by usingthese regularities to initialize an HMM that is thenrefined with EM (Baldridge, 2008).
Here, we ex-ploit CCG?s category structure to motivate a novelprior over HMM parameters for use in Bayesianlearning (?3).
This prior encourages (i) cross-linguistically common tag types, (ii) tag bigramsthat can combine using CCG?s combinators, and(iii) sparse transition distributions.
We also go be-yond the use of these universals to show how ad-ditional, corpus-specific information can be auto-matically extracted from a combination of the tagdictionary and raw data, and how that informationcan be combined with the universal knowledge forintegration into the model to improve the prior.We use a blocked sampling algorithm to sam-ple supertag sequences for the sentences in thetraining data, proportional to their posterior prob-ability (?4).
We experimentally verify thatour Bayesian formulation is effective and sub-stantially outperforms the state-of-the-art base-line initialization/EM strategy in several languages(?5).
We also evaluate using tag dictionaries thatare unpruned and have only partial word coverage,finding even greater improvements in these morerealistic scenarios.1412 CCG and SupertaggingCCG (Steedman, 2000; Steedman and Baldridge,2011) is a grammar formalism in which each lex-ical token is associated with a structured category,often referred to as a supertag.
CCG categories aredefined by the following recursive definition:C ?
{S, N, NP, PP, ...}C ?
{C/C,C\C}A CCG category can either be an atomic cate-gory indicating a particular type of basic gram-matical phrase (S for a sentence, N for a noun,NP for a noun phrase, etc), or a complex categoryformed from the combination of two categoriesby one of two slash operators.
In CCG, complexcategories indicate a grammatical relationship be-tween the two operands.
For example, the cate-gory (S\NP)/NP might describe a transitive verb,looking first to its right (indicated by /) for an ob-ject, then to its left (\) for a subject, to produce asentence.
Further, atomic categories may be aug-mented with features, such as Sdcl, to restrict theset of atoms with which they may unify.
The taskof assigning a category to each word in a text iscalled supertagging (Bangalore and Joshi, 1999).Because they are recursively defined, there isan infinite number of potential CCG categories(though in practice it is limited by the numberof actual grammatical contexts).
As a result, thenumber of supertags appearing in a corpus far ex-ceeds the number of POS tags (see Table 1).
Sincesupertags specify the grammatical context of a to-ken, and high frequency words appear in manycontexts, CCG grammars tend to have very highlexical ambiguity, with frequent word types asso-ciating with a large number of categories.
Thisambiguity has made type-supervised supertaggerlearning very difficult because the typical ap-proaches to initializing parameters for EM becomemuch less effective.Grammar-informed supertagger learning.Baldridge (2008) was successful in extending thestandard type-supervised tagger learning to thetask of CCG supertagging by setting the initialparameters for EM training of an HMM usingtwo intrinsic properties of the CCG formalism:the tendency for adjacent tags to combine, andthe tendency to use less complex tags.
Theseproperties are explained in detail in the originalwork, but we restate the ideas briefly throughoutthis paper for completeness.X/Y Y ?
X (>)Y X\Y ?
X (<)X/Y Y/Z ?
X/Z (>B)Y \Z X\Y ?
X\Z (<B)Y/Z X\Y ?
X/Z (<B?
)Figure 1: Combination rules used by CCGBank.SNPNP/NNS\NP(S\NP)/NPNPNP/NNThemanwalksadogFigure 2: CCG parse for ?The man walks a dog.
?Tag combinability.
A CCG parse of a sentence isderived by recursively combining the categories ofsub-phrases.
Category combination is performedusing only a small set of generic rules (see Fig-ure 1).
In the tree in Figure 2, we can see thata and dog can combine via Forward Application(>), with NP/N and N combining to produce NP.The associativity engendered by CCG?s compo-sition rules means that most adjacent lexical cate-gories may be combined.
In the Figure 2 tree, wecan see that instead of combining (walks?
(a?dog)),we could have combined ((walks?a)?dog) since(S\NP)/NP and NP/N can combine using >B.3 ModelIn this section we define the generative processwe use to model a corpus of sentences.
We beginby generating the model parameters: for eachsupertag type t in the tag set T , the transitionprobabilities to the next state (pit) and the emis-sion probabilities (?t) are generated by drawsfrom Dirichlet distributions parameterized withper-tag mean distributions (pi0tand ?0t, respec-tively) and concentration parameters (?piand??).
By setting ?piclose to zero, we can encodeour prior expectation that transition distributionsshould be relatively peaked (i.e., that each tagtype should be followed by relatively few tagtypes).
The prior means, discussed below, encodeboth linguistic intuitions about expected tag-tagtransition behavior and automatically-extractedcorpus information.
Given these parameters, wenext generate the sentences of the corpus.
Thisprocess is summarized as follows:142Parameters:?t?
Dirichlet(?
?, ?0t) ?t ?
Tpit?
Dirichlet(?pi, pi0t) ?t ?
TSentence:y1?
Categorical(pi?S?
)for i ?
{1, 2, .
.
.
}, until yi= ?E?xi| yi?
Categorical(?yi)yi+1| yi?
Categorical(piyi)This model can be understood as a BayesianHMM (Goldwater and Griffiths, 2007).
We nextdiscuss how the prior distributions are constructedto build in additional inductive bias.3.1 Transition Prior Means (pi0t)We use the prior mean for each tag?s transition dis-tribution to build in two kinds of bias.
First, wewant to favor linguistically probable tags.
Second,we want to favor transitions that result in a tagpair that combines according to CCG?s combina-tors.
For simplicity, we will define pi0tas a mixtureof two components, the first, Ppi(u) is an (uncon-ditional) distribution over category types u that fa-vors cross-linguistically probable categories.
Thesecond component, Ppi(u | t), conditions on theprevious tag type, t, and assigns higher probabil-ity to pairs of tags that can be combined.
That is,the probability of transitioning from t to u in theDirichlet mean distribution is given by1pi0t(u) = ?
?
Ppi(u) + (1?
?)
?
Ppi(u | t).We discuss the two mixture components in turn.3.1.1 Unigram Category Generator (Ppi(u))In this section, we define a CCG category gener-ator that generates cross-linguistically likely cat-egory types.
Baldridge?s approach estimated thelikelihood of a category using the inverse numberof sub-categories: PCPLX(u) ?
1/complexity(u).We propose an improvement, PG, expressed as aprobabilistic grammar:2C ?
a pterm?patom(a)C ?
A/A pterm?pfw?pmod?PG(A)C ?
A/B, A 6=B pterm?pfw?pmod?PG(A) ?PG(B)C ?
A\A pterm?pfw?pmod?PG(A)C ?
A\B, A 6=B pterm?pfw?pmod?PG(A) ?PG(B)1Following Baldridge (2008), we fix ?
= 0.5 for our ex-periments.2For readability, we use the notation p = (1?
p).where A,B,C are categories and a is an atomiccategory (and terminal): a ?
{S, N, NP, ...}.3We have designed this grammar to capture sev-eral important CCG characteristics.
In particularwe encode four main ideas, each captured througha different parameter of the grammar and dis-cussed in greater detail below:1.
Simpler categories are more likely: e.g.
N/N isa priori more likely than (N/N)/(N/N).2.
Some atoms are more likely than others: e.g.NP is more likely than S, much more than NPnb.3.
Modifiers are more likely: e.g.
(S\NP)/(S\NP)is more likely than (S\NP)/(NP\NP).4.
Operators occur with different frequencies.The first idea subsumes the complexity measureused by Baldridge, but accomplishes the goal nat-urally by letting the probabilities decrease as thecategory grows.
The rate of decay is governedby the ptermparameter: the marginal probabilityof generating a terminal (atomic) category in eachexpansion.
A higher ptermmeans a stronger em-phasis on simplicity.
The probability distributionover categories is guaranteed to be proper so longas pterm>12since the probability of the depth of atree will decrease geometrically (Chi, 1999).The second idea is a natural extension of thecomplexity concept and is particularly relevantwhen features are used.
The original complex-ity measure treated all atoms uniformly, but e.g.we would expect NPexpl/N to be less likely thanNP/N since it contains the more specialized, andthus rarer, atom NPexpl.
We define the distributionpatom(a) as the prior over atomic categories.Due to our weak, type-only supervision, wehave to estimate patomfrom just the tag dictionaryand raw corpus, without frequency data.
Our goalis to estimate the number of each atom in the su-pertags that should appear on the raw corpus to-kens.
Since we don?t know what the correct su-pertags are, we first estimate counts of supertags,from which we can extract estimated atom counts.Our strategy is to uniformly distribute each rawcorpus token?s counts over all of its possible su-pertags, as specified in the tag dictionary.
Wordtypes not appearing in the tag dictionary are ig-3While very similar to standard probabilistic context-freegrammars seen in NLP work, this grammar is not context-freebecause modifier categories must have matching operands.However, this is not a problem for our approach since thegrammar is unambiguous, defines a proper probability distri-bution, and is only used for modeling the relative likelihoodsof categories (not parsing categories).143nored for the purposes of these estimates.
Assum-ing that C(w) is the number of times that wordtype w is seen in the raw corpus, atoms(a, t) is thenumber of times atom a appears in t, TD(w) is theset of tags associated with w, and TD(t) is the setof word types associated with t:Csupertag(t) =?w?TD(t)(C(w)+?
)/|TD(w)|Catom(a) =?t?Tatoms(a, t) ?
Csupertag(t)patom(a) ?
Catom(a) + ?Adding ?
smooths the estimates.Using the raw corpus and tag dictionary data toset patomallows us to move beyond Baldridge?swork in another direction: it provides us with anatural way to combine CCG?s universal assump-tions with corpus-specific data.The third and fourth ideas pertain only to com-plex categories.
If the category is complex, thenwe consider two additional parameters.
The pa-rameter pfwis the marginal probability that thecomplex category?s operator specifies a forwardargument.
The parameter pmodgives the amountof marginal probability mass that is allocated formodifier categories.
Note that it is not necessaryfor pmodto be greater than12to achieve the de-sired result of making modifier categories morelikely than non-modifier categories: the numberof potential modifiers make up only a tiny fractionof the space of possible categories, so allocatingmore than that mass as pmodwill result in a cate-gory grammar that gives disproportionate weightto modifiers, increasing the likelihood of any par-ticular modifier from what it would otherwise be.3.1.2 Bigram Category Generator (Ppi(u | t))While the above processes encode important prop-erties of the distribution over categories, the in-ternal structure of categories is not the full story:cross-linguistically, the categories of adjacent to-kens are much more likely to be combinable viasome CCG rule.
This is the second component ofour mixture model.Baldridge derives this bias by allocating the ma-jority of the transition probability mass from eachtag t to tags that can follow t according to somecombination rule.
Let ?
(t,u) be an indicator ofwhether t connects to u; for ?
?
[0, 1]:4P?
(u | t) ={?
?
uniform(u) if ?(t,u)(1?
?)
?
uniform(u) otherwise4Again, following Baldridge (2008), we fix ?
= 0.95 forour experiments.There are a few additional considerations thatmust be made in defining ?, however.
In assum-ing the special tags ?S?
and ?E?
for the start andend of the sentence, respectively, we can define?
(?S?,u) = 1 when u seeks no left-side argu-ments (since there are no tags to the left withwhich to combine) and ?
(t, ?E?)
= 1 when t seeksno right-side arguments.
So ?
(?S?, NP/N) = 1, but?
(?S?, S\NP) = 0.
If atoms have features asso-ciated, then the atoms are allowed to unify if thefeatures match, or if at least one of them doesnot have a feature.
So ?
(NPnb, S\NP) = 1, but?
(NPnb, S\NPconj) = 0.
In defining ?, it is also im-portant to ignore possible arguments on the wrongside of the combination since they can be con-sumed without affecting the connection betweenthe two.
To achieve this for ?
(t,u), it is assumedthat it is possible to consume all preceding argu-ments of t and all following arguments of u.
So?
(NP, (S\NP)/NP) = 1.
This helps to ensure theassociativity discussed earlier.
Finally, the atomNP is allowed to unify with N if N is the argument.So ?
(N, S\NP) = 1, but ?
(NP/N, NP) = 0.
This isdue to the fact that CCGBank assumes that N canbe rewritten as NP.Type-supervised initialization.
As above, wewant to improve upon Baldridge?s ideas by en-coding not just universal CCG knowledge, butalso automatically-induced corpus-specific infor-mation where possible.
To that end, we can de-fine a conditional distribution Ptr(u | t) based onstatistics from the raw corpus and tag dictionary.We use the same approach as we did above for set-ting patom(and the definition of ?0tbelow): we esti-mate by evenly distributing raw corpus counts overthe tag dictionary entries.
Assume that C(w1, w2)is the (?-smoothed) count of times word type w1was directly followed byw2in the raw corpus, andignoring any words not found in the tag dictionary:C(t,u) = ?+?w1?TD(t), w2?TD(u)C(w1, w2)|TD(w1)| ?
|TD(w2)|Ptr(u | t) = C(t,u)/?u?C(t,u?
)Then the alternative definition of the compatibilitydistribution is as follows:Ptr?
(u | t) ={?
?
Ptr(u | t) if ?(t,u)(1??)
?
Ptr(u | t) otherwise144Our experiments compare performance whenpi0tis set using Ppi(u)=PCPLX(experiment 3) ver-sus our category grammar PG(4?6), and usingPpi(u | t) = P?as the compatibility distribution(3?4) versus Ptr?
(5?6).3.2 Emission Prior Means (?0t)For each supertag type t, ?0tis the mean distri-bution over words it emits.
While Baldridge?sapproach used a uniform emission initialization,treating all words as equally likely, we can,again, induce token-level corpus-specific informa-tion:5To set ?0t, we use a variant and simplifica-tion of the procedure introduced by Garrette andBaldridge (2012) that takes advantage of our priorover categories PG.Assuming that C(w) is the count of word typew in the raw corpus, TD(w) is the set of supertagsassociated with word type w in the tag dictionary,and TD(t) is the set of known word types associ-ated with supertag t, the count of word/tag pairsfor known words (words appearing in the tag dic-tionary) is estimated by uniformly distributing aword?s (?-smoothed) raw counts over its tag dic-tionary entries:Cknown(t, w) ={C(w)+?|TD(w)|if t ?
TD(w)0 otherwiseFor unknown words, we first use the idea of tag?openness?
to estimate the likelihood of a partic-ular tag t applying to an unknown word: if a tagapplies to many word types, it is likely to apply tosome new word type.P (unk | t) ?
|known words w s.t.
t ?
TD(w)|Then, we apply Bayes?
rule to get P (t | unk), anduse that to estimate word/tag counts for unknownwords:P (t | unk) ?
P (unk | t) ?
PG(t)Cunk(t, w) = C(w) ?
P (t | unk)Thus, with the estimated counts for all words:Pem(w | t) =Cknown(t, w) + Cunk(t, w)?w?Cknown(t, w?)
+ Cunk(t, w?
)We perform experiments comparing perfor-mance when ?0tis uniform (3?5) and when?0t(w) = Pem(w | t) (6).5Again, without gold tag frequencies.4 Posterior InferenceWe wish to find the most likely supertag of eachword, given the model we just described and a cor-pus of training data.
Since there is exact inferencewith these models is intractable, we resort to Gibbssampling to find an approximate solution.
At ahigh level, we alternate between resampling modelparameters (?t, pit) given the current tag sequenceand resampling tag sequences given the currentmodel parameters and observed word sequences.It is possible to sample a new tagging from theposterior distribution over tag sequences for a sen-tence, given the sentence and the HMM parametersusing the forward-filter backward-sample (FFBS)algorithm (Carter and Kohn, 1996).
To effi-ciently sample new HMM parameters, we exploitDirichlet-multinomial conjugacy.
By repeatingthese alternating steps and accumulating the num-ber of times each supertag is used in each position,we obtain an approximation of the required poste-rior quantities.Our inference procedure takes as input the tran-sition prior means pi0t, the emission prior means?0t, and concentration parameters ?piand ?
?,along with the raw corpus and tag dictionary.
Theset of supertags associated with a word w will beknown as TD(w).
We will refer to the set of wordtypes included in the tag dictionary as ?known?words and others as ?unknown?
words.
For sim-plicity, we will assume that TD(w), for any un-known word w, is the full set of CCG categories.During sampling, we always restrict the possibletag choices for a word w to the categories found inTD(w).
We refer to the sequence of word tokensas x and tags as y.We initialize the sampler by setting pit= pi0tand ?t= ?0tand then sampling tagging sequencesusing FFBS.To sample a tagging for a sentence x, the strat-egy is to inductively compute, for each token xistarting with i = 0 and going ?forward?, the prob-ability of generating x0, x1, .
.
.
, xivia any tag se-quence that ends with yi= u:p(yi= u | x0:i) =?u(xi) ?
?t?Tpit(u) ?
p(yi?1= t | x0:i?1)We then pass through the sequence again, this time?backward?
starting at i = |x| ?
1 and samplingyi| yi+1?
p(yi= t | x0:i) ?
pit(yi+1).145num.
raw TD TD ambiguity dev testCorpus tags tokens tokens entries type token tokens tokensEnglishCCGBank POS 50158k 735k45k 3.75 13.11 ?
?CCGBank 1,171 65k 56.98 296.18 128k 127kChinese CTB-CCG 829 99k 439k 60k 96.58 323.37 59k 85kItalian CCG-TUT 955 6k 27k 9k 178.88 426.13 5k 5kTable 1: Statistics for the various corpora used.
CCGBank is English, CCG-CTB is Chinese, and TUTis Italian.
The number of tags includes only those tags found in the tag dictionary (TD).
Ambiguity ratesare the average number of entries in the unpruned tag dictionary for each word in the raw corpus.
EnglishPOS statistics are shown only for comparison; only CCG experiments were run.The block-sampling approach of choosing newtags for a sentence all at once is particularly ben-eficial given the sequential nature of the model ofthe HMM.
In an HMM, a token?s adjacent tags tendto hold onto its current tag due to the relation-ships between the three.
Resampling all tags atonce allows for more drastic changes at each it-eration, providing better opportunities for mixingduring inference.
The FFBS approach has the ad-ditional advantage that, by resampling the distri-butions only once per iteration, we are able to re-sample all sentences in parallel.
This is not strictlytrue of all HMM problems with FFBS, but becauseour data is divided by sentence, and each sentencehas a known start and end tag, the tags chosen dur-ing the sampling of one sentence cannot affect thesampling of another sentence in the same iteration.Once we have sampled tags for the entire cor-pus, we resample pi and ?.
The newly-sampledtags y are used to compute C(w, t), the count oftokens with word type w and tag t, and C(t,u),the number of times tag t is directly followed bytag u.
We then sample, for each t ?
T where T isthe full set of valid CCG categories:pit?
Dir(??pi?
pi0t(u) + C(t,u)?u?T)?t?
Dir(????
?0t(w) + C(w, t)?w?V)It is important to note that this method of re-sampling allows the draws to incorporate both thedata, in the form of counts, and the prior mean,which includes all of our carefully-constructed bi-ases derived from both the intrinsic, universal CCGproperties as well as the information we inducedfrom the raw corpus and tag dictionary.With the distributions resampled, we can con-tinue the procedure by resampling tags as above,and then resampling distributions again, until amaximum number of iterations is reached.5 Experiments6To evaluate our approach, we used CCGBank(Hockenmaier and Steedman, 2007), which isa transformation of the English Penn Treebank(Marcus et al., 1993); the CTB-CCG (Tse andCurran, 2010) transformation of the Penn ChineseTreebank (Xue et al., 2005); and the CCG-TUTcorpus (Bos et al., 2009), built from the TUT cor-pus of Italian text (Bosco et al., 2000).
Statisticson the size and ambiguity of these datasets areshown in Table 1.For CCGBank, sections 00?15 were used forextracting the tag dictionary, 16?18 for the rawcorpus, 19?21 for development data, and 22?24for test data.
For TUT, the first 150 sentences ofeach of the CIVIL LAW and NEWSPAPER sectionswere used for raw data, the next sentences 150?249 of each was used for development, and thesentences 250?349 were used for test; the remain-ing data, 457 sentences from CIVIL LAW and 548from NEWSPAPER, plus the much smaller 132-sentence JRC ACQUIS data, was used for the tagdictionary.
For CTB-CCG, sections 00?11 wereused for the tag dictionary, 20?24 for raw, 25?27for dev, and 28?31 for test.Because we are interested in showing the rel-ative gains that our ideas provide over Baldridge(2008), we reimplemented the initialization pro-cedure from that paper, allowing us to evaluateall approaches consistently.
For each dataset, weran a series of experiments in which we made fur-ther changes from the original work.
We first rana baseline experiment with uniform transition andemission initialization of EM (indicated as ?1.?
inTable 2) followed by our reimplementation of theinitialization procedure by Baldridge (2).
We then6All code and experimental scripts are availableat http://www.github.com/dhgarrette/2014-ccg-supertagging146Corpus English Chinese ItalianTD cutoff 0.1 0.01 0.001 no 0.1 0.01 0.001 no 0.1 0.01 0.001 no1.
uniform EM 77 62 47 38 64 39 30 26 51 32 30 302. init (Baldridge) EM 78 67 55 41 66 43 33 28 54 36 33 323. init Bayes 74 68 56 42 65 56 47 37 52 46 40 404.
PGBayes 74 70 59 42 64 57 47 36 52 40 39 405.
PG, Ptr?Bayes 75 72 61 50 66 58 49 44 52 44 41 436.
PG, Ptr?, PemBayes 80 80 73 51 69 62 56 49 53 47 45 46Table 2: Experimental results: test-set per-token supertag accuracies.
?TD cutoff?
indicates the level oftag dictionary pruning; see text.
(1) is uniform EM initialization.
(2) is a reimplementation of (Baldridge,2008).
(3) is Bayesian formulation using only the ideas from Baldridge: PCPLX, P?, and uniform emis-sions.
(4?6) are our enhancements to the prior: using our category grammar in PGinstead of PCPLX, usingPtr?instead of P?, and using Peminstead of uniform.experimented with the Bayesian formulation, firstusing the same information used by Baldridge, andthen adding our enhancements: using our categorygrammar in PG, using Ptr?as the transition com-patability distribution, and using Pemas ?0t(w).For each dataset, we ran experiments using fourdifferent levels of tag dictionary pruning.
Prun-ing is the process of artificially removing noisefrom the tag dictionary by using token-level anno-tation counts to discard low-probability tags; foreach word, for cutoff x, any tag with probabilityless than x is excluded.
Tag dictionary pruningis a standard procedure in type-supervised train-ing, but because it requires information that doesnot truly conform to the type-supervised scenario,we felt that it was critical to demonstrate the per-formance of our approach under situations of lesspruning, including no artificial pruning at all.We emphasize that unlike in most previouswork, we use incomplete tag dictionaries.
Mostprevious work makes the unrealistic assumptionthat the tag dictionary contains an entry for ev-ery word that appears in either the training or test-ing data.
This is a poor approximation of a realtagging system, which will never have completelexical knowledge about the test data.
Even workthat only assumes complete knowledge of the tag-ging possibilities for the lexical items in the train-ing corpus is problematic (Baldridge, 2008; Raviet al., 2010).
This still makes learning unrealisti-cally easy since it dramatically reduces the ambi-guity of words that would have been unseen, and,in the case of CCG, introduces additional tags thatwould not have otherwise been known.
To ensurethat our experiments are more realistic, we drawour tag dictionary entries from data that is totallydisjoint from both the raw and test corpora.
Dur-ing learning, any unknown words (words not ap-pearing in the tag dictionary) are unconstrained sothat they may take any tag, and are, thus, maxi-mally ambiguous.We only performed minimal parameter tuning,choosing instead to stay consistent with Baldridge(2008) and simply pick reasonable-seeming val-ues for any additional parameters.
Any tuning thatwas performed was done with simple hill-climbingon the development data of English CCGBank.All parameters were held consistent across exper-iments, including across languages.
For EM, weused 50 iterations; for FFBS we used 100 burn-in iterations and 200 sampling iterations.7Forall experiments, we used ?
= 0.95 for P(tr)?and?
= 0.5 for pi0tto be consistent with previouswork, ?pi= 3000, ?
?= 7000, pterm= 0.6,pfw= 0.5, pmod= 0.8, and ?
= 1000 for patom.Test data was run only once, for the final figures.The final results reported were achieved by us-ing the following training sequence: initialize pa-rameters according to the scenario, train an HMMusing EM or FFBS starting with that set of parame-ters, tag the raw corpus with the trained HMM, add-0.1 smooth counts from the now-tagged raw cor-pus, and train a maximum entropy Markov model(MEMM) from this ?auto-supervised?
data.8Results are shown in Table 2.
Most notably, thecontributions described in this paper improve re-sults in nearly every experimental scenario.
Wecan see immediate, often sizable, gains in most7Final counts are averaged across the sampling iterations.8Auto-supervised training of an MEMM increases accu-racy by 1?3% on average (Garrette and Baldridge, 2013).
Weuse the OpenNLP MEMM implementation with its standardset of features: http://opennlp.apache.org147cases simply by using the Bayesian formulation.Further gains are seen from adding each of theother various contributions of this paper.
Perhapsmost interestingly, the gains are only minimal withmaximum pruning, but the gains increase as thepruning becomes less aggressive ?
as the scenar-ios become more realistic.
This indicates that ourimprovements make the overall procedure morerobust.Error Analysis Like POS-taggers, the learnedsupertagger frequently confuses nouns (N) andtheir modifiers (N/N), but the most frequent er-ror made by the English (6) experiment was(((S\NP)\(S\NP))/N) instead of (NPnb/N).
How-ever, these are both determiner types, indicating aninteresting problem for the supertagger: it oftenpredicts an object type-raised determiner insteadof the vanilla NP/N, but in many contexts, both cat-egories are equally valid.
(In fact, for parsers thatuse type-raising as a rule, this distinction in lexicalcategories does not exist.
)6 Related WorkRavi et al.
(2010) also improved upon the work byBaldridge (2008) by using integer linear program-ming to find a minimal model of supertag transi-tions, thereby generating a better starting point forEM than the grammatical constraints alone couldprovide.
This approach is complementary to thework presented here, and because we have shownthat our work yields gains under tag dictionariesof various levels of cleanliness, it is probable thatemploying minimization to set the base distribu-tion for sampling could lead to still higher gains.On the Bayesian side, Van Gael et al.
(2009)used a non-parametric, infinite HMM for truly un-supervised POS-tagger learning (Van Gael et al.,2008; Beal et al., 2001).
While their model is notrestricted to the standard set of POS tags, and maylearn a more fine-grained set of labels, the inducedlabels are arbitrary and not grounded in any gram-matical formalism.Bisk and Hockenmaier (2013) developed an ap-proach to CCG grammar induction that does notuse a tag dictionary.
Like ours, their procedurelearns from general properties of the CCG formal-ism.
However, while our work is intended to pro-duce categories that match those used in a partic-ular training corpus, however complex they mightbe, their work produces categories in a simplifiedform of CCG in which N and S are the only atomsand no atoms have features.
Additionally, they as-sume that their training corpus is annotated withPOS tags, whereas we assume truly raw text.Finally, we find the task of weakly-supervisedsupertagger learning to be particularly relevantgiven the recent surge in popularity of CCG.An array of NLP applications have begun usingCCG, including semantic parsing (Zettlemoyer andCollins, 2005) and machine translation (Weese etal., 2012).
As CCG finds more applications, andas these applications move to lower-resource do-mains and languages, there will be increased needfor the ability to learn without full supervision.7 Conclusion and Future WorkStandard strategies for type-supervised HMM es-timation are less effective as the number of cat-egories increases.
In contrast to POS tag sets,CCG supertags, while quite numerous, have struc-tural clues that can simplify the learning prob-lem.
Baldridge (2008) used this formalism-specific structure to inform an initialization pro-cedure for EM.
In this work, we have shown thatCCG structure can instead be used to motivate aneffective prior distribution over the parameters ofan HMM supertagging model, allowing our workto outperform Baldridge?s previously state-of-the-art approach, and to do so in a principled mannerthat lends itself better to future extensions such asincorporation in more complex models.This work also improves on Baldridge?s simple?complexity?
measure, developing instead a prob-abilistic category grammar over supertags that al-lows our prior to capture a wider variety of inter-esting and useful properties of the CCG formalism.Finally, we were able to achieve further gainsby augmenting the universal CCG knowledge withcorpus-specific information that could be automat-ically extracted from the weak supervision that isavailable: the raw corpus and the tag dictionary.This allows us to combine the cross-linguisticproperties of the CCG formalism with corpus- orlanguage-specific information in the data into asingle, unified Bayesian prior.Our model uses a relatively large number of pa-rameters, e.g., pterm, pfw, pmod, patom, in the prior.Here, we fixed each to a single value (i.e., a ?fullyBayesian?
approach).
Future work might exploresensitivity to these choices, or empirical Bayesianor maximum a posteriori inference for their values(Johnson and Goldwater, 2009).148In this work, as in most type-supervised work,the tag dictionary was automatically extractedfrom an existing tagged corpus.
However, a tagdictionary could instead be automatically inducedvia multi-lingual transfer (Das and Petrov, 2011)or generalized from human-provided information(Garrette and Baldridge, 2013; Garrette et al.,2013).
Again, since the approach presented herehas been shown to be somewhat robust to tag dic-tionary noise, it is likely that the model wouldperform well even when using an automatically-induced tag dictionary.AcknowledgementsThis work was supported by the U.S. Departmentof Defense through the U.S. Army Research Of-fice (grant number W911NF-10-1-0533).
Exper-iments were run on the UTCS Mastodon Cluster,provided by NSF grant EIA-0303609.ReferencesJason Baldridge.
2008.
Weakly supervised supertag-ging with grammar-informed initialization.
In Pro-ceedings of COLING.Srinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational Linguistics, 25(2).Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-ward Rasmussen.
2001.
The innite hidden Markovmodel.
In NIPS.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofNAACL.Yonatan Bisk and Julia Hockenmaier.
2013.
An HDPmodel for inducing combinatory categorial gram-mars.
Transactions of the Association for Compu-tational Linguistics, 1.Phil Blunsom and Trevor Cohn.
2011.
A hierarchicalPitman-Yor process HMM for unsupervised part ofspeech induction.
In Proceedings of ACL.Johan Bos, Cristina Bosco, and Alessandro Mazzei.2009.
Converting a dependency treebank to a cat-egorial grammar treebank for Italian.
In M. Pas-sarotti, Adam Przepi?orkowski, S. Raynaud, andFrank Van Eynde, editors, Proceedings of the EighthInternational Workshop on Treebanks and LinguisticTheories (TLT8).Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,and Leonardo Lesmo.
2000.
Building a treebankfor Italian: a data-driven annotation schema.
In Pro-ceedings of LREC.Christopher K. Carter and Robert Kohn.
1996.
OnGibbs sampling for state space models.
Biometrika,81(3):341?553.Zhiyi Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguistics,25(1).Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of ACL-HLT.Dan Garrette and Jason Baldridge.
2012.
Type-supervised hidden Markov models for part-of-speech tagging with incomplete tag dictionaries.
InProceedings of EMNLP.Dan Garrette and Jason Baldridge.
2013.
Learning apart-of-speech tagger from two hours of annotation.In Proceedings of NAACL.Dan Garrette, Jason Mielens, and Jason Baldridge.2013.
Real-world semi-supervised learning of POS-taggers for low-resource languages.
In Proceedingsof ACL.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
EM can find pretty good HMM POS-taggers(when given a good start).
In Proceedings of ACL.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of ACL.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3).Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric Bayesian inference: Ex-periments on unsupervised word segmentation withadaptor grammars.
In Proceedings of NAACL.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of EMNLP-CoNLL.Julian Kupiec.
1992.
Robust part-of-speech taggingusing a hidden Markov model.
Computer Speech &Language, 6(3).Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised pos tagging.In Proceedings of EMNLP.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2).Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2).149Sujith Ravi and Kevin Knight.
2009.
Minimized mod-els for unsupervised part-of-speech tagging.
In Pro-ceedings of ACL-AFNLP.Sujith Ravi, Jason Baldridge, and Kevin Knight.
2010.Minimized models and grammar-informed initial-ization for supertagging with highly ambiguous lex-icons.
In Proceedings of ACL, pages 495?503.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of ACL.Mark Steedman and Jason Baldridge.
2011.
Combina-tory categorial grammar.
In Robert Borsley and Ker-sti Borjars, editors, Non-Transformational Syntax:Formal and Explicit Models of Grammar.
Wiley-Blackwell.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Daniel Tse and James R. Curran.
2010.
Chinese CCG-bank: Extracting CCG derivations from the PennChinese treebank.
In Proceedings of COLING.Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam sampling for theinfinite hidden Markov model.
In Proceedings ofICML.Jurgen Van Gael, Andreas Vlachos, and ZoubinGhahramani.
2009.
The infinite HMM for unsu-pervised PoS tagging.
In Proceedings of EMNLP.Jonathan Weese, Chris Callison-Burch, and AdamLopez.
2012.
Using categorial grammar to labeltranslation rules.
In Proceedings of WMT.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proceedings of UAI.150
