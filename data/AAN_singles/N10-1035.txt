Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276?284,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEfficient Parsing of Well-Nested Linear Context-Free Rewriting SystemsCarlos G?mez-Rodr?guez1, Marco Kuhlmann2, and Giorgio Satta31Departamento de Computaci?n, Universidade da Coru?a, Spain, cgomezr@udc.es2Department of Linguistics and Philology, Uppsala University, Sweden, marco.kuhlmann@lingfil.uu.se3Department of Information Engineering, University of Padua, Italy, satta@dei.unipd.itAbstractThe use of well-nested linear context-freerewriting systems has been empirically moti-vated for modeling of the syntax of languageswith discontinuous constituents or relativelyfree word order.
We present a chart-based pars-ing algorithm that asymptotically improves theknown running time upper bound for this classof rewriting systems.
Our result is obtainedthrough a linear space construction of a binarynormal form for the grammar at hand.1 IntroductionSince its earliest years, one of the main goals ofcomputational linguistics has been the modeling ofnatural language syntax by means of formal gram-mars.
Following results by Huybregts (1984) andShieber (1985), special attention has been given toformalisms that enlarge the generative power of con-text-free grammars, but still remain below the fullgenerative power of context-sensitive grammars.
Onthis line of investigation, mildly context-sensitivegrammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad-joining grammars (TAGs) of Joshi et al (1975).Linear context-free rewriting system (LCFRS), in-troduced by Vijay-Shanker et al (1987), is a mildlycontext-sensitive formalism that allows the deriva-tion of tuples of strings, i.e., discontinuous phrases.This feature has been used to model phrase structuretreebanks with discontinuous constituents (Maier andS?gaard, 2008), as well as to map non-projective de-pendency trees into discontinuous phrase structures(Kuhlmann and Satta, 2009).Informally, in an LCFRS G, each nonterminal cangenerate string tuples with a fixed number of compo-nents.
The fan-out of G is defined as the maximumnumber of tuple components generated by G. Duringa derivation of an LCFRS, tuple components gener-ated by the nonterminals in the right-hand side ofa production are concatenated to form new tuples,possibly adding some terminal symbols.
The only re-striction applying to these generalized concatenationoperations is linearity, that is, components cannot beduplicated or deleted.The freedom in the rearrangement of componentshas specific consequences in terms of the computa-tional and descriptional complexity of LCFRS.
Evenfor grammars with bounded fan-out, the universalrecognition problem is NP-hard (Satta, 1992), andthese systems lack Chomsky-like normal forms forfixed fan-out (Rambow and Satta, 1999) that are es-pecially convenient in tabular parsing.
This is in con-trast with other mildly context-sensitive formalisms,and TAG in particular: TAGs can be parsed in poly-nomial time both with respect to grammar size andstring size, and they can be cast in normal formshaving binary derivation trees only.It has recently been argued that LCFRS might betoo powerful for modeling languages with discontin-uous constituents or with relatively free word order,and that additional restrictions on the rearrangementof components might be needed.
More specifically,analyses of both dependency and constituency tree-banks (Kuhlmann and Nivre, 2006; Havelka, 2007;Maier and Lichte, 2009) have shown that rearrange-ments of argument tuples almost always satisfy theso-called well-nestedness condition, a generalization276of the standard condition on balanced brackets.
Thiscondition states that any two components x1, x2 ofsome tuple will never be composed with any twocomponents y1, y2 of some other tuple in such a waythat a ?crossing?
configuration is realized.In this paper, we contribute to a better understand-ing of the formal properties of well-nested LCFRS.We show that, when fan-out is bounded by any inte-ger ?
?
1, these systems can always be transformed,in an efficient way, into a specific normal form withno more than two nonterminals in their productions?right-hand sides.
On the basis of this result, wethen develop an efficient parsing algorithm for well-nested LCFRS, running in timeO(?
?
|G| ?
|w|2?+2),where G and w are the input grammar and string,respectively.
Well-nested LCFRS with fan-out ?
= 2are weakly equivalent to TAG, and our complex-ity result reduces to the well-known upper boundO(|G| ?
|w|6) for this class.
For ?
> 2, our upperbound is asymptotically better than the one obtainedfrom existing parsing algorithms for general LCFRSor equivalent formalisms (Seki et al, 1991).Well-nested LCFRS are generatively equivalentto (among others) coupled context-free grammars(CCFG), introduced by Hotz and Pitsch (1996).These authors also provide a normal form and de-velop a parsing algorithm for CCFGs.
One differencewith respect to our result is that the normal form forCCFGs allows more than two nonterminals to appearin the right-hand side of a production, even though nononterminal may contribute more than two tuple com-ponents.
Also, the construction in (Hotz and Pitsch,1996) results in a blow-up of the grammar that is ex-ponential in its fan-out, and the parsing algorithm thatis derived runs in time O(4?
?
|G| ?
|w|2?+2).
Ourresult is therefore a considerable asymptotic improve-ment over the CCFG result, both with respect to thenormal form construction and the parsing efficiency.Finally, under a practical perspective, our parser is asimple chart-based algorithm, while the algorithm in(Hotz and Pitsch, 1996) involves two passes and isconsiderably more complex to analyze and to imple-ment than ours.Kanazawa and Salvati (2010) mention a normalform for well-nested multiple context-free grammars.Structure In Section 2, we introduce LCFRS andthe class of well-nested LCFRS that is the focus ofthis paper.
In Section 3, we discuss the parsing com-plexity of LCFRS, and show why grammars usingour normal form can be parsed efficiently.
Section 4presents the transformation of a well-nested LCFRSinto the normal form.
Section 5 concludes the paper.2 Linear Context-Free Rewriting SystemsWe write [n] to denote the set of positive integers upto and including n: [n] = {1, .
.
.
, n}.2.1 Linear, non-erasing functionsLet ?
be an alphabet.
For integers m ?
0 andk1, .
.
.
, km, k ?
1, a total functionf : (??
)k1 ?
?
?
?
?
(??
)km ?
(??
)kis called a linear, non-erasing function over ?
withtype k1 ?
?
?
?
?
km ?
k, if it can be defined by anequation of the formf(?x1,1, .
.
.
, x1,k1?, .
.
.
, ?xm,1, .
.
.
, xm,km?)
= ~?
,where ~?
is a k-tuple of strings over the variables onthe left-hand side of the equation and ?
with theproperty that each variable occurs in ~?
exactly once.The values m and k are called the rank and the fan-out of f , and denoted by ?
(f) and ?
(f).2.2 Linear Context-Free Rewriting SystemsFor the purposes of this paper, a linear context-freerewriting system, henceforth LCFRS, is a constructG = (N,T, P, S), where N is an alphabet of nonter-minal symbols in which each symbol A is associatedwith a positive integer ?
(A) called its fan-out, T isan alphabet of terminal symbols, S ?
N is a distin-guished start symbol with ?
(S) = 1; and P is a finiteset of productions of the formp = A?
f(A1, .
.
.
, Am) ,where m ?
0, A,A1, .
.
.
, Am ?
N , and f is a linear,non-erasing function over the terminal alphabet Twith type ?(A1)?
?
?
?
??(Am)?
?
(A), called thecomposition operation associated with p. The rankof G and the fan-out of G are defined as the maximalrank and fan-out of the composition operations of G,and are denoted by ?
(G) and ?
(G).The sets of derivation trees of G are the smallestindexed family of sets DA, A ?
N , such that, ifp = A?
f(A1, .
.
.
, Am)277N = {S,R} , T = {a, b, c, d} , P = { p1 = S ?
f1(R), p2 = R?
f2(R), p3 = R?
f3 } ,where: f1(?x1,1, x1,2?)
= ?x1,1 x1,2?
, f2(?x1,1, x1,2?)
= ?a x1,1 b, c x1,2 d?
, f3 = ?
?, ??
.Figure 1: An LCFRS that generates the string language { anbncndn | n ?
0 }.is a production of G and ti ?
DAi for all i ?
[m],then t = p(t1, .
.
.
, tm) ?
DA.
By interpreting pro-ductions as their associated composition operationsin the obvious way, a derivation tree t ?
DA evalu-ates to a ?
(A)-tuple of strings over T ; we denote thistuple by val(t).
The string language generated by G,denoted by L(G), is then defined asL(G) = {w ?
T ?
| t ?
DS , ?w?
= val(t) } .Two LCFRS are called weakly equivalent, if theygenerate the same string language.Example Figure 1 shows a sample LCFRS G with?
(G) = 1 and ?
(G) = 2.
The sets of its deriva-tion trees are DR = { pn2 (p3) | n ?
0 } andDS = { p1(t) | t ?
DR }.
The string languagegenerated by G is { anbncndn | n ?
0 }.2.3 Characteristic stringsIn the remainder of this paper, we use the followingconvenient syntax for tuples of strings.
Instead of?v1, .
.
.
, vk?
, we write v1 $ ?
?
?
$ vk ,using the $-symbol to mark the component bound-aries.
We call this the characteristic string of the tu-ple, and an occurrence of the symbol $ a gap marker.We also use this notation for composition operations.For example, the characteristic string of the operationf(?x1,1, x1,2?, ?x2,1?)
= ?a x1,1 x2,1, x1,2 b?is a x1,1 x2,1 $ x1,2 b.
If we assume the variables onthe left-hand side of an equation to be named ac-cording to the schema used in Section 2.1, then thecharacteristic string of a composition operation deter-mines that operation completely.
We will thereforefreely identify the two, and write productions asp = A?
[v1 $ ?
?
?
$ vk](A1, .
.
.
, Am) ,where the string inside the brackets is the charac-teristic string of some composition operation.
Thesubstrings v1, .
.
.
, vk are called the components ofthe characteristic string.
Note that the character-istic string of a composition operation with typek1 ?
?
?
?
?
km ?
k is a sequence of terminalsymbols, gap markers, and variables from the set{xi,j | i ?
[m], j ?
[ki] } in which the number ofgap markers is k?1, and each variable occurs exactlyonce.
When in the context of such a composition op-eration we refer to ?a variable of the form xi,j?, thenit will always be the case that i ?
[m] and j ?
[ki].The identification of composition operations andtheir characteristic strings allows us to construct newoperations by string manipulations: if, for example,we delete some variables from a characteristic string,then the resulting string still defines a compositionoperation (after a suitable renaming of the remainingvariables, which we leave implicit).2.4 Canonical LCFRSTo simplify our presentation, we will assume thatLCFRS are given in a certain canonical form.
Intu-itively, this canonical form requires the variables inthe characteristic string of a composition operationto be ordered in a certain way.Formally, the defining equation of a compositionoperation f with type k1 ?
?
?
?
?
km ?
k is calledcanonical, if (i) the sequence obtained from f byreading variables of the form xi,1 from left to righthas the form x1,1 ?
?
?xm,1; and (ii) for each i ?
[m],the sequence obtained from f by reading variablesof the form xi,j from left to right has the formxi,1 ?
?
?xi,ki .
An LCFRS is called canonical, if eachof its composition operations is canonical.We omit the proof that every LCFRS can be trans-formed into a weakly equivalent canonical LCFRS.However, we point out that both the normal form andthe parsing algorithm that we present in this papercan be applied also to general LCFRS.
This is in con-trast to some left-to-right parsers in the literature onLCFRS and equivalent formalisms (de la Clergerie,2002; Kallmeyer and Maier, 2009), which actuallydepend on productions in canonical form.2.5 Well-nested LCFRSWe now characterize the class of well-nested LCFRSthat are the focus of this paper.
Well-nestednesswas first studied in the context of dependency gram-mars (Kuhlmann and M?hl, 2007).
Kanazawa (2009)278defines well-nested multiple context-free grammars,which are weakly equivalent to well-nested LCFRS.A composition operation is called well-nested, if itdoes not contain a substring of the formxi,i1 ?
?
?xj,j1 ?
?
?xi,i2 ?
?
?xj,j2 , where i 6= j .For example, the operation x1,1 x2,1$x2,2 x1,2 is well-nested, while x1,1 x2,1 $ x1,2 x2,2 is not.
An LCFRSis called well-nested, if it contains only well-nestedcomposition operations.The class of languages generated by well-nestedLCFRS is properly included in the class of languagesgenerated by general LCFRS; see Kanazawa and Sal-vati (2010) for further discussion.3 Parsing LCFRSWe now discuss the parsing complexity of LCFRS,and motivate our interest in a normal form for well-nested LCFRS.3.1 General parsing schemaA bottom-up, chart-based parsing algorithm for theclass of (not necessarily well-nested) LCFRS can bedefined by using the formalism of parsing schemata(Sikkel, 1997).
The parsing schemata approach con-siders parsing as a deduction process (as in Shieberet al (1995)), generating intermediate results calleditems.
Starting with an initial set of items obtainedfrom each input sentence, a parsing schema definesa set of deduction steps that can be used to infernew items from existing ones.
Each item containsinformation about the sentence?s structure, and a suc-cessful parsing process will produce at least one finalitem containing a full parse for the input.The item set used by our bottom-up algorithm toparse an input string w = a1 ?
?
?
an with an LCFRSG = (N,T, P, S) will beI = {[A, (l1, r1), .
.
.
, (lk, rk)] | A ?
N ?0 ?
li ?
ri ?
n ?i ?
[k]},where an item [A, (l1, r1), .
.
.
, (lk, rk)] can be inter-preted as the set of those derivation trees t ?
DAof G for whichval(t) = al1+1 ?
?
?
ar1 $ ?
?
?
$ alk+1 ?
?
?
ark .The set of final items is thus F = {[S, (0, n)]}, con-taining full derivation trees that evaluate to w.For simplicity of definition of the sets of initialitems and deduction steps, let us assume that pro-ductions of rank > 0 in our grammar do not containterminal symbols in their right-hand sides.
This canbe easily achieved from a starting grammar by cre-ating a nonterminal Aa for each terminal a ?
T , acorresponding rank-0 production pa = Aa ?
[a](),and then changing each occurrence of a in the char-acteristic string of a production to the single variableassociated with the fan-out 1 nonterminal Aa.
Withthis, our initial item set for a string a1 ?
?
?
an will beH = {[Aai , (i?
1, i)] | i ?
[n]} ,and each production p = A0 ?
f(A1, .
.
.
, Am) ofG (excluding the ones we created for the terminals)will produce a deduction step of the form given inFigure 2a, where the indexes are subject to the fol-lowing constraints, imposed by the semantics of f .1.
If the kth component of the characteristic stringof f starts with xi,j , then l0,k = li,j .2.
If the kth component of the characteristic stringof f ends with xi,j , then r0,k = ri,j .3.
If xi,jxi?,j?
is an infix of the characteristic stringof f , then ri,j = li?,j?
.4.
If the kth component of the characteristic stringof f is the empty string, then l0,k = r0,k.3.2 General complexityThe time complexity of parsing LCFRS with respectto the length of the input can be analyzed by countingthe maximum number of indexes that can appear inan instance of the inference rule above.
Although thetotal number of indexes is?mi=0 2 ?
?
(Ai), some ofthese indexes are equated by the constraints.To count the number of independent indexes, con-sider all the indexes of the form l0,i (corresponding tothe left endpoints of each component of the character-istic string of f ) and those of the form rj,k for j > 0(corresponding to the right endpoints of each vari-able in the characteristic string).
By the constraintsabove, these indexes are mutually independent, and itis easy to check that any other index is equated to oneof these: indexes r0,i are equated to the index rj,kcorresponding to the last variable xj,k of the ith com-ponent of the characteristic string, or to l0,i if thereis no such variable; while indexes lj,k with j > 0are equated to an index l0,i if the variable xj,k is atthe beginning of a component of the characteristicstring, or to an index rj?,k?(j?
> 1) if the variable xj,kfollows another variable xj?,k?
.279[A1, (l1,1, r1,1), .
.
.
, (l1,?
(A1), r1,?
(A1))] ?
?
?
[Am, (lm,1, rm,1), .
.
.
, (lm,?
(Am), rm,?
(Am))][A0, (l0,1, r0,1), .
.
.
, (l0,?
(A0), r0,?
(A0))](a) The general rule for a parsing schema for LCFRS[B, (l1, r1), .
.
.
, (lm, rm)] [C, (l?1, r?1), .
.
.
(l?n, r?n)][A, (l1, r1), .
.
.
, (lm, r?1), .
.
.
(l?n, r?n)]rm = l?1(b) Deduction step for concatenation[B, (l1, r1), .
.
.
, (lm, rm)] [C, (l?1, r?1), .
.
.
(l?n, r?n)][A, (l1, r1), .
.
.
, (li, r?1), .
.
.
(l?n, ri+1), .
.
.
, (lm, rm)]ri = l?1, r?n = li+1(c) Deduction step for wrappingFigure 2: Deduction steps for parsing LCFRS.Thus, the parsing complexity (Gildea, 2010) of aproduction p = A0 ?
f(A1, .
.
.
, Am) is determinedby ?
(A0) l-indexes and?i?
[m] ?
(Ai) r-indexes, fora total complexity ofO(|w|?(A0)+?i?
[m] ?
(Ai))where |w| is the length of the input string.
The pars-ing complexity of an LCFRS will correspond to themaximum parsing complexity among its productions.Note that this general complexity matches the resultgiven by Seki et al (1991).In an LCFRS of rank ?
and fan-out ?, the maxi-mum possible parsing complexity is O(|w|?
(?+1)),obtained by applying the above expression to a pro-duction of rank ?
and where each nonterminal has fan-out ?.
The asymptotic time complexity of LCFRSparsing is therefore exponential both in its rank andits fan-out.
This means that it is interesting to trans-form LCFRS into equivalent forms that reduce theirrank while preserving the fan-out.
For sets of LCFRSthat can be transformed into a binary form (i.e., suchthat all its rules have rank at most 2), the ?
factor inthe complexity is reduced to a constant, and complex-ity is improved to O(|w|3?)
(see G?mez-Rodr?guezet al (2009) for further discussion).
Unfortunately,it is known by previous results (Rambow and Satta,1999) that it is not always possible to convert anLCFRS into such a binary form without increasingthe fan-out.
However, we will show that it is alwayspossible to build such a binarization for well-nestedLCFRS.
Combining this result with the inferencerule and complexity analysis given above, we wouldobtain a parser for well-nested LCFRS running inO(|w|3?)
time.
But the construction of our binarynormal form additionally restricts binary compositionoperations in the binarized LCFRS to be of two spe-cific forms, concatenation and wrapping, which fur-ther improves the parsing complexity to O(|w|2?+2),as we will see below.3.3 Concatenation and wrappingA composition operation is called a concatenationoperation, if its characteristic string has the formx1,1 $ ?
?
?
$ x1,m x2,1 $ ?
?
?
$ x2,n ,where m,n ?
1.
Intuitively, such an operation corre-sponds to the bottom-up combination of two adjacentdiscontinuous constituents into one.
An example ofa concatenation operation is the binary parsing ruleused by the standard CKY parser for context-freegrammars, which combines continuous constituents(represented as 1-tuples of strings in the LCFRS nota-tion).
In the general case, a concatenation operationwill take an m-tuple and an n-tuple and return an(m + n ?
1)-tuple, as the joined constituents mayhave gaps that will also appear in the resulting tuple.If we apply the general parsing rule given in Fig-ure 2a to a production A?
conc(B,C), where concis a concatenation operation, then we obtain the de-duction step given in Figure 2b.
This step uses 2mdifferent l- and r-indexes, and 2n?
1 different l?-and r?-indexes (excluding l?1 which must equal rm),for a total of 2m+2n?1 = 2(m+n?1)+1 indexes.Since m+ n?
1 is the fan-out of the nonterminal A,we conclude that the maximum number of indexes inthe step associated with a concatenation operation inan LCFRS of fan-out ?
is 2?+ 1.280before: p?
?
?t1 tmafter: p?q?
?
?tq,1 tq,mqr?
?
?tr,1 tr,mrFigure 3: Transformation of derivation treesA linear, non-erasing function is called a wrappingoperation, if its characteristic string has the formx1,1 $ ?
?
?
$ x1,i x2,1 $ ?
?
?
$ x2,n x1,i+1 $ ?
?
?
$ x1,m ,where m,n ?
1 and i ?
[m?
1].
Intuitively, such anoperation wraps the tuple derived from a nontermi-nal B around the tuple derived from a nonterminal C,filling the ith gap in the former.
An example of awrapping operation is the adjunction of an auxiliarytree in tree-adjoining grammar.
In the general case, awrapping operation will take an m-tuple and an n-tu-ple and return an (m + n ?
2)-tuple of strings: thegaps of the argument tuples appear in the obtainedtuple, except for one gap in the tuple derived from Bwhich is filled by the tuple derived from C.By applying the general parsing rule in Figure 2ato a production A ?
wrapi(B,C), where wrapi isa wrapping operation, then we obtain the deductionstep given in Figure 2c.
This step uses 2m different l-and r-indexes, and 2n?
2 different l?- and r?-indexes(discounting l?1 and r?n which are equal to other in-dexes), for a total of 2m+2n?2 = 2(m+n?2)+2indexes.
Since the fan-out of A is m + n ?
2, thismeans that a wrapping operation needs at most 2?+2indexes for an LCFRS of fan-out ?.From this, we conclude that an LCFRS of fan-out ?
in which all composition operations are ei-ther concatenation operations, wrapping operations,or operations of rank 0 or 1, can be parsed in timeO(|w|2?+2).
In particular, nullary and unary compo-sition operations do not affect this worst-case com-plexity, since their associated deduction steps cannever have more than 2?
indexes.4 TransformationWe now show how to transform a well-nested LCFRSinto the normal form that we have just described.4.1 Informal overviewConsider a production p = A ?
f(A1, .
.
.
, Am),where m ?
2 and f is neither a concatenation nor awrapping operation.
We will construct new produc-tions p?, q, r such that every derivation that uses p canbe rewritten into a derivation that uses the new pro-ductions, and the new productions do not license anyother derivations.
Formally, this can be understood asimplementing a tree transformation, where the inputtrees are derivations of the original grammar, and theoutput trees are derivations of the new grammar.
Thesituation is illustrated in Figure 3.
The tree on toprepresents a derivation in the original grammar; thisderivation starts with the rewriting of the nontermi-nal A using the production p, and continues with thesubderivations t1, .
.
.
, tm.
The tree at the bottom rep-resents a derivation in the transformed grammar.
Thisderivation starts with the rewriting ofA using the newproduction p?, and continues with two independentsubderivations that start with the new productions qand r, respectively.
The sub-derivations t1, .
.
.
, tmhave been partitioned into two sequencest1,1, .
.
.
, t1,m1 and t2,1, .
.
.
, t2,m2 .The new production p?
will be either a concatenationor a wrapping operation, and the rank of both q and rwill be strictly smaller than the rank of p. The trans-formation will continue with q and r, unless thesehave rank one.
By applying this strategy exhaustively,we will thus eventually end up with a grammar thatonly has productions with rank at most 2, and inwhich all productions with rank 2 are either concate-nation or wrapping operations.4.2 Constructing the composition operationsTo transform the production p, we first factorize thecomposition operation f associated with p into threenew composition operations f ?, g, h as follows.
Re-call that we represent composition operations by theircharacteristic strings.In the following, we will assume that no charac-teristic string starts or ends with a gap marker, orcontains immediate repetitions of gap markers.
This281property can be ensured, without affecting the asymp-totic complexity, by adding intermediate steps to thetransformation that we report here; we omit the de-tails due to space reasons.
When this property holds,we are left with the following two cases.
Let us call asequence of variables joint, if it contains all and onlyvariables associated with a given nonterminal.Case 1 f = x1 f1 x2 ?
?
?xk?1 fk?1 xk f?
,where k ?
1, x1, .
.
.
, xk are joint variables, and thesuffix f?
contains at least one variable.
Letg = x1 f1 x2 ?
?
?xk?1 fk?1 xk ,let h = f?, and let f ?
= conc.
As f is well-nested,both g and h define well-nested composition opera-tions.
By the specific segmentation of f , the ranks ofthese operations are strictly smaller than the rank of f .Furthermore, we have ?
(f) = ?
(g) + ?(h)?
1 .Case 2 f = x1 f1 x2 ?
?
?xk?1 fk?1 xk ,where k ?
2, x1, .
.
.
, xk are joint variables, and thereexist at least one i such that the sequence fi containsat least one variable.
Choose an index j as follows:if there is at least one i such that fi contains at leastone variable and one gap marker, let j be the minimalsuch i; otherwise, let j be the minimal i such that ficontains at least one variable.
Now, letg = x1 f1 x2 ?
?
?xj $ xj+1 ?
?
?xk?1 fk?1 xk ,let h = fj , and let f ?
= wrapj .
As in Case 1, both gand h define well-nested composition operationswhose ranks are strictly smaller than the rank of f .Furthermore, we have ?
(f) = ?
(g) + ?(h)?
2 .Note that at most one of the two cases can applyto f .
Furthermore, since f is well-nested, it is alsotrue that at least one of the two cases applies.
Thisis so because for two distinct nonterminals Ai, Ai?
,either all variables associated with Ai?
precede theleftmost variable associated with Ai, succeed therightmost variable associated with Ai, or are placedbetween two variables associated with Ai without an-other variable associated with Ai intervening.
(Here,we have left out the symmetric cases.
)4.3 Constructing the new productionsBased on the composition operations, we now con-struct three new productions p?, q, r as follows.
LetBand C be two fresh nonterminals with ?
(B) = ?
(g)and ?
(C) = ?
(h), and let p?
= A ?
f ?
(B,C).The production p?
rewrites A into B and C andcombines the two subderivations that originate atthese nonterminals using either a concatenation or awrapping operation.
Now, let Aq,1, .
.
.
, Aq,mq andAr,1, .
.
.
, Ar,mr be the sequences of nonterminalsthat are obtained from the sequence A1, .
.
.
, Am bydeleting those nonterminals that are not associatedwith any variable in g or h, respectively.
Then, letq = B ?
g(Aq,1, .
.
.
, Aq,mq) andr = C ?
h(?Ar,1, .
.
.
, Ar,mr) .4.4 ExampleWe now illustrate the transformation using the con-crete production p = A?
f(A1, A2, A3), wheref = x1,1 x2,1 $ x1,2 $ x3,1 .Note that this operation has rank 3 and fan-out 3.The composition operations are constructed as fol-lows.
The operation f matches the pattern of Case 1,and hence induces the operationsg1 = x1,1 x2,1 $ x1,2 , h1 = $ x3,1 , f ?1 = conc .The productions constructed from these arep?1 = A?
conc(B1, C1) ,q1 = B1 ?
g1(A1, A2) , r1 = C1 ?
h1(A3) .where B1 and C1 are fresh nonterminals with fan-out 2.
The production r1 has rank one, so it does notrequire any further transformations.
The transforma-tion thus continues with q1.
The operation g1 matchesthe pattern of Case 2, and induces the operationsg2 = x1,1 $ x1,2 , h2 = x2,1$ , f ?2 = wrap1 .The productions constructed from these arep?2 = B1 ?
wrap1(B2, C2) ,q2 = B2 ?
g2(A1) , r2 = C2 ?
h2(A2) ,where B2 and C2 are fresh nonterminals with fan-out 2.
At this point, the transformation terminates.We can now delete p from the original grammar, andreplace it with the productions {p?1, r1, p?2, q2, r2}.4.5 CorrectnessTo see that the transformation is correct, we need toverify that each production of the original grammaris transformed into a set of equivalent normal-formproductions, and that the fan-out of the new grammardoes not exceed the fan-out of the old grammar.For the first point, we note that the transformationpreserves well-nestedness, decreases the rank of aproduction, and is always applicable as long as the282rank of a production is at most 2 and the productiondoes not use a concatenation or wrapping operation.That the new productions are equivalent to the oldones in the sense of Figure 3 can be proved by induc-tion on the length of a derivation in the original andthe new grammar, respectively.Let us now convince ourselves that the fan-out ofthe new grammar does not exceed the fan-out of theold grammar.
This is clear in Case 1, where?
(f) = ?
(g) + ?(h)?
1implies that both ?
(g) ?
?
(f) and ?
(h) ?
?
(f).For Case 2, we reason as follows.
The fan-out of theoperation h, being constructed from an infix of thecharacteristic string of the original operation f , isclearly bounded by the fan-out of f .
For g, we have?
(g) = ?(f)?
?
(h) + 2 ,Now suppose that the index j was chosen accordingto the first alternative.
In this case, ?
(h) ?
2, and?
(g) ?
?(f)?
2 + 2 = ?
(f) .For the case where j was chosen according to thesecond alternative, ?
(f) < k (since there are noimmediate repetitions of gap markers), ?
(h) = 1,and ?
(g) ?
k. If we assume that each nonterminalis productive, then this means that the underlyingLCFRS has at least one production with fan-out k ormore; therefore, the fan-out of g does not increasethe fan-out of the original grammar.4.6 ComplexityTo conclude, we now briefly discuss the space com-plexity of the normal-form transformation.
We mea-sure it in terms of the length of a production, definedas the length of its string representation, that is, thestring A?
[v1 $ ?
?
?
$ vk](A1, .
.
.
, Am) .Looking at Figure 3, we note that the normal-formtransformation of a production p can be understoodas the construction of a (not necessarily complete)binary-branching tree whose leaves correspond to theproductions obtained by splitting the characteristicstring of p and whose non-leaf nodes are labeled withconcatenation and wrapping operations.
By construc-tion, the sum of the lengths of leaf-node productionsis O(|p|).
Since the number of inner nodes of a bi-nary tree with n leaves is bounded by n ?
1, weknow that the tree hasO(?
(p)) inner nodes.
As thesenodes correspond to concatenation and wrappingoperations, each inner-node production has lengthO(?(p)).
Thus, the sum of the lengths of the produc-tions created from |p| is O(|p|+ ?(p)?(p)).
Sincethe rank of a production is always smaller than itslength, this is reduced to O(|p|?
(p)).Therefore, the size of the normal-form transfor-mation of an LCFRS G of fan-out ?
is O(?|G|) inthe worst case, and linear space in practice, sincethe fan-out is typically bounded by a small integer.Taking the normal-form transformation into account,our parser therefore runs in timeO(?
?
|G| ?
|w|2?+2)where |G| is the original grammar size.5 ConclusionIn this paper, we have presented an efficient parsingalgorithm for well-nested linear context-free rewrit-ing systems, based on a new normal form for thisformalism.
The normal form takes up linear spacewith respect to grammar size, and the algorithm isbased on a bottom-up process that can be appliedto any LCFRS, achieving O(?
?
|G| ?
|w|2?+2) timecomplexity when applied to LCFRS of fan-out ?in our normal form.
This complexity is an asymp-totic improvement over existing results for this class,both from parsers specifically geared to well-nestedLCFRS or equivalent formalisms (Hotz and Pitsch,1996) and from applying general LCFRS parsingtechniques to the well-nested case (Seki et al, 1991).The class of well-nested LCFRS is an interest-ing syntactic formalism for languages with discon-tinuous constituents, providing a good balance be-tween coverage of linguistic phenomena in natu-ral language treebanks (Kuhlmann and Nivre, 2006;Maier and Lichte, 2009) and desirable formal prop-erties (Kanazawa, 2009).
Our results offer a furtherargument in support of well-nested LCFRS: whilethe complexity of parsing general LCFRS dependson two dimensions (rank and fan-out), this bidimen-sional hierarchy collapses into a single dimensionin the well-nested case, where complexity is onlyconditioned by the fan-out.Acknowledgments G?mez-Rodr?guez has beensupported by MEC/FEDER (HUM2007-66607-C04)and Xunta de Galicia (PGIDIT07SIN005206PR, Re-des Galegas de PL e RI e de Ling.
de Corpus, BolsasEstad?as INCITE/FSE cofinanced).
Kuhlmann hasbeen supported by the Swedish Research Council.283References?ric Villemonte de la Clergerie.
2002.
Parsing mildlycontext-sensitive languages with thread automata.
In19th International Conference on Computational Lin-guistics (COLING), pages 1?7, Taipei, Taiwan.Daniel Gildea.
2010.
Optimal parsing strategies for linearcontext-free rewriting systems.
In Human LanguageTechnologies: The Eleventh Annual Conference of theNorth American Chapter of the Association for Compu-tational Linguistics, Los Angeles, USA.Carlos G?mez-Rodr?guez, Marco Kuhlmann, GiorgioSatta, and David J. Weir.
2009.
Optimal reductionof rule length in linear context-free rewriting systems.In Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 539?547,Boulder, CO, USA.Jir??
Havelka.
2007.
Beyond projectivity: Multilin-gual evaluation of constraints and measures on non-projective structures.
In 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages608?615.G?nter Hotz and Gisela Pitsch.
1996.
On parsing coupled-context-free languages.
Theoretical Computer Science,161(1?2):205?233.Riny Huybregts.
1984.
The weak inadequacy of context-free phrase structure grammars.
In Ger de Haan, MiekeTrommelen, and Wim Zonneveld, editors, Van periferienaar kern, pages 81?99.
Foris, Dordrecht, The Nether-lands.Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.1975.
Tree Adjunct Grammars.
Journal of Computerand System Sciences, 10(2):136?163.Aravind K. Joshi.
1985.
Tree Adjoining Grammars: Howmuch context-sensitivity is required to provide reason-able structural descriptions?
In Natural LanguageParsing, pages 206?250.
Cambridge University Press.Laura Kallmeyer and Wolfgang Maier.
2009.
An incre-mental Earley parser for simple range concatenationgrammar.
In Proceedings of the 11th International Con-ference on Parsing Technologies (IWPT 2009), pages61?64.
Association for Computational Linguistics.Makoto Kanazawa and Sylvain Salvati.
2010.
The copy-ing power of well-nested multiple context-free gram-mars.
In Fourth International Conference on Languageand Automata Theory and Applications, Trier, Ger-many.Makoto Kanazawa.
2009.
The pumping lemma for well-nested multiple context-free languages.
In Develop-ments in Language Theory.
13th International Confer-ence, DLT 2009, Stuttgart, Germany, June 30?July 3,2009.
Proceedings, volume 5583 of Lecture Notes inComputer Science, pages 312?325.Marco Kuhlmann and Mathias M?hl.
2007.
Mildlycontext-sensitive dependency languages.
In 45th An-nual Meeting of the Association for Computational Lin-guistics (ACL), pages 160?167.Marco Kuhlmann and Joakim Nivre.
2006.
Mildly non-projective dependency structures.
In 21st InternationalConference on Computational Linguistics and 44th An-nual Meeting of the Association for Computational Lin-guistics (COLING-ACL), Main Conference Poster Ses-sions, pages 507?514, Sydney, Australia.Marco Kuhlmann and Giorgio Satta.
2009.
Treebankgrammar techniques for non-projective dependencyparsing.
In Twelfth Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), pages 478?486, Athens, Greece.Wolfgang Maier and Timm Lichte.
2009.
Characterizingdiscontinuity in constituent treebanks.
In 14th Confer-ence on Formal Grammar, Bordeaux, France.Wolfgang Maier and Anders S?gaard.
2008.
Treebanksand mild context-sensitivity.
In 13th Conference onFormal Grammar, pages 61?76, Hamburg, Germany.Owen Rambow and Giorgio Satta.
1999.
Independentparallelism in finite copying parallel rewriting systems.Theoretical Computer Science, 223(1?2):87?120.Giorgio Satta.
1992.
Recognition of Linear Context-Free Rewriting Systems.
In 30th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 89?95, Newark, DE, USA.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami.
1991.
On Multiple Context-Free Gram-mars.
Theoretical Computer Science, 88(2):191?229.Stuart M. Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductive pars-ing.
Journal of Logic Programming, 24(1?2):3?36.Stuart M. Shieber.
1985.
Evidence against the context-freeness of natural language.
Linguistics and Philoso-phy, 8(3):333?343.Klaas Sikkel.
1997.
Parsing Schemata: A Frameworkfor Specification and Analysis of Parsing Algorithms.Springer.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions producedby various grammatical formalisms.
In 25th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 104?111, Stanford, CA, USA.284
