Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 550?559,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsWeighted Krippendorff's alpha is a more reliable metrics for multi-coders ordinal annotations: experimental studies on emotion, opinionand coreference annotationJean-Yves AntoineUniversit?
Fran?ois Rabelais deTours, LI (EA 6300)Blois, FranceJean-Yves.Antoine@univ-tours.frJeanne VillaneauUniversit?
Europ?enne deBretagne, IRISALorient, FranceJeanne.Villaneau@univ-ubs.frAna?s LefeuvreUniversit?
Fran?ois Rabelaisde Tours, LI (EA 6300)Blois, Franceanais.lefeuvre@univ-tours.frAbstractThe question of data reliability is of first im-portance to assess the quality of manually an-notated corpora.
Although Cohen?s ?
is theprevailing reliability measure used in NLP, al-ternative statistics have been proposed.
Thispaper presents an experimental study with fourmeasures (Cohen?s ?, Scott?s pi, binary andweighted Krippendorff ?
s ?)
on three tasks:emotion, opinion and coreference annotation.The reported studies investigate the factors ofinfluence (annotator bias, category prevalence,number of coders, number of categories) thatshould affect reliability estimation.
Resultsshow that the use of a weighted measure re-stricts this influence on ordinal annotations.They suggest that weighted ?
is the most reli-able metrics for such an annotation scheme.1 IntroductionThe newly intensive use of machine learningtechniques as well as the need of evaluation datahas led Natural Language Processing (NLP) todevelop large annotated corpora.
The interest forsuch enriched language resources has reacheddomains (semantics, pragmatics, affective com-puting) where the annotation process is highlyaffected by the coders subjectivity.
The reliabil-ity of the resulting annotations must be trusted bymeasures that assess the inter-coders agreement.While medecine, psychology, and more gener-ally content analysis, have considered for yearsthe issue of data reliability, NLP has only inves-tigated this question from the mid 1990s.
Theinfluential work of Carletta (1996) has led the ?statistic (Cohen, 1960) to become the prevailingstandard for measuring the reliability of corpusannotation.
Many studies have however ques-tioned the limitations of the ?
statistic and haveproposed alternative measures of reliability.Krippendorff claims that ?popularity of ?
not-withstanding, Cohen?s ?
is simply unsuitable asa measure of the reliability of data?
in a paperpresenting his ?
coefficient (Krippendorff,2008).Except for some rare but noticeable studies(Arstein and Poesio, 2005), most of these criticalworks restrict to theoretical issues about chanceagreement estimation or limitations due to vari-ous statistical biases (Arstein and Poesio, 2008).On the opposite, this paper investigates experi-mentally these questions on three different tasks:emotion, opinion and coreference annotation.Four measures of reliability will be considered:Cohen?s ?
(Cohen, 1960), Scott?s pi (Scott, 1955)and two measures of Krippendorff?s ?
(Krippen-dorff, 2004) with different distance.Section 2 gives a comprehensive presentationof these metrics.
Section 3 details the potentialmethodological biases that should affect the reli-ability estimation.
In section 4, we explain themethodology we followed for this study.
Lastly,experimental results are presented in section 5.2 Reliability measuresAny reliability measure considers the most perti-nent criterion to estimate data reliability to bereproducibility.
Reproducibility can be estimatedby observing the agreement among independentannotators (Krippendorff, 2004): the more thecoders agree on the data they have produced, themore their annotations are likely to be repro-duced by any other set of coders.Pure observed agreement is not considered asa good estimator since it does not give any ac-count to the amount of chance that yields to thisagreement.
For instance, a restricted number ofcoding categories should favor chance agree-ment.
What must be estimated is the proportionof observed agreement beyond the one that isexpected by chance:(1)  Measure =eeoAAA?
?1550where Ao is the observed agreement betweencoders and Ae is an estimation of the possiblechance agreement.
Reliability metrics differ bythe way they estimate this chance agreement.Cohen?s ?
(Cohen, 1960) defines chance asthe statistical independence of the use of codingcategories by the annotators.
It postulates thatchance annotation is governed by prior distribu-tions that are specific to each coder (annotatorbias).
?
was originally developed for two codersand nominal data.
(Davies and Fleiss, 1982) hasproposed a generalization to any number of cod-ers, while (Cohen, 1968) has defined a weightedversion of the ?
measure that fulfils better theneed of reliability estimation for ordinal annota-tions: the disagreement between two ordinal an-notations is no more binary, but depends on aEuclidian distance.
This weighted generalizationrestricts however to a two coders scheme (Art-stein and Poesio, 2008): a weighted version ofthe multi-coders ?
statistics is still missing.Unlike Cohen?s ?, Scott?s pi (Scott, 1955)does not aim at modelling annotator bias.
It de-fines chance as the statistical independence ofthe data and the set of coding categories, inde-pendently from the coders.
It considers thereforethe annotation process and not the behaviour ofthe annotators.
Scott?s original proposal con-cerned only two coders.
(Fleiss 1971) gave ageneralisation of the statistics to any number ofcoders through a measure of pairwise agreement.Krippendorff?s ?
(Krippendorff, 2004) con-siders chance independently from coders likeScott?s pi, but data reliability is estimated de-pending on disagreement instead of agreement:(2)  Alpha  =eoeDDD ?where Do is the observed disagreement be-tween coders and De is an estimation of the pos-sible chance disagreement.
Another original as-pect of this metrics is to allow disagreement es-timation between two categories through anydistance measure.
This implies that ?
handlesdirectly any number of coders and any kind ofannotation (nominal or ordinal coding scheme).In this paper, we will consider the ?
statisticswith a binary as well as a Euclidian distance, inorder to assess separately the influence of thedistance measure and the metrics by itself.3 Quality criteria for reliability metricsThere is an abundant literature about the criteriaof quality a reliability measure should satisfy(Hayes, 2007).
These works emphasize on twoimportant points:?
A trustworthy measure should provide sta-ble results: measures must be reasonablyindependent of any factor of influence.?
The magnitude of the measure must be in-terpreted in terms of absolute level of reli-ability: the statistics must come up withtrustworthy reliability thresholds.These questions have mainly been investigatedfrom a theoretical point of view.
This sectionsummarizes the main conclusions that should bedrawn from these critical studies.3.1 Annotator bias and number of codersAnnotator bias refers to the influence of the idio-syncratic behavior of the coders.
It can be esti-mated by a bias index which measures the extentto which the distribution of categories differsfrom one coder?s annotation to another (Sim andWright, 2005).
Annotator bias has an influenceon the magnitude of the reliability measures(Feinstein and Cicchetti,1990).
Besides, it con-cerns the invariance of the measures to the per-mutation or selection of annotators but also to thenumber of coders.
A review of the literatureshows that theoretical studies on annotator biasare not convergent.
In particular, opposite argu-ments have been proposed concerning Cohen?s ?
(Di Eugenio and Glass 2004, Arstein and Poesio2008, Hayes, 2007).
This is why we have carriedon experiments that investigate:?
to what extent measures depend on the se-lection of a specific set of coders (?
5.3),?
to what extent the stability of the measuresdepends on the number of coders (?
5.4).Arstein and Poesio (2005) have shownthat the greater the number of coders is,the lower the annotator bias decreases.Our aim is to go further this conclusion:we will study whether one measure needsfewer coders than another one to convergetowards an acceptable annotator bias.3.2 Category prevalencePrevalence refers to the influence on reliabilityestimation of a coding category under which adisproportionate amount of annotated data falls.It can be estimated by a prevalence index whichmeasures the frequency differences of categorieson cases where the coders agree (Sim andWright, 2005).
When the prevalence index is551high, chance-corrected measures are spuriouslyreduced since chance agreement is higher in thissituation (Brennan and Sliman, 1992; Di Eugenioand Glass, 2004).
This yields some authors topropose corrected coefficients like the PABAKmeasure (Byrt and al., 1993), which is a preva-lence adjusted and annotator bias adjusted ver-sion of Cohen?s ?.
The influence of prevalencewill not be investigated here, since no category issignificantly prevalent in our data.3.3 Number of coding categoriesThe number of coding categories has an influ-ence on the reliability measures magnitude: thelarger the number of categories is, the less thecoders have a chance to agree.
Even if this de-crease should concern chance agreement too,lower reliability estimations are observed withhigh numbers of categories (Brenner andKliebsch, 1996).
This paper investigates this in-fluence by comparing reliability values obtainedwith a 3-categories and a 5-categories codingscheme applied on the same data (see ?
5.1).3.4  Interpreting the magnitude of meas-ures in terms of effective reliabilityOne last question concerns the interpretation ofthe reliability measures magnitude.
It has beenparticularly investigated with Cohen?s ?.
Carletta(1996) advocates 0.8 to be a threshold of goodreliability, while a value between 0.67 and 0.8 isconsidered sufficient to allow tentative conclu-sion to be drawn.
On the opposite, Krippendorff(2004b) claims that this 0.67 cutoff is a prettylow standard while Neuendorf (2002) supportsan even more restrictive interpretation.Thus, the definition of relevant levels of reli-ability remains an open problem.
We will seehow our experiments should draw a methodo-logical framework to answer this crucial issue.4 Experiments: methodology4.1 IntroductionWe have conducted experiments on three dif-ferent annotation tasks in order to guarantee anappreciable generality of our findings.
The firsttwo experiments correspond to an ordinal anno-tation.
They concern the affective dimension oflanguage (emotion and opinion annotation).
Theyhave been conducted with na?ve coders to pre-serve the spontaneity of judgment which issearched for in affective computing.The third experiment concerns coreferenceannotation.
It is a nominal annotation that hasbeen designed to be used as a comparison withthe previous ordinal annotations tasks.The corresponding annotated corpora areavailable (TestAccord database) on the frenchParole_Publique1 corpus repository under a CC-BY-SA Creative Commons licence.4.2 Emotion corpusEmotion annotation consists in adding emo-tional information to written messages or speechtranscripts.
There is no real consensus about howan emotion has to be described in an annotationscheme.
Two main approaches can be found inthe literature.
On the one hand, emotions arecoded by affective modalities (Scherer, 2005),among which sadness, disgust, enjoyment, fear,surprise and anger are the most usual (Ekman,1999; Cowie and Cornelius, 2003).
On the otherhand, an ordinal classification in a multidimen-sional space is considered.
Several dimensionshave been proposed among which three are pre-vailing (Russell, 1980): valence, intensity andactivation.
Activation distinguishes passive fromactive emotional states.
Valence describeswhether the emotional state conveyed by the textis positive, negative or neutral.
Lastly, intensitydescribes the level of emotion conveyed.Whatever the approach, low to moderate inter-annotator agreements are observed, what ex-plains that reference annotation must be achievedthrough a majority vote with a significant num-ber of coders (Schuller and al.
2009).
Inter-coderagreement is particularly low when emotions arecoded into modalities (Devillers and al., 2005;Callejas and Lopez-Cozar, 2008).
This is whythis study focuses on an ordinal annotation.Our works on emotion detection (Le Tallecand al., 2011) deal with a specific context: affec-tive robotics.
We consider an affective multimo-dal interaction between hospitalized children anda companion robot.
Consequently, this experi-ment will concern a child-dedicated corpus.
Al-though many works already focused on childlanguage (MacWhinney, 2000), no emotionalchild corpus is currently available in French, ourstudied language.
We have decided to create alittle corpus (230 sentences) of fairy tales, whichare regularly used in works related to child affectanalysis (Alm and al., 2005; Volkova and al.,2010).
The selected texts come from modernfairy tales (Vassallo, 2004; Vanderheyden, 1995)which present the interest of being quite confi-dential.
This guarantees that the coders discover1www.info.univ-tours.fr/~antoine/parole_publique552the text during the annotation.
We asked 25 sub-jects to characterize the emotional value con-veyed by every sentence through a 5-items scaleof values, ranging from very negative to verypositive.As shown on Table 1, this affective scale en-compasses valence and intensity dimensions.
Itenables to compare without methodological biasan annotation with 3 coding categories (valence:negative, positive, neutral) and the original 5-categories (valence+intensity) annotation.A preliminary experiment showed us thatchildren meet difficulties to handle a 5-valuesemotional scale.
This is why the annotation wasconducted on the fairy tales corpus with adults(11 men/14 women; average age: 31.6 years).
Allthe coders have a superior level of education (atleast, high-school diploma), they did not knoweach other and worked separately during the an-notation task.
Only four of them had a prior ex-perience in corpus annotation.Value Meaning Valence /PolarityIntensity /Strength-2 very negative negative strong-1 moderatelynegativenegative moderate0 no emotion neutral none1 moderatelypositivepositive moderate2 very positive positive strongTable 1. emotion or opinion annotation schemesThe coders were not trained but were givenprecise annotation guidelines providing someexplanations and examples on the emotional val-ues they had to use.
They achieved the annota-tion once, without any restriction on time.
Theyhad to rely on their own judgment, without con-sidering any additional information.
Sentenceswere given in a random order to investigate anout-of-context perception of emotion.
We con-ducted a second experiment where the order ofthe sentences followed the original fairy tale, inorder to study the influence of the discourse con-text.
The criterion of data significance ?
at leastfive chance agreements per category ?
proposedby (Krippendorff, 2004) is greatly satisfied forthe valence annotation (3 categories).
It is ap-proached on the complete annotation where wecan assure 4 chance agreements per category.4.3 Opinion corpusThe second experiment concerns opinion an-notation.
Emotion detection can be related to acertain extent, with opinion mining (or sentimentanalysis), whose aim is to detect the attitude ofpeople in the texts they produce.
A basic task inopinion mining consists in classifying the polar-ity of a given text, which should be either a sen-tence (Wilson and al., 2005), a speech turn or acomplete document (Turney, 2002).
Polarityplays the same role as valence does for affectanalysis: it describes whether the expressedjudgment is positive, negative, or neutral.
Oneshould also characterize the sentiment strength(Thelwall and al., 2010).
This feature can be re-lated to the notion of intensity used in emotionalannotation.
Both polarity and sentiment strengthare considered in our annotation task.This experiment has been carried out on a cor-pus of film reviews.
The reviews were relativelyshort texts written by ordinary people on dedi-cated French websites (www.senscritique.comand www.allocine.fr).
They concerned the sameFrench movie.
The corpus contains 183 sen-tences.
Its annotation was conducted by the 25previous subjects.
The methodology is identicalto the emotion annotation task.
The subjects wereasked to qualify the opinion that was conveyedby every sentence of the reviews by means ofthe same scale of values (Table 1).
This scaleencompasses this time the polarity and sentimentstrength dimensions.
Once again, the sentenceswere given in a random order and contextual or-der respectively.
The criterion of data signifi-cance is satisfied here too.On both annotations, experiments with therandom or the contextual order give similar re-sults.
Results from the contextual annotation willbe given only when necessary.4.4 Coreference corpusThe last experiment concerns coreference an-notation.
We have developed an annotated cor-pus (ANCOR) which clusters various types ofspontaneous and conversational speech.
With atotal of 488,000 lexical units, it is one of thelargest coreference corpora dedicated to spokenlanguage (Muzerelle and al.
2014).
Its annotationwas split into three successive phases:?
Entity mentions marking,?
Referential relations marking,?
Referential relations characterizationThe experiment described in this paper con-cerns the characterization of the referential rela-tions.
This nominal annotation consists in classi-fying relations among five different types:553?
Direct coreference (DIR) ?
Coreferentmentions are NPs with same lexical heads.?
Indirect coreference (IND) ?
These men-tions are NPs with distinct lexical heads.?
Pronominal anaphora (PRO) ?
The subse-quent coreferent mention is a pronoun.?
Bridging anaphora (BRI) ?
The subse-quent mention does not refer to its antece-dent but depends on it for its referential in-terpretation (example: meronymy).?
Bridging pronominal anaphora (BPA) ?Bridging anaphora where the subsequentmention is a pronoun.
This type empha-sizes metonymies (example: Avoid Cen-tral Hostel?
they are unpleasant)The subjects (3 men / 6 women) were adultpeople (average age: 41.2 years) with a high pro-ficiency in linguistics (researchers in NLP or cor-pus linguistics).
They know each other butworked separately during the annotation, withoutany restriction on time.
They are considered asexperts since they participated to the definitionof the annotation guide.
The study was con-ducted on an extract of 10 dialogues, represent-ing 384 relations.
Krippendorff?s (2004) criterionof significance is therefore satisfied here too.4.5 Reliability measuresThe experiments have been conducted with fourchance-balanced reliability measures2 :?
Multi-?
: multiple coders/binary distanceCohen?s ?
(Davies and Fleiss, 1982),?
Multi-pi : multiple coders/binary distanceScott?s pi  (Fleiss, 1971),?
?b : Krippendorff?s ?
with binary distance,?
?
: standard Krippendorff?s ?
with a 1-dimension Euclidian distance.The use of Euclidian distance is unfounded oncoreference which handles a nominal annotation.Thus, ?
will not be computed on this last corpus.2Experiments were also conducted with Cronbach??c?
(Cronbach, 1951).
This metrics is based on a correlationmeasure.
Krippendorff (2009) considers soundly that corre-lation coefficients are inappropriate to estimate reliability.Our results show that ?c is systematically outperformed bythe other metrics.
In particular, it is highly dependent tocoder bias.
For instance we observed a relative standarddeviation of ?c measures higher than 22% when measuringthe influence of coders set permuation (?
5.3, table 5).
Thisobservation discards Cronbach?
?c ?as a trustworthy measure.5 Results5.1 Influence of the number of categoriesOur affective coding scheme enables a directcomparison between a 3-classes (valence or po-larity) and a 5-classes annotation.
The 3-classesscheme clusters the coding categories with thesame valence or polarity.
For instance {-2,-1}negative values are clustered in the same cate-gory which receive the index 1.
For the computa-tion of the weighted ?, the distance betweennegative (-1) and positive (1) classes will beequal to 2.
Table 2 presents the reliability meas-ures observed on all of the corpora.Corpus Emotion (fairy tales)Metric M-?
M-pi ?b ?3-classes 0.41 0.41 0.41 0.575-classes 0.29 0.29 0.29 0.57Abs.
diff.
0.12 0.12 0.12 0.0Corpus Opinion (film reviews)Metric M-?
M-pi ?b ?3-classes 0.58 0.58 0.58 0.755-classes 0.45 0.45 0.45 0.80Abs.
diff.
0.13 0.13 0.13 0.05Corpus Coreference (spoken dialogues)Metric M-?
M-pi ?b ?5-classes 0.69 0.69 0.69 n.s.Table 2.
Reliability measures: emotion and opinionrandom annotation as well as coreference annotationSeveral general conclusions can be drawnfrom these figures.
At first, low inter-coderagreements are observed on affective annotation,which is coherent with many other studies (Dev-illers and al., 2005; Callejas and Lopez-Cozar,2008).
Non-weighted metrics (multi-?, multi-pi,?b) range from 0.29 to 0.58, depending on theannotation scheme.
This confirms that these an-notation tasks are prone to high subjectivity.Higher levels of agreement may have been ob-tained if the annotators were trained with super-vision.
As said before, this would have reducedthe spontaneity of judgment.
Furthermore, acomprehensive meta-analysis (Bayerl and Paul,2011) has shown that no difference may be foundon data reliability between experts and novices.The reliability measures given by the weightedversion of Krippendorff?s ?
on the two affectivetasks are significantly higher: ?
values rangefrom 0.57 to 0.80, which suggests a rather suffi-cient reliability.
These results are not an artifact.They come from better disagreement estimation.For instance, the difference between a positive554and a negative annotation is more serious thanbetween the positive and the neutral emotion,what a weighted metrics accounts for.Satisfactory measures are found on the con-trary on the coreference task (0.69 with everymetric).
This result was expected, since a largepart of the annotation decisions are based on ob-jective (syntactic or semantic) considerations.Whatever the experiment you consider, multi-?, multi-pi and ?b coefficients present very closevalues (identical until the 3rd decimal).
A similarobservation was made by (Arstein and Poesio,2005) with 18 coders.
This validates the theoreti-cal hypothesis on the convergence of individual-distribution and single-distribution measureswhen the number of coders increases.
Our ex-periments show that annotator bias is moderatewith 25 coders when inter-coders agreement israther low (affective tasks), while 9 coders areenough to guarantee a low annotator bias whendata reliability is higher (coreference task).Lastly, the comparison between the two anno-tation schemes (3 or 5 classes) in affective tasksprovides some indications on the influence of thenumber of coding categories on reliability esti-mation3.
As expected (see ?
3.3), multi-?, multi-piand ?b values increase significantly when thenumber of classes decreases.On the contrary, weighted ?
is significantlyless affected by the increase of the number ofcategories.
The ?
value remains unchanged onthe emotional corpus and its variation restricts to0.05 on the opinion task.
It seems that the use ofa Euclidian distance counterbalances the higherrisk of disagreement when the number of catego-ries grows.
Such an independence of the numberof coding categories is an interesting property fora reliability measure, which has never been re-ported as far as we know.Metric M-?
M-pi ?b ?3-classes 0.61 0.61 0.61 0.785-classes 0.49 0.49 0.49 0.83Abs.
diff.
0.12 0.12 0.12 0.05Table 3.
Reliability measures with 3 and 5 annotationclasses: opinion contextual annotation (film reviews).Finally, Table 3 presents as an illustration thereliabilities measures we obtained with the con-textual annotation of the opinion corpus.
These3The 3-classes coding scheme is a semantic reduction of the5-classes one.
One should wonder whether the same resultscan be observed with unrelated categories.
(Chu-Ren andal., 2002) shows indeed that expanding PoS tags with sub-categories does not increase categorical ambiguity.results are fully coherent with the previous ones.One should note in addition that reliability meas-ures are significantly higher on these contextualannotations: the context of discourse helps thecoders to qualify opinions more objectively.5.2 Influence of prevalenceTable 4 presents the distribution of the annota-tions on the three corpora.
(Devillers and al.,2005; Callejas and Lopez-Cozar, 2008) reportedthat more than 80% of the speech turns are clas-sified as neutral in their emotional corpora.
Thisprevalence was not found on our affective cor-pora.
Positive annotations are nearly as frequentas the neutral ones on the emotion task.
This ob-servation is due to the deliberate emotional na-ture of fairy tales.
Likewise, the neutral opinionis minority among the film reviews, which aimfrequently at expressing pronounced judgments.Positive opinions are slightly majority on theopinion corpus but this prevalence is limited: itrepresents an increase of only 50% of frequency,by comparison with a uniform distribution.Corpus Emotion (fairy tales)5-classes?2 ?1 0 1 2Distribution 8% 17% 38% 23%   14%3-classes Negative neutral PositiveDistribution 25% 38% 37%Corpus Opinion (film reviews)5-classes -2 -1 0 1 2Distribution 15% 21% 14% 26% 25%3-classes negative neutral positiveDistribution 36% 14% 51%Corpus Coreference (spoken dialogues)5-classes DIR IND PRO BRI BPADistribution 40% 7% 42% 10% 1%Table 4.
Distribution of the coding categoriesIn the coreference corpus, two classes arehighly dominant, but they are not prevalentalone.
There is no indication in the literature thatthe prevalence of two balanced categories has abias on data reliability measure.
For all these rea-sons, we didn't investigate the influence of preva-lence.
Besides, relevant works are questioningthe importance of the influence of prevalence oninter-coders agreement measures (Vach, 2005).5.3 Influence of coders set permutation?a coefficient for assessing the reliability of datamust treat coders as interchangeable (Krippen-dorff, 2004b).
We have studied the stability ofreliability measures computed on any combina-tion of 10 coders (among 25) on the affectivecorpora, and 4 coders (among 9) on the corefer-555ence corpus.
The influence of permutation isquantified by a measure of relative standard de-viation (e.g.
related to the average value) amongthe sets of coders (Table 5).Corpus Emotion (fairy tales)Metric M-?
M-pi ?b ?3-classes 7.4% 7.7% 7.6% 6.2%5-classes 9.0% 9.1% 9.1% 6.1%Corpus Opinion (film reviews)3-classes 3.4% 3.3% 3.3% 2.6%5-classes 4.0% 4.0% 4.1% 1.7%Corpus Coreference (spoken dialogues)5-classes 4.6% 4.6% 4.6% n.c.Table 5.
Relative standard deviation of measures onany independent sets of codersBinary metrics do not differ on this criterion:multi-?, multi-pi and ?b present very similar re-sults.
On the opposite, the benefit of a Euclidiandistance of agreement is clear: ?
is significantlyless influenced by coders set permutation.5.4 Influence of the number of codersA good way to limit annotator bias is to enroll animportant number of annotators.
This need isunfortunately contradictory with a restriction ofannotation costs.
The estimation of data reliabil-ity must thereby remain trustworthy with aminimal number of coders.
As far as we know,there is no clear indication in the literature aboutthe definition of such a minimal size.We have conducted an experiment which in-vestigates the influence of the number of coderson the relevancy of reliability estimation.
Con-sidering N annotations (N=25 for affective anno-tation and N=9 for coreference annotation), wecompute all the possible reliability values withany subsets of S coders, S varying from 2 to N.As an estimation of the trustworthiness of thecoefficients, the relative standard deviation of thereliability values is computed for every size S(Figures 1 to 3).
The influence of the number ofcoders is obvious: detrimental standard devia-tions are found with small coders set sizes.
Thisfinding concerns above all multi-?, multi-pi and?b, which present very close behaviors on allannotations.
One the opposite, the weighted?
coefficient converges significantly faster to atrustworthy reliability measure The comparisonbetween ?b and ?
is enlightening.
It shows againthat the main benefit of Krippendorff?s proposalresults from its accounting for a weighted dis-tance in a multi-coders ordinal annotation.0%10%20%30%40%2 4 6 8 10 12 14 16 18 20 22 24Number of codersRelativestddev(%)multi-pialphamulti-kalpha binary0%5%10%15%20%2 4 6 8 10 12 14 16 18 20 22 24Number of codersRelativestddev(%)multi-pialphamulti-kbinary alphaFigure 1.
Relative standard deviation on any set ofcoders of a given size.
5-classes coding scheme.
Emo-tion (top) and opinion (bottom) random annotation.0%10%20%30%2 4 6 8 10 12 14 16 18 20 22 24Number of codersRelativestddev(%)multi-pialphamulti-kbinary alpha0%5%10%15%2 4 6 8 10 12 14 16 18 20 22 24Number of codersRelativestddev(%)multi-pialphamulti-kalpha binaryFigure 2.
Relative standard deviation on any set ofcoders of a given size.
3-classes coding scheme.
Emo-tion (top) and opinion (bottom) random annotation.5560%5%10%2 3 4 5 6 7 8Number of codersRelativestddev(%) multi-pimulti-kbinary alphaFigure 3.
Relative std deviation of measures on anysets of coders for a given coders set size: coreference6 Conclusion and perspectivesOur experiments were conducted on various an-notation tasks which assure a certain representa-tiveness of our conclusions:?
Cohen?s ?, Krippendorff?s ?
?and Scott?s pi?provide close values when they use thesame measure of disagreement.?
A convergence of these measures has beennoticed in the literature when the numberof coders is high.
We observed it even onvery restricted sets of annotators.?
The use of a weighted measure (Euclidiandistance) has several benefits on ordinaldata.
It restricts the influence on reliabilitymeasure of both the number of categoriesand the number of coders.
Unfortunately,Cohen?s ?
?
?statistics cannot consider aweighted distance in a multi-codersframework contrary to Krippendorff?s ?.?
There is no benefit of using Krippendorff?s?
on nominal data, since a binary distanceis mandatory on this situation.To conclude, the main interest of Krippen-dorff?s ?
is thus its ability to integrate any kindof distance.
In light of our results, the weightedversion of this coefficient must be preferredevery time an ordinal annotation with multiplecoders is considered.Our experiments leave open an essential ques-tion: the objective definition of trustworthythresholds of reliability.
We propose to investi-gate this question in terms of expected modifica-tions of the reference annotation.
A majority voteis generally used as a gold standard to create thisreference with multiple coders.
As a preliminaryexperiment, we have compared our referenceaffective annotations (25 coders) with those ob-tained on any other included set of coders.0%10%20%30%40%50%1 3 5 7 9 11 13 15 17 19 21 23number of coders%of modifications 3 classes5 classes0,0%10,0%20,0%30,0%40,0%50,0%1 3 5 7 9 11 13 15 17 19 21 23 25 27 29number of coders%of modifications HC 3 classesHC 5 classesFigure 4.
Average modifications of the reference ac-cording to the number of coders.
Emotion annotation(top) and opinion annotation (bottom)Figure 4 presents the average percentage ofmodifications of the reference according to thenumber of coders.
We wonder to what extentthese curves can be related to reliability meas-ures.
It seems indeed that the higher the meas-ures are, the lower the modifications are too.
Forinstance, almost all of the coefficients presenthigher or equal reliability values with 3 codingcategories (Tables 2 & 3), which corresponds tolower levels of modifications on Figure 3.
Like-wise, reliability measures are higher on the opin-ion annotation, where we observe lower modifi-cations of the reference.As a result, we expect results like those pre-sented on figure 4 to enable a direct interpreta-tion of reliability measures.
For instance, with amulti-?
values of 0.41, or a ?b value of 0.57 (Ta-ble 2, 3-classes emotion annotation), one shouldexpect around 8% of errors on our reference an-notation if 10 coders are considered.
We plan toextend these experiments with simultated syn-thetic data to characterize precisely the relationsbetween absolute reliability measures and ex-pected confidence in the reference annotation.We expect to obtain with simulated annotation asufficient variety of agreement to establish soundrecommendations on data reliability thresholds.We intend to modify randomly human annota-tions to conduct this simulation.557ReferencesCecilia Alm, Dan Roth, Richard Sproat.
2005.
Emo-tions from Text: Machine Learning for Text-basedEmotion Prediction, In Proc.
HLT&EMNLP?2005.Vancouver, Canada.
579-586Ron Arstein and Masimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics.
34(4):555-596.Ron Artstein and Massimo Poesio.
2005.
Bias de-creases in proportion to the number of annotators.In Proceedings FG-MoL?2005, 141:150, Edin-burgh, UK.Petra Saskia Bayerl and Karsten Ingmar Paul, 2011.What Determines Inter-Coder Agreement in Man-ual Annotations?
A Meta-Analytic Investigation  .Computational Linguistics.
37(4), 699:725.Paul Brennan and Alan Silman.
1992.
Statisticalmethods for assessing observer variability in clini-cal measures.
BMJ, 304:1491-1494.Ted Byrt, Janet Bishop, John Carlin.
1993.
Bias,prevalence and kappa.
Journal of Clinical Epide-miology, 46:423-429.Hermann Brenner and Ulrike Kliebsch.
1996.
Depen-dance of weighted kappa coefficients on the num-ber of categories.
Epidemiology.
7:199-202.Zoraida Callejas and Ramon Lopez-Cozar.
2008.
In-fluence of contextual information in emotion anno-tation for spoken dialogue systems, Speech Com-munication, 50:416-433Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: the Kappa statistic.
ComputationalLinguistics, 22(2):249-254Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
Educational and PsychologicalMeasurement, 20:37-46.Jacob Cohen.
1968.
Weighted kappa: nominal scaleagreement with provision for scaled disagreementor partial credit.
Psychol.
Bulletin, 70(4):213?220Roddy Cowie and Randolph Cornelius.
2003.
De-scribing the emotional states that are expressed inspeech.
Speech Communication.
40 :5-32.Lee J. Cronbach.
1951.
Coefficient alpha and the in-ternal structure of tests.
Psychometrica.
16:297-334Laurence Devillers, Laurence Vidrascu, Lori Lamel.2005.
Emotion detection in real-life spoken dialogsrecorded in call center.
Journal of Neural Net-works, 18(4):407-422.Paul Ekman.
1999.
Patterns of emotions: New Analy-sis of Anxiety and Emotion.
Plenum Press, New-York, NY.Barbara Di Eugenio and Michael Glass.
2004.
Thekappa statistic: A second look.
Computational Lin-guistics, 30(1):95?101Mark Davies and Joseph Fleiss.
1982.
Measuringagreement for multinomial data.
Biometrics,38(4):1047-1051.Alvan Feinstein and Domenic Cicchetti.
1990.
Highagreement but low Kappa : the problem of twoparadoxes.
J. of Clinical Epidemiology, 43:543-549Joseph L. Fleiss.
1971 Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5): 378?382Andrew Hayes.
2007.
Answering the call for a stan-dard reliability measure for coding data.
Communi-cation Methods and Measures 1, 1:77-89.Klaus Krippendorff.
2004.
Content Analysis: an In-troduction to its Methodology.
Chapter 11.
Sage:Thousand Oaks, CA.Klaus Krippendorff.
2004b.
Reliability in ContentAnalysis: Some Common Misconceptions andRecommendations.
Human Communication Re-search, 30(3): 411-433, 2004Klaus Krippendorff.
2008.
Testing the reliability ofcontent analysis data: what is involved and why.
InKlaus Krippendorff, Mark Angela Bloch (Eds) Thecontent analysis reader.
Sage Publications.
Thou-sand Oaks, CA.Klaus Krippendorff.
2009.
Testing the reliability ofcontent analysis data: what is involved and why.
InKlaus Krippendorff , Mary Angela Bock.
The Con-tent Analysis Reader.
Sage: Thousand Oaks, CAMarc Le Tallec, Jeanne Villaneau, Jean-Yves An-toine, Dominique Duhaut.
2011 Affective Interac-tion with a Companion Robot for vulnerable Chil-dren: a Linguistically based Model for EmotionDetection.
In Proc.
Language Technology Confer-ence 2011, Poznan, Poland, 445-450.Brian MacWhinney.
2000.
The CHILDES project :Tools for analyzing talk.
3rd edition.
Lawrence Erl-baum associates Mahwah, NJ.Judith Muzerelle, Ana?s Lefeuvre, Emmanuel Schang,Jean-Yves Antoine, Aurore Pelletier, Denis Mau-rel, Iris Eshkol, Jeanne Villaneau.
2014.
AN-COR_Centre, a large free spoken French corefer-ence corpus: description of the resource and reli-ability measures.
In Proc.
LREC?2014 (submitted).Kimberly Neuendorf.
2002.
The Content AnalysisGuidebook.
Sage Publications, Thousand Oaks, CAJames Russell.
1980.
A Circumplex Model of Affect,J.
Personality and Social Psy., 39(6): 1161-1178.Klaus Scherer.
2005.
What are emotions?
and howcan they be measured?
Social Science Information,44 (4):694?729.558Bj?rn Schuller, Stefan Steidl, Anto Batliner.
2009.The Interspeech'2009 emotion challenge.
In Pro-ceedings Interspeech'2009, Brighton, UK.
312:315.William Scott.
1955.
Reliability of content analysis:the case of nominal scale coding.
Public OpinionsQuaterly, 19:321-325.Julius Sim and Chris Wright.
2005.
The Kappa Statis-tic in Reliability Studies: Use, Interpretation, andSample Size Requirements.
Physical Therapy,85(3):257:268.Mike Thelwall, Kevan Buckley, Georgios Paltoglou,Di Cai, Arvid Kappas.
2010.
Sentiment strengthdetection in short informal text.
Journal of theAmerican Society for Information Science andTechnology, 61 (12): 2544?2558.Peter Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised clas-sification of reviews, In Proceedings ACL?02,Philadelphia, Pennsylvania, 417-424.Werner Vach, 2005.
The dependence of Cohen?skappa on the prevalence does not matter, Journalof Clinical Epidemiology, 58, 655-661).Rose-Marie Vassallo.
2004.
Comment le Grand Nordd?couvrit l??t?.
Flammarion, Paris, France.Kees Vanderheyden.
1995.
Le Noel des animaux de lamontagne.
Fairy tale available at the URL :http://www.momes.net/histoiresillustrees/contesdemontagne/noelanimaux.htmlEkaterina Volkova, Betty Mohler, Detmar Meurers,Dale Gerdemann and Heinrich B?lthoff.
2010.Emotional perception of fairy tales: achievingagreement in emotion annotation of   text, In Pro-ceedings NAACL HLT 2010.
Los Angeles, CA.Theresa Wilson, Janyce Wiebe, Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proc.
of HLT-EMNLP?2005.347-354.559
