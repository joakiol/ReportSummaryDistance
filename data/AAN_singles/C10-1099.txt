Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 877?885,Beijing, August 2010Knowing What to Believe(when you already know something)Jeff Pasternack Dan RothUniversity of Illinois, Urbana-Champaign{jpaster2, danr}@uiuc.eduAbstractAlthough much work in NLP has focusedon simply determining what a documentmeans, we also must know whether or notto believe it.
Fact-finding algorithms at-tempt to identify the ?truth?
among com-peting claims in a corpus, but fail totake advantage of the user?s prior knowl-edge and presume that truth itself is uni-versal and objective rather than subjec-tive.
We introduce a framework for incor-porating prior knowledge into any fact-finding algorithm, expressing both gen-eral ?common-sense?
reasoning and spe-cific facts already known to the user asfirst-order logic and translating this intoa tractable linear program.
As our resultsshow, this approach scales well to evenlarge problems, both reducing error andallowing the system to determine truth re-spective to the user rather than the major-ity.
Additionally, we introduce three newfact-finding algorithms capable of outper-forming existing fact-finders in many ofour experiments.1 IntroductionAlthough establishing the trustworthiness of theinformation presented to us has always been achallenge, the advent of the Information Age andthe Internet has made it more critical.
Blogs,wikis, message boards and other collaborativemedia have eliminated the high entry barrier?and, with it, the enforced journalistic standards?ofolder, established media such as newspapers andtelevision, and even these sometimes loosen theirfact-checking in the face of increased competitivepressure.
Consequently, we find that corpora de-rived from these sources now offer far more nu-merous views of far more questionable veracity.If one author claims Mumbai is the largest city inthe world, and another claims it is Seoul, who dowe believe?
One or both authors could be inten-tionally lying, honestly mistaken or, alternatively,of different viewpoints of what constitutes a ?city?
(the city proper?
The metropolitan area?)
Truth isnot objective: there may be many valid definitionsof ?city?, but we should believe the claim that ac-cords with our user?s viewpoint.
Note that the usermay be another computational system rather thana human (e.g.
building a knowledge base of citysizes for question answering), and often neitherthe user?s nor the information source?s perspectivewill be explicit (e.g.
an author will not fully elabo-rate ?the largest city by metropolitan area boundedby...?)
but will instead be implied (e.g.
a user?sstatement that ?I already know the population ofcity A is X, city B is Y...?
implies that his defini-tion of a city accords with these figures).The most basic approach is to take a vote: ifmultiple claims are mutually exclusive of eachother, select the one asserted by the most sources.In our experiments, sources will be the authorsof the document containing the claim, but othersources could be publishers/websites (when noauthorship is given), an algorithm that outputsclaims, etc.
Although sometimes competitive, wefound voting to be generally lackluster.
A class ofalgorithms called fact-finders are often a dramaticimprovement, but are incapable of taking advan-tage of the user?s prior knowledge.
Our frameworktranslates prior knowledge (expressed as first-order logic) into a linear program that constrainsthe claim beliefs produced by a fact-finder, en-suring that our belief state is consistent with bothcommon sense (?cities usually grow?)
and knownfacts (?Los Angeles is more populous than Wi-chita?).
While in the past first-order logic has beentranslated to NP-hard integer linear programs, weuse polynomial-time-solvable linear programs, al-877lowing us to readily scale to large problems withextensive prior knowledge, as demonstrated byour experiments.We next discuss related work, followed bya more in-depth description of the fact-findingalgorithms used in our experiments, includ-ing three novel, high-performing algorithms:Average?Log, Investment, and PooledInvestment.We then present the framework?s mechanics andthe translation of first-order logic into a linear pro-gram.
Finally, we present our experimental setupand results over three domains chosen to illustratedifferent aspects of the framework, demonstratingthat both our new fact-finders and our frameworkoffer performance improvements over the currentstate of the art.2 Related WorkThe broader field of trust can be split into three ar-eas of interest1: theoretical, reputation-based, andinformation-based.2.1 TheoreticalMarsh (1994) observes that trust can be global(e.g.
eBay?s feedback scores), personal (each per-son has their own trust values), or situational (per-sonal and specific to a context).
Fact-finding algo-rithms are based on global trust, while our frame-work establishes personal trust by exploiting theuser?s individual prior knowledge.Probabilistic logics have been explored as analternate method of reasoning about trust.
Man-chala (1998) utilizes fuzzy logic (Novak et al,1999), an extension of propositional logic permit-ting [0,1] belief over propositions.
Yu and Singh(2003) employs Dempster-Shafer theory (Shafer,1976), with belief triples (mass, belief, and plausi-bility) over sets of possibilities to permit the mod-eling of ignorance, while Josang et al (2006) usesthe related subjective logic (Josang, 1997).
Whileour belief in a claim is decidedly Bayesian (theprobability that the claim is true), ?unknowns?
(discussed later) allow us to reason about igno-rance as subjective logic and Dempster-Shafer do,but with less complexity.1Following the division proposed by Artz and Gil (2007);see also (Sabater and Sierra, 2005) for a survey from a dif-ferent perspective.2.2 Reputation-basedReputation-based systems determine an entity?strust or standing among peers via transitive rec-ommendations, as PageRank (Brin and Page,1998) does among web pages, Advogato (Levien,2008) does among people, and Eigentrust (Kam-var et al, 2003) does among peers in a net-work.
Some, such as Hubs and Authorities (Klein-berg, 1999), are readily adapted to fact-finding, asdemonstrated later.2.3 Information-BasedInformation-based approaches utilize content(rather than peer recommendations) to computetrust, and are often specialized for a particular do-main.
For example, (Zeng et al, 2006) and Wik-itrust (Adler and de Alfaro, 2007) determine trustin a wiki?s text passages from sequences of revi-sions but lack the claim-level granularity and gen-eral applicability of fact-finders.Given a large set of sources making conflictingclaims, fact-finders determine ?the truth?
by iter-atively updating their parameters, calculating be-lief in facts based on the trust in their sources, andthe trust in sources based on belief in their facts.TruthFinder (Yin et al, 2008) is a straightforwardimplementation of this idea.
AccuVote (Dong etal., 2009a; Dong et al, 2009b) improves on thisby using calculated source dependence (whereone source derives its information from another)to give higher credibility to independent sources.
(Galland et al, 2010)?s 3-Estimates algorithm in-corporates the estimated ?hardness?
of a fact, suchthat knowing the answer to an easy question earnsless trust than to a hard one.
Except for AccuVote(whose model of repeated source-to-source copy-ing is inapplicable to our experimental domains)we experimented over all of these algorithms.3 Fact-FindingWe have a set of sources S each asserting a set ofclaims Cs, with C = ?s?S Cs.
Each claim c ?
Cbelongs to a mutual exclusion set Mc ?
C, a setof claims (including c) that are mutually exclusivewith one another; for example, ?John was bornin 1960?
and ?John was born in 1965?
are mutu-ally exclusive because a person cannot be born inmore than one year.
If c is not mutually exclusive878to any other claims, then Mc = {c}.
Assumingthere exists exactly one true claim c in each mu-tual exclusion set M , our goal is to predict c foreach M , with accuracy measured by the numberof successful predictions divided by the numberof mutual exclusion sets, ignoring trivially cor-rect claims that are the sole members of their mu-tual exclusion set.
To this end, fact-finding algo-rithms iterate to find the trustworthiness of eachsource T i(s) at iteration i in terms of the beliefin its claims in the previous iteration Bi?1(Cs),and belief in each claim Bi(c) in terms of T i(Sc),where Sc = {s : s ?
S, c ?
Cs} is the set ofall sources asserting c. Note that ?trustworthiness?and ?belief?
as used within a fact-finding algo-rithm typically do not have meaningful semantics(i.e.
they are not [0, 1] Bayesian probabilities).
It-eration continues until convergence or some pre-defined stop criteria.3.1 PriorsExcept for 3-Estimates (where the priors are dic-tated by the algorithm itself), every fact-finderrequires priors for B0(C).
For each fact-finderwe chose from B0voted(c) = |Sc|/?d?Mc |Sd|,B0uniform(c) = 1/|Mc|, and B0fixed(c) = 0.5.3.2 Algorithms3.2.1 Sums (Hubs and Authorities)Hubs and Authorities (Kleinberg, 1999) giveseach page a hub score and an authority score,where its hub score is the sum of the authority oflinked pages and its authority is the sum of thehub scores of pages linking to it.
This is adaptedto fact-finding by viewing sources as hubs (with0 authority) and claims as authorities (with 0 hubscore):T i(s) =?c?CsBi?1(c) Bi(c) =?s?ScT i(s)We normalize to prevent T i(s) and Bi(c) fromgrowing unbounded (dividing by maxs T i(s) andmaxc Bi(c), respectively), a technique also usedwith the Investment and Average?Log algorithms(discussed next); this avoids numerical overflow.B0fixed priors are used.3.2.2 Average?LogComputing T (s) as an average of belief inits claims overestimates the trustworthiness ofa source with relatively few claims; certainly asource with 90% accuracy over a hundred ex-amples is more trustworthy than a source with90% accuracy over ten.
However, summing thebelief in claims allows a source with 10% accu-racy to obtain a high trustworthiness score by sim-ply making many claims.
Average?Log attemptsa compromise, while still using Sums?
Bi updaterule and B0fixed priors.T i(s) = log |Cs| ?
?c?Cs Bi?1(c)|Cs|3.2.3 InvestmentIn the Investment algorithm, sources ?in-vest?
their trustworthiness uniformly among theirclaims.
The belief in each claim then grows ac-cording to a non-linear function G, and a source?strustworthiness is calculated as the sum of the be-liefs in their claims, weighted by the proportionof trust previously contributed to each (relative tothe other investors).
Since claims with higher-trustsources get higher belief, these claims become rel-atively more believed and their sources becomemore trusted.
We used G(x) = xg with g = 1.2 inour experiments, together with B0voted priors.T i(s) =?c?CsBi?1(c) ?
Ti?1(s)|Cs| ?
?r?ScT i?1(r)|Cr|Bi(c) = G(?s?ScT i(s)|Cs|)3.2.4 PooledInvestmentLike Investment, sources uniformly invest theirtrustworthiness in claims and obtain correspond-ing returns, so T i(s) remains the same, but nowafter the belief in the claims of mutual exclusionset M have grown according to G, they are lin-early scaled such that the total belief of the claimsin M remains the same as it was before apply-ing G(x) = xg, with g = 1.4 and B0uniformpriors used in our experiments.
Given H i(c) =?s?ScT i(s)|Cs| , we have:Bi(c) = H i(c) ?
G(Hi(c))?d?Mc G(H i(d))8793.3 TruthFinderTruthFinder (Yin et al, 2008) is pseudoprobabilis-tic: the basic version of the algorithm below cal-culates the ?probability?
of a claim by assumingthat each source?s trustworthiness is the proba-bility of it being correct and then averages claimbeliefs to obtain trustworthiness scores.
We alsoused the ?full?, more complex TruthFinder, omit-ted here for brevity.
B0uniform priors are used forboth.T i(s) =?c?Cs Bi?1(c)|Cs|Bi(c) = 1??s?Sc(1?
T i(s))3.3.1 3-Estimates3-Estimates (Galland et al, 2010), also omit-ted for brevity, differs from the other fact-findersby adding a third set of parameters to capture the?difficulty?
of a claim, such that correctly assert-ing a difficult claim confers more trustworthinessthan asserting an easy one; knowing the exact pop-ulation of a city is harder than knowing the popu-lation of Mars (presumably 0) and we should nottrust a source merely because they provide what isalready common knowledge.4 The FrameworkTo apply prior knowledge to a fact-finding algo-rithm, we translate the user?s prior knowledge intoa linear program.
We then iterate the following un-til convergence or other stopping criteria:1.
Compute T i(s) for all s ?
S2.
Compute Bi(c) for all c ?
C3.
?Correct?
beliefs Bi(C) with the LP4.1 Propositional Linear ProgrammingTo translate prior knowledge into a linear pro-gram, we first propositionalize our first-orderformulae into propositional logic (Russell andNorvig, 2003).
For example, assume we know thatTom is older than John and a person has exactlyone age (?x,yAge(Tom, x)?Age(John, y)?x >y) ?
(?x,y,zAge(x, y) ?
y 6= z ?
?Age(x, z)),and our system is considering the follow-ing claims: Age(Tom, 30), Age(Tom, 40),Age(John, 25), Age(John, 35).
Our proposi-tional clauses (after removing redundancies) arethen Age(Tom, 30) ?
Age(John, 25) ?
(Age(Tom, 30) ?
Age(Tom, 40)) ?
(Age(John, 25)?Age(John, 35)).Each claim c will be represented by a propo-sition, and ultimately a [0, 1] variable in thelinear program corresponding, informally, toP (c).2 Propositionalized constraints have previ-ously been used with integer linear programming(ILP) using binary {0, 1} values correspondingto {false, true}, to find an (exact) consistenttruth assignment minimizing some cost and solvea global inference problem, e.g.
(Roth and Yih,2004; Roth and Yih, 2007).
However, proposi-tional linear programming has two significant ad-vantages:1.
ILP is ?winner take all?, shifting all belief toone claim in each mutual exclusion set (evenwhen other claims are nearly as plausible)and finding the single most believable con-sistent binary assignment; we instead wish tofind a distribution of belief over the claimsthat is consistent with our prior knowledgeand as close as possible to the distributionproduced by the fact-finder.2.
Linear programs can be solved in polynomialtime (e.g.
by interior point methods (Kar-markar, 1984)), but ILP is NP-hard.To create our constraints, we first convert ourpropositional formula into conjunctive normalform.
Then, for each disjunctive clause consistingof a set P of positive literals (claims) and a setN of negations of literals, we add the constraint?c?P cv +?c?N (1?
cv) ?
1, where cv de-notes the [0, 1] variable corresponding to each c.The left-hand side is the union bound of at leastone of the claims being true (or false, in the caseof negated literals); if this bound is at least 1, theconstraint is satisfied.
This optimism can dilutethe strength of our constraints by ignoring poten-tial dependence among claims: x ?
y, x ?
y im-plies y is true, but since we demand only yv ?
xvand xv + yv ?
1 we accept any yv ?
0.5 where2This is a slight mischaracterization, since our linear con-straints only approximate intersections and unions of events(where each event is ?claim c is true?
), and we will be satis-fying them subject to a linear cost function.880yv ?
xv ?
1 ?
yv.
However, when the claimsare mutually exclusive, the union bound is exact; acommon constraint is of the form q ?
r1?r2?.
.
.,where the r literals are mutually exclusive, whichtranslates exactly to r1v + r2v + .
.
.
?
qv.
Fi-nally, observe that mutual exclusion amongst nclaims c1, c2, .
.
., cn can be compactly written asc1v + c2v + .
.
.+ cnv = 1.4.2 The Cost FunctionHaving seen how first-order logic can be con-verted to linear constraints, we now consider thecost function, a distance between the new distri-bution of belief satisfying our constraints and theoriginal distribution produced by the fact-finder.First we determine the number of ?votes?
re-ceived by each claim c, computed as ?c =?
(B(c)), which should scale linearly with the cer-tainty of the fact-finder?s belief in c. Recall thatthe semantics of the belief score are particularto the fact-finder, so different fact-finders requiredifferent vote functions.
TruthFinder has pseudo-probabilistic [0,1] beliefs, so we use ?inv(x) =min((1 ?
x)-1,minv) with minv = 1010 limitingthe maximum number of votes possible; we as-sume 1/0 = ?.
?inv intuitively scales with ?er-ror?
: a belief of 0.99 receives ten times the votesof 0.9 and has a tenth the error (0.01 vs. 0.1).For the remainder of the fact-finders whose beliefsare already ?linear?, we use the identity function?idn(x) = x.The most obvious choice for the cost func-tion might be to minimize ?frustrated votes?
:?c?C ?c(1 ?
cv).
Unfortunately, this results inthe linear solver generally assigning 1 to the vari-able in each mutual exclusion set with the mostvotes and 0 to all others (except when constraintsprevent this), shifting all belief to the highest-voteclaim and yielding poor performance.
Instead, wewish to satisfy the constraints while keeping eachcv close to ?c/?Mc , where ?Mc =?d?Mc ?d,and so shift belief among claims as little as possi-ble.
We use a weighted Manhattan distance calledVoteDistance, where the cost for increasing thebelief in a claim is proportional to the number ofvotes against it, and the cost for decreasing beliefis proportional to the number of votes for it:?c?Cmax((?Mc ?
?c) ?
(cv ?
?c/?Mc),?c ?
(?c/?Mc ?
cv))Thus, the belief distribution found by our LPwill be the one that satisfies the constraints whilesimultaneously minimizing the number of votesfrustrated by the change from the original dis-tribution.
Note that for any linear expressions eand f we can implement max(e, f) in the objec-tive function by replacing it with a new [??,?
]helper variable x and adding the linear constraintsx ?
e and x ?
f .4.3 From Values to Votes to BeliefSolving the LP gives us [0, 1] values for each vari-able cv, but we need to calculate an updated beliefB(c).
We propose two methods for this:Vote Conservation: B(c) = ?
?1(cv ?
?Mc)Vote Loss: B(c) = ?
?1(min(?c, cv ?
?Mc))?
?1 is an inverse of the vote function:?
?1idn(x) = x and ?
?1inv(x) = 1 ?
(1 + y)?1.
VoteConservation reallocates votes such that the totalnumber of votes in each mutual exclusion set, ?M ,remains the same after the redistribution.
How-ever, if the constraints force c to lose votes, shouldwe believe the other claims in Mc more?
UnderVote Loss, a claim can only lose votes, ensuringthat if other claims in Mc become less believable,c does not itself become more believable relativeto claims in other mutual exclusion sets.
We foundVote Loss just slightly better on average and usedit for all reported results.4.4 ?Unknown?
AugmentationAugmenting our data with ?Unknown?
claims en-sures that every LP is feasible and can be usedto model our ignorance given a lack of suffi-cient information or conflicting constraints.
AnUnknown claim UM is added to every mutual ex-clusion set M (but invisible to the fact-finder) andrepresents our belief that none of the claims inM are sufficiently supported.
Now we can writethe mutual exclusion constraint for M as UM +?c?M cv = 1.
When propositionalizing FOL, ifa disjunctive clause contains a non-negated literalfor a claim c, then we add ?UMc to the clause.881For example, Age(John, 35) ?
Age(Tom, 40)becomes Age(John, 35) ?
Age(Tom, 40) ?Age(Tom,Unknown).
The only exception iswhen the clause contains claims from only onemutual exclusion set (e.g.
?I know Sam is 50or 60?
), and so the LP can only be infeasibleif the user directly asserts a contradiction (e.g.
?Sam is 50 and Sam is 60?).
The Unknown it-self has a fixed number of votes that cannot belost; this effectively ?smooths?
our belief in theclaims and imposes a floor for believability.
IfAge(Kim, 30) has 5 votes, Age(Kim, 35) has3 votes, and Age(Kim,Unknown) is fixed at 6votes, we hold that Kim?s age is unknown due tolack of evidence.
The number of votes that shouldbe given to each Unknown for this purpose de-pends, of course, on the particular fact-finder and?
function used; in our experiments, we are notconcerned with establishing ignorance and thusassign 0 votes.5 ExperimentsExperiments were conducted over three domains(city population, basic biographies, and Ameri-can vs. British spelling) with four datasets, allusing the VoteDistance cost function and VoteLoss vote redistribution.
We fixed the number ofiterations of the framework (calculating T i(S),Bi(S) and then solving the LP) at 20, whichwas found sufficient for all fact-finders.
To eval-uate accuracy, after the final iteration we lookat each mutual exclusion set M and predict thehighest-belief claim c ?
M (or, if uM had thehighest belief, the second-highest claim), break-ing ties randomly, and check that it is the trueclaim tM .
We omit any M that does not containa true claim (all known claims are false) and anyM that is trivially correct (containing only oneclaim other than uM ).
All results are shown inTable 1.
Vote is the baseline, choosing either theclaim occurring in the most Wikipedia revisions(in the Pop dataset) or claimed by the most sources(for all other datasets).
Sum is Sums (Hubs andAuthorities), 3Est is 3-Estimates, TFs is simpli-fied TruthFinder, TFc is ?full?
TruthFinder, A?L isAverage?Log, Inv1.2 is Investment with g = 1.2,and Pool1.4 is PooledInvestment with g = 1.4.5.1 IBT vs. L+IWe can enforce our prior knowledge against thebeliefs produced by the fact-finder in each itera-tion, or we can apply these constraints just once,after running the fact-finder for 20 iterations with-out interference.
By analogy to (Punyakanok etal., 2005), we refer to these approaches as infer-ence based training (IBT) and learning + inference(L+I), respectively.
Our results show that whileL+I does better when prior knowledge is not en-tirely correct (e.g.
?Growth?
in the city popula-tion domain), generally performance is compara-ble when the effect of the constraints is mild, butIBT can outperform when prior knowledge is vital(as in the spelling domain) by allowing the fact-finder to learn from the provided corrections.5.2 Wikipedia InfoboxesTo focus on the performance of the framework,we (like previous fact-finding work) naively as-sume that our data are accurately extracted, but wealso require large corpora.
Wikipedia Infoboxes(Wu and Weld, 2007) are a semi-structured sourcecovering many domains with readily available au-thorship, and we produced our city population andbasic biographic datasets from the most recentfull-history dump of the English Wikipedia (takenJanuary 2008).
However, attribution is difficult: ifan author edits the page but not the claim withinthe infobox, is the author implicitly agreeing with(and asserting) the claim?
The best performancewas achieved by being strict for City Populationdata, counting only the direct editing of a claim,and lax for Biography data, counting any edit.We hypothesize this is because editors may lackspecific knowledge about a city?s population (andthus fail to correct an erroneous value) but incor-rect birth or death dates are more noticeable.5.3 Results5.3.1 City PopulationWe collected infoboxes for settlements(Geobox, Infobox Settlement, Infobox City, etc.
)to obtain 44,761 populations claims qualifiedby year (e.g.
pop(Denver, 598707, 2008)), with4,107 authors total.
We took as our ?truth?U.S.
census data, which gave us 308 non-trivial true facts to test against.
Our ?commonsense?
knowledge is that population grows882Table 1: Experimental Results (?
indicates no prior knowledge; all values are percent accuracy)Some results are omitted here (see text).
A?L, Inv1.2, Pool1.4 are our novel algorithmsDataset Prior Knowledge Vote Sum 3Est TFs TFc A?L Inv1.2 Pool1.4Pop ?
81.49 81.82 81.49 82.79 84.42 80.84 87.99 80.19Pop GrowthIBT 82.79 79.87 77.92 82.79 86.36 80.52 85.39 79.87Pop GrowthL+I 82.79 79.55 77.92 83.44 85.39 80.52 89.29 80.84Pop Larger2500IBT 85.39 85.06 80.52 86.04 87.34 84.74 89.29 84.09Pop Larger2500L+I 85.39 85.06 80.52 86.69 86.69 84.42 89.94 84.09SynPop ?
73.45 87.76 84.87 56.12 87.07 90.23 89.41 90.00SynPop Pop?8%IBT 88.31 95.46 92.16 96.42 95.46 96.15 95.46 96.42SynPop Pop?8%L+I 88.31 94.77 92.43 82.39 95.32 95.59 96.29 96.01Bio ?
89.80 89.53 89.80 73.04 90.09 89.24 88.34 90.01Bio CSIBT 89.20 89.61 89.20 72.44 89.91 89.35 88.60 90.20Bio CSL+I 89.20 89.61 89.20 57.10 90.09 89.35 88.49 90.24Bio CS+DecadesIBT 90.58 90.88 90.58 80.30 91.25 90.91 90.02 91.32Bio CS+DecadesL+I 90.58 90.91 90.58 69.27 90.95 90.91 90.09 91.17Spell ?
13.54 9.37 11.96 41.93 7.93 10.23 9.36 9.65Spell Words100IBT 13.69 9.02 12.72 44.28 8.05 9.98 11.11 8.86Spell Words100L+I 13.69 8.86 12.08 46.54 8.05 9.98 9.34 7.89Spell CS+Words100IBT 35.10 31.88 35.10 56.52 29.79 32.85 73.59 80.68Spell CS+Words100L+I 35.10 31.72 34.62 55.39 22.06 32.21 30.92 29.95over time (?Growth?
in table 1); therefore,?v,w,x,y,zpop(v, w, y) ?
pop(v, x, z) ?
y < z ?x > w. Of course, this often does not holdtrue: cities can shrink, but performance wasnevertheless superior to no prior knowledgewhatsoever.
The L+I approach does appreciablybetter because it avoids forcing these sometimes-incorrect constraints onto the claim beliefs whilethe fact-finder iterates (which would propagatethe resulting mistakes), instead applying themonly at the end where they can correct more errorsthan they create.
The sparsity of the data playsa role?only a fraction of cities have populationclaims for multiple years, and those that do aretypically larger cities where the correct claim isasserted by an overwhelming majority, greatlylimiting the potential benefit of our Growthconstraints.
We also considered prior knowledgeof the relative sizes of some cities, randomlyselecting 2500 pairs of them (a, b), where awas more populous than b in year t, asserting?x,ypop(a, x, t) ?
pop(b, y, t) ?
x > y.
This?Larger?
prior knowledge proved more effectivethan our oft-mistaken Growth constraint, withmodest improvement to the highest-performingInvestment fact-finder, and InvestmentL+Ireaches 90.91% with 10,000 such pairs.5.3.2 Synthetic City PopulationWhat if attribution were certain and the datamore dense?
To this end we created a syntheticdataset.
We chose 100 random (real) cities andcreated 100 authors whose individual accuracya was drawn uniformly from [0, 1].
Between 1and 10 claims (also determined uniformly) weremade about each city in each year from 2000to 2008 by randomly-selected authors.
For eachcity with true population p and year, four incor-rect claims were created with populations selecteduniformly from [0.5p, 1.5p], each author claimingp with probability a and otherwise asserting oneof the four incorrect claims.
Our common-senseknowledge was that population did not changeby more than 8% per year (also tried on theWikipedia dataset but with virtually no effect).Like ?Growth?, ?Pop?8%?
does not always hold,but a change of more than 8% is much rarer than ashrinking city.
These constraints greatly improvedresults, although we note this would diminish ifinaccurate claims had less variance around thetrue population.8835.3.3 Basic BiographiesWe scanned infoboxes to find 129,847 claimedbirth dates, 34,201 death dates, 10,418 parent-child pairs, and 9,792 spouses.
To get ?true?
birthand death dates, we extracted data from sev-eral online repositories (after satisfying ourselvesthat they were independent and not derived fromWikipedia!
), eliminating any date these sourcesdisagreed upon, and ultimately obtained a totalof 2,685 dates to test against.
Our common sense(?CS?)
knowledge was: nobody dies before theyare born, people are infertile before the age of 7,nobody lives past 125, all spouses have overlap-ping lifetimes, no child is born more than a yearafter a parent?s (father?s) death, nobody has morethan two parents, and nobody is born or dies after2008 (the ?present day?, the year of the Wikipediadump).
Applying this knowledge roughly halvedconvergence times, but had little effect on the re-sults due to data sparsity similar to that seen inthe population data?while we know many birth-days and some death dates, relatively few biogra-phies had parent-child and spouse claims.
To thiswe also added knowledge of the decade (but notthe exact date) in which 15,145 people were born(?CS+Decades?).
Although common sense alonedoes not notably improve results, it does very wellin conjunction with specific knowledge.5.3.4 American vs. British SpellingPrior knowledge allows us to find a truth thatconforms with the user?s viewpoint, even if thatviewpoint differs from the norm.
After obtaininga list of words with spellings that differed be-tween American and British English (e.g.
?color?vs.
?colour?
), we examined the British NationalCorpus as well as Washington Post and Reutersnews articles, taking the source?s (the article au-thor?s) use of a disputed word as a claim thathis spelling was correct.
Our goal was to find the?true?
British spellings that conformed to a Britishviewpoint, but American spellings predominateby far.
Consequently, without prior knowledge thefact-finders do very poorly against our test set of694 British words, predicting American spellinginstead in accordance with the great majority ofauthors (note that accuracy from an Americanperspective is 1??British?
accuracy).
Next weassumed that the user already knew the correctspelling of 100 random words (removing thesefrom the test set, of course), but with little ef-fect.
Finally, we added our common sense (?CS?
)knowledge: if a spelling a is correct and of length?
4, then if a is a substring of b, a?
b (e.g.
colour?
colourful).
Furthermore, while we do not knowa priori whether a spelling is American or British,we do know if e and f are different spellingsof the same word, and, if two such spellingshave a chain of implication between them, wecan break all links in this chain (while someAmerican spellings will still be linked to Britishspellings, this removes most such errors).
Interest-ingly, common sense alone actually hurts results(e.g.
PooledInvestment (IBT) gets 6.2%), as it es-sentially makes the fact-finders more adept at find-ing the predominant American spellings!
How-ever, when some correct spellings are known, re-sults improve greatly and demonstrate IBT?s abil-ity to spread strong prior knowledge, easily sur-passing L+I.
Results improve further with moreknown spellings (PooledInvestment gets 84.86%with CS+Words200IBT ).6 ConclusionWe have introduced a new framework for in-corporating prior knowledge into a fact-findingsystem, along with several new high-performingfact-finding algorithms (Investment, PooledIn-vestment, and Average?Log).
While the bene-fits of prior knowledge were most dramatic inthe Spelling domain, we saw gains from both?common sense?
and specific knowledge in allexperiments?even the difficult Biography domainsaw faster convergence with common sense aloneand notably higher results when specific knowl-edge was added.
We find that while prior knowl-edge is helpful in reducing error, when the user?sviewpoint disagrees with the norm it becomes ab-solutely essential and, formulated as a linear pro-gram, it need not be the computational burden thatmight otherwise be expected.AcknowledgementsThis research was partly sponsored by the Army ResearchLaboratory (ARL) (accomplished under Cooperative Agree-ment Number W911NF-09-2-0053).
Any opinions, findings,and conclusion or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflect theview of the ARL.884ReferencesAdler, B T and L de Alfaro.
2007.
A content-driven reputa-tion system for the Wikipedia.
WWW ?07, 7:261?270.Artz, D and Y Gil.
2007.
A survey of trust in computerscience and the Semantic Web.
Web Semantics: Science,Services and Agents on the World Wide Web, 5(2):58?71,June.Brin, S and L Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networks andISDN Systems, 30(1-7):107?117.Dong, X, L Berti-equille, and D Srivastava.
2009a.
Integrat-ing conflicting data: the role of source dependence.
Tech-nical report, AT&T Labs-Research, Florham Park, NJ.Dong, X.L., L. Berti-Equille, and Divesh Srivastava.
2009b.Truth discovery and copying detection in a dynamicworld.
VLDB, 2(1):562?573.Galland, Alban, Serge Abiteboul, A. Marian, and PierreSenellart.
2010.
Corroborating information from dis-agreeing views.
In Proceedings of the third ACM interna-tional conference on Web search and data mining, pages131?140.
ACM.Josang, A., S. Marsh, and S. Pope.
2006.
Exploring differenttypes of trust propagation.
Lecture Notes in ComputerScience, 3986:179.Josang, A.
1997.
Artificial reasoning with subjective logic.2nd Australian Workshop on Commonsense Reasoning.Kamvar, S, M Schlosser, and H Garcia-molina.
2003.
TheEigentrust algorithm for reputation management in P2Pnetworks.
WWW ?03.Karmarkar, N. 1984.
A new polynomial-time algorithm forlinear programming.
Combinatorica, 4(4):373?395.Kleinberg, J M. 1999.
Authoritative sources in a hyperlinkedenvironment.
Journal of the ACM, 46(5):604?632.Levien, R. 2008.
Attack-resistant trust metrics.
Computingwith Social Trust, pages 121?132.Manchala, D.W. 1998.
Trust metrics, models and protocolsfor electronic commerce transactions.
Proceedings.
18thInternational Conference on Distributed Computing Sys-tems (Cat.
No.98CB36183), pages 312?321.Marsh, S. 1994.
Formalising Trust as a Computational Con-cept.
PhD thesis, University of Stirling.Novak, V, I Perfilieva, and J Mockof.
1999.
Mathematicalprinciples of fuzzy logic.
Kluwer Academic Publishers.Punyakanok, V., D. Roth, W. Yih, and D. Zimak.
2005.Learning and inference over constrained output.
In Inter-national Joint Conference on Artificial Intelligence, vol-ume 19.Roth, Dan and Wen-tau Yih.
2004.
A linear programmingformulation for global inference in natural language tasks.In Proc.
of the Annual Conference on Computational Nat-ural Language Learning (CoNLL), pages 1?8.Roth, D and W Yih.
2007.
Global Inference for Entity andRelation Identification via a Linear Programming Formu-lation.
In Getoor, Lise and Ben Taskar, editors, Introduc-tion to Statistical Relational Learning.
MIT Press.Russell, Stuart and Peter Norvig.
2003.
Artificial Intelli-gence: A Modern Approach.
Prentice Hall, second edi-tion.Sabater, Jordi and Carles Sierra.
2005. Review on Compu-tational Trust and Reputation Models.
Artificial Intelli-gence Review, 24(1):33?60, September.Shafer, G. 1976.
A mathematical theory of evidence.
Prince-ton University Press Princeton, NJ.Wu, Fei and Daniel S. Weld.
2007.
Autonomously se-mantifying wikipedia.
Proceedings of the sixteenth ACMconference on Conference on information and knowledgemanagement - CIKM ?07, page 41.Yin, Xiaoxin, Philip S. Yu, and Jiawei Han.
2008.
Truth Dis-covery with Multiple Conflicting Information Providerson the Web.
IEEE Transactions on Knowledge and DataEngineering, 20(6):796?808.Yu, Bin and Munindar P. Singh.
2003.
Detecting deceptionin reputation management.
Proceedings of the second in-ternational joint conference on Autonomous agents andmultiagent systems - AAMAS ?03, page 73.Zeng, H, M Alhossaini, L Ding, R Fikes, and D L McGuin-ness.
2006.
Computing trust from revision history.
Intl.Conf.
on Privacy, Security and Trust.885
