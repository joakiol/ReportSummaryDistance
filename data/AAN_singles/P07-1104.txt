Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824?831,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Comparative Study of Parameter Estimation Methods forStatistical Natural Language ProcessingJianfeng Gao*, Galen Andrew*, Mark Johnson*&, Kristina Toutanova**Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com&Brown University, Providence, RI 02912,  mj@cs.brown.eduAbstractThis paper presents a comparative study offive parameter estimation algorithms on fourNLP tasks.
Three of the five algorithms arewell-known in the computational linguisticscommunity: Maximum Entropy (ME) estima-tion with L2 regularization, the AveragedPerceptron (AP), and Boosting.
We also in-vestigate ME estimation with L1 regularizationusing a novel optimization algorithm, andBLasso, which is a version of Boosting withLasso (L1) regularization.
We first investigateall of our estimators on two re-ranking tasks: aparse selection task and a language model(LM) adaptation task.
Then we apply the bestof these estimators to two additional tasksinvolving conditional sequence models: aConditional Markov Model (CMM) for part ofspeech tagging and a Conditional RandomField (CRF) for Chinese word segmentation.Our experiments show that across tasks, threeof the estimators ?
ME estimation with L1 orL2 regularization, and AP ?
are in a near sta-tistical tie for first place.1 IntroductionParameter estimation is fundamental to many sta-tistical approaches to NLP.
Because of thehigh-dimensional nature of natural language, it isoften easy to generate an extremely large number offeatures.
The challenge of parameter estimation isto find a combination of the typically noisy, re-dundant features that accurately predicts the targetoutput variable and avoids overfitting.
Intuitively,this can be achieved either by selecting a smallnumber of highly-effective features and ignoringthe others, or by averaging over a large number ofweakly informative features.
The first intuitionmotivates feature selection methods such asBoosting and BLasso (e.g., Collins 2000; Zhao andYu, 2004), which usually work best when manyfeatures are completely irrelevant.
L1 or Lassoregularization of linear models, introduced byTibshirani (1996), embeds feature selection intoregularization so that both an assessment of thereliability of a feature and the decision aboutwhether to remove it are done in the same frame-work, and has generated a large amount of interestin the NLP community recently (e.g., Goodman2003; Riezler and Vasserman 2004).
If on the otherhand most features are noisy but at least weaklycorrelated with the target, it may be reasonable toattempt to reduce noise by averaging over all of thefeatures.
ME estimators with L2 regularization,which have been widely used in NLP tasks (e.g.,Chen and Rosenfeld 2000; Charniak and Johnson2005; Johnson et al 1999), tend to produce modelsthat have this property.
In addition, the perceptronalgorithm and its variants, e.g., the voted or aver-aged perceptron, is becoming increasingly populardue to their competitive performance, simplicity inimplementation and low computational cost intraining (e.g., Collins 2002).While recent studies claim advantages for L1regularization, this study is the first of which we areaware to systematically compare it to a range ofestimators on a diverse set of NLP tasks.
Gao et al(2006) showed that BLasso, due to its explicit use ofL1 regularization, outperformed Boosting in the LMadaptation task.
Ng (2004) showed that for logisticregression, L1 regularization outperforms L2 regu-larization on artificial datasets which contain manycompletely irrelevant features.
Goodman (2003)showed that in two out of three tasks, an ME esti-mator with a one-sided Laplacian prior (i.e., L1regularization with the constraint that all featureweights are positive) outperformed a comparableestimator using a Gaussian prior (i.e., L2 regulari-zation).
Riezler and Vasserman (2004) showed thatan L1-regularized ME estimator outperformed anL2-regularized estimator for ranking the parses of astochastic unification-based grammar.824While these individual estimators are well de-scribed in the literature, little is known about therelative performance of these methods because thepublished results are generally not directly compa-rable.
For example, in the parse re-ranking task,one cannot tell whether the L2- regularized MEapproach used by Charniak and Johnson (2005)significantly outperforms the Boosting method byCollins (2000) because different feature sets andn-best parses were used in the evaluations of thesemethods.This paper conducts a much-needed comparativestudy of these five parameter estimation algorithmson four NLP tasks: ME estimation with L1 and L2regularization, the Averaged Perceptron (AP),Boosting, and BLasso, a version of Boosting withLasso (L1) regularization.
We first investigate all ofour estimators on two re-ranking tasks: a parseselection task and a language model adaptation task.Then we apply the best of these estimators to twoadditional tasks involving conditional sequencemodels: a CMM for POS tagging and a CRF forChinese word segmentation.
Our results show thatME estimation with L2 regularization achieves thebest performing estimators in all of the tasks, andAP achieves almost as well and requires much lesstraining time.
L1 (Lasso) regularization also per-forms well and leads to sparser models.2 EstimatorsAll the four NLP tasks studied in this paper arebased on linear models (Collins 2000) which re-quire learning a mapping from inputs ?
?
?
tooutputs ?
?
?.
We are given:?
Training samples (??
,??)
for ?
= 1??,?
A procedure ???
to generate a set of candi-dates ???(?)
for an input x,?
A feature mapping ?:?
?
?
?
??
to mapeach (?,?)
to a vector of feature values, and?
A parameter vector ?
?
??
, which assigns areal-valued weight to each feature.For all models except the CMM sequence model forPOS tagging, the components ??
?, ?
and ?
di-rectly define a mapping from an input ?
to an output?(?)
as follows:?
?
= arg max?????
?
?
?,?
?
?.
(1)In the CMM sequence classifier, locally normalizedlinear models to predict the tag of each word tokenare chained together to arrive at a probability esti-mate for the entire tag sequence, resulting in aslightly different decision rule.Linear models, though simple, can capture verycomplex dependencies because the features can bearbitrary functions of the input/output pair.
Forexample, we can define a feature to be the log con-ditional probability of the output as estimated bysome other model, which may in turn depend onarbitrarily complex interactions of ?basic?
features.In practice, with an appropriate feature set, linearmodels achieve very good empirical results onvarious NLP tasks.
The focus of this paper howeveris not on feature definition (which requires domainknowledge and varies from task to task), but onparameter estimation (which is generic acrosstasks).
We assume we are given fixed featuretemplates from which a large number of features aregenerated.
The task of the estimator is to use thetraining samples to choose a parameter vector ?,such that the mapping ?(?)
is capable of correctlyclassifying unseen examples.
We will describe thefive estimators in our study individually.2.1 ME estimation with L2 regularizationLike many linear models, the ME estimator chooses?
to minimize the sum of the empirical loss on thetraining set and a regularization term:?
= arg min?
?
?
+ ?
?
.
(2)In this case, the loss term L(w) is the negative con-ditional log-likelihood of the training data,?
?
= ?
log?
??
??)?
?=1 ,  where?
?
?)
=exp ?
?,?
?
?exp(?
?,?
?
?
?)?
?????
?and the regularizer term ?
?
= ?
??2?
is theweighted squared L2 norm of the parameters.
Here,?
is a parameter that controls the amount of regu-larization, optimized on held-out data.This is one of the most popular estimators,largely due to its appealing computational proper-ties: both ?
?
and ?(?)
are convex and differen-tiable, so gradient-based numerical algorithms canbe used to find the global minimum efficiently.In our experiments, we used the limited memoryquasi-Newton algorithm (or L-BFGS, Nocedal andWright 1999) to find the optimal ?
because thismethod has been shown to be substantially fasterthan other methods such as Generalized IterativeScaling (Malouf 2002).825Because for some sentences there are multiplebest parses (i.e., parses with the same F-Score), weused the variant of ME estimator described inRiezler et al (2002), where ?
?
is defined as thelikelihood of the best parses ?
?
?(?)
relative tothe n-best parser output ???
?
,  (i.e., ?
?
????(?
)): ?
?
= ?
log ?(??
|??)????(??)?
?=1 .We applied this variant in our experiments ofparse re-ranking and LM adaptation, and found thaton both tasks it leads to a significant improvementin performance for the L2-regularied ME estimatorbut not for the L1-regularied ME estimator.2.2 ME estimation with L1 regularizationThis estimator also minimizes the negative condi-tional log-likelihood, but uses an L1 (or Lasso)penalty.
That is, ?(?)
in Equation (2) is definedaccording to ?
?
= ?
??
?
.
L1 regularizationtypically leads to sparse solutions in which manyfeature weights are exactly zero, so it is a naturalcandidate when feature selection is desirable.
Bycontrast, L2 regularization produces solutions inwhich most weights are small but non-zero.Optimizing the L1-regularized objective functionis challenging because its gradient is discontinuouswhenever some parameter equals zero.
Kazama andTsujii (2003) described an estimation method thatconstructs an equivalent constrained optimizationproblem with twice the number of variables.However, we found that this method is impracti-cally slow for large-scale NLP tasks.
In this workwe use the orthant-wise limited-memory qua-si-Newton algorithm (OWL-QN), which is a mod-ification of L-BFGS that allows it to effectivelyhandle the discontinuity of the gradient (Andrewand Gao 2007).
We provide here a high-level de-scription of the algorithm.A quasi-Newton method such as L-BFGS usesfirst order information at each iterate to build anapproximation to the Hessian matrix, ?, thus mod-eling the local curvature of the function.
At eachstep, a search direction is chosen by minimizing aquadratic approximation to the function:?
?
=12?
?
?0??
?
?
?0 + ?0?
(?
?
?0)where ?0 is the current iterate, and ?0 is the func-tion gradient at ?0 .
If ?
is positive definite, theminimizing value of ?
can be computed analyticallyaccording to: ??
= ?0 ??
?1?0.L-BFGS maintains vectors of the change in gradient??
?
??
?1 from the most recent iterations, and usesthem to construct an estimate of the inverse Hessian???.
Furthermore, it does so in such a way that?
?1?0 can be computed without expanding out thefull matrix, which is typically unmanageably large.The computation requires a number of operationslinear in the number of variables.OWL-QN is based on the observation that whenrestricted to a single orthant, the L1 regularizer isdifferentiable, and is in fact a linear function of ?.Thus, so long as each coordinate of any two con-secutive search points does not pass through zero,?(?)
does not contribute at all to the curvature ofthe function on the segment joining them.
There-fore, we can use L-BFGS to approximate the Hes-sian of ?
?
alone, and use it to build an approxi-mation to the full regularized objective that is validon a given orthant.
To ensure that the next point is inthe valid region, we project each point during theline search back onto the chosen orthant.1 At eachiteration, we choose the orthant containing thecurrent point and into which the direction giving thegreatest local rate of function decrease points.This algorithm, although only a simple modifi-cation of L-BFGS, works quite well in practice.
Ittypically reaches convergence in even fewer itera-tions than standard L-BFGS takes on the analogousL2-regularized objective (which translates to lesstraining time, since the time per iteration is onlynegligibly higher, and total time is dominated byfunction evaluations).
We describe OWL-QN morefully in (Andrew and Gao 2007).
We also show thatit is significantly faster than Kazama and Tsujii?salgorithm for L1 regularization and prove that it isguaranteed converge to a parameter vector thatglobally optimizes the L1-regularized objective.2.3 BoostingThe Boosting algorithm we used is based on Collins(2000).
It optimizes the pairwise exponential loss(ExpLoss) function (rather than the logarithmic lossoptimized by ME).
Given a training sample(??
,??
), for each possible output ??
?
???(??
), we1 This projection just entails zeroing-out any coordinatesthat change sign.
Note that it is possible for a variable tochange sign in two iterations, by moving from a negativevalue to zero, and on a the next iteration moving fromzero to a positive value.826define the margin of the pair (??
,?? )
with respect to?
as ?
??
,??
= ?
??
,??
?
?
?
?
??
,??
?
?.Then ExpLoss is defined asExpLoss ?
=  exp  ?M yi , yj??????
??
?
(3)Figure 1 summarizes the Boosting algorithm weused.
It is an incremental feature selection proce-dure.
After initialization, Steps 2 and 3 are repeatedT times; at each iteration, a feature is chosen and itsweight is updated as follows.First, we define Upd(?,?, ?)
as an updatedmodel, with the same parameter values as ?
withthe exception of ??
, which is incremented by ?
:Upd ?, ?, ?
= (?1 ,?
,??
+ ?,?
,??
)Then, Steps 2 and 3 in Figure 1 can be rewritten asEquations (4) and (5), respectively.?
?, ??
= arg min?
,?ExpLoss(Upd ?, ?, ? )
(4)??
= Upd(??
?1, ?
?, ??)
(5)Because Boosting can overfit we update the weightof ???
by a small fixed step size ?, as in Equation (6),following the FSLR algorithm (Hastie et al 2001).??
= Upd(??
?1, ?
?, ?
?
sign ?? )
(6)By taking such small steps, Boosting imposes akind of implicit regularization, and can closelyapproximate the effect of L1 regularization in a localsense (Hastie et al 2001).
Empirically, smallervalues of ?
lead to smaller numbers of test errors.2.4 Boosted LassoThe Boosted Lasso (BLasso) algorithm was origi-nally proposed in Zhao and Yu (2004), and wasadapted for language modeling by Gao et al (2006).BLasso can be viewed as a version of Boosting withL1 regularization.
It optimizes an L1-regularizedExpLoss function:LassoLoss ?
= ExpLoss(?)
+ ?(?)
(7)where ?
?
= ?
??
?
.BLasso also uses an incremental feature selec-tion procedure to learn parameter vector ?, just asBoosting does.
Due to the explicit use of the regu-larization term ?(?
), however, there are two majordifferences from Boosting.At each iteration, BLasso takes either a forwardstep or a backward step.
Similar to Boosting, ateach forward step, a feature is selected and itsweight is updated according to Eq.
(8) and (9).?
?, ??
= ???
????
,?=?
?ExpLoss(Upd ?, ?, ? )
(8)??
= Upd(??
?1, ?
?, ?
?
sign ?? )
(9)There is a small but important difference betweenEquations (8) and (4).
In Boosting, as shown inEquation (4), a feature is selected by its impact onreducing the loss with its optimal update ??
.
Bycontrast, in BLasso, as shown in Equation (8),rather than optimizing over ?
for each feature, theloss is calculated with an update of either +?
or ?
?,i.e., grid search is used for feature weight estima-tion.
We found in our experiments that this mod-ification brings a consistent improvement.The backward step is unique to BLasso.
At eachiteration, a feature is selected and the absolute valueof its weight is reduced by ?
if and only if it leads toa decrease of the LassoLoss, as shown in Equations(10) and (11), where ?
is a tolerance parameter.??
= arg min?
:??
?0ExpLoss(Upd(?, ?,?
?sign ?? )
(10)??
= Upd(??
?1 , ??,sign(???)
?
?)
(11)if LassoLoss ???1,??
?1 ?
LassoLoss ??
,??
> ?Figure 2 summarizes the BLasso algorithm weused.
After initialization, Steps 4 and 5 are repeatedT times; at each iteration, a feature is chosen and itsweight is updated either backward or forward by afixed amount ?.
Notice that the value of ?
is adap-tively chosen according to the reduction of ExpLossduring training.
The algorithm starts with a largeinitial ?, and then at each forward step the value of?
decreases until ExpLoss stops decreasing.
This isintuitively desirable: it is expected that most highlyeffective features are selected in early stages oftraining, so the reduction of ExpLoss at each step inearly stages are more substantial than in later stages.These early steps coincide with the Boosting stepsmost of the time.
In other words, the effect ofbackward steps is more visible at later stages.
It canbe proved that for a finite number of features and?
=0, the BLasso algorithm shown in Figure 2converges to the Lasso solution when ?
?
0.
SeeGao et al (2006) for implementation details, andZhao and Yu (2004) for a theoretical justificationfor BLasso.1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1?D2 Select a feature fk* which has largest estimatedimpact on reducing ExpLoss of Equation (3)3 Update ?k* ?
?k* + ?
*, and return to Step 2Figure 1: The boosting algorithm8272.5 Averaged PerceptronThe perceptron algorithm can be viewed as a formof incremental training procedure (e.g., using sto-chastic approximation) that optimizes a minimumsquare error (MSE) loss function (Mitchell, 1997).As shown in Figure 3, it starts with an initial pa-rameter setting and updates it for each trainingexample.
In our experiments, we used the AveragedPerceptron algorithm of Freund and Schapire(1999), a variation that has been shown to be moreeffective than the standard algorithm (Collins2002).
Let ??,?
be the parameter vector after the ?thtraining sample has been processed in pass ?
overthe training data.
The average parameters are de-fined as?
=?????,???
where T is the number ofepochs, and N is the number of training samples.3 EvaluationsFrom the four tasks we consider, parsing and lan-guage model adaptation are both examples ofre-ranking.
In these tasks, we assume that we havebeen given a list of candidates ???(?)
for eachtraining or test sample  ?,?
, generated using abaseline model.
Then, a linear model of the form inEquation (1) is used to discriminatively re-rank thecandidate list using additional features which mayor may not be included in the baseline model.
Sincethe mapping from ?
to ?
by the linear model maymake use of arbitrary global features of the outputand is performed ?all at once?, we call such a linearmodel a global model.In the other two tasks (i.e., Chinese word seg-mentation and POS tagging), there is no explicitenumeration of ???(?).
The mapping from ?
to ?is determined by a sequence model which aggre-gates the decisions of local linear models via adynamic program.
In the CMM, the local linearmodels are trained independently, while in the CRFmodel, the local models are trained jointly.
We callthese two linear models local models because theydynamically combine the output of models that useonly local features.While it is straightforward to apply the five es-timators to global models in the re-rankingframework, the application of some estimators tothe local models is problematic.
Boosting andBLasso are too computationally expensive to beapplied to CRF training and we compared the otherthree better performing estimation methods for thismodel.
The CMM is a probabilistic sequence modeland the log-loss used by ME estimation is mostnatural for it; thus we limit the comparison to thetwo kinds of ME models for CMMs.
Note that ourgoal is not to compare locally trained models toglobally trained ones; for a study which focuses onthis issue, see (Punyakanok et al 2005).In each task we compared the performance ofdifferent estimators using task-specific measures.We used the Wilcoxon signed rank test to test thestatistical significance of the difference among thecompeting estimators.
We also report other resultssuch as number of non-zero features after estima-tion, number of training iterations, and computationtime (in minutes of elapsed time on an XEONTM MP3.6GHz machine).3.1 Parse re-rankingWe follow the experimental paradigm of parsere-ranking outlined in Charniak and Johnson(2005), and fed the features extracted by their pro-gram to the five rerankers we developed.
Each usesa linear model trained using one of the five esti-mators.
These rerankers attempt to select the bestparse ?
for a sentence ?
from the 50-best list ofpossible parses ???
?
for the sentence.
The li-near model combines the log probability calculatedby the Charniak (2000) parser as a feature with1,219,272 additional features.
We trained the fea-1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0for d=1?D.2 Take a forward step according to Eq.
(8) and (9), andthe updated model is denoted by w13 Initialize ?
= (ExpLoss(w0)-ExpLoss(w1))/?4 Take a backward step if and only if it leads to a de-crease of LassoLoss according to Eq.
(10) and (11),where ?
= 0; otherwise5 Take a forward step according to Eq.
(8) and (9);update ?
= min(?, (ExpLoss(wt-1)-ExpLoss(wt))/?
);and return to Step 4.Figure 2: The BLasso algorithm1 Set w0 = 1 and wd = 0 for d=1?D2 For t = 1?T (T = the total number of iterations)3    For each training sample (xi, yi), i = 1?N4??
= arg max?????
?_??
??
, ?
?
?Choose the best candidate zi from GEN(xi) usingthe current model w,5       w = w +  ?(?
(xi, yi) ?
?
(xi, zi)), where ?
is the size oflearning step, optimized on held-out data.Figure 3: The perceptron algorithm828ture weights w on Sections 2-19 of the Penn Tree-bank, adjusted the regularizer constant ?
to max-imize the F-Score on Sections 20-21 of the Tree-bank, and evaluated the rerankers on Section 22.The results are presented in Tables 12 and 2, whereBaseline results were obtained using the parser byCharniak (2000).The ME estimation with L2 regularization out-performs all of the other estimators significantlyexcept for the AP, which performs almost as welland requires an order of magnitude less time intraining.
Boosting and BLasso are feature selectionmethods in nature, so they achieve the sparsestmodels, but at the cost of slightly lower perfor-mance and much longer training time.
TheL1-regularized ME estimator also produces a rela-tively sparse solution whereas the Averaged Per-ceptron and the L2-regularized ME estimator assignalmost all features a non-zero weight.3.2 Language model adaptationOur experiments with LM adaptation are based onthe work described in Gao et al (2006).
The va-riously trained language models were evaluatedaccording to their impact on Japanese text inputaccuracy, where input phonetic symbols ?
aremapped into a word string ?.
Performance of theapplication is measured in terms of character error2The result of ME/L2 is better than that reported inAndrew and Gao (2007) due to the use of the variant ofL2-regularized ME estimator, as described in Section 2.1.CER # features time (min) #train iterBaseline 10.24%MAP 7.98%ME/L2 6.99% 295,337 27 665ME/L1 7.01% 53,342 25 864AP 7.23% 167,591 6 56Boost 7.54% 32,994 175 71,000BLasso 7.20% 33,126 238 250,000Table 3.
Performance summary of estimators(lower is better) on language model adaptationME/L2 ME/L1 AP Boost BLassoME/L2  ~ >> >> >>ME/L1 ~  >> >> >>AP << <<  >> ~Boost << << <<  <<BLasso << << ~ >>Table 4.
Statistical significance test results.rate (CER), which is the number of characterswrongly converted from ?
divided by the number ofcharacters in the correct transcript.Again we evaluated five linear rerankers, one foreach estimator.
These rerankers attempt to select thebest conversions ?
for an input phonetic string ?from a 100-best list ???(?
)of possible conver-sions proposed by a baseline system.
The linearmodel combines the log probability under a trigramlanguage model as base feature and additional865,190 word uni/bi-gram features.
Theseuni/bi-gram features were already included in thetrigram model which was trained on a backgrounddomain corpus (Nikkei Newspaper).
But in thelinear model their feature weights were traineddiscriminatively on an adaptation domain corpus(Encarta Encyclopedia).
Thus, this forms a crossdomain adaptation paradigm.
This also implies thatthe portion of redundant features in this task couldbe much larger than that in the parse re-rankingtask, especially because the background domain isreasonably similar to the adaptation domain.We divided the Encarta corpus into three setsthat do not overlap.
A 72K-sentences set was usedas training data, a 5K-sentence set as developmentdata, and another 5K-sentence set as testing data.The results are presented in Tables 3 and 4, whereBaseline is the word-based trigram model trainedon background domain corpus, and MAP (maxi-mum a posteriori) is a traditional model adaptationmethod, where the parameters of the backgroundmodel are adjusted so as to maximize the likelihoodof the adaptation data.F-Score # features time (min) # train iterBaseline 0.8986ME/L2 0.9176 1,211,026 62     129ME/L1 0.9165 19,121 37 174AP 0.9164 939,248 2 8Boosting 0.9131 6,714 495 92,600BLasso 0.9133 8,085 239 56,500Table 1: Performance summary of estimators onparsing re-ranking (ME/L2: ME with L2 regulari-zation; ME/L1:  ME with L1 regularization)ME/L2 ME/L1 AP Boost BLassoME/L2  >> ~ >> >>ME/L1 <<  ~ > ~AP ~ ~  >> >Boost << < <<  ~Blasso << ~ < ~Table 2: Statistical significance test results (?>>?or ?<<?
means P-value < 0.01; > or < means 0.01 <P-value ?
0.05; ?~?
means P-value > 0.05)829The results are more or less similar to those inthe parsing task with one visible difference: L1regularization achieved relatively better perfor-mance in this task.
For example, while in theparsing task ME with L2 regularization significantlyoutperforms ME with L1 regularization, their per-formance difference is not significant in this task.While in the parsing task the performance differ-ence between BLasso and Boosting is not signifi-cant, BLasso outperforms Boosting significantly inthis task.
Considering that a much higher propor-tion of the features are redundant in this task thanthe parsing task, the results seem to corroborate theobservation that L1 regularization is robust to thepresence of many redundant features.3.3 Chinese word segmentationOur third task is Chinese word segmentation(CWS).
The goal of CWS is to determine theboundaries between words in a section of Chinesetext.
The model we used is the hybrid Mar-kov/semi- Markov CRF described by Andrew(2006), which was shown to have state-of-the-artaccuracy.
We tested models trained with the variousestimation methods on the Microsoft Research Asiacorpus from the Second International Chinese WordSegmentation, and we used the same train/test splitused in the competition.
The model and experi-mental setup is identical with that of Andrew (2006)except for two differences.
First, we extractedfeatures from both positive and negative trainingexamples, while Andrew (2006) uses only featuresthat occur in some positive training example.Second, we used the last 4K sentences of thetraining data to select the weight of the regularizersand to determine when to stop perceptron training.We compared three of the best performing es-timation procedures on this task: ME with L2 regu-larization, ME with L1 regularization, and the Av-eraged Perceptron.
In this case, ME refers to mi-nimizing the negative log-probability of the correctsegmentation, which is globally normalized, whilethe perceptron is trained using at each iteration theexact maximum-scoring segmentation with thecurrent weights.
We observed the same pattern as inthe other tasks: the three algorithms have nearlyidentical performance, while L1 uses only 6% of thefeatures, and the Averaged Perceptron requiressignificantly fewer training iterations.
In this case,L1 was also several times faster than L2.
The resultsare summarized in Table 5.3We note that all three algorithms performedslightly better than the model used by Andrew(2006), which also used L2 regularization (96.84F1).
We believe the difference is due to the use offeatures derived from negative training examples.3.4 POS taggingFinally we studied the impact of the regularizationmethods on a Maximum Entropy conditionalMarkov Model (MEMM, McCallum et al 2000) forPOS tagging.
MEMMs decompose the conditionalprobability of a tag sequence given a word sequenceas follows:?
?1 ?
??
?1 ???
= ?(??|??
?1 ?????
,?1 ???)?
?=1where the probability distributions for each taggiven its context are ME models.
Following pre-vious work (Ratnaparkhi, 1996), we assume that thetag of a word is independent of the tags of all pre-ceding words given the tags of the previous twowords (i.e., ?=2 in the equation above).
The localmodels at each position include features of thecurrent word, the previous word, the next word, andfeatures of the previous two tags.
In addition tolexical identity of the words, we used features ofword suffixes, capitalization, and number/specialcharacter signatures of the words.We used the standard splits of the Penn Treebankfrom the tagging literature (Toutanova et al 2003)for training, development and test sets.
The trainingset comprises Sections 0-18, the development set ?Sections 19-21, and the test set ?
Sections 22-24.We compared training the ME models using L1 andL2 regularization.
For each of the two types ofregularization we selected the best value of theregularization constant using grid search to optim-ize the accuracy on the development set.
We reportfinal accuracy measures on the test set in Table 6.The results on this task confirm the trends wehave seen so far.
There is almost no difference in3 Only the L2 vs. AP comparison is significant at a 0.05level according to the Wilcoxon signed rank test.Test F1 # features # train iterME/L2 0.9719 8,084,086 713ME/L1 0.9713 317,146 201AP 0.9703 1,965,719 162Table 5.
Performance summary of estimators onCWS830accuracy of the two kinds of regularizations, andindeed the differences were not statistically signif-icant.
Estimation with L1 regularization requiredconsiderably less time than estimation with L2, andresulted in a model which is more than ten timessmaller.4 ConclusionsWe compared five of the most competitive para-meter estimation methods on four NLP tasks em-ploying a variety of models, and the results wereremarkably consistent across tasks.
Three of themethods ?
ME estimation with L2 regularization,ME estimation with L1 regularization, and the Av-eraged Perceptron ?
were nearly indistinguishablein terms of test set accuracy, with ME estimationwith L2 regularization perhaps enjoying a slightlead.
Meanwhile, ME estimation with L1 regulari-zation achieves the same level of performance whileat the same time producing sparse models, and theAveraged Perceptron provides an excellent com-promise of high performance and fast training.These results suggest that when deciding whichtype of parameter estimation to use on these orsimilar NLP tasks, one may choose any of thesethree popular methods and expect to achieve com-parable performance.
The choice of which to im-plement should come down to other considerations:if model sparsity is desired, choose ME estimationwith L1 regularization (or feature selection methodssuch as BLasso); if quick implementation andtraining is necessary, use the Averaged Perceptron;and ME estimation with L2 regularization may beused if it is important to achieve the highest ob-tainable level of performance.ReferencesAndrew, G. 2006.
A hybrid Markov/semi-Markov condi-tional random field for sequence segmentation.
In EMNLP,465-472.Andrew, G. and Gao, J.
2007.
Scalable training ofL1-regularized log-linear models.
In ICML.Charniak, E. 2000.
A maximum-entropy-inspired parser.
InNAACL, 132-139.Charniak, E. and Johnson, M. 2005.
Coarse-to-fine n-bestparsing and MaxEnt discriminative re-ranking.
In ACL.173-180.Chen, S.F., and Rosenfeld, R. 2000.
A survey of smoothingtechniques for ME models.
IEEE Trans.
On Speech and AudioProcessing, 8(2): 37-50.Collins, M. 2000.
Discriminative re-ranking for naturallanguage parsing.
In ICML, 175-182.Collins, M. 2002.
Discriminative training methods for hid-den Markov models: Theory and experiments with per-ceptron algorithms.
In EMNLP, 1-8.Freund, Y, R. Iyer, R. E. Schapire, and Y.
Singer.
1998.
Anefficient boosting algorithm for combining preferences.
InICML?98.Freund, Y. and Schapire, R. E. 1999.
Large margin classifica-tion using the perceptron algorithm.
In Machine Learning,37(3): 277-296.Hastie, T., R. Tibshirani and J. Friedman.
2001.
The elements ofstatistical learning.
Springer-Verlag, New York.Gao, J., Suzuki, H., and Yu, B.
2006.
Approximation lassomethods for language modeling.
In ACL.Goodman, J.
2004.
Exponential priors for maximum entropymodels.
In NAACL.Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S.1999.
Estimators for stochastic ?Unification-based?grammars.
In ACL.Kazama, J. and Tsujii, J.
2003.
Evaluation and extension ofmaximum entropy models with inequality constraints.
InEMNLP.Malouf, R. 2002.
A comparison of algorithms for maximumentropy parameter estimation.
In HLT.McCallum A, D. Freitag and F. Pereira.
2000.
Maximumentropy markov models for information extraction andsegmentation.
In ICML.Mitchell, T. M. 1997.
Machine learning.
The McGraw-HillCompanies, Inc.Ng, A. Y.
2004.
Feature selection, L1 vs. L2 regularization,and rotational invariance.
In ICML.Nocedal, J., and Wright, S. J.
1999.
Numerical Optimization.Springer, New York.Punyakanok, V., D. Roth, W. Yih, and D. Zimak.
2005.Learning and inference over constrained output.
In IJCAI.Ratnaparkhi, A.
1996.
A maximum entropy part-of-speechtagger.
In EMNLP.Riezler, S., and Vasserman, A.
2004.
Incremental featureselection and L1 regularization for relax maximum entro-py modeling.
In EMNLP.Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J.,and Johnson, M. 2002.
Parsing the wall street journal usinga lexical-functional grammar and discriminative estima-tion techniques.
In ACL.
271-278.Tibshirani, R. 1996.
Regression shrinkage and selection viathe lasso.
J. R. Statist.
Soc.
B, 58(1): 267-288.Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.2003.
Feature-rich Part-of-Speech tagging with a cyclicdependency network.
In HLT-NAACL, 252-259.Zhao, P. and B. Yu.
2004.
Boosted lasso.
Tech Report, StatisticsDepartment, U. C. Berkeley.Accuracy (%) # features # train iterMEMM/L2 96.39 926,350 467MEMM/L1 96.41 84,070 85Table 6.
Performance summary of estimators onPOS tagging831
