Gloss-Based Semantic Similarity Metrics for Predominant Sense AcquisitionRyu IidaNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192, Japanryu-i@is.naist.jpDiana McCarthy and Rob KoelingUniversity of SussexFalmer, East SussexBN1 9QH, UK{dianam,robk}@sussex.ac.ukAbstractIn recent years there have been various ap-proaches aimed at automatic acquisition ofpredominant senses of words.
This infor-mation can be exploited as a powerful back-off strategy for word sense disambiguationgiven the zipfian distribution of word senses.Approaches which do not require manuallysense-tagged data have been proposed forEnglish exploiting lexical resources avail-able, notably WordNet.
In these approachesdistributional similarity is coupled with a se-mantic similarity measure which ties the dis-tributionally related words to the sense in-ventory.
The semantic similarity measuresthat have been used have all taken advantageof the hierarchical information in WordNet.We investigate the applicability to Japaneseand demonstrate the feasibility of a mea-sure which uses only information in the dic-tionary definitions, in contrast with previ-ous work on English which uses hierarchi-cal information in addition to dictionary def-initions.
We extend the definition basedsemantic similarity measure with distribu-tional similarity applied to the words in dif-ferent definitions.
This increases the recallof our method and in some cases, precisionas well.1 IntroductionWord sense disambiguation (WSD) has been an ac-tive area of research over the last decade becausemany researches believe it will be important forapplications which require, or would benefit from,some degree of semantic interpretation.
There hasbeen considerable skepticism over whether WSDwill actually improve performance of applications,but we are now starting to see improvement in per-formance due to WSD in cross-lingual informationretrieval (Clough and Stevenson, 2004; Vossen etal., 2006) and machine translation (Carpuat and Wu,2007; Chan et al, 2007) and we hope that other ap-plications such as question-answering, text simplifi-cation and summarisation might also benefit as WSDmethods improve.In addition to contextual evidence, most WSD sys-tems exploit information on the most likely mean-ing of a word regardless of context.
This is a pow-erful back-off strategy given the skewed nature ofword sense distributions.
For example, in the En-glish coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base-line of choosing the most frequent sense using thefirst WordNet sense attained precision and recall of78.9%which is only a few percent lower than the topscoring system which obtained 82.5%.
This findingis in line with previous results (Snyder and Palmer,2004).
Systems using a first sense heuristic haverelied on sense-tagged data or lexicographer judg-ment as to which is the predominant sense of a word.However sense-tagged data is expensive and further-more the predominant sense of a word will vary de-pending on the domain (Koeling et al, 2005; Chanand Ng, 2007).One direction of research following McCarthy etal.
(2004) has been to learn the most predominant561sense of a word automatically.
McCarthy et alsmethod relies on two methods of similarity.
Firstly,distributional similarity is used to estimate the pre-dominance of a sense from the number of distribu-tionally similar words and the strength of their dis-tributional similarity to the target word.
This is doneon the premise that more prevalent meanings havemore evidence in the corpus data used for the distri-butional similarity calculations and the distribution-ally similar words (nearest neighbours) to a targetreflect the more predominant meanings as a conse-quence.
Secondly, the senses in the sense inventoryare linked to the nearest neighbours using semanticsimilarity which incorporates information from thesense inventory.
It is this semantic similarity mea-sure which is the focus of our paper in the context ofthe method for acquiring predominant senses.Whilst the McCarthy et al?s method works wellfor English, other inventories do not always haveWordNet style resources to tie the nearest neigh-bours to the sense inventory.
WordNet has many se-mantic relations as well as glosses associated withits synsets (near synonym sets).
While traditionaldictionaries do not organise senses into synsets, theydo typically have sense definitions associated withthe senses.
McCarthy et al (2004) suggest that dic-tionary definitions can be used with their method,however in the implementation of the measure basedon dictionary definitions that they use, the dictionarydefinitions are extended to those of related words us-ing the hierarchical structure of WordNet (Banerjeeand Pedersen, 2002).
This extension to the originalmethod (Lesk, 1986) was proposed because there isnot always sufficient overlap of the individual wordsfor which semantic similarity is being computed.
Inthis paper we refer to the original method (Lesk,1986) as lesk and the extended measure proposedby Banerjee and Pedersen as Elesk.This paper investigates the potential of usingthe overlap of dictionary definitions with the Mc-Carthy et al?s method.
We test the method forobtaining a first sense heuristic using two publiclyavailable datasets of sense-tagged data in Japanese,EDR (NICT, 2002) and the SENSEVAL-2 Japanesedictionary task (Shirai, 2001).
We contrast an imple-mentation of lesk (Lesk, 1986) which uses only dic-tionary definitions with the Jiang-Conrath measure(jcn) (Jiang and Conrath, 1997) which uses man-ually produced hyponym links and was used pre-viously for this purpose on English datasets (Mc-Carthy et al, 2004).
The jcn measure is only ap-plicable to the EDR dataset because the dictionaryhas hyponymy links which are not available in theSENSEVAL-2 Japanese dictionary task.
We also pro-pose a new extension to lesk which does not requirehand-crafted hyponym links but instead uses distri-butional similarity to increase the possibilities foroverlap of the word definitions.
We refer to this newmeasure as DSlesk.
We compare this to the originallesk on both datasets and show that it increases re-call, and sometimes precision too whilst not requir-ing hyponym links.In the next section we place our contribution in re-lation to previous work.
In section 3 we summarisethe methods we adopt from previous work, and de-scribe our proposal for a semantic similarity methodthat can supplement the information from dictionarydefinitions with information from raw text.
In sec-tion 4 we describe the experiments on EDR and theSENSEVAL-2 Japanese dictionary task and we con-clude in section 5.2 Related WorkThis work builds upon that of McCarthy et al (2004)which acquires predominant senses for target wordsfrom a large sample of text using distributional sim-ilarity (Lin, 1998) to provide evidence for predomi-nance.
The evidence from the distributional similar-ity is allocated to the senses using semantic similar-ity fromWordNet (Patwardhan and Pedersen, 2003).We will describe the method more fully below insection 3.
McCarthy et al (2004) reported resultsfor English using their automatically acquired firstsense heuristic on SemCor (Miller et al, 1993) andthe SENSEVAL-2 English all words dataset (Sny-der and Palmer, 2004).
The results from this arepromising, given that hand-labelled data is not re-quired.
On polysemous nouns from SemCor theyobtained 48% WSD using their method with Eleskand 46% with jcn where the random baseline was24% and the upper-bound was 67% (derived fromthe SemCor test data itself).
On SENSEVAL-2 allwords dataset using the jcn measure 1 they obtained63% recall which is encouraging compared to the1They did not apply lesk to this dataset.562SemCor heuristic which obtained 68% but requireshand-labelled data.
The upper-bound on the datasetwas 72% from the test data itself.
These results cru-cially depend on the information in the sense inven-tory WordNet.
WordNet contains hierarchical rela-tions between word senses which are used in bothjcn and Elesk.
There is an issue that such infor-mation may not be available in other sense invento-ries, and other inventories will be needed for otherlanguages.
In this paper, we implement the lesk se-mantic similarity (Lesk, 1986) for the two Japaneselexicons used in our test datasets, i) the EDR dic-tionary (NICT, 2002) ii) the Iwanami Kokugo JitenDictionary (Nishio et al, 1994).
We investigate thepotential of lesk and jcn, where the latter is applica-ble.
In addition to implementing the original leskmeasure, we propose an extension to the methodinspired by Mihalcea et al (2006).
Mihalcea etal.
(2006) used various text based similarity mea-sures, including WordNet and corpus based similar-ity methods, to determine if two phrases are para-phrases.
They contrasted this approach with previ-ous methods which used overlap of the words be-tween the candidate paraphrases.
For each word ineach of the two texts they obtain the maximum sim-ilarity between the word and any of the words fromthe putative paraphrase.
The similarity scores foreach word of both phrases contribute to an overallsemantic similarity between 0 and 1 and a thresholdof 0.5 is used to decide if the candidate phrases areparaphrases.
In our work, we compare glosses ofwords senses (senses of the target word and sensesof the nearest neighbour) rather than paraphrases.
Inthis approach we extend the definition overlap byconsidering the distributional similarity (Lin, 1998)rather than identify of the words in the two defini-tions.In addition to McCarthy et al (2004) there areother approaches to finding predominant senses.Chan and Ng (2005) use parallel data to provideestimates for sense frequency distributions to feedinto a supervised WSD system.
Mohammad andHirst (2006) propose an approach to acquiring pre-dominant senses from corpora which makes useof the category information in the Macquarie The-saurus (Barnard, 1986).
Lexical chains (Galley andMcKeown, 2003) may also provide a useful firstsense heuristic (Brody et al, 2006) but are producedusingWordNet relations.
We use theMcCarthy et alapproach because this is applicable without alignedcorpus data, semantic category and relation informa-tion and is applicable to any language assuming theminimum requirements of i) dictionary definitionsassociated with the sense inventory and ii) raw cor-pus data.
We adapt their technique to remove thereliance on hyponym links.3 Gloss-based semantic similarityWe first summarise the McCarthy et al methodand the WordNet based semantic similarity func-tions (jcn and Elesk) that they use for automaticacquisition of a first sense heuristic applied to dis-ambiguation of English WordNet datasets.
We thendescribe the additional semantic similarity methodthat we propose for comparison with lesk and jcn.McCarthy et al use a distributional similarity the-saurus acquired from corpus data using the methodof Lin (1998) for finding the predominant sense ofa word where the senses are defined by WordNet.The thesaurus provides the k nearest neighbours toeach target word, along with the distributional sim-ilarity score between the target word and its neigh-bour.
The WordNet similarity package (Patwardhanand Pedersen, 2003) is used to weight the contribu-tion that each neighbour makes to the various sensesof the target word.Let w be a target word and Nw = {n1,n2...nk}be the ordered set of the top scoring kneighbours of w from the thesaurus withassociated distributional similarity scores{dss(w,n1),dss(w,n2), ...dss(w,nk)} using (Lin,1998).
Let senses(w) be the set of senses of wfor each sense of w (wsi ?
senses(w)) a ranking isobtained using:Prevalence Score(wsi) =?n j?Nwdss(w,n j)?wnss(wsi,n j)?wsi??senses(w)wnss(wsi?
,n j)(1)where wnss is the maximum WordNet similarityscore between wsi and the WordNet sense of theneighbour (n j) that maximises this score.
McCarthyet al compare two different WordNet similarityscores, jcn and Elesk.jcn (Jiang and Conrath, 1997) uses corpus datato estimate a frequency distribution over the classes563(synsets) in the WordNet hierarchy.
Each synset, isincremented with the frequency counts from the cor-pus of all words belonging to that synset, directly orvia the hyponymy relation.
The frequency data isused to calculate the ?information content?
(IC) of aclass or sense (s):IC(s) =?log(p(s))Jiang and Conrath specify a distance measure be-tween two senses (s1,s2):D jcn(s1,s2) = IC(s1)+ IC(s2)?2?
IC(s3)where the third class (s3) is the most informative, ormost specific, superordinate synset of the two sensess1 and s2.
This is transformed from a distance mea-sure in the WordNet Similarity package by takingthe reciprocal:jcn(s1,s2) = 1/D jcn(s1,s2)McCarthy et al use the above measure with wsias s1 and whichever sense of the neigbour (n j) thatmaximises this WordNet similarity score.Elesk (Banerjee and Pedersen, 2002) extends theoriginal lesk algorithm (Lesk, 1986) so we describethat original algorithm lesk first.
This simply cal-culates the overlap of the content words in the defi-nitions, frequently referred to as glosses, of the twoword senses.lesk(s1,s2) =?a?g1member(a,g2)member(a,g2) ={1 if a appears in g20 otherwisewhere g1 is the gloss of word sense s1, g2 is the glossof s2 and a is one of words appearing in g1.
In Eleskwhich McCarthy et al use the measure is extendedby considering related synsets to s1 and s2, againwhere s1 is wsi and s2 is the sense from all sensesof n j that maximises the Elesk WordNet similar-ity score.
Elesk relies heavily on the relationshipsthat are encoded in WordNet such as hyponymy andmeronymy.
Not all languages have resources sup-plied with these relations, and where they are sup-plied there may not be as much detail as there is inWordNet.In this paper we will examine the use of jcn andthe original lesk in Japanese on the EDR datasetto see how well the pure definition based measurefares compared to one using hyponym links.
EDRhas hyponym links so we can make this comparison.The performance of jcn will depend on the coverageof the hyponym links.
For lesk meanwhile there isan issue that using only overlap of sense definitionsmay give poor results because the sense definitionsare usually succinct and the overlap of words maybe low.
For example, given the glosses for the wordspigeon and bird:2pigeon: a fat grey and white bird withshort legs.bird: a creature that is covered with feath-ers and has wings and two legs.If only content words are considered then thereis only one word (leg) which overlaps in the twoglosses, so the resultant lesk score is low (1) eventhough the word pigeon is intuitively similar to bird.The Elesk extension addressed this issue usingWordNet relations to extend the definitions overwhich the overlap is calculated for a given pair ofsenses.
We propose addressing the same issue us-ing corpus data to supplement the lesk overlap mea-sure.
We propose using distributional similarity (us-ing (Lin, 1998)) as an approximation of semanticdistance between the words in the two glosses, ratherthan requiring an exact match.
We refer to this mea-sure as DSlesk as defined:DSlesk(s1,s2) = 1|a ?
g1| ?a?g1maxb?g2dss(a,b) (2)where g1 is the gloss of word sense s1, g2 is the glossof s2, again s1 is the target word sense wsi in equa-tion 1 for which we are obtaining the predominanceranking score and s2 is whichever sense of the neigh-bour (n j) in equation 1 which maximises this seman-tic similarity score, as McCarthy et al did with thewnss in equation 1. a (b) is a word appearing in g1(g2).In the calculation of equation (2), we first extractthe most similar word b from g2 to each word (a) in2These two glosses are defined in OXFORD AdvancedLearner?s Dictionary.564dss(bird,creature) = 0.84, dss(bird, f eather) = 0.77,dss(bird,wing) = 0.55, dss(bird, leg) = 0.43,dss(leg,creature) = 0.56, dss(leg, f eather) = 0.66,dss(leg,wing) = 0.74, dss(leg, leg) = 1.00Figure 1: Examples of distributional similaritythe gloss of s1.
We then output the average of themaximum distributional similarity of all the wordsin g1 to any of the words in g2 as the similarity scorebetween s1 and s2.
We acknowledge that DSlesk isnot symmetrical since it depends on the number ofwords in the gloss of s1, but not s2.
Also our sum-mation is over these words in s1 and we are not look-ing for identity but maximum distributional similar-ity with any of the words in g2 so the summationwill not give the same result as if we did the sum-mation over the words in g2.
It is perfectly reason-able to have a semantic similarity measure which isnot symmetrical.
One may want a measure wherea more specific sense, such as the meat sense ofchicken is closer to the ?animal flesh used as food?sense of meat than vice versa.
We do not believethat this asymmetry is problematic for our applica-tion as all the senses of w which we are ranking areall treated equally with respect to the neighbour n,and the ranking measure is concerned with findingevidence for the meaning of w, which we do by fo-cusing on its definitions, and not the meaning of n.It would however be worthwhile investigating sym-metrical versions of the score in the future.Here is an example given the definitions of birdand pigeon above and the distributional similarityscores of all combinations of the two nouns as shownin Figure 1.
In this case, the similarity is estimatedas 1/2(0.84+1.00) = 0.92.4 ExperimentsTo investigate how well the McCarthy et al methodports to other language, we conduct empirical eval-uation of word sense disambiguation by using thetwo available sense-tagged datasets, EDR and theSENSEVAL-2 Japanese dictionary task.
In the ex-periments, we compare the three semantic similari-ties, jcn, lesk and DSlesk3, for use in the method to3Elesk can be used when several semantic relations such ashypnoymy and meronomy are available.
However, we cannotdirectly apply Elesk as it was used in (McCarthy et al, 2004) tofind the most likely sense in the set of word sensesdefined in each inventory following the approachof McCarthy et al (2004).
For the thesaurus con-struction we used <verb, case, noun> triplets ex-tracted from Japanese newspaper articles (9 years ofthe Mainichi Shinbun (1991-1999) and 10 years ofthe Nihon Keizai Shinbun (1991-2000)) and parsedby CaboCha (Kudo and Matsumoto, 2002).
This re-sulted in 53 million triplet instances for acquiringthe distributional thesaurus.
We adopt the similarityscore proposed by Lin (1998) as the distributionalsimilarity score and use 50 nearest neighbours inline with McCarthy et alFor the random baseline we select one word senseat random for each word token and average the pre-cision over 100 trials.
For contrast with a supervisedapproach we show the performance if we use hand-labelled training data for obtaining the predominantsense of the test words.
This method usually outper-forms an automatic approach, but crucially relies onthere being hand-labelled data which is expensive toproduce.
The method cannot be applied where thereis no hand-labelled training data, it will be unreli-able for low frequency data and a general datasetmay not be applicable when one moves to domainspecific text (Koeling et al, 2005).
Since we arenot using context for disambiguation, but just a firstsense heuristic, we also give the upper-bound whichis the first sense heuristic calculated from the testdata itself.4.1 EDRWe conduct empirical evaluation using 3,836 poly-semous nouns in the sense-tagged corpus providedwith EDR (183,502 instances) where the glosses aredefined in the EDR dictionary.
We evaluated on thisdataset using WSD precision and recall of this corpususing only our first-sense heuristic (no context).
Theresults are shown in Table 1.
The WSD performanceof all the automatic methods is much lower than thesupervised method, however, the main point of thispaper is to compare the McCarthy et al method forfinding a first sense in Japanese using jcn, lesk andour experiments because the meronomy relation is not definedin the EDR dictionary.
In the experiments reported here we fo-cus on the comparison of the three similarity measures jcn, leskand DSlesk for use in the method to determine the predomi-nant sense of each word.
We leave further exploration of otheradaptations of semantic similarity scores for future work.565Table 1: Results of EDRrecall precisionbaseline 0.402 0.402jcn 0.495 0.495lesk 0.474 0.488DSlesk 0.495 0.495upper-bound 0.745 0.745supervised 0.731 0.731Table 2: Precision on EDR at low frequenciesall freq ?
10 freq ?
5baseline 0.402 0.405 0.402jcn 0.495 0.445 0.431lesk 0.474 0.448 0.426DSlesk 0.495 0.453 0.433upper-bound 0.745 0.674 0.639supervised 0.731 0.519 0.367DSlesk.
Table 1 shows that DSlesk is comparable tojcn without the requirement for semantic relationssuch as hyponymy.Furthermore, we evaluate precision of eachmethod at low frequencies of words (?
10, ?
5),shown in Table 2.
Table 2 shows that all methods forfinding a predominant sense outperform the super-vised one for items with little data (?
5), indicatingthat these methods robustly work even for low fre-quency data where hand-tagged data is unreliable.Whilst the results are significantly different to thebaseline 4 we note that the difference to the randombaseline is less than for McCarthy et al who ob-tained 48% for Elesk on polysemous nouns in Sem-Cor and 46% for jcn against a random baseline of24%.
These differences are probably explained bydifferences in the lexical resources.
Both Elesk andjcn rely on semantic relations including hyponymywith Elesk also using the glosses.
jcn in both ap-proaches use the hyponym links.
WordNet 1.6 (usedby McCarthy et al) has 66025 synsets with 66910hyponym links between these 5.
For EDR there are166868 nodes (word sense groupings) and 537474For significance testing we used McNemar?s test ?
= 0.05.5These figures are taken fromhttp://www.lsi.upc.es/?batalla/wnstats.html#wn16Table 3: Results of SENSEVAL-2precision = recallfine coarsebaseline 0.282 0.399lesk 0.344 0.501DSlesk 0.386 0.593upper-bound 0.747 0.834supervised 0.742 0.842hyponym links.
So in EDR the ratio of these linksto the nodes is much lower.
This and other differ-ences between EDR and WordNet are likely to bethe reason for the difference in results.4.2 SENSEVAL-2We also evaluate the performance using the Japanesedictionary task in SENSEVAL-2 (Shirai, 2001).
Inthis experiment, we use 50 nouns (5,000 instances).For this task, since semantic relations such as hy-ponym links are not defined, use of jcn is not pos-sible.
Therefore, we just compare lesk and DSleskalong with our random baseline, the supervised ap-proach and the upper-bound as before.The results are evaluated in two ways; one is forfine-grained senses in the original task definition andthe other is coarse-grained version which is evalu-ated discarding the finer categorical information ofeach definition.
The results are shown in Table 3.
Aswith the EDR results, all unsupervised methods sig-nificantly outperform the baseline method, thoughthe supervised methods still outperform the unsu-pervised ones.
In this experiment, DSlesk is alsosignificantly better than lesk in both fine and coarse-grained evaluations.
It indicates that applying dis-tributional similarity score to calculating inter-glosssimilarities improves performance.5 ConclusionIn this paper, we examined different measures of se-mantic similarity for finding a first sense heuristicfor WSD automatically in Japanese.
We defined anew gloss-based similarity (DSlesk) and evaluatedthe performance on two Japanese WSD datasets, out-performing lesk and achieving a performance com-parable to the jcn method which relies on hyponymlinks which are not always available.566There are several issues for future directions ofautomatic detection of a first sense heuristic.
In thispaper, we proposed an adaptation of the lesk mea-sure of gloss-based similarity, by using the aver-age similarity between nouns in the two glosses un-der comparison in a bag-of-words approach withoutrecourse to other information.
However, it wouldbe worthwhile exploring other information in theglosses, such as words of other PoS and predicateargument relations.
We also hope to investigate ap-plying alignment techniques introduced for entail-ment recognition (Hickl and Bensley, 2007).Another important issue in WSD is to group fine-grained word senses into clusters, making the tasksuitable for NLP applications (Ide and Wilks, 2006).We believe that our gloss-based similarity DSleskmight be very suitable for this task and we plan toinvestigate the possibility.There are other approaches we would like to ex-plore in future.
Mihalcea (2005) uses dictionary def-initions alongside graphical algorithms for unsuper-vised WSD.
Whilst the results are not directly com-parable to ours because we have not included con-textual evidence in our models, it would be worth-while exploring if unsupervised graphical modelsusing only the definitions we have in our lexical re-sources can perform WSD on a document and givemore reliable first sense heuristics.AcknowledgementsThis work was supported by the UK EPSRC projectEP/C537262 ?Ranking Word Senses for Disam-biguation: Models and Applications?, and a UKRoyal Society Dorothy Hodgkin Fellowship to thesecond author.
We would like to thank John Carrollfor several useful discussions on this work.ReferencesSatanjeev Banerjee and Ted Pedersen.
2002.
An adaptedLesk algorithm for word sense disambiguation usingWordNet.
In Proceedings of the Third InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLing-02), Mexico City.J.R.L.
Barnard, editor.
1986.
Macquaire Thesaurus.Macquaire Library, Sydney.Samuel Brody, Roberto Navigli, and Mirella Lapata.2006.
Ensemble methods for unsupervised wsd.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the Association for Computational Linguistics, Syd-ney, Australia, July.
Association for ComputationalLinguistics.Marine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL 2007), pages 61?72, Prague, Czech Republic,June.
Association for Computational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2005.
Word sensedisambiguation with distribution estimation.
In Pro-ceedings of the 19th International Joint Conference onArtificial Intelligence (IJCAI 2005), Edinburgh, Scot-land.Yee Seng Chan and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense disam-biguation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguis-tics, Prague, Czech Republic, June.
Association forComputational Linguistics.Paul Clough and Mark Stevenson.
2004.
Evaluating thecontribution of EuroWordNet and word sense disam-biguation to cross-language retrieval.
In Second In-ternational Global WordNet Conference (GWC-2004),pages 97?105.Michel Galley and Kathleen McKeown.
2003.
Improv-ing word sense disambiguation in lexical chaining.
InIJCAI-03, Proceedings of the Eighteenth InternationalJoint Conference on Artificial Intelligence, Acapulco,Mexico, August 9-15, 2003, pages 1486?1488.
MorganKaufmann.Andrew Hickl and Jeremy Bensley.
2007.
A discoursecommitment-based framework for recognizing textualentailment.
In Proceedings of the ACL-PASCAL Work-shop on Textual Entailment and Paraphrasing, pages171?176.Nancy Ide and Yorick Wilks.
2006.
Making sense aboutsense.
In Eneko Agirre and Phil Edmonds, editors,Word Sense Disambiguation, Algorithms and Applica-tions, pages 47?73.
Springer.Jay Jiang and David Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In In-ternational Conference on Research in ComputationalLinguistics, Taiwan.567Rob Koeling, Diana McCarthy, and John Carroll.
2005.Domain-specific sense distributions and predominantsense acquisition.
In Proceedings of the joint confer-ence on Human Language Technology and Empiricalmethods in Natural Language Processing, pages 419?426, Vancouver, B.C., Canada.Taku Kudo and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of the 6th Conference on Natural LanguageLearning 2002 (CoNLL), pages 63?69.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: how to tell a pine conefrom and ice cream cone.
In Proceedings of the ACMSIGDOC Conference, pages 24?26, Toronto, Canada.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL 98,Montreal, Canada.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses in un-tagged text.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,pages 280?287, Barcelona, Spain.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings of theAmerican Association for Artificial Intelligence (AAAI2006), Boston, MA, July.Rada Mihalcea.
2005.
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedings ofthe joint conference on Human Language Technologyand Empirical methods in Natural Language Process-ing, Vancouver, B.C., Canada.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T Bunker.
1993.
A semantic concordance.
InProceedings of the ARPA Workshop on Human Lan-guage Technology, pages 303?.308.
Morgan Kaufman.Saif Mohammad and Graeme Hirst.
2006.
Determiningword sense dominance using a thesaurus.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL-2006), pages 121?128, Trento, Italy, April.Roberto Navigli, C. Litkowski, Kenneth, and Orin Har-graves.
2007.
SemEval-2007 task 7: Coarse-grained English all-words task.
In Proceedings ofACL/SIGLEX SemEval-2007, pages 30?35, Prague,Czech Republic.NICT.
2002.
EDR electronic dic-tionary version 2.0, technical guide.http://www2.nict.go.jp/kk/e416/EDR/.Minoru Nishio, Etsutaro Iwabuchi, and ShizuoMitzutani.1994.
Iwanami kokugo jiten dai go han.Siddharth Patwardhan and Ted Pedersen.
2003.The CPAN WordNet::Similarity Package.http://search.cpan.org/author/SID/WordNet-Similarity-0.03/.Kiyoaki Shirai.
2001.
SENSEVAL-2 Japanese Dictio-nary Task.
In Proceedings of the SENSEVAL-2 work-shop, pages 33?36.Benjamin Snyder and Martha Palmer.
2004.
The Englishall-words task.
In Proceedings of the ACL SENSEVAL-3 workshop, pages 41?43, Barcelona, Spain.Piek Vossen, German Rigau, Inaki Alegria, Eneko Agirre,David Farwell, and Manuel Fuentes.
2006.
Mean-ingful results for information retrieval in the meaningproject.
In Proceedings of the 3rd Global WordNetConference.
http://nlpweb.kaist.ac.kr/gwc/.568
