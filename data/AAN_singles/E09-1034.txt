Proceedings of the 12th Conference of the European Chapter of the ACL, pages 291?299,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsParsing Mildly Non-projective Dependency Structures?Carlos Go?mez-Rodr?
?guezDepartamento de Computacio?nUniversidade da Corun?a, Spaincgomezr@udc.esDavid Weir and John CarrollDepartment of InformaticsUniversity of Sussex, United Kingdom{davidw,johnca}@sussex.ac.ukAbstractWe present parsing algorithms for vari-ous mildly non-projective dependency for-malisms.
In particular, algorithms are pre-sented for: all well-nested structures ofgap degree at most 1, with the same com-plexity as the best existing parsers for con-stituency formalisms of equivalent genera-tive power; all well-nested structures withgap degree bounded by any constant k;and a new class of structures with gap de-gree up to k that includes some ill-nestedstructures.
The third case includes all thegap degree k structures in a number of de-pendency treebanks.1 IntroductionDependency parsers analyse a sentence in termsof a set of directed links (dependencies) express-ing the head-modifier and head-complement rela-tionships which form the basis of predicate argu-ment structure.
We take dependency structures tobe directed trees, where each node corresponds toa word and the root of the tree marks the syn-tactic head of the sentence.
For reasons of effi-ciency, many practical implementations of depen-dency parsing are restricted to projective struc-tures, in which the subtree rooted at each wordcovers a contiguous substring of the sentence.However, while free word order languages suchas Czech do not satisfy this constraint, parsingwithout the projectivity constraint is computation-ally complex.
Although it is possible to parsenon-projective structures in quadratic time under amodel in which each dependency decision is inde-pendent of all the others (McDonald et al, 2005),?Partially supported by MEC and FEDER (HUM2007-66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR,INCITE08E1R104022ES, INCITE08ENA305025ES, IN-CITE08PXIB302179PR, Rede Galega de Proc.
da Linguaxee RI, Bolsas para Estad?
?as INCITE ?
FSE cofinanced).the problem is intractable in the absence of this as-sumption (McDonald and Satta, 2007).Nivre and Nilsson (2005) observe that mostnon-projective dependency structures appearingin practice are ?close?
to being projective, sincethey contain only a small proportion of non-projective arcs.
This has led to the study ofclasses of dependency structures that lie be-tween projective and unrestricted non-projectivestructures (Kuhlmann and Nivre, 2006; Havelka,2007).
Kuhlmann (2007) investigates several suchclasses, based on well-nestedness and gap degreeconstraints (Bodirsky et al, 2005), relating themto lexicalised constituency grammar formalisms.Specifically, he shows that: linear context-freerewriting systems (LCFRS) with fan-out k (Vijay-Shanker et al, 1987; Satta, 1992) induce the setof dependency structures with gap degree at mostk ?
1; coupled context-free grammars in whichthe maximal rank of a nonterminal is k (Hotz andPitsch, 1996) induce the set of well-nested depen-dency structures with gap degree at most k ?
1;and LTAGs (Joshi and Schabes, 1997) induce theset of well-nested dependency structures with gapdegree at most 1.These results establish that there must bepolynomial-time dependency parsing algorithmsfor well-nested structures with bounded gap de-gree, since such parsers exist for their correspond-ing lexicalised constituency-based formalisms.However, since most of the non-projective struc-tures in treebanks are well-nested and have a smallgap degree (Kuhlmann and Nivre, 2006), devel-oping efficient dependency parsing strategies forthese sets of structures has considerable practicalinterest, since we would be able to parse directlywith dependencies in a data-driven manner, ratherthan indirectly by constructing intermediate con-stituency grammars and extracting dependenciesfrom constituency parses.We address this problem with the followingcontributions: (1) we define a parsing algorithm291for well-nested dependency structures of gap de-gree 1, and prove its correctness.
The parser runsin time O(n7), the same complexity as the bestexisting algorithms for LTAG (Eisner and Satta,2000), and can be optimised to O(n6) in the non-lexicalised case; (2) we generalise the previous al-gorithm to any well-nested dependency structurewith gap degree at most k in time O(n5+2k); (3)we generalise the previous parsers to be able toanalyse not only well-nested structures, but alsoill-nested structures with gap degree at most k sat-isfying certain constraints1, in time O(n4+3k); and(4) we characterise the set of structures covered bythis parser, which we call mildly ill-nested struc-tures, and show that it includes all the trees presentin a number of dependency treebanks.2 PreliminariesA dependency graph for a string w1 .
.
.
wn is agraph G = (V,E), where V = {w1, .
.
.
, wn}and E ?
V ?
V .
We write the edge (wi, wj)as wi ?
wj , meaning that the word wi is a syn-tactic dependent (or a child) of wj or, conversely,that wj is the governor (parent) of wi.
We writewi ??
wj to denote that there exists a (possi-bly empty) path from wi to wj .
The projectionof a node wi, denoted bwic, is the set of reflexive-transitive dependents of wi, that is: bwic = {wj ?V | wj ??
wi}.
An interval (with endpoints i andj) is a set of the form [i, j] = {wk | i ?
k ?
j}.A dependency graph is said to be a tree if it is:(1) acyclic: wj ?
bwic implies wi ?
wj 6?
E; and(2) each node has exactly one parent, except forone node which we call the root or head.
A graphverifying these conditions and having a vertex setV ?
{w1, .
.
.
, wn} is a partial dependency tree.Given a dependency tree T = (V,E) and a nodeu ?
V , the subtree induced by the node u is thegraph Tu = (buc, Eu) where Eu = {wi ?
wj ?E | wj ?
buc}.2.1 Properties of dependency treesWe now define the concepts of gap degree andwell-nestedness (Kuhlmann and Nivre, 2006).
LetT be a (possibly partial) dependency tree forw1 .
.
.
wn: We say that T is projective if bwic isan interval for every word wi.
Thus every nodein the dependency structure must dominate a con-tiguous substring in the sentence.
The gap degree1Parsing unrestricted ill-nested structures, even when thegap degree is bounded, is NP-complete: these structures areequivalent to LCFRS for which the recognition problem isNP-complete (Satta, 1992).of a particular node wk in T is the minimum g ?
Nsuch that bwkc can be written as the union of g+1intervals; that is, the number of discontinuities inbwkc.
The gap degree of the dependency tree T isthe maximum among the gap degrees of its nodes.Note that T has gap degree 0 if and only if T isprojective.
The subtrees induced by nodes wp andwq are interleaved if bwpc ?
bwqc = ?
and thereare nodes wi, wj ?
bwpc and wk, wl ?
bwqc suchthat i < k < j < l. A dependency tree T iswell-nested if it does not contain two interleavedsubtrees.
A tree that is not well-nested is said tobe ill-nested.
Note that projective trees are alwayswell-nested, but well-nested trees are not alwaysprojective.2.2 Dependency parsing schemataThe framework of parsing schemata (Sikkel,1997) provides a uniform way to describe, anal-yse and compare parsing algorithms.
Parsingschemata were initially defined for constituency-based grammatical formalisms, but Go?mez-Rodr?
?guez et al (2008a) define a variant of theframework for dependency-based parsers.
Weuse these dependency parsing schemata to de-fine parsers and prove their correctness.
Due tospace constraints, we only provide brief outlinesof the main concepts behind dependency parsingschemata.The parsing schema approach considers pars-ing as deduction, generating intermediate resultscalled items.
An initial set of items is obtainedfrom the input sentence, and the parsing processinvolves deduction steps which produce new itemsfrom existing ones.
Each item contains informa-tion about the sentence?s structure, and a success-ful parsing process produces at least one final itemproviding a full dependency analysis for the sen-tence or guaranteeing its existence.
In a depen-dency parsing schema, items are defined as sets ofpartial dependency trees2.
To define a parser bymeans of a schema, we must define an item setand provide a set of deduction steps that operateon it.
Given an item set I, the set of final itemsfor strings of length n is the set of items in I thatcontain a full dependency tree for some arbitrarystring of length n. A final item containing a de-pendency tree for a particular string w1 .
.
.
wn issaid to be a correct final item for that string.
These2The formalism allows items to contain forests, and thedependency structures inside items are defined in a notationwith terminal and preterminal nodes, but these are not neededhere.292concepts can be used to prove the correctness ofa parser: for each input string, a parsing schema?sdeduction steps allow us to infer a set of items,called valid items for that string.
A schema is saidto be sound if all valid final items it produces forany arbitrary string are correct for that string.
Aschema is said to be complete if all correct finalitems are valid.
A correct parsing schema is onewhich is both sound and complete.In constituency-based parsing schemata, deduc-tion steps usually have grammar rules as side con-ditions.
In the case of dependency parsers it isalso possible to use grammars (Eisner and Satta,1999), but many algorithms use a data-driven ap-proach instead, making individual decisions aboutwhich dependencies to create by using probabilis-tic models (Eisner, 1996) or classifiers (Yamadaand Matsumoto, 2003).
To represent these algo-rithms as deduction systems, we use the notionof D-rules (Covington, 1990).
D-rules take theform a ?
b, which says that word b can have aas a dependent.
Deduction steps in non-grammar-based parsers can be tied to the D-rules associatedwith the links they create.
In this way, we ob-tain a representation of the underlying logic of theparser while abstracting away from control struc-tures (the particular model used to create the de-cisions associated with D-rules).
Furthermore, thechoice points in the parsing process and the infor-mation we can use to make decisions are made ex-plicit in the steps linked to D-rules.3 The WG1 parser3.1 Parsing schema for WG1We define WG1, a parser for well-nested depen-dency structures of gap degree ?
1, as follows:The item set is IWG1 = I1 ?
I2, withI1 = {[i, j, h, , ] | i, j, h ?
N, 1 ?
h ?
n,1 ?
i ?
j ?
n, h 6= j, h 6= i?
1},where each item of the form [i, j, h, , ] repre-sents the set of all well-nested partial dependencytrees3 with gap degree at most 1, rooted at wh, andsuch that bwhc = {wh} ?
[i, j], andI2 = {[i, j, h, l, r] | i, j, h, l, r ?
N, 1 ?
h ?
n,1 ?
i < l ?
r < j ?
n, h 6= j, h 6= i?
1,h 6= l ?
1, h 6= r}3In this and subsequent schemata, we use D-rules to ex-press parsing decisions, so partial dependency trees are as-sumed to be taken from the set of trees licensed by a set ofD-rules.where each item of the form [i, j, h, l, r] representsthe set of all well-nested partial dependency treesrooted at wh such that bwhc = {wh} ?
([i, j] \[l, r]), and all the nodes (except possibly h) havegap degree at most 1.
We call items of this formgapped items, and the interval [l, r] the gap ofthe item.
Note that the constraints h 6= j, h 6=i + 1, h 6= l ?
1, h 6= r are added to items toavoid redundancy in the item set.
Since the resultof the expression {wh} ?
([i, j] \ [l, r]) for a givenhead can be the same for different sets of values ofi, j, l, r, we restrict these values so that we cannotget two different items representing the same de-pendency structures.
Items ?
violating these con-straints always have an alternative representationthat does not violate them, that we can expresswith a normalising function nm(?)
as follows:nm([i, j, j, l, r]) = [i, j ?
1, j, l, r] (if r ?
j ?
1 or r = ),or [i, l ?
1, j, , ] (if r = j ?
1).nm([i, j, l ?
1, l, r]) = [i, j, l ?
1, l ?
1, r](if l > i + 1),or [r + 1, j, l ?
1, , ] (if l = i + 1).nm([i, j, i ?
1, l, r]) = [i ?
1, j, i ?
1, l, r].nm([i, j, r, l, r]) = [i, j, r, l, r ?
1] (if l < r),or [i, j, r, , ] (if l = r).nm([i, j, h, l, r]) = [i, j, h, l, r] for all other items.When defining the deduction steps for this andother parsers, we assume that they always producenormalised items.
For clarity, we do not explicitlywrite this in the deduction steps, writing ?
insteadof nm(?)
as antecedents and consequents of steps.The set of initial items is defined as the setH = {[h, h, h, , ] | h ?
N, 1 ?
h ?
n},where each item [h, h, h, , ] represents the setcontaining the trivial partial dependency tree con-sisting of a single node wh and no links.
Thissame set of hypotheses can be used for all theparsers, so we do not make it explicit for subse-quent schemata.
Note that initial items are sepa-rate from the item set IWG1 and not subject to itsconstraints, so they do not require normalisation.The set of final items for strings of length n inWG1 is defined as the setF = {[1, n, h, , ] | h ?
N, 1 ?
h ?
n},which is the set of items in IWG1 containing de-pendency trees for the complete input string (fromposition 1 to n), with their head at any word wh.The deduction steps of the parser can be seen inFigure 1A.The WG1 parser proceeds bottom-up, by build-ing dependency subtrees and joining them to formlarger subtrees, until it finds a complete depen-dency tree for the input sentence.
The logic of293A.
WG1 parser:Link Ungapped:[h1, h1, h1, , ][i2, j2, h2, , ][i2, j2, h1, , ] wh2 ?
wh1such that wh2 ?
[i2, j2] ?
wh1 /?
[i2, j2],Link Gapped:[h1, h1, h1, , ][i2, j2, h2, l2, r2][i2, j2, h1, l2, r2] wh2 ?
wh1such that wh2 ?
[i2, j2] \ [l2, r2] ?
wh1 /?
[i2, j2] \ [l2, r2],Combine Ungapped:[i, j, h, , ] [j + 1, k, h, , ][i, k, h, , ]Combine Opening Gap:[i, j, h, , ] [k, l, h, , ][i, l, h, j + 1, k ?
1]such that j < k ?
1,Combine Keeping Gap Left:[i, j, h, l, r] [j + 1, k, h, , ][i, k, h, l, r]Combine Keeping Gap Right:[i, j, h, , ] [j + 1, k, h, l, r][i, k, h, l, r]Combine Closing Gap:[i, j, h, l, r] [l, r, h, , ][i, j, h, , ]Combine Shrinking Gap Left:[i, j, h, l, r] [l, k, h, , ][i, j, h, k + 1, r]Combine Shrinking Gap Right:[i, j, h, l, r] [k, r, h, , ][i, j, h, l, k ?
1]Combine Shrinking Gap Centre:[i, j, h, l, r] [l, r, h, l2, r2][i, j, h, l2, r2]B. WGK parser:Link:[h1, h1, h1, []][i2, j2, h2, [(l1, r1), .
.
.
, (lg, rg)]][i2, j2, h1, [(l1, r1), .
.
.
, (lg, rg)]]wh2 ?
wh1such that wh2 ?
[i2, j2] \?gp=1[lp, rp]?wh1 /?
[i2, j2] \?gp=1[lp, rp].Combine Shrinking Gap Right:[i, j, h, [(l1, r1), .
.
.
, (lq?1, rq?1), (lq, r?
), (ls, rs), .
.
.
, (lg, rg)]][rq + 1, r?, h, [(lq+1, rq+1), .
.
.
, (ls?1, rs?1)]][i, j, h, [(l1, r1), .
.
.
, (lg, rg)]]such that g ?
kCombine Opening Gap:[i, lq ?
1, h, [(l1, r1), .
.
.
, (lq?1, rq?1)]][rq + 1, m, h, [(lq+1, rq+1), .
.
.
, (lg, rg)]][i, m, h, [(l1, r1), .
.
.
, (lg, rg)]]such that g ?
k and lq ?
rq ,Combine Shrinking Gap Left:[i, j, h, [(l1, r1), .
.
.
, (lq, rq), (l?, rs), (ls+1, rs+1), .
.
.
, (lg, rg)]][l?, ls ?
1, h, [(lq+1, rq+1), .
.
.
, (ls?1, rs?1)]][i, j, h, [(l1, r1), .
.
.
, (lg, rg)]]such that g ?
kCombine Keeping Gaps:[i, j, h, [(l1, r1), .
.
.
, (lq, rq)]][j + 1, m, h, [(lq+1, rq+1), .
.
.
, (lg, rg)]][i, m, h, [(l1, r1), .
.
.
, (lg, rg)]]such that g ?
k,Combine Shrinking Gap Centre:[i, j, h, [(l1, r1), .
.
.
, (lq, rq), (l?, r?
), (ls, rs), .
.
.
, (lg, rg)]][l?, r?, h, [(lq+1, rq+1), .
.
.
, (ls?1, rs?1)]][i, j, h, [(l1, r1), .
.
.
, (lg, rg)]]such that g ?
kC.
Additional steps to turn WG1 into MG1:Combine Interleaving:[i, j, h, l, r] [l, k, h, r + 1, j][i, k, h, , ]Combine Interleaving Gap C:[i, j, h, l, r] [l, k, h, m, j][i, k, h, m, r]such that m < r + 1,Combine Interleaving Gap L:[i, j, h, l, r][l, k, h, r + 1, u][i, k, h, j + 1, u]such that u > j,Combine Interleaving Gap R:[i, j, h, l, r][k, m, h, r + 1, j][i, m, h, l, k ?
1]such that k > l.D.
General form of the MGk Combine step:[ia1 , iap+1 ?
1, h, [(ia1+1, ia2 ?
1), .
.
.
, (iap?1+1, iap ?
1)]][ib1 , ibq+1 ?
1, h, [(ib1+1, ib2 ?
1), .
.
.
, (ibq?1+1, ibq ?
1)]][imin(a1,b1), imax(ap+1,bq+1) ?
1, h, [(ig1 , ig1+1 ?
1), .
.
.
, (igr , igr+1 ?
1)]]for each string of length n with a?s located at positions a1 .
.
.
ap(1 ?
a1 < .
.
.
< ap ?
n), b?s at positions b1 .
.
.
bq(1 ?
b1 <.
.
.
< bq ?
n), and g?s at positions g1 .
.
.
gr(2 ?
g1 < .
.
.
< gr ?
n ?
1), such that 1 ?
p ?
k, 1 ?
q ?
k, 0 ?
r ?
k ?
1,p + q + r = n, and the string does not contain more than one consecutive appearance of the same symbol.Figure 1: Deduction steps for the parsers defined in the paper.the parser can be understood by considering howit infers the item corresponding to the subtree in-duced by a particular node, given the items for thesubtrees induced by the direct dependents of thatnode.
Suppose that, in a complete dependencyanalysis for a sentence w1 .
.
.
wn, the word whhas wd1 .
.
.
wdp as direct dependents (i.e.
we havedependency links wd1 ?
wh, .
.
.
, wdp ?
wh).Then, the item corresponding to the subtree in-duced by wh is obtained from the ones correspond-ing to the subtrees induced by wd1 .
.
.
wdp by: (1)applying the Link Ungapped or Link Gapped stepto each of the items corresponding to the subtreesinduced by the direct dependents, and to the hy-pothesis [h, h, h, , ].
This allows us to infer pitems representing the result of linking each of thedependent subtrees to the new head wh; (2) ap-plying the various Combine steps to join all of the294items obtained in the previous step into a singleitem.
The Combine steps perform a union oper-ation between subtrees.
Therefore, the result is adependency tree containing all the dependent sub-trees, and with all of them linked to h: this isthe subtree induced by wh.
This process is ap-plied repeatedly to build larger subtrees, until, ifthe parsing process is successful, a final item isfound containing a dependency tree for the com-plete sentence.3.2 Proving correctnessThe parsing schemata formalism can be used toprove the correctness of a parsing schema.
Toprove that WG1 is correct, we need to proveits soundness and completeness.4 Soundness isproven by checking that valid items always con-tain well-nested trees.
Completeness is proven byinduction, taking initial items as the base case andshowing that an item containing a correct subtreefor a string can always be obtained from itemscorresponding to smaller subtrees.
In order toprove this induction step, we use the concept oforder annotations (Kuhlmann, 2007; Kuhlmannand Mo?hl, 2007), which are strings that lexicalisethe precedence relation between the nodes of a de-pendency tree.
Given a correct subtree, we dividethe proof into cases according to the order annota-tion of its head and we find that, for every possibleform of this order annotation, we can find a se-quence of Combine steps to infer the relevant itemfrom smaller correct items.3.3 Computational complexityThe time complexity of WG1 is O(n7), as thestep Combine Shrinking Gap Centre works with 7free string positions.
This complexity with respectto the length of the input is as expected for thisset of structures, since Kuhlmann (2007) showsthat they are equivalent to LTAG, and the best ex-isting parsers for this formalism also perform inO(n7) (Eisner and Satta, 2000).
Note that theCombine step which is the bottleneck only uses the7 indexes, and not any other entities like D-rules,so its O(n7) complexity does not have any addi-tional factors due to grammar size or other vari-ables.
The space complexity of WG1 is O(n5)for recognition, due to the 5 indexes in items, andO(n7) for full parsing.4Due to space constraints, correctness proofs for theparsers are not given here.
Full proofs are provided in theextended version of this paper, see (Go?mez-Rodr?
?guez et al,2008b).It is possible to build a variant of this parserwith time complexity O(n6), as with parsers forunlexicalised TAG, if we work with unlexicalisedD-rules specifying the possibility of dependenciesbetween pairs of categories instead of pairs ofwords.
In order to do this, we expand the item setwith unlexicalised items of the form [i, j, C, l, r],where C is a category, apart from the existingitems [i, j, h, l, r].
Steps in the parser are dupli-cated, to work both with lexicalised and unlex-icalised items, except for the Link steps, whichalways work with a lexicalised item and an un-lexicalised hypothesis to produce an unlexicaliseditem, and the Combine Shrinking Gap steps, whichcan work only with unlexicalised items.
Steps areadded to obtain lexicalised items from their unlex-icalised equivalents by binding the head to partic-ular string positions.
Finally, we need certain vari-ants of the Combine Shrinking Gap steps that take2 unlexicalised antecedents and produce a lexi-calised consequent; an example is the following:Combine Shrinking Gap Centre L:[i, j, C, l, r][l + 1, r, C, l2, r2][i, j, l, l2, r2]such that cat(wl)=CAlthough this version of the algorithm reducestime complexity with respect to the length of theinput to O(n6), it also adds a factor related to thenumber of categories, as well as constant factorsdue to using more kinds of items and steps thanthe original WG1 algorithm.
This, together withthe advantages of lexicalised dependency parsing,may mean that the original WG1 algorithm is morepractical than this version.4 The WGk parserThe WG1 parsing schema can be generalised toobtain a parser for all well-nested dependencystructures with gap degree bounded by a constantk(k ?
1), which we call WGk parser.
In order todo this, we extend the item set so that it can containitems with up to k gaps, and modify the deductionsteps to work with these multi-gapped items.4.1 Parsing schema for WGkThe item set IWGk is the set of all[i, j, h, [(l1, r1), .
.
.
, (lg, rg)]] where i, j, h, g ?
N, 0 ?
g ?
k, 1 ?
h ?
n, 1 ?
i ?
j ?
n , h 6= j,h 6= i?
1; and for each p ?
{1, 2, .
.
.
, g}:lp, rp ?
N, i < lp ?
rp < j, rp < lp+1 ?
1,h 6= lp ?
1, h 6= rp.An item [i, j, h, [(l1, r1), .
.
.
, (lg, rg)]] repre-sents the set of all well-nested partial dependency295trees rooted at wh such that bwhc = {wh}?
([i, j]\?gp=1[lp, rp]), where each interval [lp, rp] is calleda gap.
The constraints h 6= j, h 6= i + 1, h 6=lp ?
1, h 6= rp are added to avoid redundancy, andnormalisation is defined as in WG1.
The set of fi-nal items is defined as the set F = {[1, n, h, []] |h ?
N, 1 ?
h ?
n}.
Note that this set is the sameas in WG1, as these are the items that we denoted[1, n, h, , ] in the previous parser.The deduction steps can be seen in Figure 1B.As expected, the WG1 parser corresponds to WGkwhen we make k = 1.
WGk works in the sameway as WG1, except for the fact that Combinesteps can create items with more than one gap5.The correctness proof is also analogous to that ofWG1, but we must take into account that the set ofpossible order annotations is larger when k > 1,so more cases arise in the completeness proof.4.2 Computational complexityThe WGk parser runs in time O(n5+2k): as inthe case of WG1, the deduction step with mostfree variables is Combine Shrinking Gap Cen-tre, and in this case it has 5 + 2k free indexes.Again, this complexity result is in line with whatcould be expected from previous research in con-stituency parsing: Kuhlmann (2007) shows thatthe set of well-nested dependency structures withgap degree at most k is closely related to cou-pled context-free grammars in which the maxi-mal rank of a nonterminal is k + 1; and the con-stituency parser defined by Hotz and Pitsch (1996)for these grammars also adds an n2 factor for eachunit increment of k. Note that a small value ofk should be enough to cover the vast majority ofthe non-projective sentences found in natural lan-guage treebanks.
For example, the Prague Depen-dency Treebank contains no structures with gapdegree greater than 4.
Therefore, a WG4 parserwould be able to analyse all the well-nested struc-tures in this treebank, which represent 99.89% ofthe total.
Increasing k beyond 4 would not pro-duce further improvements in coverage.5 Parsing ill-nested structuresThe WGk parser analyses dependency structureswith bounded gap degree as long as they arewell-nested.
This covers the vast majority of5In all the parsers in this paper, Combine steps may beapplied in different orders to produce the same result, causingspurious ambiguity.
In WG1 and WGk, this can be avoidedwhen implementing the schemata, by adding flags to itemsso as to impose a particular order.the structures that occur in natural-language tree-banks (Kuhlmann and Nivre, 2006), but there isstill a significant minority of sentences that con-tain ill-nested structures.
Unfortunately, the gen-eral problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded:this set of structures is closely related to LCFRSwith bounded fan-out and unbounded productionlength, and parsing in this formalism has beenproven to be NP-complete (Satta, 1992).
Thereason for this high complexity is the problemof unrestricted crossing configurations, appearingwhen dependency subtrees are allowed to inter-leave in every possible way.
However, just asit has been noted that most non-projective struc-tures appearing in practice are only ?slightly?
non-projective (Nivre and Nilsson, 2005), we charac-terise a sense in which the structures appearing intreebanks can be viewed as being only ?slightly?ill-nested.
In this section, we generalise the algo-rithms WG1 and WGk to parse a proper supersetof the set of well-nested structures in polynomialtime; and give a characterisation of this new setof structures, which includes all the structures inseveral dependency treebanks.5.1 The MG1 and MGk parsersThe WGk parser presented previously is based ona bottom-up process, where Link steps are used tolink completed subtrees to a head, and Combinesteps are used to join subtrees governed by a com-mon head to obtain a larger structure.
As WGk is aparser for well-nested structures of gap degree upto k, its Combine steps correspond to all the waysin which we can join two sets of sibling subtreesmeeting these constraints, and having a commonhead, into another.
Thus, this parser does not useCombine steps that produce interleaved subtrees,since these would generate items corresponding toill-nested structures.We obtain a polynomial parser for a wider set ofstructures of gap degree at most k, including someill-nested ones, by having Combine steps repre-senting every way in which two sets of sibling sub-trees of gap degree at most k with a common headcan be joined into another, including those produc-ing interleaved subtrees, like the steps for gap de-gree 1 shown in Figure 1C.
Note that this does notmean that we can build every possible ill-nestedstructure: some structures with complex crossedconfigurations have gap degree k, but cannot bebuilt by combining two structures of that gap de-gree.
More specifically, our algorithm will be able296to parse a dependency structure (well-nested ornot) if there exists a binarisation of that structurethat has gap degree at most k. The parser im-plicitly works by finding such a binarisation, sinceCombine steps are always applied to two items andno intermediate item generated by them can ex-ceed gap degree k (not counting the position ofthe head in the projection).More formally, let T be a dependency structurefor the string w1 .
.
.
wn.
A binarisation of T isa dependency tree T ?
over a set of nodes, each ofwhich may be unlabelled or labelled with a wordin {w1 .
.
.
wn}, such that the following conditionshold: (1) each node has at most two children, and(2) wi ?
wj in T if and only if wi ??
wj inT ?.
A dependency structure is mildly ill-nestedfor gap degree k if it has at least one binarisationof gap degree ?
k. Otherwise, we say that it isstrongly ill-nested for gap degree k. It is easyto prove that the set of mildly ill-nested structuresfor gap degree k includes all well-nested structureswith gap degree up to k.We define MG1, a parser for mildly ill-nestedstructures for gap degree 1, as follows: (1) theitem set is the same as that of WG1, except thatitems can now contain any mildly ill-nested struc-tures for gap degree 1, instead of being restrictedto well-nested structures; and (2) deduction stepsare the same as in WG1, plus the additional stepsshown in Figure 1C.
These extra Combine stepsallow the parser to combine interleaved subtreeswith simple crossing configurations.
The MG1parser still runs in O(n7), as these new steps donot use more than 7 string positions.The proof of correctness for this parser is sim-ilar to that of WG1.
Again, we use the conceptof order annotations.
The set of mildly ill-nestedstructures for gap degree k can be defined as thosethat only contain annotations meeting certain con-straints.
The soundness proof involves showingthat Combine steps always generate items contain-ing trees with such annotations.
Completeness isproven by induction, by showing that if a subtreeis mildly ill-nested for gap degree k, an item forit can be obtained from items for smaller subtreesby applying Combine and Link steps.
In the caseswhere Combine steps have to be applied, the orderin which they may be used to produce a subtreecan be obtained from its head?s order annotation.To generalise this algorithm to mildly ill-nestedstructures for gap degree k, we need to add a Com-bine step for every possible way of joining twostructures of gap degree at most k into another.This can be done systematically by considering aset of strings over an alphabet of three symbols:a and b to represent intervals of words in the pro-jection of each of the structures, and g to repre-sent intervals that are not in the projection of ei-ther structure, and will correspond to gaps in thejoined structure.
The legal combinations of struc-tures for gap degree k will correspond to stringswhere symbols a and b each appear at most k + 1times, g appears at most k times and is not the firstor last symbol, and there is no more than one con-secutive appearance of any symbol.
Given a stringof this form, the corresponding Combine step isgiven by the expression in Figure 1D.
As a particu-lar example, the Combine Interleaving Gap C stepin Figure 1C is obtained from the string abgab.Thus, we define the parsing schema for MGk, aparser for mildly ill-nested structures for gap de-gree k, as the schema where (1) the item set islike that of WGk, except that items can now con-tain any mildly ill-nested structures for gap degreek, instead of being restricted to well-nested struc-tures; and (2) the set of deduction steps consists ofa Link step as the one in WGk, plus a set of Com-bine steps obtained as expressed in Figure 1D.As the string used to generate a Combine stepcan have length at most 3k + 2, and the result-ing step contains an index for each symbol of thestring plus two extra indexes, the MGk parser hascomplexity O(n3k+4).
Note that the item and de-duction step sets of an MGk parser are always su-persets of those of WGk.
In particular, the stepsfor WGk are those obtained from strings that donot contain abab or baba as a scattered substring.5.2 Mildly ill-nested dependency structuresThe MGk algorithm defined in the previous sec-tion can parse any mildly ill-nested structure for agiven gap degree k in polynomial time.
We havecharacterised the set of mildly ill-nested structuresfor gap degree k as those having a binarisation ofgap degree ?
k. Since a binarisation of a depen-dency structure cannot have lower gap degree thanthe original structure, this set only contains struc-tures with gap degree at most k. Furthermore, bythe relation between MGk and WGk, we know thatit contains all the well-nested structures with gapdegree up to k.Figure 2 shows an example of a structure thathas gap degree 1, but is strongly ill-nested for gapdegree 1.
This is one of the smallest possible suchstructures: by generating all the possible trees upto 10 nodes (without counting a dummy root node297LanguageStructuresTotalNonprojectiveTotalBy gap degree By nestednessGapdegree 1Gapdegree 2Gapdegree 3Gapdeg.
> 3Well-NestedMildlyIll-NestedStronglyIll-NestedArabic 2995 205 189 13 2 1 204 1 0Czech 87889 20353 19989 359 4 1 20257 96 0Danish 5430 864 854 10 0 0 856 8 0Dutch 13349 4865 4425 427 13 0 4850 15 0Latin 3473 1743 1543 188 10 2 1552 191 0Portuguese 9071 1718 1302 351 51 14 1711 7 0Slovene 1998 555 443 81 21 10 550 5 0Swedish 11042 1079 1048 19 7 5 1008 71 0Turkish 5583 685 656 29 0 0 665 20 0Table 1: Counts of dependency trees classified by gap degree, and mild and strong ill-nestedness (for their gap degree); appear-ing in treebanks for Arabic (Hajic?
et al, 2004), Czech (Hajic?
et al, 2006), Danish (Kromann, 2003), Dutch (van der Beek et al,2002), Latin (Bamman and Crane, 2006), Portuguese (Afonso et al, 2002), Slovene (Dz?eroski et al, 2006), Swedish (Nilssonet al, 2005) and Turkish (Oflazer et al, 2003; Atalay et al, 2003).Figure 2: One of the smallest strongly ill-nested structures.This dependency structure has gap degree 1, but is onlymildly ill-nested for gap degree ?
2.located at position 0), it can be shown that all thestructures of any gap degree k with length smallerthan 10 are well-nested or only mildly ill-nestedfor that gap degree k.Even if a structure T is strongly ill-nested fora given gap degree, there is always some m ?
Nsuch that T is mildly ill-nested for m (since everydependency structure can be binarised, and binari-sations have finite gap degree).
For example, thestructure in Figure 2 is mildly ill-nested for gap de-gree 2.
Therefore, MGk parsers have the propertyof being able to parse any possible dependencystructure as long as we make k large enough.In practice, structures like the one in Figure 2do not seem to appear in dependency treebanks.We have analysed treebanks for nine different lan-guages, obtaining the data presented in Table 1.None of these treebanks contain structures that arestrongly ill-nested for their gap degree.
There-fore, in any of these treebanks, the MGk parser canparse every sentence with gap degree at most k.6 Conclusions and future workWe have defined a parsing algorithm for well-nested dependency structures with bounded gapdegree.
In terms of computational complexity,this algorithm is comparable to the best parsersfor related constituency-based formalisms: whenthe gap degree is at most 1, it runs in O(n7),like the fastest known parsers for LTAG, and canbe made O(n6) if we use unlexicalised depen-dencies.
When the gap degree is greater than 1,the time complexity goes up by a factor of n2for each extra unit of gap degree, as in parsersfor coupled context-free grammars.
Most of thenon-projective sentences appearing in treebanksare well-nested and have a small gap degree, sothis algorithm directly parses the vast majority ofthe non-projective constructions present in naturallanguages, without requiring the construction of aconstituency grammar as an intermediate step.Additionally, we have defined a set of struc-tures for any gap degree k which we call mildlyill-nested.
This set includes ill-nested structuresverifying certain conditions, and can be parsed inO(n3k+4) with a variant of the parser for well-nested structures.
The practical interest of mildlyill-nested structures can be seen in the data ob-tained from several dependency treebanks, show-ing that all of the ill-nested structures in them aremildly ill-nested for their corresponding gap de-gree.
Therefore, our O(n3k+4) parser can analyseall the gap degree k structures in these treebanks.The set of mildly ill-nested structures for gapdegree k is defined as the set of structures that havea binarisation of gap degree at most k. This defini-tion is directly related to the way the MGk parserworks, since it implicitly finds such a binarisation.An interesting line of future work would be to findan equivalent characterisation of mildly ill-nestedstructures which is more grammar-oriented andwould provide a more linguistic insight into thesestructures.
Another research direction, which weare currently working on, is exploring how vari-ants of the MGk parser?s strategy can be appliedto the problem of binarising LCFRS (Go?mez-Rodr?
?guez et al, 2009).298ReferencesSusana Afonso, Eckhard Bick, Renato Haber, and Di-ana Santos.
2002.
?Floresta sinta?(c)tica?
: a tree-bank for Portuguese.
In Proc.
of LREC 2002, pages1968?1703, Las Palmas, Spain.Nart B. Atalay, Kemal Oflazer, and Bilge Say.
2002.The annotation process in the Turkish treebank.
InProc.
of EACL Workshop on Linguistically Inter-preted Corpora - LINC, Budapest, Hungary.David Bamman and Gregory Crane.
2006.
The designand use of a Latin dependency treebank.
In Proc.
of5th Workshop on Treebanks and Linguistic Theories(TLT2006), pages 67?78.Manuel Bodirsky, Marco Kuhlmann, and MathiasMo?hl.
2005.
Well-nested drawings as modelsof syntactic structure.
Technical Report, Saar-land University.
Electronic version available at:http://www.ps.uni-sb.de/Papers/.Michael A. Covington.
1990.
A dependency parserfor variable-word-order languages.
Technical Re-port AI-1990-01, Athens, GA.Sas?o Dz?eroski, Tomaz?
Erjavec, Nina Ledinek, Petr Pa-jas, Zdene?k ?Zabokrtsky?, and Andreja ?Zele.
2006.Towards a Slovene dependency treebank.
In Proc.of LREC 2006, pages 1388?1391, Genoa, Italy.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In Proc.
of ACL-99, pages 457?464, Morristown, NJ.
ACL.Jason Eisner and Giorgio Satta.
2000.
A faster parsingalgorithm for lexicalized tree-adjoining grammars.In Proc.
of 5th Workshop on Tree-Adjoining Gram-mars and Related Formalisms (TAG+5), pages 14?19, Paris.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proc.
ofCOLING-96, pages 340?345, Copenhagen.Carlos Go?mez-Rodr?
?guez, John Carroll, and DavidWeir.
2008a.
A deductive approach to dependencyparsing.
In Proc.
of ACL?08:HLT, pages 968?976,Columbus, Ohio.
ACL.Carlos Go?mez-Rodr?
?guez, David Weir, and John Car-roll.
2008b.
Parsing mildly non-projective depen-dency structures.
Technical Report CSRP 600, De-partment of Informatics, University of Sussex.Carlos Go?mez-Rodr?
?guez, Marco Kuhlmann, GiorgioSatta, and David Weir.
2009.
Optimal reduction ofrule length in linear context-free rewriting systems.In Proc.
of NAACL?09:HLT (to appear).Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan ?Snaidauf,and Emanuel Bes?ka.
2004.
Prague Arabic depen-dency treebank: Development in data and tools.
InProc.
of NEMLAR International Conference on Ara-bic Language Resources and Tools, pages 110?117.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, JarmilaPanevova?, Petr Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??
?Havelka, and Marie Mikulova?.
2006.
Prague depen-dency treebank 2.0.
CDROM CAT: LDC2006T01,ISBN 1-58563-370-4.Jir???
Havelka.
2007.
Beyond projectivity: Multilin-gual evaluation of constraints and measures on non-projective structures.
In Proc.
of ACL 2007, Prague,Czech Republic.
ACL.Gu?nter Hotz and Gisela Pitsch.
1996.
On pars-ing coupled-context-free languages.
Theor.
Comput.Sci., 161(1-2):205?233.
Elsevier, Essex, UK.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Handbook of for-mal languages, pages 69?124.
Springer-Verlag,Berlin/Heidelberg/NY.Matthias T. Kromann.
2003.
The Danish dependencytreebank and the underlying linguistic theory.
InProc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT2003).Marco Kuhlmann and Mathias Mo?hl.
2007.
Mildlycontext-sensitive dependency languages.
In Proc.
ofACL 2007, Prague, Czech Republic.
ACL.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In Proc.of COLING/ACL main conference poster sessions,pages 507?514, Morristown, NJ, USA.
ACL.Marco Kuhlmann.
2007.
Dependency Structures andLexicalized Grammars.
Doctoral dissertation, Saar-land University, Saarbru?cken, Germany.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In IWPT 2007: Proc.
of the 10th Confer-ence on Parsing Technologies.
ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proc.
ofHLT/EMNLP 2005, pages 523?530, Morristown,NJ, USA.
ACL.Jens Nilsson, Johan Hall, and Joakim Nivre.
2005.MAMBA meets TIGER: Reconstructing a Swedishtreebank from antiquity.
In Proc.
of NODALIDA2005 Special Session on Treebanks, pages 119?132.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proc.
of ACL?05,pages 99?106, Morristown, NJ, USA.
ACL.Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?rand Go?khan Tu?r.
2003.
Building a Turkish tree-bank.
In A. Abeille, ed., Building and Exploit-ing Syntactically-annotated Corpora.
Kluwer, Dor-drecht.Giorgio Satta.
1992.
Recognition of linear context-free rewriting systems.
In Proc.
of ACL-92, pages89?95, Morristown, NJ.
ACL.Klaas Sikkel.
1997.
Parsing Schemata ?
A Frame-work for Specification and Analysis of Parsing Al-gorithms.
Springer-Verlag, Berlin/Heidelberg/NY.L.
van der Beek, G. Bouma, R. Malouf, and G. vanNoord.
2002.
The Alpino dependency treebank.In Computational Linguistics in the Netherlands(CLIN), Twente University.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions pro-duced by various grammatical formalisms.
In Proc.of ACL-87, pages 104?111, Morristown, NJ.
ACL.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proc.
of 8th International Workshop onParsing Technologies (IWPT 2003), pages 195?206.299
