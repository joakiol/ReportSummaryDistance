Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 96?100,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsFeature Space Selection and Combination for Native LanguageIdentificationCyril GoutteNational Research Council1200 Montreal Rd,Ottawa, ON K1A 0R6Cyril.Goutte@nrc.caSerge Le?gerNational Research Council100, des Aboiteaux St.,Moncton, NB E1A 7R1Serge.Leger@nrc.caMarine CarpuatNational Research Council1200 Montreal Rd,Ottawa, ON K1A 0R6Marine.Carpuat@nrc.caAbstractWe decribe the submissions made by the Na-tional Research Council Canada to the NativeLanguage Identification (NLI) shared task.Our submissions rely on a Support Vector Ma-chine classifier, various feature spaces usinga variety of lexical, spelling, and syntacticfeatures, and on a simple model combinationstrategy relying on a majority vote betweenclassifiers.
Somewhat surprisingly, a clas-sifier relying on purely lexical features per-formed very well and proved difficult to out-perform significantly using various combina-tions of feature spaces.
However, the com-bination of multiple predictors allowed to ex-ploit their different strengths and provided asignificant boost in performance.1 IntroductionWe describe the National Research CouncilCanada?s submissions to the Native Language Iden-tification 2013 shared task (Tetreault et al 2013).Our submissions rely on fairly straightforwardstatistical modelling techniques, applied to variousfeature spaces representing lexical and syntacticinformation.
Our most successful submission wasactually a combination of models trained on differ-ent sets of feature spaces using a simple majorityvote.Much of the work on Natural Language Process-ing is motivated by the desire to have machinesthat can help or replace humans on language-relatedtasks.
Many tasks such as topic or genre classifi-cation, entity extraction, disambiguation, are fairlystraightforward for humans to complete.
Machinestypically trade-off some performance for ease of ap-plication and reduced cost.
Equally fascinating aretasks that seem non-trivial to humans, but on whichmachines, through appropriate statistical analysis,discover regularities and dependencies that are farfrom obvious to humans.
Examples may include cat-egorizing text by author gender (Koppel et al 2003)or detecting whether a text is an original or a trans-lation (Baroni and Bernardini, 2006).
This is onemotivation for addressing the problem of identify-ing the native language of an author in this sharedtask.In the following section, we describe various as-pects of the models and features we used on thistask.
In section 3, we describe our experimental set-tings and summarize the results we obtained.
Wediscuss and conclude in section 4.2 ModellingOur submissions rely on straightforward statisticalclassifiers trained on various combinations of fea-tures and feature spaces.
We first describe the clas-sifier we used, then give the list of features that wehave been combining.
Our best performing submis-sion used a combination of the three systems we sub-mitted in a majority vote, which we also describe atthe end of this section.2.1 Classification ModelWe decided to use a straightforward and state-of-the-art statistical classifier, in order to focus our at-tention on the combination of features and modelsrather than on the design of the classifier.96We used freely available implementations of Sup-port Vector Machines (SVM) provided in SVM-light(Joachims, 1999) and SVM-perf (Joachims, 2006).SVM performance may be influenced by at least twoimportant factors: the choice of the kernel and thetrade-off parameter ?C?.
In our experiments, we didnot observe any gain from using either polynomialor RBF kernels.
All results below are therefore ob-tained with linear models.
Similarly, we investigatedthe optimization of parameter ?C?
on a held-out val-idation set, but found out that the resulting perfor-mance was not consistently significantly better thanthat provided by the default value.
As a consequenceour results were obtained using the SVM-light de-fault.One important issue in this shared task was tohandle multiple classes (the 11 languages).
Thereare essentially two easy approaches to handle sin-gle label, multiclass classification with binary SVM:one-versus-all and one-versus-one.
We adopted theone-versus-all setting, combined with a calibrationstep.
We first trained 11 classifiers using the docu-ments for each language in turn as ?positive?
exam-ples, and the documents for the remaining 10 lan-guages as negative examples.
The output score foreach class-specific SVM model was then mappedinto a probability using isotonic regression with thepair-adjacent violators (PAV) algorithm (Zadroznyand Elkan, 2002).
A test document is then assignedto the class with the highest probability.2.2 Feature Space ExtractionWe extracted the following features from the docu-ments provided for the shared task.Character ngrams: We index trigrams of charac-ters within each word (Koppel et al 2005).
Thebeginning and end of a word are treated as specialcharacter.
For example, the word ?at?
will producetwo trigrams: ?
at?
and ?at ?.
These features allow usto capture for example typical spelling variants.
Ina language with weak morphology such as English,they may also be able to capture patterns of usageof, e.g.
suffixes, which provides a low-cost proxyfor syntactic information.Word ngrams: We index unigrams and bigramsof words within each sentence.
For bigrams, the be-ginning and end of a sentence are treated as specialtokens.
Note that we do not apply any stoplist fil-tering.
As a consequence, function words, an often-used feature (Koppel et al 2005; Brooke and Hirst,2012), are naturally included in the unigram featurespace.Spelling features: Misspelled words are identifiedusing GNU Aspell V0.60.41 and indexed with theircounts.
Some parser artifacts such as ?n?t?
are re-moved from the final mispelled word index.
Al-though misspellings may seem to provide clues asto the author?s native language, we did not find thesefeatures to be useful in any of our experiments.
Notehowever, that misspelled words will also appear inthe unigram feature space.Part-of-speech ngrams: The texts were taggedwith the Stanford tagger v. 3.02 using the largestand best (bidirectional) model.
Note that the lan-guage in a couple of documents was so poor that thetagger was unable to complete, and we reverted to aslightly weaker (left three words) model for those.After tagging, we indexed all ngrams of part-of-speech tags, with n = 2, 3, 4, 5.
We experimentedwith the choice of n and found out that n > 2 didnot bring any significant difference in performance.Syntactic dependencies: We ran the StanfordParser v2.0.0 on all essays, and use the typeddependency output to generate features.
Ourgoal is to capture phenomena such as preposi-tion selection which might be influenced by thenative language of the writer.
In order to reducesparsity, each observed dependency is used togenerate three features: one feature for the fulllexicalized dependency relation; one feature forthe head (which generalizes over all observedmodifiers); one feature for the modifier (whichgeneralizes over all possible heads).
For instance,in the sentence ?they participate to one ?s appear-ance?, the parser extracts the following depen-dency: ?prepto(participate,appearance)?.
It yieldsthree features ?prepto(participate,appearance)?,?prepto(participate,X)?
and?prepto(X,appearance)?.
We experimented with allthree feature types, but the systems used for the1http://aspell.net2http://nlp.stanford.edu/software/tagger.shtml97official evaluation results only used the last two(head and modifier features.)
Note that while thesefeatures can capture long distance dependencies intheory, they significantly overlap with word ngramfeatures in practice.For each feature space, we used a choice of twoweighting schemes inspired by SMART (Manninget al 2008):ltc: log of the feature count, combined with the loginverse document frequency (idf), with a cosinenormalization;nnc: straight feature count, no idf, with cosine nor-malization.Normalization is important with SVM classifiers asthey are not scale invariant and tend to be sensitiveto large variations in the scale of features.2.3 Voting CombinationInvestigating the differences in predictions madeby different models, it became apparent that therewere significant differences between systems thatdisplayed similar performance.
For example, ourfirst two submissions, which perform within 0.2% ofeach other on the test data, disagree on almost 20%of the examples.This suggests that there is potentially a lot of in-formation to gain by combining systems trained ondifferent feature spaces.
An attempt to directly com-bine the predictions of different systems into a newpredictive score proved unsuccessful and failed toprovide a significant gain over the systems used inthe combination.A more successful combination was obtained us-ing a simple majority vote.
Our method relies onsimply looking at the classes predicted by an en-semble of classifier for a given document.
The pre-diction for the ensemble will be the most predictedclass, breaking possible ties according to the overallscores of the component models: for example, for anensemble of only 2 models, the decision in the caseof a tie will be that of the best model.3 ExperimentsWe describe the experimental setting that we usedto prepare our submissions, and the final perfor-mance we obtained on the shared task (Tetreault etal., 2013).3.1 Experimental SettingIn order to test the performance of various choicesof feature spaces and their combination, we set up across-validation experimental setting.
We originallysampled 9 equal sized disjoint folds of 1100 docu-ments each from the training data.
We used strati-fied sampling across the languages and the prompts.This made sure that the folds respected the uniformdistribution across languages, as well as the distri-bution across prompts, which was slightly unevenfor some languages.
These 9 folds were later aug-mented with a 10th fold containing the developmentdata released during the evaluation.All systems were evaluated by computing the ac-curacy (or equivalently the micro-averaged F-score)on the cross-validated predictions.3.2 Experimental ResultsWe submitted four systems to the shared task evalu-ation:1.
BOW2ltc+CHAR3ltc: Uses counts of word bi-grams and character trigrams, both weightedindependently with the ltc weighting scheme(tf-idf with cosine normalization);2.
BOW2ltc+DEPltc: Uses counts of wordbigrams and syntactic dependencies, bothweighted independently with the ltc weightingscheme;3.
BOW2ltc+CHAR3ltc+POS2nnc: Same as sys-tem #1, adding counts of bigrams of part-of-speech tags, independently cosine-normalized;4.
3-system vote: Combination of the three sub-missions using majority vote.The purpose of submission #1 was to check theperformance that we could get using only surfaceform information (words and spelling).
As shownon Table 1, it reached an average test accuracy of79.5%, which places it in the middle of the pack overall submissions.
For us, it establishes a baseline ofwhat is achievable without any additional syntacticinformation provided by either taggers or parsers.98Model # Acc(%)BOW2ltc+CHAR3ltc 1 79.27BOW2ltc+DEPltc 2 79.55BOW2ltc+CHAR3ltc+POS2nnc 3 78.823-system vote 4 81.8210-system vote - 84.00Table 1: The four systems submitted by NRC, plus amore extensive voting combination.
System 1 uses onlysurface information.
Systems 2 and 3 use two types ofsyntactic information and system #4 uses a majority voteamong the three previous submissions.
The last (unsub-mitted) uses a majority vote among ten systems.Our submissions #2 and #3 were meant to checkthe effect of adding syntactic features to basic lexi-cal information.
We evaluated various combinationsof feature spaces using cross-validation performanceand found out that these two combinations seemed tobring a small boost in performance.
Unfortunately,as shown on Table 1, this did not reflect on the actualtest results.
The test performance of submission #2was a mere 0.2% higher than our baseline, when weexpected +0.6% from the cross-validation estimate.The test performance for submission #3 was 0.5%below that of the baseline, whereas we expected asmall increase.Submission #4 was our majority voting submis-sion.
Due to lack of time, we could not generatetest predictions for all the systems that we wanted toinclude in the combination.
As a consequence, weperformed a majority voting over just the 3 previ-ous submissions.
Despite this, the majority votingproved remarkaby effective, yielding a 2.5% perfor-mance boost over our baseline, and a 2.3% increaseover our best single system.In order to further test the potential of the major-ity vote, we later applied it to the 10 best systems ina pool generated from various combinations of fea-ture spaces (10-system vote in Table 1).
That (unsub-mitted) combination outperformed our official sub-missions by another 2.2% accuracy, and in fact out-performed the best system in the official evaluationresults by a small (and very likely not significant)margin.In comparison with submissions from othergroups, our top submission was 1.8% below the topperforming system (Table 2).
According to the re-Model Accuracy(%) p-valueJarvis 83.6 0.082Oslo NLI 83.4 0.1Unibuc 82.7 0.361MITRE-Carnie 82.6 0.448Tuebingen 82.2 0.715NRC 81.8CMU-Haifa 81.5 0.807Cologne-Nijmegen 81.4 0.665NAIST 81.1 0.472UTD 80.9 0.401UAlberta 80.3 0.194Toronto 80.2 0.167MQ 80.1 0.097Table 2: Resulting accuracy scores and significance vs.NRC top submission (3-system vote).sults of significance tests released by the organizers,the difference is slightly below the traditional thresh-old of statistical significance (0.05).4 Discussion and ConclusionOur results suggest that on the shared task, a combi-nation of features relying only on word and characterngrams provided a strong baseline.
Our best systemended up being a combination of models trained onvarious sets of lexical and syntactic features, using asimple majority vote.
Our submission #4 combinedonly our three other submissions, but we later exper-imented with a larger pool of models.
Table 3 showsthat the best performance is obtained using the top10 models, and many of the combinations are com-petitive with the best performance achieved duringthe evaluation.
Our cross-validation estimate wasalso maximized for 10 models, with as estimated ac-curacy of 83.23%.
It is interesting that adding someof the weaker models does not seem to hurt the vot-ing combination very much.One obvious limitation of this study is that it wasapplied to a well defined and circumscribed setting.There is definitely no guarantee on the performancethat may be obtained on a different corpus of docu-ments.Another limitation is that although the resultingperformance of our models seems encouraging, itis not obvious that we have learned particularly99Model VoteRank score score Feature set1 79.55 79.55 BOW2+DEP2 79.36 79.55 BOW1+DEP3 79.27 82.18 BOW2+CHAR34 79.00 82.27 BOW1+DEPL5 78.91 82.91 BOW2+CHAR3+POS36 78.82 83.18 BOW2+CHAR3+POS27 78.73 83.45 BOW2+DEPL8 78.36 83.55 BOW29 77.09 83.82 BOW1+POS310 76.82 84.00 BOW2+POS211 76.55 83.64 BOW2+POS312 76.55 83.82 BOW1+POS213 75.27 83.55 BOW114 74.36 83.73 BOW1+CHAR315 74.27 83.73 DEP16 66.91 83.91 DEPL17 64.18 83.82 CHAR318 51.64 83.82 POS319 49.64 83.36 POS2Table 3: Majority vote among the top-N mod-els.
BOWn=word ngrams; CHAR3=char trigrams;POSn=POS ngrams; DEP/DEPL=syntactic dependecies.useful clues about what differentiates the Englishwritten by authors with different native languages.This is of course a side effect of a format wheresystems compete on a specific performance met-ric, which encourages using large, well-regularizedmodels which optimize the relevant metric, at the ex-pense of sparser models focusing on a few markersthat may be more easily understandable.During the workshop, we plan to show more com-plete results using the majority vote strategy, involv-ing a wider array of base models.ReferencesM.
Baroni and S. Bernardini.
2006.
A new approach tothe study of translationese: Machine-learning the dif-ference between original and translated text.
Literaryand Linguistic Computing, 21(3):259?274.Julian Brooke and Graeme Hirst.
2012.
Robust, lexical-ized native language identification.
In Proceedings ofCOLING 2012.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In Advances in Kernel Methods - SupportVector Learning.T.
Joachims.
2006.
Training linear SVMs in linear time.In Proceedings of the ACM Conference on KnowledgeDiscovery and Data Mining (KDD).M.
Koppel, S. Argamon, and A. R. Shimoni.
2003.
Auto-matically categorizing written texts by author gender.Literary and Linguistic Computing, 17:401?412.M.
Koppel, J. Schler, and K. Zigdon.
2005.
Determin-ing an authors native language by mining a text for er-rors.
In Proceedings of the 11th ACM SIGKDD Inter-national Conference on Knowledge Discovery in DataMining (KDD 05), pages 624?628, Chicago, Ilinois,USA.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
2008.
Document and query weightingschemes.
In Introduction to Information Retrieval.Cambridge University Press.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, Atlanta, GA, USA, June.
Association forComputational Linguistics.B.
Zadrozny and C. Elkan.
2002.
Transforming classifierscores into accurate multiclass probability estimates.In Proceedings of the Eighth International Conferenceon Knowledge Discovery and Data Mining (KDD?02).100
