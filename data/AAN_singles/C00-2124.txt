Applying System Combinat ion to Base Noun Phrase Identif icationEr ik  F. T jong  K im Sang",  Wal ter  Dae lemans  '~, Herv6  D6 jean  ~,Rob  Koel ingT,  Yuva l  Krymolowski /~,  Vas in  Punyakanok  '~, Dan Roth"~University of Antwert)Uifiversiteitsplohl 113.-261.0 Wilri jk, Belgium{erikt,daelem}@uia.ua.ac.ber Unive.rsitiil; Tii l) ingenKleine Wilhehnstrat./e 113I)-72074 T/il)ingen, Germany(lejean((~sl:q, ni)hil.ulfi-l;uebingen.de,7S1{,I Cambridge23 Millers Yard,Mil l  LaneCambridge, CB2 ll{Q, UKkoeling@caln.sri.coIn;~Bal'-Ilan Universitylbunat  Gan, 52900, Israelyuwdk(c~)macs.
1)iu.ac.il"University of Illinois1304: W. Sl)ringfield Ave.Url)ana, IL 61801, USA{lmnyakan,(lanr} ((~cs.uiuc.eduA1)s t rac tWe us('.
seven machine h;arning algorithms tbrone task: idenl;it~ying l)ase holm phrases.
Theresults have 1)een t)rocessed by ditt'erent systemcombination methods and all of these (mtt)er-formed the t)est individual result.
We have ap-t)lied the seven learners with the best (:omt)ina-tot, a majority vote of the top tive systenls, to astandard (lata set and lllallage(1 I;O ilnl)rov(', 1;11('t)est pul)lished result %r this (lata set.1 In t roduct ionVan Haltor(m eta\ ] .
(1998) and Brill and Wu(1998) show that part-ofst)ee(:h tagger l)erfor-mance can 1)e iml)roved 1)y (:oml)ining ditl'erenttatters.
By using te(:hni(tues su(:h as majorityvoting, errors made l)y 1;11(; minority of the tag-gers can 1)e r(;moved.
Van Ilaltere, n et al (1998)rel)ort that the results of such a ('oml)ined al)-proach can improve ll\])Oll the aCcllracy error ofthe best individual system with as much as 19%.Tim positive (;tl'e(:t of system combination tbrnon-language t)ro(:essing tasks has t)een shownin a large l)o(ly of mac\]fine l arning work.In this 1)aper we will use system (:omt)inationfor identifying base noun 1)hrases (1)aseNt)s).W(; will at)l)ly seven machine learning algo-rithms to the same 1)aseNP task.
At two l)ointswe will al)ply confl)ination methods.
We willstart with making the systems process five out-trot representations and combine the l'esults t)y(:hoosing the majority of the outtmt tL'atures.Three of the seven systems use this al)l)roaeh.Afl, er this w(; will make an overall eoml)inationof the results of the seven systems.
There wewill evaluate several system combination meth-()(Is.
The 1)est l)erforming method will 1)e at)-t)lied to a standard ata set tbr baseNP identi-tication.2 Methods  and exper imentsin this se(:tion we will describe our lem:ning task:recognizing 1)ase noun phrases.
After this wewill (tes(:ril)e the data representations we usedand the ma('hine learning algorithms that wewill at)l)ly to the task.
We will con(:ludc withan overview of the (:ombination metllo(ls thatwe will test.2.1 Task descript ionBase noun \])hrases (1)aseNPs) are n(mn phraseswhi(:h do not (:ontain another noun l)hrase.
\]?orcxamt)le , the sentenceIn \[early trading\] in \[ IIong Kong\]\[ Mo,l,tay \], \[ g,,la \] was q, loted at\[ $ 366.
0 \] \ [a .
o1,,,.
(; \] .contains six baseN1)s (marked as phrases be-tween square 1)rackets).
The phrase $ 266.50an  ounce  ix a holm phrase as well.
However, itis not a baseNP since it contains two other nounphrases.
Two baseNP data sets haw.'
been putforward by Ramshaw and Marcus (1995).
Themain data set consist of tbur sections of the WallStreet Journal (WSJ) part of the Penn Tree-bank (Marcus et al, 1.993) as training mate-rial (sections 15-18, 211727 tokens) and one sec-tion aS test material (section 20, 47377 tokens)5.The data contains words, their part-of-speech1This Ramshaw and Marcus (1995) bascNP data setis availal)le via ffp://fti).cis.upe,m.edu/pub/chunker/857(POS) tags as computed by the Brill tagger andtheir baseNP segmentation asderived from the%'eebank (with some modifications).In the baseNP identitication task, perfor-mance is measured with three rates.
First,with the percentage of detected noun phrasesthat are correct (precision).
Second, with the1)ercentage of noun phrases in the data thatwere found by the classifier (recall).
And third,with the F#=~ rate which is equal to (2*preci-sion*recall)/(precision+recall).
The latter ratehas been used as the target for optimization.2.2 Data representat ionIn our example sentence in section 2.1, nounphrases are represented by bracket structures.It has been shown by Mufioz et al (1999)that for baseNP recognition, the representa-tion with brackets outperforms other data rep-resentations.
One classifier can be trained torecognize open brackets (O) and another canhandle close brackets (C).
Their results can becombined by making pairs of open and closebrackets with large probability scores.
We haveused this bracket representation (O+C) as well.However, we have not used the combinationstrategy from Mufioz et al (1999) trot in-stead used the strategy outlined in Tjong KimSang (2000): regard only the shortest possi-ble phrases between candidate open and closebrackets as base noun phrases.An alternative representation for baseNPshas been put tbrward by Ramshaw and Mar-cus (1995).
They have defined baseNP recog-nition as a tagging task: words can be inside abaseNP (I) or outside a baseNP (O).
In the casethat one baseNP immediately follows anotherbaseNP, the first word in the second baseNPreceives tag B.
Example:Ino early1 trading1 ino Hongi KongiMondayB ,o gold1 waso quotedo ato$I 366.501 anu ounce1 .oThis set of three tags is sufficient for encod-ing baseNP structures since these structures arenonrecursive and nonoverlapping.Tjong Kiln Sang (2000) outlines alternativeversions of this tagging representation.
First,the B tag can be used for tile first word of ev-ery baseNP (IOB2 representation).
Second, in-stead of the B tag an E tag can be used tonlark the last word of a baseNP immediatelybefore another baseNP (IOE1).
And third, theE tag call be used for every noun phrase finalword (IOE2).
He used the Ramshaw and Mar-cus (1995) representation as well (IOB1).
Wewill use these tbur tagging representations andthe O+C representation for the system-internalcombination experiments.2.a Machine learning algorithmsThis section contains a brief description of tileseven machine learning algorithms that we willapply to the baseNP identification task: AL-LiS, c5.0, IO~?ee, MaxEnt, MBL, MBSL andSNOW.ALLiS 2 (Architecture for Learning LinguisticStructures) is a learning system which uses the-ory refinement in order to learn non-recursiveNP and VP structures (Ddjean, 2000).
ALLiSgenerates a regular expression grammar whichdescribes the phrase structure (NP or VP).
Thisgrammar is then used by the CASS parser (Ab-hey, 1996).
Following the principle of theory re-finement, tile learning task is composed of twosteps.
The first step is the generation of aninitial wa, mmar.
The generation of this grmn-mar uses the notion of default values and somebackground knowledge which provides generalexpectations concerning the immr structure ofNPs and VPs.
This initial grammar providesan incomplete and/or incorrect analysis of tiledata.
The second step is the refinement of thisgrammar.
During this step, the validity of therules of the initial grammar is checked and therules are improved (refined) if necessary.
Thisrefinement relies on the use of two operations:the contextualization (i which contexts uch atag always belongs to the phrase) and lexical-ization (use of information about the words andnot only about POS).05.0 a, a commercial version of 04.5 (Quin-lan, 1993), performs top-do,vn induction of de-cision trees (TDIDT).
O,1 the basis of an in-stance base of examples, 05.0 constructs a deci-sion tree which compresses the classification i -formation in the instance base by exploiting dif-tbrences in relative importance of different fea-tures.
Instances are stored in the tree as paths2A demo f the NP and VP ctmnker is available atht;t:p: / /www.sfb441.unituebingen.de/~ dej an/chunker.htmlaAvailable fl'om http://www.rulequest.com858of commcted nodes ending in leaves which con-tain classification information.
Nodes are con-nected via arcs denoting feature wflues.
Featureinff)rmation gain (nmt;ual inforniation 1)etweenfeatures and class) is used to determine the or-der in which features are mnt)loyed as tests at alllevels of the tree (Quinlan, 1993), With the fullinlmt representation (words and POS tags)~ wewere not able to run comt)lete xperiments.
Wetherefore xperimented only with the POS tags(with a context of two left; and right).
We haveused the default parameter setting with decisiontrees coml)ined with wflue groul)ing.We have used a nearest neighbor algoritlml(IBI.-1G, here listed as MBL) and a decision treealgoritlmi (llG\[lh:ee) from the TiMBL learningpackage (Da(flmnans et al, 19991)).
Both algo-rithms store the training data and ('lassi(y newit;eros by choosing the most frequent (:lassiti(:a-lion among training items which are closest tothis new item.
l)ata it(uns rare rel)resented assets of thature-vahu; 1)airs.
Each ti;ature recc'ivesa weight which is t)ased on the amount of in-formation whi(:h it t/rovides fi)r comtmting theclassification of t;t1(; items in the training data.IBI-IG uses these weights tbr comt)uting the dis-lance l)etween a t)air of data items and IGTreeuses them fi)r deciding which feature-value de-cisions shouM t)e made in the top nod(;s of thedecision tree (l)a(;lenJans et al, 19991)).
Wewill use their det, mlt pm:amet('a:s excel)t for theIBI-IG t)arameter for the numl)er of exmnine(tm',arest n(,ighl)ors (k) whi('h we h~ve s(,t to 3(Daelemans et al, 1999a).
The classifiers use aleft and right context of four words and part-ofsl)eech tags.
t~i)r |;lie four IO representationswe have used a second i)rocessing stage whichused a smaller context lint which included in-formation at)out the IO tags 1)redicted by thefirst processing phase (Tjong Kim Sang, 2000).When /)uilding a classifier, one must gatherevidence ti)r predicting the correct class of anitem from its context.
The Maxinmm Entropy(MaxEnt) fl:mnework is especially suited tbrintegrating evidence tiom various inti)rmal;ionsources.
Frequencies of evidence/class combi~nations (called features) are extracted fl'om asample corlms and considere(t to be t)ropertiesof the classification process.
Attention is con-strained to models with these l)roperties.
TheMaxEnt t)rinciph; now demands that among all1;11(; 1)robability distributions that obey theseconstraints, the most mfiform is chosen, l)ur-ing training, features are assigned weights insuch a way that, given the MaxEnt principle,the training data is matched as well as possible.During evaluation it is tested which features areactive (i.e.
a feature is active when the contextmeets the requirements given by t;11(', feature).For every class the weights of the active fea-tures are combined and the best scoring classis chosen (Berger et al, 1996).
D)r the classi-tier built here the surromlding words, their POStags and lmseNP tags predicted for the previouswords are used its evidence.
A mixture of simplefeatures (consisting of one of the mentioned in-formation sources) and complex features (com-binations thereof) were used.
The left contextnever exceeded 3 words, the right context wasmaximally 2 words.
The model wits (:ah:ulatedusing existing software (l)ehaspe, 1997).MBSL (Argalnon et al, 1999) uses POS datain order to identit~y t/aseNPs, hfferenee re-lies on a memory which contains all the o(:-cm:rences of P()S sequences which apt)ear inthe t)egimfing, or the end, of a 1)aseNl?
(in-(:hiding complete t)hrases).
These sequencesmay include a thw context tags, up to a 1)re-st)ecifi('d max_(:ont<~:t. \])uring inti',rence, MBSLtries to 'tile' each POS string with parts ofnoun-l)hrases from l;he memory.
If the stringcoul(1 l)e fully covered t)y the tiles, il; becomesl)art of a (:andidate list, anfl)iguities 1)etweencandidates are resolved by a constraint )ropa-gation algorithm.
Adding a (:ontext extends thepossil)ilities for tiling, thereby giving more op-portunities to 1)etter candidates.
The at)t)roaehof MBSL to the i)rot)lem of identifying 1)aseNPsis sequence-1)ased rather than word-based, thatis, decisions are taken per POS sequence, or percandidate, trot not for a single word.
In addi-tion, the tiling l)rocess gives no preference toany (tirection in the sentence.
The tiles may 1)eof any length, up to the maximal ength of a1)hrase in the training (ILl;L, which gives MBSLa generalization power that compensates for thesetup of using only POS tags.
The results t)re-seated here were obtained by optimizing MBSLparameters based on 5-fold CV on the trainingdata.SNoW uses the Open/Close model, describedin Mufioz et al (1999).
As is shown there, this859section 21IOB1IOB2IOE1IOE2O+C097.81%97.63%97.80%97.72%97.72%MBLMajority 98.04% 98.20%C Ffl=l97.97% 91.6897.96% 91.7997.92% 91.5497.94% 92.0698.04% 92.0392.82MaxEntO C97.90% 98.11%97.81% 98.14%97.88% 98.12%97.84% 98.12%97.82% 98.15%97.94% 98.24%Ffl=l92.4392.1492.3792.1392.2692.60IGTreeO C96.62% 96.89%97.27% 97.30%95.88% 96.01%97.19% 97.62%96.89% 97.49%97.70% 97.99%F\[~=187.8890.0382.8089.9889.3791.92Table 1: The effects of system-internal combination by using different output representations.
Astraight-forward majority vote of the output yields better bracket accuracies and Ffl=l rates thanany included individual classifier.
The bracket accuracies in the cohmms O and C show whatpercentage of words was correctly classified as baseNP start, baseNP end or neither.model produced better results than the otherparadigm evaluated there, the Inside/Outsideparadigm.
The Open/Close model consists oftwo SNoW predictors, one of which predicts thebeginning of baseNPs (Open predictor), and theother predicts the end of the ptlrase (Close pre-dictor).
The Open predictor is learned usingSNoW (Carlson el; al., 1999; Roth, 1998) as aflmction of features that utilize words and POStags in the sentence and, given a new sentence,will predict for each word whether it is the firstword in the phrase or not.
For each Open, theClose predictor is learned using SNoW as a func-tion of features that utilize the words ill the sen-tence, the POS tags and the open prediction.
Itwill predict, tbr each word, whether it Call bethe end of" the I)hrase, given the previously pre-dicted Open.
Each pair of predicted Open midClose forms a candidate of a baseNP.
These can-didates may conflict due to overlapping; at thisstage, a graph-based constraint satisfaction al-gorithm that uses the confidence values SNoWassociates with its predictions i elnployed.
Thisalgorithln ("the combinator') produces tile listof" the final baseNPs fbr each sentence.
Detailsof SNOW, its application in shallow parsing andthe combinator% Mgorithm are in Mufioz et al(1999).2.4 Combinat ion techniquesAt two points in our noun phrase recognitionprocess we will use system combination.
We willstart with system-internal combination: applythe same learning algorithm to variants of thetask and combine the results.
The approachwe have chosen here is the same as in TjongKim Sang (2000): generate different variantsof the task by using different representationsof the output (IOB1, IOB2, IOE1, IOE2 andO+C).
The five outputs will converted to theopen bracket representation (O) and the closebracket; representation (C) and M'ter this, tilemost frequent of the five analyses of each wordwill chosen (inajority voting, see below).
Weexpect the systems which use this combinationphase to perform better than their individuMmembers (Tjong Kim Sang, 2000).Our seven learners will generate different clas-sifications of tile training data and we need tofind out which combination techniques are mostappropriate.
For the system-external combi-nation experiment, we have evaluated itfi;rentvoting lllechanisms~ effectively the voting meth-ods as described in Van Halteren et al (1998).In the first method each classification receivesthe same weight and the most frequent classifi-cation is chosen (Majority).
The second nmthodregards as tile weight of each individual clas-sification algorithm its accuracy on solne partof the data, tile tuning data (TotPrecision).The third voting method computes the preci-sion of each assigned tag per classifer and usesthis value as a weight for tile classifier in thosecases that it chooses the tag (TagPrecision).The fourth method uses both the precision ofeach assigned tag and tile recall of the com-peting tags (Precision-Recall).
Finally, tile fifthlnethod uses not only a weight for tile currentclassification but it also computes weights tbrother possible classifications.
The other classi-fications are deternfined by exalnining the tun-860ing data and registering the correct wflues for(;very pair of classitier esults (pair-wise voting,see Van Halteren et al (1998) tbr an elaborateexplanation).Apart from these five voting methods we havealso processed the output streams with two clas-sifters: MBL and IG%'ee.
This approach iscalled classifier stacking.
Like Van Halteren etal.
(1998), we have used diff'erent intmt ver-sions: olle containing only the classitier Otltl)utand another containing both classifier outlmtand a compressed representation of the dataitem tamer consideration.
\]?or the latter lmr-pose we have used the part-of-speech tag of thecarrent word.3 Resul ts  4We want to find out whether system combi-nation could improve performmlce of baseNPrecognition and, if this is the fact, we want toseJect the best confl)ination technique.
For thislmrpose we have pertbrmed an experiment withsections 15-18 of the WSJ part of the Prom %'ee-bank as training data (211727 tokens) and sec-tion 21 as test data (40039 tokens).
Like thedata used by Ramshaw and Marcus (1995), thisdata was retagged by the Brill tagger in orderto obtain realistic part-of  speech (POS) tags 5.The data was seglnente.d into baseNP parts andnon-lmseNP t)arts ill a similar fitshion as thedata used 1)y Ramshaw and Marcus (1995).
Ofthe training data, only 90% was used for train-ing.
The remaining 10% was used as lamingdata for determining the weights of the combi-nation techniques.D)r three classifiers (MBL, MaxEnt andIGTree) we haw; used system-internal coral)i-nation.
These learning algorithms have pro-cessed five dittbrent representations of the out-put (IOB1, IOB2, IOE1, IOE2 and O-t-C) andthe results have been combined with majorityvoting.
The test data results can 1)e fimnd inTable 1.
In all cases, the combined results werebetter than that of the best included system.Tile results of ALLiS, 05.0, MB SL and SNoWhave tmen converted to the O and the C repre-4Detailed results of our experiments me available onhttp: / /lcg-www.uia.ae.be/-erikt /np('oml,i /SThe retagging was necessary to assure that the per-formance rates obtained here would be similar to ratesobtained for texts for which no Treebank POS tags areavailable.section 21ClassifierALLiS05.0IGTreeMaxEntMBLMBSLSNoWSimple VotingMajorityTotPrecisionTagPrecisionPrecision-Recall097.87%97.05%97.70%97.94%98.04%97.27%97.78%98.08%98.08%98.08%98.08%C FS=j98.08% 92.1597.76% 89.9797.99% 91.9298.24% 92.6098.20% 92.8297.66% 90.7197.68% 91.8798.21% 92.9598.21% 92.9598.21% 92.9598.21% 92.95Pairwise VotingTagPair 98.13% 98.23%Memory-BasedTags 98.24% 98.35%Tags 4- P()S 98.14% 98.33%Deeision TreesTags 98.24% 98.35%Tags + POS 98.13% 98.32%93.0793.3993.2493.3993.21Table 2: Bracket accuracies and Ff~=l scoresfor section WSJ 21 of the Penn ~15'eebank withseve, n individual classifiers and combinations ofthem.
Each combination t)erforms t)etter thanits best individual me, tuber.
The stacked classi-tiers without COllte, xt intbrmation perform best.sentation.
Together with the bracket; ret)resen-tations of the other three techniques, this gaveus a total of seven O results and seven C results.These two data streams have been combinedwith the combination techniques described insection 2.4.
After this, we built baseNPs fromthe, O and C results of each combinatkm tech-nique, like, described in section 2.2.
The bracketaccuracies and tile F~=I scores tbr test data canbe found in Table 2.All combinations iml)rove the results of thebest individual classifier.
The best results wereobtained with a memory-based stacked classi-ter.
This is different from the combination re-sults presented in Van Ilalteren et al (1998),in which pairwise voting pertbrmed best.
How-eves, in their later work stacked classifiers out-perIbrm voting methods as well (Van Halterenet al, to appear).861section 20 accuracy precision recallBest-five combination 0:98.32% C:98.41% 94.18% 93.55%Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89%Mufioz et al (1999) O:98.1% C:98.2% 92.4% 93.1%Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27%Argamon et al (1999) - 91.6% 91.6%F/3=193.8693.2692.892.0391.6Table 3: The overall pertbrmance of the majority voting combination of our best five systems(selected on tinting data perfbrnmnce) applied to the standard data set pnt tbrward by Ramshawand Marcus (1995) together with an overview of earlier work.
The accuracy scores indicate howoften a word was classified correctly with the representation used (O, C or IOB1).
The combinedsystem outperforms all earlier reported results tbr this data set.Based on an earlier combination study(Tjong Kim Sang, 2000) we had expected thevoting methods to do better.
We suspect hattheir pertbrmance is below that of the stackedclassifiers because the diflhrence between tilebest and the worst individual system is largerthan in our earlier study.
We assume that thevoting methods might perform better if theywere only applied to the classifiers that per-form well on this task.
In order to test thishypothesis, we have repeated the combinationexperiments with the best n classitiers, wheren took vahms from 3 to 6 and the classifierswere ranked based on their performance on thetnning data.
The t)est pertbrmances were ob-tained with five classifiers: F/~=1=93.44 for allfive voting methods with tile best stacked classi-tier reaching 93.24.
With the top five classifiers,tile voting methods outpertbrm the best; combi-nation with seven systems G. Adding extra clas-sification results to a good combination systemshould not make overall performance worse soit is clear that there is some room left for im-provement of our combination algorithms.We conclude that the best results ill thistask can be obtained with tile simplest votingmethod, majority voting, applied to the bestfive of our classifiers.
Our next task was toapply the combination apt)roach to a standarddata set so that we could compare our resultswith other work.
For this purpose we have used6V~re are unaware of a good method for determiningthe significance of F~=I differences but we assume thatthis F~=I difference is not significant.
However, we be-lieve that the fact that more colnbination methods per-tbrm well, shows that it easier to get a good pertbrmmlceout of the best; five systems than with all seven.tile data put tbrward by ll,amshaw and Marcus(1995).
Again, only 90% of the training datawas used tbr training while the remaining 11)%was reserved tbr ranking the classifiers.
Theseven learners were trained with the same pa-rameters as in the previous experiment.
Threeof the classifiers (MBL, MaxEnt and iG%'ee)used system-internal combination by processingdifferent output representations.The classifier output was converted to theO and the C representation.
Based on thetuning data performance, the classifiers ALLiS,IGTREE, MaxEnt, MBL and SNoW were se-lected for being combined with majority vot-ing.
After this, the resulting O and C repre-sentations were combined to baseNPs by usingthe method described in section 2.2.
The re-sults can be found in Table 3.
Our combinedsystem obtains an F/~=I score of 93.86 whichcorresponds to an 8% error reduction comparedwith tile best published result tbr this data set(93.26).4 Conc lud ing  remarksIn this paper we have examined two methods forcombining the results of machine learuing algo-rithms tbr identii}cing base noun phrases.
Ill thefirst Inethod, the learner processed ifferent out-put data representations and tile results werecombined by majority voting.
This approachyielded better results than the best includedclassifier.
Ill the second combination approachwe have combined the results of seven learningsystems (ALLiS, c5.0, IGTree, MaxEnt, MBL,MBSL and SNOW).
Here we have tested d i fferent confl)ination methods.
Each coilfl)ination862nmthod outt)erformed the best individual learn-ing algorithm and a majority vote of the tol)five systems peribrmed best.
We, have at}i}lie, dthis approach of system-internal nd system-external coral}|nation to a standard ata set forbase noun phrase identification and the 1}ertbr-mance of our system was 1)etter than any othertmblished result tbr this data set.Our study shows that the c, omt)ination meth-(}{Is that we have tested are sensitive for the in-clusion of classifier esults of poor quality.
Thisleaves room for imt)rovement of our results t}yevaluating other coml}inators.
Another interest-ing apl)roach which might lead to a l}etter t)er-f{}rmance is taking into a{-com~t more contextinibrmation, for example by coral)in|rig com-plete 1}hrases instead of indet}endent t}ra{:kets.It would also be worthwhile to evaluate usingmore elaborate me, thods lbr building baseNPsout of ot}en and close t}ra{:ket (:an{ti{tates.Acknowledgementsl)djean, Koeling and 'l?jong Kim Sang arefunded by the TMII.
1\]etwork Learning (Jompu-tational Grammars r. 1}unyakanok and Roth areSUl)t}orted by NFS grants IIS-98{}1638 an{t SBR-9873450.Re ferencesSteven Alm{',y.
1996.
Partial t)a\]'sing via finite-state cascades.
In l'n, l}~wce, di'ngs of the /~,gS-LLI '95 l?,obust 1)arsi'n9 Worlcsh, op.SMomo Argam(m, Ido l)agan, an(l YllV~t\] Kry-molowsld.
1999.
A memory-1}ased at}proachto learning shalh}w natural anguage patterns.Journal of E:rperimental and Th, eovetical AL11(3).Adam L. Berge, r, SteI}hen A. l)ellaPietra, andVincent J. DellaPietra.
1996.
A inaximumentrol)y apI)roach to natural language pro-cessing.
Computational Linguistics, 22(1).Eric Bri\]l and ,lun Wu.
1998.
Classifier com-bination tbr improved lexical disaml)iguation.In P~vccedings o.f COLING-A 6'15 '98.
Associ-ation for Computational Linguistics.A.
Carlson, C. Cunfl)y, J. Rosen, andD.
l/,oth.
1.999.
The SNoW learning archi-tecture.
Technical Report UIUCDCS-11,-99-2101, UIUC Computer Science Department,May.r httl): / /lcg-www.ui',,.ac.be~/Walter Daelemans, A.ntal van den Bosch, andJakub Zavrel.
1999a.
\])brgetting exceptionsis harmflll in language learning.
MachineLearning, 34(1).Walter Daelemans, Jakub Zavrel, Ko wmderSloot, and Antal van den Bosch.
1999b.TiMBL: Tilb'arg Memory Bused Learner, ver-sion 2.G Rqfi;rence Guide.
ILK Te(:hnicalth',port 99-01. http:// i lk.kub.nl/.Luc Dehaspe.
1997.
Maximum entropy model-ing with clausal constraints, in PTvcecdings oJ'th, c 7th, 1}l, ternational Workshop on ind'uctivcLogic Programming.Hervd Ddjean.
200(I.
Theory refinement andnatural language processing.
In Proceedingsof the ColingEO00.
Association for Computa-tional Linguistics.Mitchell 17 }.
Marcus, Beatrice Santorini, andMary Aim Marcinkiewicz.
1993.
Building alarge mmotated corpus of english: the penntreebank.
Computational Linguistics, 19(2).Marcia Munoz,  Vasin Punyakanok, l)an l l,oth,and Day Zimak.
1999.
A learning ap-t}roa(:h to shallow t)arsing.
In P~vceedings ofEMNLP-WVLC'99.
Asso('iation for Coml)u-tational Linguisti(:s.J.
Ross Quinlan.
1993. c/t.5: Programs for Ma-th,|he Learning.
Morgan Kauflnann.Lance A. Ramshaw and Mitchell P. Marcus.1995.
Text chunking using transformation-l)ase{t learn|Jig.
In 1}roceeding s o\[ the Th, i'rdA CL Worksh, op on Ve, r~.l LacTic Corpora.
As-sociation for Comlmtational Linguistics.D.
Roth.
1.9!t8.
Learning to resolve natural an-guage aml}iguities: A unified approach.
InAAAL98.Erik F. Tjong Kim Sang.
2000.
N{mn phraserecognition by system {:ombination.
In Pro-ceedings of th, e ANLP-NAA CL-2000.
Seattle,Washington, USA.
Morgan Kauflnan Pub-lishers.Hans van Halteren, Jakub Zavrel, and Wal-ter Daelemans.
1998.
Iml)roving data drivenwordclass tagging by system corot}|nation.
InP~veeedings of COLING-ACL '98.
Associa-tion tbr Computational Linguistics.Hans van Halteren, Jakub Zavrel, and WalterDaelemans.
to appear, hnproving accuracyill nlp through coati)|nation ofmachine learn-ing systems.863
