Robust Understanding inMultimodal InterfacesSrinivas Bangalore?AT&T Labs ?
ResearchMichael Johnston?
?AT&T Labs ?
ResearchMultimodal grammars provide an effective mechanism for quickly creating integration andunderstanding capabilities for interactive systems supporting simultaneous use of multipleinput modalities.
However, like other approaches based on hand-crafted grammars, multimodalgrammars can be brittle with respect to unexpected, erroneous, or disfluent input.
In this article,we show how the finite-state approach to multimodal language processing can be extendedto support multimodal applications combining speech with complex freehand pen input, andevaluate the approach in the context of a multimodal conversational system (MATCH).
Weexplore a range of different techniques for improving the robustness of multimodal integrationand understanding.
These include techniques for building effective language models for speechrecognition when little or no multimodal training data is available, and techniques for robustmultimodal understanding that draw on classification, machine translation, and sequence editmethods.
We also explore the use of edit-based methods to overcome mismatches between thegesture stream and the speech stream.1.
IntroductionThe ongoing convergence of the Web with telephony, driven by technologies such asvoice over IP, broadband Internet access, high-speed mobile data networks, and hand-held computers and smartphones, enables widespread deployment of multimodal in-terfaces which combine graphical user interfaces with natural modalities such as speechand pen.
The critical advantage of multimodal interfaces is that they allow user inputand system output to be expressed in the mode or modes to which they are best suited,given the task at hand, user preferences, and the physical and social environment ofthe interaction (Oviatt 1997; Cassell 2001; Andre?
2002; Wahlster 2002).
There is also anincreasing body of empirical evidence (Hauptmann 1989; Nishimoto et al 1995; Cohenet al 1998a; Oviatt 1999) showing user preference and task performance advantages ofmultimodal interfaces.In order to support effective multimodal interfaces, natural language processingtechniques, which have typically operated over linear sequences of speech or text,?
180 Park Avenue, Florham Park, NJ 07932.
E-mail: srini@research.att.com.??
180 Park Avenue, Florham Park, NJ 07932.
E-mail: johnston@research.att.com.Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication:11 July 2008.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 3need to be extended in order to support integration and understanding of multimodallanguage distributed over multiple different input modes (Johnston et al 1997; Johnston1998b).
Multimodal grammars provide an expressive mechanism for quickly creatinglanguage processing capabilities for multimodal interfaces supporting input modessuch as speech and gesture (Johnston and Bangalore 2000).
They support compositemultimodal inputs by aligning speech input (words) and gesture input (representedas sequences of gesture symbols) while expressing the relation between the speechand gesture input and their combined semantic representation.
Johnston and Bangalore(2005) show that such grammars can be compiled into finite-state transducers, enablingeffective processing of lattice input from speech and gesture recognition and mutualcompensation for errors and ambiguities.In this article, we show how multimodal grammars and their finite-state imple-mentation can be extended to support more complex multimodal applications.
Theseapplications combine speech with complex pen input including both freehand gesturesand handwritten input.
More general mechanisms are introduced for representation ofgestures and abstraction over specific content in the gesture stream along with a newtechnique for aggregation of gestures.
We evaluate the approach in the context of theMATCH multimodal conversational system (Johnston et al 2002b), an interactive cityguide.
In Section 2, we present the MATCH application, the architecture of the system,and our experimental method for collection and annotation of multimodal data.
InSection 3, we evaluate the baseline approach on the collected data.The performance of this baseline approach is limited by the use of hand-craftedmodels for speech recognition and multimodal understanding.
Like other approachesbased on hand-crafted grammars, multimodal grammars can be brittle with respect toextra-grammatical, erroneous, and disfluent input.
This is particularly problematic formultimodal interfaces if they are to be used in noisy mobile environments.
To overcomethis limitation we explore a broad range of different techniques for improving therobustness of both speech recognition and multimodal understanding components.For automatic speech recognition (ASR), a corpus-driven stochastic languagemodel(SLM) with smoothing can be built in order to overcome the brittleness of a grammar-based language model.
However, for multimodal applications there is often very littletraining data available and collection and annotation of realistic data can be veryexpensive.
In Section 5, we examine and evaluate various different techniques for rapidprototyping of the language model for the speech recognizer, including transforma-tion of out-of-domain data, grammar sampling, adaptation from wide-coverage gram-mars, and speech recognition models built on conversational corpora (Switchboard).Although some of the techniques presented have been reported in the literature, weare not aware of work comparing the effectiveness of these techniques on the samedomain and using the same data sets.
Furthermore, the techniques are general enoughthat they can be applied to bootstrap robust gesture recognition models as well.
Thepresentation here focuses on speech recognitionmodels, partly due to the greater impactof speech recognition performance compared to gesture recognition performance on themultimodal application described here.
However, in Section 7 we explore the use ofrobustness techniques on gesture input.Although the use of an SLM enables recognition of out-of-grammar utterances,resulting in improved speech recognition accuracy, this may not help overall systemperformance unless the multimodal understanding component itself is made robustto unexpected inputs.
In Section 6, we describe and evaluate several different tech-niques for making multimodal understanding more robust.
Given the success of dis-criminative classification models in related applications such as natural language call346Bangalore and Johnston Robust Understanding in Multimodal Interfacesrouting (Haffner, Tur, and Wright 2003; Gupta et al 2004) and semantic role label-ing (Punyakanok, Roth, and Yih 2005), we first pursue a purely data-driven approachwhere the predicate of a multimodal command and its arguments are determined byclassifiers trained on an annotated corpus of multimodal data.
However, given thelimited amount of data available, this approach does not provide an improvement overthe grammar-based approach.
We next pursue an approach combining grammar anddata where robust understanding is viewed as a statistical machine translation problemwhere out-of-grammar or misrecognized language must be translated to the closestlanguage the system can understand.
This approach provides modest improvementover the grammar-based approach.
Finally we explore an edit-distance approach whichcombines grammar-based understanding with knowledge derived from the underlyingapplication database.
Essentially, if a string cannot be parsed, we attempt to identifythe in-grammar string that it is most similar to, just as in the translation approach.
Thisis achieved by using a finite-state edit transducer to compose the output of the ASRwith the grammar-based multimodal alignment and understanding models.
We havepresented these techniques as methods for improving the robustness of the multimodalunderstanding by processing the speech recognition output.
Given the higher chance oferror in speech recognition compared to gesture recognition, we focus on processing thespeech recognition output to achieve robustmultimodal understanding.
However, thesetechniques are also equally applicable to gesture recognition output.
In Section 7, weexplore the use of edit techniques on gesture input.
Section 8 concludes and discussesthe implications of these results.2.
The MATCH ApplicationUrban environments present a complex and constantly changing body of informa-tion regarding restaurants, cinema and theater schedules, transportation topology, andtimetables.
This information is most valuable if it can be delivered effectively while mo-bile, since users?
needs change rapidly and the information itself is dynamic (e.g., traintimes change and shows get cancelled).
MATCH (Multimodal Access To City Help) is aworking city guide and navigation system that enables mobile users to access restaurantand subway information for urban centers such as New York City and Washington,DC (Johnston et al 2002a, 2002b).
MATCH runs stand-alone on a tablet PC (Figure 1) orin client-server mode across a wireless network.
There is also a kiosk version of thesystem (MATCHkiosk) (Johnston and Bangalore 2004) which incorporates a life-liketalking head.
In this article, we focus on the mobile version of MATCH, in which theuser interacts with a graphical interface displaying restaurant listings and a dynamicmap showing locations and street information.
The inputs can be speech, drawings onthe display with a stylus, or synchronous multimodal combinations of the two modes.The user can ask for reviews, cuisine, phone number, address, or other informationabout restaurants and for subway directions to restaurants and locations.
The systemresponds with graphical callouts on the display, synchronized with synthetic speechoutput.For example, a user can request to see restaurants using the spoken command showcheap italian restaurants in chelsea.
The system will then zoom to the appropriate maplocation and show the locations of restaurants on the map.
Alternatively, the user couldgive the same command multimodally by circling an area on the map and saying showcheap italian restaurants in this neighborhood.
If the immediate environment is too noisy orpublic, the same command can be given completely using a pen stylus as in Figure 2,by circling an area and writing cheap and italian.347Computational Linguistics Volume 35, Number 3Figure 1MATCH on tablet.Similarly, if the user says phone numbers for these two restaurants and circles tworestaurants as in Figure 3(a) [A], the system will draw a callout with the restaurantname and number and say, for example, Time Cafe can be reached at 212-533-7000, foreach restaurant in turn (Figure 3(a) [B]).
If the immediate environment is too noisy orpublic, the same command can be given completely in pen by circling the restaurantsand writing phone (Figure 3(b)).The system also provides subway directions.
For example, if the user says How do Iget to this place?
and circles one of the restaurants displayed on the map the system willaskWhere do you want to go from?.
The user can then respond with speech (for example,25th Street and 3rd Avenue), with pen by writing (for example, 25th St & 3rd Ave), ormultimodally (for example, from here, with a circle gesture indicating the location).The system then calculates the optimal subway route and generates a multimodalpresentation coordinating graphical presentation of each stage of the route with spokeninstructions indicating the series of actions the user needs to take (Figure 4).Map-based systems have been a common application area for exploringmultimodalinteraction techniques.
One of the reasons for this is the effectiveness and naturalnessof combining graphical input to indicate spatial locations with spoken input to specifycommands.
See Oviatt (1997) for a detailed experimental investigation illustrating theFigure 2Unimodal pen command.348Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 3(a) Two area gestures.
(b) Phone command in pen.Figure 4Multimodal subway route.advantages of multimodal input for map-based tasks.
Previous map-based multimodalprototypes can be broken down into two main task domains: map annotation tasks andinformation search tasks.
Systems such as QuickSet (Cohen et al 1998b) focus on the useof speech and pen input in order to annotate the location of features on a map.
Othersystems use speech and pen input to enable users to search and browse for informationthrough direct interaction with a map display.
In the ADAPT system (Gustafson et al2000), users browse for apartments using combinations of speaking and pointing.
In theMultimodal Maps system (Cheyer and Julia 1998), users perform travel planning taskssuch as searching for hotels and points of interest.
MATCH is an information searchapplication providing local search capabilities combined with transportation directions.As such it is most similar to the Multimodal Maps application, though it providesmore powerful and robust language processing and multimodal integration capabili-ties, while the language processing in the Multimodal Maps application is limited tosimple Verb Object Argument constructions (Cheyer and Julia 1998).In the next section we explain the underlying architecture and the series of compo-nents which enable the MATCH user interface.2.1 MATCH Multimodal ArchitectureThe underlying architecture that supports MATCH consists of a series of re-usablecomponents which communicate over IP through a facilitator (MCUBE) (Figure 5).Figure 6 shows the flow of information among components in the system.
In earlier349Computational Linguistics Volume 35, Number 3Figure 5Multimodal architecture.versions of the system, communication was over socket connections.
In later versions ofthe system communication between components uses HTTP.Users interact with the system through a Multimodal User Interface client (MUI)which runs in a Web browser.
Their speech is processed by the WATSON speech recog-nition server (Goffin et al 2005) resulting in aweighted lattice of word strings.When theuser draws on the map their ink is captured and any objects potentially selected, such ascurrently displayed restaurants, are identified.
The electronic ink is broken into a latticeof strokes and sent to both gesture and handwriting recognition components whichFigure 6Multimodal architecture flowchart.350Bangalore and Johnston Robust Understanding in Multimodal Interfacesenrich this stroke lattice with possible classifications of strokes and stroke combinations.The gesture recognizer uses a variant of the template matching approach describedby Rubine (1991).
This recognizes symbolic gestures such as lines, areas, points, arrows,and so on.
The stroke lattice is then converted into an ink lattice which represents all ofthe possible interpretations of the user?s ink as either symbolic gestures or handwrittenwords.
The word lattice and ink lattice are integrated and assigned a combinedmeaningrepresentation by the multimodal integration and understanding component (Johnstonand Bangalore 2000; Johnston et al 2002b).
Because we implement this componentusing finite-state transducers, we refer to this component as the Multimodal Finite StateTransducer (MMFST).
The approach used in the MMFST component for integratingand interpreting multimodal inputs (Johnston et al 2002a, 2002b) is an extension ofthe finite-state approach previously proposed (Bangalore and Johnston 2000; Johnstonand Bangalore 2000, 2005).
(See Section 3 for details.)
This provides as output alattice encoding all of the potential meaning representations assigned to the user?sinput.
The meaning is represented in XML, facilitating parsing and logging by othersystem components.
MMFST can receive inputs and generate outputs using multiplecommunication protocols, including the W3C EMMA standard for representation ofmultimodal inputs (Johnston et al 2007).
Themeaning lattice is flattened to an n-best listand passed to a multimodal dialog manager (MDM) (Johnston et al 2002b), which re-ranks the possible meanings in accordance with the current dialogue state.
If additionalinformation or confirmation is required, the MDM enters into a short informationgathering dialogue with the user.
Once a command or query is complete, it is passedto the multimodal generation component (MMGEN), which builds amultimodal scoreindicating a coordinated sequence of graphical actions and TTS prompts.
This score ispassed back to the MUI.
The MUI then coordinates presentation of graphical contentwith synthetic speech output using the AT&T Natural Voices TTS engine (Beutnagelet al 1999).
The subway route constraint solver (SUBWAY) is a backend server built forthe prototype which identifies the best route between any two points in the city.In the given example where the user says phone for these two restaurantswhile circlingtwo restaurants (Figure 3(a) [A]), assume the speech recognizer returns the lattice inFigure 7 (Speech).
The gesture recognition component also returns a lattice (Figure 7,Gesture) indicating that the user?s ink is either a selection of two restaurants or a geo-graphical area.
The multimodal integration and understanding component (MMFST)combines these two input lattices into a lattice representing their combined meaning(Figure 7, Meaning).
This is passed to the multimodal dialog manager (MDM) and fromthere to the MUI where it results in the display in Figure 3(a) [B] and coordinated TTSoutput.The multimodal integration and understanding component utilizes a declarativemultimodal grammar which captures both the structure and the interpretation of mul-timodal and unimodal commands.
This formalism and its finite-state implementationfor the MATCH system are explained in detail in Section 3.This multimodal grammar is in part derived automatically by reference to an un-derlying ontology of the different kinds of objects in the application.
Specific categoriesin the ontology, such as located entity, are associated with templates and macros thatare used to automatically generate the necessary grammar rules for the multimodalgrammar and to populate classes in a class-based language model (Section 5).
Forexample, in order to add support for a new kind of entity, for example, bars, a categorybar is added to the ontology as a subtype of located entity along with specification of thehead nouns used for this new category, the attributes that apply to it, the symbol to usefor it in the gesture representation, and a reference to the appropriate table to find bars351Computational Linguistics Volume 35, Number 3Figure 7Multimodal example.in the underlying application database.
The appropriate multimodal grammar rules arethen derived automatically as part of the grammar compilation process.
Because thenew entity type bar is assigned the ontology category located entity, the grammar willautomatically support deictic reference to bars with expressions such as this place inaddition to the more specific this bar.In the next section, we explain the data collection procedure we employed in orderto evaluate the system and provide a test set for experimentingwith different techniquesfor multimodal integration and understanding.2.2 Multimodal Data CollectionA corpus of multimodal data was collected in a laboratory setting from a gender-balanced set of 16 first-time novice users.
The subjects were AT&T personnel withno prior knowledge of the system and no experience building spoken or multimodalsystems.
A total of 833 user interactions (218 multimodal/491 speech-only/124 pen-only) resulting from six sample task scenarios involving finding restaurants of varioustypes and getting their names, phones, addresses, or reviews, and getting subwaydirections between locations were collected and annotated.Figure 8 shows the experimental set-up.
Subjects interacted with the system in asoundproof room separated from the experimenter by one-way glass.
Two video feedswere recorded, one from a scan converter connected to the system, the other from acamera located in the subject room, which captured a side-on view of the subject and thedisplay.
The system ran on a Fujitsu tablet computer networked to a desktop PC loggingserver located next to the experimenter.
The subject?s audio inputs were captured usingboth a close-talking headset microphone and a desktop microphone (which capturedboth user input and system audio).As the user interacted with the system a multimodal log in XML format wascaptured on the logging server (Ehlen, Johnston, and Vasireddy 2002).
The log contains adetailed record of the subject?s speech and pen inputs and the system?s internal process-ing steps and responses, with links to the relevant audio files and speech recognitionlattices.352Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 8Experimenter and subject set-up.The experimenter started out each subject with a brief tutorial on the system, show-ing them the pen and how to click on the display in order to turn on themicrophone.
Thetutorial was intentionally vague and broad in scope so the subjects might overestimatethe system?s capabilities and approach problems in new ways.
The experimenter thenleft the subject to complete, unassisted, a series of six sample task scenarios of vary-ing complexity.
These involved finding restaurants of various types and getting theirnames, phones, addresses, or reviews, and getting subway directions between locations.The task scenarios were presented in a GUI on the tablet next to the map display.
Inour pilot testing, we presented users with whole paragraphs describing scenarios.
Wefound that users would often just rephrase the wording given in the paragraph, therebylimiting the utility of the data collection.
Instead, in this data collection we presentedwhat the user had to find as a table (Table 1).
This approach elicited a broader range ofinputs from users.After completing the scenarios the user then completed an online questionnaire onthe tablet regarding their experience with the system.
This consisted of a series of Likertscale questions to measure user satisfaction (Walker, Passonneau, and Boland 2001).After the questionnaire the experimenter came into the experiment room and conductedan informal qualitative post-experiment feedback interview.The next phase of the data collection process was to transcribe and annotate theusers?
input.
Transcription is more complex for multimodal systems than for speech-only systems because the annotator needs not just to hear what the user said but alsoto see what they did.
The browser-based construction of the multimodal user interfaceenabled us to rapidly build a custom version of the system which serves as an onlinemultimodal annotation tool (Figure 9).
This tool extends the approach described inEhlen, Johnston, and Vasireddy (2002) with a graphical interface for construction ofTable 1Example scenario.Use MATCH to find the name, address, and phone number of a restaurant matchingthe following criteria:Food Type LocationVegetarian Union Square353Computational Linguistics Volume 35, Number 3Figure 9Multimodal log annotation tool.gesture annotations and a tool for automatically deriving the meaning annotation forout-of-grammar examples.
This tool allows the annotator to dynamically replay theusers?
inputs and system responses on the interactive map system itself, turn by turn,and add annotations to a multimodal log file, encoded in XML.
The annotation utilizesthe map component of the system (Figure 9(1)).
It provides coordinated playback ofthe subject?s audio with their electronic ink, enabling the user to rapidly annotatemultimodal data without having to replay video of the interaction.
The user interfaceof the multimodal log viewer provides fields for the annotator to transcribe the speechinput, the gesture input, and the meaning.
A series of buttons and widgets are providedto enable the annotator to rapidly and accurately transcribe the user?s gesture and theappropriate meaning representation without having to remember the specifics of thegesture and meaning representations (Figure 9(2)).After transcribing the speech and gesture, the annotator hits a button to confirmthese, and they are recorded in the log and copied down to a second field used forannotating the meaning of the input (Figure 9(3)).
It would be both time consuming anderror-prone to have the annotator code in the meaning representation for each input byhand.
Instead the multimodal understanding system is integrated into the multimodalannotation tool directly.
The interface allows the annotator to adjust the speech andgesture inputs and send them through the multimodal understander until they get themeaning they are looking for (Figure 9(4)).
When the multimodal understander returnsmultiple possibilities an n-best list is presented and the annotator hits the button nextto the appropriate interpretation in order to select it as the annotated meaning.
Wefound this to be a very effective method of annotating meaning, although it does requirethe annotator to have some knowledge of what inputs are acceptable to the system.
Inaddition to annotating the speech, gesture, and meaning, annotators also checked off aseries of flags indicating various properties of the exchange, such as whether the inputwas partial, whether there was a user error, and so on.
The result of this effort was a354Bangalore and Johnston Robust Understanding in Multimodal Interfacescorpus of 833 user interactions all fully annotated with speech, gesture, and meaningtranscriptions.3.
Multimodal Grammars and Finite-State Multimodal Language ProcessingOne of the most critical technical challenges in the development of effective multimodalsystems is that of enabling multimodal language understanding; that is, determining theuser?s intent by integrating and understanding inputs distributed over multiple modes.In early work on this problem (Neal and Shapiro 1991; Cohen 1991, 1992; Brison andVigouroux 1993; Koons, Sparrell, and Thorisson 1993; Wauchope 1994), multimodal un-derstanding was primarily speech-driven,1 treating gesture as a secondary dependentmode.
In these systems, incorporation of information from the gesture input into themultimodal meaning is triggered by the appearance of expressions in the speech inputwhose reference needs to be resolved, such as definite and deictic noun phrases (e.g.,this one, the red cube).
Multimodal integration was essentially a procedural add-on to aspeech or text understanding system.Johnston et al (1997) developed a more declarative approach where multimodalintegration is modeled as unification of typed feature structures (Carpenter 1992) as-signed to speech and gesture inputs.
Johnston (1998a, 1998b) utilized techniques fromnatural language processing (unification-based grammars and chart parsers) to extendthe unification-based approach and enable handling of inputs with more than onegesture, visual parsing, and more flexible and declarative encoding of temporal andspatial constraints.
In contrast to the unification-based approaches, which separatespeech parsing and multimodal integration into separate processing stages, Johnstonand Bangalore (2000, 2005) proposed a one-stage approach to multimodal understandingin which a single grammar specified the integration and understanding of multimodallanguage.
This avoids the complexity of interfacing between separate speech under-standing and multimodal parsing components.
This approach is highly efficient andenables tight coupling with speech recognition, because the grammar can be directlycompiled into a cascade of finite-state transducers which can compose directly withlattices from speech recognition and gesture recognition components.In this section, we explain how the finite-state approach to multimodal languageunderstanding can be extended beyond multimodal input with simple pointing ges-tures made on a touchscreen (as in Johnston and Bangalore [2000, 2005]) to applica-tions such as MATCH with complex gesture input combining freeform drawings withhandwriting recognition.
This involves three significant extensions to the approach: thedevelopment of a gesture representation language for complex pen input combiningfreehand drawing with selections and handwriting (Section 3.1); a new more scalableapproach to abstraction over the specific content of gestures within the finite-statemechanism (Section 3.3); and a new gesture aggregation algorithmwhich enables robusthandling of the integration of deictic phrases with a broad range of different selectiongestures (Section 3.4).
In Section 3.2, we illustrate the use of multimodal grammars forthis application with a fragment of the multimodal grammar for MATCH and illustratehow this grammar is compiled into a cascade of finite-state transducers.
Section 3.5addresses the issue of temporal constraints onmultimodal integration.
In Section 3.6, wedescribe the multimodal dialog management mechanism used in the system and how1 To be more precise, they are ?verbal language?-driven, in that either spoken or typed linguisticexpressions are the driving force of interpretation.355Computational Linguistics Volume 35, Number 3Figure 10Speech lattice.contextual resolution of deictic expressions is accounted for.
In Section 3.7, we evaluatethe performance of this approach to multimodal integration and understanding usingthe multimodal data collected as described in Section 2.2.3.1 Lattice Representations for Gesture and MeaningOne of the goals of our approach to multimodal understanding is to allow for am-biguities and errors in the recognition of the individual modalities to be overcomethrough combination with the other mode (Oviatt 1999; Bangalore and Johnston 2000).To maximize the potential for error compensation, we maintain multiple recognitionhypotheses by representing input modes as weighted lattices of possible recognitionstrings.
For speech input, the lattice is a network of word hypotheses with associatedweights.
Figure 10 presents a simplified speech lattice from the MATCH application.2Representation of Gesture.
Like speech, gesture input can also be represented as a tokenstream, but unlike speech there is no pre-established tokenization of gestures (wordsof a gesture language) other than for handwritten words.
We have developed a gesturerepresentation language for pen input which enables representation of symbolic ges-tures such as areas, lines, and arrows, selection gestures, and handwritten words.
Thislanguage covers a broad range of pen-based input for interactive multimodal applica-tions and can easily be extended to new domains with different gesture symbols.
Eachgesture is represented as a sequence of symbols indicating different characteristics of thegesture.
These symbol sequences can be concatenated in order to represent sequencesof gestures and assembled into a lattice representation in order to represent a range ofpossible segmentations and interpretations of a sequence of ink strokes.
In the MATCHsystem, when the user draws on the map, their ink points are captured along with in-formation about potentially selected items, and these are passed to a gesture processingcomponent.
First, the electronic ink is rotated and scaled and broken into a lattice ofstrokes.
This stroke lattice is processed by both gesture and handwriting recognizersto identify possible pen gestures and handwritten words in the ink stream.
The resultsare combined with selection information to derive the gesture lattice representationspresented in this section.
The gesture recognizer uses a variant of the trained templatematching approach described in Rubine (1991).
The handwriting recognizer is neural-network based.
Table 2 provides the full set of eight gestures supported and the symbolsequences used to represent them in the gesture lattice.2 The lattices in the actual system are weighted but for ease of exposition here we leave out weights inthe figures.356Bangalore and Johnston Robust Understanding in Multimodal InterfacesTable 2Gesture inputs supported.For symbolic gestures and selections, the gesture symbol complexes have the basicform: G FORM MEANING (NUMBER TYPE) SEM.
FORM indicates the physical formof the gesture, and has values such as area, point, line, and arrow.
MEANING providesa rough characterization of the specific meaning of that form; for example, an area canbe either a loc (location) or a sel (selection), indicating the difference between gestureswhich delimit a spatial location on the screen and gestures which select specific dis-played icons.
NUMBER and TYPE are only found with sel.
They indicate the numberof entities selected (1, 2, 3, many) and the specific type of entity (e.g., rest (restaurant) orthtr (theater)).
The TYPE value mix is used for selections of entities of different types.Recognition of inputs as handwritten words is also encoded in the gesture lattice.
Theseare indicated by the sequence G hwWORD.
For example, if the user wrote phone numberthe gesture sequence would be G hw phone G hw number.As an example, if the user draws an area on the screen which contains two restau-rants (as in Figure 3(a) [A]), and the restaurants have associated identifiers id1 and id2,357Computational Linguistics Volume 35, Number 3the gesture lattice will be as in Figure 11.
The first two paths through this gesture latticerepresent the ambiguity between the use of the gesture to indicate a spatial locationversus a selection of objects on the screen.
As defined in the subsequent multimodalgrammar, if the speech is show me chinese restaurants in this neighborhood then the firstpathwill be chosen.
If the speech is tell me about these two restaurants then the second pathwill be chosen.
The third path represents the recognition hypothesis from handwritingrecognition that this is a handwritten O.
If instead the user circles a restaurant and atheatre, the lattice would be as in Figure 12.
If they say tell me about this theater, the thirdpath will be taken.
If they say tell me about these two, the fourth path will be taken.
Thisallows for cases where a user circles several entities and selects a specific one by type.The underlying ontology of the application domain plays a critical role in the han-dling of multimodal expressions.
For example, if place in tell me about this place can referto either a restaurant or a theatre, then it can be aligned with both gesture symbols inthe multimodal grammar.
The noun place is associated in the lexicon with a general typein the ontology: located entity.
When the multimodal grammar is compiled, by virtue ofthis type assignment, the expression this place is associated with gesture representationsfor all of the specific subtypes of located entity in the ontology, such as restaurant andtheater.
The approach also extends to support deictic references to collections of objectsof different types.
For example, the noun building is associated in the lexicon with thetype building.
In the grammar these buildings is associated with the gesture type building.If the user selects a collection of objects of different types they are assigned the typebuilding in the gesture lattice and so the expression these buildingswill pick out that path.In the application domain of our prototype, where restaurants and theaters are the onlyselectable object types, we use a simpler ontology with a single general object type mixfor collections of objects as in Figure 12, and this integrates with spoken phrases such asthese places.Representation of Meaning.
Understanding multimodal language is about extracting themeaning from multimodal utterances.
Although there continue to be endless debates inFigure 11Gesture lattice G: Selection of two restaurants.Figure 12Gesture lattice G: Restaurant and theater.358Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 13Meaning lattice.Figure 14XML meaning representation.linguistics, philosophy, psychology, and neuroscience on what constitutes the meaningof a natural language utterance (Jackendoff 2002), for the purpose of human?computerinteractive systems, ?meaning?
is generally regarded as a representation that can beexecuted by an interpreter in order to change the state of the system.Similar to the input speech and gesture representations, in our approach the outputmeaning is also represented in a lattice format.
This enables compact representationof multiple possible interpretations of the user?s inputs and allows for later stagesof processing, such as the multimodal dialog manager, to use contextual informationto rescore the meaning lattice.
In order to facilitate logging and parsing by othercomponents (dialog manager, backend servers), the meaning representation languageis encoded in XML.3 The meaning lattice resulting from combination of speech andgesture is such that for every path through the lattice, the concatenation of symbolsfrom that path will result in a well-formed XML expression which can be evaluated withrespect to the underlying application semantics.
In the city information application thisincludes elements such as<show>which contains a specification of a kind of restaurantto show, with elements <cuis> (cuisine), <loc> (location), and so on.
Figure 13 showsthe meaning lattice that would result when the speech lattice (Figure 10) combines withthe gesture lattice (Figure 11).The first path through the lattice results from the combination of the speech stringshow chinese restaurants here with an area gesture.
Concatenating the symbols on thispath, we have the well-formed XML expression in Figure 14.3.2 Multimodal Grammars and Finite-State UnderstandingContext-free grammars have generally been used to encode the sequences of inputtokens (words) in a language which are considered grammatical or acceptable for pro-cessing in a single input stream.
In some cases grammar rules are augmented with oper-ations used to simultaneously build a semantic representation of an utterance (Ades and3 In our earlier work (Johnston and Bangalore 2000, 2005), we generated a predicate logic representation,for example: email([person(id1), organization(id2)]).359Computational Linguistics Volume 35, Number 3Steedman 1982; Pollard and Sag 1994; van Tichelen 2004).
Johnston and Bangalore (2000,2005) present a multimodal grammar formalism which directly captures the relation-ship between multiple input streams and their combined semantic representation.
Thenon-terminals in the multimodal grammar are atomic symbols.
The multimodal aspectsof the grammar become apparent in the terminals.
Each terminal contains three compo-nentsW:G:M corresponding to the two input streams and one output stream, whereWis for the spoken language input stream, G is for the gesture input stream, andM is forthe combined meaning output stream.
These correspond to the three representationsdescribed in Section 3.1.
The epsilon symbol () is used to indicate when one of theseis empty within a given terminal.
In addition to the gesture symbols (G area loc ...), Gcontains a symbol SEM used as a placeholder for specific content (see Section 3.3).In Figure 15, we present a fragment of the multimodal grammar used for thecity information application described in this article.
This grammar is simplified forease of exposition.
The rules capture spoken, multimodal, and pen-only commands forshowing restaurants (SHOW), getting information about them (INFO), requesting subwaydirections (ROUTE), and zooming the map (ZOOM).As in Johnston and Bangalore (2000, 2005), this multimodal grammar is com-piled into a cascade of finite-state transducers.
Finite-state machines have been exten-sively applied to many aspects of language processing, including speech recognition(Riccardi, Pieraccini, and Bocchieri 1996; Pereira and Riley 1997), phonology (Kartunnen1991; Kaplan and Kay 1994), morphology (Koskenniemi 1984), chunking (Abney 1991;Joshi and Hopely 1997; Bangalore 1997), parsing (Roche 1999), and machine transla-tion (Bangalore and Riccardi 2000).
Finite-state models are attractive mechanisms forlanguage processing since they are (a) efficiently learnable from data; (b) generallyeffective for decoding; and (c) associated with a calculus for composingmachines whichallows for straightforward integration of constraints from various levels of languageprocessing.
Furthermore, software implementing the finite-state calculus is availablefor research purposes (Noord 1997; Mohri, Pereira, and Riley 1998; Kanthak and Ney2004; Allauzen et al 2007).We compile the multimodal grammar into a finite-state device operating over twoinput streams (speech and gesture) and one output stream (meaning).
The transitionsymbols of the FSA correspond to the terminals of the multimodal grammar.
For thesake of illustration here and in the following examples we will only show the portion ofthe three-tape finite-state device which corresponds to theDEICNP rule in the grammarin Figure 15.
The corresponding finite-state device is shown in Figure 16.
This three-tapemachine is then factored into two transducers: R:G ?
W and T :(G?W)?
M. The Rmachine (e.g., Figure 17) aligns the speech and gesture streams through a compositionwith the speech and gesture input lattices (G o (G:W oW)).
The result of this operationis then factored onto a single tape and composed with the T machine (e.g., Figure 18)in order to map these composite gesture?speech symbols into their combined meaning(G W:M).
Essentially the three-tape transducer is simulated by increasing the alphabetsize by adding composite multimodal symbols that include both gesture and speechinformation.
A lattice of possible meanings is derived by projecting on the output ofG W:M.Because the speech and gesture inputs to multimodal integration and under-standing are represented as lattices, this framework enables mutual compensationfor errors (Johnston and Bangalore 2005); that is, it allows for information from onemodality to be used to overcome errors in the other.
For example, a lower confidencespeech result may be selected through the integration process because it is semanticallycompatible with a higher confidence gesture recognition result.
It is even possible for360Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 15Multimodal grammar fragment.Figure 16Multimodal three-tape FSA.the system to overcome errors in both modalities within a single multimodal utterance.The multimodal composition process prunes out combinations of speech and gesturewhich are not semantically compatible and through combination of weights from thetwo different modalities it provides a ranking of the remaining semantically compatiblecombinations.
This aspect of the approach is not the focus of this article and for ease361Computational Linguistics Volume 35, Number 3Figure 17Gesture/speech alignment transducer.Figure 18Gesture/speech to meaning transducer.of exposition we have left out weights from the examples given.
For the sake of com-pleteness, we provide a brief description of the treatment of weights in the multimodalintegration mechanism.
The speech and gesture lattices contain weights.
These weightsare combined through the process of finite-state composition, so the finite-state deviceresulting from multimodal integration sums the weights from both the input lattices.In order to account for differences in reliability between the speech lattice weights andgesture lattice weights, the weights on the lattices are scaled according to a weightingfactor ?
learned from held-out training data.
The speech lattice is scaled by ?
: 0 < ?
< 1and the gesture lattice by 1?
?.
Potentially this scaling factor could be dynamicallyadapted based on environmental factors and specific users?
performance with the indi-vidual modes, though in the system described here the scaling factor was fixed for theduration of the experiment.3.3 Abstraction over Specific Gesture ContentThe semantic content associated with gesture inputs frequently involves specific infor-mation such as a sequence of map coordinates (e.g., for area gestures) or the identitiesof selected entities (e.g., restaurants or theaters).
As part of the process of multimodalintegration and understanding this specific content needs to be copied from the gesturestream into the resulting combined meaning.
Within the finite-state mechanism, theonlyway to copy content is to havematching symbols on the gesture input andmeaningoutput tapes.
It is not desirable and in some cases infeasible to enumerate all of thedifferent possible pieces of specific content (such as sequences of coordinates) so thatthey can be copied from the gesture input tape to the meaning output tape.
This willsignificantly increase the size of themachine.
In order to capturemultimodal integrationusing finite-state methods, it is necessary to abstract over certain aspects of the gesturalcontent.We introduce here an approach to abstraction over specific gesture content usinga number of additional finite-state operations.
The first step is to represent the gestureinput as a transducer I:Gwhere the input side contains gesture symbols and the specificcontent and the output side contains the same gesture symbols but a reserved symbolSEM appears in place of any specific gestural content such as lists of points or entityidentifiers.
The I:G transducer for the gesture lattice G in Figure 11 is as shown inFigure 19.362Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 19I:G transducer: Two restaurants.Figure 20Gesture lattice G.In any location in the multimodal grammar (Figure 15) and corresponding three-tape finite-state device (Figure 16) where content needs to be copied from the gestureinput into the meaning, the transition :SEM:SEM is used.
In the T :(G?W)?
M(Figure 17) transducer these transitions are labeled SEM :SEM.For composition with the G:W gesture/speech alignment transducer (Figure 18) wetake a projection of the output of the I:G transducer.
For the example I:G transducer(Figure 19) the output projection G is as shown in Figure 20.
This projection operationprovides the abstraction over the specific content.After composing the G andWwith G:W, factoring this transducer into an FSA G Wand composing it with T :(G?W)?
M, we are left with a transducer G W:M. Thistransducer combines a meaning latticeMwith a specification of the gesture and speechsymbols and is used to determine the meaning of G W.The next step is to factor out the speech information (W), resulting in a transducerG:M which relates a meaning lattice M to the gestures involved in determining thosemeaningsG.
This machine can be composedwith the original I:G transducer (I:G o G:M),yielding a transducer I:M. The final step is to read off meanings from the I:M transducer.For each path through the meaning lattice we concatenate symbols from the output Mside, unless theM symbol is SEM in which case we take the input I symbol for that arc.Essentially, the I:G transducer provides an index back from the gesture symbol sequenceassociated with each meaning in the meaning lattice to the specific content associatedwith each gesture.For our example case, if the speech these two restaurants is aligned with the gesturelattice (Figure 20) using R:G ?
W (Figure 18) and the result is then factored andcomposed with T :(G?W)?
M (Figure 17), the resulting G W:M transducer is as inFigure 21.
This is then factored in the G:M transducer Figure 22 and composed with I:G(Figure 19), yielding the I:M transducer shown in Figure 23.Figure 21G W:M transducer.363Computational Linguistics Volume 35, Number 3Figure 22G:M transducer.Figure 23I:M transducer.The meaning is generated by reading off and concatenating meaning symbols fromthe output of the I:M transducer, except for cases in which the output symbol is SEM,where instead the input symbol is taken.
Alternatively, for all arcs in the I:M transducerwhere the output is SEM, the input and output symbols can be swapped (because theinput label represents the value of the SEM variable), and then all paths in M will bethe full meanings with the specific content.
For our example case this results in thefollowing meaning representation: <rest> [r12,r15] </rest>.
This example was only forthe DEICNP subgrammar.
With the full string phone numbers for these two restaurantsthe complete resulting meaning is:<cmd><info><type> phone</type><obj><rest>[r12,r15] </rest> </obj> </info> </cmd>.A critical advantage of this approach is that, because the gesture lattice itself is usedto store the specific contents, the retrieval mechanism scales as the size and complexityof the gesture lattice increases.
In the earlier approach more and more variable namesare required as lattices increase in size, and in all places in the grammar where content iscopied from gesture to meaning, arcs must be present for all of these variables.
Insteadhere we leverage the fact that the gesture lattice itself can be used as a data structurefrom which the specific contents can be retrieved using the finite-state operation ofcomposing I:G and G:M. This has the advantage that the algorithms required for ab-stracting over the specific contents and then reinserting the content are not required,and these operations are instead captured within the finite-state mechanism.
One of theadvantages of this representation of the abstraction is that it encodes not just the type ofeach gesture but also its position within the gesture lattice.3.4 Gesture AggregationJohnston (2000) identifies problems involved inmultimodal understanding and integra-tion of deictic numeral expressions such as these three restaurants.
The problem is that fora particular spoken phrase there are a multitude of different lexical choices of gestureand combinations of gestures that can be used to select the specified plurality of entitiesand all of these need to be integrated with the spoken phrase.
For example, as illustratedin Figure 24, the user might circle all three restaurants with a single pen stroke, circleeach in turn, or circle a group of two and group of one.In the unification-based approach to multimodal parsing (Johnston 1998b), captur-ing all of these possibilities in the spoken language grammar significantly increases itssize and complexity and any plural expression is made massively ambiguous.
The sug-gested alternative in Johnston (2000) is to have the deictic numeral subcategorize for aplurality of the appropriate number and predictively apply a set of gesture combinationrules in order to combine elements of gestural input into the appropriate pluralities.364Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 24Multiple ways to select.In the finite-state approach described here this can be achieved using a process weterm gesture aggregation, which serves as a pre-processing phase on the gesture inputlattice.
A gesture aggregation algorithm traverses the gesture input lattice and addsnew sequences of arcs which represent combinations of adjacent gestures of identicaltype.
The operation of the gesture aggregation algorithm is described in pseudo-codein Algorithm 1.
The function plurality() retrieves the number of entities in a selectiongesture; for example, for a selection of two entities g1, plurality(g1) = 2.
The functiontype() yields the type of the gesture; for example rest for a restaurant selection gesture.The function specific content() yields the specific IDs.Algorithm 1 Gesture aggregation.P ?
the list of all paths through the gesture lattice GLwhile P = ?
dop ?
pop(P)G ?the list of gestures in path pi ?
1while i < length(G) doif g[i] and g[i+ 1] are both selection gestures thenif type(g[i]) == type(g[i+ 1]) thenplurality ?
plurality(g[i])+ plurality(g[i+ 1)start ?
start state(g[i])end ?
end state(g[i+ 1])type ?
type(g[i])specific ?
append(specific content(g[i]), specific content(g[i+ 1])g?
?
G area sel plurality type specificAdd g?
to GL starting at state start and ending at state endp?
?
the path p but with the arcs from start to end replaced with g?push p?
onto Pi ?
i+ 1end ifend ifend whileend while365Computational Linguistics Volume 35, Number 3Essentially what this algorithm does is perform closure on the gesture lattice of afunction which combines adjacent gestures of identical type.
For each pair of adjacentgestures in the lattice which are of identical type, a new gesture is added to the lattice.This new gesture starts at the start state of the first gesture and ends at the end state ofthe second gesture.
Its plurality is equal to the sum of the pluralities of the combininggestures.
The specific content for the new gesture (lists of identifiers of selected objects)results from appending the specific contents of the two combining gestures.
This oper-ation feeds itself so that sequences of more than two gestures of identical type can becombined.For our example case of three selection gestures on three different restaurantsas in Figure 24(2), the gesture lattice before aggregation is as in Figure 25(a).
Afteraggregation the gesture lattice is as in Figure 25(b).
Three new sequences of arcs havebeen added.
The first, from state 3 to state 8, results from the combination of the firsttwo gestures; the second, from state 14 to state 24, from the combination of the last twogestures; and the third, from state 3 to state 24, from the combination of all three ges-tures.
The resulting lattice after the gesture aggregation algorithm has applied is shownin Figure 25(b).
Note that minimization has been applied to collapse identical paths.A spoken expression such as these three restaurants is aligned with the gesturesymbol sequence G area sel 3 rest SEM in the multimodal grammar.
This will be ableto combine not just with a single gesture containing three restaurants but also with ourexample gesture lattice, since aggregation adds the path: G area sel 3 rest [id1,id2,id3].We term this kind of aggregation type specific aggregation.
The aggregation processcan be extended to support type non-specific aggregation for cases where users refer to setsof objects of mixed types and select them using multiple gestures.
For example in thecase where the user says tell me about these two and circles a restaurant and then a theater,non-type specific aggregation applies to combine the two gestures into an aggregate ofmixed type G area sel 2 mix [(id1,id2)] and this is able to combine with these two.
Forapplications with a richer ontology with multiple levels of hierarchy, the type non-specificaggregation should assign to the aggregate to the lowest common subtype of the setof entities being aggregated.
In order to differentiate the original sequence of gesturesthat the user made from the aggregate, paths added through aggregation are assignedadditional cost.Figure 25(c) shows how these new processes of gesture abstraction and aggregationintegrate into the overall finite-state multimodal language processing cascade.
Aggre-gation applies to the I:G representation of the gesture.
A projection G on the I:G iscomposed with the gesture/speech alignment transducer R:G ?
W, then the result iscomposed with the speech lattice.
The resulting G:W transducer is factored into an FSAwith a composite alphabet of symbols.
This is then composed with the T :(G?W)?
Myielding a result transducer G W:M. The speech is factored out of the input yieldingG:M which can then be composed with I:G, yielding a transducer I:M from which thefinal meanings can be read.3.5 Temporal Constraints on Multimodal IntegrationIn the approach taken here, temporal constraints for speech and gesture alignmentare not needed within the multimodal grammar itself.
Bellik (1995, 1997) providesexamples indicating the importance of precise temporal constraints for proper interpre-tation of multimodal utterances.
Critically, though, Bellik?s examples involve not single366Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 25(a) Three gestures.
(b) Aggregated lattice.
(c) Multimodal language processing cascade.367Computational Linguistics Volume 35, Number 3multimodal utterances but sequences of two utterances.4 The multimodal integrationmechanism andmultimodal grammars described herein enumerate the content of singleturns of user input, be they unimodal or multimodal.
The multimodal integrationcomponent and multimodal grammars are not responsible for combination of contentfrom different modes that occur in separate dialog turns.
This is treated as part of dialogmanagement and reference resolution.
Temporal constraints do, however, play a role insegmenting parallel multimodal input streams into single user turns.
This is one of thefunctions of the multimodal understanding component.
In order to determine whichgestures and speech should be considered part of a single user utterance, a dynamictimeout adaptation mechanismwas used.
In initial versions of the system, fixed timeoutintervals were used on receipt of input from one modality to see if the input is in factunimodal or whether input in the other modality is forthcoming.
In pilot studies wedetermined that the system latency introduced by these timeouts could be significantlyreduced by making the timeouts sensitive to activity in the other mode.
In additionto messages containing the results of speech recognition and gesture processing, weinstrumented the multimodal understanding component (MMFST) to receive eventsindicating when the pen first touches the screen (pen-down event) and when the click-to-speak button is pressed (click-to-speak event).
When theMMFST component receivesa speech lattice, if a gesture lattice has already been received then the two lattices areprocessed immediately as a multimodal input.
If gesture has not yet been receivedand there is no pen-down event, the multimodal component waits for a short timeoutinterval before interpreting the speech as a unimodal input.
If gesture has not beenreceived, but there has been a pen-down event, the multimodal component will waitfor a longer timeout period for the gesture lattice message to arrive.
Similarly, whengesture is received, if the speech lattice has already been received the two are integratedimmediately.
If speech has not yet arrived, and there was no click-to-speak event, thenthe systemwill wait for a short timeout before processing the gesture lattice as unimodalinput.
If speech has not yet arrived but the click-to-speak event has been received thenthe component will wait for the speech lattice to arrive for a longer timeout period.Longer timeouts are used instead of waiting indefinitely to account for cases wherethe speech or gesture processing does not return a result.
In pilot testing we determinedthat with the adaptive mechanism the short timeouts could be kept as low as a second orless, significantly reducing system latency for unimodal inputs.
With the non-adaptivemechanism we required timeouts of as much as two to three seconds.
For the longertimeouts we found 15 seconds to be an appropriate time period.
A further extension ofthis approach would be to make the timeout mechanism adapt to specific users, sinceempirical studies have shown that users tend to fall into specific temporal integrationpatterns (Oviatt, DeAngeli, and Kuhn 1997).The adaptive timeout mechanism could also be used with other speech activationmechanisms.
In an ?open microphone?
setting where there is no explicit click-to-speakevent, voice activity detection could be used to signal that a speech event is forthcom-ing.
For our application we chose a ?click-to-speak?
strategy over ?open microphone?because it is more robust to noise andmobile multimodal interfaces are intended for usein environments subject to noise.
The other alternative, ?click-and-hold,?
where the userhas to hold down a button for the duration of their speech, is also problematic becauseit limits the ability of the user to use pen input while they are speaking.4 See Johnston and Bangalore (2005) for a detailed explanation.368Bangalore and Johnston Robust Understanding in Multimodal Interfaces3.6 Multimodal Dialog Management and Contextual ResolutionThemultimodal dialog manager (MDM) is based on previous work on speech-act basedmodels of dialog (Rich and Sidner 1998; Stent et al 1999).
It uses a Java-based toolkitfor writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al1999).
It includes several rule-based processes that operate on a shared state.
The stateincludes system and user intentions and beliefs, a dialog history and focus space, andinformation about the speaker, the domain, and the available modalities.
The processesinclude interpretation, update, selection, and generation.The interpretation process takes as input an n-best list of possible multimodalinterpretations for a user input from the MMFST.
It rescores them according to a setof rules that encode the most likely next speech act given the current dialogue context,and picks the most likely interpretation from the result.
The update process updatesthe dialogue context according to the system?s interpretation of user input.
It augmentsthe dialogue history, focus space, models of user and system beliefs, and model of userintentions.
It also alters the list of current modalities to reflect those most recently usedby the user.The selection process determines the system?s next move(s).
In the case of a com-mand, request, or question, it first checks that the input is fully specified (using thedomain ontology, which contains information about required and optional roles fordifferent types of actions); if it is not, then the system?s next move is to take theinitiative and start an information-gathering subdialogue.
If the input is fully specified,the system?s next move is to perform the command or answer the question; to do this,MDM communicates directly with the UI.The generation process performs template-based generation for simple responsesand updates the system?s model of the user?s intentions after generation.
A text plan-ning component (TEXTPLAN) is used for more complex generation, such as the gener-ation of comparisons (Walker et al 2002, 2004).In the case of a navigational query, such as the example in Section 2, MDM firstreceives a route query in which only the destination is specified: How do I get to thisplace?.
In the selection phase it consults the domain ontology and determines that asource is also required for a route.
It adds a request to query the user for the source to thesystem?s next moves.
This move is selected and the generation process selects a promptand sends it to the TTS component.
The system asks Where do you want to go from?.
Ifthe user says or writes 25th Street and 3rd Avenue then the MMFST will assign this inputtwo possible interpretations: either this is a request to zoom the display to the specifiedlocation or it is an assertion of a location.
Because theMDMdialogue state indicates thatit is waiting for an answer of the type location, MDM reranks the assertion as the mostlikely interpretation.
A generalized overlay process (Alexandersson and Becker 2001)is used to take the content of the assertion (a location) and add it into the partial routerequest.
The result is determined to be complete.
The UI resolves the location to mapcoordinates and passes on a route request to the SUBWAY component.We found this traditional speech-act based dialogue manager worked well for ourmultimodal interface.
Critical in this was our use of a common semantic representationacross spoken, gestured, andmultimodal commands.
The majority of the dialogue rulesoperate in a mode-independent fashion, giving users flexibility in the mode they chooseto advance the dialogue.One of the roles of the multimodal dialog manager is to handle contextual res-olution of deictic expressions.
Because they can potentially be resolved either by in-tegration with a gesture, or from context, deictic expressions such as this restaurant are369Computational Linguistics Volume 35, Number 3ambiguous in the multimodal grammar.
There will be one path through the grammarwhere this expression is associated with a sequence of gesture symbols, such as Garea selection 1 rest r123, and another where it is not associated with any gesture sym-bols and assigned a semantic representation which indicates that it must be resolvedfrom context: <rest><discourseref></discourseref></rest>.
If at the multimodal un-derstanding stage there is a gesture of the appropriate type in the gesture lattice,then the first of these paths will be chosen and the identifier associated with thegesture will be added to the semantics during the multimodal integration and under-standing process: <rest>r123</rest>.
If there is no gesture, then this restaurant willbe assigned the semantic representation <rest><discourseref></discourseref></rest>and the dialog manager will attempt to resolve the gesture from the dialog context.The update process in the multimodal dialog manager maintains a record in the fo-cus space of the last mention of entities of each semantic type, and the last men-tioned entity.
When the interpretation process receives a semantic representationcontaining the marker <rest><discourseref></discourseref></rest> it replaces <dis-courseref></discourseref> with the identifier of the last-mentioned entity of the typerestaurant.Cases where the gesture is a low-confidence recognition result, in fact, wherethe gesture is spurious and not an intentional input, are handled using back-offs inthe multimodal grammar as follows: In the multimodal grammar, productions areadded which consume a gesture from the gesture lattice, but assign the semantics<rest><discourseref></discourseref></rest>.
Generally these are assigned a higher costthan paths through the model where the gesture is meaningful, so that these back-off paths will only be chosen if there is no alternative.
In practice for speech andpen systems of the kind described here, we have found that spurious gestures areuncommon, though they are likely to be considerably more of a problem for other kindsof modalities, such as freehand gesture recognized using computer vision.3.7 Experimental EvaluationTo determine the baseline performance of the finite-state approach to multimodal in-tegration and understanding, and to collect data for the experiments on multimodalrobustness described in this article, we collected and annotated a corpus of multimodaldata as described in Section 2.2.
To enable this initial experiment and data collection,because no corpus data had already been collected, to bootstrap the process we initiallyused a handcrafted multimodal grammar using grammar templates combined withdata from the underlying application database.
As shown in Figure 26, the multimodalgrammar can be used to create language models for ASR, align the speech and gestureresults from the respective recognizers, and transform the multimodal utterance to ameaning representation.
All these operations are achieved using finite-state transduceroperations.For the 709 inputs that involve speech (491 unimodal speech and 218 multimodal)we calculated the speech recognition accuracy (word and sentence level) for resultsusing the grammar-based language model projected from the multimodal grammar.
Wealso calculated a series of measures of concept accuracy on the meaning representationsresulting from taking the results from speech recognition and combining them with thegesture lattice using the gesture speech alignment model, and then the multimodal un-derstanding model.
The concept accuracy measures: Concept Sentence Accuracy, PredicateSentence Accuracy, and Argument Sentence Accuracy are explained subsequently.370Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 26Multimodal grammar compilation for different processes of MATCH.The hierarchically-nested XML representation described in Section 3.1 is effectivefor processing by the backend application, but is not well suited for the automateddetermination of the performance of the language understanding mechanism.
We de-veloped an approach, similar to Ciaramella (1993) and Boros et al (1996), in whichthe meaning representation, in our case XML, is transformed into a sorted flat list ofattribute?value pairs indicating the core contentful concepts of each command.
Theattribute?value meaning representation normalizes over multiple different XML rep-resentations which correspond to the same underlying meaning.
For example, phoneand address and address and phone receive different XML representations but the sameattribute?value representation.
For the example phone number of this restaurant, the XMLrepresentation is as in Figure 27, and the corresponding attribute?value representationis as in Figure 28.Figure 27XML meaning representation.cmd:info type:phone object:selection.
(1)Figure 28Attribute?value meaning representation.371Computational Linguistics Volume 35, Number 3Table 3ASR and concept accuracy for the grammar-based finite-state approach (10-fold).Speech recognition Word accuracy 41.6%Sentence accuracy 38.0%Understanding Concept sentence accuracy 50.7%Predicate accuracy 67.2%Argument accuracy 52.8%This transformation of the meaning representation allows us to calculate the per-formance of the understanding component using string-matching metrics parallel tothose used for speech recognition accuracy.
Concept Sentence Accuracy measuresthe number of user inputs for which the system got the meaning completely right.5Predicate Sentence Accuracymeasures whether the main predicate of the sentence wascorrect (similar to call type in call classification).
Argument Sentence Accuracy is anexact string match between the reference list of arguments and the list of argumentsidentified for the command.
Note that the reference and hypothesized argument se-quences are lexicographically sorted before comparison so the order of the argumentsdoes not matter.
We do not utilize the equivalent of word accuracy on the concept tokensequence.
The concept-level equivalent of word accuracy is problematic because it caneasily be manipulated by increasing or decreasing the number of tokens used in themeaning representation.To provide a baseline for the series of techniques explored in the rest of the article,we performed recognition and understanding experiments on the same 10 partitionsof the data as in Section 4.
The numbers are all averages over all 10 partitions.
Table 3shows the speech recognition accuracy using the grammar-based language model pro-jected from the multimodal grammar.
It also shows the concept accuracy results for themultimodal?grammar-based finite-state approach to multimodal understanding.The multimodal grammars described here provide an expressive mechanism forquickly creating language processing capabilities for multimodal interfaces support-ing input modes such as speech and pen, but like other approaches based on hand-crafted grammars, multimodal grammars are brittle with respect to extra-grammaticalor erroneous input.
The language model directly projected from the speech portion ofthe hand-crafted multimodal grammar is not able to recognize any strings that are notencoded in the grammar.
In our data, 62% of user?s utterances were out of the multi-modal grammar, a major problem for recognition (as illustrated in Table 3).
The poorASR performance has a direct impact on concept accuracy.
The fact that the score forconcept sentence accuracy is higher than that for sentence accuracy is not unexpectedsince recognition errors do not always result in changes in meaning and also to a certainextent the grammar-based language model will force fit out-of-grammar utterances tosimilar in-grammar utterances.4.
Robustness in Multimodal Language ProcessingA limitation of grammar-based approaches to (multimodal) language processing isthat the user?s input is often not covered by the grammar and hence fails to receivean interpretation.
This issue is present in grammar-based speech-only dialog systems5 This metric is called Sentence Understanding in Ciaramella (1993).372Bangalore and Johnston Robust Understanding in Multimodal Interfacesas well.
The lack of robustness in such systems is due to limitations in (a) languagemodeling and (b) understanding of the speech recognition output.The brittleness of using a grammar as a language model is typically alleviated bybuilding SLMs that capture the distribution of the user?s interactions in an applicationdomain.
However, such SLMs are trained on large amounts of spoken interactionscollected in that domain?a tedious task in itself, in speech-only systems, but an ofteninsurmountable task in multimodal systems.
The problem we face is how to makemultimodal systems more robust to disfluent or unexpected multimodal language inapplications for which little or no training data is available.
The reliance on multimodalgrammars as a source of data is inevitable in such situations.
In Section 5, we exploreand evaluate a range of different techniques for building effective SLMs for spokenand multimodal systems under constraints of limited training data.
The techniques arepresented in the context of SLMs, since spoken language interaction tends to be a dom-inant mode in our application and has higher perplexity than the gesture interactions.However, most of these techniques can also be applied to improve the robustness of thegesture recognition component in applications with higher gesture language perplexity.The second source of brittleness in a grammar-based multimodal/unimodal inter-active system is in the assignment of meaning to the multimodal output.
The grammartypically encodes the relation between the multimodal inputs and their meanings.
Theassignment of meaning to a multimodal output is achieved by parsing the utteranceusing the grammar.
In a grammar-based speech-only system, if the language model ofASR is derived directly from the grammar, then every ASR output can be parsed andassigned a meaning by the grammar.
However, using an SLM results in ASR outputsthat may not be parsable by the grammar and hence cannot be assigned a meaning bythe grammar.
Robustness in such cases is achieved by either (a) modifying the parser toaccommodate for unparsable substrings in the input (Ward 1991; Dowding et al 1993;Allen et al 2001) or (b) modifying the meaning representation to make it learnable asa classification task using robust machine learning techniques as is done in large scalehuman-machine dialog systems (e.g., Gorin, Riccardi, and Wright 1997).In our grammar-based multimodal system, the grammar serves as the speech-gesture alignment model and assigns a meaning representation to the multimodalinput.
Failure to parse amultimodal input implies that the speech and gesture inputs arenot fused together and consequently may not be assigned a meaning representation.
Inorder to improve robustness in multimodal understanding, more flexible mechanismsmust be employed in the integration and the meaning-assignment phases.
In Section 6,we explore and evaluate approaches that transform the multimodal input so as tobe parsable by the multimodal grammar, as well as methods that directly map themultimodal input to the meaning representation without the use of the grammar.
Weagain present these approaches in the context of transformation of the ASR output,but they are equally applicable to gesture recognition outputs.
Transformation of themultimodal inputs so as to be parsable by the multimodal grammar directly improvesrobustness of multimodal integration and understanding.Although some of the techniques presented in the next two sections are knownin the literature, they have typically been applied in the context of speech-only dialogsystems and on different application domains.
As a result, comparing the strengths andweaknesses of these techniques is very difficult.
By evaluating them on the MATCHdomain, we are able to compare and extend these techniques for robust multimodalunderstanding.
Other factors such as contextual information including dialog context,graphical display context, geographical context, as well as meta-information such asuser preferences and profiles, can be used to further enhance the robustness of a373Computational Linguistics Volume 35, Number 3multimodal application.
However, here we focus on techniques for improving robust-ness of multimodal understanding that do not rely on such factors.5.
Robustness of Language Models for Speech RecognitionThe problem of speech recognition can be succinctly represented as a search for themost likely word sequence (w?)
through the network created by the composition of alanguage of acoustic observations (O), an acoustic model which is a transduction fromacoustic observations to phone sequences (A), a pronunciation model which is a trans-duction from phone sequences to word sequences (L), and a language model acceptor(G) (Pereira and Riley 1997) (Equation 2).
The language model acceptor encodes the(weighted) word sequences permitted in an application.w?
= argmaxw?2(O ?
A ?
L ?
G)(w) (2)Typically, G is built using either a hand-crafted grammar or using a statistical lan-guagemodel derived from a corpus of sentences from the application domain.
Althougha grammar could bewritten so as to be easily portable across applications, it suffers frombeing too prescriptive and has no metric for the relative likelihood of users?
utterances.In contrast, in the data-driven approach a weighted grammar is automatically inducedfrom a corpus and the weights can be interpreted as a measure of the relative likeli-hoods of users?
utterances.
However, the reliance on a domain-specific corpus is oneof the significant bottlenecks of data-driven approaches, because collecting a corpusspecific to a domain is an expensive and time-consuming task, especially formultimodalapplications.In this section, we investigate a range of techniques for producing a domain-specificcorpus using resources such as a domain-specific grammar as well as an out-of-domaincorpus.
We refer to the corpus resulting from such techniques as a domain-specific de-rived corpus in contrast to a domain-specific collected corpus.
We are interested in techniquesthat would result in corpora such that the performance of language models trained onthese corpora would rival the performance of models trained on corpora collected fora specific domain.
We investigate these techniques in the context of MATCH.
We usethe notation Cd for the corpus, ?d for the language model built using the corpus Cd, andG?d for the language model acceptor representation of the model ?d which can be usedin Equation (2).5.1 Language Model Using In-Domain CorpusWe used the MATCH domain corpus from the data collection to build a class-basedtrigram language model (?MATCH) using the 709 multimodal and speech-only utterancesas the corpus (CMATCH).
We used the names of cuisine types, areas of interest, points ofinterest, and neighborhoods as classes when building the trigram language model.
Thetrigram language model is represented as a weighted finite-state acceptor (Allauzen,Mohri, and Roark 2003) for speech recognition purposes.
The performance of this modelserves as the point of reference to compare the performance of language models trainedon derived corpora.374Bangalore and Johnston Robust Understanding in Multimodal Interfaces5.2 Grammar as Language ModelThe multimodal context-free grammar (CFG; a fragment is presented in Section 2 and alarger fragment is shown in Section 3.2) encodes the repertoire of language and gesturecommands allowed by the system and their combined interpretations.
The CFG canbe approximated by a finite state machine (FSM) with arcs labeled with language,gesture, and meaning symbols, using well-known compilation techniques (Nederhof1997).
Selecting the language symbol of each arc (projecting the FSM on the speechcomponent) results in an FSM that can be used as the language model acceptor (Ggram)for speech recognition.
Note that the resulting languagemodel acceptor is unweighted ifthe grammar is unweighted and suffers from not being robust to language variations inusers?
input.
However, due to the tight coupling of the grammars used for recognitionand interpretation, every recognized string can be assigned a meaning representation(though it may not necessarily be the intended interpretation).5.3 Grammar-Based n-gram Language ModelAs mentioned earlier, a hand-crafted grammar typically suffers from the problem ofbeing too restrictive and inadequate to cover the variations and extra-grammaticality ofusers?
input.
In contrast, an n-gram languagemodel derives its robustness by permittingall strings over an alphabet, albeit with different likelihoods.
In an attempt to providerobustness to the grammar-based model, we created a corpus (Cgram) of k sentences byrandomly sampling the set of paths of the grammar (Ggram)6 and built a class-basedn-gram language model (?gram) using this corpus.
Although this corpus does not rep-resent the true distribution of sentences in the MATCH domain, we are able to derivesome of the benefits of n-gram language modeling techniques.
Similar approaches havebeen presented in Galescu, Ringger, and Allen (1998) and Wang and Acero (2003).5.4 Combining Grammar and CorpusA straightforward extension of the idea of sampling the grammar in order to create acorpus is to select those sentences out of the grammar which make the resulting corpus?similar?
to the corpus collected in the pilot studies.
In order to create this corpus Cclose,we choose the k most likely sentences as determined by a language model (?MATCH)built using the collected corpus.
A mixture model (?mix) with mixture weight (?)
isbuilt by interpolating the model trained on the corpus of extracted sentences (?close)and the model trained on the collected corpus (?MATCH).
This method is summarizedin Equation (4), where L(M) represents the language recognized by the multimodalgrammar (M).Cclose = {S1, .
.
.
Sk|Si ?
L(M) ?
Pr?MATCH (Si) > Pr?MATCH (Si+1)?
 ?
j Pr?MATCH (Si) > Pr?MATCH (Sj) > Pr?MATCH (Si+1) (3)?mix = ?
?
?close + (1?
?)
?
?MATCH (4)6 We can also randomly sample a sub-network without expanding the k paths.375Computational Linguistics Volume 35, Number 35.5 Class-Based Out-of-Domain Language ModelAn alternative to using in-domain corpora for building language models is to ?migrate?a corpus of a different domain to our domain.
The process of migrating a corpusinvolves suitably generalizing the corpus to remove information that is specific onlyto the other domain and instantiating the generalized corpus to our domain.
Althoughthere are a number of ways of generalizing the out-of-domain corpus, the generalizationwe have investigated involved identifying linguistic units, such as noun and verbchunks, in the out-of-domain corpus and treating them as classes.
These classes arethen instantiated to the corresponding linguistic units from the MATCH domain.
Theidentification of the linguistic units in the out-of-domain corpus is done automaticallyusing a supertagger (Bangalore and Joshi 1999).
We use a corpus collected in the contextof a software help-desk application as an example out-of-domain corpus.
In cases wherethe out-of-domain corpus is closely related to the domain at hand, a more semanticallydriven generalization might be more suitable.
Figure 29 illustrates the process of mi-grating data from one domain to another.5.6 Adapting the Switchboard Language ModelWe investigated the performance of a large-vocabulary conversational speech recogni-tion system when applied to a specific domain such as MATCH.
We used the Switch-board corpus (Cswbd) as an example of a large-vocabulary conversational speech corpus.We built a trigrammodel (?swbd) using the 5.4-million-word corpus and investigated theeffect of adapting the Switchboard language model given k in-domain untranscribedspeech utterances ({OiM}).
The adaptation is done by first recognizing the in-domainspeech utterances and then building a language model (?adapt) from the corpus ofrecognized text (Cadapt).
This bootstrapping mechanism can be used to derive a domain-specific corpus and language model without any transcriptions.
Similar techniques forFigure 29A method for migration of data from one domain to another domain.376Bangalore and Johnston Robust Understanding in Multimodal Interfacesunsupervised language model adaptation are presented in Bacchiani and Roark (2003)and Souvignier and Kellner (1998).Cadapt = {S1,S2, .
.
.
,Sk} (5)Si = argmaxS?2(OiM ?
A ?
L ?
Gswbd)(S)5.7 Adapting a Wide-Coverage GrammarThere have been a number of computational implementations of wide-coverage,domain-independent, syntactic grammars for English in various grammar formalisms(Flickinger, Copestake, and Sag 2000; XTAG 2001; Clark and Hockenmaier 2002).
Here,we describe a method that exploits one such grammar implementation in the Lexical-ized Tree-Adjoining Grammar (LTAG) formalism, for deriving domain-specific corpora.An LTAG consists of a set of elementary trees (supertags) (Bangalore and Joshi 1999)each associated with a lexical item (the head).
Supertags encode predicate?argumentrelations of the head and the linear order of its arguments with respect to the head.In Figure 30, we show the supertag associated with the word show in an imperativesentence such as show the Empire State Building.
A supertag can be represented as afinite-state machine with the head and its arguments as arc labels (Figure 31).
The setof sentences generated by an LTAG can be obtained by combining supertags usingsubstitution and adjunction operations.
In related work (Rambow et al 2002), it hasbeen shown that for a restricted version of LTAG, the combinations of a set of supertagscan be represented as an FSM.
This FSM compactly encodes the set of sentences gen-erated by an LTAG grammar.
It is composed of two transducers, a lexical FST, and asyntactic FSM.The lexical FST transduces input words to supertags.
We assume that as input to theconstruction of the lexical machine we have a list of words with their parts-of-speech.Once we have determined for each word the set of supertags they should be associatedFigure 30Supertag tree for the word show.
The NP nodes permit substitution of all supertags with rootnode labeled NP.Figure 31FSM for the word show.
The ?NP arc permits replacement with FSMs representing NP.377Computational Linguistics Volume 35, Number 3with, we create a disjunctive finite-state transducer (FST) for all words which transducesthe words to their supertags.For the syntactic FSM, we take the union of all the FSMs for each supertag whichcorresponds to an initial tree (i.e., a tree which need not be adjoined).
We then performa series of iterative replacements: In each iteration, we replace each arc labeled by asupertag by its lexicalized version of that supertag?s FSM.
Of course, in each iteration,there are many more replacements than in the previous iteration.
Based on the syntacticcomplexity in our domain (such as number of modifiers, clausal embedding, and prepo-sitional phrases), we use five rounds of iteration.
The number of iterations restricts thesyntactic complexity but not the length of the input.
This construction is in many wayssimilar to constructions proposed for CFGs, in particular that of Nederhof (1997).
Onedifference is that, because we start from TAG, recursion is already factored, andwe neednot find cycles in the rules of the grammar.We derive a MATCH domain-specific corpus by constructing a lexicon consistingof pairings of words with their supertags that are relevant to this domain.
We thencompile the grammar to build an FSM of all sentences up to a given depth of recursion.We sample this FSM and build a language model as discussed in Section 5.3.
Givenuntranscribed utterances from a specific domain, we can also adapt the language modelas discussed in Section 5.6.5.8 Speech Recognition ExperimentsWe describe a set of experiments to evaluate the performance of the language modelin the MATCH multimodal system.
We use word accuracy and string accuracy forevaluating ASR output.
All results presented in this section are based on 10-fold cross-validation experiments run on the 709 spoken andmultimodal exchanges collected fromthe pilot study described in Section 2.2.Table 4 presents the performance results for ASR word and sentence accuracy usinglanguage models trained on the collected in-domain corpus as well as on corporaderived using the different methods discussed in Sections 5.2?5.7.
For the class-basedmodels mentioned in the table, we defined different classes based on areas of interest(e.g., riverside park, turtle pond), points of interest (e.g., Ellis Island, United Nations Build-ing), type of cuisine (e.g., Afghani, Indonesian), price categories (e.g., moderately priced,expensive), and neighborhoods (e.g., Upper East Side, Chinatown).It is immediately apparent that the hand-crafted grammar as a language modelperforms poorly and a language model trained on the collected domain-specific corpusperforms significantly better than models trained on derived data.
However, it is en-couraging to note that a model trained on a derived corpus (obtained from combiningthe migrated out-of-domain corpus and a corpus created by sampling the in-domaingrammar) is within 10% word accuracy as compared to the model trained on the col-lected corpus.
There are several other noteworthy observations from these experiments.The performance of the languagemodel trained on data sampled from the grammaris dramatically better as compared to the performance of the hand-crafted grammar.This technique provides a promising direction for authoring portable grammars that canbe sampled subsequently to build robust language models when no in-domain corporaare available.
Furthermore, combining grammar and in-domain data, as described inSection 5.4, outperforms all other models significantly.For the experiment on the migration of an out-of-domain corpus, we used a corpusfrom a software help-desk application.
Table 4 shows that the migration of data using378Bangalore and Johnston Robust Understanding in Multimodal InterfacesTable 4Performance results for ASR word and sentence accuracy using models trained on data derivedfrom different methods of bootstrapping domain-specific data.Scenario ASR Word SentenceAccuracy Accuracy1 Grammar-based Grammar as language model 41.6 38.0(Section 5.2)Class-based n-gram language 60.6 42.9model (Section 5.3)2 In-domain Data Class-based n-gram model 73.8 57.1(Section 5.1)3 Grammar+ Class-based n-gram model 75.0 59.5In-domain Data (Section 5.4)4 Out-of-domain n-gram model 17.6 17.5(Section 5.5) Class-based n-gram model 58.4 38.8Class-based n-gram modelwith Grammar-based n-gramLanguage Model 64.0 45.45 Switchboard n-gram model 43.5 25.0(Section 5.6) Language model trained onrecognized in-domain data 55.7 36.36 Wide-coverage n-gram model 43.7 24.8Grammar Language model trained on(Section 5.7) recognized in-domain data 55.8 36.2linguistic units as described in Section 5.5 significantly outperforms a model trainedonly on the out-of-domain corpus.
Also, combining the grammar sampled corpus withthe migrated corpus provides further improvement.The performance of the Switchboard model on the MATCH domain is presentedin the fifth row of Table 4.
We built a trigram model using a 5.4-million-word Switch-board corpus and investigated the effect of adapting the resulting language model onin-domain untranscribed speech utterances.
The adaptation is done by first runningthe recognizer on the training partition of the in-domain speech utterances and thenbuilding a language model from the recognized text.
We observe that although theperformance of the Switchboard language model on the MATCH domain is poorer thanthe performance of a model obtained by migrating data from a related domain, theperformance can be significantly improved using the adaptation technique.The last row of Table 4 shows the results of using the MATCH specific lexicon togenerate a corpus using a wide-coverage grammar, training a language model, andadapting the resulting model using in-domain untranscribed speech utterances as wasdone for the Switchboard model.
The class-based trigrammodel was built using 500,000randomly sampled paths from the network constructed by the procedure described inSection 5.7.
It is interesting to note that the performance is very similar to the Switch-board model given that the wide-coverage grammar is not designed for conversationalspeech unlike models derived from Switchboard data.
The data from the domain hassome elements of conversational-style speech which the Switchboard model modelswell, but it also has syntactic constructions that are adequately modeled by the wide-coverage grammar.379Computational Linguistics Volume 35, Number 3In this section, we have presented a range of techniques to build language modelsfor speech recognition which are applicable at different development phases of anapplication.
Although the utility of in-domain data cannot be obviated, we have shownthat there are ways to approximate this data with a combination of grammar and out-of-domain data.
These techniques are particularly useful in the initial phases of applicationdevelopment when there is very little in-domain data.
The technique of authoring adomain-specific grammar that is sampled for n-gram model building presents a goodtrade-off between time-to-create and the robustness of the resulting language model.Thismethod can be extended by incorporating suitably generalized out-of-domain data,in order to approximate the distribution of n-grams in the in-domain data.
If time todevelop is of utmost importance, we have shown that using a large out-of-domaincorpus (Switchboard) or a wide-coverage domain-independent grammar can yield areasonable language model.6.
Robust Multimodal UnderstandingIn Section 3, we showed how multimodal grammars can be compiled into finite-statetransducers enabling effective processing of lattice input from speech and gesturerecognition and mutual compensation for errors and ambiguities.
However, like otherapproaches based on hand-crafted grammars, multimodal grammars can be brittlewith respect to extra-grammatical, erroneous, and disfluent input.
Also, the primaryapplications of multimodal interfaces include use in noisymobile environments and useby inexperienced users (for whom they provide a more natural interaction); therefore itis critical that multimodal interfaces provide a high degree of robustness to unexpectedor ill-formed inputs.In the previous section, we presented methods for bootstrapping domain-specificcorpora for the purpose of training robust languagemodels used for speech recognition.Thesemethods overcome the brittleness of a grammar-based languagemodel.
Althoughthe corpus-driven language model might recognize a user?s utterance correctly, therecognized utterance may not be assigned a semantic representation by the multimodalgrammar if the utterance is not part of the grammar.In this section, we address the issue of robustness in multimodal understanding.Robustness in multimodal understanding results from improving robustness to speechrecognition and gesture recognition errors.
Although the techniques in this section arepresented as applying to the output of a speech recognizer, they are equally applicable tothe output of a gesture recognizer.We chose to focus on robustness to speech recognitionerrors because the errors in a gesture recognizer are typically smaller than in a speechrecognizer due to smaller vocabulary and lower perplexity.There have been two main approaches to improving robustness of the under-standing component in the spoken language understanding literature.
First, a parsing-based approach attempts to recover partial parses from the parse chart when theinput cannot be parsed in its entirety due to noise, in order to construct a (partial)semantic representation (Ward 1991; Dowding et al 1993; Allen et al 2001).
Second,a classification-based approach, adopted from the Information Extraction literature,views the problem of understanding as extracting certain bits of information from theinput.
It attempts to classify the utterance and identifies substrings of the input as slot-filler values to construct a frame-like semantic representation.
Both approaches havelimitations.
Although in the first approach the grammar can encode richer semanticrepresentations, the method for combining the fragmented parses is quite ad hoc.
In the380Bangalore and Johnston Robust Understanding in Multimodal Interfacessecond approach, the robustness is derived from training classifiers on annotated data;this data is very expensive to collect and annotate, and the semantic representationis fairly limited.
There is some more recent work on using structured classificationapproaches to transduce sentences to logical forms (Papineni, Roukos, and Ward 1997;Thompson and Mooney 2003; Zettlemoyer and Collins 2005).
However, it is not clearhow to extend these approaches to apply to lattice input?an important requirementfor multimodal processing.6.1 Evaluation IssueBefore we present the methods for robust understanding, we discuss the issue of datapartitions to evaluate these methods on.
Due to the limited amount of data, we runcross-validation experiments in order to arrive at reliable performance estimates forthese methods.
However, we have a choice in terms of how the data is split into trainingand test partitions for the cross-validation runs.
We could randomly split the data foran n-fold (for example, 10-fold) cross-validation test.
However, the data contain severalrepeated attempts by users performing the six scenarios.
A random partitioning ofthese data would inevitably have the same multimodal utterances in training and testpartitions.
We believe that this would result in an overly optimistic estimate of theperformance.
In order to address this issue, we run 6-fold cross-validation experimentsby using five scenarios as the training set and the sixth scenario as the test set.
This wayof partitioning the data overly handicaps data-driven methods because the distributionof data in the training and test partitions for each cross-validation run would be sig-nificantly different.
In the experiment results for each method, we present 10-fold and6-fold cross-validation results where appropriate in order to demonstrate the strengthsand limitations of each method.
For all the experiments in this section, we used a data-driven language model for ASR.
The word accuracy of the ASR is 73.8%, averaged overall scenarios and all speakers.6.2 Classification-Based ApproachIn this approach we view robust multimodal understanding as a sequence of classifica-tion problems in order to determine the predicate and arguments of an utterance.
Theset of predicates are the same set of predicates used in the meaning representation.The meaning representation shown in Figure 28 consists of a predicate (the commandattribute) and a sequence of one or more argument attributes which are the parame-ters for the successful interpretation of the user?s intent.
For example, in Figure 28,cmd:info is the predicate and type:phone object:selection are the arguments to thepredicate.We determine the predicate (c?)
for a N token multimodal utterance (SN1 ) bysearching for the predicate (c) that maximizes the posterior probability as shown inEquation (6).c?
= argmaxcP(c | SN1 ) (6)We view the problem of identifying and extracting arguments from a multimodalinput as a problem of associating each token of the input with a specific tag that encodes381Computational Linguistics Volume 35, Number 3the label of the argument and the span of the argument.
These tags are drawn froma tagset which is constructed by extending each argument label by three additionalsymbols I,O,B, following Ramshaw and Marcus (1995).
These symbols correspond tocases when a token is inside (I) an argument span, outside (O) an argument span, or atthe boundary of two argument spans (B) (See Table 5).Given this encoding, the problem of extracting the arguments amounts to a searchfor the most likely sequence of tags (T?)
given the input multimodal utterance SN1as shown in Equation (7).
We approximate the posterior probability P(T | SN1 ) usingindependence assumptions to include the lexical context in an n-word window and thepreceding two tag labels, as shown in Equation (8).T?
= argmaxTP(T | SN1 ) (7)?
argmaxT?iP(ti | Sii?n,Si+n+1i+1 , ti?1, ti?2) (8)Owing to the large set of features that are used for predicate identification andargument extraction, which typically result in sparseness problems for generative mod-els, we estimate the probabilities using a classification model.
In particular, we usethe Adaboost classifier (Schapire 1999) wherein a highly accurate classifier is built bycombining many ?weak?
or ?simple?
base classifiers fi, each of which may only bemoderately accurate.
The selection of the weak classifiers proceeds iteratively, pickingthe weak classifier that correctly classifies the examples that are misclassified by thepreviously-selected weak classifiers.
Each weak classifier is associated with a weight(wi) that reflects its contribution towards minimizing the classification error.
The pos-terior probability of P(c | x) is computed as in Equation 9.
For our experiments, we usesimple n-grams of the multimodal utterance to be classified as weak classifiers.P(c | x) = 1(1+ e?2?
?i wi?fi(x) )(9)For the experiments presented subsequently we use the data collected from thedomain to train the classifiers.
However, the data could be derived from an in-domaingrammar using techniques similar to those presented in Section 5.Table 5The {I,O,B} encoding for argument extraction.User cheap thai upper west sideUtteranceArgument <price> cheap </price> <cuisine>Annotation thai </cuisine> <place> upper westside </place>IOB cheap price<B> thai cuisine<B>Encoding upper place<I> west place<I>side place<I>382Bangalore and Johnston Robust Understanding in Multimodal Interfaces6.2.1 Experiments and Results.
We used a total of 10 predicates such as help, assert,inforequest, and 20 argument types such as cuisine, price, location for our experiments.These were derived from our meaning representation language.
We used unigrams,bigrams, and trigrams appearing in the multimodal utterance as weak classifiers forthe purpose of predicate classification.
In order to predict the tag of a word for ar-gument extraction, we used the left and right trigram context and the tags for thepreceding two tokens as weak classifiers.
The results are presented in Table 6.
Wepresent the concept sentence accuracy and the predicate and argument string accuracyof the grammar-based understandingmodel and the classification-based understandingmodel.
The corresponding accuracy results on the 10-fold cross-validation experimentsare shown in parentheses.
As can be seen, the grammar-based model significantly out-performs the classification-based approach on the 6-fold cross-validation experimentsand the classification-based approach outperforms the grammar-based approach on the10-fold cross-validation experiments.
This is to be expected since the classification-based approach needs to generalize significantly from the training set to the test set,and these have different distributions of predicates and arguments in the 6-fold cross-validation experiments.A significant shortcoming of the classification approach is that it does not exploitthe semantic grammar from the MATCH domain to constrain the possible choicesfrom the classifier.
Also, the classifier is trained using the data that is collected inthis domain.
However, the grammar is a rich source of distribution-free data.
It isconceivable to sample the grammar in order to increase the training examples for theclassifier, in the same spirit as was done for building a language model using thegrammar (Section 5.3).
Furthermore, knowledge encoded in the grammar and datacan be combined by techniques presented in Schapire et al (2002) to improve classifierperformance.Another limitation of this approach is that it is unclear how to extend it to applyto speech and gesture lattices.
As shown in earlier sections, multimodal understandingreceives ambiguous speech and gesture inputs encoded as lattices.
Mutual disambigua-tion between these two modalities needs to be exploited.
Although the classificationapproach can be extended to apply to n-best lists of speech and gesture inputs, we preferan approach that can apply to lattice inputs directly.6.3 Noisy Channel Model for Error CorrectionIn order to address the limitations of the classification-based approach, we explore analternate method for robust multimodal understanding.
We translate the user?s inputto a string that can be assigned a meaning representation by the grammar.
We canTable 6Concept accuracy results from classification-based model using data-driven language model forASR.
(Numbers are percent for 6-fold cross validation by scenario.
Corresponding percent for10-fold cross validation are given in parentheses.
)Model Concept Predicate ArgumentSentence Sentence SentenceAccuracy % Accuracy % Accuracy %Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)Classification-based 34.0 (58.3) 71.4 (85.5) 32.8 (61.4)383Computational Linguistics Volume 35, Number 3apply this technique on a user?s gesture input as well in order to compensate forgesture recognition errors.We couch the problem of error correction in the noisy channelmodeling framework.
In this regard, we follow Ringger and Allen (1996) and Ristadand Yianilos (1998); however, we encode the error correction model as a weighted FSTso we can directly edit speech/gesture input lattices.
As mentioned earlier, we rely onintegrating speech and gesture lattices to avoid premature pruning of admissible so-lutions for robust multimodal understanding.
Furthermore, unlike Ringger and Allen,the language grammar from our application filters out edited strings that cannot beassigned an interpretation by the multimodal grammar.
Also, whereas in Ringger andAllen the goal is to translate to the reference string and improve recognition accuracy, inour approach the goal is to translate the input in order to assign the reference meaningand improve concept accuracy.We let Sg be the string that can be assigned a meaning representation by thegrammar and Su be the user?s input utterance.
If we consider Su to be the noisy versionof the Sg, we view the decoding task as a search for the string S?g as shown in Equa-tion (10).
Note we formulate this as a joint probability maximization as in Equation (11).Equation (12) expands the sequence probability by the chain rule where Siu and Sigare the ith tokens from Su and Sg respectively.
We use a Markov approximation (limitingthe dependence on the history to the past two time steps: trigram assumption for ourpurposes) to compute the joint probability P(Su,Sg), shown in Equation (13).S?g = argmaxSgP(Sg|Su) (10)= argmaxSgP(Sg,Su) (11)= argmaxSgP(S0u,S0g) ?
P(S1u,S1g|S0u,S0g) .
.
.
?
P(Snu,Sng |S0u,S0g, .
.
.
,Sn?1u ,Sn?1g ) (12)S?g = argmaxSg?P(Siu,Sig|Si?1u ,Si?2u ,Si?1g ,Si?2g ) (13)where Su = S1uS2u .
.
.
Snu and Sg = S1gS2g .
.
.
Smg .In order to compute the joint probability, we need to construct an alignmentbetween tokens (Siu,Sig).
We use the Viterbi alignment provided by the GIZA++toolkit (Och and Ney 2003) for this purpose.
We convert the Viterbi alignment into abilanguage representation that pairs words of the string Su with words of Sg.
A fewexamples of bilanguage strings are shown in Figure 32.
We compute the joint n-grammodel using a language modeling toolkit (Goffin et al 2005).
Equation (13) thus allowsus to edit a user?s utterance to a string that can be interpreted by the grammar.Figure 32A few examples of bilanguage strings.384Bangalore and Johnston Robust Understanding in Multimodal Interfaces6.3.1 Deriving a Translation Corpus.
Because our multimodal grammar is implementedas a finite-state transducer it is fully reversible and can be used not just to provide ameaning for input strings but can also be run in reverse to determine possible inputstrings for a given meaning.
Our multimodal corpus was annotated for meaning usingthemultimodal annotation tools described in Section 2.2.
In order to train the translationmodel we built a corpus that pairs the reference speech string for each utterance inthe training data with a target string.
The target string is derived in two steps.
First,the multimodal grammar is run in reverse on the reference meaning yielding a latticeof possible input strings.
Second, the closest string (as defined by Levenshtein edit-distance [Levenshtein 1966]) in the lattice to the reference speech string is selected asthe target string.6.3.2 FST-Based Decoder.
In order to facilitate editing of ASR lattices, we represent then-gram translation model as a weighted finite-state transducer (Bangalore and Riccardi2002).
We first represent the joint n-gram model as a finite-state acceptor (Allauzenet al 2004).
We then interpret the symbols on each arc of the acceptor as having twocomponents?a word from the user?s utterance (input) and a word from the editedstring (output).
This transformation makes a transducer out of an acceptor.
In doingthis, we can directly compose the editing model (?MT) with ASR lattices (?S) to producea weighted lattice of edited strings.
We further constrain the set of edited strings to thosethat are interpretable by the grammar.
We achieve this by composing with the languagefinite-state acceptor derived from the multimodal grammar (?G) and searching for thebest edited string, as shown in Equation (14).S?MT = argmaxS?S ?
?MT ?
?G (14)If we were to apply this approach to input gesture lattices, then the translationmodel would be built from pairings of the gesture recognition output and the corre-sponding gesture string that would be interpretable by the grammar.
Typical errors ingesture input could include misrecognition of a spurious gesture that ought to havebeen treated as noise (caused, for example, by improper detection of a pen-down event)and non-recognition of pertinent gestures due to early end-pointing of ink input.Figure 33 shows two examples.
In the first example, the unimodal speech utterancewas edited by the model to produce a string that was correctly interpreted by themultimodal grammar.
In the second example, the speech and gesture integration failedand resulted in an empty meaning representation.
However, after the edit on the speechstring, the multimodal utterance was correctly interpreted.6.3.3 Experiments and Results.
Table 7 summarizes the results of the translation model(TM) and compares its accuracy to a grammar-based model.
We provide concept ac-curacy and predicate and argument string accuracy of the translation models appliedto one-best and lattice ASR input.
We also provide concept accuracy results on thelattice of edited strings resulting from applying the translation models to the user?sinput.
As can be seen from the table, the translation models outperform the grammar-based models significantly in all cases.
It is also interesting to note that there is someimprovement in concept accuracy using an ASR lattice over a one-best ASR output withone-best translation output.
However, the improvement is much more significant usinga lattice output from the translation model, suggesting that delaying selection of theedited string until the grammar/domain constraints are applied is paying off.385Computational Linguistics Volume 35, Number 3Figure 33Sample inputs and the edited outputs from the translation model.One of the limitations of the translation model is that the edit operations that arelearned are entirely driven by the parallel data.
However, when the data are limited,as is the case here, the edit operations learned are also restricted.
We would like toincorporate domain-specific edit operations in addition to the ones that are reflected inthe data.
In the next section, we explore this approach.6.4 Edit-Based ApproachIn this section, we extend the approach of translating the ?noisy?
version of the user?sinput to the ?clean?
input to incorporate arbitrary editing operations.
We encode thepossible edits on the input string as an edit FST with substitution, insertion, deletion,and identity arcs.
These operations could be either word-based or phone-based andare associated with a cost.
This allows us to incorporate, by hand, a range of editsthat may not have been observed in the data used in the noisy?channel-based error-correction model.
The edit transducer coerces the set of strings (S) encoded in the latticeresulting from ASR (?S ) to the closest strings in the grammar that can be assignedan interpretation.
We are interested in the string with the least-cost sequence of editsTable 7Concept sentence accuracy of grammar-based and translation-based models with data-drivenlanguage model for ASR.
(Numbers are percent for 6-fold cross validation by scenario.Corresponding percent for 10-fold cross validation are given in parentheses.
)Model Concept Predicate ArgumentSentence Sentence SentenceAccuracy (%) Accuracy (%) Accuracy (%)Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)ASR 1-best/1-best TM 46.1 (61.6) 68.0 (70.5) 47.0 (62.6)ASR 1-best/Lattice TM 50.3 (61.6) 70.5 (70.5) 51.2 (62.6)ASR Lattice/1-best TM 46.7 (61.9) 70.8 (69.9) 47.1 (63.3)ASR Lattice/Lattice TM 54.2 (60.8) 81.9 (67.2) 54.5 (61.6)386Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 34Basic edit machine.
(argmin) that can be assigned an interpretation by the grammar.7 This can be achievedby composition (?)
of transducers followed by a search for the least-cost path through aweighted transducer as shown in Equation (15).s?
= argmins?S?S ?
?edit ?
?g (15)We first describe the machine introduced in Bangalore and Johnston (2004) (Basicedit) then go on to describe a smaller edit machine with higher performance (4-edit) andan edit machine which incorporates additional heuristics (Smart edit).6.4.1 Basic Edit.
The edit machine described in Bangalore and Johnston (2004) is es-sentially a finite-state implementation of the algorithm to compute the Levenshteindistance.
It allows for unlimited insertion, deletion, and substitution of any word foranother (Figure 34).
The costs of insertion, deletion, and substitution are set as equal,except for members of classes such as price (expensive), cuisine (turkish), and so on,which are assigned a higher cost for deletion and substitution.6.4.2 Limiting the Number of Edits.
Basic edit is effective in increasing the number ofstrings that are assigned an interpretation (Bangalore and Johnston 2004) but is quitelarge (15Mb, 1 state, 978,120 arcs) and adds an unacceptable amount of latency (5 sec-onds on average) in processing one-best input and is computationally prohibitive to useon lattices.
In order to overcome these performance limitations, we experimented withrevising the topology of the edit machine so that it allows only a limited number of editoperations (e.g., at most four edits) and removed the substitution arcs, because theygive rise to O(|?|2) arcs, where?is the vocabulary.
Substitution is still possible butrequires one delete and one insert.
For the same grammar, the resulting edit machine isabout 300Kb with 4 states and 16,796 arcs.
The topology of the 4-editmachine is shownin Figure 35.
In addition to 4-edit, we also investigated 6-edit and 8-edit machines whoseresults we report in the subsequent sections.There is a significant savings in bounding the number of edits on the number ofpaths in the resulting lattice.
After composing with the basic edit machine, the latticewould contain O(n ?
|?|) arcs where n is the length of the input being edited.
Forthe bounded k-edit machines this reduces to O(k ?
|?|) arcs and O(|?|k) paths for aconstant k.7 Note that the closest string according to the edit metric may not be the closest string in meaning.387Computational Linguistics Volume 35, Number 3Figure 354-edit machine.6.4.3 Smart Edit.
We incorporate a number of additional heuristics and refinements totune the 4-edit machine based on the underlying application database.i.
Deletion of SLM-only wordsWe add arcs to the edit transducer to allow for freedeletion of words in the SLM training data which are not found in the grammar: forexample, listings in thai restaurant listings in midtown?
thai restaurant in midtown.ii.
Deletion of doubled words A common error observed in SLM output was dou-bling of monosyllabic words: for example, subway to the cloisters recognized as subwayto to the cloisters.
We add arcs to the edit machine to allow for free deletion of any shortword when preceded by the same word.iii.
Extended variable weighting of words Insertion and deletion costs were furthersubdivided from two to three classes: a low cost for ?dispensable?
words, (e.g., please,would, looking, a, the), a high cost for special words (slot fillers, e.g., chinese, cheap,downtown), and a medium cost for all other words, (e.g., restaurant, find).iv.
Auto completion of place names It is unlikely that grammar authors will includeall of the different ways to refer to named entities such as place names.
For example, ifthe grammar includes metropolitan museum of art the user may just say metropolitan mu-seum.
These changes can involve significant numbers of edits.
A capability was addedto the edit machine to complete partial specifications of place names in a single edit.This involves a closed world assumption over the set of place names.
For example, ifthe onlymetropolitan museum in the database is themetropolitan museum of artwe assumethat we can insert of art after metropolitan museum.
The algorithm for construction ofthese auto-completion edits enumerates all possible substrings (both contiguous andnon-contiguous) for place names.
For each of these it checks to see if the substring isfound in more than one semantically distinct member of the set.
If not, an edit sequenceis added to the edit machine which freely inserts the words needed to complete theplace name.
Figure 36 illustrates one of the edit transductions that is added for the placename metropolitan museum of art.
The algorithm which generates the autocomplete editsalso generates new strings to add to the place name class for the SLM (expanded class).In order to limit over-application of the completion mechanism, substrings startingin prepositions (of art ?
metropolitan museum of art) or involving deletion of parts ofabbreviations are not considered for edits (b c building?
n b c building).Note that the application-specific structure and weighting of Smart edit (iii, iv) canbe derived automatically: We use the place-name list for auto completion of place namesand use the domain entities, as determined by which words correspond to fields in theunderlying application database, to assign variable costs to different entities.Figure 36Auto-completion edits.388Bangalore and Johnston Robust Understanding in Multimodal InterfacesTable 8Concept accuracy for different edit models on 6-fold cross-validation experiments using adata-driven language model for ASR.Model Concept Predicate ArgumentSentence Sentence SentenceAccuracy (%) Accuracy (%) Accuracy (%)Grammar-based 38.9 40.3 40.7(No edits)Basic edit 51.5 63.1 52.64-edit 53.0 62.6 53.96-edit 58.2 74.7 59.08-edit 57.8 75.7 58.6Smart 4-edit 60.2 69.9 60.9Smart 6-edit 60.2 73.7 61.3Smart 8-edit 60.9 76.0 61.9Smart edit (exp) 59.7 70.8 60.5Smart edit (exp, lattice) 62.0 73.1 63.0Smart edit (lattice) 63.2 73.7 64.06.4.4 Experiments and Results.We summarize the concept accuracy results from the 6-foldcross-validation experiments using the different edit machines previously discussed.We also repeat the concept accuracy results from the grammar-based model with noedits for a point of comparison.
When compared to the baseline of 38.9% conceptaccuracy without edits (No edits), Basic edit gave a relative improvement of 32%,yielding 51.5% concept accuracy (Table 8).
Interestingly, by limiting the number of editoperations as in 4-edit, we improved the concept accuracy (53%) compared to Basic edit.The reason for this improvement is that for certain input utterances, the Basic editmodelcreates a very large edited lattice and the composition with the grammar fails due tomemory restrictions.8 We also show improvement in concept accuracy by increasingthe number of allowable edit operations (up to 8-edit).
The concept accuracy improveswith increasing number of edits but with diminishing relative improvements.The heuristics in Smart edit clearly improve on the concept accuracy of the basicedit models with a relative improvement of 55% over the baseline.
Smart edit (exp)shows the concept accuracy of Smart edit running on input from an ASR model with theexpanded classes required for auto completion of place names.
Inspection of individualpartitions showed that, while the expanded classes did allow for the correction oferrors on place names, the added perplexity in the ASR model from expanding classesresulted in errors elsewhere and an overall drop in concept accuracy of 0.5% comparedto Smart edit without expanded classes.
Using ASR lattices as input to the edit modelsfurther improved the accuracy to the best concept sentence accuracy score of 63.2%, arelative improvement of 62.5% over the no-edit model.
Lattice input also improved theperformance of Smart edit with the expanded classes from 59.7% to 62%.To summarize, in Table 9 we tabulate the concept accuracy results from the bestperforming configuration of each of the robust understandingmethods discussed in thissection.
It is clear that the techniques such as translation-based edit and Smart edit that canexploit the domain-specific grammar improve significantly over the classification-based8 However, the concept accuracy for the 70% of utterances which are assigned a meaning using the basicedit model was about 73%.389Computational Linguistics Volume 35, Number 3Table 9Summary of concept accuracy results from the different robust understanding techniques usingdata-driven language models for ASR.Model Concept Predicate ArgumentSentence Sentence SentenceAccuracy (%) Accuracy (%) Accuracy (%)Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)(No edits)Classification-based 34.0 (58.3) 71.4 (85.5) 32.8 (61.4)Translation-based edit 54.2 (60.8) 81.9 (67.2) 54.5 (61.6)Smart edit 63.2 (68.4) 73.7 (73.8) 64.0 (69.4)approach.
Furthermore, the heuristics encoded in the smart edit technique that exploitthe domain constraints outperform the translation-based edit technique that is entirelydata-dependent.We also show the results from the 10-fold cross-validation experiments in thetable.
As can be seen there is a significant improvement in concept accuracy for data-driven techniques (classification and translation-based edit) compared to the 6-foldcross-validation experiments.
This is to be expected because the distributions estimatedfrom the training set fit the test data better in the 10-fold experiments as against 6-foldexperiments.Based on the results we have presented in this section, it would be pragmatic torapidly build a hand-crafted grammar-based conversational system that can be maderobust using stochastic language modeling techniques and edit-based understandingtechniques.
Once the system is deployed and data collected, then a judicious balanceof data-driven and grammar-based techniques would maximize the performance of thesystem.7.
Robust Gesture ProcessingGesture recognition has a lower error rate than speech recognition in this application.Even so, gesture misrecognitions and incompleteness of the multimodal grammar inspecifying speech and gesture combinations contribute to the number of utterances notbeing assigned a meaning.
We address the issue of robustness to gesture errors in thissection.We adopted the edit-based technique used on speech utterances to improve ro-bustness of multimodal understanding.
However, unlike a speech utterance, a gesturestring has a structured representation.
The gesture string is represented as a sequence ofattribute?values (e.g., gesture type takes values from {area, line, point, handwriting}) andediting a gesture representation implies allowing for replacements within the value set.We adopted a simple approach that allows for substitution and deletion of values foreach attribute, in addition to the deletion of any gesture.
We did not allow for insertionsof gestures as it is not clear what specific content should be assigned to an insertedgesture.
One of the problems is that if you have, for example, a selection of two itemsand you want to increase it to three selected items, it is not clear a priori which entity toadd as the third item.
We encoded the edit operations for gesture editing as a finite-statetransducer just as we did for editing speech utterances.
Figure 37 illustrates the gestureedit transducer with delc representing the delection cost and substc the substitutioncost.
This method of manipulating the gesture recognition lattice is similar to gesture390Bangalore and Johnston Robust Understanding in Multimodal InterfacesFigure 37A finite-state transducer for editing gestures.aggregation, introduced in Section 3.4.
In contrast to substitution and deletion of ges-tures, gesture aggregation involves insertion of new gestures into the lattice; however,each introduced gesture has a well-definedmeaning based on the combination of valuesof the gestures being aggregated.We evaluated the effectiveness of the gesture edit machine on the MATCH data set.The data consisted of 174 multimodal utterances that were covered by the grammar.
Weused the transcribed speech utterance and the gesture lattice from the gesture recognizeras inputs to the multimodal integration and understanding system.
For 55.4% of theutterances, we obtained the identical attribute?value meaning representation as thehuman-transcribed meaning representation.Applying the gesture edit transducer on the gesture recognition lattices, and thenintegrating the result with the transcribed speech utterance produced a significantimprovement in the accuracy of the attribute?value meaning representation.
For 68.9%of the utterances, we obtained the identical attribute?value meaning representationas the human-transcribed meaning representation, a 22.5% absolute improvement inthe robustness of the system that can be directly attributed to robustness in gestureintegration and understanding.
In future work, we would like to explore learning fromdata how to balance gesture editing and speech editing based on the relative reliabilitiesof the two modalities.8.
ConclusionWe view the contributions of the research presented in this article from two perspec-tives.
First, we have shown how the finite-state approach to multimodal languageprocessing (Johnston and Bangalore 2005) can be extended to support applicationswith complex pen input and how the approach can be made robust through couplingwith a stochastic speech recognition model using translation techniques or finite-stateedit machines.
We have investigated the options available for bootstrapping domain-specific corpora for language models by exploiting domain-specific and wide-coveragegrammars, linguistic generalization of out-of-domain data, and adapting domain-independent corpora.
We have shown that such techniques can closely approximatethe accuracy of speech recognizers trained on domain-specific corpora.
For robust391Computational Linguistics Volume 35, Number 3multimodal understanding we have presented and comparatively evaluated three dif-ferent techniques based on discriminative classification, statistical translation, and editmachines.
We have investigated the strengths and limitations of these approachesin terms of their ability to process lattice input, their ability to exploit constraintsfrom a domain-specific grammar, and their ability to utilize domain knowledge fromthe underlying application database.
The best performing multimodal understandingsystem, using a stochastic ASR model coupled with the smart 4-edit transducer onlattice input, is significantly more robust than the grammar-based system, achieving68.4% concept sentence accuracy (10-fold) on data collected from novice first time usersof a multimodal conversational system.
This is a substantial 35% relative improve-ment in performance compared to 50.7% concept sentence accuracy (10-fold) using thegrammar-based language and multimodal understanding models without edits.
In ourexploration of applying edit techniques to the gesture lattices we saw a 22.5% absoluteimprovement in robustness.The second perspective on the work views it as an investigation of a range oftechniques that balance the robustness provided by data-driven techniques and theflexibility provided by grammar-based approaches.
In the past four decades of speechand natural language processing, both data-driven approaches and rule-based ap-proaches have been prominent at different periods in time.
Moderate-sized rule-basedspoken language models for recognition and understanding are easy to develop andprovide the ability to rapidly prototype conversational applications.
However, scalabil-ity of such systems is a bottleneck due to the heavy cost of authoring and maintenanceof rule sets and inevitable brittleness due to lack of coverage.
In contrast, data-drivenapproaches are robust and provide a simple process of developing applications givenavailability of data from the application domain.
However, this reliance on domain-specific data is also one of the significant bottlenecks of data-driven approaches.
Devel-opment of conversational systems using data-driven approaches cannot proceed untildata pertaining to the application domain is available.
The collection and annotationof such data is extremely time-consuming and tedious, which is aggravated by thepresence of multiple modalities in the user?s input, as in our case.
Also, extending anexisting application to support an additional feature requires adding additional datasets with that feature.
We have shown how a balanced approach where statistical lan-guage models are coupled with grammar-based understanding using edit machines canbe highly effective in a multimodal conversational system.
It is important to note thatthese techniques are equally applicable for speech-only conversational systems as well.Given that the combination of stochastic recognition models with grammar-basedunderstanding models provides robust performance, the question which remains is,after the initial bootstrapping phase, as more data becomes available, should thisgrammar-based approach be replaced with a data-driven understanding component?There are a number of advantages to the hybrid approach we have proposed whichextend beyond the initial deployment of an application.1.
The expressiveness of the multimodal grammar allows us to specifyany compositional relationships and meaning that we want.
The rangeof meanings and their relationship to the input string can be arbitrarilysimple or complex.2.
The multimodal grammar provides an alignment between speechand gesture input and enables multimodal integration of contentfrom different modes.392Bangalore and Johnston Robust Understanding in Multimodal Interfaces3.
With the grammar-based approach it is straightforward to quickly addsupport for new commands to the grammar or change the representationof existing commands.
The only retraining that is needed is for the ASRmodel, and data for the ASR model can either be migrated from anotherrelated domain or derived through grammar sampling.4.
Most importantly, this approach has the significant advantage that itdoes not require annotation of speech data with meaning representationsand alignment of the meaning representations with word strings.
Thiscan be complex and expensive, involving a detailed labeling guide andinstructions for annotators.
In contrast in this approach, if data is used, allthat is needed is transcription of the audio, a far more straightforwardannotation task.
If no data is used then grammar sampling can be usedinstead and no annotation of data is needed whatsoever.5.
Although data-driven approaches to understanding are commonplacein research, rule-based techniques continue to dominate in much of theindustry (Pieraccini 2004).
See, for example, the W3C SRGS standard(www.w3.org/TR/speech-grammar/).AcknowledgmentsWe dedicate this article to the memory ofCandy Kamm whose continued supportfor multimodal research made this workpossible.
We thank Patrick Ehlen, HelenHastie, Preetam Maloor, Amanda Stent,Gunaranjan Vasireddy, Marilyn Walker,and Steve Whittaker for their contributionsto the MATCH system.
We also thankRichard Cox and Mazin Gilbert for theirongoing support of multimodal research atAT&T Labs?Research.
We would also liketo thank the anonymous reviewers fortheir many helpful comments andsuggestions for revision.ReferencesAbney, S. P. 1991.
Parsing by chunks.
InR.
Berwick, S. Abney, and C. Tenny,editors, Principle-Based Parsing.
IEEE,Los Alamitos, CA, pages 257?278.Ades, A. E. and M. Steedman.
1982.
On theorder of words.
Linguistics and Philosophy,4:517?558.Alexandersson, J. and T. Becker.
2001.Overlay as the basic operation fordiscourse processing in a multimodaldialogue system.
In Proceedings of the IJCAIWorkshop: Knowledge and Reasoning inPractical Dialogue Systems, pages 8?14,Seattle, WA.Allauzen, C., M. Mohri, M. Riley, andB.
Roark.
2004.
A generalized constructionof speech recognition transducers.
InInternational Conference on Acoustics, Speech,and Signal Processing, pages 761?764,Montreal.Allauzen, C., M. Mohri, and B. Roark.
2003.Generalized algorithms for constructingstatistical language models.
In Proceedingsof the Association for ComputationalLinguistics, pages 40?47, Sapporo.Allauzen, C., M. Riley, J. Schalkwyk,W.
Skut, and M. Mohri.
2007.
Openfst:A general and efficient weightedfinite-state transducer library.
InProceedings of the Ninth InternationalConference on Implementation and Applicationof Automata, (CIAA 2007), Lecture Notes inComputer Science Vol.
4783, pages 11?23.Springer, Berlin, Heidelberg.Allen, J., D. Byron, M. Dzikovska,G.
Ferguson, L. Galescu, and A. Stent.2001.
Towards conversationalhuman-computer interaction.AI Magazine, 22(4):27?38.Andre?, E. 2002.
Natural language inmultimedia/multimodal systems.In R. Mitkov, editor, Handbook ofComputational Linguistics.
OxfordUniversity Press, Oxford,pages 650?669.Bacchiani, M. and B. Roark.
2003.Unsupervised language modeladaptation.
In Proceedings of theInternational Conference on Acoustics,Speech, and Signal Processing,pages 224?227, Hong Kong.Bangalore, S. 1997.
Complexity of LexicalDescriptions and its Relevance to Partial393Computational Linguistics Volume 35, Number 3Parsing.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Bangalore, S. and M. Johnston.
2000.Tight-coupling of multimodal languageprocessing with speech recognition.In Proceedings of the International Conferenceon Spoken Language Processing,pages 126?129, Beijing.Bangalore, S. and M. Johnston.
2004.Balancing data-driven and rule-basedapproaches in the context of amultimodal conversational system.In Proceedings of the North AmericanAssociation for Computational Linguistics/Human Language Technology, pages 33?40,Boston, MA.Bangalore, S. and A. K. Joshi.
1999.Supertagging: An approach to almostparsing.
Computational Linguistics,25(2):237?265.Bangalore, S. and G. Riccardi.
2000.Stochastic finite-state models for spokenlanguage machine translation.
InProceedings of the Workshop on EmbeddedMachine Translation Systems, pages 52?59,Seattle, WA.Bangalore, S. and G. Riccardi.
2002.Stochastic finite-state models of spokenlanguage machine translation.MachineTranslation, 17(3):165?184.Bellik, Y.
1995.
Interface Multimodales:Concepts, Mode`les et Architectures.
Ph.D.thesis, University of Paris XI (Orsay),France.Bellik, Y.
1997.
Media integration inmultimodal interfaces.
In Proceedingsof the IEEE Workshop on MultimediaSignal Processing, pages 31?36,Princeton, NJ.Beutnagel, M., A. Conkie, J. Schroeter,Y.
Stylianou, and A. Syrdal.
1999.
TheAT&T next-generation TTS.
In JointMeeting of ASA; EAA and DAGA,pages 18?24, Berlin.Boros, M., W. Eckert, F. Gallwitz, G. Go?rz,G.
Hanrieder, and H. Niemann.1996.
Towards understandingspontaneous speech: word accuracyvs.
concept accuracy.
In Proceedings ofthe International Conference on SpokenLanguage Processing, pages 41?44,Philadelphia, PA.Brison, E. and N. Vigouroux.
1993.Multimodal references: A generic fusionprocess.
Technical report, URIT-URACNRS, Universit Paul Sabatier, Toulouse.Carpenter, R. 1992.
The Logic of Typed FeatureStructures.
Cambridge University Press,Cambridge.Cassell, J.
2001.
Embodied conversationalagents: Representation and intelligence inuser interface.
AI Magazine, 22:67?83.Cheyer, A. and L. Julia.
1998.
MultimodalMaps: An Agent-Based Approach.
LectureNotes in Computer Science, 1374:103?113.Ciaramella, A.
1993.
A prototypeperformance evaluation report.
TechnicalReport WP8000-D3, Project Esprit 2218,SUNDIAL.Clark, S. and J. Hockenmaier.
2002.Evaluating a wide-coverage CCG parser.In Proceedings of the LREC 2002, BeyondParseval Workshop, pages 60?66,Las Palmas.Cohen, P. R. 1991.
Integrated interfacesfor decision support with simulation.In Proceedings of the Winter SimulationConference, pages 1066?1072, Phoenix, AZ.Cohen, P. R. 1992.
The role of naturallanguage in a multimodal interface.In Proceedings of the User Interface Softwareand Technology, pages 143?149,Monterey, CA.Cohen, P. R., M. Johnston, D. McGee, S. L.Oviatt, J. Clow, and I. Smith.
1998a.
Theefficiency of multimodal interaction: Acase study.
In Proceedings of the InternationalConference on Spoken Language Processing,pages 249?252, Sydney.Cohen, P. R., M. Johnston, D. McGee, S. L.Oviatt, J. Pittman, I. Smith, L. Chen, andJ.
Clow.
1998b.
Multimodal interaction fordistributed interactive simulation.
InM.
Maybury and W. Wahlster, editors,Readings in Intelligent Interfaces.
MorganKaufmann Publishers, San Francisco, CA,pages 562?571.Dowding, J., J. M. Gawron, D. E. Appelt,J.
Bear, L. Cherny, R. Moore, and D. B.Moran.
1993.
GEMINI: A naturallanguage system for spoken-languageunderstanding.
In Proceedings of theAssociation for Computational Linguistics,pages 54?61, Columbus, OH.Ehlen, P., M. Johnston, and G. Vasireddy.2002.
Collecting mobile multimodaldata for MATCH.
In Proceedings of theInternational Conference on Spoken LanguageProcessing, pages 2557?2560, Denver, CO.Flickinger, D., A. Copestake, and I. Sag.2000.
HPSG analysis of English.In W. Wahlster, editor, Verbmobil:Foundations of Speech-to-Speech Translation.Springer?Verlag, Berlin, pages 254?263.Galescu, L., E. K. Ringger, and J. F. Allen.1998.
Rapid language model developmentfor new task domains.
In Proceedings of theELRA First International Conference on394Bangalore and Johnston Robust Understanding in Multimodal InterfacesLanguage Resources and Evaluation (LREC),pages 807?812, Granada.Goffin, V., C. Allauzen, E. Bocchieri,D.
Hakkani-Tur, A. Ljolje, S. Parthasarathy,M.
Rahim, G. Riccardi, and M. Saraclar.2005.
The AT&T WATSON speechrecognizer.
In Proceedings of theInternational Conference on Acoustics, Speech,and Signal Processing, pages 1033?1036,Philadelphia, PA.Gorin, A. L., G. Riccardi, and J. H. Wright.1997.
How May I Help You?
SpeechCommunication, 23(1-2):113?127.Gupta, N., G. Tur, D. Tur, S. Bangalore,G.
Riccardi, and M. Rahim.
2004.
TheAT&T spoken language understandingsystem.
IEEE Transactions on Speech andAudio Processing, 14(1):213?222.Gustafson, J., L. Bell, J. Beskow, J. Boye,R.
Carlson, J. Edlund, B. Granstro?m,D.
House, and M. Wirn.
2000.
Adapt?a multimodal conversational dialoguesystem in an apartment domain.
InInternational Conference on Spoken LanguageProcessing, pages 134?137, Beijing.Haffner, P., G. Tur, and J. Wright.
2003.Optimizing SVMs for complex callclassification.
In International Conference onAcoustics, Speech, and Signal Processing,pages 632?635, Hong Kong.Hauptmann, A.
1989.
Speech and gesturefor graphic image manipulation.
InProceedings of CHI ?89, pages 241?245,Austin, TX.Jackendoff, R. 2002.
Foundations of Language:Brain, Meaning, Grammar, and EvolutionChapter 9.
Oxford University Press,New York.Johnston, M. 1998a.
Multimodal languageprocessing.
In Proceedings of theInternational Conference on Spoken LanguageProcessing, pages 893?896, Sydney.Johnston, M. 1998b.
Unification-basedmultimodal parsing.
In Proceedings of theAssociation for Computational Linguistics,pages 624?630, Montreal.Johnston, M. 2000.
Deixis and conjunction inmultimodal systems.
In Proceedings of theInternational Conference on ComputationalLinguistics (COLING), pages 362?368,Saarbru?cken.Johnston, M., P. Baggia, D. C. Burnett,J.
Carter, D. Dahl, G. McCobb, andD.
Raggett.
2007.
EMMA: ExtensibleMultiModal Annotation markup language.Technical report, W3C CandidateRecommendation.Johnston, M. and S. Bangalore.
2000.Finite-state multimodal parsing andunderstanding.
In Proceedings of theInternational Conference on ComputationalLinguistics (COLING), pages 369?375,Saarbru?cken.Johnston, M. and S. Bangalore.
2004.Matchkiosk: A multimodal interactive cityguide.
In Proceedings of the Association ofComputational Linguistics (ACL) Poster andDemonstration Session, pages 222?225,Barcelona.Johnston, M. and S. Bangalore.
2005.Finite-state multimodal integration andunderstanding.
Journal of Natural LanguageEngineering, 11(2):159?187.Johnston, M., S. Bangalore, A. Stent,G.
Vasireddy, and P. Ehlen.
2002a.Multimodal language processing formobile information access.
In Proceedingsof the International Conference on SpokenLanguage Processing, pages 2237?2240,Denver, CO.Johnston, M., S. Bangalore, G. Vasireddy,A.
Stent, P. Ehlen, M. Walker, S. Whittaker,and P. Maloor.
2002b.
MATCH: Anarchitecture for multimodal dialogsystems.
In Proceedings of the Association ofComputational Linguistics, pages 376?383,Philadelphia, PA.Johnston, M., P. R. Cohen, D. McGee, S. L.Oviatt, J.
A. Pittman, and I. Smith.1997.
Unification-based multimodalintegration.
In Proceedings of theAssociation of Computational Linguistics,pages 281?288, Madrid.Joshi, A. and P. Hopely.
1997.
A parser fromantiquity.
Journal of Natural LanguageEngineering, 2(4):6?15.Kanthak, S. and H. Ney.
2004.
FSA: AnEfficient and Flexible C++ Toolkitfor Finite State Automata UsingOn-Demand Computation.
In Proceedingsof the Association for ComputationalLinguistics Conference, pages 510?517,Barcelona.Kaplan, R. M. and M. Kay.
1994.
Regularmodels of phonological rule systems.Computational Linguistics, 20(3):331?378.Kartunnen, L. 1991.
Finite-state constraints.In Proceedings of the InternationalConference on Current Issues inComputational Linguistics, UniversitiSains Malaysia, Penang.Koons, D. B., C. J. Sparrell, and K. R.Thorisson.
1993.
Integrating simultaneousinput from speech, gaze, and handgestures.
In M. T. Maybury, editor,Intelligent Multimedia Interfaces.
AAAIPress/MIT Press, Cambridge, MA,pages 257?276.395Computational Linguistics Volume 35, Number 3Koskenniemi, K. K. 1984.
Two-levelMorphology: A General Computation Modelfor Word-form Recognition and Production.Ph.D.
thesis, University of Helsinki.Larsson, S., P. Bohlin, J. Bos, and D. Traum.1999.
TrindiKit manual.
Technical report,TRINDI Deliverable D2.2, GothenburgUniversity, Sweden.Levenshtein, V. I.
1966.
Binary codes capableof correcting deletions, insertion andreversals.
Soviet Physics Doklady,10:707?710.Mohri, M., F. C. N. Pereira, and M. Riley.1998.
A rational design for a weightedfinite-state transducer library.
Lecture Notesin Computer Science, 1436:144?158.Neal, J. G. and S. C. Shapiro.
1991.
Intelligentmulti-media interface technology.
In J. W.Sullivan and S. W. Tyler, editors, IntelligentUser Interfaces.
Addison Wesley, New York,pages 45?68.Nederhof, M. J.
1997.
Regularapproximations of CFLs: A grammaticalview.
In Proceedings of the InternationalWorkshop on Parsing Technology,pages 159?170, Boston, MA.Nishimoto, T., N. Shida, T. Kobayashi,and K. Shirai.
1995.
Improving humaninterface in drawing tool using speech,mouse, and keyboard.
In Proceedings of the4th IEEE International Workshop on Robotand Human Communication, ROMAN95,pages 107?112, Tokyo.Noord, G. 1997.
FSA utilities: A toolboxto manipulate finite-state automata.Lecture Notes in Computer Science,1260:87?108.Och, F. J. and H. Ney.
2003.
A systematiccomparison of various statisticalalignment models.
ComputationalLinguistics, 29(1):19?51.Oviatt, S., A. DeAngeli, and K. Kuhn.
1997.Integration and synchronization ofinput modes during multimodalhuman-computer interaction.
In CHI ?97:Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems,pages 415?422, New York, NY.Oviatt, S. L. 1997.
Multimodal interactivemaps: Designing for human performance.Human-Computer Interaction, 12(1):93?129.Oviatt, S. L. 1999.
Mutual disambiguationof recognition errors in a multimodalarchitecture.
In Proceedings of theConference on Human Factors in ComputingSystems: CHI?99, pages 576?583,Pittsburgh, PA.Papineni, K. A., S. Roukos, and T. R. Ward.1997.
Feature-based languageunderstanding.
In Proceedings ofEuropean Conference on SpeechCommunication and Technology,pages 1435?1438, Rhodes.Pereira, F. C. N. and M. D. Riley.
1997.Speech recognition by composition ofweighted finite automata.
In E. Rocheand Y. Schabes, editors, Finite StateDevices for Natural Language Processing.MIT Press, Cambridge, MA, USA,pages 431?456.Pieraccini, R. 2004.
Spoken languageunderstanding: The research/industrychasm.
In HLT-NAACL 2004 Workshopon Spoken Language Understanding forConversational Systems and Higher LevelLinguistic Information for Speech Processing,Boston, MA.Pollard, C. and I.
A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
Centerfor the Study of Language andInformation, University of ChicagoPress, IL.Punyakanok, V., D. Roth, and W. Yih.
2005.Generalized inference with multiplesemantic role labeling systems sharedtask paper.
In Proceedings of the AnnualConference on Computational NaturalLanguage Learning (CoNLL), pages 181?184,Ann Arbor, MI.Rambow, O., S. Bangalore, T. Butt, A. Nasr,and R. Sproat.
2002.
Creating a finite-stateparser with application semantics.
InProceedings of the International Conference onComputational Linguistics (COLING 2002),pages 1?5, Taipei.Ramshaw, L. and M. P. Marcus.
1995.
Textchunking using transformation-basedlearning.
In Proceedings of the ThirdWorkshop on Very Large Corpora,pages 82?94, Cambridge, MA.Riccardi, G., R. Pieraccini, and E. Bocchieri.1996.
Stochastic automata for languagemodeling.
Computer Speech and Language,10(4):265?293.Rich, C. and C. Sidner.
1998.
COLLAGEN:A collaboration manager for softwareinterface agents.
User Modeling andUser-Adapted Interaction, 8(3?4):315?350.Ringger, E. K. and J. F. Allen.
1996.
Afertility channel model for post-correctionof continuous speech recognition.In International Conference on SpokenLanguage Processing, pages 897?900,Philadelphia, PA.Ristad, E. S. and P. N. Yianilos.
1998.Learning string-edit distance.
IEEETransaction on Pattern Analysis andMachine Intelligence, 20(5):522?532.396Bangalore and Johnston Robust Understanding in Multimodal InterfacesRoche, E. 1999.
Finite-state transducers:parsing free and frozen sentences.
InA.
Kornai, editor, Extended Finite-StateModels of Language.
Cambridge UniversityPress, Cambridge, pages 108?120.Rubine, D. 1991.
Specifying gestures byexample.
Computer Graphics, 25(4):329?337.Schapire, R., M. Rochery, M. Rahim, andN.
Gupta.
2002.
Incorporating priorknowledge in boosting.
In Proceedingsof the Nineteenth International Conferenceon Machine Learning, pages 538?545,Sydney.Schapire, R. E. 1999.
A brief introduction toboosting.
In Proceedings of InternationalJoint Conference on Artificial Intelligence,pages 1401?1406, Stockholm.Souvignier, B. and A. Kellner.
1998.
Onlineadaptation for language models inspoken dialogue systems.
In InternationalConference on Spoken Language Processing,pages 2323?2326, Sydney.Stent, A., J. Dowding, J. Gawron, E. Bratt,and R. Moore.
1999.
The CommandTalkspoken dialogue system.
In Proceedings ofthe 27th Annual Meeting of the Association forComputational Linguistics, pages 183?190,College Park, MD.Thompson, C. A. and R. J. Mooney.
2003.Acquiring word-meaning mappings fornatural language interfaces.
Journal ofArtificial Intelligence Research, 18:1?44.van Tichelen, L. 2004.
Semanticinterpretation for speech recognition.Technical Report W3C.Wahlster, W. 2002.
SmartKom: Fusion andfission of speech, gestures, and facialexpressions.
In Proceedings of the 1stInternational Workshop on Man-MachineSymbiotic Systems, pages 213?225, Kyoto.Walker, M., R. Passonneau, and J. Boland.2001.
Quantitative and qualitativeevaluation of DARPA Communicatorspoken dialogue systems.
In Proceedingsof the 39rd Annual Meeting of theAssociation for Computational Linguistics(ACL/EACL-2001), pages 515?522,Toulouse.Walker, M. A., S. Whittaker, A. Stent,P.
Maloor, J. D. Moore, M. Johnston, andG.
Vasireddy.
2004.
Generation andevaluation of user tailored responses inmultimodal dialogue.
Cognitive Science,28(5):811?840.Walker, M. A., S. J. Whittaker, P. Maloor,J.
D. Moore, M. Johnston, andG.
Vasireddy.
2002.
Speech-Plans:Generating evaluative responses inspoken dialogue.
In Proceedings of theInternational Natural LanguageGeneration Conference, pages 73?80,Ramapo, NY.Wang, Y. and A. Acero.
2003.
Combination ofCFG and n-gram modeling in semanticgrammar learning.
In Proceedings of theEurospeech Conference, pages 2809?2812,Geneva.Ward, W. 1991.
Understanding spontaneousspeech: The Phoenix system.
InProceedings of the International Conferenceon Acoustics, Speech, and Signal Processing,pages 365?367, Washington, DC.Wauchope, K. 1994.
Eucalyptus: Integratingnatural language input with a graphicaluser interface.
Technical ReportNRL/FR/5510?94-9711, NavalResearch Laboratory, Washington, DC.XTAG.
2001.
A lexicalized tree-adjoininggrammar for English.
Technical report,University of Pennsylvania.
Available atwww.cis.upenn.edu/?xtag/gramrelease.html.Zettlemoyer, L. S. and M. Collins.
2005.Learning to map sentences to logicalform: Structured classification withprobabilistic categorial grammars.In Proceedings of the Twenty-FirstConference on Uncertainty in ArtificialIntelligence (UAI-05), pages 658?66,Arlington, VA.397
