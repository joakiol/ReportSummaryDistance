Towards History-based Grammars:Using Richer Models for Probabilistic Parsing*Ezra Black Fred Jelinek John Lafferty David M. MagermanRobert  Mercer Salim RoukosIBM T. J. Watson Research CenterAbstractWe describe a generative probabilistic model ofnatural anguage, which we call HBG, that takesadvantage of detailed linguistic information to re-solve ambiguity.
HBG incorporates lexical, syn-tactic, semantic, and structural information fromthe parse tree into the disambiguation process in anovel way.
We use a corpus of bracketed sentences,called a Treebank, in combination with decisiontree building to tease out the relevant aspects of aparse tree that will determine the correct parse ofa sentence.
This stands in contrast to the usual ap-proach of further grammar tailoring via the usuallinguistic introspection i the hope of generatingthe correct parse.
In head-to-head tests againstone of the best existing robust probabilistic pars-ing models, which we call P-CFG, the HBG modelsignificantly outperforms P-CFG, increasing theparsing accuracy rate from 60% to 75%, a 37%reduction in error.I n t roduct ionAlmost any natural anguage sentence is ambigu-ous in structure, reference, or nuance of mean-ing.
Humans overcome these apparent ambigu-ities by examining the contez~ of the sentence.But what exactly is context?
Frequently, the cor-rect interpretation is apparent from the words orconstituents immediately surrounding the phrasein question.
This observation begs the followingquestion: How much information about the con-text of a sentence or phrase is necessary and suffi-cient to determine its meaning?
This question is atthe crux of the debate among computational lin-guists about the application and implementationof statistical methods in natural anguage under-standing.Previous work on disambiguation a d proba-bilistic parsing has offered partial answers to thisquestion.
Hidden Markov models of words and*Thanks to Philip Resnik and Stanley Chen fortheir valued input.their tags, introduced in (5) and (5) and pop-ularized in the natural language community byChurch (5), demonstrate he power of short-termn-gram statistics to deal with lexical ambiguity.Hindle and Rooth (5) use a statistical measureof lexical associations to resolve structural am-biguities.
Brent (5) acquires likely verb subcat-egorization patterns using the frequencies of verb-object-preposition triples.
Magerman and Mar-cus (5) propose a model of context hat combinesthe n-gram model with information from dominat-ing constituents.
All of these aspects of contextare necessary for disambiguation, yet none is suf-ficient.We propose a probabilistic model of contextfor disambiguation in parsing, HBG, which incor-porates the intuitions of these previous works intoone unified framework.
Let p(T, w~) be the jointprobability of generating the word string w~ andthe parse tree T. Given w~, our parser chooses asits parse tree that tree T* for whichT" =arg  maxp(T, w~) (1)T6~(~)where ~(w~) is the set of all parses produced bythe grammar for the sentence w~.
Many aspects ofthe input sentence that might be relevant o thedecision-making process participate in the prob-abilistic model, providing a very rich if not therichest model of context ever attempted in a prob-abilistic parsing model.In this paper, we will motivate and define theHBG model, describe the task domain, give anoverview of the grammar, describe the proposedHBG model, and present the results of experi-ments comparing HBG with an existing state-of-the-art model.Motivation for History-basedGrammarsOne goal of a parser is to produce a grammaticalinterpretation of a sentence which represents the31syntactic and semantic intent of the sentence.
Toachieve this goal, the parser must have a mecha-nism for estimating the coherence of an interpreta-tion, both in isolation and in context.
Probabilis-tic language models provide such a mechanism.A probabilistic language model attemptsto estimate the probability of a sequenceof sentences and their respective interpreta-tions (parse trees) occurring in the language,:P(SI TI S2 T2 ... S,, T,~).The difficulty in applying probabilistic mod-els to natural anguage is deciding what aspectsof the sentence and the discourse are relevant othe model.
Most previous probabilistic models ofparsing assume the probabilities of sentences in adiscourse are independent of other sentences.
Infact, previous works have made much stronger in-dependence assumptions.
The P-CFG model con-siders the probability of each constituent rule in-dependent of all other constituents in the sen-tence.
The :Pearl (5) model includes a slightlyricher model of context, allowing the probabilityof a constituent rule to depend upon the immedi-ate parent of the rule and a part-of-speech trigramfrom the input sentence.
But none of these mod-els come close to incorporating enough context odisambiguate many cases of ambiguity.A significant reason researchers have limitedthe contextual information used by their mod-els is because of the difficulty in estimating veryrich probabilistic models of context.
In this work,we present a model, the history-based grammarmodel, which incorporates a very rich model ofcontext, and we describe a technique for estimat-ing the parameters for this model using decisiontrees.
The history-based grammar model providesa mechanism for taking advantage of contextualinformation from anywhere in the discourse his-tory.
Using decision tree technology, any questionwhich can be asked of the history (i.e.
Is the sub-ject of the previous entence animate?
Was theprevious entence a question?
etc.)
can be incor-porated into the language model.The  H is tory -based  Grammar  Mode lThe history-based grammar model defines contextof a parse tree in terms of the leftmost derivationof the tree.Following (5), we show in Figure 1 a context-free grammar (CFG) for a'~b "~ and the parse treefor the sentence aabb.
The leftmost derivation ofthe tree T in Figure 1 is:"P1 'r2 'P3 S --~ ASB --* aSB --~ aABB ~-~ aaBB ~-h aabB Y-~(2)where the rule used to expand the i-th node ofthe tree is denoted by ri.
Note that we have in-aabbS ---, ASB IABA ---, aB --~ b(, 6/ ".,4-5.
:a a b bFigure h Grammar and parse tree for aabb.dexed the non-terminal (NT) nodes of the treewith this leftmost order.
We denote by ~- the sen-tential form obtained just before we expand nodei.
Hence, t~ corresponds to the sentential formaSB or equivalently to the string rlr2.
In a left-most derivation we produce the words in left-to-right order.Using the one-to-one correspondence b tweenleftmost derivations and parse trees, we canrewrite the joint probability in (1) as:~r~p(T, w~) = H p(r, \]t\[)i=1In a probabilistic context-free grammar (P-CFG),the probability of an expansion at node i dependsonly on the identity of the non-terminal Ni, i.e.,p(r lq) = Thusv(T, = I Ii ----1So in P-CFG the derivation order does not affectthe probabilistic model 1.A less crude approximation than the usual P-CFG is to use a decision tree to determine whichaspects of the leftmost derivation have a bear-ing on the probability of how node i will be ex-panded.
In other words, the probability distribu-tion p(ri \]t~) will be modeled by p(ri \[E\[t~\]) whereE\[t\] is the equivalence class of the history ~ asdetermined by the decision tree.
This allows our1Note the abuse of notation since we denote byp(ri) the conditional probability of rewriting the non-terminal AT/.32probabilistic model to use any information any-where in the partial derivation tree to determinethe probabil ity of different expansions of the i-thnon-terminal.
The use of decision trees and a largebracketed corpus may shift some of the burden ofidentifying the intended parse from the grammar-ian to the statistical estimation methods.
We referto probabilistic methods based on the derivationas History-based Grammars  (HBG).In this paper, we explored a restricted imple-mentation of this model in which only the pathfrom the current node to the root of the deriva-tion along with the index of a branch (index ofthe child of a parent ) are examined in the decisiontree model to build equivalence classes of histories.Other parts of the subtree are not examined in theimplementation of HBG.\[N It_PPH1 N\]IV indicates_VVZ\[Fn \[Fn~whether_CSW\[N a_AT1 call_NN1 N\]\[V completed_VVD successfully_RR V\]Fn&\]or_CC\[Fn+ iLCSW\[N some_DD error_NN1 N\]@\[V was_VBDZ detected_VVN V\]@\[Fr that_CST\[V caused_VVDIN the_AT call_NN1 N\]\[Ti to_TO fail_VVI Wi\]V\]Fr\]Fn+\]Fn\]V\]._.Figure 2: Sample bracketed sentence from Lan-caster Treebank.Task  DomainWe have chosen computer manuals as a task do-main.
We picked the most frequent 3000 wordsin a corpus of 600,000 words from 10 manuals asour vocabulary.
We then extracted a few mil-lion words of sentences that are completely cov-ered by this vocabulary from 40,000,000 words ofcomputer manuals.
A randomly chosen sentencefrom a sample of 5000 sentences from this corpusis:396.
It indicates whether a call completed suc-cessfully or if some error was detected thatcaused the call to fail.To define what we mean by a correct parse,we use a corpus of manually bracketed sentencesat the University of Lancaster called the Tree-bank.
The Treebank uses 17 non-terminal labelsand 240 tags.
The bracketing of the above sen-tence is shown in Figure 2.A parse produced by the grammar is judgedto be correct if it agrees with the Treebank parsestructurally and the NT labels agree.
The gram-mar has a significantly richer NT label set (morethan 10000) than the Treebank but we have de-fined an equivalence mapping between the gram-mar NT labels and the Treebank NT labels.
Inthis paper, we do not include the tags in the mea-sure of a correct parse.We have used about 25,000 sentences to helpthe grammarian develop the grammar with thegoal that the correct (as defined above) parse isamong the proposed (by the grammar) parses forsentence.
Our most common test set consists of1600 sentences that are never seen by the gram-marian.The  GrammarThe grammar used in this experiment is a broad-coverage, feature-based unification grammar.
Thegrammar is context-free but uses unification to ex-press rule templates for the the context-free pro-ductions.
For example, the rule template:(3): n unspec  : ncorresponds to three CFG productions where thesecond feature : n is either s, p, or : n. This ruletemplate may elicit up to 7 non-terminals.
Thegrammar has 21 features whose range of valuesmaybe from 2 to about 100 with a median of 8.There are 672 rule templates of which 400 are ac-tually exercised when we parse a corpus of 15,000sentences.
The number of productions that arerealized in this training corpus is several hundredthousand.P -CFGWhile a NT in the above grammar is a featurevector, we group several NTs into one class we calla mnemonic  represented by the one NT that isthe least specified in that class.
For example, themnemonic VBOPASTSG* corresponds to all NTsthat unify with:pos - -v  1 v - -  ~.ype = be  (4)tense  - aspect  : pas tWe use these mnemonics to label a parse treeand we also use them to estimate a P-CFG, wherethe probability of rewriting a NT is given by theprobability of rewriting the mnemonic.
So froma training set we induce a CFG from the actualmnemonic productions that are elicited in pars-ing the training corpus.
Using the Inside-Outside33algorithm, we can estimate P-CFG from a largecorpus of text.
But since we also have a largecorpus of bracketed sentences, we can adapt theInside-Outside algorithm to reestimate the prob-ability parameters subject to the constraint hatonly parses consistent with the Treebank (whereconsistency is as defined earlier) contribute to thereestimation.
From a training run of 15,000 sen-tences we observed 87,704 mnemonic productions,with 23,341 NT mnemonics of which 10,302 werelexical.
Running on a test set of 760 sentences 32%of the rule templates were used, 7% of the lexi-cal mnemonics, 10% of the constituent mnemon-ics, and 5% of the mnemonic productions actuallycontributed to parses of test sentences.Grammar  and  Mode l  Per fo rmanceMet r i csTo evaluate the performance of a grammar and anaccompanying model, we use two types of mea-surements:?
the any-consistent rate, defined as the percent-age of sentences for which the correct parse isproposed among the many parses that the gram-mar provides for a sentence.
We also measurethe parse base, which is defined as the geomet-ric mean of the number of proposed parses on aper word basis, to quantify the ambiguity of thegrammar.?
the Viterbi rate defined as the percentage of sen-tences for which the most likely parse is consis-tent.The any-contsistentt ra e is a measure of the gram-mar 's  coverage of linguistic phenomena.
TheViterbi rate evaluates the grammar 's  coveragewith the statistical model imposed on the gram-mar.
The goal of probabilistic modelling is to pro-duce a Viterbi rate close to the anty-contsistentt ra e.The any-consistent rate is 90% when we re-quire the structure and the labels to agree and96% when unlabeled bracketing is required.
Theseresults are obtained on 760 sentences from 7 to 17words long from test material that has never beenseen by the grammarian.
The parse base is 1.35parses/word.
This translates to about 23 parsesfor a 12-word sentence.
The unlabeled Viterbi ratestands at 64% and the labeled Viterbi rate is 60%.While we believe that the above Viterbi rateis close if not the state-of-the-art performance,there is room for improvement by using a more re-fined statistical model to achieve the labeled any-contsistent rate of 90% with this grammar.
Thereis a significant gap between the labeled Viterbiandany-consistent rates: 30 percentage points.Instead of the usual approach where a gram-marian tries to fine tune the grammar in the hopeof improving the Viterbi rate we use the combina-tion of a large Treebank and the resulting deriva-tion histories with a decision tree building algo-r i thm to extract statistical parameters that wouldimprove the Viterbi rate.
The grammarian's  taskremains that of improving the any-consistent rate.The history-based grammar  model is distin-guished from the context-free grammar  model inthat each constituent structure depends not onlyon the input string, but also the entire history upto that point in the sentence.
In HBGs, historyis interpreted as any element of the output struc-ture, or the parse tree, which has already been de-termined, including previous words, non-terminalcategories, constituent structure, and any otherlinguistic information which is generated as partof the parse structure.The HBG ModelUnlike P-CFG which assigns a probabil ity to amnemonic production, the HBG model assigns aprobability to a rule template.
Because of this theHBG formulation allows one to handle any gram-mar formalism that has a derivation process.For the HBG model, we have defined about50 syntactic categories, referred to as Syn, andabout 50 semantic ategories, referred to as Sere.Each NT (and therefore mnemonic) of the gram-mar has been assigned a syntactic (Syn) and asemantic (Sem) category.
We also associate witha non-terminal a pr imary lexical head, denoted byH1, and a secondary lexical head, denoted by H~.
2When a rule is applied to a non-terminal, it indi-cates which child will generate the lexical pr imaryhead and which child will generate the secondarylexical head.The proposed generative model associates foreach constituent in the parse tree the probability:p( Syn, Sern, R, H1, H2\[Synp, Setup, P~, Ipc, Hip, H2p )In HBG, we predict the syntactic and seman-tic labels of a constituent, its rewrite rule, and itstwo lexical heads using the labels of the parentconstituent, the parent's lexical heads, the par-ent's rule P~ that lead to the constituent andthe constituent's index Ipc as a child of R~.
Aswe discuss in a later section, we have also usedwith success more information about the deriva-tion tree than the immediate parent in condition-ing the probabil ity of expanding a constituent.2The primary lexical head H1 corresponds(roughly) to the linguistic notion of a lexicai head.The secondary lexical head H2 has no linguistic par-allel.
It merely represents a word in the constituentbesides the head which contains predictive informationabout the constituent.34We have approximated the above probabilityby the following five factors:1. p(Syn IP~, X~o, X~,  Sy~, Se.~)2. p( Sern ISyn, Rv, /pc, Hip, H2p, Synp, Sern; )3. p( R \]Syn, Sem, 1~, Ipc, Hip, H2p, Synp, Semi)4. p(H  IR, Sw, Sere, I o,5.
p(n2 IH1,1< Sy , Sere, Ipc, Sy, p)While a different order for these predictions i pos-sible, we only experimented with this one.Parameter  Es t imat ionWe only have built a decision tree to the rule prob-ability component (3) of the model.
For the mo-ment, we are using n-gram models with the usualdeleted interpolation for smoothing for the otherfour components of the model.We have assigned bit strings to the syntacticand semantic ategories and to the rules manually.Our intention is that bit strings differing in theleast significant bit positions correspond to cate-gories of non-terminals or rules that are similar.We also have assigned bitstrings for the words inthe vocabulary (the lexical heads) using automaticclustering algorithms using the bigram mutual in-formation clustering algorithm (see (5)).
Giventhe bitsting of a history, we then designed a deci-sion tree for modeling the probability that a rulewill be used for rewriting a node in the parse tree.Since the grammar produces parses which maybe more detailed than the Treebank, the decisiontree was built using a training set constructed inthe following manner.
Using the grammar withthe P-CFG model we determined the most likelyparse that is consistent with the Treebank andconsidered the resulting sentence-tree pair as anevent.
Note that the grammar parse will also pro-vide the lexical head structure of the parse.
Then,we extracted using leftmost derivation order tu-pies of a history (truncated to the definition of ahistory in the HBG model) and the correspondingrule used in expanding a node.
Using the resultingdata set we built a decision tree by classifying his-tories to locally minimize the entropy of the ruletemplate.With a training set of about 9000 sentence-tree pairs, we had about 240,000 tuples and wegrew a tree with about 40,000 nodes.
This re-quired 18 hours on a 25 MIPS RISC-based ma-chine and the resulting decision tree was nearly100 megabytes.Immediate  vs .
Funct iona l  ParentsThe HBG model employs two types of parents, theimmediate parent and the functional parent.
ThewithR: PP ISyn  : PPSem:  Wi th -DataHI  : l i s t}{2 : w i thSem:  DataH I  : l i s tH2:  aSyn  :a Sem:HI :H2 :NDatal i s tIl i s tFigure 3: Sample representation of "with a l ist"in HBG model.35immediate parent is the constituent that immedi-ately dominates the constituent being predicted.If the immediate parent of a constituent has a dif-ferent syntactic type from that of the constituent,then the immediate parent is also the functionalparent; otherwise, the functional parent is thefunctional parent of the immediate parent.
Thedistinction between functional parents and imme-diate parents arises primarily to cope with unitproductions.
When unit productions of the formXP2 ~ XP1 occur, the immediate parent of XP1is XP2.
But, in general, the constituent XP2 doesnot contain enough useful information for ambi-guity resolution.
In particular, when consideringonly immediate parents, unit rules such as NP2 --*NP1 prevent he probabilistic model from allow-ing the NP1 constituent to interact with the VPrule which is the functional parent of NP1.When the two parents are identical as it of-ten happens, the duplicate information will be ig-nored.
However, when they differ, the decisiontree will select that parental context which bestresolves ambiguities.Figure 3 shows an example of the represen-tation of a history in HBG for the prepositionalphrase "with a list."
In this example, the imme-diate parent of the N1 node is the NBAR4 nodeand the functional parent of N1 is the PP1 node.ResultsWe compared the performance of HBG to the"broad-coverage" probabilistic ontext-free gram-mar, P-CFG.
The any-consistent rate of the gram-mar is 90% on test sentences of 7 to 17 words.
TheVi$erbi rate of P-CFG is 60% on the same test cor-pus of 760 sentences used in our experiments.
Onthe same test sentences, the HBG model has aViterbi rate of 75%.
This is a reduction of 37% inerror rate.AccuracyP-CFG 59.8%HBG 74.6%Error Reduction 36.8%Figure 4: Parsing accuracy: P-CFG vs. HBGIn developing HBG, we experimented withsimilar models of varying complexity.
One discov-ery made during this experimentation is that mod-els which incorporated more context han HBGperformed slightly worse than HBG.
This suggeststhat the current raining corpus may not containenough sentences to estimate richer models.
Basedon the results of these experiments, it appearslikely that significantly increasing the sise of thetraining corpus should result in a correspondingimprovement in the accuracy of HBG and richerHBG-like models.To check the value of the above detailed his-tory, we tried the simpler model:1. p(H1 \[HI~, H~,  P~, Z~o)2. p(H2 \[H~, H~p, H2p, 1%, Ip~)3. p(syn IH ,4. v(Sem ISYn, H,, Ip,)5. p(R \[Syn, Sere, H~, H2)This model corresponds to a P-CFG with NTsthat are the crude syntax and semantic ategoriesannotated with the lexical heads.
The Viterbi ratein this case was 66%, a small improvement over theP-CFG model indicating the value of using morecontext from the derivation tree.ConclusionsThe success of the HBG model encourages fu-ture development of general history-based gram-mars as a more promising approach than the usualP-CFG.
More experimentation is needed with alarger Treebank than was used in this study andwith different aspects of the derivation history.
Inaddition, this paper illustrates a new approach togrammar development where the parsing problemis divided (and hopefully conquered) into two sub-problems: one of grammar coverage for the gram-marian to address and the other of statistical mod-eling to increase the probability of picking the cor-rect parse of a sentence.REFERENCESBaker, J. K., 1975.
Stochastic Modeling for Au-tomatic Speech Understanding.
In SpeechRecognition, edited by Raj Reddy, AcademicPress, pp.
521-542.Brent, M. R. 1991.
Automatic Acquisition of Sub-categorization Frames from Untagged Free-text Corpora.
In Proceedings of the 29th An-nual Meeting of the Association for Computa-tional Linguistics.
Berkeley, California.Brill, E., Magerman, D., Marcus, M., and San-torini, B.
1990.
Deducing Linguistic Structurefrom the Statistics of Large Corpora.
In Pro-ceedings of the June 1990 DARPA Speech andNatural Language Workshop.
Hidden Valley,Pennsylvania.Brown, P. F., Della Pietra, V. J., deSouza, P. V.,Lai, J. C., and Mercer, R. L. Class-based n-gram Models of Natural Language.
In Pro-ceedings of ~he IBM Natural Language ITL,March, 1990.
Paris, France.36Church, K. 1988.
A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
InProceedings of the Second Conference on Ap-plied Natural Language Processing.
Austin,Texas.Gale, W. A. and Church, K. 1990.
Poor Estimatesof Context are Worse than None.
In Proceed-ings of the June 1990 DARPA Speech andNatural Language Workshop.
Hidden Valley,Pennsylvania.Harrison, M. A.
1978.
Introduction to FormalLanguage Theory.
Addison-Wesley PublishingCompany.Hindle, D. and Rooth, M. 1990.
Structural Am-biguity and Lexical Relations.
In Proceedingsof the :June 1990 DARPA Speech and NaturalLanguage Workshop.
Hidden Valley, Pennsyl-vania.
:Jelinek, F. 1985.
Self-organizing Language Model-ing for Speech Recognition.
IBM Report.Magerman, D. M. and Marcus, M. P. 1991.
Pearl:A Probabilistic Chart Parser.
In Proceedingsof the February 1991 DARPA Speech and Nat-ural Language Workshop.
Asilomar, Califor-nia.Derouault, A., and Merialdo, B., 1985.
Probabilis-tic Grammar for Phonetic to French Tran-scription.
ICASSP 85 Proceedings.
Tampa,Florida, pp.
1577-1580.Sharman, R. A., :Jelinek, F., and Mercer, R. 1990.Generating a Grammar for Statistical Train-ing.
In Proceedings of the :June 1990 DARPASpeech and Natural Language Workshop.
Hid-den Valley, Pennsylvania.37
