CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143?150Manchester, August 2008A Tree-to-String Phrase-based Model for Statistical Machine TranslationThai Phuong NguyenCollege of TechnologyVietnam National University, Hanoithainp@vnu.edu.vnAkira Shimazu1, Tu-Bao Ho2, Minh Le Nguyen1, and Vinh Van Nguyen11School of Information Science2School of Knowledge ScienceJapan Advanced Institute of Science and Technology{shimazu,bao,nguyenml,vinhnv}@jaist.ac.jpAbstractThough phrase-based SMT has achieved hightranslation quality, it still lacks of generaliza-tion ability to capture word order differencesbetween languages.
In this paper we describea general method for tree-to-string phrase-based SMT.
We study how syntactic trans-formation is incorporated into phrase-basedSMT and its effectiveness.
We design syntac-tic transformation models using unlexicalizedform of synchronous context-free grammars.These models can be learned from source-parsed bitext.
Our system can naturally makeuse of both constituent and non-constituentphrasal translations in the decoding phase.
Weconsidered various levels of syntactic analy-sis ranging from chunking to full parsing.Our experimental results of English-Japaneseand English-Vietnamese translation showeda significant improvement over two baselinephrase-based SMT systems.1 IntroductionBased on the kind of linguistic information whichis made use of, syntactic SMT can be divided intofour types: tree-to-string, string-to-tree, tree-to-tree,and hierarchical phrase-based.
The tree-to-string ap-proach (Collins et al, 2005; Nguyen and Shimazu,2006; Liu et al, 2006 and 2007) supposes that syn-tax of the source language is known.
This approachcan be applied when a source language parser isavailable.
The string-to-tree approach (Yamada andKnight, 2001; Galley et al, 2006) focuses on syntacticmodelling of the target language in cases it has syn-tactic resources such as treebanks and parsers.
Thetree-to-tree approach models the syntax of both lan-guages, therefore extra cost is required.
The fourthapproach (Chiang, 2005) constraints phrases undercontext-free grammar structure without any require-ment of linguistic annotation.In this paper, we present a tree-to-string phrase-based method which is based on synchronous CFGs.This method has two important properties: syntactictransformation is used in the decoding phase includ-ing a word-to-phrase tree transformation model anda phrase reordering model; phrases are the basic unitof translation.
Since we design syntactic transforma-tion models using un-lexicalized synchronous CFGs,the number of rules is small1.
Previous studies ontree-to-string SMT are different from ours.
Collinset al Collins et al (2005) used hand crafted rules tocarry out word reordering in the preprocessing phasebut not decoding phase.
Nguyen and Shimazu (2006)presented a more general method in which lexicalizedsyntactic reordering models based on PCFGs can belearned from source-parsed bitext and then applied inthe preprocessing phase.
Liu et al (2006) changed thetranslation unit from phrases to tree-to-string align-ment templates (TATs) while we do not.
TATs wasrepresented as xRs rules while we use synchronousCFG rules.
In order to overcome the limitation thatTATs can not capture non-constituent phrasal transla-tions, Liu et al (2007) proposed forest-to-string ruleswhile our system can naturally make use of such kindof phrasal translation by word-to-phrase tree transfor-mation.We carried out experiments with two languagepairs English-Japanese and English-Vietnamese.
Oursystem achieved significant improvements overPharaoh, a state-of-the-art phrase-based SMT system.We also analyzed the dependence of translation qual-ity on the level of syntactic analysis (shallow or deep).Figure 1 shows the architecture of our system.
Theinput of this system is a source-language tree and theoutput is a target-language string.
This system usesall features of conventional phrase-based SMT as in(Koehn et al, 2003).
There are two new features in-cluding a word-to-phrase tree transformation modeland a phrase reordering model.
The decoding algo-1See Section 6.2.143rithm is a tree-based search algorithm.Figure 1: A syntax-directed phrase-based SMT archi-tecture.2 Translation ModelWe use an example of English-Vietnamese translationto demonstrate the translation process as in Figure 2.Now we describe a tree-to-string SMT model basedon synchronous CFGs.
The translation process is:Figure 2: The translation process.T1?
T2?
T3?
T4(1)where T1is a source tree, T2is a source phrase tree,T3is a reordered source phrase tree, and T4is a targetphrase tree.Using the first order chain rule, the join probabilityover variables (trees) in graphical representation 1 isapproximately calculated by:P (T1, T2, T3, T4) = P (T1)?P (T2|T1)?P (T3|T2)?P (T4|T3)(2)P (T1) can be omitted since only one syntactic treeis used.
P (T2|T1) is a word-to-phrase tree transfor-mation model we describe later.
P (T3|T2) is a re-ordering model.
P (T4|T3) can be calculated using aphrase translation model and a language model.
Thisis the fundamental equation of our study representedin this paper.
In the next section, we will describe howto transform a word-based CFG tree into a phrase-based CFG tree.3 Word-to-Phrase Tree Transformation3.1 Penn Treebank?s Tree StructureAccording to this formalism, a tree is represented byphrase structure.
If we extract a CFG from a tree orset of trees, there will be two possible rule forms:?
A ?
?
where ?
is a sequence of nonterminals(syntactic categories).?
B ?
?
where ?
is a terminal symbol (or a wordin this case).We consider an example of a syntactic tree and asimple CFG extracted from that tree.Sentence: ?I am a student?Syntactic tree: (S (NP (NN I)) (VP (VBP am) (NP (DT a) (NNstudent))))Rule set: S ?
NP VP; VP ?
VBP NP; NP ?
NN | DT NN; NN?
I | student;VBP ?
am; DT ?
aHowever, we are considering phrase-based transla-tion.
Therefore the right hand side of the second ruleform must be a sequence of terminal symbols (or aphrase) but not a single symbol (a word).
Supposethat the phrase table contains a phrase ?am a student?which leads to the following possible tree structure:Phrase segmentation: ?I | am a student?Syntactic tree: (S (NP (NN I)) (VP (VBP am a student)))Rule set: S ?
NP VP; VP ?
VBP; NP ?
NN; NN ?
I; VBP ?am a studentWe have to find out some way to transform a CFGtree into a tree with phrases at leaves.
In the next sub-section we propose such an algorithm.3.2 An Algorithm for Word-to-Phrase TreeTransformationTable 1 represents our algorithm to transform a CFGtree to a phrase CFG tree.
When designing this algo-rithm, our criterion is to preserve the original struc-ture as much as possible.
This algorithm includes twosteps.
There are a number of notions concerning thisalgorithm:?
A CFG rule has a head symbol on the right handside.
Using this information, head child of anode on a syntactic tree can be determined.144+ Input: A CFG tree, a phrase segmentation+ Output: A phrase CFG tree+ Step 1: Allocate phrases to leaf nodes in a top-down manner: A phrase is allocated to head word of a node if thephrase contains the head word.
This head word is then considered as the phrase head.+ Step 2: Transform the syntactic tree by replacing leaf nodes by their allocated phrase and removing all nodes whosespan is a substring of phrases.Table 1: An algorithm to transform a CFG tree to a phrase CFG tree.?
If a node is a pre-terminal node (containing POStag), its head word is itself.
If a node is an in-ner node (containing syntactic constituent tag),its head word is retrieved through the head child.?
Word span of a node is a string of its leaves.
Forinstance, word span of subtree (NP (PRP$ your)(NN class)) is ?your class?.Now we consider an example depicted in Figure 3and 4.
Head children are tagged with functional labelH.
There are two phrases: ?is a?
and ?in your class?.After the Step 1, the phrase ?is a?
is attached to (VBZis).
The phrase ?in your class?
is attached to (IN in).In Step 2, the node (V is) is replaced by (V ?is a?)
and(DT a) is removed from its father NP.
Similarly, (INin) is replaced by (IN ?in your class?)
and the subtreeNP on the right is removed.S[is]NP[Fred]VP-H[is]VBZ-HNP[student]NNP-HisNP-H[student]DTNN-HPP[in]IN-HNP[class]PRP$NN-HFredastudentinyourclass{is a}{in your class}Figure 3: Tree transformation - step 1.
Solid arrowsshow the allocation process of ?is a?.
Dotted arrowsdemonstrate the allocation process of ?in your class?The proposed algorithm has some properties.
Westate these properties without presenting proof2.?
Uniqueness: Given a CFG tree and a phrase seg-mentation, by applying Algorithm 1, one andonly one phrase tree is generated.2Proofs are simple.Figure 4: Tree transformation - step 2.?
Constituent subgraph: A phrase CFG tree isa connected subgraph of input tree if leaves areignored.?
Flatness: A phrase CFG tree is flatter than inputtree.?
Outside head: The head of a phrase is always aword whose head outside the phrase.
If there ismore than one word satisfying this condition, theword at the highest level is chosen.?
Dependency subgraph: Dependency graph of aphrase CFG tree is a connected subgraph of in-put tree?s dependency graph if there exist no de-tached nodes.The meaning of uniqueness property is that our al-gorithm is a deterministic procedure.
The constituent-subgraph property will be employed in the next sec-tion for an efficient decoding algorithm.
When a syn-tactic tree is transformed, a number of subtrees arereplaced by phrases.
The head word of a phrase is thecontact point of that phrase with the remaining partof a sentence.
From the dependency point of view, ahead word should depend on an outer word rather thanan inner word.
About dependency-subgraph property,when there is a detached node, an indirect dependencywill become a direct one.
In any cases, there is no145change in dependency direction.
We can observe de-pendency trees in Figure 5.
The first two trees aresource dependency tree and phrase dependency treeof the previous example.
The last one corresponds tothe case in which a detached node exists.FredisROOTstudentinyourclassaFredisaROOTstudentinyour classFredisa studentROOTinyour classFigure 5: Dependency trees.
The third tree corre-sponds with phrase segmentation: ?Fred | is a student| in your class?3.3 Probabilistic Word-to-Phrase TreeTransformationWe have proposed an algorithm to create a phraseCFG tree from a pair of CFG tree and phrase seg-mentation.
Two questions naturally arise: ?is therea way to evaluate how good a phrase tree is??
and ?issuch an evaluation valuable??
Note that phrase treesare the means to reorder the source sentence repre-sented as phrase segmentations.
Therefore a phrasetree is surely not good if no right order can be gen-erated.
Now the answer to the second question isclear.
We need an evaluation method to prevent ourprogram from generating bad phrase trees.
In otherwords, good phrase trees should be given a higher pri-ority.We define the phrase tree probability as the productof its rule probability given the original CFG rules:P (T?)
=?iP (LHSi?
RHS?i|LHSi?
RHSi)(3)where T ?
is a phrase tree whose CFG rules areLHSi?
RHS?i.
LHSi?
RHSiare origi-nal CFG rules.
RHS?iare subsequences of RHSi.Since phrase tree rules should capture changes madeby the transformation from word to phrase, we use?+?
to represent an expansion and ?-?
to show anoverlap.
These symbol will be added to a nonter-minal on the side having a change.
In the previ-ous example, since a head noun in the word treehas been expanded on the right, the correspond-ing symbol in phrase tree is NN-H+.
A nonter-minal X can become one of the following symbolsX,?X,+X,X?, X+,?X?,?X+,+X?,+X+.Conditional probabilities are computed in a sepa-rate training phase using a source-parsed and word-aligned bitext.
First, all phrase pairs consistent withthe word alignment are collected.
Then using thisphrase segmentation and syntactic trees we can gener-ate phrase trees by word-to-phrase tree transformationand extract rules.4 Phrase Reordering ModelReordering rules are represented as SCFG ruleswhich can be un-lexicalized or source-side lexicalized(Nguyen and Shimazu, 2006).
In this paper, we usedun-lexicalized rules.
We used a learning algorithmas in (Nguyen and Shimazu, 2006) to learn weightedSCFGs.
The training requirements include a bilingualcorpus, a word alignment tool, and a broad coverageparser of the source language.
The parser is a con-stituency analyzer which can produce parse tree inPenn Tree-bank?s style.
The model is applicable tolanguage pairs in which the target language is poorin resources.
We used phrase reorder rules whose ?+?and ?-?
symbols are removed.5 DecodingA source sentence can have many possible phrase seg-mentations.
Each segmentation in combination with asource tree corresponds to a phrase tree.
A phrase-treeforest is a set of those trees.
A naive decoding algo-rithm is that for each segmentation, a phrase tree isgenerated and then the sentence is translated.
This al-gorithm is very slow or even intractable.
Based onthe constituent-subgraph property of the tree trans-formation algorithm, the forest of phrase trees willbe packed into a tree-structure container whose back-bone is the original CFG tree.5.1 Translation OptionsA translation option encodes a possibility to translatea source phrase (at a leaf node of a phrase tree) toanother phrase in target language.
Since our decoderuses a log-linear translation model, it can exploit var-ious features of translation options.
We use the samefeatures as (Koehn et al, 2003).
Basic information ofa translation option includes:?
source phrase?
target phrase?
phrase translation score (2)146?
lexical translation score (2)?
word penaltyTranslation options of an input sentence are col-lected before any decoding takes place.
This allows afaster lookup than consulting the whole phrase trans-lation table during decoding.
Note that the entirephrase translation table may be too big to fit intomemory.5.2 Translation HypothesesA translation hypothesis represents a partial or fulltranslation of an input sentence.
Initial hypothesescorrespond to translation options.
Each translationhypothesis is associated with a phrase-tree node.
Inother words, a phrase-tree node has a collection oftranslation hypotheses.
Now we consider basic infor-mation contained in a translation hypothesis:?
the cost so far?
list of child hypotheses?
left language model state and right languagemodel state5.3 Decoding AlgorithmFirst we consider structure of a syntactic tree.
A treenode contains fields such as syntactic category, childlist, and head child index.
A leaf node has an ad-ditional field of word string.
In order to extend thisstructure to store translation hypotheses, a new fieldof hypothesis collection is appended.
A hypothe-sis collection contains translation hypotheses whoseword spans are the same.
Actually, it corresponds toa phrase-tree node.
A hypothesis collection whoseword span is [i1, i2] at a node whose tag is X ex-presses that:?
There is a phrase-tree node (X, i1, i2).?
There exist a phrase [i1, i2] or?
There exist a subsequence of X?s child list:(Y1, j0, j1), (Y2, j1+1, j2), ..., (Yn, jn?1+1, jn)where j0= i1and jn= i2?
Suppose that [i, j] is X?s span, then [i1, i2] is avalid phrase node?s span if and only if: i1<= ior i < i1<= j and there exist a phrase [i0, i1?1] overlapping X?s span at [i, i1?
1].
A similarcondition is required of j.Table 2 shows our decoding algorithm.
Step 1 dis-tributes translation options to leaf nodes using a pro-cedure similar to Step 1 of algorithm in Table 1.
StepCorpus Size Training Development TestingConversation 16,809 15,734 403 672Reuters 57,778 55,757 1,000 1,021Table 3: Corpora and data sets.English VietnameseSentences 16,809Average sent.
len.
8.5 8.0Words 143,373 130,043Vocabulary 9,314 9,557English JapaneseSentences 57,778Average sent.
len.
26.7 33.5Words 1,548,572 1,927,952Vocabulary 31,702 29,406Table 4: Corpus statistics of translation tasks.2 helps check valid subsequences in Step 3 fast.
Step3 is a bottom-up procedure, a node is translated if allof its child nodes have been translated.
Step 3.1 callssyntactic transformation models.
After reordered inStep 3.2, a subsequence will be translated in Step 3.3using a simple monotonic decoding procedure result-ing in new translation hypotheses.
We used a beampruning technique to reduce the memory cost and toaccelerate the computation.6 Experimental Results6.1 Experimental SettingsWe used Reuters3, an English-Japanese bilingual cor-pus, and Conversation, an English-Vietnamese corpus(Table 4).
These corpora were split into data sets asshown in Table 3.
Japanese sentences were analyzedby ChaSen4, a word-segmentation tool.A number of tools were used in our experiments.Vietnamese sentences were segmented using a word-segmentation program (Nguyen et al, 2003).
Forlearning phrase translations and decoding, we usedPharaoh (Koehn, 2004), a state-of-the-art phrase-based SMT system which is available for researchpurpose.
For word alignment, we used the GIZA++tool (Och and Ney, 2000).
For learning languagemodels, we used SRILM toolkit (Stolcke, 2002).
ForMT evaluation, we used BLEU measure (Papineni etal., 2001) calculated by the NIST script version 11b.For the parsing task, we used Charniak?s parser (Char-niak, 2000).
For experiments with chunking (or shal-low parsing), we used a CRFs-based chunking tool 5to split a source sentence into syntactic chunks.
Thena pseudo CFG rule over chunks is built to generate atwo-level syntactic tree.
This tree can be used in the3http://www2.nict.go.jp/x/x161/members/mutiyama/index.html4http://chasen.aist-nara.ac.jp/chasen/distribution.html.en5http://crfpp.sourceforge.net/147+ Input: A source CFG tree, a translation-option collection+ Output: The best target sentence+ Step 1: Allocate translation options to hypothesis collections at leaf nodes.+ Step 2: Compute overlap vector for all nodes.+ Step 3: For each node, if all of its children have been translated, then for each validsub-sequence of child list, carry out the following steps:+ Step 3.1: Retrieve transformation rules+ Step 3.2: Reorder the sub-sequence+ Step 3.3: Translate the reordered sub-sequence and update correspondinghypothesis collectionsTable 2: A bottom-up dynamic-programming decoding algorithm.Corpus CFG PhraseCFG W2PTT ReorderConversation 2,784 2,684 8,862 2,999Reuters 7,668 5,479 13,458 7,855Table 5: Rule induction statistics.Corpus Pharaoh PB system SD system SD system(chunking) (full-parsing)Conversation 35.47 35.66 36.85 37.42Reuters 24.41 24.20 20.60 25.53Table 6: BLEU score comparison between phrase-based SMT and syntax-directed SMT.
PB=phrase-based; SD=syntax-directedsame way as trees produced by Charniak?s parser.We built a SMT system for phrase-based log-lineartranslation models.
This system has two decoders:beam search and syntax-based.
We implemented thealgorithm in Section 5 for the syntax-based decoder.We also implemented a rule induction module and amodule for minimum error rate training.
We used thesystem for our experiments reported later.6.2 Rule InductionIn Table 5, we report statistics of CFG rules,phrase CFG rules, word-to-phrase tree transformation(W2PTT) rules, and reordering rules.
All countedrules were in un-lexicalized form.
Those numbers arevery small in comparison with the number of phrasaltranslations (up to hundreds of thousands on our cor-pora).
There were a number of ?un-seen?
CFG ruleswhich did not have a corresponding reordering rule.A reason is that those rules appeared once or severaltimes in the training corpus; however, their hierarchi-cal alignments did not satisfy the conditions for in-ducing a reordering rule since word alignment is notperfect (Nguyen and Shimazu, 2006).
Another reasonis that there were CFG rules which required nonlocalreordering.
This may be an issue for future research:a Markovization technique for SCFGs.6.3 BLEU ScoresTable 6 shows a comparison of BLEU scores be-tween Pharaoh, our phrase-based SMT system, andour syntax-directed (SD) SMT system with chunkingand full parsing respectively.
On both Conversationcorpus and Reuters corpus: The BLEU score of ourphrase-based SMT system is comparable to that ofPharaoh; The BLEU score of our SD system with fullparsing is higher than that of our phrase-based sys-tem.
On Conversation corpus, our SD system withchunking has a higher performance in terms of BLEUscore than our phrase-based system.
Using sign test(Lehmann, 1986), we verified the improvements arestatistically significant.
However, on Reuters corpus,performance of the SD system with chunking is muchlower than the phrase-based system?s.
The reason isthat in English-Japanese translation, chunk is a tooshallow syntactic structure to capture word order in-formation.
For example, a prepositional chunk of-ten includes only preposition and adverb, thereforesuch information does not help reordering preposi-tional phrases.6.4 The Effectiveness of the W2PTT ModelWithout this feature, BLEU scores decreased around0.5 on both corpora.
We now consider a linguisticallymotivated example of English-Vietnamese translationto show that phrase segmentation can be evaluatedthrough phrase tree scoring.
This example was ex-tracted from Conversation test set.English sentence: for my wife ?s motherVietnamese word order: for mother ?s wife myPhrase segmentation 1: for my wife | ?s | motherP1=P(PP?IN+ -NP | PP?IN NP)xP(-NP?-NP NN | NP?NPNN)xP(-NP?POS | NP?PRP$ NNPOS)=log(0.00001)+log(0.14)+log(0.048)=-5-0.85-1.32=-7.17Phrase segmentation 2: for | my wife ?s | motherP2=P(PP?IN NP | PP?IN NP)xP(NP?NP NN | NP?NPNN) xP(NP?POS | NP?PRP$ NN POS)=log(0.32)+log(0.57)+log(0.048)=-0.5-0.24-1.32=-2.06The first phrase segmentation is bad (or even un-acceptable) since the right word order can not beachieved from this segmentation by phrase reorder-ing and word reordering within phrases.
The secondphrase segmentation is much better.
Source syntaxtree and phrase trees are shown in Figure 6.
The firstphrase tree has a much smaller probability (P1=-7.17)than the second (P2=-2.06).148Figure 6: Two phrase trees.Corpus Level-1 Level-2 Level-3 Level-4 FullConversation 36.85 36.91 37.11 37.23 37.42Reuters 20.60 22.76 24.49 25.12 25.53Table 7: BLEU score with different syntactic levels.Level-i means syntactic transformation was applied totree nodes whose level smaller than or equal to i. Thelevel of a pre-terminal node (POS tag) is 0.
The levelof an inner node is the maximum of its children?s lev-els.6.5 Levels of Syntactic AnalysisSince in practice, chunking and full parsing are oftenused, in Table 6, we showed translation quality of thetwo cases.
It is interesting if we can find how syn-tactic analysis can affect BLEU score at more inter-mediate levels (Table 7).
On the Conversation corpus,using syntax trees of level-1 is effective in comparisonwith baseline.
The increase of syntactic level makes asteady improvement in translation quality.
Note thatwhen we carried out experiments with chunking (con-sidered as level-1 syntax) the translation speed (in-cluding chunking) of our tree-to-string system wasmuch faster than baseline systems?.
This is an optionfor developing applications which require high speedsuch as web translation.7 Related Works7.1 A Comparison of Syntactic SMT MethodsTo advance the state of the art, SMT system design-ers have experimented with tree-structured transla-tion models.
The underlying computational modelswere synchronous context-free grammars and finite-state tree transducers which conceptually have a bet-ter expressive power than finite-state transducers.
Wecreate Tables 8 and 9 in order to compare syntac-tic SMT methods including ours.
The first row is abaseline phrasal SMT approach.
The second columnin Table 8 only describes input types because the out-put is often string.
Syntactic SMT methods are dif-ferent in many aspects.
Methods which make use ofphrases (in either explicit or implicit way) can beatthe baseline approach (Table 8) in terms of BLEUmetric.
Two main problems these models aim to dealwith are word order and word choice.
In order to ac-complish this purpose, the underlying formal gram-mars (including synchronous context-free grammarsand tree transducers) can be fully lexicalized or un-lexicalized (Table 9).7.2 Non-constituent Phrasal TranslationsLiu et al (2007) proposed forest-to-string rules tocapture non-constituent phrasal translation while oursystem can naturally make use of such kind of phrasaltranslation by using word-to-phrase tree transforma-tion.
Liu et al (2007) also discussed about howthe phenomenon of non-syntactic bilingual phrasesis dealt with in other SMT methods.
Galley et al(2006) handled non-constituent phrasal translation bytraversing the tree upwards until reaches a node thatsubsumes the phrase.
Marcu et al (2006) reportedthat approximately 28% of bilingual phrases are non-syntactic on their English-Chinese corpus.
They pro-posed using a pseudo nonterminal symbol that sub-sumes the phrase and corresponding multi-headedsyntactic structure.
One new xRs rule is required toexplain how the new nonterminal symbol can be com-bined with others.
This technique brought a signif-icant improvement in performance to their string-to-tree noisy channel SMT system.8 ConclusionsWe have presented a general tree-to-string phrase-based method.
This method employs a syntax-basedreordering model in the decoding phase.
By word-to-phrase tree transformation, all possible phrasesare considered in translation.
Our method doesnot suppose a uniform distribution over all possiblephrase segmentations as (Koehn et al, 2003) sinceeach phrase tree has a probability.
We believe thatother kinds of translation unit such as n-gram (Joset al, 2006), factored phrasal translation (Koehn andHoang, 2007), or treelet (Quirk et al, 2005) can beused in this method.
We would like to consider thisproblem as a future study.
Moreover we would like touse n-best trees as the input of our system.
A number149Method Input Theoretical Decoding style Linguistic Phrase Performancemodel information usageKoehn et al (2003) string FSTs beam search no yes baselineYamada and Knight (2001) string SCFGs parsing target no not betterMelamed (2003) string SCFGs parsing both sides no not betterChiang (2005) string SCFGs parsing no yes betterQuirk et al (2005) dep.
tree TTs parsing source yes betterGalley et al (2006) string TTs parsing target yes betterLiu et al (2006) tree TTs tree transf.
source yes betterOur work tree SCFGs tree transf.
source yes betterTable 8: A comparison of syntactic SMT methods (part 1).
FST=Finite State Transducer; SCFG=SynchronousContext-Free Grammar; TT=Tree Transducer.Method Rule form Rule function Rule lexicalization levelKoehn et al (2003) no no noYamada and Knight (2001) SCFG rule reorder and function-word ins./del.
unlexicalizedMelamed (2003) SCFG rule reorder and word choice fullChiang (2005) SCFG rule reorder and word choice fullQuirk et al (2005) Treelet pair word choice fullGalley et al (2006) xRs rule reorder and word choice fullLiu et al (2006) xRs rule reorder and word choice fullOur work SCFG rule reorder unlexicalizedTable 9: A comparison of syntactic SMT methods (part 2).
xRs is a kind of rule which maps a syntactic patternto a string, for example VP(AUX(does), RB(not),x0:VB) ?
ne, x0, pas.
In the column Rule lexicalizationlevel: full=lexicalization using vocabularies of both source language and target language.of non-local reordering phenomena such as adjunctattachment should be handled in the future.ReferencesCharniak, E. 2000.
A maximum entropy inspired parser.In Proceedings of HLT-NAACL.Galley, M., Jonathan Graehl, Kevin Knight, Daniel Marcu,Steve DeNeefe, Wei Wang, Ignacio Thayer 2006.
Scal-able Inference and Training of Context-Rich SyntacticTranslation Models.
In Proceedings of ACL.Jos B. Mario, Rafael E. Banchs, Josep M. Crego, Adri deGispert, Patrik Lambert, Jos A. R. Fonollosa, Marta R.Costa-juss.
2006.
N-gram-based Machine Translation.Computational Linguistics, 32(4): 527?549.Koehn, P. 2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
InProceedings of AMTA.Koehn, P. and Hieu Hoang.
2007.
Factored TranslationModels.
In Proceedings of EMNLP.Koehn, P., F. J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of HLT-NAACL.Lehmann, E. L. 1986.
Testing Statistical Hypotheses (Sec-ond Edition).
Springer-Verlag.Liu, Y., Qun Liu, Shouxun Lin.
2006.
Tree-to-StringAlignment Template for Statistical Machine Transla-tion.
In Proceedings of ACL.Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007.Forest-to-String Statistical Translation Rules.
In Pro-ceedings of ACL.Marcu, D., Wei Wang, Abdessamad Echihabi, and KevinKnight.
2006.
SPMT: Statistical Machine Translationwith Syntactified Target Language Phrases.
In Proceed-ings of EMNLP.Melamed, I. D. 2004.
Statistical machine translation byparsing.
In Proceedings of ACL.Nguyen, Thai Phuong and Akira Shimazu.
2006.
Improv-ing Phrase-Based Statistical Machine Translation withMorphosyntactic Transformation.
Machine Translation,20(3): 147?166.Nguyen, Thai Phuong, Nguyen Van Vinh and Le AnhCuong.
2003.
Vietnamese Word Segmentation UsingHidden Markov Model.
In Proceedings of InternationalWorkshop for Computer, Information, and Communica-tion Technologies in Korea and Vietnam.Och, F. J. and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of ACL.Papineni, K., S. Roukos, T. Ward, W.-J.
Zhu.
2001.BLEU: a method for automatic evaluation of machinetranslation.
Technical Report RC22176 (W0109-022),IBM Research Report.Quirk, C., A. Menezes, and C. Cherry.
2005.
Dependencytreelet translation: Syntactically informed phrasal SMT.In Proceedings of ACL.Stolcke, A.
2002.
SRILM - An Extensible Language Mod-eling Toolkit.
In Proc.
Intl.
Conf.
Spoken LanguageProcessing.Yamada, K. and K. Knight.
2001.
A syntax-based statisti-cal translation model.
In Proceedings of ACL.150
