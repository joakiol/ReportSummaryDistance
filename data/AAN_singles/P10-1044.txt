Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Latent Dirichlet Allocation method for Selectional PreferencesAlan Ritter, Mausam and Oren EtzioniDepartment of Computer Science and EngineeringBox 352350, University of Washington, Seattle, WA 98195, USA{aritter,mausam,etzioni}@cs.washington.eduAbstractThe computation of selectional prefer-ences, the admissible argument values fora relation, is a well-known NLP task withbroad applicability.
We present LDA-SP,which utilizes LinkLDA (Erosheva et al,2004) to model selectional preferences.By simultaneously inferring latent top-ics and topic distributions over relations,LDA-SP combines the benefits of pre-vious approaches: like traditional class-based approaches, it produces human-interpretable classes describing each re-lation?s preferences, but it is competitivewith non-class-based methods in predic-tive power.We compare LDA-SP to several state-of-the-art methods achieving an 85% increasein recall at 0.9 precision over mutual in-formation (Erk, 2007).
We also eval-uate LDA-SP?s effectiveness at filteringimproper applications of inference rules,where we show substantial improvementover Pantel et al?s system (Pantel et al,2007).1 IntroductionSelectional Preferences encode the set of admissi-ble argument values for a relation.
For example,locations are likely to appear in the second argu-ment of the relation X is headquartered in Y andcompanies or organizations in the first.
A large,high-quality database of preferences has the po-tential to improve the performance of a wide rangeof NLP tasks including semantic role labeling(Gildea and Jurafsky, 2002), pronoun resolution(Bergsma et al, 2008), textual inference (Pantelet al, 2007), word-sense disambiguation (Resnik,1997), and many more.
Therefore, much atten-tion has been focused on automatically computingthem based on a corpus of relation instances.Resnik (1996) presented the earliest work inthis area, describing an information-theoretic ap-proach that inferred selectional preferences basedon the WordNet hypernym hierarchy.
Recent work(Erk, 2007; Bergsma et al, 2008) has moved awayfrom generalization to known classes, insteadutilizing distributional similarity between nounsto generalize beyond observed relation-argumentpairs.
This avoids problems like WordNet?s poorcoverage of proper nouns and is shown to improveperformance.
These methods, however, no longerproduce the generalized class for an argument.In this paper we describe a novel approach tocomputing selectional preferences by making useof unsupervised topic models.
Our approach isable to combine benefits of both kinds of meth-ods: it retains the generalization and human-interpretability of class-based approaches and isalso competitive with the direct methods on pre-dictive tasks.Unsupervised topic models, such as latentDirichlet alocation (LDA) (Blei et al, 2003) andits variants are characterized by a set of hiddentopics, which represent the underlying semanticstructure of a document collection.
For our prob-lem these topics offer an intuitive interpretation ?they represent the (latent) set of classes that storethe preferences for the different relations.
Thus,topic models are a natural fit for modeling our re-lation data.In particular, our system, called LDA-SP, usesLinkLDA (Erosheva et al, 2004), an extension ofLDA that simultaneously models two sets of dis-tributions for each topic.
These two sets representthe two arguments for the relations.
Thus, LDA-SPis able to capture information about the pairs oftopics that commonly co-occur.
This informationis very helpful in guiding inference.We run LDA-SP to compute preferences on amassive dataset of binary relations r(a1, a2) ex-424tracted from the Web by TEXTRUNNER (Bankoand Etzioni, 2008).
Our experiments demon-strate that LDA-SP significantly outperforms stateof the art approaches obtaining an 85% increasein recall at precision 0.9 on the standard pseudo-disambiguation task.Additionally, because LDA-SP is based on a for-mal probabilistic model, it has the advantage thatit can naturally be applied in many scenarios.
Forexample, we can obtain a better understanding ofsimilar relations (Table 1), filter out incorrect in-ferences based on querying our model (Section4.3), as well as produce a repository of class-basedpreferences with a little manual effort as demon-strated in Section 4.4.
In all these cases we obtainhigh quality results, for example, massively out-performing Pantel et al?s approach in the textualinference task.12 Previous WorkPrevious work on selectional preferences canbe broken into four categories: class-based ap-proaches (Resnik, 1996; Li and Abe, 1998; Clarkand Weir, 2002; Pantel et al, 2007), similaritybased approaches (Dagan et al, 1999; Erk, 2007),discriminative (Bergsma et al, 2008), and genera-tive probabilistic models (Rooth et al, 1999).Class-based approaches, first proposed byResnik (1996), are the most studied of the four.They make use of a pre-defined set of classes, ei-ther manually produced (e.g.
WordNet), or auto-matically generated (Pantel, 2003).
For each re-lation, some measure of the overlap between theclasses and observed arguments is used to iden-tify those that best describe the arguments.
Thesetechniques produce a human-interpretable output,but often suffer in quality due to an incoherent tax-onomy, inability to map arguments to a class (poorlexical coverage), and word sense ambiguity.Because of these limitations researchers haveinvestigated non-class based approaches, whichattempt to directly classify a given noun-phraseas plausible/implausible for a relation.
Of these,the similarity based approaches make use of a dis-tributional similarity measure between argumentsand evaluate a heuristic scoring function:Srel(arg)=?arg?
?Seen(rel)sim(arg, arg?)
?
wtrel(arg)1Our repository of selectional preferences is availableat http://www.cs.washington.edu/research/ldasp.Erk (2007) showed the advantages of this ap-proach over Resnik?s information-theoretic class-based method on a pseudo-disambiguation evalu-ation.
These methods obtain better lexical cover-age, but are unable to obtain any abstract represen-tation of selectional preferences.Our solution fits into the general categoryof generative probabilistic models, which modeleach relation/argument combination as being gen-erated by a latent class variable.
These classesare automatically learned from the data.
This re-tains the class-based flavor of the problem, with-out the knowledge limitations of the explicit class-based approaches.
Probably the closest to ourwork is a model proposed by Rooth et al (1999),in which each class corresponds to a multinomialover relations and arguments and EM is used tolearn the parameters of the model.
In contrast,we use a LinkLDA framework in which each re-lation is associated with a corresponding multi-nomial distribution over classes, and each argu-ment is drawn from a class-specific distributionover words; LinkLDA captures co-occurrence ofclasses in the two arguments.
Additionally weperform full Bayesian inference using collapsedGibbs sampling, in which parameters are inte-grated out (Griffiths and Steyvers, 2004).Recently, Bergsma et.
al.
(2008) proposed thefirst discriminative approach to selectional prefer-ences.
Their insight that pseudo-negative exam-ples could be used as training data allows the ap-plication of an SVM classifier, which makes use ofmany features in addition to the relation-argumentco-occurrence frequencies used by other meth-ods.
They automatically generated positive andnegative examples by selecting arguments havinghigh and low mutual information with the rela-tion.
Since it is a discriminative approach it isamenable to feature engineering, but needs to beretrained and tuned for each task.
On the otherhand, generative models produce complete prob-ability distributions of the data, and hence can beintegrated with other systems and tasks in a moreprincipled manner (see Sections 4.2.2 and 4.3.1).Additionally, unlike LDA-SP Bergsma et al?s sys-tem doesn?t produce human-interpretable topics.Finally, we note that LDA-SP and Bergsma?s sys-tem are potentially complimentary ?
the output ofLDA-SP could be used to generate higher-qualitytraining data for Bergsma, potentially improvingtheir results.425Topic models such as LDA (Blei et al, 2003)and its variants have recently begun to see usein many NLP applications such as summarization(Daume?
III and Marcu, 2006), document align-ment and segmentation (Chen et al, 2009), andinferring class-attribute hierarchies (Reisinger andPasca, 2009).
Our particular model, LinkLDA, hasbeen applied to a few NLP tasks such as simul-taneously modeling the words appearing in blogposts and users who will likely respond to them(Yano et al, 2009), modeling topic-aligned arti-cles in different languages (Mimno et al, 2009),and word sense induction (Brody and Lapata,2009).Finally, we highlight two systems, developedindependently of our own, which apply LDA-stylemodels to similar tasks.
O?
Se?aghdha (2010) pro-poses a series of LDA-style models for the taskof computing selectional preferences.
This worklearns selectional preferences between the fol-lowing grammatical relations: verb-object, noun-noun, and adjective-noun.
It also focuses onjointly modeling the generation of both predicateand argument, and evaluation is performed on aset of human-plausibility judgments obtaining im-pressive results against Keller and Lapata?s (2003)Web hit-count based system.
Van Durme andGildea (2009) proposed applying LDA to generalknowledge templates extracted using the KNEXTsystem (Schubert and Tong, 2003).
In contrast,our work uses LinkLDA and focuses on modelingmultiple arguments of a relation (e.g., the subjectand direct object of a verb).3 Topic Models for Selectional Prefs.We present a series of topic models for the task ofcomputing selectional preferences.
These modelsvary in the amount of independence they assumebetween a1 and a2.
At one extreme is Indepen-dentLDA, a model which assumes that both a1 anda2 are generated completely independently.
Onthe other hand, JointLDA, the model at the otherextreme (Figure 1) assumes both arguments of aspecific extraction are generated based on a singlehidden variable z. LinkLDA (Figure 2) lies be-tween these two extremes, and as demonstrated inSection 4, it is the best model for our relation data.We are given a set R of binary relations and acorpus D = {r(a1, a2)} of extracted instances forthese relations.
2 Our task is to compute, for eachargument ai of each relation r, a set of usual ar-gument values (noun phrases) that it takes.
Forexample, for the relation is headquartered in thefirst argument set will include companies like Mi-crosoft, Intel, General Motors and second argu-ment will favor locations like New York, Califor-nia, Seattle.3.1 IndependentLDAWe first describe the straightforward applicationof LDA to modeling our corpus of extracted rela-tions.
In this case two separate LDA models areused to model a1 and a2 independently.In the generative model for our data, each rela-tion r has a corresponding multinomial over topics?r, drawn from a Dirichlet.
For each extraction, ahidden topic z is first picked according to ?r, andthen the observed argument a is chosen accordingto the multinomial ?z .Readers familiar with topic modeling terminol-ogy can understand our approach as follows: wetreat each relation as a document whose contentsconsist of a bags of words corresponding to all thenoun phrases observed as arguments of the rela-tion in our corpus.
Formally, LDA generates eachargument in the corpus of relations as follows:for each topic t = 1 .
.
.
T doGenerate ?t according to symmetric Dirich-let distribution Dir(?
).end forfor each relation r = 1 .
.
.
|R| doGenerate ?r according to Dirichlet distribu-tion Dir(?
).for each tuple i = 1 .
.
.
Nr doGenerate zr,i from Multinomial(?r).Generate the argument ar,i from multi-nomial ?zr,i .end forend forOne weakness of IndependentLDA is that itdoesn?t jointly model a1 and a2 together.
Clearlythis is undesirable, as information about whichtopics one of the arguments favors can help informthe topics chosen for the other.
For example, classpairs such as (team, game), (politician, political is-sue) form much more plausible selectional prefer-ences than, say, (team, political issue), (politician,game).2We focus on binary relations, though the techniques pre-sented in the paper are easily extensible to n-ary relations.4263.2 JointLDAAs a more tightly coupled alternative, we firstpropose JointLDA, whose graphical model is de-picted in Figure 1.
The key difference in JointLDA(versus LDA) is that instead of one, it maintainstwo sets of topics (latent distributions over words)denoted by ?
and ?, one for classes of each ar-gument.
A topic id k represents a pair of topics,?k and ?k, that co-occur in the arguments of ex-tracted relations.
Common examples include (Per-son, Location), (Politician, Political issue), etc.The hidden variable z = k indicates that the nounphrase for the first argument was drawn from themultinomial ?k, and that the second argument wasdrawn from ?k.
The per-relation distribution ?r isa multinomial over the topic ids and represents theselectional preferences, both for arg1s and arg2sof a relation r.Although JointLDA has many desirable proper-ties, it has some drawbacks as well.
Most notably,in JointLDA topics correspond to pairs of multi-nomials (?k, ?k); this leads to a situation in whichmultiple redundant distributions are needed to rep-resent the same underlying semantic class.
Forexample consider the case where we we need torepresent the following selectional preferences forour corpus of relations: (person, location), (per-son, organization), and (person, crime).
BecauseJointLDA requires a separate pair of multinomialsfor each topic, it is forced to use 3 separate multi-nomials to represent the class person, rather thanlearning a single distribution representing personand choosing 3 different topics for a2.
This resultsin poor generalization because the data for a singleclass is divided into multiple topics.In order to address this problem while maintain-ing the sharing of influence between a1 and a2, wenext present LinkLDA, which represents a com-promise between IndependentLDA and JointLDA.LinkLDA is more flexible than JointLDA, allow-ing different topics to be chosen for a1, and a2,however still models the generation of topics fromthe same distribution for a given relation.3.3 LinkLDAFigure 2 illustrates the LinkLDA model in theplate notation, which is analogous to the modelin (Erosheva et al, 2004).
In particular note thateach ai is drawn from a different hidden topic zi,however the zi?s are drawn from the same distri-bution ?r for a given relation r. To facilitate learn-?a1a2?|R|N?
?1?T?2zFigure 1: JointLDA?z1z2a1a2?|R|N?
?1?T?2Figure 2: LinkLDAing related topic pairs between arguments we em-ploy a sparse prior over the per-relation topic dis-tributions.
Because a few topics are likely to beassigned most of the probability mass for a givenrelation it is more likely (although not necessary)that the same topic number k will be drawn forboth arguments.When comparing LinkLDA with JointLDA thebetter model may not seem immediately clear.
Onthe one hand, JointLDA jointly models the gen-eration of both arguments in an extracted tuple.This allows one argument to help disambiguatethe other in the case of ambiguous relation strings.LinkLDA, however, is more flexible; rather thanrequiring both arguments to be generated from oneof |Z| possible pairs of multinomials (?z, ?z), Lin-kLDA allows the arguments of a given extractionto be generated from |Z|2 possible pairs.
Thus,instead of imposing a hard constraint that z1 =z2 (as in JointLDA), LinkLDA simply assigns ahigher probability to states in which z1 = z2, be-cause both hidden variables are drawn from thesame (sparse) distribution ?r.
LinkLDA can thusre-use argument classes, choosing different com-binations of topics for the arguments if it fits thedata better.
In Section 4 we show experimentallythat LinkLDA outperforms JointLDA (and Inde-pendentLDA) by wide margins.
We use LDA-SPto refer to LinkLDA in all the experiments below.3.4 InferenceFor all the models we use collapsed Gibbs sam-pling for inference in which each of the hid-den variables (e.g., zr,i,1 and zr,i,2 in LinkLDA)are sampled sequentially conditioned on a full-assignment to all others, integrating out the param-eters (Griffiths and Steyvers, 2004).
This producesrobust parameter estimates, as it allows computa-tion of expectations over the posterior distribution427as opposed to estimating maximum likelihood pa-rameters.
In addition, the integration allows theuse of sparse priors, which are typically more ap-propriate for natural language data.
In all exper-iments we use hyperparameters ?
= ?1 = ?2 =0.1.
We generated initial code for our samplers us-ing the Hierarchical Bayes Compiler (Daume III,2007).3.5 Advantages of Topic ModelsThere are several advantages to using topic mod-els for our task.
First, they naturally model theclass-based nature of selectional preferences, butdon?t take a pre-defined set of classes as input.Instead, they compute the classes automatically.This leads to better lexical coverage since the is-sue of matching a new argument to a known classis side-stepped.
Second, the models naturally han-dle ambiguous arguments, as they are able to as-sign different topics to the same phrase in differentcontexts.
Inference in these models is also scalable?
linear in both the size of the corpus as well asthe number of topics.
In addition, there are severalscalability enhancements such as SparseLDA (Yaoet al, 2009), and an approximation of the GibbsSampling procedure can be efficiently parallelized(Newman et al, 2009).
Finally we note that, oncea topic distribution has been learned over a set oftraining relations, one can efficiently apply infer-ence to unseen relations (Yao et al, 2009).4 ExperimentsWe perform three main experiments to assess thequality of the preferences obtained using topicmodels.
The first is a task-independent evaluationusing a pseudo-disambiguation experiment (Sec-tion 4.2), which is a standard way to evaluate thequality of selectional preferences (Rooth et al,1999; Erk, 2007; Bergsma et al, 2008).
We usethis experiment to compare the various topic mod-els as well as the best model with the known stateof the art approaches to selectional preferences.Secondly, we show significant improvements toperformance at an end-task of textual inference inSection 4.3.
Finally, we report on the quality ofa large database of Wordnet-based preferences ob-tained after manually associating our topics withWordnet classes (Section 4.4).4.1 Generalization CorpusFor all experiments we make use of a corpusof r(a1, a2) tuples, which was automatically ex-tracted by TEXTRUNNER (Banko and Etzioni,2008) from 500 million Web pages.To create a generalization corpus from thislarge dataset.
We first selected 3,000 relationsfrom the middle of the tail (we used the 2,000-5,000 most frequent ones)3 and collected all in-stances.
To reduce sparsity, we discarded all tu-ples containing an NP that occurred fewer than 50times in the data.
This resulted in a vocabulary ofabout 32,000 noun phrases, and a set of about 2.4million tuples in our generalization corpus.We inferred topic-argument and relation-topicmultinomials (?, ?, and ?)
on the generalizationcorpus by taking 5 samples at a lag of 50 aftera burn in of 750 iterations.
Using multiple sam-ples introduces the risk of topic drift due to lackof identifiability, however we found this to not bea problem in practice.
During development wefound that the topics tend to remain stable acrossmultiple samples after sufficient burn in, and mul-tiple samples improved performance.
Table 1 listssample topics and high ranked words for each (forboth arguments) as well as relations favoring thosetopics.4.2 Task Independent EvaluationWe first compare the three LDA-based approachesto each other and two state of the art similaritybased systems (Erk, 2007) (using mutual informa-tion and Jaccard similarity respectively).
Thesesimilarity measures were shown to outperform thegenerative model of Rooth et al (1999), as wellas class-based methods such as Resnik?s.
In thispseudo-disambiguation experiment an observedtuple is paired with a pseudo-negative, whichhas both arguments randomly generated from thewhole vocabulary (according to the corpus-widedistribution over arguments).
The task is, for eachrelation-argument pair, to determine whether it isobserved, or a random distractor.4.2.1 Test SetFor this experiment we gathered a primary corpusby first randomly selecting 100 high-frequency re-lations not in the generalization corpus.
For eachrelation we collected all tuples containing argu-ments in the vocabulary.
We held out 500 ran-domly selected tuples as the test set.
For each tu-3Many of the most frequent relations have very weak se-lectional preferences, and thus provide little signal for infer-ring meaningful topics.
For example, the relations has and iscan take just about any arguments.428Topic t Arg1 Relations which assignhighest probability to tArg218 The residue - The mixture - The reactionmixture - The solution - the mixture - the re-action mixture - the residue - The reaction -the solution - The filtrate - the reaction - Theproduct - The crude product - The pellet -The organic layer - Thereto - This solution- The resulting solution - Next - The organicphase - The resulting mixture - C. )was treated with, istreated with, waspoured into, wasextracted with, waspurified by, was di-luted with, was filteredthrough, is disolved in,is washed withEtOAc - CH2Cl2 - H2O - CH.sub.2Cl.sub.2- H.sub.2O - water - MeOH - NaHCO3 -Et2O - NHCl - CHCl.sub.3 - NHCl - drop-wise - CH2Cl.sub.2 - Celite - Et.sub.2O -Cl.sub.2 - NaOH - AcOEt - CH2C12 - themixture - saturated NaHCO3 - SiO2 - H2O- N hydrochloric acid - NHCl - preparativeHPLC - to0 C151 the Court - The Court - the Supreme Court- The Supreme Court - this Court - Court- The US Supreme Court - the court - ThisCourt - the US Supreme Court - The court- Supreme Court - Judge - the Court of Ap-peals - A federal judgewill hear, ruled in, de-cides, upholds, struckdown, overturned,sided with, affirmsthe case - the appeal - arguments - a case -evidence - this case - the decision - the law- testimony - the State - an interview - anappeal - cases - the Court - that decision -Congress - a decision - the complaint - oralarguments - a law - the statute211 President Bush - Bush - The President -Clinton - the President - President Clinton- President George W. Bush - Mr. Bush -The Governor - the Governor - Romney -McCain - The White House - President -Schwarzenegger - Obamahailed, vetoed, pro-moted, will deliver,favors, denounced,defendedthe bill - a bill - the decision - the war - theidea - the plan - the move - the legislation -legislation - the measure - the proposal - thedeal - this bill - a measure - the program -the law - the resolution - efforts - the agree-ment - gay marriage - the report - abortion224 Google - Software - the CPU - Clicking -Excel - the user - Firefox - System - TheCPU - Internet Explorer - the ability - Pro-gram - users - Option - SQL Server - Code- the OS - the BIOSwill display, to store, toload, processes, cannotfind, invokes, to searchfor, to deletedata - files - the data - the file - the URL -information - the files - images - a URL - theinformation - the IP address - the user - text- the code - a file - the page - IP addresses -PDF files - messages - pages - an IP addressTable 1: Example argument lists from the inferred topics.
For each topic number t we list the mostprobable values according to the multinomial distributions for each argument (?t and ?t).
The middlecolumn reports a few relations whose inferred topic distributions ?r assign highest probability to t.ple r(a1, a2) in the held-out set, we removed alltuples in the training set containing either of therel-arg pairs, i.e., any tuple matching r(a1, ?)
orr(?, a2).
Next we used collapsed Gibbs samplingto infer a distribution over topics, ?r, for each ofthe relations in the primary corpus (based solelyon tuples in the training set) using the topics fromthe generalization corpus.For each of the 500 observed tuples in the test-set we generated a pseudo-negative tuple by ran-domly sampling two noun phrases from the distri-bution of NPs in both corpora.4.2.2 PredictionOur prediction system needs to determine whethera specific relation-argument pair is admissible ac-cording to the selectional preferences or is a ran-dom distractor (D).
Following previous work, weperform this experiment independently for the tworelation-argument pairs (r, a1) and (r, a2).We first compute the probability of observinga1 for first argument of relation r given that it isnot a distractor, P (a1|r,?D), which we approx-imate by its probability given an estimate of theparameters inferred by our model, marginalizingover hidden topics t. The analysis for the secondargument is similar.P (a1|r,?D) ?
PLDA(a1|r) =TXt=0P (a1|t)P (t|r)=TXt=0?t(a1)?r(t)A simple application of Bayes Rule gives theprobability that a particular argument is not adistractor.
Here the distractor-related proba-bilities are independent of r, i.e., P (D|r) =P (D), P (a1|D, r) = P (a1|D), etc.
We estimateP (a1|D) according to their frequency in the gen-eralization corpus.P (?D|r, a1) =P (?D|r)P (a1|r,?D)P (a1|r)?P (?D)PLDA(a1|r)P (D)P (a1|D) + P (?D)PLDA(a1|r)4.2.3 ResultsFigure 3 plots the precision-recall curve for thepseudo-disambiguation experiment comparing thethree different topic models.
LDA-SP, which usesLinkLDA, substantially outperforms both Inde-pendentLDA and JointLDA.Next, in figure 4, we compare LDA-SP withmutual information and Jaccard similarities us-ing both the generalization and primary corpus for4290.0 0.2 0.4 0.6 0.8 1.00.40.60.81.0recallprecisionLDA?SPIndependentLDAJointLDAFigure 3: Comparison of LDA-based approacheson the pseudo-disambiguation task.
LDA-SP (Lin-kLDA) substantially outperforms the other mod-els.0.0 0.2 0.4 0.6 0.8 1.00.40.60.81.0recallprecisionLDA?SPJaccardMutual InformationFigure 4: Comparison to similarity-based selec-tional preference systems.
LDA-SP obtains 85%higher recall at precision 0.9.computation of similarities.
We find LDA-SP sig-nificantly outperforms these methods.
Its edge ismost noticed at high precisions; it obtains 85%more recall at 0.9 precision compared to mutualinformation.
Overall LDA-SP obtains an 15% in-crease in the area under precision-recall curve overmutual information.
All three systems?
AUCs areshown in Table 2; LDA-SP?s improvements overboth Jaccard and mutual information are highlysignificant with a significance level less than 0.01using a paired t-test.In addition to a superior performance in se-lectional preference evaluation LDA-SP also pro-duces a set of coherent topics, which can be use-ful in their own right.
For instance, one could usethem for tasks such as set-expansion (Carlson etal., 2010) or automatic thesaurus induction (Et-LDA-SP MI-Sim Jaccard-SimAUC 0.833 0.727 0.711Table 2: Area under the precision recall curve.LDA-SP?s AUC is significantly higher than bothsimilarity-based methods according to a paired t-test with a significance level below 0.01.zioni et al, 2005; Kozareva et al, 2008).4.3 End Task EvaluationWe now evaluate LDA-SP?s ability to improve per-formance at an end-task.
We choose the task ofimproving textual entailment by learning selec-tional preferences for inference rules and filteringinferences that do not respect these.
This applica-tion of selectional preferences was introduced byPantel et.
al.
(2007).
For now we stick to infer-ence rules of the form r1(a1, a2) ?
r2(a1, a2),though our ideas are more generally applicable tomore complex rules.
As an example, the rule (Xdefeats Y) ?
(X plays Y) holds when X and Yare both sports teams, however fails to produce areasonable inference if X and Y are Britain andNazi Germany respectively.4.3.1 Filtering InferencesIn order for an inference to be plausible, both re-lations must have similar selectional preferences,and further, the arguments must obey the selec-tional preferences of both the antecedent r1 andthe consequent r2.4 Pantel et al (2007) madeuse of these intuitions by producing a set of class-based selectional preferences for each relation,then filtering out any inferences where the argu-ments were incompatible with the intersection ofthese preferences.
In contrast, we take a proba-bilistic approach, evaluating the quality of a spe-cific inference by measuring the probability thatthe arguments in both the antecedent and the con-sequent were drawn from the same hidden topicin our model.
Note that this probability capturesboth the requirement that the antecedent and con-sequent have similar selectional preferences, andthat the arguments from a particular instance of therule?s application match their overlap.We use zri,j to denote the topic that generatesthe jth argument of relation ri.
The probabilitythat the two arguments a1, a2 were drawn fromthe same hidden topic factorizes as follows due tothe conditional independences in our model:5P (zr1,1 = zr2,1, zr1,2 = zr2,2|a1, a2) =P (zr1,1 = zr2,1|a1)P (zr1,2 = zr2,2|a2)4Similarity-based and discriminative methods are not ap-plicable to this task as they offer no straightforward wayto compare the similarity between selectional preferences oftwo relations.5Note that all probabilities are conditioned on an estimateof the parameters ?, ?, ?
from our model, which are omittedfor compactness.430To compute each of these factors we simplymarginalize over the hidden topics:P (zr1,j = zr2,j |aj) =TXt=1P (zr1,j = t|aj)P (zr2,j = t|aj)where P (z = t|a) can be computed usingBayes rule.
For example,P (zr1,1 = t|a1) =P (a1|zr1,1 = t)P (zr1,1 = t)P (a1)=?t(a1)?r1(t)P (a1)4.3.2 Experimental ConditionsIn order to evaluate LDA-SP?s ability to filter in-ferences based on selectional preferences we needa set of inference rules between the relations inour corpus.
We therefore mapped the DIRT In-ference rules (Lin and Pantel, 2001), (which con-sist of pairs of dependency paths) to TEXTRUN-NER relations as follows.
We first gathered all in-stances in the generalization corpus, and for eachr(a1, a2) created a corresponding simple sentenceby concatenating the arguments with the relationstring between them.
Each such simple sentencewas parsed using Minipar (Lin, 1998).
Fromthe parses we extracted all dependency paths be-tween nouns that contain only words present inthe TEXTRUNNER relation string.
These depen-dency paths were then matched against each pairin the DIRT database, and all pairs of associatedrelations were collected producing about 26,000inference rules.Following Pantel et al (2007) we randomlysampled 100 inference rules.
We then automati-cally filtered out any rules which contained a nega-tion, or for which the antecedent and consequentcontained a pair of antonyms found in WordNet(this left us with 85 rules).
For each rule we col-lected 10 random instances of the antecedent, andgenerated the consequent.
We randomly sampled300 of these inferences to hand-label.4.3.3 ResultsIn figure 5 we compare the precision and recall ofLDA-SP against the top two performing systemsdescribed by Pantel et al (ISP.IIM-?
and ISP.JIM,both using the CBC clusters (Pantel, 2003)).
Wefind that LDA-SP achieves both higher precisionand recall than ISP.IIM-?.
It is also able to achievethe high-precision point of ISP.JIM and can tradeprecision to get a much larger recall.0.0 0.2 0.4 0.6 0.8 1.00.40.60.81.0recallprecisionXOXOLDA?SPISP.JIMISP.IIM?ORFigure 5: Precision and recall on the inference fil-tering task.Top 10 Inference Rules Ranked by LDA-SPantecedent consequent KL-divwill begin at will start at 0.014999shall review shall determine 0.129434may increase may reduce 0.214841walk from walk to 0.219471consume absorb 0.240730shall keep shall maintain 0.264299shall pay to will notify 0.290555may apply for may obtain 0.313916copy download 0.316502should pay must pay 0.371544Bottom 10 Inference Rules Ranked by LDA-SPantecedent consequent KL-divlose to shall take 10.011848should play could do 10.028904could play get in 10.048857will start at move to 10.060994shall keep will spend 10.105493should play get in 10.131299shall pay to leave for 10.131364shall keep return to 10.149797shall keep could do 10.178032shall maintain have spent 10.221618Table 3: Top 10 and Bottom 10 ranked inferencerules ranked by LDA-SPafter automatically filter-ing out negations and antonyms (using WordNet).In addition we demonstrate LDA-SP?s abil-ity to rank inference rules by measuring theKullback Leibler Divergence6 between the topic-distributions of the antecedent and consequent, ?r1and ?r2 respectively.
Table 3 shows the top 10 andbottom 10 rules out of the 26,000 ranked by KLDivergence after automatically filtering antonyms(using WordNet) and negations.
For slight varia-tions in rules (e.g., symmetric pairs) we mentiononly one example to show more variety.6KL-Divergence is an information-theoretic measure ofthe similarity between two probability distributions, and de-fined as follows: KL(P ||Q) =Px P (x) logP (x)Q(x) .4314.4 A Repository of Class-Based PreferencesFinally we explore LDA-SP?s ability to produce arepository of human interpretable class-based se-lectional preferences.
As an example, for the re-lation was born in, we would like to infer thatthe plausible arguments include (person, location)and (person, date).Since we already have a set of topics, ourtask reduces to mapping the inferred topics to anequivalent class in a taxonomy (e.g., WordNet).We experimented with automatic methods suchas Resnik?s, but found them to have all the sameproblems as directly applying these approaches tothe SP task.7 Guided by the fact that we have arelatively small number of topics (600 total, 300for each argument) we simply chose to label themmanually.
By labeling this small number of topicswe can infer class-based preferences for an arbi-trary number of relations.In particular, we applied a semi-automaticscheme to map topics to WordNet.
We first appliedResnik?s approach to automatically shortlist a fewcandidate WordNet classes for each topic.
We thenmanually picked the best class from the shortlistthat best represented the 20 top arguments for atopic (similar to Table 1).
We marked all incoher-ent topics with a special symbol ?.
This processtook one of the authors about 4 hours to complete.To evaluate how well our topic-class associa-tions carry over to unseen relations we used thesame random sample of 100 relations from thepseudo-disambiguation experiment.8 For each ar-gument of each relation we picked the top two top-ics according to frequency in the 5 Gibbs samples.We then discarded any topics which were labeledwith ?
; this resulted in a set of 236 predictions.
Afew examples are displayed in table 4.We evaluated these classes and found the accu-racy to be around 0.88.
We contrast this with Pan-tel?s repository,9 the only other released databaseof selectional preferences to our knowledge.
Weevaluated the same 100 relations from his websiteand tagged the top 2 classes for each argument andevaluated the accuracy to be roughly 0.55.7Perhaps recent work on automatic coherence ranking(Newman et al, 2010) and labeling (Mei et al, 2007) couldproduce better results.8Recall that these 100 were not part of the original 3,000in the generalization corpus, and are, therefore, representativeof new ?unseen?
relations.9http://demo.patrickpantel.com/Content/LexSem/paraphrase.htmarg1 class relation arg2 classpolitician#1 was running for leader#1people#1 will love show#3organization#1 has responded to accusation#2administrative unit#1 has appointed administrator#3Table 4: Class-based Selectional Preferences.We emphasize that tagging a pair of class-basedpreferences is a highly subjective task, so these re-sults should be treated as preliminary.
Still, theseearly results are promising.
We wish to undertakea larger scale study soon.5 Conclusions and Future WorkWe have presented an application of topic mod-eling to the problem of automatically computingselectional preferences.
Our method, LDA-SP,learns a distribution over topics for each rela-tion while simultaneously grouping related wordsinto these topics.
This approach is capable ofproducing human interpretable classes, however,avoids the drawbacks of traditional class-based ap-proaches (poor lexical coverage and ambiguity).LDA-SP achieves state-of-the-art performance onpredictive tasks such as pseudo-disambiguation,and filtering incorrect inferences.Because LDA-SP generates a complete proba-bilistic model for our relation data, its results areeasily applicable to many other tasks such as iden-tifying similar relations, ranking inference rules,etc.
In the future, we wish to apply our modelto automatically discover new inference rules andparaphrases.Finally, our repository of selectional pref-erences for 10,000 relations is available athttp://www.cs.washington.edu/research/ldasp.AcknowledgmentsWe would like to thank Tim Baldwin, ColinCherry, Jesse Davis, Elena Erosheva, StephenSoderland, Dan Weld, in addition to the anony-mous reviewers for helpful comments on a previ-ous draft.
This research was supported in part byNSF grant IIS-0803481, ONR grant N00014-08-1-0431, DARPA contract FA8750-09-C-0179, aNational Defense Science and Engineering Grad-uate (NDSEG) Fellowship 32 CFR 168a, and car-ried out at the University of Washington?s TuringCenter.432ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InACL-08: HLT.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Discriminative learning of selectional pref-erence from unlabeled text.
In EMNLP.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In EACL, pages 103?111,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Andrew Carlson, Justin Betteridge, Richard C. Wang,Estevam R. Hruschka Jr., and Tom M. Mitchell.2010.
Coupled semi-supervised learning for infor-mation extraction.
In WSDM 2010.Harr Chen, S. R. K. Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Global models of documentstructure using latent permutations.
In NAACL.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Comput.
Linguist.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-based models of word cooccur-rence probabilities.
In Machine Learning.Hal Daume?
III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics.Hal Daume III.
2007. hbc: Hierarchical bayes com-piler.
http://hal3.name/hbc.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics.Elena Erosheva, Stephen Fienberg, and John Lafferty.2004.
Mixed-membership models of scientific pub-lications.
Proceedings of the National Academy ofSciences of the United States of America.Oren Etzioni, Michael Cafarella, Doug Downey,Ana maria Popescu, Tal Shaked, Stephen Soderl,Daniel S. Weld, and Alex Yates.
2005.
Unsuper-vised named-entity extraction from the web: An ex-perimental study.
Artificial Intelligence.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Comput.
Linguist.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proc Natl Acad Sci U S A.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Comput.Linguist.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In ACL-08: HLT.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the mdl principle.Comput.
Linguist.Dekang Lin and Patrick Pantel.
2001.
Dirt-discoveryof inference rules from text.
In KDD.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proc.
Workshop on the Evaluation ofParsing Systems.Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topicmodels.
In KDD.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In EMNLP.David Newman, Arthur Asuncion, Padhraic Smyth,and Max Welling.
2009.
Distributed algorithms fortopic models.
JMLR.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation of topiccoherence.
In NAACL-HLT.Diarmuid O?
Se?aghdha.
2010.
Latent variable mod-els of selectional preference.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard H. Hovy.
2007.Isp: Learning inferential selectional preferences.
InHLT-NAACL.Patrick Andre Pantel.
2003.
Clustering by commit-tee.
Ph.D. thesis, University of Alberta, Edmonton,Alta., Canada.Joseph Reisinger and Marius Pasca.
2009.
Latent vari-able models of concept-attribute attachment.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP.P.
Resnik.
1996.
Selectional constraints: aninformation-theoretic model and its computationalrealization.
Cognition.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proc.
of the ACL SIGLEX Work-shop on Tagging Text with Lexical Semantics: Why,What, and How?433Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via em-based clustering.
InProceedings of the 37th annual meeting of the Asso-ciation for Computational Linguistics on Computa-tional Linguistics.Lenhart Schubert and Matthew Tong.
2003.
Extract-ing and evaluating general world knowledge fromthe brown corpus.
In In Proc.
of the HLT-NAACLWorkshop on Text Meaning, pages 7?13.Benjamin Van Durme and Daniel Gildea.
2009.
Topicmodels for corpus-centric knowledge generalization.In Technical Report TR-946, Department of Com-puter Science, University of Rochester, Rochester.Tae Yano, William W. Cohen, and Noah A. Smith.2009.
Predicting response to political blog postswith topic models.
In NAACL.L.
Yao, D. Mimno, and A. Mccallum.
2009.
Effi-cient methods for topic model inference on stream-ing document collections.
In KDD.434
