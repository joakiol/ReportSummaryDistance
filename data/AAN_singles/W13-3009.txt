Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 83?92,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsWhy Letter Substitution Puzzles are Not Hard to Solve: A Case Study inEntropy and Probabilistic Search-ComplexityEric CorlettUniversity of Toronto10 King?s College Rd., Room 3302Toronto, ON, Canada M5S 3G4ecorlett@cs.toronto.eduGerald PennUniversity of Toronto10 King?s College Rd., Room 3302Toronto, ON, Canada M5S 3G4gpenn@cs.toronto.eduAbstractIn this paper we investigate the theoreticalcauses of the disparity between the theo-retical and practical running times for theA?
algorithm proposed in Corlett and Penn(2010) for deciphering letter-substitutionciphers.
We argue that the difference seenis due to the relatively low entropies of theprobability distributions of character tran-sitions seen in natural language, and wedevelop a principled way of incorporat-ing entropy into our complexity analysis.Specifically, we find that the low entropyof natural languages can allow us, withhigh probability, to bound the depth of theheuristic values expanded in the search.This leads to a novel probabilistic boundon search depth in these tasks.1 IntroductionWhen working in NLP, we can find ourselvesusing algorithms whose worst-case running timebounds do not accurately describe their empiri-cally determined running times.
Specifically, wecan often find that the algorithms that we are us-ing can be made to run efficiently on real-worldinstances of their problems despite having theo-retically high running times.
Thus, we have an ap-parent disparity between the theoretical and prac-tical running times of these algorithms, and so wemust ask why these algorithms can provide resultsin a reasonable time frame.
We must also ask towhat extent we can expect our algorithms to re-main practical as we change the downstream do-mains from which we draw problem instances.At a high level, the reason such algorithms canwork well in the real world is that the real worldapplications from which we draw our inputs donot tend to include the high complexity inputs.
Inother words, our problem space either does notcover all possible inputs to the algorithm, or itdoes, but with a probability distribution that givesa vanishingly small likelihood to the ?hard?
inputs.Thus, it would be beneficial to incorporate into ourrunning time analysis the fact that our possible in-puts are restricted, even if only restricted in rela-tive frequency rather than in absolute terms.This means that any running time that we ob-serve must be considered to be dependent on thedistribution of inputs that we expect to samplefrom.
It probably does not come as a surprise thatany empirical analysis of running time carries withit the assumption that the data on which the testswere run are typical of the data which we expectto see in practice.
Yet the received wisdom on theasymptotic complexity of algorithms in computa-tional linguistics (generally what one might seein an advanced undergraduate algorithms curricu-lum) has been content to consider input only interms of its size or length, and not the distributionfrom which it was sampled.
Indeed, many algo-rithms in NLP actually take entire distributions asinput, such as language models.
Without a moremature theoretical understanding of time complex-ity, it is not clear exactly what any empirical run-ning time results would mean.
A worst-case com-plexity result gives a guarantee that an algorithmwill take no more than a certain number of stepsto complete.
An average-case result gives the ex-pected number of steps to complete.
But an empir-ical running time found by sampling from a distri-bution that is potentially different from what thealgorithm was designed for is only a lesson in howtruly different the distribution is.It is also common for the theoretical study ofasymptotic time complexity in NLP to focus onthe worst-case complexity of a problem or algo-rithm rather than an expected complexity, in spiteof the existence for now over 20 years of methodsfor average-case analysis of an algorithm.
Eventhese, however, often assume a uniform distribu-83tion over input, when in fact the true expectationmust consider the probability distribution that wewill draw the inputs from.
Uniform distributionsare only common because we may not know whatthe distribution is beforehand.Ideally, we should want to characterize the run-ning time of an algorithm using some known prop-erties of its input distribution, even if the precisedistribution is not known.
Previous work that at-tempts this does exist.
In particular, there is a vari-ant of analysis referred to as smoothed analysiswhich gives a bound on the average-case runningtime of an algorithm under the assumption that allinputs are sampled with Gaussian measurement er-ror.
As we will argue in Section 2, however, thisapproach is of limited use to us.We instead approach the disparity of theoreticaland practical running time by making use of statis-tics such as entropy, which are taken from the in-put probability distributions, as eligible factors inour analysis of the running time complexity.
Thisis a reasonable approach to the problem, in view ofthe numerous entropic studies of word and charac-ter distributions dating back to Shannon.Specifically, we analyze the running time of theA?
search algorithm described in Corlett and Penn(2010).
This algorithm deciphers text that hasbeen enciphered using a consistent letter substitu-tion, and its running time is linear in the length ofthe text being deciphered, but theoretically expo-nential in the size of the input and output alpha-bets.
This na?
?ve theoretical analysis assumes thatcharacters are uniformly distributed, however.
Afar more informative bound is attainable by mak-ing reference to the entropy of the input.
Be-cause the algorithm takes a language model as oneof its inputs (the algorithm is guaranteed to findthe model-optimal letter substitution over a giventext), there are actually two input distributions: thedistribution assumed by the input language model,and the distribution from which the text to be de-ciphered was sampled.
Another way to view thisproblem is as a search for a permutation of lettersas the outcomes of one distribution such that thetwo distributions are maximally similar.
So ourinformative bound is attained through reference tothe cross-entropy of these two distributions.We first formalize our innate assumption thatthese two distributions are similar, and build anupper bound for the algorithm?s complexity thatincorporates the cross-entropy between the twodistributions.
The analysis concludes that, ratherthan being exponential in the length of the input orin the size of the alphabets, it is merely exponen-tial in the cross-entropy of these two distributions,thus exposing the importance of their similarity.Essentially, our bound acts as a probability distri-bution over the necessary search depth.2 Related WorkThe closest previous work to the analysis pre-sented here is the use of smoothed analysis to ex-plain the tractable real-world running time of anumber of algorithms with an exponential worst-case complexity.
These algorithms include thesimplex algorithm, as described by Spielman andTeng (2004), the k-means clustering algorithm, asdescribed by Arthur et al(2009) and others.
Asin our current approach, smoothed analysis worksby running a general average-case analysis of thealgorithms without direct knowledge of the distri-bution from which the problem inputs have beendrawn.
The assumption made in smoothed anal-ysis is that every input has been read with someGaussian measurement error.
That is, in a typi-cal worst-case analysis, we may have an adversarychoose any input for our algorithm, after which wemust calculate how bad the resulting running timemight be, but in a smoothed analysis, the adver-sary gives us input by placing it into the real worldso that we may measure it, and this measurementadds a small error drawn from a Gaussian dis-tribution to the problem instance.
The point ofsmoothed analysis is to find the worst average-caserunning time, under these conditions, that the ad-versary can subject us to.
Thus the analysis is anaverage case, subject to this error, of worst cases.In the papers cited above, this method of analysiswas able to drop running times from exponentialto polynomial.It is unfortunate that this approach does notreadily apply to many of the algorithms that weuse in NLP.
To see why this is, simply note thatwe can only add a small Gaussian error to our in-puts if our inputs themselves are numerical.
If theinputs to our algorithms are discrete, say, in theform of strings, then Gaussian errors are not mean-ingful.
Rather, we must ask what sort of error wecan expect to see in our inputs, and to what extentthese errors contribute to the running time of ouralgorithms.
In the case of decipherment, ?error?is committed by substituting one character for an-84other consistently.The strongest known result on the search com-plexity of A?
is given in Pearl (1984).
This workfound that, under certain assumptions, a bound onthe absolute error between the heuristic used andthe true best cost to reach the goal yields a polyno-mial worst-case depth for the search.
This happenswhen the bound is constant across search instancesof different sizes.
On the other hand, if the relativeerror does not have this constant bound, the searchcomplexity can still be exponential.
This analy-sis assumes that the relative errors in the heuristicare independent between nodes of the search tree.It is also often very difficult even to calculate thevalue of a heuristic that possesses such a bound,as it might involve calculating the true best cost,which can be as difficult as completely solving asearch problem instance (Korf et al 2001).
Thus,most practical heuristics still give rise to theoreti-cally exponential search complexities in this view.In Korf and Reid (1998) and Korf et al(2001),on the other hand, several practical problems aretreated, such as random k-SAT, Rubik?s cubes, orsliding tile puzzles, which are not wholly unlikedeciphering letter substitution puzzles in that theycalculate permutations, and therefore can assume,as we do, that overall time complexity directly cor-responds to the number of nodes visited at differ-ent depths in the search tree that have a heuris-tic low enough to guarantee node expansion.
Buttheir analysis assumes that it is possible to both es-timate and use a probability distribution of heuris-tic values on different nodes of the search graph,whereas in our task, this distribution is very dif-ficult to sample because almost every node in thesearch graph has a worse heuristic score than thegoal does, and would therefore never be expanded.Without an accurate idea of what the distributionof the heuristic is, we cannot accurately estimatethe complexity of the algorithm.
On the otherhand, their analysis makes no use of any estimatesof the cost of reaching the goal, because the prac-tical problems that they consider do not allow forparticularly accurate estimates.
In our treatment,we find that the cost to reach the goal can be esti-mated with high probability, and that this estimateis much less than the cost of most nodes in thesearch graph.
These different characteristics allowus to formulate a different sort of bound on thesearch complexity for the decipherment problem.3 The AlgorithmWe now turn to the algorithm given in Corlett andPenn (2010) which we will investigate, and we ex-plain the model we use to find our bound.The purpose of the algorithm is to allow us toread a given ciphertext C which is assumed tobe generated by putting an unknown plaintext Pthrough an unknown monoalphabetic cipher.We will denote the ciphertext alphabet as ?cand the plaintext alphabet as ?p.
Given any stringT , we will denote n(T ) as the length of T .
Fur-thermore, we assume that the plaintext P is drawnfrom some string distribution q.
We do not assumeq to be a trigram distribution, but we do require itto be a distribution from which trigrams can becalculated (e.g, a 5-gram corpus will in generalhave probabilities that cannot be predicted usingthe associated trigrams, but the associated trigramcorpus can be recovered from the 5-grams).It is important to realize in the algorithm de-scription and analysis that q may also not beknown exactly, but we only assume that it exists,and that we can approximate it with a known tri-gram distribution p. In Corlett and Penn (2010),for example, p is the trigram distribution found us-ing the Penn treebank.
It is assumed that this is agood approximation for the distribution q, whichin Corlett and Penn (2010) is the text in Wikipediafrom which ciphers are drawn.
As is commonwhen dealing with probability distributions overnatural languages, we assume that both p and qare stationary and ergodic, and we furthermore as-sume that p is smooth enough that any trigram thatcan be found in any string generated by q occurs inp (i.e., we assume that the cross entropyH(p, q) isfinite).The algorithm works in a model in which, forany run of the algorithm, the plaintext string Pis drawn according to the distribution q.
We donot directly observe P , but instead its encodingusing the cipher key, which we will call piT .
Weobserve the ciphertext C = pi?1T (P ).
We note thatpiT is unknown, but that it does not change as newciphertexts are drawn.Now, the way that the algorithm in Corlett andPenn (2010) works is by searching over the pos-sible keys to the cipher to find the one that maxi-mizes the probability of the plaintext according tothe distribution p. It does so as follows.In addition to the possible keys to the cipher,85weakened cipher keys called partial solutions areadded to the search space.
A partial solution ofsize k (denoted as pik) is a section of a possible fullcipher key which is only defined on k charactertypes in the cipher.
We consider the charactertypes to be fixed according to some preset order,and so the k fixed letters in pik do not changebetween different partial solutions of size k.Given a partial solution pik, a string pin(C)k (C)is defined whose probability we use as an upperbound for the probability of the plaintext when-ever the true solution to the cipher contains pikas a subset.
The string pin(C)k (C) is the mostlikely string that we can find that is consistentwith C on the letters fixed by pik.
That is, wedefine the set ?k so that S ?
?k iff wheneversi and ci are the characters at index i in S andC, then si = pik(ci) if ci is fixed in pik.
Notethat if ck is not fixed in pik, we let si take anyvalue.
We extend the partial character functionto the full string function pin(C)k on ?n(C)c so thatpin(C)k (C) = argmax(S?
?k)probp(S).In Corlett and Penn (2010), the value pin(C)k (C)is efficiently computed by running it throughthe Viterbi algorithm.
That is, given C, p andpik, a run of the Viterbi algorithm is set up inwhich the letter transition probabilities are thosethat are given in p. In order to describe theemission probabilities, suppose that we partitionthe ciphertext alphabet ?c into two sets ?1 and?2, where ?1 is the set of ciphertext letters fixedby pik.
For any plaintext letter y ?
?p, if thereis a ciphertext letter x ?
?1 such that y ?
x isa rule in pik, then the emission probability that ywill be seen as x is set to 1, and the probabilitythat y will be seen as any other letter is set to 0.On the other hand, if there is no rule y ?
x inpik for any ciphertext letter x, then the emissionprobability associated with y is uniform over theletters x ?
?2 and 0 for the letters x ?
?1.The search algorithm described in Corlett andPenn (2010) uses the probability of the stringpin(C)k (C), or more precisely, the log probabil-ity ?logprobp(pin(C)k (C)), as an A?
heuristic overthe partial solutions pik.
In this search, an edgeis added from a size k partial solution pik to asize k + 1 partial solution pik+1 if pik agrees withpik+1 wherever it is defined.
The score of a nodepik is the log probability of its associated string:?logprobp(pin(C)k (C)).
We can see that if pik hasan edge leading to pik+1, then ?k+1 ?
?k, so that?logprobp(pin(C)k+1 (C)) ?
?logprobp(pin(C)k (C)).Thus, the heuristic is nondecreasing.
Moreover,by applying the same statement inductively we cansee that any full solution to the cipher that has pikas a subset must have a score at least as great asthat of pik.
This means that the score never over-estimates the cost of completing a solution, andtherefore that the heuristic is admissible.4 AnalysisThe bound that we will prove is that for any k > 0and for any ?, ?
> 0, there exists an n ?
N suchthat if the length n(C) of the cipher C is at leastn, then with probability at least 1 ?
?, the searchfor the key to the cipher C requires no more than2n?(H(p,q)+?)
expansions of any partial solution ofsize k to complete.
Applying the same bound overevery size k of partial solution will then give usthat for any ?, ?
> 0, there exists a n0 > 0 suchthat if the length n(C) of the cipher C is at leastn, then with probability at least 1 ?
?, the searchfor the key to the cipher C requires no more than2n(H(p,q)+?)
expansions of any partial solution ofsize greater than 0 to complete (note that there isonly one partial solution of size 0).Let pi?
be the solution that is found by thesearch.
This solution has the property that it is thefull solution that induces the most probable plain-text from the cipher, and so it produces a plaintextthat is at least as likely as that of the true solutionP .
Thus, we have that ?logprobp(pi?n(C)(C)) ?
?logprobp(pin(C)T (C)) = ?logprobp(P ).We find our bound by making use of the fact thatan A?
search never expands a node whose scoreis greater than that of the goal node pi?.
Thus, apartial solution pik is expanded only if?logprobp(pin(C)k (C)) ?
?logprobp(pi?n(C)(C)).Since?logprobp(pi?n(C)(C)) ?
?logprobp(P ),we have that pik is expanded only if?logprobp(pin(C)k (C)) ?
?logprobp(P ).So we would like to count the number of solutionssatisfying this inequality.86We would first like to approximate the value of?logprobp(P ), then.
But, since P is drawn froman ergodic stationary distribution q, this valuewill approach the cross entropy H(p, q) with highprobability: for any ?1, ?1 > 0, there exists ann1 > 0 such that if n(C) = n(P ) > N1, then| ?
logprobp(P )/n(C)?H(p, q)| < ?1with probability at least 1 ?
?1.
In this case, wehave that ?logprobp(P ) < n(C)(H(p, q) + ?1).Now, if k is fixed, and if pik and pi?k are two dif-ferent size k partial solutions, then pik and pi?k mustdisagree on at least one letter assignment.
Thus,the sets ?k and ?
?k must be disjoint.
But then wealso have that pin(C)k (C) 6= pin(C)?k (C).
Therefore,if we can find an upper bound for the size of theset{S ?
?n(C)p |S = pin(C)k (C) for some pik},we will have an upper bound on the number oftimes the search will expand any partial solutionof size k. We note that under the previous assump-tions, and with probability at least 1?
?1, none ofthese strings can have a log probability larger thann(C)(H(p, q) + ?1).For any plaintext string C drawn from q, we letaPb be the substring of P between the indices aand b.
Similarly, we let aSb be the substring ofS = pin(C)k (C) between the indices a and b.We now turn to the proof of our bound: Let?, ?
> 0 be given.
We give the following threebounds on n:(a) As stated above, we can choose n1 so that forany string P drawn from q with length at leastn1,| ?
logprobp(P )/n(P )?H(p, q)| < ?1/2with probability at least 1?
?/3.
(b) We have noted that if k is fixed then any twosize k partial solutions must disagree on atleast one of the letters that they fix.
So if wehave a substring aPb of P with an instance ofevery letter type fixed by the partial solutionsof size k, then the substrings aSb of S mustbe distinct for every S ?
{S ?
?n(C)p |S =pin(C)k (C) for some pik}.
Since q is ergodic,we can find an n2 such that for any string Pdrawn from q with length at least n2, everyletter fixed in pik can be found in some lengthn2 substring P2 of P , with probability at least1?
?/3.
(c) By the Lemma below, there exists an n?
> 0such that for all partial solutions pik, there ex-ists a trigram distribution rk on the alphabet?p such that if S = pin(C)k (C) and b ?
a =n > n?, then????
?logprob(aSb)n?H(p, rk)????
< ?/4with a probability of at least 1?
?/3.Let n = max(n1, n2, n?).
Then, the probabilityof any single one of the properties in (a), (b) or (c)failing in a string of length at least n is at most ?/3,and so the probability of any of them failing is atmost ?.
Thus, with a probability of at least 1?
?, allthree of the properties hold for any string P drawnfrom q with length at least n. Let P be drawn fromq, and suppose n(P ) > n. Let aPb be a length nsubstring of P containing a token of every lettertype fixed by the size k partial solutions.Suppose that pik is a partial solution such that?logprobp(pin(C)k (C)) ?
n(P )(H(p, q) + ?/2).Then, letting S = pin(C)k (C), we have that if????
?logprob(S)n(P )?H(p, rk)????
< ?/4and????
?logprob(aSb)n?H(p, rk)????
< ?/4it follows that????
?logprob(S)n(P )+logprob(aSb)n?????????
?logprob(S)n(P )?H(p, rk)????+????
?H(p, rk)?logprob(aSb)n?????
?/4 + ?/4 = ?/2But then,?logprob(aSb)n<?logprob(S)n(P )+ ?/2?n(P )(H(p, q) + ?/2)n(P )+ ?/2= H(p, q) + ?.87So, for our bound we will simply need to find thenumber of substrings aSb such that?
log probp(aSb) < n(H(p, q) + ?
).Letting IH(aSb) = 1 if ?logprobp(aSb) <n(H(p, q) + ?)
and 0 otherwise, the number ofstrings we need becomesXaSb?
?n(C)pIH(aSb) = 2n?(H(p,q)+?)XaSb??n(C)pIH(aSb)2?n?(H(p,q)+?)<2n?(H(p,q)+?)XaSb?
?n(C)pIH(aSb)probp(aSb)(since ?
log probp(aSb) < n(H(p, q) + ?
)implies probp(aSb) > 2?n?(H(p,q)+?))?
2n?(H(p,q)+?)XaSb?
?n(C)pprobp(aSb)= 2n?(H(p,q)+?
)Thus, we have a bound of 2n?(H(p,q)+?)
onthe number of substrings of length n satisfying?
log probp(aSb) < n(H(p, q) + ?).
Since weknow that with probability at least 1?
?, these arethe only strings that need be considered, we haveproven our bound.
4.1 Lemma:We now show that for any fixed k > 0and ?
?, ??
> 0, there exists some n?
> 0such that for all partial solutions pik, thereexists a trigram distribution rk on the al-phabet ?p such that if S = pin(C)k (C) andb ?
a = n > n?, |?logprob(aSb)n ?
H(p, rk)| < ?
?with a probability of at least 1?
?
?.Proof of Lemma: Given any partial solution pik,it will be useful in this section to consider thestrings S = pin(C)k (C) as functions of the plain-text P rather than the ciphertext C. Since C =pi?1T (P ), then, we will compose pin(C)k and pi?1Tto get pin(C)?k (P ) = pin(C)k (pi?1T (P )).
Now, sincepiT is derived from a character bijection between?c and ?p, and since pin(C)k fixes the k charactertypes in ?c that are defined in pik, we have thatpin(C)?k fixes k character types in ?p.
Let ?P1 bethe set of k character types in ?p that are fixed bypin(C)?k , and let ?P2 = ?p \?P1 .
We note that ?P1and ?P2 do not depend on which pik we use, butonly on k.Now, any string P which is drawn from qcan be decomposed into overlapping substringsby splitting it whenever it has see two adjacentcharacters from ?P1 .
When we see a bigram inP of this form, say, y1y2, we split P so that boththe end of the initial string and the beginning ofthe new string are y1y2.
Note that when we havemore than two adjacent characters from ?P1 wewill split the string more than once, so that we geta series of three-character substrings of P in ourdecomposition.
As a matter of bookkeeping wewill consider the initial segment to begin with twostart characters s with indices corresponding to 0and ?1 in P .
As an example, consider the stringP = friends, romans, countrymen, lend meyour earsWhere ?P1 = {?
?, ?, ?, ?a?, ?y?}.
In this case,we would decompose P into the strings ?ssfriends,?, ?, romans, ?, ?, countrymen, ?, ?, lend me ?, ?e y?,?
your e?
and ?
ears?.Let M be the set of all substrings that can begenerated in this way by decomposing strings Pwhich are drawn from q.
Since the end of anystring m ?M contains two adjacent characters in?P1 and since the presence of two adjacent char-acters in ?P1 signals a position at which a stringwill be decomposed into segments, we have thatthe set M is prefix-free.
Every string m ?
Mis a string in ?p, and so they will have probabili-ties probq(m) in q.
It should be noted that for anym ?
M the probability probq(m) may be differ-ent from the trigram probabilities predicted by q,but will instead be the overall probability in q ofseeing the string m.For any pair T, P of strings, let #(T, P ) be thenumber of times T occurs in P .
Since we as-sume that the strings drawn from q converge tothe distribution q, we have that for any ?3, ?3 >0 and any n4 > 0, there exists an n3 > 0such that for any substring P3 of P of lengthat least n3, where P is drawn from q, and forany m ?
M of length at most n4, the number|#(m,P )/len(P3) ?
probq(m)| < ?3 with prob-ability greater than 1?
?3.Now suppose that for some P drawn from qwe have a substring aPb of P such that aPb =m,m ?
M .
If S = pin(C)?k (P ), consider the sub-string aSb of S. Recall that the string functionpin(C)?k can map the characters in P to S in oneof two ways: if a character xi ?
?P1 is found atindex i in P , then the corresponding character in S88is pik(xi).
Otherwise, xi is mapped to whichevercharacter yi in ?P maximizes the probability in pof S given pin(C)?k (xi?2)pin(C)?k (xi?1)yi.
Since thevalues of pin(C)?k (xi?2), pin(C)?k (xi?1) and yi are in-terdependent, and since pin(C)?k (xi?2) is dependenton its previous two neighbors, the value that yitakes may be dependent on the values taken bypin(C)?k (xj) for indices j quite far from i. How-ever, we see that no dependencies can cross overa substring in P containing two adjacent charac-ters in ?P1 , since these characters are not trans-formed by pin(C)?k in a way that depends on theirneighbors.
Thus, if aPb = m ?
M , the endpointsof aPb are made up of two adjacent characters in?P1 , and so the substring aSb of S depends onlyon the substring aPb of P .
Specifically, we see thataSb = pin(C)?k (aPb).Since we can decompose any P into overlap-ping substrings m1,m2, .
.
.
,mt in M , then, wecan carry over this decomposition into S to breakS into pin(C)?k (m1), pin(C)?k (m2), .
.
.
, pin(C)?k (mt).Note that the score generated by S inthe A?
search algorithm is the sum?1?i?
logprobp(yi?2yi?1yi), where yi isthe ith character in S. Also note that ev-ery three-character sequence yi?2yi?1yioccurs exactly once in the decompositionpin(C)?k (m1), pin(C)?k (m2), .
.
.
, pin(C)?k (mt).
Sincefor anym the number of occurrences of pin(C)?k (m)in S under this decomposition will be equal to thenumber of occurrences of m in P , we have that?logprobp(S) =X1?i?n(P )logprobp(yi?2yi?1yi)=Xm?M#(m,P ) ?
(?logprobp(pin(C)?k (m))).Having finished these definitions, we cannow define the distribution rk.
In princi-ple, this distribution should be the limit ofthe frequency of trigram counts of the stringsS = pin(C)?k (P ), where n(P ) approaches infin-ity.
Given a string S = pin(C)?k (P ), where Pis drawn from q, and given any trigram y1y2y3of characters in ?p, this frequency count is#(y1y2y3,S)n(P ) .
Breaking S into its component sub-strings pin(C)?k (m1), pin(C)?k (m2), .
.
.
, pin(C)?k (mt),as we have done above, we see that any instanceof the trigram y1y2y3 in S occurs in exactly one ofthe substrings pin(C)?k (mi), 1 ?
i ?
t. Groupingtogether similar mis, we find#(y1y2y3, S)n(P )=tPi=1#(y1y2y3, pin(C)?k (mi))n(P )=Pm?M#(y1y2y3, pin(C)?k (m)) ?#(m,P )n(P )As n(P ) approaches infinity, we find that #(m,P )n(P )approaches probq(m), and so we can writeprobrk (y1y2y3) =Xm?M#(y1y2y3, pin(C)?k (m))probq(m).Since 0 ?
?m?M #(y1y2y3, pin(C)?k (m))probq(m)when P is sampled from q we have thatXy1y2y3probrk (y1y2y3)=Xy1y2y3Xm?M#(y1y2y3, pin(C)?k (m))probq(m)= limn(P )?
?Xy1y2y3Xm?M#(y1y2y3, pin(C)?k (m))#(m,P )n(P )= limn(P )?
?Xm?MXy1y2y3#(y1y2y3, pin(C)?k (m))#(m,P )n(P )= limn(P )?
?Xm?M(n(pin(C)?k (m))?
2)#(m,P )n(P )= limn(P )??Xm?M(n(m)?
2)#(m,P )n(P )= limn(P )?
?n(P )n(P )= 1,so we have that probrk is a valid probability distri-bution.
In the above calculation we can rearrangethe terms, so convergence implies absolute conver-gence.
The sum?y1y2y3 #(y1y2y3, pin(C)?k (m))gives (n(pin(C)?k (m)) ?
2) because there is onetrigram for every character in pin(C)?k (m), less twoto compensate for the endpoints.
However, sincethe different m overlap by two in a decompositionfrom P , the sum (n(m) ?
2)#(m,P ) just givesback the length n(P ), allowing for the fact thatthe initial m has two extra start characters.Having defined rk, we can now find the value ofH(p, rk).
By definition, this term will be89Xy1y2y3?logprobp(y1y2y3)probrk (y1y2y3)=Xy1y2y3?logprobp(y1y2y3)Xm?M#(y1y2y3, pin(C)?k (m))probq(m)=Xm?MXy1y2y3?logprobp(y1y2y3)#(y1y2y3, pin(C)?k (m))probq(m)=Xm?M?logprobp(m)probq(m).Now, we can finish the proof of the Lemma.Holding k fixed, let ?
?, ??
> 0 be given.
Since wehave assumed that p does not assign a zero proba-bility to any trigram generated by q, we can find atrigram x1x2x3 generated by q whose probabilityin p is minimal.
Let X = ?logprobp(x1x2x3),and note that probp(x1x2x3) > 0 impliesX < ?.
Since we know by the argu-ment above that when P is sampled from q,limn(P )??
(?m?M(npin(C)?k (m)?2)?#(m,P )n(P ) ) = 1,we have that?m?M(npin(C)?k (m)?
2)probq(m) = 1.Thus, we can choose n4 so that?m?M,n(m)?n4(npin(C)?k (m)?
2)probq(m)> 1?
?
?/4X.Let Y = |{m ?
M,n(m) ?
n4}|, and choosen?
such that if P is sampled from q and aPb is asubstring of P with length greater than n?, thenwith probability at least 1 ?
?
?, for every m ?
Mwe will have that???
?#(m, aPb)n(aPb)?
probq(m)????
< ?
?/4XY (n4 ?
2).Let pik be any partial solution of length k, and letrk be the trigram probability distribution describedabove.
Then let P be sampled from q, and let S =pin(C)k (C) = pin(C)?k (P ), and let a, b be indices ofS such that b ?
a = n > n?.
Finally, we willpartition the set M as follows: we let M ?
be theset {m ?M |n(n) ?
n4} andM ??
be the set {m ?M |n(m) > n4}.
Thus, we have that????
?logprob(aSb)n?H(p, rk)????=????
?Pm?M #(m, aPb)(?logprobp(pin(C)?k (m))n?Xm?Mprobq(m) ?
(?logprobp(pin(C)?k (m))????
?.Grouping the terms of these sums into the indexsets M ?
and M ?
?, we find that this value is at most?????Xm?M?
?#(m, aPb)n?
probq(m)?
(?logprobp(pin(C)?k (m))?????+?????Xm?M??
?#(m, aPb)n?
probq(m)?
(?logprobp(pin(C)?k (m))????
?Furthermore, we can break up the sum over theindex M ??
to bound this value by?????Xm?M?
?#(m, aPb)n?
probq(m)?
(?logprobp(pin(C)?k (m))?????+?????Xm?M?
?#(m, aPb)n(?logprobp(pin(C)?k (m))?????+?????Xm?M?
?probq(m)(?logprobp(pin(C)?k (m))????
?Now, for any m ?
M , we have thatthe score ?logprobp(pin(C)?k (m) equals?1?i?n(m)?2?logprobp(yiyi+1yi+2), where yiis the character at the index i in pin(C)?k (m).Taking the maximum possible values for?logprobp(yiyi+1yi+2), we find that this sum isat most (n(m)?
2)X .
Applying this bound to theprevious formula, we find that it is at most?????Xm?M?
?#(m, aPb)n?
probq(m)?(n(m)?
2)X?????+?????Xm?M?
?#(m, aPb)n(n(m)?
2)X?????+?????Xm?M?
?probq(m) ?
(n(m)?
2)X????
?.We can bound each of these three terms separately.Looking at the first sum in this series, we find thatwith probability at least 1?
??,90?????Xm?M?
?#(m, aPb)n?
probq(m)?(n(m)?
2)X?????(*)?Xm?M????
?#(m, aPb)n?
probq(m)????
(n(m)?
2)X?Xm?M??????
?4(n4 ?
2)XY????
?
(n(m)?
2)X?Xm?M???????4Y????=?
?4YXm?M?1 =?
?4YY = ?/4.In order to bound the second sum, we make useof the fact that?m?M #(m, aPb)(n(m) ?
2) =n(aPb) = n to find that once again, with probabil-ity greater than 1?
??,?????Xm?M?
?#(m, aPb)n(n(m)?
2)X??????Xm?M?????
?#(m, aPb)n(n(m)?
2)X????
.Since M ??
= M ?M ?, this value isXm?M???
?#(m, aPb)n(n(m)?
2)X?????Xm?M????
?#(m, aPb)n(n(m)?
2)X???
?=X ?Xm?M????
?#(m, aPb)n(n(m)?
2)X????
.This value can further be split into=X?Xm?M?????
?#(m, aPb)n+(1?1)probq(m)?(n(m)?2)X????
?X ?Xm?M?|probq(m)(n(m)?
2)X|?Xm?M????
?#(m, aPb)n?
probq(m)????
(n(m)?
2)X!Using our value for the sum in (*), we find thatthis is=X ?Xm?M?|probq(m)(n(m)?
2)X|+Xm?M????
?#(m, aPb)n?
probq(m)????
(n(m)?
2)X?X ?Xm?M?|probq(m)(n(m)?
2)X|+?
?4,Using our definition of n4, we can further boundthis value by=X1?Xm?M?probq(m)(n(m)?
2)!+??4<X?1??1???4X??+??4=X??4X+??4=?
?2.Finally, we once again make use of the definitionof n4 to find that the last sum is?????Xm?M?
?probq(m) ?
(n(m)?
2)X?????=Xm?M?
?probq(m) ?
(n(m)?
2)X= XXm?M?
?probq(m) ?
(n(m)?
2)< X??4X=?
?4.Adding these three sums together, we get??4+??2+?
?4= ??.Thus,???
?logprob(aSb)n ?H(p, rk)???
< ??
with prob-ability greater than 1?
?
?, as required.
5 ConclusionIn this paper, we discussed a discrepancy betweenthe theoretical and practical running times of cer-tain algorithms that are sensitive to the entropiesof their input, or the entropies of the distributionsfrom which their inputs are sampled.
We thenused the algorithm from Corlett and Penn (2010)as a subject to allow us to investigate ways totalk about average-case complexity in light ofthis discrepancy.
Our analysis was sufficientto give us a bound on the search complexityof this algorithm which is exponential in thecross-entropy between the training distributionand the input distribution.
Our method in effectyields a probabilistic bound on the depth of thesearch heuristic used.
This leads to an exponen-tially smaller search space for the overall problem.We must note, however, that our analysis doesnot fully reconcile the discrepancy between the91theoretical and practical running time for thisalgorithm.
In particular, our bound still does notexplain why the number of search nodes expandedby this algorithm tends to converge on one perpartial solution size as the length of the stringgrows very large.
As such, we are interested infurther studies as to how to explain the runningtime of this algorithm.
It is our opinion that thiscan be done by refining our description of the sets?k to exclude strings which cannot be consideredby the algorithm.
Not only would this allow usto reduce the overall number of strings we wouldhave to count when determining the bound, butwe would also have to consider fewer stringswhen determining the value of n?.
Both changeswould reduce the overall complexity of our bound.This general strategy may have the potential toilluminate the practical time complexities of ap-proximate search algorithms as well.ReferencesDavid Arthur, Bodo Manthey, and Heiko Ro?glin.k-means has polynomial smoothed complex-ity.
In The 50th Annual Symposium on Foun-dations of Computer Science.
IEEE ComputerSociety Technical Committee on MathematicalFoundations of Computing, 2009.
URL http://arxiv.org/abs/0904.1113.Eric Corlett and Gerald Penn.
An exact A?
methodfor deciphering letter-substitution ciphers.
InProceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 1040?1047, 2010.Richard E Korf and Michael Reid.
Complexityanalysis of admissible heuristic search.
In Pro-ceedings of the Fifteenth National Conferenceon Artificial Intelligence, 1998.Richard E Korf, Michael Reid, and StefanEdelkamp.
Time complexity of iterative-deepening-A?.
Artificial Intelligence, 129(1?2):199?218, 2001.Judea Pearl.
Heuristics: Intelligent Search Strate-gies for Computer Problem Solving.
Addison-Wesley, 1984.Daniel A Spielman and Shang-Hua Teng.Smoothed analysis of algorithms: Why thesimplex algorithm usually takes polynomialtime.
Journal of the ACM, 51(3):385?463,2004.92
