Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532?1543,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsGloVe: Global Vectors for Word RepresentationJeffrey Pennington, Richard Socher, Christopher D. ManningComputer Science Department, Stanford University, Stanford, CA 94305jpennin@stanford.edu, richard@socher.org, manning@stanford.eduAbstractRecent methods for learning vector spacerepresentations of words have succeededin capturing fine-grained semantic andsyntactic regularities using vector arith-metic, but the origin of these regularitieshas remained opaque.
We analyze andmake explicit the model properties neededfor such regularities to emerge in wordvectors.
The result is a new global log-bilinear regression model that combinesthe advantages of the two major modelfamilies in the literature: global matrixfactorization and local context windowmethods.
Our model efficiently leveragesstatistical information by training only onthe nonzero elements in a word-word co-occurrence matrix, rather than on the en-tire sparse matrix or on individual contextwindows in a large corpus.
The model pro-duces a vector space with meaningful sub-structure, as evidenced by its performanceof 75% on a recent word analogy task.
Italso outperforms related models on simi-larity tasks and named entity recognition.1 IntroductionSemantic vector space models of language repre-sent each word with a real-valued vector.
Thesevectors can be used as features in a variety of ap-plications, such as information retrieval (Manninget al., 2008), document classification (Sebastiani,2002), question answering (Tellex et al., 2003),named entity recognition (Turian et al., 2010), andparsing (Socher et al., 2013).Most word vector methods rely on the distanceor angle between pairs of word vectors as the pri-mary method for evaluating the intrinsic qualityof such a set of word representations.
Recently,Mikolov et al.
(2013c) introduced a new evalua-tion scheme based on word analogies that probesthe finer structure of the word vector space by ex-amining not the scalar distance between word vec-tors, but rather their various dimensions of dif-ference.
For example, the analogy ?king is toqueen as man is to woman?
should be encodedin the vector space by the vector equation king ?queen = man ?
woman.
This evaluation schemefavors models that produce dimensions of mean-ing, thereby capturing the multi-clustering idea ofdistributed representations (Bengio, 2009).The two main model families for learning wordvectors are: 1) global matrix factorization meth-ods, such as latent semantic analysis (LSA) (Deer-wester et al., 1990) and 2) local context windowmethods, such as the skip-gram model of Mikolovet al.
(2013c).
Currently, both families suffer sig-nificant drawbacks.
While methods like LSA ef-ficiently leverage statistical information, they dorelatively poorly on the word analogy task, indi-cating a sub-optimal vector space structure.
Meth-ods like skip-gram may do better on the analogytask, but they poorly utilize the statistics of the cor-pus since they train on separate local context win-dows instead of on global co-occurrence counts.In this work, we analyze the model propertiesnecessary to produce linear directions of meaningand argue that global log-bilinear regression mod-els are appropriate for doing so.
We propose a spe-cific weighted least squares model that trains onglobal word-word co-occurrence counts and thusmakes efficient use of statistics.
The model pro-duces a word vector space with meaningful sub-structure, as evidenced by its state-of-the-art per-formance of 75% accuracy on the word analogydataset.
We also demonstrate that our methodsoutperform other current methods on several wordsimilarity tasks, and also on a common named en-tity recognition (NER) benchmark.We provide the source code for the model aswell as trained word vectors at http://nlp.stanford.edu/projects/glove/.15322 Related WorkMatrix Factorization Methods.
Matrix factor-ization methods for generating low-dimensionalword representations have roots stretching as farback as LSA.
These methods utilize low-rank ap-proximations to decompose large matrices thatcapture statistical information about a corpus.
Theparticular type of information captured by suchmatrices varies by application.
In LSA, the ma-trices are of ?term-document?
type, i.e., the rowscorrespond to words or terms, and the columnscorrespond to different documents in the corpus.In contrast, the Hyperspace Analogue to Language(HAL) (Lund and Burgess, 1996), for example,utilizes matrices of ?term-term?
type, i.e., the rowsand columns correspond to words and the entriescorrespond to the number of times a given wordoccurs in the context of another given word.A main problem with HAL and related meth-ods is that the most frequent words contribute adisproportionate amount to the similarity measure:the number of times two words co-occur with theor and, for example, will have a large effect ontheir similarity despite conveying relatively littleabout their semantic relatedness.
A number oftechniques exist that addresses this shortcoming ofHAL, such as the COALS method (Rohde et al.,2006), in which the co-occurrence matrix is firsttransformed by an entropy- or correlation-basednormalization.
An advantage of this type of trans-formation is that the raw co-occurrence counts,which for a reasonably sized corpus might span8 or 9 orders of magnitude, are compressed so asto be distributed more evenly in a smaller inter-val.
A variety of newer models also pursue thisapproach, including a study (Bullinaria and Levy,2007) that indicates that positive pointwise mu-tual information (PPMI) is a good transformation.More recently, a square root type transformationin the form of Hellinger PCA (HPCA) (Lebret andCollobert, 2014) has been suggested as an effec-tive way of learning word representations.Shallow Window-Based Methods.
Anotherapproach is to learn word representations that aidin making predictions within local context win-dows.
For example, Bengio et al.
(2003) intro-duced a model that learns word vector representa-tions as part of a simple neural network architec-ture for language modeling.
Collobert and Weston(2008) decoupled the word vector training fromthe downstream training objectives, which pavedthe way for Collobert et al.
(2011) to use the fullcontext of a word for learning the word represen-tations, rather than just the preceding context as isthe case with language models.Recently, the importance of the full neural net-work structure for learning useful word repre-sentations has been called into question.
Theskip-gram and continuous bag-of-words (CBOW)models of Mikolov et al.
(2013a) propose a sim-ple single-layer architecture based on the innerproduct between two word vectors.
Mnih andKavukcuoglu (2013) also proposed closely-relatedvector log-bilinear models, vLBL and ivLBL, andLevy et al.
(2014) proposed explicit word embed-dings based on a PPMI metric.In the skip-gram and ivLBL models, the objec-tive is to predict a word?s context given the worditself, whereas the objective in the CBOW andvLBL models is to predict a word given its con-text.
Through evaluation on a word analogy task,these models demonstrated the capacity to learnlinguistic patterns as linear relationships betweenthe word vectors.Unlike the matrix factorization methods, theshallow window-based methods suffer from thedisadvantage that they do not operate directly onthe co-occurrence statistics of the corpus.
Instead,these models scan context windows across the en-tire corpus, which fails to take advantage of thevast amount of repetition in the data.3 The GloVe ModelThe statistics of word occurrences in a corpus isthe primary source of information available to allunsupervised methods for learning word represen-tations, and although many such methods now ex-ist, the question still remains as to how meaningis generated from these statistics, and how the re-sulting word vectors might represent that meaning.In this section, we shed some light on this ques-tion.
We use our insights to construct a new modelfor word representation which we call GloVe, forGlobal Vectors, because the global corpus statis-tics are captured directly by the model.First we establish some notation.
Let the matrixof word-word co-occurrence counts be denoted byX , whose entries Xi j tabulate the number of timesword j occurs in the context of word i.
Let Xi =?k Xik be the number of times any word appearsin the context of word i.
Finally, let Pi j = P( j |i) =Xi j/Xi be the probability that word j appear in the1533Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6billion token corpus.
Only in the ratio does noise from non-discriminative words like water and fashioncancel out, so that large values (much greater than 1) correlate well with properties specific to ice, andsmall values (much less than 1) correlate well with properties specific of steam.Probability and Ratio k = solid k = gas k = water k = fashionP(k |ice) 1.9 ?
10?46.6 ?
10?53.0 ?
10?31.7 ?
10?5P(k |steam) 2.2 ?
10?57.8 ?
10?42.2 ?
10?31.8 ?
10?5P(k |ice)/P(k |steam) 8.9 8.5 ?
10?21.36 0.96context of word i.We begin with a simple example that showcaseshow certain aspects of meaning can be extracteddirectly from co-occurrence probabilities.
Con-sider two words i and j that exhibit a particular as-pect of interest; for concreteness, suppose we areinterested in the concept of thermodynamic phase,for which we might take i = ice and j = steam.The relationship of these words can be examinedby studying the ratio of their co-occurrence prob-abilities with various probe words, k. For wordsk related to ice but not steam, say k = solid, weexpect the ratio Pik/Pjk will be large.
Similarly,for words k related to steam but not ice, say k =gas, the ratio should be small.
For words k likewater or fashion, that are either related to both iceand steam, or to neither, the ratio should be closeto one.
Table 1 shows these probabilities and theirratios for a large corpus, and the numbers confirmthese expectations.
Compared to the raw probabil-ities, the ratio is better able to distinguish relevantwords (solid and gas) from irrelevant words (waterand fashion) and it is also better able to discrimi-nate between the two relevant words.The above argument suggests that the appropri-ate starting point for word vector learning shouldbe with ratios of co-occurrence probabilities ratherthan the probabilities themselves.
Noting that theratio Pik/Pjk depends on three words i, j, and k,the most general model takes the form,F (wi ,w j , w?k ) =PikPjk, (1)where w ?
Rd are word vectors and w?
?
Rdare separate context word vectors whose role willbe discussed in Section 4.2.
In this equation, theright-hand side is extracted from the corpus, andF may depend on some as-of-yet unspecified pa-rameters.
The number of possibilities for F is vast,but by enforcing a few desiderata we can select aunique choice.
First, we would like F to encodethe information present the ratio Pik/Pjk in theword vector space.
Since vector spaces are inher-ently linear structures, the most natural way to dothis is with vector differences.
With this aim, wecan restrict our consideration to those functions Fthat depend only on the difference of the two targetwords, modifying Eqn.
(1) to,F (wi ?
w j , w?k ) =PikPjk.
(2)Next, we note that the arguments of F in Eqn.
(2)are vectors while the right-hand side is a scalar.While F could be taken to be a complicated func-tion parameterized by, e.g., a neural network, do-ing so would obfuscate the linear structure we aretrying to capture.
To avoid this issue, we can firsttake the dot product of the arguments,F((wi ?
w j )T w?k)=PikPjk, (3)which prevents F from mixing the vector dimen-sions in undesirable ways.
Next, note that forword-word co-occurrence matrices, the distinctionbetween a word and a context word is arbitrary andthat we are free to exchange the two roles.
To do soconsistently, we must not only exchange w ?
w?but also X ?
XT.
Our final model should be in-variant under this relabeling, but Eqn.
(3) is not.However, the symmetry can be restored in twosteps.
First, we require that F be a homomorphismbetween the groups (R,+) and (R>0,?
), i.e.,F((wi ?
w j )T w?k)=F (wTi w?k )F (wTj w?k ), (4)which, by Eqn.
(3), is solved by,F (wTi w?k ) = Pik =XikXi.
(5)The solution to Eqn.
(4) is F = exp, or,wTi w?k = log(Pik ) = log(Xik ) ?
log(Xi ) .
(6)1534Next, we note that Eqn.
(6) would exhibit the ex-change symmetry if not for the log(Xi ) on theright-hand side.
However, this term is indepen-dent of k so it can be absorbed into a bias bi forwi .
Finally, adding an additional bias ?bk for w?krestores the symmetry,wTi w?k + bi + ?bk = log(Xik ) .
(7)Eqn.
(7) is a drastic simplification over Eqn.
(1),but it is actually ill-defined since the logarithm di-verges whenever its argument is zero.
One reso-lution to this issue is to include an additive shiftin the logarithm, log(Xik ) ?
log(1 + Xik ), whichmaintains the sparsity of X while avoiding the di-vergences.
The idea of factorizing the log of theco-occurrence matrix is closely related to LSA andwe will use the resulting model as a baseline inour experiments.
A main drawback to this modelis that it weighs all co-occurrences equally, eventhose that happen rarely or never.
Such rare co-occurrences are noisy and carry less informationthan the more frequent ones ?
yet even just thezero entries account for 75?95% of the data in X ,depending on the vocabulary size and corpus.We propose a new weighted least squares re-gression model that addresses these problems.Casting Eqn.
(7) as a least squares problem andintroducing a weighting function f (Xi j ) into thecost function gives us the modelJ =V?i, j=1f(Xi j) (wTi w?
j + bi + ?bj ?
log Xi j)2,(8)where V is the size of the vocabulary.
The weight-ing function should obey the following properties:1. f (0) = 0.
If f is viewed as a continuousfunction, it should vanish as x ?
0 fastenough that the limx?0 f (x) log2x is finite.2.
f (x) should be non-decreasing so that rareco-occurrences are not overweighted.3.
f (x) should be relatively small for large val-ues of x, so that frequent co-occurrences arenot overweighted.Of course a large number of functions satisfy theseproperties, but one class of functions that we foundto work well can be parameterized as,f (x) ={(x/xmax)?if x < xmax1 otherwise .
(9)0.20.40.60.81.00.0Figure 1: Weighting function f with ?
= 3/4.The performance of the model depends weakly onthe cutoff, which we fix to xmax= 100 for all ourexperiments.
We found that ?
= 3/4 gives a mod-est improvement over a linear version with ?
= 1.Although we offer only empirical motivation forchoosing the value 3/4, it is interesting that a sim-ilar fractional power scaling was found to give thebest performance in (Mikolov et al., 2013a).3.1 Relationship to Other ModelsBecause all unsupervised methods for learningword vectors are ultimately based on the occur-rence statistics of a corpus, there should be com-monalities between the models.
Nevertheless, cer-tain models remain somewhat opaque in this re-gard, particularly the recent window-based meth-ods like skip-gram and ivLBL.
Therefore, in thissubsection we show how these models are relatedto our proposed model, as defined in Eqn.
(8).The starting point for the skip-gram or ivLBLmethods is a model Qi j for the probability thatword j appears in the context of word i.
For con-creteness, let us assume that Qi j is a softmax,Qi j =exp(wTi w?
j )?Vk=1 exp(wTi w?k ).
(10)Most of the details of these models are irrelevantfor our purposes, aside from the the fact that theyattempt to maximize the log probability as a con-text window scans over the corpus.
Training pro-ceeds in an on-line, stochastic fashion, but the im-plied global objective function can be written as,J = ?
?i?corpusj?context(i)logQi j .
(11)Evaluating the normalization factor of the soft-max for each term in this sum is costly.
To al-low for efficient training, the skip-gram and ivLBLmodels introduce approximations to Qi j .
How-ever, the sum in Eqn.
(11) can be evaluated much1535more efficiently if we first group together thoseterms that have the same values for i and j,J = ?V?i=1V?j=1Xi j logQi j , (12)where we have used the fact that the number oflike terms is given by the co-occurrence matrix X .Recalling our notation for Xi =?k Xik andPi j = Xi j/Xi , we can rewrite J as,J = ?V?i=1XiV?j=1Pi j logQi j =V?i=1XiH (Pi ,Qi ) ,(13)where H (Pi ,Qi ) is the cross entropy of the dis-tributions Pi and Qi , which we define in analogyto Xi .
As a weighted sum of cross-entropy error,this objective bears some formal resemblance tothe weighted least squares objective of Eqn.
(8).In fact, it is possible to optimize Eqn.
(13) directlyas opposed to the on-line training methods used inthe skip-gram and ivLBL models.
One could inter-pret this objective as a ?global skip-gram?
model,and it might be interesting to investigate further.On the other hand, Eqn.
(13) exhibits a number ofundesirable properties that ought to be addressedbefore adopting it as a model for learning wordvectors.To begin, cross entropy error is just one amongmany possible distance measures between prob-ability distributions, and it has the unfortunateproperty that distributions with long tails are of-ten modeled poorly with too much weight givento the unlikely events.
Furthermore, for the mea-sure to be bounded it requires that the model dis-tribution Q be properly normalized.
This presentsa computational bottleneck owing to the sum overthe whole vocabulary in Eqn.
(10), and it would bedesirable to consider a different distance measurethat did not require this property of Q.
A naturalchoice would be a least squares objective in whichnormalization factors in Q and P are discarded,?J =?i, jXi(?Pi j ?
?Qi j)2(14)where?Pi j = Xi j and ?Qi j = exp(wTi w?
j ) are theunnormalized distributions.
At this stage anotherproblem emerges, namely that Xi j often takes verylarge values, which can complicate the optimiza-tion.
An effective remedy is to minimize thesquared error of the logarithms of?P and?Q instead,?J =?i, jXi(log?Pi j ?
log ?Qi j)2=?i, jXi(wTi w?
j ?
log Xi j)2.
(15)Finally, we observe that while the weighting factorXi is preordained by the on-line training methodinherent to the skip-gram and ivLBL models, it isby no means guaranteed to be optimal.
In fact,Mikolov et al.
(2013a) observe that performancecan be increased by filtering the data so as to re-duce the effective value of the weighting factor forfrequent words.
With this in mind, we introducea more general weighting function, which we arefree to take to depend on the context word as well.The result is,?J =?i, jf (Xi j )(wTi w?
j ?
log Xi j)2, (16)which is equivalent1to the cost function ofEqn.
(8), which we derived previously.3.2 Complexity of the modelAs can be seen from Eqn.
(8) and the explicit formof the weighting function f (X ), the computationalcomplexity of the model depends on the number ofnonzero elements in the matrix X .
As this num-ber is always less than the total number of en-tries of the matrix, the model scales no worse thanO( |V |2).
At first glance this might seem like a sub-stantial improvement over the shallow window-based approaches, which scale with the corpussize, |C |.
However, typical vocabularies have hun-dreds of thousands of words, so that |V |2can be inthe hundreds of billions, which is actually muchlarger than most corpora.
For this reason it is im-portant to determine whether a tighter bound canbe placed on the number of nonzero elements ofX .In order to make any concrete statements aboutthe number of nonzero elements in X , it is neces-sary to make some assumptions about the distribu-tion of word co-occurrences.
In particular, we willassume that the number of co-occurrences of wordi with word j, Xi j , can be modeled as a power-lawfunction of the frequency rank of that word pair,ri j :Xi j =k(ri j )?.
(17)1We could also include bias terms in Eqn.
(16).1536The total number of words in the corpus is pro-portional to the sum over all elements of the co-occurrence matrix X ,|C | ?
?i jXi j =|X |?r=1kr?= kH|X |,?
, (18)where we have rewritten the last sum in terms ofthe generalized harmonic number Hn,m .
The up-per limit of the sum, |X |, is the maximum fre-quency rank, which coincides with the number ofnonzero elements in the matrix X .
This number isalso equal to the maximum value of r in Eqn.
(17)such that Xi j ?
1, i.e., |X | = k1/?.
Therefore wecan write Eqn.
(18) as,|C | ?
|X |?H|X |,?
.
(19)We are interested in how |X | is related to |C | whenboth numbers are large; therefore we are free toexpand the right hand side of the equation for large|X |.
For this purpose we use the expansion of gen-eralized harmonic numbers (Apostol, 1976),Hx,s =x1?s1 ?
s+ ?
(s) + O(x?s) if s > 0, s , 1 ,(20)giving,|C | ?|X |1 ?
?+ ?
(?)
|X |?+ O(1) , (21)where ?
(s) is the Riemann zeta function.
In thelimit that X is large, only one of the two terms onthe right hand side of Eqn.
(21) will be relevant,and which term that is depends on whether ?
> 1,|X | ={O(|C |) if ?
< 1,O(|C |1/?)
if ?
> 1.
(22)For the corpora studied in this article, we observethat Xi j is well-modeled by Eqn.
(17) with ?
=1.25.
In this case we have that |X | = O(|C |0.8).Therefore we conclude that the complexity of themodel is much better than the worst case O(V2),and in fact it does somewhat better than the on-linewindow-based methods which scale like O(|C |).4 Experiments4.1 Evaluation methodsWe conduct experiments on the word analogytask of Mikolov et al.
(2013a), a variety of wordsimilarity tasks, as described in (Luong et al.,2013), and on the CoNLL-2003 shared benchmarkTable 2: Results on the word analogy task, givenas percent accuracy.
Underlined scores are bestwithin groups of similarly-sized models; boldscores are best overall.
HPCA vectors are publiclyavailable2; (i)vLBL results are from (Mnih et al.,2013); skip-gram (SG) and CBOW results arefrom (Mikolov et al., 2013a,b); we trained SG?and CBOW?using the word2vec tool3.
See textfor details and a description of the SVD models.Model Dim.
Size Sem.
Syn.
Tot.ivLBL 100 1.5B 55.9 50.1 53.2HPCA 100 1.6B 4.2 16.4 10.8GloVe 100 1.6B 67.5 54.3 60.3SG 300 1B 61 61 61CBOW 300 1.6B 16.1 52.6 36.1vLBL 300 1.5B 54.2 64.8 60.0ivLBL 300 1.5B 65.2 63.0 64.0GloVe 300 1.6B 80.8 61.5 70.3SVD 300 6B 6.3 8.1 7.3SVD-S 300 6B 36.7 46.6 42.1SVD-L 300 6B 56.6 63.0 60.1CBOW?300 6B 63.6 67.4 65.7SG?300 6B 73.0 66.0 69.1GloVe 300 6B 77.4 67.0 71.7CBOW 1000 6B 57.3 68.9 63.7SG 1000 6B 66.1 65.1 65.6SVD-L 300 42B 38.4 58.2 49.2GloVe 300 42B 81.9 69.3 75.0dataset for NER (Tjong Kim Sang and De Meul-der, 2003).Word analogies.
The word analogy task con-sists of questions like, ?a is to b as c is to ?
?The dataset contains 19,544 such questions, di-vided into a semantic subset and a syntactic sub-set.
The semantic questions are typically analogiesabout people or places, like ?Athens is to Greeceas Berlin is to ??.
The syntactic questions aretypically analogies about verb tenses or forms ofadjectives, for example ?dance is to dancing as flyis to ??.
To correctly answer the question, themodel should uniquely identify the missing term,with only an exact correspondence counted as acorrect match.
We answer the question ?a is to bas c is to ??
by finding the word d whose repre-sentation wd is closest to wb ?
wa + wc accordingto the cosine similarity.42http://lebret.ch/words/3http://code.google.com/p/word2vec/4Levy et al.
(2014) introduce a multiplicative analogyevaluation, 3COSMUL, and report an accuracy of 68.24% on15370 100 200 300 400 500 60020304050607080Vector DimensionAccuracy [%]SemanticSyntacticOverall(a) Symmetric context2 4 6 8 1040505560657045Window SizeAccuracy [%]SemanticSyntacticOverall(b) Symmetric context2 4 6 8 1040505560657045Window SizeAccuracy [%]SemanticSyntacticOverall(c) Asymmetric contextFigure 2: Accuracy on the analogy task as function of vector size and window size/type.
All models aretrained on the 6 billion token corpus.
In (a), the window size is 10.
In (b) and (c), the vector size is 100.Word similarity.
While the analogy task is ourprimary focus since it tests for interesting vectorspace substructures, we also evaluate our model ona variety of word similarity tasks in Table 3.
Theseinclude WordSim-353 (Finkelstein et al., 2001),MC (Miller and Charles, 1991), RG (Rubensteinand Goodenough, 1965), SCWS (Huang et al.,2012), and RW (Luong et al., 2013).Named entity recognition.
The CoNLL-2003English benchmark dataset for NER is a collec-tion of documents from Reuters newswire articles,annotated with four entity types: person, location,organization, and miscellaneous.
We train mod-els on CoNLL-03 training data on test on threedatasets: 1) ConLL-03 testing data, 2) ACE Phase2 (2001-02) and ACE-2003 data, and 3) MUC7Formal Run test set.
We adopt the BIO2 annota-tion standard, as well as all the preprocessing stepsdescribed in (Wang and Manning, 2013).
We use acomprehensive set of discrete features that comeswith the standard distribution of the Stanford NERmodel (Finkel et al., 2005).
A total of 437,905discrete features were generated for the CoNLL-2003 training dataset.
In addition, 50-dimensionalvectors for each word of a five-word context areadded and used as continuous features.
With thesefeatures as input, we trained a conditional randomfield (CRF) with exactly the same setup as theCRFjoinmodel of (Wang and Manning, 2013).4.2 Corpora and training detailsWe trained our model on five corpora of varyingsizes: a 2010 Wikipedia dump with 1 billion to-kens; a 2014 Wikipedia dump with 1.6 billion to-kens; Gigaword 5 which has 4.3 billion tokens; thecombination Gigaword5 + Wikipedia2014, whichthe analogy task.
This number is evaluated on a subset of thedataset so it is not included in Table 2.
3COSMUL performedworse than cosine similarity in almost all of our experiments.has 6 billion tokens; and on 42 billion tokens ofweb data, from Common Crawl5.
We tokenizeand lowercase each corpus with the Stanford to-kenizer, build a vocabulary of the 400,000 mostfrequent words6, and then construct a matrix of co-occurrence counts X .
In constructing X , we mustchoose how large the context window should beand whether to distinguish left context from rightcontext.
We explore the effect of these choices be-low.
In all cases we use a decreasing weightingfunction, so that word pairs that are d words apartcontribute 1/d to the total count.
This is one wayto account for the fact that very distant word pairsare expected to contain less relevant informationabout the words?
relationship to one another.For all our experiments, we set xmax= 100,?
= 3/4, and train the model using AdaGrad(Duchi et al., 2011), stochastically sampling non-zero elements from X , with initial learning rate of0.05.
We run 50 iterations for vectors smaller than300 dimensions, and 100 iterations otherwise (seeSection 4.6 for more details about the convergencerate).
Unless otherwise noted, we use a context often words to the left and ten words to the right.The model generates two sets of word vectors,W and?W .
When X is symmetric, W and?W areequivalent and differ only as a result of their ran-dom initializations; the two sets of vectors shouldperform equivalently.
On the other hand, there isevidence that for certain types of neural networks,training multiple instances of the network and thencombining the results can help reduce overfittingand noise and generally improve results (Ciresanet al., 2012).
With this in mind, we choose to use5To demonstrate the scalability of the model, we alsotrained it on a much larger sixth corpus, containing 840 bil-lion tokens of web data, but in this case we did not lowercasethe vocabulary, so the results are not directly comparable.6For the model trained on Common Crawl data, we use alarger vocabulary of about 2 million words.1538the sum W +?W as our word vectors.
Doing so typ-ically gives a small boost in performance, with thebiggest increase in the semantic analogy task.We compare with the published results of a va-riety of state-of-the-art models, as well as withour own results produced using the word2vectool and with several baselines using SVDs.
Withword2vec, we train the skip-gram (SG?)
andcontinuous bag-of-words (CBOW?)
models on the6 billion token corpus (Wikipedia 2014 + Giga-word 5) with a vocabulary of the top 400,000 mostfrequent words and a context window size of 10.We used 10 negative samples, which we show inSection 4.6 to be a good choice for this corpus.For the SVD baselines, we generate a truncatedmatrix Xtruncwhich retains the information of howfrequently each word occurs with only the top10,000 most frequent words.
This step is typi-cal of many matrix-factorization-based methods asthe extra columns can contribute a disproportion-ate number of zero entries and the methods areotherwise computationally expensive.The singular vectors of this matrix constitutethe baseline ?SVD?.
We also evaluate two relatedbaselines: ?SVD-S?
in which we take the SVD of?Xtrunc, and ?SVD-L?
in which we take the SVDof log(1+Xtrunc).
Both methods help compress theotherwise large range of values in X .74.3 ResultsWe present results on the word analogy task in Ta-ble 2.
The GloVe model performs significantlybetter than the other baselines, often with smallervector sizes and smaller corpora.
Our results us-ing the word2vec tool are somewhat better thanmost of the previously published results.
This isdue to a number of factors, including our choice touse negative sampling (which typically works bet-ter than the hierarchical softmax), the number ofnegative samples, and the choice of the corpus.We demonstrate that the model can easily betrained on a large 42 billion token corpus, with asubstantial corresponding performance boost.
Wenote that increasing the corpus size does not guar-antee improved results for other models, as can beseen by the decreased performance of the SVD-7We also investigated several other weighting schemes fortransforming X ; what we report here performed best.
Manyweighting schemes like PPMI destroy the sparsity of X andtherefore cannot feasibly be used with large vocabularies.With smaller vocabularies, these information-theoretic trans-formations do indeed work well on word similarity measures,but they perform very poorly on the word analogy task.Table 3: Spearman rank correlation on word simi-larity tasks.
All vectors are 300-dimensional.
TheCBOW?vectors are from the word2vec websiteand differ in that they contain phrase vectors.Model Size WS353 MC RG SCWS RWSVD 6B 35.3 35.1 42.5 38.3 25.6SVD-S 6B 56.5 71.5 71.0 53.6 34.7SVD-L 6B 65.7 72.7 75.1 56.5 37.0CBOW?6B 57.2 65.6 68.2 57.0 32.5SG?6B 62.8 65.2 69.7 58.1 37.2GloVe 6B 65.8 72.7 77.8 53.9 38.1SVD-L 42B 74.0 76.4 74.1 58.3 39.9GloVe 42B 75.9 83.6 82.9 59.6 47.8CBOW?100B 68.4 79.6 75.4 59.4 45.5L model on this larger corpus.
The fact that thisbasic SVD model does not scale well to large cor-pora lends further evidence to the necessity of thetype of weighting scheme proposed in our model.Table 3 shows results on five different wordsimilarity datasets.
A similarity score is obtainedfrom the word vectors by first normalizing eachfeature across the vocabulary and then calculat-ing the cosine similarity.
We compute Spearman?srank correlation coefficient between this score andthe human judgments.
CBOW?denotes the vec-tors available on the word2vec website that aretrained with word and phrase vectors on 100Bwords of news data.
GloVe outperforms it whileusing a corpus less than half the size.Table 4 shows results on the NER task with theCRF-based model.
The L-BFGS training termi-nates when no improvement has been achieved onthe dev set for 25 iterations.
Otherwise all config-urations are identical to those used by Wang andManning (2013).
The model labeled Discrete isthe baseline using a comprehensive set of discretefeatures that comes with the standard distributionof the Stanford NER model, but with no word vec-tor features.
In addition to the HPCA and SVDmodels discussed previously, we also compare tothe models of Huang et al.
(2012) (HSMN) andCollobert and Weston (2008) (CW).
We trainedthe CBOW model using the word2vec tool8.The GloVe model outperforms all other methodson all evaluation metrics, except for the CoNLLtest set, on which the HPCA method does slightlybetter.
We conclude that the GloVe vectors areuseful in downstream NLP tasks, as was first8We use the same parameters as above, except in this casewe found 5 negative samples to work slightly better than 10.1539Table 4: F1 score on NER task with 50d vectors.Discrete is the baseline without word vectors.
Weuse publicly-available vectors for HPCA, HSMN,and CW.
See text for details.Model Dev Test ACE MUC7Discrete 91.0 85.4 77.4 73.4SVD 90.8 85.7 77.3 73.7SVD-S 91.0 85.5 77.6 74.3SVD-L 90.5 84.8 73.6 71.5HPCA 92.6 88.7 81.7 80.7HSMN 90.5 85.7 78.7 74.7CW 92.2 87.4 81.7 80.2CBOW 93.1 88.2 82.2 81.1GloVe 93.2 88.3 82.9 82.2shown for neural vectors in (Turian et al., 2010).4.4 Model Analysis: Vector Length andContext SizeIn Fig.
2, we show the results of experiments thatvary vector length and context window.
A contextwindow that extends to the left and right of a tar-get word will be called symmetric, and one whichextends only to the left will be called asymmet-ric.
In (a), we observe diminishing returns for vec-tors larger than about 200 dimensions.
In (b) and(c), we examine the effect of varying the windowsize for symmetric and asymmetric context win-dows.
Performance is better on the syntactic sub-task for small and asymmetric context windows,which aligns with the intuition that syntactic infor-mation is mostly drawn from the immediate con-text and can depend strongly on word order.
Se-mantic information, on the other hand, is more fre-quently non-local, and more of it is captured withlarger window sizes.4.5 Model Analysis: Corpus SizeIn Fig.
3, we show performance on the word anal-ogy task for 300-dimensional vectors trained ondifferent corpora.
On the syntactic subtask, thereis a monotonic increase in performance as the cor-pus size increases.
This is to be expected sincelarger corpora typically produce better statistics.Interestingly, the same trend is not true for the se-mantic subtask, where the models trained on thesmaller Wikipedia corpora do better than thosetrained on the larger Gigaword corpus.
This islikely due to the large number of city- and country-based analogies in the analogy dataset and the factthat Wikipedia has fairly comprehensive articlesfor most such locations.
Moreover, Wikipedia?s5055606570758085 OverallSyntacticSemanticWiki20101B tokensAccuracy [%]Wiki20141.6B tokens Gigaword54.3B tokens Gigaword5 + Wiki20146B tokens Common Crawl 42B tokensFigure 3: Accuracy on the analogy task for 300-dimensional vectors trained on different corpora.entries are updated to assimilate new knowledge,whereas Gigaword is a fixed news repository withoutdated and possibly incorrect information.4.6 Model Analysis: Run-timeThe total run-time is split between populating Xand training the model.
The former depends onmany factors, including window size, vocabularysize, and corpus size.
Though we did not do so,this step could easily be parallelized across mul-tiple machines (see, e.g., Lebret and Collobert(2014) for some benchmarks).
Using a singlethread of a dual 2.1GHz Intel Xeon E5-2658 ma-chine, populating X with a 10 word symmetriccontext window, a 400,000 word vocabulary, anda 6 billion token corpus takes about 85 minutes.Given X , the time it takes to train the model de-pends on the vector size and the number of itera-tions.
For 300-dimensional vectors with the abovesettings (and using all 32 cores of the above ma-chine), a single iteration takes 14 minutes.
SeeFig.
4 for a plot of the learning curve.4.7 Model Analysis: Comparison withword2vecA rigorous quantitative comparison of GloVe withword2vec is complicated by the existence ofmany parameters that have a strong effect on per-formance.
We control for the main sources of vari-ation that we identified in Sections 4.4 and 4.5 bysetting the vector length, context window size, cor-pus, and vocabulary size to the configuration men-tioned in the previous subsection.The most important remaining variable to con-trol for is training time.
For GloVe, the rele-vant parameter is the number of training iterations.For word2vec, the obvious choice would be thenumber of training epochs.
Unfortunately, thecode is currently designed for only a single epoch:15401 2 3 4 5 6606264666870725 10 15 20 251357 10 15 20 25 30 40 50Accuracy[%]Iterations (GloVe)Negative Samples (CBOW)Training Time (hrs)GloVeCBOW(a) GloVe vs CBOW3 6 9 12 15 18 21 246062646668707220 40 60 80 1001 2 3 4 5 6 7 10 12 15 20GloVeSkip-GramAccuracy[%]Iterations (GloVe)Negative Samples (Skip-Gram)Training Time (hrs)(b) GloVe vs Skip-GramFigure 4: Overall accuracy on the word analogy task as a function of training time, which is governed bythe number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram(b).
In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.it specifies a learning schedule specific to a singlepass through the data, making a modification formultiple passes a non-trivial task.
Another choiceis to vary the number of negative samples.
Addingnegative samples effectively increases the numberof training words seen by the model, so in someways it is analogous to extra epochs.We set any unspecified parameters to their de-fault values, assuming that they are close to opti-mal, though we acknowledge that this simplifica-tion should be relaxed in a more thorough analysis.In Fig.
4, we plot the overall performance onthe analogy task as a function of training time.The two x-axes at the bottom indicate the corre-sponding number of training iterations for GloVeand negative samples for word2vec.
We notethat word2vec?s performance actually decreasesif the number of negative samples increases be-yond about 10.
Presumably this is because thenegative sampling method does not approximatethe target probability distribution well.9For the same corpus, vocabulary, window size,and training time, GloVe consistently outperformsword2vec.
It achieves better results faster, andalso obtains the best results irrespective of speed.5 ConclusionRecently, considerable attention has been focusedon the question of whether distributional wordrepresentations are best learned from count-based9In contrast, noise-contrastive estimation is an approxi-mation which improves with more negative samples.
In Ta-ble 1 of (Mnih et al., 2013), accuracy on the analogy task is anon-decreasing function of the number of negative samples.methods or from prediction-based methods.
Cur-rently, prediction-based models garner substantialsupport; for example, Baroni et al.
(2014) arguethat these models perform better across a range oftasks.
In this work we argue that the two classesof methods are not dramatically different at a fun-damental level since they both probe the under-lying co-occurrence statistics of the corpus, butthe efficiency with which the count-based meth-ods capture global statistics can be advantageous.We construct a model that utilizes this main ben-efit of count data while simultaneously capturingthe meaningful linear substructures prevalent inrecent log-bilinear prediction-based methods likeword2vec.
The result, GloVe, is a new globallog-bilinear regression model for the unsupervisedlearning of word representations that outperformsother models on word analogy, word similarity,and named entity recognition tasks.AcknowledgmentsWe thank the anonymous reviewers for their valu-able comments.
Stanford University gratefullyacknowledges the support of the Defense ThreatReduction Agency (DTRA) under Air Force Re-search Laboratory (AFRL) contract no.
FA8650-10-C-7020 and the Defense Advanced ResearchProjects Agency (DARPA) Deep Exploration andFiltering of Text (DEFT) Program under AFRLcontract no.
FA8750-13-2-0040.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the view of the DTRA,AFRL, DEFT, or the US government.1541ReferencesTom M. Apostol.
1976.
Introduction to AnalyticNumber Theory.
Introduction to Analytic Num-ber Theory.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
Asystematic comparison of context-counting vs.context-predicting semantic vectors.
In ACL.Yoshua Bengio.
2009.
Learning deep architecturesfor AI.
Foundations and Trends in MachineLearning.Yoshua Bengio, R?ejean Ducharme, Pascal Vin-cent, and Christian Janvin.
2003.
A neural prob-abilistic language model.
JMLR, 3:1137?1155.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.Behavior Research Methods, 39(3):510?526.Dan C. Ciresan, Alessandro Giusti, Luca M. Gam-bardella, and J?urgen Schmidhuber.
2012.
Deepneural networks segment neuronal membranesin electron microscopy images.
In NIPS, pages2852?2860.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language process-ing: deep neural networks with multitask learn-ing.
In Proceedings of ICML, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou,Michael Karlen, Koray Kavukcuoglu, and PavelKuksa.
2011.
Natural Language Processing (Al-most) from Scratch.
JMLR, 12:2493?2537.Scott Deerwester, Susan T. Dumais, George W.Furnas, Thomas K. Landauer, and RichardHarshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society forInformation Science, 41.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learn-ing and stochastic optimization.
JMLR, 12.Lev Finkelstein, Evgenly Gabrilovich, Yossi Ma-tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,and Eytan Ruppin.
2001.
Placing search in con-text: The concept revisited.
In Proceedingsof the 10th international conference on WorldWide Web, pages 406?414.
ACM.Eric H. Huang, Richard Socher, Christopher D.Manning, and Andrew Y. Ng.
2012.
ImprovingWord Representations via Global Context andMultiple Word Prototypes.
In ACL.R?emi Lebret and Ronan Collobert.
2014.
Wordembeddings through Hellinger PCA.
In EACL.Omer Levy, Yoav Goldberg, and Israel Ramat-Gan.
2014.
Linguistic regularities in sparse andexplicit word representations.
CoNLL-2014.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexicalco-occurrence.
Behavior Research Methods, In-strumentation, and Computers, 28:203?208.Minh-Thang Luong, Richard Socher, and Christo-pher D Manning.
2013.
Better word represen-tations with recursive neural networks for mor-phology.
CoNLL-2013.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient Estimation of WordRepresentations in Vector Space.
In ICLRWork-shop Papers.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregCorrado, and Jeffrey Dean.
2013b.
Distributedrepresentations of words and phrases and theircompositionality.
In NIPS, pages 3111?3119.Tomas Mikolov, Wen tau Yih, and GeoffreyZweig.
2013c.
Linguistic regularities in con-tinuous space word representations.
In HLT-NAACL.George A. Miller and Walter G. Charles.
1991.Contextual correlates of semantic similarity.Language and cognitive processes, 6(1):1?28.Andriy Mnih and Koray Kavukcuoglu.
2013.Learning word embeddings efficiently withnoise-contrastive estimation.
In NIPS.Douglas L. T. Rohde, Laura M. Gonnerman,and David C. Plaut.
2006.
An improvedmodel of semantic similarity based on lexicalco-occurence.
Communications of the ACM,8:627?633.Herbert Rubenstein and John B. Goodenough.1965.
Contextual correlates of synonymy.
Com-munications of the ACM, 8(10):627?633.Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM ComputingSurveys, 34:1?47.Richard Socher, John Bauer, Christopher D. Man-ning, and Andrew Y. Ng.
2013.
Parsing WithCompositional Vector Grammars.
In ACL.1542Stefanie Tellex, Boris Katz, Jimmy Lin, AaronFernandes, and Gregory Marton.
2003.
Quanti-tative evaluation of passage retrieval algorithmsfor question answering.
In Proceedings of theSIGIR Conference on Research and Develop-ment in Informaion Retrieval.Erik F. Tjong Kim Sang and Fien De Meul-der.
2003.
Introduction to the CoNLL-2003shared task: Language-independent named en-tity recognition.
In CoNLL-2003.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: a simple and gen-eral method for semi-supervised learning.
InProceedings of ACL, pages 384?394.Mengqiu Wang and Christopher D. Manning.2013.
Effect of non-linear deep architecture insequence labeling.
In Proceedings of the 6thInternational Joint Conference on Natural Lan-guage Processing (IJCNLP).1543
