mm/////////////////Finding Structure via Compress ionJason L. HutchensDept.
of E&E EngineeringUniversity of Western AustraliaNedlands W.A.
6907, Australiahut ch~ci ips,  ee.
uwa.
edu.
auMichael D. AlderDept.
of MathematicsUniversity of Western AustraliaNedlands W.A.
6907, Australiamike?maths,  uwa.
edu.
auAbstractA statistical language model may be used tosegment a data sequence by thresholding itsinstantaneous entropy.
In this paper we de-scribe how this process works, and we apply itto the problem of discovering separator sym-bols in a text.
Our results how that languagemodels which bootstrap themselves with struc-ture found in this way undergo a reduction inperplexity.
We conclude that these techniquesmay be useful in the design of generic gram-matical inference systems.1 IntroductionThe modelling of a symbol sequence requires omeassumptions about the nature of the process whichgenerated it, and the modelling of English textwould, for example, commonly make the assumptionthat the text consists of words, short strings whichusually recur and which are separated by whitespace,and punctuation symbols.
The whitespace symbol(which we shall represent explicitly by A) and itsdistinctive function do not seem to occur in spokenEnglish, and would not seem to be essential in writ-ten English.
We are concerned with finding suchstructure using weak assumptions, rather than be-ing given it as part of a model.In this paper we show that a statistical model maybe used to do just that, and our results indicate thata model which bootstraps itself using this structureundergoes a reduction in perplexity.2 Entropic ChunkingA predictive model M is a model which, when pre-sented with a sequence of symbols , is able to makea prediction about the next symbol in the sequencein the form of a probability distribution over the al-phabet E (for the purposes of this investigation,is the set of ASCII characters).
We assume thatthe estimated probability distribution is smoothedto avoid the zero-frequency problem.
The specifics ofthe model are unimportant; he methods presentedin this paper are intended to be generic, but it isclear that n-th order Markov models, for n less thanthe length of s, would qualify.The information of a symbol w with respect toa statistical model M and a context s is defined inEquation 1.
Intuitively we may think of the infor-mation as the surprise the model experiences uponreceipt of the symbol w; it is low if the model'sexpectations are vindicated, high if they are erro-neous (Shannon and Weaver, 1949).I(w\[M, s) = - log 2 P(w\[M, s) (1)The entropy of a language model, defined in Equa-tion 2, is the expected value of the information.The entropy is a measure of the model's uncertaintyabout the future; it will be low if the model expectsone particular symbol to occur with a high proba-bility, and it increases as the estimated probabilitydistribution approaches the uniform.H(M, s) = ~ P(wIM, s)I(wJM, s) (2)wE~If one monitors the instantaneous entropy of a lan-guage model as it scans across an English text, onegenerally finds that regions of high entropy corre-spond with word boundaries (Alder, 1988).
This isconvincingly demonstrated by Figure 1, which plotsthe entropy of a second-order Markov model acrossthe first sentence of "A Scandal in Bohemia", by SirArthur Conan Doyle.
The training corpus used inthis example was 3.5 megabytes ofSherlock Holmesstories, minus the testing sentence.
1Segmentation is a matter of chunking the datawhenever the instantaneous entropy exceeds omethreshold value (Wolff, 1977).
A chunk is merely a1Email the authors for further information about thiscorpus.Hutchens and Alder 79 Finding Structure via CompressionJason L. Hutchens and Michael D. Alder (1998) Finding Structure via Compression.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98: New Methods in Language Processing and Computational N tural Language Learning, ACL, pp 79-82.OEO.OLUI I I I I I I I I I I I | I10 t t l t f l t l l t t t l l t t t l l l l l t  I t f l i t t  I t !
t l t l lTo^Sher lock^Holmes^she ^ ^a lways  ^ he^woman.Figure 1: Entropy across a text segment.string of symbols which constitute a higher-level lex-eme.
Throughout this paper a chunking threshold of?
log 2 IIE\[\[ bits is used, although this is almost cer-tainly not an optimal value.
The problem of findinga good threshold automatically warrants investiga-tion.The Sherlock Holmes corpus was segmented inthis way.
Table 1 lists, in decreasing order of fre-quency, the most common chunks found in the text.The fact that they agree rather well with the mostfrequent words in the English language is encourag-ing.Chunk  Number  of  OccurrencestheA 21790aA 11018of A 10263toA 9838andA 9444thatA 6035Table 1: The most common chunks discovered in theSherlock Holmes corpus.A total of 70171 distinct chunks were found in thecorpus.
Of these, a massive 66821 chunks occurredten times or less--these chunks were discarded ueto their infrequency (all anomalous chunks, such as"halfc" and "ichth', occurred in this group).
Themajority of the remaining 3350 chunks were foundto be valid English words.
Those that weren'twere strings of two or more English words, such as"inAtheA", "itAwasA" and "doAyouAthinkAthatA".The previous experiment was repeated using aversion of the Sherlock Holmes corpus which hadmany clues to word boundaries removed; all char-acters were replaced with their uppercase quiva-lents, and whitespace and punctuation symbols weredeleted.
Many good chunks were discovered, such as"THE", "TO", "WAS", "OFTHE", "HAVEBEEN"and "POLICE".
However, anomalous chunks wereprevalent, with "REWAS" and "STO" occurring asfrequently as the chunks a human being would iden-tify as English words.Even so, entropic chunking provides a techniquefor discovering structure which makes very few as-sumptions about the information that the data con-tains.2.1 Finding Separator  SymbolsIn natural anguage text, words are typically sepa-rated by whitespace.
Entropic hunking may be usedto discover this automatically, by recording whichsymbols occur immediately prior to a large jump inentropy.Table 2 lists, in decreasing order of frequency, sep-arator symbols discovered in the Sherlock Holmescorpus.
The A symbol precedes the majority of sud-Hutchens and Alder 80 Finding Structure via CompressionIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIImmiIIIIiIIIIden jumps in entropy, which agrees with our expec-tations.
The - symbol occurs within hyphenatedwords, which were usually broken up into their con-stituents, while the " symbol occurs as a chunk sep-arator whenever two pieces of dialogue appear back-to-back.
The remaining probability mass was dis-tributed over 43 symbols, which were discarded asanomalies.\[ Separator  Symbol  I Frequency- 0.23% _', 0.11%Table 2: Separator symbols discovered in the Sher-lock Holmes corpus.Once one or more separator symbols have beenfound, traditional parsing techniques may be usedto segment the text.Many data sequences simply will not have sepa-rator symbols.
For example, a database may storefields in a file based on their bit length only.
In suchsituations entropic hunking must be used if no priorassumptions about the structure of the data are tobe made.3 Data  Compress ionIn order to test the value of adding chunks to a lan-guage model's alphabet, we conducted a simple ex-periment.
The Sherlock Holmes corpus was dividedinto three non-overlapping parts, each of roughly amegabyte in size.
These three corpora were used fortraining, chunking and testing respectively.A standard PPMC model was inferred from thetraining corpus and used to segment he chunkingcorpus (Moffat, 1990).
The most common chunkwas then added to the alphabet of the PPMC modelin a process we refer to as the upwrite (Hutchens,1997).Evaluation was performed by measuring the per-plexity of the PPMC model with respect o the test-ing corpus (Jelinek and Lafferty, 1991).
The perplex-ity, defined in Equation 3 for a corpus of N symbols,is a monotone function of the average information ofthe model, and is therefore a measure of compres-sion.PP = P(wl, w~,..., wlv)- r~ (3)It should be mentioned that PPMC is usually usedin adaptive data compression systems.
In our exper-iment we used it in a non-adaptive way; the modelwas inferred from one corpus and tested on another.Although true compression systems avoid this two-pass approach due to the expense of transmittingthe model, evaluation is performed this way in thespeech recognition literature.An iteration of this process was used to producethe plot shown in Figure 2.
Perplexity is given inunits of characters rather than Symbols--this  nec-essary because the alphabet size increases with everychunk added.A minimum perplexity of 4.48 characters was at-tained after 154 chunks had been added to themodel's alphabet.
This represents a 9.5% reductionof the model's initial perplexity of 4.95 characters,equivalent to a 6.2% improvement in compressionperformance.
Although this result is by no meansground-breaking, webelieve that it illustrates theadvantage of chunking.The initial reduction in perplexity is rapid, as thefirst chunks discovered correspond to the most fre-quent English words.
The continued addition ofchunks reduces the perplexity further, discountingminor local variations.
We expect that the per-formance of the model will degrade once too manychunks are added to its alphabet, but the experimentdidn't proceed long enough to make this apparent.4 Conc lus ion  and  Future  WorkWe have shown that a statistical language modelmay discover high-level structure in a data sequenceby thresholding its instantaneous entropy.
Whenthis structure isused to augment the model, its com-pression performance improves.
Although the exam-ple presented in this paper used a natural anguagecorpus, we stress that these techniques are suited tothe analysis of all kinds of data.We plan to investigate how much structure canbe learned by the most trivial of language models.The upwrite process provides caffolding which al-lows high-level structure to be found: we believethat a low-order language model which uses the bi-nary alphabet may be able to find characters, thenwords, and eventually larger scale structures in nat-ural language corpora.Methods to select appropriate entropic thresholdsneed to be investigated, and the application of en-tropic chunking to adaptive data compression sys-tems is being explored and looks promising.Hutchens and Alder 81 Finding Structure via CompressionF03O..4.954.9 t4.854.84.754.74.654.64.554.54.45I I I I I II I I f I I I20 40 60 80 1 O0 120 140 160Number of chunks addedFigure 2: Effect of chunking on model perplexity.Re ferencesAlder, Mike.
1988.
Stochastic grammatical infer-ence.
Master's thesis, University of Western Aus-tralia.Hutchens, Jason L. 1997.
Language acquisition anddata compression.
In Sarah Boyd, editor, 1997Australasian Natural Language Processing Sum-mer Workshop, pages 39-49, February.Jelinek, Frederick and John D. Lafferty.
1991.
Com-putation of the probability of initial substringgeneration by stochastic ontext free grammars.Computational Linguistics, 17(3):315-323.Moffat, Alistair.
1990.
Implementing the PPM datacompression scheme.
IEEE Transactions on Com-munications, 38(11):1917-1921, November.Shannon, Claude E. and Warren Weaver.
1949.
TheMathematical Theory of Communication.
Univer-sity of Illinois Press.Wolff, J. G. 1977.
The discovery of segments innatural language.
British Journal of Psychology,68:97-106.Hutchens and Alder 82 Finding Structure via CompressionIIIIIIIII!IIIIIIII|ilIIIIIIIIIIIIII
