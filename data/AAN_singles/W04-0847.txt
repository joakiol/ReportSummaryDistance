Optimizing Feature Set for Chinese Word Sense DisambiguationZheng-Yu Niu, Dong-Hong JiInstitute for Infocomm Research21 Heng Mui Keng Terrace119613 Singapore{zniu, dhji}@i2r.a-star.edu.sgChew-Lim TanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2117543 Singaporetancl@comp.nus.edu.sgAbstractThis article describes the implementation of I2Rword sense disambiguation system (I2R ?WSD)that participated in one senseval3 task: Chinese lex-ical sample task.
Our core algorithm is a supervisedNaive Bayes classifier.
This classifier utilizes an op-timal feature set, which is determined by maximiz-ing the cross validated accuracy of NB classifier ontraining data.
The optimal feature set includes part-of-speech with position information in local con-text, and bag of words in topical context.1 IntroductionWord sense disambiguation (WSD) is to assign ap-propriate meaning to a given ambiguous word ina text.
Corpus based method is one of the suc-cessful lines of research on WSD.
Many supervisedlearning algorithms have been applied for WSD,ex.
Bayesian learning (Leacock et al, 1998), ex-emplar based learning (Ng and Lee, 1996), decisionlist (Yarowsky, 2000), neural network (Towel andVoorheest, 1998), maximum entropy method (Danget al, 2002), etc..
In this paper, we employ NaiveBayes classifier to perform WSD.Resolving the ambiguity of words usually relieson the contexts of their occurrences.
The featureset used for context representation consists of lo-cal and topical features.
Local features include partof speech tags of words within local context, mor-phological information of target word, local collo-cations, and syntactic relations between contextualwords and target word, etc.. Topical features arebag of words occurred within topical context.
Con-textual features play an important role in providingdiscrimination information for classifiers in WSD.In other words, an informative feature set will helpclassifiers to accurately disambiguate word senses,but an uninformative feature set will deteriorate theperformance of classifiers.
In this paper, we opti-mize feature set by maximizing the cross validatedaccuracy of Naive Bayes classifier on sense taggedtraining data.2 Naive Bayes ClassifierLet C = {c1, c2, ..., cL} represent class labels,F = {f1, f2, ..., fM} be a set of features.
Thevalue of fj , 1 ?
j ?
M , is 1 if fj is present inthe context of target word, otherwise 0.
In classi-fication process, the Naive Bayes classifier tries tofind the class that maximizes P (ci|F ), the proba-bility of class ci given feature set F , 1 ?
i ?
L.Assuming the independence between features, theclassification procedure can be formulated as:i?
= arg max1?i?Lp(ci)?Mj=1 p(fj |ci)?Mj=1 p(fj), (1)where p(ci), p(fj |ci) and p(fj) are estimated usingmaximum likelihood method.
To avoid the effectsof zero counts when estimating p(fj |ci), the zerocounts of p(fj |ci) are replaced with p(ci)/N , whereN is the number of training examples.3 Feature SetFor Chinese WSD, there are two strategies to extractcontextual information.
One is based on Chinesecharacters, the other is to utilize Chinese words andrelated morphological or syntactic information.
Inour system, context representation is based on Chi-nese words, since words are less ambiguous thancharacters.We use two types of features for Chinese WSD:local features and topical features.
All of these fea-tures are acquired from data at senseval3 withoututilization of any other knowledge resource.3.1 Local featuresTwo sets of local features are investigated, whichare represented by LocalA and LocalB.
Let nl de-note the local context window size.LocalA contains only part of speech tagswith position information: POS?nl , ...,POS?1, POS0, POS+1, ..., POS+nl , wherePOS?i (POS+i) is the part of speech (POS) of thei-th words to the left (right) of target word w, andPOS0 is the POS of w.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsLocalB enriches the local context by includingthe following features: local words with position in-formation (W?nl , ..., W?1, W+1, ..., W+nl), bigramtemplates ((W?nl , W?
(nl?1)), ..., (W?1, W+1),..., (W+(nl?1), W+nl)), local words with POS tags(W POS) (position information is not considered),and part of speech tags with position information.All of these POS tags, words, and bigrams aregathered and each of them contributed as one fea-ture.
For a training or test example, the value ofsome feature is 1 if it occurred in local context, oth-erwise it is 0.
In this paper, we investigate two val-ues of nl for LocalA and LocalB, 1 and 2, whichresults in four feature sets.3.2 Topical featuresWe consider all Chinese words within a contextwindow size nt as topical features.
For each trainingor test example, senseval3 data provides one sen-tence as the context of ambiguous word.
In sense-val3 Chinese training data, all contextual sentencesare segmented into words and tagged with part ofspeech.Words which contain non-Chinese character areremoved, and remaining words occurred withincontext window size nt are gathered.
Each remain-ing word is considered as one feature.
The value oftopical feature is 1 if it occurred within window sizent, otherwise it is 0.In later experiment, we set different values for nt,ex.
1, 2, 3, 4, 5, 10, 20, 30, 40, 50.
Our experimen-tal result indicated that the accuracy of sense dis-ambiguation is related to the value of nt.
For differ-ent ambiguous words, the value of nt which yieldsbest disambiguation accuracy is different.
It is de-sirable to determine an optimal value, n?t, for eachambiguous word by maximizing the cross validatedaccuracy.4 Data SetIn Chinese lexical sample task, training data con-sists of 793 sense-tagged examples for 20 ambigu-ous Chinese words.
Test data consists of 380 un-tagged examples for the same 20 target words.
Ta-ble 1 shows the details of training data and test data.5 Criterion for Evaluation of Feature SetsIn this paper, five fold cross validation method wasemployed to estimate the accuracy of our classi-fier, which was the criterion for evaluation of fea-ture sets.
All of the sense tagged examples of sometarget word in senseval3 training data were shuf-fled and divided into five equal folds.
We used fourfolds as training set and the remaining fold as testset.
This procedure was repeated five times underdifferent division between training set and test set.The average accuracy over five runs is defined as theaccuracy of our classifier.6 Evaluation of Feature SetsFour feature sets were investigated:FEATUREA1: LocalA with nl = 1, and topicalfeature within optimal context window size n?t;FEATUREA2: LocalA with nl = 2, and topicalfeature within optimal context window size n?t;FEATUREB1: LocalB with nl = 1, and topicalfeature within optimal context window size n?t;FEATUREB2: LocalB with nl = 2, and topicalfeature within optimal context window size n?t.We performed training and test procedure usingexactly same training and test set for each featureset.
For each word, the optimal value of topical con-text window size n?t was determined by selecting aminimal value of nt which maximized the cross val-idated accuracy.Table 2 summarizes the results of Naive Bayesclassifier using four feature sets evaluated on sen-seval3 Chinese training data.
Figure 1 shows theaccuracy of Naive Bayes classifier as a function oftopical context window size on four nouns and threeverbs.
Several results should be noted specifically:If overall accuracy over 20 Chinese charac-ters is used as evaluation criterion for featureset, the four feature sets can be sorted as fol-lows: FEATUREA1 > FEATUREA2 ?FEATUREB1 > FEATUREB2.
This indi-cated that simply increasing local window size orenriching feature set by incorporating bigram tem-plates, local word with position information, and lo-cal words with POS tags did not improve the perfor-mance of sense disambiguation.In table 2, it showed that with FEATUREA1, theoptimal topical context window size was less than10 words for 13 out of 20 target words.
Figure1 showed that for most of nouns and verbs, NaiveBayes classifier achieved best disambiguation accu-racy with small topical context window size (<10words).
This gives the evidence that for most ofChinese words, including nouns and verbs, the neardistance context is more important than the long dis-tance context for sense disambiguation.7 Experimental ResultThe empirical study in section 6 showed that FEA-TUREA1 performed best among all the feature sets.A Naive Bayes classifier with FEATUREA1 as fea-ture set was learned from all the senseval3 Chinesetraining data for each target word.
Then we usedTable 1: Details of training data and test data in Chinese lexical sample task.POS occurred # senses occurredAmbiguous word in training data # training examples in training data # test examplesba3wo4 n v vn 31 4 15bao1 n nr q v 76 8 36cai2liao4 n 20 2 10chong1ji1 v vn 28 3 13chuan1 v 28 3 14di4fang1 b n 36 4 17fen1zi3 n 36 2 16huo2dong4 a v vn 36 5 16lao3 Ng a an d j 57 6 26lu4 n nr q 57 6 28mei2you3 d v 30 3 15qi3lai2 v 40 4 20qian2 n nr 40 4 20ri4zi5 n 48 3 21shao3 Ng a ad j v 42 5 20tu1chu1 a ad v 30 3 15yan2jiu1 n v vn 30 3 15yun4dong4 n nz v vn 54 3 27zou3 v vn 49 5 24zuo4 v 25 3 12this classifier to determine the senses of occurrencesof target words in test data.
The official result ofI2R?WSD system in Chinese lexical sample taskis listed below:Precision: 60.40% (229.00 correct of 379.00 at-tempted).Recall: 60.40% (229.00 correct of 379.00 in to-tal).Attempted: 100.00% (379.00 attempted of379.00 in total).8 ConclusionIn this paper, we described the implementation ofI2R ?
WSD system that participated in one sen-seval3 task: Chinese lexical sample task.
An op-timal feature set was selected by maximizing thecross validated accuracy of supervised Naive Bayesclassifier on sense-tagged data.
The senses of occur-rences of target words in test data were determinedusing Naive Bayes classifier with optimal featureset learned from training data.
Our system achieved60.40% precision and recall in Chinese lexical sam-ple task.ReferencesDang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D.
(2002) Simple Features for Chinese Word SenseDisambiguation.
In Proc.
of COLING.Leacock, C., Chodorow, M., & Miller G. A.
(1998)Using Corpus Statistics and WordNet Relationsfor Sense Identification.
Computational Linguis-tics, 24:1, 147?165.Mooney, R. J.
(1996) Comparative Experiments onDisambiguating Word Senses: An Illustration ofthe Role of Bias in Machine Learning.
In Proc.of EMNLP, pp.
82-91, Philadelphia, PA.Ng, H. T., & Lee H. B.
(1996) Integrating Multi-ple Knowledge Sources to Disambiguate WordSense: An Exemplar-Based Approach.
In Proc.of ACL, pp.
40-47.Pedersen, T. (2001) A Decision Tree of Bigrams isan Accurate Predictor of Word Sense.
In Proc.
ofNAACL.Towel, G., & Voorheest, E. M. (1998) Disambiguat-ing Highly Ambiguous Words.
ComputationalLinguistics, 24:1, 125?146.Yarowsky, D. (2000) Hierarchical Decision Listsfor Word Sense Disambiguation.
Computers andthe Humanities, 34(1-2), 179?186.Table 2: Accuracy of Naive Bayes classifier with different feature sets on Senseval3 Chinese training data.FEATUREA1 FEATUREA2 FEATUREB1 FEATUREB2Ambiguous word n?t Accuracy n?t Accuracy n?t Accuracy n?t Accuracyba3wo4 5 30.0 4 23.3 4 30.0 3 30.0bao1 2 30.7 20 34.0 2 33.3 20 32.0cai2liao4 2 85.0 2 80.0 2 75.0 2 60.0chong1ji1 20 40.0 3 40.0 30 36.0 1 28.0chuan1 3 72.0 5 68.0 3 56.0 5 64.0di4fang1 2 74.3 1 62.9 1 71.4 1 65.7fen1zi3 20 91.4 50 91.4 20 88.6 20 85.7huo2dong4 5 40.0 20 51.4 10 42.9 4 40.0lao3 3 49.1 4 47.3 3 52.7 20 52.7lu4 1 83.6 2 78.2 2 81.8 1 76.4mei2you3 20 50.0 20 47.9 4 43.3 3 50.0qi3lai2 4 75.0 1 75.0 1 80.0 1 77.5qian2 3 57.5 4 57.5 3 60.0 5 57.5ri4zi5 4 62.2 4 57.8 10 55.6 4 55.6shao3 4 45.0 3 50.0 10 42.5 20 50.0tu1chu1 10 83.3 10 80.0 10 80.0 10 76.7yan2jiu1 20 43.3 20 46.7 10 50.0 20 36.7yun4dong4 10 64.0 10 66.0 10 62.0 10 58.0zou3 5 44.4 5 44.4 4 51.1 4 51.1zuo4 20 64.0 30 60.0 20 64.0 20 64.0Overall 57.7 56.9 57.0 55.10 1 2 3 4 5 10 20 30 40 500.40.50.60.70.80.91ntAccuracy0 1 2 3 4 5 10 20 30 40 500.30.40.50.60.70.8ntAccuracychuan1qi3lai2zuo4cai2liao4fen1zi3qian2ri4zi5Figure 1: Accuracy of Naive Bayes classifier with the optimal feature set FEATUREA1 on four nouns (topfigure) and three verbs (bottom figure).
The horizontal axis represents the topical context window size.
