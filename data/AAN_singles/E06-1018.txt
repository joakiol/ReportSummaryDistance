Word Sense Induction:Triplet-Based Clustering and Automatic EvaluationStefan BordagNatural Language Processing DepartmentUniversity of LeipzigGermanysbordag@informatik.uni-leipzig.deAbstractIn this paper a novel solution to auto-matic and unsupervised word sense induc-tion (WSI) is introduced.
It represents aninstantiation of the ?one sense per colloca-tion?
observation (Gale et al, 1992).
Likemost existing approaches it utilizes clus-tering of word co-occurrences.
This ap-proach differs from other approaches toWSI in that it enhances the effect of theone sense per collocation observation byusing triplets of words instead of pairs.The combination with a two-step cluster-ing process using sentence co-occurrencesas features allows for accurate results.
Ad-ditionally, a novel and likewise automaticand unsupervised evaluation method in-spired by Schu?tze?s (1992) idea of evalu-ation of word sense disambiguation algo-rithms is employed.
Offering advantageslike reproducability and independency ofa given biased gold standard it also en-ables automatic parameter optimization ofthe WSI algorithm.1 IntroductionThe aim of word sense induction1 (WSI) is to findsenses of a given target word (Yarowski, 1995)automatically and if possible in an unsupervisedmanner.
WSI is akin to word sense disambiguation(WSD) both in methods employed and in prob-lems encountered, such as vagueness of sense dis-tinctions (Kilgarriff, 1997).
The input to a WSI al-gorithm is a target word to be disambiguated, e.g.1Sometimes called word sense discovery (Dorow andWiddows, 2003) or word sense discrimination (Purandare,2004; Velldal, 2005)space, and the output is a number of word sets rep-resenting the various senses, e.g.
(3-dimensional,expanse, locate) and (office, building, square).Such results can be at the very least used as empir-ically grounded suggestions for lexicographers oras input for WSD algorithms.
Other possible usesinclude automatic thesaurus or ontology construc-tion, machine translation or information retrieval.But the usefulness of WSI in real-world applica-tions has yet to be tested and proved.2 Related workA substantial number of different approaches toWSI has been proposed so far.
They are all basedon co-occurrence statistics, albeit using differ-ent context representations such as co-occurrenceof words within phrases (Pantel and Lin, 2002;Dorow and Widdows, 2003; Velldal, 2005), bi-grams (Schu?tze, 1998; Neill, 2002; Udani et al,2005), small windows around a word (Gauch andFutrelle, 1993), or larger contexts such as sen-tences (Bordag, 2003; Rapp, 2004) or large win-dows of up to 20 words (Ferret, 2004).
Moreoverthey all employ clustering methods to partition theco-occurring words into sets describing conceptsor senses.
Some algorithms aim for a global clus-tering of words into concepts (Yarowski, 1995;Pantel and Lin, 2002; Velldal, 2005).
But the ma-jority of algorithms are based on a local cluster-ing: Words co-occurring with the target word aregrouped into the various senses the target wordhas.
It is not immediately clear which approachto favor, however aiming at global senses has theinherent property to produce a uniform granular-ity of distinctions between senses that might notbe desired (Rapp, 2004).Graph-based algorithms differ from the ma-jority of algorithms in several aspects.
Words137can be taken as nodes and co-occurrence of twowords defines an edge between the two respec-tive nodes.
Activation spreading on the resultinggraph can be employed (Barth, 2004) in order toobtain most distinctly activated areas in the vicin-ity of the target word.
It is also possible to usegraph-based clustering techniques to obtain senserepresentations based on sub-graph density mea-sures (Dorow and Widdows, 2003; Bordag, 2003).However, it is not yet clear, whether this kind ofapproach differs qualitatively from the standardclustering approaches.
Generally though, the no-tion of sub-graph density seems to be more intu-itive compared to the more abstract clustering.There are different types of polysemy, themost significant distinction probably being be-tween syntactic classes of the word (e.g.
to plantvs.
a plant) and conceptually different senses (e.g.power plant vs. green plant).
As known fromwork on unsupervised part-of-speech tagging (Ro-hwer and Freitag, 2004; Rapp, 2005), the size ofthe window in which words will be found simi-lar to the target word plays a decisive role.
Us-ing most significant direct neighbours as contextrepresentations to compare words results in pre-dominantly syntactical similarity to be found.
Onthe other hand, using most significant sentence co-occurrences results in mostly semantical similarity(Curran, 2003).
However, whereas various contextrepresentations, similarity measures and cluster-ing methods have already been compared againsteach other (Purandare, 2004), there is no evidenceso far, whether the various window sizes or otherparameters have influence on the type of ambigu-ity found, see also (Manning and Schu?tze, 1999,p.
259).Pantel & Lin (2002) introduced an evalua-tion method based on comparisons of the ob-tained word senses with senses provided in Word-Net.
This method has been successfully used byother authors as well (Purandare, 2004; Ferret,2004) because it is straightforward and producesintuitive numbers that help to directly estimatewhether the output of a WSI algorithm is mean-ingful.
On the other hand, any gold standard suchasWordNet is biased and hence also lacks domain-specific sense definitions while providing an abun-dance of sense definitions that occur too rarely inmost corpora.
For example in the British NationalCorpus (BNC), the sense #2 of MALE ([n] thecapital of Maldives) from WordNet is representedby a single sentence only.
Furthermore, compar-ing results of an algorithm to WordNet automat-ically implies another algorithm that matches thefound senses with the senses in WordNet.
This isvery similar to the task of WSD and therefore canbe assumed to be similarly error prone.
These rea-sons have led some researchers to opt for a man-ual evaluation of their algorithms (Neill, 2002;Rapp, 2004; Udani et al, 2005).
Manual evalu-ation, however, has its own disadvantages, mostnotably the poor reproducability of results.
In thiswork a pseudoword based evaluation method simi-lar to Schu?tze?s (1992) pseudoword method is em-ployed.
It is automatic, easy to reproduce andadapts well to domain specificity of a given cor-pus.3 Triplet-based algorithmThe algorithm proposed in this work is based onthe one sense per collocation observation (Galeet al, 1992).
That essentially means that when-ever a pair of words co-occurs significantly of-ten in a corpus (hence a collocation), the con-cept referenced by that pair is unambiguous, e.g.growing plant vs. power plant.
However, asalso pointed out by Yarowsky (1995), this ob-servation does not hold uniformly over all possi-ble co-occurrences of two words.
It is strongerfor adjacent co-occurrences or for word pairs in apredicate-argument relationship than for arbitraryassociations at equivalent distance, e.g.
a plantis much less clear-cut.
To alleviate this problem,the first step of the presented algorithm is to buildtriplets of words (target word and two of it?s co-occurrences) instead of pairs (target word and oneco-occurrence).
This means that a plant is furtherrestricted by another word and even a stop wordsuch as on rules several possibilities of interpreta-tion of a plant out or at least makes them a lot lessimprobable.The algorithm was applied to two types of co-occurrence data.
In order to show the influence ofwindow size, both the most significant sentence-wide co-occurrences and direct neighbour co-occurrences were computed for each word.
Thesignificance values are obtained using the log-likelihood measure assuming a binomial distrib-ution for the unrelatedness hypothesis (Dunning,1993).
For each word, only the 200 most signifi-cant co-occurrences were kept.
This threshold andall others to follow were chosen after experiment-138ing with the algorithm.
However, as will be shownin section 4, the exact set-up of these numbersdoes not matter.
The presented evaluation methodenables to find the optimal configuration of para-meters automatically using a genetic algorithm.The core assumption of the triplet-based al-gorithm is, that any three (or more) words ei-ther uniquely identify a topic, concept or sense.Using the previously acquired most significantco-occurrences (of both types), the lists of co-occurrences for all three words of a triplet areintersected to retain words contained in all threelists.
If the three words cover a topic, e.g.
space,NASA, Mars, then the intersection will not beempty, e.g.
launch, probe, cosmonaut, ....
If thethree words do not identify a meaningful topic,e.g.
space, NASA, cupboard, then the intersectionwill most likely contain few to no words at all.
In-tersections of triplets built from function words arevery likely to contain many co-occurrences evenif they do not identify a unique topic.
These so-called ?stop words?
are thus removed both from theco-occurrences from which triplets are built andfrom the co-occurrences which are used as fea-tures.It is straightforward then to create all possibletriplets of the co-occurrences of the target wordw and to compute the intersection of their co-occurrence lists.
Using these intersections as fea-tures of the triplets, it is possible to group tripletsof words together that have similar features bymeans of any standard clustering algorithm.
How-ever, in order to ?tie?
the referenced meanings ofthe triplets to the target word w, the resultingset of triplets can be restricted only to those thatalso contain the target word.
This has the usefulside-effect that it reduces the number of tripletsto cluster.
To further reduce the remaining num-ber of(2002)= 19900 items to be clustered, aniterative incremental windowing mechanism hasbeen added.
Instead of clustering all triplets inone step, 30 co-occurrences beginning from themost significant ones are taken in each step tobuild(302)= 435 triplets and their intersections.The resulting elements (triplets and intersectionsof their respective co-occurrences as features) arethen clustered with the clusters remaining from theprevious step.In each step of the clustering algorithm, thewords from the triplets and the features aremerged, if the overlap factor similarity measure(Curran, 2003) found them to be similar enough(over 80% overlapping words out of 200).
Thus, ifthe element (space, NASA, Mars) : (orbital, satel-lite, astronauts,...) and (space, launch, Mars) :(orbit, satellite, astronaut, ...) were found to besimilar, they are merged to (space=2, NASA=1,Mars=1, launch=1) : (orbital=1, satellite=2, as-tronauts=1, orbit=1, astronaut=1, ...).
Since themeasure utilizes only the features for comparisons,the result can contain two or more clusters havingalmost identical key sets (which result from merg-ing triplets).
A post-clustering step is thereforeapplied in order to compare clusters by the for-merly triplet words and merge spurious sense dis-tinctions.
After having thus established the finalclusters, the words that remain unclustered can beclassified to the resulting clusters.
Classificationis performed by comparing the co-occurrences ofeach remaining word to the agglomerated featurewords of each sense.
If the overlap similarity tothe most similar sense is below 0.8 the given wordis not classified.
The entire cluster algorithm canthen be summarized as follows:?
Target word is w?
for each step take the next 30 co-occurrencesof w?
Build all possible pairs of the 30 co-occurrences and add w to each to makethem triplets?
Compute intersections of co-occurrences of each triplet?
Cluster the triplets using their intersec-tions as features together with clustersremaining from previous step?
Whenever two clusters are foundto belong together, both the wordsfrom the triplets and the featuresare merged together, increasing theircounts?
Cluster results of the loop by using themerged words of the triplets as features?
Classify unused words to the resulting clus-ters if possibleIn order to reduce noise, for example introducedby triplets of unrelated words still containing a fewwords, there is a threshold of minimum intersec-tion size which was set to 4.
Another parameter139worth mentioning is that after the last clusteringstep all clusters are removed which contain lessthan 8 words.
Keeping track of how many timesa given word has ?hit?
a certain cluster (in eachmerging step) enables to add a post-processingstep.
In this step a word is removed from a clus-ter if it has ?hit?
another cluster significantly moreoften.There are several issues and open questions thatarise from this entire approach.
Most obviously,why to use a particular similarity measure, a par-ticular clustering method or why to merge the vec-tors instead of creating proper centroids.
It is pos-sible that another combination of decisions of thiskind would produce better results.
However, theoverall observation is that the results are fairly sta-ble with respect to such decisions whereas para-meters such as frequency of the target word, sizeof the corpus, balance of the various senses andothers have a much greater impact.4 EvaluationSchu?tze (1992) introduced a pseudoword-basedevaluation method for WSD algorithms.
The ideais to take two arbitrarily chosen words like ba-nana and door and replace all occurrences of ei-ther word by the new pseudoword bananadoor.Then WSD is applied to each sentence and theamount of correctly disambiguated sentences ismeasured.
A disambiguation in this case is cor-rect, if the sentence like I ate the banana is as-signed to sense #1 (banana) instead of #2 (door).In other words all sentences where one of the twowords occurs are viewed as one set and the WSDalgorithm is then supposed to sort them correctlyapart.
This, in fact, is very similar to the WSItask, which is supposed to sort the set of wordsapart that co-occur with the target word and referto its different meanings.
Thus, again it is possibleto take two words, view their co-occurrences asone set and let the WSI algorithm sort them apart.For example, the word banana might have co-occurrences such as apple, fruit, coconut, ... andthe word door co-occurrences such as open, front,locked, ....
The WSI algorithm would thereforehave to disambiguate the pseudoword bananadoorwith the co-occurrences apple, open, fruit, front,locked, ....In short, the method merges the co-occurrencesof two words into one set of words.
Then, the WSIalgorithm is applied to that set of co-occurrencesand the evaluation measures the result by compar-ing it to the original co-occurrence sets.
In order tofind out whether a given sense has been correctlyidentified by the WSI algorithm, its retrieval pre-cision (rP ) - the similarity of the found sensewith the original sense using the overlap measure- can be computed.
In the present evaluations, thethreshold of 0.6 was chosen, which means that atleast 60% of words of the found sense must over-lap with the original sense in order to be counted asa correctly found sense.
The average numbers ofsimilarity are much higher, ranging between 85%and 95%.It is further informative to measure retrieval re-call (rR) - the amount of words that have beencorrectly retrieved into the correct sense.
If, e.g.,two words are merged into a pseudoword and themeaning of each of these two words is representedby 200 co-occurring words, then it could happenthat one of the senses has been correctly found bythe WSI algorithm containing 110 words with anoverlap similarity of 0.91.
That means that only100 words representing the original sense were re-trieved, resulting in a 50% retrieval recall.
Thisretrieval recall also has an upper bound for tworeasons.
The average overlap ratio of the co-occurrences of the word pairs used for the evalua-tion was 3.6%.
Another factor lowering the upperbound by an unknown amount is the fact that someof the words are ambiguous.
If the algorithm cor-rectly finds different senses of one of the two orig-inal words, then only one of the found senses willbe chosen to represent the original ?meaning?
ofthe original word.
All words assigned to the othersense are lost to the other sense.Using terms from information retrieval makessense because this task can be reformulated as fol-lows: Given a set of 400 words and one out of sev-eral word senses, try to retrieve all words belong-ing to that sense (retrieval recall) without retriev-ing any wrong ones (retrieval precision).
A senseis then defined as correctly found by the WSI al-gorithm, if its retrieval precision is above 60% andretrieval recall above 25%.
The latter number im-plies that at least 50 words have to be retrievedcorrectly since the initial co-occurrence sets con-tained 200 words.
This also assumes that 50 wordswould be sufficient to characterize a sense if theWSI algorithm is not only used to evaluate itself.The reason to set the minimum retrieval precisionto any value above 50% is to avoid a too strong140baseline, see below.Using these prerequisites it is possible to defineprecision and recall (based on retrieval precisionand retrieval recall) which will be used to measurethe quality of the WSI algorithm.Precision (P ) is defined as the number of timesthe original co-occurrence sets are properly re-stored divided by the number of different setsfound.
Precision has therefore an unknown upperbound below 100%, because any two words cho-sen could be ambiguous themselves.
Thus, if thealgorithm finds three meanings of the pseudowordthat might be because one of the two words wasambiguous and had two meanings, and hence pre-cision will only be 66%, although the algorithmoperated flawlessly.Recall (R) is defined as the number of sensesfound divided by the number of words merged tocreate the pseudoword.
For example, recall is 60%if five words are used to create the pseudoword,but only three senses were found correctly (ac-cording to retrieval precision and retrieval recall).There is at least one possible baseline for thefour introduced measures.
One is an algorithmthat does nothing, resulting in a single set of 400co-occurrences of the pseudo-word.
This set hasa retrieval Precision rP of 50% compared to ei-ther of the two original ?senses?
because for any ofthe two senses only half of the ?retrieved?
wordsmatch.
This is below the allowed 60% and thusdoes not count as a correctly found sense.
Thismeans that also retrieval Recall rR, Recall R areboth 0% and Precision P in such a case (noth-ing correctly retrieved, but also nothing wrong re-trieved) is defined to be 100%.As mentioned in the previous sections, there areseveral parameters that have a strong impact onthe quality of a WSI algorithm.
One interestingquestion is, whether the quality of disambigua-tion depends on the type of ambiguity: Wouldthe WSI based on sentence co-occurrences (andhence on the bag-of-words model) produce bet-ter results for two syntactically different senses orfor two senses differing by topic (as predicted bySchu?tze (1992)).
This can be simulated by choos-ing two words of different word classes to createthe pseudoword, such as the (dominantly) nouncommittee and the (dominantly) verb accept.Another interesting question concerns the influ-ence of frequency of either the word itself or thesense to be found.
The latter, for example, canbe simulated by choosing one high-frequent wordand one low-frequent word, thus representing awell-represented vs. a poorly represented sense.The aim of the evaluation is to test the describedparameters and produce an overall average of pre-cision and recall and at the same time make it com-pletely reproducable by third parties.
Thereforethe raw BNCwithout baseform reduction (becauselemmatization introduces additional ambiguity) orPOS-tags was used and nine groups each contain-ing five words were picked semi-randomly (avoid-ing extremely ambiguous words, with respect toWordNet, if possible):?
high frequent nouns (Nh): picture, average,blood, committee, economy?
medium frequent nouns (Nm): disintegra-tion, substrate, emigration, thirst, saucepan?
low frequent nouns (Nl): paratuberculosis,gravitation, pharmacology, papillomavirus,sceptre?
high frequent verbs (Vh): avoid, accept, walk,agree, write?
medium frequent verbs (Vm): rend, confine,uphold, evoke, varnish?
low frequent verbs (Vl): immerse, disengage,memorize, typify, depute?
high frequent adjectives (Ah): useful, deep,effective, considerable, traditional?
medium frequent adjectives (Am): ferocious,normative, phenomenal, vibrant, inactive?
low frequent adjectives (Al): astrological,crispy, unrepresented, homoclinic, bitchyThese nine groups were used to design fourstests, each focussing on a different variable.
Thehigh frequent nouns are around 9000 occurrences,medium frequent around 300 and low frequentaround 50.4.1 Influence of word class and frequencyIn the first run of all four tests, sentence co-occurrences were used as features.
In the firsttest, all words of equal word class were viewedas one set of 15 words.
This results in(152)= 105possibilities to combine two of these words into141a pseudoword and test the results of the WSI al-gorithm.
The purpose of this test is to examinewhether there is a tendency for senses of certainword classes to be easier induced.
As can be seenfrom Table 1, sense induction of verbs using sen-tence co-occurrences performs worse compared tonouns.
This could be explained by the fact thatverbs are less semantically specific and need moresyntactic cues or generalizations - both hardly cov-ered by the underlying bag-of-words model - inorder to be disambiguated properly.
At the sametime, nouns and adjectives are much better dis-tinguishable by topical key words.
These resultsseem to be in unison with the prediction made bySchu?tze (1992).P R rP rRNhml 86.97% 86.67% 90.94% 64.21%Vhml 78.32% 64.29% 80.23% 55.20%Ahml 88.57% 70.95% 87.96% 65.38%Table 1: Influence of the syntactic class of the in-put word in Test 1.
Showing precision P and re-call R, as well as average retrieval precision rPand recall rR.In the second test, all three types of possiblecombinations of the word classes are tested, i.e.pseudowords consisting of a noun and a verb, anouns and an adjective and a verb with an adjec-tive.
For each combination there are 15 ?
15 = 225possibilities of combining a word from one wordclass with a word from another word class.
Thepurpose of this test was to demonstrate possibledifferences between WSI of different word classcombinations.
This corresponds to cases when oneword form can be both a nound and a verb, e.g.
awalk and to walk or a noun and an adjective, forexample a nice color and color TV.
However, theresults in Table 2 show no clear tendencies otherthan perhaps that WSI of adjectival senses fromverb senses seems to be slightly more difficult.P R rP rRN/V 86.58% 77.11% 90.51% 61.87%N/A 90.87% 78.00% 90.36% 66.75%V/A 80.84% 63.56% 81.98% 60.89%Table 2: Influence of the syntactic classes of thesenses to be found in Test 2.The third test was designed to show the in-fluence of frequency of the input word.
Allwords of equal frequency are taken as one groupwith(152)= 105 possible combinations.
The re-sults in Table 3 show a clear tendency for higher-frequent word combinations to achieve a betterquality of WSI over lower frequency words.
Thesteep performance drop in recall becomes imme-diately clear when looking at the retrieval recallof the found senses.
This is not surprising, sincewith the low frequency words, each occuring onlyabout 50 times in the BNC, the algorithm runs intothe data sparseness problem that has already beenpointed out as problematic for WSI (Ferret, 2004).P R rP rRhigh 93.65% 78.10% 90.25% 80.70%med.
84.59% 85.24% 89.91% 54.55%low 74.76% 49.52% 71.01% 41.66%Table 3: Influence of frequency of the input wordin Test 3.The fourth test finally shows which influencethe overrepresentation of one sense over anotherhas on WSI.
For this purpose, three possible com-binations of frequency classes, high-frequentwith middle, high with low and middle with low-frequent words were created with 15 ?
15 = 225possible word pairs.
Table 4 demonstrates a steepdrop in recall whenever a low-frequent word ispart of the pseudoword.
This reflects the fact thatit is more difficult for the algorithm to find thesense that was represented by the less frequentword.
The unusually high precision value for thehigh/low combination can be explained by the factthat in this case mostly only one sense was found(the one of the frequent word).
Therefore recall isclose to 50% whereas precision is closer to 100%.P R rP rRh/m 86.43% 79.56% 92.72% 72.08%h/l 91.19% 67.78% 90.85% 74.52%m/l 82.33% 74.00% 85.29% 49.87%Table 4: Influence of different representation ofsenses based on frequency of the two constituentsof the pseudoword in Test 4.Finally it is possible to provide the averages forthe entire test runs comprising 1980 tests.
Themacro averages over all tests are P = 85.42%,R = 72.90%, rP = 86.83% and rR = 62.30%,the micro averages are almost the same.
Using thesame thresholds but only pairs instead of triplets142results in P = 91.00%, R = 60.40%, rP =83.94% and rR = 62.58%.
Or in other words,more often only one sense is retrieved and the F-measures of F = 78.66% for triplets compared toF = 72.61% for pairs confirm an improvement by6% by using triplets.4.2 Window sizeThe second run of all four tests using direct neigh-bors as features failed due to the data sparse-ness problem.
There were 17.5 million wordpairs co-occurring significantly within sentencesin the BNC according to the log-likelihood mea-sure used.
Even there, words with low frequencyshowed a strong performance loss as compared tothe high-frequent words.
Compared to that therewere only 2.3 million word pairs co-occurring di-rectly next to each other.
The overall results ofthe second run with macro averages P = 56.01%,R = 40.64%, rP = 54.28% and rR = 26.79%will not be reiterated here in detail because theyare highly inconclusive due to the data sparseness.The inconclusiveness derives from the fact thatcontrary to the results of the first run, the resultshere vary strongly for various parameter settingsand cannot be considered as stable.Although these results are insufficient to showthe influence of context representations on the typeof induced senses as they were supposed to, theyallow several other insights.
Firstly, corpus sizedoes obviously matter for WSI as more data wouldprobably have alleviated the sparseness problem.Secondly, while perhaps one context representa-tion might be theoretically superior to another(such as neighbor co-occurrences vs. sentenceco-occurrences), the effect various representationshave on the data richness were by far stronger inthe presented tests.4.3 ExamplesIn the light of rather abstract, pseudoword-basedevaluations some real examples sometimes helpto reduce the abstractness of the presented results.Three words, sheet, line and space were chosen ar-bitrarily and some words representing the inducedsenses are listed below.?
sheet?
beneath, blank, blanket, blotting, bot-tom, canvas, cardboard?
accounts, amount, amounts, asset, as-sets, attributable, balance?
line?
angle, argument, assembly, axis, bot-tom, boundary, cell, circle, column?
lines, link, locomotive, locomotives,loop, metres, mouth, north, parallel?
space?
astronaut, launch, launched, manned,mission, orbit, rocket, satellite?
air, allocated, atmosphere, blank,breathing, buildings, ceiling, confinedThese examples show that the found differentia-tions between senses of words indeed are intuitive.They also show that the found senses are only themost distinguishable ones and many futher sensesare missing even though they do appear in theBNC, some of them even frequently.
It seemsthat for finer grained distinctions the bag-of-wordsmodel is not appropriate, although it might proveto be sufficient for other applications such as In-formation Retrieval.
Varying contextual represen-tations might prove to be complementary to the ap-proach presented here and enable the detection ofsyntactic differences or collocational usages of aword.5 ConclusionsIt has been shown that the approach presented inthis work enables automatic and knowledge-freeword sense induction on a given corpus with highprecision and sufficient recall values.
The inducedsenses of the words are inherently domain-specificto the corpus used.
Furthermore, the inducedsenses are only the most apparent ones while thetype of ambiguity matters less than expected.
Butthere is a clear preference for topical distinctionsover syntactic ambiguities.
The latter effect isdue to the underlying bag-of-words model, hencealternative contextual representations might yielddifferent (as opposed to better/worse) results.
Thisbag-of-words limitation also implies some sensesto be found that would be considered as spuriousin other circumstances.
For example, the wordchallenger induces 5 senses, three of them de-scribing the opponent in a game.
The differencesfound are strong, however, as the senses distin-guished are between a chess-challenger, a GrandPrix challenger and a challenger in boxing, eachhave a large set of specific words distinguishingthe senses.143There are several questions that remain open.As the frequency of a word has a great impacton the possibility to disambiguate it correctly us-ing the presented methods, the question is to whatextent corpus size plays a role in this equation ascompared to balancedness of the corpus and there-fore the senses to be found.
Another question isconnected to the limitation of the presented algo-rithm which requires that any sense to be inducedhas to be representable by a rather large amount ofwords.
The question then is, whether this (or anyother similar) algorithm can be improved to dis-cern ?small?
senses from random noise.
A com-bination with algorithms finding collocational us-ages of words probably offers a feasible solution.The evaluation method employed can be usedfor automatic optimization of the algorithm?s ownparameters using genetic algorithms.
Moreover, itwould be interesting to employ genetic program-ming in order to let an optimal word sense induc-tion algorithm design itself.ReferencesMichael Barth.
2004.
Extraktion von Textele-menten mittels ?spreading activation?
fu?r indikativeTextzusammenfassungen.
Master?s thesis, Univer-sity of Leipzig.Stefan Bordag.
2003.
Sentence co-occurrences assmall-world-graphs: A solution to automatic lexi-cal disambiguation.
In Proceedings of CICling-03,LNCS 2588, pages 329?333.
Springer.James Richard Curran.
2003.
From Distributional toSemantic Similarity.
Ph.D. thesis, Institute for Com-municating and Collaborative Systems, School ofInformatics.
University of Edinburgh.Beate Dorow and Dominic Widdows.
2003.
Discover-ing corpus-specific word senses.
In Proceedings ofEACL 2003, pages 79?82, Budapest, Hungary.Ted E. Dunning.
1993.
Accurate methods for the sta-tistics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Olivier Ferret.
2004.
Discovering word senses froma network of lexical cooccurrences.
In Proceedingsof Coling 2004, pages 1326?1332, Geneva, Switzer-land, August.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992.
Work on statistical methods forword sense disambiguation.
Intelligent Probabilis-tic Approaches to Natural Language, Fall Sympo-sium Series(FS-92-04):54?60, March.Susan Gauch and Robert P. Futrelle.
1993.
Experi-ments in automatic word class and word sense iden-tification for information retrieval.
In Proceedingsof 3rd Annual Symposium on Document Analysisand Information Retrieval, pages 425?434.Adam Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31(2):91?113.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural LanguageProcessing.
MIT Press.Daniel B. Neill.
2002.
Fully automatic word senseinduction by semantic clustering.
Master?s thesis,Cambridge University.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of ACMSIGKDD, pages 613?619, Edmonton.Amruta Purandare.
2004.
Word sense discrimina-tion by clustering similarity contexts.
Master?s the-sis, Department of Computer Science, University ofMinnesota, Duluth.Reinhard Rapp.
2004.
Mining text for word sensesusing independent component analysis.
In Pro-ceedings of SIAM International Conference on DataMining 2004.Reinhard Rapp.
2005.
A practical solution to the prob-lem of automatic part-of-speech induction from text.In Proceedings of the ACL Interactive Poster andDemonstration Sessions, pages 77?80, Ann Arbor,June.
ACL.Richard Rohwer and Dayne Freitag.
2004.
Towardsfull automation of lexicon construction.
In Proceed-ings of HLT-NAACL 04: Computational Lexical Se-mantics Workshop, Boston, MA.Hinrich Schu?tze.
1992.
Context space.
In WorkingNotes of the AAAI Fall Symposium on ProbabilisticApproaches to Natural Language, pages 113?120,Menlo Park, CA.
AAAI Press.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24:97?124.Goldee Udani, Shachi Dave, Anthony Davis, and TimSibley.
2005.
Noun sense induction using websearch results.
In Proceedings of 28th ACM SIGIR,pages 657?658, Salvador, Brazil.Erik Velldal.
2005.
A fuzzy clustering approach toword sense discrimination.
In Proceedings of the7th International conference on Terminology andKnowledge Engineering, Copenhagen, Denmark.David Yarowski.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
ACL,33:189?196.144
