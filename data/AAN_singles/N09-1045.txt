Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397?405,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving the Arabic Pronunciation Dictionary for Phone and WordRecognition with Linguistically-Based Pronunciation RulesFadi Biadsy?
and Nizar Habash?
and Julia Hirschberg?
?Department of Computer Science, Columbia University, New York, USA{fadi,julia}@cs.columbia.edu?Center for Computational Learning Systems, Columbia University, New York, USAhabash@ccls.columbia.eduAbstractIn this paper, we show that linguistically mo-tivated pronunciation rules can improve phoneand word recognition results for Modern Stan-dard Arabic (MSA).
Using these rules andthe MADA morphological analysis and dis-ambiguation tool, multiple pronunciations perword are automatically generated to build twopronunciation dictionaries; one for trainingand another for decoding.
We demonstratethat the use of these rules can significantlyimprove both MSA phone recognition andMSA word recognition accuracies over a base-line system using pronunciation rules typi-cally employed in previous work on MSA Au-tomatic Speech Recognition (ASR).
We ob-tain a significant improvement in absolute ac-curacy in phone recognition of 3.77%?7.29%and a significant improvement of 4.1% in ab-solute accuracy in ASR.1 IntroductionThe correspondence between orthography and pro-nunciation in Modern Standard Arabic (MSA) fallssomewhere between that of languages such as Span-ish and Finnish, which have an almost one-to-onemapping between letters and sounds, and languagessuch as English and French, which exhibit a morecomplex letter-to-sound mapping (El-Imam, 2004).The more complex this mapping is, the more diffi-cult the language is for Automatic Speech Recogni-tion (ASR).An essential component of an ASR system is itspronunciation dictionary (lexicon), which maps theorthographic representation of words to their pho-netic or phonemic pronunciation variants.
For lan-guages with complex letter-to-sound mappings, suchdictionaries are typically written by hand.
However,for morphologically rich languages, such as MSA,1pronunciation dictionaries are difficult to create byhand, because of the large number of word forms,each of which has a large number of possible pro-nunciations.
Fortunately, the relationship betweenorthography and pronunciation is relatively regu-lar and well understood for MSA.
Moreover, re-cent automatic techniques for morphological anal-ysis and disambiguation (MADA) can also be usefulin automating part of the dictionary creation process(Habash and Rambow, 2005; Habash and Rambow,2007) Nonetheless, most documented Arabic ASRsystems appear to handle only a subset of Arabicphonetic phenomena; very few use morphologicaldisambiguation tools.In Section 2, we briefly describe related work, in-cluding the baseline system we use.
In Section 3, weoutline the linguistic phenomena we believe are crit-ical to improving MSA pronunciation dictionaries.In Section 4, we describe the pronunciation rules wehave developed based upon these linguistic phenom-ena.
In Section 5, we describe how these rules areused, together with MADA, to build our pronuncia-tion dictionaries for training and decoding automat-ically.
In Section 6, we present results of our eval-uations of our phone- and word-recognition systems(XPR and XWR) on MSA comparing these systemsto two baseline systems, BASEPR and BASEWR.1MSA words have fourteen features: part-of-speech, person,number, gender, voice, aspect, determiner proclitic, conjunctiveproclitic, particle proclitic, pronominal enclitic, nominal case,nunation, idafa (possessed), and mood.
MSA features are real-ized using both concatenative (affixes and stems) and templatic(root and patterns) morphology with a variety of morphologicaland phonological adjustments that appear in word orthographyand interact with orthographic variations.397We conclude in Section 7 and identify directions forfuture research.2 Related WorkMost recent work on ASR for MSA uses a sin-gle pronunciation dictionary constructed by map-ping every undiacritized word in the training cor-pus to all of the diacritized Buckwalter analyses andthe diacritized versions of this word in the ArabicTreebank (Maamouri et al, 2003; Afify et al, 2005;Messaoudi et al, 2006; Soltau et al, 2007).
In thesepapers, each diacritized word is converted to a sin-gle pronunciation with a one-to-one mapping using?very few?
unspecified rules.
None of these systemsuse morphological disambiguation to determine themost likely pronunciation of the word given its con-text.
Vergyri et al (2008) do use morphological in-formation to predict word pronunciation.
They se-lect the top choice from the MADA (MorphologicalAnalysis and Disambiguation for Arabic) system foreach word to train their acoustic models.
For the testlexicon they used the undiacritized orthography, aswell as all diacritizations found for each word in thetraining data as possible pronunciation variants.
Weuse this system as our baseline for comparison.3 Arabic Orthography and PronunciationMSA is written in a morpho-phonemic orthographicrepresentation using the Arabic script, an alphabetaccented with optional diacritical marks.2 MSA has34 phonemes (28 consonants, 3 long vowels and 3short vowels).
The Arabic script has 36 basic let-ters (ignoring ligatures) and 9 diacritics.
Most Ara-bic letters have a one-to-one mapping to an MSAphoneme; however, there are a small number ofcommon exceptions (Habash et al, 2007; El-Imam,2004) which we summarize next.3.1 Optional DiacriticsArabic script commonly uses nine optional diacrit-ics: (a) three short-vowel diacritics representing thevowels /a/, /u/ and /i/; (b) one long-vowel diacritic(Dagger Alif ?)
representing the long vowel /A/ in a2We provide Arabic script orthographic transliteration inthe Buckwalter transliteration scheme (Buckwalter, 2004).
ForModern Standard Arabic phonological transcription, we use avariant of the Buckwalter transliteration with the following ex-ceptions: glottal stops are represented as /G/ and long vowels as/A/, /U/ and /I/.
All Arabic script diacritics are phonologicallyspelled out.small number of words; (c) three nunation diacrit-ics (F /an/, N /un/, K /in/) representing a combina-tion of a short vowel and the nominal indefinitenessmarker /n/ in MSA; (d) one consonant lengtheningdiacritic (called Shadda ?)
which repeats/elongatesthe previous consonant (e.g., kat?ab is pronounced/kattab/); and (e) one diacritic for marking whenthere is no diacritic (called Sukun o).Arabic diacritics can only appear after a let-ter.
Word-initial diacritics (in practice, only shortvowels) are handled by adding an extra Alif A A(also called Hamzat-Wasl) at the beginning of theword.
Sentence/utterance initial Hamzat-Wasl ispronounced like a glottal stop preceding the shortvowel; however, the sentence medial Hamzat-Waslis silent except for the short vowel.
For exam-ple, Ainkataba kitAbN is /Ginkataba kitAbun/ butkitAbN Ainkataba is /kitAbun inkataba/.
A ?real?Hamza (glottal stop) is always pronounced as a glot-tal stop.
The Hamzat-Wasl appears most commonlyas the Alif of the definite article Al.
It also appearsin specific words and word classes such as relativepronouns (e.g., Aly ?who?
and verbs in pattern VII(Ain1a2a3).Arabic short vowel diacritics are used togetherwith the glide consonant letters w and y to denotethe long vowels /U/ (as uw) and /I/ (iy).
This makesthese two letters ambiguous.Diacritics are largely restricted to religious textsand Arabic language school textbooks.
In othertexts, fewer than 1.5% of words contain a diacritic.Some diacritics are lexical (where word meaningvaries) and others are inflectional (where nominalcase or verbal mood varies).
Inflectional diacriticsare typically word final.
Since nominal case, verbalmood and nunation have all disappeared in spokendialectal Arabic, Arabic speakers do not always pro-duce these inflections correctly or at all.Much work has been done on automatic Arabicdiacritization (Vergyri and Kirchhoff, 2004; Anan-thakrishnan et al, 2005; Zitouni et al, 2006; Habashand Rambow, 2007).
In this paper, we use theMADA (Morphological Analysis and Disambigua-tion for Arabic) system to diacritize Arabic (Habashand Rambow, 2005; Habash and Rambow, 2007).MADA, which uses the Buckwalter Arabic mor-phological Analyzer databases (Buckwalter, 2004),provides the necessary information to determineHamzat-Wasl through morphologically tagging thedefinite article; in most other cases it outputs the spe-cial symbol ?{?
for Hamzat-Wasl.3983.2 Hamza SpellingThe consonant Hamza (glottal stop /G/) has multi-ple forms in Arabic script:  ?,  >,  <,  &, ?
},  |.
There are complex rules for Hamza spellingthat primarily depend on its vocalic context.
For ex-ample, ? }
is used word medially and finally whenpreceded or followed by an /i/ vowel.
Similarly, theHamza form  | is used when the Hamza is followedby the long vowel /A/.Hamza spelling is further complicated by the factthat Arabic writers often replace hamzated letterswith the un-hamzated form ( > ?
A A) or use atwo-letter spelling, e.g.
? }
?
?
Y ?.
Due tothis variation, the un-hamzated forms (particularlyfor  > and  <) are ignored in Arabic ASR evalua-tion.
The MADA system regularizes most of thesespelling variations as part of its analysis.3.3 Morpho-phonemic SpellingArabic script includes a small number of mor-phemic/lexical phenomena, some very common:?
Ta-Marbuta The Ta-Marbuta (p) is typically afeminine ending.
It appears word-finally, op-tionally followed by a diacritic.
In MSA itis pronounced as /t/ when followed by a di-acritic; otherwise it is silent.
For example,maktabapN ?a library?
is pronounced / mak-tabatun/.?
Alif-Maqsura The Alif-Maqsura (Y ) is a silentderivational marker, which always follows ashort vowel /a/ at the end of a word.
For ex-ample, rawaY ?to tell a story?
is pronounced/rawa/.?
Definite Article The Arabic definite article isa proclitic that assimilates to the first conso-nant in the noun it modifies if this consonantis alveolar or dental (except for j).
These arethe so-called Sun Letters: t, v, d, *, r, z, s, $,S, D, T, Z, l, and n. For example, the wordAl$ams ?the sun?
is pronounced /a$$ams/ not*/al$ams/.
The definite article does not assimi-late to the other consonants, the Moon Letters.For example, the word Alqamar ?the moon?
ispronounced /alqamar/ not */aqqamar/.?
Silent Letters A silent Alif appears in the mor-pheme +uwA /U/ which indicates masculineplural conjugation in verbs.
Another silent Alifappears after some nunated nouns, e.g., ki-taAbAF /kitAban/.
In some poetic readings,this Alif can be produced as the long vowel/A/: /kitAbA/.
Finally, a common odd spellingis that of the proper name Eamrw /Eamr/?Amr?where the final w is silent.4 Pronunciation RulesAs noted in Section 3, diacritization alone does notpredict actual pronunciation in MSA.
In this sectionwe describe a set of rules based on MSA phonol-ogy which will extend a diacritized word to a setof possible pronunciations.
It should be noted thateven MSA-trained speakers, such as broadcast newsanchors, may not follow the ?proper?
pronunciationaccording to Arabic syntax and phonology.
So weattempt to accommodate these pronunciation vari-ants in our pronunciation dictionary.The following rules are applied on each dia-critized word.3 These rules are divided into fourcategories: (I) a shared set of rules used in allsystems compared (BASEPR, BASEWR, XPRand XWR);4 (II) a set of rules in BASEPR andBASEWR which we modified for XPR and XWR;(III) a first set of new rules devised for our systemsXPR and XWR; and (IV) a second set of new rulesthat generate additional pronunciation variants.Below we indicate, for each rule, how many wordsin the training corpus (335,324 words) had theirpronunciation affected by the rule.I.
Shared Pronunciation Rules1.
Dagger Alif: ?
?
/A/(e.g., h?
*A ?
hA*A) (This rule affected 1.8%of all the words in our training data)2.
Madda: | ?
/G A/(e.g., Al|n ?
AlGAn) (affected 1.9%)3.
Nunation: AF ?
/a n/, F ?
/a n/, /K/ ?
/i n/,N ?
/u n/(e.g., kutubAF ?
kutuban) (affected 9.7%)4.
Hamza: All Hamza forms: ?, },&, <,> ?
/G/(e.g., >kala ?
Gakala) (affected 21.3%)3Our script that generates the pronunciation dictio-naries from MADA output can be downloaded fromwww.cs.columbia.edu/speech/software.cgi.4We have attempted to replicate the baseline pronunciationrules for (Vergyri et al, 2008) based on published work andpersonal communications with the authors.3995.
Ta-Marbuta: p ?
/t/(e.g., madrasapa ?
madrasata) (affected15.3%)II.
Modified Pronunciation Rules1.
Alif-Maqsura: Y ?
/a/(e.g., salomY ?
saloma) (affected 4.2%)(Baseline: Y ?
/A/)2.
Shadda: Shadda is always removed(e.g., ba$?ara ?
ba$ara) (affected 23.8%)(Baseline: the consonant was doubled)3.
U and I: uwo ?
/U/, iyo ?
/I/(e.g., makotuwob ?
makotUb) (affected25.07%) (Baseline: same rule but it inaccu-rately interacted with the baseline Shadda rule)III.
New Pronunciation Rules1.
Waw Al-jamaa: suffixes uwoA ?
/U/(e.g., katabuwoA ?
katabU) (affected 0.4%)2.
Definite Article: Al ?
/a l/ (if tagged as Al+by MADA)(e.g., wAlkitAba ?
walkitAba) (affected30.0%)3.
Hamzat-Wasl: { is always removed.
(affected 3.0%)4.
?Al?
in relative pronouns: Al ?
/a l/(affected 1.3%)5.
Sun letters: if the definite article (Al) is fol-lowed by a sun letter, remove the l.(e.g., Al$amsu ?
A$amsu) (affected 8.1%)IV.
New Pronunciation Rules Generating Addi-tional Variants?
Ta-Marbuta: if a word ends with Ta-Marbuta(p) followed by any diacritic, remove the Ta-Marbuta and its diacritic.
Apply the rules above(I-III) on the modified word and add the outputpronunciation.
(e.g., marbwTapF ?
marbwTa) (affected15.3%)?
Case ending: if a word ends with a short vowel(a, u, i), remove the short vowel.
Apply rules(I-III) on the modified word, and add the outputpronunciation(e.g., yaktubu ?
yaktub (affected 60.9%)As a post-processing step in all systems, we re-move the Sukun diacritic and convert every letter Xto phoneme /X/.
In XPR and XWR, we also removeshort vowels that precede or succeed a long vowel.5 Building the Pronunciation DictionariesAs noted above, pronunciation dictionaries mapwords to one or more phonetically expressed pro-nunciation variants.
These dictionaries are usedfor training and decoding in ASR systems.
Typi-cally, most data available to train large vocabularyASR systems is orthographically (not phonetically)transcribed.
There are two well-known alternativesfor training acoustic models in ASR: (1) bootstraptraining, when some phonetically annotated data isavailable, and (2) flat-start, when such data is notavailable (Young et al, 2006).
In flat-start training,for example, the pronunciation dictionary is usedto map the orthographic transcription of the train-ing data to a sequence of phonetic labels to trainthe initial monophone models.
Next, the dictionaryis employed again to produce networks of possiblepronunciations which can be used in forced align-ment to obtain the most likely phone sequence thatmatches the acoustic data.
Finally, the monophoneacoustic models are re-estimated.
In our work, werefer to this dictionary as the training pronuncia-tion dictionary.
The second usage of the pronun-ciation dictionary is to generate the pronunciationmodels while decoding.
We refer to this dictionaryas the decoding pronunciation dictionary.For languages like English, no distinction be-tween decoding and training pronunciation dictio-naries is necessary.
However, as noted in Section3, short vowels and other diacritic markers are typi-cally not orthographically represented in MSA texts.Thus ASR systems typically do not output fully di-acritized transcripts.
Diacritization is generally notnecessary to make the transcript readable by Arabic-literate readers.
Therefore, entries in the decod-ing pronunciation dictionary consist of undiacritizedwords that are mapped to a set of phonetically-represented diacritizations.
However, every entry inthe training pronunciation dictionary is a fully dia-critized word mapped to a set of possible context-dependent pronunciations.
Particularly in the train-ing step, contextual information for each word isavailable from the transcript, so, for our work, wecan use the MADA morphological tagger to obtainthe most likely diacritics.
As a result, the speechsignal is mapped to a more accurate representation400of the training transcript, which we hypothesize willlead to a better estimation of the acoustic models.As noted in Section 1, pronunciation dictionariesfor ASR systems are usually written by hand.
How-ever, Arabic?s morphological richness makes it dif-ficult to create a pronunciation dictionary by handsince there are a very large number of word forms,each of which has a large number of possible pro-nunciations.
The relatively regular relationship be-tween orthography and pronunciation and tools formorphological analysis and disambiguation such asMADA, however, make it possible to create suchdictionaries automatically with some success.55.1 Training Pronunciation DictionaryIn this section, we describe an automatic approachto building a pronunciation dictionary for MSA thatcovers all words in the orthographic transcripts ofthe training data.
First, for each word in each ut-terance, we run MADA to disambiguate the wordbased on its context in the transcript.
MADA outputsall possible fully-diacritized morphological analy-ses, ranked by their likelihood, the MADA confi-dence score.6 We thus obtain a fully-diacritized or-thographic transcription for training.
Second, wemap the highest-ranked diacritization of each wordto a set of pronunciations, which we obtain from thepronunciation rules described in Section 4.
SinceMADA may not always rank the best analysis as itstop choice, we also run the pronunciation rules onthe second best choice returned by MADA, whenthe difference between the top two choices is lessthan a threshold determined empirically (in our im-plementation we chose 0.2).
In Figure 1, the trainingpronunciation dictionary maps the 2nd column (theentry keys) to the 3rd column.We generate the baseline training pronunciationdictionary using only the baseline rules from Section4.
This dictionary also makes use of MADA, but itmaps the MADA-diacritized word to only one pro-nunciation.
The baseline training dictionary mapsthe 2nd column (the entry keys) to only one pronun-ciation in the 3rd column in Figure 1.5The MADA system (Habash and Rambow, 2005; Habashand Rambow, 2007) reports 4.8% diacritic error rate (DER) onall diacritics and 2.2% (DER) when ignoring the last (inflec-tional) diacritic.6In our training data, only about 1% of all words are notdiacritized because of lack of coverage in the morphologicalanalysis component.				!
"#$%			& '!
'"(#'$%		& '!
)"'#*$%					%Figure 1: Mapping an undiacritized word to MADA out-puts to possible pronunciations.5.2 Decoding Pronunciation DictionaryThe decoding pronunciation dictionary is used inASR to build the pronunciation models while decod-ing.
Since, as noted above, it is standard to produceunvocalized transcripts when recognizing MSA, wemust map word pronunciations to unvocalized ortho-graphic output.
Therefore, for each diacritized wordin our training pronunciation dictionary, we removediacritic markers and replace Hamzat-Wasl ({), <,and > by the letter ?A?, and then map the modifiedword to the set of pronunciations for that word.
Forexample, in Figure 1 the undiacritized word mdrspin the 1st column is mapped to the pronunciationsin the 3rd column.
The baseline decoding pronun-ciation dictionary is constructed similarly from thebaseline training pronunciation dictionary.6 EvaluationTo determine whether our pronunciation rules areuseful in speech processing applications, we eval-uated their impact on two tasks, automatic phonerecognition and ASR.
For our experiments, we usedthe broadcast news TDT4 corpus (Arabic Set 1), di-vided into 47.61 hours of speech (89 news shows)for training and 5.18 hours (11 shows); test andtraining shows were selected at random.
Both train-ing and test data were segmented based on silenceand non-speech segments and down-sampled to8Khz.7 This segmentation produced 20,707 speechsegments for our training data and 2,255 segmentsfor testing.7One of our goals is phone recognition telephone conversa-tion for Arabic dialect identifaction, hence the down-sampling.4016.1 Acoustic ModelsOur monophone acoustic models are built using 3-state continuous HMMs without state-skipping witha mixture of 12 Gaussians per state.
We extractstandard MFCC (Mel Frequency Cepstral Coeffi-cients) features from 25 ms frames, with a frameshift of 10 ms. Each feature vector is 39D: 13 fea-tures (12 cepstral features plus energy), 13 deltas,and 13 double-deltas.
The features are normalizedusing cepstral mean normalization.
For our ASRexperiments, tied context-dependent cross-word tri-phone HMMs are created with the same settings asmonophones.
The acoustic models are speaker- andgender-independent, trained using ML (maximumlikelihood) with flat-start.8 We build our frameworkusing the HMM Toolkit (HTK) (Young et al, 2006).6.2 Phone Recognition EvaluationWe hypothesize that improved pronunciation ruleswill have a profound impact on phone recognitionaccuracy.
To compare our phone recognition (XPR)system with the baseline (BASEPR), we train twophone recognizers using HTK.
The BASEPR rec-ognizer uses the training-pronunciation dictionarygenerated using the baseline rules; the XPR sys-tem uses a pronunciation dictionary generated usingthese rules plus our modified and new rules (cf.
Sec-tion 5).
The two systems are identical except fortheir pronunciation dictionaries.We evaluate the two systems under two condi-tions: (1) phone recognition with a bigram phonelanguage model (LM)9 and (2) phone recognitionwith an open-loop phone recognizer, such that anyphoneme can follow any other phoneme with a uni-form distribution.
Results of this evaluation are pre-sented in Table 1.Ideally, we would like to compare the perfor-mance of these systems against a common MSAphonetically-transcribed gold standard.
Unfortu-nately, to our knowledge, such a data set does notexist.
So we approximate such a gold standardon a blind test set through forced alignment, us-ing the trained acoustic models and pronunciation8Since our focus is a comparison of different approaches topronunciation modeling on Arabic recognition tasks, we havenot experimented with different features, parameters, and differ-ent machine learning approaches (such as discriminative train-ing and/or the combination of both).9The bigram phoneme LM of each phone recognizer istrained on the phonemes obtained from forced aligning thetraining transcript to the speech data using that recognizer?straining pronunciation dictionary and acoustic models.dictionaries.
Since our choice of acoustic model(of BASEPR or XPR) and pronunciation dictionary(again of BASEPR or XPR) can bias our results,we consider four gold variants (GV) with differ-ent combinations of acoustic model and pronunci-ation dictionary, to set expected lower and upperbounds.
These combinations are represented in Ta-ble 1 as GV1?4, where the source of acoustic mod-els is BASEPR or XPR and source of pronuncia-tion rules are BASEPR, XPR or XPR and BASEPRcombined.
These GV are described in more detailbelow, as we describe our results.Since BASEPR system uses a pronunciation dic-tionary with a one-to-one mapping of orthographyto phones, the GV1 phone sequence for any testutterance?s orthographical transcript according toBASEPR can be obtained directly from the ortho-graphic transcript.
Note that if, in fact, GV1 doesrepresent the true gold standard (i.e., the correctphone sequence for the test utterances) then if XPRobtains a lower phone error rate using this gold stan-dard than BASEPR does, we can conclude that infact XPR?s acoustic models are better estimated.This is in fact the case.
In Table 1, first line, wesee that XPR under both conditions (open-loop andbigram LM) significantly (p-value < 2.2e?16) out-performs the corresponding BASEPR phone recog-nizer using GV1.10If GV1 does not accurately represent the phonesequences of the test data, then there must be somephones in the GV1 sequences that should be deleted,inserted, or substituted.
On the hypothesis that ourtraining-pronunciation dictionary might improve theBASEPR assignments, we enrich the baseline pro-nunciation dictionary with XPR?s dictionary.
Now,we force-align the orthographic transcript usingthis extended pronunciation dictionary, still usingBASEPR?s acoustic models, with the acoustic sig-nal.
We denote the output phone sequences as GV2.If a pronunciation generated using the BASEPR dic-tionary was already correct (in GV1) according tothe acoustic signal, this forced alignment processstill has the option of choosing it.
We hypothesizethat the result, GV2, is a more accurate represen-tation of the true phone sequences in the test data,since it should be able to model the acoustic sig-nal more accurately.
On GV2, as on GV1, we seethat XPR, under both conditions, significantly (p-10Throughout this discussion we use paired t-tests to measuresignificant difference, where the sample values are the phonerecognizer accuracies on the utterances.402Gold Variants Open-loop (Accuracy) Bigram Phone LM (Accuracy)GV Acoustic Model of Pron.
Dict.
of BASEPR XPR BASEPR XPR1 BASEPR BASEPR 37.40 39.21 41.56 45.172 BASEPR BASEPR+XPR 38.64 42.41 43.44 50.733 XPR XPR 37.06 42.38 42.21 51.414 XPR BASEPR+XPR 37.47 42.74 42.59 51.51Table 1: Comparing the effect of BASEPR and XPR pronunciation rules, alone and in combination, using 4 GoldVariants under two conditions (Open-loop and LM)value < 2.2e ?
16) outperforms the correspondingBASEPR phone recognizers (see Table 1, secondline).We also compared the performance of the twosystems using upper bound variants.
For GV3 weused the forced alignment of the orthographic tran-scription using only XPR?s pronuncation dictionarywith XPR?s acoustic models.
In GV4 we combinethe pronunciation dictionary of XPR with BASEPRdictionary and use XPR?s acoustic models.
Unsur-prisingly, we find that the XPR recognizer signifi-cantly (p-value <2.2e ?
16) outperforms BASEPRwhen using these two variants under both conditions(see Table 1, third and fourth lines).The results presented in Table 1 compare the ro-bustness of the acoustic models as well as the pro-nunciation components of the two systems.
We alsowant to evaluate the accuracy of our pronunciationpredictions in representing the actual acoustic sig-nal.
One way to do this is to see how often the forcedalignment process choose phone sequences usingthe BASEPR pronunciation dictionary as opposedto XPR?s.
We forced aligned the test transcript ?using the XPR acoustic models and only the XPRpronunciation dictionary ?
with the acoustic sig-nal.
We then compare the output sequences to theoutput of the forced alignment process where thecombined pronunciations from BASEPR+XPR andthe XPR acoustic models were used.
We find thatthe difference between the two is only 1.03% (with246,376 phones, 557 deletions, 1696 substitutions,and 277 insertions).
Thus, adding the BASEPR rulesto XPR does not appear to contribute a great deal tothe representation chosen by forced alignment.
Ina similar experiment, we use the BASEPR acous-tic models instead of the XPR models and comparethe results of using BASEPR-pronunciation dictio-nary with the combination of XPR+BASEPR?s dic-tionaries for forced alignment.
Interestingly, in thisexperiment we do find a significantly larger differ-ence between the two outputs 17.04% (with 233,787phones, 1404 deletions, 14013 substitutions, and27040 insertions).
We can hypothesize from theseexperiments that the baseline pronunciation dictio-nary alone is not sufficient to represent the acousticsignal accurately, since large numbers of phonemesare edited when adding the XPR pronunciations.
Incontrast, adding the BASEPR?s pronunciation dic-tionary to XPR?s shows a relatively small percent-age of edits, which suggests that the XPR pronun-ciation dictionary extends and covers more accu-rately the pronunciations already contained in theBASEPR dictionary.6.3 Speech Recognition EvaluationWe have also conducted an ASR experiment to eval-uate the usefulness of our pronunciation rules forthis application.11 We employ the baseline pro-nunciation rules to generate the baseline trainingand decoding pronunciation dictionaries.
Usingthese dictionaries, we build the baseline ASR sys-tem (BASEWR).
Using our extended pronunciationrules, we generate our dictionaries and train ourASR system (XWR).
Both systems have the samemodel settings, as described in Section 6.1.
Theyalso share the same language model (LM), a trigramLM trained on the undiacritized transcripts of thetraining data and a subset of Arabic gigawords (ap-proximately 281 million words, in total), using theSRILM toolkit (Stolcke, 2002).Table 2 presents the comparison of BASEWRwith the XWR system.
In Section 5.1, we noted thatthe top two choices from MADA may be included inthe XWR pronunciation dictionary when the differ-ence in MADA confidence scores for these two isless than a given threshold.
So we analyze the im-pact of including this second MADA option in boththe training and decoding dictionaries on ASR re-sults.
In all cases, whether the second MADA choice11It should be noted that we have not attempted to build astate-of-the-art Arabic speech recognizer; our goal is purely toevaluate our approach to pronunciation modeling for Arabic.403is included or not, XWR significantly (p-values <8.1e-15) outperforms BASEWR.
Our best results areobtained when we include the top first and secondMADA option in the decoding pronunciation dictio-nary but only the top MADA choice in the trainingpronunciation dictionary.
The difference betweenthis version of XWR and an XWR version whichincludes the top second MADA choice in the train-ing dictionary is significant (p-value = 0.017).To evaluate the impact of the set of rules that gen-erate additional pronunciation variants (described inSection 4 - IV) on word recognition, we built asystem, denoted as XWR_I-III, that uses only thefirst three sets of rules (I?III) and compared its per-formance to that of both BASEWR and the corre-sponding XWR system.
As shown in Table 2, weobserve that XWR_I-III significantly outperformsBASEWR in 2.27 (p-value < 2.2e-16).
Also, thecorresponding XWR that uses all the rules (includ-ing IV set) significantly outperforms XWR_I-III in1.24 (p-value < 2.2e-16).The undiacritized vocabulary size used in our ex-periment was 34,511.
We observe that 6.38% ofthe words in the test data were out of vocabulary(OOV), which may partly explain our low absoluterecognition accuracy.
The dictionary size statistics(for entries generated from the training data only)used in these experiments are shown in Table 3.
Wehave done some error analysis to understand the rea-son behind high absolute error rate for both systems.We observe that many of the test utterances are verynoisy.
We wanted to see whether XWR still out-performs BASEWR if we remove these utterances.Removing all utterances for which BASEWR ob-tains an accuracy of less than 25%, we are left with1720/2255 utterances.
On these remaining utter-ances, the BASEWR accuracy is 64.4% and XWR?saccuracy is 67.23% ?
a significant difference de-spite the bias in favor of BASEWR.7 Conclusion and Future WorkIn this paper we have shown that the use of morelinguistically motivated pronunciation rules can im-prove phone recognition and word recognition re-sults for MSA.
We have described some of the pho-netic, phonological, and morphological features ofMSA that are rarely modeled in ASR systems andhave developed a set of pronunciation rules that en-capsulate these features.
We have demonstrated howthe use of these rules can significantly improve bothMSA phone recognition and MSA word recognitionSystem Acc Corr Del Sub InsBASEWR 52.78 65.36 360 12297 4598XWR_I?III (1 TD/DD) 55.05 66.84 324 11791 4308XWR (1 TD/DD) 56.29 69.06 274 11031 4665XWR (2 TD, 2 DD) 56.28 69.12 274 11008 4694XWR (2 TD, 1 DD) 55.53 68.55 285 11206 4759XWR (1 TD, 2 DD) 56.88 69.42 284 10891 4579Table 2: Comparing the performance of BASEWR toXWR, where the top 1 or 2 MADA options are includedin the training dictionary (TD) and decoding dictionary(DD).
XWR I?III uses only the first three sets of pro-nunciation rules in Section 4.
Accuracy = (100 - WER);Correct is Accuracy without counting insertions (%).
To-tal number of words is 36,538.Dictionary # entries PPWBASEPR TD 45,117 1BASEPR DD 44,383 1.3XPR TD (MADA top 1) 80,200 1.78XPR TD (MADA top 1 and 2) 128,663 2.85XWR DD (MADA top 1) 71,853 2.08XWR DD (MADA top 1 and 2) 105,402 3.05Table 3: Dictionary sizes generated fom the training dataonly (PPW: pronunciations per word, TD: Training pro-nunciation dictionary, DD: Decoding pronunciation dic-tionary).accuracy by a series of experiments comparing ourXPR and XWR systems to the corresponding base-line systems BASEPR and BASEWR.
We obtain animprovement in absolute accuracy in phone recogni-tion of 3.77%?7.29% and a significant improvementof 4.1% in absolute accuracy in ASR.In future work, we will address several issueswhich appear to hurt our recognition accuracy, suchas handling the words that MADA fails to analyze.We also will develop a similar approach to handlingdialectical Arabic speech using the MAGEAD mor-phological analyzer (Habash and Rambow, 2006).A larger goal is to employ the MSA and dialecticalphone recognizers to aid in spoken Arabic dialectidentification using phonotactic modeling (see (Bi-adsy et al, 2009)).AcknowledgmentsWe are very thankful to Dimitra Vergyri for providing uswith the details of the baseline pronunciation dictionaryand for her useful suggestions.
This material is basedupon work supported by the Defense Advanced ResearchProjects Agency (DARPA) under Contract No.
HR0011-06-C-0023 (approved for public release, distribution un-limited).
Any opinions, findings and conclusions or rec-ommendations expressed in this material are those ofthe authors and do not necessarily reflect the views ofDARPA.404ReferencesM.
Afify, L. Nguyen, B. Xiang, S. Abdou, andJ.
Makhoul.
2005.
Arabic broadcast news transcrip-tion using a one million word vocalized vocabulary.
InProceedings of Interspeech 2005, pages 1637?1640.S.
Ananthakrishnan, S. Narayanan, and S. Bangalore.2005.
Automatic diacritization of arabic transcriptsfor asr.
In Proceedings of ICON, Kanpur, India.F.
Biadsy, J. Hirschberg, and N. Habash.
2009.
Spo-ken Arabic Dialect Identification Using PhonotacticModeling.
In Proceedings of EACL 2009 Workshopon Computational Approaches to Semitic Languages,Athens, Greece.T.
Buckwalter.
2004.
Buckwalter Arabic Morphologi-cal Analyzer Version 2.0.
Linguistic Data Consortium,University of Pennsylvania, 2002.
LDC Cat alog No.
:LDC2004L02, ISBN 1-58563-324-0.Y.
A. El-Imam.
2004.
Phonetization of Arabic: rulesand algorithms.
In Computer Speech and Language18, pages 339?373.N.
Habash and O. Rambow.
2005.
Arabic Tokeniza-tion, Part-of-Speech Tagging and Morphological Dis-ambiguation in One Fell Swoop.
In Proceedings of the43rd Annual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 573?580.N.
Habash and O. Rambow.
2006.
MAGEAD: A Mor-phological Analyzer and Generator for the Arabic Di-alects.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 681?688, Sydney, Australia.N.
Habash and O. Rambow.
2007.
Arabic Diacritizationthrough Full Morphological Tagging.
In Proceedingsof the 8th Meeting of the North American Chapter ofthe Association for Computational Linguistics/HumanLanguage Technologies Conference (HLT-NAACL07).N.
Habash, A. Soudi, and T. Buckwalter.
2007.
OnArabic Transliteration.
In A. van den Bosch andA.
Soudi, editors, Arabic Computational Morphology:Knowledge-based and Empirical Methods.
Springer.M.
Maamouri, A. Bies, H. Jin, and T. Buckwalter.2003.
Arabic treebank: Part 1 v 2.0.
Distributed bythe Linguistic Data Consortium.
LDC Catalog No.:LDC2003T06.A.
Messaoudi, J. L. Gauvain, and L. Lamel.
2006.
Ara-bic broadcast news transcription using a one millionword vocalized vocabulary.
In Proceedings of ICASP2006, volume 1, pages 1093?1096.H.
Soltau, G. Saon, D. Povey, L. Mangu, B. Kingsbury,J.
Kuo, M. Omar, and G. Zweig.
2007.
The IBM 2006GALE Arabic ASR System.
In Proceedings of ICASP2007.S.
Stolcke.
2002.
Tokenization, Morphological Anal-ysis, and Part-of-Speech Tagging for Arabic in OneFell Swoop.
In Proceedings of ICSLP 2002, volume 2,pages 901?904.D.
Vergyri and K. Kirchhoff.
2004.
Automatic Dia-critization of Arabic for Acoustic Modeling in SpeechRecognition.
In Ali Farghaly and Karine Megerdoo-mian, editors, COLING 2004 Workshop on Computa-tional Approaches to Arabic Script-based Languages,pages 66?73, Geneva, Switzerland.D.
Vergyri, A. Mandal, W. Wang, A. Stolcke, J. Zheng,M.
Graciarena, D. Rybach, C. Gollan, R. Schluter,K.
Kirchhoff, A. Faria, and N. Morgan.
2008.
Devel-opment of the SRI/Nightingale Arabic ASR system.
InProceedings of Interspeech 2008, pages 1437?1440.S.
Young, G. Evermann, M. Gales, D. Kershaw,G.
Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev,and P. Woodland.
2006.
The HTK Book, version 3.4:htk.eng.cam.ac.uk.
Cambridge University Engineer-ing Department.I.
Zitouni, J. S. Sorensen, and R. Sarikaya.
2006.
Maxi-mum Entropy Based Restoration of Arabic Diacritics.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 577?584, Sydney, Australia.405
