Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 95?99,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsMATREX: The DCU MT System for WMT 2009Jinhua Du, Yifan He, Sergio Penkale, Andy WayCentre for Next Generation LocalisationDublin City UniversityDublin 9, Ireland{jdu,yhe,spenkale,away}@computing.dcu.ieAbstractIn this paper, we describe the machinetranslation system in the evaluation cam-paign of the Fourth Workshop on Statisti-cal Machine Translation at EACL 2009.We describe the modular design of ourmulti-engine MT system with particularfocus on the components used in this par-ticipation.We participated in the translation taskfor the following translation directions:French?English and English?French, inwhich we employed our multi-engine ar-chitecture to translate.
We also partic-ipated in the system combination taskwhich was carried out by the MBR de-coder and Confusion Network decoder.We report results on the provided devel-opment and test sets.1 IntroductionIn this paper, we present a multi-engine MTsystem developed at DCU, MATREX (MachineTranslation using Examples).
This system exploitsEBMT, SMT and system combination techniquesto build a cascaded translation framework.We participated in both the French?English andEnglish-French News tasks.
In these two tasks,we employ three individual MT system which are1) Baseline: phrase-based system (PB); 2) EBMT:Monolingually chunking both source and targetsides of the dataset using a marker-based chun-ker (Gough and Way, 2004).
3) HPB: a typicalhierarchical phrase-based system (Chiang, 2005).Meanwhile, we also use a word-level combina-tion framework (Rosti et al, 2007) to combine themultiple translation hypotheses and employ a newrescoring model to generate the final result.For the system combination task, we first usethe minimum Bayes-risk (MBR) (Kumar andByrne, 2004) decoder to select the best hypothe-sis as the alignment reference for the ConfusionNetwork (CN) (Mangu et al, 2000).
We then buildthe CN using the TER metric (Snover et al, 2006),and finally search and generate the translation.The remainder of this paper is organised as fol-lows: Section 2 details the various components ofour system, in particular the multi-engine strate-gies used for the shared task.
In Section 3, weoutline the complete system setup for the sharedtask and provide results on the development andtest sets.
Section 4 is our conclusion.2 The MATREX System2.1 System ArchitectureThe MATREX system is a combination-basedmulti-engine architecture, which exploits aspectsof both the EBMT and SMT paradigms.This architecture includes three individual sys-tems which are phrase-based, example-based andhierarchical phrase-based.The combination structure is the MBR decoderand CN decoder, which is based on the word-levelcombination strategy.In the final stage, we use a new rescoring mod-ule to process the N -best list generated by thecombination module.
See Figure 1 as a detailedillustration.2.2 Example-Based Machine TranslationEBMT obtains resources using the Marker Hy-pothesis (Green, 1979), a psycholinguistic con-straint which posits that all languages are markedfor surface syntax by a specific closed set of lex-emes or morphemes which signify context.
Givena set of closed-class words we segment each sen-tence into chunks, creating a chunk at each newoccurrence of a marker word, with the restrictionthat each segment must contain at least one non-marker word (Gough and Way, 2004).95Mutiple 1-bestMBR DecoderCN/MERTSystemCombinationHPB Baseline EBMTDev/MERTDecodingRescore/MERTRescore/MERTTestSetRecaserRescoreMutiple 1-bestMBR DecoderCN DecoderRescoreRecaserFigure 1: System FrameworkWe then align these segments using an edit-distance-style algorithm, in which the insertionand deletion probabilities depend on word-to-word translation probabilities and word-to-wordcognates (Stroppa and Way, 2006).We extracted phrases of at most 7 words oneach side.
We then merged these phrases with thephrases extracted by the baseline system addingword alignment information, and used this systemseeded with this additional information.2.3 Hierarchical Machine TranslationHPB translation system is a re-implementation ofthe hierarchical phrase translation model which isbased on PSCFG (Chiang, 2005).
We generate re-cursively PSCFG rules from the initial rules asN ?
f1 .
.
.
fm/e1 .
.
.
enwhere N is a rule which is initial or includes non-terminals.M ?
fi .
.
.
fj/eu .
.
.
evwhere 1 ?
i ?
j ?
m and 1 ?
u ?
v ?
n, atwhich point a new rule can be obtained, named,N ?
f i?11 Xkfmj+1/eu?11 Xkenv+1where k is an index for the nonterminal X .
Thenumber of nonterminals permitted in a rule is nomore than two.When extracting hierarchical rules,we set somelimitations that initial rules are of no more than7 words in length and other rules should haveno more than 5 terminals and nonterminals, andwe disallow rules with adjacent source-side andtarget-side nonterminals.The decoder is an enhanced CYK-style chartparser that maximizes the derivation probabilityand spans up to 12 source words.
A 4-gram lan-guage model generated by SRI Language Model-ing toolkit (SRILM) (Stolcke, 2002) is used in thecube-pruning process.
The search space is prunedwith a chart cell size limit of 50.2.4 System CombinationFor multiple system combination, we implementan MBR-CN framework as shown in Figure 1.
In-stead of using a single system output as the skele-ton, we employ a minimum Bayes-risk decoderto select the best single system output from themerged N -best list by minimizing the BLEU (Pa-pineni et al, 2002) loss.The confusion network is built by the output ofMBR as the backbone which determines the wordorder of the combination.
The other hypothesesare aligned against the backbone based on the TERmetric.
NULL words are allowed in the alignment.Each arc in the CN represents an alternative wordat that position in the sentence and the number ofvotes for each word is counted when constructingthe network.
The features we used are as follows:?
word posterior probability (Fiscus, 1997);?
3, 4-gram target language model;?
word length penalty;?
Null word length penalty;Also, we use MERT (Och, 2003) to tune theweights of confusion network.2.5 RescoreRescore is a very important part in post-processingwhich can select a better hypothesis from the N -best list.
We add some new global features inrescore model.
The features we used are as fol-lows:?
Direct and inverse IBM model;?
3, 4-gram target language model;?
3, 4, 5-gram POS language model (Ratna-parkhi, 1996; Schmid, 1994);96?
Sentence length posterior probability (Zensand Ney, 2006);?
N -gram posterior probabilities within the N -Best list (Zens and Ney, 2006);?
Minimum Bayes Risk probability;?
Length ratio between source and target sen-tence;The weights are optimized via MERT algorithm.3 Experimental SetupThe following section describes the system andexperimental setup for the French-English andEnglish-French translation tasks.3.1 Statistics of DataParallel CorpusWe used Europarl and Giga data for this evalua-tion.
The statistics of parallel data are shown inTable 1.Corpra Sen Token-En Token-Fr LenEuroparl 1.46M 39,240,672 42,252,067 80Giga 2M 48,648,104 57,869,002 65Table 1: Statistics of Parallel DataIn this table, Sen indicates the number of sentencepairs; Len denotes the maximum sentence lengthof each corpus.
This year the translation task isonly evaluated on News Domain.
Experimental re-sults showed that giga data is more correlated thanEuroparl and the BLEU score is significantly im-proved(See Table 4).Monolingual CorpusIn this evaluation, we trained a small 4-gram lan-guage model using data in Table 1 and a large 4-gram language model using data in Table 2.
Weconfigured these two LMs for Baseline and EBMTsystems while HPB only used the large one.Language Sen Token SourceEnglish 9,966,838 240,849,221 E/N/NCFrench 9,966,838 260,520,313 E/N/NCTable 2: Statistics of Monolingual DataIn the above table, E/N/NC refers to Eu-roparl/News/New Commentary corpus.3.2 Pre-ProcessingWe preprocessed both Europarl and Giga Release1 corpus.
For the Europarl corpus, we removedthe reserved characters in GIZA++ and tokenizedand lowercased the corpus with tools provided byWMT09.
The Giga corpus was too large for ourresource, so we performed sentence selection be-fore cleaning, in the following steps.?
We split the Giga corpus into even segments,each segment consisting of 20 lines.?
We trained an SVM classifier on English sidewith positive examples from the monolin-gual news data and negative examples fromnoisy sentences (numbers, meaningless wordcombinations, and random segments) fromthe Giga corpus.
We used ?-ly?
and ?-ing?to approximate adverbs and present partici-ples and did not use other POS-induced fea-tures, as in (Ferizis and Bailey, 2006).
Weadded these features to remove noise: aver-age length of sentences, frequency of capital-ized characters, frequency of numerical char-acters and short word penalty (equals to 1when average length of words < 4, and 0otherwise).
We used the classifier to remove20% segments of lowest scores.?
We selected 1, 600 words having the highestmutual information scores with monolingualtraining data against the Giga corpus.?
We selected 100, 000 segments where thesewords occurred most frequently.
Howeverthe sentence was dropped if the length ratiobetween English and French was larger than1.5 or less than 0.67.3.3 System ConfigurationThe two language models were done using theSRILM employing linear interpolation and modi-fied K-N discounting (Chen and Goodman, 1996).The configuration for the three systems is listedin Table 3.System P-Table Length LM FeaturesBaseline-E 55.9M 7 2 15Baseline-G 58.4M 7 2 15EBMT 59.4M 7 2 15HPB 122M 5 1 8Table 3: Statistics of MT SystemsIn this table, E indicates the Europarl corpus97which is used for all three systems, and G standsfor the Giga corpus which is only used for theBaseline system.
We can see from Table 3 thatthe size of the HPB phrase-table is more than 2times as large as the other phrase tables.
How tofilter and process such a huge hierarchical table isa challenging problem.We tuned our systems on the development setdevset2009-a and devset2009-b, and performedthe crossover experiment by these two devsets.3.4 Experimental ResultsThe system output is evaluated with respect toBLEU score.
In Table 4, we used devset2009-bto tune the various parameters in our three singlesystems and devset2009-a for testing.
In terms ofthe Europarl data, we can see that the three sys-tems we used achieved similar performance on thetest set for both translation directions, with theBaseline-E system yielding slightly better resultsthan the other two.System Fr-En En-FrBaseline-E 22.24 22.68Baseline-G 24.90 ?1EBMT 22.04 22.12HPB 21.69 21.12MBR 25.11 22.68CN 25.24 22.76Rescore 25.40 22.97Table 4: Experimental Results on Devset2009-aWe then used the translations of the devset2009-a produced by each system to tune the parame-ters of our system combination module.
From Ta-ble 4, we can see that using MBR and confusionnetwork decoding leads to a slight improvementover the strongest single system, i.e.
the baselinePhrase-Based SMT system.
Rescoring the N -bestlists yielded an increase of 0.5 (2.0 relative) ab-solute BLEU points over the baseline for French?English Translation and 0.29 (1.28 relative) abso-lute BLEU points for English?French Translation.Table 5 is the results on 2009 Test Data.
Thescores with a slash in the last two rows are low-ercased and cased respectively.
From the table we1Not much time to do the experiments on English-Frenchdirection.
EBMT and HPB just used the Europarl corpus.2The official automatic result is scored on 2525 sentencesout of the whole 3007 sentences in test set.
The other 502sentences are used as the development set for combinationevaluation task.System Fr-En En-FrBaseline-E 25.64 24.47Baseline-G 26.75 ?EBMT 25.67 24.43HPB 25.20 24.19Combination 27.20/25.14 25.26/22.28Official-Auto2 26.86/24.93 23.78/22.14Table 5: Summary of Results on 2009 Test Datacan see that combination yielded 0.45 and 0.79 ab-solute BLEU points over the best single system forFr-En and En-Fr direction respectively.
However,1.93 (7.2 relative) and 1.64 (6.58 relative) BLEUpoints are dropped between cased and lowercasedresults of both directions.
Accordingly, training aneffective recasing model is very important for ourfuture work.4 ConclusionThis paper presents our machine translation sys-tem in WMT2009 shared task campaign.
We de-veloped a multi-engine framework which com-bined the output results of the three MT sys-tems and generated a new N -best list after CNdecoding.
Then by using some global featuresthe rescoring model generated the final translationoutput.
The experimental result proved that thecombination module and rescoring module are ef-fective in our framework.We also applied simple yet effective methodsof genre and topical classification to remove noiseand out-of-domain sentences in the Giga corpus,from which we built better translation models thanfrom Europarl.In future work, we will refine our system frame-work to investigate its effect on the tasks pre-sented here, and we will develop more powerfulpost-processing tools such as recaser to reduce theBLEU loss.AcknowledgmentsThis work is supported by Science Foundation Ireland (GrantNo.
07/CE/I1142).
Thanks also to the reviewers for theirinsightful comments and suggestions.ReferencesChen, S. F. and Goodman, J.
(1996).
An Empirical Study ofSmoothing Techniques for Language Modeling.
In Pro-ceedings of the Thirty-Fourth Annual Meeting of the As-sociation for Computational Linguistics, pages 310?318,San Francisco, CA.Chiang, D. (2005).
A Hierarchical Phrase-Based Model forStatistical Machine Translation.
In Proceedings of the9843rd Annual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 263?270, Ann Arbor,MI.Ferizis, G. and Bailey, P. (2006).
Towards practical genreclassification of web documents.
In Proceedings of the15th international conference on World Wide Web (WWW?06), pages 1013?1014, New York, USA.Fiscus, J. G. (1997).
A post-processing system to yield re-duced word error rates: Recognizer output voting errorreduction (ROVER).
In Proceedings 1997 IEEE Work-shop on Automatic Speech Recognition and Understand-ing (ASRU), pages 347?352, Santa Barbara, CA.Gough, N. and Way, A.
(2004).
Robust Large-Scale EBMTwith Marker-Based Segmentation.
In Proceedings ofthe 10th International Conference on Theoretical andMethodological Issues in Machine Translation (TMI-04),pages 95?104, Baltimore, MD.Green, T. (1979).
The Necessity of Syntax Markers.
Twoexperiments with artificial languages.
Journal of VerbalLearning and Behavior, 18:481?496.Kumar, S. and Byrne, W. (2004).
Minimum Bayes-Risk De-coding for Statistical Machine Translation.
In Proceed-ings of the Joint Meeting of the Human Language Tech-nology Conference and the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL2004), pages 169?176, Boston, MA.Mangu, L., Brill, E., and Stolcke, A.
(2000).
Finding con-sensus in speech recognition: Word error minimizationand other applications of confusion networks.
ComputerSpeech and Language, 14(4):373?400.Och, F. (2003).
Minimum error rate training in statisticalmachine translation.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics(ACL), pages 160?167, Sapporo, Japan.Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002).BLEU: a Method for Automatic Evaluation of MachineTranslation.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL-02),pages 311?318, Philadelphia, PA.Ratnaparkhi, A.
(1996).
A Maximum Entropy Model forPart-Of-Speech Tagging.
In Proceedings of the Empiri-cal Methods in Natural Language Processing Conference(EMNLP), pages 133?142, Philadelphia, PA.Rosti, A.-V.
I., Xiang, B., Matsoukas, S., Schwartz, R., Ayan,N.
F., and Dorr, B. J.
(2007).
Combining outputs frommultiple machine translation systems.
In Proceedingsof the Joint Meeting of the Human Language Technol-ogy Conference and the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL2007), pages 228?235, Rochester, NY.Schmid, H. (1994).
Probabilistic Part-of-Speech Tagging Us-ing Decision Trees.
In Proceedings of International Con-ference on New Methods in Language Processing, pages44?49, Manchester, UK.Snover, M., Dorr, B., Schwartz, R., Micciula, L., andMakhoul, J.
(2006).
A study of translation edit rate withtargeted human annotation.
In Proceedings of the 7th Con-ference of the Association for Machine Translation in theAmericas (AMTA 2006), pages 223?231, Cambridge, MA.Stolcke, A.
(2002).
SRILM - An Extensible Language Mod-eling Toolkit.
In Proceedings of the International Confer-ence Spoken Language Processing, pages 901?904, Den-ver, CO.Stroppa, N. and Way, A.
(2006).
MaTrEx: the DCU machinetranslation system for IWSLT 2006.
In Proceedings of theInternational Workshop on Spoken Language Translation,pages 31?36, Kyoto, Japan.Zens, R. and Ney, H. (2006).
N-gram Posterior Probabilitiesfor Statistical Machine Translation.
In Proceedings of theJoint Meeting of the Human Language Technology Con-ference and the North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL 2006),pages 72?77, New York, USA.99
