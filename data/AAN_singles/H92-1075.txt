Co l lec t ion  and  Ana lyses  o f  WSJ -CSR Data  at M IT  1Michael Phillips, James Glass, Joseph Polifroni, and Victor ZueSpoken Language Systems GroupLaboratory  for  Computer  Sc ienceMassachuset ts  Ins t i tu te  of Techno logyCambr idge ,  Massachuset ts  02139ABSTRACTRecently, the DARPA community started a new data col-lection initiative in the Wall Street Journal (WSJ) domainto support research and development of very large vocabu-lary continuous speech recognition (CSR) systems.
Since Au-gust 1991, our group has actively participated in the develop-ment of the WSJ-CSR corpus.
The purpose of this paper isto document our involvement in this process, from recordingand transcription to analyses and distribution.
We will alsopresent the results of an experiment investigating the prepro-cessing of the prompt ext.INTRODUCTIONOne of the key ingredients that has contributed tothe steady improvement in speech recognition technol-ogy in recent years is the availability of large speech cor-pora \[1,3,7,8\].
With the help of these corpora, researchershave been able to develop recognition systems and obtainreliable estimates of system parameters.
Perhaps justas important, these corpora, together with standardizedperformance evaluation procedures and metrics, have en-couraged objective comparison of different systems, lead-ing to better understanding and cross fertilization of re-search ideas \[4\].The various speech corpora that the DARPA commu-nity has collected serve a wide range of purposes.
TheTIMIT corpus was designed with acoustic-phonetic re-search in mind.
The Resource Management corpus ad-dresses the needs for developing recognition systems withmoderate vocabulary (1,000 words) and perplexity (60,with a word-pair language model).
The VOYACER andATIS corpora contain spontaneously generated speech,and are useful for spoken language system development.All the presently available corpora have moderate vocab-ulary sizes and perplexities, and thus cannot adequatelysupport research and development of very large vocab-ulary continuous peech recognition (CSR) systems inAmerican English 2.
As a result, the DARPA community1This research was supported by DARPA under ContractN00014-89-J-1332, monitored through the Office of Naval Research.2A large corpus of spoken French has recently been collected by367recently initiated an effort towards the construction of anew corpus to meet these needs.The domain chosen by the community is the WallStreet Journal (WSJ), and the text prompts are selectedfrom the CD-ROM distributed by ACL/DCI \[5\].
Whilethe ultimate goal is to collect around 300 hours of speechfrom more than 100 speakers, it was thought that weshould collect a pilot corpus of approximately 40 hours,partly to satisfy near term needs and partly to debug thetext preprocessing and data collection processes.
SinceAugust 1991, our group is one of three that actively par-ticipated in the collection of the WSJ-CSR pilot corpus 3.The purpose of this paper is to document our involvementin this process, present some comparative analyses of theresulting data, and describe an experiment investigatingthe preparation of the prompt text.DATA COLLECTIONThe Env i ronmentAll the MIT data are collected in an office environ-ment, where the ambient noise level is approximately50dB on the A scale of a sound-level meter.
All ut-terances are collected simultaneously using two micro-phones.
A Sennheiser HMD-410 noise cancelling micro-phone is always used for one of the channels.
For theother channel, we rotate among the sessions three micro-phones: a Crown PCC-160 phase coherent cardioid desk-top microphone, a Crown PZM-6FS boundary desk-topmicrophone, and a Sony ECM-50PS electret condenserlavaliere microphone.
The data are collected using a SunSPARCstation-II, which has been augmented with anAriel DSP S-32C board and ProPort-656 audio interfaceunit for data capture.
The sampling rate is 16 kHz, andthe signal is lowpass filtered at 7.2 kHz.
The input gainis held constant, for all subjects, at a setting that maxi-mizes the signal-to-noise ratio without clipping.
Ratherthan transferring each collected sentence immediately toa remote file server for storage, and thus increasing theFrench researchers\[2\].
TheBREF corpus contains over 200 hoursof speech, collected from over 100 subjects./ 3The other two participants are SRI and Texas Instruments.
/ /2-11-0~ 6-491-0 1-1LegendMale - FemaleMisc.Canada 0-1India 0-1Korea 1-0Puerto Rico 1-0Romania 1-0unknown 0-1Figure 1: Geographical distributions of the subjects.amount of delay between sentences, we store the speechdata temporarily on a 200 MByte local disk.The prompt text, i.e., the text used to elicit speechmaterial from the subjects, has been preprocessed byDoug Paul of Lincoln Lab to remove reading ambiguitiesinherent in written text \[5\].
Approximately half of theprompt text contains verbalized punctuation, whereasthe remainder does not.
The prompt text is displayedone paragraph at a time in the hope that this will en-courage the subjects to produce sentence-level prosodicphenomena.
The sentence to be recorded is highlightedin yellow, and the highlighting automatically moved for-ward to the next sentence once the previous entence hasbeen accepted.
Four buttons (icons that can be activatedwith the mouse) are available for the subject o record,playback, accept, or unaccept an utterance.
A push-and-hold mechanism is used for recording.
We developed thisuser interface nvironment in the hope that it will enablesubjects to record the data with minimum supervision.Our experience with pilot data collection indicates thatthis is indeed the case.
In fact, this software and hard-ware environment has also been adopted by one of thetwo remaining sites collecting WSJ-CSR data.The  ProcessSubjects were recruited from the MIT community andvicinity via e-mail and posters.
They were separated intothree categories depending on how their data would beused for system development and evaluation: speaker-independent (SI), speaker-adaptive (SA), and speaker-dependent (SD).
An attempt was made to balance thespeakers by sex, dialect, and age, particularly for thelatter two groups, since the total number of speakers inthese groups is relatively small.368Data were collected in sessions of approximately 100utterances (about 40 minutes per session).
Each new sub-ject was asked to read a set of instructions introducingthem to the task.
After that, the experimenter helpedthe subjects practice using the mouse for recording.
Theentire introduction took about 5 minutes.
The subjectswere then asked to read the designated set of 40 speakeradaptation sentences provided by Dragon Systems.
Theexperimenter monitored the recording of the adaptationsentences, and asked the subject o repeat a sentence if amistake was made.
All subsequent recordings were madewithout supervision.
Approximately half of the prompttexts for each subject contained verbalized punctuations.Subjects belonging to the SA and SD categories returnedfor multiple sessions.
However, the introduction and thereading of the adaptation sentences took place only dur-ing the first session.Once the data were recorded, they were authenti-cated.
To this end, we developed an interactive nvi-ronment in which an experimenter could listen to an ut-terance, visually examine the waveform to detect runca-tion, and edit the orthographic transcription when nec-essary.
Finally, the speech data and the correspondingorthographic transcriptions were written onto CD-ROM-compatible WORM disks for distribution.The  Sta tusWe started the collection of WSJ-CSR data in earlyOctober, 1991, and completed the pilot collection by yearend.
Figure 1 shows the geographical distribution of allthe subjects that we have recorded thus far.
Their ageranges from 17 to 52 years, with an average of 27.1 yearsand a standard eviation of 6.6 years.
A breakdown ofthe amount of data collected in each of the three cate-Category TralningSet \] Development Set \] Test Set:~ sentences ~ speaker ~ sentences ~ speaker ~ sentences I~  speakerSI \]6867 (6720)SA ' 3206 (3840)SD 4879 (4880)49 (48) 747 (1600) 4 (8) 808 (1600) 4 (8)5 (6) 755 (960) 5 (6) 805 (960) 5 (6)2 (2) 295 (320) 2 (2) 324 (320) 2 (2)Table 1: Statistics on the amount of data collected, expressed in terms of the number of sentences and the number of speakers,for each category and each data sets.
The numbers in parentheses are the goals for the entire pilot effort.Measurements Adaptation Iw/o vPI~: Sentences 2240 6410# Words 29232 105533Ave.
-~ Words per Sentence 13.1 16.1Duration (s) 11404 39053Ave.
Sentence Duration (s) 5.1 6.1Ave.
:g: Words per Minute 153.8 162.1\[I :g: Words Read with Errors 28 337 1w VP \[ Tota l6302 14952120051 25481619.0 17.047579 980377.5 6.5151.4 155.9332 II 697Table 2: Statistics of various measures for the adaptation sentences and sentences with and without verbalized punctuation.gories is shown in Table 1.
While we only committedourselves at the onset to collect up to 50% of the pilotdata, in the final analysis we were able to collect nearlytwice as much data in all categories.
All the data that wecollected, totaling more than 8 GBytes, have been deliv-ered to NIST and other research institutions for systemdevelopment, training, and evaluation.DATA ANALYSESSince the WSJ-CSR speech corpus differs in manydimensions from the other corpora that we have collectedthus far in the DARPA community, we thought it wouldbe useful to compute some of its vital statistics.
In thissection, we will describe some of the analyses that wehave performed thus far.All the analyses are based on only the data from thetraining set, including the SI, SA, and SD categories ~.The results are summarized in Table 2.
In addition tocomputing various measures for the entire data set, wehave also analyzed the adaptation sentences, and thosewith and without verbalized punctuation.Table 2 indicates that the MIT training set containsnearly 15,000 sentences, and the number of sentenceswith and without verbalized punctuation are approxi-mately equal.
These sentences contain over 250,000 words,resulting in an average of approximately 17 words persentence.
The sentence length ranges from one word to31 words and has a standard eviation of 6.6 words.
The4We have excluded the development and test sets because of ourdesire to keep them uncontaminated for future system developmentand evaluation.369sentences are considerably longer than any of the datathat we have collected in other domains \[1,6,8\].
Theadaptation sentences are generally shorter than the WSJsentences.
Some speakers found them difficult to pro-nounce, and needed to be corrected repeatedly, whereasothers uttered them with no apparent difficulty.
On aver-age, verbalizing the punctuations adds an extra 2.5 wordsto each sentence.To compute the duration of these sentences we firstpassed each sentence through an automatic begin-and-end detector to remove any extraneous ilences.
Alto-gether, the MIT training set contains almost 100,000 sec-onds of speech material, or about 27 hours.
The averageduration of the sentences i 6.5 seconds.
The correspond-ing speaking rate is 156 words per minute, which is 30%higher than that for the spontaneous speech that we havecollected \[6\].
This discrepancy is presumably due to theinherent difference in the way speech is elicited.In collecting the WSJ-CSR data, we hoped to pro-vide an interface that was easy for the subjects to use, sothat costly on-line monitoring was not necessary.
How-ever, this potential cost reduction may be offset by thecost of authentication if the subjects produce too manyerrors.
The sentences containing errors have the addeddisadvantage of not being well matched to the languagemodel, which is constructed from the prompt text.To gain some insight into the magnitude of this prob-lem, we tabulated the discrepancies between the final or-thographic transcription and the corresponding prompttext.
The result, summarized in the last row of Table 2,show that 697, or 0.27% of the words were read witherror (including substitutions, insertions, and deletions).
/ONED.ANPromptN.A.S.A.MSN.J.VOLATILITYMESSRSW.W.I.I.
WORLD WAR TWOSPOKESWOMANTHEIRR.I.
RHODE ISLANDSAYSTELEPHONECONCLUSIONS CONCLUSIONTOWASSAIDFUTURESMPHETPERCENTAGEN.Y.SIDSSAIDNONETHELESS NEVERTHELESSBECOMEMSCHARGESSpoken Number \[NASA 6THE 4A 3DEMOCRAT 3A 3MISS 3NEW JERSEY 3VALIDITY 3MISTERS 33SPOKESMAN 3THE 22SAID 2PHONE 22INTO 2IS 2SAY 2FUTURE 2MPH 2EASTERN 2TIME 2PERCENT 2NEW YORK 2S IDS  2SAYS 22BECAME 2MR 2CHANGES 2Table 3: Examples of most common reading confusions.Note that, while the number of words read with errorsfor the adaptation sentences were one-tenth of that forthe WSJ sentences, the percentage of errors for the adap-tation sentences is only about one-third of that for theWSJ sentences.
Recall that the adaptation sentenceswere read with an experimenter monitoring the processand instructing the subject to repeat when an error isdetected.
Thus, while monitoring the data collectionprocess can reduce the errors by a factor of three, themagnitude of the problem is relatively small.
Therefore,we believe our original hypothesis was reasonable.Example confusions can be seen in Table 3 which listsall substitutions (computed by finding the best alignmentbetween the prompt and spoken word strings) which oc-curred two or more times in the training portion of thecorpus.
Note that many of these are due to the speakerexpanding abbreviations ("R.
I."
becomes "Rhode Is-land" for example).
Since this would not occur in theverbalized punctuation text (the prompt would be "R.period I .period"), it is likely that these expanded ab-breviations accounted for the slightly higher error rate inthe non-verbalized punctuation portions.In the final analysis, the entire MIT training set, con-taining 27 hours of usable speech, was collected in ap-proximately 125 40-minute sessions (approximately 30minutes of speaking with 10 minutes of setup and in-370struction).
Thus three hours of subject time is requiredto collect one hour of speech.
Adding the overhead ofrecruiting and scheduling subjects, authentication, andother related administrative matters, we estimate that6-8 hours of time is needed for one hour of speech.EXPERIMENT ON TEXTPREPROCESSINGAs mentioned earlier, the WSJ-CSR pilot effort is in-tended to satisfy our near term research needs, so thatresearchers can begin to develop very large vocabularyspeech recognition algorithms and systems.
The pilot ef-fort also affords us the opportunity to experiment withprompt ext preprocessing and data collection procedures,so that we can refine the procedure for the final, and con-siderably larger data collection initiative.
In this section,we describe an experiment that we have conducted con-cerning the preprocessing of the text prompts.The prompt ext used for the pilot collection has beenpreprocessed by Lincoln Lab \[5\].
The rationale for thispreprocessing step is at least two-fold.
First, by con-verting numbers and abbreviations to a standard format,one removes any ambiguity concerning how these itemsshould be read.
Second, forcing the subjects to read thetext in some pre-determined format will result in speechdata that is consistent with the language model, which isderived from a considerably larger quantity of text data.However, some researchers felt that this preprocessingstep may unnecessarily restrict the ways these items canbe pronounced.
Thus the data that we collect may notaccurately reflect realistic situations in which a user isasked to dictate.In order to gain some understanding of the effect ofthis preprocessing step, we recently conducted a smallexperiment.
We first selected 100 sentences in the train-ing set that contain one or more items that are candi-dates for preprocessing.
Examples of some of the se-lected sentences are shown in Table 4.
These sentencesare presented to the subjects, unprocessed, for recording.Following the recording, each utterance is carefully tran-scribed orthographically, and the resulting transcriptionis then compared with the processed prompt text usedduring the pilot data collection to determine if there ex-ist any discrepancies.
For this experiment, we recruited12 subjects, 6 male and 6 female.
Three male and threeBack then the distribution was $2.10 annually.For the 1987 first 9 months, it had a $2.4 M net loss.A W-4 form can be revived whenever necessary.Table 4: Example of sentences used for the text preprocess-ing experiment.m uoMi25, 70G |\[\] P rompt  Text6oo.~ \[\] Numbers20'?
Abbreviat ions$ soo..~ ?
Dates,05 ~ 2o0~o .
.
- .
.
~i.
.
.
.
.
.Number of  D ist inct  Rend i t ions /SentenceFigure 2: A histogram of the number of distinct renditionsproduced by the 12 subjects for the 100 sentences.female subjects had served previously as subjects for thepilot collection effort.
Thus 12 readings were obtainedfor each of the 100 sentences.0'Figure 3: A histogram of nine most common causes for dis-crepancy with the processed prompt ext prompt ext.The results of the experiment can be analyzed in sev-eral ways.
Figure 2 shows a histogram of the numberof distinct renditions produced by the 12 subjects forthe 100 sentences.
There is considerable variation in theproduction of these sentences.
The average number ofdistinct renditions is 3.9, with a standard eviation of2.4.
The figure shows that only 12 of the 100 sentencesresulted in readings that agreed unanimously with theprocessed prompt text.
Approximately half of the sen-tence tokens (601 out of  1,200) are identical to the cor-responding prompt ext.
5Figure 2 shows that, for almost 90% of the sentencesused in this experiment, he subjects produced at leastone rendition that differed in some way from the pro-cessed prompt text.
But is this prompt text the pre-ferred way of producing the sentences by our subjects?To answer this question, we computed the rank of theprocessed prompt text for each sentence which showedthat the processed prompt ext corresponds to (or is atleast tied with) the most frequently produced renditionin over 60% of our sentences.
Over 90% of the time, it iswithin the top three.A closer examination ofthe 100 sentences showed thatthere were 171 locations where there was a discrepancy5Although the data set size is small, we observed only smalldifferences due to prior experience with the WSJ data collection.Experienced subjects agreed with the processed prompt text 315times, whereas new subjects agreed only 286 times.371between the processed prompt text and at least one ofthe 12 recorded orthographies.
49 of these seemed to bereading errors and consisted of a single word deletion,insertion, or substitution, and were typically producedby only one of the 12 speakers.
An additional 14 dis-crepancies were due the addition of verbalized sentencepunctuation (the subjects were not asked to verbalizepunctuation).Figure 3 shows a breakdown of the orthographies a -sociated with the remaining 108 discrepancies (which cor-responds to 1296 substrings).
635 or 49% of these stringscorresponded to the processed prompt ext.
Our analy-sis divided the majority of the remainder into three cat-egories: numbers, abbreviations, and dates.Numbers were involved in 81 of the 108 discrepan-cies and, as shown in Figure 3, were mainly due to fivefactors.
The most frequently occurring variation (169 in-stances) was where the word %nd" was inserted into astring in order to break up a large number sequence (e.g.
"two hundred and thirty four" instead of "two hundredthirty four").
The second most common source of vari-ation (122 instances) involved monetary denominations.In these cases the word "dollar" was often deleted.
Thethird factor involved variations in the way decimal num-bers were spoken (108 instances).
These changes typi-cally involved changing a digit sequence to tens or teens(e.g.
"two point thirty four" instead of C'two point threefour"), or substituting the word "zero" for the word "oh"(e.g.
"one point zero two" instead of "one point oh two").The remaining two most common factors involved 60 in-stances where the word "zero" was deleted (or replacedby the word "oh") from a purely decimal number (e.g.
"point three percent" instead of "zero point three per-cent"), and 33 instances where the word "one" was re-placed by "a" in a number or fraction beginning with aone (e.g.
"one and a half" instead of "one and one half").Abbreviations accounted for 20 additional discrepan-cies.
As shown in Figure 3, eleven of these discrepanciesinvolved 40 instances where subjects aid the contractedform of an abbreviation (e.g.
"Corp" or "In,") insteadof the expanded form used in the processed prompt text.Conversely, there were five substrings where nearly halfthe subjects (a total of 29 out of 60 instances) did ex-pand a string which was not expanded in the processedprompt text (e.g.
"E.S.T" spoken as "Eastern StandardTime").
The third factor which accounted for variationsin the way abbreviations were pronounced was the word"slash" as in "P.S.
slash two".
Subjects had a definitepreference for deleting the slash in this context, althoughtwo returning subjects did remember to say the slash in3 instances out of 24.The remaining seven discrepancies involved dates andwere nearly all due to the day being spoken as a cardi-nal number (e.g.
"ten") rather than the ordinal number(e.g.
"tenth") provided by the prompt text.
The cardinalnumber was used 18 times in our data.
The single excep-tion to this was one instance where a subject said "theseventh" instead of "seventh".Taken together, these nine factors were involved in104 of the 108 discrepancies, and accounted for all but44 of the 1296 substrings uttered by the subjects (96.6%).These remaining differences nearly all involved numbers,and could be analyzed further of course (for instance,three of the remaining discrepancies involved report num-bers, where the number was often spoken as a sequenceof single digits).
However, the results of our investigationindicated to us that although there is a large variationin the way the subjects have spoken these unprocessedsentences, the types of variation is fairly limited.
In ad-dition, the magnitude of the these variations would besmaller in the overall corpus since we only presented un-processed sentences that seemed to have ambiguous re-alizations.
Nevertheless, we are still faced with the ques-tion of whether or not to preprocess the data.
Before wecan answer this question definitively, it is important thatwe conduct further study on a larger sample of sentencesusing a larger number of subjects.
In the end, the de-cision of whether to preprocess the text will have to bedetermined by the community who will be the consumersof the resulting data, after considering the objectives ofthe research program and the trade-offs between a morereliable language model and more realistic speech data.372SUMMARYThis paper describes our involvement in the collec-tion of the WSJ-CSR pilot corpus.
By paying close at-tention to developing a computer interface that is easyto use, we were able to collect over 33 hours of speechfrom 64 subjects over a relatively short period.
By us-ing in-house quipment to produce CD-ROM-compatibleWORM disks, we were able to distribute the data tointerested researchers apidly.
Our analyzes of the col-lected data show that the WSJ-CSR corpus differs signifi-cantly from other corpora in the research community.
Weexpect that it will have long-lasting impacts on speechrecognition research within the DARPA community andaround the world.The preliminary text preprocessing experiment thatwe conducted suggests that the current preprocessingscheme may not be adequate in capturing the ways peo-ple would naturally speak the sentences.
Clearly, moreextensive xperiments must be performed.
Whether oneshould preprocess the text at all is a decision that thecommunity must decide collectively.ACKNOWLEDGEMENTSThe collection of the WSJ-CSR data received helpfrom many members of the Spoken Language SystemsGroup at the MIT Laboratory for Computer Science.
Inparticular, Christie Clark Winterton was responsible forrecruiting, scheduling, and assisting the subjects.
Shealso authenticated a large fraction of the orthographictranscriptions of the collected ata.REFERENCES\[1\] Lamel, L. F., R. H.Kassel, and S. Seneff, "SpeechDatabase Development: Design and Analysis of theAcoustic-Phonetic Corpus," Pro,.
DARPA Speech Recog-nition Workshop: 100-109, February, 1986.\[2\] Lamel, L.F., Gauvain, 5.L., and Eskenazi, M., "BREF,a Large Vocabulary Spoken Corpus for French," Pro,.Eurospeech-91: 505-508, September, 1991\[3\] MADCOW, "Multi-Site Data Collection for a SpokenLanguage Corpus," These Proceedings.\[4\] Pallett, D., "Benchmark Tests for DARPA ResourceManagement Database Performance Evaluations," Proc.1CASSP-89: 536-539, May, 1989.\[5\] Paul, D. and Baker, J., "The Design for the Wall StreetJournal-Based CSR Corpus," These Proceedings.\[6\] Polifroni, J. Seneff, S., and Zue, V., "Collection of Spon-taneous Speech for the ATIS Domain and Compara-tive Analyses of Data Collected at MIT and TI," Proc.DARPA Speech and Natural Language Workshop: 360-365, February 1991.\[7\] Price, P., Fisher, W., Bernstein, J., Pallett, D., ``TheDARPA 1000-Word Resource Management Database,"Proc.
ICASSP-88: 651-654, April, 1988.\[8\] Zue, V., Daly, N., Glass, J., Goodine, D., Leung, H.,Phillips, M., Polifroni, J., Seneff, S. and Soclof, M. "TheCollection and Preliminary Analysis of a SpontaneousSpeech Database," Proc.
DARPA Speech and NaturalLanguage Workshop: 126-134, October 1989.
