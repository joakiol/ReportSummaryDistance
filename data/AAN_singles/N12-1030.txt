2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 295?304,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsThe Challenges of Parsing Chinese with Combinatory Categorial GrammarDaniel Tse and James R. CurranSchool of Information TechnologiesUniversity of SydneyAustralia{dtse6695,james}@it.usyd.edu.auAbstractWe apply Combinatory Categorial Grammarto wide-coverage parsing in Chinese with thenew Chinese CCGbank, bringing a formalismcapable of transparently recovering non-localdependencies to a language in which they areparticularly frequent.We  train  two  state-of-the-art  English ??
?parsers: the parser of Petrov and Klein (P&K),and the Clark and Curran (C&C) parser, uncov-ering a surprising performance gap betweenthem not observed in English ?
72.73 (P&K)and 67.09 (C&C) F -score on ????
6.We explore the challenges of  Chinese ??
?parsing through three novel  ideas: develop-ing corpus variants rather than treating the cor-pus as fixed; controlling noun/verb and other???
ambiguities; and quantifying the impactof constructions like pro-drop.1 IntroductionAutomatic corpus conversions from the Penn Tree-bank (Marcus et al, 1994) have driven research inlexicalised grammar formalisms, such as ????
(Xia,1999), ????
(Miyao et al, 2004) and ???
(Hock-enmaier and Steedman, 2007), producing the lexicalresources key to wide-coverage statistical parsing.The Chinese Penn Treebank (????
;  Xue et al,2005) has filled a comparable niche, enabling thedevelopment of a Chinese ????
(Xia et al, 2000),a wide-coverage ????
parser (Yu et al, 2011), andrecently Chinese CCGbank (Tse and Curran, 2010),a 750 000-word corpus of Combinatory CategorialGrammar (???
; Steedman, 2000) derivations.We train two ???
parsers, Clark and Curran (C&C;2007), and the Petrov and Klein (P&K; 2007) ???
?parser, on Chinese CCGbank.
We follow Fowler andPenn (2010), who treat the English CCGbank (Hock-enmaier and Steedman, 2007) grammar as a ???
andtrain and evaluate the P&K parser directly on it.We obtain the first Chinese ???
parsing results:F -scores  of  72.73 (P&K) and 67.09 (C&C)  on la-belled dependencies computed over the ????
6 testset.
While the state-of-the-art in Chinese syntacticparsing has always lagged behind English, this largegap is surprising, given that Fowler and Penn (2010)found only a small margin separated the two parserson English CCGbank (86.0 versus 85.8).Levy and Manning (2003) established that prop-erties of Chinese such as noun/verb ambiguity con-tribute to the difficulty of Chinese parsing.
We focuson two factors within our control: annotation deci-sions and parser architecture.Existing research has varied parsers whilst keep-ing the corpus fixed.
We vary the corpus whilst keep-ing the parsers fixed by exploring multiple designchoices for particular constructions.
By exploitingthe fully automatic CCGbank extraction process, wecan immediately implement these choices and assesstheir impact on parsing performance.Secondly, we contrast the performance of C&C,with its tagging/parsing pipeline, with P&K, a parserwhich performs joint tagging and parsing, and estab-lish that P&K is less sensitive to the greater lexicalcategory ambiguity in Chinese CCGbank.We demonstrate that Chinese ???
parsing is verydifficult, and propose novel techniques for identify-ing where the challenges lie.295?
???
?
trap ?
??
??
princess ?
I ???
rescued(S[dcl]\NP)/((S[dcl]\NP)/NP) (S[dcl]\NP)/NP (N/N)\(S[dcl]/NP) N NP (S[dcl]\NP)/NP?
?TS[dcl]\NP S/(S\NP)?
?BN/N S[dcl]/NP?N ??
NPTS/(S/NP)?S[dcl]Figure 1: 3 types of non-local dependencies in 6 words: ?
(As for) the trapped princess, I rescued (her).
?2 BackgroundBikel and Chiang (2000) developed the first ???
?parser, demonstrating  that  Chinese  was  similarenough to English for techniques such as a Collins-style head-driven parser or ???
to succeed.
Later????
parsers  used  Tree  Insertion  Grammar  (Chi-ang  and  Bikel,  2002), ???
?s  (Levy  and  Man-ning, 2003), the Collins models (Bikel, 2004) andtransition-based discriminative models (Wang et al,2006; Zhang and Clark, 2009; Huang et al, 2009).These systems also established the relative difficultyof parsing Chinese and English; while ???????
?scores over 92% are possible for English (McCloskyet al, 2006), systems for Chinese have achieved only87% (Zhang and Clark, 2009) on the same metric.Non-local dependencies (??
?s) are lexical depen-dencies which hold over unbounded distances.
Guoet al (2007) observed that despite the importance of??
?s for correct semantic interpretation, and the factthat Chinese syntax generates more ??
?s than En-glish, few parsers in Chinese are equipped to recoverthe traces which mark ???s.
For instance, extrac-tion, a common ???
type, occurs more frequently in????
sentences (38%) compared to ???
(17%).A more satisfying approach is to use a grammarformalism, such as ???
(Steedman, 2000), whichgenerates them inherently, enabling a unified parsingmodel over local and non-local dependencies.
Thisapproach is taken in the C&C parser (Clark and Cur-ran, 2007), which can directly and transparently re-cover ??
?s in English (Rimell et al, 2009).Chinese CCGbank (Tse and Curran, 2010) demon-strates that a parsimonious account of Chinese syn-tax with ???
is  possible.
Many familiar  objectsof Chinese syntax which generate ??
?s, includingthe ?
ba/?
bei constructions, topicalisation andextraction receive natural ???
analyses in Chinese(a) Derivational...S/S..NP..NP/N...N(b) Lexical...S/S..(S/S)/N...NFigure 2: Two types of ambiguityCCGbank.
Figure 1 shows the CCGbank analysisof passivisation, topicalisation and extraction, creat-ing ??
?s between??
princess and each of?
???,?
trap and??
rescue respectively.We take two state-of-the-art parsers and train themto establish the difficulty of parsing Chinese with???.
The first is the Clark and Curran (C&C; 2007)parser, which uses supertagging (Clark and Curran,2004), a local, linear-time tagging technique whichdrastically  prunes  the  space  of  lexical  categorieswhich the polynomial-time parsing algorithm laterconsiders.
The second is the coarse-to-fine parserof Petrov and Klein (2007) which iteratively refinesits grammar by splitting production rules to uncoverlatent distinctions.
Fowler and Penn (2010) demon-strate that the English CCGbank grammar is stronglycontext-free, allowing them to treat it as a ???
andtrain the Petrov and Klein (2007) parser directly.2.1 Derivational vs. lexical ambiguityThe designer of a CCGbank must frequently choosebetween derivational and lexical ambiguity (Hock-enmaier, 2003; Tse and Curran, 2010).
Derivationalambiguity analyses special constructions through ar-bitrary label-rewriting phrase structure rules, whilelexical ambiguity assigns additional categories to lex-ical items for when they participate in special con-structions.296Derivational and lexical ambiguity often arise in???
because  of  the form-function  distinction ?when the syntactic form of a constituent does not co-incide with its semantic function (Honnibal, 2010).For instance, in English, topicalisation causes an NPto appear in clause-initial position, fulfilling the func-tion of a sentential pre-modifier while maintainingthe form of an NP.
Figure 2 shows two distinct ??
?analyses which yield the same dependency edges.Derivational ambiguity increases the parser searchspace, while lexical ambiguity enlarges the tag set,and hence the complexity of the supertagging task.3 Three versions of Chinese CCGbankWe extract three versions of Chinese CCGbank to ex-plore the trade-off between lexical and derivationalambiguity, training both parsers on each corpus todetermine the impact of the annotation changes.
Ourhypothesis is that the scarcity of training data in Chi-nese means that derivational ambiguity results in bet-ter coverage and accuracy, at the cost of increasingtime and space requirements of the resulting parser.3.1 The lexical category LC (localiser)In the following sentences, the words in bold have of-ten been analysed as belonging to a lexical categorylocaliser (Chao, 1968; Li and Thompson, 1989).
(1) a.
??house??inside:?
?the inside of the house/inside the houseb.
?big?tree??beside:??
(the area) beside the big treeLocalisers, like English prepositions, identify a (tem-poral, spatial, etc.)
extent of their complement.
How-ever, the combination Noun + Localiser is ambigu-ous between noun function (the inside of the house)and modifier function (inside the house).We consider two possibilities to represent localis-ers in ??
?, which trade derivational for lexical am-biguity.
In (2-a), a direct ???
transfer of the ???
?analysis, the preposition?
at expects arguments oftype LCP.
In (2-b),?
at now expects only NP argu-ments, and the unary promotion LCP ?
NP allowsLCP-form constituents to function as NPs.
(2) a. ?
at ??
room ?
in:?
?PP/LCP NP LCP\NP?LCP?PPb.
?
at ??
room ?
in:?
?PP/NP NP LCP\NP?LCP ?
NP?PPThe analysis in (2-a) exhibits greater lexical ambigu-ity, with the lexical item?
at carrying at least twocategories, PP/NP and PP/LCP, while (2-b) tradesoff derivational for lexical ambiguity: the unary pro-motion LCP ?
NP becomes necessary, but ?
atno longer needs the category PP/LCP.The base release of Chinese CCGbank, corpus A,like (2-a), makes the distinction between categoriesLCP and NP.
However, in corpus B, we test the im-pact of applying (2-b), in which the unary promotionLCP ?
NP is available.3.2 The bare/non-bare NP distinctionThe most frequent unary rule in English CCGbank,occurring in over 91% of sentences, is the promotionfrom bare to non-bare nouns: N ?
NP.
Hocken-maier (2003) explains that the rule accounts for theform-function distinction in determiner-less Englishnouns  which nevertheless  have  definite  reference,while preventing certain over-generations (e.g.
?thethe car).
The N-NP distinction also separates adjec-tives and noun modifiers (category N/N), from pre-determiners (category NP/NP)  (Hockenmaier  andSteedman, 2005), a distinction also made in Chinese.While Chinese has strategies to mark definite or in-definite reference, they are not obligatory, and a barenoun is referentially ambiguous, calling into ques-tion whether the distinction is justified in ???
:(3) a.
?dog?very?
?cleverDogs are clever.b.
?????
?see?dogI saw a dog/dogs.c.
?dog??run-away???
?The dog/dogs ran away.297The fact that the Chinese determiner is not necessar-ily a maximal projection of the noun ?
in other words,the determiner does not ?close off?
a level of NP ?also argues against importing the English analysis.In contrast, the English CCGbank determiner cate-gory NP/N reflects the fact that determiners ?closeoff?
NP?
further modification by noun modifiers isblocked after combining with a determiner.
(4) ??
?Republican Party?this?
?actthis action by the Republican PartyTo test its impact on Chinese parsing, we create aversion of Chinese CCGbank (corpus C) which neu-tralises the distinction.
This eliminates the atomiccategory N, as well as the promotion rule N ?
NP.4 ExperimentsWhile a standard split of ????
5 exists, as definedby Zhang and Clark (2008), we are not aware of aconsistently used split for ????
6.
We present anew split in Table 1 which adds data from the ??
?broadcast section of ????
6, maintaining the sametrain/dev/test set proportions as the ????
5 split.We train C&C using the hybrid model, the best-performing model for English, which extracts fea-tures from the dependency structure (Clark and Cur-ran, 2007).
We use ?
= ?0.055, 0.01, 0.05, 0.1?
dur-ing training with a Gaussian smoothing parameter?
= 2.4 (optimised on the corpus A dev set).
Weuse ?
= ?0.15, 0.075, 0.03, 0.01, 0.005, 0.001?
dur-ing parsing, with the maximum number of supercats(chart entries) set to 5,000,000, reflecting the greatersupertagging ambiguity of Chinese parsing.The P&K parser is used ?off-the-shelf?
and trainedwith its default parameters, only varying the numberof split-merge iterations and enabling the Chinese-specific lexicon features.
The P&K parser involvesno explicit ???
tagging step, as the (super)tags cor-respond directly to non-terminals in a ??
?.Fowler  and  Penn  (2010)  use  the C&C toolgenerate to convert P&K output to the C&C evalu-ation dependency format.
generate critically doesnot depend on the C&C parsing model, permitting afair comparison of the parsers?
output.????
5 +????
6 #sentsTrain 1?815, 1001?1136 2000?2980 22033Test 816?885, 1137?1147 3030?3145 2758Dev 900?931, 1148?1151 2981?3029 1101Table 1: ????
5 and 6 dev/train/test splits4.1 EvaluationCarroll  et al  (1998) argued against ????????
infavour of a dependency-based evaluation.
Rimellet al  (2009)  focus  on  evaluating ???
recovery,proposing a dependency-based evaluation and a ?
?mapping procedure for inter-parser comparison.Since the P&K parser plus generate produce de-pendencies in the same format as C&C, we can usethe standard Clark and Curran (2007) dependency-based evaluation from the ???
literature: labelledF -score (LF ) over dependency tuples, as used for ??
?parser evaluation in English.
Critically, this metricis also ???-sensitive.
We also report labelled sen-tence accuracy (Lsa), the proportion of sentences forwhich the parser returned all and only the gold stan-dard dependencies.
Supertagger accuracy comparesleaf categories against the gold standard (stag).For C&C, we report on two configurations: ???
?,evaluated using gold standard ???
tags; and ???
?,with automatic ???
tags provided by the C&C tagger(Curran and Clark, 2003).
For P&K, we vary the num-ber of split-merge iterations from one to six (follow-ing Fowler and Penn (2010), the k-iterations modelis called I-k).
Because the P&K parser does not use???
tags, the most appropriate comparison is againstthe ????
configuration of C&C.
For C&C, we use theaverage of the logarithm of the chart size (logC) asa measure of ambiguity, that is, the number of alter-native analyses the parser must choose between.Following Fowler and Penn (2010), we performtwo sets of experiments: one evaluated over all sen-tences in a section, and another evaluated only oversentences for which both parsers successfully parseand generate dependencies.We define the size of a ???
grammar as the num-ber of categories it contains.
The size of a grammaraffects the difficulty of the supertagging task (as thesize of a grammar is the size of the supertag set).
Wealso consider the number of categories of each shape,as defined in Table 2.
Decomposing the category in-298Shape PatternV (predicate-like) (S[dcl]\NP)$M (modifier) X|XP (preposition-like) (X|X)|YN (noun-like) N or NPO (all others)Table 2: Shapes of categoriesmodel LF Lsa % stag cov logCAI-3 68.97 13.45 83.64 95.7 -I-6 71.67 15.70 85.00 96.4 -????
75.45 16.70 89.43 99.4 14.55????
66.32 12.81 83.88 98.6 14.69BI-3 69.75 14.15 84.07 96.0 -I-5 71.40 14.83 84.97 96.4 -????
75.41 16.67 89.50 99.6 14.74????
66.24 12.61 83.95 98.7 14.75CI-3 70.22 16.49 84.37 96.5 -I-5 72.74 18.59 85.61 96.5 -????
76.73 20.56 89.66 99.5 13.58????
66.95 14.62 83.90 99.2 13.86Table 3: Dev set evaluation for P&K and C&Cventory into shapes demonstrates how changes to thecorpus annotation affect the distribution of types ofcategory.
Finally, we calculate the average numberof tags per lexical item (Avg.
Tags/Word), as a metricof the degree of lexical ambiguity in each corpus.5 ResultsTable 3 shows the performance of P&K and C&C onthe three dev sets, and Table 4 only over sentencesparsed by both parsers.
(A is the base release, Bincludes the unary rule LCP ?
NP, and C alsocollapses the N-NP distinction.)
For P&K on corpusA, F -score and supertagger accuracy increase mono-tonically as further split-merge iterations refine themodel.
P&K on B and C overfits at 6 iterations, con-sistent with Fowler and Penn?s findings for English.The ?9% drop in F -score between the ????
and????
figures shows that C&C is highly sensitive to???
tagging accuracy (92.56% on the dev set, com-pared to 96.82% on English).
Considering Table 4,each best P&K model outperforms the corresponding????
model by 3-5%.
However, while P&K is sub-stantially better without gold-standard information,gold ???
tags allow C&C to outperform P&K, againmodel LF Lsa % stag covAI-6 71.74 15.87 85.29 100.0????
67.50 15.36 84.52 100.0BI-5 71.40 14.97 85.26 100.0????
67.72 14.97 84.68 100.0CI-5 72.84 18.69 86.04 100.0????
68.43 16.17 84.57 100.0Table 4: Dev set evaluation for P&K and C&C on????
6 sentences parsed by both parsersmodel LF Lsa % stag cov logCCI-5 72.73 20.28 85.43 97.1 -????
76.89 22.90 89.63 99.1 14.53????
67.09 15.28 83.95 98.7 14.89Table 5: Test set evaluation for P&K and C&Cdemonstrating the impact of incorrect ???
tags.Supertagging  and  parsing  accuracy  are  not  en-tirely correlated between the parsers ?
in corpora Aand B, ????
supertagging is comparable or betterthan I-3, but F -score is substantially worse.Comparing A and B in  Table 3, C&C receivessmall increases in supertagger accuracy and cover-age, but  parsing performance remains  largely  un-changed; P&K performance degrades slightly.
Onboth parsers,C yields the best results out of the threecorpora, with LF gains of 1.07 (P&K), 1.28 (????
)and 0.63 (????)
over the base Chinese CCGbank.We select C for our remaining parser experiments.Both C&C?s ????
and ????
results show highercoverage than P&K (a combination of parse failuresin P&K itself, and in generate).
Since F -score isonly computed over successful parses, it is possiblethat P&K is avoiding harder sentences.
In Table 4,evaluated only over sentences parsed by both parsersshows that as expected, C&C gains more (1.15%)than P&K on the common sentences.Table 5 shows that the behaviour of both parserson the test section is consistent with the dev section.corpusAvg.
Grammar sizetags/word all f ?
10A 1.84 1177 324B 1.83 1084 303C 1.79 964 274Table 6: Corpus statistics299corpus V P M N O TotalA 791 158 56 2 170 1177B 712 149 55 2 166 1084C 670 119 41 1 133 964Table 7: Grammar size, categorised by shape5.1 Corpus ambiguityTo understand why corpus C is superior for parsing,we compare the ambiguity and sparsity characteris-tics of the three corpora.
Examining logC, the aver-age log-chart size (Table 3) shows that the corpus Bchanges (the addition of the unary rule LCP ?
NP)increase ambiguity, while the additional corpus Cchanges (eliminating the N-NP distinction, resultingin the removal of the unary rule N ?
NP) have thenet effect of reducing ambiguity.Table 6 shows that the changes reduce the sizeof the lexicon, thus reducing the average number oftags each word can potentially receive, and thereforethe difficulty of the supertagging task.
This, in part,contributes to the reduced logC values in Table 3.While the size of the lexicon is reduced in B, the cor-responding logC figure in Table 3 increases slightly,because of the additional unary rule.Table 7 breaks  down the  size  of  each  lexiconaccording to category shape.
Introducing the ruleLCP ?
NP reduces the number of V-shaped cat-egories by 10%, while not substantially affecting thequantity of other category shapes, because the sub-categorisation frames which previously referred toLCP are no longer necessary.
Eliminating the N-NPdistinction, however, reduces the number of P andM-shaped categories by over 20%, as the distinctionis no longer made between attachment at N and NP.6 Error analysisThe  well-known noun/verb  ambiguity  in  Chinese(where, e.g., ????
?design-build?
is both a ver-bal compound ?design and build?
and a noun com-pound ?design  and  construction?)
greatly  affectsparsing accuracy (Levy and Manning, 2003).However, little work has quantified the impact ofnoun/verb ambiguity on parsing, and for that mat-ter, the  impact  of  other  frequent  confusion types.To quantify C&C?s sensitivity to ???
tagging errors,Confusion LF ?LF stag covBase (????)
76.73 89.66 99.50NR ??
NN 76.72 -0.01 89.64 99.37JJ ??
NN 76.60 -0.12 89.57 99.37DEC ??
DEG 75.10 -1.50 89.07 98.83VV ??
NN 73.35 -1.75 87.68 98.74All (????)
66.95 83.90 99.20Table 8: Corrupting C&C gold ???
tags piecemeal on????
6 dev set of corpus C. ?LF is the change inLF when each additional confusion type is allowed.which we saw in Table 3, we perform an experimentwhere we corrupt the gold ???
tags, by gradually re-introducing automatic ???
errors on a cumulative ba-sis, one confusion type at a time.The notation X ??
Y indicates that the ???
tags Xand Y are frequently confused with each other by the???
tagger.
For example, VV ??
NN represents theproblematic noun/verb ambiguity, allowing the in-clusion of noun/verb confusion errors.Table 8 shows  that  while  the  confusion  typesNR ??
NN and JJ ??
NN have no impact on the evalua-tion, the confusions DEC ??
DEG and VV ??
NN, intro-duced one at a time, cause reductions in F -score of1.50 and 1.75% respectively.
This is expected; Chi-nese CCGbank does not distinguish between nounmodifiers  (NN) and  adjectives  (JJ).
On  the  otherhand, the critical noun/verb ambiguity, and the con-fusion between DEC/DEG (two senses of the particle?
de) adversely impact F -score.
We performed anexperiment with C&C to merge DEC and DEG into asingle tag, but found that this increased category am-biguity without improving accuracy.The VV ??
NN confusion  is  particularly  damag-ing to the ???
labelled dependency evaluation, be-cause verbs generate a large number of dependencies.While Fowler and Penn (2010) report a gap of 6.31%between C&C?s labelled and unlabelled F -score onthe development set in English, we observe a gap of10.35% for Chinese.Table 10 breaks down the 8,414 false positivesgenerated  by C&C on  the  dev  set, according  towhether the head of each dependency was incorrectly??
?-tagged and/or supertagged.
The top-left  cellshows that despite the correct ???
and supertag, C&Cmakes a large number of pure attachment location er-rors.
The vast majority of false positives, though, are300C&C ????
P&K I-5category ????
dependency functionLF freq LF freq0.78 4204 0.78 3106 NP/NP noun modifier attachment0.73 2173 0.81 1765 (S[dcl]\NP)/NP transitive object0.65 1717 0.72 1459 (S[dcl]\NP)/NP transitive subject0.68 870 0.74 643 (S[dcl]\NP)/(S[dcl]\NP) control/raising S complement0.70 862 0.67 697 S[dcl]\NP intransitive subject0.60 670 0.69 499 (S[dcl]\NP)/(S[dcl]\NP) ?
control/raising subject0.55 626 0.54 412 (NP/NP)/(NP/NP) noun modifier modifier attachment0.57 370 0.68 321 (NP/NP)\(S[dcl]\NP) subject extraction S complement0.59 343 0.70 314 (NP/NP)\(S[dcl]\NP) ?
subject extraction modifier attachment0.59 110 0.69 84 (NP/NP)\(S[dcl]/NP) object extraction S complement0.63 106 0.75 86 (NP/NP)\(S[dcl]/NP) ?
object extraction modifier attachmentTable 9: Accuracy per dependency, for selected dependency typescorrect ???
incorrect ??
?correct stag 2307 (27.42%) 51 (0.61% )incorrect stag 4493 (53.40%) 1563 (18.58%)Table 10: Analysis of the 8,414 false positive depen-dencies from C&C on ????
6 dev setcaused by supertagging errors (the bottom row), butmost of these are not a result of incorrect ???
tags,demonstrating that supertagging and parsing are dif-ficult even with correct ???
tags.The  sensitivity  of C&C to  tagging  errors, andthe higher performance of the P&K parser, whichdoes not directly use ???
tags, calls into questionwhether ???
tagging yields a net gain in a languagewhere distinctions such as the noun/verb ambiguityare often difficult to resolve using local tagging ap-proaches.
The approach of Auli and Lopez (2011),which achieves superior results in English ???
pars-ing with a joint supertagging/parsing model, may bepromising in light of the performance difference be-tween P&K and C&C.6.1 Non-local dependenciesTable 9 shows how well the best models of eachparser recovered selected local and non-local depen-dencies.
The slot represented by each row appearsin boldface.
While C&C and P&K perform similarlyrecovering NP-internal structure, the ability of P&Kto recover verbal arguments, unbounded long-rangedependencies such as subject and object extraction,and bounded long-range dependencies such as con-trol/raising constructions, is superior.The C&C ????
parser appears to be biased to-wards generating far  more of  the frequent  depen-dency types, yet does not typically have a higher re-call for these dependency types than P&K.6.2 Pro-drop and its impact on ???
parsingOne of the most common types of unary rules inChinese  CCGbank, occurring  in  36% of  ChineseCCGbank sentences, is  the subject  pro-drop  ruleS[dcl]\NP ?
S[dcl], which accounts for the op-tional absence of the subject pronoun of a verb forpragmatic reasons where the referent can be recov-ered from the discourse (Li and Thompson, 1989).The subject pro-drop rule is problematic in Chi-nese parsing because its left hand side, S[dcl]\NP, isa very common category, and also because severalsyntactic distinctions in Chinese CCGbank hinge onthe difference between S[dcl]\NP and S[dcl].The latter point is illustrated by two of the sensesof ?
de, the Chinese subordinating particle.
Twocategories  which ?
de receives  in  the  grammarare (NP/NP)\(S[dcl]\NP) (introducing  a  relativeclause) and (NP/NP)\S[dcl] (in the construction S deNP).
Because subject pro-drop promotes any unsatu-rated S[dcl]\NP to S[dcl], whenever the supertaggerreturns both of the above categories for the lexicalitem?
de, the parser must consider two alternativeanalyses which yield different dependencies:(5) a.
titi?
?come out????
?iquestionithe questions which arise301English ChinesePTB/PCTB-based 92.1% (McClosky et al, 2006) 86.8% (Zhang and Clark, 2009)CCGbank-based86.0% (Fowler and Penn, 2010) 72.7% (this work)85.8% (Clark and Curran, 2007) 67.1% (this work)Table 11: Summary of Chinese parsing approachesmodel LF Lsa % stag cov logCC????
74.99 7.42 89.36 98.6 18.35(76.73 20.56 89.66 99.5 13.58)????
65.42 4.82 83.73 97.9 18.67(66.95 14.62 83.90 99.2 13.86)I-5 70.67 8.62 84.99 93.8 -(72.74 18.59 85.61 96.5 -)Table 12: Dev set evaluation for C&C over pro-dropsentences only (and over full set in parentheses)b.
pro?
?pro come out????
?questionthe question of (him, her) coming out38.1% of sentences in the development set containat least one instance of pro-drop.
The evaluationover only these sentences is given in Table 12.
Thisrestricted  evaluation  shows  that  while  we  cannotconclude that pro-drop is the causative factor, sen-tences with pro-drop are much more difficult for bothparsers to analyse correctly, although the drops in F -score and supertagging accuracy are largest for P&K.Critically, the fact that supertagging performanceon these more difficult sentences is reasonably com-parable with performance on the full set suggeststhat the bottleneck is in the parser rather than thesupertagger.
One measure of the complexity of pro-drop sentences is the substantial increase in the logCvalue of these sentences.
This suggests that a key tobringing parser performance on Chinese in line withEnglish lies in reining in the ambiguity caused byvery productive unary rules such as pro-drop.7 ConclusionUsing Chinese CCGbank (Tse and Curran, 2010), wehave trained and evaluated the first ???
parsers forChinese in the literature: the Clark and Curran (C&C;2007)  and Petrov and Klein  (P&K; 2007)  parsers.The P&K parser substantially outperformed (72.73)C&C with automatic ???
tags (67.09).Table 11 summarises  the  best  performance  ofparsers on ???
and CCGbank, for English and Chi-nese.
We observe a drop in performance between En-glish and Chinese ???
parsers which is much largerthan, but consistent with, ???
parsers.
To close thisgap, future research in Chinese parsing should be in-formed by quantifying the aspects of Chinese whichaccount most for the deficit.We start by using corpus conversion to comparedifferent  linguistic  representation  choices, ratherthan  for  generating  a  single  immutable  resource.This can also be exploited to develop syntactic cor-pora parameterised for particular applications.
Wefound that  collapsing categorial  distinctions  moti-vated by theory can yield less ambiguous corpora,and hence, more accurate parsers.
We have alsotaken a novel approach to investigating the impactof noun/verb and other ???
ambiguities on parsing.The large gap between Chinese C&C and P&K issurprising, given that Fowler and Penn (2010) foundonly a small gap for English.
We found that C&Cis very sensitive to ???
tagging performance, whichleads to its inferior performance given automaticallyassigned ???
tags.
This suggests that joint supertag-ging/parsing approaches, as performed by P&K, aremore suitable for Chinese.
Finally, we have shownthat pro-drop is correlated with poor performanceon both parsers, suggesting an avenue to closing theChinese-English parsing gap.While developing the first wide-coverage Chinese???
parsers, we have shed light on the nature of theChinese-English parsing gap, and identified new andsignificant challenges for ???
parsing.AcknowledgementsWe thank our anonymous reviewers for their insight-ful and detailed feedback.
James R. Curran was sup-ported by Australian Research Council (???)
Dis-covery grant DP1097291 and the Capital MarketsCooperative Research Centre.302ReferencesMichael  Auli  and Adam Lopez.
2011.
A comparisonof loopy belief propagation and dual decompositionfor integrated ccg supertagging and parsing.
In 49thAnnual Meeting of the Association for ComputationalLinguistics, pages 470?480.
Association for Computa-tional Linguistics.DanielM.
Bikel.
2004.
On the parameter space of genera-tive lexicalized statistical parsing models.
Ph.D. thesis,Citeseer.Daniel M. Bikel and David Chiang.
2000.
Two statisti-cal parsing models applied to the Chinese Treebank.
InSecond workshop on Chinese language processing, vol-ume 12, pages 1?6.
Morristown, NJ, USA.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser evaluation: a survey and a new proposal.
In Pro-ceedings of the 1st International Conference on Lan-guage Resources and Evaluation, pages 447?454.Yuen-Ren Chao.
1968.
A grammar of spoken Chinese.University of California Press.David Chiang and Daniel M. Bikel.
2002.
Recovering la-tent information in treebanks.
In Proceedings of the19th international  conference on Computational  lin-guistics, volume 1, pages 1?7.
Association for Compu-tational Linguistics.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG parsing.In Proceedings of the 20th international conference onComputational Linguistics.
Association for Computa-tional Linguistics.Stephen  Clark  and  James R.  Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
In Computational Linguistics, vol-ume 33, pages 493?552.James R. Curran and Stephen Clark.
2003.
InvestigatingGIS and smoothing for maximum entropy taggers.
InProceedings of the 10th Meeting of the EACL, pages91?98.
Budapest, Hungary.Timothy A.D. Fowler and Gerald Penn.
2010.
Accu-rate context-free parsing with combinatory categorialgrammar.
Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages335?344.Yuqing Guo, Haifeng  Wang, and  Josef  Van Genabith.2007.
Recovering non-local dependencies for Chinese.In EMNLP/CoNLL, pages 257?266.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D.thesis, University of Edinburgh.Julia Hockenmaier and Mark Steedman.
2005.
CCGbank:Users?
manual.
Technical report, MS-CIS-05-09, Com-puter and Information Science, University of Pennsyl-vania.Julia Hockenmaier and Mark Steedman.
2007.
CCGbank:A Corpus of CCG Derivations and Dependency Struc-tures Extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.Matthew Honnibal.
2010.
Hat Categories: Represent-ing Form and Function Simultaneously in CombinatoryCategorial Grammar.
Ph.D. thesis, University of Syd-ney.Liang  Huang, Wenbin  Jiang, and  Qun  Liu.
2009.Bilingually-constrained  (monolingual)  shift-reduceparsing.
In Proceedings of the 2009 Conference onEmpirical  Methods  in  Natural  Language  Process-ing, volume 3, pages  1222?1231.
Association  forComputational Linguistics.Roger Levy and Christopher Manning.
2003.
Is it harderto parse Chinese, or the Chinese Treebank?
In AnnualMeeting of the Association for Computational Linguis-tics, volume 1, pages 439?446.
Morristown, NJ, USA.Charles N. Li and Sandra A. Thompson.
1989.
MandarinChinese: A functional reference grammar.
Universityof California Press.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a Large Annotated Cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the main conference on Human Language Tech-nology Conference of the North American Chapter ofthe Association of  Computational Linguistics, pages152?159.
Association for Computational Linguistics.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi  Tsujii.2004.
Corpus-Oriented Grammar Development for Ac-quiring a Head-Driven Phrase Structure Grammar fromthe Penn Treebank.
pages 684?693.Slav Petrov and Dan Klein.
2007.
Improved inference forunlexicalized parsing.
In Proceedings of NAACL HLT2007, pages 404?411.Laura Rimell, Stephen Clark, and Mark Steedman.
2009.Unbounded dependency recovery for parser evaluation.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 2-Volume 2, pages 813?821.
Association for Computa-tional Linguistics.Mark Steedman.
2000.
The Syntactic Process.
MIT Press.Cambridge, MA, USA.303Daniel Tse and James R. Curran.
2010.
Chinese CCG-bank: extracting CCG derivations from the Penn Chi-nese Treebank.
Proceedings of the 23rd InternationalConference on Computational  Linguistics  (COLING2010), pages 1083?1091.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006.A fast, accurate deterministic parser for Chinese.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages  425?432.
Association  for  Computational  Lin-guistics.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of  Natural Lan-guage Processing Pacific Rim Symposium ?99, pages398?403.Fei Xia, Chung-hye Han, Martha Palmer, and AravindJoshi.
2000.
Comparing lexicalized treebank grammarsextracted from Chinese, Korean, and English corpora.In Proceedings of the second workshop on Chinese lan-guage processing: held in conjunction with the 38thAnnual Meeting of the Association for ComputationalLinguistics, volume 12, pages 52?59.
Association forComputational Linguistics.Nianwen  Xue, Fei  Xia, Fu-Dong  Chiou, and  MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(02):207?238.Kun  Yu, Yusuke  Miyao, Takuya  Matsuzaki, XiangliWang, and Jun?ichi Tsujii.
2011.
Analysis of the dif-ficulties in chinese deep parsing.
In 12th InternationalConference on Parsing Technologies, page 48.Yue Zhang and Stephen Clark.
2008.
A tale  of  twoparsers: investigating  and  combining  graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages562?571.
Association for Computational Linguistics.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the Chinese treebank using a global discrim-inative model.
In Proceedings of the 11th InternationalConference on Parsing Technologies, pages 162?171.Association for Computational Linguistics.304
