Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 369?374, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUNITOR: Combining Syntactic and Semantic Kernels forTwitter Sentiment AnalysisGiuseppe Castellucci(?
), Simone Filice(?
), Danilo Croce(?
), Roberto Basili(?)(?)
Dept.
of Electronic Engineering(?)
Dept.
of Civil Engineering and Computer Science Engineering(?)
Dept.
of Enterprise EngineeringUniversity of Rome, Tor VergataRome, Italy{castellucci,filice,croce,basili}@info.uniroma2.itAbstractIn this paper, the UNITOR system participat-ing in the SemEval-2013 Sentiment Analysisin Twitter task is presented.
The polarity de-tection of a tweet is modeled as a classifica-tion task, tackled through a Multiple Kernelapproach.
It allows to combine the contribu-tion of complex kernel functions, such as theLatent Semantic Kernel and Smoothed Par-tial Tree Kernel, to implicitly integrate syn-tactic and lexical information of annotated ex-amples.
In the challenge, UNITOR systemachieves good results, even considering thatno manual feature engineering is performedand no manually coded resources are em-ployed.
These kernels in-fact embed distri-butional models of lexical semantics to deter-mine expressive generalization of tweets.1 IntroductionWeb 2.0 and Social Networks technologies allowusers to generate contents on blogs, forums and newforms of communication (such as micro-blogging)writing their opinion about facts, things, events.
Theanalysis of this information is crucial for companies,politicians or other users in order to learn what peo-ple think, and consequently to adjust their strategies.In such a scenario, the interest in the analysis of thesentiment expressed by people is rapidly growing.Twitter1 represents an intriguing source of informa-tion as it is used to share opinions and sentimentsabout brands, products, or situations (Jansen et al2009).1http://www.twitter.comOn the other hand, tweet analysis represents achallenging task for natural language processingsystems.
Let us consider the following tweets, evok-ing a positive (1), and negative (2) polarity, respec-tively.Porto amazing as the sun sets... http://bit.ly/c28w (1)@knickfan82 Nooooo ;( they delayed the knicks gameuntil Monday!
(2)Tweets are short, informal and characterized bytheir own particular language with ?Twitter syntax?,e.g.
retweets (?RT?
), user references (?@?
), hash-tags (?#?)
or other typical web abbreviations, suchas emoticons or acronyms.Classical approaches to sentiment analysis (Panget al 2002; Pang and Lee, 2008) are not directly ap-plicable to tweets: most of them focus on relativelylarge texts, e.g.
movie or product reviews, and per-formance drops are experimented in tweets scenario.Some recent works tried to model the sentiment intweets (Go et al 2009; Pak and Paroubek, 2010;Kouloumpis et al 2011; Davidov et al 2010; Bifetand Frank, 2010; Croce and Basili, 2012; Barbosaand Feng, 2010; Agarwal et al 2011).
Specific ap-proaches and feature modeling are used to achievegood accuracy levels in tweet polarity recognition.For example, the use of n-grams, POS tags, polar-ity lexicon and tweet specific features (e.g.
hash-tag, retweet) are some of the component exploitedby these works in combination with different ma-chine learning algorithms (e.g.
Naive Bayes (Pakand Paroubek, 2010), k-NN strategies (Davidov etal., 2010), SVM and Tree Kernels (Agarwal et al2011)).In this paper, the UNITOR system participating369in the SemEval-2013 Sentiment Analysis in Twit-ter task (Wilson et al 2013) models the senti-ment analysis stage as a classification task.
A Sup-port Vector Machine (SVM) classifier learns the as-sociation between short texts and polarity classes(i.e.
positive, negative, neutral).
Different kernelfunctions (Shawe-Taylor and Cristianini, 2004) havebeen used: each kernel aims at capturing specific as-pects of the semantic similarity between two tweets,according to syntactic and lexical information.
Inparticular, in line with the idea of using convolu-tion tree kernels to model complex semantic tasks,e.g.
(Collins and Duffy, 2001; Moschitti et al 2008;Croce et al 2011), we adopted the Smoothed Par-tial Tree Kernel (Croce et al 2011) (SPTK).
It isa state-of-the-art convolution kernel that allows tomeasure the similarity between syntactic structures,which are partially similar and whose nodes can dif-fer but are nevertheless semantically related.
More-over, a Bag-of-Word and a Latent Semantic Kernel(Cristianini et al 2002) are also combined with theSPTK in a multi-kernel approach.Our aim is to design a system that exhibits wideapplicability and robustness.
This objective is pur-sued by adopting an approach that avoids the useof any manually coded resource (e.g.
a polaritylexicon), but mainly exploits distributional analysisof unlabeled corpora: the generalization of wordsmeaning is achieved through the construction of aWord Space (Sahlgren, 2006), which provides an ef-fective distributional model of lexical semantics.In the rest of the paper, in Section 2 we willdeeply explain our approach.
In Section 3 the re-sults achieved by our system in the SemEval-2013challenge are described and discussed.2 System DescriptionThis section describes the approach behind theUNITOR system.
Tweets pre-processing and lin-guistic analysis is described in Section 2.1, while thecore modeling is described in 2.2.2.1 Tweet PreprocessingTweets are noisy texts and a pre-processing phase isrequired to reduce data sparseness and improve thegeneralization capability of the learning algorithms.The following set of actions is performed before ap-plying the natural language processing chain:?
fully capitalized words are converted in theirlowercase counterparts;?
reply marks are replaced with the pseudo-tokenUSER, and POS tag is set to $USR;?
hyperlinks are replaced by the token LINK,whose POS is $URL;?
hashtags are replaced by the pseudo-tokenHASHTAG, whose POS is imposed to $HTG;?
characters consecutively repeated more thanthree times are cleaned as they cause high lev-els of lexical data sparseness (e.g.
?nooo!!!!!
?and ?nooooo!!!?
are both converted into?noo!!?);?
all emoticons are replaced by SML CLS, whereCLS is an element of a list of classified emoti-cons (113 emoticons in 13 classes).For example, the tweet in the example 2 is nor-malized in ?user noo sml cry they delayed the knicksgame until monday?.
Then, we apply an almost stan-dard NLP chain with Chaos (Basili and Zanzotto,2002).
In particular, we process each tweet to pro-duce chunks.
We adapt the POS Tagging and Chunk-ing phases in order to correctly manage the pseudo-tokens introduced in the normalization step.
This isnecessary because tokens like SML SAD are taggedas nouns, and they influence the chunking quality.2.2 Modeling Kernel FunctionsFollowing a summary of the employed kernel func-tions is provided.Bag of Word Kernel (BOWK) A basic kernel func-tion that reflects the lexical overlap between tweets.Each text is represented as a vector whose dimen-sions correspond to different words.
Each dimen-sion represents a boolean indicator of the presenceor not of a word in the text.
The kernel function isthe cosine similarity between vector pairs.Lexical Semantic Kernel (LSK) A kernel functionis obtained to generalize the lexical information oftweets, without exploiting any manually coded re-source.
Basic lexical information is obtained bya co-occurrence Word Space built accordingly tothe methodology described in (Sahlgren, 2006) and(Croce and Previtali, 2010).
A word-by-context ma-trixM is obtained through a large scale corpus anal-ysis.
Then the Latent Semantic Analysis (Lan-370dauer and Dumais, 1997) technique is applied as fol-lows.
The matrix M is decomposed through Singu-lar Value Decomposition (SVD) (Golub and Kahan,1965) into the product of three new matrices: U , S,and V so that S is diagonal and M = USV T .
M isthen approximated by Mk = UkSkV Tk , where onlythe first k columns of U and V are used, correspond-ing to the first k greatest singular values.
The orig-inal statistical information about M is captured bythe new k-dimensional space, which preserves theglobal structure while removing low-variant dimen-sions, i.e.
distribution noise.
The result is that everyword is projected in the reduced Word Space andan entire tweet is represented by applying an addi-tive linear combination.
Finally, the resulting ker-nel function is the cosine similarity between vectorpairs, in line with (Cristianini et al 2002).Smoothed Partial Tree Kernel (SPTK) In orderto exploit the syntactic information of tweets, theSmoothed Partial Tree Kernel proposed in (Croce etal., 2011) is adopted.
Tree kernels exploit syntacticsimilarity through the idea of convolutions amongsubstructures.
Any tree kernel evaluates the numberof common substructures between two trees T1 andT2 without explicitly considering the whole frag-ment space.
Its general equation is reported here-after:TK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2), (3)where NT1 and NT2 are the sets of the T1?s andT2?s nodes, respectively and ?
(n1, n2) is equal tothe number of common fragments rooted in the n1and n2 nodes.
The function ?
determines the na-ture of the kernel space.
In the SPTK formulation(Croce et al 2011) this function emphasizes lexicalnodes.
It computes the similarity between lexicalnodes as the similarity between words in the WordSpace.
So, this kernel allows a generalization bothfrom the syntactic and the lexical point of view.However, tree kernel methods are biased by pars-ing accuracy and standard NLP parsers suffer accu-racy loss in this scenario (Foster et al 2011).
Itis mainly due to the complexities of the languageadopted in tweets.
In this work, we do not use arepresentation that depends on full parse trees.
Asyntactic representation derived from tweets chunk-ing (Tjong Kim Sang and Buchholz, 2000) is hereadopted, as shown in Figure 1.Notice that no explicit manual feature engineeringis applied.
On the contrary we expect that discrim-inative lexical and syntactic information (e.g.
nega-tion) is captured by the kernel in the implicit featurespace, as discussed in (Collins and Duffy, 2001).A multiple kernel approach Kernel methods areappealing as they can be integrated in various ma-chine learning algorithms, such as SVM.
Moreovera combination of kernels is still a kernel function(Shawe-Taylor and Cristianini, 2004).
We employeda linear combination ?BOWK + ?LSK + ?SPTKin order to exploit the lexical properties captured byBOWK (and generalized by LSK) and the syntac-tic information of the SPTK.
In our experiments, thekernel weights ?, ?
and ?
are set to 1.3 Results and DiscussionIn this section experimental results of the UNITORsystem are reported.3.1 Experimental setupIn the Sentiment Analysis in Twitter task, twosubtasks are defined: Contextual PolarityDisambiguation (Task A), and MessagePolarity Classification (Task B).
The for-mer deals with the polarity classification (positive,negative or neutral) of a marked occurrence of aword or phrase in a tweet context.
For examplethe adjective ?amazing?
in example 1 expresses apositive marked word.
The latter deals with theclassification of an entire tweet with respect tothe three classes positive, negative and neutral.
Inboth subtasks, we computed a fixed (80%-20%)split of the training data for classifiers parametertuning.
Tuned parameters are the regularizationparameter and the cost factor (Morik et al 1999)of the SVM formulation.
The former represents thetrade off between a training error and the margin.The latter controls the trade off between positiveand negative examples.
The learning phase is madeavailable by an extended version of SVM-LightTK2,implementing the smooth matching between treenodes.We built a Word Space based on about 1.5 mil-lion of tweets downloaded during the challenge pe-riod using the topic name from the trial material as2http://disi.unitn.it/moschitti/Tree-Kernel.htm371TWLINK$URLlink::$punt..::.VerFinVBZset::vPrepNNsun::nDTthe::dINas::iAggJJamazing::jNomNNPporto::n(a)TWpunt.!
::.PrepNNPmonday::nINuntil::iNomNNgame::nNNSknicks::nVerFinVBDdelay::vNomPRPthey::pSMILE$SMLsml cry::UHUHnoo::uUSER$USRuser::$(b)Figure 1: Chunk-based tree derived from examples (1) and (2)query terms.
We normalized and analyzed tweets asdescribed in section 2.1.
Words occurring more than100 times in the source corpus are represented asvectors.
The 10, 000 most frequent words in the cor-pus are considered as contexts and the co-occurrencescores are measured in a window of size n = ?5.Vector components are weighted through the Point-wise Mutual Information (PMI), and dimensional-ity reduction is applied through SVD with a cut ofk = 250.The task requires to classify two different texts:tweets and sms.
Sms classification is intended toverify how well a system can scale on a differentdomain.
In the testing phase two types of submis-sions are allowed.
Constrained results refer to thecase where systems are trained only with the re-leased data.
Unconstrained results refer to the casewhere additional training material is allowed.
Eval-uation metrics adopted to compare systems are Pre-cision, Recall and F1-Measure.
Average F1 of thepositive and negative classes is then used to generateranks.
Further information about the task is avail-able in (Wilson et al 2013).3.2 Results over Contextual PolarityDisambiguationWe tackled Task A with a multi-kernel approachcombining the kernel functions described in Section2.2.
The final kernel is computed as the linear com-bination of the kernels, as shown in Equation 4.k(t1, t2) = SPTK(?A(t1), ?A(t2))+BOWK(?A(t1), ?A(t2))+ LSK(?A(t1), ?A(t2)) (4)where t1, t2 are two tweet examples.
The ?A(x)function extracts the 4-level chunk tree from thetweet x; nodes (except leaves) covering the markedinstance in x are highlighted in the tree with -POL.The ?A(x) function extracts the vector representingthe Bag-of-Word of the words inside the marked in-stance of x, while ?A builds the LSA vectors of thewords occurring within the marked span of x. Re-ferring to example 1, both ?A(x) and ?A point tothe ?amazing?
adjective.
Finally, k(t1, t2) returnsthe similarity between t1 and t2 accordingly to ourmodeling.
As three polarity classes are considered,we adopt a multi-classification schema accordinglyto a One-Vs-All strategy (Rifkin and Klautau, 2004):the final decision function consists in the selectionof the category associated with the maximum SVMmargin.Rank 4/19class precision recall f1positive .8375 .7750 .8050Avg-F1 .8249negative .8103 .8822 .8448neutral .3475 .3082 .3267Table 1: Task A results for the sms datasetRank 7/21class precision recall f1positive .8739 .8844 .8791Avg-F1 .8460negative .8273 .7988 .8128neutral .2778 .3125 .2941Table 2: Task A results for the twitter datasetTables 1 and 2 report the results of the UNITORsystem in the Task A.
Only the constrained set-ting has been submitted.
The performance of theproposed approach is among the best ones and weranked 4th and 7th among about 20 systems.The system seems to be able to generalize wellfrom the provided training data, and results are re-markable, especially considering that no manual an-notated lexical resources were adopted and no man-ual feature engineering is exploited.
It demonstratesthat a multi-kernel approach, with the proposed shal-low syntactic representation, is able to correctlyclassify the sentiment in out-of-domain contexts too.Syntax is well captured by the SPTK and the lexicalgeneralization provided by the Word Space allowsto generalize in the sms scenario.3723.3 Results over Message PolarityClassificationA multi-kernel approach is adopted for this task too,as described in the following Equation 5:k(t1, t2) = SPTK(?B(t1), ?B(t2))+BOWK(?B(t1), ?B(t2))+ LSK(?B(t1), ?B(t2)) (5)The ?B(x) function extracts a tree representation ofx.
In this case no nodes in the trees are marked.The ?B(x) function extracts Bag-of-Word vectorsfor all the words in the tweet x, while ?B(x) extractsthe linear combination of vectors in the Word Spacefor adjectives, nouns, verbs and special tokens (e.g.hashtag, smiles) of the words in x.
Again, a One-Vs-All strategy (Rifkin and Klautau, 2004) is applied.Constrained run.
Tables 3 and 4 report the resultin the constrained case.
In the sms dataset our sys-tem suffers more with respect to the tweet one.
Inboth cases, the system shows a performance dropon the negative class.
It seems that the multi-kernelapproach needs more examples to correctly disam-biguate elements within this class.
Indeed, nega-tive class cardinality was about 15% of the trainingdata, while the positive and neutral classes approxi-mately equally divided the remaining 85%.
More-over, it seems that our system confuses polarizedclasses with the neutral one.
For example, the tweet?going Hilton hotel on Thursday for #cantwait?
isclassified as neutral (the gold label is positive).
Inthis case, the hashtag is the sentiment bearer, andour model is not able to capture this information.Rank 13/29class precision recall f1positive .5224 .7358 .6110Avg-F1 .5122negative .6019 .3147 .4133neutral .7883 .7798 .7840Table 3: Task B results for the sms dataset in theconstrained caseRank 13/36class precision recall f1positive .7394 .6514 .6926Avg-F1 .5827negative .6366 .3760 .4728neutral .6397 .8085 .7142Table 4: Task B results for the twitter dataset in theconstrained caseUnconstrained run.
In the unconstrained case wetrained our system adding 2000 positive examplesand 2000 negative examples to the provided trainingset.
These additional tweets were downloaded fromTwitter during the challenge period using positiveand negative emoticons as query terms.
The under-lying hypothesis is that the polarity of the emoticonscan be extended to the tweet (Pak and Paroubek,2010; Croce and Basili, 2012).
In tables 5 and 6performance measures in this setting are reported.Rank 10/15class precision recall f1positive .4337 .7317 .5446Avg-F1 .4888negative .3294 .6320 .4330neutral .8524 .3584 .5047Table 5: Task B results for the sms dataset in theunconstrained caseRank 5/15class precision recall f1positive .7375 .6399 .6853Avg-F1 .5950negative .5729 .4509 .5047neutral .6478 .7805 .7080Table 6: Task B results for the twitter dataset in theunconstrained caseIn this scenario, sms performances are againlower than the twitter case.
This is probably due tothe fact that the sms context is quite different fromthe twitter one.
This is not true for Task A: polar ex-pressions are more similar in sms and tweets.
Again,we report a performance drop on the negative class.However, using more negative tweets seems to bebeneficial.
The F1 for this class increased of about3 points for both datasets.
Our approach thus needsmore examples to better generalize from data.In the future, we should check the redundancy andnovelty of the downloaded material, as early dis-cussed in (Zanzotto et al 2011).
Moreover, we willexplore the possibility to automatically learn the ker-nel linear combination coefficients in order to op-timize the balancing between kernel contributions(Go?nen and Alpaydin, 2011).AcknowledgementsThis work has been partially funded by the Ital-ian Ministry of Industry within the ?Industria2015?
Framework, under the project DIVINO(MI01 00234).373ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysis oftwitter data.
In Proceedings of the Workshop on Lan-guages in Social Media, pages 30?38, Stroudsburg,PA, USA.
Association for Computational Linguistics.Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.In COLING, pages 36?44, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Roberto Basili and Fabio Massimo Zanzotto.
2002.
Pars-ing engineering and empirical robustness.
Nat.
Lang.Eng., 8(3):97?120, June.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In Proceed-ings of the 13th international conference on Discov-ery science, pages 1?15, Berlin, Heidelberg.
Springer-Verlag.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of NeuralInformation Processing Systems (NIPS?2001), pages625?632.Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.2002.
Latent semantic kernels.
J. Intell.
Inf.
Syst.,18(2-3):127?152.Danilo Croce and Roberto Basili.
2012.
Grammaticalfeature engineering for fine-grained ir tasks.
In IIR,pages 133?143.Danilo Croce and Daniele Previtali.
2010.
Manifoldlearning for the semi-supervised induction of framenetpredicates: an empirical investigation.
In GEMS 2010,pages 7?16, Stroudsburg, PA, USA.
Association forComputational Linguistics.Danilo Croce, Alessandro Moschitti, and Roberto Basili.2011.
Structured lexical similarity via convolutionkernels on dependency trees.
In Proceedings ofEMNLP, Edinburgh, Scotland, UK.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In COLING, pages 241?249, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011.
#hard-toparse: Pos tagging and parsing the twitterverse.
InAnalyzing Microtext.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twittersentiment classification using distant supervision.G.
Golub and W. Kahan.
1965.
Calculating the singularvalues and pseudo-inverse of a matrix.
Journal of theSociety for Industrial and Applied Mathematics: Se-ries B, Numerical Analysis, 2(2):pp.
205?224.Mehmet Go?nen and Ethem Alpaydin.
2011.
Multi-ple kernel learning algorithms.
Journal of MachineLearning Research, 12:2211?2268.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Twitter power: Tweets as elec-tronic word of mouth.
J.
Am.
Soc.
Inf.
Sci.
Technol.,60(11):2169?2188, November.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In ICWSM.Tom Landauer and Sue Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis theoryof acquisition, induction and representation of knowl-edge.
Psychological Review, 104.Katharina Morik, Peter Brockhausen, and ThorstenJoachims.
1999.
Combining statistical learning with aknowledge-based approach - a case study in intensivecare monitoring.
In ICML, pages 268?277, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Alessandro Moschitti, Daniele Pighin, and Robert Basili.2008.
Tree kernels for semantic role labeling.
Com-putational Linguistics, 34.Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InLREC.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In EMNLP, volume 10,pages 79?86, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ryan Rifkin and Aldebaro Klautau.
2004.
In defense ofone-vs-all classification.
J. Mach.
Learn.
Res., 5:101?141, December.Magnus Sahlgren.
2006.
The Word-Space Model.
Ph.D.thesis, Stockholm University.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress, New York, NY, USA.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.
In-troduction to the conll-2000 shared task: chunking.
InConLL ?00, pages 127?132, Stroudsburg, PA, USA.Association for Computational Linguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyonov.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.Fabio Massimo Zanzotto, Marco Pennacchiotti, andKostas Tsioutsiouliklis.
2011.
Linguistic redundancyin twitter.
In EMNLP, pages 659?669.374
