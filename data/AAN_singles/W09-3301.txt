Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 1?9,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPA Novel Approach to Automatic Gazetteer Generation usingWikipediaZiqi ZhangUniversity of Sheffield, UKz.zhang@dcs.shef.ac.ukJos?
IriaUniversity of Sheffield, UKj.iria@dcs.shef.ac.ukAbstractGazetteers or entity dictionaries are importantknowledge resources for solving a wide range ofNLP problems, such as entity extraction.
We in-troduce a novel method to automatically generategazetteers from seed lists using an externalknowledge resource, the Wikipedia.
Unlike pre-vious methods, our method exploits the rich con-tent and various structural elements of Wikipe-dia, and does not rely on language- or domain-specific knowledge.
Furthermore, applying theextended gazetteers to an entity extraction task ina scientific domain, we empirically observed asignificant improvement in system accuracywhen compared with those using seed gazetteers.1 IntroductionEntity extraction is the task of identifying andclassifying atomic text elements into predefinedcategories such as person names, place names,and organization names.
Entity extraction oftenserves as a fundamental step for complex NaturalLanguage Processing (NLP) applications such asinformation retrieval, question answering, andmachine translation.
It has been recognized thatin this task, gazetteers, or entity dictionaries, playa crucial role (Roberts et al 2008).
In addition,they serve as important resources for other stu-dies, such as assessing level of ambiguities of alanguage, and disambiguation (Maynard et al2004).Because building and maintaining high qualitygazetteers by hand is very time consuming (Ka-zama and Torisawa, 2008), many solutions haveproposed generating gazetteers automaticallyfrom existing resources.
In particular, the successthat solutions which exploit Wikipedia1 havebeen enjoying in many other NLP applicationshas encouraged a number of research works onautomatic gazetteer generation to use Wikipedia,1 http://en.wikipedia.orgsuch as works by Toral and Mu?oz (2006), andKazama and Torisawa (2007).Unfortunately, current systems still presentseveral limitations.
First, none have exploited thefull content and structure of Wikipedia articles,but instead, only make use of the article?s firstsentence.
However, the full content and structureof Wikipedia carry rich information that has beenproven useful in many other NLP problems, suchas document classification (Gabrilovich andMarkovitch, 2006), entity disambiguation (Bu-nescu and Pa?ca, 2006), and semantic relatedness(Strube and Ponzetto, 2006).
Second, no otherworks have evaluated their methods in the con-text of entity extraction tasks.
Evaluating thesegenerated gazetteers in real NLP applications isimportant, because the quality of these gazetteershas a major impact on the performance of NLPapplications that make use of them.
Third, themajority of approaches focus on newswire do-main and the four classic entity types location(LOC), person (PER), organization (ORG) andmiscellaneous (MISC), which have been studiedextensively.
However, it has been argued thatentity extraction is often much harder in scientif-ic domains due to complexity of domain lan-guages, density of information and specificity ofclasses (Murphy et al 2006; Byrne, 2007; Noba-ta et al 2000).In this paper we propose a novel approach toautomatically generating gazetteers using exter-nal knowledge resources.
Our method is lan-guage- and domain- independent, and scalable.We show that the content and various structuralelements of Wikipedia can be successfully ex-ploited to generate high quality gazetteers.
Toassess gazetteer quality, we evaluate it in thecontext of entity extraction in the scientific do-main of Archaeology, and demonstrate that thegenerated gazetteers improve the performance ofan SVM-based entity tagger across all entitytypes on an archaeological corpus.The rest of the paper is structured as follows.In the next section, we review related work.
Insection 3 we explain our methodology for auto-1matic gazetteer generation.
Section 4 introducesthe problem domain and describes the experi-ments conducted.
Section 5 presents and dis-cusses the results.
Finally we conclude with anoutline of future work.2 Related WorkCurrently, existing methods to automatic gazet-teer generation can be categorized into twomainstreams; pattern driven approach and know-ledge resource approach.The pattern driven approach uses domain-and language specific patterns to extract candi-date entities from unlabeled corpora.
The idea isto include features derived from unlabeled datato improve a supervised learning model.
For ex-ample, Riloff and Jones (1999) introduced abootstrapping algorithm which starts from seedlists and, iteratively learns and refines domainspecific extraction patterns for a semantic cate-gory that are then used for building dictionariesfrom unlabeled data.
Talukdar et al(2006), alsostarting with seed entity lists, apply pattern in-duction to an unlabeled corpus and then use theinduced patterns to extract candidate entitiesfrom the corpus to build extended gazetteers.They showed that using the token membershipfeature with the extended gazetteer improved theperformance of a Conditional Random Field(CRF) entity tagger; Kozareva (2006) designedlanguage specific extraction patterns and valida-tion rules to build Spanish location (LOC), per-son (PER) and organization (ORG) gazetteersfrom unlabeled data, and used these to improve asupervised entity tagger.However, the pattern driven approach hasbeen criticized for weak domain adaptability andinadequate extensibility due to the specificity ofderived patterns.
(Toral and Mu?oz, 2006; Ka-zama and Torisawa, 2008).
Also, often it is dif-ficult and time-consuming to develop domain-and language-specific patterns.The knowledge resource approach, attemptsto solve these problems by relying on the abun-dant information and domain-independent struc-tures in existing large-scale knowledge re-sources.
Magnini et al(2002) used WordNet as agazetteer together with rules to extract entitiessuch as LOC, PER and ORG.
They used two re-lations in WordNet; Word_Class, referring toconcepts bringing external evidence; andWord_Instance, referring to particular instancesof those concepts.
Concepts belonging toWord_Class are used to identify trigger wordsfor candidate entities in corpus, while conceptsof Word_Instance are used directly as lookupdictionaries.
They achieved good results on anewswire corpus.
The main limitation of Word-Net is lack of domain specific vocabulary, whichis critical to domain specific applications(Sch?tze and Pedersen, 1997).
Roberts et al(2008) used terminology extracted from UMLSas gazetteers and tested it in an entity extractiontask over a medical corpus.
Contrary to Word-Net, UMLS is an example of a domain specificknowledge resource, thus its application is alsolimited.Recently, the exponential growth in informa-tion content in Wikipedia has made this Webresource increasingly popular for solving a widerange of NLP problems and across different do-mains.Concerning automatic gazetteer generation,Toral and Mu?oz (2006) tried to build gazetteersfor LOC, PER, and ORG by extracting all nounphrases from the first sentences of Wikipediaarticles.
Next they map the noun phrases toWorldNet synsets, and follow the hyperonymyhierarchy until they reach a synset belonging tothe entity class of interest.
However, they did notevaluate the generated gazetteers in the contextof entity extraction.
Due to lack of domain spe-cific knowledge in WordNet, their method is li-mited if applied to domain specific gazetteergeneration.
In contrast, our method overcomesthis limitation since it doesn?t rely on any re-sources other than Wikipedia.
Another funda-mental difference is that our method exploitsmore complex structures of Wikipedia.Kazama and Torisawa (2007) argued thatwhile traditional gazetteers map word sequencesto predefined entity categories such as ?London?
{LOCATION}?, a gazetteer is useful as longas it returns consistent labels even if these are notpredefined categories.
Following this hypothesis,they mapped Wikipedia article titles to theirhypernyms by extracting the first noun phraseafter be in the first sentence of the article, andused these as gazetteers in an entity extractiontask.
In their experiment, they mapped over39,000 search candidates to approximately 1,200hypernyms; and using these hypernyms as cate-gory labels in an entity extraction task showed animprovement in system performance.
Later, Ka-zama and Torisawa (2008) did the same inanother experiment on a Japanese corpus andachieved consistent results.
Although novel, theirmethod in fact bypasses the real problem of ge-2nerating gazetteers of specific entity types.
Ourmethod is essentially different in this aspect.
Inaddition, they only use the first sentence of Wi-kipedia articles.3 Automatic Gazetteer Generation ?
theMethodologyIn this section, we describe our methodology forautomatic gazetteer generation using the know-ledge resource approach.3.1 Wikipedia as the knowledge resourceTo demonstrate the validity of our approach, wehave selected the English Wikipedia as the ex-ternal knowledge resource.
Wikipedia is a freemultilingual and collaborative online encyclope-dia that is growing rapidly and offers good quali-ty of information (Giles, 2005).
Articles in Wiki-pedia are identified by unique names, and referto specific entities.
Wikipedia articles have manyuseful structures for knowledge extraction; forexample, articles are inter-connected by hyper-links carrying relations (Gabrilovich and Marko-vitch, 2006); articles about similar topics are ca-tegorized under the same labels, or grouped inlists; categories are organized as taxonomies, andeach category is associated with one or moreparent categories (Bunescu and Pa?ca, 2006).These relations are useful for identifying relatedarticles and thus entities, which is important forautomatic gazetteer generation.
Compared toother knowledge resources such as WordNet andUMLS, Wikipedia covers significantly largeramounts of information across different domains,therefore, it is more suitable for building domain-specific gazetteers.
For example, as of February2009, there are only 147,287 unique words inWordNet2, whereas the English Wikipedia issignificantly larger with over 2.5 million articles.A study by Holloway (2007) identified that by2005 there were already 78,977 unique catego-ries divided into 1,069 disconnected categoryclusters, which can be considered as the samenumber of different domains.3.2 The methodologyWe propose an automatic gazetteer generationmethod using Wikipedia article contents, hyper-links, and category structures, which can gener-ate entity gazetteers of any type.
Our method2 According tohttp://wordnet.princeton.edu/man/wnstats.7WN , February2009takes input seed entities of any type, and extendsthem to more complete lists of the same type.
Itis based on three hypotheses;1.
Wikipedia contains articles about domainspecific seed entities.2.
Using articles about the seed entities, wecan extract fine-grained type labels forthem, which can be considered as a listof hypernyms of the seed entities, andpredefined entity type hyponyms of theseeds.3.
Following the links on Wikipedia ar-ticles, we can reach a large collection ofarticles that are related to the source ar-ticles.
If a related article?s type label (asextracted above) matches any of thoseextracted for seed entities, we consider ita similar entity of the predefined type.Naturally, we divide our methods into threesteps; firstly we match a seed entity to a Wikipe-dia article (the matching phase); next we labelseed entities using the articles extracted for themand build a pool of fine-grained type labels forthe seed entities (the labeling phase); finally weextract similar entities by following links in ar-ticles of seed entities (the expansion phase).
Thepseudo-algorithm is illustrated in Figure 1.3.2.1 Matching seed entities to WikipediaarticleFor a given seed entity, we firstly use the exactphrase to retrieve Wikipedia articles.
If notfound, we use the leftmost longest match, asdone by Kazama and Torisawa (2007).
In Wiki-pedia, searches for ambiguous phrases are redi-rected to a Disambiguation Page, from whichusers have to manually select a sense.
We filterout any matches that are directed to disambigua-tion pages.
This filtering strategy is also appliedto step 3 in extracting candidate entities.3.2.2 Labeling seed entitiesAfter retrieving Wikipedia articles for all seedentities, we extract fine-grained type labels fromthese articles.
We identified two types of infor-mation from Wikipedia that can extract potential-ly reliable labels.3Figure 1.
The proposed pseudo-algorithm for gazet-teer generation from the content and various structuralelements of WikipediaAs Kazama and Torisawa (2007) observed, in thefirst sentence of an article, the head noun of thenoun phrase just after be is most likely thehypernym of the entity of interest, and thus agood category label.
There are two pitfalls to thisapproach.
First, the head noun may be too gener-ic to represent a domain-specific label.
For ex-ample, following their approach the label ex-tracted for the archaeological term ?ClassicalStage?3 from the sentence ?The Classic Stage isan archaeological term describing a particulardevelopmental level.?
is ?term?, which is thehead noun of ?archaeological term?.
Clearly insuch case the phrase is more domain-specific.For this reason we use the exact noun phrase ascategory label in our work.
Second, their methodignores a correlative conjunction which in mostcases indicates equivalently useful labels.
Forexample, the two noun phrases in italic in thesentence ?Sheffield is a city and metropolitanborough in South Yorkshire, England?
are equal-ly useful labels for the article ?Sheffield?.
There-fore, we also extract the noun phrase connectedby a correlative conjunction as the label.
We ap-ply this method to articles retrieved in 3.2.1.
For3Any Wikipedia examples for illustration in this paper makeuse of the English Wikipedia, February 2009, unless other-wise stated.simplicity, we refer to this approach to labelingseed entities as FirstSentenceLabeling, and thelabels created as Ls.
Note that our method is es-sentially different from Kazama and Torisawa aswe do not add these extracted nouns to gazet-teers; instead, we only use them for guiding theextraction of candidate entities, as described insection 3.2.3.As mentioned in section 3.1, similar articlesin Wikipedia are manually grouped under thesame categories by their authors, and categoriesare further organized as a taxonomy.
As a result,we extract category labels of articles as fine-grained type labels and consider them to behypernyms of the entity?s article.
We refer to thismethod as CategoryLabeling, and apply it to theseed entities to create a list of category labels,which we denote by Lc.Three situations arise in which the Category-Labeling introduces noisy labels.
First, somearticles are categorized under a category with thesame title as the article itself.
For example, thearticle about ?Bronze Age?
is categorized undercategory ?Bronze Age?.
In this case, we explorethe next higher level of the category tree, i.e., weextract categories of the category ?Bronze Age?,including ?2nd Millennium?, ?3rd millenniumBC?, ?Bronze?, ?Periods and stages in Archaeo-logy?, and ?Prehistory?.
Second, some categoriesare meaningless and for management purposes,such as ?Articles to be Merged since 2008?,?Wikipedia Templates?.
For these, we manuallycreate a small list of ?stop?
categories to be dis-carded.
Third, according to Strube and Ponzetto(2008), the category hierarchy is sometimes noi-sy.
To reduce noisy labels, we only keep labelsthat are extracted for at least 2 seed entities.Once a pool of fine-grained type labels havebeen created, in the next step we consider themas fine-grained and immediate hypernyms of theseed entities, and use them as control vocabularyto guide the extraction of candidate entities.3.2.3 Extracting candidate entitiesTo extract candidate entities, we first identifyfrom Wikipedia the entities that are related to theseed entities.
Then we select from them thosecandidates that share one or more commonhypernyms with the seed entities.
The intuition isthat in the taxonomy, nodes that share commonimmediate parents are mostly related, and, there-fore, good candidates for extended gazetteers.Input: seed entities SE of type TOutput: new entities NE of type TSTEP 1 (section 3.2.1)1.1.
Initialize Set P as articles for SE;1.2.
For each entity e: SE1.3.
Retrieve Wikipedia article p for e;1.4.
Add p to P;STEP 2 (section 3.2.2)2.1.
Initialize Set L2.2.
For each p: P2.3.
Extract fine grained type labels l;2.4.
Add l to L;STEP 3 (section 3.2.3)3.1.
Initialize Set HL;3.2.
For each p: P3.3.
Add hyperlinks from p to HL;3.4.
If necessary, recursively crawl extractedhyperlinks and repeat 3.2 and 3.33.5.
For each link hl: HL3.6.
Extract fine grained type labels l?;3.7.
If L contains l?3.8.
Add title of hl to NE;3.9.
Add titles of redirect links of hl toNE;4We extract related entities by following thehyperlinks from the articles retrieved for the seedentities, as by section 3.2.1.
This is because inWikipedia, articles often contain mentions ofentities that also have a corresponding article,and these mentions are represented as outgoinghyperlinks.
They link the main article of an enti-ty (source entity) to other sets of entities (relatedentities).
Therefore, by following these links wecan reach a large set of related entities to the seedlist.
To reduce noise, we also filter out links todisambiguation pages as in section 3.2.1.
Next,for each candidate in the related set, we use thetwo labeling approaches introduced in section3.2.2 to extract its type labels.
If any of these areincluded by the control vocabulary built with thesame labeling approach, we accept them into theextended gazetteers.
That is, if the control voca-bulary is built by FirstSentenceLabeling we on-ly use FirstSentenceLabeling to label the candi-date.
The same applies to CategoryLabeling.One can easily extend this stage by recursivelycrawling the hyperlinks contained in the re-trieved pages.
In addition, some Wikipedia ar-ticles have one or more redirecting links, whichgroups several surface forms of a single entity.For example a search for ?army base?
is redi-rected to article ?military base?.
These surfaceforms can be considered as synonyms, and wethus also select them for extend gazetteers.After applying the above processes to all seedentity articles, we obtain the output extendedgazetteers of domain-specific types.
To eliminatepotentially ambiguous entities, for each extendedgazetteer, we exclude entities that are found indomain-independent gazetteers.
For example, weuse a generic person name gazetteer to excludeambiguous person names from the extended ga-zetteers for LOC.4 ExperimentsIn this section we describe our experiments.
Ourgoal is to build extended gazetteers using themethods proposed in section 3, and test them inan entity extraction task to improve a baselinesystem.
First we introduce the setting, an entityextraction task in the archaeological domain;next we describe data preparation includingtraining data annotation and gazetteer generation;then, we introduce our baseline; and finallypresent the results.4.1 The Problem DomainThe problem of entity extraction has been stu-died extensively across different domains, par-ticularly in newswire articles (Talukdar et al2006), bio-medical science (Roberts et al 2008).In this experiment, we present the problem with-in the domain of archaeology, which is a discip-line that has a long history of active fieldworkand a significant amount of legacy data datingback to the nineteenth century and earlier.
Jeffreyet al(2009) reports that despite the existing fast-growing large corpora, little has been done todevelop high quality meta-data for efficientaccess to information in these datasets, which hasbecome a pressing issue in archaeology.
To ourbest knowledge, three works have piloted theresearch on using information extraction tech-niques for automatic meta-data generation in thisfield.
Greengrass et al(2008) applied entity andrelation extraction to historical court records toextract names, locations and trial names and theirrelations; Amrani et al(2008) used a series oftext-mining technologies to extract archaeologi-cal knowledge from specialized texts, one ofthese tasks concerns entity extraction.
Byrne(2007) applied entity and relation extraction to acorpus of archaeology site records.
Her workconcentrated on nested entity recognition of 11entity types.Our work deals with archaeological entity ex-traction from un-structured legacy data, whichmostly consist of full-length archaeological re-ports varying from 5 to over a hundred pages.According to Jeffrey et al(2009), three types ofentities are most useful to an archaeologist;?
Subject (SUB) ?
topics that reports referto, such as findings of artifacts and mo-numents.
It is the most ambiguous typebecause it covers various specializeddomains such as warfare, architecture,agriculture, machinery, and education.For example ?Roman pottery?, ?spear-head?, and ?courtyard?.?
Temporal terms (TEM) ?
archaeologicaldates of interest, which are written in anumber of ways, such as years ?1066 -1211?, ?circa 800AD?
; centuries ?C11?,?the 1st century?
; concepts ?BronzeAge?, ?Medieval?
; and acronyms such as?BA?
(Bronze Age), ?MED?
(Medieval).?
Location (LOC) ?
place names of inter-est, such as place names and site ad-dresses related to a finding or excava-tion.
In our study, these refer to UK-specific places.5Source Domain Tag Densityastro-ph Astronomy 5.4%MUC7 Newswire 11.8%GENIA Biomedical 33.8%AHDS-selectedArchaeology 9.2%Table 1.
Comparison of tag density in four test corpo-ra for entity extraction tasks.
The ?AHDS-selected?corpus used in this work has a tag density comparableto that of MUC74.2 Corpus and resourcesWe developed and tested our system on 30 fulllength UK archaeological reports archived by theArts and Humanities Data Service (AHDS)4.These articles vary from 5 to 120 pages, with atotal of 225,475 words.
The corpus is tagged bythree archaeologists, and is used for building andtesting the entity extraction system.
Compared toother test data reported in Murphy et al(2006),our task can be considered hard, due to the hete-rogeneity of information of the entity types andlower tag density in the corpus (the percentage ofwords tagged as entities), see Table 1.
Also, ac-cording to Vlachos (2007), full length articles areharder than abstracts, which are found commonin biomedical domain.
This corpus is then splitinto five equal parts for a five-fold cross valida-tion experiment.For seed gazetteers, we used the MIDAS Pe-riod list5 as the gazetteer for TEM, the Thesaurusof Monuments Types (TMT2008) from EnglishHeritage6 and the Thesaurus of Archaeology Ob-jects from the STAR project7 as gazetteers forSUB, and the UK Government list of administra-tive areas as the gazetteer for LOC.
In the fol-lowing sections, we will refer to these gazetteersas GAZ_original.4.3 Automatic gazetteer generationWe used the seed gazetteers together with themethods presented in section 3 to build new ga-zetteers for each entity type, and merge themwith the seeds as extended gazetteers to be testedin our experiments.
Since we introduced two me-thods for labeling seed entities (section 3.2.2),which are also used separately for selecting ex-tracted candidate entities (section 3.2.3), we de-sign four experiments to test the methods sepa-4 http://ahds.ac.uk/5 http://www.midas-heritage.info and http://www.fish-forum.info6 http://thesaurus.english-heritage.org.uk7 http://hypermedia.research.glam.ac.uk/kos/STAR/rately as well as in combination; specifically foreach entity type, GAZ_EXTfirstsent denotes the ex-tended gazetteer built using FirstSentenceLabe-ling for labeling seed entities and selecting can-didate entities; GAZ_EXTcategory refers to the ex-tended gazetteer built with CategoryLabeling;GAZ_EXTunion merges entities in two extendedgazetteers into a single gazetteer; whileGAZ_EXTintersect is the intersection ofGAZ_EXTfirstsent and GAZ_EXTcategory i.e., takingonly entities that appear in both.
Table 2 listsstatistics of the gazetteers and Table 3 displaysexample type labels extracted by the two me-thods.To implement the entity extraction system, weused Runes8 data representation framework, acollection of information extraction modulesfrom T-rex9, and the machine learning frame-work Aleph10.
The core of the tagger system is aSVM classifier.
We used the Java Wikipedia Li-brary11 (JWPL v0.452b) and the Wikipedia dumpof Feb 2007 published with it.4.4 Feature selection and baseline systemWe trained our baseline system by tuning featuresets used and the size of the token window toconsider for feature generation; and we select thebest performing setting as the baseline.
Later weadd official gazetteers in section 4.1 and ex-tended gazetteers as in section 4.3 to the base-lines and use gazetteer membership as an addi-tional feature to empirically verify the improve-ment in system accuracy.The baseline setting thus used a window size of 5and the following feature set:?
Morphological root of a token?
Exact token string?
Orthographic type (e.g., lowercase, up-percase)?
Token kind (e.g., number, word)4.5 ResultTable 4 displays the results obtained under eachsetting, using the standard metrics of Recall (R),Precision (P) and F-measure (F1).
The bottomrow illustrates Inter Annotator Agreement (IAA)8 http://runes.sourceforge.net/9 http://t-rex.sourceforge.net/10 http://aleph-ml.sourceforge.net/11 http://www.ukp.tu-darmstadt.de/software/jwpl/6LOC SUB TEMGAZ_original 11,786 (8,228 found) 5,725 (4,320 found) 61 (43 found)GAZ_EXTfirstsent 19,385 (7,599)  11,182 (5,457) 163 (102)GAZ_EXTcategory 18,861 (7,075) 13,480 (7,745) 305 (245)GAZ_EXTunion 23,741 (11,955) 16,697 (10,972) 333 (272)GAZ_EXTintersect 14,022 (2,236) 7,455 (1,730) 133 (72)Table 2.
Number of unique entities in each gazetteer, including official and extended versions.GAZ_EXT includes GAZ_original.
For GAZ_original, numbers in brackets are the number of entitiesfound in Wikipedia.
For others, they are the number of extracted entities that are new to the correspond-ing GAZ_originalLOC SUB TEMFirstSentence-Labeling (597)CategoryLabeling(779)FirstSentence-Labeling (1342)CategoryLabe-ling (761)FirstSentence-Labeling (11)CategoryLabe-ling(10)village,small village,place,town,civil parishvillages in northYorkshire,north Yorkshire geo-graphy stubs,villages in Norfolk,villages in Somerset,English market townsfacility,building,ship,tool,device,establishmentship types,monumenttypes,gardening,fortification,architecturestubsperiod,archaeologicalperiod,era,century,millenniumPeriods andstages in arc-haeology,Bronze age,middle ages,historical eras,centuriesTable 3.
Top 5 most frequently extracted (counted by number of seed entities sharing that label) fine-grained type labels for each entity type.
Numbers in brackets are the number of unique labels extractedLOC SUB TEMP R F1 P R F1 P R F1Baseline (B) 69.4 67.4 68.4 69.6 62.3 65.7 82.3 81.4 81.8B+ GAZ_original 69.0 72.1 70.5 69.7 65.4 67.5 82.3 82.7 82.5B+ GAZ_EXTfirstsent 69.9 76.7 73.1 70.0 68.3 69.1 82.6 84.6 83.6B+ EXTcategory 69.1 75.1 72.0 68.8 67.0 67.9 82.0 83.7 82.8B+ EXTunion 68.9 75.0 71.8 69.8 66.5 68.1 82.4 83.4 82.9B+ EXTintersect 69.3 76.2 72.6 69.7 67.6 68.6 82.6 84.3 83.4IAA - - 75.3 - - 63.6 - - 79.9Table 4.
Experimental results showing accuracy of systems in the entity extraction task for each type of entities,varying the feature set used.
Baseline performances are marked in italic.
Better performances than baselinesachieved by our systems are highlighted in bold.between the annotators on a shared sample cor-pus of the same kind as that for building the sys-tem, calculated using the metric by Hripcsak andRothschild (2005).
The metric is equivalent toscoring one annotator against the other using theF1 metric, and in practice system performancecan be slightly higher than IAA (Roberts et al2008).
The IAA figures for all types of entitiesare low, indicating that the entity extraction taskfor the archaeological domain is difficult, whichis consistent with Byrne (2007)?s finding.5 DiscussionAs shown in Table 2, our methods have generat-ed domain specific gazetteers that almostdoubled the original seed gazetteers in every oc-casion, even for the smallest seed gazetteer ofTEM.
This proves our hypotheses formulated insection 3.1, that by utilizing the hyperonymy re-lation and exploring information in an externalresource, one can extend a gazetteer by entitiesof similar types without utilizing language- anddomain-specific knowledge.
Also by taking theintersection of entities generated by the two labe-ling methods (bottom row of table 2), we see thatthe overlap is relatively small (from 30%-40% ofthe list generated by either method), indicatingthat the extended gazetteers produced by the twomethods are quite different, and may be used tocomplement each other.
Combining figures inTable 3, we see that both methods extract fine-grained type-labels that on average extract 4 - 14candidate entities.The quality of the gazetteers can be checkedusing the figures in Table 4.
First, all extendedgazetteers improved over the baselines for thethree entity types, with the highest increase in F1of 4.7%, 3.4% and 1.8% for LOC, SUB, and7TEM respectively.
In addition, they all outper-form the original gazetteers, indicating that thequality of extended gazetteers is good for theentity extraction task.By comparing the effects of each extendedgazetteer, we notice that using the gazetteersbuilt with type-labels extracted from the firstsentence of Wikipedia article always outper-forms using those built via the Wikipedia catego-ries, indicating that the first method (FirstSen-tenceLabeling) results in better quality gazet-teers.
This is due to two reasons.
First, the cate-gory tree in Wikipedia is not a strict taxonomy,and does not always contain is-a relationships(Strube and Ponzetto, 2006).
Although we haveeliminated categories that are extracted for onlyone seed entity, the results indicate the extendedgazetteers are still noisier than those built byFirstSentenceLabeling.
To illustrate, the articlesfor SUB seed entities ?quiver?
and ?arrowhead?are both categorized under ?Archery?, whichpermits noisy candidates such as ?Bowhunting?,?Camel archer?
and ?archer?.
Applying a stricterfiltering threshold may resolve this problem.Second, compared to Wikipedia categories, thelabels extracted from the first sentences aresometimes very fine-grained and restrictive.
Forexample, the labels extracted for ?Buckingham-shire?
from the first sentence are ?ceremonialHome County?
and ?Non-metropolitan County?,both of which are UK-specific LOC concepts.These rather restrictive labels help control thegazetteer expansion within the domain of inter-est.
The better performance with FirstSentence-Labeling indicates that such restrictions haveplayed a positive role in reducing noise in thelabels generated, and then improving the qualityof candidate entities.We also tested effects of combining the twoapproaches, and noticed that taking the intersec-tion of gazetteers generated by the two ap-proaches outperform the union, but figures arestill lower than the single best method.
This isunderstandable because by permitting membersof noisier gazetteers the system performance de-grades.6 ConclusionWe have presented a novel language- and do-main- independent approach for automaticallygenerating domain-specific gazetteers for entityrecognition tasks using Wikipedia.
Unlike pre-vious approaches, our approach makes use ofricher content and structural elements of Wikipe-dia.
By applying this approach to a corpus of theArchaeology domain, we empirically observed asignificant improvement in system accuracywhen compared with the baseline systems, andthe baselines plus original gazetteers.The extensibility and domain adaptability ofour methods still need further investigation.
Inparticular, our methods can be extended to intro-duce several statistical filtering thresholds tocontrol the label generation and candidate entityextraction in an attempt to reduce noise; also theeffect of recursively crawling Wikipedia articlesin the candidate extraction stage is worth study-ing.
Additionally, it would be interesting to studyother structures of Wikipedia, such as list struc-tures and info boxes, in gazetteer generation.
Infuture we will investigate into these possibilities,and also test our approach in different domains.AcknowledgementThis work is funded by the Archaeotools12 project thatis carried out by Archaeology Data Service, Universi-ty of York, UK and the Organisation, Information andKnowledge Group (OAK) of University of Sheffield,UK.ReferencesAhmed Amrani, Vichken Abajian, Yves Kodratoff,and Oriane Matte-Tailliez.
2008.
A Chain of Text-mining to Extract Information in Archaeology.
InProceedings of Information and CommunicationTechnologies: From Theory to Applications, ICT-TA 2008, 1-5.Razva Bunescu and Marius Pa?ca.
Using Encycloped-ic Knowledge for Named Entity Disambiguation.In Proceedings of EACL2006Kate Byrne.
Nested Named Entity Recognition inHistorical Archive Text.
In Proceedings of Interna-tional Conference on Semantic Computing, 2007.Evgeniy Gabrilovich and Shaul Markovitch.
Over-coming the Brittleness Bottleneck using Wikipe-dia: Enhancing Text Categorization with Encyclo-pedic Knowledge.
In Proceedings of the Twenty-First National Conference on Artificial Intelli-gence, 1301-1306, Boston, 2006.Jim Giles.
Internet Encyclopedias Go Head to Head.In Nature 438.
2005.
900-901.Mark Greengras, Sam Chapman, Jamie McLaughlin,Ravish Bhagdev and Fabio Ciravegna.
FindingNeedles in Haystacks: Data-mining in DistributedHistorical Datasets.
In The Virtual Representationof the Past.
London, Ashgate.
200812 http://ads.ahds.ac.uk/project/archaeotools/8George Hripcsak and Adam S. Rothschild.
Agree-ment, the F-measure and Reliability in InformationRetrieval: In Journal of the American Medical In-formatics Association, 296-298.
2005Todd Holloway, Miran Bozicevic and Katy B?rner.Analyzing and Visualizing the Semantic Coverageof Wikipedia and its Authors.
In Complexity, Vo-lumn 12, issue 3, 30-40.
2007Stuart Jeffrey, Julian Richards, Fabio Ciravegna, Ste-wart Waller, Sam Chapman and Ziqi Zhang.
2009.The Archaeotools project: Faceted Classificationand Natural Language Processing in an Archaeo-logical Context.
To appear in special Theme Issuesof the Philosophical Transactions of the Royal So-ciety A,"Crossing Boundaries: ComputationalScience, E-Science and Global E-Infrastructures".Jun?ichi Kazama and Kentaro Torisawa.
2008.
Induc-ing Gazetteers for Named Entity Recognition byLarge-scale Clustering of Dependency Relations.In Proceedings of ACL-2008: HLT, 407-415.Jun?ichi Kazama and Kentaro Torisawa.
ExplotingWikipedia as External Knowledge for Named Enti-ty Recognition.
In Proceedings of EMNLP-2007and Computational Natural Language Learning2007.
698-707.Zornista Kozareva.
2006.
Bootstrapping Named Enti-ty Recognition with Automatically Generated Ga-zetteer Lists.
In EACL-2006-SRW.Bernardo Magnini, Matto Negri, Roberto Prevete andHristo Tanev.
AWordNet-Based Approach toNamed Entity Recognition.
In Proceedings ofCOLING-2002 on SEMANET: building and usingsemantic networks.
1-7Diana Maynard, Kalina Bontcheva and Hamish Cun-ningham.
Automatic Language-Independent Induc-tion of Gazetteer Lists.
In Proceedings ofLREC2004.Tara Murphy, Tara Mcintosh and James R Curran.Named Entity Recognition for Astronomy Litera-ture.
In Proceedings of the Australasian LanguageTechnology Workshop, 2006.Chikashi Nobata, Nigel Collier and Jun?ichi Tsujii.Comparison between Tagged Corpora for theNamed Entity Task.
In Proceedings of the Work-shop on Comparing Corpora at ACL2000.Ellen Riloff and Rosie Jones.
1999.
Learning Dictio-naries for Information Extraction by Multi-levelBootstrapping.
In Proceedings of the Sixteenth Na-tional Conference on Artificial Intelligence, 474-479.Angus Roberts, Robert Gaizauskas, Mark Hepple andYikun Guo.
Combining Terminology Resourcesand Statistical Methods for Entity Recognition: anEvaluation.
In Proceedings of LREC2008.Hinrich Sch?tze and Jan O. Pedersen.
A co-occurrence-based thesaurus and two applications toInformation Retrieval.
In Information Processingand Management: an International Journal, 1997.33(3): 307-318Michael Strube and Simone Paolo Ponzetto.
WikiRe-late!
Computing Semantic Relatedness Using Wi-kipedia.
In Proceedings of the 21st National Confe-rence on Artificial Intelligence, 2006.
1419 - 1424Partha Pratim Talukdar, Thorsten Brants, Mark Li-berman and Fernando Pereira.
2006.
A ContextPattern Induction Method for Named Entity Ex-traction.
In Proceedings of CoNLL-2006, 141-148.Antonio Toral and Rafael Mu?oz.
2006.
A Proposalto Automatically Build and Maintain Gazetteers forNamed Entity Recognition by using Wikipedia.
InProceedings of Workshop on New Text, 11th Con-ference of the European Chapter of the Associationfor Computational Linguistics 2006.Andreas Vlachos.
Evaluating and Combining Bio-medical Named Entity Recognition Systems.
InWorkshop: Biological translational and clinicallanguage processing.
20079
