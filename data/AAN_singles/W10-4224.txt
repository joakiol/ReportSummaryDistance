Anchor-Progression in Spatially Situated Discourse:a Production ExperimentHendrik Zender and Christopher Koppermann and Fai Greeve and Geert-Jan M. KruijffLanguage Technology LabGerman Research Center for Artificial Intelligence (DFKI)Saarbru?cken, Germanyzender@dfki.deAbstractThe paper presents two models for produc-ing and understanding situationally appro-priate referring expressions (REs) duringa discourse about large-scale space.
Themodels are evaluated against an empiricalproduction experiment.1 Introduction and BackgroundFor situated interaction, an intelligent systemneeds methods for relating entities in the world,its representation of the world, and the natural lan-guage references exchanged with its user.
Hu-man natural language processing and algorithmicapproaches alike have been extensively studiedfor application domains restricted to small visualscenes and other small-scale surroundings.
Still,rather little research has addressed the specific is-sues involved in establishing reference to entitiesoutside the currently visible scene.
The challengethat we address here is how the focus of attentioncan shift over the course of a discourse if the do-main is larger than the currently visible scene.The generation of referring expressions (GRE)has been viewed as an isolated problem, focussingon efficient algorithms for determining which in-formation from the domain must be incorporatedin a noun phrase (NP) such that this NP allowsthe hearer to optimally understand which referentis meant.
The domains of such approaches usu-ally consist of small, static domains or simple vi-sual scenes.
In their seminal work Dale and Reiter(1995) present the Incremental Algorithm (IA) forGRE.
Recent extensions address some of its short-comings, such as negated and disjoined properties(van Deemter, 2002) and an account of salience forgenerating contextually appropriate shorter REs(Krahmer and Theune, 2002).
Other, alternativeGRE algorithms exist (Horacek, 1997; Bateman,1999; Krahmer et al, 2003).
However, all these al-gorithms rely on a given domain of discourse con-stituting the current context (or focus of attention).The task of the GRE algorithm is then to single outthe intended referent against the other members ofthe context, which act as potential distractors.
Aslong as the domains are such closed-context sce-narios, the intended referent is always in the cur-rent focus.
We address the challenge of producingand understanding of references to entities that areoutside the current focus of attention, because theyhave not been mentioned yet and are beyond thecurrently observable scene.Our approach relies on the dichotomy betweensmall-scale space and large-scale space for hu-man spatial cognition.
Large-scale space is ?aspace which cannot be perceived at once; its globalstructure must be derived from local observationsover time?
(Kuipers, 1977).
In everyday situa-tions, an office environment, one?s house, or a uni-versity campus are large-scale spaces.
A table-topor a part of an office are examples of small-scalespace.
Despite large-scale space being not fullyobservable, people can nevertheless have a rea-sonably complete mental representation of, e.g.,their domestic or work environments in their cog-nitive maps.
Details might be missing, and peo-ple might be uncertain about particular things andstates of affairs that are known to change fre-quently.
Still, people regularly engage in a con-versation about such an environment, making suc-cessful references to spatially located entities.It is generally assumed that humans adopt a par-tially hierarchical representation of spatial orga-nization (Stevens and Coupe, 1978; McNamara,1986).
The basic units of such a representationare topological regions (i.e., more or less clearlybounded spatial areas) (Hirtle and Jonides, 1985).Paraboni et al (2007) are among the few to ad-dress the issue of generating references to entitiesoutside the immediate environment, and presentan algorithm for context determination in hierar-......... ......office1 office4 office1floor1 floor2building 1A building 3Bold campuskitchen office2 helpdesk office3office5floor1 floor2 floor1building 2C building 3Bnew campusDienstag, 14.
April 2009(a) Example for a hierarchical representation of space.
(b) Illustration of the TA principle: starting from the atten-tional anchor (a), the smallest sub-hierarchy containing botha and the intended referent (r) is formed incrementally.Figure 1: TA in a spatial hierarchy.chically ordered domains.
However, since it ismainly targeted at producing textual references toentities in written documents (e.g., figures and ta-bles in book chapters), they do not address thechallenges of physical and perceptual situated-ness.
Large-scale space can be viewed as a hier-archically ordered domain.
To keep track of thereferential context in such a domain, in our previ-ous work we propose the principle of topologicalabstraction (TA, summarized in Fig.
1) for contextextension (Zender et al, 2009a), similar to Ances-tral Search (Paraboni et al, 2007).
In (Zender etal., 2009b), we describe the integration of the ap-proach in an NLP system for situated human-robotdialogues and present two algorithms instantiatingthe TA principle for GRE and resolving referringexpressions (RRE), respectively.
It relies on twoparameters: the location of the intended referentr, and the attentional anchor a.
As discussed inour previous works, for single utterances the an-chor is the physical position where it is made (i.e.,the utterance situation (Devlin, 2006)).
Below, wepropose models for attentional anchor-progressionfor longer discourses about large-scale space, andevaluate them against real-world data.2 The ModelsIn order to account for the determination of theattentional anchor a, we propose a model calledanchor-progression A.
The model assumes thateach exophoric reference1 serves as attentionalanchor for the subsequent reference.
It is basedon observations on ?principles for anchoring re-source situations?
by Poesio (1993), where the ex-pression of movement in the domain determines1This excludes pronouns as well as other descriptions thatpick up an existing referent from the linguistic context.the updated current mutual focus of attention.
aand r are then passed to the TA algorithm.
Takinginto account the verbal behavior observed in ourexperiment, we also propose a refined model ofanchor-resetting R, where for each new turn (e.g.,a new instruction), the anchor is re-set to the utter-ance situation.
R leads to the inclusion of naviga-tional information for each first RE in a turn, thusreassuring the hearer of the focus of attention.3 The ExperimentWe are interested in the way the disambiguationstrategies change when producing REs during adiscourse about large-scale space versus discourseabout small-scale space.
In our experiment, wegathered a corpus of spoken instructions in twodifferent situations: small-scale space (SSS) andlarge-scale space (LSS).
We use the data to evalu-ate the utility of the A and R models.
We specifi-cally evaluate them against the traditional (global)model G in which the indented referent must besingled out from all entities in the domain.The cover story for the experiment was torecord spoken instructions to help improve aspeech recognition system for robots.
The partici-pants were asked to imagine an intelligent servicerobot capable of understanding natural languageand familiar with its environment.
The task of theparticipants was to instruct the robot to clean upa working space, i.e., a table-top (SSS) and an in-door environment (LSS) by placing target objects(cookies or balls) in boxes of the same color.
Theuse of color terms to identify objects was discour-aged by telling the participants that the robot is un-able to perceive color.
The stimuli consisted of 8corresponding scenes of the table-top and the do-mestic setting (cf.
Fig.
2).
In order to preclude thespecific phenomena of collaborative, task-orienteddialogue (cf., e.g., (Garrod and Pickering, 2004)),the participants had to instruct an imaginary recip-ient of orders.
The choice of a robot was made torule out potential social implications when imag-ining, e.g., talking to a child, a butler, or a friend.The SSS scenes show a bird?s-eye view of thetable including the robot?s position (similar to (Fu-nakoshi et al, 2004)).
The way the objects are ar-ranged allows to refer to their location with respectto the corners of the table, with plates as additionallandmarks.
The LSS scenes depict an indoor envi-ronment with a corridor and, parallel to SSS, fourrooms with tables as landmarks.
The scenes showTable 1: Example from the small-scale (1?2) and large-scale space (3?4) scenes in Fig.
2.1. nimm [das pla?tzchen unten links]mG,A , leg es [in die schachtel unten rechts auf dem teller]oG,A?take the cookie on the bottom left, put it into the bottom right box on the plate?2.
nimm [das pla?tzchen unten rechts]mG,oA , leg es [in die schachtel oben links auf dem teller]mG,A?take the cookie on the bottom right, put it into the top left box on the plate?3.
geh [ins wohnzimmer]mG,A,R und nimm [den ball]uG,mA,R und bring ihn [ins arbeitszimmer]mG,A,R , leg ihn [in diekiste auf dem tisch]uG,oA,R?go to the living room and take the ball and bring it to the study; put it into the box on the table?4.
und nimm [den ball]uG,R,mA und bring ihn [in die ku?che]mG,A,R und leg ihn [in die kiste auf dem boden]uG,mA,R?and take the ball and bring it to the kitchen and put it into the box on the floor?
(a) Small-scale space: squares represent small boxes,stars cookies, and white circles plates.ArbeitszimmerK?cheWohnzimmer Bad(b) Large-scale space: squares represent boxes placed on thefloor or on a table, circles represent balls, rooms are labeled.Figure 2: Two stimuli scenes from the experiment.the robot and the participant in the corridor.In order to gather more comparable data weopted for a within-participants approach.
Eachperson participated in the SSS treatment and in theLSS treatment.
To counterbalance potential carry-over effects, half of the participants were shownthe treatments in inverse order, and the sequenceof the 8 scenes in each treatment was varied in aprincipled way.
In order to make the participantsproduce multi-utterance discourses, they were re-quired to refer to all target object pairs.
The exactwording of their instructions was up to them.Participants were placed in front of a screen anda microphone into which they spoke their ordersto the imaginary robot, followed by a self-pacedkeyword after which the experimenter showed thenext scene.
The experiment was conducted in Ger-man and consisted of a pilot study (10 partici-pants) and the main part (19 female and 14 malestudents, aged 19?53, German native speakers).The data of three participants who did not behaveaccording to the instructions was discarded.
Theindividual sessions took 20?35 min., and the par-ticipants were paid for their efforts.Using the UAM CorpusTool software, tran-scriptions of the recorded spoken instructionswere annotated for occurrences of the linguisticphenomenon we are interested in, i.e., REs.
Sam-ples were cross-checked by a second annotator.REs were marked as shallow ?refex?
segments,i.e., complex NPs were not decomposed into theirconstituents.
Only definite NPs representing ex-ophoric REs (cf.
Sec.
2) qualify as ?refex?
seg-ments.
If a turn contained an indefinite NP, thewhole turn was discarded.
The ?refex?
segmentswere coded according to the amount of informa-tion they contain, and under which disambigua-tion model M ?
{G,A,R} (R only for LSS)they succeed in singling out the described refer-ent.
Following Engelhardt et al (2006), we dis-tinguish three types of semantic specificity.
A REis an over-description with respect to M (overM )if it contains redundant information, and it is anunder-description (underM ) if it is ambiguous ac-cording to M .
Minimal descriptions (minM ) con-tain just enough information to uniquely identifythe referent.
Table 1 shows annotated examples.4 ResultsThe collected corpus consists of 30 annotated ses-sions with 2 treatments comprising 8 scenes with4 turns.
In total, it contains 4,589 annotated REs,out of which only 83 are errors.
Except for theerror rate calculation, we only consider non-error?refex?
segments as the universe.
The SSS treat-Table 2: Mean frequencies (with standard deviation in italics) of minimal (min), over-descriptions(over), and under-descriptions (under) with respect to the models (A, R, G) in both treatments.overG overA overR minG minA minR underG underA underRsmall-scale 13.94% 34.45% 78.90% 60.11% 7.16% 5.43%space 15.85% 14.37% 17.66% 13.13% 12.07% 10.50%large-scale 6.81% 34.75% 20.06 % 68.04% 64.55% 76.73% 25.16% 0.69% 3.21%space 7.53% 12.13% 10.10% 17.87% 13.13% 10.66% 19.48% 1.72% 5.06%ment contains 1,902 ?refex?, with a mean numberof 63.4 and a std.
dev.
?=1.98 per participant.
Thiscorresponds to the expected number of 64 REs tobe uttered: 8 scenes ?
4 target object pairs.
TheLSS treatment contains 2,604 ?refex?
with an aver-age of 86.8 correct REs (?=18.19) per participant.As can be seen in Table 1 (3?4), this differenceis due to the participants?
referring to intermediatewaypoints in addition to the target objects.
Table 2summarizes the analysis of the annotated data.Overall, the participants had no difficulties withthe experiment.
The mean error rates are low inboth treatments: 1.78% (?=3.36%) in SSS, and1.80% (?=2.98%) in LSS.
A paired sample t-test of both scores for each participant shows thatthere is no significant difference between the errorrates in the treatments (p=0.985), supporting theclaim that both treatments were of equal difficulty.Moreover, a MANOVA shows no significant effectof treatment-order for the verbal behavior understudy, ruling out potential carry-over effects.Production experiments always exhibit a con-siderable variation between participants.
Whenmodeling natural language processing systems,one needs to take this into account.
A GRE com-ponent should produce REs that are easy to un-derstand, i.e., ambiguities should be avoided andover-descriptions should occur sparingly.
A GREalgorithm will always try to produce minimal de-scriptions.
The generation of an under-descriptionmeans a failure to construct an identifying RE,while over-descriptions are usually the result ofa globally ?bad?
incremental construction of thegenerated REs (as is the case, e.g., in the IA).
AnRRE component, on the other hand, should be ableto identify as many referents as possible by treat-ing as few as possible REs as under-descriptions.The analysis of the SSS data with respect toG establishes the baseline for a comparison withother experiments and GRE approaches.
13.9% ofthe REs contain redundant information (overG),compared to 21% in (Viethen and Dale, 2006).
Incontrast, however, our SSS scenes did not providethe possibility for producing more-than-minimalREs for every target object, which might accountfor the difference.
underG REs occur with a fre-quency of 7.2% in the SSS data.
Because under-descriptions result in the the hearer being unable toreliably resolve the reference, this means that therobot in our experiment cannot fulfill its task.
Thismight explain the difference to the 16% observedin the task-independent study by Viethen and Dale(2006).
The significantly (p<0.001) higher meanfrequency of minG than minA underpins that Gis an accurate model for the verbal behavior inSSS.
However, G does not fit the LSS data well.An RRE algorithm with model G would fail toresolve the intended referent in 1 out of 4 cases(cf.
underG in LSS).
With only 0.7% underAREs on average, A models the LSS data signifi-cantly better (p<0.001).
Still, there is is a highrate of overA REs.
In comparison, R yields asignificantly (p<0.001) lower amount of overR.The mean frequency of underR is significantly(p=0.010) higher than for underA, but still belowunderG in the SSS data.
With a mean frequencyof 76.7% minR, R models the data better thanboth G and A.
For the REs in LSS minR is inthe same range as minG for the REs in SSS.5 ConclusionsOverall, the data exhibit a high mean frequency ofover-descriptions.
However, since this means thatthe human-produced REs contain more informa-tion than minimally necessary, this does not nega-tively affect the performance of an RRE algorithm.For a GRE algorithm, however, a more cautiousapproach might be desirable.
In situated discourseabout LSS, we thus suggest that A is suitable forthe RRE task because it yields the least amountof unresolvable under-descriptions.
For the GREtask R is more appropriate.
It strikes a balancebetween producing short descriptions and supple-menting navigational information.AcknowledgmentsThis work was supported by the EU Project CogX(FP7-ICT-215181).
Thanks to Mick O?Donnellfor his support with the UAM CorpusTool.ReferencesJohn A. Bateman.
1999.
Using aggregation for select-ing content when generating referring expressions.In Proceedings of the 37th annual meeting of the As-sociation for Computational Linguistics on Compu-tational Linguistics (ACL?99), pages 127?134, Mor-ristown, NJ, USA.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean Maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Keith Devlin.
2006.
Situation theory and situation se-mantics.
In Dov M. Gabbay and John Woods, edi-tors, Logic and the Modalities in the Twentieth Cen-tury, volume 7 of Handbook of the History of Logic,pages 601?664.
Elsevier.Paul E. Engelhardt, Karl G.D. Bailey, and FernandaFerreira.
2006.
Do speakers and listeners observethe Gricean Maxim of Quantity?
Journal of Mem-ory and Language, 54(4):554?573.Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,and Takenobu Tokunaga.
2004.
Generation ofrelative referring expressions based on perceptualgrouping.
In COLING ?04: Proceedings of the 20thinternational conference on Computational Linguis-tics, Morristown, NJ, USA.Simon Garrod and Martin J. Pickering.
2004.
Why isconversation so easy?
Trends in Cognitive Sciences,8(1):8?11, January.Stephen C. Hirtle and John Jonides.
1985.
Evidencefor hierarchies in cognitive maps.
Memory and Cog-nition, 13:208?217.Helmut Horacek.
1997.
An algorithm for generatingreferential descriptions with flexible interfaces.
InProceedings of the 35th Annual Meeting of the As-sociation for Computational Linguistics and EighthConference of the European Chapter of the Associa-tion for Computational Linguistics (ACL-97), pages206?213, Morristown, NJ, USA.Emiel Krahmer and Marie?t Theune.
2002.
Effi-cient context-sensitive generation of referring ex-pressions.
In Kees van Deemter and R. Kibble, ed-itors, Information Sharing: Givenness and Newnessin Language Processing, pages 223?264.
CSLI Pub-lications, Stanford, CA, USA.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.Benjamin Kuipers.
1977.
Representing Knowledge ofLarge-scale Space.
PhD thesis, MIT-AI TR-418,Massachusetts Institute of Technology, Cambridge,MA, USA, May.Timothy P. McNamara.
1986.
Mental representationsof spatial relations.
Cognitive Psychology, 18:87?121.Ivandre?
Paraboni, Kees van Deemter, and Judith Mas-thoff.
2007.
Generating referring expressions:Making referents easy to identify.
ComputationalLinguistics, 33(2):229?254, June.Massimo Poesio.
1993.
A situation-theoretic formal-ization of definite description interpretation in planelaboration dialogues.
In Peter Aczel, David Israel,Yasuhiro Katagiri, and Stanley Peters, editors, Sit-uation Theory and its Applications Volume 3, CSLILecture Notes No.
37, pages 339?374.
Center for theStudy of Language and Information, Menlo Park,CA, USA.Albert Stevens and Patty Coupe.
1978.
Distortionsin judged spatial relations.
Cognitive Psychology,10:422?437.Kees van Deemter.
2002.
Generating referring expres-sions: boolean extensions of the incremental algo-rithm.
Computational Linguistics, 28(1):37?52.Jette Viethen and Robert Dale.
2006.
Algorithmsfor generating referring expressions: Do they dowhat people do?
In Proceedings of the 4th Inter-national Natural Language Generation Conference(INLG 2006), pages 63?70, Sydney, Australia.Hendrik Zender, Geert-Jan M. Kruijff, and IvanaKruijff-Korbayova?.
2009a.
A situated contextmodel for resolution and generation of referring ex-pressions.
In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation (ENLG2009), pages 126?129, Athens, Greece, March.Hendrik Zender, Geert-Jan M. Kruijff, and IvanaKruijff-Korbayova?.
2009b.
Situated resolutionand generation of spatial referring expressions forrobotic assistants.
In Proceedings of the Twenty-First International Joint Conference on Artificial In-telligence (IJCAI-09), pages 1604?1609, Pasadena,CA, USA, July.
