Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1212?1221,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCrowdsourcing Interaction Logs to Understand Text Reuse from the WebMartin Potthast Matthias Hagen Michael V?lske Benno SteinBauhaus-Universit?t Weimar99421 Weimar, Germany<first name>.<last name>@uni-weimar.deAbstractWe report on the construction of the Webistext reuse corpus 2012 for advanced re-search on text reuse.
The corpus compilesmanually written documents obtained froma completely controlled, yet representativeenvironment that emulates the web.
Eachof the 297 documents in the corpus is aboutone of the 150 topics used at the TRECWeb Tracks 2009?2011, thus forming astrong connection with existing evaluationefforts.
Writers, hired at the crowdsourc-ing platform oDesk, had to retrieve sourcesfor a given topic and to reuse text fromwhat they found.
Part of the corpus aredetailed interaction logs that consistentlycover the search for sources as well as thecreation of documents.
This will allow forin-depth analyses of how text is composedif a writer is at liberty to reuse texts from athird party?a setting which has not beenstudied so far.
In addition, the corpus pro-vides an original resource for the evalua-tion of text reuse and plagiarism detectors,where currently only less realistic resourcesare employed.1 IntroductionThe web has become one of the most commonsources for text reuse.
When reusing text fromthe web, humans may follow a three step ap-proach shown in Figure 1: searching for appro-priate sources on a given topic, copying of textfrom selected sources, modification and paraphras-ing of the copied text.
A considerable body ofresearch deals with the detection of text reuse, and,in particular, with the detection of cases of plagia-rism (i.e., the reuse of text with the intent of disguis-ing the fact that text has been reused).
Similarly,a large number of commercial software systems isbeing developed whose purpose is the detection ofplagiarism.
Both the developers of these systems aswell as researchers working on the subject matterfrequently claim their approaches to be searchingthe entire web or, at least, to be scalable to websize.
However, there is hardly any evidence tosubstantiate this claim?rather the opposite can beobserved: commercial plagiarism detectors havenot been found to reliably identify plagiarism fromthe web (K?hler and Weber-Wulff, 2010), and theevaluation of research prototypes even under lab-oratory conditions shows that there is still a longway to go (Potthast et al, 2010b).
We explain thedisappointing state of the art by the lack of realistic,large-scale evaluation resources.With our work, we want to contribute to closingthe gap.
In this regard the paper in hand introducesthe Webis text reuse corpus 2012 (Webis-TRC-12),which, for the first time, emulates the entire processof reusing text from the web, both at scale and ina controlled environment.
The corpus comprises anumber of features that set it apart from previousones: (1) the topic of each document in the corpusis derived from a topic of the TREC Web Track,and the sources to copy from have been retrievedmanually from the ClueWeb corpus.
(2) The searchfor sources is logged, including click-through andbrowsing data.
(3) A fine-grained edit history hasbeen recorded for each document.
(4) A total of297 documents were written with an average lengthof about 5700 words, whereas diversity is ensuredvia crowdsourcing.
Altogether, this corpus formsthe current most realistic sample of writers reusingtext.
The corpus is publicly available.11.1 Related WorkAs organizers of the annual PAN plagiarism de-tection competitions,2 we have introduced the firststandardized evaluation framework for that pur-1http://www.webis.de/research/corpora2http://pan.webis.de1212Search I?m Feeling LuckySearch Copy & Paste ModificationFigure 1: The basic steps of reusing text from the web (Potthast, 2011).pose (Potthast et al, 2010b).
Among others, it com-prises a series of corpora that consist of automat-ically generated cases of plagiarism, provided inthe form of the PAN plagiarism corpora 2009-2011.The corpora have been used to evaluate dozens ofplagiarism detection approaches within the respec-tive competitions in these years;3 but even thoughthey have been adopted by the community, a num-ber of shortcomings render them less realistic:1.
All plagiarism cases were generated by ran-domly selecting text passages from documentsand inserting them at random positions in ahost document.
This way, the reused passagesdo not match the topic of the host document.2.
The majority of the reused passages were mod-ified in order to obfuscate the reuse.
However,the applied modification strategies, again, arebasically random: shuffling, replacing, insert-ing, or deleting words randomly.
An effortwas made to avoid non-readable text, yet noneof it bears any semantics.3.
The corpus documents are parts of books fromthe Project Gutenberg.
Many of these booksare pretty old, whereas today the web is thepredominant source for text reuse.To overcome the second issue, about 4 000 pas-sages were rewritten manually via crowdsourcingon Amazon?s Mechanical Turk for the 2011 cor-pus.
But, because of the first issue (random passageinsertion), a topic drift analysis can spot a reusedpassage more easily than a search within the doc-ument set containing the original source (Potthastet al, 2011).
From these observations it becomesclear that there are limits for the automatic con-struction of such kinds of corpora.
The Webis textreuse corpus 2012 addresses all of the mentionedissues since it has been constructed manually.3See (Potthast et al, 2009; Potthast et al, 2010a; Potthastet al, 2011) for overviews of approaches and evaluation resultsof each competition.Besides the PAN corpora, there are two othercorpora that comprise ?genuinely reused?
text: theClough09 corpus, and the Meter corpus.
The for-mer corpus consists of 57 answers to one of fivecomputer science questions that were reused froma respective Wikipedia article (Clough and Steven-son, 2011).
While the text was genuinely written bya number of volunteer students, the choice of topicsis narrow, and text lengths range from 200 to 300words, which is hardly more than 2-3 paragraphs.Also, the sources from which text was reused weregiven up front, so that there is no data about theirretrieval.
The Meter corpus annotates 445 casesof text reuse among 1 716 news articles (Clough etal., 2002).
The cases of text reuse in this corpusare realistic for the news domain; however, theyhave not been created by the reuse process outlinedin Figure 1.
Note that in the news domain, text isoften reused directly from a news wire without theneed for retrieval.
Our new corpus complementsthese two resources.2 Corpus ConstructionTwo data sets form the basis for constructing ourcorpus, namely (1) a set of topics to write aboutand (2) a set of web pages to research about a giventopic.
With regard to the former, we resort to topicsused at TREC, specifically to those used at the WebTracks 2009?2011.
With regard to the latter, we em-ploy the ClueWeb corpus from 20094 (and not the?web in the wild?).
The ClueWeb comprises morethan one billion documents from ten languages andcan be considered as a representative cross-sectionof the real web.
It is a widely accepted resourceamong researchers and became one of the primaryresources to evaluate the retrieval performance ofsearch engines within several TREC tracks.
Ourcorpus?s strong connection to TREC will allow forunforeseen synergies.
Based on these decisions our4http://lemurproject.org/clueweb091213corpus construction steps can be summarized asfollows:1.
Rephrasing of the 150 topics used at theTREC Web Tracks 2009?2011 so that theyexplicitly invite people to write an essay.2.
Indexing of the ClueWeb corpus category A(the entire English portion with about 0.5 bil-lion documents) using the BM25F retrievalmodel plus additional features.3.
Development of a search interface that allowsfor answering queries within milliseconds andthat is designed along the lines of commercialsearch interfaces.4.
Development of a browsing API for theClueWeb, which serves ClueWeb pages ondemand and which rewrites links of deliveredpages, now pointing to their correspondingClueWeb pages on our servers (instead of tothe originally crawled URL).5.
Recruiting 27 writers, 17 of whom with aprofessional writing background, hired at thecrowdsourcing platform oDesk from a widerange of hourly rates for diversity.6.
Instructing the writers to write one essay ata time of at least 5000 words length (cor-responding to an average student?s home-work assignment) about an open topic oftheir choice, using our search engine?hencebrowsing only ClueWeb pages.7.
Logging all writers?
interactions with thesearch engine and the ClueWeb on a per-essaybasis at our site.8.
Logging all writers?
edits to their essays in afine-grained edit log: a snapshot was takenwhenever a writer stopped writing for morethan 300ms.9.
Double-checking all of the essays for quality.After having deployed the search engine andcompleted various usability tests, the actual corpusconstruction took nine months, from April 2012through December 2012.Obviously, the outlined experimental setup canserve different lines of research and is publiclyavailable as well.
The remainder of the sectionpresents elements of our setup in greater detail.2.1 Topic PreparationSince the topics used at the TREC Web Tracks werenot amenable for our purpose as is, we rephrasedthem so that they ask for writing an essay instead ofsearching for facts.
Consider for example topic 001of the TREC Web Track 2009:Query.
obama family treeDescription.
Find information on Pres-ident Barack Obama?s family history,including genealogy, national origins,places and dates of birth, etc.Sub-topic 1.
Find the TIME magazinephoto essay ?Barack Obama?s FamilyTree.
?Sub-topic 2.
Where did Barack Obama?sparents and grandparents come from?Sub-topic 3.
Find biographical informa-tion on Barack Obama?s mother.This topic is rephrased as follows:Obama?s family.
Write about PresidentBarack Obama?s family history, includ-ing genealogy, national origins, placesand dates of birth, etc.
Where did BarackObama?s parents and grandparents comefrom?
Also include a brief biography ofObama?s mother.In the example, Sub-topic 1 is considered toospecific for our purposes while the other sub-topicsare retained.
TREC Web Track topics divide intofaceted and ambiguous topics.
While topics ofthe first kind can be directly rephrased into essaytopics, from topics of the second kind one of theavailable interpretations was chosen.2.2 A Controlled Web Search EnvironmentTo give the oDesk writers a familiar search experi-ence while maintaining reproducibility at the sametime, we developed a tailored search engine calledChatNoir (Potthast et al, 2012b).5 Besides ours,the only other public search engine for the ClueWebis Carnegie Mellon?s Indri,6 which, unfortunately,is far from our efficiency requirements.
Moreover,its search interface does not follow the standard interms of result page design, and it does not giveaccess to interaction logs.
Our search engine ison the order of milliseconds in terms of retrieval5http://chatnoir.webis.de6http://lemurproject.org/clueweb09.php/index.php#Services1214time, its interface follows industry standards, andit features an API that allows for user tracking.ChatNoir is based on the BM25F retrievalmodel (Robertson et al, 2004), uses the anchortext list provided by (Hiemstra and Hauff, 2010),the PageRanks provided by the Carnegie MellonUniversity alongside the ClueWeb corpus, and theSpam rank list provided by (Cormack et al, 2011).ChatNoir comes with a proximity feature withvariable-width buckets as described by (Elsayedet al, 2011).
Our choice of retrieval model andranking features is intended to provide a reasonablebaseline performance.
However, it is neither nearas mature as those of commercial search enginesnor does it compete with the best-performing mod-els from TREC.
Yet, it is among the most widelyaccepted models in information retrieval, whichunderlines our goal of reproducibility.In addition to its retrieval model, ChatNoir im-plements two search facets: text readability scoringand long text search.
The first facet, similar to thatprovided by Google, scores the readability of a textfound on a web page via the well-known Flesch-Kincaid grade level formula (Kincaid et al, 1975):it estimates the number of years of education re-quired in order to understand a given text.
Thisnumber is mapped onto the three categories ?Sim-ple?
(up to 5 years), ?Intermediate?
(between 5 and9 years) and ?Expert?
(at least 9 years).
The ?LongText?
search facet omits search results which donot contain at least one continuous paragraph oftext that exceeds 300 words.
The two facets can becombined with each other.When clicking on a search result, ChatNoir doesnot link into the real web but redirects into theClueWeb.
Though the ClueWeb provides the orig-inal URLs from which the web pages have beenobtained, many of these pages have gone or beenupdated since.
We hence set up an API that servesweb pages from the ClueWeb on demand: whenaccessing a web page, it is pre-processed beforebeing shipped, removing automatic referrers andreplacing all links to the real web with links totheir counterpart inside the ClueWeb.
This way,the ClueWeb can be browsed as if surfing the realweb, whereas it becomes possible to track a user.The ClueWeb is stored in the HDFS of our 40 nodeHadoop cluster, and web pages are fetched directlyfrom there with latencies of about 200ms.
Chat-Noir?s inverted index has been optimized to guaran-tee fast response times, and it is deployed alongsideHadoop on the same cluster.Table 1: Demographics of the 12 Batch 2 writers.Writer DemographicsAge Gender Native language(s)Minimum 24 Female 67% English 67%Median 37 Male 33% Filipino 25%Maximum 65 Hindi 17%Academic degree Country of origin Second language(s)Postgraduate 41% UK 25% English 33%Undergraduate 25% Philippines 25% French 17%None 17% USA 17% Afrikaans, Dutch,n/a 17% India 17% German, Spanish,Australia 8% Swedish each 8%South Africa 8% None 8%Years of writing Search engines used Search frequencyMinimum 2 Google 92% Daily 83%Median 8 Bing 33% Weekly 8%Standard dev.
6 Yahoo 25% n/a 8%Maximum 20 Others 8%2.3 Two Batches of WritingIn order to not rely only on the retrieval modelimplemented in our controlled web search envi-ronment, we divided the task into two batches, sothat two essays had to be written for each of the150 topics, namely one in each batch.
In Batch 1,our writers did not search for sources themselves,but they were provided up front with an averageof 20 search results to choose from for each topic.These results were obtained from the TREC WebTrack relevance judgments (so-called ?qrels?
): onlydocuments that were found to be relevant or keydocuments for a given topic by manual inspectionof the NIST assessors were provided to our writ-ers.
These documents result from the combinedwisdom of all retrieval models of the TREC WebTracks 2009?2011, and hence can be consideredas optimum retrieval results produced by the stateof the art in search engine technology.
In Batch 2,in order to obtain realistic search interaction logs,our writers were instructed to search for sourcedocuments using ChatNoir.2.4 Crowdsourcing WritersOur ideal writer has experience in writing, is ca-pable of writing about a diversity of topics, cancomplete a text in a timely manner, possesses de-cent English writing skills, and is well-versed inusing the aforementioned technologies.
After boot-strapping our setup with 10 volunteers recruited atour university, it became clear that, because of theworkload involved, accomplishing our goals wouldnot be possible with volunteers only.
Therefore, weresorted to hiring (semi-)professional writers andmade use of the crowdsourcing platform oDesk.7Crowdsourcing has quickly become one of the7http://www.odesk.com1215Table 2: Key figures of the Webis text reuse corpus 2012.Corpus Distribution Totalcharacteristic min avg max stdevWriters (Batch 1+2) 27Essays (Topics) (Two essays per topic) 297 (150)Essays / Writer 1 2 66 15.9Queries (Batch 2) 13 655Queries / Essay 4 91.0 616 83.1Clicks (Batch 2) 16 739Clicks / Essay 12 111.6 443 80.3Clicks / Query 1 2.3 76 3.3Irrelevant (Batch 2) 5 962Irrelevant / Essay 1 39.8 182 28.7Irrelevant / Query 0 0.5 60 1.4Relevant (Batch 2) 251Relevant / Essay 0 1.7 7 1.5Relevant / Query 0 0.0 4 0.2Key (Batch 2) 1 937Key / Essay 1 12.9 46 7.5Key / Query 0 0.2 22 0.7Corpus Distribution Totalcharacteristic min avg max stdevSearch Sessions (Batch 2) 931Sessions / Essay 1 12.3 149 18.9Days (Batch 2) 201Days / Essay 1 4.9 17 2.7Hours (Batch 2) 2 068Hours / Writer 3 129.3 679 167.3Hours / Essay 3 7.5 10 2.5Edits (Batch 1+2) 633 334Edits / Essay 45 2 132.4 6 975 1 444.9Edits / Day 5 2 959.5 8 653 1 762.5Words (Batch 1+2) 1 704 354Words / Essay 260 5 738.8 15 851 1 604.3Words / Writer 2 078 63 124.2 373 975 89 246.7Sources (Batch 1+2) 4 582Sources / Essay 0 15.4 69 10.0Sources / Writer 5 169.7 1 065 269.6cornerstones for constructing evaluation corpora,which is especially true for paid crowdsourcing.Compared to Amazon?s Mechanical Turk (Barrand Cabrera, 2006), which is used more frequentlythan oDesk, there are virtually no workers at oDesksubmitting fake results because of its advanced rat-ing features for workers and employers.
Moreover,oDesk tracks their workers by randomly takingscreenshots, which are provided to employers in or-der to check whether the hours logged correspondto work-related activity.
This allowed us to checkwhether our writers used our environment insteadof other search engines and editors.During Batch 2, we have conducted a surveyamong the twelve writers who worked for us atthat time.
Table 1 gives an overview of the demo-graphics of these writers, based on a questionnaireand their resumes at oDesk.
Most of them comefrom an English-speaking country, and almost allof them speak more than one language, which sug-gests a reasonably good education.
Two thirds ofthe writers are female, and all of them have yearsof writing experience.
Hourly wages were negoti-ated individually and range from 3 to 34 US dollars(dependent on skill and country of residence), withan average of about 12 US dollars.
For ethical rea-sons, we payed at least the minimum wage of therespective countries involved.
In total, we spent20 468 US dollars to pay the writers?an amountthat may be considered large compared to otherscientific crowdsourcing efforts from the literature,but small in terms of the potential of crowdsourcingto make a difference in empirical science.3 Corpus AnalysisThis section presents selected results of a prelim-inary corpus analysis.
We overview the data andshed some light onto the search and writing behav-ior of writers.3.1 Corpus StatisticsTable 2 shows key figures of the collected inter-action logs, including the absolute numbers ofqueries, relevance judgments, working times, num-ber of edits, words, and retrieved sources, as wellas their relation to essays, writers, and work time,where applicable.
On average, each writer wrote2 essays while the standard deviation is 15.9, sinceone very prolific writer managed to write 66 essays.From a total of 13 655 queries submitted by thewriters within Batch 2, each essay got an aver-age of 91 queries.
The average number of resultsclicked per query is 2.3.
For comparison, we com-puted the average number of clicks per query inthe AOL query log (Pass et al, 2006), which is 2.0.In this regard, the behavior of our writers on indi-vidual queries does not differ much from that ofthe average AOL user in 2006.
Most of the clicksthat we recorded are search result clicks, whereas2 457 of them are browsing clicks on web pagelinks.
Among the browsing clicks, 11.3% are clickson links that point to the same web page (i.e., an-chor links using the hash part of a URL).
Thelongest click trail contains 51 unique web pages,but most trails are very short.
This is a surprisingresult, since we expected a larger proportion ofbrowsing clicks, but it also shows that our writers1216relied heavily on the ChatNoir?s ranking.
Regard-ing search facets, we observed that our writers usedthem only for about 7% of their queries.
In thesecases, the writers used either the ?Long Text?
facet,which retrieves web pages containing at least onecontinuous passage of at least 300 words, or set thedesired reading level to ?Expert.
?The query log of each writer in Batch 2 dividesinto 931 search sessions with an average of 12.3 ses-sions per topic.
Here, a session is defined as a se-quence of queries recorded for a given topic whichis not divided by a break longer than 30 minutes.Despite other claims in the literature (Jones andKlinkner, 2008; Hagen et al, 2013) we argue that,in our case, sessions can be reliably identified bytimeouts because we have a priori knowledge aboutwhich query belongs to which essay.
Typically,completing an essay took 4.9 days, which includesto a long-lasting exploration of the topic at hand.The 297 essays submitted within the two batcheswere written with a total of 633 334 edits.
Eachtopic was edited 2 132 times on average, whereasthe standard deviation gives an idea about howdiverse the modifications of the reused text were.Writers were not specifically instructed to modify atext as much as possible?rather they were encour-aged to paraphrase in order to foreclose the detec-tion by an automatic text reuse detector.
This way,our corpus captures each writer?s idea of the nec-essary modification effort to accomplish this goal.The average lengths of the essays is 5 739 words,but there are also some short essays if hardly anyuseful information could be found on the respectivetopics.
About 15 sources have been reused in eachessay, whereas some writers reused text from asmany as 69 unique documents.3.2 Relevance JudgmentsIn the essays from Batch 2, writers reused textsfrom web pages they found during their search.This forms an interesting relevance signal whichallows us to separate web pages relevant to a giventopic from those which are irrelevant.
Followingthe terminology of TREC, we consider web pagesfrom which text is reused as key documents forthe respective essay?s topic, while web pages thatare on a click trail leading to a key document aretermed relevant.
The unusually high number ofkey documents compared to relevant documentsis explained by the fact that there are only fewclick trails of this kind, whereas most web pagesTable 3: Confusion matrix of TREC judgmentsversus writer judgments.TREC Writer judgmentjudgment irrelevant relevant key unjudgedspam (-2) 3 0 1 2 446spam (-1) 64 4 18 16 657irrelevant (0) 219 13 73 33 567relevant (1) 114 8 91 10 676relevant (2) 44 5 56 3 711key (3) 12 0 8 526unjudged 5 506 221 1 690 ?have been retrieved directly.
The remainder of webpages that were viewed but discarded by our writersare considered as irrelevant.Each year, the NIST assessors employed for theTREC conference manually review hundreds ofweb pages that have been retrieved by experimentalretrieval systems that are submitted to the variousTREC tracks.
This was also the case for the TRECWeb Tracks from which the topics of our corpusare derived.
We have compared the relevance judg-ments provided by TREC for these tracks with theimplicit judgments from our writers.
Table 3 con-trasts the two judgment scales in the form of a con-fusion matrix.
TREC uses a six-point Likert scaleranging from -2 (extreme Spam) to 3 (key docu-ment).
For 733 of the documents visited by ourwriters, TREC relevance judgments can be found.From these, 456 documents (62%) have been con-sidered irrelevant for the purposes of reuse by ourwriters, however, the TREC assessor disagree withthis judgment in 170 cases.
Regarding the docu-ments considered as key documents for reuse byour writers, the TREC assessors disagree on 92 ofthe 247 documents.
An explanation for the dis-agreement can be found in the differences betweenthe TREC ad hoc search task and our text reusetask: the information nuggets (small chunks oftext) that satisfy specific factual information needsfrom the original TREC topics are not the same asthe information ?ingots?
(big chunks of text) thatsatisfy our writers?
needs.3.3 Research BehaviorTo analyze the writers?
search behavior during es-say writing in Batch 2, we have recorded detailedsearch logs of their queries while they used oursearch engine.
Figure 2 shows for each of the150 essays of this batch a curve of the percentageof queries at times between a writer?s first queryand an essay?s completion.
We have normalizedthe time axis and excluded working breaks of more121716533201615821058701132318119282719623347109248401481531131543196426301820835243411428446526052486697501383642347034571206167410162326910641362810898474610555088481989421848198112762014717013956106323706074104514230111169441502744892155992415884181401354611818514291336117237824668033681216260766210822424275691814720830241731616522636960864427464AFEDCB1 5 10 15 20 25Figure 2: Spectrum of writer search behavior.
Each grid cell corresponds to one of the 150 essays ofBatch 2 and shows a curve of the percentage of submitted queries (y-axis) at times between the first queryuntil the essay was finished (x-axis).
The numbers denote the amount of queries submitted.
The cells aresorted by area under the curve, from the smallest area in cell A1 to the largest area in cell F25.than five minutes.
The curves are organized so asto highlight the spectrum of different search behav-iors we have observed: in row A, 70-90% of thequeries are submitted toward the end of the writ-ing task, whereas in row F almost all queries aresubmitted at the beginning.
In between, however,sets of queries are often submitted in the form of?bursts,?
followed by extended periods of writing,which can be inferred from the steps in the curves(e.g., cell C12).
Only in some cases (e.g., cell C10)a linear increase of queries over time can be ob-served for a non-trivial amount of queries, whichindicates continuous switching between searchingand writing.
From these observations, it can beinferred that our writers sometimes conducted a?first fit?
search and reused the first texts they foundeasily.
However, as the essay progressed and thelow hanging fruit in terms of search were used up,they had to search more intensively in order to com-plete their essay.
More generally, this data givesan idea of how humans perform exploratory searchin order to learn about a given topic.
Our currentresearch on this aspect focuses on the predictionof search mission types, since we observe that thesearch mission type does not simply depend on thewriter or the perceived topic difficulty.3.4 Visualizing Edit HistoriesTo analyze the writers?
writing style, that is tosay, how writers reuse texts and how the essayis completed in both batches, we have recordedthe edit logs of their essays.
Whenever a writerstopped writing for more than 300ms, a new editwas stored in a version control system at our site.The edit logs document the entire text evolution,from first the keystroke until an essay was com-pleted.
We have used the so-called history flowvisualization to analyze the writing process (Vi?-gas et al, 2004).
Figure 3 shows four examplesfrom the set of 297 essays.
Based on these visu-alizations, a number of observations can be made.In general, we identify two distinct writing-styletypes to perform text reuse, namely to build up anessay during writing, or, to first gather material andthen to boil down a text until the essay is completed.Later in this section, we will analyze this observa-tion in greater detail.
Within the plots, a numberof events can be spotted that occurred during writ-ing: in the top left plot, encircled as area A, theinsertion of a new piece of text can be observed.Though marked as original text at first, the writerworked on this passage and then revealed that itwas reused from another source.
At area B in thetop right plot, one can observe the reorganization oftwo passages as they exchange places from one editto another.
Area C in the bottom right plot showsthat the writer, shortly before completing this essay,reorganized substantial parts.
Area D in the sameplot shows how the writer went about boiling downthe text by incorporating contents from differentpassages that have been collected beforehand and,then, from one edit to another, discarded most ofthe rest.
The saw-tooth shaped pattern in area Ein the bottom left plot reveals that, even thoughthe writer of this essay adopts a build-up style, shestill pastes passages from her sources into the textone at a time, and then individually boils downeach.
Our visualizations also include informationabout the text positions where writers have beenworking at a given point in time; these positionsare shown as blue dots in the plots.
In this regarddistinct writing patterns are discernible of writerswho go through a text linearly versus those who donot.
Future work will include an analysis of thesewriting patterns.1218ABCDEFigure 3: Types of text reuse: build-up reuse (left) versus boil-down reuse (right).
Each plot shows the textlength at text edit between first keystroke and essay completion; edits have been recorded during writingwhenever a writer stopped for more than 300ms.
Colors encode different source documents.
Original textis white; blue dots indicate the text position of the writer?s last edit.3.5 Build-up Reuse versus Boil-down ReuseBased on the edit history visualizations, we havemanually classified the 297 essays of both batchesinto two categories, corresponding to the two stylesbuild-up reuse and boil-down reuse.
We foundthat 40% are instances of build-up reuse, 45% areinstances of boil-down reuse, and 13% fall in be-tween, excluding 2% of the essays as outliers dueto errors or for being too short.
The in-betweencases show that a writer actually started one wayand then switched to the respective other style ofreuse so that the resulting essays could not be at-tributed to a single category.
An important questionthat arises out of this observation is whether differ-ent writers habitually exert different reuse stylesor whether they apply them at random.
To obtaina better overview, we envision the applied reusestyle of an essay by the skyline curve of its edithistory visualization (i.e., by the curve that plotsthe length of an essay after each edit).
Aggregatingthese curves on a per-writer basis reveals distinctTable 4: Contingency table: writers over reuse style.Reuse Writer IDStyle A02A05A06A07A10A17A18A19A20A21A24build-up 4 27 11 4 9 13 12 4 9 18 2boil-down 52 5 0 14 2 13 11 3 0 0 24mixed 10 3 0 1 1 7 6 0 0 3 1patterns.
For eight of our writers Figure 4 showsthis characteristic.
The plots are ordered by theshape of the averaged curve, starting from a linearincrease (left) to a compound of steep increase toa certain length after which the curve levels out(right).
The former shape corresponds to writerswho typically apply build-up reuse, while the lat-ter can be attributed to writers who typically applyboil-down reuse.When comparing the plots we notice a very in-teresting effect: it appears that writers who conductboil-down reuse vary more wildly in their behavior.The reuse style of some writers, however, falls inbetween the two extremes.
Besides the visual anal-ysis, Table 4 shows the distribution of reuse styles1219Text length(%)Text length(%)A10 (12 essays) A18 (32 essays) A24 (27 essays)A21 (21 essays)A06 (12 essays) A17 (33 essays) A02 (66 essays)A05 (37 essays)Edits (%)Edits (%) Edits (%)Edits (%)build up boil downText reuse styleFigure 4: Text reuse styles ranging from build-up reuse (left) to boil-down reuse (right).
A gray curveshows the normalized length of an essay over the edits that went into it during writing.
Curves are groupedby writers.
The black curve marks the average of all other curves in a plot.for the eleven writers who contributed at least fiveessays.
Most writers use one style for about 80%of their essays, whereas two writers (A17, A18) areexactly on par between the two styles.
Based onPearson?s chi-squared test, one can safely reject thenull hypothesis that writers and text reuse stylesare independent: ?2 = 139.0 with p = 7.8 ?
10?20.Since our sample of authors and essays is sparse,Pearson?s chi-squared test may not be perfectlysuited which is why we have also applied Fisher?sexact test, which computes probability p = 0.0005that the null hypothesis is true.4 Summary and OutlookThis paper details the construction of the Webis textreuse corpus 2012 (Webis-TRC-12), a new corpusfor text reuse research that has been created en-tirely manually on a large scale.
We have recordedconsistent interaction logs of human writers with asearch engine as well as with the used text proces-sor; these logs serve the purpose of studying howtexts from the web are being reused for essay writ-ing.
Our setup is entirely reproducible: we havebuilt a static web search environment consisting ofa search engine along with a means to browse alarge corpus of web pages as if it were the ?real?web.
Yet, in terms of scale, this environment is rep-resentative of the real web.
Besides our corpus alsothis infrastructure is available to other researchers.The corpus itself goes beyond existing resources inthat it allows for a much more fine-grained analysisof text reuse, and in that it significantly improvesthe realism of the data underlying evaluations ofautomatic tools to detect text reuse and plagiarism.Our analysis gives an overview of selected as-pects of the new corpus.
This includes corpusstatistics about important variables, but also ex-ploratory studies of search behaviors and strategiesfor reusing text.
We present new insights about howtext is composed, revealing two types of writers:those who build up a text as they go, and those whofirst collect a lot of material which then is boileddown until the essay is finished.Parts of our corpus have been successfully em-ployed to evaluate plagiarism detectors in thePAN plagiarism detection competition 2012 (Pot-thast et al, 2012a).
Future work will include analy-ses that may help to understand the state of mind ofwriters when reusing text as well as of plagiarists.We also expect insights with regard to the develop-ment of algorithms for detection purposes and forlinguists studying the process of writing.AcknowledgementsWe thank our writers at oDesk and all volunteersfor their contribution.
We also thank Jan Gra?eggerand Martin Tippmann who kept the search engineup and running during corpus construction.1220ReferencesJeff Barr and Luis Felipe Cabrera.
2006.
AI gets abrain.
Queue, 4(4):24?29.Paul Clough and Mark Stevenson.
2011.
Develop-ing a corpus of plagiarised short answers.
LanguageResources and Evaluation, 45:5?24.Paul Clough, Robert Gaizauskas, Scott S. L. Piao, andYorick Wilks.
2002.
METER: MEasuring TExt Reuse.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL 2002),Philadelphia, PA, USA, July 6?12, 2002, pages 152?159.Gordon V. Cormack, Mark D. Smucker, and Charles L.A. Clarke.
2011.
Efficient and effective spam filteringand re-ranking for large web datasets.
InformationRetrieval, 14(5):441?465.Tamer Elsayed, Jimmy J. Lin, and Donald Metzler.2011.
When close enough is good enough: approxi-mate positional indexes for efficient ranked retrieval.In Proceedings of the 20th ACM Conference on Infor-mation and Knowledge Management (CIKM 2011),Glasgow, United Kingdom, October 24?28, 2011,pages 1993?1996.Matthias Hagen, Jakob Gomoll, Anna Beyer, andBenno Stein.
2013.
From Search Session Detection toSearch Mission Detection.
In Proceedings of the 10thInternational Conference Open Research Areas in In-formation Retrieval (OAIR 2013), Lisbon, Portugal,May 22?24, 2013, to appear.Djoerd Hiemstra and Claudia Hauff.
2010.
MIREX:MapReduce information retrieval experiments.
Tech-nical Report TR-CTIT-10-15, University of Twente.Rosie Jones and Kristina Lisa Klinkner.
2008.
Be-yond the session timeout: automatic hierarchical seg-mentation of search topics in query logs.
In Proceed-ings of the 17th ACM Conference on Information andKnowledge Management (CIKM 2008), Napa Valley,California, USA, October 26?30, 2008, pages 699?708.J.
Peter Kincaid, Robert P. Fishburne, Richard L.Rogers, and Brad S. Chissom.
1975.
Derivation ofnew readability formulas (automated readability index,Fog count and Flesch reading ease formula) for Navyenlisted personnel.
Research Branch Report 8-75,Naval Air Station Memphis, Millington, TN.Katrin K?hler and Debora Weber-Wulff.
2010.
Pla-giarism detection test 2010. http://plagiat.htw-berlin.de/wp-content/uploads/PlagiarismDetectionTest2010-final.pdf.Greg Pass, Abdur Chowdhury, and Cayley Torgeson.2006.
A picture of search.
In Proceedings of the 1stInternational Conference on Scalable InformationSystems (Infoscale 2006), Hong Kong, May 30?June 1,2006, paper 1.Martin Potthast, Benno Stein, Andreas Eiselt, AlbertoBarr?n-Cede?o, and Paolo Rosso.
2009.
Overviewof the 1st international competition on plagiarismdetection.
In SEPLN 2009 Workshop on UncoveringPlagiarism, Authorship, and Social Software Misuse(PAN 2009), pages 1?9.Martin Potthast, Alberto Barr?n-Cede?o, AndreasEiselt, Benno Stein, and Paolo Rosso.
2010a.Overview of the 2nd international competition onplagiarism detection.
In Working Notes Papers of theCLEF 2010 Evaluation Labs.Martin Potthast, Benno Stein, Alberto Barr?n-Cede?o,and Paolo Rosso.
2010b.
An evaluation frameworkfor plagiarism detection.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING 2010), Beijing, China, August 23?27,2010, pages 997?1005.Martin Potthast, Andreas Eiselt, Alberto Barr?n-Cede?o, Benno Stein, and Paolo Rosso.
2011.Overview of the 3rd international competition on pla-giarism detection.
In Working Notes Papers of theCLEF 2011 Evaluation Labs.Martin Potthast, Tim Gollub, Matthias Hagen, JanGra?egger, Johannes Kiesel, Maximilian Michel,Arnd Oberl?nder, Martin Tippmann, Alberto Barr?n-Cede?o, Parth Gupta, Paolo Rosso, and Benno Stein.2012a.
Overview of the 4th international competitionon plagiarism detection.
In Working Notes Papers ofthe CLEF 2012 Evaluation Labs.Martin Potthast, Matthias Hagen, Benno Stein, JanGra?egger, Maximilian Michel, Martin Tippmann, andClement Welsch.
2012b.
ChatNoir: a search enginefor the ClueWeb09 corpus.
In Proceedings of the35th International ACM Conference on Research andDevelopment in Information Retrieval (SIGIR 2012),Portland, OR, USA, August 12?16, 2012, page 1004.Martin Potthast.
2011.
Technologies for ReusingText from the Web.
Dissertation, Bauhaus-Universit?tWeimar.Stephen E. Robertson, Hugo Zaragoza, and Michael J.Taylor.
2004.
Simple BM25 extension to multipleweighted fields.
In Proceedings of the 13th ACM Con-ference on Information and Knowledge Management(CIKM 2004), Washington, DC, USA, November 8?13,2004, pages 42?49.Fernanda B. Vi?gas, Martin Wattenberg, and KushalDave.
2004.
Studying cooperation and conflict be-tween authors with history flow visualizations.
In Pro-ceedings of the 2004 Conference on Human Factorsin Computing Systems (CHI 2004), Vienna, Austria,April 24?29, 2004, pages 575?582.1221
