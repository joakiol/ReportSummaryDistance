Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320?1331,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Semantic Role Induction with Graph PartitioningJoel Lang and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKJ.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we present a method for unsuper-vised semantic role induction which we for-malize as a graph partitioning problem.
Ar-gument instances of a verb are represented asvertices in a graph whose edge weights quan-tify their role-semantic similarity.
Graph par-titioning is realized with an algorithm that it-eratively assigns vertices to clusters based onthe cluster assignments of neighboring ver-tices.
Our method is algorithmically and con-ceptually simple, especially with respect tohow problem-specific knowledge is incorpo-rated into the model.
Experimental results onthe CoNLL 2008 benchmark dataset demon-strate that our model is competitive with otherunsupervised approaches in terms of F1 whilstattaining significantly higher cluster purity.1 IntroductionRecent years have seen increased interest in the shal-low semantic analysis of natural language text.
Theterm is most commonly used to describe the au-tomatic identification and labeling of the seman-tic roles conveyed by sentential constituents (Gildeaand Jurafsky, 2002).
Semantic roles describe the se-mantic relations that hold between a predicate andits arguments (e.g., ?who?
did ?what?
to ?whom?,?when?, ?where?, and ?how?)
abstracting over sur-face syntactic configurations.In the example sentences below, window occu-pies different syntactic positions ?
it is the object ofbroke in sentences (1a,b), and the subject in (1c) ?while bearing the same semantic role, i.e., the phys-ical object affected by the breaking event.
Analo-gously, ball is the instrument of break both whenrealized as a prepositional phrase in (1a) and as asubject in (1b).
(1) a.
[Jim]A0 broke the [window]A1 with a[ball]A2.b.
The [ball]A2 broke the [window]A1.c.
The [window]A1 broke [last night]TMP.The semantic roles in the examples are labeled inthe style of PropBank (Palmer et al, 2005), a broad-coverage human-annotated corpus of semantic rolesand their syntactic realizations.
Under the Prop-Bank annotation framework (which we will assumethroughout this paper) each predicate is associatedwith a set of core roles (named A0, A1, A2, and soon) whose interpretations are specific to that predi-cate1 and a set of adjunct roles such as location ortime whose interpretation is common across predi-cates (e.g., last night in sentence (1c)).The availability of PropBank and related re-sources (e.g., FrameNet; Ruppenhofer et al (2006))has sparked the development of great many seman-tic role labeling systems most of which conceptu-alize the task as a supervised learning problem andrely on role-annotated data for model training.
Mostof these systems implement a two-stage architec-ture consisting of argument identification (determin-ing the arguments of the verbal predicate) and ar-gument classification (labeling these arguments withsemantic roles).
Despite being relatively shallow, se-1More precisely, A0 and A1 have a common interpretationacross predicates as proto-agent and proto-patient in the senseof Dowty (1991).1320mantic role analysis has the potential of benefiting awide spectrum of applications ranging from infor-mation extraction (Surdeanu et al, 2003) and ques-tion answering (Shen and Lapata, 2007), to machinetranslation (Wu and Fung, 2009) and summarization(Melli et al, 2005).Current approaches have high performance ?
asystem will recall around 81% of the arguments cor-rectly and 95% of those will be assigned a cor-rect semantic role (see Ma`rquez et al (2008) fordetails), however only on languages and domainsfor which large amounts of role-annotated trainingdata are available.
For instance, systems trained onPropBank demonstrate a marked decrease in per-formance (approximately by 10%) when tested onout-of-domain data (Pradhan et al, 2008).
Unfortu-nately, the reliance on role-annotated data which isexpensive and time-consuming to produce for everylanguage and domain, presents a major bottleneck tothe widespread application of semantic role labeling.In this paper we argue that unsupervised meth-ods offer a promising yet challenging alternative.
Ifsuccessful, such methods could lead to significantsavings in terms of annotation effort and ultimatelyyield more portable semantic role labelers that re-quire overall less engineering effort.
Our approachformalizes semantic role induction as a graph parti-tioning problem.
Given a verbal predicate, it con-structs a weighted graph whose vertices correspondto argument instances of the verb and whose edgeweights quantify the similarity between these in-stances.
The graph is partitioned into vertex clus-ters representing semantic roles using a variant ofChinese Whispers, a graph-clustering algorithm pro-posed by Biemann (2006).
The algorithm iterativelyassigns cluster labels to graph vertices by greedilychoosing the most common label amongst the neigh-bors of the vertex being updated.
Beyond extend-ing Chinese Whispers to the semantic role induc-tion task, we also show how it can be understoodas a type of Gibbs sampling when our graph is inter-preted as a Markov random field.Experimental results on the CoNLL 2008 bench-mark dataset demonstrate that our method, de-spite its simplicity, improves upon competitive ap-proaches in terms of F1 and achieves significantlyhigher cluster purity.2 Related WorkAlthough the bulk of previous work on semantic rolelabeling has primarily focused on supervised meth-ods (Ma`rquez et al, 2008), a few semi-supervisedand unsupervised approaches have been proposedin the literature.
The majority of semi-supervisedmodels have been developed within a frameworkknown as annotation projection.
The idea is to com-bine labeled and unlabeled data by projecting an-notations from a labeled source sentence onto anunlabeled target sentence within the same language(Fu?rstenau and Lapata, 2009) or across different lan-guages (Pado?
and Lapata, 2009).
Outwith annota-tion projection, Gordon and Swanson (2007) pro-pose to increase the coverage of PropBank to un-seen verbs by finding syntactically similar (labeled)verbs and using their annotations as surrogate train-ing data.Swier and Stevenson (2004) were the first to intro-duce an unsupervised semantic role labeling system.Their algorithm induces role labels following a boot-strapping scheme where the set of labeled instancesis iteratively expanded using a classifier trained onpreviously labeled instances.
Their method startswith a dataset containing no role annotations at all,but crucially relies on VerbNet (Kipper et al, 2000)for identifying the arguments of predicates and mak-ing initial role assignments.
VerbNet is a manuallyconstructed lexicon of verb classes each of which isexplicitly associated with argument realization andsemantic role specifications.Subsequent work has focused on unsupervisedmethods for argument identification and classifica-tion.
Abend et al (2009) recognize the arguments ofpredicates by relying solely on part of speech anno-tations whereas Abend and Rappoport (2010) distin-guish between core and adjunct roles, using an unsu-pervised parser and part-of-speech tagger.
Grenagerand Manning (2006) address the role induction prob-lem and propose a directed graphical model whichrelates a verb, its semantic roles, and their possiblesyntactic realizations.
Latent variables represent thesemantic roles of arguments and role induction cor-responds to inferring the state of these latent vari-ables.Following up on this work, Lang and Lapata(2010) formulate role induction as the process of de-1321tecting alternations and finding a canonical syntacticform for them.
Verbal arguments are then assignedroles, according to their position in this canonicalform, since each position references a specific role.Their model extends the logistic classifier with hid-den variables and is trained in a manner that takesadvantage of the close relationship between syntac-tic functions and semantic roles.
More recently,Lang and Lapata (2011) propose a clustering algo-rithm which first splits the argument instances ofa verb into fine-grained clusters based on syntac-tic cues and then executes a series of merge steps(mainly) based on lexical cues.
The split phase cre-ates a large number of small clusters with high puritybut low collocation, i.e., while the instances in a par-ticular cluster typically belong to the same role theinstances for a particular role are commonly scat-tered amongst many clusters.
The subsequent mergephase conflates clusters with the same role in orderto increase collocation.Like Grenager and Manning (2006) and Langand Lapata (2010; 2011), this paper describes anunsupervised method for semantic role induction,i.e., one that does not require any role annotated dataor additional semantic resources for training.
Con-trary to these previous approaches, we conceptualizerole induction in a novel way, as a graph partitioningproblem.
Our method is simple, computationally ef-ficient, and does not rely on hidden variables.
More-over, the graph-based representation for verbs andtheir arguments affords greater modeling flexibility.A wide range of methods exist for finding partitionsin graphs (Schaeffer, 2007), besides Chinese Whis-pers (Biemann, 2006), which could be easily appliedto the semantic role induction problem.
However,we leave this to future work.Graph-based methods are popular in natural lan-guage processing, especially with unsupervisedlearning problems (Chen and Ji, 2010).
The ChineseWhispers algorithm itself (Biemann, 2006) has beenpreviously applied to several tasks including wordsense induction (Klapaftis and M., 2010) and unsu-pervised part-of-speech tagging (Christodoulopou-los et al, 2010).
The same algorithm is also de-scribed in Abney (2007, pp.
146-147) under thename ?clustering by propagation?.
The term makesexplicit the algorithm?s connection to label propa-gation, a general framework2 for semi-supervisedlearning (Zhu et al, 2003) with applications tomachine translation (Alexandrescu and Kirchhoff,2009), information extraction (Talukdar and Pereira,2010) and structured part-of-speech tagging (Sub-ramanya et al, 2010).
The basic idea behind la-bel propagation is to represent labeled and unlabeledinstances as vertices in an undirected graph withedges whose weights express similarity (and possi-bly dissimilarity) between the instances.
Label in-formation is then propagated between the verticesin such a way that similar instances tend to be as-signed the same label.
Analogously, Chinese Whis-pers works by propagating cluster membership in-formation along the edges of a graph, even thoughthe graph does not contain any human-labeled in-stance vertices.3 Problem SettingWe adopt the standard architecture of supervised se-mantic role labeling systems where argument identi-fication and argument classification are treated sep-arately.
Our role labeler is fully unsupervised withrespect to both tasks ?
it does not rely on any roleannotated data or semantic resources.
However, oursystem does not learn from raw text.
In commonwith most semantic role labeling research, we as-sume that the input is syntactically analyzed in theform of dependency trees.We view argument identification as a syntacticprocessing step that can be largely undertaken deter-ministically through structural analysis of the depen-dency tree.
We therefore use a small set of rules todetect arguments with high precision and recall (seeSection 4).
Argument classification is more chal-lenging and must take into account syntactic as wellas lexical-semantic information.
Both types of in-formation are incorporated into our model througha similarity function that assigns similarity scoresto pairs of argument instances.
Following previouswork (Lang and Lapata, 2010; Grenager and Man-ning, 2006), our system outputs verb-specific rolesby grouping argument instances into clusters and la-beling each argument instance with an identifier cor-2For example, Haffari and Sarkar (2007) use label propa-gation to analyze other semi-supervised algorithms such as theYarowsky (1995) algorithm.1322responding to the cluster it has been assigned to.Such identifiers are similar to PropBank-style corelabels (e.g., A0, A1).4 Argument IdentificationSupervised semantic role labelers often employ aclassifier in order to decide for each node in theparse tree whether or not it represents a semanticargument.
Nodes classified as arguments are thenassigned a semantic role.
In the unsupervised set-ting, we slightly reformulate argument identificationas the task of discarding as many non-semantic ar-guments as possible.
This means that the argumentidentification component does not make a final posi-tive decision for any of the argument candidates; in-stead, a final decision is only made in the subsequentargument classification stage.We discard or select argument candidates us-ing the set of rules developed in Lang and Lap-ata (2011).
These are mainly based on the partsof speech and syntactic relations encountered whentraversing a dependency tree from the predicatenode to the argument node.
For each candidate,rules are considered in a prespecified order and thefirst matching rule is applied.
When evaluated onits own, the argument identification component ob-tained 88.1% precision (percentage of semantic ar-guments out of those identified) and 87.9% recall(percentage of identified arguments out of all goldarguments).5 Argument ClassificationAfter identifying likely arguments for each verb,the next step is to infer a label for each argumentinstance.
Since we aim to induce verb-specificroles (see Section 3), we construct an undirected,weighted graph for each verb.
Vertices corre-spond to verb argument instances and edge weightsquantify the similarities between them.
Thisargument-instance graph is then partitioned intoclusters of vertices representing semantic roles andeach argument instance is assigned a label that indi-cates the cluster it belongs to.
In what follows wefirst describe how the graph is constructed and thenprovide the details of our graph partitioning algo-rithm.CAE DB0.4 0.10.8 ?110.30.2 0.7Figure 1: Simplified example of an argument-instancegraph.
All pairs of vertices with non-zero similarity areconnected through edges that are weighted with a simi-larity score ?
(vi,v j).
Upon updating the label for a vertexall neighboring vertices propagate their label to the vertexbeing updated.
The score for each label is determined bysumming together the weighted votes for that label andthe label with the maximal score is chosen.5.1 Graph ConstructionFor each verb we construct an undirected, weightedgraph G = (V,E,?)
with vertices V , edges E, andedge weight function ?
as follows.
Each argu-ment instance in the corpus that belongs to theverb is added as a vertex.
Then, for each possi-ble pair of vertices (vi,v j) we compute a weight?
(vi,v j) ?
R according to the function ?.
If theweight is non-zero, an undirected edge e = (vi,v j)with weight ?
(vi,v j) is added to the graph.
The func-tion ?
quantifies the similarity or dissimilarity be-tween instances; positive values indicate that rolesare likely to be the same, negative values indicatethat roles are likely to differ, and zero values indicatethat there is no evidence for either case.
Our simi-larity function is symmetric, i.e., ?
(vi,v j) = ?
(v j,vi)and permits negative values (see Section 5.4 for adetailed description).Figure 1 shows an example of a graph for a verbwith five argument instances (vertices A?E).
Edgesare drawn between pairs of vertices with non-zerosimilarity values.
For instance, vertex D is con-nected to vertex A with weight 0.2, to vertex Ewith 1, and vertex C with?1.
Since edges are drawnbetween all pairs of vertices with non-zero simi-larity, the resulting graphs tend to be densely con-nected, which for large datasets may be prohibitively1323inefficient.
A solution would be to sample a subsetfrom all possible pairs, but we did not make use ofany kind of edge pruning in our experiments.5.2 Graph PartitioningGraph partitioning is realized with a variant of Chi-nese Whispers (Biemann, 2006) whose details aregiven below.
In addition, we discuss how our algo-rithm relates to other graph-based models in order tohelp provide a better theoretical understanding.We assume each vertex vi is assigned a labelli ?
{1 .
.
.L} indicating the cluster it belongs to.
Ini-tially, each vertex belongs to its own cluster, i.e., welet the number of clusters L = |V | and set li?
i.Given this initial vertex labeling, the algorithm pro-ceeds by iteratively updating the label for each ver-tex.
The update is based on the labels of neighbor-ing vertices and reflects their similarity to the vertexbeing updated.
Intuitively, each neighboring vertexvotes for the cluster it is currently assigned to, wherethe strength of the vote is determined by the similar-ity (i.e., edge weight) to the vertex being updated.The label li of vertex vi is thus updated according tothe following equation:li?
arg maxl?
{1...L} ?v j?Ni(l)?
(vi,v j) (2)where Ni(l) = {v j|(vi,v j) ?
E ?
l = l j} denotes theset of vi?s neighbors with label l. In other words,for each label we compute a score by summing to-gether the weights of edges to neighboring verticeswith that label and select the label with the maximalscore.
Note that negative edges decrease the scorefor a particular label, thus demoting the label.Consider again Figure 1.
Assume we wish to up-date vertex A.
In addition, assume that B and E arecurrently assigned the same label (i.e., they belongto the same cluster) whereas C and D are each indifferent clusters.
The score for cluster {B,E} is0.4+0.8 = 1.2, the score for cluster {C} is 0.3 andthe score for cluster {D} is 0.2.
We would thus as-sign A to cluster {B,E} as it has the highest score.The algorithm is run for several iterations.
Ateach iteration it passes over all vertices, and the up-date order of the vertices is chosen randomly.
Asthe updates proceed, labels can disappear from thegraph, whereby the number of clusters decreases.Empirically, we observe that for sufficiently manyiterations the algorithm converges to a fixed labelingor oscillates between labelings that differ only in afew vertices.
The result of the algorithm is a hardpartitioning of the given graph, where the number ofclusters is determined automatically.5.3 Propagation PrioritizationWe make one important modification to the basic al-gorithm described so far based on the intuition thathigher scores for a label indicate more reliable prop-agations.
More precisely, when updating vertex vi tolabel l we define the confidence of the update as theaverage similarity to neighbors with label l:con f (li?
l) = 1|Ni(l)| ?v j?Ni(l)?
(vi,v j) (3)We can then prioritize high-confidence updates bysetting a threshold ?
and allowing only updates withconfidence greater or equal to ?.
The threshold isinitially set to 1 (i.e., the maximal possible confi-dence) and then lowered by some small constant ?after each iteration until it reaches a minimum ?min,at which point the algorithm terminates.
This im-proves the resulting clustering, since it promotesreliable updates in earlier phases of the algorithmwhich in turn has a positive effect on successive up-dates.5.4 Argument-Instance SimilarityAs described earlier, the edge weights in our graphare similarity scores, with positive values indicatingsimilarity and negative values indicating dissimilar-ity.
Determining the similarity function ?
withoutaccess to labeled training data poses a major diffi-culty which we resolve by relying on prior linguis-tic knowledge.
Specifically, we measure the sim-ilarity of argument instances based on three sim-ple and intuitive criteria: (1) whether the instancesare lexically similar; (2) whether the instances oc-cur in the same syntactic position; and (3) whetherthe instances occur in the same frame (i.e., are argu-ments in the same clause).
The same criteria wereused in (Lang and Lapata, 2011) and shown effec-tive in quantifying role-semantic similarity betweenclusters of argument instances.
Lexical and syntac-tic similarity are scored through functions lex(vi,v j)and syn(vi,v j) with range [?1,1], whereas the thirdcriterion enters the scoring function directly:1324?
(vi,v j)={??
if vi and v j are in same frame (4)?lex(vi,v j)+(1??
)syn(vi,v j) otherwise.The first case in the function expresses a com-mon linguistic assumption, i.e., that two argumentinstances vi and v j occurring in the same frame can-not have the same semantic role.
The function im-plements this constraint by returning?
?.3 The syn-tactic similarity function s(vi,v j) indicates whethertwo argument instances occur in a similar syntacticposition.
We define syntactic positions through fourcues: the relation of the argument head word to itsgovernor, verb voice (active/passive), the linear po-sition of the argument relative to the verb (left/right)and the preposition used for realizing the argument(if any).
The score is S4 where S is the number of cueswhich agree, i.e., have the same value.
The syntac-tic score is set to zero when the governor relationof the arguments is not the same.
Lexical similar-ity l(vi,v j) is measured in terms of the cosine of theangle between vectors hi and h j representing the ar-gument head words:lex(vi,v j) = cos(hi,h j) = hi?h j?hi?
?h j?
(5)We obtain hi and h j from a simple semantic spacemodel (Turney and Pantel, 2010) which requires nosupervision (Section 6 describes the details of themodel used in our experiments).Our similarity function weights the contributionof syntax vs. semantics equally, i.e., ?
is set to 0.5.This reflects the linguistic intuition that lexical andsyntactic information are roughly of equal impor-tance.5.5 Relation to Other ModelsThis section briefly points out some connections torelated models.
The averaging procedure used forupdating the graph vertices (Equation 2) appears insome form in most label propagation algorithms (seeTalukdar (2010) for details).
Label propagation al-gorithms are commonly interpreted as random walks3Formally, ?
has range ran(?)
= [?1,1] ?
{??}
and forx ?
ran(?)
we define x+(??)
=??.
This means that the over-all score computed for a label (Equation 2) is ??
if one of thesummands is ?
?.2?3 31Figure 2: The update rule (Equation 2) can be under-stood as choosing a minimal edge-cut, thereby greedilymaximizing intra-cluster similarity and minimizing inter-cluster similarity.
Assuming equal weight for all edgesabove, label 3 is chosen for the vertex being updated suchthat the sum of weights of edges crossing the cut is mini-mal.on graphs.
In our case such an interpretation isnot directly possible due to the presence of negativeedge weights.
This could be changed by transform-ing the edge weights onto a non-negative scale, butwe find the current setup more expedient for model-ing dissimilarity.Our model could be also transformed into a prob-abilistic graphical model that specifies a distributionover vertex labels.
In the transformed model eachvertex corresponds to a random variable over labelsand edges are associated with binary potential func-tions over vertex-pairs.
Let 1(vi = v j) denote an in-dicator function which takes value 1 iff.
li = l j andvalue 0, otherwise.
Then pairwise potentials can bedefined in terms of the original edge weights4 as?
(vi,v j) = exp(1(vi = v j)?
(vi,v j)).
A Gibbs sam-pler used to sample from the distribution of theresulting pairwise Markov random field (Bishop,2006; Wainwright and Jordan, 2008) would employalmost the same update procedure as in Equation 2,the difference being that labels would be sampledaccording to their probabilities, rather than chosendeterministically based on scores.A third way of understanding the update ruleis as a heuristic for maximizing intra-cluster sim-ilarity and minimizing inter-cluster similarity.
By4Including weights with value zero and thus connecting allvertex pairs.1325assigning the label with maximal score to vi, wegreedily maximize the sum of intra-cluster edgeweights while minimizing the sum of inter-clusteredge weights, i.e., the weight of the edge-cut.
Thisis illustrated in Figure 2.
Cut-based methods area common method in graph clustering (Schaeffer,2007) and are also used for inference in pairwiseMarkov random fields like the one described in theprevious paragraph (Boykov et al, 2001).Note that while it would be possible to transformour model into a model with a formal probabilisticinterpretation (either as a graph random walk or as aprobabilistic graphical model) this would not changethe non-empirical nature of the similarity function,which is unavoidable in the unsupervised setting andis also common in the semi-supervised methods dis-cussed in Section 2.6 Experimental SetupIn this section we describe how we assessed theperformance of our model.
We discuss the dataseton which our experiments were carried out, explainhow our system?s output was evaluated and presentthe methods used for comparison with our approach.Data We compared the output of our modelagainst the PropBank gold standard annotations con-tained in the CoNLL 2008 shared task dataset (Sur-deanu et al, 2008).
The latter was taken from theWall Street Journal portion of the Penn Treebankand converted into a dependency format (Surdeanuet al, 2008).
In addition to gold standard depen-dency parses, the dataset alo contains automaticparses obtained from the MaltParser (Nivre et al,2007).
The dataset provides annotations for ver-bal and nominal predicate-argument constructions,but we only considered the former, following previ-ous work on semantic role labeling (Ma`rquez et al,2008).
All the experiments described in this paperuse the CoNLL 2008 training dataset.Evaluation Metrics For each verb, we determinethe extent to which argument instances in the clus-ters share the same gold standard role (purity) andthe extent to which a particular gold standard role isassigned to a single cluster (collocation).More formally, for each group of verb-specificclusters we measure the purity of the clusters as thepercentage of instances belonging to the majoritygold class in their respective cluster.
Let N denotethe total number of instances, G j the set of instancesbelonging to the j-th gold class and Ci the set of in-stances belonging to the i-th cluster.
Purity can bethen written as:PU = 1N ?i maxj |G j ?Ci| (6)Collocation is defined as follows.
For each goldrole, we determine the cluster with the largest num-ber of instances for that role (the role?s primary clus-ter) and then compute the percentage of instancesthat belong to the primary cluster for each gold role:CO = 1N ?j maxi |G j ?Ci| (7)Per-verb scores are aggregated into an overallscore by averaging over all verbs.
We use themicro-average obtained by weighting the scores forindividual verbs proportionately to the number of in-stances for that verb.Finally, we use the harmonic mean of purity andcollocation as a single measure of clustering quality:F1 = 2?CO?PUCO+PU (8)Model Parameters Recall that our algorithm pri-oritizes updates with confidence higher than athreshold ?.
Initially, ?
is set to 1 and its valuedecreases at each iteration by a small constant ?which we set to 0.0025.
The algorithm terminateswhen a minimum confidence ?min is reached.
Whilechoosing a value for ?
is straightforward ?
it sim-ply has to be a small fraction of the maximal pos-sible confidence ?
specifying ?min on the basis ofobjective prior knowledge is less so.
And althougha human judge could determine the optimal termina-tion point based on several criteria such as clusteringquality or the number of clusters, we used a develop-ment set instead for the sake of reproducibility andcomparability.
Specifically, we optimized ?min onthe CoNLL test set and obtained best results with?min = 13 .
This value was used for all our experi-ments and was also kept fixed for all verbs.
Impor-tantly, the development set was not used for any kindof supervised training.1326Syntactic Function Latent Logistic Split-Merge Graph PartitioningPU CO F1 PU CO F1 PU CO F1 PU CO F1auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2 82.5 68.8 75.0gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9 84.0 73.5 78.4auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3 87.4 65.9 75.2gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1 88.6 70.7 78.6Table 1: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baselinethat assigns arguments to clusters based on their syntactic function.0 10 20 30 40 50 60 70Average number of clusters per verb60708090100Cluster purity(%)Figure 3: Purity (vertical axis) against average numberof clusters per verb (horizontal axis) on the auto/autodataset.Recall that one of the components in our simi-larity function is lexical similarity which we mea-sure using a vector-based model (see Section 5.4).We created such a model from the Google N-Gramscorpus (Brants and Franz, 2006) using a contextwindow of two words on both sides of the targetword and co-occurrence frequencies as vector com-ponents (no weighting was applied).
The large sizeof this corpus allows us to use bigram frequencies,rather than frequencies of individual words and todistinguish between left and right bigrams.
We usedrandomized algorithms (Ravichandran et al, 2005)to build the semantic space efficiently.Comparison Models We compared our graph par-titioning algorithm against three competitive ap-proaches.
The first one assigns argument instancesto clusters according to their syntactic function(e.g., subject, object) as determined by a parser.
Thisbaseline has been previously used as a point of com-parison by other unsupervised semantic role induc-tion systems (Grenager and Manning, 2006; Langand Lapata, 2010) and shown difficult to outperform.0 100 200 300 400 500Number of iterations020406080F1 score (%)Syntactic FunctionGraph PartitioningFigure 4: F1 (vertical axis) against number of iterations(horizontal axis) on the auto/auto dataset.Our implementation allocates up to N = 21 clusters5for each verb, one for each of the 20 most frequentsyntactic functions and a default cluster for all otherfunctions.
We also compared our approach to Langand Lapata (2010) using the same model settings(with 10 latent variables) and feature set proposedin that paper.
Finally, our third comparison modelis Lang and Lapata?s (2011) split-merge clusteringalgorithm.
Again we used the same parameters andnumber of clusters (on average 10 per verb).
Ourgraph partitioning method uses identical cues for as-sessing role-semantic similarity as the method de-scribed in Lang and Lapata (2011).7 ResultsOur results are summarized in Table 1.
We reportcluster purity (PU), collocation (CO) and their har-monic mean (F1) for the baseline (Syntactic Func-tion), our two previous models (the Latent Logisticclassifier and Split-Merge) and the graph partition-ing algorithm on four datasets.
These result from thecombination of automatic parses with automaticallyidentified arguments (auto/auto), gold parses with5This is the number of gold standard roles.1327Syntactic FunctionPU 91.4 68.6 45.1 59.7 62.4 61.9 63.5 75.9 76.7 69.6 63.1 53.7CO 91.3 71.9 56.0 68.4 72.7 76.8 65.6 79.7 76.0 63.8 73.4 58.9F1 91.4 70.2 49.9 63.7 67.1 68.6 64.5 77.7 76.3 66.6 67.9 56.2Graph PartitioningPU 95.6 83.5 72.3 75.4 83.3 84.4 74.8 84.8 89.5 83.0 73.2 66.3CO 89.1 62.7 42.1 64.2 56.2 66.3 57.2 73.2 64.1 54.3 66.0 57.7F1 92.2 71.6 53.2 69.4 67.1 74.3 64.8 78.5 74.7 65.7 69.4 61.7Verb say make go increase know tell consider acquire meet send open breakFreq 15238 4250 2109 1392 983 911 753 704 574 506 482 246Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and thesyntactic function baseline; the scores were taken from a single run.automatic arguments (gold/auto), automatic parseswith gold arguments (auto/gold) and gold parseswith gold arguments (gold/gold).
Table 1 reportsaverages across multiple runs.
This was necessaryin order to ensure that the results of our randomizedgraph partitioning algorithm are stable.6 The argu-ments for the auto/auto and gold/auto datasets wereidentified using the rules described in Lang and Lap-ata (2011) (see Section 4).
Bold-face is used to high-light the best performing system under each measure(PU, CO, or F1) on each dataset.Compared to the Syntactic Function baseline,the Graph Partitioning algorithm has higher F1 onthe auto/auto and auto/gold datasets but lags be-hind by 0.5 points on the gold/auto dataset andby 0.9 points on the gold/gold dataset.
It attainshighest purity on all datasets except for gold/gold,where it is 0.1 points below Split-Merge.
When con-sidering F1 in conjunction with purity and colloca-tion, we observe that Graph Partitioning can attainhigher purity than the comparison models by tradingoff collocation.
If we were to hand label the clustersoutput by our system, purity would correspond to thequality of the resulting labeling, while collocationwould determine the labeling effort.
The relation-ship is illustrated more explicitly in Figure 3, whichplots purity against the average number of clustersper verb on the auto/auto dataset.
As the algorithm6For example, on the auto/auto dataset and over 10 runs,the standard deviation in F1 was 0.11 points in collocation 0.16points and in purity 0.08 points.
The worst F1 was 0.20 pointsbelow the average, the worst collocation was 0.32 points be-low the average and the worst purity was 0.17 points below theaverage.proceeds the number of clusters is reduced whichresults in a decrease of purity.
The latter decreasesmore rapidly once the number of 20 clusters per verbis reached.
This is accompanied by a decreasingtradeoff ratio between collocation and purity: at thisstage decreasing purity by one point increases collo-cation by roughly one point, whereas in earlier itera-tions a decrease of purity by one point goes togetherwith several points increase in collocation.
This ismost likely due to the fact that the number of goldstandard classes is around 20.Figure 4 shows the complete learning curve of ourgraph partitioning method on the auto/auto dataset(F1 is plotted against the number of iterations).The algorithm naturally terminates at iteration 266(when ?min = 1/3), but we have also plotted itera-tions beyond that point.
Since lower values of ?
per-mit unreliable propagations, F1 eventually falls be-low the baseline (see Section 5.2).
The importanceof our propagation prioritization mechanism is fur-ther underlined by the fact that when it is not em-ployed (i.e., when using the vanilla Chinese Whis-pers algorithm without any modifications), it per-forms substantially worse than the comparison mod-els.
On the auto/auto dataset, F1 converges to 59.1(purity is 55.5 and collocation 63.2) within 10 itera-tions.Finally, Table 2 shows how performance variesacross verbs.
We report results for the Syntac-tic Function baseline and Graph Partitioning on theauto/auto dataset for 12 verbs.
These were selectedso as to exhibit varied occurrence frequencies andalternation patterns.
As can be seen, the macro-1328scopic result ?
increase in F1 and purity ?
alsoholds across verbs.8 ConclusionsIn this paper we described an unsupervised methodfor semantic role induction, in which argument-instance graphs are partitioned into clusters repre-senting semantic roles.
The approach is conceptu-ally and algorithmically simple and novel in its for-malization of role induction as a graph partitioningproblem.
We believe this constitutes an interestingalternative for two reasons.
Firstly, eliciting andencoding problem-specific knowledge in the formof instance-wise similarity judgments can be easierthan encoding it into model structure e.g., by mak-ing statistical independence assumptions or assump-tions about latent structure.
Secondly, the approachis general and amenable to other graph partitioningalgorithms and relates to well-known graph-basedsemi-supervised learning methods.The similarity function in this paper is by neces-sity rudimentary, since it cannot be estimated fromdata.
Nevertheless, the resulting system attains com-petitive F1 and notably higher purity than the com-parison models.
Arguably, performance could beimproved by developing a better similarity function.Therefore, in the future we intend to investigate howour system performs in a weakly supervised setting,where the similarity function is estimated from asmall amount of labeled instances, since this wouldallow us to incorporate richer syntactic features andresult in more precise similarity scores.Acknowledgments We are grateful to CharlesSutton for his valuable feedback on this work.
Theauthors acknowledge the support of EPSRC (grantGR/T04540/01).ReferencesO.
Abend and A. Rappoport.
2010.
Fully unsupervisedcore-adjunct argument classification.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, pages 226?236, Uppsala,Sweden.O.
Abend, R. Reichart, and A. Rappoport.
2009.
Un-supervised Argument Identification for Semantic RoleLabeling.
In Proceedings of the 47th Annual Meet-ing of the Association for Computational Linguisticsand the 4th International Joint Conference on NaturalLanguage Processing of the Asian Federation of Natu-ral Language Processing, pages 28?36, Singapore.S.
Abney.
2007.
Semisupervised Learning for Computa-tional Linguistics.
Chapman & Hall/CRC.A.
Alexandrescu and K. Kirchhoff.
2009.
Graph-basedlearning for statistical machine translation.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages119?127, Boulder, Colorado.C.
Biemann.
2006.
Chinese Whispers: an efficientgraph clustering algorithm and its application to nat-ural language processing problems.
In Proceedingsof TextGraphs: the First Workshop on Graph BasedMethods for Natural Language Processing, pages 73?80, New York City.C.
Bishop.
2006.
Pattern Recognition and MachineLearning.
Springer.Y.
Boykov, O. Veksler, and R. Zabih.
2001.
Fast Ap-proximate Energy Minimization via Graph Cuts.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 23(11):1222?1239.T.
Brants and A. Franz.
2006.
Web 1T 5-gram Version 1.Linguistic Data Consortium, Philadelphia.Z.
Chen and H. Ji.
2010.
Graph-based clustering forcomputational linguistics: A survey.
In Proceedings ofTextGraphs-5 - 2010 Workshop on Graph-based Meth-ods for Natural Language Processing, pages 1?9, Up-psala, Sweden.C.
Christodoulopoulos, S. Goldwater, and M. Steedman.2010.
Two decades of unsupervised POS induction:How far have we come?
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 575?584, Cambridge, MA.D.
Dowty.
1991.
Thematic Proto Roles and ArgumentSelection.
Language, 67(3):547?619.H.
Fu?rstenau and M. Lapata.
2009.
Graph Aligmentfor Semi-Supervised Semantic Role Labeling.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 11?20, Singa-pore.D.
Gildea and D. Jurafsky.
2002.
Automatic Label-ing of Semantic Roles.
Computational Linguistics,28(3):245?288.A.
Gordon and R. Swanson.
2007.
Generalizing Se-mantic Role Annotations Across Syntactically SimilarVerbs.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages192?199, Prague, Czech Republic.T.
Grenager and C. Manning.
2006.
Unsupervised Dis-covery of a Statistical Verb Lexicon.
In Proceedingsof the Conference on Empirical Methods on NaturalLanguage Processing, pages 1?8, Sydney, Australia.1329G.
Haffari and A. Sarkar.
2007.
Analysis of Semi-Supervised Learning with the Yarowsky Algorithm.
InProceedings of the 23rd Conference on Uncertainty inArtificial Intelligence, Vancouver, BC.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class-Based Construction of a Verb Lexicon.
In Proceedingsof the 17th AAAI Conference on Artificial Intelligence,pages 691?696.
AAAI Press / The MIT Press.I.
Klapaftis and Suresh M. 2010.
Word sense induction& disambiguation using hierarchical random graphs.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 745?755, Cambridge, MA.J.
Lang and M. Lapata.
2010.
Unsupervised Inductionof Semantic Roles.
In Proceedings of the 11th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 939?947, Los Angeles, California.J.
Lang and M. Lapata.
2011.
Unsupervised SemanticRole Induction via Split-Merge Clustering.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics, Portland, Oregon.To appear in.L.
Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.2008.
Semantic Role Labeling: an Introduction to theSpecial Issue.
Computational Linguistics, 34(2):145?159.G.
Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,B.
Gu, A. Sarkar, and F. Popowich.
2005.
Descriptionof SQUASH, the SFU Question Answering SummaryHandler for the DUC-2005 Summarization Task.
InProceedings of the Human Language Technology Con-ference and the Conference on Empirical Methods inNatural Language Processing Document Understand-ing Workshop, Vancouver, Canada.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit A. Chanev,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A Language-independent System for Data-driven Dependency Parsing.
Natural Language Engi-neering, 13(2):95?135.S.
Pado?
and M. Lapata.
2009.
Cross-lingual AnnotationProjection of Semantic Roles.
Journal of Artificial In-telligence Research, 36:307?340.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1):71?106.S.
Pradhan, W. Ward, and J. Martin.
2008.
Towards Ro-bust Semantic Role Labeling.
Computational Linguis-tics, 34(2):289?310.D.
Ravichandran, P. Pantel, and E. Hovy.
2005.
Ran-domized Algorithms and NLP: Using Locality Sensi-tive Hash Function for High Speed Noun Clustering.In Proceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, page 622629,Ann Arbor, Michigan.J.
Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,and J. Scheffczyk.
2006.
FrameNet II: Extended The-ory and Practice, version 1.3.
Technical report, In-ternational Computer Science Institute, Berkeley, CA,USA.S.
Schaeffer.
2007.
Graph clustering.
Computer ScienceReview, 1(1):27?64.D.
Shen and M. Lapata.
2007.
Using Semantic Rolesto Improve Question Answering.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing and the Conference on Com-putational Natural Language Learning, pages 12?21,Prague, Czech Republic.A.
Subramanya, S. Petrov, and F. Pereira.
2010.
Effi-cient graph-based semi-supervised learning of struc-tured tagging models.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 167?176, Cambridge, MA.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.2003.
Using Predicate-Argument Structures for Infor-mation Extraction.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 8?15, Sapporo, Japan.M.
Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.2008.
The CoNLL-2008 Shared Task on Joint Parsingof Syntactic and Semantic Dependencies.
In Proceed-ings of the 12th CoNLL, pages 159?177, Manchester,England.R.
Swier and S. Stevenson.
2004.
Unsupervised Seman-tic Role Labelling.
In Proceedings of the Conferenceon Empirical Methods on Natural Language Process-ing, pages 95?102, Barcelona, Spain.P.
Talukdar and F. Pereira.
2010.
Experiments in graph-based semi-supervised learning methods for class-instance acquisition.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 1473?1481, Uppsala, Sweden.P.
Talukdar.
2010.
Graph-Based Weakly SupervisedMethods for Information Extraction & Integration.Ph.D.
thesis, CIS Department, University of Pennsyl-vania.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37:141?188.M.
Wainwright and M. Jordan.
2008.
Graphical Mod-els, Exponential Families, and Variational Inference.Foundations and Trends in Machine Learning, 1(1-2):1?305.D.
Wu and P. Fung.
2009.
Semantic Roles for SMT:A Hybrid Two-Pass Model.
In Proceedings of North1330American Annual Meeting of the Association for Com-putational Linguistics HLT 2009: Short Papers, pages13?16, Boulder, Colorado.D.
Yarowsky.
1995.
Unsupervised Word Sense Disam-biguation Rivaling Supervised Methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA.X.
Zhu, Z. Ghahramani, and J. Lafferty.
2003.
Semi-Supervised Learning Using Gaussian Fields and Har-monic Functions.
In Proceedings of the 20th Interna-tional Conference on Machine Learning, Washington,DC.1331
