Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410?413,Prague, June 2007. c?2007 Association for Computational LinguisticsUNT: SubFinder: Combining Knowledge Sources forAutomatic Lexical SubstitutionSamer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea?Department of Computer Science and EngineeringUniversity of North Texassamer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.eduAbstractThis paper describes the University of NorthTexas SUBFINDER system.
The system isable to provide the most likely set of sub-stitutes for a word in a given context, bycombining several techniques and knowl-edge sources.
SUBFINDER has successfullyparticipated in the best and out of ten (oot)tracks in the SEMEVAL lexical substitutiontask, consistently ranking in the first or sec-ond place.1 IntroductionLexical substitution is defined as the task of identify-ing the most likely alternatives (substitutes) for a tar-get word, given its context (McCarthy, 2002).
Manynatural language processing applications can bene-fit from the availability of such alternative words,including word sense disambiguation, lexical ac-quisition, machine translation, information retrieval,question answering, text simplification, and others.The task is closely related to the problem of wordsense disambiguation, with the substitutes acting assynonyms for the input word meaning.
Unlike wordsense disambiguation however, lexical substitutionis not performed with respect to a given sense inven-tory, but instead candidate synonyms are generated?on the fly?
for a given word occurrence.
Thus, lexi-cal substitution can be regarded in a way as a hybridtask that combines word sense disambiguation anddistributional similarity, targeting the identificationof semantically similar words that fit the context.2 A system for lexical substitutionSUBFINDER is a system able to provide the mostlikely set of substitutes for a word in a given context.
?Contact author.In SUBFINDER, the lexical substitution task is car-ried out as a sequence of two steps.
First, candidatesare extracted from a variety of knowledge sources;so far, we experimented with WordNet (Fellbaum,1998), Microsoft Encarta encyclopedia, Roget, aswell as synonym sets generated from bilingual dic-tionaries, but additional knowledge sources can beintegrated as well.
Second, provided a list of candi-dates, a number of ranking methods are applied ina weighted combination, resulting in a final list oflexical substitutes ranked by their semantic fit withboth the input target word and the context.3 Candidate ExtractionCandidates are extracted using several lexical re-sources, which are combined into a larger compre-hensive resource.WordNet: WordNet is a large lexical database ofEnglish, with words grouped into synonym setscalled synsets.
A problem we encountered with thisresource is that often times the only candidate in thesynset is the target word itself.
Thus, to enlarge theset of candidates, we use both the synonyms and thehypernyms of the target word.
We also remove thetarget word from the synset, to ensure that only vi-able candidates are considered.Microsoft Encarta encyclopedia: The MicrosoftEncarta is an online encyclopedia and thesaurus re-source, which provides for each word the part ofspeech and a list of synonyms.
Using the part ofspeech as identified in the context, we are able to ex-tract synsets for the target word.
An important fea-ture in the Encarta Thesaurus is that the first wordin the synset acts as a definition for the synset, andtherefore disambiguates the target word.
This defi-nition is maintained as a separate entry in the com-410prehensive resource, and it is also added to its corre-sponding synset.Other Lexical Resources: We have also experi-mented with two other lexical resources, namely theRoget thesaurus and a thesaurus built using bilingualdictionaries.
In evaluations carried out on the devel-opment data set, the best results were obtained usingonly WordNet and Encarta, and thus these are theresources used in the final SUBFINDER system.All these resources entail different forms of synsetclustering.
In order to merge them, we use thelargest overlap among them.
It is important to notethat the choice of the first resource considered hasa bearing on the way the synsets are clustered.
Inexperiments ran on the development data set, thebest results were obtained using a lexical resourceconstructed starting with the Microsoft Encarta The-saurus and then mapping the WordNet synsets to it.4 Candidate RankingSeveral ranking methods are used to score the can-didate substitutes, as described below.Lexical Baseline (LB): In this approach we usethe pre-existing lexical resources to provide a rank-ing over the candidate substitutes.
We rank the can-didates based on their occurrence in the two selectedlexical resources WordNet and Encarta, with thoseoccurring in both resources being assigned a higherranking.
This technique emphasizes the resourcesannotators?
agreement that the candidates belong in-deed to the same synset.Machine Translation (MT): We use machinetranslation to translate the test sentences back-and-forth between English and a second language.
Fromthe resulting English translation, we extract the re-placement that the machine translation engine pro-vides for the target word.
To locate the translatedword we scan the translation for any of the can-didates (and their inflections) as obtained from thecomprehensive resource, and score the candidatesynset accordingly.We experimented with a range of languages suchas French, Italian, Spanish, Simplified Chinese, andGerman, but the best results obtained on the devel-opment data were based on the French translations.This could be explained because French is part ofthe Romance languages family and synonyms to En-glish words often find their roots in Latin.
If weconsider again the word bright, it was translatedinto French as intelligent and then translated backinto English as intelligent for obvious reasons.
Inone instance, intelligent was the best replacementfor bright in the trial data.
Despite the fact that wealso used Italian and Spanish (which are both Latin-based) we can only assume that French worked bet-ter because translation engines are better trained onFrench.
From the resulting English translation, weextract the replacement that the machine translationengine provides for the target word.
To locate thetranslated word we scan the translation for any of thecandidates (and their inflections) as obtained fromthe comprehensive resource, and score the candidatesynset accordingly.
The translation process was car-ried out using Google and AltaVista translation en-gines resulting in two systems MTG and MTA re-spectively.
The translation systems feature high pre-cision when a candidate is found (about 20% of thetime), at the cost of low recall.
The lexical baselinemethod is therefore used when no candidates are re-turned by the translation method.Most Common Sense (MCS): Another methodwe use for ranking candidates is to consider thefirst word appearing in the first synset returned byWordNet.
When no words other than the targetword are available in this synset, the method recur-sively searches the next synset available for the tar-get word.
In order to guarantee a sufficient numberof candidates, we use the lexical baseline method asa baseline.Language Model (LM): We model the semanticfit of a candidate substitute within the given contextusing a language model, expressed using the condi-tional probability:P (c|g) = P (c, g)/P (g) ?
Count(c, g) (1)where c represents a possible candidate and g rep-resents the context.
The probability P (g) of thecontext is the same for all the candidates, hence wecan ignore it and estimate P (c|g) as the N-gram fre-quency of the context where the target word is re-placed by the proposed candidate.
To avoid skewedcounts that can arise from the different morpholog-ical inflections of the target word or the candidateand the bias that the context might have toward anyspecific inflection, we generalize P (c|g) to take intoaccount all the inflections of the selected candidateas shown in equation 2.Pn(c|g) ?n?i=1Count(ci, g) (2)where n is the number of possible inflections for thecandidate c.We use the Google N-gram dataset to calculate theterm Count(ci g).
The Google N-gram corpus is a411collection of English N-grams, ranging from one tofive N-grams, and their respective frequency countsobserved on the Web (Brants and Franz, 2006).
Inorder for the model to give high preference to thelonger N-grams, while maintaining the relative fre-quencies of the shorter N-grams (typically more fre-quent), we augment the counts of the higher orderN-grams with the maximum counts of the lower or-der N-grams, hence guaranteeing that the score as-signed to an N-gram of order N is higher than thethe score of an N-gram of order N ?
1.Semantic Relatedness using Latent SemanticAnalysis (LSA): We expect to find a strong se-mantic relationship between a good candidate andthe target context.
A relatively simple and efficientway to measure such a relatedness is the Latent Se-mantic Analysis (Landauer et al, 1998).
Documentsand terms are mapped into a 300 dimensional latentsemantic space, providing the ability to measure thesemantic relatedness between two words or a wordand a context.
We use the InfoMap package fromStanford University?s Center for the Study of Lan-guage and Information, trained on a collection ofapproximately one million Wikipedia articles.
Therank of a candidate is given by its semantic related-ness to the entire context sentence.Information Retrieval (IR): Although the Lan-guage Model approach is successful in ranking thecandidates, it suffers from the small N-gram size im-posed by using the Google N-grams corpus.
Sucha restriction is obvious in the following 5-gram ex-ample who was a bright boy in which the contextis not sufficient to disambiguate between happy andsmart as possible candidates.
As a result, we adaptan information retrieval approach which uses all thecontent words available in the given context.
Similarto the previous models, the target word in the con-text is replaced by all the generated inflections ofthe selected candidate and then queried using a websearch engine.
The resulting rank represents the sumof the total number of pages in which the candidateor any of its inflections occur together with the con-text.
This also reflects the semantic relatedness orthe relevance of the candidate to the context.Word Sense Disambiguation (WSD): Since pre-vious work indicated the usefulness of word sensedisambiguation systems in lexical substitution (Da-gan et al, 2006), we use the SenseLearner wordsense disambiguation tool (Mihalcea and Csomai,2005) to disambiguate the target word and, accord-ingly, to propose its synonyms as candidates.Final System: Our candidate ranking methods areaimed at different aspects of what constitutes a goodcandidate.
On one hand, we measure the semanticrelatedness of a candidate with the original context(the LSA and WSD methods fall under this cate-gory).
On the other hand, we also want to ensurethat the candidate fits the context and leads to a wellformed English sentence (e.g., the language modelmethod).
Given that the methods described earlieraim at orthogonal aspects of the problem, it is ex-pected that a combination of these will provide abetter overall ranking.We use a voting mechanism, where we considerthe reciprocal of the rank of each candidates as givenby one of the described methods.
The final score ofa candidate is given by the decreasing order of theweighted sum of the reciprocal ranks:score (ci) =?m?rankings?m1rmciTo determine the weight ?
of each individualranking we run a genetic algorithm on the develop-ment data, optimized for the mode precision and re-call.
Separate sets of weights are obtained for thebest and oot tasks.
Table 1 shows the weights ofthe individual ranking methods.
As expected, forthe best task, the language model type of methodsobtain higher weights, whereas for the oot task, thesemantic methods seem to perform better.5 Results and DiscussionThe SUBFINDER system participated in the best andthe oot tracks of the lexical substitution task.
Thebest track calls for any number of best guesses,with the most promising one listed first.
The creditfor each correct guess is divided by the number ofguesses.
The oot track allows systems to make up to10 guesses, without penalizing, and without being ofany benefit if less than 10 substitutes are provided.The ordering of guesses in the oot metric is unim-portant.For both tracks, the evaluation is carried out usingprecision and recall, calculated based on the numberof matching responses between the system and thehuman annotators, respectively.
A ?mode?
evalua-tion is also conducted, which measures the ability ofthe systems to capture the most frequent response(the ?mode?)
from the gold standard annotations.For details, please refer to the official task descrip-tion document (McCarthy and Navigli, 2007).Tables 2 and 3 show the results obtained by SUB-FINDER in the best and oot tracks respectively.
Thetables also show a breakdown of the results based412on: only target words that were not identified asmultiwords (NMWT); only substitutes that were notidentified as multiwords (NMWS); only items withsentences randomly selected from the Internet cor-pus (RAND); only items with sentences manually se-lected from the Internet corpus (MAN).WSD LSA IR LB MCS MTA MTG LMbest 34 2 64 63 56 69 38 97oot 6 82 7 28 46 14 32 68Table 1: Weights of the individual ranking methodsP R Mode P Mode ROVERALL 12.77 12.77 20.73 20.73Further AnalysisNMWT 13.46 13.46 21.63 21.63NMWS 13.79 13.79 21.59 21.59RAND 12.85 12.85 20.18 20.18MAN 12.69 12.69 21.35 21.35BaselinesWORDNET 9.95 9.95 15.28 15.28LIN 8.84 8.53 14.69 14.23Table 2: BEST resultsP R Mode P Mode ROVERALL 49.19 49.19 66.26 66.26Further AnalysisNMWT 51.13 51.13 68.03 68.03NMWS 54.01 54.01 70.15 70.15RAND 51.71 51.71 68.04 68.04MAN 46.26 46.26 64.24 64.24BaselinesWORDNET 29.70 29.35 40.57 40.57LIN 27.70 26.72 40.47 39.19Table 3: OOT resultsCompared to other systems participating in thistask, our system consistently ranks on the first orsecond place.
SUBFINDER clearly outperforms allthe other systems for the ?mode?
evaluation, show-ing the ability of the system to find the substitutemost often preferred by the human annotators.
Inaddition, the system exceeds by a large margin allthe baselines calculated for the task, which selectsubstitutes based on existing lexical resources (e.g.,WordNet or Lin distributional similarity).Separate from the ?official?
submission, we rana second experiment where we optimized the com-bination weights targeting high precision and recall(rather than high mode).
An evaluation of the systemusing this new set of weights yields a precision andrecall of 13.34 with a mode of 21.71 for the best task,surpassing the best system according to the anony-mous results report.
For the oot task, the precisionand recall increased to 50.30, still maintaining sec-ond place.6 ConclusionsThe lexical substitution task goes beyond simpleword sense disambiguation.
To approach such atask, we first need a good comprehensive and preciselexical resource for candidate extraction.
Secondly,we need to semantically filter the highly diverse andambiguous set of candidates, while taking into ac-count their fitness in the context in order to forma proper linguistic expression.
To accomplish this,we built a system that incorporates lexical, semantic,and probabilistic methods to capture both the seman-tic similarity with the target word and the semanticfit in the context.
Compared to other systems partic-ipating in this task, our system consistently ranks onthe first or second place.
SUBFINDER clearly out-performs all the other systems for the ?mode?
eval-uation, proving its ability to find the substitute mostoften preferred by the human annotators.AcknowledgmentsThis work was supported in part by the Texas Ad-vanced Research Program under Grant #003594.The authors are grateful to the Language and Infor-mation Technologies research group at the Univer-sity of North Texas for many useful discussions andfeedback on this work.ReferencesT.
Brants and A. Franz.
2006.
Web 1t 5-gram version 1.Linguistic Data Consortium.I.
Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,and C. Strapparava.
2006.
Direct word sense match-ing for lexical substitution.
In Proceedings of the In-ternational Conference on Computational LinguisticsACL/COLING 2006.C.
Fellbaum.
1998.
WordNet, An Electronic LexicalDatabase.
The MIT Press.T.
K. Landauer, P. Foltz, and D. Laham.
1998.
Introduc-tion to latent semantic analysis.
Discourse Processes,25.D.
McCarthy and R. Navigli.
2007.
The semeval Englishlexical substitution task.
In Proceedings of the ACLSemeval workshop.D.
McCarthy.
2002.
Lexical substitution as a task forwsd evaluation.
In Proceedings of the ACL Workshopon Word Sense Disambiguation: Recent Successes andFuture Directions, Philadelphia.R.
Mihalcea and A. Csomai.
2005.
Senselearner: Wordsense disambiguation for all words in unrestricted text.In Proceedings of the 43nd Annual Meeting of the As-sociation for Computational Linguistics, Ann Arbor,MI.413
