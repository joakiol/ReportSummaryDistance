THE BOUNDARY BETWEEN WORD KNOWLEDGEAND WORLD KNOWLEDGEJudy KeglThe Program in LinguisticsPrinceton UniversityPrinceton, New Jersey 08544I will focus my comments on linguistic considerations concerning the interrelation betweenwords and world representation and will argue that word knowledge must be kept distinct fromworld knowledge.
Knowing everything about a referent object or event with which a word is as-sociated is not enough to allow one to use a word appropriately.
World knowledge must be sup-plemented and constrained by linguistic knowledge to yield an appropriate account of wordknowledge.Let's start with a relatively straightforward example, one which is now so commonplace thatI am unsure of who to cite for its introduction into the literature.
Consider the worddanceJdanser in both English and French.
These cognates might at first glance appear to be al-most interchangeable glosses for one another.
Certainly, when looking at the event we think of asdancing, both English and French speakers ee and experience the same physical event.
Detailsof the exact instantiation of dancing might vary from culture to culture, but I think we wouldagree that we all share the same concept of what constitutes a dancing event.
Yet speakers ofEnglish and French do not use their "words" for this event in parallel ways.
In English, for ex-ample, we use the verb dance to refer to both a translatory and a nontranslatory event (wheretranslatory means to move along a path from one place to another).
So, 'for example, the Englishsentence John and Mary danced across the room is ambiguous.
It can mean either ' John andMary went from one end of the room to the other while dancing' or 'John and Mary were locatedat the other end of the room dancing.'
French, on the other hand, allows only the nontranslatoryreading where John and Mary are at some location engaged in the act of dancing.
French speak-ers have no problem describing the translatory event of dancing from one end of the room to theother, they just don't use the word danser in this context.
They speak of crossing the room whiledancing.
Somewhere along the road between French and English world knowledge and wordknowledge seem to have diverged.
What's interesting about such examples is that this diver-gence isn't a freak occurrence.
The divergence seen with dance and danser extends to a wholeclass of related verbs.
In other words, the behavior of these verbs is a reflex of some language-specific regularities.However, even though there exists a considerable amount of freedom with regard to what as-pects of our world knowledge of an object or event are encoded in its linguistic alter ego (theword), the range of possibilities is still severely constrained.
Not everything we know about anobject or an event is a candidate for inclusion in its linguistic representation.
For example, coloris a salient property we associate with the objects around us, but no language in the world exhi-bits a linguistic classifier system based on color.
Shape, solidity and flexibility, on the otherhand, are at the heart of a large majority of the world's classifier systems.
Time and space seemto have equal status in the world of conceptual knowledge, yet languages prefer to speak of timein spatial terms (at 6:00, from Tuesday to Thursday, toward evening, in 1976, Winter is fast ap-28proaching) and not vice versa.
From the time of the Byzantine Planudes through the writings ofHjelmslev (1935) and even into the more recent work of Cruber (1976) and Jackendoff (1983), theclaim has been made that motion and location play a special role in the organization of language.Even abstract verb classes (verbs of cognition, perception, emotion, etc.)
involve figurative ex-tensions of basic motion/location relations (to feel down, to think something through).
When westop to think about it, many possibilities for classification come to mind, but the world'slanguages seem to select their options from a very restricted set.
Determining these preferred en-coding strategies across languages is one way to get at a linguistically relevant set of semanticprimitives.
The relevant linguistic phenomena to examine are verb classes (cognition, perception,emotion, motion/location, change of state, bodily care, etc.
), thematic roles (source, goal, theme)and predicate argument structure, mappings between semantic roles (agent, patient) and gram-matical relations (subject, object), and grammatical functions such as causation.
All of thesephenomena straddle the boundary between linguistic conceptual structure and conceptual struc-ture in general.Acquisition work by the Clarks (E. Clark 1973, H. Clark 1973) presents ome ties betweenperception and language and raises some interesting issues concerning the interface between per-ceptual and linguistic knowledge, and the mechanism by which a child might use perceptualknowledge as a bootstraping device to link words and world knowledge into an already existantinnate linguistic knowledge.Moving to evidence from language typology, we find that even though grammatical relationsand semantic roles (agent, patient) are evidently part of universal grammar, the link betweenthem is subject to variation and must be determined on the basis of language-specific evidence.Otherwise, how could Nominative/Accusative languages (languages where the object is themarked case) map agent to subject and patient to object, whereas Ergative languages (thosewhere the subject of a transitive is the marked case) do the reverse, mapping agent to object andpatient o subject.
This is a simplified statement of the facts (see Levin 1983 for more detail), butit is explicit enough to make my point here, which is that certain links between world knowledgeand linguistic knowledge are in a very constrained sense arbitrary.Recent work on lexical conceptual structures coming out of the Lexicon Project at MIT castssome of the words and world representation issues in a new light.
Hale and his colleagues (Haleand Laughren 1983, Hale and Keyser 1986) have proposed a level of representation called Iexicalconceptual structure (LCS) which follows from and extends Jackendoff's work on predicatedecomposition.
This work involves breaking the definition of a predicator into a series of sub-components of meaning, subcomponents which prove to  be relevant to the predicate argumentstructure of these lexical items.
The LCS offers insight into the mapping between lexicalrepresentations and syntactic configurations.
In my opinion, when we speak of lexical semanticrepresentation we must keep in mind that the semantic representation f a word relevant o itsfunctioning as a linguistic entity is separate from the much wider range of information whichmust be considered to be part of its meaning.
For example, connotation is relevant o the mean-ing of a word but it is not part of its lexical semantic representation.Consider another crosslinguistic discussion.
The meaning of a word like thirst has the poten-tial for yielding multiple LCSs only some of which might be relevant o its linguistic behavior.29Take the sensation of being thirsty, which we share with all other human beings, and let's agreefor the sake of argument that we can all share the same concept of a thirst event.
When we lookat how that event is spoken of across languages, we find that it is linguistically encoded in avariety of different ways.
In some languages the lexical semantic representation for 'to thirst'might be nontranslatory (I have thirst, Thirst is at me, I am at thirst (in a state of being thir-sty)); or it might be translatory (I go to thirst, I am getting to be thirsty, Thirst has come uponme, etc.).
Notice that even within t~he set of translatory versus nontranslatory options thethematic roles assigned to thirst can vary.
In some instances it serves as the theme (the entitywhich moves or is located), and in other instances it serves as an anchorpoint which the ex-periencer moves to (goal) or is located at (location).
\[ am sure if we looked hard enough wewould also find examples where a source is involved (I am running from thirst, Thirst comes fromme).
All of the above figure/ground relations and translatory/nontranslatory options are compa-tible with the meaning of thirst, and they all pick from the same set of building blocks comprisedof motion/location relations and thematic roles.
Still, which encoding to use as the basis of aLCS for a verb of thirsting is a language-specific choice.
And, certain specific syntactic and se-mantic consequences will follow from that choice--similar in nature to the syntactic and semanticdifferences between dance and danser.Actually, the work on LCSs raises two even more basic questions: What is a word?
And,what does it mean to know a word?
A word is not the header associated with a lexical entry.That is simply a label associated with a given word.
A word is a more abstract entity which isprobably best thought of as a LCS plus information concerning what components of that LCS areprojected into the syntax, its predicate argument structure.
The label move in English, for exam-ple, is associated with at least two words, one nontraaslatory (I saw it move), and the othertranslatory (She moved to the other side of the room).
In fact, some languages give distinct la-bels to these two words.Knowing a word entails not only knowing what it means, but also knowing how to use it.Part of knowing a verb is knowing the various ways it can realize its arguments, and, if it per-mits more than one way, what sets each of the alternatives apart.
Examination of this aspect oflexical knowledge gets us into the nifty gritty of what word representation is all about.
It alsobrings to light some of the more perplexing problems which must be faced in the process ofdeveloping "reverse engineering" techniques for extracting information from dictionaries.
I willmention not only the problems, but also the potential benefits to be gained from systematic ex-amination of learner's dictionaries and large corpora of texts.
Atkins, Kegl and Levin (in press)discusses the explicit and implicit information to be found in learner's dictionaries uch as theLongman Dictionary of Contemporary English (LDOCE), the Ozford Advanced Learner's Diction-ary of Contemporary English (OALD) and the Collins English-Learner's Dictionary.
In this pa-per, we pointed out complex form/meaning interdependencies which can only be extracted fromexisting dictionaries by detailed examination of converging information provided in the three cen-tral components of a dictionary entry: the syntactic ode, the sense information and the examplesentences.Consider the problem.
Many verbs exhibit transitivity alternations where they appear as ei-ther transitive or intransitive.
See the examples below which are discussed in the Atkins, Kegland Levin paper:30Indefinite (Unspecified) Object Alternation:(1) a. Mike ate the cake.b.
Mike ate.
(=Mike ate food or a meal.
)Characteristic Property Alternation:(2) a.
That dog bites people.b.
That dog bites.
(=That dog is a biter.
)Reciprocal Alternation:(3) a. Anne met Cathy.b.
Anne and Cathy met.
(=Anne and Cathy met each other.
)Reflexive Alternation:(4) a. Jill dressed herself hurriedly.b.
Jill dressed hurriedly.
(=Jill dressed herself hurriedly.
)Causative/Inchoative (also Anti-Causative or Ergative) Alternation:(5) a. Janet broke the cup.
(=Janet caused the cup to break.)b.
The cup broke.
(how the cup broke is left unspecified)Instrumental Alternation:(6) a.
The scissors won't cut the denim.b.
The scissors won't cut.The preceding examples not only participate in transitivity alternations, but the interpreta-tion assigned to their intransitive forms and the thematic role assigned to their subjects variesaccording to the verb class to which each belongs.
When examining pairs such as those above, itquickly becomes clear that a simple transitive/intransitive distinction will not suffice.
There is acomplex interaction between verb class, sense and transitivity which must be recognized.
Workon extracting significant lexical information from machine-readable dictionaries such as theresearch described by Boguraev (1986) requires extremely complex computations on a data basenot originally designed with such a use in mind.
Although learner's dictionaries are phenomenal-ly comprehensive, they are not as consistent in their presentation of the lexical data as one mighthope.
Therefore, a lot of cleaning up needs to be done before even the best machine-readable dlc-tionary can reveal the form/meaning interdependencies mentioned above.
The project is, howev-er, well worth the effort.
One suggestion I would like to make is that we not confine ourselves todecoding simply the syntactic code and sense portions of the dictionary.
The example sentenceshave a lot to offer.
Contrary to their role in etymological dictionaries or more general purposedictionaries where they serve to document he chronological occurrences of a lexical item or itsattested occurrences in literary texts, example sentences in learner's dictionaries are another formof code.
The sentences which exemplify certain verb classes or types of transitivity alternationsare parallel in form, so much so that one can often determine the possibilities for syntactic pat-terns and verb class membership from the examples alone.
And, frequently the examples cue theuser into usage patterns that the entry itself has failed to include in its metalinguistic om-ponents.
Finally, a pair of sentences and their associated interpretations are frequently the mosteffective means of conveying cross-classificatory information to a user.
Even if a dictionary wereable to exhaustively convey this information in its code, it is not clear that such information31would be easily understood by the user.
Furthermore, the more we develop tools for extractingdictionary information from carefully chosen example sentences, the closer we will get to extract-ing such information from unrestricted texts.Determining verb class membership and the syntactic and semantic properties of a given lex-ical item also depends upon world knowledge that the user brings to the dictionary, knowledgewhich is pre-assumed by the sense definitions.
In analyzing dictionary entries in LDOCE andOALD, we found many inconsistencies in the presentation of lexical entries which only becameobvious when we examined what one would get from the explicit and implicit regularitiespresented alone, without certain common sense and experiential knowledge we are assumed tobring to the task.
Unless the "reverse ngineering device" also comes to the task with such as-sumed knowledge, we won't reap the maximum benefits from this dictionary research.A final postscript to this discussion concerns current efforts to develop a theory neutral exi-con which can be used by a variety of parsing and generation programs.
Such a goal can beachieved, but I think it is important o keep in mind how that end product will be achieved andwhat it has to offer theory-based parsers and generation systems.
The neutral exicon must bereductionist in nature.
It can only be arrived at by continually breaking into subcomponentsthose units currently utilized by well-articulated linguistic theories.
Then we will still need aseries of overlay systems which will combine those commonly shared units into the chunks madereference to in each individual framework.
Thus it seems that building a neutral exicon requiresa well articulated lexicon from each theory purporting to use this neutral exicon and a well arti-culated set of rules to translate the neutral exicon into a form the framework can utilize.
On theoptimistic sid.e, although we work hard to maintain our individual identity, most linguists doseem to agree on most of the basic units of analysis as is evidenced by the fact that we continual-ly translate the data from one theory into another in order to argue against it or in order to usethe basic facts in support of our own analyses.
Work toward a neutral lexicon will, if nothingelse, lead us to a better understanding of the subsets of data successfully handled in differentframeworks and to a more precise characterization of those points on which we agree anddisagree.REFERENCESAtkins, B.T., J. Kegl and B. Levin (in press) "Explicit and Implicit Information in Dictionaries.
"In Proceedings of the Conference on Advances in Lexicology, Center for the New OxfordEnglish Dictionary, Waterloo.Boguraev, B.
(1986) "Machine-Readable Dictionaries and Research in Computational Linguis-tics."
In Walker, et al, eds., Proceedings of the Workshop on Automating the Lexicon,Grosseto, Italy.Carver, D.J.
(1974) et al, eds, Collins English Learner's Dictionary, Collins, Ltd., London.Clark, E. (1973) "Non-Linguistic Strategies and the Acquisition of Word Meaning," Cognition,vol.
2, pp.
161-182.Clark, H. (1973) "Space, Time, Semantics and the Child."
In T.E.
Moore, Cognitive Develop-32Kegl TINLAP3ment and the Acquisition of Language, Academic Press, New York.Gruber, J.
(1976) Lezical Structures in Syntaz and Semantics, North-Holland, Amsterdam.Hale, K. and S.J.
Keyser (1986) "A View from the Middle," Lexicon Working Papers 10, Centerfor Cognitive Science, MIT, Cambridge, MA.Hale, K. and M. Laughren (1983) "Preface to Dictionary Entries of Verbs."
Unpublishedmanuscript, MIT, Cambridge, MA.Hjelmslev, L. (1935) "La Categoric des Cas," Acta Jutlandica, vol.
7, pp.
i-xiii, 1-184.Hornby, A.S. and A.P.
Cowie, eds.
(1974, first edition 1948) Ozford Advanced Learner's Diction-ary of Current English, Oxford University Press, Oxford.Jackendoff, R. (1983) Semantics and Cognition, MIT Press, Cambridge, MA.Levin, B.
(1983) On the Nature of Ergativity.
Unpublished octoral dissertation, MIT, Cam-bridge, MA.Proctor, P., ed.
(1978) Longman Dictionary of Contemporary English, Longman, London.33
