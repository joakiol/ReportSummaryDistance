Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 136?143,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsNot a simple yes or no: Uncertainty in indirect answersMarie-Catherine de Marneffe, Scott Grimm and Christopher PottsLinguistics DepartmentStanford UniversityStanford, CA 94305{mcdm,sgrimm,cgpotts}@stanford.eduAbstractThere is a long history of using logic tomodel the interpretation of indirect speechacts.
Classical logical inference, how-ever, is unable to deal with the combina-tions of disparate, conflicting, uncertainevidence that shape such speech acts indiscourse.
We propose to address this bycombining logical inference with proba-bilistic methods.
We focus on responsesto polar questions with the following prop-erty: they are neither yes nor no, butthey convey information that can be usedto infer such an answer with some de-gree of confidence, though often not withenough confidence to count as resolving.We present a novel corpus study and asso-ciated typology that aims to situate theseresponses in the broader class of indirectquestion?answer pairs (IQAPs).
We thenmodel the different types of IQAPs usingMarkov logic networks, which combinefirst-order logic with probabilities, empha-sizing the ways in which this approach al-lows us to model inferential uncertaintyabout both the context of utterance and in-tended meanings.1 IntroductionClark (1979), Perrault and Allen (1980), and Allenand Perrault (1980) study indirect speech acts,identifying a wide range of factors that govern howspeakers convey their intended messages and howhearers seek to uncover those messages.
Prior dis-course conditions, the relationship between the lit-eral meaning and the common ground, and spe-cific lexical, constructional, and intonational cuesall play a role.
Green and Carberry (1992, 1994)provide an extensive computational model that in-terprets and generates indirect answers to polarquestions.
Their model focuses on inferring cat-egorical answers, making use of discourse plansand coherence relations.This paper extends such work by recasting theproblem in terms of probabilistic modeling.
Wefocus on the interpretation of indirect answerswhere the respondent does not answer with yes orno, but rather gives information that can be usedby the hearer to infer such an answer only withsome degree of certainty, as in (1).
(1) A: Is Sue at work?B: She is sick with the flu.In this case, whether one can move from the re-sponse to a yes or no is uncertain.
Based on typicalassumptions about work and illness, A might takeB?s response as indicating that Sue is at home, butB?s response could be taken differently dependingon Sue?s character ?
B could be reproaching Suefor her workaholic tendencies, which risk infect-ing the office, or B could be admiring Sue?s stead-fast character.
What A actually concludes aboutB?s indirect reply will be based on some combi-nation of this disparate, partially conflicting, un-certain evidence.
The plan and logical inferencemodel of Green and Carberry falters in the face ofsuch collections of uncertain evidence.
However,natural dialogues are often interpreted in the midstof uncertain and conflicting signals.
We thereforepropose to enrich a logical inference model withprobabilistic methods to deal with such cases.This study addresses the phenomenon of indi-rect question?answer pairs (IQAP), such as in (1),from both empirical and engineering perspectives.136First, we undertake a corpus study of polar ques-tions in dialogue to gather naturally occurring in-stances and to determine how pervasive indirectanswers that indicate uncertainty are in a natu-ral setting (section 2).
From this empirical base,we provide a classification of IQAPs which makesa new distinction between fully- and partially-resolving answers (section 3).
We then show howinference in Markov logic networks can success-fully model the reasoning involved in both typesof IQAPs (section 4).2 Corpus studyPrevious corpus studies looked at how pervasiveindirect answers to yes/no questions are in dia-logue.
Stenstro?m (1984) analyzed 25 face-to-faceand telephone conversations and found that 13%of answers to polar questions do not contain anexplicit yes or no term.
In a task dialogue, Hockeyet al (1997) found 38% of the responses wereIQAPs.
(This higher percentage might reflect thegenre difference in the corpora used: task dialoguevs.
casual conversations.)
These studies, how-ever, were not concerned with how confidently onecould infer a yes or no from the response given.We therefore conducted a corpus study to ana-lyze the types of indirect answers.
We used theSwitchboard Dialog Act Corpus (Jurafsky et al,1997) which has been annotated for approximately60 basic dialog acts, clustered into 42 tags.
Weare concerned only with direct yes/no questions,and not with indirect ones such as ?May I remindyou to take out the garbage??
(Clark, 1979; Per-rault and Allen, 1980).
From 200 5-minute con-versations, we extracted yes/no questions (tagged?qy?)
and their answers, but discarded tag ques-tions as well as disjunctive questions, such as in(2), since these do not necessarily call for a yesor no response.
We also did not take into accountquestions that were lost in the dialogue, nor ques-tions that did not really require an answer (3).
Thisyielded a total of 623 yes/no questions.
(2) [sw 0018 4082]A: Do you, by mistakes, do you mean justlike honest mistakesA: or do you think they are deliberate sortsof things?B: Uh, I think both.
(3) [sw 0070 3435]A: How do you feel about your game?A: I guess that?s a good question?B: Uh, well, I mean I?m not a seriousgolfer at all.To identify indirect answers, we looked at theanswer tags.
The distribution of answers is givenin Table 1.
We collapsed the tags into 6 categories.Category I contains direct yes/no answers as wellas ?agree?
answers (e.g., That?s exactly it.).
Cate-gory II includes statement?opinion and statement?non-opinion: e.g., I think it?s great, Me I?m in thelegal department, respectively.
Affirmative non-yes answers and negative non-no answers formcategory III.
Other answers such as I don?t knoware in category IV.
In category V, we put utterancesthat avoid answering the question: by holding (I?mdrawing a blank), by returning the question ?
wh-question or rhetorical question (Who would steala newspaper?)
?
or by using a backchannel inquestion form (Is that right?).
Finally, categoryVI contains dispreferred answers (Schegloff et al,1977; Pomerantz, 1984).We hypothesized that the phenomenon we arestudying would appear in categories II, III and VI.However, some of the ?na/ng?
answers are dis-guised yes/no answers, such as Right, I think so,or Not really, and as such do not interest us.
In thecase of ?sv/sd?
and ?nd?
answers, many answersinclude reformulation, question avoidance (see 4),or a change of framing (5).
All these cases are notreally at issue for the question we are addressing.
(4) [sw 0177 2759]A: Have you ever been drug tested?B: Um, that?s a good question.
(5) [sw 0046 4316]A: Is he the guy wants to, like, deregulateheroin, or something?B: Well, what he wants to do is take all themoney that, uh, he gets for drugenforcement and use it for, uh, drugeducation.A: Uh-huh.B: And basically, just, just attack theproblem at the demand side.137Definition Tag TotalI yes/no answers ny/nn/aa 341II statements sv/sd 143III affirmative/negative non-yes/no answers na/ng 91IV other answers no 21V avoid answering ?h/qw/qh/bh 18VI dispreferred answers nd 9Total 623Table 1: Distribution of answer tags to yes/no questions.
(6) [sw 0046 4316]A: That was also civil?B: The other case was just traffic, and youknow, it was seat belt law.We examined by hand all yes/no questions forIQAPs and found 88 examples (such as (6), and(7)?
(11)), which constitutes thus 14% of the totalanswers to direct yes/no questions, a figure simi-lar to those of Stenstro?m (1984).
The next sectionintroduces our classification of answers.3 Typology of indirect answersWe can adduce the general space of IQAPs fromthe data assembled in section 2 (see also Bolinger,1978; Clark, 1979).
One point of departure is that,in cooperative dialogues, a response to a ques-tion counts as an answer only when some relationholds between the content of the response and thesemantic desiderata of the question.
This is suc-cinctly formulated in the relation IQAP proposedby Asher and Lascarides (2003), p.
403:IQAP(?,?)
holds only if there is a truedirect answer p to the question J?K, andthe questioner can infer p from J?K inthe utterance context.The apparent emphasis on truth can be set asidefor present purposes; Asher and Lascarides?s no-tions of truth are heavily relativized to the currentdiscourse conditions.
This principle hints at twodimensions of IQAPs which must be considered,and upon which we can establish a classification:(i) the type of answer which the proffered responseprovides, and (ii) the basis on which the inferencesare performed.
The typology established here ad-heres to this, distinguishing between fully- andpartially-resolving answers as well as between thetypes of knowledge used in the inference (logical,linguistic, common ground/world).3.1 Fully-resolving responsesAn indirect answer can fully resolve a questionby conveying information that stands in an inclu-sion relation to the direct answer: if q ?
p (or?p), then updating with the response q also re-solves the question with p (or ?p), assuming thequestioner knows that the inclusion relation holdsbetween q and p. The inclusion relation can bebased on logical relations, as in (7), where the re-sponse is an ?over-answer?, i.e., a response wheremore information is given than is strictly neces-sary to resolve the question.
Hearers supply moreinformation than strictly asked for when they rec-ognize that the speaker?s intentions are more gen-eral than the question posed might suggest.
In (7),the most plausible intention behind the query isto know more about B?s family.
The hearer canalso identify the speaker?s plan and any necessaryinformation for its completion, which he then pro-vides (Allen and Perrault, 1980).
(7) [sw 0001 4325]A: Do you have kids?B: I have three.While logical relations between the content ofthe question and the response suffice to treat exam-ples such as (7), other over-answers often requiresubstantial amounts of linguistic and/or world-knowledge to allow the inference to go through,as in (8) and (9).
(8) [sw 0069 3144]A: Was that good?B: Hysterical.
We laughed so hard.
(9) [sw 0057 3506]A: Is it in Dallas?B: Uh, it?s in Lewisville.138In the case of (8), a system must recognizethat hysterical is semantically stronger than good.Similarly, to recognize the implicit no of (9), a sys-tem must recognize that Lewisville is a distinctlocation from Dallas, rather than, say, containedin Dallas, and it must include more general con-straints as well (e.g., an entity cannot be in twophysical locations at once).
Once the necessaryknowledge is in place, however, the inferences areproperly licensed.3.2 Partially-resolving responsesA second class of IQAPs, where the content ofthe answer itself does not fully resolve the ques-tion, known as partially-resolved questions (Groe-nendijk and Stokhof, 1984; Zeevat, 1994; Roberts,1996; van Rooy, 2003), is less straightforward.One instance is shown in (10), where the gradableadjective little is the source of difficulty.
(10) [sw 0160 3467]A: Are they [your kids] little?B: I have a seven-year-old and aten-year-old.A: Yeah, they?re pretty young.The response, while an answer, does not, in andof itself, resolve whether the children should beconsidered little.
The predicate little is a grad-able adjective, which inherently possesses a de-gree of vagueness: such adjectives contextuallyvary in truth conditions and admit borderline cases(Kennedy, 2007).
In the case of little, while somechildren are clearly little, e.g., ages 2?3, and someclearly are not, e.g., ages 14?15, there is anotherclass in between for which it is difficult to as-sess whether little can be truthfully ascribed tothem.
Due to the slippery nature of these predi-cates, there is no hard-and-fast way to resolve suchquestions in all cases.
In (10), it is the questionerwho resolves the question by accepting the infor-mation proffered in the response as sufficient tocount as little.The dialogue in (11) shows a second example ofan answer which is not fully-resolving, and inten-tionally so.
(11) [sw 0103 4074]A: Did he raise him [the cat] orsomething1?1The disjunct or something may indicate that A is openB: We bought the cat for him and so he?sbeen the one that you know spent themost time with him.Speaker B quibbles with whether the relationhis son has to the cat is one of raising, instead cit-ing two attributes that go along with, but do notdetermine, raising.
Raising an animal is a com-posite relation, which typically includes the rela-tions owning and spending time with.
However,satisfying these two sub-relations does not strictlyentail satisfying the raising relation as well.
Itis not obvious whether a system would be mis-taken in attributing a fully positive response to thequestion, although it is certainly a partially posi-tive response.
Similarly, it seems that attributinga negative response would be misguided, thoughthe answer is partly negative.
The rest of the dia-logue does not determine whetherA considers thisequivalent to raising, and the dialogue proceedshappily without this resolution.The preceding examples have primarily hingedupon conventionalized linguistic knowledge, viz.what it means to raise X or for X to be little.
Afurther class of partially-resolving answers relieson knowledge present in the common ground.
Ourinitial example (1) illustrates a situation where dif-ferent resolutions of the question were possible de-pending on the respondent?s intentions: no if sym-pathetic, yes if reproachful or admiring.The relationship between the response andquestion is not secured by any objective worldfacts or conventionalized meaning, but ratheris variable ?
contingent on specialized worldknowledge concerning the dialogue participantsand their beliefs.
Resolving such IQAPs positivelyor negatively is achieved only at the cost of a de-gree of uncertainty: for resolution occurs againstthe backdrop of a set of defeasible assumptions.3.3 IQAP classificationTable 2 is a cross-classification of the examplesdiscussed by whether the responses are fully- orpartially-resolving answers and by the types ofknowledge used in the inference (logical, linguis-tic, world).
It gives, for each category, the countsof examples we found in the corpus.
The partially-resolved class contains more than a third of the an-swers.to hearing about alternatives to raise.
We abstract away fromthis issue for present purposes and treat the more general caseby assuming A?s contribution is simply equivalent to ?Did heraise him?
?139Logic Linguistic World TotalFully-Resolved 27 (Ex.
7) 18 (Ex.
8) 11 (Ex.
9) 56Partially-Resolved ?
20 (Ex.
10;11) 12 (Ex.
1) 32Table 2: Classification of IQAPs by knowledge type and resolvedness: counts and examples.The examples given in (7)?
(9) are fully resolv-able via inferences grounded in logical relations,linguistic convention or objective facts: the an-swer provides enough information to fully resolvethe question, and the modeling challenge is secur-ing and making available the correct information.The partially-resolved pairs are, however, qualita-tively different.
They involve a degree of uncer-tainty that classical inference models do not ac-commodate in a natural way.4 Towards modeling IQAP resolutionTo model the reasoning involved in all types ofIQAPs, we can use a relational representation, butwe need to be able to deal with uncertainty, ashighlighted in section 3.
Markov logic networks(MLNs; Richardson and Domingos, 2006) exactlysuit these needs: they allow rich inferential reason-ing on relations by combining the power of first-order logic and probabilities to cope with uncer-tainty.
A logical knowledge-base is a set of hardconstraints on the set of possible worlds (set ofconstants and grounded predicates).
In Markovlogic, the constraints are ?soft?
: when a world vi-olates a relation, it becomes less probable, but notimpossible.
A Markov logic network encodes aset of weighted first-order logic constraints, suchthat a higher weight implies a stronger constraint.Given constants in the world, the MLN creates anetwork of grounded predicates which applies theconstraints to these constants.
The network con-tains one feature fj for each possible grounding ofeach constraint, with a value of 1 if the groundedconstraint is true, and 0 otherwise.
The probabilityof a world x is thus defined in terms of the con-straints j satisfied by that world and the weights wassociated with each constraint (Z being the parti-tion function):P (X = x) =1Z?jwjfj(x)In practice, we use the Alchemy implemen-tation of Markov logic networks (Kok et al,2009).
Weights on the relations can be hand-setor learned.
Currently, we use weights set by hand,which suffices to demonstrate that an MLN han-dles the pragmatic reasoning we want to model,but ultimately we would like to learn the weights.In this section, we show by means of a fewexamples how MLNs give a simple and elegantway of modeling the reasoning involved in bothpartially- and fully-resolved IQAPs.4.1 Fully-resolved IQAPsWhile the use of MLNs is motivated by partially-resolved IQAPs, to develop the intuitions behindMLNs, we show how they model fully-resolvedcases, such as in (9).
We define two distinct places,Dallas and Lewisville, a relation linking a per-son to a place, and the fact that person K is inLewisville.
We also add the general constraint thatan individual can be in only one place at a time,to which we assign a very high weight.
Markovlogic allows for infinite weights, which Alchemydenotes by a closing period.
We also assume thatthere is another person L, whose location is un-known.Constants and facts:Place = {Dallas, Lewisville}Person = {K,L}BeIn(Person,Place)BeIn(K,Lewisville)Constraints:// ?If you are in one place, you are not in another.?
(BeIn(x,y) ?
(y != z))?
!BeIn(x,z).Figure 1 represents the grounded Markov networkobtained by applying the constraint to the con-stants K, L, Dallas and Lewisville.
The graphcontains a node for each predicate grounding, andan arc between each pair of nodes that appear to-gether in some grounding of the constraint.
Giventhat input, the MLN samples over possible worlds,and infers probabilities for the predicate BeIn,based on the constraints satisfied by each worldand their weights.
The MLN returns a very lowprobability for K being in Dallas, meaning that theanswer to the question Is it in Dallas?
is no:BeIn(K,Dallas): 4.9995e-05140BeIn(K, Dallas) BeIn(K, Lewisville)BeIn(L, Lewisville)BeIn(L, Dallas)Figure 1: Grounded Markov network obtained by applying the constraints to the constants K, L, Dallasand Lewisville.Since no information about L?s location has beengiven, the probabilities of L being in Dallas orLewisville will be equal and low (0.3), which isexactly what one would hope for.
The probabili-ties returned for each location will depend on thenumber of locations specified in the input.4.2 Partially-resolved IQAPsTo model partially-resolved IQAPs appropriately,we need probabilities, since such IQAPs featurereasoning patterns that involve uncertainty.
Wenow show how we can handle three examples ofpartially-resolved IQAPs.Gradable adjectives.
Example (10) is a bor-derline case of gradable adjectives: the questionbears on the predicate be little for two children ofages 7 and 10.
We first define the constants andfacts about the world, which take into account therelations under consideration, ?BeLittle(X)?
and?Age(X, i)?, and specify which individuals we aretalking about, K and L, as well as their ages.Constants and facts:age = {0 .
.
.
120}Person = {K, L}Age(Person,age)BeLittle(Person)Age(K,7)Age(L,10)The relation between age and being little involvessome uncertainty, which we can model using a lo-gistic curve.
We assume that a 12-year-old childlies in the vague region for determining ?little-ness?
and therefore 12 will be used as the centerof the logistic curve.Constraints:// ?If you are under 12, you are little.
?1.0 (Age(x,y) ?
y < 12)?
BeLittle(x)// ?If you are above 12, you are not little.
?1.0 (Age(x,y) ?
y > 12)?
!BeLittle(x)// The constraint below links two instances of Be-Little.(Age(x,u)?Age(y,v)?
v>u?BeLittle(y))?Be-Little(x).Asking the network about K being little and Lbeing little, we obtain the following results, whichlead us to conclude that K and L are indeed littlewith a reasonably high degree of confidence, andthat the indirect answer to the question is heavilybiased towards yes.BeLittle(K): 0.92BeLittle(L): 0.68If we now change the facts, and say that K and Lare respectively 12 and 16 years old (instead of 7and 10), we see an appropriate change in the prob-abilities:BeLittle(K): 0.58BeLittle(L): 0.16L, the 16-year-old, is certainly not to be consid-ered ?little?
anymore, whereas the situation is lessclear-cut for K, the 12-year-old (who lies in thevague region of ?littleness?
that we assumed).Ideally, we would have information about thespeaker?s beliefs, which we could use to updatethe constraints?
weights.
Absent such information,we could use general knowledge from the Web tolearn appropriate weights.
In this specific case, wecould find age ranges appearing with ?little kids?in data, and fit the logistic curve to these.This probabilistic model adapts well to caseswhere categorical beliefs fit uneasily: for border-line cases of vague predicates (whose interpreta-tion varies by participant), there is no determinis-tic yes or no answer.141Composite relations.
In example (11), we wantto know whether the speaker?s son raised the catinasmuch as he owned and spent time with him.We noted that raise is a composite relation, whichentails simpler relations, in this case spend timewith and own, although satisfying any one of thesimpler relations does not suffice to guarantee thetruth of raise itself.
We model the constants, facts,and constraints as follows:Constants and Facts:Person = {K}Animal = {Cat}Raise(Person,Animal)SpendTime(Person,Animal)Own(Person,Animal)SpendTime(K,Cat)Own(K,Cat)Constraints:// ?If you spend time with an animal, you helpraise it.
?1.0 SpendTime(x,y)?
Raise(x,y)// ?If you own an animal, you help raise it.
?1.0 Own(x,y)?
Raise(x,y)The weights on the relations reflect how central wejudge them to be in defining raise.
For simplicity,here we let the weights be identical.
Clearly, thegreater number of relevant relations a pair of en-tities fulfills, the greater the probability that thecomposite relation holds of them.
Consideringtwo scenarios helps illustrate this.
First, suppose,as in the example, that both relations hold.
We willthen have a good indication that by owning andspending time with the cat, the son helped raisehim:Raise(K,Cat): 0.88Second, suppose that the example is different inthat only one of the relations holds, for instance,that the son only spent time with the cat, but didnot own it, and accordingly the facts in the net-work do not contain Own(K,Cat).
The probabilitythat the son raised the cat decreases:Raise(K,Cat): 0.78Again this can easily be adapted depending on thecentrality of the simpler relations to the compositerelation, as well as on the world-knowledge con-cerning the (un)certainty of the constraints.Speaker beliefs and common ground knowl-edge.
The constructed question?answer pairgiven in (1), concerning whether Sue is at work,demonstrated that how an indirect answer is mod-eled depends on different and uncertain evidence.The following constraints are intended to capturesome background assumptions about how we re-gard working, being sick, and the connections be-tween those properties:// ?If you are sick, you are not coming to work.?Sick(x)?
!AtWork(x)// ?If you are hardworking, you are at work.?HardWorking(x)?
AtWork(x)// ?If you are malicious and sick, you come towork.?
(Malicious(x) ?
Sick(x))?
AtWork(x)// ?If you are at work and sick, you are maliciousor thoughtless.?
(AtWork(x) ?
Sick(x)) ?
(Malicious(x) ?Thoughtless(x))These constraints provide different answers aboutSue being at work depending on how they areweighted, even while the facts remain the samein each instance.
If the first constraint is heavilyweighted, we get a high probability for Sue notbeing at work, whereas if we evenly weight all theconstraints, Sue?s quality of being a hard-workerdramatically raises the probability that she is atwork.
Thus, MLNs permit modeling inferencesthat hinge upon highly variable common groundand speaker beliefs.Besides offering an accurate treatment of fully-resolved inferences, MLNs have the ability to dealwith degrees of certitude.
This power is requiredif one wants an adequate model of the reasoninginvolved in partially-resolved inferences.
Indeed,for the successful modeling of such inferences, itis essential to have a mechanism for adding factsabout the world that are accepted to various de-grees, rather than categorically, as well as for up-dating these facts with speakers?
beliefs if such in-formation is available.5 ConclusionsWe have provided an empirical analysis and ini-tial treatment of indirect answers to polar ques-tions.
The empirical analysis led to a catego-rization of IQAPs according to whether their an-swers are fully- or partially-resolving and accord-ing to the types of knowledge used in resolving142the question by inference (logical, linguistic, com-mon ground/world).
The partially-resolving indi-rect answers injected a degree of uncertainty intothe resolution of the predicate at issue in the ques-tion.
Such examples highlight the limits of tradi-tional logical inference and call for probabilisticmethods.
We therefore modeled these exchangeswith Markov logic networks, which combine thepower of first-order logic and probabilities.
Asa result, we were able to provide a robust modelof question?answer resolution in dialogue, onewhich can assimilate information which is not cat-egorical, but rather known only to a degree of cer-titude.AcknowledgementsWe thank Christopher Davis, Dan Jurafsky, andChristopher D. Manning for their insightful com-ments on earlier drafts of this paper.
We also thankKaren Shiells for her help with the data collectionand Markov logic.ReferencesJames F. Allen and C. Raymond Perrault.
1980.
Ana-lyzing intention in utterances.
Artificial Intelligence,15:143?178.Nicholas Asher and Alex Lascarides.
2003.
Logics ofConversation.
Cambridge University Press, Cam-bridge.Dwight Bolinger.
1978.
Yes?no questions are not al-ternative questions.
In Henry Hiz, editor, Questions,pages 87?105.
D. Reidel Publishing Company, Dor-drecht, Holland.Herbert H. Clark.
1979.
Responding to indirect speechacts.
Cognitive Psychology, 11:430?477.Nancy Green and Sandra Carberry.
1992.
Conver-sational implicatures in indirect replies.
In Pro-ceedings of the 30th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 64?71, Newark, Delaware, USA, June.
Association forComputational Linguistics.Nancy Green and Sandra Carberry.
1994.
A hybridreasoning model for indirect answers.
In Proceed-ings of the 32nd Annual Meeting of the Associationfor Computational Linguistics, pages 58?65, LasCruces, New Mexico, USA, June.
Association forComputational Linguistics.Jeroen Groenendijk and Martin Stokhof.
1984.
Studiesin the Semantics of Questions and the Pragmatics ofAnswers.
Ph.D. thesis, University of Amsterdam.Beth Ann Hockey, Deborah Rossen-Knill, BeverlySpejewski, Matthew Stone, and Stephen Isard.1997.
Can you predict answers to Y/N questions?Yes, No and Stuff.
In Proceedings of Eurospeech1997.Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997.
Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft13.
Technical Report 97-02, University of Colorado,Boulder Institute of Cognitive Science.Christopher Kennedy.
2007.
Vagueness and grammar:The semantics of relative and absolute gradable ad-jectives.
Linguistics and Philosophy, 30(1):1?45.Stanley Kok, Marc Sumner, Matthew Richardson,Parag Singla, Hoifung Poon, Daniel Lowd, JueWang, and Pedro Domingos.
2009.
The Alchemysystem for statistical relational AI.
Technical report,Department of Computer Science and Engineering,University of Washington, Seattle, WA.C.
Raymond Perrault and James F. Allen.
1980.
Aplan-based analysis of indirect speech acts.
Amer-ican Journal of Computational Linguistics, 6(3-4):167?182.Anita M. Pomerantz.
1984.
Agreeing and dis-agreeing with assessment: Some features of pre-ferred/dispreferred turn shapes.
In J. M. Atkinsonand J.
Heritage, editors, Structure of Social Action:Studies in Conversation Analysis.
Cambridge Uni-versity Press.Matt Richardson and Pedro Domingos.
2006.
Markovlogic networks.
Machine Learning, 62(1-2):107?136.Craige Roberts.
1996.
Information structure: To-wards an integrated formal theory of pragmatics.
InJae Hak Yoon and Andreas Kathol, editors, OSUWorking Papers in Linguistics, volume 49: Papersin Semantics, pages 91?136.
The Ohio State Uni-versity Department of Linguistics, Columbus, OH.Revised 1998.Robert van Rooy.
2003.
Questioning to resolvedecision problems.
Linguistics and Philosophy,26(6):727?763.Emanuel A. Schegloff, Gail Jefferson, and HarveySacks.
1977.
The preference for self-correctionin the organization of repair in conversation.
Lan-guage, 53:361?382.Anna-Brita Stenstro?m.
1984.
Questions and re-sponses in English conversation.
In Claes Schaarand Jan Svartvik, editors, Lund Studies in English68, Malmo?
Sweden.
CWK Gleerup.Henk Zeevat.
1994.
Questions and exhaustivity in up-date semantics.
In Harry Bunt, Reinhard Muskens,and Gerrit Rentier, editors, Proceedings of the In-ternational Workshop on Computational Semantics,pages 211?221.
ITK, Tilburg.143
