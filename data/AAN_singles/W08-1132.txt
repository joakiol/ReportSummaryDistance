The Fingerprint of Human Referring Expressions and their SurfaceRealization with Graph TransducersBernd BohnetUniversity of StuttgartVisualization and Interactive Systems GroupUniversita?tstr.
58, 70569 Stuttgart, Germanybohnet@informatik.uni-stuttgart.deAbstractThe algorithm IS-FP takes up the idea fromthe IS-FBN algorithm developed for theshared task 2007.
Both algorithms learn theindividual attribute selection style for each hu-man that provided referring expressions to thecorpus.
The IS-FP algorithm was developedwith two additional goals (1) to improve theindentification time that was poor for the FBNalgorithm and (2) to push the dice score evenhigher.
In order to generate a word string forthe selected attributes, we build based on indi-vidual preferences a surface syntactic depen-dency tree as input.
We derive the individualpreferences from the training set.
Finally, agraph transducer maps the input strucutre to adeep morphologic structure.1 IS-FP: Generating Referring Expressionwith a Human ImprintA review of the referring expressions shows thathumans prefer frequently distinct attributes and at-tribute combination such as in the following exam-ples.grey desk (30t1), a red chair (30t2), red sofa (30t3), bluechair (30t5), a small green desk (30t6)the one in the top left corner (31t1), the one to the left inthe middle row (31t2), the bottom right most one (31t3),the blue chair at the bottom center (31t5), etc.The first individual (#30) seems to prefer colourand size while the second one (#31) seems to preferthe relative position (to the left) and places (the topleft corner).
Because of the review, we checked, ifthe Incremental Algorithm (Dale and Reiter, 1995)using the order for the attributes due to the frequencycalculated for each individual can outperform the al-gorithm using the order for the attributes due to thefrequency of the complete training set.
This was thecase.
Table 1 shows the results.
Using the individualattribute order, the IA performed as good as the FBNalgorithm, cf.
Table 1.Algorithm Furniture People Avg.IA (complete) 0.796 0.710 0.753IA (individual) 0.835 0.736 0.7855FBN 0.810 0.762 0.786Table 1: Incremental algorithm and FBNHowever, the FBN algorithm generates all pos-sible referring expressions and selects based on thedice metric the most similar expressions of the samehuman.
Since there is usually a set of equal good re-ferring expressions, it is possible to select a referringexpression among these results due to another met-ric.
That this would improve the results shows theexperiment to selected among these results the refer-ring expression that is closest to the correct result.The outcome was that the FBN algorithm has stillabout 9% room for improvements.
The followingsections investigates possibilities to use this chance.1.1 Identification TimeAn important metric is the identification time that isthe time which is used by a human to identify an en-tity due to a given referring expression.
The identi-fication time is very loosely related with the numberof minimal referring expressions and therefore likelywith the length of a referring expression.
The bestidentification times had a system with 74% minimalreferring expressions and the second and third best207systems had about 41%.
Good identification timeshad nearly all systems with only a maximum differ-ence of 0.38 seconds except FBN and FBS whichare about 1.05 and 1.49 seconds behind the best one.This is a huge difference compared to all other sys-tems.
What could be the reason for that?
We knowof two differences of FBN to all other systems: (1)the lowest portion of minimal referring expressionsof all systems and (2) the nearest neighbour learn-ing technique.
The number of minimal referring ex-pressions is also different to the number of expres-sions found in the training set.
Table 2 shows in thecolumns human the average length and portion ofminimal human referring expressions.
Because ofthe different length of the human and the generatedexpressions, we conducted the experiment to chosealways the shortest.
Table 2 shows the change be-tween the random selection (FBN) and the selectionof the shortest (FP).
The experiment leads to a resultthat have a length and percentage of minimal refer-ring expressions in average similar to the humansones, cf.
columns of human and shortest.selection human random (FBM) shortest (FP)RE Len.
Min.
Len.
Min.
Len.
Min.Furniture 3.1 26.3 3.5 9.4 3.1 15.9People 3.0 30.9 2.8 28.8 2.8 30.8Table 2: Length and portion of min.
REThe second difference is the use of the nearestneighbour technique.
Could the poor identificationtime be caused by the nearest neighbour technique?How does it influence referring expressions?
?
Thereferring expressions are generated in all the dif-ferent styles like the human expressions of the cor-pus.
Do humans learn the style of referring expres-sions and expect then the next expression in the samestyle?
And are we confused when we don?t get whatwe expact?
Or does FBN look too much on the ex-pressions of the humans and too less on the domain?We hope to get answers for these questions from theshared task evaluation of IS-FP.1.2 The IS-FP AlgorithmThe basis for the IS-FP algorithm is an extended fullbrevity implementation in terms of problem solvingby search which computes all referring expression,cf.
(Bohnet and Dale, 2005).
IS-FP uses also thenearest neighbour technique like the IS-FBN algo-rithm that was introduced by Bohnet (2007).
Withthe nearest neighbour technique, IS-FP selects theexpressions which are most similar to the referringexpressions of the same human and a human thatbuilds referring expressions similar.
The similarityis computed as the average of all dice values be-tween all combinations of the available trails for twohumans.
From the result of the nearest neighbourevaluation, FP selects the shortest and if still morethan one expressions remain then it computes thesimilarity among them and chooses the most typi-cal and finally, if still alternatives remain, it selectsone with the attributes having the highest frequency.Table 3 shows the results for IS-FP trained on thetraining set and applied to the development set.Set Dice MASI Accuracy Uniq.
Min.Furniture 0.881 0.691 51.25% 100% 1.25%People 0.790 0.558 36.8% 100% 0%Total 0.836 0.625 44% 100% 0.62%Table 3: Results for the IS-FP algorithm2 IS-GT: Realization with GraphTransducersWe build the input depedency tree for the text gener-ator due to the statistical information that we collectfrom the training data for each person.
This pro-cedure is consistent with our referring expressiongenerator IS-FP that reproduces the individual im-print in a referring expression for the target person.We start with the realization of the referring expres-sions from a surface syntactic dependency tree, cf.
(Mel?c?uk, 1988).
For the realization of the text, weuse the Text Generator and Linguistic EnvironmentMATE, cf.
(Bohnet, 2006).
We reportet the firsttime about MATE on the first International Natu-ral Language Generation Conference, cf.
(Bohnet etal., 2000).
It was since then continuously enhancedand in the last years, large grammars for several lan-guages such as Catalan, English, Finnish, French,German, Polish, Portougees have been developedwithin the European Project MARQUIS and PatEx-pert, cf.
(Wanner et al, 2007), (Lareau and Wanner,2007) and (Mille and Wanner, 2008).2.1 The Referring Expression ModelsA learning program builds a Referring ExpressionModel for each person that contributed referring ex-pression to the corpus.
The model contains the fol-lowing information: (1) The lexicalization for the208values of a attribute such as couch for the value sofa,man for value person, etc.
(2) The prefered usage ofdeterminers for the type that can be definite (the), in-definite (a), no article.
(3) The syntactic preferencessuch as the top left chair, the chair at the bottom tothe left, etc.The information about the determiner and thelexicalization is collected from the annotated wordstring and the word string itself.
We collect the mostfrequent usage for each person in the coprpus.
Inorder to collect the prefered syntax, we annotatedthe word strings with syntactic dependency trees.Each of the dependency tress contains additional at-tributes, which describe the information content of abranch outgoing from the root as well as the possi-ble value of the attriube at the nodes which carry theinformation.
The learning program cuts the syntac-tic tree at edges starting at the root node and storesthe branches in the referring expression model forthe person.
For instance, the complete referring ex-pression model of a person would contain due to thetraining data the following information:article: definitelexicalization: person ?
man, light ?
whitesyntax:t21a: wearing glasses {t:hasGlasses a1:1 v1:glasses}t21b: with compl ?
beard {t:hasBeard a1:1 v1:beard} det ?
abeard compl ?
white {t:hairColour a1:light v1:whitea2:dark v2:dark}t22: with compl ?
beard {t:hasBeard a1:1 v1:beard}beard det ?
at23: wearing obj ?glasses {t:hasGlasses a1:1 v1:glasses}t26: with compl ?
glasses {t:hasGlasses a1:1 v1:glasses}glasses coord ?
and compl ?
heair mod ?dark{t:hairColour a1:dark v1:dark}2.2 Setting up the Input for the GeneratorOne of the input attribute sets of the people domainlooks like the following one:<TRIAL CONDITION=?-LOC?
ID=?s81t25?>...<ATTRIBUTE-SET><ATTRIBUTE ID=?a4?
NAME=?hasBeard?
VALUE=?1?/ ><ATTRIBUTE ID=?a3?
NAME=?hairColour?
VALUE=?light?/ ><ATTRIBUTE ID=?a2?
NAME=?hasGlasses?
VALUE=?1?/ ><ATTRIBUTE ID=?a1?
NAME=?type?
VALUE=?person?/ >< /ATTRIBUTE-SET>< /TRIAL>We start to set up the input structure with the topnode which is labeled with the lexicalization of thetype or in seldom cases with elision, when the typeis not in the attribute set.
Then we look up in the re-ferring expression model which determiner the per-manthe wearingglasseswithbearda darkmod post_mod post_moddobj compldet modFigure 1: The input to the graph transducerNPPPNPbeardadarkwithVPNPglasseswearingthe manbbbbbbbbbFigure 2: The output of the graph transducerson prefers.
If she prefers any then a node is build,labeled with the determiner and connected with anedge to the type node.
After that we add the lexi-calized values of that attributes which are nearly al-ways directly attached to the type node such as agein the people domain or colour and size in the fur-niture domain.
Then the program searches in themodel the syntactic annotations of attribute combi-nations.
If IS-FP has build the referring expressionthen it starts to search in the trail selected by thenearest neighbour algorithm otherwise it calculatesthe closest due to the dice metric.
In our exampleIS-FP might have build as well the given combi-nation since it is equal to the attribute set of trails81t21.
Then the program would select the syntacticpart t21b first and adapt the value of the node labelwhite to dark.
After that the the syntactic part t21awould be selected since the attribute hasGlasses isstill not covered in the structure.
This part does notneed any adaption.
Figure 1 shows the result of theprocess.2.3 Realization of the Word StringFor the realization, we use a handcrafted grammarthat generates out of the dependency trees roughly209deep morphologic structure / topologic graph.
Themain task of the grammar is to determine the wordorder.
The grammar contains four main rule groups.The vertical rules order the parent in relation toone of its dependent.
The horizontal rules odertwo childs.
The constituent creation rules buildconstituents and the constituent adjoin rules adjoinsconstituents with constituents.
Special considerationneeded the order of prepositional constituents afterthe type and the adjective before the type.
The pre-postional constituents are order because of the orderof the prepostions in the corpus.
In order to be ableto derive the order of the adjective, we used the func-tional class of the adjectives.
Halliday (1994) pro-poses for English, the classes deictic (this, those, ...),numerative (many, second, , ...), epithet (old, blue,...), and classifier (vegetarian, Spanish, ...).
The or-der of the adjectives in a noun phrase is in the givenorder of the classes.
In the lexicon entry of the ad-jectives, we store only a number between one andfour which refers to the adjective class.Table 5 shows the result for the TUNA-R task.The system was developed only by looking on thetraining data without any consideration of the devel-opment data as well without any annotation of thesyntax of the development data.
We used as guidefor the optimization cross validation of training data.Set Accuracy String-Edit DistanceFurniture 35 % 3,163People 22,06 % 3,647Total 28,53 3,405Table 4: Results for the TUNA-R Task3 IS-FP-GT: The Combination ofAttribute Selection and RealizationThe only change, we made in compare to IS-FP isthat we switched off the feature to add the mostsimilar referring expressions of another human fromthe training set for the nearest neighbour evaluationsince the results have been lower.
The reason for thisis that other human preferes similar attributes but theindividual preferences such as the chosen words andsyntax of the other human is different.
Table 5 showsthe results.4 ConclusionThe IS-FP algorithm reproduces the imprint of hu-man referring expressions.
It generates combina-Set Accuracy String-Edit DistanceFurniture 15 % 3,8625People 8,82 % 4,764Total 11,91 4,313Table 5: Results for the TUNA-REG Tasktions such as the x-dimension and y-dimension.
El-ements of a combination have not to occur alwaystogether, however they tent to occur together.
Thisis an advantage over incremental algorithms whichmight have to include other attributes ordered be-tween elements of a combination.
FP has the advan-tage over its predecessor FBN to generate expres-sions which are additionally mostly equal in respectto the length to human referring expressions, it en-larges automatically the training set for an individ-ual human and it takes into account properties of thedomain like the frequency of the attributes.ReferencesB.
Bohnet and R. Dale.
2005.
Viewing referring expres-sion generation as search.
In IJCAI, pages 1004?1009.B.
Bohnet, A. Langjahr, and L. Wanner.
2000.
A De-velopment Environment for an MTT-Based SentenceGenerator.
In Proceedings of the First InternationalNatural Language Generation Conference.B.
Bohnet.
2006.
Textgenerierung durch Transduk-tion linguistischer Strukturen.
Ph.D. thesis, UniversityStuttgart.B.
Bohnet.
2007.
IS-FBN, IS-FBS, IS-IAC: The Adapta-tion of Two Classic Algorithms for the Generation ofReferring Expressions in order to Produce Expressionslike Humans Do.
In MT Summit XI, UCNLG+MT,pages 84?86.R.
Dale and E. Reiter.
1995.
Computational Interpreta-tions of the Gricean Maxims in the Generation of Re-ferring Expressions.
Cognitive Science, 19:233?263.M.
A. K. Halliday.
1994.
An Introduction to FunctionalGrammar.
Edward Arnold, London.F.
Lareau and L. Wanner.
2007.
Towards a Generic Mul-tilingual Dependency Grammar for Text Generation.In GEAF-07, Palo Alto.I.A.
Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press, Albany.S.
Mille and L. Wanner.
2008.
Making Text ResourcesAvailable to the Reader: The Case of Patent Claims.In LREC, Morocco, Marrakech.L.
Wanner, B. Bohnet, N. Bouayad-Agha, F. Lareau,A.
Lohmeyer, and D. Nickla.
2007.
On the Challengeof Creating and Communicating Air Quality Informa-tion.
In In: Swayne A., Hrebicek J.(Eds.
): Environ-mental Software Systems Dimensions of Environmen-tal Informatics.
Vol.7.210
