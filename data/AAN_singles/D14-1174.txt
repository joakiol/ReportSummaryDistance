Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsImproving Pivot-Based Statistical Machine Translation by Pivotingthe Co-occurrence Count of Phrase PairsXiaoning Zhu1*, Zhongjun He2, Hua Wu2, Conghui Zhu1,Haifeng Wang2, and Tiejun Zhao1Harbin Institute of Technology, Harbin, China1{xnzhu,chzhu,tjzhao}@mtlab.hit.edu.cnBaidu Inc., Beijing, China2{hezhongjun,wu_hua,wanghaifeng}@baidu.com* This work was done when the first author was visiting Baidu.AbstractTo overcome the scarceness of bilingualcorpora for some language pairs in ma-chine translation, pivot-based SMT usespivot language as a "bridge" to generatesource-target translation from source-pivot and pivot-target translation.
One ofthe key issues is to estimate the probabili-ties for the generated phrase pairs.
In thispaper, we present a novel approach tocalculate the translation probability bypivoting the co-occurrence count ofsource-pivot and pivot-target phrase pairs.Experimental results on Europarl dataand web data show that our method leadsto significant improvements over thebaseline systems.1 IntroductionStatistical Machine Translation (SMT) relies onlarge bilingual parallel data to produce high qual-ity translation results.
Unfortunately, for somelanguage pairs, large bilingual corpora are notreadily available.
To alleviate the parallel datascarceness, a conventional solution is to intro-duce a ?bridge?
language (named pivot language)to connect the source and target language (deGispert and Marino, 2006; Utiyama and Isahara,2007; Wu and Wang, 2007; Bertoldi et al., 2008;Paul et al., 2011; El Kholy et al., 2013; Zahabi etal., 2013), where there are large amounts ofsource-pivot and pivot-target parallel corpora.Among various pivot-based approaches, thetriangulation method (Cohn and Lapata, 2007;Wu and Wang, 2007) is a representative work inpivot-based machine translation.
The approachproposes to build a source-target phrase table bymerging the source-pivot and pivot-target phrasetable.
One of the key issues in this method is toestimate the translation probabilities for the gen-erated source-target phrase pairs.
Conventionally,the probabilities are estimated by multiplying theposterior probabilities of source-pivot and pivot-target phrase pairs.
However, it has been shownthat the generated probabilities are not accurateenough (Cui et al., 2013).
One possible reasonmay lie in the non-uniformity of the probabilityspace.
Through Figure 1.
(a), we can see that theprobability distributions of source-pivot and piv-ot-target language are calculated separately, andthe source-target probability distributions areinduced from the source-pivot and pivot-targetprobability distributions.
Because of the absenceof the pivot language (e.g., p2 is in source-pivotprobability space but not in pivot-target one), theinduced source-target probability distribution isnot complete, which will result in inaccurateprobabilities.To solve this problem, we propose a novel ap-proach that utilizes the co-occurrence count ofsource-target phrase pairs to estimate phrasetranslation probabilities more precisely.
Differentfrom the triangulation method, which merges thesource-pivot and pivot-target phrase pairs aftertraining the translation model, we propose tomerge the source-pivot and pivot-target phrasepairs immediately after the phrase extraction step,and estimate the co-occurrence count of thesource-pivot-target phrase pairs.
Finally, wecompute the translation probabilities accordingto the estimated co-occurrence counts, using thestandard training method in phrase-based SMT(Koehn et al., 2003).
As Figure 1.
(b) shows, the1665source-target probability distributions are calcu-lated in a complete probability space.
Thus, itwill be more accurate than the traditional trian-gulation method.
Figure 2.
(a) and (b) show thedifference between the triangulation method andour co-occurrence count method.Furthermore, it is common that a small stand-ard bilingual corpus can be available between thesource and target language.
The direct translationmodel trained with the standard bilingual corpusexceeds in translation performance, but its weak-ness lies in low phrase coverage.
However, thepivot model has characteristics characters.
Thus,it is important to combine the direct and pivottranslation model to compensate mutually andfurther improve the translation performance.
Todeal with this problem, we propose a mixedmodel by merging the phrase pairs extracted bypivot-based method and the phrase pairs extract-ed from the standard bilingual corpus.
Note that,this is different from the conventional interpola-tion method, which interpolates the direct andpivot translation model.
See Figure 2.
(b) and (c)for further illustration.
(a) the triangulation method                         (b) the co-occurrence count methodFigure 1: An example of probability space evolution in pivot translation.Large SPcorpusLarge PTcorpusSP phrasepairsPT phrasepairsSP model PT modelST pivotmodelPhrase Extraction Phrase ExtractionTrain TrainMergeStandardST corpusST phrasepairsST directmodelPhrase ExtractionTrainInterpolateST interpolatedmodelLarge SPcorpusLarge PTcorpusSP phrasepairsST pivotmodelST phrasepairsPhrase Extraction Phrase ExtractionTrainPT phrasepairsMergeStandardST corpusST phrasepairsST directmodelPhrase ExtractionTrainInterpolateST interpolatedmodelLarge SPcorpusLarge PTcorpusSP phrasepairsPT phrasepairsST mixedpairsST phrasepairsPhrase Extraction Phrase ExtractionMergeStandardST corpusST phrasepairsPhrase ExtractionTrainST mixedmodelMix(a) the triangulation method        (b) the co-occurrence count method            (c) the mixed modelFigure 2: Framework of the triangulation method, the co-occurrence count method and the mixedmodel.
The shaded box in (b) denotes difference between the co-occurrence count method and thetriangulation method.
The shaded box in (c) denotes the difference between the interpolation modeland the mixed model.1666The remainder of this paper is organized asfollows.
In Section 2, we describe the relatedwork.
We introduce the co-occurrence countmethod in Section 3, and the mixed model inSection 4.
In Section 5 and Section 6, we de-scribe and analyze the experiments.
Section 7gives a conclusion of the paper.2 Related WorkSeveral methods have been proposed for pivot-based translation.
Typically, they can be classi-fied into 3 kinds as follows:Transfer Method: The transfer method(Utiyama and Isahara, 2007; Wang et al., 2008;Costa-juss?
et al., 2011) connects two translationsystems: a source-pivot MT system and a pivot-target MT system.
Given a source sentence, (1)the source-pivot MT system translates it into thepivot language, (2) and the pivot-target MT sys-tem translates the pivot sentence into the targetsentence.
During each step (source to pivot andpivot to target), multiple translation outputs willbe generated, thus a minimum Bayes-risk systemcombination method is often used to select theoptimal sentence (Gonz?lez-Rubio et al., 2011;Duh et al., 2011).
The problem with the transfermethod is that it needs to decode twice.
On onehand, the time cost is doubled; on the other hand,the translation error of the source-pivot transla-tion system will be transferred to the pivot-targettranslation.Synthetic Method: It aims to create a synthet-ic source-target corpus by: (1) translate the pivotpart in source-pivot corpus into target languagewith a pivot-target model; (2) translate the pivotpart in pivot-target corpus into source languagewith a pivot-source model; (3) combine thesource sentences with translated target sentencesor/and combine the target sentences with trans-lated source sentences (Utiyama et al., 2008; Wuand Wang, 2009).
However, it is difficult tobuild a high quality translation system with acorpus created by a machine translation system.Triangulation Method: The triangulationmethod obtains source-target phrase table bymerging source-pivot and pivot-target phrasetable entries with identical pivot languagephrases and multiplying corresponding posteriorprobabilities (Wu and Wang, 2007; Cohn andLapata, 2007), which has been shown to workbetter than the other pivot approaches (Utiyamaand Isahara, 2007).
A problem of this approach isthat the probability space of the source-targetphrase pairs is non-uniformity due to the mis-matching of the pivot phrase.3 Our ApproachIn this section, we will introduce our method forlearning a source-target phrase translation modelwith a pivot language as a bridge.
We extract theco-occurrence count of phrase pairs for each lan-guage pair with a source-pivot and a pivot-targetcorpus.
Then we generate the source-targetphrase pairs with induced co-occurrence infor-mation.
Finally, we compute translation proba-bilities using the standard phrase-based SMTtraining method.3.1 Phrase Translation ProbabilitiesFollowing the standard phrase extraction method(Koehn et al., 2003), we can extract phrase pairs??
?, ???
and ??
?, ???
from the corresponding word-aligned source-pivot and pivot-target trainingcorpus, where ??
, ??
and ??
denotes the phrase insource, pivot and target language respectively.Formally, given the co-occurrence count???
?, ???
and ???
?, ??
?, we can estimate  ???
?, ???
byEquation 1:???
?, ???
?
??????
?, ??
?, ???
?, ??????
(1)where ????
is a function to merge the co-occurrences count ???
?, ???
and ???
?, ???
.
We pro-pose four calculation methods for function ???
?.Given the co-occurrence count ???
?, ???
and???
?, ??
?, we first need to induce the co-occurrencecount ???
?, ?,?
???
.
The ???
?, ?,?
???
is counted whenthe source phrase, pivot phrase and target phraseoccurred together, thus we can infer that???
?, ?,?
???
is smaller than ???
?, ???
and ???
?, ???
.
Inthis circumstance, we consider that ???
?, ?,?
???
isapproximately equal to the minimum value of???
?, ???
and ???
?, ??
?, as shown in Equation 2.???
?, ?
?, ???
?
?min????
?, ??
?, ???
?, ??????
(2)Because the co-occurrence count of source-target phrase pairs needs the existence of pivotphrase ??
, we intuitively believe that the co-occurrence count ???
?, ???
is equal to the co-occurrence count ???
?, ?,?
???.
Under this assump-tion, we can obtain the co-occurrence count???
?, ???
as shown in Equation 3.
Furthermore, totestify our assumption, we also try the maximumvalue (Equation 4) to infer the co-occurrencecount of ??
?, ???
phrase pair.1667???
?, ???
?
?min????
?, ??
?, ???
?, ??????(3)???
?, ???
?
?max????
?, ??
?, ???
?, ??????
(4)In addition, if source-pivot and pivot-targetparallel corpus greatly differ in quantities, thenthe minimum function would likely just take thecounts from the smaller corpus.
To deal with theproblem of the imbalance of the parallel corpora,we also try the arithmetic mean (Equation 5) andgeometric mean (Equation 6) function to inferthe co-occurrence count of source-target phrasepairs.???
?, ???
?
?????
?, ???
?
???
?, ????/2??(5)???
?, ???
?
?????
?, ???
?
???
?, ?????
(6)When the co-occurrence count of source-targetlanguage is calculated, we can estimate thephrase translation probabilities with the follow-ing Equation 7 and Equation 8.?????|??
?
???
?, ????
???
?, ?????
(7)????|???
?
???
?, ????
???
?, ?????
(8)3.2 Lexical WeightGiven a phrase pair ??
?, ???
and a word alignmenta between the source word positions ?
?
1,?
, ?and the target word positions ?
?
0,?
,?
, thelexical weight of phrase pair ??
?, ???
can be calcu-lated by the following Equation 9 (Koehn et al.,2003).?????
?|?, ??
??1|??|?
?, ??
?
?
?| ?
????|??????,????????
(9)The lexical translation probability distribution???|??
between source word s and target word tcan be estimated with Equation 10.???|??
?
??
?, ???
???
?, ????
(10)To compute the lexical weight for a phrasepair ??
?, ???
generated by ??
?, ???
and ??
?, ??
?, we needthe alignment information ?, which can be ob-tained as Equation 11 shows.?
?
??
?, ??|??
: ?
?, ??
?
??&?
?, ??
?
???
(11)where ??
and ??
indicate the word alignmentinformation in the phrase pair ??
?, ???
and ??
?, ??
?respectively.4 Integrate with Direct TranslationIf a standard source-target bilingual corpus isavailable, we can train a direct translation model.Thus we can integrate the direct model and thepivot model to obtain further improvements.
Wepropose a mixed model by merging the co-occurrence count in direct translation and pivottranslation.
Besides, we also employ an interpo-lated model (Wu and Wang, 2007) by mergingthe direct translation model and pivot translationmodel using a linear interpolation.4.1 Mixed ModelGiven ?
pivot languages, the co-occurrencecount can be estimated using the method de-scribed in Section 3.1.
Then the co-occurrencecount and the lexical weight of the mixed modelcan be estimated with the following Equation 12and 13.??
?, ??
?????
?, ??????(12)?????
?|?, ??
??????????,????
?|?, ??
(13)where ???
?, ??
and ??,????
?|?, ??
are the co-occurrence count and lexical weight in the directtranslation model respectively.
???
?, ??
and??,????
?|?, ??
denote the co-occurrence count andlexical weight in the pivot translation model.
?
?is the interpolation coefficient, requiring?
??????
?
1.4.2 Interpolated ModelFollowing Wu and Wang (2007), the interpolatedmodel can be modelled with Equation 14.?????|??
?
?????????|??????
(14)where ??????|??
is the phrase translation probabil-ity in direct translation model; ??????|??
is thephrase translation probability in pivot translationmodel.
The lexical weight is obtained with Equa-tion 13.
??
is the interpolation coefficient, requir-ing ?
??
?
1????
.16685 Experiments on Europarl CorpusOur first experiment is carried out on Europarl1corpus, which is a multi-lingual corpus including21 European languages (Koehn, 2005).
In ourwork, we perform translations among French (fr),German (de) and Spanish (es).
Due to the rich-ness of available language resources, we chooseEnglish (en) as the pivot language.
Table 1summarized the statistics of training data.
For thelanguage model, the same monolingual data ex-tracted from the Europarl are used.The word alignment is obtained by GIZA++(Och and Ney, 2000) and the heuristics ?grow-diag-final?
refinement rule (Koehn et al., 2003).Our translation system is an in-house phrase-based system analogous to Moses (Koehn et al.,2007).
The baseline system is the triangulationmethod (Wu and Wang, 2007), including an in-terpolated model which linearly interpolate thedirect and pivot translation model.1 http://www.statmt.org/europarlWe use WMT082  as our test data, which con-tains 2000 in-domain sentences and 2051 out-of-domain sentences with single reference.
Thetranslation results are evaluated by case-insensitive BLEU-4 metric (Papineni et al.,2002).
The statistical significance tests using95% confidence interval are measured withpaired bootstrap resampling (Koehn, 2004).5.1 ResultsWe compare 4 merging methods with the base-line system.
The results are shown in Table 2 andTable 3.
We find that the minimum method out-performs the others, achieving significant im-provements over the baseline on all translationdirections.
The absolute improvements rangefrom 0.61 (fr-de) to 1.54 (es-fr) in BLEU% scoreon in-domain test data, and range from 0.36 (fr-de) to 2.05 (fr-es) in BLEU% score on out-of-domain test data.
This indicates that our methodis effective and robust in general.2 http://www.statmt.org/wmt08/shared-task.htmlLanguagePairsSentencePairsSourceWordsTargetWordsde-en 1.9M 48.5M 50.9Mes-en 1.9M 54M 51.7Mfr-en 2M 58.1M 52.4MTable 1: Training data of Europarl corpusSystemBLEU%de-es de-fr es-de es-fr fr-de fr-esBaseline 27.04 23.01 20.65 33.84 20.87 38.31Minimum 27.93* 23.94* 21.52* 35.38* 21.48* 39.62*Maximum 25.70 21.59 20.26 32.58 20.50 37.30Arithmetic mean 26.01 22.24 20.13 33.38 20.37 37.37Geometric mean 27.31 23.49* 21.10* 34.76* 21.15* 39.19*Table 2: Comparison of different merging methods on in-domain test set.
* indicates the results aresignificantly better than the baseline (p<0.05).SystemBLEU%de-es de-fr es-de es-fr fr-de fr-esBaseline 15.34 13.52 11.47 21.99 12.19 25.00Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05*Maximum 13.41 11.83 10.17 20.48 10.83 22.75Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22*Table 3: Comparison of different merging methods on out-of-domain test set.1669The geometric mean method also achieves im-provement, but not as significant as the minimummethod.
However, the maximum and the arith-metic mean methods show a decrement in BLEUscores.
This reminds us that how to choose aproper merging function for the co-occurrencecount is a key problem.
In the future, we willexplore more sophisticated method to merge co-occurrence count.5.2 AnalysisThe pivot-based translation is suitable for thescenario that there exists large amount of source-pivot and pivot-target bilingual corpora and onlya little source-target bilingual data.
Thus, werandomly select 10K, 50K, 100K, 200K, 500K,1M, 1.5M sentence pairs from the source-targetbilingual corpora to simulate the lack of source-target data.
With these corpora, we train severaldirect translation models with different scales ofbilingual data.
We interpolate each direct transla-tion model with the pivot model (both triangula-tion method and co-occurrence count method) toobtain the interpolated model respectively.
Wealso mix the direct model and pivot model usingthe method described in Section 4.1.
Following(a) German-English-Spanish                                        (b) German-English-French(c) Spanish-English-German                                        (d) Spanish-English-French(e) French-English-German                                         (f) French-English-SpanishFigure 3: Comparisons of pivot-based methods on different scales of source-target standard corpora.
(direct: direct model; tri: triangulation model; co: co-occurrence count model; tri+inter: triangulationmodel interpolated with direct model ; co+inter: co-occurrence count model interpolated with directmodel; co+mix: mixed model).
X-axis represents the scale of the standard training data.22.52323.52424.52525.5BLEU%directtricotri+interco+interco+mix26.52727.52828.52929.5BLEU%directtricotri+interco+interco+mix33.53434.53535.53636.537BLEU%directtricotri+interco+interco+mix19.52020.52121.52222.5BLEU%directtricotri+interco+interco+mix37.53838.53939.54040.541BLEU%directtricotri+interco+interco+mix19.52020.52121.52222.5BLEU%directtricotri+interco+interco+mix1670Wu and Wang (2007), we set ??
?
0.9, ??
?
0.1,??
?
0.9  and ??
?
0.1  empirically.
The experi-ments are carried out on 6 translation directions:German-Spanish, German-French, Spanish-German, Spanish-French, French-German andFrench-Spanish.
The results are shown in Figure3.
We only list the results on in-domain test sets.The trend of the results on out-of domain testsets is similar with in-domain test sets.The results are explained as follows:(1) Comparison of Pivot Translation and Di-rect TranslationThe pivot translation models are better thanthe direct translation models trained on a smallsource-target bilingual corpus.
With the incre-ment of source-target corpus, the direct modelfirst outperforms the triangulation model andthen outperforms the co-occurrence count modelconsecutively.Taking Spanish-English-French translation asan example, the co-occurrence count modelachieves BLEU% scores of 35.38, which is closeto the direct translation model trained with 200Ksource-target bilingual data.
Compared with theco-occurrence count model, the triangulationmodel only achieves BLEU% scores of 33.84,which is close to the direct translation modeltrained with 50K source-target bilingual data.
(2) Comparison of Different InterpolatedModelsFor the pivot model trained by triangulationmethod and co-occurrence count method, weinterpolate them with the direct translation modeltrained with different scales of bilingual data.Figure 3 shows the translation results of the dif-ferent interpolated models.
For all the translationdirections, our co-occurrence count method in-terpolated with the direct model is better than thetriangulation model interpolated with the directmodel.The two interpolated model are all better thanthe direct translation model.
With the incrementof the source-target training corpus, the gap be-comes smaller.
This indicates that the pivot mod-el and its affiliated interpolated model are suita-ble for language pairs with small bilingual data.Even if the scale of source-pivot and pivot-targetcorpora is close to the scale of source-target bi-lingual corpora, the pivot translation model canhelp the direct translation model to improve thetranslation performance.
Take Spanish-English-French translation as an issue, when the scale ofSpanish-French parallel data is 1.5M sentencespairs, which is close to the Spanish-English andEnglish-French parallel data, the performance ofco+mix model is still outperforms the directtranslation model.
(3) Comparison of Interpolated Model andMixed ModelWhen only a small source-target bilingualcorpus is available, the mix model outperformsthe interpolated model.
With the increasing ofsource-target corpus, the mix model is close tothe interpolated model or worse than the interpo-lated model.
This indicates that the mix modelhas a better performance when the source-targetcorpus is small which is close to the realistic sce-nario.5.3 Integrate the Co-occurrence CountModel and Triangulation ModelExperimental results in the previous sectionshow that, our co-occurrence count models gen-erally outperform the baseline system.
In thissection, we carry out experiments that integratesco-occurrence count model into the triangulationmodel.For French-English-German translation, weapply a linear interpolation method to integratethe co-occurrence count model into triangulationmodel following the method described in Section4.2.
We set ?
as the interpolation coefficient oftriangulation model and 1 ?
?
as the interpola-tion coefficient of co-occurrence count modelrespectively.
The experiments take 9 values forinterpolation coefficient, from 0.1 to 0.9.
Theresults are shown in Figure 4.Figure 4: Results of integrating the co-occurrence count model and the triangulationmodel.When using interpolation coefficient rangingfrom 0.2 to 0.7, the integrated models outperformthe triangulation and the co-occurrence countmodel.
However, for the other intervals, the inte-20.420.620.82121.221.421.621.80.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9BLEU%Interpolation Coefficientintegrated triangulationco-occurrence1671grated models perform slightly lower than theco-occurrence count model, but still show betterresults than the triangulation model.
The trend ofthe curve infers that the integrated model synthe-sizes the contributions of co-occurrence countmodel and triangulation model.
Additionally, italso indicates that, the choice of the interpolationcoefficient affects the translation performances.6 Experiments on Web DataThe experimental on Europarl is artificial, as thetraining data for directly translating betweensource and target language actually exists in theoriginal data sets.
Thus, we conducted severalexperiments on a more realistic scenario: trans-lating Chinese (zh) to Japanese (jp) via English(en) with web crawled data.As mentioned in Section 3.1, the source-pivotand pivot-target parallel corpora can be imbal-anced in quantities.
If one parallel corpus wasmuch larger than another, then minimum heuris-tic function would likely just take the countsfrom the smaller corpus.In order to analyze this issue, we manually setup imbalanced corpora.
For source-pivot parallelcorpora, we randomly select 1M, 2M, 3M, 4Mand 5M Chinese-English sentence pairs.
On theother hand, we randomly select 1M English-Japanese sentence pairs as pivot-target parallelcorpora.
The training data of Chinese-Englishand English-Japanese language pairs are summa-rized in Table 4.
For the Chinese-Japanese directcorpus, we randomly select 5K, 10K, 20K, 30K,40K, 50K, 60K, 70K, 80K, 90K and 100K sen-tence pairs to simulate the lack of bilingual data.We built a 1K in-house test set with four refer-ences.
For Japanese language model training, weused the monolingual part of English-Japanesecorpus.Table 5 shows the results of different co-occurrence count merging methods.
First, theminimum method and the geometric mean meth-od outperform the other two merging methodsand the baseline system with different trainingcorpus.
When the scale of source-pivot and piv-ot-target corpus is roughly balanced (zh-en-jp-1),the minimum method achieves an absolute im-provement of 2.06 percentages points on BLEUover the baseline, which is also better than theother merging methods.
While, with the growthof source-pivot corpus, the gap between source-pivot corpus and pivot-target corpus becomesbigger.
In this circumstance, the geometric meanmethod becomes better than the minimum meth-od.
Compared to the minimum method, the geo-metric mean method considers both the source-pivot and the pivot-target corpus, which maylead to a better result in the case of imbalancedtraining corpus.LanguagePairsSentencePairsSourceWordsTargetWordszh-en-1 1M 18.1M 17.7Mzh-en-2 2M 36.2M 35.5Mzh-en-3 3M 54.2M 53.2Mzh-en-4 4M 72.3M 70.9Mzh-en-5 5M 90.4M 88.6Men-jp 1M 9.2M 11.1MTable 4: Training data of web corpusSystemBLEU%zh-en-jp-1* zh-en-jp-2 zh-en-jp-3 zh-en-jp-4 zh-en-jp-5Baseline 29.07 29.39 29.44 29.67 29.80Minimum 31.13* 31.28* 31.43* 31.62* 32.02*Maximum 28.88 29.01 29.12 29.37 29.59Arithmetic mean 29.08 29.36 29.51 29.79 30.01Geometric mean 30.77* 31.30* 31.75* 32.07* 32.34*Table 5: Comparison of different merging methods on the imbalanced web data.
( zh-en-jp-1 meansthe translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus,and so on.
)1672Furthermore, with the imbalanced corpus zh-en-jp-5, we compared the translation perfor-mance of our co-occurrence count model (withgeometric mean merging method), triangulationmodel, interpolated model, mixed model and thedirect translation models.
Figure 5 summarizedthe results.The co-occurrence count model can achieve anabsolute improvement of 2.54 percentages pointson BLEU over the baseline.
The triangulationmethod outperforms the direct translation whenonly 5K sentence pairs are available.
Meanwhile,the number is 10K when using the co-occurrencecount method.
The co-occurrence count modelsinterpolated with the direct model significantlyoutperform the other models.Figure 5: Results on Chinese-Japanese Web Data.X-axis represents the scale of the standard train-ing data.In this experiment, the training data containsparallel sentences on various domains.
And thetraining corpora (Chinese-English and English-Japanese) are typically very different, since theyare obtained on the web.
It indicates that our co-occurrence count method is robust in the realisticscenario.7 ConclusionThis paper proposed a novel approach for pivot-based SMT by pivoting the co-occurrence countof phrase pairs.
Different from the triangulationmethod merging the source-pivot and pivot-target language after training the translationmodel, our method merges the source-pivot andpivot-target language after extracting the phrasepairs, thus the computing for phrase translationprobabilities is under the uniform probabilityspace.
The experimental results on Europarl dataand web data show significant improvementsover the baseline systems.
We also proposed amixed model to combine the direct translationand pivot translation, and the experimental re-sults show that the mixed model has a better per-formance when the source-target corpus is smallwhich is close to the realistic scenario.A key problem in the approach is how to learnthe co-occurrence count.
In this paper, we use theminimum function on balanced corpora and thegeometric mean function on imbalanced corporato estimate the co-occurrence count intuitively.In the future, we plan to explore more effectiveapproaches.AcknowledgmentsWe would like to thank Yiming Cui for insight-ful discussions, and three anonymous reviewersfor many invaluable comments and suggestionsto improve our paper.
This work is supported byNational Natural Science Foundation of China(61100093), and the State Key DevelopmentProgram for Basic Research of China (973 Pro-gram, 2014CB340505).ReferenceNicola Bertoldi, Madalina Barbaiani, MarcelloFederico, and Roldano Cattoni.
2008.
Phrase-Based statistical machine translation with Piv-ot Languages.
In Proceedings of the 5th Inter-national Workshop on Spoken LanguageTranslation (IWSLT), pages 143-149.Trevor Cohn and Mirella Lapata.
2007.
MachineTranslation by Triangulation: Make EffectiveUse of Multi-Parallel Corpora.
In Proceedingsof 45th Annual Meeting of the Association forComputational Linguistics, pages 828-735.Marta R.
Costa-juss?, Carlos Henr?quez, and Ra-fael E. Banchs.
2011.
Enhancing Scarce-Resource Language Translation through PivotCombinations.
In Proceedings of the 5th In-ternational Joint Conference on Natural Lan-guage Processing, pages 1361-1365.Yiming Cui, Conghui Zhu, Xiaoning Zhu, TiejunZhao and Dequan Zheng.
2013.
Phrase TableCombination Deficiency Analyses in Pivot-based SMT.
In Proceedings of 18th Interna-tional Conference on Application of NaturalLanguage to Information Systems, pages 355-358.Adria de Gispert and Jose B. Marino.
2006.Catalan-English statistical machine translationwithout parallel corpus: bridging throughSpanish.
In Proceedings of 5th InternationalConference on Language Resources and Eval-uation (LREC), pages 65-68.2931333537395K 20K 40K 60K 80K 100KBLEU%directtrico-occurtri+interco+interco+mix1673Kevin Duh, Katsuhito Sudoh, Xianchao Wu,Hajime Tsukada and Masaaki Nagata.
2011.Generalized Minimum Bayes Risk SystemCombination.
In Proceedings of the 5th Inter-national Joint Conference on Natural Lan-guage Processing, pages 1356-1360.Ahmed El Kholy, Nizar Habash, Gregor Leusch,Evgeny Matusov and Hassan Sawaf.
2013.Language Independent Connectivity StrengthFeatures for Phrase Pivot Statistical MachineTranslation.
In Proceedings of the 51st AnnualMeeting of the Association for ComputationalLinguistics, pages 412-418.Ahmed El Kholy, Nizar Habash, Gregor Leusch,Evgeny Matusov and Hassan Sawaf.
2013.
Se-lective Combination of Pivot and Direct Sta-tistical Machine Translation Models.
In Pro-ceedings of the 6th International Joint Confer-ence on Natural Language Processing, pages1174-1180.Jes?s Gonz?lez-Rubio, Alfons Juan and Francis-co Casacuberta.
2011.
Minimum Bayes-riskSystem Combination.
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics, pages 1268-1277.Philipp Koehn, Franz J. Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InHLT-NAACL: Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics,pages 127-133.Philipp Koehn.
2004.
Statistical significancetests for machine translation evaluation.
InProceedings of the 2004 Conference on Em-pirical Methods in Natural Language Pro-cessing (EMNLP), pages 388-395.Philipp Koehn.
2005.
Europarl: A Parallel Cor-pus for Statistical Machine Translation.
InProceedings of MT Summit X, pages 79-86.Philipp Koehn, Hieu Hoang, Alexanda Birch,Chris Callison-Burch, Marcello Federico, Ni-cola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ondrej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proceed-ings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics, demon-stration session, pages 177-180.Philipp Koehn, Alexandra Birch, and Ralf Stein-berger.
2009.
462 Machine Translation Sys-tems for Europe.
In Proceedings of the MTSummit XII.Gregor Leusch, Aur?lien Max, Josep MariaCrego and Hermann Ney.
2010.
Multi-PivotTranslation by System Combination.
In Pro-ceedings of the 7th International Workshop onSpoken Language Translation, pages 299-306.Franz Josef Och and Hermann Ney.
2000.
Acomparison of alignment models for statisticalmachine translation.
In Proceedings of the18th International Conference on Computa-tional Linguistics, pages 1086-1090.Michael Paul, Andrew Finch, Paul R. Dixon andEiichiro Sumita.
2011.
Dialect Translation: In-tegrating Bayesian Co-segmentation Modelswith Pivot-based SMT.
In Proceedings of the2011 Conference on Empirical Methods inNatural Language Processing, pages 1-9.Michael Paul and Eiichiro Sumita.
2011.
Trans-lation Quality Indicators for Pivot-based Sta-tistical MT.
In Proceedings of the 5th Interna-tional Joint Conference on Natural LanguageProcessing, pages 811-818.Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu.
2002.
BLEU: a Method for Au-tomatic Evaluation of Machine Translation.
InProceedings of the 40th Annual Meeting of theAssociation for Computation Linguistics, pag-es 311-319.Rie Tanaka, Yohei Murakami and Toru Ishida.2009.
Context-Based Approach for PivotTranslation Services.
In the Twenty-first In-ternational Conference on Artificial Intelli-gence, pages 1555-1561.J?rg Tiedemann.
2012.
Character-Based PivotTranslation for Under-Resourced Languagesand Domains.
In Proceedings of the 13th Con-ference of the European Chapter of the Asso-ciation for Computational Linguistics, pages141-151.Masatoshi Tsuchiya, Ayu Purwarianti, Toshiyu-kiWakita and Seiichi Nakagawa.
2007.
Ex-panding Indonesian-Japanese Small Transla-tion Dictionary Using a Pivot Language.
InProceedings of the ACL 2007 Demo and Post-er Sessions, pages 197-200.Takashi Tsunakawa, Naoaki Okazaki andJun'ichi Tsujii.
2010.
Building a BilingualLexicon Using Phrase-based Statistical Ma-chine Translation via a Pivot Language.
In1674Proceedings of the 22th International Confer-ence on Computational Linguistics (Coling),pages 127-130.Masao Utiyama and Hitoshi Isahara.
2007.
AComparison of Pivot Methods for Phrase-Based Statistical Machine Translation.
In Pro-ceedings of Human Language Technology: theConference of the North American Chapter ofthe Association for Computational Linguistics,pages 484-491.Masao Utiyama, Andrew Finch, Hideo Okuma,Michael Paul, Hailong Cao, Hirofumi Yama-moto, Keiji Yasuda,and Eiichiro Sumita.
2008.The NICT/ATR speech Translation System forIWSLT 2008.
In Proceedings of the Interna-tional Workshop on Spoken Language Trans-lation, pages 77-84.Haifeng Wang, Hua Wu, Xiaoguang Hu, ZhanyiLiu, Jianfeng Li, Dengjun Ren, and ZhengyuNiu.
2008.
The TCH Machine TranslationSystem for IWSLT 2008.
In Proceedings ofthe International Workshop on Spoken Lan-guage Translation, pages 124-131.Hua Wu and Haifeng Wang.
2007.
Pivot Lan-guage Approach for Phrase-Based StatisticalMachine Translation.
In Proceedings of 45thAnnual Meeting of the Association for Compu-tational Linguistics, pages 856-863.Hua Wu and Haifeng Wang.
2009.
RevisitingPivot Language Approach for Machine Trans-lation.
In Proceedings of the 47th AnnualMeeting of the Association for ComputationalLinguistics and the 4th IJCNLP of the AFNLP,pages 154-162.Samira Tofighi Zahabi, Somayeh Bakhshaei andShahram Khadivi.
Using Context Vectors inImproving a Machine Translation System withBridge Language.
In Proceedings of the 51stAnnual Meeting of the Association for Compu-tational Linguistics, pages 318-322.1675
