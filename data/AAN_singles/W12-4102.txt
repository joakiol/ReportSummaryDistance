Proceedings of the TextGraphs-7 Workshop at ACL, pages 6?14,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsExtracting Signed Social Networks From TextAhmed HassanMicrosoft ResearchRedmond, WA, USAhassanam@microsoft.comAmjad Abu-JbaraEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduDragomir RadevEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractMost of the research on social networks has al-most exclusively focused on positive links be-tween entities.
There are much more insightsthat we may gain by generalizing social net-works to the signed case where both positiveand negative edges are considered.
One of thereasons why signed social networks have re-ceived less attention that networks based onpositive links only is the lack of an explicitnotion of negative relations in most social net-work applications.
However, most such appli-cations have text embedded in the social net-work.
Applying linguistic analysis techniquesto this text enables us to identify both positiveand negative interactions.
In this work, wepropose a new method to automatically con-struct a signed social network from text.
Theresulting networks have a polarity associatedwith every edge.
Edge polarity is a means forindicating a positive or negative affinity be-tween two individuals.
We apply the proposedmethod to a larger amount of online discus-sion posts.
Experiments show that the pro-posed method is capable of constructing net-works from text with high accuracy.
We alsoconnect out analysis to social psychology the-ories of signed network, namely the structuralbalance theory.1 IntroductionA great body of research work has focused on so-cial network analysis.
Social network analysis playsa huge role in understanding and improving so-cial computing applications.
Most of this researchhas almost exclusively focused on positive links be-tween individuals (e.g.
friends, fans, followers,etc.).
However, if we carefully examine the relation-ships between individuals in online communities,we will find out that limiting links to positive inter-actions is a very simplistic assumption.
It is true thatpeople show positive attitude by labeling others asfriends, and showing agreement, but they also showdisagreement, and antagonism toward other mem-bers of the online community.
Discussion forumsare one example that makes it clear that consideringboth positive and negative interactions is essentialfor understanding the rich relationships that developbetween individuals in online communities.If considering both negative and positive interac-tions will provide much more insight toward under-standing the social network, why did most of pre-vious work only focus on positive interactions?
Wethink that one of the main reasons behind this is thelack of a notion for explicitly labeling negative re-lations.
For example, most social web applicationsallow people to mark others as friends, like them,follow them, etc.
However, they do not allow peopleto explicitly label negative relations with others.Previous work has built networks from discus-sions by linking people who reply to one another.Even though, the mere fact that X replied to Y ?spost does show an interaction, it does not tell us any-thing about the type of that interaction.
In this case,the type of interaction is not readily available; how-ever it may be mined from the text that underliesthe social network.
Hence, if we examine the textexchanged between individuals, we may be able tocome up with conclusions about, not only the exis-tence of an interaction, but also its type.In this work, we apply Natural Language Pro-cessing techniques to text correspondences ex-changed between individuals to identify the under-6lying signed social structure in online communities.We present and compare several algorithms for iden-tifying user attitude and for automatically construct-ing a signed social network representation.
We ap-ply the proposed methods to a large set of discussionposts.
We evaluate the performance using a manu-ally labeled dataset.The input to our algorithm is a set of text corre-spondences exchanged between users (e.g.
posts orcomments).
The output is a signed network whereedges signify the existence of an interaction betweentwo users.
The resulting network has polarity asso-ciated with every edge.
Edge polarity is a means forindicating a positive or negative affinity between twoindividuals.The proposed method was applied to a very largedataset of online discussions.
To evaluate our auto-mated procedure, we asked human annotators to ex-amine text correspondences exchanged between in-dividuals and judge whether their interaction is pos-itive or negative.
We compared the edge signs thathad been automatically identified to edges manuallycreated by human annotators.We also connected our analysis to social psychol-ogy theories, namely the Structural Balance The-ory (Heider, 1946).
The balance theory has beenshown to hold both theoretically (Heider, 1946) andempirically (Leskovec et al, 2010b) for a varietyof social community settings.
Showing that it alsoholds for our automatically constructed network fur-ther validates our results.The rest of the paper is structured as follows.
Insection 2, we review some of the related prior workon mining sentiment from text, mining online dis-cussions, extracting social networks from text, andanalyzing signed social networks.
We define ourproblem and explain our approach in Section 3.
Sec-tion 4 describes our dataset.
Results and discussionare presented in Section 5.
We present a possibleapplication for the proposed approach in Section 6.We conclude in Section 7.2 Related WorkIn this section, we survey several lines of researchthat are related to our work.2.1 Mining Sentiment from TextOur general goal of mining attitude from one in-dividual toward another makes our work related toa huge body of work on sentiment analysis.
Onesuch line of research is the well-studied problemof identifying the of individual words.
In previ-ous work, Hatzivassiloglou and McKeown (1997)proposed a method to identify the polarity of ad-jectives based on conjunctions linking them in alarge corpus.
Turney and Littman (2003) used sta-tistical measures to find the association between agiven word and a set of positive/negative seed words.Takamura et al (2005) used the spin model to ex-tract word semantic orientation.
Finally, Hassan andRadev (2010) use a random walk model defined overa word relatedness graph to classify words as eitherpositive or negative.Subjectivity analysis is yet another research linethat is closely related to our general goal of miningattitude.
The objective of subjectivity analysis is toidentify text that presents opinion as opposed to ob-jective text that presents factual information (Wiebe,2000).
Prior work on subjectivity analysis mainlyconsists of two main categories: subjectivity of aphrase or word is analyzed regardless of the context(Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;Banea et al, 2008), or within its context (Riloff andWiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-sukawa and Yi, 2003; Popescu and Etzioni, 2005).Hassan et al (2010) presents a method for identify-ing sentences that display an attitude from the textwriter toward the text recipient.
Our work is dif-ferent from subjectivity analysis because we are notonly interested in discriminating between opinionsand facts.
Rather, we are interested in identifyingthe polarity of interactions between individuals.
Ourmethod is not restricted to phrases or words, rather itgeneralizes this to identifying the polarity of an in-teraction between two individuals based on severalposts they exchange.2.2 Mining Online DiscussionsOur use of discussion threads as a source of dataconnects us to some previous work on miningonline discussions.
Lin et al (2009) proposeda sparse coding-based model that simultaneouslymodels semantics and structure of threaded discus-7sions.
Huang et al (2007) learn SVM classifiersfrom data to extract (thread-title, reply) pairs.
Theirobjective was to build a chatbot for a certain do-main using knowledge from online discussion fo-rums.
Shen et al (2006) proposed three clusteringmethods for exploiting the temporal information indiscussion streams, as well as an algorithm based onlinguistic features to analyze discourse structure in-formation.2.3 Extracting Social Networks from TextLittle work has been done on the front of extractingsocial relations between individuals from text.
El-son et al (2010) present a method for extracting so-cial networks from nineteenth-century British nov-els and serials.
They link two characters based onwhether they are in conversation or not.
McCal-lum et al (2007) explored the use of structured datasuch as email headers for social network construc-tion.
Gruzd and Hyrthonthwaite (2008) explored theuse of post text in discussions to study interactionpatterns in e-learning communities.Our work is related to this line of research becausewe employ natural language processing techniquesto reveal embedded social structures.
Despite sim-ilarities, our work is uniquely characterized by thefact that we extract signed social networks from text.2.4 Signed Social NetworksMost of the work on social networks analysis hasonly focused on positive interactions.
A few recentpapers have taken the signs of edges into account.Brzozowski et al (2008) study the positive andnegative relationships between users of Essembly.Essembly is an ideological social network that dis-tinguishes between ideological allies and nemeses.Kunegis et al (2009) analyze user relationships inthe Slashdot technology news site.
Slashdot allowsusers of the website to tag other users as friends orfoes, providing positive and negative endorsements.Leskovec et al (2010c) study signed social networksgenerated from Slashdot, Epinions, and Wikipedia.They also connect their analysis to theories of signednetworks from social psychology.
A similar studyused the same datasets for predicting positive andnegative links given their context (Leskovec et al,2010a).
Other work addressed the problem of clus-tering signed networks by taking both positive andnegative edges into consideration (Yang et al, 2007;Doreian and Mrvar, 2009).All this work has been limited to analyzing ahandful of datasets for which an explicit notion ofboth positive and negative relations exists.
Our workgoes beyond this limitation by leveraging the powerof natural language processing to automate the dis-covery of signed social networks using the text em-bedded in the network.3 ApproachThe general goal of this work is to mine attitude be-tween individuals engaged in an online discussion.We use that to extract a signed social network rep-resenting the interactions between different partici-pants.
Our approach consists of several steps.
Inthis section, we will explain how we identify senti-ment at the word level (i.e.
polarity), at the sentencelevel (i.e.
attitude), and finally generalize over thisto find positive/negative interactions between indi-viduals based on their text correspondences.The first step toward identifying attitude is toidentify polarized words.
Polarized words are verygood indicators of subjective sentences and hencewe their existence will be highly correlated with theexistence of attitude.
The method we use for identi-fying word polarity is a Random Walk based methodover a word relatedness graph (Hassan and Radev,2010).The following step is to move to the sentence levelby examining different sentences to find out whichsentences display an attitude from the text writer tothe recipient.
We train a classifier based on severalsources of information to make this prediction (Has-san et al, 2010).
We use lexical items, polarity tags,part-of-speech tags, and dependency parse trees totrain a classifier that identifies sentences with atti-tude.Finally, we build a network connecting partici-pants based on their interactions.
We use the predic-tions we made both at the word and sentence levelsto associate a sign to every edge.3.1 Identified Positive/Negative WordsThe first step toward identifying attitude is to iden-tify words with positive/negative semantic orienta-tion.
The semantic orientation or polarity of a word8indicates the direction the word deviates from thenorm (Lehrer, 1974).
Past work has demonstratedthat polarized words are very good indicators ofsubjective sentences (Hatzivassiloglou and Wiebe,2000; Wiebe et al, 2001).
We use a Random Walkbased method to identify the semantic orientationof words (Hassan and Radev, 2010).
We constructa graph where each node represents a word/part-of-speech pair.
We connect nodes based on syn-onyms, hypernyms, and similar-to relations fromWordNet (Miller, 1995).
For words that do notappear in WordNet, we use distributional similar-ity (Lee, 1999) as a proxy for word relatedness.We use a list of words with known polarity (Stoneet al, 1966) to label some of the nodes in the graph.We then define a random walk model where the setof nodes correspond to the state space, and transi-tion probabilities are estimated by normalizing edgeweights.
We assume that a random surfer walksalong the word relatedness graph starting from aword with unknown polarity.
The walk continuesuntil the surfer hits a word with a known polarity.Seed words with known polarity act as an absorb-ing boundary for the random walk.
We calculate themean hitting time (Norris, 1997) from any word withunknown polarity to the set of positive seeds and theset of negative seeds.
If the absolute difference ofthe two mean hitting times is below a certain thresh-old, the word is classified as neutral.
Otherwise, itis labeled with the class that has the smallest meanhitting time.3.2 Identifying Attitude from TextThe first step toward identifying attitude is to iden-tify words with positive/negative semantic orienta-tion.
The semantic orientation or polarity of a wordindicates the direction the word deviates from thenorm (Lehrer, 1974).
We use OpinionFinder (Wil-son et al, 2005a) to identify words with positiveor negative semantic orientation.
The polarity of aword is also affected by the context where the wordappears.
For example, a positive word that appearsin a negated context should have a negative polarity.Other polarized words sometimes appear as neutralwords in some contexts.
Hence, we use the methoddescribed in (Wilson et al, 2005b) to identify thecontextual polarity of words given their isolated po-larity.
A large set of features is used for that purposeincluding words, sentences, structure, and other fea-tures.Our overall objective is to find the direct attitudebetween participants.
Hence after identifying the se-mantic orientation of individual words, we move onto predicting which polarized expressions target theaddressee and which are not.Sentences that show an attitude are different fromsubjective sentences.
Subjective sentences are sen-tences used to express opinions, evaluations, andspeculations (Riloff and Wiebe, 2003).
While ev-ery sentence that shows an attitude is a subjectivesentence, not every subjective sentence shows an at-titude toward the recipient.
A discussion sentencemay display an opinion about any topic yet no atti-tude.We address the problem of identifying sentenceswith attitude as a relation detection problem in a su-pervised learning setting (Hassan et al, 2010).
Westudy sentences that use second person pronouns andpolarized expressions.
We predict whether the sec-ond person pronoun is related to the polarized ex-pression or not.
We regard the second person pro-noun and the polarized expression as two entitiesand try to learn a classifier that predicts whether thetwo entities are related or not.
The text connectingthe two entities offers a very condensed represen-tation of the information needed to assess whetherthey are related or not.
For example the two sen-tences ?you are completely unqualified?
and ?youknow what, he is unqualified ...?
show two differ-ent ways the words ?you?, and ?unqualified?
couldappear in a sentence.
In the first case the polarizedword unqualified refers to the word you.
In the sec-ond case, the two words are not related.
The se-quence of words connecting the two entities is avery good predictor for whether they are related ornot.
However, these paths are completely lexicalizedand consequently their performance will be limitedby data sparseness.
To alleviate this problem, weuse higher levels of generalization to represent thepath connecting the two tokens.
These representa-tions are the part-of-speech tags, and the shortestpath in a dependency graph connecting the two to-kens.
We represent every sentence with several rep-resentations at different levels of generalization.
Forexample, the sentence your ideas are very inspiringwill be represented using lexical, polarity, part-of-9speech, and dependency information as follows:LEX: ?YOUR ideas are very POS?POS: ?YOUR NNS VBP RB JJ POS?DEP: ?YOUR poss nsubj POS?3.2.1 A Text Classification ApproachIn this method, we treat the problem as a topicclassification problem with two topics: having pos-itive attitude and having negative attitude.
As weare only interested in attitude between participantsrather than sentiment in general, we restrict the textwe analyze to sentences that contain mentions of theaddressee (e.g.
name or second person pronouns).A similar approach for sentiment classification hasbeen presented in (Pang et al, ).We represent text using the popular bag-of-wordsapproach.
Every piece of text is represented usinga high dimensional feature space.
Every word isconsidered a feature.
The tf-idf weighting schemais used to calculate feature weights.
tf, or term fre-quency, is the number of time a term t occurred ina document d. idf, or inverse document frequency,is a measure of the general importance of the term.It is obtained by dividing the total number of doc-uments by the number of documents containing theterm.
The logarithm of this value is often used in-stead of the original value.We used Support Vector Machines (SVMs) forclassification.
SVM has been shown to be highlyeffective for traditional text classification.
We usedthe SVM Light implementation with default param-eters (Joachims, 1999).
All stop words were re-moved and all documents were length normalizedbefore training.The set of features we use are the set of unigrams,and bigrams representing the words, part-of-speechtags, and dependency relations connecting the twoentities.
For example the following features will beset for the previous example:YOUR ideas, YOUR NNS, YOUR poss,poss nsubj, ...., etc.We use Support Vector Machines (SVM) as alearning system because it is good with handlinghigh dimensional feature spaces.3.3 Extracting the Signed NetworkIn this subsection, we describe the procedure weused to build the signed network given the compo-nents we described in the previous subsections.
Thisprocedure consists of two main steps.
The first isbuilding the network without signs, and the secondis assigning signs to different edges.To build the network, we parse our data to identifydifferent threads, posts and senders.
Every sender isrepresented with a node in the network.
An edgeconnects two nodes if there exists an interaction be-tween the corresponding participants.
We add a di-rected edge A?
B, if A replies to B?s posts at leastn times in m different threads.
We set m, and n to2 in most of our experiments.
The interaction infor-mation (i.e.
who replies to whom) can be extracteddirectly from the thread structure.Once we build the network, we move to the morechallenging task in which we associate a sign withevery edge.
We have shown in the previous sectionhow sentences with positive and negative attitudecan be extracted from text.
Unfortunately the signof an interaction cannot be trivially inferred from thepolarity of sentences.
For example, a single negativesentence written by A and directed to B does notmean that the interaction between A and B is neg-ative.
One way to solve this problem would be tocompare the number of negative sentences to posi-tive sentences in all posts between A and B and clas-sify the interaction according to the plurality value.We will show later, in our experiment section, thatsuch a simplistic method does not perform well inpredicting the sign of an interaction.As a result, we decided to pose the problemas a classical supervised learning problem.
Wecame up with a set of features that we think aregood predictors of the interaction sign, and wetrain a classifier using those features on a labeleddataset.
Our features include numbers and percent-ages of positive/negative sentences per post, postsper thread, and so on.
A sentence is labeled as posi-tive/negative if a relation has been detected in thissentence between a second person pronoun and apositive/negative expression.
A post is consideredpositive/negative based on the majority of relationsdetected in it.
We use two sets of features.
The firstset is related to A only or B only.
The second set10Participant FeaturesNumber of posts per month for A (B)Percentage of positive posts per month for A (B)Percentage of negative posts per month for A (B)genderInteraction FeaturesPercentage/number of positive (negative) sentences per postPercentage/number of positive (negative) posts per threadDiscussion TopicTable 1: Features used by the Interaction Sign Classifier.is related to the interactions between A and B. Thefeatures are outlined in Table 1.4 DataOur data consists of a large amount of discussionthreads collected from online discussion forums.
Wecollected around 41, 000 threads and 1.2M postsfrom the period between the end of 2008 and the endof 2010.
All threads were in English and had 5 postsor more.
They covered a wide range of topics in-cluding: politics, religion, science, etc.
The data wastokenized, sentence-split, and part-of-speech taggedwith the OpenNLP toolkit.
It was parsed with theStanford parser (Klein and Manning, 2003).We randomly selected 5300 posts (having approx-imately 1000 interactions), and asked human anno-tators to label them.
Our annotators were instructedto read all the posts exchanged between two partic-ipants and decide whether the interaction betweenthem is positive or negative.
We used Amazon Me-chanical Turk for annotations.
Following previouswork (Callison-Burch, 2009; Akkaya et al, 2010),we took several precautions to maintain data in-tegrity.
We restricted annotators to those based inthe US to maintain an acceptable level of Englishfluency.
We also restricted annotators to those whohave more than 95% approval rate for all previouswork.
Moreover, we asked three different annota-tors to label every interaction.
The label was com-puted by taking the majority vote among the threeannotators.
We refer to this data as the InteractionsDataset.The kappa measure between the three groups ofannotations was 0.62.
To better assess the qualityof the annotations, we asked a trained annotator tolabel 10% of the data.
We measured the agreementbetween the expert annotator and the majority labelfrom the Mechanical Turk.
The kappa measure wasClass Pos.
Neg.
Weigh.
Avg.TP Rate 0.847 0.809 0.835FP Rate 0.191 0.153 0.179Precision 0.906 0.71 0.844Recall 0.847 0.809 0.835F-Measure 0.875 0.756 0.838Accuracy - - 0.835Table 2: Interaction sign classifier evaluation.0.69.We trained the classifier that detects sentenceswith attitude (Section 3.1) on a set of 4000 manu-ally annotated sentences.
None of this data overlapswith the dataset described earlier.
A similar annota-tion procedure was used to label this data.
We referto this data as the Sentences Dataset.5 Results and DiscussionWe performed experiments on the data describedin the previous section.
We trained and tested thesentence with attitude detection classifiers describedin Section 3.1 using the Sentences Dataset.
Wealso trained and tested the interaction sign classi-fier described in Section 3.3 using the InteractionsDataset.
We build one unsigned network from ev-ery topic in the data set.
This results in a signedsocial network for every topic (e.g.
politics, eco-nomics,etc.).
We decided to build a network for ev-ery topic as opposed to one single network becausethe relation between any two individuals may varyacross topics.
In the rest of this section, we will de-scribe the experiments we did to assess the perfor-mance of the sentences with attitude detection andinteraction sign prediction steps.In addition to classical evaluation, we evaluate ourresults using the structural balance theory which hasbeen shown to hold both theoretically (Heider, 1946)and empirically (Leskovec et al, 2010b).
We val-idate our results by showing that the automaticallyextracted networks mostly agree with the theory.5.1 Identifying Sentences with AttitudeWe tested this component using the SentencesDataset described in Section 4.
In a 10-fold crossvalidation mode, the classifier achieves 80.3% accu-racy, 81.0% precision, %79.4 recall, and 80.2% F1.1100.10.20.30.40.50.60.70.80.91 Balanced Triangles Balanced Triangles (Random)Figure 1: Percentage of balanced triangles in extractednetwork vs. random network.5.2 Interaction Sign ClassifierWe used the relation detection classifier described inSection 3.2 to find sentences with positive and neg-ative attitude.
The output of this classifier was usedto compute the the features described in Section 3.3,which were used to train a classifier that predicts thesign of an interaction between any two individuals.We used Support Vector Machines (SVM) to trainthe sign interaction classifier.
We report several per-formance metrics for them in Table 2.
All resultswere computed using 10 fold cross validation on thelabeled data.
To better assess the performance ofthe proposed classifier, we compare it to a baselinethat labels the relation as negative if the percentageof negative sentences exceeds a particular threshold,otherwise it is labeled as positive.
The thresholdswas empirically evaluated using a separate develop-ment set.
The accuracy of this baseline is only 71%.We evaluated the importance of the features listedin Table 1 by measuring the chi-squared statistic forevery feature with respect to the class.
We foundout that the features describing the interaction be-tween the two participants are more informative thanthe ones describing individuals characteristics.
Thelater features are still helpful though and they im-prove the performance by a statistically significantamount.
We also noticed that all features based onpercentages are more informative than those basedon count.
The most informative features are: per-centage of negative posts per tread, percentage ofnegative sentences per post, percentage of positiveposts per thread, number of negative posts, and dis-cussion topic.5.3 Structural Balance TheoryThe structural balance theory is a psychological the-ory that tries to explain the dynamics of signed so-cial interactions.
It has been shown to hold both the-oretically (Heider, 1946) and empirically (Leskovecet al, 2010b).
In this section, we study the agree-ment between the theory and the automatically ex-tracted networks.
The theory has its origins in thework of Heider (1946).
It was then formalized in agraph theoretic form in (Cartwright and Harary, ).The theory is based on the principles that ?the friendof my friend is my friend?, ?the enemy of my friendis my enemy?, ?the friend of my enemy is my en-emy?, and variations on these.There are several possible ways in which trian-gles representing the relation of three people can besigned.
The structural balance theory states that tri-angles that have an odd number of positive signs (++ + and + - -) are balanced, while triangles that havean even number of positive signs (- - - and + + -) arenot.Even though the structural balance theory positssome triangles as unbalanced, that does not elimi-nate the chance of their existence.
Actually, for mostobserved signed structures for social groups, exactstructural balance does not hold (Doreian and Mr-var, 1996).
Davis (1967) developed the theory fur-ther into the weak structural balance theory.
In thistheory, he extended the structural balance theory tocases where there can be more than two such mu-tually antagonistic subgroups.
Hence, he suggestedthat only triangles with exactly two positive edgesare implausible in real networks, and that all otherkinds of triangles should be permissible.In this section, we connect our analysis to thestructural balance theory.
We compare the predic-tions of edge signs made by our system to the struc-tural balance theory by counting the frequencies ofdifferent types of triangles in the predicted network.Showing that our automatically constructed networkagrees with the structural balance theory further val-idates our results.We compute the frequency of every type of trian-gle for ten different topics.
We compare these fre-quencies to the frequencies of triangles in a set ofrandom networks.
We shuffle signs for all edges onevery network keeping the fractions of positive and120.10.150.20.250.30.35Figure 2: Percentage of negative edges across topics.negative edges constant.We repeat shuffling for 1000 times.
Every time,we compute the frequencies of different types of tri-angles.
We find that the all-positive triangle (+++)is overrepresented in the generated network com-pared to chance across all topics.
We also see thatthe triangle with two positive edges (++?
), and theall-negative triangle (?
?
?)
are underrepresentedcompared to chance across all topics.
The trian-gle with a single positive edge is slightly overrep-resented in most but not all of the topics comparedto chance.
This shows that the predicted networksmostly agree with the structural balance theory.
Ingeneral, the percentage of balanced triangles in thepredicted networks is higher than in the shuffled net-works, and hence the balanced triangles are signif-icantly overrepresented compared to chance.
Fig-ure 1 compares the percentage of balanced trianglesin the predicted networks and the shuffled networks.This proves that our automatically constructed net-work is similar to explicit signed networks in thatthey both mostly agree with the balance theory.6 Application: Dispute Level PredictionThere are many applications that could benefit fromthe signed network representation of discussionssuch as community finding, stance recognition, rec-ommendation systems, and disputed topics identifi-cation.
In this section, we will describe one suchapplication.Discussion forums usually respond quickly tonew topics and events.
Some of those topics usu-ally receive more attention and more dispute thanothers.
We can identify such topics and in generalmeasure the amount of dispute every topic receivesusing the extracted signed network.
We computedthe percentage of negative edges to all edges for ev-ery topic.
We believe that this would act as a mea-sure for how disputed a particular topic is.
We see,from Figure 2, that ?environment?, ?science?, and?technology?
topics are among the least disputedtopics, whereas ?terrorism?, ?abortion?
and ?eco-nomics?
are among the most disputed topics.
Thesefindings are another way of validating our predic-tions.
They also suggest another application for thiswork that focuses on measuring the amount of dis-pute different topics receive.
This can be done formore specific topics, rather than high level topics asshown here, to identify hot topics that receive a lotof dispute.7 ConclusionsIn this paper, we have shown that natural languageprocessing techniques can be reliably used to extractsigned social networks from text correspondences.We believe that this work brings us closer to un-derstanding the relation between language use andsocial interactions and opens the door to further re-search efforts that go beyond standard social net-work analysis by studying the interplay of positiveand negative connections.
We rigorously evaluatedthe proposed methods on labeled data and connectedour analysis to social psychology theories to showthat our predictions mostly agree with them.
Finally,we presented potential applications that benefit fromthe automatically extracted signed network.ReferencesCem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon mechanical turk forsubjectivity word sense disambiguation.
In CSLDAMT?10.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.
InLREC?08.Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.2008.
Friends and foes: ideological social network-ing.
In SIGCHI.Chris Callison-Burch.
2009.
Fast, cheap, and creative:evaluating translation quality using amazon?s mechan-ical turk.
In EMNLP ?9, EMNLP ?09.Dorwin Cartwright and Frank Harary.
Structure balance:A generalization of heiders theory.
Psych.
Rev.J.
A. Davis.
1967.
Clustering and structural balance ingraphs.
Human Relations.Patrick Doreian and Andrej Mrvar.
1996.
A partitioningapproach to structural balance.
Social Networks.13Patrick Doreian and Andrej Mrvar.
2009.
Partitioningsigned social networks.
Social Networks.David Elson, Nicholas Dames, and Kathleen McKeown.2010.
Extracting social networks from literary fiction.In ACL 2010, Uppsala, Sweden.Anatoliy Gruzd and Caroline Haythornthwaite.
2008.Automated discovery and analysis of social networksfrom threaded discussions.
In (INSNA).Ahmed Hassan and Dragomir Radev.
2010.
Identifyingtext polarity using random walks.
In ACL?10.Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.2010.
What?s with the attitude?
: identifying sentenceswith attitude in online discussions.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing.V.
Hatzivassiloglou and K. McKeown.
1997.
Predictingthe semantic orientation of adjectives.
In EACL?97.Vasileios Hatzivassiloglou and Janyce Wiebe.
2000.
Ef-fects of adjective orientation and gradability on sen-tence subjectivity.
In COLING.Fritz Heider.
1946.
Attitudes and cognitive organization.Journal of Psychology.J.
Huang, M. Zhou, and D. Yang.
2007.
Extracting chat-bot knowledge from online discussion forums.
In IJ-CAI?07.Thorsten Joachims, 1999.
Making large-scale supportvector machine learning practical.
MIT Press.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL?03.Je?ro?me Kunegis, Andreas Lommatzsch, and ChristianBauckhage.
2009.
The slashdot zoo: mining a socialnetwork with negative edges.
In WWW ?09.Lillian Lee.
1999.
Measures of distributional similarity.In ACL-1999.A.
Lehrer.
1974.
Semantic fields and lezical structure.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010a.
Predicting positive and negative links in onlinesocial networks.
In WWW ?10.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010b.
Signed networks in social media.
In CHI 2010.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010c.
Signed networks in social media.
In Proceed-ings of the 28th international conference on Humanfactors in computing systems, pages 1361?1370, NewYork, NY, USA.Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,and Wei Wang.
2009.
Simultaneously modeling se-mantics and structure of threaded discussions: a sparsecoding approach and its applications.
In SIGIR ?09.Andrew McCallum, Xuerui Wang, and Andre?s Corrada-Emmanuel.
2007.
Topic and role discovery in so-cial networks with experiments on enron and academicemail.
J. Artif.
Int.
Res., 30:249?272, October.George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Commun.
ACM.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural languageprocessing.
In K-CAP ?03.J.
Norris.
1997.
Markov chains.
Cambridge UniversityPress.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.Thumbs up?
: sentiment classification using machinelearning techniques.
In EMNLP.
Association for Com-putational Linguistics.A.
Popescu and O. Etzioni.
2005.
Extracting product fea-tures and opinions from reviews.
In HLT-EMNLP?05.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In EMNLP?03.Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.2006.
Thread detection in dynamic text messagestreams.
In SIGIR ?06, pages 35?42.Philip Stone, Dexter Dunphy, Marchall Smith, and DanielOgilvie.
1966.
The general inquirer: A computer ap-proach to content analysis.
The MIT Press.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In ACL?05.P.
Turney and M. Littman.
2003.
Measuring praise andcriticism: Inference of semantic orientation from asso-ciation.
Transactions on Information Systems.Janyce Wiebe, Rebecca Bruce, Matthew Bell, MelanieMartin, and Theresa Wilson.
2001.
A corpus study ofevaluative and speculative language.
In Proceedingsof the Second SIGdial Workshop on Discourse and Di-alogue, pages 1?10.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In AAAI-IAAI.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.2005a.
Opinionfinder: a system for subjectivity anal-ysis.
In HLT/EMNLP.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT/EMNLP?05.Bo Yang, William Cheung, and Jiming Liu.
2007.
Com-munity mining from signed social networks.
IEEETrans.
on Knowl.
and Data Eng., 19(10).Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: separating facts fromopinions and identifying the polarity of opinion sen-tences.
In EMNLP?03.14
