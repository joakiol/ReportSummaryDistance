Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532?1541,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAll Words Domain Adapted WSD: Finding a Middle Ground betweenSupervision and UnsupervisionMitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak BhattacharyyaIndian Institute of Technology Bombay,Mumbai - 400076, India.
{miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.inAbstractIn spite of decades of research on wordsense disambiguation (WSD), all-wordsgeneral purpose WSD has remained a dis-tant goal.
Many supervised WSD systemshave been built, but the effort of creat-ing the training corpus - annotated sensemarked corpora - has always been a matterof concern.
Therefore, attempts have beenmade to develop unsupervised and knowl-edge based techniques for WSD which donot need sense marked corpora.
Howeversuch approaches have not proved effective,since they typically do not better Word-net first sense baseline accuracy.
Our re-search reported here proposes to stick tothe supervised approach, but with far lessdemand on annotation.
We show that ifwe have ANY sense marked corpora, be itfrom mixed domain or a specific domain, asmall amount of annotation in ANY otherdomain can deliver the goods almost asif exhaustive sense marking were avail-able in that domain.
We have tested ourapproach across Tourism and Health do-main corpora, using also the well knownmixed domain SemCor corpus.
Accuracyfigures close to self domain training lendcredence to the viability of our approach.Our contribution thus lies in finding a con-venient middle ground between pure su-pervised and pure unsupervised WSD.
Fi-nally, our approach is not restricted to anyspecific set of target words, a departurefrom a commonly observed practice in do-main specific WSD.1 IntroductionAmongst annotation tasks, sense marking surelytakes the cake, demanding as it does high levelof language competence, topic comprehension anddomain sensitivity.
This makes supervised ap-proaches to WSD a difficult proposition (Agirreet al, 2009b; Agirre et al, 2009a; McCarthy etal., 2007).
Unsupervised and knowledge based ap-proaches have been tried with the hope of creatingWSD systems with no need for sense marked cor-pora (Koeling et al, 2005; McCarthy et al, 2007;Agirre et al, 2009b).
However, the accuracy fig-ures of such systems are low.Our work here is motivated by the desire to de-velop annotation-lean all-words domain adaptedtechniques for supervised WSD.
It is a commonobservation that domain specific WSD exhibitshigh level of accuracy even for the all-words sce-nario (Khapra et al, 2010) - provided training andtesting are on the same domain.
Also domainadaptation - in which training happens in one do-main and testing in another - often is able to attaingood levels of performance, albeit on a specific setof target words (Chan and Ng, 2007; Agirre andde Lacalle, 2009).
To the best of our knowledgethere does not exist a system that solves the com-bined problem of all words domain adapted WSD.We thus propose the following:a.
For any target domain, create a small amountof sense annotated corpus.b.
Mix it with an existing sense annotated cor-pus ?
from a mixed domain or specific do-main ?
to train the WSD engine.This procedure tested on four adaptation scenar-ios, viz., (i) SemCor (Miller et al, 1993) toTourism, (ii) SemCor to Health, (iii) Tourism toHealth and (iv) Health to Tourism has consistentlyyielded good performance (to be explained in sec-tions 6 and 7).The remainder of this paper is organized as fol-lows.
In section 2 we discuss previous work in thearea of domain adaptation for WSD.
In section 31532we discuss three state of art supervised, unsuper-vised and knowledge based algorithms for WSD.Section 4 discusses the injection strategy for do-main adaptation.
In section 5 we describe thedataset used for our experiments.
We then presentthe results in section 6 followed by discussions insection 7.
Section 8 examines whether there is anyneed for intelligent choice of injections.
Section9 concludes the paper highlighting possible futuredirections.2 Related WorkDomain specific WSD for selected target wordshas been attempted by Ng and Lee (1996), Agirreand de Lacalle (2009), Chan and Ng (2007), Koel-ing et al (2005) and Agirre et al (2009b).
Theyreport results on three publicly available lexicalsample datasets, viz., DSO corpus (Ng and Lee,1996), MEDLINE corpus (Weeber et al, 2001)and the corpus made available by Koeling et al(2005).
Each of these datasets contains a handfulof target words (41-191 words) which are sensemarked in the corpus.Our main inspiration comes from the target-word specific results reported by Chan and Ng(2007) and Agirre and de Lacalle (2009).
Theformer showed that adding just 30% of the targetdata to the source data achieved the same perfor-mance as that obtained by taking the entire sourceand target data.
Agirre and de Lacalle (2009) re-ported a 22% error reduction when source andtarget data were combined for training a classi-fier, as compared to the case when only the targetdata was used for training the classifier.
However,both these works focused on target word specificWSD and do not address all-words domain spe-cific WSD.In the unsupervised setting, McCarthy et al(2007) showed that their predominant sense acqui-sition method gives good results on the corpus ofKoeling et al (2005).
In particular, they showedthat the performance of their method is compa-rable to the most frequent sense obtained from atagged corpus, thereby making a strong case forunsupervised methods for domain-specific WSD.More recently, Agirre et al (2009b) showed thatknowledge based approaches which rely only onthe semantic relations captured by the Wordnetgraph outperform supervised approaches when ap-plied to specific domains.
The good results ob-tained by McCarthy et al (2007) and Agirre etal.
(2009b) for unsupervised and knowledge basedapproaches respectively have cast a doubt on theviability of supervised approaches which rely onsense tagged corpora.
However, these conclusionswere drawn only from the performance on certaintarget words, leaving open the question of theirutility in all words WSD.We believe our work contributes to the WSDresearch in the following way: (i) it shows thatthere is promise in supervised approach to all-word WSD, through the instrument of domainadaptation; (ii) it places in perspective some veryrecently reported unsupervised and knowledgebased techniques of WSD; (ii) it answers somequestions arising out of the debate between super-vision and unsupervision in WSD; and finally (iv)it explores a convenient middle ground betweenunsupervised and supervised WSD ?
the territoryof ?annotate-little and inject?
paradigm.3 WSD algorithms employed by usIn this section we describe the knowledge based,unsupervised and supervised approaches used forour experiments.3.1 Knowledge Based ApproachAgirre et al (2009b) showed that a graph basedalgorithm which uses only the relations betweenconcepts in a Lexical Knowledge Base (LKB) canoutperform supervised approaches when tested onspecific domains (for a set of chosen target words).We employ their method which involves the fol-lowing steps:1.
Represent Wordnet as a graph where the con-cepts (i.e., synsets) act as nodes and the re-lations between concepts define edges in thegraph.2.
Apply a context-dependent PersonalizedPageRank algorithm on this graph by intro-ducing the context words as nodes into thegraph and linking them with their respectivesynsets.3.
These nodes corresponding to the contextwords then inject probability mass into thesynsets they are linked to, thereby influencingthe final relevance of all nodes in the graph.We used the publicly available implementationof this algorithm1 for our experiments.1http://ixa2.si.ehu.es/ukb/15333.2 Unsupervised ApproachMcCarthy et al (2007) used an untagged corpus toconstruct a thesaurus of related words.
They thenfound the predominant sense (i.e., the most fre-quent sense) of each target word using pair-wiseWordnet based similarity measures by pairing thetarget word with its top-k neighbors in the the-saurus.
Each target word is then disambiguatedby assigning it its predominant sense ?
the moti-vation being that the predominant sense is a pow-erful, hard-to-beat baseline.
We implemented theirmethod using the following steps:1.
Obtain a domain-specific untagged corpus (wecrawled a corpus of approximately 9M wordsfrom the web).2.
Extract grammatical relations from this text us-ing a dependency parser2 (Klein and Manning,2003).3.
Use the grammatical relations thus extracted toconstruct features for identifying the k nearestneighbors for each word using the distributionalsimilarity score described in (Lin, 1998).4.
Rank the senses of each target word in the testset using a weighted sum of the distributionalsimilarity scores of the neighbors.
The weightsin the sum are based on Wordnet Similarityscores (Patwardhan and Pedersen, 2003).5.
Each target word in the test set is then disam-biguated by simply assigning it its predominantsense obtained using the above method.3.3 Supervised approachKhapra et al (2010) proposed a supervised algo-rithm for domain-specific WSD and showed that itbeats the most frequent corpus sense and performson par with other state of the art algorithms likePageRank.
We implemented their iterative algo-rithm which involves the following steps:1.
Tag all monosemous words in the sentence.2.
Iteratively disambiguate the remaining words inthe sentence in increasing order of their degreeof polysemy.3.
At each stage rank the candidate senses ofa word using the scoring function of Equa-tion (1) which combines corpus based param-eters (such as, sense distributions and corpusco-occurrence) and Wordnet based parameters2We used the Stanford parser - http://nlp.stanford.edu/software/lex-parser.shtml(such as, semantic similarity, conceptual dis-tance, etc.)S?
= arg maxi(?iVi +?j?JWij ?
Vi ?
Vj)(1)where,i ?
Candidate SynsetsJ = Set of disambiguated words?i = BelongingnessToDominantConcept(Si)Vi = P (Si|word)Wij = CorpusCooccurrence(Si, Sj)?
1/WNConceptualDistance(Si, Sj)?
1/WNSemanticGraphDistance(Si, Sj)4.
Select the candidate synset with maximizes theabove score as the winner sense.4 Injections for Supervised AdaptationThis section describes the main interest of ourwork i.e.
adaptation using injections.
For su-pervised adaptation, we use the supervised algo-rithm described above (Khapra et al, 2010) in thefollowing 3 settings as proposed by Agirre et al(2009a):a.
Source setting: We train the algorithm on amixed-domain corpus (SemCor) or a domain-specific corpus (say, Tourism) and test it on adifferent domain (say, Health).
A good perfor-mance in this setting would indicate robustnessto domain-shifts.b.
Target setting: We train and test the algorithmusing data from the same domain.
This gives theskyline performance, i.e., the best performancethat can be achieved if sense marked data fromthe target domain were available.c.
Adaptation setting: This setting is the main fo-cus of interest in the paper.
We augment thetraining data which could be from one domainor mixed domain with a small amount of datafrom the target domain.
This combined data isthen used for training.
The aim here is to reachas close to the skyline performance using as lit-tle data as possible.
For injecting data from thetarget domain we randomly select some sensemarked words from the target domain and add1534Polysemous words Monosemous wordsCategory Tourism Health Tourism HealthNoun 53133 15437 23665 6979Verb 15528 7348 1027 356Adjective 19732 5877 10569 2378Adverb 6091 1977 4323 1694All 94484 30639 39611 11407Avg.
no.
of instances perpolysemous wordCategory Health Tourism SemCorNoun 7.06 12.56 10.98Verb 7.47 9.76 11.95Adjective 5.74 12.07 8.67Adverb 9.11 19.78 25.44All 6.94 12.17 11.25Table 1: Polysemous and Monosemous words percategory in each domainTable 2: Average number of instances per polyse-mous word per category in the 3 domainsAvg.
degree of Wordnet polysemyfor polysemous wordsCategory Health Tourism SemCorNoun 5.24 4.95 5.60Verb 10.60 10.10 9.89Adjective 5.52 5.08 5.40Adverb 3.64 4.16 3.90All 6.49 5.77 6.43Avg.
degree of Corpus polysemyfor polysemous wordsCategory Health Tourism SemCorNoun 1.92 2.60 3.41Verb 3.41 4.55 4.73Adjective 2.04 2.57 2.65Adverb 2.16 2.82 3.09All 2.31 2.93 3.56Table 3: Average degree of Wordnet polysemy ofpolysemous words per category in the 3 domainsTable 4: Average degree of Corpus polysemy ofpolysemous words per category in the 3 domainsthem to the training data.
An obvious ques-tion which arises at this point is ?Why were thewords selected at random??
or ?Can selectionof words using some active learning strategyyield better results than a random selection?
?We discuss this question in detail in Section 7and show that a random set of injections per-forms no worse than a craftily selected set ofinjections.5 DataSet PreparationDue to the lack of any publicly available all-wordsdomain specific sense marked corpora we set uponthe task of collecting data from two domains, viz.,Tourism and Health.
The data for Tourism do-main was downloaded from Indian Tourism web-sites whereas the data for Health domain was ob-tained from two doctors.
This data was manu-ally sense annotated by two lexicographers adeptin English.
Princeton Wordnet 2.13 (Fellbaum,1998) was used as the sense inventory.
A totalof 1,34,095 words from the Tourism domain and42,046 words from the Health domain were man-ually sense marked.
Some files were sense markedby both the lexicographers and the Inter TaggerAgreement (ITA) calculated from these files was83% which is comparable to the 78% ITA reportedon the SemCor corpus considering the domain-specific nature of the corpus.We now present different statistics about thecorpora.
Table 1 summarizes the number of poly-semous and monosemous words in each category.3http://wordnetweb.princeton.edu/perl/webwnNote that we do not use the monosemous wordswhile calculating precision and recall of our algo-rithms.Table 2 shows the average number of instancesper polysemous word in the 3 corpora.
We notethat the number of instances per word in theTourism domain is comparable to that in the Sem-Cor corpus whereas the number of instances perword in the Health corpus is smaller due to theoverall smaller size of the Health corpus.Tables 3 and 4 summarize the average degreeof Wordnet polysemy and corpus polysemy of thepolysemous words in the corpus.
Wordnet poly-semy is the number of senses of a word as listedin the Wordnet, whereas corpus polysemy is thenumber of senses of a word actually appearing inthe corpus.
As expected, the average degree ofcorpus polysemy (Table 4) is much less than theaverage degree of Wordnet polysemy (Table 3).Further, the average degree of corpus polysemy(Table 4) in the two domains is less than that in themixed-domain SemCor corpus, which is expecteddue to the domain specific nature of the corpora.Finally, Table 5 summarizes the number of uniquepolysemous words per category in each domain.No.
of unique polysemous wordsCategory Health Tourism SemCorNoun 2188 4229 5871Verb 984 1591 2565Adjective 1024 1635 2640Adverb 217 308 463All 4413 7763 11539Table 5: Number of unique polysemous words per categoryin each domain.1535The data is currently being enhanced by manu-ally sense marking more words from each domainand will be soon freely available4 for research pur-poses.6 ResultsWe tested the 3 algorithms described in section 4using SemCor, Tourism and Health domain cor-pora.
We did a 2-fold cross validation for su-pervised adaptation and report the average perfor-mance over the two folds.
Since the knowledgebased and unsupervised methods do not need anytraining data we simply test it on the entire corpusfrom the two domains.6.1 Knowledge Based approachThe results obtained by applying the PersonalizedPageRank (PPR) method to Tourism and Healthdata are summarized in Table 6.
We also reportthe Wordnet first sense baseline (WFS).Domain Algorithm P(%) R(%) F(%)Tourism PPR 53.1 53.1 53.1WFS 62.5 62.5 62.5Health PPR 51.1 51.1 51.1WFS 65.5 65.5 65.5Table 6: Comparing the performance of Person-alized PageRank (PPR) with Wordnet First SenseBaseline (WFS)6.2 Unsupervised approachThe predominant sense for each word in the twodomains was calculated using the method de-scribed in section 4.2.
McCarthy et al (2004)reported that the best results were obtained us-ing k = 50 neighbors and the Wordnet Similar-ity jcn measure (Jiang and Conrath, 1997).
Fol-lowing them, we used k = 50 and observed thatthe best results for nouns and verbs were obtainedusing the jcn measure and the best results for ad-jectives and adverbs were obtained using the leskmeasure (Banerjee and Pedersen, 2002).
Accord-ingly, we used jcn for nouns and verbs and leskfor adjectives and adverbs.
Each target word inthe test set is then disambiguated by simply as-signing it its predominant sense obtained usingthe above method.
We tested this approach onlyon Tourism domain due to unavailability of large4http://www.cfilt.iitb.ac.in/wsd/annotated corpusuntagged Health corpus which is needed for con-structing the thesaurus.
The results are summa-rized in Table 7.Domain Algorithm P(%) R(%) F(%)Tourism McCarthy 51.85 49.32 50.55WFS 62.50 62.50 62.50Table 7: Comparing the performance of unsuper-vised approach with Wordnet First Sense Baseline(WFS)6.3 Supervised adaptationWe report results in the source setting, target set-ting and adaptation setting as described earlierusing the following four combinations for sourceand target data:1.
SemCor to Tourism (SC?T) where SemCor isused as the source domain and Tourism as thetarget (test) domain.2.
SemCor to Health (SC?H) where SemCor isused as the source domain and Health as the tar-get (test) domain.3.
Tourism to Health (T?H) where Tourism isused as the source domain and Health as the tar-get (test) domain.4.
Health to Tourism (H?T) where Health isused as the source domain and Tourism as thetarget (test) domain.In each case, the target domain data was dividedinto two folds.
One fold was set aside for testingand the other for injecting data in the adaptationsetting.
We increased the size of the injected targetexamples from 1000 to 14000 words in incrementsof 1000.
We then repeated the same experiment byreversing the role of the two folds.Figures 1, 2, 3 and 4 show the graphs of the av-erage F-score over the 2-folds for SC?T, SC?H,T?H and H?T respectively.
The x-axis repre-sents the amount of training data (in words) in-jected from the target domain and the y-axis rep-resents the F-score.
The different curves in eachgraph are as follows:a. only random : This curve plots the perfor-mance obtained using x randomly selectedsense tagged words from the target domain andzero sense tagged words from the source do-main (x was varied from 1000 to 14000 wordsin increments of 1000).1536354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyonly_randomrandom+semcor354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyonly_randomrandom+semcorFigure 1: Supervised adaptation fromSemCor to Tourism using injectionsFigure 2: Supervised adaptation fromSemCor to Health using injections354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyonly_randomrandom+tourism354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyonly_randomrandom+healthFigure 3: Supervised adaptation fromTourism to Health using injectionsFigure 4: Supervised adaptation fromHealth to Tourism using injectionsb.
random+source : This curve plots the perfor-mance obtained by mixing x randomly selectedsense tagged words from the target domain withthe entire training data from the source domain(again x was varied from 1000 to 14000 wordsin increments of 1000).c.
source baseline (srcb) : This represents the F-score obtained by training on the source dataalone without mixing any examples from thetarget domain.d.
wordnet first sense (wfs) : This represents theF-score obtained by selecting the first sensefrom Wordnet, a typically reported baseline.e.
target skyline (tsky) : This represents the av-erage 2-fold F-score obtained by training onone entire fold of the target data itself (Health:15320 polysemous words; Tourism: 47242 pol-ysemous words) and testing on the other fold.These graphs along with other results are dis-cussed in the next section.7 DiscussionsWe discuss the performance of the three ap-proaches.7.1 Knowledge Based and UnsupervisedapproachesIt is apparent from Tables 6 and 7 that knowl-edge based and unsupervised approaches do notperform well when compared to the Wordnet firstsense (which is freely available and hence can beused for disambiguation).
Further, we observe thatthe performance of these approaches is even lessthan the source baseline (i.e., the case when train-ing data from a source domain is applied as it isto a target domain - without using any injections).These observations bring out the weaknesses ofthese approaches when used in an all-words set-ting and clearly indicate that they come nowhereclose to replacing a supervised system.15377.2 Supervised adaptation1.
The F-score obtained by training on SemCor(mixed-domain corpus) and testing on the twotarget domains without using any injections(srcb) ?
F-score of 61.7% on Tourism and F-score of 65.5% on Health ?
is comparable to thebest result reported on the SEMEVAL datasets(65.02%, where both training and testing hap-pens on a mixed-domain corpus (Snyder andPalmer, 2004)).
This is in contrast to previ-ous studies (Escudero et al, 2000; Agirre andMartinez, 2004) which suggest that instead ofadapting from a generic/mixed domain to a spe-cific domain, it is better to completely ignorethe generic examples and use hand-tagged datafrom the target domain itself.
The main rea-son for the contrasting results is that the ear-lier work focused only on a handful of targetwords whereas we focus on all words appearingin the corpus.
So, while the behavior of a fewtarget words would change drastically when thedomain changes, a majority of the words willexhibit the same behavior (i.e., same predomi-nant sense) even when the domain changes.
Weagree that the overall performance is still lowerthan that obtained by training on the domain-specific corpora.
However, it is still better thanthe performance of unsupervised and knowl-edge based approaches which tilts the scale infavor of supervised approaches even when onlymixed domain sense marked corpora is avail-able.2.
Adding injections from the target domain im-proves the performance.
As the amount of in-jection increases the performance approachesthe skyline, and in the case of SC?H and T?Hit even crosses the skyline performance showingthat combining the source and target data cangive better performance than using the targetdata alone.
This is consistent with the domainadaptation results reported by Agirre and de La-calle (2009) on a specific set of target words.3.
The performance of random+source is alwaysbetter than only random indicating that the datafrom the source domain does help to improveperformance.
A detailed analysis showed thatthe gain obtained by using the source data is at-tributable to reducing recall errors by increasingthe coverage of seen words.4.
Adapting from one specific domain (Tourism orHealth) to another specific domain (Health orTourism) gives the same performance as that ob-tained by adapting from a mixed-domain (Sem-Cor) to a specific domain (Tourism, Health).This is an interesting observation as it suggeststhat as long as data from one domain is avail-able it is easy to build a WSD engine that worksfor other domains by injecting a small amountof data from these domains.To verify that the results are consistent, we ran-domly selected 5 different sets of injections fromfold-1 and tested the performance on fold-2.
Wethen repeated the same experiment by reversingthe roles of the two folds.
The results were in-deed consistent irrespective of the set of injectionsused.
Due to lack of space we have not includedthe results for these 5 different sets of injections.7.3 Quantifying the trade-off betweenperformance and corpus sizeTo correctly quantify the benefit of adding injec-tions from the target domain, we calculated theamount of target data (peak size) that is neededto reach the skyline F-score (peak F) in the ab-sence of any data from the source domain.
Thepeak size was found to be 35000 (Tourism) and14000 (Health) corresponding to peak F values of74.2% (Tourism) and 73.4% (Health).
We thenplotted a graph (Figure 5) to capture the rela-tion between the size of injections (expressed asa percentage of the peak size) and the F-score (ex-pressed as a percentage of the peak F).808590951001050  20  40  60  80  100%peak_F% peak_sizeSize v/s PerformanceSC --> HT --> HSC --> TH --> TFigure 5: Trade-off between performanceand corpus sizeWe observe that by mixing only 20-40% of thepeak size with the source domain we can obtain upto 95% of the performance obtained by using the1538entire target data (peak size).
In absolute terms,the size of the injections is only 7000-9000 poly-semous words which is a very small price to payconsidering the performance benefits.8 Does the choice of injections matter?An obvious question which arises at this point is?Why were the words selected at random??
or?Can selection of words using some active learn-ing strategy yield better results than a randomselection??
An answer to this question requiresa more thorough understanding of the sense-behavior exhibited by words across domains.
Inany scenario involving a shift from domain D1 todomain D2, we will always encounter words be-longing to the following 4 categories:a. WD1 : This class includes words which are en-countered only in the source domain D1 and donot appear in the target domain D2.
Since weare interested in adapting to the target domainand since these words do not appear in the tar-get domain, it is quite obvious that they are notimportant for the problem of domain adapta-tion.b.
WD2 : This class includes words which are en-countered only in the target domain D2 and donot appear in the source domain D1.
Again, itis quite obvious that these words are importantfor the problem of domain adaptation.
They fallin the category of unseen words and need han-dling from that point of view.c.
WD1D2conformists : This class includes wordswhich are encountered in both the domains andexhibit the same predominant sense in both thedomains.
Correct identification of these wordsis important so that we can use the predomi-nant sense learned from D1 for disambiguatinginstances of these words appearing in D2.d.
WD1D2non?conformists : This class includeswords which are encountered in both the do-mains but their predominant sense in the tar-get domain D2 does not conform to the pre-dominant sense learned from the source domainD1.
Correct identification of these words is im-portant so that we can ignore the predominantsenses learned from D1 while disambiguatinginstances of these words appearing in D2.Table 8 summarizes the percentage of words thatfall in each category in each of the three adapta-tion scenarios.
The fact that nearly 50-60% of thewords fall in the ?conformist?
category once againmakes a strong case for reusing sense tagged datafrom one domain to another domain.Category SC?T SC?H T?HWD2 7.14% 5.45% 13.61%Conformists 49.54% 60.43% 54.31%Non-Conformists 43.30% 34.11% 32.06%Table 8: Percentage of Words belonging to eachcategory in the three settings.The above characterization suggests that an idealdomain adaptation strategy should focus on in-jecting WD2 and WD1D2non?conformists as thesewould yield maximum benefits if injected into thetraining data.
While it is easy to identify theWD2 words, ?identifying non-conformists?
is ahard problem which itself requires some type ofWSD5.
However, just to prove that a random in-jection strategy does as good as an ideal strategywe assume the presence of an oracle which iden-tifies the WD1D2non?conformists.
We then augmentthe training data with 5-8 instances for WD2 andWD1D2non?conformists words thus identified.
Weobserved that adding more than 5-8 instances perword does not improve the performance.
This isdue to the ?one sense per domain?
phenomenon ?seeing only a few instances of a word is sufficientto identify the predominant sense of the word.
Fur-ther, to ensure a better overall performance, theinstances of the most frequent words are injectedfirst followed by less frequent words till we ex-haust the total size of the injections (1000, 2000and so on).
We observed that there was a 75-80% overlap between the words selected by ran-dom strategy and oracle strategy.
This is becauseoracle selects the most frequent words which alsohave a high chance of getting selected when a ran-dom sampling is done.Figures 6, 7, 8 and 9 compare the performanceof the two strategies.
We see that the random strat-egy does as well as the oracle strategy thereby sup-porting our claim that if we have sense markedcorpus from one domain then simply injecting ANYsmall amount of data from the target domain will5Note that the unsupervised predominant sense acquisi-tion method of McCarthy et al (2007) implicitly identifiesconformists and non-conformists1539354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyrandom+semcororacle+semcor354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyrandom+semcororacle+semcorFigure 6: Comparing random strategywith oracle based ideal strategy for Sem-Cor to Tourism adaptationFigure 7: Comparing random strategywith oracle based ideal strategy for Sem-Cor to Health adaptation354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyrandom+tourismoracle+tourism354045505560657075800  2000  4000  6000  8000  10000  12000  14000F-score(%)Injection Size (words)Injection Size v/s F-scorewfssrcbtskyrandom+healthoracle+healthFigure 8: Comparing random strat-egy with oracle based ideal strategy forTourism to Health adaptationFigure 9: Comparing random strat-egy with oracle based ideal strategy forHealth to Tourism adaptationdo the job.9 Conclusion and Future WorkBased on our study of WSD in 4 domain adap-tation scenarios, we make the following conclu-sions:1.
Supervised adaptation by mixing small amountof data (7000-9000 words) from the target do-main with the source domain gives nearly thesame performance (F-score of around 70% inall the 4 adaptation scenarios) as that obtainedby training on the entire target domain data.2.
Unsupervised and knowledge based approacheswhich use distributional similarity and Word-net based similarity measures do not comparewell with the Wordnet first sense baseline per-formance and do not come anywhere close tothe performance of supervised adaptation.3.
Supervised adaptation from a mixed domain toa specific domain gives the same performanceas that from one specific domain (Tourism) toanother specific domain (Health).4.
Supervised adaptation is not sensitive to thetype of data being injected.
This is an interest-ing finding with the following implication: aslong as one has sense marked corpus - be it froma mixed or specific domain - simply injectingANY small amount of data from the target do-main suffices to beget good accuracy.As future work, we would like to test our work onthe Environment domain data which was releasedas part of the SEMEVAL 2010 shared task on ?All-words Word Sense Disambiguation on a SpecificDomain?.1540ReferencesEneko Agirre and Oier Lopez de Lacalle.
2009.
Su-pervised domain adaption for wsd.
In EACL ?09:Proceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 42?50, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Eneko Agirre and David Martinez.
2004.
The effect ofbias on an automatically-built word sense corpus.
InProceedings of the 4rd International Conference onLanguages Resources and Evaluations (LREC).Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Andrea Marchetti, Antonio Toral, and PiekVossen.
2009a.
Semeval-2010 task 17: all-wordsword sense disambiguation on a specific domain.
InDEW ?09: Proceedings of the Workshop on Seman-tic Evaluations: Recent Achievements and FutureDirections, pages 123?128, Morristown, NJ, USA.Association for Computational Linguistics.Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.2009b.
Knowledge-based wsd on specific domains:Performing better than generic supervised wsd.
InIn Proceedings of IJCAI.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using wordnet.
In CICLing ?02: Proceedingsof the Third International Conference on Compu-tational Linguistics and Intelligent Text Processing,pages 136?145, London, UK.
Springer-Verlag.Yee Seng Chan and Hwee Tou Ng.
2007.
Do-main adaptation with active learning for word sensedisambiguation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 49?56, Prague, Czech Republic,June.
Association for Computational Linguistics.Gerard Escudero, Llu?
?s Ma`rquez, and German Rigau.2000.
An empirical study of the domain depen-dence of supervised word sense disambiguation sys-tems.
In Proceedings of the 2000 Joint SIGDAT con-ference on Empirical methods in natural languageprocessing and very large corpora, pages 172?180,Morristown, NJ, USA.
Association for Computa-tional Linguistics.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In Proc.
of the Int?l.
Conf.
on Research in Computa-tional Linguistics, pages 19?33.Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-pak Bhattacharyya.
2010.
Domain-specific wordsense disambiguation combining corpus based andwordnet based parameters.
In 5th InternationalConference on Global Wordnet (GWC2010).Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In IN PROCEEDINGSOF THE 41ST ANNUAL MEETING OF THE ASSO-CIATION FOR COMPUTATIONAL LINGUISTICS,pages 423?430.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In HLT ?05: Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 419?426, Morristown, NJ, USA.Association for Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of the 17thinternational conference on Computational linguis-tics, pages 768?774, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In ACL ?04: Proceedings of the 42ndAnnual Meeting on Association for ComputationalLinguistics, page 279, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of predom-inant word senses.
Comput.
Linguist., 33(4):553?590.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InHLT ?93: Proceedings of the workshop on HumanLanguage Technology, pages 303?308, Morristown,NJ, USA.
Association for Computational Linguis-tics.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: an exemplar-based approach.
In Proceedingsof the 34th annual meeting on Association for Com-putational Linguistics, pages 40?47, Morristown,NJ, USA.
Association for Computational Linguis-tics.Siddharth Patwardhan and Ted Pedersen.
2003.The cpan wordnet::similarity package.
http://search.cpan.org/ sid/wordnet-similarity/.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Rada Mihalcea and PhilEdmonds, editors, Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, pages 41?43, Barcelona,Spain, July.
Association for Computational Linguis-tics.Marc Weeber, James G. Mork, and Alan R. Aronson.2001.
Developing a test collection for biomedicalword sense disambiguation.
In In Proceedings ofthe AMAI Symposium, pages 746?750.1541
