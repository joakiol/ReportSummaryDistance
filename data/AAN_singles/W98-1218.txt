IIl/I////////Applications and Explanations of Zipf's LawDavid M. W. PowersDepartment of Computer ScienceThe Flinders University of South Australiapowers@acm.orgAbstractRecently I have been intrigued by the reappearance of anold friend, George Kingsley Zipf, in a number of notentirely expected places.
The law named for him isubiquitous, but Zipf did not actually discover the law somuch as provide a plausible explanation.
Others haveproposed modifications to Zipf's Law, and closerexamination uncovers systematic deviations from itsnormative form.
We demonstrate how Zipf's analysis canbe extended to include some of these phenomena.1.
Introduction and MotivationIn this paper we wish to revisit Zipf's study of therelationship between rank and frequency of variouslinguistic and social units and constructions.
The paperarises out of observations in Natural Language Learningexperiments of deviations from the received version ofZipf's Law.
As it may not be immediately obvious whythis relationship is of significance in NLL, we verybriefly mention some of the places where the relationshipaffects research in our field, and which we feel couldusefully be further explored.1.1 Quantitative LinguisticsThe field of which Zipf was the pioneer is discoveringlots of interesting empirical aws, but how much has itadvanced in explanation or application (K6hler,1991)?1.2 Statistical Learning MethodsZipf's Law tells us how much text we have to look atand how precise our statistics have to be to achieve whatlevel of expected error.
(Finch, 1993; Powers, 1996).
Forexample, the most frequent 150 words typically accountfor around half the words of a corpus, although this figurevaries significantly with the size of the corpus, the size ofthe lexicon, the genre, register and medium ofcommunication a d the linguistic complexity of the text- -  and this is one of the phenomena we wish to start toexamine in this paper.
Zipf's Law is also closely relatedto the Good-Turing smoothing technique, and a betterlaw could lead to better smoothing (Samuelsson, 1996).Note that Samuelsson showed that Zipf's Law implies asmoothing function slightly different from Geod-Turing.1.3 Semantics and Information RetrievalZipf's Law provides a base-line model for expectedoccurence of target terms and the answers to certainquestions may provide considerable information aboutits role in the corpus (Steele,1998): What does it mean toask if a word is significant in a corpus, beyond mereoecurence or relative probability?
What is the range ofthe semantic influence of a word in a corpus?
What doesthe pattern ofoceurences contribute to our assessment ofits relevance in the corpus?1.4 Parser EvaluationZipf's Law provides abasis for evaluating parsers andtaggers (Entwisle and Powers, 1998).
Again wesummarize the potential role in the form of a series ofquestions: How does a language model developed on onecorpus transfer to another?
How do we translateperformance estimates on a few test corpora to estimatesfor the language as a whole?
How do differences inregister, genre and medium affect he utility of a system,and how do we compensate for these differences?1.5 Computational PsycholinguisticsZipf's Law provides a distributional foundation formodels of the language learner's exposure to segments,words and constructs, and permits evaluation of learningmodels (Brent, 1997).
It also provides a basis forevaluation of models of linguistic and cognitive accessand storage models (Segui, Mehler, Frauenfelder andMorton, 1982).
Whilst qualitative explanations andevaluations have been given on the basis of anassumption of the general relationship, a more preciseaccount .will lead to more quantitative models.Whilst the law's qualitative or coarsely quantitativeroles across these areas may seem rather fuzzy, and itstretches the imagination to see how a more precisecharacterization of the law could improve theperformance in these applications and models, we notethat the relationships, particularly from a Psycho-linguistic point of view, demonstrate that the law isrelevant to several aspects of our field, and thatexplanation and understanding of the law is anintrinsically valuable scientific objective.Powers 151 Applications and Explanations of Zipf s LawDavid M.W.
Powers (1998) Applications and Explanations ofZipfs Law.
In D.M.W.
Powers (ed.)
NeMLaP3/CoNLL98: NewMethods in Language Processing and Computational Natural Language Learning, ACL, pp 151-160.2.
Zipf's Principle of Least EffortZipf's major work on this subject explores atheory basedon a competitive process balancing the minimization ofthe effort of  both speaker and hearer.
He uses an analogyin which words are regarded as tools, which are soconstructed and arranged as to be able to achieve thecommunication task as efficiently as possible.
Note thatthis culmination of his research into this relationshipcoincided with the publication of Sharmon's proposals ininformation theory, and we will seek to make theconnections clear shortly.Zipf considered that the speaker had to build acontinuous stream of specified products, that is anongoing stream of utterances conveying specifiedmeanings, in such a way as to minimize his effort asspeaker consistent with effective communication to thehearer, her task being simplified as the relationshipbetween utterances and meanings approached one to one:the work involved in producing aconstruction consists ofthe work involved in fetching the tool, which is directlyin proportion to the cost of fetching the tool and includesboth the mass of the tool, m, and the distance, d, that itneeds to be fetched, given increasing either increases theeffort required.
Mass corresponds to length in Zipf'smodel, and distance to access time.
Fequency, f andwork, w, must also be directly related, so: w =f*  m * d,assuming direct proportionality to work in each case.Also the age of the tool (word) and number of differentuses (meanings) vary directly with frequency.2.1 Access MethodWe now consider what Zipf called the "close packing"of our tools.
Zipf in fact considered only one modelwhich fitted the empirical Pacts, but we will considermore in order to explore to what extent he law reallydoes correspond to optimality: What is the optimumaccess time for a set of N tools?
In computer science, theoptimum organization structures which we typicallythink of  our hash tables and trees, with o(1) and o(logN)access times respectively.
The former assumes thatencoding of arbitrary length words is done in the sameamount of  time, and thus implies both a limit on thelength of  words and suboptimality of this hash codingscheme since best ease and worst ease axe the same (inmachine architecture terms, the machine uses fixedlength words and is synchronous and cycle limited, andthis fixed length must be at least o(logN) in order topermit full addressability).
The tree access techniquemakes similar assumptions except that lengthindependence may be relaxed (in machine architectures,the access would be pipelined or serialized so that lengthof the word and depth of storage add without increasingthe order).The theoretical o(1) and o(logN) access times whichwe are familiar with in computer science are however notphysically sustainable (Powers,1995).
Thus encodingprocess for hashing also takes at least o(logN) time givena limitation on the degree (fanin/fanoot) of the logicelements or neurons (which turns it into a tree anyway).Worse still, as we seek to pack neurons into an n-dimensional space the speed of propogation limits ouraccess time to o(N TM) and our optimal tree is notpractically achievable (this can be hidden in the cycletime, which then defines an upper limit for N).Thus our class of optimal solutions is limited to the setof o(N TM) solutions where n is 3, 2 Or 1, whichcorrespond to volumetric, areal and linear constraintsrespectively.
Hence our access time or storage depth fora word of rank r, d r is related by w/m r =fr * dr =fr * rl/n"(Note that we simplify equations for the moment byleaving units and constants.
)The three dimensional solution clearly leads to themost efficient packing, with n = 3, but Zipf's law seemsto corresponds to linear packing, with n = 1.
Does thismean that optimality is not reached?We answer this question in two parts: we look atinformation theory as a measure of efficiency, and weconsider the physical constraints further.
But first welook at how we move from rank to a more naturalmeasure.
The rank of a word type represents he numberof word types of greater frequency - -  our conventionaldefinition of the most frequent word as having rank 1 isslightly defective when ties are taken into consideration,and calling this rank 0 would lead to a more consistentdefinition.
Thus sometimes a constant 1 needs to beadded or subtracted in our formulae to allow for this.
Therank associated with a particular frequency, ~ is thus thesum of the numbers of words, nf, of greater frequency,and rfmay thus be approximated by the integral of nf10.90.80.70.60.50.40.30.20.I0p(x) = l.O/sqr(x)pOnt(x)) *(int('x+ 1 )-x~p(int(x+ l ))*ix-int(x')) -p(int(x+l)) - -p0nt(x))p(x)\3 4 5 6 7 8 9Figure 1: Error approximating series by integralII1II11II11III IkPowers 152 Applications and Explanations of Zipf s LawIIIIIIIIIIIIIIIIIIIIIIIIIIIII2.2 Er ror  Est imatesIn fact, approximating a monotonic series by anintegral eads to an error which can be characterized asbeing slightly more than half of  the first term of the series(as illustrated in Figure 1), or alternatively as representingan error of slightly under a half in the index, l Zipf's lawfor rank is thus approximated ither by r = l / f  r+ 0.64/f~given that we use Zipfs law for number as nf= 1/fr 2 or byr = l/fr if we use  n/= 1/0rr+0.43) 2.
Figure 2 illustrates thegeneral inverse and inverse square laws, where we plotrank and number against frequency both for individualfrequencies (ragged plots near gradient -1 and -2 resp.
)and aggregated frequencies ( tep function and piecewiselinear curves near gradient -1).
The aggregation was bypowers of two (n' = 2 II?gn*l ) as suggested by the scale.Note that we see the integrating effect not only for rankbut for aggregated number.These approximations may be used to estimate stepsizeand expected error as indicated in Figure 3.
The centreline is Zipf's law for rank based on the highest frequency(the .word 'the' occurs 1642 times in "Alice inWonderland") and the outermost pair of curves are basedon on the highest and lowest ranks associated with thefrequency 1, namely 1486 and 2620, plus or minus themaximum error.
Note that the difference between theseamber & rank against individual & inflog grouped freqnumber for aggregated frequencies "aliee.nintln'p" --rank for aggregated frequencies "alice.rintlnp" --~.
nuraber for raw frequencies "aliee.nrawlnpi'--rank for raw frequencies "aliee.rrawlnp ' -.1000 X~.,,,~ .... 2620/x-' '~- ~.
2620/sqr(x)1001014 16 64 256 1024 freqFigure 2.
Effect of aggregation of numbers to ranksThis characterization f the error, closely related to a formulationdue to Euler (Stanaitis, 1967), is actually considerably more accu-rate than that used by Zipf, and may be verified graphically fromFigure I where the inscribed step function represents the sumwhose area is underestimated by the integral of the continuouscurve.
The circumscribed step function represents the sum dis-placed by l, corresponding to omission of the first term, f(1), Theerror is not only bounded above by the sum of the areas enclosedbetween the two stcpfunctions, which is equal to the value of thefirst term, but it can be see to be bounded even more closely belowby the chord function which excludes half this difference, c(1).Thus f(1)/2 < e(1) < ffl).
Since \]~n "2 converges to rc2/6 ~ 1.64, wehave that e(1) ~ 0.64.
Alternatively we  can use ~(n+0.43) "2 ~ I.rankI000100I0rank vs freq with number bounds.... "alice.rankp"..... 2620/0.64*( 1/x-0.36/~r(x)) .
.
.
.
.~".'..-..:~...
1486/I.64*(I/x+0.64/sqr(x)) ..... ~: ".-.
'.~..~ 1642/x .....IO Ioo looo freqFigure 3.
Actual and expected range of rank vs freqactual and predicted freq range vs rank: f(x)=1486:1642:2620/x12f req  ~ .
.
.
.
.
.
.
.  '
.
.
.
.
.
.
.
.  "
" .
.
.
.
.
.
.
.
, " ~ ?\[-.
"alice.rankp" - -g,-""-, f(x)+sqrt(sqr( f(x))+2*0.64* f(x)) ..........lO00 \ [ - '~- .
.
r(x)+sqrt(sqr(f(x))-2*0.64* f(x)) ....." ~  fl (x)+sqn(sqr( fl (x))+2"0.64 * fl(x)1 .....I ""--~~~(x~'sqrt~sqrt~tx))-2 *0.64" fotx)~ .....10 100 1000 rankFigure 4.
Actual and expected range of freq vs rankranks for frequency 1 (plus one) gives the number ofwords of frequency 1.
This number, hi, sets the maximaof the aggregated and unaggregated number curves(2 II?gnd and nlresp. )
that we saw in Figure 2.
The numbernfat any given frequencyfis represented asthe stepsize,and the number of words that would have been expectedto occur with this frequency isassumed to be of this order.The error bound functions may easily be inverted toallow the more conventional plotting of frequency (anderror estimates) as a function of rank, as in Figure 4.
Heref(x) represents half the estimated frequency based on thehighest frequency, and f0(x) and fl(x) represent thosebased directly on the upper and lower bounds onfrequency I.
In this ease we we use error e(1) = 0.64 butdo not allow for error in the frequency 1ranks themselves.Note that Zipf associates the "top-downwardconcavity' with'informal colloquial speech' (1949, p82),an association which had been recognized by otherresearchers a  early as 1936.
The effect 'is not found inmore formal material' and is attributed by Zipf to anexpansion of the dosed class vocabulary to include thepersonal pronouns (1949, p122).
Both the phenomenonand the role of closed class words are of interest to us here.Powers 153 Applications and Explanations of Zipfls Law2.3 In fo rmat ion  TheoryZipf's book on Human Behaviour and the Principle ofLeast Effort and Shannon's book on The MathematicalTheory of Communication were both published in 1949,and were developed totally independently, so it isinteresting to look at how their concepts of efficiencyinterrelate.
Interesting Crystal finds Zipf's explanationsunsatisfactory and appeals to "a more conventionalexplanation i  terms of probability theory" (1987, p87),by which he presumably means information theory, buthe cites no literature in support of this claim.Let us consider the probability distribution defined bydividing the frequency of each word by the length of ourcorpns, pr =fr/L (possibly taken as a limit as our corpusincreases indefinitely).
An assumption that the lexiconcan increase without bound is inconsistent with Zipf'sLaws prediction that Pr = C/r since summing over thedistribution gives a non-convergent series, violating theconstraint that the probabilities must sum to 1.
Someprefer to hold onto this assumption and to seek a fasterconverging probability distribution for which the seriesconverges to 1 (Brent, 1997).
Such series include 1/r 2,1/r.log2r, 1/r.log r.log21og r, ... all of  which converge,whilst the series 1/r, 1/r.logr, 1/r.log r.log log r, ... all failto converge.Interestingly, the terms of both sequences of seriesapproach those of the series Y~ 2 "L*(r) (where L*(x-1) isdefined as log c + log x + log log x + ...) which doesconverge and is optimal in the sense that any monotonicdecreasing distribution which satisfies our constraintmust equal or exceed L*(x)-2k*(x) infinitely often(Rissanen, 1989, p35), where k*(x) is the number ofpositive log terms in L*(x) excluding the constant.
Notethat the integrals from r (upto infinity) of the convergentseries are 1/r, I/log r, l/log log r, ... whilst for thedivergent series the integrals upto r are log r, log log r,...1 L(x) = x<=l ?
2.865 : (lg(x) + L(lg(x)))0.33/(x*sqr(lg(x))) - -2**(-L(x))0.1 0.64/sqr(x) - -1/x0.010.0010.0001le-05 - .
10 2"0 3'0 4'0 50 60 70 80 90 100Figure 5.
Comparison of convering series with 1/xNow the information corresponding to probability pr is1 r = log Pr For Pr = l/r, optimal encoding of theinformation should take log r bits, and must at leastspecify the rank r which requires log r bits too, but asequence of such codes could not be decoded.
Adding aboolean 'finished' flag after each bit doubles the length,corresponding to squaring the probability, and allowsdecoding and convergence m which follows as soon aseach code is a leaf in the decoding tree.
Another way ofdelimiting is to specify a length using a more primitivescheme, then allowing minimum length encoding of theactual rank.
In the extreme we specify lengths recursivelytill we flag we reach a length of 1, when we use ourboolean flag - -  this corresponds to the near optimalL*(r), however for the range of lexicon size we need, onelevel of  length encoding, l/r.log2r, is sufficient and inFigures 5 and 6 the corresponding curves are scarcelyseparable.
In Figure 5 we see that for the first 100 wordsthis applies to I/r 2 too.1 L(x) = x<=l ?
2.865 : (lg(x) + L0g(x)))2**(-L(x)) .---0.64/sqr(x) wle-1010 100 1000 le+O41e+O51e+061e+O71'e~-0"8 'Figure 6.
Comparison of convering series with 1/xAs discussed above, deviations from Zipf's Law areknown, and the logseale which Zipf used actually hidconsiderable deviations for high values of either ank orfrequency (and can amplify deviations for low values).We therefore now show the reeiprecal of frequencyagainst rank using a linear scale, and this in factcorresponds to a particular definition of the averageinterval between words.
We show how this looks inFigure 7 for three different definitions of the averageinterval: 'interval' corresponding to dividing the corpuslength by frequency (valid if imagine that the followingtext segments of this size have the same structure);'initival' corresponding totreating the start of the corpusas the first reference point (valid if the interval to the firstoccurence is a good predictor of the interval betweenoeeurences); and 'intraval' corresponding toconsideringonly the f -1  intervals between actual successiveoceurences.
Note that the 'interval' which correspondsPowers 154 Applications and Explanations of Zipfls LawII1IIIIIIIIIlIIIIlIIIIIII!1IIIIII1 Av.
Interval Intraval and Initival0.6'/...;j" x**L05 - -  0.2 ~.s in i t i26685 x ---~ ~ ~ i n t r a 2 6 6 8 5 "  - -~ c e .
s i n t e r 2 6 6 8 5 "00 0.2 0.4 0.6 0.8Figure 7.
Comparison of interval definitions and law.directly to a sealed reciprocal of the rank, is guaranteed tohave a step shape as many words have the samefrequency.
This diseretization is avoided by the otherdefinitions.
In Figure 7 we show these as a normalizedintervals (divided by the corpus size, 26685) againstnormalized rank (divided by the lexicon size, 2620)along with lines corresponding to Zipf's Law,Mandelbrot's Modification (proposing an exponent of1.05), the best fit power for our small test corpus (1.2)and the quadratic model of Brent (1997).There are many details of Zipf's theory which we areunable to go into in the confines of this paper, but at thispoint we need to note two things.
First, that Zipfelaimedthat claimed the reciprocal law applies only to anoptimum sample size corresponding toa single cycle forthe least frequent word, such that the maximumfrequency and the maximum rank were equal to theintercepts of the line of best fit of gradient -I (leastsquares in log scale).
Of course as the corpus grows, newwords of frequency 1enter, so his principle is to select hesize which gives gradient closest o -1 across a largenumber of samples of the corpus (which should beconsistent as to genre, register, age of speaker, etc.)
Formost published literature this should correspond toaround 10000 words (thus this is the size of the usualactive lex!con) while for children around the time ofstarting school it is around 2000 words.
Our corpus isaround 2.5 times his optimum for literature, and as we areusing Alice in Wonderland which is supposedly achildren's book about a child, and is of informalcharacter, perhaps he would probably have suggestedusing even smaller sample sizes.
However, we are notcontent o characterize samples of an optimum size, andwe are aiming to determine how the law should beadjusted to take into account sample size.The second eviation from Zipf's practice is implicit inthe mechanism for determining the optimum sample sizeas just explained: The correct line to draw is a line of bestfit, minimizing the least squares error in the prediction ofthe log of frequency from the log of rank.
This means areInterval: over entire corpus (vs rel.
ra_nki avrfl=2620)0.?"
(a) ~.
.~o.
~ ~ r 2 6 6 8 5 "o.
/~~b~i , ;~- i i i/ - / ~ a v r O _  ,L(k'2620)/L(avrfl}.~ ' -~gCx '2620) / Ig (a .v rn) )  ,1 Interval: over entire corpus (vs rel.
rank; avrfl=1486).
_o.8, (b) ~0.4/ "alice.sinter26685" / ~ - J  x*2620/avffl ~!0.2 ./" : sqr(x*2620/avrfl) -..---'.
x,*2620/avr~..*I~x*2620)/L(avrfl )0 /~" f -  x*2620/a-vrfl*sqrOg(x*2620)/lg(avrfl)) I1 In terval: over entire corpus (vs. rel.
rank; avrfl=2153)0.8 (e) .
~ 6 ~  m .60.40.2 ~0 / ~ " ~ v r f l  *sqr(lg(-x *2620)/15(~vrf'l ))0 0.2 0.4 0.6 0.8Figure g. Comparison of top step cut pointsline should pass through the middle of the steps inFigure 7 (as in Figure 2).
This may also be viewed inanother way.
The midpoint of a step may be viewed asshowing close to its correct frequency and rank, whilstwords which should not occur an integral number oftimes in a sample of the selected size will be rounded toan integral frequency.
They will necessarily occur moreor less than the expected number of times.
This problemdoes not apply to our alternate definitions of averageintervals.
Thus for 'intervals' we should fit the midpointof the intervals, and in particular the midpoint of thefrequency 1step, for 'initivals' we have the full range ofthe corpus available and should fit the high point of thefrequency 1step, and for 'intravals' we should fit the lowpoint since frequency 1 does not define an intraval andthe value is arbritrarily set to a limit of 1.Figures 8 and 9 show fits to these different points onthe top step, and allow comparison with the convergentinformation theoretic functions we have discussed.
InFigure 8c there is an evident log-squared bias.Powers 155 Applications and Explanations of Zipf s LawFrom Figure 9 it will be observed that our newdefinitions of average interval have a totally differentcharacteristic from the old.
Not only have we eliminatedthe 'steps', but the resulting functions are clearly far fromlinear, and from the slowly convergent log-based series.Clearly for words that occur locally rather than globallyin the corpus, these methods progressively add a biasrelating to the location of a cluster of oecurenees-- andbetter eflect he frequenct of clusters.
This is somethingelse Zipf has considered: for words of a particularaverage frequency (and thus interval), the number ofintervals of a particular size also varies inversely withthat interval size (1949, p42).
In Zipf's model this resultsfrom spreading the workload away from costly words.10.80.60.40.2o10.80.60.40.2o10.80.60.40.2OqInitival: vs reciprocal rank; avrfl =2620?
x*2620/avffl *sqr(ig(x*2620)/lg(avrfl )) " /-~x*2620/avrfl *L(x*2620)/L(avrfl ) / f '~sqr(x*2620/avrfl) / //fInt~val: vs reciprocal_ra~; avrf1--!486"alice.sintra2.6685"x*2620/avrfl03 ) / / ~r sqKx*2620/avrfl) ~1/ / /.~.
: ...*L(x*2620)/L(avrfl)/ /  /~.-...*sqr(lg(x*2620)/Ig(avrfl)) ?.
---~ .
.
.
.
.
.
(x*2620/avrfl)**6"(\]642*(l-x*2620/26685))**(x*2620/av~-l!
- -Initival: vs reciprocal ranks, avrfl=2620/0.6- -  "aliee.siniti26685" /.... x*2620/avrfl , ,~sqr(x*2620/avrfl) r//x*2620/avrfl *L(x*2620)qL(avrfl) / /x*2620/avrfl *sqr(Ig(x*2620)/Ig(avrfl )) / / /'.
... (x-0.6)/0.40.2 0.4 0.6 0.8Figure 9.
Comparison of interval definitionsWhilst it is possible to fit higher order polynomials andexponentials to to these curves, the fits are not good.
InFigure 9c we have fitted a 6th order polynomial which isindistinguishable in the range from an exponentialformulated in terms of the probability that a word doesnot occur.
We do not see this as something where anexplanation as a single distribution is appropriate, andlater we view these as a joint distribution for open andclosed class words, and in Figure 9c we show that thebends in the curves look more like transitions betweentwo distinct distributions obeying different parameter-izations of some form of Zipf's law.2.4 Psychological Pred ic t ionsIf we believe Zipf's law in its standard form, and scalefrequencies to probabilities for a finite lexicon, theninformation theory suggest hat the length of wordsshould look like the log of frequency, and the access timefor words should follow the log of word frequency.Assuming that the lexicon is unbounded, theninformation theory suggest hat the length of wordsshould be L(x) or, less optimally, log x + 2 log log x.Zipf went further and predicted that the older wordswould be the more frequent, both in an etymological nda psycholinguistic sense, and performed experiments odemonstrate he law in relation to the etymology ofEnglish, as well as performing some analyses ofchildren's peech which were also consistent with hismodel.
However, his experiments on length did notquantiatively demonstrate what relationship wasachieved, and he was expecting a negative powerrelationship again.
Moreover, he did not perform anyexperiments tocheck the validity that access time wouldreflect an inverse relationship, and expected that length(mr) and access time (dr) would be proportional to r ?
'5.Studies of latencies in various linguistics task have,however, been extensively studied by psychologists, andalthough the interpretation f the results is controversial,and the results are more qualitative than quantitative,considerable vidence xists to support a logarithmicaccess time, and have been the basis for one of the mostinfluential models of word recognition, the Logogenmodel (Morton, 1969).
There is also Event-RelatedPotential evidence from EEG studies, but these resultsare even less precise and we ignore them here (althoughwe have undertaken some ERP experiments ourselvesand hope to further elucidate certain factors in this way).Looking more closely at the experimental data, wefind, just as with the frequency data, that there are strongcontextual effects (Becket, 1979) which tend to beadditive, particularly for lew-frequeney words.
Anadditional confusion factor is that subjective measures offamiliarity which actually can better predict access timeI///I/I//kPowers 156 Applications and Explanations of Zipf s Law kIIIIIIIIIIIIilIIIIilIIIIIIthan more objective frequency measures (Gemsbacher,1984).
As Zipf also knew, the number of distinct alsoplays a role, and Zipf himself ound that the number ofmeanings decreased with the square root of frequency(1949, p75).
Other reported confusion factors includeconcreteness, level of education, age, age of acquisitionand word length.
Also there is a correlation betweenword frequency and the signal to noise ratio which maybe tolerated by a word, as well as the fixation time inreading aword.
The mode of presentation a d the methodof testing may also influence the relationships found, ascan even such factors as stress pattem and syntactic role.Thus the role of frequency as a primary determiner ofaccess time is highly controversial although therelationship itself is well accepted (Balota & Chumbley,1984 & 1990; Monsell, Doyle & Haggard, 1989).As we can see from Figure 10, the length data doessupport a logarithmic relationship, notwithstanding that alog-squared bias was observed in Figure 8, However thesquare root of the logarithm is best for this data.Nevertheless the information theoretic optimum isapproximated for this corpus and we would furtherpredict a similar function for the access time for words.Although the age of acquisition and length both showstronger correlation with latency than frequency innaming tasks, and this has been cited as evidence againstword frequency having a significant effect on access time(Morrison, Ellis and Quinlan, 1992), this observationsupports the predictions we made on the basis of Zipf'sLaw and Information Theory.
Even if the age ofacquisition and phoneme length fully determine accesstime, the fact of correlation between frequency andlatency is not disturbed, and we can hypothesize that high?
frequency leads to early and frequent exposure to, andthus learning of, a word; and furthermore, that the earlylearning, in combination with constant refreshment;maintains the word at a relatively greater level ofaccessibility than less frequent words.
Similarly, wepredicted a strong correlation between length and latency_ .
Wo~l~th , ,m~ F~,~c~,  .7654 * "alice.lenp" ?- x**-O.S*50 : "-: ~ \3,  L(2620)-L(x) .
.
.
.
.
.
.
.
.
x~N~ ".... lg(2620)-Ig(x) " ~ ~ .x~.Xx2 sqrt(Ig(2620)-lg(x))*2 .
.
.
.
.
.
.
.
.
.1 - Ig(2620)-Ig(x)+2*(lg(lg(2620))-Ig(lg(x))) .
.
.
.
.
.~10 100 I000Figure 10.
Wordlength versus Frequencylg(2620/x)/(2620/x),i60'i .......... 2620/sqr(x)100 _4 16 64 256 1024Figure 11.
Approximation tonumbers given frequencyon the basis that the logarithmic relationship is requiredfor optimality in each case.
The results do however seemto contradict Zipf's prediction that length, m, and accesstime, d, would be proportional to f  "0-5, although it mustbe emphasized that the correlations are far from perfectand the precise trends cannot be distinguished to anygreat degree of accuracy.
Our own data in Firure 10suggests that he relation is actually log?
'Sf, and the curvefor f  "0"5 moves right away from the data at both extremes.However we have no accurate information for accesstime and we will simply note that both length and accesstime have a generally logarithmic relationship withfrequency, and that L(1/f) is also a better fit thanf -0-5.Nonetheless, this has implications for the principle ofleast effort in that the Zipf's relationship for work,,w =f*  m * d =f  / log2f = l/r.log2r, is not constant, andindeed Information Theory says we should actually doless work for more frequent items.
Moreover, thisrelationship for work now obeys a law consistent withnear optimality in an unbounded lexicon model.
Wecould moreover eplace Zipf's law by f= I/r.log2r,consistent with the improved empirical fit of Figure 8c.This gives us a new relation for the number of wordsaround eaeh frequency.
For Zipf's Law, f=  c/r gave us n= c/f 2 = r/f.
For our new version,f= c/r.log2r gives us n= log r/bf (where our new constant, b, depends on thebase of our log and is given by b = 2 log e).
Thus forZipf's Law, the number expected for each frequency isthe corresponding fraction of the number of words withhigher frequency, but our new numbers grow moreslowly, the frequency specifying a fraction of the log ofthe number of words with higher frequency.
Substituingan overestimate for the rank using Zipf's Law, inFigure 11 we approximate he number at frequeneyfbyn = log(c/f)/bf (which is a bit steeper than the correctinverse would be) and is at least as good a fit as c/f 2 andindeed better eflects the distribution of the sparse rankswhere the expected number of words for a frequency isless than one.
The corresponding optimal lengthfunction, again with rank overestimated using Zipf's law(and hence also too steep), is shown in Figure 10.Powers 157 Applications and Explanations of Zipf s LawFrom the perspective of our own research in NaturalLanguage Learning, the most significant results fromthese psychological explorations ofaccess time are thosewhich suggest hat open class words show a moresignificant effect than closed class words, and havedistinct roles and mechanisms in the early stages ofmorphological processing (Segui, Mehler, Frauenfelderand Morton, 1982; Matthei and Kean, 1989).3.
Corpus Characteristics and Sample SizeOne of Zipf's claims was that a given text had particularcharacteristics which included a characteristic optimumsample size and lexicon size.
He chose the optimumsample size to be the one with the best fit to Zipf's Law,and this implicitly specified a lexicon.
However, as thesample is increased above that point, new word typescontinue to enter the lexicon.
He furthermore notes thatinformal colloquial speech gets a hump in the first 150words, one which has generally been associated withincreased usage of the personal pronouns.
We furthernoted that there is evidence (Matthei & Kean, 1989;Segui et al, 1982) that open and closed class words aretreated ifferently, and it could also be assumed that hereis a primary subject, and hence lexicon, for any specificwork, as well as secondary or incidental topics.
As anaverage of independent topics, we might expect the lawto re-emerge, but the closed class (including generics)cannot increase in size as the lexicon does.One of our motivations for undertaking this study wasthe observation during our language learningexperiments hat as corpus size increased, Zipf's Lawtended to be increasingly invalidated, with the curvatureincreasing consistent with a move to a continually higherexponents.
While it would be predicted for the highestranks of each step to exhibit a quadratic component, dueto the size of the step reflecting the number of words withthat frequency, the tendency affected the lowest andmedian ranks as well.To investigate this analytically, we assume that wehave two samples each of which has a lexicon of size Nwhich includes a common vocabulary of pN, and thateach sample obeys Zipf's Law.
We further assume thatthe common vocabulary includes the closed class words,and more generally the most frequent words in thelanguage and relating to the topic of the corpus.
IfthepNwords were the most frequent words of the individualsamples, and were exhibiting their characteristicfrequencies, they would exhibit exactly double thefrequency for the same rank, thus retaining the sameslope in a log-log plot.
The converse is true for theremainder of the lexicon: the words will all the samefrequency as in the smaller samples, but their ranks willhave increased in proportion to their distance above theFrequency vs Rank - common-high hypothesis 1024 \ w i512 ~ ~ ,  ",2561286400 64 128 256 512 1024Figure 12.
Model with high freq.
common vocabularycommon set.
These word types will thus exhibit areduced slope, and there will be a jump and a suddendiscontinuity in slope between the common and thedistinct words as in Figure 12 In general, whereever new(or displaced) vocabulary enters the picture, there will bereduced slope-- rank will increase without much changein frequency.
Similarly, where disused vocabulary isdisplaced, rank will decrease without much change infrequency, giving rise to increased slope.
In fact there isevidence of a jump and discontinuity which occur at arank logarithmic in the size of the lexicon.In another model, we can imagine replacing a pair ofequally likely synonyms by one member of the pair, andnote that his will double it's frequency and halve its rank.This is consistent with Zipf's Law, although in betweenthe old and new positions, ranks will increase without anincrease in frequency, and atter the old position, rankswill decrease without a decrease in frequency.
This willproduce the kind of bulge Zipf identified with informaltext.
Shifting words can thus cause discontinuities too.At this stage, it may be worth saying a few words aboutthe corpus used throughout this paper, Alice's Adventuresin Wonderland (Carroll,1865) is an edited collection ofchildren's stories, originally delivered verbally, andculminating at a Picnic in 1862 when Alice going downthe rabbit hole provided the framework and cast the spellwhich eventually led to publication.
As a series ofadventures, there are some characters in common, someof whom recur, but entire vocabularies are limited to asingle chapter.
We used the Millennium Fulcrum Edition2.9, available through the Gutenberg Project, which issignificant since this attribution occurs at the beginningof the book and affects our analysis.Powers 158 Applications and Explanations of Zipf s LawIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIHaIIIIIIIIIIHaII1000100I0I, .
.
, .
.
":'.\[i~'~: r;m~: iO': .?
...."alice.rank20""alice.rank40" --?---~- - ' - - ' - - ,~  "alice.rankS0" - ........... .
...... ~ ~'~ " "?e.rankNO" ~~ ...... "~"- x...-.
- "~,~.
~ atlce.ranKl.
:~tl .........~ , .
, ~ , ~ , , \ ,  ~ "1~,._ "alice.rank10240" -----~ ~_~' \~7~;~ "~.L  ~ "alice.ran k20480" --x--"~.
-~-~: I~.
.?
.
, i i i .
.
.
.
.
.
i i i i i i i i i I i I i .
.
.
.
.
i ,  i1 10 100 1000Figure 13.
Frequency against Rank as corpus doubles in size ~ Alice's Adventures in WonderlandHaving made the above predictions about he shape ofthe curves under both extreme conditions andincremental change, we then proceeded to an analysis ofAlice produced by taking successive prefixes of the book.These are shown in Figure 13, where each prefix is twicethe size of its predecessor.
Note the discontinuities thatstart in our first doubling, where closed class words startto sift above above the non-recurring words of the titlepage, some of which only occur once in the wholevolume.
This shift starts in the first order of ranks and isvisible well into the second order of ranks.
The dipswhich appear and disappear around rank 10 in the largersegments are due to the competition between the words'said', ' in' and 'i ', and the name 'Alice' as the storyalternately focuses on her involvement and scenesinvolving other characters, and changes its balancebetween arrative, soliloquy, and reported speech.
Thebeginning of the second order ranks marks the transitionbetween closed class words and focal words.
The first 20words are: the, and, to, a, she, it, of  said, L Alice, in, you,was, that, as, her, at, on, all, with, and the remainder ofthe first 100 are all dosed class words (plus narrativedevices like think and looked) orcharacters--  with thesingle exception of the word head (which is closelylinked to the one character who wasn't in danger oflosing hers).Thus in this range we see a number of discontinuitiesas words move into and out of the focal range, and twodifferent slopes corresonding tothe closed class and focalwords, and the open class words.We have carried out a similar and more extensive studyon the Bible in four languages.
It also has the character ofsequences of stories focusing on different people andevents, and is also largely and edited version of verbalaccounts.
Using multiple versions retains the thematicbiases, so that if the artifacts we are observing areprimarily thematic, we should see similar artificacts insimilar places.
We should separately be able to see theeffects of language and translation style.
Although sometranslations are targeted at a more popular level and useless teelmical vocabularly, we have selected fourtraditional translations three of which use reasonablycontemporary language (the versions are KJV, RSV,Louis Segond, Elberfelder).Whereas Alice allowed us to double only 11 times, theBible allows us to double 16 times.
We have however forconsistency and convenience kept with the smaller Alicecorpus for the graphs shown here (the equivalent ofFigure 13 for each version of the bible is about 1Meg ofPostScript).
In Figure 14 we show the results for Frenchfor the last seven samples, this being the one where thediscontinuities were least pronounce.
In each case therewere one or two deep drops around rank 10, followed bya steepening of slope.4.
ConclusionsAt this stage, only tentative conclusions can be madefrom this preliminary studies, although furtherinvestigations are being undertaken using larger corporain multiple languages.
Zipf's theory requires effort o beconstant independent of frequency, however InformationTheory and Psychological experiments both indicate thatthis ought not to be the case, and that it in fact decreasesin a way consistent with an optimal strategy for anunbounded lexicon.
We have not been able to establishthe validity of an optimum sample size for a particularcorpus, genre or lexicon, but observe that new words tendto enter faster than they repeat, as evidenced by the factthat he number of words of frequency 1tends to increasePowers 159 Applications and Explanations of Zipfls LawI0000010000100010010?
~ '~ 's , .
.~  .... .
.
,, I , * * I , i * !
* I i I i i * II 10 100 1000 10000Figure 14.
Frequency against Rank as corpus doubles in s~.e--French Bible (Louis Segond)as the size of sample increase.
Given that language isproductive, and an unbounded lexicon model has beenindicated (or at least possible) in each of our experiments,this trend may well continue indefinitely, although itdoesseem to slow as the sample is increased (even though weincrease by doubling).5.
AcknowledgementsAll plots were made with GnuPlot and reformatted inFrameMaker.
Aspects of this work were undertakenwhile a guest researcher atENSSAT in Lannion, Franceand at the University of Antwerp, Belgium, with supportfrom IRISA and CLIF respectively.6.
ReferencesBalota D.A.
and Chumbley, J.I.
(1984) Are LexicalDecisions aGood Measure of Lexical Access?
The Roleof Word Frequency in the Neglected Decision Stage,?Exp.
Psych: Hum.Perc.
& Pe~ 10#3:340-357Balota D.A.
and Chumbley, J.I.
(1990) Where are theEffects of Frequency in Visual Word Recognition Tasks?d.
Exp.
Psych: General 119#2:231-237Beeker, C.A.
(1979) Semantic Context and Word FrequencyEffects in Visual Word Recognition, J. Exp.
Psychology:Human Perc.
& Pe~ 5#2:252-259Brent, M.R.
(1997).
Toward a Unified Model of LexicalAcquisition and Lexical Access.
Journal ofPsycholinguistic Research 26:363-375.Carroll, L. (1865).
Alice's Adventures in Wonderland.
TheMillennium Fulcrum Edition 2.9, Gutenberg Project.Crystal, D. (1987).
The Cambridge Encycolpedia ofLanguage, CUPEntwisle, J. and Powers, D.M.W.
(1998).
The Present Useof Statistics in the Evaluation of NLP Parsers.
ubmittedi"bl) ?
',rzmk 1024(F' : , .
-"bfre.rank20480""bfre.rank40960""bfre.mnk81920"'!,ih:.
:'m~i 163:~(c .
:...."bfre.rank655360"100000Finch, S. (1993) Finding Structure in Language.
Ph.DDissertation, U. EdinburghGernsbacher, M.A.
(1984) Resolving 20 years ofInconsistent Interactions between Lexieal Familiarity andOrthography, Concreteness, and Polysemy.
J. Exp.Psych: General 113#2:256-281K6hler, R. and Rieger, B.B., eds (t991) Contributions toQuantitative Linguistics, KluwerMatthei, E.H. and Kean, M-L. (1989) Postaecess Processesin the Open vs Closed Class Distinction.
Brain andLanguage 36:163-180Monsell, S. Doyle, M.C., and Haggard, P.N.
(1989) Effectsof Frequency on Visual Word Recognition Tasks: Whereare they?,/.
Exp.
Psych: General 118#1:43-71Morton, J.
(1969) Interaction of information in wordrecognition.
Psychological Review 76:165-178Powers, D.M.W.
(1995).
Parallel Unification: PracticalComplexity, Australasian Computer ArchitectureWorkshop, AdelaidePowers, D.M.W.
(1996).
Learning and Application ofDifferential Grammars.
CoNLL97: ACL/SigNLLWorkshop on Computational Natural LanguageLearning, MadridRissanen, J.
(1989).
Stochastic Complexity in Statisticallnquiry.
Singapore:World ScientificSamuelsson, C. (1996).
Relating Turing's Formula andZipf's Law, WVLC'96Segni, J., Mehler, J., Frauenfelder, U. and Morton, J.
(1982).The word frequency effect and lexical access,Neuropsychologia 20:6 615-627Shannon, C.E.
and Weaver, W. (1949) The MathematicalTheory of Communication.
Urbana: U. Illinois PressStanaitis, O.E.
(1967).
An introduction tosequences, series,and improper integrals.
San Franc:Holden-DaySteele, R. and Powers, D.M.W.
(1998) Evolution andEvaluation of Document Retrieval Queries.
submittedZipf, G.K. (1949) Human Behaviour and the Principle ofLeast Effort: An Introduction toHuman Ecology.
AWPowers 160 Applications and Explanations of Zipf s LawIIIIIIIIIIIIIIIIIIIIIIIIII
