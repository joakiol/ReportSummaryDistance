Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?609,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing CCG categories to improve Hindi dependency parsingBharat Ram Ambati Tejaswini DeoskarInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburghbharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.ukMark SteedmanAbstractWe show that informative lexical cate-gories from a strongly lexicalised for-malism such as Combinatory CategorialGrammar (CCG) can improve dependencyparsing of Hindi, a free word order lan-guage.
We first describe a novel way toobtain a CCG lexicon and treebank froman existing dependency treebank, using aCCG parser.
We use the output of a su-pertagger trained on the CCGbank as afeature for a state-of-the-art Hindi depen-dency parser (Malt).
Our results show thatusing CCG categories improves the accu-racy of Malt on long distance dependen-cies, for which it is known to have weakrates of recovery.1 IntroductionAs compared to English, many Indian languagesincluding Hindi have a freer word order and arealso morphologically richer.
These characteristicspose challenges to statistical parsers.
Today, thebest dependency parsing accuracies for Hindi areobtained by the shift-reduce parser of Nivre etal.
(2007) (Malt).
It has been observed that Maltis relatively accurate at recovering short distancedependencies, like arguments of a verb, but is lessaccurate at recovering long distance dependencieslike co-ordination, root of the sentence, etc(Mcdonald and Nivre, 2007; Ambati et al, 2010).In this work, we show that using CCG lexicalcategories (Steedman, 2000), which contain sub-categorization information and capture long dis-tance dependencies elegantly, can help Malt withthose dependencies.
Section 2 first shows how weextract a CCG lexicon from an existing Hindi de-pendency treebank (Bhatt et al, 2009) and thenuse it to create a Hindi CCGbank.
In section 3, wedevelop a supertagger using the CCGbank and ex-plore different ways of providing CCG categoriesfrom the supertagger as features to Malt.
Our re-sults show that using CCG categories can helpMalt by improving the recovery of long distancerelations.2 A CCG Treebank from a DependencyTreebankThere have been some efforts at automatically ex-tracting treebanks of CCG derivations from phrasestructure treebanks (Hockenmaier and Steedman,2007; Hockenmaier, 2006; Tse and Curran, 2010),and CCG lexicons from dependency treebanks(C?ak?c?, 2005).
Bos et al (2009) created aCCGbank from an Italian dependency treebank byconverting dependency trees into phrase structuretrees and then applying an algorithm similarto Hockenmaier and Steedman (2007).
In thiswork, following C?ak?c?
(2005), we first extract aHindi CCG lexicon from a dependency treebank.We then use a CKY parser based on the CCGformalism to automatically obtain a treebankof CCG derivations from this lexicon, a novelmethodology that may be applicable to obtainingCCG treebanks in other languages as well.2.1 Hindi Dependency TreebankIn this paper, we work with a subset of the HindiDependency Treebank (HDT ver-0.5) releasedas part of Coling 2012 Shared Task on parsing(Bharati et al, 2012).
HDT is a multi-layereddependency treebank (Bhatt et al, 2009) an-notated with morpho-syntactic (morphological,part-of-speech and chunk information) andsyntactico-semantic (dependency) information(Bharati et al, 2006; Bharati et al, 2009).Dependency labels are fine-grained, and mark de-pendencies that are syntactico-semantic in nature,such as agent (usually corresponding to subject),patient (object), and time and place expressions.There are special labels to mark long distancerelations like relative clauses, co-ordination etc604(Bharati et al, 1995; Bharati et al, 2009).The treebank contains 12,041 training, 1,233development and 1,828 testing sentences with anaverage of 22 words per sentence.
We used theCoNLL format1 for our purposes, which containsword, lemma, pos-tag, and coarse pos-tag in theWORD, LEMMA, POS, and CPOS fields respectivelyand morphological features and chunk informationin the FEATS column.2.2 AlgorithmWe first made a list of argument and adjunctdependency labels in the treebank.
For e.g.,dependencies with the label k1 and k2 (corre-sponding to subject and object respectively) areconsidered to be arguments, while labels likek7p and k7t (corresponding to place and timeexpressions) are considered to be adjuncts.
Forreadability reasons, we will henceforth refer todependency labels with their English equivalents(e.g., SUBJ, OBJ, PURPOSE, CASE for k1, k2,rt, lwg psp respectively).Starting from the root of the dependency tree,we traverse each node.
The category of a node de-pends on both its parent and children.
If the nodeis an argument of its parent, we assign the chunktag of the node (e.g., NP, PP) as its CCG category.Otherwise, we assign it a category of X|X, whereX is the parent?s result category and | is direction-ality (\ or /), which depends on the position ofthe node w.r.t.
its parent.
The result category ofa node is the category obtained once its argumentsare resolved.
For example, S, is the result categoryfor (S\NP)\NP.
Once we get the partial categoryof a node based on the node?s parent information,we traverse through the children of the node.
Ifa child is an argument, we add that child?s chunktag, with appropriate directionality, to the node?scategory.
The algorithm is sketched in Figure 1and an example of a CCG derivation for a simplesentence (marked with chunk tags; NP and VGFare the chunk tags for noun and finite verb chunksrespectively.)
is shown in Figure 2.
Details ofsome special cases are described in the followingsubsections.We created two types of lexicon.
In Type 1,we keep morphological information in noun cate-gories and in Type 2, we don?t.
For example, con-sider a noun chunk ?raam ne (Ram ERG)?.
In Type1, CCG categories for ?raam?
and ?ne?
are NP and1http://nextens.uvt.nl/depparse-wiki/DataFormatModifyTree(DependencyTree tree);for (each node in tree):handlePostPositionMarkers(node);handleCoordination(node);handleRelativeClauses(node);if (node is an argument of parent):cat = node.chunkTag;else:prescat = parent.resultCategory;cat = prescat + getDir(node, parent) + prescat;for(each child of node):if (child is an argument of node):cat = cat + getDir(child, node) + child.chunkTag;Figure 1: Algorithm for extracting CCG lexiconfrom a dependency tree.ROOT mohan ne raam ke lie kitaab khariidiiMohan Erg Ram for book buy-past-femROOTOBJPURPOSECASESUBJCASE[NP mohan ne] [NP raam ke lie] [NP kitaab] [VGF khariidii]NP NP\NP NP (VGF/VGF)\NP NP (VGF\NP)\NP< < <NP VGF/VGF VGF\NP< B?VGF\NP<VGF?Mohan bought a book for Ram.
?Figure 2: An example dependency tree with itsCCG derivation (Erg = Ergative case).NP[ne]\NP respectively.
In Type 2, respectiveCCG categories for ?raam?
and ?ne?
are NP andNP\NP.
Morphological information such as case(e.g., Ergative case - ?ne?)
in noun categories is ex-pected to help with determining their dependencylabels, but makes the lexicon more sparse.2.3 Morphological MarkersIn Hindi, morphological information is encoded inthe form of post-positional markers on nouns, andtense, aspect and modality markers on verbs.
Apost-positional marker following a noun plays therole of a case-marker (e.g., ?raam ne (Ram ERG)?,here ?ne?
is the ergative case marker) and can alsohave a role similar to English prepositions (e.g.,?mej par (table on)?).
Post-positional markers onnouns can be simple one word expressions like?ne?
or ?par?
or can be multiple words as in ?raamke lie (Ram for)?.
Complex post position markersas a whole give information about how the headnoun or verb behaves.
We merged complex postposition markers into single words like ?ke lie?
so605that the entire marker gets a single CCG category.For an adjunct like ?raam ke lie (for Ram)?in Figure 2, ?raam?
can have a CCG categoryVGF/VGF as it is the head of the chunk and?ke lie?
a category of (VGF/VGF)\(VGF/VGF).Alternatively, if we pass the adjunct informationto the post-position marker (?ke lie?
), and use thechunk tag ?NP?
as the category for the head word(?raam?
), then categories of ?raam?
and ?ke lie?
areNP and (VGF/VGF)\NP respectively.
Thoughboth these analysis give the same semantics, wechose the latter as it leads to a less sparse lexi-con.
Also, adjuncts that modify adjacent adjunctsare assigned identical categories X/X making useof CCG?s composition rule and following C?ak?c?
(2005).2.4 Co-ordinationThe CCG category of a conjunction is (X\X)/X,where a conjunction looks for a child to its rightand then a child to its left.
To handle conjunc-tion with multiple children, we modified the de-pendency tree, as follows.For the example given below, in the original de-pendency tree, the conjunction ora ?and?
has threechildren ?Ram?
, ?Shyam?
and ?Sita?.
We modifiedthe original dependency tree and treat the comma?,?
as a conjunction.
As a result, ?,?
will have ?Ram?and ?Shyam?
as children and ?and?
will have ?,?
and?Sita?
as children.
It is straightforward to convertthis tree into the original dependency tree for thepurpose of evaluation/comparison with other de-pendency parsers.ROOT raam , shyam ora siitaa skoola gayeROOTDESTSUBJCOORDCOORDCOORDCOORDraam , shyam ora siitaa skoola gayeRam , Shyam and Sita school wentNP (NP\NP)/NP NP (NP\NP)/NP NP NP (VGF\NP)\NP> > <NP\NP NP\NP VGF\NP<NP <NP <VGF?Ram , Shyam and Sita went to school.
?2.5 Relative ClausesIn English, relative clauses have the category typeNP\NP, where they combine with a noun phraseon the left to give a resulting noun phrase.
Hindi,due to its freer word order, has relative clauses ofthe type NP\NP or NP/NP based on the position ofthe relative clause with respect to the head noun.Similar to English, the relative pronoun has a CCGcategory of (NP|NP)|X where directionality de-pends on the position of the relative pronoun in theclause and the category X depends on the gram-matical role of the relative pronoun.
In the follow-ing example, X is VGF\NPROOT vaha ladakaa jo baithaa hai raam haiROOTOBJSUBJDEM RELCSUBJ AUXvaha ladakaa jo baithaa hai raam haithat boy who sitting be-1P-pres Ram be-1P-presNP/NP NP (NP\NP)/X VGF\NP VGF\VGF NP (VGF\NP)\NP> > B?
<NP VGF\NP VGF\NP>NP\NP>NP <VGF?The boy who is sitting is Ram?2.6 CCG Lexicon to Treebank conversionWe use a CCG parser to convert the CCG lexiconto a CCG treebank as conversion to CCG treesdirectly from dependency trees is not straight-forward.
Using the above algorithm, we get oneCCG category for every word in a sentence.
Wethen run a non-statistical CKY chart parser basedon the CCG formalism2, which gives CCG deriva-tions based on the lexical categories.
This givesmultiple derivations for some sentences.
We rankthese derivations using two criteria.
The first cri-terion is correct recovery of the gold dependencytree.
Derivations which lead to gold dependenciesare given higher weight.
In the second criterion,we prefer derivations which yield intra-chunk de-pendencies (e.g., verb and auxiliary) prior to inter-chunk (e.g., verb and its arguments).
For exam-ple, morphological markers (which lead to intra-chunk dependencies) play a crucial role in identi-fying correct dependencies .
Resolving these de-pendencies first helps parsers in better identifica-tion of inter-chunk dependencies such as argumentstructure of the verb (Ambati, 2011).
We thus ex-tract the best derivation for each sentence and cre-ate a CCGbank for Hindi.
Coverage, i.e., numberof sentences for which we got at least one com-plete derivation, using this lexicon is 96%.
Theremaining 4% are either cases of wrong annota-tions in the original treebank, or rare constructionswhich are currently not handled by our conversionalgorithm.2http://openccg.sourceforge.net/6063 ExperimentsIn this section, we first describe the method of de-veloping a supertagger using the CCGbank.
Wethen describe different ways of providing CCGcategories from the supertagger as features to astate-of-the-art Hindi Dependency parser (Malt).We did all our experiments using both gold fea-tures (pos, chunk and morphological information)provided in the treebank and automatic featuresextracted using a Hindi shallow parser3.
We re-port results with automatic features but we alsoobtained similar improvements with gold features.3.1 Category SetFor supertagging, we first obtained a category setfrom the CCGbank training data.
There are 2,177and 718 category types in Type 1 (with morph.
in-formation) and Type 2 (without morph.
informa-tion) data respectively.
Clark and Curran (2004)showed that using a frequency cutoff can signif-icantly reduce the size of the category set withonly a small loss in coverage.
We explored dif-ferent cut-off values and finally used a cutoff of10 for building the tagger.
This reduced the cat-egory types to 376 and 202 for Type 1 and Type2 respectively.
The percent of category tokens indevelopment data that don?t appear in the categoryset entrailed by this cut-off are 1.39 & 0.47 forType 1 and Type 2 respectively.3.2 SupertaggerFollowing Clark and Curran (2004), we useda Maximum Entropy approach to build our su-pertagger.
We explored different features in thecontext of a 5-word window surrounding the tar-get word.
We used features based on WORD (w),LEMMA (l), POS (p), CPOS (c) and the FEATS (f )columns of the CoNLL format.
Table 1 shows theimpact of different features on supertagger perfor-mance.
Experiments 1, 2, 3 have current word (wi)features while Experiments 4, 5, 6 show the im-pact of contextual and complex bi-gram features.Accuracy of the supertagger after Experiment 6is 82.92% and 84.40% for Type 1 and Type 2 datarespectively.
As the number of category types inType 1 data (376) are much higher than in Type 2(202), it is not surprising that the performance ofthe supertagger is better for Type 2 as compared toType 1.3http://ltrc.iiit.ac.in/analyzer/hindi/Experiments: Features AccuracyType 1 Type 2Exp 1: wi, pi 75.14 78.47Exp 2: Exp 1 + li, ci 77.58 80.17Exp 3: Exp 2 + fi 80.43 81.88Exp 4: Exp 3 +wi?1,wi?2, pi?1,pi?2, 82.72 84.15wi+1, wi+2, pi+1, pi+2Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29Exp 6: Exp 5 + wi?2wi?1, wi?1wi, 82.92 84.40wiwi+1, wi+1wi+2, pi?2pi?1,pi?1pi, pipi+1, pi+1pi+2Table 1: Impact of different features on the su-pertagger performance for development data.3.3 Dependency ParsingThere has been a significant amount of work onHindi dependency parsing in the recent past (Hu-sain, 2009; Husain et al, 2010; Bharati et al,2012).
Out of all these efforts, state-of-the-art ac-curacy is achieved using the Malt parser.
We firstrun Malt with previous best settings (Bharati etal., 2012) which use the arc-standard parsing al-gorithm with a liblinear learner, and treat this asour baseline.
We compare and analyze results af-ter adding supertags as features with this baseline.3.4 Using Supertags as Features to MaltC?ak?c?
(2009) showed that using gold CCG cate-gories extracted from dependency trees as featuresto MST parser (McDonald et al, 2006) boostedthe performance for Turkish.
But using automaticcategories from a supertagger radically decreasedperformance in their case as supertagger accuracywas very low.
We have explored different waysof incorporating both gold CCG categories andsupertagger-provided CCG categories into depen-dency parsing.
Following C?ak?c?
(2009), insteadof using supertags for all words, we used supertagswhich occurred at least K times in the trainingdata, and backed off to coarse POS-tags otherwise.We experimented with different values of K andfound that K=15 gave the best results.We first provided gold CCG categories as fea-tures to the Malt parser and then provided the out-put of the supertagger described in section 3.2.
Wedid all these experiments with both Type 1 andType 2 data.
Unlabelled Attachment Scores (UAS)and Labelled Attachment Scores (LAS) for Malt607are shown in Table 2.
As expected, gold CCGcategories boosted UAS and LAS by around 6%and 7% respectively, for both Type 1 and Type 2data.
This clearly shows that the rich subcatego-rization information provided by CCG categoriescan help a shift-reduce parser.
With automatic cat-egories from a supertagger, we also got improve-ments over the baseline, for both Type 1 and Type2 data.
All the improvements are statistically sig-nificant (McNemar?s test, p < 0.01).With gold CCG categories, Type 1 data gaveslightly better improvements over Type 2 as Type1 data has richer morphological information.
But,in the case of supertagger output, Type 2 datagave more improvements over the baseline Maltas compared to Type 1.
This is because the perfor-mance of the supertagger on Type 2 data is slightlybetter than that of Type 1 data (see Table 1).Experiment Development TestingUAS LAS UAS LASMalt: Baseline 89.09 83.46 88.67 83.04Malt + Type 1 Gold 95.87* 90.79* 95.27* 90.22*Malt + Type 2 Gold 95.73* 90.70* 95.26* 90.18*Malt + Type 1 ST 89.54* 83.68* 88.93* 83.23*Malt + Type 2 ST 89.90* 83.96* 89.04* 83.35*Table 2: Supertagger impact on Hindi dependencyparsing (ST=Supertags).
McNemar?s test, * = p <0.01.It is interesting to notice the impact of usingautomatic CCG categories from a supertagger onlong distance dependencies.
It is known that Maltis weak at long-distance relations (Mcdonald andNivre, 2007; Ambati et al, 2010).
ProvidingCCG categories as features improved handling oflong-distance dependencies for Malt.
Figure 3shows the F-score of the impact of CCG categorieson three dependency labels, which take the ma-jor share of long distance dependencies, namely,ROOT, COORD, and RELC, the labels for sentenceroot, co-ordination, and relative clause respec-tively.
For these relations, providing CCG cate-gories gave an increment of 1.2%, 1.4% and 1.6%respectively over the baseline.We also found that the impact of CCG cate-gories is higher when the span of the dependencyis longer.
Figure 4 shows the F-score of the impactof CCG categories on dependencies based on thedistance between words.
Using CCG categories73.12 80.929.8974.35 82.2831.46255075100ROOT COORD RELCF-scoreDependency LabelsMalt Malt + Type 2 STFigure 3: Label-wise impact of supertag features.does not have much impact on short distance de-pendencies (1?5), which Malt is already good at.For longer range distances, 6?10, and >10, thereis an improvement of 1.8% and 1.4% respectively.97.379.0 76.497.580.8 77.87080901001 - 5 6 - 10 > 10F-scoreDistance rangesMalt Malt + Type 2 STFigure 4: Impact of supertags on distance ranges.4 Conclusion and Future DirectionWe have presented an approach for automaticallyextracting a CCG lexicon from a dependency tree-bank for Hindi.
We have also presented a novelway of creating a CCGbank from a dependencytreebank using a CCG parser and the CCG lex-icon.
Unlike previous work, we obtained im-provements in dependency recovery using auto-matic supertags, as well as gold information.
Wehave shown that informative CCG categories im-prove the performance of a shift-reduce depen-dency parser (Malt) in recovering some long dis-tance relations.
In future work, we would like todirectly train a CCG shift-reduce parser (such asZhang and Clark (2011)?s English parser) on theHindi CCGbank.
We would also like to see theimpact of generalisation of our lexicon using thefree-word order formalism for CCG categories ofBaldridge (2002).AcknowledgementsWe would like to thank three anonymous review-ers for their useful suggestions.
This work wassupported by ERC Advanced Fellowship 249520GRAMPLUS.608ReferencesBharat Ram Ambati, Samar Husain, Sambhav Jain,Dipti Misra Sharma, and Rajeev Sangal.
2010.Two Methods to Incorporate ?Local Morphosyntac-tic?
Features in Hindi Dependency Parsing.
In Pro-ceedings of the NAACL HLT 2010 First Workshopon Statistical Parsing of Morphologically-Rich Lan-guages, pages 22?30, Los Angeles, CA, USA, June.Bharat Ram Ambati.
2011.
Hindi Dependency Parsingand Treebank Validation.
Master?s Thesis, Interna-tional Institute of Information Technology - Hyder-abad, India.Jason M. Baldridge.
2002.
Lexically Specified Deriva-tional Control in Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh, UK.Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.1995.
Natural Language Processing: A PaninianPerspective.
Prentice-Hall of India, pages 65?106.Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma,and Lakshmi Bai.
2006.
AnnCorra: AnnotatingCorpora Guidelines for POS and Chunk Annotationfor Indian Languages.
In Technical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad.Akshar Bharati, Dipti Misra Sharma, SamarHusain, Lakshmi Bai, Rafiya Begum, andRajeev Sangal.
2009.
AnnCorra: Tree-Banks for Indian Languages, Guidelines forAnnotating Hindi TreeBank (version 2.0).http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-guidelines/DS-guidelines-ver2-28-05-09.pdf.Akshar Bharati, Prashanth Mannem, and Dipti MisraSharma.
2012.
Hindi Parsing Shared Task.
In Pro-ceedings of Coling Workshop on Machine Transla-tion and Parsing in Indian Languages, Kharagpur,India.Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,Owen Rambow, Dipti Misra Sharma, and Fei Xia.2009.
A multi-representational and multi-layeredtreebank for Hindi/Urdu.
In Proceedings of theThird Linguistic Annotation Workshop at 47th ACLand 4th IJCNLP, pages 186?189, Suntec, Singapore.Johan Bos, Cristina Bosco, and Alessandro Mazzei.2009.
Converting a Dependency Treebank to a Cat-egorial Grammar Treebank for Italian.
In M. Pas-sarotti, Adam Przepio?rkowski, S. Raynaud, andFrank Van Eynde, editors, Proceedings of the EighthInternational Workshop on Treebanks and LinguisticTheories (TLT8), pages 27?38, Milan, Italy.Ruken C?ak?c?.
2005.
Automatic induction of a CCGgrammar for Turkish.
In Proceedings of Student Re-search Workshop, 43rd Annual Meeting of the ACL,pages 73?78.Ruket C?ak?c?.
2009.
Parser Models for a Highly In-flected Language.
Ph.D. thesis, University of Edin-burgh, UK.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In Proceedings of COLING-04, pages 282?288.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396, Septem-ber.Julia Hockenmaier.
2006.
Creating a CCGbank anda wide-coverage CCG lexicon for German.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, ACL-44, pages 505?512, Sydney, Aus-tralia.Samar Husain, Prashanth Mannem, Bharat Ram Am-bati, and Phani Gadde.
2010.
The ICON-2010 Tools Contest on Indian Language Depen-dency Parsing.
In Proceedings of ICON-2010 ToolsContest on Indian Language Dependency Parsing,Kharagpur, India.Samar Husain.
2009.
Dependency Parsers for IndianLanguages.
In Proceedings of the ICON09 NLPTools Contest: Indian Language Dependency Pars-ing, India.Ryan Mcdonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency pars-ing models.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processingand Natural Language Learning.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 216?220, NewYork City, New York.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Mark Steedman.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.Daniel Tse and James R. Curran.
2010.
Chinese CCG-bank: extracting CCG derivations from the PennChinese Treebank.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,COLING ?10, pages 1083?1091, Beijing, China.Yue Zhang and Stephen Clark.
2011.
Shift-ReduceCCG Parsing.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages683?692, Portland, Oregon, USA, June.609
