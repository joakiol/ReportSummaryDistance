Reducing Parsing Complexity by Intra-Sentence Segmentationbased on Maximum Entropy ModelSung Dong K im,  ByoungoTak  Zhang,  Yung Taek  K imSchool of Computer Science and Engineering,Seoul National University, Korea{sdkim,btzhang}@scai.
snu.
ac.
kr, ytkim@cse, snu.
ac.
krAbstractLong sentence analysis has been a criticalproblem because of high complexity.
This pa-per addresses the reduction of parsing com-plexity by intra-sentence segmentation, andpresents max imum entropy model for deter-mining segmentation positions.
The modelfeatures lexical contexts of segmentation posi-tions, giving a probability to each potentialposition.
Segmentation coverage and accu-racy of the proposed method are 96% and88% respectively.
The parsing efficiency is im-proved by 77% in time and 71% in space.1 IntroductionLong sentence analysis has been a criticalproblem in machine translation because ofhigh complexity.
In EBMT (example-basedmachine translation), the longer a sentenceis, the less possible it is that the sentencehas an exact match in the translation archive,and the less flexible an EBMT system will be(Cranias et al, 1994).
In idiom-based ma-chine translation (Lee, 1993), long sentenceparsing is difficult because more resources arespent during idiom recognition phase as sen-tence length increases.
A parser is often un-able to analyze long sentences owing to theircomplexity, though they have no grammaticalerrors (Nasukawa, 1995).In English-Korean machine translation,idiom-based approach is adopted to overcomethe structural differences between two lan-guages and to get more accurate translation.The parser is a chart parser with a capabil-ity of idiom recognition and translation, whichis adapted to English-Korean machine trana-lation.
Idioms are recognized prior to syn-tactic analysis and the part of a sentence foran idiom takes an edge in a chart (Winograd,1983).
When parsing long sentences, an am-biguity of an idiom's range may cause moreedges than the number of words included inthe idiom (Yoon, 1994), which increases pars-ing complexity much.
A parser of practicalmachine translation system should be able toanalyze long sentences in a reasonable time.Most context-free parsing algorithms haveO(n 3) parsing complexities in terms of timeand space, where n is the length of a sen-tence (Tomita, 1986).
Our work is moti-vated by the fact that parsing becomes moreefficient, if n becomes horter.
This paperdeals with the problem of parsing complex-ity by way of reducing the length of sentenceto be analyzed.
This reduction is achievedby in t ra -sentence  segmentat ion,  which isdistinguished from inter--sentence s gmen-tat ion that is used for text categorization(Beeferman et al, 1997) or sentence boundaryidentification (Palmer and Hearst, 1997) (Rey-nar and Ratnaparkhi, 1997).
Intra-sentencesegmentation plays a role as a preliminarystep to a chart-based, context-free parser inEnglish-Korean machine translation.There have been several methods forreducing parsing complexities by intra-sentence segmentation.
In (Lyon and Frank,1995)(Lyon and Dickerson, 1997), they tookadvantage of the fact that the declarativesentences almost always consist of three seg-ments: \[pre-subject : subject:predicate\] .The complexity could be reduced by decom-posing a sentence into three sections.
Patternrules (Li et al, 1990) and sentence patterns(Kim and Khn, 1995) were used to segmentlong English sentences.
They showed low seg-mentation coverage, which means that manyof long sentences are not segmented by thepattern rules or sentence patterns.
And theyrequire much human efforts to construct pat-tern rules or collect sentence patterns.
Thesefactors may prevent hem being applicable topractical machine translation sYstems.This paper presents a trainable model foridentifying potential segmentation positions164in a sentence and determining appropriatesegmentation positions.
Given a corpus anno-tated with segmentation positions, our modelautomatically earns the contextual evidencesabout segmentation positions, which relieveshuman of burden to construct pattern rules orsentence patterns.
These evidences are com-bined under the maximum entropy framework(Jaynes, 1957) to estimate the probability foreach position.
By intra-sentence s gmenta-tion based on the proposed model, we achievemore improved parsing efficiency by 77% intime and 71% in space.In Section 2 we introduce the maximum en-tropy model.
Section 3 describes features in-corporated into the model and the process ofidentifying potential segmentation positions.The determination schemes of segmentationpositions are described in Section 4.
Segmen-tation performance of the model is presentedwith the degree of contribution to efficientparsing by the segmentation in Section 5.
Wealso compare our approach with other intra-sentence segmentation approaches.
Section 6draws conclusions and presents ome furtherworks.2 Max imum Ent ropy  Mode l ingSentence patterns or pattern ruels specify thesub-structures of the sentences.
That is, seg-mentation positions are determined in view ofthe global sentence structure.
If there is nomatched rules or patterns with a given sen-tence, the sentence could not be segmented.We assume that whether aword is a segmenta-tion position depends on its surrounding con-text.
We try to find factors that affect the de-termination of segmentation positions.
Maxi-mum entropy is a technique for automaticallyacquiring knowledge from incomplete infor-mation, without making any unsubstantiatedassumptions.
It masters ubtle effects o thatwe may accurately model subtle dependencies.It does not make any unwarranted assump-tions, which means that maximum entropylearns exactly what the data says.
Thereforeit can perform well on unseen data.The idea is to construct a model that as-signs a probability to each potential segmen-tation position in a sentence.
We build a prob-ability distribution p(ylx), where y ?
{0, 1}is a random variable specifying the potentialsegmentation position in a context x.
A fea-tu re  of a context is a binary-valued indicatorfunction \] expressing the information about aspecific context.Given a training sample of size N,(Xl,Yl),..-, (XN,YN), an empir ica l  proba-bi l i ty d i s t r ibut ion  can be defined asy) = y)Nwhere #(x,  y) is the number of occurrences of(x, y).
The expected value of feature fi withrespect o the empirical distribution i~(x, y) isexpressed asx,yand the expected value of fi with respect othe probability distribution p(ylx) isp(.fi) -~ ~(x)pCylx) .h(x ,  y),x~ywhere l~(x) is the empirical distribution of xin the corpus.
We want to build probabilitydistribution p(ylx) that is required to accordto the feature fi useful in selecting segmenta-tion positions: P(fi) = IS(fi) for all fi ?
.T,where Y is the set of candidate features.
Thismakes the probability distribution be built ononly training data.Given a feature set .T, let C be the subsetof all distributions P that satisfies the require-ment P(fi) = P(fi):C ~- {p ?
~ \] P(fi) = P(fi), for all fi ?
.T}.
(1)We choose a probability distribution consis-tent with all the facts, but otherwise as uni-form as possible.
The uniformity of the prob-ability distribution p(ylx) is measured by theconditional entropy:H(p) = - ~p(x ,  y) logp(ylx)x~y=   (x)PCulx) logp(ylx)x,yThus, the probability distribution with maxi-mum entropy is the most uniform distribution.In building a model, we consider the linearexponential family Q given as1Q(f) = {p(ylx)= ~exp(  E ~ifi(x,y))},$(2)165where Ai are real-valued parameters andZA(x) is a normalizing constant:= exp(  y)).y iAn intersection of the class Q of exponentialmodels with the class of desired distribution(1) is nonempty, and the intersection containsthe maximum entropy distribution and fur-thermore it is unique (Ratnaparkhi, 1994).Finding p. E C that maximizes H(p) is aproblem in constrained optimization, whichcannot be explicitly written in general.
There-fore, we take advantage of the fact that themodels in Q that satisfy p(fi) = 15(fi) canbe explained under the maximum likelihoodframework (Ratnaparkhi, 1994).
Maximumlikelihood principle also gives the unique dis-tribution p., the intersection of the class Qwith C.We assume each occurrence of (x,y) issampled independently.
Thus, log-likelihoodL#(p) of the empirical distribution i5 as pre-dicted by a model p can be defined asL~)  _= log I I  p(ylx) ~(~'y) = ~\]p(x,  y) logp(ylx ).x,y x ,yThat is, the model we want to build isp.
= arg  xc = arg  maxqE~The parameters A~ of exponential model (2)are obtained by the Generalized Iterative Scal-ing algorithm (Darroch and Ratcliff, 1972).3 Const ruct ion  o f  FeaturesThis section describes the features.
From acorpus, contextual evidences of segmentationpositions are collected and combined, result-ing in features.
The features are used in iden-tifying potential segmentation positions andincluded in the model.3.1 Segmentab le  Posit ions and  SafeSegmentationA sentence is constructed by the combina-tion of words, phrases, and clauses under thewell-defined grammar.
A sentence can be seg-mented into shorter segments that correspondto the constituents of the sentence.
That is,segments correspond to the nonterminal sym-bols of the context-free grammar 1.
The posi-1Nonterminal symbols include the ones for phrases,such as NP  (noun phrase) and VP  (verb phrase),tion of a word is called segmentab le  posi-t ion that can be a starting position of a spe-cific segment.Though the analysis complexity can be re-duced by segmenting a sentence, there isa mis-segmentation risk that causes pars-ing failures.
A segmentation can be calledsafe segmentat ion  that results in a coherentblocks of words.
In English-Korean transla-tion, safe segmentation is defined as the onewhich generates safe segments.
A segment issafe, when there is a syntactic ategory sym-bol N P dominating the segment and the seg-ment can be combined with adjacent segmentsunder a given grammar.
In Figure 1, (a) is anunsafe segmentation because the second seg-ment cannot be analyzed into one syntacticcategory, resulting in parsing failure.
By thesafe segmentation (b), the first segment cor-responds to a noun phrase and the second toa verb phrase, so that we can get a correctanalysis result.
(a) IThe students Iwho study hard will pass the exam\](b) I The students who study hard II will pass the exam\[Figure h Examples of unsafe/safe segmenta-tion in English-Korean translation.3.2 Lexical Contextua l  Const ra in tsA lexical context  of a word includes even-word window: three to the left of a word andthree to the right of a word and a word itself.It also includes the part-of-speeches of thesewords, subcategorization information for twowords to the left, and position value.
Theposition value posi_v of the ith word wi is cal-culated aspos _v = r ?
R\ ] ,where n is the number of words and R 2 repre-sents the number of regions in the sentence.Region is the sequentially ordered block ofand the ones for clauses like RLCL (relative clause),SUBCL (subordinate clause).sit is a heuristically set value, and we set R as 4.166words in a sentence, and posi_v represents heregion in which a word lies.
It is included toreflect the influence of the position of a wordon being a segmentation position.
Thus, thelexical context of a word is represented by 17attributes as shown in Figure 2.s_position?wordiWi-3~ ?
.
.
,  Wi+3Pi-3, " ?, Pi+38.-Cgtti-2, S-carl- 1posi_vFigure 2: The structure of lexical context.An example of a training data and a re-sulting lexical context is shown in Figure 3.A symbol '# '  represents a segmentation posi-tion marked by human annotators.
Therefore,the lexical context of word when includes thevalue 1 for attribute s_position?
and follow-ings: three words to the left of when (became,terribly, and worried) and part-of-speecheso f  each word (VERB ADV ADJ), three wordsto the right (they, saw, and what) and part-of-speeches (PRON VERB PRON), subcat-egorization information for two words to theleft (0 1), and position value (2).Of course his parents became terribly worried#when they saw what was happeningto Atzel.
( 1 when became terribly worried they sawwhat VERB ADV ADJ PRON VERBPRON 0 1 2 )Figure 3: An example of a training data anda lexical context.To get reliable statistics, much trainingdata is required.
To alleviate this prob-lem, we generate lexical contextua l  con-st ra lnts  by combining lexical contexts andcollect statistics for them.
To generate lex-ical contextual constraints and to identifysegmentable positions, we define two oper-ations join (E9) and consistency (=).
Let(a l , .
.
.
,an)  and (bl , .
.
.
,bn) be lexical con-texts and (C1,... ,On) be lexical contextualconstraint.
The operation join is defined as(a l , .
.
.
,  an) ?
(bl, .- .
,  bn) = (C1,... ,  Ca),,.i if ai # biCi = ai if ai = bi 'where ' , '  is don't-care term accepting anyvalue.
A lexical contextual constraint is gen-erated as a result of jo in operation.
Theconsistency is defined as1 if (C i=a iorC i= '* r )  fo ra l l l< i<nk = 0 otherwiseThe algorithm for generating lexical contex-tual constraints i shown in Figure 4.?
Input: a set of active lexical contextsLCw = { lc l .
.
.
lcn} for word w,where lcc/= (a l , .
.
.
,  an).?
Output: a set of lexical contextualconstraints LCCw = {/ccl .../cck},where lcc /= (C1,.
.
.
,  Cn).1.
Initialize LCCw = 02.
Do the followings for each l~ E LCw(a) For all lcj(j # i), Count(lcj) = # ofmatched attributes with Ic/(b) max_cnt = arg maxlc?
eLC.
Count( Icj )(c) For all lcj, where Count(lcj) = max..cnt,Icc= lc~ ?
lc~, LCCw e- LCC,  U {/cc}Figure 4: Algorithm for generating lexicalcontextual constraints.A Icc plays the role of a feature.
Followingis an example of a feature.f (x ,y )  =1 i f  Xward  = "that" andxi-1 = "say" and y = 10 otherwiseWe collect the statistics for each Icc.
The fre-quency of each lcc is counted as the numberof lexical contexts that satisfy the consistencyoperation with the lcc.ni=1167Identifying segmentable positions is per-formed with the consistency operation withthe lexical context of word w and lcc E LCCw.The word whose lexical context is consistentwith lcc is identified as a segmentable posi-tion.4 Determinat ion  Schemes  ofSegmentat ion  Pos i t ionsSegmentation positions are determinedthrough two steps: identifying segmentablepositions and selecting the most appropriateposition among them.
Segmentable positionsare identified using the consistency operation.Maximum entropy model in Section 2 gives aprobability to each position.Segmentation performance is measured interms of coverage and accuracy.
Coverage isthe ratio of the number of actually segmentedsentences to the number of segmentation tar-get sentences that are longer than ot words,where o~ is a fixed constant distinguishing longsentences from short ones.
Accuracy is evalu-ated in terms of the safe segmentation ratio.They are defined as follows:# of actually segmented Sent.coverage = ~ of Sent.
to be segmented(3)# of Sent.
with safe segmentationaccuracy = ~ of actually segmented Sent.
(a)4.1 Basel ine SchemeNo contextual information is used in identify-ing segmentable positions.
They are empiri-cally identified.
A word that is tagged as asegmentation position more than 5 times isidentified as a segmentable position.
A set ofsegmentable positions, 9 ,  is as follows.~D = {wi \[ wi is tagged as segmentation positionand #(tagged wi) >_ 5}In order to select the most appropriate po-sition, the segmentation appropriateness ofeach position is evaluated by the probabilityof word wi:# of tagged wip(Wi) = # of wi in the corpusp(wi) represents the tendency that word wiwill be used as a segmentation position.
Asegmentation position w. is selected as the onethat has highest p(wi) value:w. = arg max p(wi).
wiE~This scheme serves as a baseline for comparingthe segmentation performance of the models.4.2 A Scheme using LexicalContextua l  Const ra intsLexical contextual constraints are used inidentifying segmentable positions.
Comparedwith the baseline scheme, this scheme con-siders contextual information of a word.
Allconsistent words with the defined lexical con-textual constraints form a set of segmentablepositions 79.The maximum likelihood principle gives aprobability distribution for p(y I lcc~), wherey E {0, 1}.
Segmentation appropriateness isevaluated by p(1 I lcew,).
A position with thehighest p(1 I lcc~) becomes a segmentationposition:w. = arg max p(1 I/CCwi).wi E~4.3 A Scheme using LexicalContextua l  Const ra ints  w i thWord SetsDue to insufficient raining samples for con-structing lexical contextual constraints, omesegmentable positions may not be identified.To alleviate this problem we introduce wordsets whose elements have linguistically similarfeatures.
We define four word sets: coordinateconjunction set, subordinate conjunction set,interogative set, auxiliary verb set.
The cate-gories of word sets and the examples of theirmembers are shown in Table 1.Table 1: The word sets and examples.Word Set ExamplesCoordinate Conjunctions and, or, butSubordinate Conjunctions if, when, .
.
.Interogatives how, what, .
.
.Auxiliary Verbs can, should, .
.
.Coordinate conjunctions haveonly 3 mem-bers, but they frequently apprear in long sen-tences.
Subordinate conjunctions have 25168members, interogatives 5 members, and aux-iliary verbs have 12 members now.
The wordsbelonging to each word set are treated equally.Lexical contextual constraints are constructedfor words and word sets, so the statistics iscollected for both of them.
The set of seg-mentable positions T~ is defined somewhat dif-ferently as::D = {wi, wsj I (Icc, v, -= lcc~,) = 1or (Icws~ =-- IcC.ws~) = 1},where wsj denotes a word set to which the j thword in a sentence belongs.In this scheme, p(1 I Iccc,,,) or p(1 \] lccws,)expresses the segmentation appropriateness ofthe position.
Therefore, a segmentation posi-tion is determined byw, = arg max {p(1 I lcc ,), p(1 I lcc s )}.
{w,,ws~}~95 Exper iments5.1 Corpus  and  Const ruct ion  of  theMax imum Ent ropy  Mode lWe construct he corpus from two differentdomains, where the sentences longer than 15-words are extracted 3.
The training portion isused to generate l xical contextual constraintsand to collect statistics for maximum entropymodel construction.
From high school Englishtexts, 1500 sentences are tagged with segmen-tation positions by human.
Two people whohave some knowledge about English syntacticstructures read sentences, and marked wordsas segmentation positions where they paused.After generating lexical contextual con-straints, we constructed the maximum en-tropy model p(ylx), where x is a lexical con-textual constraint and y E {0,1}.
The modelincorporates features that occur more than 5times in the training data.
3626 candidate fea-tures were generated without word sets and3878 features with word sets.
In Table 2,training time and the number of active fea-tures of the model are shown.Segmentation performance is evaluated us-ing test portion that consists of 1800 sentencesffrom two domains: high school English textsand the Byte Magazine.3The sentences with commas are excluded becausecomma is an explicit segmentation position.
Segmentsresulting from a segmentation at commas may be themanageable-sized ones.
Our work is to segment longsentences without explicit segmentation positions.Table 2: Construction of models.Training # ofTime Active FeaturesWithout 10 rain 2720Word SetsWith 12 mln 2910Word Sets5.2 Segmentat ion  Per fo rmanceIn addition to coverage and accuracy, SCvalue is also defined to express the degree ofcontribution to efficient parsing by segmenta-tion.
It is the ratio of the sentences that canbenefit from intra-sentence s gmentation.
If along sentence isnot segmented or is segmentedat unsafe segmentation positions, the sentenceis called a segmentat ion  er ror  sentence .SC value is calculated as# of segmentation error sentences SG= I -# of segmentation target sentences"A sentence longer than vt words is con-sidered as the segmentation target sentence,where c~ is set to 12.
Table 3 compares eg-mentation performance for each determina-tion scheme.Table 3: Segmentation performance of the de-termination schemes of segmentation position.Determination Coverage/Schemes Accuracy (%)Baseline 100/77.6LCC 90.7/89LCC with 95.8/87.9Word SetsSC0.7760.8080.865By the comparison of the baseline schemewith others, the accuracy is observed to de-pend on the context information.
Word setsare helpful for increasing coverage with lessdegradation of accuracy.
Each scheme has su-periority in terms of the different measures.But in terms of applicability to practical sys-tems, the third scheme is best for our purpose.Table 4 shows the segmentation performanceof the scheme using LCC with word sets.SU value for the sentences from the samedomain as training data is about 0.88, and169Table 4: Segmentation performance of LCCwith word sets.Domain Sent.
Coverage/ ILength Accuracy(%) I15~19High-SchoolEnglish TextByteMagazineTotal20~2425~2930~15,-,1920,-~2425,,~2930,-~99.0/95.9100/94.096.0/81.3100/67.594.0/92.691.0/91.292.5/94.693.5/86.11800 95.8/87.9 \]8C0.950.940.780.6g0.870.830.880.810.87about 0.85 for the sentenes from the ByteMagazine.
Though they slightly differ be-tween test domains, about 87% of long sen-tences can be parsed with less complexity andwithout causing parsing failures.
It suggeststhat the intra-sentence s gmentation methodcan be utilized for efficient parsing of the longsentences.5.3  Pars ing  E f f i c iencyParsing efficiency is generally measured bythe required time and memory for parsing.In most cases, parsing sentences longer than30 words could not complete without intra-sentence segmentation.
Therefore, the parsingis performed for the sentences longer than 15and less than 30 words.
Ultra-Sparc 30 ma-chine is used for experiments.
The efficiencyimprovement was measured byEItime tunseg -- tseg = ,,, x 100,tunsegEImemory = rnunseg --mseg X 100,~T~unse9where $unseg and rrbanseg are time and memoryduring parsing without segmentation a d tseg,rnseg are for the parsing with segmentation.Table 5 summarizes the results.By segmenting long sentences into severalmanageable-sized s gments, we can parse longsentences with much less time and space.5.4  Compar i son  w i th  Re la ted  WorksThe intra-sentence segmentation methodbased on the maximum entropy model is corn- -pared with other approaches in terms of theTable 5: Comparison of parsing efficiencywith/without segmentation.WithSegmentationWithoutSegmentationImprovementHigh-School Byte 1English Text Magazine4.6 sec 5.4 sec 10.9 MB19.6 sec3.4 MB76.5%73.5%1.1 MB'25.1 sec3.7 MB78.5%70.3%segmentation coverage and the improvementof parsing efficiency.In (Lyon and Frank, 1995)(Lyon and Dick-erson, 1997), a sentence is segmented intothree segments.
Though parsing efficiency canbe improved by segmenting a sentence, thismethod may be applied to only simple sen-tences 4.
Long sentences are generally coordi-nate sentences 5 or complex sentences 6.
Theyhave more than two subjects, so applying thismethod to such sentences seems to be inap-propriate.In (Kim and Kim, 1995), sentence patternsare used to segment long sentences.
Thismethod improve parsing efficiency by 30% intime and 58% in space.
However collectingsentence patterns requires much hnman effortsand segmentation coverage isonly about 36%.Li's method (Li et al, 1990) for sentencesegmentation also depends upon manual-intensive pattern rules.
Segmentation cover-age seems to be unsatisfactory for practicalmachine translation system.The proposed method can be applied to co-ordinate and complex sentences a  well as sim-ple sentences.
It shows segmentation coverageof about 96%.
In addition, it needs no otherhuman efforts except for constructing trainingdata.
Human ~.nnotators have only to readsentences and mark segmentation positions,which is more simple than collecting patternrules or sentence patterns.
We can also getmuch improved parsing efficiency: about 77%in time and about 71% in space.4A simple sentence has one subject and one predi-cate.5A coordinate sentence results ~om the combina-tion of several simple sentences by the coordinate con-junctions.
-6A complex sentence consists of a main clause andseveral subordinate clauses.1706 Conc lus ion  and  Future  WorkPractical machine translation systems houldbe able to accommodate long sentences.
Thusintra-sentence s gmentation is required as ameans for reducing parsing complexity.
Thispaper presents a method for intra-sentencesegmentation based on the maximum entropymodel.
The method builds statistical modelsautomatically from a text corpus to providethe segmentation appropriateness for safe seg-mentation.In the experiments with 1800 test sentences,about 87% of them were benefited from seg-mentation.
The statistical intra-sentence seg-mentation method can also relieve human ofthe burden of constructing information, suchas segmentation rules or sentence patterns.Experiments suggest that the proposed maxi-mum entropy models can be incorporated intothe parser for practical machine translationsystems.Further works can be done in two direc-tions.
First, studies on recovery mecha-nisms for unsafe segmentation before parsingseem necessary since ungafe segmentation maycause parsing failures.
Second, parsing controlmechanisms should be studied that exploit he-characteristics of segmentation positions andthe parallelism among segments.
This will en-hance parsing efficiency further.Re ferencesD.
Beeferman, A. Berger, and J. Lafferty.
1997.Text Segmentation using Exponential Models.In Second Conference on Empirical Methods inNatural Language Processing.
Providence, RI.Lambros Cranias, Harris Papageorgiou, and Ste-lios Piperdis.
1994.
A Matching Technique inExample-Based Machine Translati on.
In Pro-ceedings of 1995 COLING, pages 100--104.J.N.
Darroch and D. Ratcliff.
1972.
GeneralizedIterative Scaling for Log-linear Models.
TheAnnals of Mathematical Statistics, 43(5):1470-1480.E.T.
Jaynes.
1957.
Information Theory and Sta-tistical Mechanics.
Physical Review, 106:620-630.Sung Dong Kim and Yung Taek Kim.
1995.Sentence Analysis using Pattern Matching inEnglish-Korean Machine Translation.
In Pro-ceedings of the 1995 ICCPOL, Oct. 25-28.Ho Suk Lee.
1993.
Automatic Construction ofTransfer Dictionary based on the Corpus forEnglish-Korean Machine Translation.
Ph.D.thesis, Seoul National University.
In Korean.Wei-Chuan Li, Tzusheng Pei, Bing-Huang Lee,and Chuei-Feng Chiou.
1990.
Parsing Long Ea-ghsh Sentences with Pattern Rules.
In Proceed-ings of 25th Conference of COLING, pages 410--412.Caroline Lyon and Bob Dickerson.
1997.
Reduc-ing the Complexity of Parsing by a Method ofDecomposition.
In International Workshop onParsing Technology, September.Caroline Lyon and Ray Frank.
1995.
Neural Net-work Design for a Natural Language Parser.In International Conference on Artificial NeuralNetworks.Tetsura Nasukawa.
1995.
Robust Parsing Basedon Discourse Information.
In 33rd AnnualMeeting of the A CL, pages 33-46.David D. Palmer and Marti A. Hearst.
1997.Adaptive Multilingual Sentence BoundaryDisambiguation.
Computational Linguistics,23(2):241-265.A.
Ratnaparkhi.
1994.
A Simple Introductionto Maximum Entropy Models for Natural Lanognage Processing.
Technical report, Institutefor Research in Cognitive Science, University ofPennsylvania 3401 Walnut Street, Suite 400APhiladelphia, PA 19104-6228, May.
IRCS Re-port 97-08.J.C.
Reynar and A. Ratnaparkhi.
1997.
A Maxi-mum Entropy Approach to Identifying SentenceBoundaries.
In Proceedings of the Fifth Confer-ence on Applied Natural Language Processing,pages 16--19.
Washington D.C.Masaru Tomita.
1986.
E~icient Parsing for Nat-ural Language.
Kluwer Academic Publishers.T.
Winograd.
1983.
Language as a Cognitive Pro-cess: Syntax, volume 1.
Addison-Wesley.Sung Hee Yoon.
1994.
Efficient Parser to FindBilingual Idiomatic Expressions for English-Korean Mvchine Translation.
In Proceedings ofthe 1994 ICCPOL, pages 455-460.171
