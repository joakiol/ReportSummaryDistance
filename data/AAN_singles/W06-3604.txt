Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 25?32,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsAll-word prediction as the ultimate confusable disambiguationAntal van den BoschILK / Dept.
of Language and Information Science, Tilburg UniversityP.O.
Box 90153, NL-5000 LE Tilburg, The NetherlandsAntal.vdnBosch@uvt.nlAbstractWe present a classification-based wordprediction model based on IGTREE, adecision-tree induction algorithm with fa-vorable scaling abilities and a functionalequivalence to n-gram models with back-off smoothing.
Through a first series ofexperiments, in which we train on Reutersnewswire text and test either on the sametype of data or on general or fictional text,we demonstrate that the system exhibitslog-linear increases in prediction accuracywith increasing numbers of training ex-amples.
Trained on 30 million wordsof newswire text, prediction accuraciesrange between 12.6% on fictional text and42.2% on newswire text.
In a second se-ries of experiments we compare all-wordsprediction with confusable prediction, i.e.,the same task, but specialized to predict-ing among limited sets of words.
Con-fusable prediction yields high accuracieson nine example confusable sets in allgenres of text.
The confusable approachoutperforms the all-words-prediction ap-proach, but with more data the differencedecreases.1 IntroductionWord prediction is an intriguing language engineer-ing semi-product.
Arguably it is the ?archetypicalprediction problem in natural language processing?
(Even-Zohar and Roth, 2000).
It is usually not anengineering end in itself to predict the next word in asequence, or fill in a blanked-out word in a sequence.Yet, it could be an asset in higher-level proofing orauthoring tools, e.g.
to be able to automatically dis-cern among confusables and thereby to detect con-fusable errors (Golding and Roth, 1999; Even-Zoharand Roth, 2000; Banko and Brill, 2001; Huang andPowers, 2001).
It could alleviate problems with low-frequency and unknown words in natural languageprocessing and information retrieval, by replacingthem with likely and higher-frequency alternativesthat carry similar information.
And also, since thetask of word prediction is a direct interpretation oflanguage modeling, a word prediction system couldprovide useful information for to be used in speechrecognition systems.A unique aspect of the word prediction task, ascompared to most other tasks in natural languageprocessing, is that real-world examples abound inlarge amounts.
Any digitized text can be used astraining material for a word prediction system capa-ble of learning from examples, and nowadays gigas-cale and terascale document collections are availablefor research purposes.A specific type of word prediction is confus-able prediction, i.e., learn to predict among lim-ited sets of confusable words such as to/two/too andthere/their/they?re (Golding and Roth, 1999; Bankoand Brill, 2001).
Having trained a confusable pre-dictor on occurrences of words within a confusableset, it can be applied to any new occurrence of aword from the set; if its prediction based on the con-text deviates from the word actually present, then25this word might be a confusable error, and the classi-fier?s prediction might be its correction.
Confusableprediction and correction is a strong asset in proof-ing tools.In this paper we generalize the word predictiontask to predicting any word in context.
This is basi-cally the task of a generic language model.
An ex-plicit choice for the particular study on ?all-words?prediction is to encode context only by words,and not by any higher-level linguistic non-terminalswhich have been investigated in related work onword prediction (Wu et al, 1999; Even-Zohar andRoth, 2000).
This choice leaves open the questionhow the same tasks can be learned from exampleswhen non-terminal symbols are taken into accountas well.The choice for our algorithm, a decision-tree ap-proximation of k-nearest-neigbor (k-NN) based ormemory-based learning, is motivated by the factthat, as we describe later in this paper, this particularalgorithm can scale up to predicting tens of thou-sands of words, while simultaneously being able toscale up to tens of millions of examples as trainingmaterial, predicting words at useful rates of hun-dreds to thousands of words per second.
Anothermotivation for our choice is that our decision-treeapproximation of k-nearest neighbor classification isfunctionally equivalent to back-off smoothing (Za-vrel and Daelemans, 1997); not only does it shareits performance capacities with n-gram models withback-off smoothing, it also shares its scaling abili-ties with these models, while being able to handlelarge values of n.The article is structured as follows.
In Section 2we describe what data we selected for our experi-ments, and we provide an overview of the exper-imental methodology used throughout the experi-ments, including a description of the IGTREE algo-rithm central to our study.
In Section 3 the results ofthe word prediction experiments are presented, andthe subsequent Section 4 contains the experimen-tal results of the experiments on confusables.
Webriefly relate our work to earlier work that inspiredthe current study in Section 5.
The results are dis-cussed, and conclusions are drawn in Section 6.2 Data preparation and experimentalsetupFirst, we identify the textual corpora used.
We thendescribe the general experimental setup of learn-ing curve experiments, and the IGTREE decision-tree induction algorithm used throughout all experi-ments.2.1 DataTo generate our word prediction examples, we usedthe ?Reuters Corpus Volume 1 (English Language,1996-08-20 to 1997-08-19)?1 .
We tokenized thiscorpus with a rule-based tokenizer, and used all130,396,703 word and punctuation tokens for exper-imentation.
In the remainder of the article we makeno difference between words and punctuation mark-ers; both are regarded as tokens.
We separated thefinal 100,000 tokens as a held-out test set, hence-forth referred to as REUTERS, and kept the rest astraining set, henceforth TRAIN-REUTERS.Additionally, we selected two test sets takenfrom different corpora.
First, we used the ProjectGutenberg2 version of the novel Alice?s Adventuresin Wonderland by Lewis Carroll (Carroll, 1865),henceforth ALICE.
As the third test set we selectedall tokens of the Brown corpus part of the Penn Tree-bank (Marcus et al, 1993), a selected portion ofthe original one-million word Brown corpus (Kuc?eraand Francis, 1967), a collection of samples of Amer-ican English in many different genres, from sourcesprinted in 1961; we refer to this test set as BROWN.In sum, we have three test sets, covering texts fromthe same genre and source as the training data, afictional novel, and a mix of genres wider than thetraining set.Table 1 summarizes the key training and test setstatistics.
As the table shows, the cross-domain cov-erages for unigrams and bigrams are rather low; notonly are these numbers the best-case performanceceilings, they also imply that a lot of contextualinformation used by the machine learning methodused in this paper will be partly unknown to thelearner, especially in texts from other domains thanthe training set.1For availability of the Reuters corpus, seehttp://about.reuters.com/researchandstandards/corpus/.2Project Gutenberg: http://www.gutenberg.net.26Data set Genre # Tokens Coverage (%)TRAIN-REUTERS news 30 million unigram bigramREUTERS news 100,000 91.0 83.6ALICE fiction 33,361 85.2 70.1BROWN mixed 453,446 75.9 72.3Table 1: Training and test set sources, genres, sizesin terms of numbers of tokens, and unigram and bi-gram coverage (%) of the training set on the test sets.2.2 Experimental setupAll experiments described in this article take theform of learning curve experiments (Banko andBrill, 2001), in which a sequence of training setsis generated with increasing size, where each sizetraining set is used to train a model for word predic-tion, which is subsequently tested on a held-out testset ?
which is fixed throughout the whole learningcurve experiment.
Training set sizes are exponen-tially grown, as earlier studies have shown that ata linear scale, performance effects tend to decreasein size, but that when measured with exponentiallygrowing training sets, near-constant (i.e.
log-linear)improvements are observed (Banko and Brill, 2001).We create incrementally-sized training sets for theword prediction task on the basis of the TRAIN-REUTERS set.
Each training subset is created back-ward from the point at which the final 100,000-wordREUTERS set starts.
The increments are exponentialwith base number 10, and for every power of 10 wecut off training sets at n times that power, where n =1, 2, 3, .
.
.
, 8, 9 (for example, 10, 20, .
.
.
, 80, 90).The actual examples to learn from are created bywindowing over all sequences of tokens.
We encodeexamples by taking a left context window spanningseven tokens, and a right context also spanning seventokens.
Thus, the task is represented by a growingnumber of examples, each characterized by 14 po-sitional features carrying tokens as values, and oneclass label representing the word to be predicted.The choice for 14 is intended to cover at least thesuperficially most important positional features.
Weassume that a word more distant than seven positionsleft or right of a focus word will almost never bemore informative for the task than any of the wordswithin this scope.2.3 IGTreeIGTree (Daelemans et al, 1997) is an algorithmfor the top-down induction of decision trees.
Itcompresses a database of labeled examples intoa lossless-compression decision-tree structure thatpreserves the labeling information of all examples,and technically should be named a trie accordingto (Knuth, 1973).
A labeled example is a feature-value vector, where features in our study representa sequence of tokens representing context, associ-ated with a symbolic class label representing theword to be predicted.
An IGTREE is composed ofnodes that each represent a partition of the originalexample database, and are labeled by the most fre-quent class of that partition.
The root node of thetrie thus represents the entire example database andcarries the most frequent value as class label, whileend nodes (leafs) represent a homogeneous partitionof the database in which all examples have the sameclass label.
A node is either a leaf, or is a non-endingnode that branches out to nodes at a deeper level ofthe trie.
Each branch represents a test on a featurevalue; branches fanning out of one node test on val-ues of the same feature.To attain high compression levels, IGTREEadopts the same heuristic that most other decision-tree induction algorithms adopt, such as C4.5 (Quin-lan, 1993), which is to always branch out testing onthe most informative, or most class-discriminativefeatures first.
Like C4.5, IGTREE uses informationgain (IG) to estimate the most informative features.The IG of feature i is measured by computing thedifference in uncertainty (i.e.
entropy) between thesituations without and with knowledge of the valueof that feature with respect to predicting the class la-bel: IGi = H(C)?
?v?Vi P (v)?H(C|v), whereC is the set of class labels, Vi is the set of valuesfor feature i, and H(C) = ?
?c?C P (c) log2 P (c)is the entropy of the class labels.
In contrast withC4.5, IGTREE computes the IG of all features onceon the full database of training examples, makes afeature ordering once on these computed IG values,and uses this ordering throughout the whole trie.Another difference with C4.5 is that IGTREEdoes not prune its produced trie, so that it performsa lossless compression of the labeling informationof the original example database.
As long as the27database does not contain fully ambiguous examples(with the same features, but different class labels),the trie produced by IGTREE is able to reproducethe classifications of all examples in the original ex-ample database perfectly.Due to the fact that IGTREE computes the IGof all features once, it is functionally equivalent toIB1-IG (Daelemans et al, 1999), a k-nearest neigh-bor classifier for symbolic features, with k = 1and using a particular feature weighting in the sim-ilarity function in which the weight of each fea-ture is larger than the sum of all weights of featureswith a lower weight (e.g.
as in the exponential se-quence 1, 2, 4, 8, .
.
.
where 2 > 1, 4 > (1 + 2),8 > (1 + 2 + 4), etc.).
Both algorithms will basetheir classification on the example that matches onmost features, ordered by their IG, and guess a ma-jority class of the set of examples represented at thelevel of mismatching.
IGTREE, therefore, can beseen as an approximation of IB1-IG with k = 1 thathas favorable asymptotic complexities as comparedto IB1-IG.IGTREE?s computational bottleneck is the trieconstruction process, which has an asymptotic com-plexity of O(n lg(v) f) of CPU, where n is the num-ber of training examples, v is the average branchingfactor of IGTREE (how many branches fan out ofa node, on average), and f is the number of fea-tures.
Storing the trie, on the other hand, costsO(n) in memory, which is less than the O(n f) ofIB1-IG.
Classification in IGTREE takes an efficientO(f lg(v)) of CPU, versus the cumbersome worst-case O(n f) of IB1-IG, that is, in the typical casethat n is much higher than f or v.Interestingly, IGTREE is functionally equiva-lent to back-off smoothing (Zavrel and Daelemans,1997), with the IG of the features determining theorder in which to back off, which in the case of wordprediction tends to be from the outer context to theinner context of the immediately neighboring words.Like with probabilistic n-gram based models witha back-off smoothing scheme, IGTREE will prefermatches that are as exact as possible (e.g.
match-ing on all 14 features), but will back-off by dis-regarding lesser important features first, down to asimple bigram model drawing on the most impor-tant feature, the immediately preceding left word.In sum, IGTREE shares its scaling abilities with n-0102030405030,000,00010,000,0001,000,000100,00010,0001,000100wordpredictionaccuracyexamplesTEST-BROWNTEST-ALICETEST-REUTERSFigure 1: Learning curves of word prediction accu-racies of IGTREE trained on TRAIN-REUTERS, andtested on REUTERS, ALICE, and BROWN.gram models, and its implementation allows it tohandle large values of n.3 All-words prediction3.1 Learning curve experimentsThe word prediction accuracy learning curves com-puted on the three test sets, and trained on increasingportions of TRAIN-REUTERS, are displayed in Fig-ure 1.
The best accuracy observed is 42.2% with30 million training examples, on REUTERS.
Appar-ently, training and testing on the same type of datayields markedly higher prediction accuracies thantesting on a different-type corpus.
Accuracies onBROWN are slightly higher than on ALICE, but thedifference is small; at 30 million training examples,the accuracy on ALICE is 12.6%, and on BROWN15.8%.A second observation is that all three learningcurves are progressing upward with more trainingexamples, and roughly at a constant log-linear rate.When estimating the rates after about 50,000 exam-ples (before which the curves appear to be morevolatile), with every tenfold increase of the num-ber of training examples the prediction accuracy onREUTERS increases by a constant rate of about 8%,while the increases on ALICE and BROWN are bothabout 2% at every tenfold.283.2 Memory requirements and classificationspeedThe numbers of nodes exhibit an interesting sublin-ear relation with respect to the number of trainingexamples, which is in line with the asymptotic com-plexity order O(n), where n is the number of train-ing instances.
An increasingly sublinear amountof nodes is necessary; while at 10,000 training in-stances the number of nodes is 7,759 (0.77 nodesper instance), at 1 million instances the number ofnodes is 652,252 (0.65 nodes per instance), and at 30million instances the number of nodes is 15,956,878(0.53 nodes per instance).A factor in classification speed is the averageamount of branching.
Conceivably, the word predic-tion task can lead to a large branching factor, espe-cially in the higher levels of the tree.
However, notevery word can be the neighbor of every other wordin finite amounts of text.
To estimate the averagebranching factor of a tree we compute the f th rootof the total number of nodes (f being the numberof features, i.e.
14).
The largest decision tree cur-rently constructed is the one on the basis of a train-ing set of 30 million examples, having 15,956,878nodes.
This tree has an average branching factor of14?15, 956, 878 ?
3.27; all other trees have smallerbranching factors.
Together with the fact that wehave but 14 features, and the asymptotic complex-ity order of classification is O(f lg(v)), where v isthe average branching factor, classification can beexpected to be fast.
Indeed, depending on the ma-chine?s CPU on which the experiment is run, weobserve quite favorable classification speeds.
Fig-ure 2 displays the various speeds (in terms of thenumber of test tokens predicted per second) attainedon the three test sets3.
The best prediction accu-racies are still attained at classification speeds ofover a hundred predicted tokens per second.
Twoother relevant observations are that first, the classi-fication speed hardly differs between the three testsets (BROWN is classified only slightly slower thanthe other two test sets), indicating that the classifieris spending a roughly comparable amount of search-ing through the decision trees regardless of genredifferences.
Second, the decrease in speed settles3Measurements were made on a GNU/Linux x86-based ma-chine with 2.0 Ghz AMD Opteron processors.10010001000030,000,00010,000,0001,000,000100,00010,0001,000100testexamplespersecondtraining examplesTEST-BROWNTEST-ALICETEST-REUTERSFigure 2: Word prediction speed, in terms of thenumber of classified test examples per second, mea-sured on the three test sets, with increasing trainingexamples.
Both axes have a logarithmic scale.on a low log-linear rate after about one million ex-amples.
Thus, while trees grow linearly, and accu-racy increases log-linearly, the speed of classifica-tion slowly diminishes at decreasing rates.4 ConfusablesWord prediction from context can be considered avery hard task, due to the many choices open to thepredictor at many points in the sequence.
Predictingcontent words, for example, is often only possiblethrough subtle contextual clues or by having the ap-propriate domain or world knowledge, or intimateknowledge of the writer?s social context and inten-tions.
In contrast, certain function words tend to bepredictable due to the positions they take in syntac-tic phrase structure; their high frequency tends to en-sure that plenty of examples of them in context areavailable.Due to the important role of function words insyntactic structure, it can be quite disruptive for aparser and for human readers alike to encounter amistyped function word that in its intended formis another function word.
In fact, confusable er-rors between frequent forms occur relatively fre-quently.
Examples of these so-called confusablesin English are there versus their and the contrac-tion they?re; or the duo than and then.
Confus-ables can arise from having the same pronunciation(homophones), or having very similar pronunciation(country or county) or spelling (dessert, desert), hav-29ing very close lexical semantics (as between amongand between), or being inflections or case variants ofthe same stem (I versus me, or walk versus walks),and may stem from a lack of concentration or expe-rience by the writer.Distinguishing between confusables is essentiallythe same task as word prediction, except that thenumber of alternative outcomes is small, e.g.
twoor three, rather than thousands or more.
The typicalapplication setting is also more specific: given that awriter has produced a text (e.g.
a sentence in a wordprocessor), it is possible to check the correctness ofeach occurrence of a word known to be part of a pairor triple of confusables.We performed a series of experiments on dis-ambiguating nine frequent confusables in Englishadopted from (Golding and Roth, 1999).
We em-ployed an experimental setting in which we use thesame experimental data as before, in which only ex-amples of the confusable words are drawn ?
notethat we ignore possible confusable errors in bothtraining and test set.
This data set generation proce-dure reduces the amount of examples considerably.Despite having over 130 million words in TRAIN-REUTERS, frequent words such as there and thanoccur just over 100,000 times.
To be able to runlearning curves with more than this relatively smallamount of examples, we expanded our training ma-terial with the New York Times of 1994 to 2002(henceforth TRAIN-NYT), part of the English Gi-gaword collection published by the Linguistic DataConsortium, offering 1,096,950,281 tokens.As a first illustration of the experimental out-comes, we focus on the three-way confusable there?
their ?
they?re for which we trained one classi-fier, which we henceforth refer to as a confusableexpert.
The learning curve results of this confus-able expert are displayed in Figure 3 as the top threegraphs.
The logarithmic x-axis displays the fullnumber of instances from TRAIN-REUTERS up to130.3 million examples, and from TRAIN-NYT afterthis point.
Counter to the learning curves in the all-words prediction experiments, and to the observa-tion by (Banko and Brill, 2001), the learning curvesof this confusable triple in the three different datasets flatten, and converge, remarkably, to a roughlysimilar score of about 98%.
The convergence onlyoccurs after examples from TRAIN-NYT are added.020406080100100  1000  10000  100000  1e+06  1e+07  1e+08  1e+09wordpredictionaccuracytraining examplesthere / their / they?reconfusible expertgeneric word predictorReuters NYTBrownAliceReutersFigure 3: Learning curves in terms of word predic-tion accuracy on deciding between the confusablepair there, their, and they?re, by IGTREE trainedon TRAIN-REUTERS, and tested on REUTERS, AL-ICE, and BROWN.
The top graphs are accuracies at-tained by the confusable expert; the bottom graphsare attained by the all-words predictor trained onTRAIN-REUTERS until 130 million examples, andon TRAIN-NYT beyond (marked by the vertical bar).In the bottom of the same Figure 3 we have alsoplotted the word prediction accuracies on the threewords there, their, and they?re attained by the all-words predictor described in the previous section onthe three test sets.
The accuracies, or rather recallfigures (i.e.
the percentage of occurrences of thethree words in the test sets which are correctly pre-dicted as such), are considerably lower than those onthe confusable disambiguation task.Table 2 presents the experimental results obtainedon nine confusable sets when training and testing onReuters material.
The third column lists the accu-racy (or recall) scores of the all-words word predic-tion system at the maximal training set size of 30million labeled examples.
The fourth columns liststhe accuracies attained by the confusable expert forthe particular confusable pair or triple, measured at30 million training examples, from which each par-ticular confusable expert?s examples are extracted.The amount of examples varies for the selected con-fusable sets, as can be seen in the second column.Scores attained by the all-words predictor onthese words vary from below 10% for relatively low-frequent words to around 60% for the more frequentconfusables; the latter numbers are higher than the30Accuracy (%) byNumber of all-words confus.Confusable set examples prediction expertcite - site - sight 2,286 0.0 100.0accept - except 3,833 46.2 76.9affect - effect 4,640 7.7 87.9fewer - less 6,503 4.7 95.2among - between 27,025 18.9 96.7I - me 28,835 55.9 98.0than - then 31,478 59.4 97.2there - their - they?re 58,081 23.1 96.8to - too - two 553,453 60.6 93.4Table 2: Disambiguation scores on nine confusableset, attained by the all-words prediction classifiertrained on 30 million examples of TRAIN-REUTERS,and by confusable experts on the same training set.The second column displays the number of exam-ples of each confusable set in the 30-million wordtraining set; the list is ordered on this column.overall accuracy of this system on REUTERS.
Nev-ertheless they are considerably lower than the scoresattained by the confusable disambiguation classi-fiers, while being trained on many more examples(i.e., all 30 million available).
Most of the confus-able disambiguation classifiers attain accuracies ofwell above 90%.When the learning curves are continued beyondTRAIN-REUTERS into TRAIN-NYT, about a thou-sand times as many training examples can be gath-ered as training data for the confusable experts.
Ta-ble 3 displays the nine confusable expert?s scores af-ter being trained on examples extracted from a totalof one billion words of text, measured on all threetest sets.
Apart from a few outliers, most scores areabove 90%, and more importantly, the scores on AL-ICE and BROWN do not seriously lag behind those onREUTERS; some are even better.5 Related workAs remarked in the cases reported in the literature di-rectly related to the current article, word predictionis a core task to natural language processing, and oneof the few that takes no annotation layer to providedata for supervised machine learning and probabilis-tic modeling (Golding and Roth, 1999; Even-ZoharAccuracy on test set (%)Confusable set REUTERS ALICE BROWNcite - site - sight 100.0 100.0 69.0accept - except 84.6 100.0 97.0affect - effect 92.3 100.0 89.5fewer - less 90.5 100.0 97.2among - between 94.4 77.8 74.4I - me 99.0 98.3 98.3than - then 97.2 92.9 95.8there - their - they?re 98.1 97.8 97.3to - too - two 94.3 93.4 92.9Table 3: Disambiguation scores on nine confusableset, attained by confusable experts trained on ex-amples extracted from 1 billion words of text fromTRAIN-REUTERS plus TRAIN-NYT, on the three testsets.and Roth, 2000; Banko and Brill, 2001).
Our dis-crete, classificatio-nased approach has the same goalas probabilistic methods for language modeling forautomatic speech recognition (Jelinek, 1998), and isalso functionally equivalent to n-gram models withback-off smoothing (Zavrel and Daelemans, 1997).The papers by Golding and Roth, and Banko andBrill on confusable correction focus on the morecommon type of than/then confusion that occurs alot in the process of text production.
Both pairs ofauthors use the confusable correction task to illus-trate scaling issues, as we have.
Golding and Rothillustrate that multiplicative weight-updating algo-rithms such as Winnow can deal with immense in-put feature spaces, where for each single classifica-tion only a small number of features is actually rel-evant (Golding and Roth, 1999).
With IGTREE wehave an arguably competitive efficient, but one-shotlearning algorithm; IGTREE does not need an itera-tive procedure to set weights, and can also handle alarge feature space.
Instead of viewing all positionalfeatures as containers of thousands of atomic wordfeatures, it treats the positional features as the basictests, branching on the word values in the tree.More generally, as a precursor to the above-mentioned work, confusable disambiguation hasbeen investigated in a string of papers discussing theapplication of various machine learning algorithmsto the task (Yarowsky, 1994; Golding, 1995; Mangu31and Brill, 1997; Huang and Powers, 2001).6 DiscussionIn this article we explored the scaling abilities ofIGTREE, a simple decision-tree algorithm with fa-vorable asymptotic complexities with respect tomulti-label classification tasks.
IGTREE is appliedto word prediction, a task for which virtually un-limited amounts of training examples are available,with very large amounts of predictable class labels;and confusable disambiguation, a specialization ofword prediction focusing on small sets of confusablewords.
Best results are 42.2% correctly predicted to-kens (words and punctuation markers) when trainingand testing on data from the Reuters newswire cor-pus; and confusable disambiguation accuracies ofwell above 90%.
Memory requirements and speedswere shown to be realistic.Analysing the results of the learning curve experi-ments with increasing amounts of training examples,we observe that better word prediction accuracy canbe attained simply by adding more training exam-ples, and that the progress in accuracy proceeds at alog-linear rate.
The best rate we observed was an 8%increase in performance every tenfold multiplicationof the number of training examples, when trainingand testing on the same data.Despite the fact that all-words prediction lags be-hind in disambiguating confusibles, in comparisonwith classifiers that are focused on disambiguatingsingle sets of confusibles, we see that this lag is onlyrelative to the amount of training material available.AcknowledgementsThis research was funded by the Netherlands Organ-isation for Scientific Research (NWO).
The authorwishes to thank Ko van der Sloot for programmingassistance.ReferencesM.
Banko and E. Brill.
2001.
Scaling to very very largecorpora for natural language disambiguation.
In Pro-ceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics, pages 26?33.
Associa-tion for Computational Linguistics.L.
Carroll.
1865.
Alice?s Adventures in Wonderland.Project Gutenberg.W.
Daelemans, A.
Van den Bosch, and A. Weijters.
1997.IGTree: using trees for compression and classificationin lazy learning algorithms.
Artificial Intelligence Re-view, 11:407?423.W.
Daelemans, A.
Van den Bosch, and J. Zavrel.
1999.Forgetting exceptions is harmful in language learning.Machine Learning, Special issue on Natural LanguageLearning, 34:11?41.Y.
Even-Zohar and D. Roth.
2000.
A classification ap-proach to word prediction.
In Proceedings of the FirstNorth-American Conference on Computational Lin-guistics, pages 124?131, New Brunswick, NJ.
ACL.A.R.
Golding and D. Roth.
1999.
A Winnow-Based Ap-proach to Context-Sensitive Spelling Correction.
Ma-chine Learning, 34(1?3):107?130.A.
R. Golding.
1995.
A Bayesian hybrid method forcontext-sensitive spelling correction.
In Proceedingsof the 3rd workshop on very large corpora, ACL-95.J.
H. Huang and D. W. Powers.
2001.
Large scale ex-periments on correction of confused words.
In Aus-tralasian Computer Science Conference Proceedings,pages 77?82, Queensland AU.
Bond University.F.
Jelinek.
1998.
Statistical Methods for Speech Recog-nition.
The MIT Press, Cambridge, MA.D.
E. Knuth.
1973.
The art of computer programming,volume 3: Sorting and searching.
Addison-Wesley,Reading, MA.H.
Kuc?era and W. N. Francis.
1967.
ComputationalAnalysis of Present-Day American English.
BrownUniversity Press, Providence, RI.L.
Mangu and E. Brill.
1997.
Automatic rule acquisitionfor spelling correction.
In Proceedings of the Interna-tional Conference on Machine Learning, pages 187?194.M.
Marcus, S. Santorini, and M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English:the Penn Treebank.
Computational Linguistics,19(2):313?330.J.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann, San Mateo, CA.D.
Wu, Z. Sui, and J. Zhao.
1999.
An information-basedmethod for selecting feature types for word predic-tion.
In Proceedings of the Sixth European Confer-ence on Speech Communication and Technology, EU-ROSPEECH?99, Budapest.D.
Yarowsky.
1994.
Decision lists for lexical ambiguityresolution: application to accent restoration in Spanishand French.
In Proceedings of the Annual Meeting ofthe ACL, pages 88?95.J.
Zavrel and W. Daelemans.
1997.
Memory-basedlearning: Using similarity for smoothing.
In Proceed-ings of the 35th Annual Meeting of the Association forComputational Linguistics, pages 436?443.32
