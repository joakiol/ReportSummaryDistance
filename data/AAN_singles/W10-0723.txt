Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 152?158,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsShedding (a Thousand Points of) Light on Biased LanguageTae YanoSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAtaey@cs.cmu.eduPhilip ResnikDepartment of Linguistics and UMIACSUniversity of MarylandCollege Park, MD 20742, USAresnik@umiacs.umd.eduNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractThis paper considers the linguistic indicators of biasin political text.
We used Amazon Mechanical Turkjudgments about sentences from American politicalblogs, asking annotators to indicate whether a sen-tence showed bias, and if so, in which political di-rection and through which word tokens.
We alsoasked annotators questions about their own politicalviews.
We conducted a preliminary analysis of thedata, exploring how different groups perceive bias indifferent blogs, and showing some lexical indicatorsstrongly associated with perceived bias.1 IntroductionBias and framing are central topics in the study of com-munications, media, and political discourse (Scheufele,1999; Entman, 2007), but they have received relativelylittle attention in computational linguistics.
What are thelinguistic indicators of bias?
Are there lexical, syntactic,topical, or other clues that can be computationally mod-eled and automatically detected?Here we use Amazon Mechanical Turk (MTurk) to en-gage in a systematic, empirical study of linguistic indi-cators of bias in the political domain, using text drawnfrom political blogs.
Using the MTurk framework, wecollected judgments connected with the two dominantschools of thought in American politics, as exhibited insingle sentences.
Since no one person can claim to be anunbiased judge of political bias in language, MTurk is anattractive framework that lets us measure perception ofbias across a population.2 Annotation TaskWe drew sentences from a corpus of American politicalblog posts from 2008.
(Details in Section 2.1.)
Sentenceswere presented to participants one at a time, without con-text.
Participants were asked to judge the following (seeFigure 1 for interface design):?
To what extent a sentence or clause is biased (none,somewhat, very);?
The nature of the bias (very liberal, moderately lib-eral, moderately conservative, very conservative, bi-ased but not sure which direction); and?
Which words in the sentence give away the author?sbias, similar to ?rationale?
annotations in Zaidan etal.
(2007).For example, a participant might identify a moderateliberal bias in this sentence,Without Sestak?s challenge, we would haveSpecter, comfortably ensconced as a Democratin name only.adding checkmarks on the underlined words.
A moreneutral paraphrase is:Without Sestak?s challenge, Specter wouldhave no incentive to side more frequently withDemocrats.It is worth noting that ?bias,?
in the sense we are us-ing it here, is distinct from ?subjectivity?
as that topichas been studied in computational linguistics.
Wiebeet al (1999) characterize subjective sentences as thosethat ?are used to communicate the speaker?s evaluations,opinions, and speculations,?
as distinguished from sen-tences whose primary intention is ?to objectively com-municate material that is factual to the reporter.?
In con-trast, a biased sentence reflects a ?tendency or preferencetowards a particular perspective, ideology or result.
?1 Asubjective sentence can be unbiased (I think that moviewas terrible), and a biased sentence can purport to com-municate factually (Nationalizing our health care system1http://en.wikipedia.org/wiki/Bias as of 13 April,2010.152is a point of no return for government interference in thelives of its citizens2).In addition to annotating sentences, each participantwas asked to complete a brief questionnaire about his orher own political views.
The survey asked:1.
Whether the participant is a resident of the UnitedStates;2. Who the participant voted for in the 2008 U.S.presidential election (Barack Obama, John McCain,other, decline to answer);3.
Which side of political spectrum he/she identifiedwith for social issues (liberal, conservative, declineto answer); and4.
Which side of political spectrum he/she identifiedwith for fiscal/economic issues (liberal, conserva-tive, decline to answer).This information was gathered to allow us to measurevariation in bias perception as it relates to the stance ofthe annotator, e.g., whether people who view themselvesas liberal perceive more bias in conservative sources, andvice versa.2.1 DatasetWe extracted our sentences from the collection of blogposts in Eisenstein and Xing (2010).
The corpus con-sists of 2008 blog posts gathered from six sites focusedon American politics:?
American Thinker (conservative),3?
Digby (liberal),4?
Hot Air (conservative),5?
Michelle Malkin (conservative),6?
Think Progress (liberal),7 and?
Talking Points Memo (liberal).813,246 posts were gathered in total, and 261,073 sen-tences were extracted using WebHarvest9 and OpenNLP1.3.0.10 Conservative and liberal sites are evenly rep-resented (130,980 sentences from conservative sites,130,093 from liberal sites).
OpenNLP was also used fortokenization.2Sarah Palin, http://www.facebook.com/note.php?note_id=113851103434, August 7, 2009.3http://www.americanthinker.com4http://digbysblog.blogspot.com5http://hotair.com6http://michellemalkin.com7http://thinkprogress.org8http://www.talkingpointsmemo.com9http://web-harvest.sourceforge.net10http://opennlp.sourceforge.netLiberal Conservativethinkprogress org exit questionvideo thinkprogress hat tipet rally ed laskyorg 2008 hot airgi bill tony rezkowonk room ed morrisseydana perino track recordphil gramm confirmed deadsenator mccain american thinkerabu ghraib illegal alienTable 1: Top ten ?sticky?
partisan bigrams for each side.2.2 Sentence SelectionTo support exploratory data analysis, we sought a di-verse sample of sentences for annotation, but we werealso guided by some factors known or likely to correlatewith bias.
We extracted sentences from our corpus thatmatched at least one of the categories below, filtering tokeep those of length between 8 and 40 tokens.
Then, foreach category, we first sampled 100 sentences without re-placement.
We then randomly extracted sentences up to1,100 from the remaining pool.
We selected the sentencesthis way so that the collection has variety, while includingenough examples for individual categories.
Our goal wasto gather at least 1,000 annotated sentences; ultimatelywe collected 1,041.
The categories are as follows.?Sticky?
partisan bigrams.
One likely indicator ofbias is the use of terms that are particular to one side orthe other in a debate (Monroe et al, 2008).
In order toidentify such terms, we independently created two listsof ?sticky?
(i.e., strongly associated) bigrams in liberaland conservative subcorpora, measuring association us-ing the log-likelihood ratio (Dunning, 1993) and omittingbigrams containing stopwords.11 We identified a bigramas ?liberal?
if it was among the top 1,000 bigrams fromthe liberal blogs, as measured by strength of association,and was also not among the top 1,000 bigrams on the con-servative side.
The reverse definition yielded the ?conser-vative?
bigrams.
The resulting liberal list contained 495bigrams, and the conservative list contained 539.
We thenmanually filtered cases that were clearly remnant HTMLtags and other markup, arriving at lists of 433 and 535,respectively.
Table 1 shows the strongest weighted bi-grams.As an example, consider this sentence (with a preced-ing sentence of context), which contains gi bill.
There isno reason to think the bigram itself is inherently biased(in contrast to, for example, death tax, which we would11We made use of Pedersen?s N -gram Statistics Package (Banerjeeand Pedersen, 2003).153perceive as biased in virtually any unquoted context), butwe do perceive bias in the full sentence.Their hard fiscal line softens in the face ofAmerican imperialist adventures.
According toCongressDaily the Bush dogs are also whiningbecause one of their members, Stephanie Her-seth Sandlin, didn?t get HERGI Bill to the floorin favor of Jim Webb?s .Emotional lexical categories.
Emotional words mightbe another indicator of bias.
We extracted four categoriesof words from Pennebaker?s LIWC dictionary: Nega-tive Emotion, Positive Emotion, Causation, and Anger.12The following is one example of a biased sentence in ourdataset that matched these lexicons, in this case the Angercategory; the match is in bold.A bunch of ugly facts are nailing the biggestscare story in history.The five most frequent matches in the corpus for eachcategory are as follows.13Negative Emotion: war attack* problem* numb* argu*Positive Emotion: like well good party* secur*Causation: how because lead* make whyAnger: war attack* argu* fight* threat*Kill verbs.
Greene and Resnik (2009) discuss the rel-evance of syntactic structure to the perception of senti-ment.
For example, their psycholinguistic experimentswould predict that when comparing Millions of peoplestarved under Stalin (inchoative) with Stalin starved mil-lions of people (transitive), the latter will be perceived asmore negative toward Stalin, because the transitive syn-tactic frame tends to be connected with semantic prop-erties such as intended action by the subject and changeof state in the object.
?Kill verbs?
provide particularlystrong examples of such phenomena, because they ex-hibit a large set of semantic properties canonically as-sociated with the transitive frame (Dowty, 1991).
Thestudy by Greene and Resnik used 11 verbs of killing andsimilar action to study the effect of syntactic ?packag-ing?
on perceptions of sentiment.14 We included mem-bership on this list (in any morphological form) as a se-lection criterion, both because these verbs may be likely12http://www.liwc.net.
See Pennebaker et al (2007) for de-tailed description of background theory, and how these lexicons wereconstructed.
Our gratitude to Jamie Pennebaker for the use of this dic-tionary.13Note that some LIWC lexical entries are specified as pre-fixes/stems, e.g.
ugl*, which matches ugly uglier, etc.14The verbs are: kill, slaughter, assassinate, shoot, poison, strangle,smother, choke, drown, suffocate, and starve.to appear in sentences containing bias (they overlap sig-nificantly with Pennebaker?s Negative Emotion list), andbecause annotation of bias will provide further data rel-evant to Greene and Resnik?s hypothesis about the con-nections among semantic propeties, syntactic structures,and positive or negative perceptions (which are stronglyconnected with bias).In our final 1,041-sentence sample, ?sticky bigrams?occur 235 times (liberal 113, conservative 122), the lexi-cal category features occur 1,619 times (Positive Emotion577, Negative Emotion 466, Causation 332, and Anger244), and ?kill?
verbs appear as a feature in 94 sentences.Note that one sentence often matches multiple selectioncriteria.
Of the 1,041-sentence sample, 232 (22.3%) arefrom American Thinker, 169 (16.2%) from Digby, 246(23.6%) from Hot Air, 73 (7.0%) from Michelle Malkin,166 (15.9%) from Think Progress, and 155 (14.9%) fromTalking Points Memo.3 Mechanical Turk ExperimentWe prepared 1,100 Human Intelligence Tasks (HITs),each containing one sentence annotation task.
1,041 sen-tences were annotated five times each (5,205 judgementstotal).
One annotation task consists of three bias judge-ment questions plus four survey questions.
We pricedeach HIT between $0.02 and $0.04 (moving from lessto more to encourage faster completion).
The total costwas $212.15 We restricted access to our tasks to thosewho resided in United States and who had above 90% ap-proval history, to ensure quality and awareness of Amer-ican political issues.
We also discarded HITs annotatedby workers with particularly low agreement scores.
Thetime allowance for each HIT was set at 5 minutes.3.1 Annotation Results3.1.1 Distribution of JudgmentsOverall, more than half the judgments are ?not biased,?and the ?very biased?
label is used sparingly (Table 2).There is a slight tendency among the annotators to assignthe ?very conservative?
label, although moderate bias isdistributed evenly on both side (Table 3).
Interestingly,there are many ?biased, but not sure?
labels, indicatingthat the annotators are capable of perceiving bias (or ma-nipulative language), without fully decoding the intent ofthe author, given sentences out of context.Bias 1 1.5 2 2.5 3% judged 36.0 26.6 25.5 9.4 2.4Table 2: Strength of perceived bias per sentence, averaged overthe annotators (rounded to nearest half point).
Annotators ratebias on a scale of 1 (no bias), 2 (some bias), and 3 (very biased).15This includes the cost for the discarded annotations.154Figure 1: HIT: Three judgment questions.
We first ask for the strength of bisa, then the direction.
For the word-level annotationquestion (right), workers are asked to check the box to indicate the region which ?give away?
the bias.Bias type VL ML NB MC VC B% judged 4.0 8.5 54.8 8.2 6.7 17.9Table 3: Direction of perceived bias, per judgment (very lib-eral, moderately liberal, no bias, moderately conservative, veryconservative, biased but not sure which).EconomicL M C NASocialL 20.1 10.1 4.9 0.7M 0.0 21.9 4.7 0.0C 0.1 0.4 11.7 0.0NA 0.1 0.0 11.2 14.1Table 4: Distribution of judgements by annotators?
self-identification on social issues (row) and fiscal issue (column);{L, C, M, NA} denote liberal, conservative, moderate, and de-cline to answer, respectively.3.1.2 Annotation QualityIn this study, we are interested in where the wisdom ofthe crowd will take us, or where the majority consensuson bias may emerge.
For this reason we did not contrive agold standard for ?correct?
annotation.
We are, however,mindful of its overall quality?whether annotations havereasonable agreement, and whether there are fraudulentresponses tainting the results.To validate our data, we measured the pair-wise Kappastatistic (Cohen, 1960) among the 50 most frequent work-ers16 and took the average over all the scores.17.
Theaverage of the agreement score for the first question is0.55, and the second 0.50.
Those are within the range ofreasonable agreement for moderately difficult task.
Wealso inspected per worker average scores for frequentworkers18 and found one with consistently low agreementscores.
We discarded all the HITs by this worker from ourresults.
We also manually inspected the first 200 HITs forapparent frauds.
The annotations appeared to be consis-tent.
Often annotators agreed (many ?no bias?
cases wereunanimous), or differed in only the degree of strength(?very biased?
vs.
?biased?)
or specificity (?biased but Iam not sure?
vs. ?moderately liberal?).
The direction ofbias, if specified, was very rarely inconsistent.Along with the annotation tasks, we asked workershow we could improve our HITs.
Some comments were16258 workers participated; only 50 of them completed more than 10annotations.17Unlike traditional subjects for a user-annotation study, our annota-tors have not judged all the sentences considered in the study.
There-fore, to compute the agreement, we considered only the case where twoannotators share 20 or more sentences.18We consider only those with 10 or more annotations.155insightful for our study (as well as for the interface de-sign).
A few pointed out that an impolite statement ora statement of negative fact is not the same as bias, andtherefore should be marked separately from bias.
Othersmentioned that some sentences are difficult to judge outof context.
These comments will be taken into account infuture research.4 Analysis and SignificanceIn the following section we report some of the interestingtrends we found in our annotation results.
We consider afew questions and report the answers the data provide foreach.4.1 Is a sentence from a liberal blog more likely beseen as liberal?In our sample sentence pool, conservatives and liberalsare equally represented, though each blog site has a dif-ferent representation.19 We grouped sentences by sourcesite, then computed the percentage representation of eachsite within each bias label; see Table 5.
In the top row,we show the percentage representation of each group inoverall judgements.In general, a site yields more sentences that match itsknown political leanings.
Note that in our annotationtask, we did not disclose the sentence?s source to theworkers.
The annotators formed their judgements solelybased on the content of the sentence.
This result canbe taken as confirming people?s ability to perceive biaswithin a sentence, or, conversely, as confirming our a pri-ori categorizations of the blogs.at ha mm db tp tpmOverall 22.3 23.6 7.0 16.2 15.9 14.9NB 23.7 22.3 6.1 15.7 17.0 15.3VC 24.8 32.3 19.3 6.9 7.5 9.2MC 24.4 33.6 8.0 8.2 13.6 12.2ML 16.6 15.2 3.4 21.1 22.9 20.9VL 16.7 9.0 4.3 31.0 22.4 16.7B 20.1 25.4 7.2 19.5 12.3 13.7Table 5: Percentage representation of each site within bias labelpools from question 2 (direction of perceived bias): very liberal,moderately liberal, no bias, moderately conservative, very con-servative, biased but not sure which.
Rows sum to 100.
Bold-face indicates rates higher than the site?s overall representationin the pool.4.2 Does a liberal leaning annotator see moreconservative bias?In Table 5, we see that blogs are very different from eachother in terms of the bias annotators perceive in their lan-19Posts appear on different sites at different rates.131032100very conservative no bias very liberalnot sureLLMMCCOverallFigure 2: Distribution of bias labels (by judgment) for socialand economic liberals (LL), social and economic moderates(MM), and social and economic conservatives (CC), and over-all.
Note that this plot uses a logarithmic scale, to tease apartthe differences among groups.guage.
In general, conservative sites seemingly producedmuch more identifiable partisan bias than liberal sites.20This impression, however, might be an artifact of thedistribution of the annotators?
own bias.
As seen in Ta-ble 4, a large portion of our annotators identified them-selves as liberal in some way.
People might call a state-ment biased if they disagree with it, while showing le-niency toward hyperbole more consistent with their opin-ions.To answer this question, we break down the judgementlabels by the annotators?
self-identification, and checkthe percentage of each bias type within key groups (seeFigure 2).
In general, moderates perceive less bias thanpartisans (another useful reality check, in the sense thatthis is to be expected), but conservatives show a muchstronger tendency to label sentences as biased, in bothdirections.
(We caution that the underrepresentation ofself-identifying conservatives in our worker pool meansthat only 608 judgments from 48 distinct workers wereused to estimate these statistics.)
Liberals in this sampleare less balanced, perceiving conservative bias at doublethe rate of liberal bias.4.3 What are the lexical indicators of perceivedbias?For a given word type w, we calculate the frequency thatit was marked as indicating bias, normalized by its totalnumber of occurrences.
To combine the judgments of dif-ferent annotators, we increment w?s count by k/n when-ever k judgments out of n marked the word as showingbias.
We perform similar calculations with a restrictionto liberal and conservative judgments on the sentence as a20Liberal sites cumulatively produced 64.9% of the moderately lib-eral bias label and 70.1 % of very liberal, while conservative sites pro-duced 66.0% of moderately conservative and 76.4% of very conserva-tive, respectively.156Overall Liberal Conservative Not Sure Whichbad 0.60 Administration 0.28 illegal 0.40 pass 0.32personally 0.56 Americans 0.24 Obama?s 0.38 bad 0.32illegal 0.53 woman 0.24 corruption 0.32 sure 0.28woman 0.52 single 0.24 rich 0.28 blame 0.28single 0.52 personally 0.24 stop 0.26 they?re 0.24rich 0.52 lobbyists 0.23 tax 0.25 happen 0.24corruption 0.52 Republican 0.22 claimed 0.25 doubt 0.24Administration 0.52 union 0.20 human 0.24 doing 0.24Americans 0.51 torture 0.20 doesn?t 0.24 death 0.24conservative 0.50 rich 0.20 difficult 0.24 actually 0.24doubt 0.48 interests 0.20 Democrats 0.24 exactly 0.22torture 0.47 doing 0.20 less 0.23 wrong 0.22Table 6: Most strongly biased words, ranked by relative frequency of receiving a bias mark, normalized by total frequency.
Onlywords appearing five times or more in our annotation set are ranked.whole.
Top-ranked words for each calculation are shownin Table 6.Some of the patterns we see are consistent with whatwe found in our automatic method for proposing biasedbigrams.
For example, the bigrams tended to includeterms that refer to members or groups on the opposingside.
Here we find that Republican and Administration(referring in 2008 to the Bush administration) tends toshow liberal bias, while Obama?s and Democrats showconservative bias.5 Discussion and Future WorkThe study we have conducted here represents an initialpass at empirical, corpus-driven analysis of bias using themethods of computational linguistics.
The results thus farsuggest that it is possible to automatically extract a sam-ple that is rich in examples that annotators would con-sider biased; that na?
?ve annotators can achieve reason-able agreement with minimal instructions and no train-ing; and that basic exploratory analysis of results yieldsinterpretable patterns that comport with prior expecta-tions, as well as interesting observations that merit furtherinvestigation.In future work, enabled by annotations of biased andnon-biased material, we plan to delve more deeply intothe linguistic characteristics associated with biased ex-pression.
These will include, for example, an analysisof the extent to which explicit ?lexical framing?
(use ofpartisan terms, e.g., Monroe et al, 2008) is used to con-vey bias, versus use of more subtle cues such as syntacticframing (Greene and Resnik, 2009).
We will also explorethe extent to which idiomatic usages are connected withbias, with the prediction that partisan ?memes?
tend to bemore idiomatic than compositional in nature.In our current analysis, the issue of subjectivity was notdirectly addressed.
Previous work has shown that opin-ions are closely related to subjective language (Pang andLee, 2008).
It is possible that asking annotators aboutsentiment while asking about bias would provide a deeperunderstanding of the latter.
Interestingly, annotator feed-back included remarks that mere negative ?facts?
do notconvey an author?s opinion or bias.
The nature of subjec-tivity as a factor in bias perception is an important issuefor future investigation.6 ConclusionThis paper considered the linguistic indicators of bias inpolitical text.
We used Amazon Mechanical Turk judg-ments about sentences from American political blogs,asking annotators to indicate whether a sentence showedbias, and if so, in which political direction and throughwhich word tokens; these data were augmented by a po-litical questionnaire for each annotator.
Our preliminaryanalysis suggests that bias can be annotated reasonablyconsistently, that bias perception varies based on personalviews, and that there are some consistent lexical cues forbias in political blog data.AcknowledgmentsThe authors acknowledge research support from HPLabs, help with data from Jacob Eisenstein, and help-ful comments from the reviewers, Olivia Buzek, MichaelHeilman, and Brendan O?Connor.ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
The design, implementa-tion and use of the ngram statistics package.
In the Fourth Interna-tional Conference on Intelligent Text Processing and ComputationalLinguistics.Jacob Cohen.
1960.
A coefficient of agreement for nominal scales.Educational and Psychological Measurement, 20(1):37?46.David Dowty.
1991.
Thematic Proto-Roles and Argument Selection.Language, 67:547?619.157Ted Dunning.
1993.
Accurate methods for the statistics of surprise andcoincidence.
Computational Linguistics, 19(1):61?74.Jacob Eisenstein and Eric Xing.
2010.
The CMU 2008 political blogcorpus.
Technical report CMU-ML-10-101.Robert M. Entman.
2007.
Framing bias: Media in the distribution ofpower.
Journal of Communication, 57(1):163?173.Stephan Greene and Philip Resnik.
2009.
More than words: Syntacticpackaging and implicit sentiment.
In NAACL, pages 503?511, June.Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn.
2008.Fightin?
words: Lexical feature selection and evaluation for identi-fying the content of political conflict.
Political Analysis, 16(4):372?403, October.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.Foundations and Trends in Information Retrieval, 2(1-2):1?135.J.W.
Pennebaker, C.K Chung, M. Ireland, A Gonzales, and R J. Booth,2007.
The development and psychometric properties of LIWC2007.Dietram A. Scheufele.
1999.
Framing as a theory of media effects.Journal of Communication, 49(1):103?122.Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O?Hara.
1999.Development and use of a gold standard data set for subjectivityclassifications.
In Proceedings of the Association for ComputationalLinguistics (ACL), pages 246?253.Omar Zaidan, Jason Eisner, and Christine Piatko.
2007.
Using ?anno-tator rationales?
to improve machine learning for text categorization.In NAACL, pages 260?267, April.158
