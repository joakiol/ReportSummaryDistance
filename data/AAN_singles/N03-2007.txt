Active Learning for Classifying Phone Sequences from UnsupervisedPhonotactic ModelsShona DouglasAT&T Labs - ResearchFlorham Park, NJ 07932, USAshona@research.att.comAbstractThis paper describes an application of activelearning methods to the classification of phonestrings recognized using unsupervised phono-tactic models.
The only training data requiredfor classification using these recognition meth-ods is assigning class labels to the audio files.The work described here demonstrates thatsubstantial savings in this effort can be ob-tained by actively selecting examples to be la-beled using confidence scores from the Boos-Texter classifier.
The saving in class label-ing effort is evaluated on two different spo-ken language system domains in terms both ofthe number of utterances to be labeled and thelength of the labeled utterances in phones.
Weshow that savings in labeling effort of around30% can be obtained using active selection ofexamples.1 IntroductionA major barrier to the rapid and cost-effective develop-ment of spoken language processing applications is theneed for time-consuming and expensive human transcrip-tion and annotation of collected data.
Extensive transcrip-tion of audio is generally undertaken to provide word-level labeling to train recognition models.
Applicationsthat use statistically trained classification as a componentof an understanding system also require this transcribedtext to train on, plus an assignment of class labels to eachutterance.In recent work by Alshawi (2003) reported in this con-ference, new methods for unsupervised training of phonestring recognizers have been developed, removing theneed for word-level transcription.
The phone-string out-put of such recognizers has been used in classificationtasks using the BoosTexter text classification algorithm,giving utterance classfication accuracy that is surprisinglyclose to that obtained using conventionally trained wordtrigram models requiring transcription.
The only train-ing data required for classification using these recogni-tion methods is assigning class labels to the audio files.The aim of the work described in this paper is to amplifythis advantage by reducing the amount of effort requiredto train classifiers for phone-based systems by activelyselecting which utterances to assign class labels.
Activelearning has been applied to classification problems be-fore (McCallum and Nigam, 1998; Tur et al, 2003), butnot to classifiying phone strings.2 Unsupervised Phone RecognitionUnsupervised recognition of phone sequences is car-ried out according to the method described byAlshawi (2003).
In this method, the training inputs torecognition model training are simply the set of audiofiles that have been recorded from the application.The recognition training phase is an iterative procedurein which a phone n-gram model is refined successively:The phone strings resulting from the current pass over thespeech files are used to construct the phone n-gram modelfor the next iteration.
We currently only re-estimate the n-gram model, so the same general-purpose HMM acousticmodel is used for ASR decoding in all iterations.Recognition training can be briefly described as fol-lows.
First, set the phone sequence model to an initialphone string model.
This initial model used can be anunweighted phone loop or a general purpose phonotac-tic model for the language being recognized.
Then, forsuccessively larger n-grams, produce the output set ofphone sequences from recognizing the training speechfiles with the current phone sequence model, and train thenext larger n-gram phone sequence model on this outputcorpus.3 Training phone sequence classifiers withactive selection of examplesThe method we use for training the phone sequence clas-sifier is as follows.1.
Choose an initial subset S of training recordings atrandom; assign class label(s) to each example.2.
Recognize these recordings using the phone recog-nizer described in section 2.3.
Train an initial classifier C on the pairs (phonestring, class label) of S.4.
Run the classifier on the recognized phone strings ofthe training corpus, obtaining confidence scores foreach classification.5.
While labeling effort is available, or until per-formance on a development corpus reaches somethreshold,(a) Choose the next subset S?
of examples from ofthe training corpus, on the basis of the confi-dence scores or other indicators.
(Selection cri-teria are discussed later.
)(b) Assign class label(s) to each selected example.
(c) Train classifier C ?
on all the data labeled so far.
(d) Run C ?
on the whole training corpus, obtainingconfidence scores for each classification.
(e) Optionally test C ?
on a separate test corpus.4 Experimental SetupThe datasets tested on and the classifier used are the sameas those in the experiments on phone sequence classifica-tion reported by Alshawi (2003).
The details are brieflyrestated here.4.1 DataTwo collections of utterances from two domains wereused in the experiments:1.
Customer care utterances (HMIHY).
These utter-ances are the customer side of live English conversationsbetween AT&T residential customers and an automatedcustomer care system.
This system is open to the publicso the number of speakers is large (several thousand).The total number of training utterances was 40,106.All tests use 9724 test utterances.
Average utterancelength was 11.19 words; there were 56 classes, with anaverage of 1.09 classes per utterance.2.
Text-to-Speech Help Desk utterances (TTSHD).This is a smaller database of utterances in which cus-tomers called an automated information system primar-ily to find out about AT&T Natural Voices text-to-speechsynthesis products.The total number of possible training utterances was10,470.
All tests use 5005 test utterances.
Average utter-ance length was 3.95 words; there were 54 classes, withan average of 1.23 classes per utterance.4.2 Phone sequencesThe phone sequences used for testing and training arethose obtained using the phone recognizer described insection 2.
Since the phone recognizer is trained with-out labeling of any sort, we can use all available train-ing utterances to train it, that is, 40,106 in the HMIHYdomain and 10,470 in the TTSHD domain.
The initialmodel used to start the iteration is, as in (Alshawi, 2003),an unweighted phone loop.4.3 ClassifierFor the experiments reported here we use the BoosT-exter classifier (Schapire and Singer, 2000).
The fea-tures used were identifiers corresponding to prompts, andphone n-grams up to length 4.
Following Schapire andSinger (2000), the confidence level for a given predictionis taken to be the difference between the scores assignedby BoosTexter to the highest ranked action (the predictedaction) and the next highest ranked action.4.4 Selection criteriaSubsets of the recognized phone sequences were selectedto be assigned class labels and used in training the clas-sifiers.
Examples were selected in order of BoosTex-ter confidence score, least confident first.
Further selec-tion by utterance length was also used in some experi-ments such that only recognized utterances with less thana given number of phones were selected.5 Experiments5.1 Evaluation metricsWe are interested in comparing the performance for agiven amount of labeling effort of classifiers trained onrandom selection of examples with that of classifierstrained on examples chosen according to the confidence-based method described in section 3.The basic measurements are:A(e): the classification accuracy at a given labelingeffort level e of the classifier trained on actively selectedlabeling examples.R(e): the classification accuracy at a given labelingeffort level e of the classifier trained on randomly selectedlabeling examples.A?1(R(e)): the effort required to achieve the perfor-mance of random selection at effort e, using active learn-ing.Derived from these is the main comparison we are in-terested in:Effort A R A?1(R) Effort(utt) (%) (%) (utt) Ratio2000 67.4 66.0 1128 0.564000 69.6 68.0 2678 0.67Table 1: HMIHY, no length limit, effort is number of ut-terancesEffort A R A?1(R) Effort(phn) (%) (%) (phn) Ratio68032 67.0 66.1 52940 0.78128636 69.3 67.9 91057 0.71Table 2: HMIHY, length limited, effort is number ofphonesEffortRatio(e) = A?1(R(e))/e: the proportion of theeffort that would be required to achieve the performanceof random selection at effort e, actually required usingactive learning: that is, low is good.We use two metrics for labeling effort: the numberof utterances to be labeled and the number of phones inthose utterances.
The number of phones is indicative ofthe length of the audio file that must be listened to in orderto make the class label assignment, so this is relevant toassessing just how much real effort is saved by any activelearning technique.5.2 ResultsTable 1 gives the results for selected levels of labeling ef-fort in the HMIHY domain, calculated in terms of numberof utterances labeled.These results suggest that we can achieve the sameaccuracy as random labeling with around 60% of theeffort by active selection of examples according to theconfidence-based method described in section 3.However, a closer inspection of the chosen examplesreveals that, on average, the actively selected utterancesare nearly 1.5 times longer than the random selection interms of number of phones.
(This is not suprising giventhat the classification method performs much worse onlonger utterances, and the confidence levels reflect this.
)In order to overcome this we introduce as part of the se-lection criteria a length limit of 50 phones.
This allows usto retain appreciable effort savings as shown in table 2.The TTSHD application is considerably less complexthan HMIHY, and this may be reflected in the greater sav-ings obtained using active learning.
Tables 3 and 4 showthe corresponding results for this domain.There is also a smaller variation in utterance length be-tween actively and randomly selected training examples(more like 110% than the 150% for HMIHY); table 4shows that defining effort in terms of number of phonesstill results in appreciable savings for active learning.
(In-Effort A R A?1(R) Effort(utt) (%) (%) (utt) Ratio2000 78.9 77.5 1327 0.664000 80.3 78.8 1971 0.49Table 3: TTSHD, effort is number of utterancesEffort A R A?1(R) Effort(phn) (%) (%) (phn) Ratio35877 78.9 77.9 27019 0.7571338 80.3 79.1 48267 0.68Table 4: TTSHD, effort is number of phonescorporating a length limit gave little additional benefithere.
)6 DiscussionBy actively choosing the examples with the lowest con-fidence scores first, we can get the same classificationresults with around 60-70% of the utterances labeled inHMIHY and TTSHD.
But we want to optimize labelingeffort, which is presumably some combination of a fixedamount of effort per utterance plus a ?listening effort?proportional to utterance length.
We therefore augmentedour active learning selection to include a constraint on thelength of the utterances, measured in recognized phones.If we simply take effort to be proportional to the numberof phones in the utterances selected (likely to result in aconservative estimate of savings), the effort reduction at4,000 utterances is around 30% even for the more com-plex HMIHY domain.
Further investigation is neededinto the best way to measure overall labeling effort, andinto refinements of the active learning process to optimizethat labeling effort.ReferencesH.
Alshawi.
2003.
Effective utterance classification withunsupervised phonotactic models.
In HLT-NAACL2003, Edmonton, Canada.A.
K. McCallum and K. Nigam.
1998.
Employing EMin pool-based active learning for text classification.
InProceedings of the 15th International Conference onMachine Learning, pages 350?358.R.
E. Schapire and Y.
Singer.
2000.
BoosTexter: Aboosting-based system for text categorization.
Ma-chine Learning, 39(2/3):135?168.Gokhan Tur, Robert E. Schapire, , and Dilek Hakkani-Tur.
2003.
Active learning for spoken language un-derstanding.
In Proceedings of International Con-ference on Acoustics, Speech and Signal Processing(ICASSP?03), Hong Kong, April.
(to appear).
