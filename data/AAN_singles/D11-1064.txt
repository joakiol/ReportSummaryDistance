Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691?699,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSyntactic Decision Tree LMs: Random Selection or Intelligent Design?Denis Filimonov??
?Human Language TechnologyCenter of ExcellenceJohns Hopkins Universityden@cs.umd.eduMary Harper?
?Department of Computer ScienceUniversity of Maryland, College Parkmharper@umd.eduAbstractDecision trees have been applied to a vari-ety of NLP tasks, including language mod-eling, for their ability to handle a variety ofattributes and sparse context space.
More-over, forests (collections of decision trees)have been shown to substantially outperformindividual decision trees.
In this work, we in-vestigate methods for combining trees in a for-est, as well as methods for diversifying treesfor the task of syntactic language modeling.We show that our tree interpolation techniqueoutperforms the standard method used in theliterature, and that, on this particular task, re-stricting tree contexts in a principled way pro-duces smaller and better forests, with the bestachieving an 8% relative reduction in WordError Rate over an n-gram baseline.1 IntroductionLanguage Models (LMs) are an essential part ofNLP applications that require selection of the mostfluent word sequence among multiple hypotheses.The most prominent applications include AutomaticSpeech Recognition (ASR) and Machine Transla-tion (MT).Statistical LMs formulate the problem as thecomputation of the model?s probability to gener-ate the word sequence w1, w2, .
.
.
, wm (denoted aswm1 ), assuming that higher probability correspondsto more fluent hypotheses.
LMs are often repre-sented in the following generative form:p(wm1 ) =m?i=1p(wi|wi?11 )Note the context space for this function, wi?11 is ar-bitrarily long, necessitating some independence as-sumption, which usually consists of reducing the rel-evant context to n?1 immediately preceding tokens:p(wi|wi?11 ) ?
p(wi|wi?1i?n+1) (1)These distributions are typically estimated from ob-served counts of n-grams wii?n+1 in the trainingdata.
The context space is still far too large1; there-fore, the models are recursively smoothed usinglower order distributions.
For instance, in a widelyused n-gram LM, the probabilities are estimated asfollows:p?
(wi|wi?1i?n+1) = ?
(wi|wi?1i?n+1) + (2)?
(wi?1i?n+1) ?
p?
(wi|wi?1i?n+2)where ?
is a discounted probability2.Note that this type of model is a simple Markovchain lacking any notion of syntax.
It is widelyaccepted that languages do have some structure.Moreover, it has been shown that incorporating syn-tax into a language model can improve its perfor-mance (Bangalore, 1996; Heeman, 1998; Chelbaand Jelinek, 2000; Filimonov and Harper, 2009).
Astraightforward way of incorporating syntax into alanguage model is by assigning a tag to each wordand modeling them jointly; then to obtain the proba-1O(|V |n?1) in n-gram model with typical order n =3 .
.
.
5, and a vocabulary size of |V | = 104 .
.
.
106.2We refer the reader to (Chen and Goodman, 1996) for asurvey of the discounting methods for n-gram models.691bility of a word sequence, the tags must be marginal-ized out:p(wm1 ) =?t1...tmp(wm1 tm1 ) =?t1...tmm?i=1p(witi|wi?11 ti?11 )An independence assumption similar to Eq.
1 can bemade:p(witi|wi?11 ti?11 ) ?
p(witi|wi?1i?n+1ti?1i?n+1) (3)A primary goal of our research is to build strongsyntactic language models and provide effectivemethods for constructing them to the research com-munity.
Note that the tags in the context of the jointmodel in Eq.
3 exacerbate the already sparse prob-lem in Eq.
1, which makes the probability estima-tion particularly challenging.
We utilize decisiontrees for joint syntactic language models to clus-ter context because of their strengths (reliance oninformation theoretic metrics to cluster context inthe face of extreme sparsity and the ability to in-corporate attributes of different types3), and at thesame time, unlike log-linear models (Rosenfeld etal., 1994), computationally expensive probabilitynormalization does not have to be postponed untilruntime.In Section 2, we describe the details of the syntac-tic decision tree LM.
Construction of a single-treemodel is difficult due to the inevitable greedinessof the tree construction process and its tendency tooverfit the data.
This problem is often addressed byinterpolating with lower order decision trees.
In Sec-tion 3, we point out the inappropriateness of backoffmethods borrowed from n-gram models for decisiontree LMs and briefly describe a generalized interpo-lation for such models.
The generalized interpola-tion method allows the addition of any number oftrees to the model, and thus raises the question: whatis the best way to create diverse decision trees so thattheir combination results in a stronger model, whileat the same time keeping the total number of trees inthe model relatively low for computational practical-ity.
In Section 4, we explore and evaluate a variety3For example, morphological features can be very helpfulfor modeling highly inflectional languages (Bilmes and Kirch-hoff, 2003).of methods for creating different trees.
To supportour findings, we evaluate several of the models onan ASR rescoring task in Section 5.
Finally, we dis-cuss our findings in Section 6.2 Joint Syntactic Decision Tree LMA decision tree provides us with a clustering func-tion ?
(wi?1i?n+1ti?1i?n+1) ?
{?1, .
.
.
,?N}, where Nis the number of clusters, and clusters ?k are disjointsubsets of the context space.
The probability estima-tion for a joint decision tree model is approximatedas follows:p(witi|wi?1i?n+1ti?1i?n+1) ?
p(witi|?
(wi?1i?n+1ti?1i?n+1))(4)In the remainder of this section, we briefly describethe techniques that we use to construct such a deci-sion tree ?
and to estimate the probability distribu-tion for the joint model in Eq.
4.2.1 Decision Tree ConstructionWe use recursive partitioning to grow decision trees.In this approach, a number of alternative binarysplits of the training data associated with a node areevaluated using some metric, the best split is chosen,checked against a stopping rule (which aims at pre-venting overfitting to the training data and usuallyinvolves a heldout set), and then the two partitionsbecome the child nodes if the stopping rule does notapply.
Then the algorithm proceeds recursively intothe newly constructed leaves.Binary splits are often referred to as questionsabout the context because a binary partition canbe represented by a binary function that decideswhether an element of context space belongs to onepartition or the other.
We utilize univariate questionswhere each question partitions the context on oneattribute, e.g., wi?2 or ti?1.
The questions aboutwords and tags are constructed differently:?
The questions q about the words are in the formq(x) ?
wi+x ?
S, where x is an integer be-tween ?n + 1 and ?1, and S ?
V is a subsetof the word vocabulary V .
To construct the setS, we take the set of words So observed at theoffset x in the training data associated with the692current node and split it into two complemen-tary subsets S ?
S?
= So using the Exchangealgorithm (Martin et al, 1998).
Because thealgorithm is greedy and depends on the initial-ization, we construct 4 questions per word po-sition using different random initializations ofthe Exchange algorithm.Since we need to account for words that werenot observed in the training data, we utilizethe structure depicted in Figure 1.
To estimatethe probability at the backoff node (B in Fig-ure 1), we can either use the probability from itsgrandparent nodeA or estimate it using a lowerorder tree (see Section 3), or combine the two.We have observed no noticeable difference be-tween these methods, which suggests that onlya small fraction of probability is estimated fromthese nodes; therefore, for simplicity, we usethe probability estimated at the backoff node?sgrandparent.?
To create questions about tags we create a hi-erarchical clustering of all tags in the form ofa binary tree.
This is done beforehand, usingthe Minimum Discriminating Information al-gorithm (Zitouni, 2007) with the entire train-ing data set.
In this tree, each leaf is an in-dividual tag and each internal node is associ-ated with the subset of tags that the node dom-inates.
Questions about tags are constructed inthe form q(x, k) ?
ti+x ?
Tk, where k is anode in the tag tree and Tk is the subset of tagsassociated with that node.
The rationale behindconstructing tag questions in this form is thatit enables a more efficient decoding algorithmthan standard HMM decoding (Filimonov andHarper, 2009).Questions are evaluated in two steps.
First thecontext attribute x is selected using a metric simi-lar to information gain ratio proposed by (Quinlan,1986):M = 1?
H(wi)?H(wi|x)H(x) = 1?I(x;wi)H(x)where x is one of the context attributes, e.g., wi?2or ti?1.
Then, among the questions about attributewi?2?
?SBackoff leafyesyesnonoABwi?2?SFigure 1: A fragment of the decision tree with a backoffnode.
S ?
S?
is the set of words observed in the trainingdata at the node A.
To account for unseen words, we addthe backoff node B.x, we select the question that maximizes the entropyreduction.Instead of dedicating an explicit heldout set forthe stopping criterion, we utilize a technique simi-lar to cross validation: the training data set is par-titioned into four folds, and the best question is re-quired to reduce entropy on each of the folds.Note that the tree induction algorithm can also beused to construct trees without tags:p(wi|wi?1i?n+1) ?
p(wi|?
(wi?1i?n+1))We refer to this model as the word-tree model.
Bycomparing syntactic and word-tree models, we areable to separate the effects of decision tree modelingand syntactic information on language modeling bycomparing both models to an n-gram baseline.2.2 In-tree SmoothingA decision tree offers a hierarchy of clusterings thatcan be exploited for smoothing.
We can interpo-late the observed distributions at leaves recursivelywith their parents, as in (Bahl et al, 1990; Heeman,1998):p?k(witi) = ?kpML(witi) + (1?
?k)p?k?
(witi) (5)where pML is the observed distribution at node kand k?
is the parent of k. The coefficients ?k areestimated using an EM algorithm.We can also combine p(witi|?
(wi?1i?n+1ti?1i?n+1))with lower order decision trees, i.e.,693p(witi|?
(wi?1i?n+2ti?1i?n+2)), and so on up untilp(witi) which is a one-node tree (essentially aunigram model).
Although superficially similar tobackoff in n-gram models, lower order decisiontrees differ substantially from lower order n-grammodels and require different interpolation methods.In the next section, we discuss this difference andpresent a generalized interpolation that is moresuitable for combining decision tree models.3 Interpolation with Backoff Tree ModelsIn this section, for simplicity of presentation, we fo-cus on the equations for word models, but the sameequations apply equally to joint models (Eq.
3) withtrivial transformations.3.1 Backoff PropertyLet us rewrite the interpolation Eq.
2 in a moregeneric way:p?
(wi|wi?11 ) = ?n(wi|?n(wi?11 )) + (6)?
(?n(wi?11 )) ?
p?
(wi|BOn?1(wi?11 ))where, ?n is a discounted distribution, ?n is a clus-tering function of order n, and ?
(?n(wi?11 )) is thebackoff weight chosen to normalize the distribution.BOn?1 is the backoff clustering function of ordern ?
1, representing a reduction of context size.
Inthe case of an n-gram model, ?n(wi?11 ) is the setof word sequences where the last n ?
1 words arewi?1i?n+1.
Similarly, BOn?1(wi?11 ) is the set of se-quences ending with wi?1i?n+2.
In the case of a de-cision tree model, the same backoff function is typ-ically used, but the clustering function can be arbi-trary.The intuition behind Eq.
6 is that the backoff con-text BOn?1(wi?11 ) allows for a more robust (butless informed) probability estimation than the con-text cluster ?n(wi?11 ).
More precisely:?wi?11 ,W : W ?
?n(wi?11 )?W ?
BOn?1(wi?11 )(7)that is, every word sequence W that belongs to acontext cluster ?n(wi?11 ), belongs to the same back-off cluster BOn?1(wi?11 ) (hence has the same back-off distribution).
For n-gram models, Property 7?nBOn?2 Backofk leyaslknol Asol ?
cl??ac?kal?eeoyockl??
c??A?l?A?
?eel?ya?yk?l Ak?
e?o?
???l?A?aeel?ya?yk?l??a?Ako?
?ckofkl ?A?
?ackofkl ?A?Figure 2: Backoff Propertytrivially holds since BOn?1(wi?11 ) and ?n(wi?11 )are defined as sets of sequences ending with wi?1i?n+2andwi?1i?n+1, with the former clearly being a supersetof the latter.
However, when ?
can be arbitrary, e.g.,a decision tree, the property is not necessarily satis-fied.
Figure 2 illustrates cases when the Property 7is satisfied (a) and violated (b).Let us consider what happens when we havetwo context sequences W and W ?
that belong tothe same cluster ?n(W ) = ?n(W ?)
but differ-ent backoff clusters BOn?1(W ) 6= BOn?1(W ?
).For example: suppose we have ?
(wi?2wi?1) =({on}, {may,june}) and two corresponding backoffclusters: BO?
= ({may}) and BO??
= ({june}).Following on, the word may is likely to be a monthrather than a modal verb, although the latter ismore frequent and will dominate in BO?.
There-fore we have much less faith in p?(wi|BO?)
than inp?(wi|BO??)
and would like a much smaller weight?
assigned to BO?.
However this would not be pos-sible in the backoff scheme in Eq.
6, thus we willhave to settle on a compromise value of ?, resultingin suboptimal performance.Hence arbitrary clustering (an advantage of deci-sion trees) leads to a violation of Property 7, whichis likely to produce a degradation in performance ifbackoff interpolation Eq.
6 is used.3.2 Generalized InterpolationRecursive linear interpolation similar to Jelinek-Mercer smoothing for n-gram models (Jelinek andMercer, 1980) has been applied to decision treemodels:694p?n(wi|wi?1i?n+1) = ?n(?n) ?
pn(wi|?n) + (8)(1?
?n(?n)) ?
p?n?1(wi|wi?1i?n+2)where ?n ?
?n(wi?1i?n+1), and ?n(?n) ?
[0, 1] areassigned to each cluster and are optimized on a held-out set using EM.
pn(wi|?n) is the probability dis-tribution at the cluster ?n in the tree of order n. Thisinterpolation method is particularly useful as, un-like count-based discounting methods (e.g., Kneser-Ney), it can be applied to already smoothed distribu-tions pn.In (Filimonov and Harper, 2011), we observedthat because of the violation of Property 7 in deci-sion tree models, the interpolation method of Eq.
8is not appropriate for such models.
Instead we pro-posed the following generalized form of linear inter-polation:p?n(wi|wi?1i?n+1) =?nm=1 ?m(?m) ?
pm(wi|?m)?nm=1 ?m(?m)(9)Note that the recursive interpolation of Eq.
8 canbe represented in this form with the additional con-straint?nm=1 ?m(?m) = 1, which is not required inthe generalized interpolation of Eq.
9; thus, the gen-eralized interpolation, albeit having the same num-ber of parameters, has more degrees of freedom.
Wealso showed that the recursive interpolation Eq.
8 isa special case of Eq.
9 that occurs when the Prop-erty 7 holds.4 From Backoff Trees to ForestNote that, in Eq.
9, individual trees do not have ex-plicit higher-lower order relations, they are treatedas a collection of trees, i.e., as a forest.
Naturally,to benefit from the forest model, its trees must differin some way.
Different trees can be created basedon differences in the training data, differences in thetree growing algorithm, or some non-determinism inthe way the trees are constructed.
(Xu, 2005) used randomization techniques to pro-duce a large forest of decision trees that were com-bined as follows:p(wi|wi?1i?n+1) =1MM?m=1pm(wi|wi?1i?n+1) (10)whereM is the number of decision trees in the forest(he proposed M = 100) and pm is the m-th treemodel4.
Note that this type of interpolation assumesthat each tree model is ?equal?
a priori and thereforeis only appropriate when the tree models are grownin the same way (particularly, using the same orderof context).
Note that Eq.
10 is a special case ofEq.
9 when all parameters ?
are equal.
(Xu, 2005) showed that, although each individualtree is a fairly weak model, their combination out-performs the n-gram baseline substantially.
How-ever, we find this approach impractical for onlineapplication of any sizable model: In our experi-ments, fourgram trees have approximately 1.8 mil-lion leaves and the tree structure itself (without prob-abilities) occupies nearly 200MB of disk space af-ter compression.
It would be infeasible to apply amodel consisting of more than a handful of suchtrees without distributed computing of some sort.Therefore, we pose the following question: If wecan afford to have only a handful of trees in themodel, what would be best approach to constructthose trees?In the remainder of this section, we will describethe experimental setup, discuss and evaluate differ-ent ways of building decision tree forests for lan-guage modeling, and compare combination methodsbased on Eq.
9 and Eq.
10 (when Eq.
10 is applica-ble).4.1 Experimental SetupTo train our models we use 35M words of WSJ94-96 from LDC2008T13.
The text was convertedinto speech-like form, namely numbers and abbrevi-ations were verbalized, text was downcased, punctu-ation was removed, and contractions and possessiveswere joined with the previous word (i.e., they ?ll be-comes they?ll).
For the syntactic modeling, we usedtags comprised of the POS tags of the word and it?shead.
Parsing of the text for tag extraction occurredafter verbalization of numbers and abbreviations but4Note that (Xu, 2005) used lower order models to estimatepm.695before any further processing; we used a latent vari-able PCFG parser as in (Huang and Harper, 2009).For reference, we include an n-gram model withmodified interpolated KN discounting.
All mod-els use the same vocabulary of approximately 50kwords.Perplexity numbers reported in Tables 1, 2, 3,and 4 are computed on WSJ section 23 (tokenizedin the same way)5.In Table 1, we show results reported in (Filimonovand Harper, 2011), which we use as the baseline forfurther experiments.
We constructed two sets of de-cision trees (a joint syntactic model and a word-treemodel) as described in Section 2.
Each set was com-prised of a fourgram tree with backoff trigram, bi-gram, and unigram trees.
We combined these treesusing either Eq.
8 or Eq.
9.
The ?
parameters inEq.
8 were estimated using EM by maximizing like-lihood of a heldout set (we utilized 4-way cross-validation); whereas, the parameters in Eq.
9 wereestimated using L-BFGS because the denominatorin Eq.
9 makes the maximization step problematic.4.2 Random Forest(Xu, 2005) evaluated a variety of randomizationtechniques that can be used to build trees.
He useda word-only model, with questions constructed us-ing the Exchange algorithm, similar to our model.He tried two methods of randomization: selectingthe positions in the history for question constructionby a Bernoulli trials6, and random initialization ofthe Exchange algorithm.
He found that when theExchange algorithm was initialized randomly, theBernoulli trial parameter did not matter; however,when the Exchange algorithm was initialized deter-ministically; lower values for the Bernoulli trial pa-rameter r yielded better overall forest performance.We implemented a similar method, namely, initial-izing the Exchange algorithm randomly and usingr = 0.1 for Bernoulli trials7.There is a key difference between the two ran-5This section was not used for training the parser or for theLM training.6In this method, positions in the history are ignored withprobability 1?
r, where r is the Bernoulli trials parameter.7Note that because in the joint model, the question abouttags are deterministic, we use a lower value of r than (Xu, 2005)to increase randomness.domization methods.
Since we do not have an apriori preference for choosing initializations for theExchange algorithm, by using random initializationsit is hoped that due to the greedy nature of the al-gorithm, the constructed trees, while being ?unde-graded,?8 will be sufficiently different so that theircombination improves over an individual tree.
Byintroducing Bernoulli trials, on the other hand, thereis a choice to purposely degrade the quality of in-dividual trees in the hope that additional diversitywould enable their combination to compensate forthe loss of quality in individual trees.Another way of introducing randomness to thetree construction without apparent degradation of in-dividual tree quality is through varying the data, e.g.,using different folds of the training data (see Sec-tion 2.1).Let us take a closer look at the effect of differ-ent types of randomization on individual trees andtheir combinations.
In the first set of experiments,we compare the performance of a single undegradedfourgram tree9 with forests of fourgram trees grownrandomly with Bernoulli trials.
Having only same-order trees in a forest allows us to apply interpola-tion of Eq.
10 (used in (Xu, 2005)) and comparewith the interpolation method presented in Eq.
9.
Bycomparing forests of different sizes with the baselinefrom Table 1, we are able to evaluate the effect ofrandomization in decision tree growing and assessthe importance of the lower order trees.The results are shown in Table 2.
Note that, whilean undegraded syntactic tree is better than the wordtree, the situation is reversed when the trees aregrown randomly.
This can be explained by the factthat the joint model has a much higher dimensional-ity of the context space, and therefore is much moresensitive to the clustering method.As we increase the number of random trees in theforest, the perplexity decreases as expected, with theinterpolation method of Eq.
9 showing improvementof a few percentile points over Eq.
10.
Note thatin the case of the word-tree model, it takes 4 ran-dom decision trees to reach the performance of a sin-gle undegraded tree, while in the joint model, even8Here and henceforth, by ?undegraded?
we mean ?accord-ing to the algorithm described in Section 2.?9Since each tree has a smooth distribution based on Eq.
5,lower order trees are not strictly required.696Eq.
8 Eq.
9 (generalized)order n-gram word-tree syntactic word-tree syntactic2-gram 261.0 257.8 214.3 258.1 214.63-gram 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)4-gram 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)Table 1: Perplexity results on PTB WSJ section 23.
Percentage numbers in parentheses denote the reduction ofperplexity relative to the lower order model of the same type.word-tree syntacticEq.
10 Eq.
9 Eq.
10 Eq.
91 ?
undgr 204.9 189.11 ?
rnd 250.2 289.92 ?
rnd 229.5 221.5 244.6 240.93 ?
rnd 227.5 214.5 226.2 220.04 ?
rnd 219.5 205.0 219.5 212.25 ?
rnd 200.9 184.1 216.5 209.0baseline N/A 155.7 N/A 147.1Table 2: Perplexity numbers obtained using fourgramtrees only.
Note that ?undgr?
and ?rnd?
denote unde-graded and randomly grown trees with Bernoulli trials,respectively, and the number indicates the number oftrees in the forest.
Also ?baseline?
refers to the fourgrammodels with lower order trees (from Table 1, Eq.
9).5 trees are much worse than a single decision treeconstructed without randomization.
Finally, com-pare the performance of single undegraded fourgramtrees in Table 2 with fourgram models in Table 1,which are constructed with lower order trees: bothword-tree and joint models in Table 1 have over20% lower perplexity compared to the correspond-ing models consisting of a single fourgram tree.In Table 3, we evaluate forests of fourgram treesproduced using randomizations without degradingthe tree construction algorithm.
That is, we use ran-dom initializations of the Exchange algorithm and,additionally, variations in the training data fold.
Allforests in this table use the interpolation methodof Eq.
9.
Note that, while these perplexity num-bers are substantially better than trees produced withBernoulli trials in Table 2, they are still significantlyworse than the baseline model from Table 1.These results suggest that, while it is beneficialto combine different decision trees, we should in-troduce differences to the tree construction processword-tree syntactic# trees Exchng.
+data Exchng.
+data1 204.9 189.12 185.9 186.5 174.5 173.73 179.5 179.9 168.8 167.24 176.2 176.4 165.1 164.05 173.7 172.0 163.0 162.0baseline 155.7 147.1Table 3: Perplexity numbers obtained using fourgramtrees produced using random initialization of the Ex-change algorithm (Exchng.
columns) and, additionally,variations in training data folds (+data columns).
Notethat ?baseline?
refers to the fourgram models with lowerorder trees (from Table 1).
All models use the interpola-tion method of Eq.
9.without degrading the trees when introducing ran-domness, especially for joint models.
In addition,lower order trees seem to play an important role forhigh quality model combination.4.3 Context-Restricted ForestAs we have mentioned above, combining higher andlower order decision trees produces much better re-sults.
A lower order decision tree is grown froma lower order context space, i.e., the context spacewhere we purposely ignore some attributes.
Notethat in this case, rather than randomly ignoring con-texts via Bernoulli trials at every node in the decisiontree, we discard some context attributes upfront ina principled manner (i.e., most distant context) andthen grow the decision tree without degradation.Since the joint model, having more context at-tributes, affords a larger variety of different contexts,we use this model in the remaining experiments.In Table 4, we present the perplexity numbers forour standard model with additional trees.
We de-note context-restricted trees by their Markovian or-697Model size PPL1w1t + 2w2t + 3w3t + 4w4t (*) 294MB 147.1(*) + 4w3t + 3w2t 579MB 143.5(*) + 4w3t + 3w4t 587MB 144.9(*) + 4w3t + 3w4t + 3w2t + 2w3t 699MB 140.7(*) + 1 ?
bernoulli-rnd 464MB 149.7(*) + 2 ?
bernoulli-rnd 632MB 150.4(*) + 3 ?
bernoulli-rnd 804MB 151.1(*) + 1 ?
data-rnd 484MB 147.0(*) + 2 ?
data-rnd 673MB 145.0(*) + 3 ?
data-rnd 864MB 145.2Table 4: Perplexity results using the standard syntacticmodel with additional trees.
?bernoulli-rnd?
and ?data-rnd?
indicate fourgram trees randomized using Bernoullitrials and varying training data, respectively.
The secondcolumn shows the combined size of decision trees in theforest.ders (words w and tags t independently), so 3w2tindicates a decision tree implementing the probabil-ity function: p(witi|wi?1wi?2ti?1).
The fourgramjoint model presented in Table 1 has four trees andis labeled with the formula ?1w1t + 2w2t + 3w3t +4w4t?
in Table 4.
The randomly grown trees (de-noted ?bernoulli-rnd?)
are grown utilizing the fullcontext 4w4t using the methods described in Sec-tion 4.2.
All models utilize the generalized interpo-lation method described in Section 3.2.As can be seen in Table 4, adding undegradedtrees consistently improves the performance of analready strong baseline, while adding random treesonly increases the perplexity because their qualityis worse than undegraded trees?.
Trees producedby data randomization (denoted ?data-rnd?)
also im-prove the performance of the model; however, theimprovement is not greater than that of additionallower order trees, which are considerably smaller insize.5 ASR Rescoring ResultsIn order to verify that the improvements in perplex-ity that we observe in Tables 1 and 4 are sufficientfor an impact on a task, we measure Word ErrorRate (WER) of our models on an Automatic SpeechRecognition (ASR) rescoring task using the WallStreet Journal corpus (WSJ) for evaluation.
The testset consists of 4,088 utterances of WSJ0.
We opti-Model PPL WERn-gram 161.7 7.81%1w1t + 2w2t + 3w3t + 4w4t (Eq.8) 156.5 7.57%1w1t + 2w2t + 3w3t + 4w4t (*) 147.1 7.32%(*) + 4w3t + 3w4t + 3w2t + 2w3t 140.7 7.20%Table 5: Perplexity and WER results.
Note that the lasttwo rows are syntactic models using the interpolationmethod of Eq.
9.mized the weights for the combination of acousticand language model scores on a separate develop-ment set comprised of 1,243 utterances from Hub25k closed vocabulary and the WSJ1 5k open vocab-ulary sets.The ASR system used to produce lattices is basedon the 2007 IBM Speech transcription system for theGALE Distillation Go/No-go Evaluation (Chen etal., 2006).
The acoustic models are state-of-the-artdiscriminatively trained models which are trained onBroadcast News (BN) Hub4 acoustic training data.Lattices were produced using a trigram LM trainedon the same data as the models we evaluate, then1,000 best unique hypotheses were extracted fromthe lattices.
WER of the 1-best hypothesis on thetest set is 8.07% and the oracle WER is 3.54%.In Table 5, we present WER results along withthe corresponding perplexity numbers from Ta-bles 1 and 4 for our lowest perplexity syntacticmodel, as well as the baselines (modified KN n-grammodel and standard decision tree models using in-terpolation methods of Eq.
8 and Eq.
9).
The in-terpolation method of Eq.
9 substantially improvesperformance over the interpolation method of Eq.
8,reducing WER by 0.25% absolute (p < 10?5).Adding four trees utilizing context restricted in dif-ferent ways further reduces WER by 0.12%, whichis also a statistically significant (p < 0.025) im-provement over the baseline models labeled (*).
Al-together, the improvements over the n-gram baselineadd up to 0.61% absolute (8% relative) WER reduc-tion.6 ConclusionIn this paper, we investigate various aspects of com-bining multiple decision trees in a single languagemodel.
We observe that the generalized interpola-698tion (Eq.
9) for decision tree models proposed in(Filimonov and Harper, 2011) is in fact a forest in-terpolation method rather than a backoff interpola-tion because, in Eq.
9, models do not have explicithigher-lower order relation as they do in backoff in-terpolation (Eq.
6).
Thus, in this paper we investi-gate the question of how to construct decision treesso that their combination results in improved per-formance (under the assumption that computationaltractability allows only a handful of decision treesin a forest).
We compare various techniques forproducing forests of trees and observe that methodsthat diversify trees by introducing random degrada-tion of the tree construction algorithm perform morepoorly (especially with joint models) than methodsin which the trees are constructed without degrada-tion and with variability being introduced via param-eters that are inherently arbitrary (e.g., training datafold differences or initializations of greedy searchalgorithms).
Additionally, we observe that simplyrestricting the context used to construct trees in dif-ferent ways, not only produces smaller trees (be-cause of the context reduction), but the resultingvariations in trees also produce forests that are atleast as good as forests of larger trees.7 AcknowledgmentsWe would like to thank Ariya Rastrow for providingword lattices for the ASR rescoring experiments.ReferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza, andRobert L. Mercer.
1990.
A tree-based statistical lan-guage model for natural language speech recognition.Readings in speech recognition, pages 507?514.Srinivas Bangalore.
1996.
?Almost parsing?
techniquefor language modeling.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 1173?1176.Jeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and generalized parallel backoff.
InProceedings of HLT/NAACL, pages 4?6.Ciprian Chelba and Frederick Jelinek.
2000.
Structuredlanguage modeling for speech recognition.
CoRR.Stanley F. Chen and Joshua Goodman.
1996.
An empiri-cal study of smoothing techniques for language model-ing.
In Proceedings of the 34th annual meeting on As-sociation for Computational Linguistics, pages 310?318.S.
Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,H.
Soltau, and G. Zweig.
2006.
Advances in speechtranscription at IBM under the DARPA EARS pro-gram.
IEEE Transactions on Audio, Speech and Lan-guage Processing, pages 1596?1608.Denis Filimonov and Mary Harper.
2009.
A joint lan-guage model with fine-grain syntactic tags.
In Pro-ceedings of the EMNLP 2009.Denis Filimonov and Mary Harper.
2011.
Generalizedinterpolation in decision tree LM.
In Proceedings ofthe 49st Annual Meeting of the Association for Com-putational Linguistics.Peter Heeman.
1998.
POS tagging versus classes in lan-guage modeling.
In Sixth Workshop on Very LargeCorpora.Zhongqiang Huang and Mary Harper.
2009.
Self-Training PCFG grammars with latent annotationsacross languages.
In Proceedings of the EMNLP 2009.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of markov source parameters fromsparse data.
In Proceedings of the Workshop on Pat-tern Recognition in Practice, pages 381?397.Sven Martin, Jorg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.
InSpeech Communication, pages 1253?1256.J.
R. Quinlan.
1986.
Induction of decision trees.
Ma-chine Learning, 1(1):81?106.Ronald Rosenfeld, Jaime Carbonell, and Alexander Rud-nicky.
1994.
Adaptive statistical language modeling:A maximum entropy approach.
Technical report.Peng Xu.
2005.
Random Forests and Data SparsenessProblem in Language Modeling.
Ph.D. thesis, Balti-more, Maryland, April.Imed Zitouni.
2007.
Backoff hierarchical class n-gram language models: effectiveness to model unseenevents in speech recognition.
Computer Speech &Language, 21(1):88?104.699
