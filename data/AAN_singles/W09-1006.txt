Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 33?40,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsA note on contextual binary feature grammarsAlexander ClarkDepartment of Computer ScienceRoyal Holloway, University of Londonalexc@cs.rhul.ac.ukRe?mi Eyraud and Amaury HabrardLaboratoire d?Informatique Fondamentalede Marseille, CNRS,Aix-Marseille Universite?, Franceremi.eyraud,amaury.habrard@lif.univ-mrs.frAbstractContextual Binary Feature Grammarswere recently proposed by (Clark et al,2008) as a learnable representation forrichly structured context-free and con-text sensitive languages.
In this pa-per we examine the representationalpower of the formalism, its relationshipto other standard formalisms and lan-guage classes, and its appropriatenessfor modelling natural language.1 IntroductionAn important issue that concerns both natu-ral language processing and machine learningis the ability to learn suitable structures of alanguage from a finite sample.
There are twomajor points that have to be taken into ac-count in order to define a learning method use-ful for the two fields: first the method shouldrely on intrinsic properties of the language it-self, rather than syntactic properties of therepresentation.
Secondly, it must be possibleto associate some semantics to the structuralelements in a natural way.Grammatical inference is clearly an impor-tant technology for NLP as it will provide afoundation for theoretically well-founded un-supervised learning of syntax, and thus avoidthe annotation bottleneck and the limitationsof working with small hand-labelled treebanks.Recent advances in context-free grammati-cal inference have established that there arelarge learnable classes of context-free lan-guages.
In this paper, we focus on the ba-sic representation used by the recent approachproposed in (Clark et al, 2008).
The authorsconsider a formalism called Contextual BinaryFeature Grammars (CBFG) which defines aclass of grammars using contexts as featuresinstead of classical non terminals.
The use offeatures is interesting from an NLP point ofview because we can associate some semanticsto them, and because we can represent com-plex, structured syntactic categories.
The no-tion of contexts is relevant from a grammaticalinference standpoint since they are easily ob-servable from a finite sample.
In this paperwe establish some basic language theoretic re-sults about the class of exact Contextual Bi-nary Feature Grammars (defined in Section 3),in particular their relationship to the Chomskyhierarchy: exact CBFGs are those where thecontextual features are associated to all thepossible strings that can appear in the corre-sponding contexts of the language defined bythe grammar.The main results of this paper are proofsthat the class of exact CBFGs:?
properly includes the regular languages(Section 5),?
does not include some context-free lan-guages (Section 6),?
and does include some non context-freelanguages (Section 7).Thus, this class of exact CBFGs is orthog-onal to the classic Chomsky hierarchy butcan represent a very large class of languages.Moreover, it has been shown that this classis efficiently learnable.
This class is thereforean interesting candidate for modeling naturallanguage and deserves further investigation.2 Basic NotationWe consider a finite alphabet ?, and ??
thefree monoid generated by ?.
?
is the emptystring, and a language is a subset of ??.
Wewill write the concatenation of u and v as uv,and similarly for sets of strings.
u ?
??
is asubstring of v ?
??
if there are strings l, r ?
?
?such that v = lur.33A context is an element of ??
?
??.
For astring u and a context f = (l, r) we write fu = lur; the insertion or wrapping operation.We extend this to sets of strings and contextsin the natural way.
A context is also known instructuralist linguistics as an environment.The set of contexts, or distribution, of astring u of a language L is, CL(u) = {(l, r) ???
?
?
?|lur ?
L}.
We will often drop thesubscript where there is no ambiguity.
Wedefine the syntactic congruence as u ?L v iffCL(u) = CL(v).
The equivalence classes un-der this relation are the congruence classes ofthe language.
In general we will assume that?
is not a member of any language.3 Contextual Binary FeatureGrammarsMost definitions and lemmas of this sectionwere first introduced in (Clark et al, 2008).3.1 DefinitionBefore the presentation of the formalism, wegive some results about contexts to help togive an intuition of the representation.
Thebasic insight behind CBFGs is that there is arelation between the contexts of a string w andthe contexts of its substrings.
This is given bythe following trivial lemma:Lemma 1.
For any language L and for anystrings u, u?, v, v?
if C(u) = C(u?)
and C(v) =C(v?
), then C(uv) = C(u?v?
).We can also consider a slightly stronger result:Lemma 2.
For any language L and for anystrings u, u?, v, v?
if C(u) ?
C(u?)
and C(v) ?C(v?
), then C(uv) ?
C(u?v?
).C(u) ?
C(u?)
means that we can replaceany occurrence of u in a sentence, with a u?,without affecting the grammaticality, but notnecessarily vice versa.
Note that none of thesestrings need to correspond to non-terminals:this is valid for any fragment of a sentence.We will give a simplified example from En-glish syntax: the pronoun it can occur every-where that the pronoun him can, but not viceversa1.
Thus given a sentence ?I gave himaway?, we can substitute it for him, to get the1This example does not account for a number of syn-tactic and semantic phenomena, particularly the distri-bution of reflexive anaphors.grammatical sentence I gave it away, but wecannot reverse the process.
For example, giventhe sentence it is raining, we cannot substi-tute him for it, as we will get the ungrammat-ical sentence him is raining.
Thus we observeC(him) ( C(it).Looking at Lemma 2 we can also say that,if we have some finite set of strings K, wherewe know the contexts, then:Corollary 1.C(w) ??u?,v?:u?v?=w?u?K:C(u)?C(u?)?v?K:C(v)?C(v?
)C(uv)This is the basis of the representation: aword w is characterised by its set of contexts.We can compute the representation of w, fromthe representation of its parts u?, v?, by lookingat all of the other matching strings u and vwhere we understand how they combine (withsubset inclusion).
In order to illustrate thisconcept, we give here a simple example.Consider the language {anbn|n > 0} andthe set K = {aabb, ab, abb, aab, a, b}.
Supposewe want to compute the set of contexts ofaaabbb, Since C(abb) ?
C(aabbb), and vacu-ously C(a) ?
C(a), we know that C(aabb) ?C(aaabbb).
More generally, the contexts of abcan represent anbn, those of aab the stringsan+1bn and the ones of abb the strings anbn+1.The key relationships are given by contextset inclusion.
Contextual binary feature gram-mars allow a proper definition of the combina-tion of context inclusion:Definition 1.
A Contextual Binary FeatureGrammar (CBFG) G is a tuple ?F, P, PL,?
?.F is a finite set of contexts, called features,where we write C = 2F for the power set of Fdefining the categories of the grammar, P ?C ?
C ?
C is a finite set of productions thatwe write x ?
yz where x, y, z ?
C and PL ?C ?
?
is a set of lexical rules, written x ?
a.Normally PL contains exactly one productionfor each letter in the alphabet (the lexicon).A CBFG G defines recursively a map fG34from ??
?
C as follows:fG(?)
= ?
(1)fG(w) =?
(c?w)?PLc iff |w| = 1(2)fG(w) =?u,v:uv=w?x?yz?P :y?fG(u)?z?fG(v)x iff |w| > 1.
(3)We give here more explanation about themap fG.
It defines in fact the analysis of astring by a CBFG.
A rule z ?
xy is appliedto analyse a string w if there is a cut uv = ws.t.
x ?
fG(u) and y ?
fG(v), recall that xand y are sets of contexts.
Intuitively, the re-lation given by the production rule is linkedwith Lemma 2: z is included in the set of fea-tures of w = uv.
From this relationship, forany (l, r) ?
z we have lwr ?
L(G).The complete computation of fG is then jus-tified by Corollary 1: fG(w) defines all thepossible features associated by G to w with allthe possible cuts uv = w (i.e.
all the possiblederivations).Finally, the natural way to define the mem-bership of a string w in L(G) is to have thecontext (?, ?)
?
fG(w) which implies that?u?
= u ?
L(G).Definition 2.
The language defined by aCBFG G is the set of all strings that are as-signed the empty context: L(G) = {u|(?, ?)
?fG(u)}.As we saw before, we are interested in caseswhere there is a correspondence between thelanguage theoretic interpretation of a context,and the occurrence of that context as a featurein the grammar.
From the basic definition ofa CBFG, we do not require any specific con-dition on the features of the grammar, exceptthat a feature is associated to a string if thestring appears in the context defined by thefeature.
However, we can also require that fGdefines exactly all the possible features thatcan be associated to a given string accordingto the underlying language.Definition 3.
Given a finite set of contextsF = {(l1, r1), .
.
.
, (ln, rn)} and a language Lwe can define the context feature map FL :??
?
2F which is just the map u 7?
{(l, r) ?F |lur ?
L} = CL(u) ?
F .Using this definition, we now need a cor-respondence between the language theoreticcontext feature map FL and the representa-tion in the CBFG fG.Definition 4.
A CBFG G is exact if for allu ?
?
?, fG(u) = FL(G)(u).Exact CBFGs are a more limited formalismthan CBFGs themselves; without any limitson the interpretation of the features, we candefine a class of formalisms that is equal tothe class of Conjunctive Grammars (see Sec-tion 4).
However, exactness is an importantnotion because it allows to associate intrinsiccomponents of a language to strings.
Contextsare easily observable from a sample and more-over it is only when the features correspond tothe contexts that distributional learning algo-rithms can infer the structure of the language.A basic example of such a learning algorithmis given in (Clark et al, 2008).3.2 A Parsing ExampleTo clarify the relationship with CFGparsing, we will give a simple workedexample.
Consider the CBFG G =?
{(?, ?
), (aab, ?
), (?, b), (?, abb), (a, ?
)(aab, ?
)},P, PL, {a, b}?
with PL ={{(?, b), (?, abb)} ?
a, {(a, ?
), (aab, ?)}
?
b}and P ={{(?, ?)}
?
{(?, b)}{(aab, ?
)},{(?, ?)}
?
{(?, abb)}{(a, ?
)},{(?, b)} ?
{(?, abb)}{(?, ?
)},{(a, ?)}
?
{(?, ?
)}{(aab, ?
)}}.If we want to parse the string w = aabb theusual way is to have a bottom-up approach.This means that we recursively compute thefG map on the substrings of w in order tocheck whether (?, ?)
belongs to fG(w).The Figure 1 graphically gives the mainsteps of the computation of fG(aabb).
Ba-sically there are two ways to split aabb thatallow the derivation of the empty context:aab|b and a|abb.
The first one correspondto the top part of the figure while the sec-ond one is drawn at the bottom.
We cansee for instance that the empty context be-longs to fG(ab) thanks to the rule {(?, ?)}
?
{(?, abb)}{(a, ?
)}: {(?, abb)} ?
fG(a) and{(a, ?)}
?
fG(b).
But for symmetrical reasons35the result can also be obtained using the rule{(?, ?)}
?
{(?, b)}{(aab, ?
)}.As we trivially have fG(aa) = fG(bb) = ?,since no right-hand side contains the concate-nation of the same two features, an inductionproof can be written to show that (?, ?)
?fG(w) ?
w ?
{anbn : n > 0}.a         a         b         bfG{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)}
{(a,?),(aab,?
)}fG fG fGRule: (?,?)
?
(?,b) (aab,?
)fG(ab)  ?
{(?,?
)}Rule: (a,?)
?
(?,?)
(aab,?
)fG(abb)  ?
{(a,?
)}Rule: (?,?)
?
(?,abb) (a,?
)fG(aabb)  ?
{(?,?
)}f G{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)}
{(a,?),(aab,?
)}f G f G f GRule: (?,?)
?
(?,abb) (a,?
)fG(ab)  ?
{(?,?
)}Rule: (?,b) ?
(?,abb) (?,?
)fG(aab)  ?
{(?,b)}Rule: (?,?)
?
(?,b) (aab,?
)fG(aabb)  ?
{(?,?
)}Figure 1: The two derivations to obtain (?, ?
)in fG(aabb) in the grammar G.This is a simple example that illustratesthe parsing of a string given a CBFG.
Thisexample does not characterize the power ofCBFG since no right handside part is com-posed of more than one context.
A more inter-esting, example with a context-sensitive lan-guage, will be presented in Section 7.4 Non exact CBFGsThe aim here is to study the expressive powerof CBFG compare to other formalism recentlyintroduced.
Though the inference can be doneonly for exact CBFG, where features are di-rectly linked with observable contexts, it isstill worth having a look at the more generalcharacteristics of CBFG.
For instance, it is in-teresting to note that several formalisms in-troduced with the aim of representing naturallanguages share strong links with CBFG.Range Concatenation GrammarsRange Concatenation Grammars are a verypowerful formalism (Boullier, 2000), that is acurrent area of research in NLP.Lemma 3.
For every CBFG G, there isa non-erasing positive range concatenationgrammar of arity one, in 2-var form that de-fines the same language.Proof.
Suppose G = ?F, P, PL,??.
Definea RCG with a set of predicates equal to Fand the following clauses, and the two vari-ables U, V .
For each production x ?
yz inP , for each f ?
x, where y = {g1, .
.
.
gi},z = {h1, .
.
.
hj} add clausesf(UV ) ?
g1(U), .
.
.
gi(U), h1(V ), .
.
.
hj(V ).For each lexical production {f1 .
.
.
fk} ?
aadd clauses fi(a) ?
.
It is straightforwardto verify that f(w) `  iff f ?
fG(w).Conjunctive GrammarA more exact correspondence is to the class ofConjunctive Grammars (Okhotin, 2001), in-vented independently of RCGs.
For every ev-ery language L generated by a conjunctivegrammar there is a CBFG representing L#(where the special character # is not includedin the original alphabet).Suppose we have a conjunctive grammarG = ?
?, N, P, S?
in binary normal form (asdefined in (Okhotin, 2003)).
We construct theequivalent CBFG G?
= ?F, P ?, PL,??
as fol-lowed:?
For every letter a we add a context (la, ra)to F such that laara ?
L;?
For every rules X ?
a in P , we create arule {(la, ra)} ?
a in PL.?
For every non terminal X ?
N , for everyrule X ?
P1Q1& .
.
.&PnQn we add dis-tinct contexts {(lPiQi , rPiQi)} to F, suchthat for all i it exists ui, lPiQiuirPiQi ?
Land PiQi?
?G ui;?
Let FX,j = {(lPiQi , rPiQi) : ?i} theset of contexts corresponding to thejth rule applicable to X.
For all36(lPiQi , rPiQi) ?
FX,j , we add to P?
therules (lPiQi , rPiQi) ?
FPi,kFQi,l (?k, l).?
We add a new context (w, ?)
to F suchthat S?
?G w and (w, ?)
?
# to PL;?
For all j, we add to P ?
the rule (?, ?)
?FS,j{(w, ?
)}.It can be shown that this construction givesan equivalent CBFG.5 Regular LanguagesAny regular language can be defined by an ex-act CBFG.
In order to show this we will pro-pose an approach defining a canonical form forrepresenting any regular language.Suppose we have a regular language L, weconsider the left and right residual languages:u?1L = {w|uw ?
L} (4)Lu?1 = {w|wu ?
L} (5)They define two congruencies: if l, l?
?
u?1L(resp.
r, r?
?
Lu?1) then for all w ?
?
?, lw ?L iff l?w ?
L (resp.
wr ?
L iff wr?
?
L).For any u ?
?
?, let lmin(u) be the lexico-graphically shortest element such that l?1minL =u?1L.
The number of such lmin is finite bythe Myhil-Nerode theorem, we denote by Lminthis set, i.e.
{lmin(u)|u ?
??}.
We de-fine symmetrically Rmin for the right residuals(Lr?1min = Lu?1).We define the set of contexts as:F (L) = Lmin ?Rmin.
(6)F (L) is clearly finite by construction.If we consider the regular language de-fined by the deterministic finite automataof Figure 2, we obtain Lmin = {?, a, b}and Rmin = {?, b, ab} and thus F (L) ={(?, ?
), (a, ?
), (b, ?
), (?, b), (a, b), (b, b), (?, ab),(a, ab), (b, ab)}.By considering this set of features, wecan prove (using arguments about congruenceclasses) that for any strings u, v such thatFL(u) ?
FL(v), then CL(u) ?
CL(v).
Thismeans the set of feature F is sufficient to rep-resent context inclusion, we call this propertythe fiduciality.Note that the number of congruence classesof a regular language is finite.
Each congru-ence class is represented by a set of contextsFigure 2: Example of a DFA.
The left residualsare defined by ?
?1L, a?1L, b?1L and the rightones by L?
?1, Lb?1, Lab?1 (note here thatLa?1 = L??1).FL(u).
Let KL be finite set of strings formedby taking the lexicographically shortest stringfrom each congruence class.
The final gram-mar can be obtained by combining elementsof KL.
For every pair of strings u, v ?
KL, wedefine a ruleFL(uv) ?
FL(u), FL(v) (7)and we add lexical productions of the formFL(a) ?
a, a ?
?.Lemma 4.
For all w ?
?
?, fG(w) = FL(w).Proof.
(Sketch) Proof in two steps: ?w ??
?, FL(w) ?
fG(w) and fG(w) ?
FL(w).
Eachstep is made by induction on the length of wand uses the rules created to build the gram-mar, the derivation process of a CBFG andthe fiduciality for the second step.
The keypoint rely on the fact that when a string w isparsed by a CBFG G, there exists a cut of win uv = w (u, v ?
??)
and a rule z ?
xy in Gsuch that x ?
fG(u) and y ?
fG(v).
The rulez ?
xy is also obtained from a substring fromthe set used to build the grammar using theFL map.
By inductive hypothesis you obtaininclusion between fG and FL on u and v.For the language of Figure 2, the followingset is sufficient to build an exact CBGF:{a, b, aa, ab, ba, aab, bb, bba} (this correspondsto all the substrings of aab and bba).
We have:FL(a) = F (L)\{(?, ?
), (a, ?)}
?
aFL(b) = F (L) ?
bFL(aa) = FL(a) ?
FL(a), FL(a)FL(ab) = F (L) ?
FL(a), FL(b) = FL(a), F (L)FL(ba) = F (L) ?
FL(b), FL(a) = F (L), FL(a)FL(bb) = F (L) ?
FL(b), FL(b) = F (L), F (L)37FL(aab) = FL(bba) = FL(ab) = FL(ba)The approach presented here gives a canon-ical form for representing a regular languageby an exact CBFG.
Moreover, this is is com-plete in the sense that every context of everysubstring will be represented by some elementof F (L): this CBFG will completely model therelation between contexts and substrings.6 Context-Free LanguagesWe now consider the relationship betweenCFGs and CBFGs.Definition 5.
A context-free grammar (CFG)is a quadruple G = (?, V, P, S).
?
is a fi-nite alphabet, V is a set of non terminals(?
?
V = ?
), P ?
V ?
(V ?
?
)+ is a finiteset of productions, S ?
V is the start symbol.In the following, we will suppose that a CFGis represented in Chomsky Normal Form, i.e.every production is in the form N ?
UW withN,U,W ?
V or N ?
a with a ?
?.We will write uNv ?G u?v if there is a pro-duction N ?
?
?
P .?
?G is the reflexive tran-sitive closure of ?G.
The language defined bya CFG G is L(G) = {w ?
??|S?
?G w}.6.1 A Simple CharacterizationA simple approach to try to represent a CFGby a CBFG is to define a bijection between theset of non terminals and the set of context fea-tures.
Informally we define each non terminalby a single context and rewrite the productionsof the grammar in the CBFG form.To build the set of contexts F , it is sufficientto choose |V | contexts such that a bijection bCcan be defined between V and F with bC(N) =(l, r) implies that S??
lNr.
Note that we fixbT (S) = (?, ?
).Then, we can define a CBFG?F, P ?, P ?L,?
?, where P?
= {bT (N) ?bT (U)bT (W )|N ?
UW ?
P} andP ?L = {bT (N) ?
a|N ?
a ?
P, a ?
?
}.A similar proof showing that this constructionproduces an equivalent CBFG can be foundin (Clark et al, 2008).If this approach allows a simple syntacticalconvertion of a CFG into a CBFG, it is notrelevant from an NLP point of view.
Thoughwe associate a non-terminal to a context, thismay not correspond to the intrinsic propertyof the underlying language.
A context couldbe associated with many non-terminals and wechoose only one.
For example, the context(He is, ?)
allows both noun phrases and ad-jective phrases.
In formal terms, the resultingCBFG is not exact.
Then, with the bijectionwe introduced before, we are not able to char-acterize the non-terminals by the contexts inwhich they could appear.
This is clearly whatwe don?t want here and we are more interestedin the relationship with exact CBFG.6.2 Not all CFLs have an exact CBFGWe will show here that the class of context-free grammars is not strictly included in theclass of exact CBFGs.
First, the grammardefined in Section 3.2 is an exact CBFG forthe context-free and non regular language{anbn|n > 0}, showing the class of exactCBFG has some elements in the class of CFGs.We give now a context-free language L thatcan not be defined by an exact CBFG:L = {anb|n > 0} ?
{amcn|n > m > 0}.Suppose that there exists an exact CBFG thatrecognizes it and let N be the length of thebiggest feature (i.e.
the longuest left part ofthe feature).
For any sufficiently large k >N , the sequences ck and ck+1 share the samefeatures: FL(ck) = FL(ck+1).
Since the CBFGis exact we have FL(b) ?
FL(ck).
Thus anyderivation of ak+1b could be a derivation ofak+1ck which does not belong to the language.However, this restriction does not mean thatthe class of exact CBFG is too restrictive formodelling natural languages.
Indeed, the ex-ample we have given is highly unnatural andsuch phenomena appear not to occur in at-tested natural languages.7 Context-Sensitive LanguagesWe now show that there are some exactCBFGs that are not context-free.
In particu-lar, we define a language closely related to theMIX language (consisting of strings with anequal number of a?s, b?s and c?s in any order)which is known to be non context-free, andindeed is conjectured to be outside the classof indexed grammars (Boullier, 2003).38Let M = {(a, b, c)?
}, we consider the languageL = Labc?Lab?Lac?
{a?a, b?b, c?c, dd?, ee?, ff ?
}:Lab = {wd|w ?
M, |w|a = |w|b},Lac = {we|w ?
M, |w|a = |w|c},Labc = {wf |w ?
M, |w|a = |w|b = |w|c}.In order to define a CBFG recognizing L, wehave to select features (contexts) that can rep-resent exactly the intrinsic components of thelanguages composing L. We propose to use thefollowing set of features for each sublanguages:?
For Lab: (?, d) and (?, ad), (?, bd).?
For Lac: (?, e) and (?, ae), (?, ce).?
For Labc: (?, f).?
For the letters a?, b?, c?, a, b, c we add:(?, a), (?, b), (?, c), (a?, ?
), (b?, ?
), (c?, ?).?
For the letters d, e, f, d?, e?, f ?
we add;(?, d?
), (?, e?
), (?, f ?
), (d, ?
), (e, ?
), (f, ?
).Here, Lab will be represented by (?, d), but wewill use (?, ad), (?, bd) to define the internalderivations of elements of Lab.
The same ideaholds for Lac with (?, e) and (?, ae), (?, ce).For the lexical rules and in order to have anexact CBFG, note the special case for a, b, c:{(?, bd), (?, ce), (a?, ?)}
?
a{(?, ad), (b?, ?)}
?
b{(?, ad), (?, ae), (c?, ?)}
?
cFor the nine other letters, each one is definedwith only one context like {(?, d?)}
?
d.For the production rules, the most impor-tant one is: (?, ?)
?
{(?, d), (?, e)}, {(?, f ?
)}.Indeed, this rule, with the presence of twocontexts in one of categories, means that anelement of the language has to be derivedso that it has a prefix u such that fG(u) ?
{(?, d), (?, e)}.
This means u is both an ele-ment of Lab and Lac.
This rule represents thelanguage Labc since {(?, f ?)}
can only repre-sent the letter f .The other parts of the language will bedefined by the following rules:(?, ?)
?
{(?, d)}, {(?, d?
)},(?, ?)
?
{(?, e)}, {(?, e?
)},(?, ?)
?
{(?, a)}, {(?, bd), (?, ce), (a?, ?
)},(?, ?)
?
{(?, b)}, {(?, ad), (b?, ?
)},(?, ?)
?
{(?, c)}, {(?, ad), (?, ae), (c?, ?
)},(?, ?)
?
{(?, d?
)}, {(d, ?
)},(?, ?)
?
{(?, e?
)}, {(e, ?
)},(?, ?)
?
{(?, f ?
)}, {(f, ?
)}.This set of rules is incomplete, since for rep-resenting Lab, the grammar must contain therules ensuring to have the same number of a?sand b?s, and similarly for Lac.
To lighten thepresentation here, the complete grammar ispresented in Annex.We claim this is an exact CBFG for acontext-sensitive language.
L is not context-free since if we intersect L with the regularlanguage {?
?d}, we get an instance of thenon context-free MIX language (with d ap-pended).
The exactness comes from the factthat we chose the contexts in order to ensurethat strings belonging to a sublanguage cannot belong to another one and that the deriva-tion of a substring will provide all the possiblecorrect features with the help of the union ofall the possible derivations.Note that the Mix language on its own isprobably not definable by an exact CBFG: itis only when other parts of the language candistributionally define the appropriate partialstructures that we can get context sensitivelanguages.
Far from being a limitation of thisformalism (a bug), we argue this is a feature:it is only in rather exceptional circumstancesthat we will get properly context sensitive lan-guages.
This formalism thus potentially ac-counts not just for the existence of non contextfree natural language but also for their rarity.8 ConclusionThe chart in Figure 3 summarises the differentrelationship shown in this paper.
The substi-tutable languages (Clark and Eyraud, 2007)and the very simple ones (Yokomori, 2003)form two different learnable class of languages.There is an interesting relationship with Mar-cus External Contextual Grammars (Mitrana,2005): if we defined the language of a CBFGto be the set {fG(u)  u : u ?
??}
we wouldbe taking some steps towards contextual gram-mars.In this paper we have discussed the weakgenerative power of Exact Contextual BinaryFeature Grammars; we conjecture that theclass of natural language stringsets lie in thisclass.
ECBFGs are efficiently learnable (see(Clark et al, 2008) for details) which is a com-39Context-freeRegularContext sensitivevery simplesubsti-tutableRange ConcatenationConjunctive = CBFG                                Exact CBFGFigure 3: The relationship between CBFG andother classes of languages.pelling technical advantage of this formalismover other more traditional formalisms such asCFGs or TAGs.ReferencesPierre Boullier.
2000.
A Cubic Time Extensionof Context-Free Grammars.
Grammars, 3:111?131.Pierre Boullier.
2003.
Counting with range con-catenation grammars.
Theoretical ComputerScience, 293(2):391?416.Alexander Clark and Re?mi Eyraud.
2007.
Polyno-mial identification in the limit of substitutablecontext-free languages.
Journal of MachineLearning Research, 8:1725?1745, Aug.Alexander Clark, Re?mi Eyraud, and AmauryHabrard.
2008.
A polynomial algorithm for theinference of context free languages.
In Proceed-ings of International Colloquium on Grammati-cal Inference, pages 29?42.
Springer, September.V.
Mitrana.
2005.
Marcus external contextualgrammars: From one to many dimensions.
Fun-damenta Informaticae, 54:307?316.Alexander Okhotin.
2001.
Conjunctive grammars.J.
Autom.
Lang.
Comb., 6(4):519?535.Alexander Okhotin.
2003.
An overview of con-junctive grammars.
Formal Language TheoryColumn, bulletin of the EATCS, 79:145?163.Takashi Yokomori.
2003.
Polynomial-time iden-tification of very simple grammars from pos-itive data.
Theoretical Computer Science,298(1):179?206.Annex(?, ?)
?
{(?, d), (?, e)}, {(?, f ?
)}(?, ?)
?
{(?, d)}, {(?, d?
)}(?, ?)
?
{(?, e)}, {(?, e?
)}(?, ?)
?
{(?, a)}, {(?, bd), (?, ce), (a?, ?
)}(?, ?)
?
{(?, b)}, {(?, ad), (b?, ?
)}(?, ?)
?
{(?, c)}, {(?, ad), (?, ae), (c?, ?
)}(?, ?)
?
{(?, d?
)}, {(d, ?
)}(?, ?)
?
{(?, e?
)}, {(e, ?
)}(?, ?)
?
{(?, f ?
)}, {(f, ?
)}(?, d) ?
{(?, d)}, {(?, d)}(?, d) ?
{(?, ad)}, {(?, bd)}(?, d) ?
{(?, bd)}, {(?, ad)}(?, d) ?
{(?, d)}, {(?, ad), (?, ae), (c?, ?
)}(?, d) ?
{(?, ad), (?, ae), (c?, ?
)}, {(?, d)}(?, ad) ?
{(?, ad), (?, ae), (c?, ?
)}, {(?, ad)}(?, ad) ?
{(?, ad)}, {(?, ad), (?, ae), (c?, ?
)}(?, ad) ?
{(?, ad), (b?, ?
)}, {(?, d)}(?, ad) ?
{(?, d)}, {(?, ad), (b?, ?
)}(?, bd) ?
{(?, ad), (?, ae), (c?, ?
)}, {(?, bd)}(?, bd) ?
{(?, bd)}, {(?, ad), (?, ae), (c?, ?
)}(?, bd) ?
{(?, bd), (?, ce), (a?, ?
)}, {(?, d)}(?, bd) ?
{(?, d)}, {(?, bd), (?, ce), (a?, ?
)}(?, e) ?
{(?, e)}, {(?, e)}(?, e) ?
{(?, ae)}, {(?, ce)}(?, e) ?
{(?, ce)}, {(?, ae)}(?, e) ?
{(?, e)}, {(?, ad), (b?, ?
)}(?, e) ?
{(?, ad), (b?, ?
)}, {(?, e)}(?, ae) ?
{(?, ad), (b?, ?
)}, {(?, ae)}(?, ae) ?
{(?, ae)}, {(?, ad), (b?, ?
)}(?, ae) ?
{(?, ad), (?, ae), (c?, ?
)}, {(?, e)}(?, ae) ?
{(?, e)}, {(?, ad), (?, ae), (c?, ?
)}(?, ce) ?
{(?, ad), (b?, ?
)}, {(?, ce)}(?, ce) ?
{(?, ce)}, {(?, ad), (b?, ?
)}(?, ce) ?
{(?, bd), (?, ce), (a?, ?
)}, {(?, e)}(?, ce) ?
{(?, e)}, {(?, bd), (?, ce), (a?, ?
)}{(?, bd), (?, ce), (a?, ?)}
?
a{(?, ad), (b?, ?)}
?
b{(?, ad), (?, ae), (c?, ?)}
?
c{(?, d?)}
?
d{(?, e?)}
?
e{(?, f ?)}
?
f{(?, a)} ?
a?
{(?, b)} ?
b?
{(?, c)} ?
c?
{(d, ?)}
?
d?
{(e, ?)}
?
e?
{(f, ?)}
?
f ?40
