Improved Alignment Models for Statistical Machine TranslationFranz  Jose f  Och,  Chr i s toph  T i l lmann,  and Hermann NeyLehrstuhl fiir Informatik VIRWTH Aachen - University of TechnologyAhornstrafie 55; 52056 Aachen; GERMANY{och, t illmann, ney}@inf ormat ik.
rwth-aachen, deAbst rac tIn this paper, we describe improved alignmentmodels for statistical machine translation.
Thestatistical translation approach uses two typesof information: a translation model and a lan-guage model.
The language model used is abigram or general m-gram model.
The transla-tion model is decomposed into a lexical and analignment model.
We describe two different ap-proaches for statistical translation and presentexperimental results.
The first approach isbased on dependencies between single words,the second approach explicitly takes shallowphrase structures into account, using two differ-ent alignment levels: a phrase level alignmentbetween phrases and a word level alignmentbetween single words.
We present results us-ing the Verbmobil task (German-English, 6000-word vocabulary) which is a limited-domainspoken-language task.
The experimental testswere performed on both the text transcriptionand the speech recognizer output.1 Stat i s t i ca l  Mach ine  Trans la t ionThe goal of machine translation is the transla-tion of a text given in some source language intoa target language.
We are given a source stringf /=  fl...fj...fJ, which is to be translated intoa target string e{ = el...ei...ex.
Among all possi-ble target strings, we will choose the string withthe highest probability:= argmax {Pr(ezIlflJ)}e 1= argmax {Pr(e\[).
Pr(f/le~) } ?
(1)The argmax operation denotes the search prob-lem, i.e.
the generation of the output sentencein the target language.
Pr(e{) is the languagemodel of the target language, whereas Pr (ff~lel I)is the translation model.Many statistical translation models (Vogel etal., 1996; Til lmann et al, 1997; Niessen et al,1998; Brown et al, 1993) try to model word-to-word correspondences between source and tar-get words.
The model is often further estrictedthat each source word is assigned exactly onetarget word.
These alignment models are sire-ilar to the concept of Hidden Markov models(HMM) in speech recognition.
The alignmentmapping is j ~ i = aj from source position jto target position i = aj.
The use of this align-ment model raises major problems as it fails tocapture dependencies between groups of words.As experiments have shown it is difficult to han-dle different word order and the translation ofcompound nouns?In this paper, we will describe two methodsfor statistical machine translation extending thebaseline alignment model in order to accountfor these problems.
In section 2, we shortly re-view the single-word based approach describedin (Tillmann et al, 1997) with some recently ira-plemented extensions allowing for one-to-manyalignments.
In section 3 we describe the align-ment template approach which explicitly mod-els shallow phrases and in doing so tries to over-come the above mentioned restrictions of single-word alignments.
The described method is animprovement of (Och and Weber, 1998), result-ing in an improved training and a faster searchorganization.
The basic idea is to model twodifferent alignment levels: a phrase level align-ment between phrases and a word level align-ment between single words within these phrases.Similar aims are pursued by (Alshawi et al,1998; Wang and Waibel, 1998) but differentlyapproached.
In section 4 we compare the twomethods using the Verbmobil task.202 S ing le -Word  Based  Approach2.1 Basic ApproachIn this section, we shortly review a translationapproach based on the so-called monotonicityrequirement (Til lmann et al, 1997).
Our aim isto provide a basis for comparing the two differ-ent translation approaches presented.In Eq.
(1), Pr(e~) is the language model,which is a trigram language model in this case.For the translation model Pr(flJ\[e{) we makethe assumption that each source word is alignedto exactly one target word (a relaxation of thisassumption is described in section 2.2).
Forour model, the probability of alignment aj forposition j depends on the previous alignmentposition aj-1 (Vogel et al, 1996).
Using thisassumption, there are two types of probabil-ities: the alignment probabilities denoted byp(aj \[aj-1) and the lexicon probabilities denotedby p(fj\[ea~).
The string translation probabilitycan be re-written:Pr(flJ\[elI) = E H \[p(ajlaj_l)'p(fj\[ea~)\]a~For the training of the above model parame-ters, we use the maximum likelihood criterion inthe so-called maximum approximation.
Whenaligning the words in parallel texts (for Indo-European lar~guage pairs like Spanish-English,French-English, Italian-German,...), we typi-cally observe a strong localization effect.
Inmany cases, although not always, there is aneven stronger restriction: over large portionsof the source string, the alignment is mono-tone.
In this approach, we first assume thatthe alignments satisfy the monotonicity require-ment.
Within the translation search, we will in-troduce suitably restricted permutations of thesource string, to satisfy this requirement.
Forthe alignment model, the monotonicity propertyallows only transitions from a j -1  to aj with ajump width 5:5 _-- a s -- a j -1  C {0, 1, 2}.
Thesesjumps correspond to the following three cases(5 = 0, 1,2):?
5 ---- 0 (horizontal transition = alignmentrepetition): This case corresponds to a tar-get word with two or more aligned sourcewords.?
5 = 1 (forward transition = regular align-ment): This case is the regular one: a singlenew target word is generated.?
5 = 2 (skip transition = non-Migned word):This case corresponds to skipping a word,i.e.
there is a word in the target string withno aligned word in the source string.The possible alignments using the monotonic-ity assumption are illustrated in Fig.
1.
Mono-tone alignments are paths through this uni-form trellis structure.
Using the concept ofI I I I I I1 2 3 4 5 6SOURCE POSITIONFigure 1: Illustration of alignments for themonotone HMM.monotone alignments a search procedure can beformulated which is equivalent o finding thebest path through a translation lattice, wherethe following auxiliary quantity is evaluated us-ing dynamic programming: Here, e and e' areQe,(j, e) probability of the best partialhypothesis (e~,a~) with ei = e,ei-1 = e ~ and aj = i.the two final words of the hypothesized targetstring.
The auxiliary quantity is evaluated ina position-synchronous way, where j is the pro-cessed position in the source string.
The resultof this search is a mapping: j ~ (aj, ea5 ), whereeach source word is mapped to a target posi-tion aj and a word eaj at this position.
For atrigram language model the following DP recur-sion equation is evaluated:21Q~,( j ,  e) = p( f j \ ]e )  .
max{p(O) .
Qe' ( J  - 1, e),p(1) .
n~ax{p(ele ' ,  e") .
Q~,,( j  - 1, e')}p(2).
~ {p(e\[e', e").
p(e'\[e", e'")?
Qe,,, (J - 1, e")}}p(5)  is the alignment probability for the threecases above, p(.\[., .)
denoting the trigram lan-guage model, e,e~,e" ,e  m are the four finalwords which are considered in the dynamic pro-gramming taking into account he monotonicityrestriction and a trigram language model.
TheDP equation is evaluated recursively to find thebest partial path to each grid point (j, e ~, e).
Noexplicit length model for the length of the gen-erated target string el / given the source stringfl J is used during the generation process.
Thelength model is implicitly given by the align-ment probabilities.
The optimal translation isobtained by carrying out the following optimiza-tion:max{Qe, ( J, e) .
p($1e, e')},el lewhere J is the length of the input sentence and$ is a symbol denoting the sentence nd.
Thecomplexity of the algorithm for full search isJ -E  4, where E is the size of the target languagevocabulary.
However, this is drastically reducedby beam-search.2.2 One-to-many al ignment modelThe baseline alignment model does not per-mit that a source word is aligned with two ormore target words.
Therefore, lexical corre-spondences like 'Zahnarz t te rmin '  for dent is t ' sappo in tment  cause problems because a singlesource word must be mapped on two or moretarget words.
To solve this problem for thealignment in training, we first reverse the trans-lation direction, i. e. English is now the sourcelanguage, and German is the target language.For this reversed translation direction, we per-form the usual training and then check thealignment paths obtained in the maximum ap-proximation.
Whenever a German word isaligned with a sequence of the adjacent Englishwords, this sequence is added to the English vo-cabulary as an additional entry.
As a result,we have an extended English vocabulary.
Usingthis new vocabulary, we then perform the stan-dard training for the original translation direc-tion.2.3 Extension to HandleNon-Monotonic i tyOur approach assumes that the alignment ismonotone with respect o the word order forthe lion's share of all word alignments.
Forthe translation direction German-English themonotonicity constraint is violated mainly withrespect o the verb group.
In German, the verbgroup usually consists of a left and a right ver-bal brace, whereas in English the words of theverb group usually form a sequence of consec-utive words.
For our DP search, we use a left-to-right beam-search oncept having been intro-duced in speech recognition, where we rely onbeam-search asan efficient pruning technique inorder to handle potentially huge search spaces.Our ultimate goal is speech translation aim-ing at a tight integration of speech recognitionand translation (Ney, 1999).
The results pre-sented were obtained by using a quasi-monotonesearch procedure, which proceeds from left toright along the position of the source sentencebut allows for a small number of source posi-tions that are not processed monotonically.
Theword re-orderings of the source sentence posi-tions were restricted to the words of the Ger-man verb group.
Details of this approach willbe presented elsewhere.3 A l ignment  Template  ApproachA general deficiency of the baseline alignmentmodels is that they are only able to model corre-spondences between single words.
A first coun-termeasure was the refined alignment model de-scribed in section 2.2.
A more systematic ap-proach is to consider whole phrases rather thansingle words as the basis for the alignment mod-els.
In other words, a whole group of adjacentwords in the source sentence may be alignedwith a whole group of adjacent words in the tar-get language.
As a result the context of wordshas a greater influence and the changes in wordorder from source to target language can belearned explicitly.3.1 The  word level al ignment:a l ignment templatesIn this section we will describe how we modelthe translation of shallow phrases.22?
3 .
?
m m m?
?
m ?
?
?T1 m .
.
.
.Ti: zwei, drei, vier, ffinf, ...T2: UhrT3: vormittags, nachmittags, abends,$1: two, three, four, five .
.
.
.$2: o'clock$3: inS4: theS5: morning, evening, afternoon, ...Figure 2: Example of an alignment emplateand bilingual word classes.The key element of our translation model arethe alignment tempJa(es.
An alignment em-plate z is a triple (F, E, A) which describes thealignment A between a source class sequenceand a target class sequence E.The alignment A is represented as a matrixwith binary values.
A matrix element" withvalue 1 means that the words at the correspond-ing positions are aligned and the value 0 meansthat the words are not aligned.
If a source wordis not aligned to a target word then it is alignedto the empty word e0 which shall be at the imag-inary position i = 0.
This alignment represen-tation is a generalization of the baseline align-ments described in (Brown et al, 1993) and al-lows for many-to-many alignments.The classes used in F and E are automati-cally trained bilingual classes using the methoddescribed in (Och, 1999) and constitute a parti-tion of the vocabulary of source and target lan-guage.
The class functions .T and E map wordsto their classes.
The use of classes instead ofwords themselves has the advantage of a bettergeneralization.
If there exist classes in sourceand target language which contain all towns itis possible that an alignment emplate learnedusing a special town can be generalized to alltowns.
In Fig.
2 an example of an alignmenttemplate is shown.An alignment emplate z = (F, E, A) is ap-plicable to a sequence of source words \] if thealignment template classes and the classes of thesource words are equal: .T(\]) = F. The appli-cation of the alignment emplate z constrainsthe target words ~ to coffrespond to the targetclass sequence: E(~) = E.The application of an alignment templatedoes not determine the target words, but onlyconstrains them.
For the selection of words fromclasses we use a statistical model for p(SIz,/)based on the lexicon probabilities of a statisticallexicon p(f\[e).
We assume a mixture alignmentbetween the source and target language wordsconstrained by the alignment matrix A:p ( \ ] I (F ,E ,A) ,~)  = ~(E(~) ,k )5(7( / ) ,F ) .II I  P(fj iA,~) (2)j= lIp(f j lA, 8) = Ep( i \ [ j ;A ) .p ( f j \ [e i ) (3 )i=OA(i , j )= E iA( i , j )  (4)3.2p(ilj; A)The phrase level alignmentIn order to describe the phrase level alignmentin a formal way, we first decompose both thesource sentence f l  J and the target sentence l /into a sequence of phrases (k = 1, .
.
.
,  K):fx g = /1 ~ , fk = f jk-x+l, '",f jkef  ---- e l  K , ek ---- e ik_ l+ l , .
.
.
,e ikIn order to simplify the notation and the pre-sentation, we ignore the fact that there can be alarge number of possible segmentations and as-sume that there is only one segmentation.
In theprevious section, we have described the align-ment within the phrases.
For the alignment 5~"?
between the source phrases ~1K and the targetphrases /~,  we obtain the following equation:Pr(f~g\[e{) = Pr(/~l~)aft= X:Pr(afl  )-K= I I  p(akla -z, K) p(Ll .k)5,1K k=l23For the phrase level alignment we use afirst-order alignment model p(Sklgl k-~,K) =p(SklSk_l, K) which is in addition constrainedto be a permutation of the K phrases.For the translation of one phrase, we intro-duce the alignment emplate as an unknownvariable:P(\]I~) = Z P(zle)"P(Ylz, e) (5)ZThe probability p(zl~ ) to apply an alignmenttemplate gets estimated by relative frequencies(see next section).
The probability p(flz, ~) isdecomposed by Eq.
(2).3.3 TrainingIn this section we show how we obtain the pa-rameters of our translation model by using aparallel training corpus:1.
We train two HMM alignment models (Vo-gel et al, 1996) for the two translation di-rections f ~ e and e ~ f by applyingthe EM-algorithm.
However we do not ap-ply maximum approximation i  training,thereby obtaining slightly improved align-ments.2.
For each translation direction we calcu-late the Viterbi-alignment of the transla-tion models determined in the previousstep.
Thus we get two alignment vectorsal J and bl / for each sentence.We increase the quality of the alignmentsby combining the two alignment vectorsinto one alignment matrix using the fol-lowing method.
A1 = {(aj,j)\[j = 1.. .
J}and A2 = {(i, bi)li = 1.. .
I} denote the setof links in the two Viterbi-alignments.
Ina first step the intersection A = A1 n A2is determined.
The elements within A arejustified by both Viterbi-alignments andare therefore very reliable.
We now ex-tend the alignment A iteratively by addinglinks (i, j) occurring only in A1 or in A2 ifthey have a neighbouring link already in Aor if neither the word fj nor the word eiare aligned in A.
The alignment (i, j) hasthe neighbouring links (i - 1,j), (i, j - 1),(i + 1, j), and (i, j + 1).
In the Verbmobiltask (Table 1) the precision of the baselineViterbi alignments i 83.3 percent with En-glish as source language and 81.8 percentwith German as source language.
Usingthis heuristic we get an alignment matrixwith a precision of 88.4 percent without lossin recall.3.
We estimate a bilingual word lexicon p(fle)by the relative frequencies of the alignmentdetermined in the previous tep:p(fle) _ hA( f ,  e) (6)n(e)Here nA(f,e) is the frequency that theword f is aligned to e and n(e) is the fre-quency of e in the training corpus.4.
We determine word classes for source andtarget language.
A naive approach for do-ing this would be the use of monolinguallyoptimized word classes in source and tar-get language.
Unfortunately we can not ex-pect that there is a direct correspondencebetween independently optimized classes.Therefore monolingually optimized wordclasses do not seem to be useful for ma-chine translation.We determine correlated bilingual classesby using the method described in (Och,1999).
The basic idea of this method is toapply a maximum-likelihood approach tothe joint probability of the parallel trainingcorpus.
The resulting optimization crite-rion for the bilingual word classes is similarto the one used in monolingual maximum-likelihood word clustering.5.
We count all phrase-pairs of the trainingcorpus which are consistent with the align-ment matrix determined in step 2.
Aphrase-pair is consistent with the align-ment if the words within the source phraseare only aligned to words within the tar-get phrase.
Thus we obtain a count n(z)of how often an alignment emplate oc-curred in the aligned training corpus.
Theprobability of using an alignment templateneeded by Eq.
(5) is estimated by relativefrequency:p(z = (F,E,.a)I~) = n(z)" ~(k,E(~))n(E(~))(7)Fig.
3 shows some of the extracted align-ment templates.
The extraction algorithm24nineteenth "the 'about ?how "O======================th .
.
.
.
.
.
.
.
.
.
.
.
I "1"in .
.
.
.
.
.
.
.
.
.
.
I "1"o.oloo  .
.
.
.
.
.
.
.
.
.
.
l "  I "two .
.
.
.
.
.
.
.
.
.
.
1?
l "m a y b e  .
.
.
.
.
.
.
.
.?
o o .
.
.
.
.
.
at  ,~?
?
.
.
.
.
.
.
.?
?
.
.
0 .
.
.
.
.
.
.
..~  ~ | ~ ~ "Y~ ~'~ ~ g'-O?
a lFigure 3: Example of a word alignment andsome learned alignment templates.does not perform a selection of good or badalignment templates - it simply extracts allpossible alignment templates.3.4 SearchFor decoding we use the following search crite-rion:arg max {p(e~).p(e~lf~)) (8)4This decision rule is an approximation to Eq.
(1)which would use the translation probabilityp(flJle{).
Using the simplification it is easy tointegrate translation and language model in thesearch process as both models predict targetwords.
As experiments have shown this simpli-fication does not affect he quality of translationresults.To allow the influence of long contexts weuse a class-based five-gram language model withbacking-off.The search space denoted by Eq.
(8) is verylarge.
Therefore we apply two preprocessingsteps before the translation of a sentence:1.
We determine the set of all source phrasesin f for which an applicable alignment tem-plate exists.
Every possible application ofan alignment emplate to a sub-sequenceof the source sentence is called alignmenttemplate instantiation.2.
We now perform a segmentation f the in-put sentence.
We search for a sequence ofphrases fl o .
.
.o /k  = fl J with:Karg max I I  maxz p(zlfk ) (9)\]lO...oh=:: k=lThis is done efficiently by dynamic pro-gramming.
Because of the simplified de-cision rule (Eq.
(8)) it is used in Eq.
(9)p(z\]fk) instead of p(z\]~k).Afterwards the actual translation process be-gins.
It has a search organization along the po-sitions of the target language string.
In searchwe produce partial hypotheses, each of whichcontains the following information:1. the last target word produced,2.
the state of the language model (the classesof the last four target words),3. a bit-vector epresenting the already cov-ered positions of the source sentence,4.
a reference to the alignment template in-stantiation which produced the last targetword,5.
the position of the last target word in thealignment template instantiation,6.
the accumulated costs (the negative loga-rithm of the probabilities) of all previousdecisions,7.
a reference to the previous partial hypoth-esis.A partial hypothesis is extended by append-ing one target word.
The set of all partial hy-potheses can be structured as a graph with asource node representing the sentence start, leafnodes representing full translations and inter-mediate nodes representing partial hypotheses.We recombine partial hypotheses which cannotbe distinguished by neither language model nortranslation model.
When the elements 1 - 5 oftwo partial hypotheses do not allow to distin-guish between two hypotheses it is possible todrop the hypothesis with higher costs for thesubsequent search process.We also use beam-search in order to handlethe huge search space.
We compare in beam-search hypotheses which cover different parts of25the input sentence.
This makes the compari-son of the costs somewhat problematic.
There-fore we integrate an (optimistic) estimation ofthe remaining costs to arrive at a full trans-lation.
This can be done efficiently by deter-mining in advance for each word in the sourcelanguage sentence a lower bound for the costsof the translation of this word.
Together withthe bit-vector stored in a partial hypothesis itis possible to achieve an efficient estimation ofthe remaining costs.4 Trans la t ion  resu l tsThe "Verbmobil Task" (Wahlster, 1993) is aspeech translation task in the domain of ap-pointment scheduling, travel planning, and ho-tel reservation.
The task is difficult because itconsists of spontaneous speech and the syntac-tic structures of the sentences are less restrictedand highly variable.The translation direction is from German toEnglish which poses special problems due to thebig difference in the word order of the two lan-guages.
We present results on both the texttranscription and the speech recognizer outputusing the alignment template approach and thesingle-word based approach.The text input was obtained by manu-ally transcribing the spontaneously spoken sen-tences.
There was no constraint on the length ofthe sentences, and some of the sentences in thetest corpus contain more than 50 words.
There-fore, for text input, each sentence is split intoshorter units using the punctuation marks.
Thesegments thus obtained were translated sepa-rately, and the final translation was obtainedby concatenation.In the case of speech input, the speech recog-nizer along with a prosodic module producedso-called prosodic markers which are equivalentto punctuation marks in written language.
Theexperiments for speech input were performed onthe single-best sentence of the recognizer.
Therecognizer had a word error rate of 31.0%.
Con-sidering only the real words without the punc-tuation marks, the word error rate was smaller,namely 20.3%.A summary of the corpus used in the experi-ments is given in Table 1.
Here the term wordrefers to full-form word as there is no morpho-logical processing involved.
In some of our ex-periments we use a domain-specific preprocess-ing which consists of a list of 803 (for German)and 458 (for English) word-joinings and word-splittings for word compounds, numbers, datesand proper names.
To improve the lexicon prob-abilities and to account for unseen words weadded a manually created German-English dic-tionary with 13 388 entries.
The classes usedwere constrained so that all proper names wereincluded in a single class.
Apart from this, theclasses were automatically trained using the de-scribed bilingual clustering method.
For each ofthe two languages 400 classes were used.For the single-word based approach, we usedthe manual dictionary as well as the preprocess-ing steps described above.
Neither the transla-tion model nor the language model used classesin this case.
In principal, when re-orderingwords of the source string, words of the Germanverb group could be moved over punctuationmarks, although it was penalized by a constantcost.Table 1: Training and test conditions for theVerbmobil task.
The extended vocabulary in-cludes the words of the manual dictionary.
Thetrigram perplexity (PP) is given.TrainTestSentencesWordsVoc.Extended Voc.SentencesWordsPPGerman I English34465363 514 383 5096 381 3 7669 062 8 4371471 968 2 173- 31.5In all experiments, we use the following threeerror criteria:?
WER (word error rate):The WER is computed as the minimumnumber of substitution, insertion and dele-tion operations that have to be performedto convert he generated string into the tar-get string.
This performance criterion iswidely used in speech recognition.?
PER (position-independent word errorrate):A shortcoming of the WER is the fact thatit requires a perfect word order.
This is26Table 2: Exper iments  for Text and Speech Input:  Word error rate (WER), position-independent word error rate (PER) and subjective sentence rror rate (SSER) with/without pre-processing (147 sentences = 1 968 words of the Verbmobil task).Single-Word Based ApproachAlignrtlent TemplatesText No 53.4 38.3 35.7Yes 56.0 41.2 35.3Speech No 67.8 50.1 54.8Yes 67.8 51.4 52.7TextSpeechNo 35.3 49.5 31.5Yes 48.3 35.1 27.2No 63.5 45.6 52.4Yes 62.8 45.6 50.3particularly a problem for the Verbmobiltask, where the word order of the German-English sentence pair can be quite different.As a result, the word order of the automat-ically generated target sentence can be dif-ferent from that of the target sentence, butnevertheless acceptable so that the WERmeasure alone could be misleading.
In or-der to overcome this problem, we intro-duce as additional measure the position-independent word error rate (PER).
Thismeasure compares the words in the two sen-tences without taking the word order intoaccount.
Words that have no matchingcounterparts are counted as substitutionerrors.
Depending on whether the trans-lated sentence is longer or shorter than thetarget ranslation, the remaining words re-sult in either insertion or deletion errors inaddition to substitution errors.
The PERis guaranteed to be less than or equal tothe WER.SSER (subjective sentence rror rate):For a more detailed analysis, subjectivejudgments by test persons are necessary.Each translated sentence was judged by ahuman examiner according to an error scalefrom 0.0 to 1.0.
A score of 0.0 means thatthe translation is semantically and syntac-tically correct, a score of 0.5 means that asentence is semantically correct but syntac-tically wrong and a score of 1.0 means thatthe sent6nce is semantically wrong.
Thehuman examiner was offered the translatedsentences ofthe two approaches atthe sametime.
As a result we expect a better possi-bility of reproduction.The results of the translation experimentsusing the single-word based approach and thealignment template approach on text input andon speech input are summarized in Table 2.
Theresults are shown with and without the use ofdomain-specific preprocessing.
The alignmenttemplate approach produces better translationresults than the single-word based approach.From this we draw the conclusion that it is im-portant o model word groups in source and tar-get language.
Considering the recognition worderror rate of 31% the degradation ofabout 20%by speech input can be expected.
The averagetranslation time on an Alpha workstation for asingle sentence is about one second for the align-ment template apprbach and 30 seconds for thesingle-word based search procedure.Within the Verbmobil project other trans-lation modules based on rule-based, example-based and dialogue-act-based translation areused.
We are not able to present results withthese methods using our test corpus.
But inthe current Verbmobil prototype the prelimi-nary evaluations show that the statistical meth-ods produce comparable or better esults thanthe other systems.
An advantage of the sys-tem is that it is robust and always produces atranslation result even if the input of the speechrecognizer is quite incorrect.5 SummaryWe have described two approaches to performstatistical machine translation which extend thebaseline alignment models.
The single-word27based approach allows for the the possibility ofone-to-many alignments.
The alignment tem-plate approach uses two different alignment lev-els: a phrase level alignment between phrasesand a word level alignment between singlewords.
As a result the context of words hasa greater influence and the changes in wordorder from source to target language can belearned explicitly.
An advantage of both meth-ods is that they learn fully automatically b  us-ing a bilingual training corpus and are capa-ble of achieving better translation results on alimited-domain task than other example-basedor rule-based translation systems.AcknowledgmentThis work has been partially supported aspart of the Verbmobil project (contract number01 IV 701 T4) by the German Federal Ministryof Education, Science, Research and Technol-ogy and as part of the EuTrans project by theby the European Community (ESPRIT projectnumber 30268).ReferencesHiyan Alshawi, Srinivas Bangalore, and ShonaDouglas.
1998.
Automatic acquisition of hi-erarchical transduction models for machinetranslation.
In COLING-ACL '98: AnnualConf.
of the Association for ComputationalLinguistics and 17th Int.
Conf.
on Compu-tational Linguistics, volume 1, pages 41-47,Montreal, Quebec, Canada, August.Peter F. Brown, Stephen A. Della Pietra, Vin-cent J. Della Pietra, and Robert L. Mercer.1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Compu-tational Linguistics, 19(2):263-311.Hermann Ney.
1999.
Speech translation: Cou-pling of recognition and translation.
In Proc.Int.
Conf.
on Acoustics, Speech, and Sig-nal Processing, pages 517-520, Phoenix, AR,March.Sonja Niessen, Stephan Vogel, Hermann Ney,and Christoph Tillmann.
1998.
A DP-basedsearch algorithm for statistical machine trans-lation.
In COLING-ACL '98: Annual Conf.of the Association for Computational Lin-guistics and 17th Int.
Conf.
on Computa-tional Linguistics, pages 960-967, Montreal,Canada, August.Franz Josef Och and Hans Weber.
1998.
Im-proving statistical natural anguage transla-tion with categories and rules.
In Proc.
ofthe 35th Annual Conf.
of the Association forComputational Linguistics and the 17th Int.Conf.
on Computational Linguistics, pages985-989, Montreal, Canada, August.Franz Josef Och.
1999.
An efficient method todetermine bilingual word classes.
In EACL'99: Ninth Conf.
of the Europ.
Chapter ofthe Association for Computational Linguis-tics, Bergen, Norway, June.Christoph Tillmann, Stephan Vogel, HermannNey, and Alex Zubiaga.
1997.
A DP-basedsearch using monotone alignments in statisti-cal translation.
In Proc.
35th Annual Conf.
ofthe Association for Computational Linguis-tics, pages 289-296, Madrid, Spain, July.Stephan Vogel, Hermann Ney, and ChristophTillmann.
1996.
HMM-based word align-ment in statistical translation.
In COLING'96: The 16th Int.
Conf.
on ComputationalLinguistics, pages 836-841, Copenhagen, Au-gust.Wolfgang Wahlster.
1993.
Verbmobih Transla-tion of face-to-face dialogs.
In Proc of the MTSummit IV, pages 127-135, Kobe, Japan.Ye-Yi Wang and Alex Waibel.
1998.
Modelingwith structures in statistcal machine transla-tion.
In COLING-A CL '98: Annual Conf.
ofthe Association for Computational Linguis-tics and 17th Int.
Conf.
on ComputationalLinguistics, volume 2, pages 1357-1363, Mon-treal, Quebec, Canada.28
