Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265?271,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPOS induction with distributional and morphological informationusing a distance-dependent Chinese restaurant processKairit SirtsInstitute of Cybernetics atTallinn University of Technologysirts@ioc.eeJacob EisensteinSchool of Interactive ComputingGeorgia Institute of Technologyjacobe@gatech.eduMicha ElsnerDepartment of LinguisticsThe Ohio State Universitymelsner0@gmail.comSharon GoldwaterILCC, School of InformaticsUniversity of Edinburghsgwater@inf.ed.ac.ukAbstractWe present a new approach to inducing thesyntactic categories of words, combiningtheir distributional and morphological prop-erties in a joint nonparametric Bayesianmodel based on the distance-dependentChinese Restaurant Process.
The priordistribution over word clusterings uses alog-linear model of morphological similar-ity; the likelihood function is the probabil-ity of generating vector word embeddings.The weights of the morphology modelare learned jointly while inducing part-of-speech clusters, encouraging them to co-here with the distributional features.
Theresulting algorithm outperforms competi-tive alternatives on English POS induction.1 IntroductionThe morphosyntactic function of words is reflectedin two ways: their distributional properties, andtheir morphological structure.
Each informationsource has its own advantages and disadvantages.Distributional similarity varies smoothly with syn-tactic function, so that words with similar syntacticfunctions should have similar distributional proper-ties.
In contrast, there can be multiple paradigmsfor a single morphological inflection (such as pasttense in English).
But accurate computation ofdistributional similarity requires large amounts ofdata, which may not be available for rare words;morphological rules can be applied to any wordregardless of how often it appears.These observations suggest that a general ap-proach to the induction of syntactic categoriesshould leverage both distributional and morpho-logical features (Clark, 2003; Christodoulopouloset al, 2010).
But these features are difficult tocombine because of their disparate representations.Distributional information is typically representedin numerical vectors, and recent work has demon-strated the utility of continuous vector represen-tations, or ?embeddings?
(Mikolov et al, 2013;Luong et al, 2013; Kim and de Marneffe, 2013;Turian et al, 2010).
In contrast, morphology isoften represented in terms of sparse, discrete fea-tures (such as morphemes), or via pairwise mea-sures such as string edit distance.
Moreover, themapping between a surface form and morphologyis complex and nonlinear, so that simple metricssuch as edit distance will only weakly approximatemorphological similarity.In this paper we present a new approach for in-ducing part-of-speech (POS) classes, combiningmorphological and distributional information in anon-parametric Bayesian generative model basedon the distance-dependent Chinese restaurant pro-cess (ddCRP; Blei and Frazier, 2011).
In the dd-CRP, each data point (word type) selects anotherpoint to ?follow?
; this chain of following linkscorresponds to a partition of the data points intoclusters.
The probability of word w1following w2depends on two factors: 1) the distributional simi-larity between all words in the proposed partitioncontaining w1and w2, which is encoded using aGaussian likelihood function over the word embed-dings; and 2) the morphological similarity betweenw1and w2, which acts as a prior distribution on theinduced clustering.
We use a log-linear model tocapture suffix similarities between words, and learnthe feature weights by iterating between samplingand weight learning.We apply our model to the English section ofthe the Multext-East corpus (Erjavec, 2004) in or-der to evaluate both against the coarse-grained and265fine-grained tags, where the fine-grained tags en-code detailed morphological classes.
We find thatour model effectively combines morphological fea-tures with distributional similarity, outperformingcomparable alternative approaches.2 Related workUnsupervised POS tagging has a long history inNLP.
This paper focuses on the POS inductionproblem (i.e., no tag dictionary is available), andhere we limit our discussion to very recent sys-tems.
A review and comparison of older systemsis provided by Christodoulopoulos et al (2010),who found that imposing a one-tag-per-word-typeconstraint to reduce model flexibility tended toimprove system performance; like other recentsystems, we impose that constraint here.
Recentwork also shows that the combination of morpho-logical and distributional information yields thebest results, especially cross-linguistically (Clark,2003; Berg-Kirkpatrick et al, 2010).
Since then,most systems have incorporated morphology insome way, whether as an initial step to obtain pro-totypes for clusters (Abend et al, 2010), or asfeatures in a generative model (Lee et al, 2010;Christodoulopoulos et al, 2011; Sirts and Alum?ae,2012), or a representation-learning algorithm (Yat-baz et al, 2012).
Several of these systems use asmall fixed set of orthographic and/or suffix fea-tures, sometimes obtained from an unsupervisedmorphological segmentation system (Abend et al,2010; Lee et al, 2010; Christodoulopoulos et al,2011; Yatbaz et al, 2012).
Blunsom and Cohn?s(2011) model learns an n-gram character modelover the words in each cluster; we learn a log-linear model, which can incorporate arbitrary fea-tures.
Berg-Kirkpatrick et al (2010) also includea log-linear model of morphology in POS induc-tion, but they use morphology in the likelihoodterm of a parametric sequence model, thereby en-couraging all elements that share a tag to have thesame morphological features.
In contrast, we usepairwise morphological similarity as a prior in anon-parametric clustering model.
This means thatthe membership of a word in a cluster requires onlymorphological similarity to some other element inthe cluster, not to the cluster centroid; which maybe more appropriate for languages with multiplemorphological paradigms.
Another difference isthat our non-parametric formulation makes it un-necessary to know the number of tags in advance.3 Distance-dependent CRPThe ddCRP (Blei and Frazier, 2011) is an extensionof the CRP; like the CRP, it defines a distributionover partitions (?table assignments?)
of data points(?customers?).
Whereas in the regular CRP eachcustomer chooses a table with probability propor-tional to the number of customers already sittingthere, in the ddCRP each customer chooses anothercustomer to follow, and sits at the same table withthat customer.
By identifying the connected compo-nents in this graph, the ddCRP equivalently definesa prior over clusterings.If ciis the index of the customer followed bycustomer i, then the ddCRP prior can be writtenP (ci= j) ?
{f(dij) if i 6= j?
if i = j,(1)where dijis the distance between customers i and jand f is a decay function.
A ddCRP is sequential ifcustomers can only follow previous customers, i.e.,dij=?
when i > j and f(?)
= 0.
In this case,if dij= 1 for all i < j then the ddCRP reduces tothe CRP.Separating the distance and decay functionmakes sense for ?natural?
distances (e.g., the num-ber of words between word i and j in a document,or the time between two events), but they can alsobe collapsed into a single similarity function.
Wewish to assign higher similarities to pairs of wordsthat share meaningful suffixes.
Because we do notknow which suffixes are meaningful a priori, weuse a maximum entropy model whose features in-clude all suffixes up to length three that are sharedby at least one pair of words.
Our prior is then:P (ci= j|w, ?)
?
{ewTg(i,j)if i 6= j?
if i = j,(2)where gs(i, j) is 1 if suffix s is shared by ith andjth words, and 0 otherwise.We can create an infinite mixture model by com-bining the ddCRP prior with a likelihood functiondefining the probability of the data given the clusterassignments.
Since we are using continuous-valuedvectors (word embeddings) to represent the distri-butional characteristics of words, we use a multi-variate Gaussian likelihood.
We will marginalizeover the mean ?
and covariance ?
of each clus-ter, which in turn are drawn from Gaussian andinverse-Wishart (IW) priors respectively:?
?
IW (?0,?0) ?
?
N (?0,?/?0) (3)266The full model is then:P (X,c,?,?|?,w, ?)
(4)=K?k=1P (?k|?)p(?k|?k,?
)?n?i=1(P (ci|w, ?
)P (xi|?zi,?zi)),where ?
are the hyperparameters for (?,?)
and ziis the (implicit) cluster assignment of the ith wordxi.
With a CRP prior, this model would be an infi-nite Gaussian mixture model (IGMM; Rasmussen,2000), and we will use the IGMM as a baseline.4 InferenceThe Gibbs sampler for the ddCRP integrates overthe Gaussian parameters, sampling only followervariables.
At each step, the follower link cifor asingle customer i is sampled, which can implicitlyshift the entire block of n customers fol(i) who fol-low i into a new cluster.
Since we marginalize overthe cluster parameters, computing P (ci= j) re-quires computing the likelihood P (fol(i),Xj|?
),where Xjare the k customers already clusteredwith j.
However, if we do not merge fol(i)with Xj, then we have P (Xj|?)
in the overalljoint probability.
Therefore, we can decomposeP (fol(i),Xj|?)
= P (fol(i)|Xj,?
)P (Xj|?)
andneed only compute the change in likelihood due tomerging in fol(i):1:P (fol(i)|Xj,?)
= pi?nd/2?d/2k|?k|?k/2?d/2n+k|?n+k|?n+k/2?d?i=1?(?n+k+1?i2)?
(?k+1?i2), (5)where the hyperparameters are updated as ?n=?0+ n, ?n= ?0+ n, and?n=?0?0+ x?
?0+ n(6)?n= ?0+Q+ ?0?0?0T?
?n?n?Tn, (7)where Q =?ni=1xixTi.Combining this likelihood term with the prior,the probability of customer i following j isP (ci= j|X,?,w, ?)?
P (fol(i)|Xj,?
)P (ci= j|w, ?).
(8)1http://www.stats.ox.ac.uk/?teh/re-search/notes/GaussianInverseWishart.pdfOur non-sequential ddCRP introduces cyclesinto the follower structure, which are handled in thesampler as described by Socher et al (2011).
Also,the block of customers being moved around can po-tentially be very large, which makes it easy for thelikelihood term to swamp the prior.
In practice wefound that introducing an additional parameter a(used to exponentiate the prior) improved results?although we report results without this exponent aswell.
This technique was also used by Titov andKlementiev (2012) and Elsner et al (2012).Inference also includes optimizing the featureweights for the log-linear model in the ddCRPprior (Titov and Klementiev, 2012).
We interleaveL-BFGS optimization within sampling, as in MonteCarlo Expectation-Maximization (Wei and Tanner,1990).
We do not apply the exponentiation parame-ter a when training the weights because this proce-dure affects the follower structure only, and we donot have to worry about the magnitude of the like-lihood.
Before the first iteration we initialize thefollower structure: for each word, we choose ran-domly a word to follow from amongst those withthe longest shared suffix of up to 3 characters.
Thenumber of clusters starts around 750, but decreasessubstantially after the first sampling iteration.5 ExperimentsData For our experiments we used the Englishword embeddings from the Polyglot project (Al-Rfou?
et al, 2013)2, which provides embeddingstrained on Wikipedia texts for 100,000 of the mostfrequent words in many languages.We evaluate on the English part of the Multext-East (MTE) corpus (Erjavec, 2004), which providesboth coarse-grained and fine-grained POS labelsfor the text of Orwell?s ?1984?.
Coarse labels con-sist of 11 main word classes, while the fine-grainedtags (104 for English) are sequences of detailedmorphological attributes.
Some of these attributesare not well-attested in English (e.g.
gender) andsome are mostly distinguishable via semantic anal-ysis (e.g.
1st and 2nd person verbs).
Many tags areassigned only to one or a few words.
Scores for thefine-grained tags will be lower for these reasons,but we argue below that they are still informative.Since Wikipedia and MTE are from differentdomains their lexicons do not fully overlap; we2https://sites.google.com/site/rmyeid/projects/polyglot267Wikipedia tokens 1843MMultext-East tokens 118KMultext-East types 9193Multext-East & Wiki types 7540Table 1: Statistics for the English Polyglot word embeddingsand English part of MTE: number of Wikipedia tokens usedto train the embeddings, number of tokens/types in MTE, andnumber of types shared by both datasets.take the intersection of these two sets for trainingand evaluation.
Table 1 shows corpus statistics.Evaluation With a few exceptions (Biemann,2006; Van Gael et al, 2009), POS induction sys-tems normally require the user to specify the num-ber of desired clusters, and the systems are evalu-ated with that number set to the number of tags inthe gold standard.
For corpora such as MTE withboth fine-grained and coarse-grained tages, pre-vious evaluations have scored against the coarse-grained tags.
Though coarse-grained tags havetheir place (Petrov et al, 2012), in many casesthe distributional and morphological distinctionsbetween words are more closely aligned with thefine-grained tagsets, which typically distinguishbetween verb tenses, noun number and gender,and adjectival scale (comparative, superlative, etc.
),so we feel that the evaluation against fine-grainedtagset is more relevant here.
For better comparisonwith previous work, we also evaluate against thecoarse-grained tags; however, these numbers arenot strictly comparable to other scores reported onMTE because we are only able to train and evalu-ate on the subset of words that also have Polyglotembeddings.
To provide some measure of the dif-ficulty of the task, we report baseline scores usingK-means clustering, which is relatively strong base-line in this task (Christodoulopoulos et al, 2011).There are several measures commonly used forunsupervised POS induction.
We report greedyone-to-one mapping accuracy (1-1) (Haghighi andKlein, 2006) and the information-theoretic score V-measure (V-m), which also varies from 0 to 100%(Rosenberg and Hirschberg, 2007).
In previouswork it has been common to also report many-to-one (m-1) mapping but this measure is particularlysensitive to the number of induced clusters (moreclusters yield higher scores), which is variable forour models.
V-m can be somewhat sensitive to thenumber of clusters (Reichart and Rappoport, 2009)but much less so than m-1 (Christodoulopouloset al, 2010).
With different number of inducedand gold standard clusters the 1-1 measure suffersbecause some induced clusters cannot be mappedto gold clusters or vice versa.
However, almost halfthe gold standard clusters in MTE contain just afew words and we do not expect our model to beable to learn them anyway, so the 1-1 measure isstill useful for telling us how well the model learnsthe bigger and more distinguishable classes.In unsupervised POS induction it is standard toreport accuracy on tokens even when the model it-self works on types.
Here we report also type-basedmeasures because these can reveal differences inmodel behavior even when token-based measuresare similar.Experimental setup For baselines we use K-means and the IGMM, which both only learn fromthe word embeddings.
The CRP prior in the IGMMhas one hyperparameter (the concentration param-eter ?
); we report results for ?
= 5 and 20.
Boththe IGMM and ddCRP have four hyperparameterscontrolling the prior over the Gaussian cluster pa-rameters: ?0, ?0, ?0and ?0.
We set the prior scalematrix ?0by using the average covariance froma K-means run with K = 200.
When setting theaverage covariance as the expected value of the IWdistribution the suitable scale matrix can be com-puted as ?0= E [X] (?0?
d?
1), where ?0is theprior degrees of freedom (which we set to d + 10)and d is the data dimensionality (64 for the Poly-glot embeddings).
We set the prior mean ?0equalto the sample mean of the data and ?0to 0.01.We experiment with three different priors for theddCRP model.
All our ddCRP models are non-sequential (Socher et al, 2011), allowing cyclesto be formed.
The simplest model, ddCRP uni-form, uses a uniform prior that sets the distancebetween any two words equal to one.3The secondmodel, ddCRP learned, uses the log-linear priorwith weights learned between each two Gibbs iter-ations as explained in section 4.
The final model,ddCRP exp, adds the prior exponentiation.
The ?parameter for the ddCRP is set to 1 in all experi-ments.
For ddCRP exp, we report results with theexponent a set to 5.Results and discussion Table 2 presents all re-sults.
Each number is an average of 5 experiments3In the sequential case this model would be equivalent tothe IGMM (Blei and Frazier, 2011).
Due to the nonsequen-tiality this equivalence does not hold, but we do expect to seesimilar results to the IGMM.268Fine types Fine tokens Coarse tokensModel K Model K-means Model K-means Model K-meansK-means 104 or 11 16.1 / 47.3 - 39.2 / 62.0 - 44.4 / 45.5 -IGMM, ?
= 5 55.6 41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0IGMM, ?
= 20 121.2 35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9ddCRP uniform 80.4 50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2ddCRP learned 89.6 50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 63.2 48.9 / 62.0 41.1 / 55.1ddCRP exp, a = 5 47.2 64.0 / 60.3 25.0 / 50.3 55.1 / 66.4 33.0 / 59.1 47.8 / 55.1 36.9 / 53.1Table 2: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokensusing coarse-grained tags.
For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-mscores.
The second column under each evaluation setting gives the scores for K-means with K equal to the number of clustersinduced by the model in that row.with different random initializations.
For each eval-uation setting we provide two sets of scores?firstare the 1-1 and V-m scores for the given model,second are the comparable scores for K-means runwith the same number of clusters as induced by thenon-parametric model.These results show that all non-parametric mod-els perform better than K-means, which is a strongbaseline in this task (Christodoulopoulos et al,2011).
The poor performace of K-means can beexplained by the fact that it tends to find clustersof relatively equal size, although the POS clus-ters are rarely of similar size.
The common nounsingular class is by far the largest in English, con-taining roughly a quarter of the word types.
Non-parametric models are able to produce cluster ofdifferent sizes when the evidence indicates so, andthis is clearly the case here.From the token-based evaluation it is hard tosay which IGMM hyperparameter value is bettereven though the number of clusters induced differsby a factor of 2.
The type-base evaluation, how-ever, clearly prefers the smaller value with fewerclusters.
Similar effects can be seen when com-paring IGMM and ddCRP uniform.
We expectedthese two models perform on the same level, andtheir token-based scores are similar, but on the type-based evaluation the ddCRP is clearly superior.
Thedifference could be due to the non-sequentiality,or becuase the samplers are different?IGMM en-abling resampling only one item at a time, ddCRPperforming blocked sampling.Further we can see that the ddCRP uniform andlearned perform roughly the same.
Although theprior in those models is different they work mainlyusing the the likelihood.
The ddCRP with learnedprior does produce nice follower structures withineach cluster but the prior is in general too weakcompared to the likelihood to influence the cluster-ing decisions.
Exponentiating the prior reduces thenumber of induced clusters and improves results,as it can change the cluster assignment for somewords where the likelihood strongly prefers onecluster but the prior clearly indicates another.The last column shows the token-based evalua-tion against the coarse-grained tagset.
This is themost common evaluation framework used previ-ously in the literature.
Although our scores are notdirectly comparable with the previous results, ourV-m scores are similar to the best published 60.5(Christodoulopoulos et al, 2010) and 66.7 (Sirtsand Alum?ae, 2012).In preliminary experiments, we found that di-rectly applying the best-performing English modelto other languages is not effective.
Different lan-guages may require different parametrizations ofthe model.
Further study is also needed to verifythat word embeddings effectively capture syntaxacross languages, and to determine the amount ofunlabeled text necessary to learn good embeddings.6 ConclusionThis paper demonstrates that morphology and dis-tributional features can be combined in a flexi-ble, joint probabilistic model, using the distance-dependent Chinese Restaurant Process.
A key ad-vantage of this framework is the ability to includearbitrary features in the prior distribution.
Futurework may exploit this advantage more thoroughly:for example, by using features that incorporateprior knowledge of the language?s morphologicalstructure.
Another important goal is the evaluationof this method on languages beyond English.Acknowledgments: KS was supported by theTiger University program of the Estonian Infor-mation Technology Foundation for Education.
JEwas supported by a visiting fellowship from theScottish Informatics & Computer Science Alliance.We thank the reviewers for their helpful feedback.269ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2010.Improved unsupervised pos induction through pro-totype discovery.
In Proceedings of the 48th An-nual Meeting of the Association of ComputationalLinguistics, pages 1298?1307.Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual nlp.
In Proceedings of the Thir-teenth Annual Conference on Natural LanguageLearning, pages 183?192, Sofia, Bulgaria.
Associ-ation for Computational Linguistics.Taylor Berg-Kirkpatrick, Alexandre B.
C?ot?e, JohnDeNero, and Dan Klein.
2010.
Painless unsuper-vised learning with features.
In Proceedings of Hu-man Language Technologies: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590.Chris Biemann.
2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 7?12.David M Blei and Peter I Frazier.
2011.
Distancedependent chinese restaurant processes.
Journal ofMachine Learning Research, 12:2461?2488.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised partof speech induction.
In Proceedings of the 49th An-nual Meeting of the Association of ComputationalLinguistics, pages 865?874.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsuper-vised POS induction: How far have we come?
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2011.
A Bayesian mixture modelfor part-of-speech induction using multiple features.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the European chapter of theACL.Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.2012.
Bootstrapping a unified model of lexical andphonetic acquisition.
In Proceedings of the 50th An-nual Meeting of the Association of ComputationalLinguistics.Toma?z Erjavec.
2004.
MULTEXT-East version 3:Multilingual morphosyntactic specifications, lexi-cons and corpora.
In LREC.A.
Haghighi and D. Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics.Joo-Kyung Kim and Marie-Catherine de Marneffe.2013.
Deriving adjectival scales from continuousspace word representations.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.Yoong Keok Lee, Aria Haghighi, and Regina Barzi-lay.
2010.
Simple type-level unsupervised pos tag-ging.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages853?861.Minh-Thang Luong, Richard Socher, and Christo-pher D Manning.
2013.
Better word representationswith recursive neural networks for morphology.
InProceedings of the Thirteenth Annual Conference onNatural Language Learning.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of HumanLanguage Technologies: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 746?751.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC, May.Carl Rasmussen.
2000.
The infinite Gaussian mixturemodel.
In Advances in Neural Information Process-ing Systems 12, Cambridge, MA.
MIT Press.Roi Reichart and Ari Rappoport.
2009.
The nvi cluster-ing evaluation measure.
In Proceedings of the NinthAnnual Conference on Natural Language Learning,pages 165?173.A.
Rosenberg and J. Hirschberg.
2007.
V-measure:A conditional entropy-based external cluster evalua-tion measure.
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 410?42.Kairit Sirts and Tanel Alum?ae.
2012.
A hierarchi-cal Dirichlet process model for joint part-of-speechand morphology induction.
In Proceedings of Hu-man Language Technologies: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 407?416.Richard Socher, Andrew L Maas, and Christopher DManning.
2011.
Spectral chinese restaurant pro-cesses: Nonparametric clustering based on similar-ities.
In Proceedings of the Fifteenth InternationalConference on Artificial Intelligence and Statistics,pages 698?706.270Ivan Titov and Alexandre Klementiev.
2012.
Abayesian approach to unsupervised semantic role in-duction.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Jurgen Van Gael, Andreas Vlachos, and ZoubinGhahramani.
2009.
The infinite HMM for unsu-pervised PoS tagging.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 678?687, Singapore.Greg CG Wei and Martin A Tanner.
1990.
Amonte carlo implementation of the em algorithmand the poor man?s data augmentation algorithms.Journal of the American Statistical Association,85(411):699?704.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.Learning syntactic categories using paradigmaticrepresentations of word context.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 940?951.271
