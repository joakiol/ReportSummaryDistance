Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 417?424Manchester, August 2008A Local Alignment Kernel in the Context of NLPSophia KatrenkoInformatics InstituteUniversity of Amsterdamthe Netherlandskatrenko@science.uva.nlPieter AdriaansInformatics InstituteUniversity of Amsterdamthe Netherlandspietera@science.uva.nlAbstractThis paper discusses local alignment ker-nels in the context of the relation extrac-tion task.
We define a local alignmentkernel based on the Smith-Waterman mea-sure as a sequence similarity metric andproceed with a range of possibilities forcomputing a similarity between elementsof sequences.
We propose to use distri-butional similarity measures on elementsand by doing so we are able to incorporateextra information from the unlabeled datainto a learning task.
Our experiments sug-gest that a LA kernel provides promisingresults on some biomedical corpora largelyoutperforming a baseline.1 IntroductionRelation extraction is one of the tasks in thenatural language processing which is constantlyrevisited.
To date, there are many methodswhich have been proposed to tackle it.
Such ap-proaches often benefit from using syntactic infor-mation (Bunescu and Mooney, 2006) and back-ground knowledge (Sekimizu et al, 1998).
How-ever, it would be interesting to employ additionalinformation not necessarily contained in the train-ing set.
This paper presents a contribution to thework on relation extraction by combining statisti-cal information with string distance measures.
Inparticular, we propose to use a local alignment ker-nel to detect relations.The paper is organized as follows.
We start withthe definition of a local alignment kernel and showc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.how it is defined on the Smith-Waterman measure.We proceed by discussing how a substitution ma-trix can be constructed in the context of naturallanguage processing tasks.
Once a method is de-scribed, we turn to the task of relation extractionand present an experimental part.
We conclude bymentioning possible future directions.2 A Local Aligment KernelKernel methods are widely used for a variety ofnatural language processing task, starting fromPoS tagging to information extraction.
Many ofthe approaches employ the idea of combining ker-nels together which leads to a convolution kernel(Haussler, 1999).
The examples of convolutionmethods being successfully used in NLP are ker-nels based on dependency trees and shallow pars-ing (Moschitti, 2006; Zelenko et al, 2003).
Localalignment (LA) kernels also belong to the familyof convolution kernels but have not yet been ap-plied to NLP problems.Although the approaches listed above proved tobe accurate, they only use kernels which are de-signed by computing inner products between vec-tors of sequences.
Intuitively, methods using moreelaborate measures of similarity could provide bet-ter results but kernels defined on such measuresare not necessarily positive semi-definite.
Recentwork in the biomedical field shows that it is pos-sible to design valid kernels based on a similaritymeasure by solving the diagonal dominance prob-lem to ensure the semi-definiteness (Saigo et al,2006).
To illustrate it, Saigo et al (2004) con-sider the Smith-Waterman (SW) similarity mea-sure (Smith and Waterman, 1981) which has of-ten been used to compare two sequences of aminoacids.
The original Smith-Waterman score is cal-culated to achieve the best local alignment allow-417ing gaps.The Smith-Waterman measure belongs to thestring distance metrics which can be divided intoterm-based, edit-distance and HMM based metrics(Cohen et al, 2003).
Term-based distances suchas metrics based on TF-IDF score, consider a pairof word sequences as two sets of words neglectingtheir order.
In contrast, edit string distances treatthe entire sequences and, by comparing them, cal-culate the minimal number of the transformationoperations converting a sequence x into a sequencey.
Examples of string edit distances are Leven-shtein, Needleman-Wunsch and Smith-Watermanmetrics.
Levenshtein distance has been used innatural language processing field as a componentin the variety of tasks, including semantic role la-beling (Tjong Kim Sang et al, 2005), construc-tion of the paraphrase corpora (Dolan et al, 2004),evaluation of machine translation output (Leuschet al, 2003), and others.
Smith-Waterman distanceis mostly used in the biological domain, there are,however, some applications of a modified Smith-Waterman distance to the text data as well (Mongeand Elkan, 1996), (Cohen et al, 2003).
HMMbased measures present probabilistic extensions ofedit distances.According to the definition of a LA kernel,two strings (sequences) are considered similar ifthey have many local alignments with high scores(Saigo et al, 2006).
Given two sequences x =x1x2.
.
.
xnand y = y1y2.
.
.
ymof length n and mrespectively, Smith-Waterman distance is definedas the local alignment score of their best align-ment:SW (x, y) = maxpi?A(x,y)s(x, y, pi) (1)In the equation above, s(x, y, pi) is a score of alocal alignment pi of sequence x and y and A de-notes the set of all possible alignments.
This defi-nition can be rewritten by means of dynamic pro-gramming as follows:SW (i, j) = max8><>:0SW (i?
1, j ?
1) + d(xi, yj)SW (i?
1, j)?GSW (i, j ?
1)?G(2)In Equation 2, d(xi, yj) denotes a substitutionscore between two elements xiand yjand Gstands for a gap penalty.Unfortunately, the direct application of theSmith-Waterman score will not result in the validkernel.
A valid kernel based on the Smith-Waterman distance can be defined by summing upthe contribution of all possible alignments as fol-lows (Saigo et al, 2004):KLA=?pi?A(x,y)??
?s(x,y,pi) (3)It is shown that in the limit a LA kernel ap-proaches the Smith-Waterman score:lim??
?ln(1?KLA(x, y))= SW (x, y) (4)The results in the biological domain suggest thatkernels based on the Smith-Waterman distance aremore relevant for the comparison of amino acidsthan string kernels.
It is not clear whether thisholds when applied to natural language process-ing tasks.
In our view, it depends on the param-eters which are used, such as a substitution ma-trix and the penalty gaps.
It has been shown bySaigo (2006) that given a substitution matrix whichis equal to the identity matrix and no penalty gap,the Smith-Waterman score is a string kernel.2.1 How to define a substitution matrixd(?, ?
)?In order to use Smith-Waterman distance for ourpurposes, it is necessary to define a substitutionmatrix.
Unlike a matrix in the original Smith-Waterman measure defined by the similarity ofamino acids or a substitution matrix in (Monge andElkan, 1996) based on the exact and approximatematch of two characters (for instance, m and n),we introduce a matrix based on the distributionalsimilarity measures.
In our view, they are the mostnatural measures for the text data.
In other words,if we are to compare any two words given two se-quences of words, the elements sharing the samecontexts should be more similar to each other thanthose that do not.
In the context of the LA kernel,such metrics can be especially useful.
Consider,for instance, the labeled sequences of words whichare used as input for a machine learning method.To compare the sequences, we have to be able tocompare their elements, i.e.
words.
Now, if thereare some words in the test data that do not occurin the training set, it is still possible to carry out a418comparison if additional evidence is present.
Suchevidence can be provided by the distributional sim-ilarity metrics.There are a number of measures proposed overthe years, including such metrics as cosine, dicecoefficient, and Jaccard distance.
Distributionalsimilarity measures have been extensively studiedin (Lee, 1999; Weeds et al, 2004).We have chosen the following metrics: dice, co-sine and l2 (euclidean) whose definitions are givenin Table 1.
Here, xiand yjdenote two words andc stands for a context.
Similarly to (Lee, 1999),we use unsmoothed relative frequencies to deriveprobability estimates P .
In the definition of thedice coefficient, F (xi) = {c : P (c|xi) > 0}.We are mainly interested in the symmetric mea-sures (d(xi, yj) = d(yj, xi)) because of a symmet-ric positive semi-definite matrix required by ker-nel methods.
Consequently, such measures as theskew divergence were excluded from the consider-ation (Lee, 1999).The Euclidean measure as defined in Table 1does not necessarily vary from 0 to 1.
It was there-fore normalized by dividing an l2 score in Table 1by a maximum score and retracting it from 1.Measure Formulacosine d(xi, yj) =PcP (c|xi)?P (c|yj)?PcP (c|xi)2PcP (c|yj)2dice d(xi, yj) =2?F (xi)?F (yj)F (xi)?F (yj)l2 d(xi, yj) =pPc(P (c|xi)?
P (c|yj))2Table 1: Distributional similarity measures.3 A relation extraction taskMany approaches to relation extraction considersyntactic information.
In this paper we focus ondependency parsing.
The experiments in the pasthave already shown syntactic analysis to be usefulfor relation learning.
Like other work we extracta path between two nodes which correspond to thearguments of a binary relation.
We also assumethat each analysis results in a tree and since it is anacyclic graph, there exists only one path betweeneach pair of nodes.
We do not consider, however,the other structures that might be derived from thefull syntactic analysis as in, for example, subtreekernels (Moschitti, 2006).Consider, for instance, an example of interac-tion among proteins (5) whose syntactic analysisis given in Fig.
1.
Here, there is a relation betweenCbf3 and three proteins, Cbf3a, Cbf3b and Cbf3cexpressed by a verb contain.
We believe that thispartial information extracted from the dependencytrees should be sufficient for relation learning andcan be used as a representation for the learningmethod.
(5) Cbf3 contains three proteins, Cbf3a, Cbf3band Cbf3c.containsnsubjdobjCbf3 proteinsconj andnumconj andconj andCbf3a three Cbf3b Cbf3cFigure 1: Stanford parser outputRepresentation: dependency pathsCfb3 nsubj?
contains dobj?
proteins conj and?
Cbf3aCfb3 nsubj?
contains dobj?
proteins conj and?
Cbf3bCfb3 nsubj?
contains dobj?
proteins conj and?
Cbf3c4 Experiments4.1 Set-upData We use two corpora which both come fromthe biomedical field and contain annotations ofeither interacting proteins BC-PPI (1,000 sen-tences)1 or the interactions among proteins andgenes LLL2 (77 sentences in the training set and87 in the test set) (Ne?dellec, 2005).
The BC-PPIcorpus was created by sampling sentences from theBioCreAtive challenge, the LLL corpus was com-posed by querying Medline with the term Bacillussubtilis.
The difference between the two corporalies in the directionality of interactions.
The for-mer corpus contains both symmetric and asymmet-ric interactions while in the latter they are strictlyasymmetric.
We analyzed the BC corpus with theStanford parser.
3 The LLL corpus has alreadybeen preprocessed by the Link parser.To estimate distributional similarity, we useTREC 2006 Genomics collection (Hersch,2006) which contains 162,259 documents from1available from http://www2.informatik.hu-berlin.de/?hakenber/2available from http://genome.jouy.inra.fr/texte/LLLchallenge/3available from http://nlp.stanford.edu/software/lex-parser.shtml\#Download41949 journals.
All documents have been prepro-cessed by removing HTML-tags, citations in thetext and reference sections and stemmed by thePorter stemmer (van Rijsbergen et al, 1980).Furthermore, the query-likelihood approach withDirichlet smoothing (Chen, 1996) is used toretrieve document passages given a query.
Allwords occurring in the set of input sequences arefed as queries.
Immediate context surroundingeach pair of words is used as features to calculatedistributional similarity of these words.
We set thecontext window to ?2 (2 tokens to the right and 2tokens to the left of a word in focus) and do notperform any kind of further preprocessing such asPoS tagging.Recall that in Section 2.1 we defined a substi-tution matrix solely based on the words.
How-ever, the representation we employ also containsinformation on syntactic functions and directions(Fig.
1).
To take this into account, we revisethe definition of d(?, ?).
We assume sequencesx = x1x2.
.
.
xnand y = y1y2.
.
.
ymto containwords (xi?W ) and syntactic functions accompa-nied by direction (xi/?W ).
Then,d?
(xi, yj) =8>><>>:d(xi, yj) xi, yj?W1 xi, yj/?W & xi= yj0 xi, yj/?W & xi6= yj0 xi?W & yj/?W0 xi/?W & yj?W(6)Baseline To test how well local alignment ker-nels perform compared to the kernels proposed inthe past, we implemented a method described in(Bunescu and Mooney, 2005) as a baseline.
Here,similarly to our approach, the shortest path be-tween relation arguments is extracted and a ker-nel between two sequences (paths) x and y is com-puted as follows:K(x, y) ={0 m 6= n?ni=1f(xi, yi) m = n(7)In Eq.
7, f(xi, yi) is the number of com-mon features shared by xiand yi.
Bunescu andMooney (2005) use several features such as word(protesters), part of speech tag (NNS), gener-alized part of speech tag (Noun), and entity type(e.g., PERSON ) if applicable.
In addition, a di-rection feature (?
or?)
is employed.
In our ex-periments we also use lemma, part of speech tagand direction but we do not consider an entity typeor negative polarity of items.Kernels that we compute are used together withLibSVM (Chang and Lin, 2001) to detect hyper-planes separating positive examples from the neg-ative ones.
Before plugging all kernel matrices intoLibSVM, they were normalized as in Eq.
8.K(x?, y?)
=K(x, y)?K(x, x)K(y, y)+ 1 (8)To compute LA matrices we use the distributedASCI supercomputer 3 (DAS-3) 4 which allows usto speed up the process of sequence comparison.In particular, because of symmetricity of the result-ing matrices for n sequences we need to carry outn(n?1)/2 comparisons to build a matrix.
Compu-tations are done in parallel by reserving a numberof nodes of DAS-3 and concatenating the outputslater on.4.2 Experiment I: Distributional measuresand their impact on the final performanceDistributional similarity measures have been usedfor various tasks in the past.
For instance, (Lee,1999) employs them to detect similar nouns basedon the verb-object cooccurrence pairs.
The resultssuggest the Jaccard?s coefficient to be one of thebest performing measures followed by some othersincluding cosine.
Euclidean distance fell into thegroup with the largest error rates.
It is of consider-able interest to test whether these metrics have animpact on the performance of a LA kernel.
We donot employ Jaccard?s measure but the dice coeffi-cient is monotonic in it.While computing a distributional similarity, itmay happen that a given word x does not occurin the corpus.
To handle such cases, we always setd(x, x) = 1.
To estimate distributional similarity,a number of hits returned by querying the TRECcollection is set to 500.
Gaps are defined throughthe gaps opening and extension costs.
In our ex-periments, the gap opening cost is set to 1.2, theextension cost to 0.2 and the scaling parameter ?to 1.The 10-fold cross-validation results on theBC-PPI corpus are presented in Table 2 and onthe LLL training data set in Table 3.
The LA kernel4http://www.cs.vu.nl/das3420based on the distributional similarity measures per-forms significantly better than the baseline.
In con-trast to the baseline, it is able to handle sequencesof different lengths including gaps.
According tothe Eq.
7, a comparison of any two sequences ofdifferent lengths results in the 0-score.
Neverthe-less it still yields high recall while precision ismuch lower.
Interestingly, the results of the short-est path approach on the ACE corpus (Bunescu andMooney, 2005) were reversed by boosting preci-sion while decreasing recall.Method Pr,% R,% F1,%LAK-dice 75.56 79.72 77.56LAK-cosine 76.4 80.66 78.13LAK-l2 77.56 79.31 78.42Baseline 32.04 75.63 45.00Table 2: Results on the BC-PPI data setAt first glance, the LA kernel based on the distri-butional similarity measures that we selected pro-vides similar performance.
We can notice that thel2 metric seems to be the best performing measure.On the BC-PPI data, the method based on the l2measure outperforms the methods based on diceand on cosine but the differences are not signifi-cant.On the LLL data set, the LA method using distri-butional similarity measures significantly outper-forms both baselines and also yields better resultsthan an approach based on shallow linguistic in-formation (Giuliano et al, 2006).
Giuliano et al(2006) use no syntactic information.
Recent workreported in (Fundel, 2007) also uses dependencyinformation but in contrast to our method, it servesas representation on which extraction rules are de-fined.The choice of the distributional measure doesnot seem to affect the overall performance verymuch.
But in contrast to the BC-PPI data set, thekernels which use dice and cosine measures sig-nificantly outperform the one based on l2 (pairedt-test, ?
= 0.01).Method Pr,% R,% F1,%LAK-dice 74.25 87.94 80.51LAK-cosine 73.99 88.23 80.48LAK-l2 69.28 87.6 77.37(Fundel, 2007) 68 83 75(Giuliano et al, 2006) 62.10 61.30 61.70Baseline 39.02 100.00 56.13Table 3: Results on the LLL data setcoreferences Pr,% R,% F1,%with (LAK-dice) 60.00 31.00 40.90w/o (LAK-dice) 71.00 50.00 58.60with (Giuliano et al, 2006) 29.00 31.00 30.00w/o (Giuliano et al, 2006) 54.80 62.90 58.60Table 4: Results on the LLL test data setWe also verified how well our method performson the LLL test data.
Surpisingly, precision isstill high (for both subsets, with co-references andwithout them) while recall suffers.
We hypothesizethat it is due to the fact that for some sentencesonly incomplete parses are provided and, conse-quently, no dependency paths between the entitiesare found.
For 91 out of 567 possible interactionpairs generated on the test data, there is no depen-dency path extracted.
In contrast, work reported in(Giuliano et al, 2006) does not make use of syn-tactic information which on the data without coref-erences yields higher recall.4.2.1 Experiment Ia: Impact of distributionalmeasures estimationWe believe that accuracy of LA kernel cruciallydepends on the substitution matrix, i.e.
an accu-rate estimate of distributional similarity.
In mostcases, to obtain accurate estimates it is needed touse a large corpus.
However, it is unclear whetherdifferences in the estimates derived from corporaof different sizes would affect the overall perfor-mance of the LA kernel.
To investigate it, we con-ducted several experiments by varying a number ofretrieved passages.Table 5 contains the most similar words to ad-here, expression and sigF detected by the dicemeasure in descending order (by varying the num-ber of passages retrieved per query).
While the or-der of the most similar words for sigF does notchange very much from one setting to another, es-timates for adhere and expression depend more onthe number of passages retrieved.
Moreover, notonly the actual ordering changes, but also the num-ber of similar words does.
For instance, whilethere are only four words similar to adhere foundwhen 100 passages per each query are used, al-ready 12 similar words to adhere are detectedwhen the count of extracted documents is set to1,000 passages per query.We also notice that the most similar words to421adhere expression sigFdice@100 contribute, belong, bind, map processing, overlap, production, cotC, tagA,localization, sequestration rocG, tagF, whiGdice@500 contribute, belong, bind, end, localization, presence, sigE, comK,occur, result processing, absence cotC, sigG, tagAdice@1,000 contribute, bind, convert, presence, assembly, localization, sigE, comK,occur, belong processing, activation cotC, sigG, tagAdice@1,500 bind, contribute, convert, localization, assembly, presence, sigE, comK,correspond, belong activation, processing cotC, sigG, tagATable 5: Top 5 similar words (LLL data set)sigF are all named entities.
Even though sigF doesnot occur in the training data, we can still hypoth-esize that it is likely to be a target of the relationbecause of sigE, cotC and tagA.
These three genescan be found in the training set and they are usuallytargets (second argument) of the interaction rela-tion.Table 6 shows results on the LLL data set byvarying the size of the data used for estimation ofdistributional similarity (dice measure).
We ob-serve the decrease in precision and in recall whenincreasing the number of hits to 1,500.
Changingthe number of hits from 500 to 1,000 results in asubtle increase in recall.Size Pr,% R,% F1,%dice@500 74.25 87.94 80.51dice@1,000 74.38 88.02 80.62dice@1,500 69.87 86.85 77.43Table 6: Estimation settings for the LLL data setSize Pr,% R,% F1,%dice@100 75.56 79.72 77.58dice@500 76.72 81.01 78.8dice@1,000 76.56 80.78 78.61Table 7: Estimation settings for the BC-PPI datasetThe results on the BC-PPI data set show a simi-lar tendency.
However the observed differences arenot statistically significant in the latter case.
Thesesubtle changes in recall and precision can be at-tributed to the relatively low absolute values of thesimilarity scores.
For instance, even though an or-der of similar words in Table 5 changes while in-creasing the data used for estimation, a differencebetween the absolute values can be quite small.4.2.2 Experiment Ib: Impact of the scalingparameter ?Saigo et al (2004) have already shown that theparameter ?
has the significant impact on the re-sults accuracy.
We have also carried out some pre-liminary experiments by setting the opening gap to12, the extension gap to 2 and by varying the pa-rameter ?.
The kernel matrices were normalizedas in Eq.
8.
The results on the BC-PPI data set(dice500) are given in Table 8.?
Pr,% R,% F1,%0.5 17.72 94.87 29.851 38.84 89.42 54.1410 67.72 76.67 71.90Table 8: Impact of ?
on the performance on theBC-PPI data setThe results indicate that decreasing ?
leads tothe decrease in the overall performance.
However,if the values of gap penalties are lower and ?
is setto 1, the results are better.
This suggests that thefinal performance of the LA kernel is influencedby a combination of parameters and their choice iscrucial for obtaining the good performance.5 Related WorkWe have already mentioned some relevant workon relation extraction while introducing the localalignment kernel.
Most work done for relationextraction considers binary relations in sententialcontext (McDonald, 2005).
Current techniques forrelation extraction include hand-written patterns(Sekimizu et al, 1998), kernel methods (Zelenkoet al, 2003), pattern induction methods (Snow etal., 2005), and finite-state automata (Pustejovskyet al, 2002).Kernel methods have become very popularin natural language processing in general and422for learning relations, in particular (Culotta andSorensen, 2004).
There are many kernels definedfor the text data.
For instance, string kernels arespecial kernels which consider inner products ofall subsequences from the given sequences of el-ements (Lodhi et al, 2002).
They can be furtherextended to syllable kernels which proved to per-form well for text categorization (Saunders et al,2002).For relation learning, Zelenko et al(2003) useshallow parsing in conjunction with contiguousand non-contiguous kernels to learn relations.Bunescu et al(2006) define several kernels to ac-complish the same task.
First, they introducethe sequential kernels and show that such methodout-performs the longest match approach.
Next,Bunescu et al (2006) propose kernels for the pathsin dependency trees (which is referred to as a short-est path between two arguments of a given rela-tion).
In this paper we used their method based ondependency parsing as one of the baselines.
Giu-liano (2006) takes this approach further by defin-ing several kernels using local context and senten-tial context.
An advantage of Giuliano?s method(2006) lies in the simpler representation whichdoes not use syntactic structure.
In this case, evenif parsing fails on certain sentences, it is still pos-sible to handle them.6 ConclusionsWe presented a novel approach to relation extrac-tion which is based on the local alignments of se-quences.
To compare two sequences, additionalinformation is used which is not necessarily con-tained in the training data.
By employing distribu-tional measures we obtain a considerable improve-ment over two baselines and work reported before.The choice of a distributional similarity measuredoes not seem to affect the overall performancevery much.
Based on the experiments we haveconducted, we conclude that the LA kernel usingdice and cosine measures perform similarly on theLLL data set and the BC-PPI corpus.
On the LLLcorpus, the LA kernel employing l2 shows a sig-nificant decrease in performance.
But concerningstatistical significance, the method using dice sig-nificantly outperforms the one based l2 measureonly on LLL corpus while there is no significantimprovement on the BC-PPI data set noticed.We use contextual information to measure dis-tributional similarity.
In this setting any two wordscan be compared no matter which parts of speechthey belong to.
As dependency paths contain vari-ous words along with nouns and verbs, other meth-ods often mentioned in the literature would bemore difficult to use.
However, in the future weare going to extend this approach by using syntac-tically analyzed corpora and by estimating distri-butional similarity from it.
It would allow us to usemore accurate estimates and to discriminate be-tween lexically ambiguous words.
Similarity mea-sures on the words that belong to other parts ofspeech can be still estimated using the local con-text only.AcknowledgmentsThe authors thank Vera Hollink, Victor de Boerand anonymous reviewers for their valuable com-ments.
This work was carried out in the con-text of the Virtual Laboratory for e-Science project(www.vl-e.nl).
This project is supported by aBSIK grant from the Dutch Ministry of Education,Culture and Science (OC&W) and is part of theICT innovation program of the Ministry of Eco-nomic Affairs (EZ).ReferencesYevgeny (Eugene) Agichtein.
2005.
Extracting Re-lations from Large Text Collections.
Ph.D. Thesis,Columbia University.Razvan C. Bunescu and Raymond J.Mooney.
2006.Extracting Relations from Text.
From Word Se-quences to Dependency Paths.
In book ?Text Miningand Natural Language Processing?, Anne Kao andSteve Poteet (Eds).Razvan C. Bunescu and Raymond J.Mooney.
2005.
AShortest Path Dependency Kernel for Relation Ex-traction.
In Proceedings of the Joint Conferenceon Human Language Technology / Empirical Meth-ods in Natural Language Processing (HLT/EMNLP),Vancouver, BC.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvmStanley F. Chen and Joshua Goodman.
1996.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
In ACL?96.William W. Cohen, Pradeep Ravikumar and StephenFienberg.
2003.
A Comparison of String DistanceMetrics for Name-Matching Tasks.
In IIWeb 2003,pages 73-78.423Aron Culotta and Jeffrey Sorensen.
2003.
DependencyTree Kernels for Relation Extraction.
In ACL 2003.William B. Dolan, Chris Quirk and Chris Brockett.2004.
Unsupervised Construction of Large Para-phrase Corpora: Exploiting Massively Parallel NewsSources.
In Proceedings of COLING 2004, Geneva,Switzerland.Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007.RelEx - Relation Extraction using dependency parsetrees.
In Bioinformatics, vol.
23, no.
3.Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-mano.
2006.
Exploiting Shallow Linguistic Infor-mation for Relation Extraction from Biomedical Lit-erature.
In EACL 2006.David Haussler.
1999.
Convolution Kernels on Dis-crete Structures.
UC Santa Cruz Technical ReportUCS-CRL-99-10.William Hersch, Aaron M. Cohen, Phoebe Roberts andHari K. Rakapalli.
2006.
TREC 2006 GenomicsTrack Overview.
In Proceedings of the 15th Text Re-trieval Conference.Lillian Lee.
1999.
Measures of distributional similar-ity.
In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Com-putational Linguistics, pages 25-32.G.
Leusch, N. Ueffing and H. Ney.
2003.
A NovelString-to-String Distance Measure with Applicationsto Machine Translation Evaluation.
In MachineTranslation Summit IX, New Orleans, LO, pages240-247.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Christianini, and Chris Watkins.
2002.
TextClassification using String Kernels.
In Journal ofMachine Learning Research, 2, pages 419-444.Ryan McDonald.
2005.
Extracting Relations from Un-structured Text.
Technical Report: MS-CIS-05-06.Alvaro E. Monge and Charles Elkan.
1996.
The FieldMatching Problem: Algorithms and Applications.
InKDD 1996, pages 267-270.Alessandro Moschitti.
Efficient Convolution Kernelsfor Dependency and Constituent Syntactic Trees.
InECML 2006, pages 318-329.Cl.
Ne?dellec.
2005.
Learning Language in Logic -Genic Interaction Extraction Challenge.
In Proceed-ings of the Learning Language in Logic workshop.J.
Pustejovsky, J. Castano, J. Zhang, B. Cochran, M.Kotecki.
2002.
Robust Relational Parsing overBiomedical Literature: Extracting Inhibit Relations.Pacific Symposium on Biocomputing.Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,James Dowdall, Christos Andronis, Andreas Per-sidis, and Ourania Konstanti.
2004.
Mining re-lations in the GENIA corpus.
In ?Second Euro-pean Workshop on Data Mining and Text Mining forBioinformatics?, in conjunction with ECML/PKDD2004, September.Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda andTatsuya Akutsu.
2004.
Protein homology detectionusing string alignment kernels.
In ?Bioinformatics?,vol.
20 no.
11, pages 1682-1689.Hiroto Saigo, Jean-Philippe Vert, and Tatsuya Akutsu.2006.
Optimizing amino acid substitution matriceswith a local alignment kernel.
In ?BMC Bioinfor-matics?, 7:246.C.
Saunders, H. Tschach, and J. Shawe-Taylor.
2002.Syllables and other String Kernel Extensions.
InProceesings of the Nineteenth International Confer-ence on Machine Learning (ICML?02).T.
Sekimizu, H. Park, and J. Tsujii.
1998.
Identify-ing the interaction between genes and gene productsbased on frequently seen verbs in medline abstracts.In Genome Informatics.T.
F. Smith and M. S. Waterman.
1987.
Identifica-tion of Common Molecular Subsequences.
In J. Mol.Biol.
147, 195?197.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
NIPS 17.Erik Tjong Kim Sang, Sander Canisius, Antal van denBosch and Toine Bogers.
2005.
Applying spellingerror correction techniques for improving semanticrole labeling.
In Proceedings of the Ninth Confer-ence on Natural Language Learning, CoNLL-2005,June 29-30, 2005, Ann Arbor, MI.Lonneke van der Plas and Jo?rg Tiedemann.
2006.
Find-ing Synonyms Using Automatic Word Alignmentand Measures of Distributional Similarity.
In Pro-ceedings of ACL/Coling.C.
J. van Rijsbergen, S. E. Robertson and M. F. Porter.1980.
New models in probabilistic information re-trieval.
London: British Library.
(British LibraryResearch and Development Report, no.
5587).Julie Weeds, David Weir and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of CoLing 2004.Dmitry Zelenko, Ch.
Aone, and A. Richardella.
2003.Kernel Methods for Relation Extraction.
Journal ofMachine Learning Research 3 (2003), 1083-1106.424
