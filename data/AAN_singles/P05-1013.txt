Proceedings of the 43rd Annual Meeting of the ACL, pages 99?106,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsPseudo-Projective Dependency ParsingJoakim Nivre and Jens NilssonSchool of Mathematics and Systems EngineeringVa?xjo?
UniversitySE-35195 Va?xjo?, Sweden{nivre,jni}@msi.vxu.seAbstractIn order to realize the full potential ofdependency-based syntactic parsing, it isdesirable to allow non-projective depen-dency structures.
We show how a data-driven deterministic dependency parser,in itself restricted to projective structures,can be combined with graph transforma-tion techniques to produce non-projectivestructures.
Experiments using data fromthe Prague Dependency Treebank showthat the combined system can handle non-projective constructions with a precisionsufficient to yield a significant improve-ment in overall parsing accuracy.
Thisleads to the best reported performance forrobust non-projective parsing of Czech.1 IntroductionIt is sometimes claimed that one of the advantagesof dependency grammar over approaches based onconstituency is that it allows a more adequate treat-ment of languages with variable word order, wherediscontinuous syntactic constructions are more com-mon than in languages like English (Mel?c?uk,1988; Covington, 1990).
However, this argumentis only plausible if the formal framework allowsnon-projective dependency structures, i.e.
structureswhere a head and its dependents may correspondto a discontinuous constituent.
From the point ofview of computational implementation this can beproblematic, since the inclusion of non-projectivestructures makes the parsing problem more com-plex and therefore compromises efficiency and inpractice also accuracy and robustness.
Thus, mostbroad-coverage parsers based on dependency gram-mar have been restricted to projective structures.This is true of the widely used link grammar parserfor English (Sleator and Temperley, 1993), whichuses a dependency grammar of sorts, the probabilis-tic dependency parser of Eisner (1996), and morerecently proposed deterministic dependency parsers(Yamada and Matsumoto, 2003; Nivre et al, 2004).It is also true of the adaptation of the Collins parserfor Czech (Collins et al, 1999) and the finite-statedependency parser for Turkish by Oflazer (2003).This is in contrast to dependency treebanks, e.g.Prague Dependency Treebank (Hajic?
et al, 2001b),Danish Dependency Treebank (Kromann, 2003),and the METU Treebank of Turkish (Oflazer et al,2003), which generally allow annotations with non-projective dependency structures.
The fact that pro-jective dependency parsers can never exactly repro-duce the analyses found in non-projective treebanksis often neglected because of the relative scarcity ofproblematic constructions.
While the proportion ofsentences containing non-projective dependencies isoften 15?25%, the total proportion of non-projectivearcs is normally only 1?2%.
As long as the mainevaluation metric is dependency accuracy per word,with state-of-the-art accuracy mostly below 90%,the penalty for not handling non-projective construc-tions is almost negligible.
Still, from a theoreticalpoint of view, projective parsing of non-projectivestructures has the drawback that it rules out perfectaccuracy even as an asymptotic goal.99(?Only one of them concerns quality.?
)RZ(Out-of ?AuxPPnichthem ?AtrVBjeisTjenonly ?AuxZCjednaone-FEM-SG ?SbRnato ?AuxPN4kvalituquality? AdvZ:..) ?AuxZFigure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1There exist a few robust broad-coverage parsersthat produce non-projective dependency structures,notably Tapanainen and Ja?rvinen (1997) and Wangand Harper (2004) for English, Foth et al (2004)for German, and Holan (2004) for Czech.
In addi-tion, there are several approaches to non-projectivedependency parsing that are still to be evaluated inthe large (Covington, 1990; Kahane et al, 1998;Duchier and Debusmann, 2001; Holan et al, 2001;Hellwig, 2003).
Finally, since non-projective con-structions often involve long-distance dependencies,the problem is closely related to the recovery ofempty categories and non-local dependencies inconstituency-based parsing (Johnson, 2002; Dienesand Dubey, 2003; Jijkoun and de Rijke, 2004; Cahillet al, 2004; Levy and Manning, 2004; Campbell,2004).In this paper, we show how non-projective depen-dency parsing can be achieved by combining a data-driven projective parser with special graph transfor-mation techniques.
First, the training data for theparser is projectivized by applying a minimal num-ber of lifting operations (Kahane et al, 1998) andencoding information about these lifts in arc labels.When the parser is trained on the transformed data,it will ideally learn not only to construct projectivedependency structures but also to assign arc labelsthat encode information about lifts.
By applying aninverse transformation to the output of the parser,arcs with non-standard labels can be lowered to theirproper place in the dependency graph, giving rise1The dependency graph has been modified to make the finalperiod a dependent of the main verb instead of being a depen-dent of a special root node for the sentence.to non-projective structures.
We call this pseudo-projective dependency parsing, since it is based on anotion of pseudo-projectivity (Kahane et al, 1998).The rest of the paper is structured as follows.In section 2 we introduce the graph transformationtechniques used to projectivize and deprojectivizedependency graphs, and in section 3 we describe thedata-driven dependency parser that is the core of oursystem.
We then evaluate the approach in two steps.First, in section 4, we evaluate the graph transfor-mation techniques in themselves, with data from thePrague Dependency Treebank and the Danish De-pendency Treebank.
In section 5, we then evaluatethe entire parsing system by training and evaluatingon data from the Prague Dependency Treebank.2 Dependency Graph TransformationsWe assume that the goal in dependency parsing is toconstruct a labeled dependency graph of the kind de-picted in Figure 1.
Formally, we define dependencygraphs as follows:1.
Let R = {r1, .
.
.
, rm} be the set of permissibledependency types (arc labels).2.
A dependency graph for a string of wordsW = w1?
?
?wn is a labeled directed graphD = (W,A), where(a) W is the set of nodes, i.e.
word tokens inthe input string, ordered by a linear prece-dence relation <,(b) A is a set of labeled arcs (wi, r, wj), wherewi, wj ?W , r ?
R,(c) for every wj ?W , there is at most one arc(wi, r, wj) ?
A.100(?Only one of them concerns quality.?
)RZ(Out-of ?AuxPPnichthem ?AtrVBjeisTjenonly ?AuxZCjednaone-FEM-SG ?SbRnato ?AuxPN4kvalituquality? AdvZ:..) ?AuxZFigure 2: Projectivized dependency graph for Czech sentence3.
A graph D = (W,A) is well-formed iff it isacyclic and connected.If (wi, r, wj) ?
A, we say that wi is the head of wjand wj a dependent of wi.
In the following, we usethe notation wir?
wj to mean that (wi, r, wj) ?
A;we also use wi ?
wj to denote an arc with unspeci-fied label and wi ??
wj for the reflexive and transi-tive closure of the (unlabeled) arc relation.The dependency graph in Figure 1 satisfies all thedefining conditions above, but it fails to satisfy thecondition of projectivity (Kahane et al, 1998):1.
An arc wi?wk is projective iff, for every wordwj occurring between wi and wk in the string(wi<wj<wk or wi>wj>wk), wi ??
wj .2.
A dependency graph D = (W,A) is projectiveiff every arc in A is projective.The arc connecting the head jedna (one) to the de-pendent Z (out-of) spans the token je (is), which isnot dominated by jedna.As observed by Kahane et al (1998), any (non-projective) dependency graph can be transformedinto a projective one by a lifting operation, whichreplaces each non-projective arc wj ?
wk by a pro-jective arc wi ?
wk such that wi ??
wj holds inthe original graph.
Here we use a slightly differentnotion of lift, applying to individual arcs and movingtheir head upwards one step at a time:LIFT(wj ?
wk) ={wi ?
wk if wi ?
wjundefined otherwiseIntuitively, lifting an arc makes the word wk depen-dent on the head wi of its original head wj (which isunique in a well-formed dependency graph), unlesswj is a root in which case the operation is undefined(but then wj ?
wk is necessarily projective if thedependency graph is well-formed).Projectivizing a dependency graph by lifting non-projective arcs is a nondeterministic operation in thegeneral case.
However, since we want to preserveas much of the original structure as possible, weare interested in finding a transformation that in-volves a minimal number of lifts.
Even this maybe nondeterministic, in case the graph contains sev-eral non-projective arcs whose lifts interact, but weuse the following algorithm to construct a minimalprojective transformation D?
= (W,A?)
of a (non-projective) dependency graph D = (W,A):PROJECTIVIZE(W , A)1 A?
?
A2 while (W,A?)
is non-projective3 a?
SMALLEST-NONP-ARC(A?
)4 A?
?
(A?
?
{a}) ?
{LIFT(a)}5 return (W,A?
)The function SMALLEST-NONP-ARC returns thenon-projective arc with the shortest distance fromhead to dependent (breaking ties from left to right).Applying the function PROJECTIVIZE to the graphin Figure 1 yields the graph in Figure 2, where theproblematic arc pointing to Z has been lifted fromthe original head jedna to the ancestor je.
Usingthe terminology of Kahane et al (1998), we say thatjedna is the syntactic head of Z, while je is its linearhead in the projectivized representation.Unlike Kahane et al (1998), we do not regard aprojectivized representation as the final target of theparsing process.
Instead, we want to apply an in-101Lifted arc label Path labels Number of labelsBaseline d p nHead d?h p n(n+ 1)Head+Path d?h p?
2n(n+ 1)Path d?
p?
4nTable 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types)verse transformation to recover the underlying (non-projective) dependency graph.
In order to facilitatethis task, we extend the set of arc labels to encodeinformation about lifting operations.
In principle, itwould be possible to encode the exact position of thesyntactic head in the label of the arc from the linearhead, but this would give a potentially infinite set ofarc labels and would make the training of the parservery hard.
In practice, we can therefore expect atrade-off such that increasing the amount of infor-mation encoded in arc labels will cause an increasein the accuracy of the inverse transformation but adecrease in the accuracy with which the parser canconstruct the labeled representations.
To explore thistradeoff, we have performed experiments with threedifferent encoding schemes (plus a baseline), whichare described schematically in Table 1.The baseline simply retains the original labels forall arcs, regardless of whether they have been liftedor not, and the number of distinct labels is thereforesimply the number n of distinct dependency types.2In the first encoding scheme, called Head, we usea new label d?h for each lifted arc, where d is thedependency relation between the syntactic head andthe dependent in the non-projective representation,and h is the dependency relation that the syntactichead has to its own head in the underlying structure.Using this encoding scheme, the arc from je to Zin Figure 2 would be assigned the label AuxP?Sb(signifying an AuxP that has been lifted from a Sb).In the second scheme, Head+Path, we in additionmodify the label of every arc along the lifting pathfrom the syntactic to the linear head so that if theoriginal label is p the new label is p?.
Thus, the arcfrom je to jedna will be labeled Sb?
(to indicate thatthere is a syntactic head below it).
In the third andfinal scheme, denoted Path, we keep the extra infor-2Note that this is a baseline for the parsing experiment only(Experiment 2).
For Experiment 1 it is meaningless as a base-line, since it would result in 0% accuracy.mation on path labels but drop the information aboutthe syntactic head of the lifted arc, using the label d?instead of d?h (AuxP?
instead of AuxP?Sb).As can be seen from the last column in Table 1,both Head and Head+Path may theoretically leadto a quadratic increase in the number of distinct arclabels (Head+Path being worse than Head only bya constant factor), while the increase is only linear inthe case of Path.
On the other hand, we can expectHead+Path to be the most useful representation forreconstructing the underlying non-projective depen-dency graph.
In approaching this problem, a vari-ety of different methods are conceivable, includinga more or less sophisticated use of machine learn-ing.
In the present study, we limit ourselves to analgorithmic approach, using a deterministic breadth-first search.
The details of the transformation proce-dure are slightly different depending on the encod-ing schemes:?
Head: For every arc of the form wid?h??
wn,we search the graph top-down, left-to-right,breadth-first starting at the head node wi.
If wefind an arc wlh??
wm, called a target arc, wereplace wid?h??
wn by wmd??
wn; otherwisewe replace wid?h??
wn by wid??
wn (i.e.
welet the linear head be the syntactic head).?
Head+Path: Same as Head, but the searchonly follows arcs of the form wjp???
wk and atarget arc must have the form wlh???
wm; if notarget arc is found, Head is used as backoff.?
Path: Same as Head+Path, but a target arcmust have the form wlp???
wm and no out-going arcs of the form wmp????
wo; no backoff.In section 4 we evaluate these transformations withrespect to projectivized dependency treebanks, andin section 5 they are applied to parser output.
Before102Feature type Top?1 Top Next Next+1 Next+2 Next+3Word form + + + +Part-of-speech + + + + + +Dep type of head +leftmost dep + +rightmost dep +Table 2: Features used in predicting the next parser actionwe turn to the evaluation, however, we need to intro-duce the data-driven dependency parser used in thelatter experiments.3 Memory-Based Dependency ParsingIn the experiments below, we employ a data-drivendeterministic dependency parser producing labeledprojective dependency graphs,3 previously tested onSwedish (Nivre et al, 2004) and English (Nivre andScholz, 2004).
The parser builds dependency graphsby traversing the input from left to right, using astack to store tokens that are not yet complete withrespect to their dependents.
At each point during thederivation, the parser has a choice between pushingthe next input token onto the stack ?
with or with-out adding an arc from the token on top of the stackto the token pushed ?
and popping a token from thestack ?
with or without adding an arc from the nextinput token to the token popped.
More details on theparsing algorithm can be found in Nivre (2003).The choice between different actions is in generalnondeterministic, and the parser relies on a memory-based classifier, trained on treebank data, to pre-dict the next action based on features of the cur-rent parser configuration.
Table 2 shows the featuresused in the current version of the parser.
At eachpoint during the derivation, the prediction is basedon six word tokens, the two topmost tokens on thestack, and the next four input tokens.
For each to-ken, three types of features may be taken into ac-count: the word form; the part-of-speech assignedby an automatic tagger; and labels on previously as-signed dependency arcs involving the token ?
the arcfrom its head and the arcs to its leftmost and right-most dependent, respectively.
Except for the left-3The graphs satisfy all the well-formedness conditions givenin section 2 except (possibly) connectedness.
For robustnessreasons, the parser may output a set of dependency trees insteadof a single tree.most dependent of the next input token, dependencytype features are limited to tokens on the stack.The prediction based on these features is a k-nearest neighbor classification, using the IB1 algo-rithm and k = 5, the modified value difference met-ric (MVDM) and class voting with inverse distanceweighting, as implemented in the TiMBL softwarepackage (Daelemans et al, 2003).
More details onthe memory-based prediction can be found in Nivreet al (2004) and Nivre and Scholz (2004).4 Experiment 1: Treebank TransformationThe first experiment uses data from two dependencytreebanks.
The Prague Dependency Treebank (PDT)consists of more than 1M words of newspaper text,annotated on three levels, the morphological, ana-lytical and tectogrammatical levels (Hajic?, 1998).Our experiments all concern the analytical annota-tion, and the first experiment is based only on thetraining part.
The Danish Dependency Treebank(DDT) comprises about 100K words of text selectedfrom the Danish PAROLE corpus, with annotationof primary and secondary dependencies (Kromann,2003).
The entire treebank is used in the experiment,but only primary dependencies are considered.4 Inall experiments, punctuation tokens are included inthe data but omitted in evaluation scores.In the first part of the experiment, dependencygraphs from the treebanks were projectivized usingthe algorithm described in section 2.
As shown inTable 3, the proportion of sentences containing somenon-projective dependency ranges from about 15%in DDT to almost 25% in PDT.
However, the over-all percentage of non-projective arcs is less than 2%in PDT and less than 1% in DDT.
The last four4If secondary dependencies had been included, the depen-dency graphs would not have satisfied the well-formedness con-ditions formulated in section 2.103# Lifts in projectivizationData set # Sentences % NonP # Tokens % NonP 1 2 3 >3PDT training 73,088 23.15 1,255,333 1.81 93.79 5.60 0.51 0.11DDT total 5,512 15.48 100,238 0.94 79.49 13.28 4.36 2.87Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective)Data set Head H+P PathPDT training (28 labels) 92.3 (230) 99.3 (314) 97.3 (84)DDT total (54 labels) 92.3 (123) 99.8 (147) 98.3 (99)Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)columns in Table 3 show the distribution of non-projective arcs with respect to the number of liftsrequired.
It is worth noting that, although non-projective constructions are less frequent in DDTthan in PDT, they seem to be more deeply nested,since only about 80% can be projectivized with asingle lift, while almost 95% of the non-projectivearcs in PDT only require a single lift.In the second part of the experiment, we appliedthe inverse transformation based on breadth-firstsearch under the three different encoding schemes.The results are given in Table 4.
As expected, themost informative encoding, Head+Path, gives thehighest accuracy with over 99% of all non-projectivearcs being recovered correctly in both data sets.However, it can be noted that the results for the leastinformative encoding, Path, are almost comparable,while the third encoding, Head, gives substantiallyworse results for both data sets.
We also see thatthe increase in the size of the label sets for Headand Head+Path is far below the theoretical upperbounds given in Table 1.
The increase is gener-ally higher for PDT than for DDT, which indicates agreater diversity in non-projective constructions.5 Experiment 2: Memory-Based ParsingThe second experiment is limited to data from PDT.5The training part of the treebank was projectivizedunder different encoding schemes and used to trainmemory-based dependency parsers, which were runon the test part of the treebank, consisting of 7,5075Preliminary experiments using data from DDT indicatedthat the limited size of the treebank creates a severe sparse dataproblem with respect to non-projective constructions.sentences and 125,713 tokens.6 The inverse trans-formation was applied to the output of the parsersand the result compared to the gold standard test set.Table 5 shows the overall parsing accuracy at-tained with the three different encoding schemes,compared to the baseline (no special arc labels) andto training directly on non-projective dependencygraphs.
Evaluation metrics used are AttachmentScore (AS), i.e.
the proportion of tokens that are at-tached to the correct head, and Exact Match (EM),i.e.
the proportion of sentences for which the depen-dency graph exactly matches the gold standard.
Inthe labeled version of these metrics (L) both headsand arc labels must be correct, while the unlabeledversion (U) only considers heads.The first thing to note is that projectivizing helpsin itself, even if no encoding is used, as seen fromthe fact that the projective baseline outperforms thenon-projective training condition by more than halfa percentage point on attachment score, although thegain is much smaller with respect to exact match.The second main result is that the pseudo-projectiveapproach to parsing (using special arc labels to guidean inverse transformation) gives a further improve-ment of about one percentage point on attachmentscore.
With respect to exact match, the improvementis even more noticeable, which shows quite clearlythat even if non-projective dependencies are rare onthe token level, they are nevertheless important forgetting the global syntactic structure correct.All improvements over the baseline are statisti-cally significant beyond the 0.01 level (McNemar?s6The part-of-speech tagging used in both training and testingwas the uncorrected output of an HMM tagger distributed withthe treebank; cf.
Hajic?
et al (2001a).104Encoding UAS LAS UEM LEMNon-projective 78.5 71.3 28.9 20.6Baseline 79.1 72.0 29.2 20.7Head 80.1 72.8 31.6 22.2Head+Path 80.0 72.8 31.8 22.4Path 80.0 72.7 31.6 22.0Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled)Unlabeled LabeledEncoding P R F P R FHead 61.3 54.1 57.5 55.2 49.8 52.4Head+Path 63.9 54.9 59.0 57.9 50.6 54.0Path 58.2 49.5 53.4 51.0 45.7 48.2Table 6: Precision, recall and F-measure for non-projective arcstest).
By contrast, when we turn to a comparisonof the three encoding schemes it is hard to find anysignificant differences, and the overall impression isthat it makes little or no difference which encodingscheme is used, as long as there is some indicationof which words are assigned their linear head insteadof their syntactic head by the projective parser.
Thismay seem surprising, given the experiments reportedin section 4, but the explanation is probably that thenon-projective dependencies that can be recovered atall are of the simple kind that only requires a singlelift, where the encoding of path information is oftenredundant.
It is likely that the more complex cases,where path information could make a difference, arebeyond the reach of the parser in most cases.However, if we consider precision, recall and F-measure on non-projective dependencies only, asshown in Table 6, some differences begin to emerge.The most informative scheme, Head+Path, givesthe highest scores, although with respect to Headthe difference is not statistically significant, whilethe least informative scheme, Path ?
with almost thesame performance on treebank transformation ?
issignificantly lower (p < 0.01).
On the other hand,given that all schemes have similar parsing accuracyoverall, this means that the Path scheme is the leastlikely to introduce errors on projective arcs.The overall parsing accuracy obtained with thepseudo-projective approach is still lower than for thebest projective parsers.
Although the best publishedresults for the Collins parser is 80% UAS (Collins,1999), this parser reaches 82% when trained on theentire training data set, and an adapted version ofCharniak?s parser (Charniak, 2000) performs at 84%(Jan Hajic?, pers.
comm.).
However, the accuracy isconsiderably higher than previously reported resultsfor robust non-projective parsing of Czech, with abest performance of 73% UAS (Holan, 2004).Compared to related work on the recovery oflong-distance dependencies in constituency-basedparsing, our approach is similar to that of Dienesand Dubey (2003) in that the processing of non-localdependencies is partly integrated in the parsing pro-cess, via an extension of the set of syntactic cate-gories, whereas most other approaches rely on post-processing only.
However, while Dienes and Dubeyrecognize empty categories in a pre-processing stepand only let the parser find their antecedents, we usethe parser both to detect dislocated dependents andto predict either the type or the location of their syn-tactic head (or both) and use post-processing only totransform the graph in accordance with the parser?sanalysis.6 ConclusionWe have presented a new method for non-projectivedependency parsing, based on a combination ofdata-driven projective dependency parsing andgraph transformation techniques.
The main result isthat the combined system can recover non-projectivedependencies with a precision sufficient to give asignificant improvement in overall parsing accuracy,105especially with respect to the exact match criterion,leading to the best reported performance for robustnon-projective parsing of Czech.AcknowledgementsThis work was supported in part by the SwedishResearch Council (621-2002-4207).
Memory-basedclassifiers for the experiments were created usingTiMBL (Daelemans et al, 2003).
Special thanks toJan Hajic?
and Matthias Trautner Kromann for assis-tance with the Czech and Danish data, respectively,and to Jan Hajic?, Toma?s?
Holan, Dan Zeman andthree anonymous reviewers for valuable commentson a preliminary version of the paper.ReferencesCahill, A., Burke, M., O?Donovan, R., Van Genabith, J. andWay, A.
2004.
Long-distance dependency resolution inautomatically acquired wide-coverage PCFG-based LFG ap-proximations.
In Proceedings of ACL.Campbell, R. 2004.
Using linguistic principles to recoverempty categories.
In Proceedings of ACL.Charniak, E. 2000.
A maximum-entropy-inspired parser.
InProceedings of NAACL.Collins, M., Hajic?, J., Brill, E., Ramshaw, L. and Tillmann, C.1999.
A statistical parser for Czech.
In Proceedings of ACL.Collins, M. 1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.Covington, M. A.
1990.
Parsing discontinuous constituents independency grammar.
Computational Linguistics, 16:234?236.Daelemans, W., Zavrel, J., van der Sloot, K. and van den Bosch,A.
2003.
TiMBL: Tilburg Memory Based Learner, version5.0, Reference Guide.
Technical Report ILK 03-10, TilburgUniversity, ILK.Dienes, P. and Dubey, A.
2003.
Deep syntactic processing bycombining shallow methods.
In Proceedings of ACL.Duchier, D. and Debusmann, R. 2001.
Topological dependencytrees: A constraint-based account of linear precedence.
InProceedings of ACL.Eisner, J. M. 1996.
Three new probabilistic models for depen-dency parsing: An exploration.
In Proceedings of COLING.Foth, K., Daum, M. and Menzel, W. 2004.
A broad-coverageparser for German based on defeasible constraints.
In Pro-ceedings of KONVENS.Hajic?, J., Krbec, P., Oliva, K., Kveton, P. and Petkevic, V. 2001.Serial combination of rules and statistics: A case study inCzech tagging.
In Proceedings of ACL.Hajic?, J., Vidova Hladka, B., Panevova?, J., Hajic?ova?, E., Sgall,P.
and Pajas, P. 2001.
Prague Dependency Treebank 1.0.LDC, 2001T10.Hajic?, J.
1998.
Building a syntactically annotated corpus:The Prague Dependency Treebank.
In Issues of Valency andMeaning, pages 106?132.
Karolinum.Hellwig, P. 2003.
Dependency unification grammar.
In Depen-dency and Valency, pages 593?635.
Walter de Gruyter.Holan, T., Kubon?, V. and Pla?tek, M. 2001.
Word-order re-laxations and restrictions within a dependency grammar.
InProceedings of IWPT.Holan, T. 2004.
Tvorba zavislostniho syntaktickeho analyza-toru.
In Proceedings of MIS?2004.Jijkoun, V. and de Rijke, M. 2004.
Enriching the output ofa parser using memory-based learning.
In Proceedings ofACL.Johnson, M. 2002.
A simple pattern-matching algorithm for re-covering empty nodes and their antecedents.
In Proceedingsof ACL.Kahane, S., Nasr, A. and Rambow, O.
1998.
Pseudo-projectivity: A polynomially parsable non-projective depen-dency grammar.
In Proceedings of ACL-COLING.Kromann, M. T. 2003.
The Danish Dependency Treebank andthe DTAG treebank tool.
In Proceedings of TLT 2003.Levy, R. and Manning, C. 2004.
Deep dependencies fromcontext-free statistical parsers: Correcting the surface depen-dency approximation.
In Proceedings of ACL.Mel?c?uk, I.
1988.
Dependency Syntax: Theory and Practice.State University of New York Press.Nivre, J. and Scholz, M. 2004.
Deterministic dependency pars-ing of English text.
In Proceedings of COLING.Nivre, J., Hall, J. and Nilsson, J.
2004.
Memory-based depen-dency parsing.
In Proceedings of CoNLL.Nivre, J.
2003.
An efficient algorithm for projective depen-dency parsing.
In Proceedings of IWPT.Oflazer, K., Say, B., Hakkani-Tu?r, D. Z. and Tu?r, G. 2003.Building a Turkish treebank.
In Treebanks: Building andUsing Parsed Corpora, pages 261?277.
Kluwer AcademicPublishers.Oflazer, K. 2003.
Dependency parsing with an extended finite-state approach.
Computational Linguistics, 29:515?544.Sleator, D. and Temperley, D. 1993.
Parsing English with alink grammar.
In Proceedings of IWPT.Tapanainen, P. and Ja?rvinen, T. 1997.
A non-projective depen-dency parser.
In Proceedings of ANLP.Wang, W. and Harper, M. P. 2004.
A statistical constraintdependency grammar (CDG) parser.
In Proceedings of theWorkshop in Incremental Parsing (ACL).Yamada, H. and Matsumoto, Y.
2003.
Statistical dependencyanalysis with support vector machines.
In Proceedings ofIWPT.106
