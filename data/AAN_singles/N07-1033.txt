Proceedings of NAACL HLT 2007, pages 260?267,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsUsing ?Annotator Rationales?
to ImproveMachine Learning for Text Categorization?Omar F. Zaidan and Jason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,jason}@cs.jhu.eduChristine D. PiatkoJHU Applied Physics Laboratory11100 Johns Hopkins RoadLaurel, MD 20723 USAchristine.piatko@jhuapl.eduAbstractWe propose a new framework for supervised ma-chine learning.
Our goal is to learn from smalleramounts of supervised training data, by collecting aricher kind of training data: annotations with ?ra-tionales.?
When annotating an example, the hu-man teacher will also highlight evidence support-ing this annotation?thereby teaching the machinelearner why the example belongs to the category.
Weprovide some rationale-annotated data and present alearning method that exploits the rationales duringtraining to boost performance significantly on a sam-ple task, namely sentiment classification of moviereviews.
We hypothesize that in some situations,providing rationales is a more fruitful use of an an-notator?s time than annotating more examples.1 IntroductionAnnotation cost is a bottleneck for many natural lan-guage processing applications.
While supervisedmachine learning systems are effective, it is labor-intensive and expensive to construct the many train-ing examples needed.
Previous research has ex-plored active or semi-supervised learning as possibleways to lessen this burden.We propose a new way of breaking this annotationbottleneck.
Annotators currently indicate what thecorrect answers are on training data.
We proposethat they should also indicate why, at least by coarsehints.
We suggest new machine learning approachesthat can benefit from this ?why?
information.For example, an annotator who is categorizingphrases or documents might also be asked to high-light a few substrings that significantly influencedher judgment.
We call such clues ?rationales.?
Theyneed not correspond to machine learning features.
?This work was supported by the JHU WSE/APL Partner-ship Fund; National Science Foundation grant No.
0347822 tothe second author; and an APL Hafstad Fellowship to the third.In some circumstances, rationales should not betoo expensive or time-consuming to collect.
As longas the annotator is spending the time to study exam-ple xi and classify it, it may not require much extraeffort for her to mark reasons for her classification.2 Using Rationales to Aid LearningWe will not rely exclusively on the rationales, butuse them only as an added source of information.The idea is to help direct the learning algorithm?sattention?helping it tease apart signal from noise.Machine learning algorithms face a well-known?credit assignment?
problem.
Given a complex da-tum xi and the desired response yi, many features ofxi could be responsible for the choice of yi.
Thelearning algorithm must tease out which featureswere actually responsible.
This requires a lot oftraining data, and often a lot of computation as well.Our rationales offer a shortcut to solving this?credit assignment?
problem, by providing thelearning algorithm with hints as to which featuresof xi were relevant.
Rationales should help guidethe learning algorithm toward the correct classifica-tion function, by pushing it toward a function thatcorrectly pays attention to each example?s relevantfeatures.
This should help the algorithm learn fromless data and avoid getting trapped in local maxima.1In this paper, we demonstrate the ?annotator ra-tionales?
technique on a text categorization problempreviously studied by others.1To understand the local maximum issue, consider the hardproblem of training a standard 3-layer feed-forward neural net-work.
If the activations of the ?hidden?
layer?s features (nodes)were observed at training time, then the network would de-compose into a pair of independent 2-layer perceptrons.
Thisturns an NP-hard problem with local maxima (Blum and Rivest,1992) to a polytime-solvable convex problem.
Although ratio-nales might only provide indirect evidence of the hidden layer,this would still modify the objective function (see section 8) ina way that tended to make the correct weights easier to discover.2603 Discriminative ApproachOne popular approach for text categorization is touse a discriminative model such as a Support Vec-tor Machine (SVM) (e.g.
(Joachims, 1998; Dumais,1998)).
We propose that SVM training can in gen-eral incorporate annotator rationales as follows.From the rationale annotations on a positive ex-ample ?
?xi , we will construct one or more ?not-quite-as-positive?
contrast examples ?
?vij .
In our text cat-egorization experiments below, each contrast docu-ment ?
?vij was obtained by starting with the originaland ?masking out?
one or all of the several rationalesubstrings that the annotator had highlighted (rij).The intuition is that the correct model should be lesssure of a positive classification on the contrast exam-ple ?
?vij than on the original example ~xi, because?
?vijlacks evidence that the annotator found significant.We can translate this intuition into additional con-straints on the correct model, i.e., on the weight vec-tor ~w.
In addition to the usual SVM constraint onpositive examples that ~w ?
?
?xi ?
1, we also want (foreach j) that ~w ?
~xi ?
~w ??
?vij ?
?, where ?
?
0 con-trols the size of the desired margin between originaland contrast examples.An ordinary soft-margin SVM chooses ~w and ~?
tominimize12?~w?2 + C(?i?i) (1)subject to the constraints(?i) ~w ?
?
?xi ?
yi ?
1?
?i (2)(?i) ?i ?
0 (3)where ?
?xi is a training example, yi ?
{?1,+1} isits desired classification, and ?i is a slack variablethat allows training example ?
?xi to miss satisfyingthe margin constraint if necessary.
The parameterC > 0 controls the cost of taking such slack, andshould generally be lower for noisier or less linearlyseparable datasets.
We add the contrast constraints(?i, j) ~w ?
(?
?xi ??
?vij) ?
yi ?
?(1?
?ij), (4)where ?
?vij is one of the contrast examples con-structed from example ?
?xi , and ?ij ?
0 is an asso-ciated slack variable.
Just as these extra constraintshave their own margin ?, their slack variables havetheir own cost, so the objective function (1) becomes12?~w?2 + C(?i?i) + Ccontrast(?i,j?ij) (5)The parameter Ccontrast ?
0 determines the impor-tance of satisfying the contrast constraints.
It shouldgenerally be less than C if the contrasts are noisierthan the training examples.2In practice, it is possible to solve this optimizationusing a standard soft-margin SVM learner.
Dividingequation (4) through by ?, it becomes(?i, j) ~w ?
?
?xij ?
yi ?
1?
?ij , (6)where ??xijdef=??xi???vij?
.
Since equation (6) takesthe same form as equation (2), we simply add thepairs (?
?xij , yi) to the training set as pseudoexam-ples, weighted by Ccontrast rather than C so that thelearner will use the objective function (5).There is one subtlety.
To allow a biased hyper-plane, we use the usual trick of prepending a 1 el-ement to each training example.
Thus we require~w ?
(1,?
?xi) ?
1 ?
?i (which makes w0 play therole of a bias term).
This means, however, that wemust prepend a 0 element to each pseudoexample:~w ?
(1,~xi)?(1,??vij)?
= ~w ?
(0,?
?xij) ?
1?
?ij .In our experiments, we optimize ?, C, andCcontrast on held-out data (see section 5.2).4 Rationale Annotation for Movie ReviewsIn order to demonstrate that annotator rationaleshelp machine learning, we needed annotated datathat included rationales for the annotations.We chose a dataset that would be enjoyable to re-annotate: the movie review dataset of (Pang et al,2002; Pang and Lee, 2004).3 The dataset consistsof 1000 positive and 1000 negative movie reviewsobtained from the Internet Movie Database (IMDb)review archive, all written before 2002 by a total of312 authors, with a cap of 20 reviews per author per2Taking Ccontrast to be constant means that all rationalesare equally valuable.
One might instead choose, for example,to reduce Ccontrast for examples xi that have many rationales,to prevent xi?s contrast examples vij from together dominatingthe optimization.
However, in this paper we assume that an xiwith more rationales really does provide more evidence aboutthe true classifier ~w.3Polarity dataset version 2.0.261category.
Pang and Lee have divided the 2000 docu-ments into 10 folds, each consisting of 100 positivereviews and 100 negative reviews.The dataset is arguably artificial in that it keepsonly reviews where the reviewer provided a ratherhigh or rather low numerical rating, allowing Pangand Lee to designate the review as positive or neg-ative.
Nonetheless, most reviews contain a difficultmix of praise, criticism, and factual description.
Infact, it is possible for a mostly critical review to givea positive overall recommendation, or vice versa.4.1 Annotation procedureRationale annotators were given guidelines4 thatread, in part:Each review was intended to give either a positive or a neg-ative overall recommendation.
You will be asked to justify whya review is positive or negative.
To justify why a review is posi-tive, highlight the most important words and phrases that wouldtell someone to see the movie.
To justify why a review is nega-tive, highlight words and phrases that would tell someone not tosee the movie.
These words and phrases are called rationales.You can highlight the rationales as you notice them, whichshould result in several rationales per review.
Do your best tomark enough rationales to provide convincing support for theclass of interest.You do not need to go out of your way to mark everything.You are probably doing too much work if you find yourself go-ing back to a paragraph to look for even more rationales in it.Furthermore, it is perfectly acceptable to skim through sectionsthat you feel would not contain many rationales, such as a re-viewer?s plot summary, even if that might cause you to miss arationale here and there.The last two paragraphs were intended to providesome guidance on how many rationales to annotate.Even so, as section 4.2 shows, some annotators wereconsiderably more thorough (and slower).Annotators were also shown the following exam-ples5 of positive rationales:?
you will enjoy the hell out of American Pie.?
fortunately, they managed to do it in an interesting andfunny way.?
he is one of the most exciting martial artists on the bigscreen, continuing to perform his own stunts and daz-zling audiences with his flashy kicks and punches.?
the romance was enchanting.and the following examples5 of negative rationales:4Available at http://cs.jhu.edu/?ozaidan/rationales.5For our controlled study of annotation time (section 4.2),different examples were given with full document context.Figure 1: Histograms of rationale counts per document (A0?sannotations).
The overall mean of 8.55 is close to that of thefour annotators in Table 1.
The median and mode are 8 and 7.?
A woman in peril.
A confrontation.
An explosion.
Theend.
Yawn.
Yawn.
Yawn.?
when a film makes watching Eddie Murphy a tedious ex-perience, you know something is terribly wrong.?
the movie is so badly put together that even the mostcasual viewer may notice themiserable pacing and strayplot threads.?
don?t go see this movieThe annotation involves boldfacing the rationalephrases using an HTML editor.
Note that a fancierannotation tool would be necessary for a task likenamed entity tagging, where an annotator must markmany named entities in a single document.
At anygiven moment, such a tool should allow the annota-tor to highlight, view, and edit only the several ra-tionales for the ?current?
annotated entity (the onemost recently annotated or re-selected).One of the authors (A0) annotated folds 0?8 ofthe movie review set (1,800 documents) with ra-tionales that supported the gold-standard classifica-tions.
This training/development set was used forall of the learning experiments in sections 5?6.
Ahistogram of rationale counts is shown in Figure 1.As mentioned in section 3, the rationale annotationswere just textual substrings.
The annotator did notrequire knowledge of the classifier features.
Thus,our rationale dataset is a new resource4 that couldalso be used to study exploitation of rationales un-der feature sets or learning methods other than thoseconsidered here (see section 8).4.2 Inter-annotator agreementTo study the annotation process, we randomly se-lected 150 documents from the dataset.
The doc-262Rationales % rationales also % rationales also % rationales also % rationales also % rationales alsoper document annotated by A1 annotated by A2 annotated by AX annotated by AY ann.
by anyone elseA1 5.02 (100) 69.6 63.0 80.1 91.4A2 10.14 42.3 (100) 50.2 67.8 80.9AX 6.52 49.0 68.0 (100) 79.9 90.9AY 11.36 39.7 56.2 49.3 (100) 75.5Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3.
A rationale by Ai (?I think this is a greatmovie!?)
is considered to have been annotated also by Aj if at least one of Aj?s rationales overlaps it (?I think this is a greatmovie!?).
In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class.
Noticethat the most thorough annotatorAY caught most rationales marked by the others (exhibiting high ?recall?
), and that most rationalesenjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high ?precision?
).uments were split into three groups, each consistingof 50 documents (25 positive and 25 negative).
Eachsubset was used for one of three tasks:6?
Task 1: Given the document, annotate only theclass (positive/negative).?
Task 2: Given the document and its class, an-notate some rationales for that class.?
Task 3: Given the document, annotate both theclass and some rationales for it.We carried out a pilot study (annotators AX andAY: two of the authors) and a later, more controlledstudy (annotators A1 and A2: paid students).
Thelatter was conducted in a more controlled environ-ment where both annotators used the same annota-tion tool and annotation setup as each other.
Theirguidelines were also more detailed (see section 4.1).In addition, the documents for the different taskswere interleaved to avoid any practice effect.The annotators?
classification accuracies in Tasks1 and 3 (against Pang & Lee?s labels) ranged from92%?97%, with 4-way agreement on the class for89% of the documents, and pairwise agreement alsoranging from 92%?97%.
Table 1 shows how manyrationales the annotators provided and how welltheir rationales agreed.Interestingly, in Task 3, four of AX?s ratio-nales for a positive class were also partiallyhighlighted by AY as support for AY?s (incorrect)negative classifications, such as:6Each task also had a ?warmup?
set of 10 documents to beannotated before that tasks?s 50 documents.
Documents forTasks 2 and 3 would automatically open in an HTML editorwhile Task 1 documents opened in an HTML viewer with noediting option.
The annotators recorded their classifications forTasks 1 and 3 on a spreadsheet.min./KB A1 time A2 time AX time AY timeTask 1 0.252 0.112 0.150 0.422Task 2 0.396 0.537 0.242 0.626Task 3 0.399 0.505 0.288 1.01min./doc.
A1 time A2 time AX time AY timeTask 1 1.04 0.460 0.612 1.73min./rat.
A1 time A2 time AX time AY timeTask 2 0.340 0.239 0.179 0.298Task 3 0.333 0.198 0.166 0.302Table 2: Average annotation rates on each task.?
Even with its numerous flaws, the movie all comes to-gether, if only for those who .
.
.?
?Beloved?
acts like an incredibly difficult chamberdrama paired with a ghost story.4.3 Annotation timeAverage annotation times are in Table 2.
As hoped,rationales did not take too much extra time for mostannotators to provide.
For each annotator exceptA2, providing rationales only took roughly twice thetime (Task 3 vs.
Task 1), even though it meant mark-ing an average of 5?11 rationales in addition to theclass.Why this low overhead?
Because marking theclass already required the Task 1 annotator to readthe document and find some rationales, even if s/hedid not mark them.
The only extra work in Task 3is in making them explicit.
This synergy betweenclass annotation and rationale annotation is demon-strated by the fact that doing both at once (Task 3)was faster than doing them separately (Tasks 1+2).We remark that this task?binary classification onfull documents?seems to be almost a worst-casescenario for the annotation of rationales.
At a purelymechanical level, it was rather heroic of A0 to at-tach 8?9 new rationale phrases rij to every bit yiof ordinary annotation.
Imagine by contrast a morelocal task of identifying entities or relations.
Each263lower-level annotation yi will tend to have fewer ra-tionales rij , while yi itself will be more complex andhence more difficult to mark.
Thus, we expect thatthe overhead of collecting rationales will be less inmany scenarios than the factor of 2 we measured.Annotation overhead could be further reduced.For a multi-class problem like relation detection, onecould ask the annotator to provide rationales only forthe rarer classes.
This small amount of extra timewhere the data is sparsest would provide extra guid-ance where it was most needed.
Another possibilityis passive collection of rationales via eye tracking.5 Experimental Procedures5.1 Feature extractionAlthough this dataset seems to demand discourse-level features that contextualize bits of praise andcriticism, we exactly follow Pang et al (2002) andPang and Lee (2004) in merely using binary uni-gram features, corresponding to the 17,744 un-stemmed word or punctuation types with count ?
4in the full 2000-document corpus.
Thus, each docu-ment is reduced to a 0-1 vector with 17,744 dimen-sions, which is then normalized to unit length.7We used the method of section 3 to place addi-tional constraints on a linear classifier.
Given a train-ing document, we create several contrast documents,each by deleting exactly one rationale substringfrom the training document.
Converting documentsto feature vectors, we obtained an original exam-ple ?
?xi and several contrast examples??vi1,?
?vi2, .
.
..8Again, our training method required each originaldocument to be classified more confidently (by amargin ?)
than its contrast documents.If we were using more than unigram features, thensimply deleting a rationale substring would not al-ways be the best way to create a contrast document,as the resulting ungrammatical sentences mightcause deep feature extraction to behave strangely(e.g., parse errors during preprocessing).
The goal increating the contrast document is merely to suppress7The vectors are normalized before prepending the 1 corre-sponding to the bias term feature (mentioned in section 3).8The contrast examples were not normalized to preciselyunit length, but instead were normalized by the same factor usedto normalize ?
?xi .
This conveniently ensured that the pseudoex-amples ??xijdef=~xi???vij?
were sparse vectors, with 0 coordinatesfor all words not in the jth rationale.features (n-grams, parts of speech, syntactic depen-dencies .
.
. )
that depend in part on material in oneor more rationales.
This could be done directly bymodifying the feature extractors, or if one prefers touse existing feature extractors, by ?masking?
ratherthan deleting the rationale substring?e.g., replacingeach of its word tokens with a special MASK tokenthat is treated as an out-of-vocabulary word.5.2 Training and testing proceduresWe transformed this problem to an SVM problem(see section 3) and applied SVMlight for training andtesting, using the default linear kernel.
We used onlyA0?s rationales and the true classifications.Fold 9 was reserved as a test set.
All accuracyresults reported in the paper are the result of testingon fold 9, after training on subsets of folds 0?8.Our learning curves show accuracy after trainingon T < 9 folds (i.e., 200T documents), for variousT .
To reduce the noise in these results, the accuracywe report for training on T folds is actually the aver-age of 9 different experiments with different (albeitoverlapping) training sets that cover folds 0?8:198?i=0acc(F9 | ?
?, Fi+1 ?
.
.
.
?
Fi+T ) (7)where Fj denotes the fold numbered j mod 9, andacc(Z | ?, Y ) means classification accuracy on theset Z after training on Y with hyperparameters ?.To evaluate whether two different training meth-ods A and B gave significantly different average-accuracy values, we used a paired permutation test(generalizing a sign test).
The test assumes in-dependence among the 200 test examples but notamong the 9 overlapping training sets.
For eachof the 200 test examples in fold 9, we measured(ai, bi), where ai (respectively bi) is the numberof the 9 training sets under which A (respectivelyB) classified the example correctly.
The p valueis the probability that the absolute difference be-tween the average-accuracy values would reach orexceed the observed absolute difference, namely| 1200?200i=1ai?bi9 |, if each (ai, bi) had an independent1/2 chance of being replaced with (bi, ai), as per thenull hypothesis that A and B are indistinguishable.For any given value of T and any given train-ing method, we chose hyperparameters ??
=264Figure 2: Classification accuracy under five different experi-mental setups (S1?S5).
At each training size, the 5 accura-cies are pairwise significantly different (paired permutation test,p < 0.02; see section 5.2), except for {S3,S4} or {S4,S5} atsome sizes.
(C, ?,Ccontrast) to maximize the following cross-validation performance:9??
= argmax?8?i=0acc(Fi | ?, Fi+1 ?
.
.
.
?
Fi+T )(8)We used a simple alternating optimization procedurethat begins at ?0 = (1.0, 1.0, 1.0) and cycles repeat-edly through the three dimensions, optimizing alongeach dimension by a local grid search with resolu-tion 0.1.10 Of course, when training without ratio-nales, we did not have to optimize ?
or Ccontrast.6 Experimental Results6.1 The value of rationalesThe top curve (S1) in Figure 2 shows that perfor-mance does increase when we introduce rationalesfor the training examples as contrast examples (sec-tion 3).
S1 is significantly higher than the baselinecurve (S2) immediately below it, which trains an or-dinary SVM classifier without using rationales.
Atthe largest training set size, rationales raise the accu-racy from 88.5% to 92.2%, a 32% error reduction.9One might obtain better performance (across all methodsbeing compared) by choosing a separate ??
for each of the 9training sets.
However, to simulate real limited-data trainingconditions, one should then find the ??
for each {i, ..., j} us-ing a separate cross-validation within {i, ..., j} only; this wouldslow down the experiments considerably.10For optimizing along the C dimension, one could use theefficient method of Beineke et al (2004), but not in SVMlight.The lower three curves (S3?S5) show that learn-ing is separately helped by the rationale and thenon-rationale portions of the documents.
S3?S5are degraded versions of the baseline S2: they areordinary SVM classifiers that perform significantlyworse than S2 (p < 0.001).Removing the rationale phrases from the train-ing documents (S3) made the test documents muchharder to discriminate (compared to S2).
This sug-gests that annotator A0?s rationales often coveredmost of the usable evidence for the true class.However, the pieces to solving the classificationpuzzle cannot be found solely in the short rationalephrases.
Removing all non-rationale text from thetraining documents (S5) was even worse than re-moving the rationales (S3).
In other words, we can-not hope to do well simply by training on just therationales (S5), although that approach is improvedsomewhat in S4 by treating each rationale (similarlyto S1) as a separate SVM training example.This presents some insight into why our methodgives the best performance.
The classifier in S1is able to extract subtle patterns from the corpus,like S2, S3, or any other standard machine learn-ing method, but it is also able to learn from a humanannotator?s decision-making strategy.6.2 Using fewer rationalesIn practice, one might annotate rationales for onlysome training documents?either when annotating anew corpus or when adding rationales post hoc toan existing corpus.
Thus, a range of options can befound between curves S2 and S1 of Figure 2.Figure 3 explores this space, showing how far thelearning curve S2 moves upward if one has time toannotate rationales for a fixed number of documentsR.
The key useful discovery is that much of the ben-efit can actually be obtained with relatively few ra-tionales.
For example, with 800 training documents,annotating (0%, 50%, 100%) of themwith rationalesgives accuracies of (86.9%, 89.2%, 89.3%).
Withthe maximum of 1600 training documents, annotat-ing (0%, 50%, 100%) with rationales gives (88.5%,91.7%, 92.2%).To make this point more broadly, we find that theR = 200 curve is significantly above the R = 0curve (p < 0.05) at all T ?
1200.
By contrast, theR = 800, R = 1000, .
.
.
R = 1600 points at each T265Figure 3: Classification accuracy for T ?
{200, 400, ..., 1600}training documents (x-axis) when only R ?
{0, 200, ..., T} ofthem are annotated with rationales (different curves).
The R =0 curve above corresponds to the baseline S2 from Figure 2.S1?s points are found above as the leftmost points on the othercurves, where R = T .value are all-pairs statistically indistinguishable.The figure also suggests that rationales and docu-ments may be somewhat orthogonal in their benefit.When one has many documents and few rationales,there is no longer much benefit in adding more doc-uments (the curve is flattening out), but adding morerationales seems to provide a fresh benefit: ratio-nales have not yet reached their point of diminishingreturns.
(While this fresh benefit was often statisti-cally significant, and greater than the benefit frommore documents, our experiments did not establishthat it was significantly greater.
)The above experiments keep all of A0?s rationaleson a fraction of training documents.
We also exper-imented with keeping a fraction of A0?s rationales(chosen randomly with randomized rounding) on alltraining documents.
This yielded no noteworthy orstatistically significant differences from Figure 3.These latter experiments simulate a ?lazy annota-tor?
who is less assiduous than A0.
Such annotatorsmay be common in the real world.
We also suspectthat they will be more desirable.
First, they shouldbe able to add more rationales per hour than the A0-style annotator from Figure 3: some rationales aresimply more noticeable than others, and a lazy anno-tator will quickly find the most noticeable ones with-out wasting time tracking down the rest.
Second, the?most noticeable?
rationales that they mark may bethe most effective ones for learning, although ourrandom simulation of laziness could not test that.7 Related WorkOur rationales resemble ?side information?
in ma-chine learning?supplementary information aboutthe target function that is available at training time.Side information is sometimes encoded as ?virtualexamples?
like our contrast examples or pseudoex-amples.
However, past work generates these byautomatically transforming the training examplesin ways that are expected to preserve or alter theclassification (Abu-Mostafa, 1995).
In another for-mulation, virtual examples are automatically gener-ated but must be manually annotated (Kuusela andOcone, 2004).
Our approach differs because a hu-man helps to generate the virtual examples.
Enforc-ing a margin between ordinary examples and con-trast examples also appears new.Other researchers have considered how to reduceannotation effort.
In active learning, the annotatorclassifies only documents where the system so far isless confident (Lewis and Gale, 1994), or in an in-formation extraction setting, incrementally correctsdetails of the system?s less confident entity segmen-tations and labelings (Culotta andMcCallum, 2005).Raghavan et al (2005) asked annotators to iden-tify globally ?relevant?
features.
In contrast, our ap-proach does not force the annotator to evaluate theimportance of features individually, nor in a globalcontext outside any specific document, nor even toknow the learner?s feature space.
Annotators onlymark text that supports their classification decision.Our methods then consider the combined effect ofthis text on the feature vector, which may includecomplex features not known to the annotator.8 Future Work: Generative modelsOur SVM contrast method (section 3) is not the onlypossible way to use rationales.
We would like to ex-plicitly model rationale annotation as a noisy pro-cess that reflects, imperfectly and incompletely, theannotator?s internal decision procedure.A natural approach would start with log-linearmodels in place of SVMs.
We can define a proba-bilistic classifierp?
(y | x)def=1Z(x)expk?h=1?hfh(x, y) (9)266where ~f(?)
extracts a feature vector from a classifieddocument.A standard training method would be to choose ?to maximize the conditional likelihood of the train-ing classifications:argmax~?n?i=1p?
(yi | xi) (10)When a rationale ri is also available for each(xi, yi), we propose to maximize a likelihood thattries to predict these rationale data as well:argmax~?n?i=1p?
(yi | xi) ?
p??
(ri | xi, yi, ?)
(11)Notice that a given guess of ?
might make equa-tion (10) large, yet accord badly with the annotator?srationales.
In that case, the second term of equa-tion (11) will exert pressure on ?
to change to some-thing that conforms more closely to the rationales.If the annotator is correct, such a ?
will generalizebetter beyond the training data.In equation (11), p??
models the stochastic processof rationale annotation.
What is an annotator actu-ally doing when she annotates rationales?
In par-ticular, how do her rationales derive from the truevalue of ?
and thereby tell us about ??
Building agood model p??
of rationale annotation will requiresome exploratory data analysis.
Roughly, we expectthat if ?hfh(xi, y) is much higher for y = yi thanfor other values of y, then the annotator?s ri is corre-spondingly more likely to indicate in some way thatfeature fh strongly influenced annotation yi.
How-ever, we must also model the annotator?s limited pa-tience (she may not annotate all important features),sloppiness (she may indicate only indirectly that fhis important), and bias (tendency to annotate somekinds of features at the expense of others).One advantage of this generative approach is thatit eliminates the need for contrast examples.
Con-sider a non-textual example in which an annotatorhighlights the line crossing in a digital image of thedigit ?8?
to mark the rationale that distinguishes itfrom ?0.?
In this case it is not clear how to mask outthat highlighted rationale to create a contrast exam-ple in which relevant features would not fire.1111One cannot simply flip those highlighted pixels to white9 ConclusionsWe have proposed a quite simple approach to im-proving machine learning by exploiting the clever-ness of annotators, asking them to provide enrichedannotations for training.
We developed and testeda particular discriminative method that can use ?an-notator rationales?
?even on a fraction of the train-ing set?to significantly improve sentiment classifi-cation of movie reviews.We found fairly good annotator agreement on therationales themselves.
Most annotators providedseveral rationales per classification without takingtoo much extra time, even in our text classificationscenario, where the rationales greatly outweigh theclassifications in number and complexity.
Greaterspeed might be possible through an improved userinterface or passive feedback (e.g., eye tracking).In principle, many machine learning methodsmight be modified to exploit rationale data.
Whileour experiments in this paper used a discriminativeSVM, we plan to explore generative approaches.ReferencesY.
S. Abu-Mostafa.
1995.
Hints.
Neural Computation, 7:639?671, July.P.
Beineke, T. Hastie, and S. Vaithyanathan.
2004.
The sen-timental factor: Improving review classification via human-provided information.
In Proc.
of ACL, pages 263?270.A.
L. Blum and R. L. Rivest.
1992.
Training a 3-node neuralnetwork is NP-complete.
Neural Networks, 5(1):117?127.A.
Culotta and A. McCallum.
2005.
Reducing labeling effortfor structured prediction tasks.
In AAAI, pages 746?751.S.
Dumais.
1998.
Using SVMs for text categorization.
IEEEIntelligent Systems Magazine, 13(4), July/August.T.
Joachims.
1998.
Text categorization with support vectormachines: Learning with many relevant features.
In Proc.
ofthe European Conf.
on Machine Learning, pages 137?142.P.
Kuusela and D. Ocone.
2004.
Learning with side informa-tion: PAC learning bounds.
J. of Computer and System Sci-ences, 68(3):521?545, May.D.
D. Lewis and W. A. Gale.
1994.
A sequential algorithm fortraining text classifiers.
In Proc.
of ACM-SIGIR, pages 3?12.B.
Pang and L. Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity summarization based onminimum cuts.
In Proc.
of ACL, pages 271?278.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbs up?Sentiment classification using machine learning techniques.In Proc.
of EMNLP, pages 79?86.H.
Raghavan, O. Madani, and R. Jones.
2005.
Interactive fea-ture selection.
In Proc.
of IJCAI, pages 41?46.or black, since that would cause new features to fire.
Possiblyone could simply suppress any feature that depends in any wayon the highlighted pixels, but this would take away too manyimportant features, including global features.267
