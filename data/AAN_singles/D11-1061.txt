Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 659?669,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLinguistic Redundancy in TwitterFabio Massimo ZanzottoUniversity of Rome ?Tor Vergata?Rome, Italyzanzotto@info.uniroma2.itMarco PennacchiottiYahoo!
LabsSunnyvale, CA, 94089pennac@yahoo-inc.comKostas TsioutsiouliklisYahoo!
LabsSunnyvale, CA, 94089kostas@yahoo-inc.comAbstractIn the last few years, the interest of the re-search community in micro-blogs and socialmedia services, such as Twitter, is growing ex-ponentially.
Yet, so far not much attention hasbeen paid on a key characteristic of micro-blogs: the high level of information redun-dancy.
The aim of this paper is to systemat-ically approach this problem by providing anoperational definition of redundancy.
We castredundancy in the framework of Textual En-tailment Recognition.
We also provide quan-titative evidence on the pervasiveness of re-dundancy in Twitter, and describe a datasetof redundancy-annotated tweets.
Finally, wepresent a general purpose system for identify-ing redundant tweets.
An extensive quantita-tive evaluation shows that our system success-fully solves the redundancy detection task, im-proving over baseline systems with statisticalsignificance.1 IntroductionMicro-blogs and social media services, such as Twit-ter, have experienced an exponential growth in thelast few years.
The interest of the research commu-nity and the industry in these services has followeda similar trend.
Web companies such as Google, Ya-hoo, and Bing are integrating more and more socialcontent to their sites.
At the same time, the compu-tational linguistic community is getting increasinglyinterested in studying social and linguistic proper-ties of Twitter and other micro-blogs (Java et al,2007; Krishnamurthy et al, 2008; Kwak et al, 2010;Zhao et al, 2007; Popescu and Pennacchiotti, 2010;Petrovic?
et al, 2010; Lin et al, 2010; Liu et al,2010; Ritter et al, 2010).
Yet, so far, not muchattention has been paid on a key characteristic ofmicro-blogs: the high level of information redun-dancy.
Users often post messages with the same, orvery similar, content, especially when reporting orcommenting on news and events.
For example, thefollowing two tweets are part of a large set of redun-dant tweets issued during the 2010 winter Olympics:(example 1)t1 : ?Swiss ski jumper Simon Ammann takes first gold ofVancouver?t2 : ?Swiss (Suisse) get the Gold on Normal Hill ski jump.#Vancouver2010?By performing an editorial study (described later inthe paper) we discovered that a large part of event-related tweets are indeed redundant.Detecting information redundancy is importantfor various reasons.
First, most applications basedon Twitter share the goal of providing tweets thatare both informative and diverse, with respect to aninitial user information need.
For example, Twittersearch engines should ideally select the most infor-mative and diverse set of tweets in return to a userquery.
Similarly, a news web portal that attachestweets to a given news article should attach thosetweets that provide the broadest and most diverseset of information, opinions, and updates about thenews item.
To keep a high level of diversity, redun-dant tweets should be removed from the set of tweetsdisplayed to the user.
Figure 1 shows an example ofa Twitter search engine where redundant tweets are659Figure 1: Twitter search: actual Twitter results and desired results after redundancy reduction.present (left) and where they are discarded (right).Also, from a computational linguistic point ofview, the high redundancy in micro-blogs gives theunprecedented opportunity to study classical taskssuch as text summarization (Haghighi and Vander-wende, 2009), textual entailment recognition (Da-gan et al, 2006) and paraphrase detection (Dolan etal., 2004) on very large corpora characterized by anoriginal and emerging linguistic style, pervaded withungrammatical and colloquial expressions, abbrevi-ations, and new linguistic forms.The aim of this paper is to formally define, for thefirst time, the problem of redundancy in micro-blogsand to systematically approach the task of automaticredundancy detection.
Note that we focus on lin-guistic redundancy, i.e.
tweets that convey the sameinformation with different wordings, and ignore themore trivial issue of detecting retweets, which canbe considered the most basic expression of redun-dancy.The main contributions of this paper are the fol-lowing:?
We formally define the problem of redundancydetection in micro-blogs within the frameworkof Textual Entailment theory;?
We report results from an editorial study andprovide quantitative evidence of the pervasive-ness of redundancy in Twitter;?
We present a set of simple and effective ma-chine learning models for solving the task ofredundancy detection;?
We provide promising experimental results thatshow that these models outperform baseline ap-proaches with statistical significance, and wereport a qualitative evaluation revealing the ad-vantages of the proposed model.The rest of the paper is organized as follows.First, we shortly describe related work in Section 2.Next, we provide our operational definition of re-dundancy and introduce our editorial study anddataset in Section 3.
In Section 4 we describe ourmodels for redundancy detection.
In Section 5 weprovide a quantitative and qualitative evaluation ofour models.
In Section 6 we conclude the paper withfinal observations and future work.2 Related WorkSo far, most research on Twitter has focused onits network structure, the social behavior of itsusers (Java et al, 2007; Krishnamurthy et al, 2008;Kwak et al, 2010), ranking tweets by relevance forweb search (Ramage et al, 2010; Duan et al, 2010),and the analysis of time series for extracting trendingnews, events and facts (Zhao et al, 2007; Popescuand Pennacchiotti, 2010; Petrovic?
et al, 2010; Linet al, 2010).
Only few studies have specifically fo-cused on the linguistic content analysis of tweets,e.g.
(Davidov et al, 2010; Barbosa and Feng, 2010).To date, our paper most closely relates to works onsemantic role labeling (SRL) on social media (Liu etal., 2010) and conversation modeling (Ritter et al,2010).Liu et al (2010) present a self-learning SRL sys-tem for news tweets, with the goal of addressing lowperformance caused by the noise and the unstruc-tured nature of the data.
The authors first clustertogether tweets that refer to the same news.
Then,for each cluster, they identify the tweets that are660well-formed (i.e.
copy-pasted from news), and in-duce role mappings between well-formed and noisytweets in the same cluster by performing word align-ment.
In our paper we are also interested in aligningand grouping tweets, although our goal is to detectredundancy, not to perform SRL.On a different ground, Ritter et al (2010) pro-pose a probabilistic model to discover dialogue actsin Twitter conversations and to classify tweets in aconversation according to those acts.
(A conversa-tion is defined as a set of tweets in the same re-ply thread.)
The authors define 10 major dialogueacts for Twitter, including status, question, responseand reaction, and automatically build a probabilis-tic transition graph for such acts.
In our paper, wealso aim at classifying tweets, but our interest is ininformation redundancy instead of acts.In the computational linguistic literature, redun-dancy detection is studied in multi-document sum-marization, where the overall document is usedto select the most informative sentences or snip-pets (Haghighi and Vanderwende, 2009).
Sincetweets are short and tweet sets cannot be considereddocuments, these methods are hard to apply.
A moreconvenient setting is paraphrase detection (Dolan etal., 2004) and textual entailment recognition (Daganet al, 2006) (RTE).In RTE the task is to recognize if a text calledthe text T (typically one or two sentences long) en-tails another text called the hypothesis H .
Many ap-proaches have been proposed for this task, mostlybased on machine learning.
Three main classesof features have been so far explored in RTE: dis-tance/similarity feature spaces (Corley and Mihal-cea, 2005; Newman et al, 2005; Haghighi et al,2005; Hickl et al, 2006), entailment trigger fea-ture spaces (de Marneffe et al, 2006; MacCartneyet al, 2006), and pair content feature spaces (Zan-zotto et al, 2009).
Distance/similarity feature spacesare more suitable to the paraphrase detection taskbecause they model the similarity between the twotexts.
On the other hand, entailment trigger and con-tent feature spaces model complex relations betweenthe texts, taking into account first-order entailmentrules, i.e.
entailment rules with variables.In this paper, one of our goals is to explore RTEtechniques and features that are usually used forclassical texts, and check if they can be successfullyadapted to the unstructured, and oftentimes ungram-matical, Twitter language.3 Redundancy in TwitterWe formally define two tweets as redundant if theyeither convey the same information (paraphrase) orif the information of one tweet subsumes the infor-mation of the other (textual entailment).
For exam-ple, the pair in (example 1) is redundant.
The firsttweet subsumes (i.e.
?textually entails?)
the other;both tweets state that Switzerland won a Gold Medalat the Vancouver winter Olympics, but the first onealso specifies the name of the athlete.
The follow-ing pair is, instead, non-redundant, because the twotweets convey different information, and they do notsubsume each other:(example 2)t1 : ?Goal!
Iniesta scores for #ESP and they have onehand on the #worldcup?t2 : ?this will be a hard final #Esp vs Ned #worldcup?Our definition of redundancy is grounded on, and in-spired by, the theory of Textual Entailment, to whichwe refer the reader for further details (Dagan et al,2006).3.1 Quantifying redundancyHow pervasive is redundancy in Twitter?
In order toanswer this question we performed an initial edito-rial study where human editors were asked to anno-tate pairs of tweets as being either redundant or non-redundant.
The editorial study also serves as a testbed for evaluating our redundancy detection models,as discussed in Section 5.In the study we focus on ?informative?
tweets,i.e.
tweets that describe or comment on relevantevents/facts.
Indeed, these are the types of tweetsfor which redundancy is a critical issue, especiallyin view of real applications, e.g.
to present a diverseset of tweets for a given news article.
Other types oftweets, such as status updates, self-promotions, andpersonal messages are of less interest in this context.Dataset extraction.
The study is performed onan automatically built dataset of informative tweets.The most critical issue for extracting the dataset isto pre-process tweets and to discard those that are661not informative.
This is not an easy task: a recentstudy (Pear-Analytics, 2009) estimates that only 4%of all tweets are factual news, and only 37% are con-versations with content.
The rest are spam, statusupdates and other types of uninformative content.In order to retain only informative tweets we firstextract buzzy snapshots (Popescu and Pennacchiotti,2010).
A snapshot is defined as a set of tweets thatexplicitly mention a specific topic within a speci-fied time period.
A buzzy snapshot is defined as asnapshot with a large number of tweets, comparedto previous time periods.
For example, given thetopic ?Haiti earthquake?, the snapshot composed bythe tweets mentioning ?Haiti earthquake?
on January12th, 2010, will constitute a buzzy snapshot, since inprevious days the topic was not mentioned often.We use two different topic lists: a celebrity listcontaining about 104K celebrity names, crawledfrom Wikipedia, including actors, musicians, politi-cians, and athletes; and an event list composed of398 hashtags related to 8 major events that hap-pened between January and July 2010, and listedin Wikipedia: 1 the earthquake in Haiti, the winterOlympics, the earthquake in Chile, the death of thePolish president, the volcano eruption in Iceland, theoil spill in the Gulf of Mexico, the Greek financialcrisis, and the FIFA world cup.We extract buzzy snapshots for the above twotopic lists by following the method describedin (Popescu and Pennacchiotti, 2010): we considertime periods of one day, and call buzzy the snapshotsthat mention a given topic ?
times more than the av-erage over the previous 2 days.
We set ?
to 20 and 5respectively for the celebrity list and the event list.We further exclude irrelevant and spam snapshotsby removing those that have: fewer than 10 tweets;more than 50% of tweets non-English; and an aver-age token overlap between tweets of more than 80%,usually corresponding to spam threads.The extraction is performed on a Twitter corpuscontaining all tweets posted between July 2009 andAugust 2010.
In all, we extract 972 snapshots forthe celebrity list, containing 205,885 tweets (i.e.
av-erage of 212 tweets per snapshot); and 674 snap-1Hashtags are keywords prefixed by ?#?, that are used by theTwitter community to mark the topic of a tweet.
We collectedour set of hashtags by semi-automatically inspecting the Twitterstream in the days the major events happened.redundant 367 (29.5%)entailment 195 (15.7%)paraphrase 172 (13.5%)non-redundant 875 (70.5%)related 541 (43.6%)unrelated 334 (26.9%)Table 1: Results of the redundancy editorial study.shots for the event list, containing 393,965 tweets(584 tweets per snapshot).The above two final snapshot corpora (i.e.
the 972celebrities?
snapshots and 674 events?
snapshots)can be considered a good representation of event de-scriptions and comments on Twitter, thus formingour initial set of ?informative?
tweets.
From thesetwo corpora, we extract the final tweet-pair datasetby randomly sampling 1500 pairs of tweets con-tained in the same snapshot.
Tweet-pairs that con-tain retweets are excluded.Dataset annotation.
The main editorial task con-sisted of annotating tweet-pairs as either redundantor non-redundant.
We also asked editors to char-acterize the specific linguistic relation between thetwo tweets of a pair.
We consider four relations: en-tailment (the first tweet entails the second or viceversa), paraphrase, contradiction (the tweets con-tradict each other), and related (the tweets are aboutthe same topic, e.g.
the Haiti earthquake, but arein none of the previous relations).
Tweets that wereabout different topics were labeled unrelated.
An-notators were asked to base their decisions on theparts of the tweets that contained information rel-evant to the selected topic, e.g.
the earthquake inHaiti.
These parts were marked in the corpus.
Fo-cusing on these parts is in line with potential appli-cations of tweet redundancy detection as tweets arefirstly grouped around a topic.
Note that pairs thatfall under the entailment or paraphrase relation areredundant, while unrelated, related, and contradic-tory tweets are non-redundant.The annotation was performed in a three stageprocess, since tweets are sometimes hard to under-stand and hence to annotate (misspellings, usage ofslang and abbreviations, lack of discourse context).In the first step, the 1500 pairs were independentlyannotated by a pool of 20 trained editors, super-662vised by an expert lead.
In the second step, the an-notations were checked by three highly trained ex-perts with background in computational linguistics:each pair was independently checked by two ex-perts.
Average kappa agreement in this second stepis kappa = 0.63 (corresponding to ?good agree-ment?).
In a final step, discordances between the twoexperts were resolved by the third expert.
Unclearand unresolved pairs after the three stages were dis-carded from the dataset, leaving a final set of 1242pairs.
2Annotation Results.
Table 1 reports the results ofour study.
Among the 1242 tweet-pairs, 367 (30%)are redundant and 875 (70%) are non-redundant.This shows that redundancy is indeed a pervasivephenomenon in Twitter, and a critical issue that hasto be solved in order to provide clean and diversesocial content.
Most cases of redundancy corre-spond to tweets that report the same fact using differ-ent wording, occasionally adding irrelevant personalcomments and sentiments (e.g.
?Johnny Depp died?vs.
?OMG, I am so sad that Johnny Depp is dead?
).4 Redundancy detection modelsThe task of redundancy detection in Twitter is atweet-pair classification problem.
Given two tweetst1 and t2, the goal is to classify the pair (t1, t2) asbeing either redundant or non-redundant.In this section we describe different models forredundancy detection, inspired by existing work inRTE.
We adopt a machine learning approach where aSupport Vector Machine (SVM) is trained on a man-ually annotated training set to classify incoming testexamples as either redundant or non-redundant.
Anevaluation of the different models adopting for train-ing and testing the dataset described in Section 3, ispresented in Section 5.4.1 Bag-of-word model (BOW)The bag-of-word model is the most simple approachfor detecting redundancy.
It is used as a baseline inour experiment.
The simple intuition of the modelis that if two tweets t1 and t2 have a high lexical2At this time, the TwitterTM Terms of Use do not allowpublication of the annotated dataset.
Should the Terms ofUse change, the dataset will become available for download athttp://art.uniroma2.it/zanzotto/datasets.overlap, then they are likely to express the same in-formation ?
i.e.
they are likely to be redundant.
Inthis model, the SVM is trained using a single fea-ture that computes the cosine similarity between thebag-of-word vectors of the two tweets.
The bag-of-word vector is built using a classical tf*idf weightingschema over the set of tokens of the pair.
This a verysimple baseline as SVM is only learning thresholdsusing this single feature.The bag-of-word model is of course a naive ap-proach, since in many cases redundant tweets canhave very different lexical content (e.g.
the fol-lowing two tweets: ?Farrah Fawcett left out of Os-car memorial?, ?No Farrah Fawcett?s memory atthe Academy Awards?
), and non-redundant tweetscan have similar lexical content (e.g.
the tweets:?Johnny Deep is dead?, ?Johnny Deep is not dead?
).4.2 WordNet-based bag-of-word model(WBOW)The second baseline model was first defined in (Cor-ley and Mihalcea, 2005) and since then has beenused by many RTE systems.
The model extendsBOW by measuring similarity at the semantic level,instead of the lexical level.For example, consider the tweet pair: ?Oscarsforgot Farrah Fawcett?, ?Farrah Fawcett snubbed atAcademy Awards?.
This pair is redundant, and,hence, should be assigned a very high similar-ity.
Yet, BOW would assign a low score, sincemany words are not shared across the two tweets.WBOW fixes this problem by matching ?Oscar?-?Academy Awards?
and ?forgot?-?snubbed?
at the se-mantic level.
To provide these matches, WBOW re-lies on specific word similarity measures over Word-Net (Miller, 1995), that allow synonymy and hyper-onymy matches: in our experiments we specificallyuse Jiang&Conrath similarity (Jiang and Conrath,1997).In practice, we implement WBOW by using thetext similarity measure defined in (Corley and Mi-halcea, 2005) as the single feature in the SVM clas-sifier that, as in BOW, learns the threshold on thissingle feature.4.3 Lexical content model (LEX)This model and the next ones (SYNT and FOR) ex-plicitly model the content of a tweet pair P =663(t1, t2) as a whole.
This is a radically different ap-proach with respect to the similarity-based modelsexplored so far, where the content of t1 and t2 weretreated independently (i.e.
each tweet with its ownbag of words), and the SVM used as the single fea-ture the similarity between the two tweets.In the LEX model we represent the content of thetweet pair in a double bag-of-word vector space.Each pair P = (t1, t2) is represented by two bag-of-word vectors, (~t1, ~t2).
Within this space, we canthen define a specific similarity measure betweenpairs using a kernel function in the SVM learningalgorithm.
Given two pairs of tweets P (a) and P (b),the LEX kernel function is defined as follows:KLEX(P (a), P (b)) = cos(t(a)1 , t(b)1 ) + cos(t(a)2 , t(b)2 )where cos(?, ?)
is the cosine similarity between thetwo vectors.
The LEX feature space is simple andcan be extremely effective in modeling the contentof tweet pairs.
Yet, in principle, it doesn?t model therelations among words in the tweet.
Different con-tent feature spaces are then needed to capture theserelations.4.4 Syntactic content model (SYNT)The SYNT model represents a tweet pair usingpairs of syntactic tree fragments from t1 and t2.Each feature is a pair < fr1, fr2 >, where fr1and fr2 are syntactic tree fragments (see figurebelow).
As defined in (Collins and Duffy, 2002),a syntactic tree fragment fri is active in ti whenfri is a subtree of the syntactic interpretation ofti.
Therefore, these features represent ground rulesconnecting the left-hand sides and the right-handsides of the tweet pair: each feature is active for apair (t1, t2) when the left-hand side fr1 is activatedby the syntactic analysis of t1 and the right-handside fr2 is activated by t2.
As an example considerthe feature:?SNP VPVBPboughtNP,SN VPVBPownsNP?This feature is active for the pair of tweets (?GMbought Opel?,?GM owns Opel?)
since the syntac-tic analysis of the pair matches the feature (giventhat the two tweets are correctly syntactically ana-lyzed).
This feature space models the relations be-tween words syntactically.
Therefore it overcomesthe limitations of the LEX feature space.
But it alsointroduces a new limitation: the above feature isin fact also active for the tweet pair (?GM boughtOpel?,?Opel owns GM?).
This pair is extremely dif-ferent from the previous one, thus possibly mislead-ing the classifier.This feature space is not represented explicitly,but it is encoded in a kernel function.
Given twopairs of tweets P (a) and P (b), the SYNT kernel func-tion is defined as follows:KSY NT (P (a), P (b)) = K(t(a)1 , t(b)1 ) + K(t(a)2 , t(b)2 )where K(?, ?)
is the tree kernel function described in(Collins and Duffy, 2002).4.5 Syntactic first-order rule content model(FOR)The FOR model overcomes the limitations of SYNT,by enriching the space with features representingfirst-order relations between the two tweets of apair.
Each feature represents a rule with variables,i.e.
a first order rule that is activated by the tweetpairs if the variables are unified.
This feature spacehas been introduced in (Zanzotto and Moschitti,2006) and shown to improve over the ones above.Each feature < fr1, fr2 > is a pair of syntactic treefragments augmented with variables.
The featureis active for a tweet pair (t1, t2) if the syntacticinterpretations of t1 and t2 can be unified with< fr1, fr2 >.
For example, consider the followingfeature:?SNP X VPVBPboughtNP Y,SNP X VPVBPownsNP Y?This feature is active for the pair (?GM boughtOpel?,?GM owns Opel?
), with the variable unifica-tion X = ?GM?
and Y = ?Opel?.
On the contrary,this feature is not active for the pair (?GM boughtOpel?,?Opel owns GM?)
as there is no possibility ofunifying the two variables.
Efficient algorithms forthe computation of the related kernel functions can664be found in (Moschitti and Zanzotto, 2007; Zanzottoand Dell?Arciprete, 2009).5 Experimental EvaluationIn this section we present an evaluation of the differ-ent redundancy detection models.
First, we definethe experimental setup in Section 5.1.
Then, we an-alyze the results of the experiments in Section 5.2.5.1 Experimental SetupWe experiment with the redundancy detectiondataset described in Section 3.
We randomly dividethe corpus into two sets: 50% for training and 50%for testing.
The training set contains 185 positivetweet-pairs and 416 negative pairs.
The test set con-tains 182 positive pairs and 466 negatives.We evaluate the performance of the SVMmodels using the following feature combina-tions: LEX+BOW, LEX+WBOW, SYNT+BOW,SYNT+WBOW, FOR+BOW, FOR+WBOW.
We com-pare to the system baselines BOW and WBOW.
3The performance of the different models is com-puted using the Area Under the ROC curve (AROC)applied to the classification score returned by theSVM.
The ROC curve allows us to study the be-havior of the classifier in detail, and also provides apowerful way to compare among systems when thedataset is unbalanced (as in our case).To determine the statistical significance of the dif-ference in the performance of the systems we ana-lyzed, we use the model described in (Yeh, 2000) asimplemented in (Pado?, 2006).We pre-process the dataset with the followingtools: the Charniak Parser (Charniak, 2000) forparsing sentences, the WordNet similarity pack-age (Pedersen et al, 2004) for computing WBOWand for linking the two tweets in a pair, and SVM-light (Joachims, 1999), extended with the syntac-tic first-order rule kernels described in (Moschittiand Zanzotto, 2007) for creating the SYNT and theFOR feature spaces.
We used the Charniak syntacticparser without any specific adaptation to the Twitterlanguage.Model AROCBOW 0.592WBOW 0.578LEX + BOW 0.725 ?LEX + WBOW 0.728 ?SYNT + BOW 0.736 ?SYNT + WBOW 0.737 ?FOR + BOW 0.739 ?FOR + WBOW 0.747 ?
?Table 2: Experimental results of the different systems.
?indicates statistical significance (p < 0.01) with respectto the two baseline methods BOW and WBOW.
?
indicatesstatistical significance (p < 0.1) with respect to FOR +BOW5.2 Experimental ResultsTable 2 reports the results of the experiment.
Thefirst and most important result is that models usingcontent features (LEX, SYNT, and FOR) along withsimilarity features (BOW and WBOW) outperform thetwo baseline models using only similarity featureswith statistical significance, up to more than 15%AROC points.At first glance, WordNet similarities are not use-ful: the performance of the WBOW model is in-deed comparable and statistically insignificant withrespect to the pure token based model BOW.
Thisseems to be intuitive as the language of the tweetscan be far from proper English, i.e.
it may containmany out-of-dictionary words that are not presentin WordNet, thus impairing the similarity measureused by WBOW.This trend is also confirmed in the case of content-based systems like LEX and SYNT.
Using BOWor WBOW in combination with these features hasthe same effect on the final performance.
Only theFOR features are positively affected by the WordNet-based distance.
This may be explained by the factthat in the FOR+WBOW system, the WordNet sim-ilarity is also used to link words in the two tweetsof a pair.
This increases the possibility of findingreasonable and useful first-order rules.
In the quali-3Note that other feature combinations would not add value,as BOW and WBOW are interchangeable, and the same standsfor LEX, SYNT and FOR.665tative analysis that follows, we show some examplesthat support this intuition.On the other hand, syntax plays a key role for de-tecting redundancy.
The two syntax based modelsSYNT and FOR outperform the lexical based modelsLEX between 1 and 2 AROC points.
This is sur-prising, since the Charniak parser used in the exper-iments has not been adapted to the Tweet language,and therefore could have produced many interpreta-tion errors, thus impairing the use of syntax.
Thisseems to suggest that if the interpretations of thepart-of speech tags of the unknown words is correct,the syntax of tweets is reasonably similar to the syn-tax of the generic English language.The best performing model is FOR+WBOW: first-order rules successfully emerge in tweets and arepositively exploited by the learning system.
In thenext section we report examples that support this ob-servation.5.3 Qualitative analysisThe experimental results reported in the previ-ous section show that first-order syntactic rules incombination with the WordNet-based bag-of-word(FOR+WBOW) are highly effective in detecting re-dundancy.
In this section, we briefly analyzesome tweet pairs where the differences between thismodel and the BOW and WBOW models are evident.Table 3 reports examples of tweet pairs, alongwith their ranking position in the test set, accord-ing to the SVM score, with respect to different mod-els.
The first column represents the editorial goldstandard (gs) for the tweet pairs we considered: ei-ther redundant (R) or non-redundant (N).
Since wefeed the classifiers with ?redundant?
as the positiveclass 4, a classifier is better than another if it ranksredundant tweet pairs (R) higher than non-redundantones (N).
The second, the third, and the fourthcolumns represent the rank given by WBOW+FOR,WBOW, and BOW respectively.
The fifth column isthe tweet-pair identifier in our dataset (id).
The lasttwo columns are the two tweets in each pair.The table reports interesting examples where re-dundant pairs have very little lexical similarity whilethe non-redundant pairs have a high lexical similar-4This is just a convention.
Results would be the same bytaking non-redundant pairs as the positive class.ity.
These are all examples where BOW and WBOWshould typically fail, while FOR+WBOW could cap-ture important syntactic first-order rules to overcomethe limitations of the pure similarity-based models.As a first example, both BOW and WBOW fail toassign a high rank (i.e.
low rank number) to theredundant pair o165: in fact, ?died?
does not lexi-cally match ?rip?, nor are these two words related inWordNet.
In contrast, FOR+WBOW assigns a highrank to this pair, since it may be able to apply therule <X died, rip X> that was most probably ac-quired from examples in the training set (the hoaxof somebody?s death is pervasive in Twitter, and itis therefore likely to fire the abovementioned rule inour dataset if enough examples are available).The third and the fourth pairs (o130 and o21)show some commonalities 5 .
According to theWordNet similarity measure we used, ?recognize?and ?snub?
are highly related as well as ?forget?
and?snub?.
Hence, the two tokens are linked as similar.For o130, the triggering syntactic rule is <(S (NP X)(VP Y),(VP (V Y) (NP X)> where X and Y are vari-ables.
For o21, the rule is: <(VP (V X) (NP Y),(VP(V X) (NP Y)>.For the non-redundant pairs (N) at the bottom ofthe table, the first-order rules are less intuitive.
Yet,it is clear why these pairs have high lexical simi-larity (and therefore are ranked high by BOW andWBOW): The two tweets in the pair oe387 share?volcanic?, ?ash?, and the hashtag ?#ashtag?.
Tweetsin oe64 share ?Icelandic?
and ?eruption?
but they aredescribing different facts.
Tweets in the pair oe43are similar since they are sharing the three hashtags?#bpoil?, ?#bp?, and ?#oilspill?.
This example showsthat hashtags alone are not very indicative and usefulfor detecting redundancy in Twitter.6 ConclusionsIn this paper we introduced the notion of linguisticredundancy in micro-blogs and the task of tweet re-dundancy detection.
We also presented an editorialstudy showing that redundancy is pervasive in Twit-ter, and that methods for its detection will be key in5In o130, the common topic is ?farrah fawcett?
: ?farrahfawcett not recognized at the Oscars memorial??
and ?snubbedfarrah fawcett.
#oscars?
are used by the annotators to make thedecision.666gs FOR+WBOWWBOWBOWid t1 t2R 11 137 130 o165 ?is that True that johnny depp died????
?Rip johnny depp?
This cannot be True?R 32 246 239 o942 ?sad...jim carrey and jenny mccarthy havecalled it quits...?
?jim carrey & jenny mccarthy broke up!omg!
bummer!
they were the cutest crazycouple ever.
?R 43 165 158 o130 ?farrah fawcett & bea arthur not recognizedat the Oscars memorial?
really??
?i dont understand how they includedmichael jackson in the memorial tribute as anactor but snubbed farrah fawcett.
#oscars?R 101 632 641 o21 ?Oscars forgot farrah fawcett???
?farrah fawcett snubbed at Oscars appearedin a movie with best actor Jeff Bridges... dis-gusting?N 467 161 155 oe387 ?We may die in volcanic ash today.
Chooseyour final pose soon to look cool for futurearchaeologists.
#ashtag?
?# Just heard about the Icelandic volcanicash thing, not really interested but it has thebest hashtag ever, #ashtag !
?N 572 96 92 oe43 ?Many Endangered Turtles Dying OnTexas Gulf Coast http://ow.ly/1FbB8 via@nprnews #bpoil #bp #oilspill?
?Species Most at Risk Because of the OilSpill http://ow.ly/1FcB7 #bpoil #bp #oil-spill?N 614 129 124 oe64 ?http://bit.ly/d8W7Xw #ashtag IN PIC-TURES: Icelandic volcanic eruption?
?So, who?s going to take a crack at pro-nouncing the part of Iceland the eruption wasin?
#ashtag?Table 3: Ranks of some tweet pairs according to the scores of the different classifiers.the future for the development of accurate Twitter-based applications.
In the second part of the pa-per we presented some promising models for redun-dancy detection that show encouraging results whencompared to typical lexical baselines.
Even with theungrammaticalities used in tweets, syntactic featurespaces are effective in modeling redundancy, espe-cially when used in first-order rules.In future work we plan to improve our system byadapting existing linguistic tools and resources toTwitter (e.g.
syntactic parsers).
We also plan to in-vestigate the use of semantic roles and contextual in-formation to improve the models.
For example, thetweets that other users post about the same topic ofthe target-pair may be of some help.
Finally, we areinvestigating the integration of our models into realapplications such a the enrichment of news articleswith related and diverse content from social media.ReferencesLuciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.In Posters Proceedings of the 23rd International Con-ference on Computational Linguistics (Coling 2010),pages 36?44, Beijing, China.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 132?139, Seattle, Wash-ington.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, pages 263?270.Courtney Corley and Rada Mihalcea.
2005.
Measur-ing the semantic similarity of texts.
In Proceedings ofthe ACL Workshop on Empirical Modeling of SemanticEquivalence and Entailment, pages 13?18, Ann Arbor,Michigan.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailment chal-lenge.
In Quionero-Candela et al, editor, LNAI 3944:MLCW 2005, pages 177?190, Milan, Italy.
Springer-Verlag.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Posters Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 241?249, Beijing, China.667Marie-Catherine de Marneffe, Bill MacCartney, TrondGrenager, Daniel Cer, Anna Rafferty, and ChristopherD.
Manning.
2006.
Learning to distinguish validtextual entailments.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Tex-tual Entailment, Venice, Italy.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th International Conference onComputational Linguistics (Coling 2004), pages 350?356, Geneva, Switzerland.Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, andHeung-Yeung Shum.
2010.
An empirical study onlearning to rank of tweets.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (Coling 2010), pages 295?303, Beijing, China.Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 362?370, Boulder, Colorado.Aria Haghighi, Andrew Ng, and Christopher Manning.2005.
Robust textual inference via graph matching.In Proceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 387?394, Vancouver, BritishColumbia, Canada.Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recogniz-ing textual entailment with LCC?s groundhog system.In Bernardo Magnini and Ido Dagan, editors, Proceed-ings of the 2nd PASCAL RTE Challenge, Venice, Italy.Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.2007.
Why we Twitter: understanding microbloggingusage and communities.
In Proceedings of the 9th We-bKDD and 1st SNA-KDD 2007.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxonomy.In Proceedings of the 10th International Conferenceon Research in Computational Linguistics ROCLING,pages 132?139, Tapei, Taiwan.Thorsten Joachims.
1999.
Making large-scale svmlearning practical.
In B. Schlkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods-Support Vector Learning.
MIT Press.Balachander Krishnamurthy, Phillipa Gill, and MartinArlitt.
2008.
A few chirps about twitter.
In Proceed-ings of the first workshop on Online social networks,pages 19?24, Seattle, WA, USA.Haewoon Kwak, Changhyun Lee, Hosung Park, and SueMoon.
2010.
What is twitter, a social network ora news media?
In Proceedings of WWW ?10: Pro-ceedings of the 19th international conference on Worldwide web, pages 591?600, Raleigh, North Carolina,USA.Cindy-Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han.2010.
Pet: a statistical model for popular eventstracking in social communities.
In Proceedings ofthe 16th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 929?938, Washington, DC, USA.Xiaohua Liu, Kuan Li, Bo Han, Ming Zhou, Long Jiang,Zhongyang Xiong, and Changning Huang.
2010.
Se-mantic role labeling for news tweets.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (Coling 2010), pages 698?706,Beijing, China, August.Bill MacCartney, Trond Grenager, Marie-Catherinede Marneffe, Daniel Cer, and Christopher D. Manning.2006.
Learning to recognize features of valid textualentailments.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 41?48, New York City,USA.George A. Miller.
1995.
WordNet: A lexical database forEnglish.
Communications of the ACM, 38(11):39?41,November.Alessandro Moschitti and Fabio Massimo Zanzotto.2007.
Fast and effective kernels for relational learn-ing from texts.
In Proceedings of the InternationalConference of Machine Learning (ICML), Corvallis,Oregon.Eamonn Newman, Nicola Stokes, John Dunnion, andJoe Carthy.
2005.
Textual entailment recognition us-ing a linguistically-motivated decision tree classifier.In Joaquin Quin?onero Candela, Ido Dagan, BernardoMagnini, and Florence d?Alche?
Buc, editors, MLCW,volume 3944 of Lecture Notes in Computer Science,pages 372?384.
Springer.Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Pear-Analytics.
2009.
Twitter study - august 2009.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity - measuring the relat-edness of concepts.
In Demonstration Papers at HLT-NAACL 2004, pages 38?41, Boston, MA.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applicationto twitter.
In Proceedings of Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 181?189, Los Angeles, Cali-fornia.668Ana-Maria Popescu and Marco Pennacchiotti.
2010.
De-tecting controversial events from twitter.
In In Pro-ceedings of the 19th ACM international conference onInformation and knowledge management, pages 1873?1876.Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing microblogs with topic models.
In Pro-ceedings of the International AAAI Conference on We-blogs and Social Media, pages 130?137.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 172?180, Los An-geles, California.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th conference on Computational linguistics,pages 947?953, Morristown, NJ, USA.Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.2009.
Efficient kernels for sentence pair classification.In Conference on Empirical Methods on Natural Lan-guage Processing, pages 91?100, 6-7 August.Fabio Massimo Zanzotto and Alessandro Moschitti.2006.
Automatic learning of textual entailments withcross-pair similarities.
In Proceedings of the 21st Col-ing and 44th ACL, pages 401?408, Sydney, Australia,July.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learning ap-proach to textual entailment recognition.
Natural Lan-guage Engineering, 15-04:551?582.Q.
Zhao, P. Mitra, and B. Chen.
2007.
Temporal and in-formation flow based event detection from social textstreams.
In Proceedings of the 22nd national confer-ence on Artificial intelligence, pages 1501?1506, Van-couver, British Columbia, Canada.669
