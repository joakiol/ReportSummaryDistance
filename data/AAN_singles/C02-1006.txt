NLP and IR Approaches to Monolingual andMultilingual Link DetectionYing-Ju ChenDepartment of Computer Science andInformation EngineeringNational Taiwan UniversityTaipei, TAIWAN, 106yjchen@nlg2.csie.ntu.edu.twHsin-Hsi ChenDepartment of Computer Science andInformation EngineeringNational Taiwan UniversityTaipei, TAIWAN, 106hh_chen@csie.ntu.edu.twAbstractThis paper considers several importantissues for monolingual and multilingual linkdetection.
The experimental results showthat nouns, verbs, adjectives and compoundnouns are useful to represent news stories;story expansion is helpful; topicsegmentation has a little effect; and atranslation model is needed to capture thedifferences between languages.IntroductionIn the digital era, how to assist users to deal withdata explosion problem becomes emergent.News stories on the Internet contain a largeamount of real-time and new information.Several attempts were made to extractinformation from news stories, e.g.,multi-lingual multi-document summarization(Chen and Huang, 1999; Chen and Lin, 2000),topic detection and tracking (abbreviated asTDT hereafter, http://www.nist.gov/TDT), andso on.
Of these, TDT, which is a long-termproject, proposed many diverse applications, e.g.,story segmentation (Greiff et al, 2000), topictracking (Levow et al, 2000; Leek et al, 2002),topic detection (Chen and Ku, 2002) and linkdetection (Allan et al, 2000).This paper will focus on the link detectionapplication.
The TDT link detection aims todetermine whether two stories discuss the sametopic.
Each story could discuss one or more thanone topic, and the sizes of two stories comparedmay not be so comparable.
For example, onestory may contain 100 sentences and the otherone may contain only 5 sentences.
In addition,the stories may be represented in differentlanguages.
These are the main challenges of thistask.
In this paper, we will discuss andcontribute on several issues:1.
How to represent a news story?2.
How to measure the similarity of newsstories?3.
How to expand a story vector usinghistoric information?4.
How to identify the subtopicsembedded in a news story?5.
How to deal with news stories indifferent languages?The multilingual issue was first introduced in1999 (TDT-3), and the source languages aremainly English and Mandarin.
Dictionary-basedtranslation strategy is applied broadly.
Inaddition, some strategies were proposed toimprove the translation accuracy.
Leek et al,(2002) proposed probabilistic term translationand co-occurrence statistics strategies.
Thealgorithm of co-occurrence statistics tended tofavour those translations consistent with the restof the document.
Hui et al, (2001) proposed anenhanced translation approach for improving thetranslation by using a parallel corpus as anadditional resource.
Levow et al, (2000)proposed a corpus-based translation preference.English translation candidates were sorted in anorder that reflected the dominant usage in thecollection.
Most of these methods need extraresources, e.g., a parallel corpus.
In this paper,we will try to resolve multilingual issues withthe lack of extra information.Topic segmentation is a technique extensivelyutilized in information retrieval and automaticdocument summarization (Hearst et al, 1993;Nakao, 2001).
The effects were shown to bevalid.
This paper will introduce topicTable 1.
Performance of Link Detection under Different Feature Selection Strategies (I)Similarity Threshold0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11 0.12All 1.6234 1.274 1.0275 0.8440 0.7245 0.6463 0.5911 0.5528 0.5268N 0.7088 0.5547 0.4553 0.4012 0.3815 0.3743 0.3775 0.3834 0.3883N&V 0.8152 0.6028 0.4899 0.4254 0.3922 0.3803 0.3780 0.3870 0.4002N&J 0.6126 0.4671 0.3918 0.3624 0.3485 0.3437 0.3481 0.3628 0.3780N&V&J 0.6955 0.5121 0.4200 0.3720 0.3498 0.3474 0.3480 0.3617 0.3795segmentation in link detection.
Severalexperiments will be conducted to investigate itseffects.1 EnvironmentLDC provides corpora to support the differentapplications of TDT (Fiscus et al, 2002).
Thecorpora used in this paper are the TDT2 corpusand the augmented version of TDT3 corpus.
Weused the TDT2 corpus as training data, andevaluated the performance with the augmentedversion of TDT3 corpus.
Both corpora are textand transcribed speech news from a number ofsources in English and in Mandarin.
The TDT2corpus spans January 1, 1998 to June 30, 1998.There are 200 topics for English, and 20 topicsfor Mandarin.
The TDT3 corpus spans October1, 1998 to December 31, 1998.
There are 120topics for both English and Mandarin.
In theaugmented version of TDT3 corpus, additionalnews data is added.
These data spans from July1, 1998 to December 31, 1998.There are 34,908 story pairs (Fiscus et al,2002) for link detection in both monolingual andmultilingual tasks.
Of these, the numbers oftarget and non-target pairs are 4,908 and 30,000,respectively.
In the monolingual task, Mandarinnews stories are translated into English onesthrough a machine translation system.
In themultilingual task, Mandarin news stories arerepresented in the original Mandarin characters.In both tasks, all the audio news stories aretranscribed through an automatic speechrecognition (ASR) system.We adopt the evaluation methodology definedin TDT to evaluate our system performance.
Thecost function for the task defined by TDT isshown as follows.
The better the link detectionis, the lower the normalized detection cost is.
Inthe next sections, all experimental results areevaluated by this metric.CDet=CMiss?PMiss?Ptarget+CFA?PFA?Pnon-target,where CMiss and CFA are the costs of Miss andFalse Alarm errors, and PMiss and PFA are theprobabilities of a Miss and a False Alarm, andPtarget and Pnon-target are a priori probabilities of astory pair chosen at random discuss the sametopic and discuss different topics.
The cost ofdetection is normalized as follows:(CDet)Norm=CDet/min(CMiss?Ptarget,CFA?Pnon-target)2 Basic Link Detection System2.1 Basic ArchitectureThe basic algorithm is shown as follows.
Eachstory in a given pair is represented as a vectorwith tf*idf weights, where tf and idf denote termfrequency and inverse document frequency astraditional IR defines.
Then, the cosine functionis used to measure the similarity of two stories.Finally, a predefined threshold, THdecision, isemployed to decide whether two stories discussthe same topic or not.
That is, two stories are onthe same topic if their similarity is larger thanthe predefined threshold.
The idf values and thethresholds are trained from TDT2 corpus.
EachEnglish story is tagged using ?Apple Pie Parser?
(version 5.9).
In addition, English words arestemmed by Porter?s algorithm, and functionwords are removed directly.2.2 Story RepresentationThe noun terms denote interesting entities suchas people names, location names, andorganization names, and so on.
The verb termsdenote the specific events.
In general, noun andverb terms are important features to identify thetopic the story discusses.
We conducted severalexperiments to investigate the performance ofdifferent story representations.
Table 1 showsthe performance of different story representationschemes under different similarity thresholds.The row denotes which lexical items are used.
"All" means any kind of lexical items isTable 2.
Performance of Link Detection under Different Feature Selection Schemes (II)Similarity Threshold0.04 0.05 0.06 0.07 0.08 0.09 0.1N&CNs 0.3825 0.3564 0.3612 0.3754 0.4026 0.4377 0.4700N&V&CNs 0.4090 0.3572 0.3520 0.3658 0.3917 0.4279 0.4617N&J&CNs 0.3372 0.3361 0.3353 0.3568 0.3845 0.4163 0.4471N&V&J&CNs 0.3451 0.3398 0.3283 0.3446 0.3751 0.4055 0.4360Table 3.
Performance of Link Detection with Story Expansion StrategyTHdecision 0.06THexpansion 0.06 0.07 0.08 0.1 0.11 0.13N&J&CNs 0.3713 0.3580 0.3392 0.3260 0.3230 0.3278N&V&J&CNs 0.3342 0.3363 0.3155 0.3061 0.3057 0.3073N&J&CNs (half) 0.2691 0.2638 0.2654 0.2785 None NoneN&V&J&CNs (half) 0.2797 0.2751 0.2826 0.3259 None Noneconsidered.
N, V and J denote nouns, verbs, andadjectives, respectively.The experimental results show that the bestperformance is 0.3437 when only noun andadjective terms are used to represent stories, andthe similarity threshold is 0.09.
Examining whynouns and adjectives terms carry moreinformation than verbs, we found that there areimportant adjectives like ?Asian?, ?financial?,etc., and some important people names aremis-tagged as adjectives.
And the matched verbterms, such as ?keep?, ?lower?, etc., carry lessinformation and the similarity would beoverestimated.In the next experiments, we investigate theeffects of compound nouns (abbreviated as CNs)in the story representation.
The results areshown in Table 2.
All performances areimproved when using CNs.
The best one is0.3283 when nouns, verbs, adjectives and CNsare adopted and the similarity threshold is 0.06.The performance is better than the result (i.e.,0.3437) in Table 1.
We found that the thresholdfor the best performance decreased in the CNsexperiments.
This is because matching CNs intwo different news stories is more difficult thanmatching single terms, but the effect is verystrong when matching is successful, such as?Red Cross?, ?Security Council?, etc.2.3 Story ExpansionThe length of stories may be diverse.
With themethod proposed in Section 2.1, there may bevery few features remaining for short stories.And different reporters would use differentwords to describe the same event.
In suchsituations, the similarity of two stories may betoo small to tell if they belong to the same topic.To deal with the problems, we try to introduce astory expansion technique in the basic algorithm.The method we employed is quite different fromthat proposed by Allan (2000), which regardedlocal context analysis (LCA) as a smoothingtechnique.
Each story is treated as a ?query?
andis expanded using LCA.Our method is described below.
When thesimilarity of two stories is higher than apredefined threshold THexpansion, which is alwayslarger than or equal to THdecision, the two storiesare related to some topic in more confidence.Thus, their relationship is kept in a database andwill be used for story expansion later.
Forexample, if the similarity of a story pair (A, B) isvery high, we will expand the vector of A with Bwhen a new pair (A, C) is considered.
Table 3shows our experiments on TDT2 data.
Weconducted different lexical combinations anddifferent weighting schemes for the expandedterms.Story expansion with the non-relevant termswould reduce the performance of a linkdetection system.
That is, it may introduce somenoise into the story and make the detection moredifficult.
We assigned the expanded terms twodifferent weights.
One is using the originalweights, and the other one is using half of theoriginal weights, which is denoted as ?half?
inTable 3.The results show that story expansionoutperforms the basic method, and assigningexpanded terms half weights would be better.The best performance when applying storyexpansion achieves 0.2638.
The total miss ratewas decreased to third fourths of the originalamount.
Sum up, story expansion is a goodstrategy to improve the link detection task.3 Topic SegmentationThere is no presumption that each storydiscusses only one topic.
Thus, we try tosegment stories into small passages according tothe discussing topics and compute passagesimilarity instead of document similarity.
Thebasic idea is: the significance of some usefulterms may be reduced in a long story becausesimilarity measure on a large number of termswill decrease the effects of those importantterms.
Computing similarities between smallpassages could let some terms be moresignificant.The first method we adopted is text tilingapproach (Hearst, 1993).
TextTiling subdividestext into multi-paragraph units that representpassages or subtopics.
The approach usesquantitative lexical analyses to segment thedocuments.
After through TextTiling algorithm,a file will be broken into tiles.
Suppose one storyis broken into three tiles and the other one isbroken into four tiles.
There are twelve (i.e., 3*4)similarities of these two stories.
We conductedthree different strategies to investigate the effectof topic segmentation.
Strategy (I) is computingthe similarity using the most similar passage pair.Strategy (II) is computing the similarity usingpassage-averaged similarity.
Strategy (III) iscomputing the similarity using a two-statedecision (Chen, 2002).
But the result is not sogood as we expected.
Up to now, the bestperformance is almost the same as the originalmethod without text tiling.Next, we applied another topic segmentationalgorithm developed by Utiyama et al (2001).The results show that this segmentationalgorithm is better than TextTiling.
But theimprovement is still not obvious.
Table 4 showsthe experimental results for topic segmentation.For strategy (III), the first threshold is 0.06,which is also the best threshold for the basicmethod, and the second threshold varies from0.04 to 0.07 for segmentation.
After applyingtopic segmentation, topic words would becentred on small passages.
The amount of newsstories discussing more than one topic is few inthe test data and the overall performancedepends on the segmentation algorithm.
Wemake an index file similar to the original TDTindex file.
In this file, at least one story of eachpair discusses multi-topics.
We conducteddifferent strategies to investigate the effect oftopic segmentation.
The experimental resultsdemonstrate that topic segmentation is useful inthis task (Chen, 2002).4 Multilingual Link DetectionAlgorithmThe multilingual link detection should tell if twostories in different languages are discussing thesame topic.
In this paper, the stories are inEnglish and in Chinese.
Comparing to Englishstories, there is no apparent word boundary inChinese stories.
We have to segment theChinese sentences into meaningful lexical units.We employed our own Chinese segmentationand tagging system to pre-process Chinesesentences.
Similar to monolingual link detection,each story in a pair is represented as a vector andthe cosine similarity is used to decide if twostories discuss the same topic.In multilingual link detection, we have todeal with terms used in different languages.Consider the following three cases.
E and Cdenote an English story and a Chinese story,respectively.
(E, E) denotes an English pair; (C,C) denotes a Chinese pair; and (C, E) or (E, C)denotes a multilingual pair.
(a) (E, E): no translation is required.
(b) (C, E) or (E, C): C is translated to E?.The new E?
could be an English vector or thevector is mixed in two languages if the originalTable 4.
Performances of Topic Segmentation in Link Detection0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10Strategy (I) None None None 0.4338 0.3891 0.3766 0.3857 0.4063Strategy (II) 0.3581 0.3490 0.3983 0.4629 0.5226 None None NoneStrategy (III) None 0.3309 0.3280 0.3282 0.3288 None None NoneChinese terms are included in the new Englishvector.
(c) (C, C): No translation is required; orboth stories are translated into English and useEnglish vectors; or these new English terms areadded into the original Chinese vectors.The reason that we included the originalChinese terms in the new English vector is thatwe could not find the corresponding Englishtranslation candidates for some Chinese words.Including the Chinese terms could not loseinformation.We employed a simple approach to translate aChinese story into an English one.
AChinese-English dictionary is consulted.
Thereare 374,595 Chinese-English pairs in thedictionary.
For each English term, there are 2.49Chinese translations.
For each Chinese term,there are 1.87 English translations.
In thisdictionary, English translations are lessambiguous.
Therefore, we translated Chinesestories into English ones.
If a Chinese wordcorresponds to more than one English word,these English words are all selected.
That is, wedid not disambiguate the meaning of a Chineseword.
To avoid the noise introduced by manyEnglish translations, each translation term isassigned a lower weight.
The weight isdetermined as follows.
We divided the weight ofa Chinese term by the total number translationequivalents.w(d, te) = w(d, tc) / N,where w(d, tc) is the weight of a Chinese term instory d, w(d, te) is the weight of its Englishtranslation in story d, and N is the number ofEnglish translation candidates for the Chineseterm.Table 5 shows the performances ofmultilingual link detection.
We conductedthree experiments using different storyrepresentation schemes for Chinese stories.
?E?denotes Chinese stories are translated intoEnglish ones.
?C?
denotes Chinese stories arecompared directly without translation, butChinese stories are translated into English onesin multilingual pairs.
?EC?
denotes Chinesestories are represented in Chinese terms andtheir corresponding English translationcandidates.
The threshold for English story pairsis set to 0.12.
The threshold for the other pairsvaries from 0.1 to 0.5.
The results reveal that?E?
is better than ?C?
and ?EC?.Table 5.
Performance of Multilingual Link Detectionwith Different Translation SchemesSimilarity Threshold0.1 0.2 0.3 0.4 0.5E 0.9925 0.6760 0.6359 0.6558 0.6864C 1.0971 0.7204 0.6546 0.6701 0.6969EC 1.1525 0.7712 0.7146 0.7410 0.7694Comparing stories in translated English termscould bring some advantages.
Some Chineseterms which denote the same concept but indifferent forms could be matched through theirEnglish translations, for example, "??"
and "??"
(kill), as well as "??"
and "??
"(behaviour).The effect of English translations for Chinesestories is similar to the effect of thesaurus.
Weemployed the CILIN (Mei et al, 1982) inmultilingual link detection.
We use the smallcategory information and synonyms to expandthe features we selected to represent a newsstory.
The experimental results are shown inTable 6.Table 6.
Performance of Multilingual Link Detectionwith Different Thesaurus Expansion SchemesSimilarity Threshold0.1 0.2 0.3 0.4 0.5SmallCategory 1.6576 0.9196 0.6656 0.6500 0.6832Synonyms 0.9486 0.6260 0.6342 0.6734 0.7059We found that the performances of ?E?translation and synonyms expansion schemes arevery close.
In our consideration, a good bilingualdictionary can be regarded as a thesaurus.The results of multilingual link detection areapparently worse than those of monolingual linkdetection.
When the threshold is 0.2, the bestperformance is 0.6260 and the miss rate is0.4547.
The value of miss rate is very high.
Toimprove the performance, we have to reduce themiss rate.
We found the similarity of two storiesin different languages is very low in comparisonwith the similarity of two stories in the samelanguage.
It is unfair to set the same thresholdfor different languages, thus we introduced atwo-threshold method to resolve this problem.The performance of the two-threshold methodfor synonyms expansion (denotes as "Syn") isshown in Table 7.
"Chinese" means theTable 8.
Performances of Multilingual Link Detection under Different Feature Selection SchemeSimilarity ThresholdChinese 0.2Multi 0.03 0.04 0.05 0.06N 0.4707 0.4421 0.4319 0.4389N&J 0.4600 0.4162 0.4082 0.4126N&V 0.5162 0.4459 0.4233 0.4299N&V&J 0.5116 0.4248 0.4042 0.4093N&CNs 0.4685 0.4399 0.4297 0.4366N&J&CNs 0.4570 0.4193 0.4106 0.4199N&V&CNs 0.5010 0.4386 0.4162 0.4219N&V&J&CNs 0.4886 0.4152 0.3931 0.3978threshold for Chinese pairs and "Multi" meansthe threshold for multilingual pairs.Table 7.
Performance of Multilingual Link Detectionwith a Two-threshold MethodSimilarity ThresholdChinese 0.2Multi 0.01 0.02 0.03 0.04 0.05 0.06Syn 1.2929 0.7804 0.5818 0.5166 0.5033 0.5124The result reveals that there is a greatimprovement when applying the two-thresholdmethod.
The threshold for Chinese story pairs is0.2, the threshold for English story pairs is 0.12,and threshold for multilingual story pairs is 0.05.The similarity distributions for story pairs indifferent languages vary.
As monolingual linkdetection, we did experiments about thecombinations of different lexical terms.
Theresults of these different combinations areshown in Table 8.
It shows that therepresentation of the best performance in themultilingual task is different from that in themonolingual task.
CNs bring positive influence.But using nouns, verbs and adjectives torepresent a story is better than using nouns andadjectives only in multilingual link detection.Words in Chinese are seldom tagged as adjective.They are tagged as verbs in Chinese, but aretagged as adjectives in English ("??"
vs.?safe?
).We also adopted story expansion mentionedin Section 2.3 before computing the similarity.Note that only stories in the same language areused to expand each other.
In Table 9, ?One?denotes the weights of expanded terms are thesame as the original ones, and ?Half?
denotesthe weights of the expanded terms are only halfof the original ones.
The results reveal thatexpanded terms with half weights are better thanwith original ones.
Giving expanded terms halfweights could reduce the effect of noise.
Nouns,verbs, adjectives and compound nouns are usedto represent stories in Table 9, and the thresholdsare set as the best ones in the previousexperiments.
The expansion threshold forChinese pairs varies from 0.2 to 0.3.Table 9.
Performances of Multilingual LinkDetection with All the Best StrategiesTHexpansion 0.2 0.25 0.3One 0.3852 0.3873 0.3916Half 0.3721 0.3718 0.37345 Results of the Evaluation on TDT3corpusWe applied the best strategies and the trainedthresholds in above experiments for bothmonolingual and multilingual link detectiontasks to TDT3 corpus.
The results of ourmethods and of the other sites participating theTDT 2001 evaluation are shown in Table 10.
Inthis evaluation, both published and unpublishedtopics are considered.For monolingual task, nouns, adjectives andCNs are used to represent story vectors.
And thethresholds for decision and expansion are 0.06and 0.07, respectively.
For multilingual task,nouns, verbs, adjectives and CNs are used torepresent story vectors.
The thresholds forEnglish pairs are set the same as those in themonolingual task, and for Chinese pairs, they are0.2 and 0.25, respectively.
The decisionthreshold for multilingual pairs is 0.05.Table 10.
Link Detection Evaluation ResultsCMU CUHK NTU UIowaMonolingual 0.2734 None 0.2963 0.3375Multilingual None 0.4143 0.3269 NoneIn the multilingual task, our result (NTU) isbetter than The Chinese University of HongKong (CUHK).
And the multilingual result isclose to the monolingual result.
This is asignificant improvement.Conclusion and Future WorkSeveral issues for link detection are consideredin this paper.
For both monolingual andmultilingual tasks, the best features to representstories are nouns, verbs, adjectives, andcompound nouns.
The story expansion usinghistoric information is helpful.
Story pairs indifferent languages have different similaritydistributions.
Using thresholds to model thedifferences is shown to be usable.Topic segmentation is an interesting issue.We expected it would bring some benefits, butthe experiments for TDT testing environmentshowed that this factor did not gain as much aswe expected.
Few multi-topic story pairs andsegmentation accuracy induced this result.
Wemade an index file containing multi-topic storypairs and did experiments to investigate.
Theexperimental results support our thought.We examined the similarities of story pairsand tried to figure out why the miss rate was notreduced.
There are 919 pairs of 4,908 ones aremistaken.
The mean similarity of miss pairs ismuch smaller than the decision threshold.
Thatmeans there are no similar words between twostories even they are discussing the same topic.None or few match words result that thesimilarity does not exceed the threshold.
That isthe problem that we have to overcome.We also find that the people names may bespelled in different ways in different newsagencies.
For example, the name of a balloonistis spelled as ?Faucett?
in VOA news stories, butis spelled as ?Fossett?
in the other news sources.And for machine translated news stories, thepeople names would not be translated into theircorresponding English names.
Therefore, wecould not find the same people name in twostories.
In substance, people names areimportant features to discriminate from topics.This is another challenge issue to overcome.ReferencesAllan J., Lavrenko V., Frey D., and Khandelwal V.(2000) UMass at TDT 2000.
In Proceedings ofTopic Detection and Tracking Workshop.Chen H.H.
and Huang S.J.
(1999).
A SummarizationSystem for Chinese News from Multiple Sources.In Proceedings of the 4th International Workshopon Information Retrieval with Asian Languages,Taiwan, pp.
1-7.Chen H.H.
and Lin C.J.
(2000) A Multilingual NewsSummarizer.
In Proceedings of 18th InternationalConference on Computational Linguistics,University of Saarlandes, pp.
159-165.Chen H.H.
and Ku L.W (2002) An NLP & IRApproach to Topic Detection.
In "Topic Detectionand Tracking: Event-based InformationOrganization", Kluwer Academic Publishers, pp.243-261.Chen Y.J (2002) Monolingual and Multilingual LinkDetection.
Master Thesis.
Department ofComputer Science and Information Engineering,National Taiwan University, 2002.Fiscus J.G., Doddington G.R.
(2002) Topic Detectionand Tracking Evaluation Overview.
In "TopicDetection and Tracking: Event-based InformationOrganization", Kluwer Academic Publishers, pp.17-32.Greiff W., Morgan A., Fish R., Richards M., KunduA.
(2000) MITRE TDT-2000 SegmentationSystem.
In Proceedings of TDT2000 Workshop.Hearst M.A.
and Plaunt C. (1993) SubtopicStructuring for Full-Length Document Access.
InProceedings of the 16th Annual InternationalACM SIGIR Conference.Hui K., Lam W., and Meng H.M. (2001) Discoveryof Unknown Events From Multi-lingual News.
InProceedings of the International Conference onComputer Processing of Oriental Languages.Leek T., Schuartz R., Sista S. (2002) ProbabilisticApproaches To Topic Detection and Tracking.
In"Topic Detection and Tracking: Event-basedInformation Organization", Kluwer AcademicPublishers, pp.
67-84.Levow G.A.
and Oard D.W. (2000) TranslingualTopic Detection: Applying Lessons from the MEIProject.
In the Proceedings of Topic Detectionand Tracking Workshop (TDT-2000).Mei, J. et al (1982) tong2yi4ci2ci2lin2 (CILIN),Shanghai Dictionary Press.Nakao Y.
(2000) An Algorithm for One-pageSummarization of a Long Text Based onThematic Hierarchy Detection.
In Proceeding ofACL 2000, pp.
302-309.Utiyama M. and Isahara H. (2001) A statisticalModel for Domain-Independent TextSegmentation.
ACL/EACL-2001, pp.
491-498.
