Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775?785,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsRobust Entity Clustering via Phylogenetic InferenceNicholas Andrews and Jason Eisner and Mark DredzeDepartment of Computer Science and Human Language Technology Center of ExcellenceJohns Hopkins University3400 N. Charles St., Baltimore, MD 21218 USA{noa,eisner,mdredze}@jhu.eduAbstractEntity clustering must determine when twonamed-entity mentions refer to the sameentity.
Typical approaches use a pipeline ar-chitecture that clusters the mentions usingfixed or learned measures of name and con-text similarity.
In this paper, we propose amodel for cross-document coreference res-olution that achieves robustness by learn-ing similarity from unlabeled data.
Thegenerative process assumes that each entitymention arises from copying and option-ally mutating an earlier name from a sim-ilar context.
Clustering the mentions intoentities depends on recovering this copyingtree jointly with estimating models of themutation process and parent selection pro-cess.
We present a block Gibbs sampler forposterior inference and an empirical evalu-ation on several datasets.1 IntroductionVariation poses a serious challenge for determin-ing who or what a name refers to.
For instance,Wikipedia contains more than 100 variations of thename Barack Obama as redirects to the U.S. Presi-dent article, including:President Obama Barack H. Obama, Jr.Barak Obamba Barry SoetoroTo relate different names, one solution is to usespecifically tailored measures of name similaritysuch as Jaro-Winkler similarity (Winkler, 1999; Co-hen et al, 2003).
This approach is brittle, however,and fails to adapt to the test data.
Another option isto train a model like stochastic edit distance fromknown pairs of similar names (Ristad and Yian-ilos, 1998; Green et al, 2012), but this requiressupervised data in the test domain.Even the best model of name similarity is notenough by itself, since two names that are similar?even identical?do not necessarily corefer.
Docu-ment context is needed to determine whether theymay be talking about two different people.In this paper, we propose a method for jointly(1) learning similarity between names and (2) clus-tering name mentions into entities, the two majorcomponents of cross-document coreference reso-lution systems (Baron and Freedman, 2008; Fininet al, 2009; Rao et al, 2010; Singh et al, 2011;Lee et al, 2012; Green et al, 2012).
Our modelis an evolutionary generative process based on thename variation model of Andrews et al (2012),which stipulates that names are often copied frompreviously generated names, perhaps with mutation(spelling edits).
This can deduce that rather thanbeing names for different entities, Barak Obambaand Barock obama more likely arose from the fre-quent name Barack Obama as a common ancestor,which accounts for most of their letters.
This canalso relate seemingly dissimilar names via multiplesteps in the generative process:Taylor Swift?
T-Swift?
T-SwizzleOur model learns without supervision that these allrefer to the the same entity.
Such creative spellingsare especially common on Twitter and other so-cial media; we give more examples of coreferentslearned by our model in Section 8.4.Our primary contributions are improvements onAndrews et al (2012) for the entity clustering task.Their inference procedure only clustered types (dis-tinct names) rather than tokens (mentions in con-text), and relied on expensive matrix inversions forlearning.
Our novel approach features:?4.1 A topical model of which entities from previ-ously written text an author tends to mentionfrom previously written text.
?4.2 A name mutation model that is sensitive tofeatures of the input and output characters andtakes a reader?s comprehension into account.
?5 A scalable Markov chain Monte Carlo sam-pler used in training and inference.775?7 A minimum Bayes risk decoding procedureto pick an output clustering.
The procedure isapplicable to any model capable of producinga posterior over coreference decisions.We evaluate our approach by comparing to sev-eral baselines on datasets from three different gen-res: Twitter, newswire, and blogs.2 Overview and Related WorkCross-document coreference resolution (CDCR)was first introduced by Bagga and Baldwin (1998b).Most approaches since then are based on the intu-itions that coreferent names tend to have ?similar?spellings and tend to appear in ?similar?
contexts.The distinguishing feature of our system is that bothnotions of similarity are learned together withoutsupervision.We adopt a ?phylogenetic?
generative model ofcoreference.
The basic insight is that coreference iscreated when an author thinks of an entity that wasmentioned earlier in a similar context, and men-tions it again in a similar way.
The author mayalter the name mention string when copying it, butboth names refer to the same entity.
Either namemay later be copied further, leading to an evolution-ary tree of mentions?a phylogeny.
Phylogeneticmodels are new to information extraction.
In com-putational historical linguistics, Bouchard-C?ot?e etal.
(2013) have also modeled the mutation of stringsalong the edges of a phylogeny; but for them thephylogeny is observed and most mentions are not,while we observe the mentions only.To apply our model to the CDCR task, we ob-serve that the probability that two name mentionsare coreferent is the probability that they arose froma common ancestor in the phylogeny.
So we designa Monte Carlo sampler to reconstruct likely phylo-genies.
A phylogeny must explain every observedname.
While our model is capable of generatingeach name independently, a phylogeny will gener-ally achieve higher probability if it explains similarnames as being similar by mutation (rather thanby coincidence).
Thus, our sampled phylogeniestend to make similar names coreferent?especiallylong or unusual names that would be expensive togenerate repeatedly, and especially in contexts thatare topically similar and therefore have a higherprior probability of coreference.For learning, we iteratively adjust our model?sparameters to better explain our samples.
That is,we do unsupervised training via Monte Carlo EM.What is learned?
An important component ofa CDCR system is its model of name similarity(Winkler, 1999; Porter and Winkler, 1997), whichis often fixed up front.
This role is played in our sys-tem by the name mutation model, which we take tobe a variant of stochastic edit distance (Ristad andYianilos, 1996).
Rather than fixing its parametersbefore we begin CDCR, we learn them (withoutsupervision) as part of CDCR, by training fromsamples of reconstructed phylogenies.Name similarity is also an important componentof within-document coreference resolution, and ef-forts in that area bear resemblance to our approach.Haghighi and Klein (2010) describe an ?entity-centered?
model where a distance-dependent Chi-nese restaurant process is used to pick previouscoreferent mentions within a document.
Similarly,Durrett and Klein (2013) learn a mention similaritymodel based on labeled data.
Our cross-documentsetting has no observed mention ordering and noobserved entities: we must sum over all possibili-ties, a challenging inference problem.The second major component of CDCR iscontext-based disambiguation of similar or iden-tical names that refer to the same entity.
LikeKozareva and Ravi (2011) and Green et al (2012)we use topics as the contexts, but learn mentiontopics jointly with other model parameters.3 Generative Model of CoreferenceLet x = (x1, .
.
.
, xN) denote an ordered sequenceof distinct named-entity mentions in documentsd = (d1, .
.
.
, dD).
We assume that each doc-ument has a (single) known language, and thatits mentions and their types have been identifiedby a named-entity recognizer.
We use the object-oriented notation x.v for attribute v of mention x.Our model generates an ordered sequence x al-though we do not observe its order.
Thus each men-tion x has latent position x.i (e.g., x729.i = 729).The entire corpus, including these entities, is gen-erated according to standard topic model assump-tions; we first generate a topic distribution for adocument, then sample topics and words for thedocument (Blei et al, 2003).
However, any topicmay generate an entity type, e.g.
PERSON, which isthen replaced by a specific name: when PERSON isgenerated, the model chooses a previous mentionof any person and copies it, perhaps mutating itsname.1Alternatively, the model may manufacture1We make the closed-world assumption that the author is776a name for a new person, though the name itselfmay not be new.If all previous mentions were equally likely, thiswould be a Chinese Restaurant Process (CRP) inwhich frequently mentioned entities are more likelyto be mentioned again (?the rich get richer?).
Werefine that idea by saying that the current topic, lan-guage, and document influence the choice of whichprevious mention to copy, similar to the distance-dependent CRP (Blei and Frazier, 2011).2This willhelp distinguish multiple John Smith entities if theytend to appear in different contexts.Formally, each mention x is derived from a par-ent mention x.p where x.p.i < x.i (the parentcame first), x.e = x.p.e (same entity) and x.n isa copy or mutation of x.p.n.
In the special casewhere x is a first mention of x.e, x.p is the specialsymbol ?, x.e is a newly allocated entity of someappropriate type, and the name x.n is generatedfrom scratch.Our goal is to reconstruct mappings p, i, z thatspecify the latent properties of the mentions x. Themapping p : x 7?
x.p forms a phylogenetic tree onthe mentions, with root?.
Each entity correspondsto a subtree that is rooted at some child of ?.
Themapping i : x 7?
x.i gives an ordering consistentwith that tree in the sense that (?x)x.p.i < x.i.Finally, the mapping z : x 7?
x.z specifies, foreach mention, the topic that generated it.
While iand z are not necessary for creating coref clusters,they are needed to produce p.4 Detailed generative storyGiven a few constants that are referenced in themain text, we assume that the corpus d was gener-ated as follows.First, for each topic z = 1, .
.
.K and each lan-guage `, choose a multinomial ?z`over the wordvocabulary, from a symmetric Dirichlet with con-centration parameter ?.
Then set m = 0 (entityonly aware of previous mentions from our corpus.
This meansthat two mentions cannot be derived from a common ancestoroutside our corpus.
To mitigate this unrealistic assumption, weallow any ordering x of the observed mentions, not respectingdocument timestamps or forcing the mentions from a givendocument to be generated as a contiguous subsequence of x.2Unlike the ddCRP, our generative story is careful to pro-hibit derivational cycles: each mention is copied from a previ-ous mention in the latent ordering.
This is why our phylogenyis a tree, and why our sampler is more complex.
Also unlikethe ddCRP, we permit asymmetric ?distances?
: if a certaintopic or language likes to copy mentions from another, thecompliment is not necessarily returned.count), i = 0 (mention count), and for each docu-ment index d = 1, .
.
.
, D:1.
Choose the document?s length L and language`.
(The distributions used to choose theseare unimportant because these variables arealways observed.)2.
Choose its topic distribution ?dfrom anasymmetric Dirichlet prior with parametersm (Wallach et al, 2009).33.
For each token position k = 1, .
.
.
, L:(a) Choose a topic zdk?
?d.
(b) Choose a word conditioned on the topicand language, wdk?
?zdk`.
(c) If wdkis a named entity type (PERSON,PLACE, ORG, .
.
. )
rather than an ordinaryword, then increment i and:i. create a new mention x withx.e.t = wdkx.d = d x.` = `x.i = i x.z = zdkx.k = kii.
Choose the parent x.p from a distri-bution conditioned on the attributesjust set (see ?4.1).iii.
If x.p = ?, increment m and setx.e = a new entity em.
Else setx.e = x.p.e.iv.
Choose x.n from a distribution con-ditioned on x.p.n and x.` (see ?4.2).Notice that the tokens wdkin document d areexchangeable: by collapsing out ?d, we can re-gard them as having been generated from a CRP.Thus, for fixed values of the non-mention tokensand their topics, the probability of generating themention sequence x is proportional to the prod-uct of the probabilities of the choices in step 3 atthe positions dk where mentions were generated.These choices generate a topic x.z (from the CRPfor document d), a type x.e.t (from ?x.z), a par-ent mention (from the distribution over previousmentions), and a name string (conditioned on theparent?s name if any).
?5 uses this fact to constructan MCMC sampler for the latent parts of x.4.1 Sub-model for parent selectionTo select a parent for a mention x of type t = x.e.t,a simple model (as mentioned above) would be aCRP: each previous mention of the same type isselected with probability proportional to 1, and?
is3Extension: This choice could depend on the language d.`.777selected with probability proportional to ?t> 0.
Alarger choice of ?tresults in smaller entity clusters,because it prefers to create new entities of type trather than copying old ones.We modify this story by re-weighting ?
andprevious mentions according to their relative suit-ability as the parent of x:Pr?
(x.p | x) =exp (?
?
f(x.p, x))Z(x)(1)where x.p ranges over ?
and all previous mentionsof the same type as x, that is, mentions p such thatp.i < x.i and p.e.t = x.e.t.
The normalizing con-stant Z(x)def=?pexp (?
?
f(x.p, x)) is chosenso that the probabilities sum to 1.This is a conditional log-linear model parameter-ized by ?, where ?k?
N (0, ?2k).
The features fare extracted from the attributes of x and x.p.
Ourmost important feature tests whether x.p.z = x.z.This binary feature has a high weight if authorsmainly choose mentions from the same topic.
Tomodel which (other) topics tend to be selected, wealso have a binary feature for each parent topicx.p.z and each topic pair (x.p.z, x.z).44.2 Sub-model for name mutationLet x denote a mention with parent p = x.p.
As inAndrews et al (2012), its name x.n is a stochastictransduction of its parent?s name p.n.
That is,Pr?
(x.n | p.n) (2)is given by the probability that applying a randomsequence of edits to the characters of p.n wouldyield x.n.
The contextual probabilities of differentedits depend on learned parameters ?.
(2) is the total probability of all edit sequencesthat derive x.n from p.n.
It can be computed intime O(|x.n| ?
|p.n|) by dynamic programming.The probability of a single edit sequence, whichcorresponds to a monotonic alignment of x.n top.n, is a product of individual edit probabilities ofthe form Pr?
((ab) | a?
), which is conditioned on thenext input character a?.
The edit (ab) replaces inputa ?
{, a?}
with output b ?
{} ?
?
(where  is4Many other features could be added.
In a multilingualsetting, one would similarly want to model whether Englishauthors select Arabic mentions.
One could also imagine fea-tures that reward proximity in the generative order (x.p.i ?x.i), local linguistic relationships (when x.p.d = x.d andx.p.k ?
x.k), or social information flow (e.g., from main-stream media to Twitter).
One could also make more specificversions of any feature by conjoining it with the entity type t.the empty string and ?
is the alphabet of languagex.`).
Insertions and deletions are the cases whererespectively a =  or b = ?we do not allow bothat once.
All other edits are substitutions.
Whena?
is the special end-of-string symbol #, the onlyallowed edits are the insertion (b) and the substi-tution (##).
We define the edit probability using alocally normalized log-linear model:Pr?
((ab) | a?)
=exp(?
?
f(a?, a, b))?a?,b?exp(?
?
f(a?, a?, b?
))(3)We use a small set of simple feature functions f ,which consider conjunctions of the attributes of thecharacters a?
and b: character, character class (letter,digit, etc.
), and case (upper vs. lower).More generally, the probability (2) may also beconditioned on other variables such as on the lan-guages p.` and x.`?this leaves room for a translit-eration model when x.` 6= p.`?and on the entitytype x.t.
The features in (3) may then depend onthese variables as well.Notice that we use a locally normalized proba-bility for each edit.
This enables faster and sim-pler training than the similar model of Dreyer et al(2008), which uses a globally normalized probabil-ity for the whole edit sequence.When p = ?, we are generating a new name x.n.We use the same model, taking?.n to be the emptystring (but with #?rather than # as the end-of-string symbol).
This yields a feature-based unigramlanguage model (whose character probabilities maydiffer from usual insertion probabilities becausethey see #?as the lookahead character).Pragmatics.
We can optionally make the modelmore sophisticated.
Authors tend to avoid namesx.n that readers would misinterpret (given the pre-viously generated names).
The edit model thinksthat Pr?
(CIA | ?)
is relatively high (because CIA isa short string) and so is Pr?
(CIA | Chuck?s Ice Art).But in fact, if CIA has already been frequently usedto refer to the Central Intelligence Agency, then anauthor is unlikely to use it for a different entity.To model this pragmatic effect, we multiplyour definition of Pr?
(x.n | p.n) by an extra fac-tor Pr(x.e | x)?, where ?
?
0 is the effectstrength.5Here Pr(x.e | x) is the probability thata reader correctly identifies the entity x.e.
Wetake this to be the probability that a reader whoknows our sub-models would guess some parent5Currently we omit the step of renormalizing this deficientmodel.
Our training procedure also ignores the extra factor.778having the correct entity (or ?
if x is a first men-tion):?p?
:p?.e=x.ew(p?, x)/?p?w(p?, x).
Here p?ranges over mentions (including ?)
that precedex in the ordering i, and w(p?, x)?defined later insec.
5.3?is proportional to the posterior probabil-ity that x.p = p?, given name x.n and topic x.z.65 Inference by Block Gibbs SamplingWe use a block Gibbs sampler, which from an ini-tial state (p0, i0, z0) repeats these steps:1.
Sample the ordering i from its conditionaldistribution given all other variables.2.
Sample the topic vector z likewise.3.
Sample the phylogeny p likewise.4.
Output the current sample st= (p, i, z).It is difficult to draw exact samples at steps 1and 2.
Thus, we sample i or z from a simplerproposal distribution, but correct the discrepancyusing the Independent Metropolis-Hastings (IMH)strategy: with an appropriate probability, reject theproposed new value and instead use another copyof the current value (Tierney, 1994).5.1 Resampling the ordering iWe resample the ordering i of the mentions x,conditioned on the other variables.
The currentphylogeny p already defines a partial order on x,since each parent must precede its children.
Forinstance, phylogeny (a) below requires ?
?
x and?
?
y.
This partial order is compatible with 2total orderings, ?
?
x ?
y and ?
?
y ?
x. Bycontrast, phylogeny (b) requires the total ordering?
?
x ?
y.?yx(a)?xy(b)We first sample an ordering i?
(the orderingof mentions with parent ?, i.e.
all mentions) uni-formly at random from the set of orderings compat-ible with the current p. (We provide details aboutthis procedure in Appendix A.
)7However, such or-derings are not in fact equiprobable given the othervariables?some orderings better explain why thatphylogeny was chosen in the first place, according6Better, one could integrate over the reader?s guess of x.z.7The full version of this paper is available athttp://cs.jhu.edu/?noa/publications/phylo-acl-14.pdfto our competitive parent selection model (?4.1).To correct for this bias using IMH, we accept theproposed ordering i?with probabilitya = min(1,Pr(p, i?, z,x | ?,?
)Pr(p, i, z,x | ?,?
))(4)where i is the current ordering.
Otherwise we rejecti?and reuse i for the new sample.5.2 Resampling the topics zEach context word and each named entity is asso-ciated with a latent topic.
The topics of contextwords are assumed exchangeable, and so we re-sample them using Gibbs sampling (Griffiths andSteyvers, 2004).Unfortunately, this is prohibitively expensive forthe (non-exchangeable) topics of the named men-tions x.
A Gibbs sampler would have to choosea new value for x.z with probability proportionalto the resulting joint probability of the full sample.This probability is expensive to evaluate becausechanging x.z will change the probability of manyedges in the current phylogeny p. (Equation (1)puts x is in competition with other parents, so ev-ery mention y that follows x must recompute howhappy it is with its current parent y.p.
)Rather than resampling one topic at a time, we re-sample z as a block.
We use a proposal distributionfor which block sampling is efficient, and use IMHto correct the error in this proposal distribution.Our proposal distribution is an undirected graph-ical model whose random variables are the topicsz and whose graph structure is given by the currentphylogeny p:Q(z) ?
?x 6=?
?x(x.z)?x.p,x(x.p.z, x.z) (5)Q(z) is an approximation to the posterior distri-bution over z.
As detailed below, a proposal canbe sampled from Q(z) in time O(|z|K2) where Kis the number of topics, because the only interac-tions among topics are along the edges of the treep.
The unary factor ?xgives a weight for eachpossible value of x.z, and the binary factor ?x.p,xgives a weight for each possible value of the pair(x.p.z, x.z).The ?x(x.z) factors in (5) approximate the topicmodel?s prior distribution over z.
?x(x.z) is pro-portional to the probability that a Gibbs samplingstep for an ordinary topic model would choose thisvalue of x.z.
This depends on whether?in the779current sample?x.z is currently common in x?sdocument and x.t is commonly generated by x.z.It ignores the fact that we will also be resamplingthe topics of the other mentions.The ?x.p,xfactors in (5) approximate Pr(p |z, i) (up to a constant factor), where p is the currentphylogeny.
Specifically, ?x.p,xapproximates theprobability of a single edge.
It ought to be givenby (1), but we use only the numerator of (1), whichavoids modeling the competition among parents.We sample from Q using standard methods, sim-ilar to sampling from a linear-chain CRF by run-ning the backward algorithm followed by forwardsampling.
Specifically, we run the sum-productalgorithm from the leaves up to the root ?, at eachnode x computing the following for each topic z:?x(z)def= ?x(z) ??y?children(x)?z?
?x,y(z, z?)
?
?y(z?
)Then we sample from the root down to the leaves,first sampling ?.z from ?
?, then at each x 6= ?sampling the topic x.z to be z with probabilityproportional to ?x.p,x(x.p.z, z) ?
?x(z).Again we use IMH to correct for the bias in Q:we accept the resulting proposal?z with probabilitymin(1,Pr(p, i,?z,x | ?,?
)Pr(p, i, z,x | ?,?
)?Q(z)Q(?z))(6)While Pr(p, i,?z,x | ?,?)
might seem slow tocompute because it contains many factors (1) withdifferent denominators Z(x), one can share workby visiting the mentions x in their order i. Mostsummands in Z(x) were already included in Z(x?
),where x?is the latest previous mention having thesame attributes as x (e.g., same topic).5.3 Resampling the phylogeny pIt is easy to resample the phylogeny.
For each x, wemust choose a parent x.p from among the possibleparents p (having p.i < x.i and p.e.t = x.e.t).Since the ordering i prevents cycles, the resultingphylogeny p is indeed a tree.Given the topics z, the ordering i, and the ob-served names, we choose an x.p value accordingto its posterior probability.
This is proportional tow(x.p, x)def= Pr?
(x.p | x) ?
Pr?
(x.n | x.p.n),independent of any other mention?s choice of par-ent.
The two factors here are given by (1) and (2)respectively.
As in the previous section, the de-nominators Z(x) in the Pr(x.p | x) factors can becomputed efficiently with shared work.With the pragmatic model (section 4.2), the par-ent choices are no longer independent; then thesamples of p should be corrected by IMH as usual.5.4 Initializing the samplerThe initial sampler state (z0,p0, i0) is obtained asfollows.
(1) We fix topics z0via collapsed Gibbssampling (Griffiths and Steyvers, 2004).
The sam-pler is run for 1000 iterations, and the final sam-pler state is taken to be z0.
This process treats alltopics as exchangeable, including those associatedwith named entities.
(2) Given the topic assignmentz0, initialize p0to the phylogeny rooted at ?
thatmaximizes?xlogw(x.p, x).
This is a maximumrooted directed spanning tree problem that can besolved in time O(n2) (Tarjan, 1977).
The weightw(x.p, x) is defined as in section 5.3?except thatsince we do not yet have an ordering i, we do notrestrict the possible values of x.p to mentions pwith p.i < x.p.i.
(3) Given p0, sample an orderingi0using the procedure described in ?5.1.6 Parameter EstimationEvaluating the likelihood and its partial derivativeswith respect to the parameters of the model requiresmarginalizing over our latent variables.
As thismarginalization is intractable, we resort to MonteCarlo EM procedure (Levine and Casella, 2001)which iterates the following two steps:E-step: Collect samples by MCMC simulation asin ?5, given current model parameters ?
and ?.M-step: Improve ?
and ?
to increase8Ldef=1SS?s=1log Pr?,?
(x,ps, is, zs) (7)It is not necessary to locally maximize L at eachM-step, merely to improve it if it is not alreadyat a local maximum (Dempster et al, 1977).
Weimprove it by a single update: at the tth M-step, weupdate our parameters to ?t= (?t,?t)?t= ?t?1+ ??t?
?L(x,?t?1) (8)where ?
is a fixed scaling term and ?tis an adap-tive learning rate given by AdaGrad (Duchi et al,2011).We now describe how to compute the gradient??L.
The gradient with respect to the parent se-8We actually do MAP-EM, which augments (7) by addingthe log-likelihoods of ?
and ?
under a Gaussian prior.780lection parameters ?
is?1S?
?f(p, x)??p?Pr?
(p?| x)f(p?, x)??
(9)The outer summation ranges over all edges in theS samples.
The other variables in (9) are associ-ated with the edge being summed over.
That edgeexplains a mention x as a mutation of some parentp in the context of a particular sample (ps, is, zs).The possible parents p?range over ?
and the men-tions that precede x according to the ordering is,while the features f and distribution Pr?
dependon the topics zs.As for the mutation parameters, let cp,xbe thefraction of samples in which p is the parent of x.This is the expected number of times that the stringp.n mutated into x.n.
Given this weighted set ofstring pairs, let ca?,a,bbe the expected number oftimes that edit (ab) was chosen in context a?
: thiscan be computed using dynamic programming tomarginalize over the latent edit sequence that mapsp.n to x.n, for each (p, x).
The gradient of L withrespect to ?
is?a?,a,bca?,a,b(f(a?, a, b)??a?,b?Pr?
(a?, b?| a?
)f(a?, a?, b?
))(10)7 Consensus ClusteringFrom a single phylogeny p, we deterministicallyobtain a clustering e by removing the root ?.
Eachof the resulting connected components correspondsto a cluster of mentions.
Our model gives a distribu-tion over phylogenies p (given observations x andlearned parameters ?
)?and thus gives a posteriordistribution over clusterings e, which can be usedto answer various queries.A traditional query is to request a single cluster-ing e. We prefer the clustering e?that minimizesBayes risk (MBR) (Bickel and Doksum, 1977):e?= argmine?
?eL(e?, e) Pr(e | x,?,?)
(11)This minimizes our expected loss, where L(e?, e)denotes the loss associated with picking e?whenthe true clustering is e. In practice, we again esti-mate the expectation by sampling e values.The Rand index (Rand, 1971)?unlike our actualevaluation measure?is an efficient choice of lossfunction L for use with (11):R(e?, e)def=TP + TNTP + FP + TN + FN=TP + TN(N2)where the true positives (TP), true negatives (TN),false positives (FP), and false negatives (FN) usethe clustering e to evaluate how well e?classi-fies the(N2)mention pairs as coreferent or not.More similar clusterings achieve larger R, withR(e?, e) = 1 iff e?= e. In all cases, 0 ?R(e?, e) = R(e, e?)
?
1.The MBR decision rule for the (negated) Randindex is easily seen to be equivalent toe?= argmaxe?E[TP] + E[TN] (12)= argmaxe?
?i,j: xi?xjsij+?i,j: xi6?xj(1?
sij)where ?
denotes coreference according to e?.
Asexplained above, the sijare coreference probabil-ities sijthat can be estimated from a sample ofclusterings e.This objective corresponds to min-max graphcut (Ding et al, 2001), an NP-hard problem withan approximate solution (Nie et al, 2010).98 ExperimentsIn this section, we describe experiments on threedifferent datasets.
Our main results are describedfirst: Twitter features many instances of name vari-ation that we would like our model to be able tolearn.
We also report the performance of differentablations of our full approach, in order to see whichconsistently helped across the different splits.
Wereport additional experiments on the ACE 2008 cor-pus, and on a political blog corpus, to demonstratethat our approach is applicable in different settings.For Twitter and ACE 2008, we report the stan-dard B3metric (Bagga and Baldwin, 1998a).
Forthe political blog dataset, the reference does notconsist of entity annotations, and so we follow theevaluation procedure of Yogatama et al (2012).8.1 TwitterData.
We use a novel corpus of Twitter posts dis-cussing the 2013 Grammy Award ceremony.
Thisis a challenging corpus, featuring many instances9In our experiments, we run the clustering algorithm fivetimes, initialized from samples chosen at random from the last10% of the sampler run, and keep the clustering that achievedhighest expected Rand score.781of name variation.
The dataset consists of five splits(by entity), the smallest of which is 604 mentionsand the largest is 1374.
We reserve the largest splitfor development purposes, and report our resultson the remaining four.
Appendix B provides moredetail about the dataset.Baselines.
We use the discriminative entity cluster-ing algorithm of Green et al (2012) as our baseline;their approach was found to outperform anothergenerative model which produced a flat cluster-ing of mentions via a Dirichlet process mixturemodel.
Their method uses Jaro-Winkler string sim-ilarity to match names, then clusters mentions withmatching names (for disambiguation) by compar-ing their unigram context distributions using theJenson-Shannon metric.
We also compare to theEXACT-MATCH baseline, which assigns all stringswith the same name to the same entity.Procedure.
We run four test experiments in whichone split is used to pick model hyperparametersand the remaining three are used for test.
For thediscriminative baseline, we tune the string matchthreshold, context threshold, and the weight of thecontext model prior (all via grid search).
For ourmodel, we tune only the fixed weight of the rootfeature, which determines the precision/recall trade-off (larger values of this feature result in moreattachments to ?
and hence more entities).
Weleave other hyperparameters fixed: 16 latent top-ics, and Gaussian priors N (0, 1) on all log-linearparameters.
For PHYLO, the entity clustering isthe result of (1) training the model using EM, (2)sampling from the posterior to obtain a distribu-tion over clusterings, and (3) finding a consensusclustering.
We use 20 iterations of EM with 100samples per E-step for training, and use 1000 sam-ples after training to estimate the posterior.
Wereport results using three variations of our model:PHYLO does not consider mention context (all men-tions effectively have the same topic) and deter-mines mention entities from a single sample ofp (the last); PHYLO+TOPIC adds context (?5.2);PHYLO+TOPIC+MBR uses the full posterior andconsensus clustering to pick the output clustering(?7).
Our results are shown in Table 1.1010Our single-threaded implementation took around 15 min-utes per fold of the Twitter corpus on a personal laptop witha 2.3 Ghz Intel Core i7 processor (including time required toparse the data files).
Typical acceptance rates for ordering andtopic proposals ranged from 0.03 to 0.08.Mean Test B3P R F1EXACT-MATCH 99.6 53.7 69.8Green et al (2012) 92.1 69.8 79.3PHYLO 85.3 91.4 88.7PHYLO+TOPIC 92.8 90.8 91.8PHYLO+TOPIC+MBR 92.9 90.9 91.9Table 1: Results for the Twitter dataset.
Higher B3scoresare better.
Note that each number is averaged over fourdifferent test splits.
In three out of four experiments,PHYLO+TOPIC+MBR achieved the highest F1 score; in onecase PHYLO+TOPIC won by a small margin.Test B3P R F1PEREXACT-MATCH 98.0 81.2 88.8Green et al (2012) 95.0 88.9 91.9PHYLO+TOPIC+MBR 97.2 88.6 92.7ORGEXACT-MATCH 98.2 78.3 87.1Green et al (2012) 92.1 88.5 90.3PHYLO+TOPIC+MBR 95.5 80.9 87.6Table 2: Results for the ACE 2008 newswire dataset.8.2 NewswireData.
We use the ACE 2008 dataset, which isdescribed in detail in Green et al (2012).
It issplit into a development portion and a test portion.The baseline system took the first mention fromeach (gold) within-document coreference chain asthe canonical mention, ignoring other mentions inthe chain; we follow the same procedure in ourexperiments.11Baselines & Procedure.
We use the same base-lines as in ?8.1.
On development data, modelingpragmatics as in ?4.2 gave large improvements fororganizations (8 points in F-measure), correctingthe tendency to assume that short names like CIAwere coincidental homonyms.
Hence we allowed?
> 0 and tuned it on development data.12Resultsare in Table 2.8.3 BlogsData.
The CMU political blogs dataset consists of3000 documents about U.S. politics (Yano et al,2009).
Preprocessed as described in Yogatama et al(2012), the data consists of 10647 entity mentions.11That is, each within-document coreference chain ismapped to a single mention as a preprocessing step.12We used only a simplified version of the pragmatic model,approximating w(p?, x) as 1 or 0 according to whether p?.n =x.n.
We also omitted the IMH step from section 5.3.
Theother results we report do not use pragmatics at all, since wefound that it gave only a slight improvement on Twitter.782Unlike our other datasets, mentions are not anno-tated with entities: the reference consists of a tableof 126 entities, where each row is the canonicalname of one entity.Baselines.
We compare to the system resultsreported in Figure 2 of Yogatama et al (2012).This includes a baseline hierarchical clustering ap-proach, the ?EEA?
name canonicalization systemof Eisenstein et al (2011), as well the model pro-posed by Yogatama et al (2012).
Like the outputof our model, the output of their hierarchical clus-tering baseline is a mention clustering, and there-fore must be mapped to a table of canonical entitynames to compare to the reference table.Procedure & Results We tune our method as inprevious experiments, on the initialization dataused by Yogatama et al (2012) which consists ofa subset of 700 documents of the full dataset.
Thetuned model then produced a mention clusteringon the full political blog corpus.
As the mappingfrom clusters to a table is not fully detailed in Yo-gatama et al (2012), we used a simple heuristic:the most frequent name in each cluster is taken asthe canonical name, augmented by any titles froma predefined list appearing in any other name inthe cluster.
The resulting table is then evaluatedagainst the reference, as described in Yogatama etal.
(2012).
We achieved a response score of 0.17and a reference score of 0.61.
Though not state-of-the-art, this result is close to the score of the ?EEA?system of Eisenstein et al (2011), as reported inFigure 2 of Yogatama et al (2012), which is specif-ically designed for the task of canonicalization.8.4 DiscussionOn the Twitter dataset, we obtained a 12.6-point F1improvement over the baseline.
To understand ourmodel?s behavior, we looked at the sampled phy-logenetic trees on development data.
One reasonour model does well in this noisy domain is thatit is able to relate seemingly dissimilar names viasuccessive steps.
For instance, our model learnedto relate many variations of LL Cool J:Cool James LLCoJ El-El Cool JohnLL LL COOL JAMES LLCOOLJIn the sample we inspected, these mentions werealso assigned the same topic, further boosting theprobability of the configuration.The ACE dataset, consisting of editorializednewswire, naturally contains less name variationthan Twitter data.
Nonetheless, we find that thevariation that does appear is often properly handledby our model.
For instance, we see several in-stances of variation due to transliteration that wereall correctly grouped together, such as MegawatiSoekarnoputri and Megawati Sukarnoputri.
The prag-matic model was also effective in grouping com-mon acronyms into the same entity.We found that multiple samples tend to give dif-ferent phylogenies (so the sampler is mobile), butessentially the same clustering into entities (whichis why consensus clustering did not improve muchover simply using the last sample).
Random restartsof EM might create more variety by choosing dif-ferent locally optimal parameter settings.
It mayalso be beneficial to explore other sampling tech-niques (Bouchard-C?ot?e, 2014).Our method assembles observed names into anevolutionary tree.
However, the true tree must in-clude many names that fall outside our small ob-served corpora, so our model would be a moreappropriate fit for a far larger corpus.
Larger cor-pora also offer stronger signals that might enableour Monte Carlo methods to mix faster and detectregularities more accurately.A common error of our system is to connectmentions that share long substrings, such as dif-ferent PERSONs who share a last name, or differ-ent ORGANIZATIONs that contain University of.
Amore powerful name mutation than the one we usehere would recognize entire words, for exampleinserting a common title or replacing a first namewith its common nickname.
Modeling the internalstructure of names (Johnson, 2010; Eisenstein etal., 2011; Yogatama et al, 2012) in the mutationmodel is a promising future direction.9 ConclusionsOur primary contribution consists of new model-ing ideas, and associated inference techniques, forthe problem of cross-document coreference resolu-tion.
We have described how writers systematicallyplunder (?)
and then systematically modify (?)
thework of past writers.
Inference under such modelscould also play a role in tracking evolving memesand social influence, not merely in establishingstrict coreference.
Our model also provides an al-ternative to the distance-dependent CRP.2Our implementation is available for re-search use at: https://bitbucket.org/noandrews/phyloinf.783ReferencesNicholas Andrews, Jason Eisner, and Mark Dredze.2012.
Name phylogeny: A generative model ofstring variation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 344?355, Jeju, Korea, July.Amit Bagga and Breck Baldwin.
1998a.
Algorithmsfor scoring coreference chains.
In In The First In-ternational Conference on Language Resources andEvaluation Workshop on Linguistics Coreference,pages 563?566.Amit Bagga and Breck Baldwin.
1998b.
Entity-based cross-document coreferencing using the vec-tor space model.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics - Volume 1, ACL ?98, pages79?85, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Alex Baron and Marjorie Freedman.
2008.
Whois who and what is what: Experiments in cross-document co-reference.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 274?283, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Peter J. Bickel and Kjell A. Doksum.
1977.
Mathe-matical Statistics : Basic Ideas and Selected Topics.Holden-Day, Inc.David M. Blei and Peter I. Frazier.
2011.
Distancedependent chinese restaurant processes.
J. Mach.Learn.
Res., 12:2461?2488, November.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
LatentDirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Alexandre Bouchard-C?ot?e, David Hall, Thomas L.Griffiths, and Dan Klein.
2013.
Automated re-construction of ancient languages using probabilis-tic models of sound change.
Proceedings of the Na-tional Academy of Sciences.Alexandre Bouchard-C?ot?e.
2014.
Sequential MonteCarlo (SMC) for Bayesian phylogenetics.
Bayesianphylogenetics: methods, algorithms, and applica-tions.William W. Cohen, Pradeep Ravikumar, and Stephen E.Fienberg.
2003.
A comparison of string metrics formatching names and records.
In KDD Workshop ondata cleaning and object consolidation.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical Soci-ety.
Series B (Methodological), 39(1):1?38.C.H.Q.
Ding, Xiaofeng He, Hongyuan Zha, Ming Gu,and H.D.
Simon.
2001.
A min-max cut algorithmfor graph partitioning and data clustering.
In DataMining, 2001.
ICDM 2001, Proceedings IEEE Inter-national Conference on, pages 107 ?114.Mark Dredze, Michael J Paul, Shane Bergsma, andHieu Tran.
2013.
Carmen: A twitter geolocationsystem with applications to public health.
In AAAIWorkshop on Expanding the Boundaries of HealthInformatics Using AI (HIAI).Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductionswith finite-state methods.
In Proceedings of the2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 1080?1089, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159, July.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing, pages 1971?1982.Association for Computational Linguistics.Jacob Eisenstein, Tae Yano, William W. Cohen,Noah A. Smith, and Eric P. Xing.
2011.
Structureddatabases of named entities from bayesian nonpara-metrics.
In Proceedings of the First Workshop onUnsupervised Learning in NLP, EMNLP ?11, pages2?12, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.T.
Finin, Z. Syed, J. Mayfield, P. McNamee, and C. Pi-atko.
2009.
Using Wikitology for cross-documententity coreference resolution.
In AAAI Spring Sym-posium on Learning by Reading and Learning toRead.Spence Green, Nicholas Andrews, Matthew R. Gorm-ley, Mark Dredze, and Christopher D. Manning.2012.
Entity clustering across languages.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 60?69, Stroudsburg, PA,USA.
Association for Computational Linguistics.Thomas L Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 101(Suppl 1):5228?5235.Stephen Guo, Ming-Wei Chang, and Emre K?c?man.2013.
To link or not to link?
a study on end-to-endtweet entity linking.
In Proceedings of NAACL-HLT,pages 1020?1030.784Aria Haghighi and Dan Klein.
2010.
Coreference res-olution in a modular, entity-centered model.
In Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, pages 385?393,Los Angeles, California, June.
Association for Com-putational Linguistics.Mark Johnson.
2010.
Pcfgs, topic models, adaptorgrammars and learning topical collocations and thestructure of proper names.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 1148?1157,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Zornitsa Kozareva and Sujith Ravi.
2011.
Unsuper-vised name ambiguity resolution using a generativemodel.
In Proceedings of the First Workshop onUnsupervised Learning in NLP, EMNLP ?11, pages105?112, Stroudsburg, PA, USA.
Association forComputational Linguistics.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entityand event coreference resolution across documents.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL).Richard A. Levine and George Casella.
2001.
Im-plementations of the Monte Carlo EM Algorithm.Journal of Computational and Graphical Statistics,10(3):422?439.Feiping Nie, Chris H. Q. Ding, Dijun Luo, and HengHuang.
2010.
Improved minmax cut graph cluster-ing with nonnegative relaxation.
In Jos?e L. Balc?azar,Francesco Bonchi, Aristides Gionis, and Mich`eleSebag, editors, ECML/PKDD (2), volume 6322 ofLecture Notes in Computer Science, pages 451?466.Springer.E.
H. Porter and W. E. Winkler, 1997.
ApproximateString Comparison and its Effect on an AdvancedRecord Linkage System, chapter 6, pages 190?199.U.S.
Bureau of the Census.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the Ameri-can Statistical Association, 66(336):846?850.Delip Rao, Paul McNamee, and Mark Dredze.
2010.Streaming cross document entity coreference reso-lution.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 1050?1058, Stroudsburg, PA,USA.
Association for Computational Linguistics.Eric Sven Ristad and Peter N. Yianilos.
1996.
Learn-ing string edit distance.
Technical Report CS-TR-532-96, Princeton University, Department of Com-puter Science.Eric Sven Ristad and Peter N. Yianilos.
1998.Learning string edit distance.
IEEE Transactionson Pattern Recognition and Machine Intelligence,20(5):522?532, May.Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.Named entity recognition in tweets: an experimentalstudy.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1524?1534.
Association for Computational Linguis-tics.Sameer Singh, Amarnag Subramanya, FernandoPereira, and Andrew McCallum.
2011.
Large-scalecross-document coreference using distributed infer-ence and hierarchical models.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 793?803, Portland, Oregon, USA, June.Association for Computational Linguistics.R E Tarjan.
1977.
Finding optimum branchings.
Net-works, 7(1):25?35.Luke Tierney.
1994.
Markov Chains for ExploringPosterior Distributions.
The Annals of Statistics,22(4):1701?1728.Hanna Wallach, David Mimno, and Andrew McCal-lum.
2009.
Rethinking lda: Why priors matter.
InAdvances in Neural Information Processing Systems,pages 1973?1981.Michael Wick, Sameer Singh, and Andrew McCallum.2012.
A discriminative hierarchical model for fastcoreference at large scale.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers - Volume 1, ACL?12, pages 379?388, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.William E. Winkler.
1999.
The state of record link-age and current research problems.
Technical report,Statistical Research Division, U.S. Census Bureau.Tae Yano, William W. Cohen, and Noah A. Smith.2009.
Predicting response to political blog postswith topic models.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, NAACL ?09, pages477?485, Stroudsburg, PA, USA.
Association forComputational Linguistics.Dani Yogatama, Yanchuan Sim, and Noah A. Smith.2012.
A probabilistic model for canonicalizingnamed entity mentions.
In Proceedings of the 50thAnnual Meeting of the Association for Computa-tional Linguistics: Long Papers - Volume 1, ACL?12, pages 685?693, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.785
