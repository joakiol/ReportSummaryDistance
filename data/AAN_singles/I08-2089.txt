Large and Diverse Language Models for Statistical Machine TranslationHolger Schwenk?LIMSI - CNRSFranceschwenk@limsi.frPhilipp KoehnSchool of InformaticsUniversity of EdinburghScotlandpkoehn@inf.ed.ac.ukAbstractThis paper presents methods to combinelarge language models trained from diversetext sources and applies them to a state-of-art French?English and Arabic?English ma-chine translation system.
We show gains ofover 2 BLEU points over a strong baselineby using continuous space language modelsin re-ranking.1 IntroductionOften more data is better data, and so it should comeas no surprise that recently statistical machine trans-lation (SMT) systems have been improved by theuse of large language models (LM).
However, train-ing data for LMs often comes from diverse sources,some of them are quite different from the target do-main of the MT application.
Hence, we need toweight and combine these corpora appropriately.
Inaddition, the vast amount of training data availablefor LM purposes and the desire to use high-ordern-grams quickly exceeds the conventional comput-ing resources that are typically available.
If we arenot able to accommodate large LMs integrated intothe decoder, using them in re-ranking is an option.In this paper, we present and compare methods tobuild LMs from diverse training corpora.
We alsoshow that complex LMs can be used in re-rankingto improve performance given a strong baseline.
Inparticular, we use high-order n-grams continuousspace LMs to obtain MT of the well-known NIST2006 test set that compares very favorably with theresults reported in the official evaluation.
?new address: LIUM, University du Maine, France,Holger.Schwenk@lium.univ-lemans.fr2 Related WorkThe utility of ever increasingly large LMs for MThas been recognized in recent years.
The effectof doubling LM size has been powerfully demon-strated by Google?s submissions to the NIST eval-uation campaigns.
The use of billions of words ofLM training data has become standard in large-scaleSMT systems, and even trillion word LMs have beendemonstrated.
Since lookup of LM scores is one ofthe fundamental functions in SMT decoding, effi-cient storage and access of the model becomes in-creasingly difficult.A recent trend is to store the LM in a distributedcluster of machines, which are queried via networkrequests (Brants et al, 2007; Emami et al, 2007).It is easier, however, to use such large LMs in re-ranking (Zhang et al, 2006).
Since the use of clus-ters of machines is not always practical (or afford-able) for SMT applications, an alternative strategyis to find more efficient ways to store the LM in theworking memory of a single machine, for instanceby using efficient prefix trees and fewer bits to storethe LM probability (Federico and Bertoldi, 2006).Also the use of lossy data structures based on Bloomfilters has been demonstrated to be effective for LMs(Talbot and Osborne, 2007a; Talbot and Osborne,2007b).
This allows the use of much larger LMs,but increases the risk of errors.3 Combination of Language ModelsLM training data may be any text in the outputlanguage.
Typically, however, we are interested inbuilding a MT system for a particular domain.
If textresources come from a diversity of domains, somemay be more suitable than others.
A common strat-661projectionlayer hiddenlayeroutputlayerinputprojectionssharedLM probabilitiesfor all wordsprobability estimationNeural Networkdiscreterepresentation:indices in wordlistcontinuousrepresentation:P dimensional vectorsNwj?1 PHNP (wj =1|hj)wj?n+1wj?n+2P (wj =i|hj)P (wj =N|hj)cloiMVdjp1 =pN =pi =Figure 1: Architecture of the continuous space LM.egy is to divide up the LM training texts into smallerparts, train a LM for each of them and combine thesein the SMT system.
Two strategies may be em-ployed to combine LMs: One is the use of interpola-tion.
LMs are combined into one by weighting eachbased on their relevance to the focus domain.
Theweighting is carried out by optimizing perplexity ofa representative tuning set that is taken from the do-main.
Standard LM toolkits like SRILM (Stolcke,2002) provide tools to estimate optimal weights us-ing the EM algorithm.The second strategy exploits the log-linear modelthat is the basis of modern SMT systems.
In thisframework, a linear combination of feature func-tions is used, which include the log of the LM prob-ability.
It is straight-forward to use multiple LMs inthis framework and treat each as a feature functionin the log-linear model.
Combining several LMs inthe log domain corresponds to multiplying the cor-responding probabilities.
Strictly speaking, this sup-poses an independence assumption that is rarely sat-isfied in practice.
The combination coefficients areoptimized on a criterion directly related to the trans-lation performance, for instance the BLEU score.In summary, these strategies differ in two points:linear versus log-linear combination, and optimizingperplexity versus optimizing BLEU scores.4 Continuous Space Language ModelsThis LM approach is based a continuous represen-tation of the words (Bengio et al, 2003).
The ba-sic idea is to convert the word indices to a continu-ous representation and to use a probability estima-tor operating in this space.
Since the resulting dis-tributions are smooth functions of the word repre-sentation, better generalization to unknown n-gramscan be expected.
This approach was successfully ap-plied to language modeling in small (Schwenk et al,2006) an medium-sized phrase-based SMT systems(De?chelotte et al, 2007).The architecture of the continuous space languagemodel (CSLM) is shown in Figure 1.
A standardfully-connected multi-layer perceptron is used.
Theinputs to the neural network are the indices of then?1 previous words in the vocabulary hj=wj?n+1,.
.
.
, wj?2, wj?1 and the outputs are the posteriorprobabilities of all words of the vocabulary:P (wj = i|hj) ?i ?
[1,N ] (1)where N is the size of the vocabulary.
The inputuses the so-called 1-of-n coding, i.e., the ith word ofthe vocabulary is coded by setting the ith element ofthe vector to 1 and all the other elements to 0.
Theith line of the N ?P dimensional projection matrixcorresponds to the continuous representation of theith word.1 Let us denote cl these projections, dj thehidden layer activities, oi the outputs, pi their soft-max normalization, and mjl, bj , vij and ki the hid-den and output layer weights and the correspondingbiases.
Using these notations, the neural networkperforms the following operations:dj = tanh(?lmjl cl + bj)(2)oi =?jvij dj + ki (3)pi = eoi /N?r=1eor (4)The value of the output neuron pi corresponds di-rectly to the probability P (wj = i|hj).Training is performed with the standard back-propagation algorithm minimizing the following er-ror function:E =N?i=1ti log pi + ???
?jlm2jl +?ijv2ij??
(5)1Typical values are P = 200 .
.
.
300662where ti denotes the desired output.
The parame-ter ?
has to be determined experimentally.
Train-ing is done using a resampling algorithm (Schwenk,2007).
It can be shown that the outputs of a neuralnetwork trained in this manner converge to the pos-terior probabilities.
Therefore, the neural networkdirectly minimizes the perplexity on the trainingdata.
Note also that the gradient is back-propagatedthrough the projection-layer, which means that theneural network learns the projection of the wordsthat is best for the probability estimation task.In general, the complexity to calculate one prob-ability is dominated by the output layer dimensionsince the size of the vocabulary (here N=273k) isusually much larger than the dimension of the hid-den layer (here H=500).
Therefore, the CSLM isonly used when the to be predicted word falls intothe 8k most frequent ones.
While this substantiallydecreases the dimension of the output layer, it stillcovers more than 90% of the LM requests.
Theother requests are obtained from a standard back-offLM.
Note that the full vocabulary is still used for thewords in the context (input of the neural network).The incorporation of the CSLM into the SMTsystem is done by using n-best lists.
In all ourexperiments, the LM probabilities provided by theCSLM are added as an additional feature function.It is also possible to use only one feature functionfor the modeling of the target language (interpola-tion between the back-off and the CSLM), but thiswould need more memory since the huge back-offLM must be loaded during n-best list rescoring.We did not try to use the CSLM directly duringdecoding since this would result in increased decod-ing times.
Calculating a LM probability with a back-off model corresponds basically to a table look-up,while a forward pass through the neural network isnecessary for the CSLM.
Very efficient optimiza-tions are possible, in particular when n-grams withthe same context can be grouped together, but a re-organization of the decoder may be necessary.5 Language Models in Decoding andRe-RankingLM lookups are one of the most time-consumingsteps in the decoding process, which makes time-efficient implementations essential.
Consequently,the LMs have to be held in the working memory ofthe machine, since disk lookups are simply too slow.Filtering LMs to the n-grams which are needed forthe decoding a particular sentence may be an option,but is useful only to a degree.
Since the order of out-put words is unknown before decoding, all n-gramsthat contain any of output words that may be gener-ated during decoding need to be preserved.00.020.040.060.080.10.120.140.160  20  40  60  80  100  120ratioof 5-gramsrequired(bag-of-words)sentence lengthFigure 2: Ratio of 5-grams required to translate onesentence.
The graph plots the ratio against sentencelength.
For a 40-word sentence, typically 5% ofthe LM is needed (numbers from German?Englishmodel trained on Europarl).See Figure 2 for an illustration that highlightswhat ratio of the LM is needed to translate a sin-gle sentence.
The ratio increases roughly linearwith sentence length.
For a typical 30-word sen-tence, about 4% of the LM 5-grams may be po-tentially generated during decoding.
For large 100-word sentences, the ratio is about 15%.2 These num-bers suggest that we may be able to use 5?10 timeslarger LMs, if we filter the LM prior to the decod-ing of each sentence.
SMT decoders such as Moses(Koehn et al, 2007) may store the translation modelin an efficient on-disk data structure (Zens and Ney,2007), leaving almost the entire working memoryfor LM storage.
However, this means for 32-bit ma-chines a limit of 3 GB for the LM.On the other hand, we can limit the use of verylarge LMs to a re-ranking stage.
In two-pass de-2The numbers were obtained using a 5-gram LM trainedon the English side of the Europarl corpus (Koehn, 2005), aGerman?English translation model trained on Europarl, and theWMT 2006 test set (Koehn and Monz, 2006).663French EnglishNews Commentary 1.2M 1.0MEuroparl 37.5M 33.8MTable 1: Combination of a small in-domain (NewsCommentary) and large out-of-domain (Europarl)training corpus (number of words).coding, the initial decoder produces an n-best listof translation candidates (say, n=1000), and a sec-ond pass exploits additional features, for instancevery large LMs.
Since the order of English wordsis fixed, the number of different n-grams that needto be looked up is dramatically reduced.
However,since the n-best list is only the tip of the icebergof possible translations, we may miss the translationthat we would have found with a LM integrated intothe decoding process.6 ExperimentsIn our experiments we are looking for answers to theopen questions on the use of LMs for SMT: Do per-plexity and BLEU score performance correlate wheninterpolating LMs?
Should LMs be combined by in-terpolation or be used as separate feature functionsin the log-linear machine translation model?
Is theuse of LMs in re-ranking sufficient to increase ma-chine translation performance?6.1 InterpolationIn the WMT 2007 shared task evaluation campaign(Callison-Burch et al, 2007) domain adaptation wasa special challenge.
Two training corpora were pro-vided: a small in-domain corpus (News Commen-tary) and the about 30 times bigger out-of-domainEuroparl corpus (see Table 1).
One method for do-main adaptation is to bias the LM towards the in-domain data.
We train two LMs and interpolate themto optimize performance on in-domain data.
In ourexperiments, the translation model is first trained onthe combined corpus without weighting.
We use theMoses decoder (Koehn et al, 2007) with default set-tings.
The 5-gram LM was trained using the SRILMtoolkit.
We only run minimum error rate trainingonce, using the in-domain LM.
Using different LMsfor tuning may change our findings reported here.When interpolating the LMs, different weights1802002202402602803000  0.2  0.4  0.6  0.8  1242526272829PerplexityBLEUscoreInterpolation coefficientBleu scoresDev perplexityDevTestFigure 3: Different weight given to the out-of-domain data and effect on perplexity of a develop-ment set (nc-dev2007) and on the BLEU score of thetest set (nc-devtest2007).TM LM BLEU (test)combined 2 features 27.30combined interpolated 0.42 27.232 features 2 features 27.642 features interpolated 0.42 27.63Table 2: Combination of the translation models(TM) by simple concatenation of the training datavs.
use of two feature functions, and combinationof the LM (LM) by interpolation or the use of twofeature functions.may be given to the out-of-domain versus the in-domain LM.
One way to tune the weight is to opti-mize perplexity on a development set (nc-dev2007).We examine values between 0 and 1, the EM proce-dure gives the lowest perplexity of 193.9 at a valueof 0.42.
Does this setting correspond with goodBLEU scores on the development and test set (nc-devtest2007) ?
See Figure 3 for a comparison.
TheBLEU score on the development data is 28.55 whenthe interpolation coefficient is used that was ob-tained by optimizing the perplexity.
A slightly bettervalue of 28.78 good be obtained when using an in-terpolation coefficient of 0.15.
The test data seemsto be closer to the out-of-domain Europarl corpussince the best BLEU scores would be obtained forsmaller values of the interpolation coefficient.The second question we raised was: Is interpola-tion of LMs preferable to the use of multiple LMs664as separate feature functions.
See Table 2 for num-bers in the same experimental setting for two dif-ferent comparisons.
First, we compare the perfor-mance of the interpolated LM with the use of twofeature functions.
The resulting BLEU scores arevery similar (27.23 vs. 27.30).
In a second experi-ment, we build two translation models, one for eachcorpus, and use separate feature functions for them.This gives a slightly better performance, but again itgives almost identical results for the use of interpo-lated LMs vs. two LMs as separate feature functions(27.63 vs. 27.64).These experiments suggest that interpolated LMsgive similar performance to the use of multiple LMs.In terms of memory efficiency, this is good news,since an interpolated LM uses less memory.6.2 Re-RankingLet us now turn our attention to the use of very largeLMs in decoding and re-ranking.
The largest freelyavailable training sets for MT are the corpora pro-vided by the LDC for the NIST and GALE evalu-ation campaigns for Arabic?English and Chinese?English.
In this paper, we concentrate on the firstlanguage pair.
Our starting point is a system us-ing Moses trained on a training corpus of about 200million words that was made available through theGALE program.
Training such a large system pushesthe limits of the freely available standard tools.For instance, GIZA++, the standard tool for wordalignment keeps a word translation table in memory.The only way to get it to process the 200 millionword parallel corpus is to stem all words to their firstfive letters (hence reducing vocabulary size).
Still,GIZA++ training takes more than a week of com-pute time on our 3 GHz machines.
Training usesdefault settings of Moses.
Tuning is carried out us-ing the 2004 NIST evaluation set.
The resulting sys-tem is competitive with the state of the art.
The bestCorpus WordsParallel training data (train) 216MAFP part of Gigaword (afp) 390MXinhua part of Gigaword (xin) 228MFull Gigaword (giga) 2,894MTable 3: Size of the training corpora for LMs innumber of words (including punctuation)Px Bleu scoreDecode LM eval04 eval04 eval063-gram train+xin+afp 86.9 50.57 43.693-gram train+giga 85.9 50.53 43.994-gram train+xin+afp 74.9 50.99 43.90Reranking with continuous space LM:5-gram train+xin+afp 62.5 52.88 46.026-gram train+xin+afp 60.9 53.25 45.967-gram train+xin+afp 60.5 52.95 45.96Table 4: Improving MT performance with largerLMs trained on more training data and using higherorder of n-grams (Px denotes perplexity).performance we obtained is a BLEU score of 46.02(case insensitive) on the most recent eval06 test set.This compares favorably to the best score of 42.81(case sensitive), obtained in 2006 by Google.
Case-sensitive scoring would drop our score by about 2-3BLEU points.To assess the utility of re-ranking with large LMs,we carried out a number of experiments, summa-rized in Table 4.
We used the English side of the par-allel training corpus and the Gigaword corpus dis-tributed by the LDC for language modeling.
SeeTable 3 for the size of these corpora.
While thisputs us into the moderate billion word range of largeLMs, it nevertheless stresses our resources to thelimit.
The largest LMs that we are able to supportwithin 3 GB of memory are a 3-gram model trainedon all the data, or a 4-gram model trained only ontrain+afp+xin.
On disk, these models take up 1.7 GBcompressed (gzip) in the standard ARPA format.
Allthese LMs are interpolated by optimizing perplexityon the tuning set (eval04).The baseline result is a BLEU score of 43.69 us-ing a 3-gram trained on train+afp+xin.
This can beslightly improved by using either a 3-gram trainedon all data (BLEU score of 43.99) or by using a4-gram trained on train+afp+xin (BLEU score of43.90).
We were not able to use a 4-gram trained onall data during the search.
Such a model would takemore than 6GB on disk.
An option would be to trainthe model on all the data and to prune or quantizeit in order to fit in the available memory.
This maygive better results than limiting the training data.Next, we examine if we can get significantly bet-ter performance using different LMs in re-ranking.665To this end, we train continuous space 5-gram to 7-gram LMs and re-rank a 1000-best list (without du-plicate translations) provided by the decoder usingthe 4-gram LM.
The CSLM was trained on the samedata as the back-off LMs.
It yields an improvementin perplexity of about 17% relative.With various higher order n-grams models, weobtain significant gains, up to just over 46 BLEUon the 2006 NIST evaluation set.
A gain of over2 BLEU points underscores the potential for re-ranking with large LM, even when the baseline LMwas already trained on a large corpus.
Note also thegood generalization behavior of this approach : thegain obtained on the test data matches or exceeds inmost cases the improvements obtained on the devel-opment data.
The CSLM is also very memory ef-ficient since it uses a distributed representation thatdoes not increase with the size of training materialused.
Overall, about 1GB of main memory is used.7 DiscussionIn this paper we examined a number of issues re-garding the role of LMs in large-scale SMT sys-tems.
We compared methods to combine trainingdata from diverse corpora and showed that interpo-lation of LMs by optimizing perplexity yields simi-lar results to combining them as feature functions inthe log-linear model.We applied for the first time continuous spaceLMs to the large-scale Arabic?English NIST eval-uation task.
We obtained large improvements (over2 BLEU points) over a strong baseline, thus validat-ing both continuous space LMs and re-ranking as amethod to exploit large LMs.AcknowledgmentsThis work has been partially funded by the FrenchGovernment under the project INSTAR (ANRJCJC06 143038) and the DARPA Gale program,Contrat No.
HR0011-06-C-0022 and the Euro-Matrix funded by the European Commission (6thFramework Programme).ReferencesYoshua Bengio, Rejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3(2):1137?1155.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In EMNLP, pages 858?867.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)evaluation of machine translation.
In Second Work-shop on SMT, pages 136?158.Daniel De?chelotte, Holger Schwenk, Hlne Bonneau-Maynard, Alexandre Allauzen, and Gilles Adda.2007.
A state-of-the-art statistical machine translationsystem based on Moses.
In MT Summit.Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.2007.
Large-scale distributed language modeling.
InICASSP.Marcello Federico and Nicola Bertoldi.
2006.
Howmany bits are needed to store probabilities for phrase-based translation?
In First Workshop on SMT, pages94?101.Philipp Koehn and Christof Monz.
2006.
Manual and au-tomatic evaluation of machine translation between Eu-ropean languages.
In First Workshop on SMT, pages102?121.Philipp Koehn et al 2007.
Moses: Open source toolkitfor statistical machine translation.
In ACL Demo andPoster Sessions, pages 177?180, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Holger Schwenk, Marta R. Costa-jussa`, and Jose?
A. R.Fonollosa.
2006.
Continuous space language modelsfor the IWSLT 2006 task.
In IWSLT, pages 166?173.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21:492?518.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In ICSLP, pages II: 901?904.David Talbot and Miles Osborne.
2007a.
Randomisedlanguage modelling for statistical machine translation.In ACL, pages 512?519.David Talbot and Miles Osborne.
2007b.
SmoothedBloom filter language models: Tera-scale LMs on thecheap.
In EMNLP, pages 468?476.Richard Zens and Hermann Ney.
2007.
Efficient phrase-table representation for machine translation with appli-cations to online MT and speech translation.
In NACL,pages 492?499.Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.2006.
Distributed language modeling for n-best listre-ranking.
In EMNLP, pages 216?223.666
