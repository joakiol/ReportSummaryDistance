Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41?48,Sydney, July 2006. c?2006 Association for Computational LinguisticsEvaluating the Accuracy of an UnlexicalizedStatistical Parser on the PARC DepBankTed BriscoeComputer LaboratoryUniversity of CambridgeJohn CarrollSchool of InformaticsUniversity of SussexAbstractWe evaluate the accuracy of an unlexi-calized statistical parser, trained on 4Ktreebanked sentences from balanced dataand tested on the PARC DepBank.
Wedemonstrate that a parser which is compet-itive in accuracy (without sacrificing pro-cessing speed) can be quickly tuned with-out reliance on large in-domain manually-constructed treebanks.
This makes it morepractical to use statistical parsers in ap-plications that need access to aspects ofpredicate-argument structure.
The com-parison of systems using DepBank is notstraightforward, so we extend and validateDepBank and highlight a number of repre-sentation and scoring issues for relationalevaluation schemes.1 IntroductionConsiderable progress has been made in accu-rate statistical parsing of realistic texts, yield-ing rooted, hierarchical and/or relational repre-sentations of full sentences.
However, muchof this progress has been made with systemsbased on large lexicalized probabilistic context-free like (PCFG-like) models trained on the WallStreet Journal (WSJ) subset of the Penn Tree-Bank (PTB).
Evaluation of these systems has beenmostly in terms of the PARSEVAL scheme usingtree similarity measures of (labelled) precision andrecall and crossing bracket rate applied to section23 of the WSJ PTB.
(See e.g.
Collins (1999) fordetailed exposition of one such very fruitful lineof research.
)We evaluate the comparative accuracy of an un-lexicalized statistical parser trained on a smallertreebank and tested on a subset of section 23 ofthe WSJ using a relational evaluation scheme.
Wedemonstrate that a parser which is competitivein accuracy (without sacrificing processing speed)can be quickly developed without reliance on largein-domain manually-constructed treebanks.
Thismakes it more practical to use statistical parsers indiverse applications needing access to aspects ofpredicate-argument structure.We define a lexicalized statistical parser as onewhich utilizes probabilistic parameters concerninglexical subcategorization and/or bilexical relationsover tree configurations.
Current lexicalized sta-tistical parsers developed, trained and tested onPTB achieve a labelled F1-score ?
the harmonicmean of labelled precision and recall ?
of around90%.
Klein and Manning (2003) argue that suchresults represent about 4% absolute improvementover a carefully constructed unlexicalized PCFG-like model trained and tested in the same man-ner.1 Gildea (2001) shows that WSJ-derived bilex-ical parameters in Collins?
(1999) Model 1 parsercontribute less than 1% to parse selection accu-racy when test data is in the same domain, andyield no improvement for test data selected fromthe Brown Corpus.
Bikel (2004) shows that, inCollins?
(1999)Model 2, bilexical parameters con-tribute less than 0.5% to accuracy on in-domaindata while lexical subcategorization-like parame-ters contribute just over 1%.Several alternative relational evaluationschemes have been developed (e.g.
Carroll et al,1998; Lin, 1998).
However, until recently, noWSJ data has been carefully annotated to supportrelational evaluation.
King et al (2003) describethe PARC 700 Dependency Bank (hereinafterDepBank), which consists of 700 WSJ sentencesrandomly drawn from section 23.
These sentenceshave been annotated with syntactic features andwith bilexical head-dependent relations derivedfrom the F-structure representation of LexicalFunctional Grammar (LFG).
DepBank facilitates1Klein and Manning retained some functional tag infor-mation from PTB, so it could be argued that their model re-mains ?mildly?
lexicalized since functional tags encode somesubcategorization information.41comparison of PCFG-like statistical parsersdeveloped from the PTB with other parsers whoseoutput is not designed to yield PTB-style trees,using an evaluation which is closer to the protypi-cal parsing task of recovering predicate-argumentstructure.Kaplan et al (2004) compare the accuracy andspeed of the PARC XLE Parser to Collins?
Model3 parser.
They develop transformation rules forboth, designed to map native output to a subset ofthe features and relations in DepBank.
They com-pare performance of a grammatically cut-downand complete version of the XLE parser to thepublically available version of Collins?
parser.One fifth of DepBank is held out to optimize thespeed and accuracy of the three systems.
Theyconclude from the results of these experiments thatthe cut-down XLE parser is two-thirds the speedof Collins?
Model 3 but 12% more accurate, whilethe complete XLE system is 20% more accuratebut five times slower.
F1-score percentages rangefrom the mid- to high-70s, suggesting that the re-lational evaluation is harder than PARSEVAL.Both Collins?
Model 3 and the XLE Parser uselexicalized models for parse selection trained onthe rest of the WSJ PTB.
Therefore, although Ka-plan et al demonstrate an improvement in accu-racy at some cost to speed, there remain questionsconcerning viability for applications, at some re-move from the financial news domain, for whichsubstantial treebanks are not available.
The parserwe deploy, like the XLE one, is based on amanually-defined feature-based unification gram-mar.
However, the approach is somewhat differ-ent, making maximal use of more generic struc-tural rather than lexical information, both withinthe grammar and the probabilistic parse selectionmodel.
Here we compare the accuracy of ourparser with Kaplan et al?s results, by repeatingtheir experiment with our parser.
This compari-son is not straightforward, given both the system-specific nature of some of the annotation in Dep-Bank and the scoring reported.
We, therefore, ex-tend DepBank with a set of grammatical relationsderived from our own system output and highlighthow issues of representation and scoring can affectresults and their interpretation.In ?2, we describe our development method-ology and the resulting system in greater detail.
?3 describes the extended Depbank that we havedeveloped and motivates our additions.
?2.4 dis-cusses how we trained and tuned our current sys-tem and describes our limited use of informationderived from WSJ text.
?4 details the various ex-periments undertaken with the extended DepBankand gives detailed results.
?5 discusses these re-sults and proposes further lines of research.2 Unlexicalized Statistical Parsing2.1 System ArchitectureBoth the XLE system and Collins?
Model 3 pre-process textual input before parsing.
Similarly,our baseline system consists of a pipeline of mod-ules.
First, text is tokenized using a deterministicfinite-state transducer.
Second, tokens are part-of-speech and punctuation (PoS) tagged using a 1st-order Hidden Markov Model (HMM) utilizing alexicon of just over 50K words and an unknownword handling module.
Third, deterministic mor-phological analysis is performed on each token-tag pair with a finite-state transducer.
Fourth, thelattice of lemma-affix-tags is parsed using a gram-mar over such tags.
Finally, the n-best parses arecomputed from the parse forest using a probabilis-tic parse selection model conditioned on the struc-tural parse context.
The output of the parser can bedisplayed as syntactic trees, and/or factored into asequence of bilexical grammatical relations (GRs)between lexical heads and their dependents.The full system can be extended in a variety ofways ?
for example, by pruning PoS tags but al-lowing multiple tag possibilities per word as in-put to the parser, by incorporating lexical subcate-gorization into parse selection, by computing GRweights based on the proportion and probabilityof the n-best analyses yielding them, and so forth?
broadly trading accuracy and greater domain-dependence against speed and reduced sensitivityto domain-specific lexical behaviour (Briscoe andCarroll, 2002; Carroll and Briscoe, 2002; Watsonet al, 2005; Watson, 2006).
However, in this pa-per we focus exclusively on the baseline unlexical-ized system.2.2 Grammar DevelopmentThe grammar is expressed in a feature-based, uni-fication formalism.
There are currently 676 phrasestructure rule schemata, 15 feature propagationrules, 30 default feature value rules, 22 categoryexpansion rules and 41 feature types which to-gether define 1124 compiled phrase structure rulesin which categories are represented as sets of fea-42tures, that is, attribute-value pairs, possibly withvariable values, possibly bound between motherand one or more daughter categories.
142 of thephrase structure schemata are manually identifiedas peripheral rather than core rules of Englishgrammar.
Categories are matched using fixed-arity term unification at parse time.The lexical categories of the grammar consistof feature-based descriptions of the 149 PoS tagsand 13 punctuation tags (a subset of the CLAWStagset, see e.g.
Sampson, 1995) which constitutethe preterminals of the grammar.
The numberof distinct lexical categories associated with eachpreterminal varies from 1 for some function wordsthrough to around 35 as, for instance, tags for mainverbs are associated with a VSUBCAT attribute tak-ing 33 possible values.
The grammar is designedto enumerate possible valencies for predicates byincluding separate rules for each pattern of pos-sible complementation in English.
The distinc-tion between arguments and adjuncts is expressedby adjunction of adjuncts to maximal projections(XP ?
XP Adjunct) as opposed to government ofarguments (i.e.
arguments are sisters within X1projections; X1 ?
X0 Arg1.
.
.
ArgN).Each phrase structure schema is associated withone or more GR specifications which can be con-ditioned on feature values instantiated at parsetime and which yield a rule-to-rule mapping fromlocal trees to GRs.
The set of GRs associated witha given derivation define a connected, directedgraph with individual nodes representing lemma-affix-tags and arcs representing named grammati-cal relations.
The encoding of this mapping withinthe grammar is similar to that of F-structure map-ping in LFG.
However, the connected graph is notconstructed and completeness and coherence con-straints are not used to filter the phrase structurederivation space.The grammar finds at least one parse rooted inthe start category for 85% of the Susanne treebank,a 140K word balanced subset of the Brown Cor-pus, which we have used for development (Samp-son, 1995).
Much of the remaining data consistsof phrasal fragments marked as independent textsentences, for example in dialogue.
Grammati-cal coverage includes the majority of constructiontypes of English, however the handling of someunbounded dependency constructions, particularlycomparatives and equatives, is limited because ofthe lack of fine-grained subcategorization infor-mation in the PoS tags and by the need to balancedepth of analysis against the size of the deriva-tion space.
On the Susanne corpus, the geometricmean of the number of analyses for a sentence oflength n is 1.31n.
The microaveraged F1-score forGR extraction on held-out data from Susanne is76.5% (see section 4.2 for details of the evaluationscheme).The system has been used to analyse about 150million words of English text drawn primarilyfrom the PTB, TREC, BNC, and Reuters RCV1datasets in connection with a variety of projects.The grammar and PoS tagger lexicon have beenincrementally improved by manually examiningcases of parse failure on these datasets.
How-ever, the effort invested amounts to a few days?effort for each new dataset as opposed to the maingrammar development effort, centred on Susanne,which has extended over some years and nowamounts to about 2 years?
effort (see Briscoe, 2006for further details).2.3 ParserTo build the parsing module, the unification gram-mar is automatically converted into an atomic-categoried context free ?backbone?, and a non-deterministic LALR(1) table is constructed fromthis, which is used to drive the parser.
The residueof features not incorporated into the backboneare unified on each rule application (reduce ac-tion).
In practice, the parser takes average timeroughly quadratic in the length of the input to cre-ate a packed parse forest represented as a graph-structured stack.
The statistical disambiguationphase is trained on Susanne treebank bracketings,producing a probabilistic generalized LALR(1)parser (e.g.
Inui et al, 1997) which associatesprobabilities with alternative actions in the LR ta-ble.The parser is passed as input the sequence ofmost probable lemma-affix-tags found by the tag-ger.
During parsing, probabilities are assignedto subanalyses based on the the LR table actionsthat derived them.
The n-best (i.e.
most proba-ble) parses are extracted by a dynamic program-ming procedure over subanalyses (represented bynodes in the parse forest).
The search is effi-cient since probabilities are associated with singlenodes in the parse forest and no weight functionover ancestor or sibling nodes is needed.
Proba-bilities capture structural context, since nodes in43the parse forest partially encode a configuration ofthe graph-structured stack and lookahead symbol,so that, unlike a standard PCFG, the model dis-criminates between derivations which only differin the order of application of the same rules andalso conditions rule application on the PoS tag ofthe lookahead token.When there is no parse rooted in the start cat-egory, the parser returns a connected sequenceof partial parses which covers the input basedon subanalysis probability and a preference forlonger and non-lexical subanalysis combinations(e.g.
Kiefer et al, 1999).
In these cases, the GRgraph will not be fully connected.2.4 Tuning and Training MethodThe HMM tagger has been trained on 3M wordsof balanced text drawn from the LOB, BNC andSusanne corpora, which are available with hand-corrected CLAWS tags.
The parser has beentrained from 1.9K trees for sentences from Su-sanne that were interactively parsed to manuallyobtain the correct derivation, and also from 2.1Kfurther sentences with unlabelled bracketings de-rived from the Susanne treebank.
These brack-etings guide the parser to one or possibly sev-eral closely-matching derivations and these areused to derive probabilities for the LR table us-ing (weighted) Laplace estimation.
Actions in thetable involving rules marked as peripheral are as-signed a uniform low prior probability to ensurethat derivations involving such rules are consis-tently lower ranked than those involving only corerules.To improve performance onWSJ text, we exam-ined some parse failures from sections other thansection 23 to identify patterns of consistent fail-ure.
We then manually modified and extended thegrammar with a further 6 rules, mostly to handlecases of indirect and direct quotation that are verycommon in this dataset.
This involved 3 days?work.
Once completed, the parser was retrainedon the original data.
A subsequent limited inspec-tion of top-ranked parses led us to disable 6 ex-isting rules which applied too freely to the WSJtext; these were designed to analyse auxiliary el-lipsis which appears to be rare in this genre.
Wealso catalogued incorrect PoS tags fromWSJ parsefailures and manually modified the tagger lexiconwhere appropriate.
These modifications mostlyconsisted of adjusting lexical probabilities of ex-tant entries with highly-skewed distributions.
Wealso added some tags to extant entries for infre-quent words.
These modifications took a furtherday.
The tag transition probabilities were not rees-timated.
Thus, we have made no use of the PTBitself and only limited use of WSJ text.This method of grammar and lexicon devel-opment incrementally improves the overall per-formance of the system averaged across all thedatasets that it has been applied to.
It is verylikely that retraining the PoS tagger on the WSJand retraining the parser using PTB would yielda system which would perform more effectivelyon DepBank.
However, one of our goals is todemonstrate that an unlexicalized parser trainedon a modest amount of annotated text from othersources, coupled to a tagger also trained ongeneric, balanced data, can perform competitivelywith systems which have been (almost) entirelydeveloped and trained using PTB, whether or notthese systems deploy hand-crafted grammars orones derived automatically from treebanks.3 Extending and Validating DepBankDepBank was constructed by parsing the selectedsection 23 WSJ sentences with the XLE systemand outputting syntactic features and bilexical re-lations from the F-structure found by the parser.These features and relations were subsequentlychecked, corrected and extended interactively withthe aid of software tools (King et al, 2003).The choice of relations and features is basedquite closely on LFG and, in fact, overlaps sub-stantially with the GR output of our parser.
Fig-ure 1 illustrates some DepBank annotations usedin the experiment reported by Kaplan et al andour hand-corrected GR output for the exampleTen of the nation?s governors meanwhile calledon the justices to reject efforts to limit abortions.We have kept the GR representation simpler andmore readable by suppressing lemmatization, to-ken numbering and PoS tags, but have left theDepBank annotations unmodified.The example illustrates some differences be-tween the schemes.
For instance, the subj andncsubj relations overlap as both annotations con-tain such a relation between call(ed) and Ten), butthe GR annotation also includes this relation be-tween limit and effort(s) and reject and justice(s),while DepBank links these two verbs to a variablepro.
This reflects a difference of philosophy about44DepBank: obl(call?0, on?2)stmt_type(call?0, declarative)subj(call?0, ten?1)tense(call?0, past)number_type(ten?1, cardinal)obl(ten?1, governor?35)obj(on?2, justice?30)obj(limit?7, abortion?15)subj(limit?7, pro?21)obj(reject?8, effort?10)subj(reject?8, pro?27)adegree(meanwhile?9, positive)num(effort?10, pl)xcomp(effort?10, limit?7)GR: (ncsubj called Ten _)(ncsubj reject justices _)(ncsubj limit efforts _)(iobj called on)(xcomp to called reject)(dobj reject efforts)(xmod to efforts limit)(dobj limit abortions)(dobj on justices)(det justices the)(ta bal governors meanwhile)(ncmod poss governors nation)(iobj Ten of)(dobj of governors)(det nation the)Figure 1: DepBank and GR annotations.resolution of such ?understood?
relations in differ-ent constructions.
Viewed as output appropriate tospecific applications, either approach is justifiable.However, for evaluation, these DepBank relationsadd little or no information not already specifiedby the xcomp relations in which these verbs alsoappear as dependents.
On the other hand, Dep-Bank includes an adjunct relation between mean-while and call(ed), while the GR annotation treatsmeanwhile as a text adjunct (ta) of governors, de-limited by balanced commas, following Nunberg?s(1990) text grammar but conveying less informa-tion here.There are also issues of incompatible tokeniza-tion and lemmatization between the systems andof differing syntactic annotation of similar infor-mation, which lead to problems mapping betweenour GR output and the current DepBank.
Finally,differences in the linguistic intuitions of the an-notators and errors of commission or omissionon both sides can only be uncovered by manualcomparison of output (e.g.
xmod vs. xcomp forlimit efforts above).
Thus we reannotated the Dep-Bank sentences with GRs using our current sys-tem, and then corrected and extended this anno-tation utilizing a software tool to highlight dif-ferences between the extant annotations and ourown.2 This exercise, though time-consuming, un-covered problems in both annotations, and yieldsa doubly-annotated and potentially more valuableresource in which annotation disagreements overcomplex attachment decisions, for instance, can beinspected.The GR scheme includes one feature in Dep-Bank (passive), several splits of relations in Dep-Bank, such as adjunct, adds some of DepBank?sfeatural information, such as subord form, as asubtype slot of a relation (ccomp), merges Dep-Bank?s oblique with iobj, and so forth.
But itdoes not explicitly include all the features of Dep-Bank or even of the reduced set of semantically-relevant features used in the experiments and eval-uation reported in Kaplan et al.
Most of thesefeatures can be computed from the full GR repre-sentation of bilexical relations between numberedlemma-affix-tags output by the parser.
For in-stance, num features, such as the plurality of jus-tices in the example, can be computed from thefull det GR (det justice+s NN2:4 the AT:3)based on the CLAWS tag (NN2 indicating ?plu-ral?)
selected for output.
The few features that can-not be computed from GRs and CLAWS tags di-rectly, such as stmt type, could be computed fromthe derivation tree.4 Experiments4.1 Experimental DesignWe selected the same 560 sentences as test data asKaplan et al, and all modifications that we madeto our system (see ?2.4) were made on the basisof (very limited) information from other sectionsof WSJ text.3 We have made no use of the further140 held out sentences in DepBank.
The resultswe report below are derived by choosing the mostprobable tag for each word returned by the PoStagger and by choosing the unweighted GR set re-turned for the most probable parse with no lexicalinformation guiding parse ranking.4.2 ResultsOur parser produced rooted sentential analyses for84% of the test items; actual coverage is higher2The new version of DepBank along with evaluationsoftware is included in the current RASP distribution:www.informatics.susx.ac.uk/research/nlp/rasp3The PARC group kindly supplied us with the experimen-tal data files they used to facilitate accurate reproduction ofthis experiment.45Relation Precision Recall F1 P R F1 Relationmod 75.4 71.2 73.3ncmod 72.9 67.9 70.3xmod 47.7 45.5 46.6cmod 51.4 31.6 39.1pmod 30.8 33.3 32.0det 88.7 91.1 89.9arg mod 71.9 67.9 69.9arg 76.0 73.4 74.6subj 80.1 66.6 72.7 73 73 73ncsubj 80.5 66.8 73.0xsubj 50.0 28.6 36.4csubj 20.0 50.0 28.6subj or dobj 82.1 74.9 78.4comp 74.5 76.4 75.5obj 78.4 77.9 78.1dobj 83.4 81.4 82.4 75 75 75 objobj2 24.2 38.1 29.6 42 36 39 obj-thetaiobj 68.2 68.1 68.2 64 83 72 oblclausal 63.5 71.6 67.3xcomp 75.0 76.4 75.7 74 73 74ccomp 51.2 65.6 57.5 78 64 70 comppcomp 69.6 66.7 68.1aux 92.8 90.5 91.6conj 71.7 71.0 71.4 68 62 65ta 39.1 48.2 43.2passive 93.6 70.6 80.5 80 83 82adegree 89.2 72.4 79.9 81 72 76coord form 92.3 85.7 88.9 92 93 93num 92.2 89.8 91.0 86 87 86number type 86.3 92.7 89.4 96 95 96precoord form 100.0 16.7 28.6 100 50 67pron form 92.1 91.9 92.0 88 89 89prt form 71.1 58.7 64.3 72 65 68subord form 60.7 48.1 53.6macroaverage 69.0 63.4 66.1microaverage 81.5 78.1 79.7 80 79 79Table 1: Accuracy of our parser, and whereroughly comparable, the XLE as reported by Kinget althan this since some of the test sentences are el-liptical or fragmentary, but in many cases are rec-ognized as single complete constituents.
Kaplanet al report that the complete XLE system findsrooted analyses for 79% of section 23 of the WSJbut do not report coverage just for the test sen-tences.
The XLE parser uses several performanceoptimizations which mean that processing of sub-analyses in longer sentences can be curtailed orpreempted, so that it is not clear what proportionof the remaining data is outside grammatical cov-erage.Table 1 shows accuracy results for each indi-vidual relation and feature, starting with the GRbilexical relations in the extended DepBank andfollowed by most DepBank features reported byKaplan et al, and finally overall macro- and mi-croaverages.
The macroaverage is calculated bytaking the average of each measure for each indi-vidual relation and feature; the microaverage mea-sures are calculated from the counts for all rela-tions and features.4 Indentation of GRs showsdegree of specificity of the relation.
Thus, modscores are microaveraged over the counts for thefive fully specified modifier relations listed imme-diately after it in Table 1.
This allows comparisonof overall accuracy on modifiers with, for instanceoverall accuracy on arguments.
Figures in italicsto the right are discussed in the next section.Kaplan et al?s microaveraged scores forCollins?
Model 3 and the cut-down and completeversions of the XLE parser are given in Table 2,along with the microaveraged scores for our parserfrom Table 1.
Our system?s accuracy results (eval-uated on the reannotated DepBank) are better thanthose for Collins and the cut-down XLE, and verysimilar overall to the complete XLE (evaluatedon DepBank).
Speed of processing is also verycompetitive.5 These results demonstrate that astatistical parser with roughly state-of-the-art ac-curacy can be constructed without the need forlarge in-domain treebanks.
However, the perfor-mance of the system, as measured by microrav-eraged F1-score on GR extraction alone, has de-clined by 2.7% over the held-out Susanne data,so even the unlexicalized parser is by no meansdomain-independent.4.3 Evaluation IssuesThe DepBank num feature on nouns is evalu-ated by Kaplan et al on the grounds that it issemantically-relevant for applications.
There areover 5K num features in DepBank so the overallmicroaveraged scores for a system will be signifi-cantly affected by accuracy on num.
We expectedour system, which incorporates a tagger with goodempirical (97.1%) accuracy on the test data, to re-cover this feature with 95% accuracy or better, asit will correlate with tags NNx1 and NNx2 (where?x?
represents zero or more capitals in the CLAWS4We did not compute the remaining DepBank featuresstmt type, tense, prog or perf as these rely on informationthat can only be extracted from the derivation tree rather thanthe GR set.5Processing time for our system was 61 seconds on one2.2GHz Opteron CPU (comprising tokenization, tagging,morphology, and parsing, including module startup over-heads).
Allowing for slightly different CPUs, this is 2.5?10times faster than the Collins and XLE parsers, as reported byKaplan et al46System Eval corpus Precision Recall F1Collins DepBank 78.3 71.2 74.6Cut-down XLE DepBank 79.1 76.2 77.6Complete XLE DepBank 79.4 79.8 79.6Our system DepBank/GR 81.5 78.1 79.7Table 2: Microaveraged overall scores from Kaplan et al and for our system.tagset).
However, DepBank treats the majorityof prenominal modifiers as adjectives rather thannouns and, therefore, associates them with an ade-gree rather than a num feature.
The PoS tag se-lected depends primarily on the relative lexicalprobabilities of each tag for a given lexical itemrecorded in the tagger lexicon.
But, regardlessof this lexical decision, the correct GR is recov-ered, and neither adegree(positive) or num(sg)add anything semantically-relevant when the lex-ical item is a nominal premodifier.
A strategywhich only provided a num feature for nominalheads would be both more semantically-relevantand would also yield higher precision (95.2%).However, recall (48.4%) then suffers against Dep-Bank as noun premodifiers have a num feature.Therefore, in the results presented in Table 1 wehave not counted cases where either DepBank orour system assign a premodifier adegree(positive)or num(sg).There are similar issues with other DepBankfeatures and relations.
For instance, the form ofa subordinator with clausal complements is anno-tated as a relation between verb and subordina-tor, while there is a separate comp relation be-tween verb and complement head.
The GR rep-resentation adds the subordinator as a subtype ofccomp recording essentially identical informationin a single relation.
So evaluation scores based onaggregated counts of correct decisions will be dou-bled for a system which structures this informa-tion as in DepBank.
However, reproducing the ex-act DepBank subord form relation from the GRccomp one is non-trivial because DepBank treatsmodal auxiliaries as syntactic heads while the GR-scheme treats the main verb as head in all ccomprelations.
We have not attempted to compensatefor any further such discrepancies other than theone discussed in the previous paragraph.
However,we do believe that they collectively damage scoresfor our system.As King et al note, it is difficult to identifysuch informational redundancies to avoid double-counting and to eradicate all system specific bi-ases.
However, reporting precision, recall and F1-scores for each relation and feature separately andmicroaveraging these scores on the basis of a hi-erarchy, as in our GR scheme, ameliorates manyof these problems and gives a better indicationof the strengths and weaknesses of a particularparser, which may also be useful in a decisionabout its usefulness for a specific application.
Un-fortunately, Kaplan et al do not report their re-sults broken down by relation or feature so it isnot possible, for example, on the basis of the ar-guments made above, to choose to compare theperformance of our system on ccomp to theirs forcomp, ignoring subord form.
King et al do re-port individual results for selected features and re-lations from an evaluation of the complete XLEparser on all 700 DepBank sentences with an al-most identical overall microaveraged F1 score of79.5%, suggesting that these results provide a rea-sonably accurate idea of the XLE parser?s relativeperformance on different features and relations.Where we believe that the information capturedby a DepBank feature or relation is roughly com-parable to that expressed by a GR in our extendedDepBank, we have included King et al?s scoresin the rightmost column in Table 1 for compari-son purposes.
Even if these features and relationswere drawn from the same experiment, however,they would still not be exactly comparable.
For in-stance, as discussed in ?3 nearly half (just over 1K)the DepBank subj relations include pro as one el-ement, mostly double counting a correspondingxcomp relation.
On the other hand, our ta rela-tion syntactically underspecifies many DepBankadjunct relations.
Nevertheless, it is possible tosee, for instance, that while both parsers performbadly on second objects ours is worse, presumablybecause of lack of lexical subcategorization infor-mation.475 ConclusionsWe have demonstrated that an unlexicalized parserwith minimal manual modification for WSJ text ?but no tuning of performance to optimize on thisdataset alne, and no use of PTB ?
can achieveaccuracy competitive with parsers employing lex-icalized statistical models trained on PTB.We speculate that we achieve these results be-cause our system is engineered to make minimaluse of lexical information both in the grammar andin parse ranking, because the grammar has beendeveloped to constrain ambiguity despite this lackof lexical information, and because we can com-pute the full packed parse forest for all the test sen-tences efficiently (without sacrificing speed of pro-cessing with respect to other statistical parsers).These advantages appear to effectively offset thedisadvantage of relying on a coarser, purely struc-tural model for probabilistic parse selection.
In fu-ture work, we hope to improve the accuracy of thesystem by adding lexical information to the statis-tical parse selection component without exploitingin-domain treebanks.Clearly, more work is needed to enable moreaccurate, informative, objective and wider com-parison of extant parsers.
More recent PTB-basedparsers show small improvements over Collins?Model 3 using PARSEVAL, while Clark and Cur-ran (2004) and Miyao and Tsujii (2005) report84% and 86.7% F1-scores respectively for theirown relational evaluations on section 23 of WSJ.However, it is impossible to meaningfully com-pare these results to those reported here.
The rean-notated DepBank potentially supports evaluationswhich score according to the degree of agreementbetween this and the original annotation and/or de-velopment of future consensual versions throughcollaborative reannotation by the research com-munity.
We have also highlighted difficulties forrelational evaluation schemes and argued that pre-senting individual scores for (classes of) relationsand features is both more informative and facili-tates system comparisons.6 ReferencesBikel, D.. 2004.
Intricacies of Collins?
parsing model, Com-putational Linguistics, 30(4):479?512.Briscoe, E.J.. 2006.
An introduction to tag sequence gram-mars and the RASP system parser, University of Cam-bridge, Computer Laboratory Technical Report 662.Briscoe, E.J.
and J. Carroll.
2002.
Robust accurate statisticalannotation of general text.
In Proceedings of the 3rd Int.Conf.
on Language Resources and Evaluation (LREC),Las Palmas, Gran Canaria.
1499?1504.Carroll, J. and E.J.
Briscoe.
2002.
High precision extractionof grammatical relations.
In Proceedings of the 19th Int.Conf.
on Computational Linguistics (COLING), Taipei,Taiwan.
134?140.Carroll, J., E. Briscoe and A. Sanfilippo.
1998.
Parser evalu-ation: a survey and a new proposal.
In Proceedings of the1st International Conference on Language Resources andEvaluation, Granada, Spain.
447?454.Clark, S. and J. Curran.
2004.
The importance of supertag-ging for wide-coverage CCG parsing.
In Proceedings ofthe 20th International Conference on Computational Lin-guistics (COLING-04), Geneva, Switzerland.
282?288.Collins, M.. 1999.
Head-driven Statistical Models for Nat-ural Language Parsing.
PhD Dissertation, Computer andInformation Science, University of Pennsylvania.Gildea, D.. 2001.
Corpus variation and parser performance.In Proceedings of the Empirical Methods in Natural Lan-guage Processing (EMNLP?01), Pittsburgh, PA.Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga.1997.
A new formalization of probabilistic GLR parsing.In Proceedings of the 5th International Workshop on Pars-ing Technologies (IWPT?97), Boston, MA.
123?134.Kaplan, R., S. Riezler, T. H. King, J. Maxwell III, A. Vasser-man and R. Crouch.
2004.
Speed and accuracy in shal-low and deep stochastic parsing.
In Proceedings of theHLT Conference and the 4th Annual Meeting of the NorthAmerican Chapter of the ACL (HLT-NAACL?04), Boston,MA.Kiefer, B., H-U.
Krieger, J. Carroll and R. Malouf.
1999.A bag of useful techniques for efficient and robust pars-ing.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics, University ofMaryland.
473?480.King, T. H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-plan.
2003.
The PARC700 Dependency Bank.
In Pro-ceedings of the 4th International Workshop on Linguisti-cally Interpreted Corpora (LINC-03), Budapest, Hungary.Klein, D. and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, Sapporo,Japan.
423?430.Lin, D.. 1998.
Dependency-based evaluation of MINIPAR.In Proceedings of the Workshop at LREC?98 on The Eval-uation of Parsing Systems, Granada, Spain.Manning, C. and H. Schu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press, Cambridge,MA.Miyao, Y. and J. Tsujii.
2005.
Probabilistic disambiguationmodels for wide-coverage HPSG parsing.
In Proceedingsof the 43rd Annual Meeting of the Association for Compu-tational Linguistics, Ann Arbor, MI.
83?90.Nunberg, G.. 1990.
The Linguistics of Punctuation.
CSLILecture Notes 18, Stanford, CA.Sampson, G.. 1995.
English for the Computer.
Oxford Uni-versity Press, Oxford, UK.Watson, R.. 2006.
Part-of-speech tagging models for parsing.In Proceedings of the 9th Conference of ComputationalLinguistics in the UK (CLUK?06), Open University, Mil-ton Keynes.Watson, R., J. Carroll and E.J.
Briscoe.
2005.
Efficient ex-traction of grammatical relations.
In Proceedings of the9th Int.
Workshop on Parsing Technologies (IWPT?05),Vancouver, Ca..48
