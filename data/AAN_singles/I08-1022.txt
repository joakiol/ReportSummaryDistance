Cross Language Text Categorization Using a Bilingual LexiconKe Wu, Xiaolin Wang and Bao-Liang Lu?Department of Computer Science and Engineering, Shanghai Jiao Tong University800 Dong Chuan Rd., Shanghai 200240, China{wuke,arthur general,bllu}@sjtu.edu.cnAbstractWith the popularity of the Internet at a phe-nomenal rate, an ever-increasing number ofdocuments in languages other than Englishare available in the Internet.
Cross lan-guage text categorization has attracted moreand more attention for the organization ofthese heterogeneous document collections.In this paper, we focus on how to con-duct effective cross language text catego-rization.
To this end, we propose a crosslanguage naive Bayes algorithm.
The pre-liminary experiments on collected documentcollections show the effectiveness of the pro-posed method and verify the feasibility ofachieving performance close to monolingualtext categorization, using a bilingual lexiconalone.
Also, our algorithm is more efficientthan our baselines.1 IntroductionDue to the popularity of the Internet, an ever-increasing number of documents in languages otherthan English are available in the Internet.
The or-ganization of these heterogeneous document collec-tions increases cost of human labor significantly.
Onthe one hand, experts who know different languagesare required to organize these collections.
On theother hand, maybe there exist a large amount of la-belled documents in a language (e.g.
English) whichare in the same class structure as the unlabelled doc-uments in another language.
As a result, how to ex-?Corresponding author.ploit the existing labelled documents in some lan-guage (e.g.
English) to classify the unlabelled doc-uments other than the language in multilingual sce-nario has attracted more and more attention (Bel etal., 2003; Rigutini et al, 2005; Olsson et al, 2005;Fortuna and Shawe-Taylor, 2005; Li and Shawe-Taylor, 2006; Gliozzo and Strapparava, 2006).
Werefer to this task as cross language text categoriza-tion.
It aims to extend the existing automated textcategorization system from one language to otherlanguages without additional intervention of humanexperts.
Formally, given two document collections{De,Df} from two different languages e and f re-spectively, we use the labelled document collectionDe in the language e to deduce the labels of the doc-ument collection Df in the language f via an algo-rithm A and some external bilingual resources.Typically, some external bilingual lexical re-sources, such as machine translation system (MT),large-scale parallel corpora and multilingual ontol-ogy etc., are used to alleviate cross language textcategorization.
However, it is hard to obtain themfor many language pairs.
In this paper, we focus onusing a cheap bilingual resource, e.g.
bilingual lexi-con without any translation information, to conductcross language text categorization.
To my knowl-edge, there is little research on using a bilingual lex-icon alone for cross language text categorization.In this paper, we propose a novel approach forcross language text categorization via a bilinguallexicon alone.
We call this approach as Cross Lan-guage Naive Bayes Classifier (CLNBC).
The pro-posed approach consists of two main stages.
Thefirst stage is to acquire a probabilistic bilingual lex-165icon.
The second stage is to employ naive Bayesmethod combined with Expectation Maximization(EM) (Dempster et al, 1977) to conduct cross lan-guage text categorization via the probabilistic bilin-gual lexicon.
For the first step, we propose two dif-ferent methods.
One is a naive and direct method,that is, we convert a bilingual lexicon into a proba-bilistic lexicon by simply assigning equal translationprobabilities to all translations of a word.
Accord-ingly, the approach in this case is named as CLNBC-D.
The other method is to employ an EM algorithmto deduce the probabilistic lexicon.
In this case, theapproach is called as CLNBC-EM.
Our preliminaryexperiments on our collected data have shown thatthe proposed approach (CLNBC) significantly out-performs the baselines in cross language case and isclose to the performance of monolingual text cate-gorization.The remainder of this paper is organized as fol-lows.
In Section 2, we introduce the naive Bayesclassifier briefly.
In Section 3, we present our crosslanguage naive Bayes algorithm.
In Section 4, eval-uation over our proposed algorithm is performed.Section 5 is conclusions and future work.2 The Naive Bayes ClassifierThe naive Bayes classifier is an effective known al-gorithm for text categorization (Domingos and Paz-zani, 1997).
When it is used for text categorizationtask, each document d ?
D corresponds to an exam-ple.
The naive Bayes classifier estimates the prob-ability of assigning a class c ?
C to a document dbased on the following Bayes?
theorem.P (c|d) ?
P (d|c)P (c) (1)Then the naive Bayes classifier makes two as-sumptions for text categorization.
Firstly, each wordin a document occurs independently.
Secondly, thereis no linear ordering of the word occurrences.Therefore, the naive Bayes classifier can be fur-ther formalized as follows:P (c|d) ?
P (c)?w?dP (w|c) (2)The estimates of P (c) and P (w|c) can be referredto (McCallum and Nigam, 1998)Some extensions to the naive Bayes classifier withEM algorithm have been proposed for various textcategorization tasks.
The naive Bayes classifier wascombined with EM algorithm to learn the class labelof the unlabelled documents by maximizing the like-lihood of both labelled and unlabelled documents(Nigam et al, 2000).
In addition, the similar waywas adopted to handle the problem with the positivesamples alone (Liu et al, 2002).
Recently, transferlearning problem was tackled by applying EM algo-rithm along with the naive Bayes classifier (Dai etal., 2007).
However, they all are monolingual textcategorization tasks.
In this paper, we apply a simi-lar method to cope with cross language text catego-rization using bilingual lexicon alone.3 Cross Language Naive Bayes ClassifierAlgorithmIn this section, a novel cross language naive Bayesclassifier algorithm is presented.
The algorithm con-tains two main steps below.
First, generate a prob-abilistic bilingual lexicon; second, apply an EM-based naive Bayes learning algorithm to deduce thelabels of documents in another language via theprobabilistic lexicon.Table 1: Notations and explanations.Notations Explanationse Language of training setf Language of test setd DocumentDe Document collection in language eDf Document collection in language fVe Vocabulary of language eVf Vocabulary of language fL Bilingual lexiconT ?
Ve ?
Vf Set of links in L??
Set of words whose translation is ?
in LE ?
Ve Set of words of language e in Lwe ?
E Word in EF ?
Vf Set of words of language f in Lwf ?
F Word in F|E| Number of distinct words in set E|F | Number of distinct words in set FN(we) Word frequency in DeN(wf , d) Word frequency in d in language fDe Data distribution in language e166For ease of description, we first define some nota-tions in Table 1.
In the next two sections, we detailthe mentioned-above two steps separately.3.1 Generation of a probabilistic bilinguallexiconTo fill the gap between different languages, there aretwo different ways.
One is to construct the multi-lingual semantic space, and the other is to transformdocuments in one language into ones in another lan-guage.
Since we concentrate on use of a bilinguallexicon, we adopt the latter method.
In this paper,we focus on the probabilistic model instead of se-lecting the best translation.
That is, we need to cal-culate the probability of the occurrence of word wein language e given a document d in language f , i.e.P (we|d).
The estimation can be calculated as fol-lows:P (we|d) =?wf?dP (we|wf , d)P (wf |d) (3)Ignoring the context information in a documentd, the above probability can be approximately esti-mated as follows:P (we|d) '?wf?dP (we|wf )P (wf |d) (4)where P (wf |d) denotes the probability of occur-rence of wf in d, which can be estimated by relativefrequency of wf in d.In order to induce P (we|d), we have to know theestimation of P (we|wf ).
Typically, we can obtain aprobabilistic lexicon from a parallel corpus.
In thispaper, we concentrate on using a bilingual lexiconalone as our external bilingual resource.
Therefore,we propose two different methods for cross languagetext categorization.First, a naive and direct method is that we assumea uniform distribution on a word?s distribution.
For-mally, P (we|wf ) = 1?wf , where (we, wf ) ?
T ; oth-erwise P (we|wf ) = 0.Second, we can apply EM algorithm to deducethe probabilistic bilingual lexicon via the bilinguallexicon L and the training document collection athand.
This idea is motivated by the work (Li and Li,2002).We can assume that each word we in language eis independently generated by a finite mixture modelas follows:P (we) =?wf?FP (wf )P (we|wf ) (5)Therefore we can use EM algorithm to estimatethe parameters of the model.
Specifically speaking,we can iterate the following two step for the purposeabove.?
E-stepP (wf |we) =P (wf )P (we|wf )?w?F P (w)P (we|w)(6)?
M-stepP (we|wf ) =(N(we) + 1)P (wf |we)?w?E (N(w) + 1) P (wf |w)(7)P (wf ) = ?
?
?we?EP (we)P (wf |we)+ (1?
?)
?
P ?
(wf ) (8)where 0 ?
?
?
1, andP ?
(wf ) =?d?Df N(wf , d) + 1?wf?F?d?Df N(wf , d) + |F |(9)The detailed algorithm can be referred to Algorithm1.
Furthermore, the probability that each word inlanguage e occurs in a document d in language f ,P (we|d), can be calculated according to Equation(4).3.2 EM-based Naive Bayes Algorithm forLabelling DocumentsIn this sub-section, we present an EM-based semi-supervised learning method for labelling documentsin different language from the language of train-ing document collection.
Its basic model is naiveBayes model.
This idea is motivated by the transferlearning work (Dai et al, 2007).
For simplicity ofdescription, we first formalize the problem.
Giventhe labelled document set De in the source languageand the unlabelled document set Df , the objective isto find the maximum a posteriori hypothesis hMAP167Algorithm 1 EM-based Word Translation Probabil-ity AlgorithmInput: Training document collectionD(l)e , bilinguallexicon L and maximum times of iterations TOutput: Probabilistic bilingual lexicon P (we|wf )1: Initialize P (0)(we|wf ) = 1|?wf | , where(we, wf ) ?
T ; otherwise P (0)(we|wf ) = 02: Initialize P (0)(wf ) = 1|F |3: for t =1 to T do4: Calculate P (t)(wf |we) based onP (t?1)(we|wf ) and P (t?1)(wf ) accord-ing to Equation (6)5: Calculate P (t)(we|wf ) and P (t)(wf ) basedon P (t)(wf |we) according to Equation (7)and Equation (8)6: end for7: return P (T )(we|wf )from the hypothesis space H under the data distri-bution of the language e, De, according to the fol-lowing formula.hMAP = arg maxh?HPDe(h|De,Df ) (10)Instead of trying to maximize PDe(h|De,Df ) inEquation (10), we can work with `(h|De,Df ), thatis, log (PDe(h)P (De,Df |h)) .
Then, using Equa-tion (10), we can deduce the following equation.`(h|De,Df ) ?
log PDe(h)+?d?Delog?c?CPDe(d|c)PDe(c|h)+?d?Dflog?c?CPDe(d|c)PDe(c|h)(11)EM algorithm is applied to find a local maximumof `(h|De,Df ) by iterating the following two steps:?
E-step:PDe(c|d) ?
PDe(c)PDe(d|c) (12)?
M-step:PDe(c) =?k?
{e,f}PDe(Dk)PDe(c|Dk) (13)PDe(we|c) =?k?
{e,f}PDe(Dk)PDe(we|c,Dk)(14)Algorithm 2 Cross Language Naive Bayes Algo-rithmInput: Labelled document collection De, unla-belled document collection Df , a bilingual lexi-con L from language e to language f and maxi-mum times of iterations T .Output: the class label of each document in Df1: Generate a probabilistic bilingual lexicon;2: Calculate P (we|d) according to Equation (4).3: Initialize P (0)De (c|d) via the traditional naiveBayes model trained from the labelled collec-tion D(l)e .4: for t =1 to T do5: for all c ?
C do6: Calculate P (t)De(c) based on P(t?1)De (c|d) ac-cording to Equation (13)7: end for8: for all we ?
E do9: Calculate P (t)De(we|c) based on P(t?1)De (c|d)and P (we|d) according to Equation (14)10: end for11: for all d ?
Df do12: Calculate P (t)De(c|d) based on P(t)De(c) andP (t)De(we|c) according to Equation (12)13: end for14: end for15: for all d ?
Df do16: c = arg maxc?CP (T )De (c|d)17: end forFor the ease of understanding, we directly put thedetails of the algorithm in cross-language text cate-gorization algorithmin which we ignore the detail ofthe generation algorithm of a probabilistic lexicon.In Equation (12), PDe(d|c) can be calculated byPDe(d|c) =?{we|we?
?wf ?wf?d}PDe(we|c)NDe (we,d)(15)where NDe(we, d) = |d|PDe(we|d).168In Equation (13), PDe(c|Dk) can be estimated asfollows:PDe(c|Dk) =?d?DkPDe(c|d)PDe(d|Dk) (16)In Equation (14), similar to section 2, we can es-timate PDe(we|c,Dk) through Laplacian smoothingas follows:PDe(we|c,Dk) =1 + NDe(we, c,Dk)|Vk|+ NDe(c,Dk)(17)whereNDe(we, c,Dk) =?d?Dk|d|PDe(we|d)PDe(c|d)(18)NDe(c,Dk) =?d?Dk|d|PDe(c|d) (19)In addition, in Equation (13) and (14), PDe(Dk)can be actually viewed as the trade-off parame-ter modulating the degree to which EM algorithmweights the unlabelled documents translated fromthe language f to the language e via a bilingual lex-icon.
In our experiments, we assume that the con-straints are satisfied, i.e.
PDe(De) + PDe(Df ) = 1and PDe(d|Dk) = 1|Dk| .4 Experiments4.1 Data PreparationWe chose English and Chinese as our experimen-tal languages, since we can easily setup our exper-iments and they are rather different languages sothat we can easily extend our algorithm to otherlanguage pairs.
In addition, to evaluate the per-formance of our algorithm, experiments were per-formed over the collected data set.
Standard evalu-ation benchmark is not available and thus we devel-oped a test data from the Internet, containing Chi-nese Web pages and English Web pages.
Specifi-cally, we applied RSS reader1 to acquire the linksto the needed content and then downloaded the Webpages.
Although category information of the con-tent can be obtained by RSS reader, we still usedthree Chinese-English bilingual speakers to organizethese Web pages into the predefined categories.
Asa result, the test data containing Chinese Web pages1http://www.rssreader.com/and English Web pages from various Web sites arecreated.
The data consists of news during Decem-ber 2005.
Also, 5462 English Web pages are from18 different news Web sites and 6011 Chinese Webpages are from 8 different news Web sites.
Data dis-tribution over categories is shown in Table 2.
Theyfall into five categories: Business, Education, Enter-tainment, Science and Sports.Some preprocessing steps are applied to Webpages.
First we extract the pure texts of all Webpages, excluding anchor texts which introduce muchnoise.
Then for Chinese corpus, all Chinese charac-ters with BIG5 encoding first were converted intoones with GB2312 encoding, applied a Chinese seg-menter tool2 by Zhibiao Wu from LDC to our Chi-nese corpus and removed stop words and wordswith one character and less than 4 occurrences; forEnglish corpus, we used the stop words list fromSMART system (Buckley, 1985) to eliminate com-mon words.
Finally, We randomly split both the En-glish and Chinese document collection into 75% fortraining and 25% for testing.we compiled a large general-purpose English-Chinese lexicon, which contains 276,889 translationpairs, including 53,111 English entries and 38,517Chinese entries.
Actually we used a subset of thelexicon including 20,754 English entries and 13,471Chinese entries , which occur in our corpus.Table 2: Distribution of documents over categoriesCategories English ChineseSports 1797 2375Business 951 1212Science 843 1157Education 546 692Entertainment 1325 575Total 5462 60114.2 Baseline AlgorithmsTo investigate the effectiveness of our algorithmson cross-language text categorization, three baselinemethods are used for comparison.
They are denotedby ML, MT and LSI respectively.ML (Monolingual).
We conducted text catego-rization by training and testing the text categoriza-2http://projects.ldc.upenn.edu/Chinese/LDC ch.htm16920 40 80 160 320 640 1280 40960.20.30.40.50.60.70.80.91# of training samplesAccuracyML MT LSI CLNBC?D CLNBC?EMFigure 1: Comparison of the best performance ofdifferent methods with various sizes of training setand the entire test set.
Training is conducted overChinese corpus and testing is conducted over En-glish corpus in the cross language case, while bothtraining and testing are performed over English cor-pus in the monolingual case.tion system on document collection in the same lan-guage.MT (Machine Translation).
We used Systranpremium 5.0 to translate training data into the lan-guage of test data, since the machine translation sys-tem is one of the best machine translation systems.Then use the translated data to learn a model forclassifying the test data.LSI (Latent Semantic Indexing).
We can usethe LSI or SVD technique to deduce language-independent representations through a bilingual par-allel corpus.
In this paper, we use SVDS commandin MATLAB to acquire the eigenvectors with thefirst K largest eigenvalues.
We take K as 400 in ourexperiments, where best performance is achieved.In this paper, we use SVMs as the classifier of ourbaselines, since SVMs has a solid theoretic founda-tion based on structure risk minimization and thushigh generalization ability.
The commonly usedone-vs-all framework is used for the multi-classcase.
SVMs uses the SV M light software pack-age(Joachims, 1998).
In all experiments, the trade-off parameter C is set to 1.4.3 ResultsIn the experiments, all results are averaged on 5 runs.Results are measured by accuracy, which is definedas the ratio of the number of labelled correctly docu-20 40 80 160 320 640 1280 40960.20.30.40.50.60.70.80.91# of training samplesAccuracyML MT LSI CLNBC?D CLNBC?EMFigure 2: Comparison of the best performance ofdifferent methods with various sizes of training setand the entire test set.
Training is conducted overEnglish corpus and testing is conducted over Chi-nese corpus in the cross language case, while bothtraining and testing are performed over Chinese cor-pus in the monolingual case.ments to the number of all documents.
When inves-tigating how different training data have effect onperformance, we randomly select the correspondingnumber of training samples from the training set 5times.
The results are shown in Figure 1 and Fig-ure 2.
From the two figures, we can draw the fol-lowing conclusions.
First, CLNBC-EM has a stableand good performance in almost all cases.
Also, itcan achieve the best performance among cross lan-guage methods.
In addition, we notice that CLNBC-D works surprisingly better than CLNBC-EM, whenthere are enough test data and few training data.
Thismay be because the quality of the probabilistic bilin-gual lexicon derived from CLNBC-EM method ispoor, since this bilingual lexicon is trained from in-sufficient training data and thus may provide biasedtranslation probabilities.To further investigate the effect of varying theamount of test data, we randomly select the cor-responding number of test samples from test set 5times.
The results are shown in Figure 3 and Fig-ure 4, we can draw the following conclusions .
First,with the increasing test data, performance of our twoapproaches is improved.
Second, CLNBC-EM sta-tistically significantly outperforms CLNBC-D.From figures 1 through 4, we also notice that MTand LSI always achieve some poor results.
For MT,1700.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.50.60.70.80.91Ratio of test dataAccuracyML MT LSI CLNBC?D CLNBC?EMFigure 3: Comparison of the best performance ofdifferent methods with the entire training set andvarious sizes of test set.
Training is conducted overChinese corpus and testing is conducted over En-glish corpus in the cross language case, while bothtraining and testing are performed over English cor-pus in the monolingual case.maybe it is due to the large difference of word usagebetween original documents and the translated ones.For example,   (Qi Shi) has two common trans-lations, which are cavalier and knight.
In sports do-main, it often means a basketball team of NationalBasketball Association (NBA) in U.S. and shouldbe translated into cavalier.
However, the transla-tion knight is provided by Systran translation systemwe use in the experiment.
In term of LSI method,one possible reason is that the parallel corpus is toolimited.
Another possible reason is that it is out-of-domain compared with the domain of the used doc-ument collections.From Table 3, we can observe that our algorithmis more efficient than three baselines.
The spent timeare calculated on the machine, which has a 2.80GHzDual Pentium CPU.5 Conclusions and Future WorkIn this paper, we addressed the issue of how to con-duct cross language text categorization using a bilin-gual lexicon.
To this end, we have developed a crosslanguage naive Bayes classifier, which contains twomain steps.
In the first step, we deduce a proba-bilistic bilingual lexicon.
In the second step, weadopt naive Bayes method combined with EM toconduct cross language text categorization.
We have0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.650.70.750.80.850.90.951Ratio of test dataAccuracyML MT LSI CLNBC?D CLNBC?EMFigure 4: Comparison of the best performance ofdifferent methods with the entire training set andvarious sizes of test set.
Training is conducted overEnglish corpus and testing is conducted over Chi-nese corpus in the cross language case, while bothtraining and testing are performed over Chinese cor-pus in the monolingual case.proposed two different methods, namely CLNBC-Dand CLNBC-EM, for cross language text categoriza-tion.
The preliminary experiments on collected datacollections show the effectiveness of the proposedtwo methods and verify the feasibility of achievingperformance near to monolingual text categorizationusing a bilingual lexicon alone.As further work, we will collect larger compara-ble corpora to verify our algorithm.
In addition, wewill investigate whether the algorithm can be scaledto more fine-grained categories.
Furthermore, wewill investigate how the coverage of bilingual lex-icon have effect on performance of our algorithm.Table 3: Comparison of average spent time by dif-ferent methods, which are used to conduct cross-language text categorization from English to Chi-nese.Methods Preparation ComputationCLNBC-D - ?1 MinCLNBC-EM - ?2 MinML - ?10 MinMT ?48 Hra ?14 MinLSI ?90 Minb ?15 MinaMachine Translation CostbSVD Decomposition Cost171Acknowledgements.
The authors would like tothank three anonymous reviewers for their valu-able suggestions.
This work was partially sup-ported by the National Natural Science Founda-tion of China under the grants NSFC 60375022 andNSFC 60473040, and the Microsoft Laboratory forIntelligent Computing and Intelligent Systems ofShanghai Jiao Tong University.ReferencesNuria Bel, Cornelis H. A. Koster, and Marta Villegas.2003.
Cross-lingual text categorization.
In ECDL,pages 126?139.Chris Buckley.
1985.
Implementation of the SMARTinformation retrieval system.
Technical report, Ithaca,NY, USA.Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and YongYu.
2007.
Transferring naive Bayes classifiers for textclassification.
In Proceedings of Twenty-Second AAAIConference on Artificial Intelligence (AAAI 2007),pages 540?545, July.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.Pedro Domingos and Michael J. Pazzani.
1997.
On theoptimality of the simple bayesian classifier under zero-one loss.
Machine Learning, 29(2-3):103?130.Blaz?
Fortuna and John Shawe-Taylor.
2005.
The useof machine translation tools for cross-lingual text min-ing.
In Learning With Multiple Views, Workshop at the22nd International Conference on Machine Learning(ICML).Alfio Massimiliano Gliozzo and Carlo Strapparava.2006.
Exploiting comparable corpora and bilingualdictionaries for cross-language text categorization.
In21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics.
The Association forComputer Linguistics, July.Thorsten Joachims.
1998.
Making large-scale sup-port vector machine learning practical.
In A. SmolaB.
Scho?lkopf, C. Burges, editor, Advances in KernelMethods: Support Vector Machines.
MIT Press, Cam-bridge, MA.Cong Li and Hang Li.
2002.
Word translation disam-biguation using bilingual bootstrapping.
In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 343?351.Yaoyong Li and John Shawe-Taylor.
2006.
Using KCCAfor Japanese-English cross-language information re-trieval and document classification.
Journal of Intel-ligent Information Systems, 27(2):117?133.Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.2002.
Partially supervised classification of text doc-uments.
In ICML ?02: Proceedings of the NineteenthInternational Conference on Machine Learning, pages387?394, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Andrew McCallum and Kamal Nigam.
1998.
A compar-ison of event models for naive bayes text classification.In Proceedings of AAAI-98, Workshop on Learning forText Categorization.Kamal Nigam, Andrew McCallum, Sebastian Thrun, andTom Mitchell.
2000.
Text classification from labeledand unlabeled documents using EM.
Machine Learn-ing, 39(2/3):103?134.J.
Scott Olsson, Douglas W. Oard, and Jan Hajic?.
2005.Cross-language text classification.
In Proceedings ofthe 28th Annual international ACM SIGIR Confer-ence on Research and Development in information Re-trieval, pages 645?646, New York, NY, August.
ACMPress.Leonardo Rigutini, Marco Maggini, and Bing Liu.
2005.An EM based training algorithm for cross-languagetext categorization.
In Proceedings of Web IntelligenceConference (WI-2005), pages 529?535, Compie`gne,France, September.
IEEE Computer Society.172
