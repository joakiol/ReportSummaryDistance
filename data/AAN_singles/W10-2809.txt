Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 57?61,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsActive Learning for Constrained Dirichlet Process Mixture ModelsAndreas VlachosComputer LaboratoryUniversity of Cambridgeav308@cl.cam.ac.ukZoubin GhahramaniDepartment of EngineeringUniversity of Cambridgezoubin@eng.cam.ac.ukTed BriscoeComputer LaboratoryUniversity of Cambridgeejb@cl.cam.ac.ukAbstractRecent work applied Dirichlet ProcessMixture Models to the task of verb cluster-ing, incorporating supervision in the formof must-links and cannot-links constraintsbetween instances.
In this work, we intro-duce an active learning approach for con-straint selection employing uncertainty-based sampling.
We achieve substantialimprovements over random selection ontwo datasets.1 IntroductionBayesian non-parametric mixture models have theattractive property that the number of componentsused to model the data is not fixed in advance butis determined by the model and the data.
Thisproperty is particularly interesting for NLP wheremany tasks are aimed at discovering novel in-formation.
Recent work has applied such mod-els to various tasks with promising results, e.g.Teh (2006) and Cohn et al (2009).Vlachos et al (2009) applied the basic modelof this class, the Dirichlet Process Mixture Model(DPMM), to lexical-semantic verb clustering withencouraging results.
The task involves discov-ering classes of verbs similar in terms of theirsyntactic-semantic properties (e.g.
MOTION classfor travel, walk, run, etc.).
Such classes can pro-vide important support for other tasks, such asword sense disambiguation, parsing and seman-tic role labeling.
(Dang, 2004; Swier and Steven-son, 2004) Although some fixed classifications areavailable these are not comprehensive and are in-adequate for specific domains.Furthermore, Vlachos et al (2009) used a con-strained version of the DPMM in order to guideclustering towards some prior intuition or consid-erations relevant to the specific task at hand.
Thissupervision was modelled as pairwise constraintsbetween instances and it informs the model of re-lations between them that cannot be recovered bythe model on the basis of the feature representa-tion used.
Like other forms of supervision, theseconstraints require manual annotation and it is im-portant to maximize the benefits obtained from it.Therefore it is natural to consider active learning(Settles, 2009) in order to focus the supervision onclusterings on which the model is uncertain.In this work, we propose a simple yet effec-tive active learning method employing uncertaintybased sampling.
The effectiveness of the ALmethod is demonstrated on two datasets, one ofwhich has multiple gold standards.2 Constrained DPMMs for clusteringIn DPMMs, the parameters of each component aregenerated by a Dirichlet Process (DP) which canbe seen as a distribution over distributions.
Eachinstance, represented by its features, is generatedby the component it is assigned to.
The compo-nents discovered correspond to the clusters.
Theprior probability of assigning an instance to a par-ticular component is proportionate to the numberof instances already assigned to it, in other words,the DPMM exhibits the ?rich get richer?
prop-erty.
A popular metaphor to describe the DPMMwhich exhibits an equivalent clustering propertyis the Chinese Restaurant Process (CRP).
Cus-tomers (instances) arrive at a Chinese restaurantwhich has an infinite number of tables (compo-nents).
Each customer sits at one of the tables thatis either occupied or vacant with popular tables at-tracting more customers.Following Navarro et al (2006), parameter es-timation is performed using Gibbs sampling bysampling the assignment zi of each instance xigiven all the others z?i and the data X:P (zi = z|z?i, X) ?p(zi = z|z?i)P (xi|zi = z,X?i) (1)57In Eq.
1 p(zi = z|z?i) is the CRP prior andP (xi|zi = z,X?i) is the distribution that gener-ates instance xi given it has been assigned to com-ponent z.
This sampling scheme is possible be-cause the assignments in the model are exchange-able, i.e.
their order is not relevant.The constrained version of the DPMM usespairwise constraints over instances in order toadapt the clustering discovered.
FollowingWagstaff & Cardie (2000), a pair of instances iseither linked together (must-link) or not (cannot-link).
For example, charge and run should form amust-link if the aim is to cluster MOTION verbstogether, but they should form a cannot-link if weare interested in BILL verbs.
All links are as-sumed to be consistent with each other.
In orderto incorporate the constraints in the DPMM, theGibbs sampling scheme is modified so that must-linked instances are generated by the same compo-nent and cannot-linked instances always by differ-ent ones.
Following Vlachos et al (2009), for eachinstance that does not belong to a linked-group, thesampler is restricted to choose components that donot contain instances cannot-linked with it.
Forinstances in a linked-group, their assignment issampled jointly, again taking into account theircannot-links.
This is performed by adding eachinstance of the linked-group successively to thesame component.
In terms of the CRP metaphor,customers connected with must-links arrive at therestaurant and choose a table jointly, respectingtheir cannot-links with other customers.3 Active Constraint SelectionIn active learning, the model selects the supervi-sion to be provided by a human expert.
In the con-text of the DPMMs, the model chooses a pair ofinstances for which a must-link or a cannot-linkmust be provided.
To select the pair, we employthe simple but effective idea of uncertainty basedsampling.
We consider the most informative linkas that on which the model is most uncertain, moreformally the link between instances l?ij that maxi-mizes the following entropy:l?ij = argmaxi,jH(zi = zj) (2)If we consider clustering as binary classification oflinks into must-links and cannot-links, it is equiv-alent to selecting the pair with the highest labelentropy.
During the sampling process used forparameter inference, component assignments varybetween samples and the components themselvesare not identifiable, i.e.
one cannot match the com-ponents of one sample with those of another.
Fur-thermore, the conditional assignments estimatedduring Gibbs sampling (Eq.
1) they do not capturethe uncertainty of the assignments z?i on whichthey condition.
Therefore, we resort to generatinga set of samples from the (possibly constrained)DPMM and pick the link on which these sam-ples maximally disagree, i.e.
we approximate thedistribution in Eq.
2 with the probability that in-stances i, j are in the same cluster or not.
Thus,in a given set of samples the most uncertain linkwould be the one between two instances which arein the same cluster in exactly half of these sam-ples.
Using multiple samples allows us to take intoaccount the uncertainty in the assignments of theother instances, as well as the varying number ofcomponents.Compared to standard pool-based AL, whenclustering with constraints the possible links be-tween two instances (ignoring transitivity) areC(N, 2) = N(N ?
1)/2 (N is the size of thedataset) and there is an equal number of candi-date queries to be considered, as opposed to Nqueries in a supervised classification task.
Anotherinteresting difference is that the the AL processcan be initiated without any supervision, since theDPMM is unsupervised.
On the other hand, inthe standard AL scenario a (usually small) labelledseed set is used.
Therefore, we rely exclusively onthe model and the features to guide the constraintselection process.
If the model combined with thefeatures is not appropriate for the task then theconstraints chosen are unlikely to be useful.4 Datasets and EvaluationIn our experiments we used two verb clusteringdatasets, one from general English (Sun et al,2008) and one from the biomedical domain (Ko-rhonen et al, 2006).
In both datasets the fea-tures for each verb are its subcategorization frames(SCFs) which capture the syntactic context inwhich it occurs.
They were acquired automati-cally using a domain-independent statistical pars-ing toolkit, RASP (Briscoe and Carroll, 2002), anda classifier which identifies verbal SCFs.
As aconsequence, they include some noise due to stan-dard text processing and parsing errors and due tothe subtlety of the argument-adjunct distinction.The general English dataset contains 204 verbs58belonging to 17 fine-grained classes in Levin?s(Levin, 1993) taxonomy so that each class con-tains 12 verbs.
The biomedical dataset consists of193 medium to high frequency verbs from a cor-pus of 2230 full-text articles from 3 biomedicaljournals.
A team of linguists and biologists cre-ated a three-level gold standard with 16, 34 and50 classes.
Both datasets were pre-processed us-ing non-negative matrix factorization (Lin, 2007)which decomposes a large sparse matrix into twodense matrices (of lower dimensionality) withnon-negative values.
In all experiments 35 dimen-sions were kept.
Preliminary experiments withdifferent number of dimensions kept did not affectthe performance substantially.We evaluate our results using three informa-tion theoretic measures: Variation of Informa-tion (Meila?, 2007), V-measure (Rosenberg andHirschberg, 2007) and V-beta (Vlachos et al,2009).
All three assess the two desirable proper-ties that a clustering should have with respect toa gold standard, homogeneity and completeness.Homogeneity reflects the degree to which eachcluster contains instances from a single class andis defined as the conditional entropy of the classdistribution of the gold standard given the clus-tering.
Completeness reflects the degree to whicheach class is contained in a single cluster and is de-fined as the conditional entropy of clustering giventhe class distribution in the gold standard.
V-betabalances these properties explicitly by taking intoaccount the ratio of the number of cluster discov-ered over the number of classes in the gold stan-dard.
While an ideal clustering should have bothproperties, naively improving one of them can beharmful for the other.
Compared to the more com-monly used F-measure (Fung et al, 2003), thesemeasures have the advantage that they do not as-sume a mapping between clusters and classes.5 ExperimentsWe performed experiments in order to assess theeffectiveness of the AL algorithm for the con-strained DPMM comparing it to random selection.In each AL round, we run the Gibbs sampler forthe (constrained) DPMM five times, using 100 it-erations for burn-in, draw 20 samples from eachrun with 5 iterations lag between samples and se-lect the most uncertain link to be labeled.
Fol-lowing Navarro et al (2006), the concentrationparameter is inferred from the data using Gibbssampling.
The performances were averaged acrossthe collected samples.
Random selection was re-peated three times.
The three levels of the biomed-ical gold standard were used independently and to-gether with the general English dataset result infour experimental setups.The comparison between AL and random se-lection for each dataset is shown in graphs 1(a)-1(d) using V-beta, noting that the observationsmade hold with all evaluation metrics used.
Con-straints selected via AL improve the performancerapidly.
Indicatively, the performance reached us-ing 1000 randomly chosen constraints is obtainedusing only 110 actively selected ones in the bio-50dataset.
AL performance levels out in later stageswith performance superior to the one achieved us-ing random selection with the same number ofconstraints.
The poor performance of random se-lection is expected, since the unsupervised DPMMpredicts more than 90% of the binary links cor-rectly.
Another interesting observation is that, dur-ing AL, homogeneity increased faster than com-pleteness (graphs 1(g) and 1(h)).
This suggeststhat the features used lead the model towards finer-grained clusters, which is further confirmed bythe fact that the highest scores on the biomedicaldataset are achieved when comparing against thefinest-grained version of the gold standard.
Whileit is possible to choose constraints to the modelthat would increase completeness with respect tothe gold standard, we argue that this would not al-low us to obtain obtain insights on the model andthe features used.We also noticed that the choice of batch sizehas a significant effect on the learning rate of themodel.
This phenomenon occurs in varying de-grees in many applications of AL.
Manual inspec-tion of the links chosen at each round revealed thatbatches often contained links involving the sameinstances.
This is expected due to transitivity: ifthe link between instances A and B is uncertainbut the link between instances B and C is certain,then the link between A and C will be uncertaintoo.
While reducing the batch size leads to bet-ter learning rates, it requires estimating the modelmore often.
In order to ameliorate this issue, af-ter obtaining the label of the most uncertain link,we remove the samples that disagreed with it andre-calculate the uncertainty of the remaining linksgiven the remaining samples.
This is repeated un-til the intended batch size is reached.
Thus, we590.650.70.750.80.850.90  50  100  150  200  250V-betalinksactiverandom(a) bio-160.70.750.80.850.90  50  100  150  200  250V-betalinksactiverandom(b) bio-340.720.760.80.840.880  50  100  150  200  250V-betalinksactiverandom(c) bio-500.550.60.650.70.750  50  100  150  200  250V-betalinksactiverandom(d) gen. English0.650.70.750.80.850.90  50  100  150  200  250V-betalinksactive10batch(e) bio-160.70.750.80.850.90  50  100  150  200  250V-betalinksactive10batch(f) bio-340.650.70.750.80.850.90  50  100  150  200  250V-betalinkshomcomp(g) bio-500.550.60.650.70.750.80.850  50  100  150  200  250V-betalinkshomcomp(h) gen. EnglishFigure 1: (a)-(d): Constrained DPMM learning curves comparing random selection and AL.
(e),(f):Batch selection comparison.
(g),(h): Homogeneity and completeness curves during AL.avoid selecting links involving the same instance,unless their uncertainty was not reduced by theconstraints added.
A consideration that arises isthat by reducing the number of samples used foruncertainty estimation, progressively we are leftwith fewer samples to rank the remaining links.Each labeled link reduces the number of samplesapproximately by half since the most uncertainlink is likely to be a must-link in half the sam-ples and a cannot-lnk in the remaining half.
Asa result, for a batch with size |B| the uncertaintyof the last link will be estimated using |S|/2|B|?1samples.
A crude solution would be to generateenough samples for the desired batch size.
How-ever, obtaining a very large number of samples canbe computationally expensive.
Therefore, we set athreshold for the minimum number of samples tobe used to estimate the link uncertainty and whenit is reached, more samples are generated using theconstraints selected.
In graphs 1(e) and 1(f) wedemonstrate the effectiveness of the batch selec-tion method proposed (labeled ?batch?)
comparedto naive batch selection (labeled ?active10?
).6 Discussion and Future WorkWe presented an AL method for constrained DP-MMs employing uncertainty based sampling.
Weapplied it to two different verb clustering datasetswith 4 gold standards in total and obtained verygood results compared to random selection.
Theidea, while explored in the context of verb cluster-ing with the constrained DPMM, is likely to be ap-plicable to other models that can incorporate must-links and cannot-links in MCMC sampling.Most literature on AL for NLP considers super-vised methods for classification or sequential tag-ging.
However, AL for clustering is a relativelyunder-explored area.
Klein et al (2002) incorpo-rated actively selected constraints in hierarchicalagglomerative clustering.
Basu et al (2006) haveapplied AL to obtain must-links and cannot-linkshowever, the clustering framework used requiresthe number of clusters to be known in advancewhich restricts counter-intuitively the clusteringsolutions that are discovered.
Moreover, semi-supervised clustering is a form of semi-supervisedlearning and in this light, our approach is relatedto the work of Zhu et al (2003).With respect to the practical application of theAL method suggested, it is worth noting that in allour experiments the constraints were obtained forthe respective gold standard of the dataset at ques-tion and consequently they are all consistent witheach other.
However, this assumption might nothold in case human experts are employed for thesame purpose.
In order to use such feedback in theframework suggested, it is necessary to filter theconstraints provided in order to obtain a consistentsubset.
To this end, it would be interesting to in-vestigate the potential of using ?soft?
constraints,i.e.
constraints that are provided with relative con-fidence.60ReferencesSugato Basu, Mikhail Bilenko, Arindam Banerjee,and Raymond J. Mooney.
2006.
Probabilis-tic semi-supervised clustering with constraints.
InO.
Chapelle, B. Schoelkopf, and A. Zien, edi-tors, Semi-Supervised Learning, pages 73?102.
MITPress.Ted Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof the 3rd International Conference on LanguageResources and Evaluation, pages 1499?1504.Trevor Cohn, Sharon Goldwater, and Phil Blun-som.
2009.
Inducing compact but accurate tree-substitution grammars.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 548?556.Hoa Trang Dang.
2004.
Investigations into the roleof lexical semantics in word sense disambiguation.Ph.D.
thesis, University of Pennsylvania, Philadel-phia, PA, USA.Benjamin C. M. Fung, Ke Wang, and Martin Ester.2003.
Hierarchical document clustering using fre-quent itemsets.
In Proceedings of SIAM Interna-tional Conference on Data Mining, pages 59?70.Dan Klein, Sepandar D. Kamvar, and Christopher D.Manning.
2002.
From instance-level constraints tospace-level constraints: Making the most of priorknowledge in data clustering.
In Proceedings of theNineteenth International Conference on MachineLearning, pages 307?314.Anna Korhonen, Yuval Krymolowski, and Nigel Col-lier.
2006.
Automatic classification of verbs inbiomedical texts.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 345?352.Beth Levin.
1993.
English Verb Classes and Alter-nations: a preliminary investigation.
University ofChicago Press, Chicago.Chih-Jen Lin.
2007.
Projected gradient methods fornonnegative matrix factorization.
Neural Compua-tion, 19(10):2756?2779.Marina Meila?.
2007.
Comparing clusterings?an in-formation based distance.
Journal of MultivariateAnalysis, 98(5):873?895.Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,and Michael D. Lee.
2006.
Modeling individual dif-ferences using Dirichlet processes.
Journal of Math-ematical Psychology, 50(2):101?122, April.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 410?420.Burr Settles.
2009.
Active learning literature survey.Computer Sciences Technical Report 1648, Univer-sity of Wisconsin?Madison.Lin Sun, Anna Korhonen, and Yuval Krymolowski.2008.
Verb class discovery from rich syntactic data.In Proceedings of the 9th International Conferenceon Intelligent Text Processing and ComputationalLinguistics.Robert S. Swier and Suzanne Stevenson.
2004.
Unsu-pervised semantic role labelling.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 95?102.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 985?992, Sydney, Australia, July.Andreas Vlachos, Anna Korhonen, and ZoubinGhahramani.
2009.
Unsupervised and ConstrainedDirichlet Process Mixture Models for Verb Cluster-ing.
In Proceedings of the EACL workshop on GEo-metrical Models of Natural Language Semantics.Kiri Wagstaff and Claire Cardie.
2000.
Clusteringwith instance-level constraints.
In Proceedings ofthe Seventeenth International Conference on Ma-chine Learning, pages 1103?1110.Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.2003.
Combining Active Learning and Semi-Supervised Learning Using Gaussian Fields andHarmonic Functions.
In ICML workshop on TheContinuum from Labeled to Unlabeled Data in Ma-chine Learning and Data Mining, pages 58?65.61
