Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?614,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsName-aware Machine TranslationHaibo Li?
Jing Zheng?
Heng Ji?
Qi Li?
Wen Wang??
Computer Science Department and Linguistics DepartmentQueens College and Graduate Center, City University of New YorkNew York, NY, USA 10016{lihaibo.c, hengjicuny, liqiearth}@gmail.com?
Speech Technology & Research LaboratorySRI InternationalMenlo Park, CA, USA 94025{zj, wwang}@speech.sri.comAbstractWe propose a Name-aware MachineTranslation (MT) approach which cantightly integrate name processing into MTmodel, by jointly annotating parallel cor-pora, extracting name-aware translationgrammar and rules, adding name phrasetable and name translation driven decod-ing.
Additionally, we also propose a newMT metric to appropriately evaluate thetranslation quality of informative words,by assigning different weights to differ-ent words according to their importancevalues in a document.
Experiments onChinese-English translation demonstratedthe effectiveness of our approach on en-hancing the quality of overall translation,name translation and word alignment overa high-quality MT baseline1.1 IntroductionA shrinking fraction of the world?s Web pages arewritten in English, therefore the ability to accesspages across a range of languages is becoming in-creasingly important.
This need can be addressedin part by cross-lingual information access taskssuch as entity linking (McNamee et al, 2011; Cas-sidy et al, 2012), event extraction (Hakkani-Turet al, 2007), slot filling (Snover et al, 2011) andquestion answering (Parton et al, 2009; Partonand McKeown, 2010).
A key bottleneck of high-quality cross-lingual information access lies in theperformance of Machine Translation (MT).
Tradi-tional MT approaches focus on the fluency andaccuracy of the overall translation but fall shortin their ability to translate certain content word-s including critical information, especially names.1Some of the resources and open source programs devel-oped in this work are made freely available for research pur-pose at http://nlp.cs.qc.cuny.edu/NAMT.tgzA typical statistical MT system can only trans-late 60% person names correctly (Ji et al, 2009).Incorrect segmentation and translation of nameswhich often carry central meanings of a sentencecan also yield incorrect translation of long con-texts.
Names have been largely neglected in theprior MT research due to the following reasons:?
The current dominant automatic MT scoringmetrics (such as Bilingual Evaluation Under-study (BLEU) (Papineni et al, 2002)) treatall words equally, but names have relative lowfrequency in text (about 6% in newswire andonly 3% in web documents) and thus are vast-ly outnumbered by function words and com-mon nouns, etc..?
Name translations pose a greater complexitybecause the set of names is open and highlydynamic.
It is also important to acknowledgethat there are many fundamental differencesbetween the translation of names and othertokens, depending on whether a name is ren-dered phonetically, semantically, or a mixtureof both (Ji et al, 2009).?
The artificial settings of assigning lowweights to information translation (comparedto overall word translation) in some large-scale government evaluations have discour-aged MT developers to spend time and ex-plore resources to tackle this problem.We propose a novel Name-aware MT (NAMT)approach which can tightly integrate name pro-cessing into the training and decoding processes ofan end-to-end MT pipeline, and a new name-awaremetric to evaluate MT which can assign differentweights to different tokens according to their im-portance values in a document.
Compared to pre-vious methods, the novel contributions of our ap-proach are:1.
Tightly integrate joint bilingual name tag-ging into MT training by coordinating tagged604names in parallel corpora, updating word seg-mentation, word alignment and grammar ex-traction (Section 3.1).2.
Tightly integrate name tagging and transla-tion into MT decoding via name-aware gram-mar (Section 3.2).3.
Optimize name translation and context trans-lation simultaneously and conduct nametranslation driven decoding with languagemodel (LM) based selection (Section 3.2).4.
Propose a new MT evaluation metric whichcan discriminate names and non-informativewords (Section 4).2 Baseline MTAs our baseline, we apply a high-performingChinese-English MT system (Zheng, 2008; Zhenget al, 2009) based on hierarchical phrase-basedtranslation framework (Chiang, 2005).
It is basedon a weighted synchronous context-free grammar(SCFG).
All SCFG rules are associated with a setof features that are used to compute derivationprobabilities.
The features include:?
Relative frequency in two directions P (?|?
)andP (?|?
), estimating the likelihoods of oneside of the rule r: X ?< ?, ?
> translatinginto the other side, where ?
and ?
are stringsof terminals and non-terminals in the sourceside and target side.
Non-terminals in ?
and?
are in one-to-one correspondence.?
Lexical weights in two directions: Pw(?|?)andPw(?|?
), estimating likelihoods of word-s in one side of the rule r: X ?< ?, ?
>translating into the other side (Koehn et al,2003).?
Phrase penalty: a penalty exp(1) for a rulewith no non-terminal being used in deriva-tion.?
Rule penalty: a penalty exp(1) for a rulewith at least one non-terminal being used inderivation.?
Glue rule penalty: a penalty exp(1) if a gluerule used in derivation.?
Translation length: number of words in trans-lation output.Our previous work showed that combining mul-tiple LMs trained from different sources can leadto significant improvement.
The LM used for de-coding is a log-linear combination of four wordn-gram LMs which are built on different Englishcorpora (details described in section 5.1), withthe LM weights optimized on a development setand determined by minimum error rate training(MERT), to estimate the probability of a word giv-en the preceding words.
All four LMs were trainedusing modified Kneser-Ney smoothing algorithm(Chen and Goodman, 1996) and converted intoBloom filter LMs (Talbot and Brants, 2008) sup-porting memory map.The scaling factors for all features are optimizedby minimum error rate training algorithm to max-imize BLEU score (Och, 2003).
Given an inputsentence in the source language, translation intothe target language is cast as a search problem,where the goal is to find the highest-probabilityderivation that generates the source-side sentence,using the rules in our SCFG.
The source-sidederivation corresponds to a synchronous target-side derivation and the terminal yield of this target-side derivation is the output of the system.
We em-ploy our CKY-style chart decoder, named SRInter-p, to solve the search problem.3 Name-aware MTWe tightly integrate name processing into theabove baseline to construct a NAMT model.
Fig-ure 1 depicts the general procedure.3.1 TrainingThis basic training process of NAMT requires usto apply a bilingual name tagger to annotate par-allel training corpora.
Traditional name taggingapproaches for single languages cannot addressthis requirement because they were all built on da-ta and resources which are specific to each lan-guage without using any cross-lingual features.In addition, due to separate decoding processesthe results on parallel data may not be consistentacross languages.
We developed a bilingual jointname tagger (Li et al, 2012) based on condition-al random fields that incorporates both monolin-gual and cross-lingual features and conducts join-t inference, so that name tagging from two lan-guages can mutually enhance each other and there-fore inconsistent results can be corrected simulta-neously.
This joint name tagger achieved 86.3%bilingual pair F-measure with manual alignmentand 84.4% bilingual pair F-measure with automat-ic alignment as reported in (Li et al, 2012).
Givena parallel sentence pair we first apply Giza++ (Ochand Ney, 2003) to align words, and apply this join-605DecodingHierarchicalPhrased-based MTTranslated TextTranslateBi-textData Source TextJointName TaggerSource LanguageName TaggerName TranslatorTrainingName Pair MinerExtract source language namesandaddthem todictionariesforsource languagename taggerExtract namepairs andaddthem totranslationdictionaryExtract and addname pairs tophrase tableGIZA++RuleExtractorExtract SCFGrules withcombinationof name-replaceddataandoriginalbi-text dataReplacenames withnon-terminalsandcombinewiththeoriginalparallel dataFigure 1: Architecture of Name-aware Machine Translation System.t bilingual name tagger to extract three types ofnames: (Person (PER), Organization (ORG) andGeo-political entities (GPE)) from both the sourceside and the target side.
We pair two entities fromtwo languages, if they have the same entity typeand are mapped together by word alignment.
Weignore two kinds of names: multi-word nameswith conflicting boundaries in two languages andnames only identified in one side of a parallel sen-tence.We built a NAMT system from such name-tagged parallel corpora.
First, we replace taggedname pairs with their entity types, and thenuse Giza++ and symmetrization heuristics to re-generate word alignment.
Since the name tags ap-pear very frequently, the existence of such tagsyields improvement in word alignment quality.The re-aligned parallel corpora are used to trainour NAMT system based on SCFG.
Since the jointname tagger ensures that each tagged source namehas a corresponding translation on the target side(and vice versa), we can extract SCFG rules bytreating the tagged names as non-terminals.However, the original parallel corpora containmany high-frequency names, which can already behandled well by the baseline MT.
Some of thesenames carry special meanings that may influencetranslations of the neighboring words, and thus re-placing them with non-terminals can lead to infor-mation loss and weaken the translation model.
Toaddress this issue, we merged the name-replacedparallel data with the original parallel data and ex-tract grammars from the combined corpus.
For ex-ample, given the following sentence pair:?
-???e???e?????
.?
China appeals to world for non involvementin Angola conflict .after name tagging it becomes?
GPE??e??
?e GPE??
.?
GPE appeals to world for non involvement inGPE conflict .Both sentence pairs are kept in the combined datato build the translation model.3.2 DecodingDuring decoding phase, we extract names withthe baseline monolingual name tagger describedin (Li et al, 2012) from a source document.
It-s performance is comparable to the best report-ed results on Chinese name tagging on Automat-ic Content Extraction (ACE) data (Ji and Grish-man, 2006; Florian et al, 2006; Zitouni and Flo-rian, 2008; Nguyen et al, 2010).
Then we ap-ply a state-of-the-art name translation system (Jiet al, 2009) to translate names into the target lan-guage.
The name translation system is composedof the following steps: (1) Dictionary matchingbased on 150,041 name translation pairs; (2) Sta-tistical name transliteration based on a structuredperceptron model and a character based MT mod-el (Dayne and Shahram, 2007); (3) Context infor-mation extraction based re-ranking.In our NAMT framework, we add the followingextensions to name translation.We developed a name origin classifier based onChinese last name list (446 name characters) andname structure parsing features to distinguish Chi-nese person names and foreign person names (Ji,2009), so that pinyin conversion is applied for Chi-nese names while name transliteration is appliedonly for foreign names.
This classifier works rea-sonably well in most cases (about 92% classifica-tion accuracy), except when a common Chineselast name appears as the first character of a foreign606name, such as ?1??
which can be translated ei-ther as ?Jolie?
or ?Zhu Li?.For those names with fewer than five instancesin the training data, we use the name translationsystem to provide translations; for the rest of thenames, we leave them to the baseline MT mod-el to handle.
The joint bilingual name tagger wasalso exploited to mine bilingual name translationpairs from parallel training corpora.
The mappingscore between a Chinese name and an Englishname was computed by the number of aligned to-kens.
A name pair is extracted if the mappingscore is the highest among all combinations andthe name types on both sides are identical.
It isnecessary to incorporate word alignment as addi-tional constraints because the order of names is of-ten changed after translation.
Finally, the extract-ed 9,963 unique name translation pairs were alsoused to create an additional name phrase table forNAMT.
Manual evaluation on 2,000 name pairsshowed the accuracy is 86%.The non-terminals in SCFG rules are rewrittento the extracted names during decoding, thereforeallow unseen names in the test data to be trans-lated.
Finally, based on LMs, our decoder ex-ploits the dynamically created phrase table fromname translation, competing with originally ex-tracted rules, to find the best translation for theinput sentence.4 Name-aware MT EvaluationTraditional MT evaluation metrics such asBLEU (Papineni et al, 2002) and Translation Ed-it Rate (TER) (Snover et al, 2006) assign thesame weights to all tokens equally.
For exam-ple, incorrect translations of ?the?
and ?Bush?
willreceive the same penalty.
However, for cross-lingual information processing applications, weshould acknowledge that certain informationallycritical words are more important than other com-mon words.
In order to properly evaluate the trans-lation quality of NAMT methods, we propose tomodify the BLEU metric so that they can dynam-ically assign more weights to names during evalu-ation.BLEU considers the correspondence between asystem translation and a human translation:BLEU = BP ?
exp( N?n=1wn log pn)(1)where BP is brevity penalty defined as follows:BP ={1 if c > r,e(1?r/c) if c ?
r. (2)where wn is a set of positive weights summing toone and usually uniformly set as wn = 1/N , c isthe length of the system translation and r is thelength of reference translation, and pn is modifiedn-gram precision defined as:pn =?C?Candidates?n-gram?CCountclip(n-gram)?C??Candidates?n-gram??C?Countclip(n-gram?
)(3)where C and C ?
are translation candidates in thecandidate sentence set, if a source sentence istranslated to many candidate sentences.As in BLEU metric, we first count the maxi-mum number of times an n-gram occurs in any s-ingle reference translation.
The total count of eachcandidate n-gram is clipped at sentence level by it-s maximum reference count.
Then we add up theweights of clipped n-grams and divide them by thetotal weight of all n-grams.Based on BLEU score, we design a name-awareBLEU metric as follows.
Depending on whether atoken t is contained in a name in reference trans-lation, we assign a weight weightt to t as follows:weightt ={1?
e?tf(t,d)?idf(t,D), if t never appears in names1 + PEZ , if t occurs in name(s)(4)where PE is the sum of penalties of non-nametokens and Z is the number of tokens within allnames:PE =?t never appears in namese?tf(t,d)?idf(t,D) (5)In this paper, the tf ?
idf score is computed at sen-tence level, therefore, D is the sentence set andeach d ?
D is a sentence.The weight of an n-gram in reference translationis the sum of weights of all tokens it contains.weightngram =?t?ngramweightt (6)Next, we compute the weighted modified n-gram precision Countweight?clip(n-gram) as fol-lows:Countweight?clip(n-gram) =?if the ngrami is correctly translatedweightngrami (7)607The Countclip(n-gram) in the equation 3 issubstituted with aboveCountweight?clip(n-gram).When we sum up the total weight of all n-grams ofa candidate translation, some n-grams may containtokens which do not exist in reference translation.We assign the lowest weight of tokens in referencetranslation to these rare tokens.We also add an item, name penalty NP , topenalize the output sentences which contain toomany or too few names:NP = e?(uv?1)2/2?
(8)where u is the number of name tokens in systemtranslation and v is the number of name tokens inreference translation.Finally the name-aware BLEU score is definedas:BLEUNA = BP ?NP ?
exp( N?n=1wn logwpn)(9)This new metric can also be applied to evalu-ate MT approaches which emphasize other typesof facts such as events, by simply replacing nametokens by other fact tokens.5 ExperimentsIn this section we present the experimental resultsof NAMT compared to the baseline MT.5.1 Data SetWe used a large Chinese-English MT training cor-pus from various sources and genres (includingnewswire, web text, broadcast news and broadcastconversations) for our experiments.
We also usedsome translation lexicon data and Wikipedia trans-lations.
The majority of the data sets were col-lected or made available by LDC for U.S. DARPATranslingual Information Detection, Extractionand Summarization (TIDES) program, Global Au-tonomous Language Exploitation (GALE) pro-gram, Broad Operational Language Translation(BOLT) program and National Institute of Stan-dards and Technology (NIST) MT evaluations.The training corpus includes 1,686,458 sentencepairs.
The joint name tagger extracted 1,890,335name pairs (295,087 Persons, 1,269,056 Geo-political entities and 326,192 Organizations).Four LMs, denoted LM1, LM2, LM3, andLM4, were trained from different English cor-pora.
LM1 is a 7-gram LM trained on the tar-get side of Chinese-English and Egyptian Arabic-English parallel text, English monolingual discus-sion forums data R1-R4 released in BOLT Phase1 (LDC2012E04, LDC2012E16, LDC2012E21,LDC2012E54), and English Gigaword Fifth Edi-tion (LDC2011T07).
LM2 is a 7-gram LM trainedonly on the English monolingual discussion fo-rums data listed above.
LM3 is a 4-gram LMtrained on the web genre among the target sideof all parallel text (i.e., web text from pre-BOLTparallel text and BOLT released discussion fo-rum parallel text).
LM4 is a 4-gram LM trainedon the English broadcast news and conversationtranscripts released under the DARPA GALE pro-gram.
Note that for LM4 training data, some tran-scripts were quick transcripts and quick rich tran-scripts released by LDC, and some were generatedby running flexible alignment of closed captions orspeech recognition output from LDC on the audiodata (Venkataraman et al, 2004).In order to demonstrate the effectiveness andgenerality of our approach, we evaluated our ap-proach on seven test sets from multiple genres anddomains.
We asked four annotators to annotatenames in four reference translations of each sen-tence and an expert annotator to adjudicate result-s.
The detailed statistics and name distribution ofeach test data set is shown in Table 1.
The per-centage of names occurred fewer than 5 times intraining data are listed in the brackets in the lastcolumn of the table.5.2 Overall PerformanceBesides the new name-aware MT metric, we alsoadopt two traditional metrics, TER to evaluate theoverall translation performance and Named EntityWeak Accuracy (NEWA) (Hermjakob et al, 2008)to evaluate the name translation performance.TER measures the amount of edits required tochange a system output into one of the referencetranslations.
Specifically:TER = # of editsaverage # of reference words (10)Possible edits include insertion, substitution dele-tion and shifts of words.The NEWA metric is defined as follows.
Us-ing a manually assembled name variant table, wealso support the matching of name variants (e.g.,?World Health Organization?
and ?WHO?
).NEWA = Count # of correctly translated namesCount # of names in references (11)608Corpus Genre Sentence # Word # Token # GPE(%) PER(%) ORG(%) All namesin source in reference (% occurred < 5)BOLT 1 forum 1,200 20,968 24,193 875(82.9) 90(8.5) 91(8.6) 1,056 (51.4)BOLT 2 forum 1,283 23,707 25,759 815(73.7) 141(12.8) 149(13.5) 1,105 (65.9)BOLT 3 forum 2,000 38,595 42,519 1,664(80.4) 204(9.8) 204(9.8) 2,072 (47.4)BOLT 4 forum 1,918 41,759 47,755 1,852(80.0) 348(25.0) 113(5.0) 2,313 (53.3)BOLT 5 blog 950 23,930 26,875 352(42.5) 235(28.3) 242(29.2) 829 (55.3)NIST2006 news&blog 1,664 38,442 45,914 1,660(58.2) 568(19.9) 625(21.9) 2,853 (73.1)NIST2008 news&blog 1,357 32,646 37,315 700(47.9) 367(25.1) 395(27.0) 1,462 (72.0)Table 1: Statistics and Name Distribution of Test Data Sets.Metric System BOLT 1 BOLT 2 BOLT 3 BOLT 4 BOLT 5 NIST2006 NIST2008BLEUBaseline 14.2 14.0 17.3 15.6 15.3 35.5 29.3NPhrase 14.1 14.4 17.1 15.4 15.3 35.4 29.3NAMT 14.2 14.6 16.9 15.7 15.5 36.3 30.0Name-aware BLEUBaseline 18.2 17.9 18.6 17.6 18.3 36.1 31.7NPhrase 18.1 18.8 18.5 18.1 18.0 35.8 31.8NAMT 18.4 19.5 19.7 18.2 18.9 39.4 33.1TERBaseline 70.6 71.0 69.4 70.3 67.1 58.7 61.0NPhrase 70.6 70.4 69.4 70.4 67.1 58.7 60.9NAMT 70.3 70.2 69.2 70.1 66.6 57.7 60.5NEWAAllBaseline 69.7 70.1 73.9 72.3 60.6 66.5 60.4NPhrase 69.8 71.1 73.8 72.5 60.6 68.3 61.9NAMT 71.4 72.0 77.7 75.1 62.7 72.9 63.2GPEBaseline 72.8 78.4 80.0 78.7 81.3 79.2 76.0NPhrase 73.6 79.3 79.2 78.9 82.3 82.6 79.5NAMT 74.2 80.2 82.8 80.4 79.3 85.5 79.3PERBaseline 53.3 44.7 45.1 49.4 48.9 54.2 51.2NPhrase 52.2 45.4 48.9 48.5 47.6 55.1 50.9NAMT 55.6 45.4 58.8 55.2 56.2 60.0 52.3ORGBaseline 56.0 49.0 52.9 38.1 41.7 44.0 41.3NPhrase 50.5 50.3 54.4 40.7 41.3 42.2 40.7NAMT 60.4 52.3 55.4 41.6 45.0 51.0 44.8Table 2: Translation Performance (%).For better comparison with NAMT, besides theoriginal baseline, we develop the other baselinesystem by adding name translation table into thephrase table (NPhrase).Table 2 presents the performance of overal-l translation and name translation.
We can seethat except for the BOLT3 data set with BLEUmetric, our NAMT approach consistently outper-formed the baseline system for all data sets withall metrics, and provided up to 23.6% relative er-ror reduction on name translation.
According toWilcoxon Matched-Pairs Signed-Ranks Test, theimprovement is not significant with BLEU metric,but is significant at 98% confidence level with allof the other metrics.
The gains are more signifi-cant for formal genres than informal genres main-ly because most of the training data for name tag-ging and name translation were from newswire.Furthermore, using external name translation tableonly did not improve translation quality in mosttest sets except for BOLT2.
Therefore, it is im-portant to use name-replaced corpora for rule ex-traction to fully take advantage of improved wordalignment.Many errors from the baseline MT approach oc-curred because some parts of out-of-vocabularynames were mistakenly segmented into commonwords.
For example, the baseline MT system mis-takenly translated a person name ?Y??
(SunHonglei)?
into ?Sun red thunder?.
In informalgenres such as discussion forums and web blogs,even common names often appear in rare form-s due to misspelling or morphing.
For example,?e8l (Obama)?
was mistakenly translated into?Ma Olympic?.
Such errors can be compoundedwhen word re-ordering was applied.
For example,the following sentence: ????????
/:'J/iy (Guo Meimei?s strength real-ly is formidable, I really admire her)?
was mis-takenly translated into ?Guo the strength of theAmerica and the America also really strong , ah, really admire her?
by the baseline MT systembecause the person name ????
(Guomeimei)?was mistakenly segmented into three words ??
(Guo)?, ??
(the America)?
and ??
(the Ameri-ca)?.
But our NAMT approach successfully iden-tified and translated this name and also generatedbetter overall translation: ?Guo Meimei ?s poweris also really strong , ah , really admire her?.609B L E U N a m e - a w a r eB L E U024681 01 21 41 61 82 0Score A u t o m a t i c  M e t r i c s H u m .
1  H u m .
2 H u m .
30 .
00 .
51 .
01 .
52 .
02 .
53 .
03 .
54 .
0 b a s e l i n e N A M T Score H u m a n  E v a l u a t i o nFigure 2: Scores based on Automatic Metrics and HumanEvaluation.5.3 Name-aware BLEU vs The HumanEvaluationIn order to investigate the correlation betweenname-aware BLEU scores and human judgmentresults, we asked three bi-lingual speakers to judgeour translation output from the baseline systemand the NAMT system, on a Chinese subset of 250sentences (each sentence has two correspondingtranslations from baseline and NAMT) extractedrandomly from 7 test corpora.
The annotators rat-ed each translation from 1 (very bad) to 5 (verygood) and made their judgments based on whetherthe translation is understandable and conveys thesame meaning.We computed the name-aware BLEU scores onthe subset and also the aggregated average scoresfrom human judgments.
Figure 2 shows thatNAMT consistently achieved higher scores withboth name-aware BLEU metric and human judge-ment.
Furthermore, we calculated three Pearsonproduct-moment correlation coefficients betweenhuman judgment scores and name-aware BLEU s-cores of these two MT systems.
Give the samplesize and the correlation coefficient value, the highsignificance value of 0.99 indicates that name-aware BLEU tracks human judgment well.5.4 Word AlignmentIt is also important to investigate the impact of ourNAMT approach on improving word alignmen-t. We conducted the experiment on the Chinese-English Parallel Treebank (Li et al, 2010) withground-truth word alignment.
The detailed pro-cedure following NAMT framework is as follows:(1) Ran the joint bilingual name tagger; (2) Re-placed each name string with its name type (PER,ORG or GPE), and ran Giza++ on the replacedsentences; (3) Ran Giza++ on the words withinWords Method P R FBaseline Giza++ 69.8 47.8 56.7Joint NameTagging70.4 48.1 57.1OverallWordsGround-truthName Tagging(Upper-bound)71.3 48.9 58.0Baseline Giza++ 86.0 31.4 46.0 WordsWithinNamesJoint NameTagging77.6 37.2 50.3Table 3: Impact of Joint Bilingual Name Tagging on WordAlignment (%).each name pair.
(4) Merged (2) and (3) to pro-duce the final word alignment results.
In order tocompare with the upper-bound gains, we also mea-sured the performance of applying ground-truthname tagging with the above procedures.The experiment results are shown in Table 3.For the words within names, our approach provid-ed significant gains by enhancing F-measure from46.0% to 50.3%.
Only 10.6% words are withinnames, therefore the upper-bound gains on over-all word alignment is only 1.3%.
Our joint nametagging approach achieved 0.4% (statistically sig-nificant) improvement over the baseline.
In Fig-ure 3 we categorized the sentences according tothe percentage of name words in each sentence andmeasured the improvement for each category.
Wecan clearly see that as the sentences include morenames, the gains achieved by our approach tend tobe greater.5.5 Remaining Error AnalysisAlthough the proposed model has significantly en-hanced translation quality, some challenges re-main.
We analyze some major sources of the re-maining errors as follows.1.
Name Structure Parsing.We found that the gains of our NAMT approachwere mainly achieved for names with one or twocomponents.
When the name structure becomestoo complicated to parse, name tagging and nametranslation are likely to produce errors, especiallyfor long nested organizations.
For example, ??0???b?@?
(Anti-malfeasance Bureau ofGutian County Procuratorate) consists of a nestedorganization name with a GPE as modifier: ??0???b?
(Gutian County Procuratorate) andan ORG name: ??@?
(Anti-malfeasance Bu-reau).2.
Name abbreviation tagging and translation.Some organization abbreviations are also dif-ficult to extract because our name taggers have6100~10 10~20 20~30 30~40 >40-0.50.00.51.01.52.02.53.03.54.04.55.05.5F-Measure Gains in Overall Word Alignment (%)#name tokens/#all tokens(%)Baseline Giza++Joint Name TaggingGround-truth Name Tagging (Upper-bound)Figure 3: Word alignment gains according to the percentageof name words in each sentence.not incorporated any coreference resolution tech-niques.
For example, without knowing that ?FAW?refers to ?First Automotive Works?
in ?FAW hasalso utilized the capital market to directly fi-nance, and now owns three domestic listed compa-nies?, our system mistakenly labeled it as a GPE.The same challenge exists in name alignment andtranslation (for example, ?
i (Min Ge)?
refer-s to ?
-??Zi}?X?
(RevolutionaryCommittee of the Chinese Kuomintang).3.
Cross-lingual information transferEnglish monolingual features normally gener-ate higher confidence than Chinese features forORG names.
On the other hand, some good prop-agated Chinese features were not able to correctEnglish results.
For example, in the following sen-tence pair: ?9n-?
?T??r???...
(in accordance with the tripartite a-greement reached by China, Laos and the UNHCRon)...?, even though the tagger can successfully la-bel ?T??r/UNHCR?
as an organizationbecause it is a common Chinese name, Englishfeatures based on previous GPE contexts still in-correctly predicted ?UNHCR?
as a GPE name.6 Related WorkTwo types of humble strategies were previouslyattempted to build name translation componentswhich operate in tandem and loosely integrate intoconventional statistical MT systems:1.
Pre-processing: identify names in the sourcetexts and propose name translations to theMT system; the name translation results canbe simply but aggressively transferred fromthe source to the target side using word align-ment, or added into phrase table in order toenable the LM to decide which translations tochoose when encountering the names in thetexts (Ji et al, 2009).
Heuristic rules or su-pervised models can be developed to create?do-not-translate?
list (Babych and Hartley,2003) or learn ?when-to-transliterate?
(Her-mjakob et al, 2008).2.
Post-processing: in a cross-lingual informa-tion retrieval or question answering frame-work, online query names can be utilizedto obtain translation and post-edit MT out-put (Parton et al, 2009; Ma and McKeown,2009; Parton and McKeown, 2010; Parton etal., 2012).It is challenging to decide when to use nametranslation results.
The simple transfer method en-sures all name translations appear in the MT out-put, but it heavily relies on word alignment anddoes not take into account word re-ordering orthe words found in a name?s context; therefore itcould mistakenly break some context phrase struc-tures due to name translation or alignment errors.The LM selection method often assigns an inap-propriate weight to the additional name transla-tion table because it is constructed independent-ly from translation of context words; therefore af-ter weighted voting most correct name translationsare not used in the final translation output.
Ourexperimental results 2 confirmed this weakness.More importantly, in these approaches the MTmodel was still mostly treated as a ?black-box?because neither the translation model nor the LMwas updated or adapted specifically for names.Recently the wider idea of incorporating seman-tics into MT has received increased interests.
Mostof them designed some certain semantic represen-tations, such as predicate-argument structure orsemantic role labeling (Wu and Fung, 2009; Liuand Gildea, 2009; Meyer et al, 2011; Bojar andWu, 2012), word sense disambiguation (Carpu-at and Wu, 2007b; Carpuat and Wu, 2007a) andgraph-structured grammar representation (Jones etal., 2012).
Lo et al (2012) proposed a semanticrole driven MT metric.
However, none of thesework declaratively exploited results from informa-tion extraction for MT.Some statistical MT systems (e.g.
(Zens et al,2005), (Aswani and Gaizauskas, 2005)) have at-tempted to use text normalization to improve wordalignment for dates, numbers and job titles.
Butlittle reported work has shown the impact of joint611name tagging on overall word alignment.Most of the previous name translation workcombined supervised transliteration approacheswith LM based re-scoring (Knight and Graehl,1998; Al-Onaizan and Knight, 2002; Huang etal., 2004).
Some recent research used compara-ble corpora to mine name translation pairs (Fenget al, 2004; Kutsumi et al, 2004; Udupa et al,2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999;Shao and Ng, 2004; Lu and Zhao, 2006; Hassanet al, 2007).
However, most of these approachesrequired large amount of seeds, suffered from In-formation Extraction errors, and relied on phonet-ic similarity, context co-occurrence and documen-t similarity for re-scoring.
In contrast, our namepair mining approach described in this paper doesnot require any machine translation or translitera-tion features.7 Conclusions and Future WorkWe developed a name-aware MT frameworkwhich tightly integrates name tagging and nametranslation into training and decoding of MT.
Ex-periments on Chinese-English translation demon-strated the effectiveness of our approach over ahigh-quality MT baseline in both overall transla-tion and name translation, especially for formalgenres.
We also proposed a new name-aware eval-uation metric.
In the future we intend to improvethe framework by training a discriminative modelto automatically assign weights to combine nametranslation and baseline translation with additionalfeatures including name confidence values, nametypes and global validation evidence, as well asconducting LM adaptation through bilingual top-ic modeling and clustering based on name anno-tations.
We also plan to jointly optimize MT andname tagging by propagating multiple word seg-mentation and name annotation hypotheses in lat-tice structure to statistical MT and conduct lattice-based decoding (Dyer et al, 2008).
Furthermore,we are interested in extending this framework totranslate other out-of-vocabulary terms.AcknowledgementThis work was supported by the U.S. Army Re-search Laboratory under Cooperative AgreementNo.
W911NF- 09-2-0053 (NS-CTA), the U.S. NS-F CAREER Award under Grant IIS-0953149, theU.S.
NSF EAGER Award under Grant No.
IIS-1144111, the U.S. DARPA FA8750-13-2-0041 -Deep Exploration and Filtering of Text (DEFT)Program and CUNY Junior Faculty Award.
Theviews and conclusions contained in this documen-t are those of the authors and should not be in-terpreted as representing the official policies, ei-ther expressed or implied, of the U.S. Govern-ment.
The U.S. Government is authorized to re-produce and distribute reprints for Governmen-t purposes notwithstanding any copyright notationhere on.
We express our gratitude to Bing Zhaowho provided the test sets and references that wereused for Broad Operational Language Translation(BOLT) evaluation and thanks to Taylor Cassidyfor constructive comments.ReferencesY.
Al-Onaizan and K. Knight.
2002.
TranslatingNamed Entities Using Monolingual and BilingualResources.
In Proceeding ACL?02, pages 400?408.N.
Aswani and R. Gaizauskas.
2005.
A Hybrid Ap-proach to Align Sentences and Words in English-Hindi Parallel Corpora.
In Proceeding ACL?05Workshop on Building and Using Parallel Texts,pages 57?64.Bogdan Babych and Anthony Hartley.
2003.
Im-proving Machine Translation Quality with Automat-ic Named Entity Recognition.
In Proceeding EAMT?03 workshop on MT and other Language Technol-ogy Tools, Improving MT through other LanguageTechnology Tools: Resources and Tools for BuildingMT, pages 1?8.O.
Bojar and D. Wu.
2012.
Towards a Predicate-Argument evaluation for MT.
In Proceeding of theSixth Workshop on Syntax, Semantics and Structurein Statistical Translation, pages 30?38, July.Marine Carpuat and Dekai Wu.
2007a.
How PhraseSense Disambiguation outperforms Word Sense Dis-ambiguation for Statistical Machine Translation.
InProceeding TMI?07, pages 43?52.Marine Carpuat and Dekai Wu.
2007b.
Improving Sta-tistical Machine Translation using Word Sense Dis-ambiguation.
In Proceeding EMNLP-CoNLL?07,pages 61?72.Taylor Cassidy, Heng Ji, Hongbo Deng, Jing Zheng,and Jiawei Han.
2012.
Analysis and Refinement ofCross-lingual Entity Linking.
In Proceeding CLE-F?12, pages 1?12.Stanley F. Chen and Joshua Goodman.
1996.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Proceeding of ACL?96, pages310?318.612David Chiang.
2005.
A Hierarchical Phrase-basedModel for Statistical Machine Translation.
In Pro-ceeding ACL?05, pages 263?270.F.
Dayne and K. Shahram.
2007.
A Sequence Align-ment Model Based on the Averaged Perceptron.
InProceeding EMNLP-CoNLL?07, pages 238?247.C.
Dyer, S. Muresan, and P. Resnik.
2008.
Generaliz-ing Word Lattice Translation.
In Proceeding ACL-HLT?08, pages 1012?1020.D.
Feng, Y. Lv, and M. Zhou.
2004.
A New Approachfor English-Chinese Named Entity Alignment.
InProceeding PACLIC?04, pages 372?379.R.
Florian, H. Jing, N. Kambhatla, and I. Zitouni.2006.
Factorizing Complex Models: A Case S-tudy in Mention Detection.
In Proceeding COLING-ACL?06, pages 473?480.P.
Fung and L. Y. Yee.
1998.
An IR Approach forTranslating New Words from Nonparallel and Com-parable Texts.
In Proceeding COLING-ACL?98,pages 414?420.D.
Hakkani-Tur, H. Ji, and R. Grishman.
2007.
Us-ing Information Extraction to Improve Cross-lingualDocument Retrieval.
In Proceeding RANLP Work-shop on Multi-source, Multilingual Information Ex-traction and Summarization, pages 17?23.A.
Hassan, H. Fahmy, and H. Hassan.
2007.
Im-proving Named Entity Translation by ExploitingComparable and Parallel Corpora.
In ProceedingRANLP?07, pages 1?6.U.
Hermjakob, K. Knight, and H. Daume III.
2008.Name Translation in Statistical Machine Transla-tion: Learning When to Transliterate.
In ProceedingACL?08, pages 389?397.F.
Huang, S. Vogel, and A. Waibel.
2004.
Im-proving Named Entity Translation Combining Pho-netic and Semantic Similarities.
In ProceedingHLT/NAACL?04, pages 281?288.H.
Ji and R. Grishman.
2006.
Analysis and Repairof Name Tagger Errors.
In Proceeding COLING-ACL?06, pages 420?427.H.
Ji, R. Grishman, D. Freitag, M. Blume, J. Wang,S.
Khadivi, R. Zens, and H. Ney.
2009.
NameExtraction and Translation for Distillation.
Hand-book of Natural Language Processing and MachineTranslation: DARPA Global Autonomous LanguageExploitation.H.
Ji.
2009.
Mining Name Translations from Com-parable Corpora by Creating Bilingual InformationNetworks.
In Proceeding ACL-IJCNLP?09 work-shop on Building and Using Comparable Corpora,pages 34?37.B.
Jones, J. Andreas, D. Bauer, K. M. Hermann, andK.
Knight.
2012.
Semantics-Based Machine Trans-lation with Hyperedge Replacement Grammars.
InProceeding COLING?12, pages 1359?1376.K.
Knight and J. Graehl.
1998.
Machine Translit-eration.
In Computational Linguistics, volume 24,pages 599?612, Cambridge, MA, USA, December.MIT Press.P.
Koehn, F. Josef Och, and D. Marcu.
2003.
Statis-tical Phrase-Based Translation.
In Proceeding HLT-NAACL?03, pages 127?133.T.
Kutsumi, T. Yoshimi, K. Kotani, and I. Sata.
2004.Integrated Use of Internal and External Evidence inThe Alignment of Multi-Word Named Entities.
InProceeding PACLIC?04, pages 187?196.X.
Li, S. Strassel, S. Grimes, S. Ismael, X. Ma, N. Ge,A.
Bies, N. Xue, and M. Maamouri.
2010.
ParallelAligned Treebank Corpora at LDC: Methodology,Annotation and Integration.
In Workshop on Anno-tation and Exploitation of Parallel Corpora (AEPC).Q.
Li, H. Li, H. Ji, W. Wang, J. Zheng, and F. Huang.2012.
Joint Bilingual Name Tagging for ParallelCorpora.
In Proceeding CIKM?12, pages 1727?1731.D.
Liu and D. Gildea.
2009.
Semantic Role Fea-tures for Machine Translation.
In Proceeding COL-ING?09, pages 716?724.C.
Lo, A. K. Tumuluru, and D. Wu.
2012.
Fully Au-tomatic Semantic MT Evaluation.
In Proceeding ofthe Seventh Workshop on Statistical Machine Trans-lation, pages 243?252.M.
Lu and J. Zhao.
2006.
Multi-feature basedChinese-English Named Entity Extraction fromComparable Corpora.
In Proceeding PACLIC?06,pages 134?141.W.
Ma and K. McKeown.
2009.
Where?s the Ver-b Correcting Machine Translation During QuestionAnswering.
In Proceeding ACL-IJCNLP?09, pages333?336.P.
McNamee, J. Mayfield, D. Lawrie, D. W. Oard, andD.
Doermann.
2011.
Cross-Language Entity Link-ing.
In Proceeding IJCNLP?11.A.
Meyer, M. Kosaka, S. Liao, and N. Xue.
2011.
Im-proving MT Word Alignment Using Aligned Multi-Stage Parses.
In Proceeding ACL-HLT 2011 Work-shop on Syntax, Semantics and Structure in Statisti-cal Translation, pages 88?97.T.
T. Nguyen, A. Moschitti, and G. Riccardi.
2010.Kernel-based Reranking for Named-Entity Extrac-tion.
In Proceeding COLING?10, pages 901?909.F.
J. Och and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
Computa-tional Linguistics, 29(1):19?51.613F.
J. Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceeding A-CL?03, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceeding ACL?02, pages311?318.K.
Parton and K. McKeown.
2010.
MT Error Detec-tion for Cross-Lingual Question Answering.
Pro-ceeding COLING?10, pages 946?954.K.
Parton, K. R. McKeown, R. Coyne, M. T. Dia-b, R. Grishman, D. Hakkani-Tur, M. Harper, H. Ji,W.
Y. Ma, A. Meyers, S. Stolbach, A.
Sun, G. Tur,W.
Xu, and S. Yaman.
2009. Who, What, When,Where, Why?
Comparing Multiple Approaches tothe Cross-Lingual 5W Task.
In Proceeding ACL-IJCNLP?09, pages 423?431.K.
Parton, N. Habash, K. McKeown, G. Iglesias,and A. de Gispert.
2012.
Can Automatic Post-Editing Make MT More Meaningful?
In ProceedingEAMT?12, pages 111?118.R.
Rapp.
1999.
Automatic Identification of WordTranslations from Unrelated English and GermanCorpora.
In Proceeding ACL?99, pages 519?526.L.
Shao and H. T. Ng.
2004.
Mining New Word Trans-lations from Comparable Corpora.
In ProceedingCOLING?04.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceeding ofAssociation for Machine Translation in the Americ-as, pages 223?231.M.
Snover, X. Li, W. Lin, Z. Chen, S. Tamang, M. Ge,A.
Lee, Q. Li, H. Li, S. Anzaroot, and H. Ji.
2011.Cross-lingual Slot Filling from Comparable Corpo-ra.
In Proceeding ACL?11 Worshop on Building andUsing Comparable Corpora, pages 110?119.D.
Talbot and T. Brants.
2008.
Randomized LanguageModels via Perfect Hash Functions.
In Proceedingof ACL/HLT?08, pages 505?513.R.
Udupa, K. Saravanan, A. Kumaran, and J. Jagarla-mudi.
2009.
MINT: A Method for Effective andScalable Mining of Named Entity Transliterationsfrom Large Comparable Corpora.
In ProceedingEACL?09, pages 799?807.A.
Venkataraman, A. Stolcke, W. Wang, D. Vergyri,V.
R. R. Gadde, and J. Zheng.
2004.
An EfficientRepair Procedure For Quick Transcriptions.
In Pro-ceeding INTERSPEECH?04, pages 1961?1964.D.
Wu and P. Fung.
2009.
Semantic Roles for SMT: AHybrid Two-Pass Model.
In NAACL HLT?09, pages13?16.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHPhrase-based Statistical Machine Translation Sys-tem.
In Proceeding IWSLT?05, pages 155?162.J.
Zheng, N. F. Ayan, W. Wang, and D. Burkett.2009.
Using Syntax in Large-Scale Audio Doc-ument Translation.
In Proceeding Interspeech?09,pages 440?443.J.
Zheng.
2008.
SRInterp: SRI?s Scalable Multipur-pose SMT Engine.
In Technical Report.I.
Zitouni and R. Florian.
2008.
Mention Detec-tion Crossing the Language Barrier.
In ProceedingEMNLP?08, pages 600?609.614
