Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256?264,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsAnnotating Participant Reference in English Spoken ConversationJohn Niekrasz and Johanna D. MooreSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UK{jniekras,jmoore}@inf.ed.ac.ukAbstractIn conversational language, references topeople (especially to the conversation par-ticipants, e.g., I, you, and we) are an es-sential part of many expressed meanings.In most conversational settings, however,many such expressions have numerous po-tential meanings, are frequently vague,and are highly dependent on social and sit-uational context.
This is a significant chal-lenge to conversational language under-standing systems ?
one which has seenlittle attention in annotation studies.
In thispaper, we present a method for annotat-ing verbal reference to people in conver-sational speech, with a focus on referenceto conversation participants.
Our goal isto provide a resource that tackles the is-sues of vagueness, ambiguity, and contex-tual dependency in a nuanced yet reliableway, with the ultimate aim of supportingwork on summarization and informationextraction for conversation.1 IntroductionSpoken conversation ?
the face-to-face verbal in-teraction we have every day with colleagues, fam-ily, and friends ?
is the most natural setting forlanguage use.
It is how we learn to use languageand is universal to the world?s societies.
Thismakes it an ideal subject for research on the ba-sic nature of language and an essential subject forthe development of technologies supporting natu-ral communication.
In this paper, we describe ourresearch on designing and applying an annotationprocedure for a problem of particular relevance toconversational language ?
person reference.The procedure is a coreference annotation of allreferences to people, and the focus of our schemeis on distinguishing different types of participantreference (references to the conversation?s partic-ipants), the predominant type of person referencein face-to-face multi-party conversation.
Partici-pant reference is exemplified by the use of propernames such as James or most commonly by thepronouns I, you, and we.Participant reference plays an essential role inmany of the most important types of expressedmeanings and actions in conversation, includingsubjective language, inter-personal agreements,commitments, narrative story-telling, establishingsocial relationships, and meta-discourse.
In fact,some person-referring words are the most frequentwords in conversation.1Perhaps contrary to intuition, however, in-terpreting person-referring expressions can berather complex.
Person-reference interpretationis strongly dependent on social, situational, anddiscourse context.
The words you and weare especially problematic.
Either can be usedfor generic, plural, or singular reference, asaddressee-inclusive or addressee-exclusive, in ref-erence to hypothetical individuals or non-humanentities, or even metonymically in reference to ob-jects connected to individuals (Mu?hlha?usler andHarre?, 1990; Wales, 1996).
In addition, these andmany other issues are not simply occasional prob-lems but arise regularly.Consider the following utterance from the AMIcorpus of remote control design meetings, whichis typical of the corpus in terms of complexity ofperson-reference.1The words I and you are the most frequently used nom-inals in several conversational corpora, including Switch-board (Godfrey et al, 1992) and the AMI Meeting Cor-pus (McCowan et al, 2005).
In the British National Corpusthey are the two most common of any words in the demo-graphic (i.e., conversational) subcorpus (Burnard, 2007), andGoogle?s Web 1T 5-gram statistics (Brants and Franz, 2006)list I and you as more frequent even than the word it.
Theword we falls within the top 10 most frequent words in all ofthese corpora.256?Current remote controls do not match well withthe operating behaviour of the user overall.
Forexample, you can see below there, seventy fivepercent of users zap a lot, so you?ve got yourperson sunk back in the sofa channel-hopping.
?As this example demonstrates, person-referringexpressions have many potential meanings and areoften vague or non-specific.
In this case, ?theuser?
refers to a non-specific representative of ahypothetical group, which is referred to itself as?users.?
The first use of ?you?
refers to the ad-dressees, but the second use has a more ?generic?meaning whilst retaining an addressee-orientedmeaning as well.
The phrase ?your person?
refersto a specific hypothetical example of the ?users?referred to previously.1.1 Purpose of the AnnotationsThe annotation research we describe here aims ataddressing the fact that if conversational languageapplications are to be useful and effective (ourinterest is primarily with abstractive summariza-tion), then accurate interpretation of reference tothe conversation?s participants is of critical impor-tance.
Our work looks at language as a means foraction (Clark, 1996), and our focus is on those ac-tions that the participants themselves consider asrelevant and salient, such as the events occurringin a meeting that might appear in the minutes ofthe meeting.
For our system to identify, distin-guish, or describe such events, it is essential forit to understand the participants?
roles and rela-tionships to those events through interpreting theirlinguistic expression within the dialogue.
This in-cludes understanding direct reference to partici-pants and recognizing discourse structure throughevidence of referential coherence.Another aim of our research is to increase un-derstanding of the nature of participant referencethrough presenting a nuanced yet reliable set oftype and property distinctions.
We propose noveldistinctions concerning three main issues.
Thefirst distinction concerns vagueness and indetermi-nacy, which is often exploited by speakers whenusing words such as you, they, and we.
Our aimis to provide a reliable basis for making an ex-plicit distinction between specific and vague uses,motivated by usefulness to the aforementioned ap-plications.
The second distinction concerns anissue faced frequently in informal conversation,where words typically used to do person-referringare also commonly used in non-person-referringways.
A principal goal is thus establishing reliableperson/non-person and referential/non-referentialdistinctions for these words.
The third issue con-cerns addressing roles (i.e., speaker, addressee,and non-addressee), which we propose can be auseful means for further distinguishing betweendifferent types of underspecified and generic refer-ences, beyond the specific/underspecified/genericdistinctions made in schemes such as ACE (Lin-guistic Data Consortium, 2008).1.2 Summary and Scope of ContributionsThe work described in this paper includes the de-sign of an annotation procedure and a statisticalanalysis of a corpus of annotations and their re-liability.
The procedure we propose (Section 3)is based on a simple non-anaphoric coreference-like scheme, modest in comparison to much pre-vious work.
The produced dataset (Section 4) in-cludes annotations of 11,000 occasions of person-referring in recorded workplace meetings.
Ouranalysis of the dataset includes a statistical sum-mary of interesting results (Section 4.1) and ananalysis of inter-coder agreement (with discussionof specific disagreements) for the introduced dis-tinctions (Section 4.2).Though our annotation procedure is designedprimarily for multi-party spoken conversation,some of the central issues that concern us, suchas addressee inclusion and vagueness, arise intextual and non-conversational settings as well.Our scheme therefore has relevance to generalwork on reference annotation, though principallyto settings where social relationships betweenthe participants (i.e., speakers/authors and ad-dressees/readers) are important.2 Related Annotation SchemesPrevious work on reference annotation has cov-ered a wide range of issues surrounding referencegenerally.
It is useful to categorize this work ac-cording to the natural language processing tasksthe annotations are designed to support.2.1 Schemes for anaphora and generationSeveral schemes have been designed with the goalof testing linguistic theoretical models of dis-course structure or for use in the study of discourseprocessing problems like anaphora resolution andreference generation.
These schemes have beenapplied to both text and dialogue and label dis-257course references with a rich set of syntactic, se-mantic, and pragmatic properties.
For example,the DRAMA scheme (Passonneau, 1997) and theGNOME scheme (Poesio, 2000; Poesio, 2004) in-clude labels for features such as bridging relationtype and NP type in addition to a rich representa-tion of referent semantics.
Other schemes label an-imacy, prosody, and information structure to studytheir relationship to the organization and salienceof discourse reference (Nissim et al, 2004; Cal-houn et al, 2005).
Recent developments includethe explicit handling of anaphoric ambiguity anddiscourse deixis (Poesio and Artstein, 2008).Despite the depth and detail of these schemes,participant reference has not been their main con-cern.
The annotations by Poesio et al (2000;2004) include dialogue source material, but therather constrained interactional situations do notelicit a rich set of references to participants.
Thescheme thus employs simple default labels forwords like I and you.
The work by Nissim etal., (2004) is an annotation of the Switchboard cor-pus (Godfrey et al, 1992), which contains onlytwo participants who are neither co-present norsocially connected.
Participant reference is thusrather constrained.
Other than labeling corefer-entiality, the Nissim scheme includes only a sin-gle distinction between referential and generic in-stances of the word you.2.2 Schemes for information extractionIn contrast to the schemes described above, whichare mainly driven toward investigating linguistictheories of discourse processing, some referenceannotation projects are motivated instead by infor-mation extraction applications.
For these projects(which includes our own), a priority is placed onentity semantics and coreference to known entitiesin the world.
For example, the objective of the Au-tomatic Content Extraction (ACE) program (Dod-dington et al, 2004) is to recognize and extractentities, events, and relations between them, di-rectly from written and spoken sources, mostlyfrom broadcast news.
The schemes thus focuson identifying and labeling the properties of en-tities in the real world, and then marking expres-sions as referring to these entities.
Recent workin the ACE project has expanded the scope ofthis task to include cross-document recognitionand resolution (Strassel et al, 2008).
In the ACEscheme (Linguistic Data Consortium, 2008), per-son reference is a central component, and in thebroadcast conversation component of the corpusthere is an extensive inventory of participant refer-ences.
The annotation scheme contains a distinc-tion between specific, underspecified, and generalentities, as well as a distinction between personsand organizations.Another closely related set of studies are fourrecent investigations of second-person referenceresolution (Gupta et al, 2007a; Gupta et al,2007b; Frampton et al, 2009; Purver et al,2009).
These studies are based upon a commonset of annotations of the word you in source mate-rial from the Switchboard and ICSI Meeting cor-pora.
The purpose for the annotations was tosupport learning of classifiers for two main prob-lems: disambiguation of the generic/referentialdistinction, and reference resolution for referentialcases.
In addition to the generic/referential dis-tinction and an addressing-based reference anno-tation, the scheme employed special classes for re-ported speech and fillers and allowed annotators toindicate vague or difficult cases.
Our work buildsdirectly upon this work by extending the annota-tion scheme to all person-referring expressions.3 Annotation MethodOur person-reference annotation method consistsof two main phases: a preliminary phase wherethe first names of the conversation participants areidentified, and a subsequent person reference la-beling process.
The first phase is not of centralconcern in this paper, though we provide a briefsummary below (Section 3.2).
The primary focusof this paper is the second phase (Section 3.3), dur-ing which every instance of person-referring oc-curring in a given meeting is labelled.
We pro-vide more detail concerning the most novel andchallenging aspects of the person-referring label-ing process in Section 3.4 and present a brief sum-mary of the annotation tool in Section 3.5.3.1 Source MaterialThe source material is drawn from two sourcecorpora: the AMI corpus (McCowan et al,2005), which contains experimentally-controlledscenario-driven design meetings, and the ICSI cor-pus (Janin et al, 2003), which contains naturallyoccurring workplace meetings.
All the meetingshave at least four participants and have an averageduration of about 45 minutes.
In the AMI corpus,258the participants are experimental subjects who areassigned institutional roles, e.g.
project managerand industrial designer.
This helps to establishcontrolled social relationships within the group,but generally limits the types of person referring.The ICSI meetings are naturally occurring and ex-hibit complex pre-existing social relationships be-tween the participants.
Person referring in this cor-pus is quite complex and often includes other in-dividuals from the larger institution and beyond.3.2 Labeling Participant NamesThe first phase of annotation consists of identify-ing the names of the participants.
We perform thistask for every participant in every meeting in theAMI and ICSI source corpora, which totals 275unique participants in 246 meetings.
Despite thefact that the participants?
are given anonymizedidentifiers by the corpus creators, determining par-ticipants?
names is possible because name men-tions are not excised from the speech transcript.This allows identification of the names of any par-ticipants who are referred to by name in the dia-logue, as long as the referent is disambiguated bycontextual clues such as addressing.To extract name information, the list of capi-talized words in the speech transcript is scannedmanually for likely person names.
This was donemanually due to the difficulty of training a suffi-ciently robust named-entity recognizer for thesecorpora.
Proceeding through each meeting forwhich any participant names are yet unidentified,and taking each potential name token in orderof frequency of occurrence in that meeting, shortsegments of the recording surrounding the occur-rences were replayed.
In most cases, the name wasused in reference to a participant and it was clearfrom discourse context which participant was theintended referent.
In the AMI meetings, 158 of223 (71%) of the participants?
first names wereidentified.
In the ICSI meetings, 36 of 52 (69%)were identified.
While these numbers may seemlow, failure to determine a name was generally as-sociated with a low level of participation of theindividual either in terms of amount of speech ornumber of meetings attended.
As such, the propor-tion of utterances across both corpora for whichthe speaker?s name is identified is actually 91%.3.3 Person-reference AnnotationThe second, principal phase of annotation con-sists of annotating person-referring ?
instancesof verbal reference to people.
The recognitionof person-referring requires the annotator to si-multaneously identify whether a referring eventhas occurred, and whether the referent is a per-son.
In practice, this is divided into four an-notation steps: markable identification, referentidentification, functional category labeling, andco-reference linking.
For non-specific references,there is an additional step of labeling addressingproperties.
For each meeting, annotators label ev-ery instance of person-referring in every utterancein the meeting, performing the steps in sequencefor each utterance.
Section 4 describes the set ofmeetings annotated.
The UML diagram in Fig-ure 1 depicts the formal data structure producedby the procedure.2The first step is markable identification, whichinvolves recognizing person-referring expres-sions in the transcript.
Only expressions that arenoun phrases are considered, and only the headnoun is actually labeled by the annotator ?
theextent of the expression is not labeled.
These iden-tified head nouns are called markables.
Note,however, that before human annotation begins, anautomatic process identifies occurrences of wordsthat are likely to be head nouns in person-referringexpressions.
The list of words includes all per-sonal pronouns except it, them, and they (theseare more likely to be non-person-referring in ourdataset) and the wh-pronouns (not labeled in ourscheme).
It also includes any occurrences ofthe previously identified proper names.
Some ofthe automatically identified words might not beperson-referring.
Also, there may be instances ofperson-referring that are not automatically iden-tified.
Annotators do not unmark any of the au-tomatically identified words, even if they are notperson-referring.
The resulting set of manuallyand automatically identified words, which may ormay not be person-referring, constitute the com-plete set of markables.The second step is the labeling of person refer-ents.
Any people or groups of people that are re-ferred to specifically and unambiguously (see Sec-tion 3.4.3 for details) are added by the annotatorto a conversation referent list.
The list is auto-matically populated with each of the conversationparticipants.2The diagram may also be viewed informally as looselyreflecting a decision tree for the main annotation steps.
Acomplete coding manual is available from the author?s website.259ATTR-QUANTIFIED-SUPERSET: Booleancategory: Enum: {FUNC-PREF-VOCATIVE,FUNC-PREF-INTRODUCTION,FUNC-PREF-TROUBLE,FUNC-PREF-DEFAULT }Person-Referring Markable1..*1Referent List*1Transcript*1category: Enum: {FUNC-FILLER,FUNC-NON-PREF }Non-Referring MarkablePerson ReferentATTR-SPEAKER-INCL: BooleanATTR-ADDRESSEE-INCL: BooleanATTR-OTHER-INCL: BooleanUnderspecified Referent (PERSON-OTHER)id: StringNICKNAME: StringSpecific Real ReferentPERSON-SINGLE PERSON-MULTIPLE2..*0..*membersspeaker: Participantword: StringstartTime: DoubleWordConversationMarkable Word Non-Markable WordFigure 1: A UML diagram depicting the data structure used to represent and store the annotations.The third step consists of labeling markableswith a functional category (FUNC-*).
The func-tional categories serve two main purposes.
Theyare used to distinguish person-referring markablesfrom all others (corresponding to the two mainboxes in the diagram), and they are used to distin-guish between specific dialogue purposes (the cat-egories listed within the boxes, see Section 3.4.4).The final step is to link the markables that werelabeled as person-referring to the appropriate ref-erent in the referent list.
This is only done forspecific and unambiguous referring.
Otherwise,the referent is said to be underspecified, and in-stead of linking the markable to a referent, it is la-beled with three binary addressing inclusion at-tributes.
Inclusion attributes label whether thespeaker, addressee, or any other individuals are in-cluded in the set of people being referred to, giventhe social, situational, and discourse context (de-tails in Section 3.4.5).3.4 Special Issues3.4.1 Defining ?person?
and ?referring?To be person-referring, an expression must sat-isfy two conditions.
First, the expression?s pri-mary contribution to the speaker?s intended mean-ing or purpose must be either to identify, label,describe, specify, or address.
These are the ba-sic types of referring.
Second, the referent beingidentified, labeled, etc., must be a person, whichwe define to include any of the following: a dis-tinct person in the real world; a fictitious or hypo-thetical person; a human agent, perceiver, or par-ticipant in a described event, scene, or fact; a class,type, or kind of person, or representative thereof;a specification or description of a person or set ofpeople; a (possibly vaguely defined) group or col-lection of any of the above; the human race as awhole, or a representative thereof.If a noun phrase is used to do person-referringas defined, the associated markable is labeled withone of the four person-referring functional cat-egories (FUNC-PREF-*).
If a markable is notperson-referring (either non-referring or referringto a non-person referent), it is labeled with thefunctional category FUNC-NON-PREF.
The oneexception to this is the use of a pre-defined list ofcommon discourse fillers such as you know and Imean.
When used as fillers, these are labeled withthe non-referential FUNC-FILLER category.2603.4.2 Joint action and referring ?trouble?Annotators are asked to consider occasions of re-ferring to be joint actions between the speaker andthe addressee(s) of the utterance.
The annotatorassumes the role of an overhearer and considersas referring any case where the speaker?s intendedpurpose is to refer.
If the instance of referringis not successfully negotiated between the partic-ipants (i.e., common ground is not achieved), butthe speaker?s intended purpose is to refer, then theannotator marks this as FUNC-PREF-TROUBLE.This is used to identify problematic cases for fu-ture study.3.4.3 Specific, Unambiguous ReferringOnly the referents of specific, unambiguous re-ferring to a person in the real world (PERSON-SINGLE) are included in the conversation referentlist and made the subject of coreference annota-tion.
References to more than one such individualcan qualify (PERSON-MULTIPLE), but only if themembers are precisely enumerable and qualify in-dividually.
The motivation for this distinction is todistinguish references that would be directly use-ful to applications.
Coreference for underspecifiedreferences is not labeled.3.4.4 Special Functional CategoriesTwo functional categories are used to distinguishspecial uses of person-referring for subsequentuse in speaker name induction (the task of auto-matically learning participants?
names).
The twocategories are FUNC-PREF-INTRODUCTION andFUNC-PREF-VOCATIVE, which specify personalintroductions such as ?Hi, I?m John,?
and vocativeaddressing such as ?What do you think, Jane?
?These categories are used only for proper names.3.4.5 Addressing-based Inclusion AttributesA major novelty in our annotation scheme is theuse of addressing-based distinctions for under-specified referents.
Rather than using the labels?generic?
or ?indeterminate?, we employ three bi-nary attributes (ATTR-*-INCL) that label whetherthe speaker, addressee or any other real individualsare members of the set of people referred to.The use of this distinction is informed by the no-tion that addressing distinctions are of central im-portance to the recognition of joint activity type,structure, and participation roles.
A generic pro-noun, for example, will often have all three cat-egories labeled positively.
But as an exampleof where this scheme creates a novel distinction,consider the phrase ?You really take a beatingout there on the pitch!
?, where the speaker is afootball player describing the nature of play tosomeone who has never played the game.
This?generic?
use of you, used in an activity of autobi-ographical description, is intuitively interpreted asnot including the addressee (ATTR-ADDRESSEE-INCL=FALSE) but including the speaker and others(ATTR-{SPEAKER,OTHER}-INCL=TRUE).
Thesedistinctions are hard to motivate linguistically yetcritical to identifying useful properties relating toparticipation in the communicative activity.3.4.6 Special or Difficult CasesIn some cases, an annotator can determine that areference is specific and unambiguous for the par-ticipants but the annotator himself is unable to de-termine the identity of the referent.
This is gener-ally due to a lack of contextual awareness such asnot having adequate video.
In such cases, the an-notator assigns a special REF-UNKNOWN referent.Other difficult aspects of our annotation proce-dure are covered in the annotation manual, includ-ing handling of disfluencies, quantification, andidentifying lexical heads.3.5 Annotation ToolThe annotations were collected using a softwaretool we have designed for discrete event-based an-notation of multi-modal corpora.
The tool uses asimple, low-latency text-based interface that dis-plays multiple streams of discrete events in tempo-ral order across the screen.
In our case, the eventsare time-synchronized words that are distributedto different rows according to speaker.
The inter-face allows keyboard input only and is synchro-nized with the MPlayer playback engine.4 Results and Analysis4.1 Statistical summaryThe dataset consists of approximately 11,000 in-dividually annotated referring expressions in 16experimentally-controlled, scenario-driven designmeetings from the AMI corpus (McCowan et al,2005) and 3 natural workplace meetings fromthe ICSI corpus (Janin et al, 2003).
Figure 2shows, for each grammatical type of referring ex-pression, the frequency of occurrence of the fiveprincipal markable types, which are defined toconsist of the two non-person-referring functional261OTHERQUANT3PP3PS2P1PP1PSFUNC?PREF / PERSON?SINGLEFUNC?PREF / PERSON?MULTIPLEFUNC?PREF / PERSON?OTHERFUNC?NON?PREFFUNC?FILLER0 500 1000 2000 3000Figure 2: Frequency of occurrence of referringtypes for the whole corpus, by grammatical typeof the referring expression.categories (FUNC-NON-PREF and FUNC-FILLER),and a breakdown of person-referring accordingto the type of person referent: a specific indi-vidual (PERSON-SINGLE), multiple specific indi-viduals (PERSON-MULTIPLE), or underspecified(PERSON-OTHER).
The grammatical types in-clude a grouping of the personal pronouns bygrammatical person and number (1PS, 1PP, 2P,3PS, 3PP), the quantified pronouns (QUANT), anda group including all other expressions (OTHER).Table 1 shows the relative frequency for the gram-matical types and the most frequent expressions.As is usually found in conversation, first-person and second-person pronouns are the mostfrequent, collectively comprising 82.0% of allperson-referring expressions.
Of particular inter-est, due to their high frequency and multiple possi-ble referential meanings, are the 1PP and 2P cate-gories (e.g., we and you), comprising respectively24.6% and 23.7% of all person-referring expres-Gram.
Freq.
Ent.
Freq.
words(%) (bits)1PS 33.7 .57 I, my, me1PP 24.6 .67 we, our, us2P 23.7 1.78 you, your, yours3PS .9 .66 he, his, she3PP 7.2 1.25 they, them, theirQUANT 1.0 1.14 everyone, everybodyOTHER 8.9 1.57 people, guys, userTable 1: A statistical summary of all the mark-ables in the dataset by grammatical type (gram.
),showing their frequency relative to all markables(freq.
), the entropy of the referring type given thegrammatical type (ent.
), and a list of the most fre-quent examples (freq.
words).sions.
In Table 1, we show the information en-tropy of the referring type, given the grammati-cal category.
This measures the uncertainty onehas about the type, given knowledge of only thegrammatical type of the expression.
The analysisreveals that second-person pronouns are a partic-ularly challenging reference resolution problem,with a broad and relatively even distribution acrossreferring types.4.2 Reliability and Error AnalysisTo show that our annotations are credible and suit-able for empirical testing, we must establish thatthe subjective distinctions defined in our schememay be applied by individuals other than thescheme developers.
To do this, we assess inter-coder agreement between two independent anno-tators on four meetings from the AMI corpus, us-ing Cohen?s Kappa (Cohen, 1960).
Each of thedecisions in the annotation procedure are assessedseparately: markable identification, labeling ref-erentiality, labeling specificity of person refer-ents, and labeling addressing inclusion attributes.Because each decision depends on the previous,we employ a hierarchical assessment procedurethat considers only instances where the annota-tors have agreed on previous decisions.
This kindof multi-level assessment corresponds to that de-scribed and used in Carletta et al, (1997).Markables The first annotation decision of in-terest is the identification of markables.
Markablesare either automatically identified occurrences ofa pre-defined list of pronouns, or they are identi-262fied manually by the annotators.
Agreement onthis task, assessed only for manually identifiedwords, was very good (?=.94).
Error analysisshows that the main issue with this decision wasnot determining lexical heads, but rather deter-mining whether phrases such as ?all age groups,?
?the older generation,?
and ?the business market?should be considered as referring to people or not.Person referentiality The next annotation deci-sion is between person-referring and non-person-referring markables.
For assessment of thischoice, we measure agreement on a three-waycategorization of the agreed markables as eitherFUNC-NON-PREF, FUNC-FILLER, or one of theFUNC-PREF-* categories.
Agreement on this taskwas good (?=.77).
The only errors occurred onfirst- and second-person pronouns and between theFUNC-NON-PREF and FUNC-PREF-* categories.Error analysis suggests confusion tends to occurwhen pronouns are used with semantically lightverbs like go, get, and have, for example in phrasessuch as ?there we go?
and ?you?ve got the mainthings on the front.?
As in the latter example,some of the difficult choices appear to involve de-scriptions of states, which the speaker can chooseto express either from various participants?
pointsof view, as above, or alternatively without ex-plicit subjectivity, e.g., ?the main things are on thefront.
?Specificity and cardinality The next choice weassess is the decision between referring specif-ically to a single person (PERSON-SINGLE), tomultiple people (PERSON-MULTIPLE), or as un-derspecified (also referred to as PERSON-OTHER).Agreement on this choice was very good (?=.91),though considering only the difficult 1PP and 2Pgrammatical categories (e.g., we and you), agree-ment was less strong (?=.75).
Note that due to thehierarchical nature of the scheme, evaluation con-sidered only cases where both annotators labeleda word as person-referring.
Errors on this decisionoften involved ambiguities in addressing, whereone annotator believed a particular individual wasbeing addressed by you and the other thought thewhole group was being addressed.
Another com-mon disagreement was on cases such as ?we wantit to be original,?
where we was interpreted by oneannotator as referring to the present group of par-ticipants, but by the other as (presumably) refer-ring to the organization to which the participantsbelong.Addressing inclusion attributes For the threeinclusion attributes for underspecified referents(ATTR-*-INCL), agreement is calculated threetimes, once for each of the binary attributes.Agreement was good, though slightly problematicfor addressee inclusion (speaker ?=.72; addressee?=.50; other ?=.66).
Disagreements were mainlyfor occurrences of you like the example of autobi-ography in Section 3.4.5.
For example, ?it?s yourbest friend?
was used to explain why a dog is thespeaker?s favorite animal, and the annotators dis-agreed on whether the addressee was included.5 ConclusionWe have presented an annotation scheme and aset of annotations that address participant refer-ence ?
a conversational language problem thathas seen little previous annotation work.
Our fo-cus has been on eliciting novel distinctions that wehypothesize will help us to distinguish, label, andsummarize conversational activities.
We also ad-dress the issues of vagueness, ambiguity, and con-textual dependency in participant referring.Based on analysis of inter-annotator agreement,the major distinctions proposed by the scheme ap-pear to be reliably codable.
In addition, our sta-tistical analysis shows that our dataset contains awide variety of participant references and shouldbe a useful resource for several reference resolu-tion problems for conversation.
Our novel methodfor distinguishing specific reference to real indi-viduals appears to be very reliably codable.
Ournovel addressing-based distinctions for underspec-ified reference are less reliable but adequate as aresource for some dialogue structuring tasks.Further work proposed for this task includeslabeling a variety of conversational and non-conversation genres.
Our immediate concern is toapply our annotations in the training and/or test-ing of machine learning approaches to discoursesegmentation and abstractive summarization.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gram, Version 1.
Linguistic Data Consortium.
Cat-alog ID: LDC2006T13.Lou Burnard, 2007.
Reference Guide for the BritishNational Corpus (XML Edition).
Research Tech-nologies Service at Oxford University ComputingServices.263Sasha Calhoun, Malvina Nissim, Mark Steedman, andJason Brenier.
2005.
A framework for annotatinginformation structure in discourse.
In Proceedingsof the ACL Workshop on Frontiers in Corpus Anno-tation II: Pie in the Sky.Jean Carletta, Stephen Isard, Anne H. Anderson,Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-line C. Kowtko.
1997.
The reliability of a dialoguestructure coding scheme.
Computational Linguis-tics, 23(1):13?31.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press, Cambridge.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20:37?46.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The Automatic Content Extrac-tion (ACE) program: Tasks, data, and evaluation.
InProc.
LREC.Matthew Frampton, Raquel Fernndez, Patrick Ehlen,Mario Christoudias, Trevor Darrell, and Stanley Pe-ters.
2009. Who is ?you??
Combining linguis-tic and gaze features to resolve second-person ref-erences in dialogue.
In Proc.
EACL.John J. Godfrey, Edward Holliman, and J. McDaniel.1992.
SWITCHBOARD: Telephone speech corpusfor research and development.
In Proc.
ICASSP,pages 517?520, San Francisco, CA.Surabhi Gupta, John Niekrasz, Matthew Purver, andDaniel Jurafsky.
2007a.
Resolving ?you?
in multi-party dialog.
In Proc.
SIGdial, pages 227?230.Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.2007b.
Disambiguating between generic and refer-ential ?you?
in dialog.
In Proc.
ACL.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The ICSI meeting cor-pus.
In Proc.
ICASSP, volume 1, pages 364?367.Linguistic Data Consortium, 2008.
ACE (Au-tomatic Content Extraction) English AnnotationGuidelines for Entities, Version 6.5.
Down-loaded from http://projects.ldc.upenn.edu/ace/annotation/.I.
McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,V.
Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-coln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Cor-pus.
In Proceedings of Measuring Behavior 2005,the 5th International Conference on Methods andTechniques in Behavioral Research, Wageningen,Netherlands.Peter Mu?hlha?usler and Rom Harre?.
1990.
Pronounsand People: The Linguistic Construction of Socialand Personal Identity.
Blackwell, Oxford.Malvina Nissim, Shipra Dingare, Jean Carletta, andMark Steedman.
2004.
An annotation scheme forinformation status in dialogue.
In Proc.
LREC.R.
Passonneau, 1997.
Instructions for applying dis-course reference annotation for multiple applica-tions (DRAMA).Massimo Poesio and Ron Artstein.
2008.
Anaphoricannotation in the ARRAU corpus.
In Proc.
LREC.Massimo Poesio, 2000.
The GNOME AnnotationScheme Manual, Version 4.
University of Edin-burgh, HCRC and Informatics.Massimo Poesio.
2004.
Discourse annotation and se-mantic annotation in the GNOME corpus.
In Pro-ceedings of the ACL 2004 Workshop on DiscourseAnnotation, pages 72?79.Matthew Purver, Raquel Fernndez, Matthew Framp-ton, and Stanley Peters.
2009.
Cascaded lexicalisedclassifiers for second-person reference resolution.In Proc.
SIGdial, pages 306?309.Stephanie Strassel, Mark Przybocki, Kay Peterson,Zhiyi Song, and Kazuaki Maeda1.
2008.
Linguisticresources and evaluation techniques for evaluationof cross-document automatic content extraction.
InProc.
LREC.Katie Wales.
1996.
Personal pronouns in present-dayEnglish.
Cambridge University Press, Cambridge.264
