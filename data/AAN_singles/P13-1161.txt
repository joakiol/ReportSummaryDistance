Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640?1649,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsJoint Inference for Fine-grained Opinion ExtractionBishan YangDepartment of Computer ScienceCornell Universitybishan@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell Universitycardie@cs.cornell.eduAbstractThis paper addresses the task of fine-grained opinion extraction ?
the identi-fication of opinion-related entities: theopinion expressions, the opinion hold-ers, and the targets of the opinions, andthe relations between opinion expressionsand their targets and holders.
Most ex-isting approaches tackle the extractionof opinion entities and opinion relationsin a pipelined manner, where the inter-dependencies among different extractionstages are not captured.
We propose a jointinference model that leverages knowledgefrom predictors that optimize subtasksof opinion extraction, and seeks a glob-ally optimal solution.
Experimental re-sults demonstrate that our joint inferenceapproach significantly outperforms tradi-tional pipeline methods and baselines thattackle subtasks in isolation for the problemof opinion extraction.1 IntroductionFine-grained opinion analysis is concerned withidentifying opinions in text at the expression level;this includes identifying the subjective (i.e., opin-ion) expression itself, the opinion holder and thetarget of the opinion (Wiebe et al, 2005).
Thetask has received increasing attention as many nat-ural language processing applications would ben-efit from the ability to identify text spans that cor-respond to these key components of opinions.
Inquestion-answering systems, for example, usersmay submit questions in the form ?What does en-tity A think about target B??
; opinion-orientedsummarization systems also need to recognizeopinions and their targets and holders.In this paper, we address the task of identifyingopinion-related entities and opinion relations.
Weconsider three types of opinion entities: opinionexpressions or direct subjective expressions as de-fined in Wiebe et al (2005) ?
expressions that ex-plicitly indicate emotions, sentiment, opinions orother private states (Quirk et al, 1985) or speechevents expressing private states; opinion targets?
expressions that indicate what the opinion isabout; and opinion holders ?
mentions of whomor what the opinion is from.
Consider the follow-ing examples in which opinion expressions (O) areunderlined and targets (T) and holders (H) of theopinion are bracketed.S1: [The workers][H1,2] were irked[O1]by [the government report][T1] andwere worried[O2] as they went abouttheir daily chores.S2: From the very start it could bepredicted[O1] that on the subject ofeconomic globalization, [the developedstates][T1,2] were going to come acrossfierce opposition[O2].The numeric subscripts denote linking relations,one of IS-ABOUT or IS-FROM.
In S1, for in-stance, opinion expression ?were irked?
(O1) IS-ABOUT ?the government report?
(T1).
Note thatthe IS-ABOUT relation can contain an empty tar-get (e.g.
?were worried?
in S1); similarly for IS-FROM w.r.t.
the opinion holder (e.g.
?predicted?
inS2).
We also allow an opinion entity to be involvedin multiple relations (e.g.
?the developed states?
inS2).Not surprisingly, fine-grained opinion extrac-tion is a challenging task due to the complexityand variety of the language used to express opin-ions and their components (Pang and Lee, 2008).Nevertheless, much progress has been made in ex-tracting opinion information from text.
Sequencelabeling models have been successfully employedto identify opinion expressions (e.g.
(Breck et al,16402007; Yang and Cardie, 2012)) and relation ex-traction techniques have been proposed to extractopinion holders and targets based on their link-ing relations to the opinion expressions (e.g.
Kimand Hovy (2006), Kobayashi et al (2007)).
How-ever, most existing work treats the extraction ofdifferent opinion entities and opinion relations in apipelined manner: the interaction between differ-ent extraction tasks is not modeled jointly and er-ror propagation is not considered.
One exceptionis Choi et al (2006), which proposed an ILP ap-proach to jointly identify opinion holders, opinionexpressions and their IS-FROM linking relations,and demonstrated the effectiveness of joint infer-ence.
Their ILP formulation, however, does nothandle implicit linking relations, i.e.
opinion ex-pressions with no explicit opinion holder; nor doesit consider IS-ABOUT relations.In this paper, we present a model that jointlyidentifies opinion-related entities, including opin-ion expressions, opinion targets and opinion hold-ers as well as the associated opinion linking rela-tions, IS-ABOUT and IS-FROM.
For each type ofopinion relation, we allow implicit (i.e.
empty) ar-guments for cases when the opinion holder or tar-get is not explicitly expressed in text.
We modelentity identification as a sequence tagging prob-lem and relation extraction as binary classifica-tion.
A joint inference framework is proposed tojointly optimize the predictors for different sub-problems with constraints that enforce global con-sistency.
We hypothesize that the ambiguity inthe extraction results will be reduced and thus,performance increased.
For example, uncertaintyw.r.t.
the spans of opinion entities can adverselyaffect the prediction of opinion relations; and evi-dence of opinion relations might provide clues toguide the accurate extraction of opinion entities.We evaluate our approach using a standard cor-pus for fine-grained opinion analysis (the MPQAcorpus (Wiebe et al, 2005)) and demonstrate thatour model outperforms by a significant margin tra-ditional baselines that do not employ joint infer-ence for extracting opinion entities and differenttypes of opinion relations.2 Related WorkSignificant research effort has been invested intofine-grained opinion extraction for open-domaintext such as news articles (Wiebe et al, 2005; Wil-son et al, 2009).
Many techniques were proposedto identify the text spans for opinion expressions(e.g.
(Breck et al, 2007; Johansson and Moschitti,2010b; Yang and Cardie, 2012)), opinion hold-ers (e.g.
(Choi et al, 2005)) and topics of opin-ions (Stoyanov and Cardie, 2008).
Some considerextracting opinion targets/holders along with theirrelation to the opinion expressions.
Kim and Hovy(2006) identifies opinion holders and targets by us-ing their semantic roles related to opinion words.Ruppenhofer et al (2008) argued that semanticrole labeling is not sufficient for identifying opin-ion holders and targets.
Johansson and Moschitti(2010a) extract opinion expressions and holdersby applying reranking on top of sequence label-ing methods.
Kobayashi et al (2007) consideredextracting ?aspect-evaluation?
relations (relationsbetween opinion expressions and targets) by iden-tifying opinion expressions first and then search-ing for the most likely target for each opinion ex-pression via a binary relation classifier.
All thesemethods extract opinion arguments and opinionrelations in separate stages instead of extractingthem jointly.Most similar to our method is Choi et al (2006),which jointly extracts opinion expressions, hold-ers and their IS-FROM relations using an ILP ap-proach.
In contrast, our approach (1) also consid-ers the IS-ABOUT relation which is arguably morecomplex due to the larger variety in the syntac-tic structure exhibited by opinion expressions andtheir targets, (2) handles implicit opinion relations(opinion expressions without any associated argu-ment), and (3) uses a simpler ILP formulation.There has also been substantial interest in opin-ion extraction from product reviews (Liu, 2012).Most existing approaches focus on the extrac-tion of opinion targets and their associated opin-ion expressions and usually employ a pipelinearchitecture: generate candidates of opinion ex-pressions and opinion targets first, and then userule-based or machine-learning-based approachesto identify potential relations between opinionsand targets (Hu and Liu, 2004; Wu et al, 2009;Liu et al, 2012).
In addition to pipeline ap-proaches, bootstrapping-based approaches wereproposed (Qiu et al, 2009; Qiu et al, 2011; Zhanget al, 2010) to identify opinion expressions andtargets iteratively; however, they suffer from theproblem of error propagation.There is much work demonstrating the bene-fit of performing global inference.
Roth and Yih1641(2004) proposed a global inference approach in theformulation of a linear program (LP) and appliedit to the task of extracting named entities and re-lations simultaneously.
Their problem is similarto ours ?
the difference is that Roth and Yih Rothand Yih (2004) assume that named entity spans areknown a priori and only their labels need to be as-signed.
Joint inference has also been applied tosemantic role labeling (Punyakanok et al, 2008;Srikumar and Roth, 2011; Das et al, 2012), wherethe goal is to jointly identify semantic argumentsfor given lexical predicates.
The problem is con-ceptually similar to identifying opinion argumentsfor opinion expressions, however, we do not as-sume prior knowledge of opinion expressions (un-like in SRL, where predicates are given).3 ModelAs proposed in Section 1, we consider the task ofjointly identifying opinion entities and opinion re-lations.
Specifically, given a sentence, our goal isto identify spans of opinion expressions, opinionarguments (targets and holders) and their associ-ated linking relations.
Training data consists oftext with manually annotated opinion expressionand argument spans, each with a list of relationids specifying the linking relation between opin-ion expressions and their arguments.In this section, we will describe how we modelopinion entity identification and opinion relationextraction, and how we combine them in a jointinference model.3.1 Opinion Entity IdentificationWe formulate the task of opinion entity identifica-tion as a sequence labeling problem and employconditional random fields (CRFs) (Lafferty et al,2001) to learn the probability of a sequence as-signment y for a given sentence x.
Through in-ference we can find the best sequence assignmentfor sentence x and recover the opinion entities ac-cording to the standard ?IOB?
encoding scheme.We consider four entity labels: D,T,H,N , whereD denotes opinion expressions, T denotes opiniontargets, H denotes opinion holders and N denotes?NONE?
entities.We define potential function fiz that gives theprobability of assigning a span i with entity labelz, and the probability is estimated based on thelearned parameters from CRFs.
Formally, givena within-sentence span i = (a, b), where a is thestarting position and b is the end position, and la-bel z ?
{D,T,H}, we havefiz = p(ya = Bz,ya+1 = Iz, ...,yb = Iz,yb+1 6= Iz|x)fiN = p(ya = O, ...,yb = O|x)These probabilities can be efficiently computedusing the forward-backward algorithm.3.2 Opinion Relation ExtractionWe consider extracting the IS-ABOUT and IS-FROM opinion relations.
In the following we willnot distinguish these two relations, since they canboth be characterized as relations between opinionexpressions and opinion arguments, and the meth-ods for relation extraction are the same.We treat the relation extraction problem as acombination of two binary classification prob-lems: opinion-arg classification, which decideswhether a pair consisting of an opinion candidate oand an argument candidate a forms a relation; andopinion-implicit-arg classification, which decideswhether an opinion candidate o is linked to an im-plicit argument, i.e.
no argument is mentioned.
Wedefine a potential function r to capture the strengthof association between an opinion candidate o andan argument candidate a,roa = p(y = 1|x)?
p(y = 0|x)where p(y = 1|x) and p(y = 0|x) are the logisticregression estimates of the positive and negativerelations.
Similarly, we define potential ro?
to de-note the confidence of predicting opinion span oassociated with an implicit argument.3.2.1 Opinion-Arg RelationsFor opinion-arg classification, we construct can-didates of opinion expressions and opinion argu-ments and consider each pair of an opinion can-didate and an argument candidate as a potentialopinion relation.
Conceptually, all possible sub-sequences in the sentence are candidates.
To filterout candidates that are less reasonable, we con-sider the opinion expressions and arguments ob-tained from the n-best predictions by CRFs1.
Wealso employ syntactic patterns from dependency1We randomly split the training data into 10 parts and ob-tained the 50-best CRF predictions on each part for the gen-eration of candidates.
We also experimented with candidatesgenerated from more CRF predictions, but did not find anyperformance improvement for the task.1642trees to generate candidates.
Specifically, we se-lected the most common patterns of the shortestdependency paths2 between an opinion candidateo and an argument candidate a in our dataset, andinclude all pairs of candidates that satisfy at leastone dependency pattern.
For the IS-ABOUT rela-tion, the top three patterns are (1) o ?dobj a, (2)o ?ccomp x ?nsubj a (x is a word in the path that isnot covered by either o nor a), (3) o ?ccomp a; forthe IS-FROM relation, the top three patterns are (1)o ?nsubj a, (2) o ?poss a, (3) o ?ccomp x ?nsubj a.Note that generating candidates this way willgive us a large number of negative examples.
Sim-ilar to the preprocessing approach in (Choi et al,2006), we filter pairs of opinion and argument can-didates that do not overlap with any gold standardrelation in our training data.Many features we use are common featuresin the SRL tasks (Punyakanok et al, 2008)due to the similarity of opinion relations to thepredicate-argument relations in SRL (Ruppen-hofer et al, 2008; Choi et al, 2006).
In general,the features aim to capture (a) local properties ofthe candidate opinion expressions and argumentsand (b) syntactic and semantic attributes of theirrelation.Words and POS tags: the words contained in thecandidate and their POS tags.Lexicon: For each word in the candidate, weinclude its WordNet hypernyms and its strengthof subjectivity in the Subjectivity Lexicon3(e.g.
weaksubj, strongsubj).Phrase type: the syntactic category of the deepestconstituent that covers the candidate in the parsetree, e.g.
NP, VP.Semantic frames: For each verb in the opinioncandidate, we include its frame types according toFrameNet4.Distance: the relative distance (number of words)between the opinion and argument candidates.Dependency Path: the shortest path in thedependency tree between the opinion candidateand the target candidate, e.g.
ccomp?nsubj?.
Wealso include word types and POS types in thepaths, e.g.
opinion?ccompsuffering?nsubjpatient,2We use the Stanford Parser to generate parse trees anddependency graphs.3http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/4https://framenet.icsi.berkeley.edu/fndrupal/NN?ccompVBG?nsubjNN.
The dependency pathhas been shown to be very useful in extractingopinion expressions and opinion holders (Johans-son and Moschitti, 2010a).3.2.2 Opinion-Implicit-Arg RelationsWhen the opinion-arg relation classifier predictsthat there is no suitable argument for the opinionexpression candidate, it does not capture the possi-bility that an opinion candidate may associate withan implicit argument.
To incorporate knowledgeof implicit relations, we build an opinion-implicit-arg classifier to identify an opinion candidate withan implicit argument based on its own propertiesand context information.For training, we consider all gold-standardopinion expressions as training examples ?including those with implicit arguments ?
aspositive examples and those associated withexplicit arguments as negative examples.
Forfeatures, we use words, POS tags, phrase types,lexicon and semantic frames (see Section 3.2.1for details) to capture the properties of the opinionexpression, and also features that capture thecontext of the opinion expression:Neighboring constituents: The words and gram-matical roles of neighboring constituents of theopinion expression in the parse tree ?
the left andright sibling of the deepest constituent containingthe opinion expression in the parse tree.Parent Constituent: The grammatical role ofthe parent constituent of the deepest constituentcontaining the opinion expression.Dependency Argument: The word types andPOS types of the arguments of the dependencypatterns in which the opinion expression isinvolved.
We consider the same dependencypatterns that are used to generate candidates foropinion-arg classification.3.3 Joint InferenceThe inference goal is to find the optimal predictionfor both opinion entity identification and opinionrelation extraction.
For a given sentence, we de-note O as a set of opinion candidates, Ak as a setof argument candidates, where k denotes the typeof opinion relation ?
IS-ABOUT or IS-FROM ?and S as a set of within-sentence spans that coverall of the opinion candidates and argument can-1643didates.
We introduce binary variable xiz , wherexiz = 1 means span i is associated with label z.We also introduce binary variable uij for everypair of opinion candidate i and argument candidatej, where uij = 1 means i forms an opinion rela-tion with j, and binary variable vik for every opin-ion candidate i in relation type k, where vik = 1means i associates with an implicit argument inrelation k. Given the binary variables xiy, uij , vik,it is easy to recover the entity and relation assign-ment by checking which spans are labeled as opin-ion entities, and which opinion span and argumentspan form an opinion relation.The objective function is defined as a linearcombination of the potentials from different pre-dictors with a parameter ?
to balance the contribu-tion of two components: opinion entity identifica-tion and opinion relation extraction.argmaxx,u,v?
?i?S?zfizxiz+ (1?
?)?k?i?O??
?j?Akrijuij + ri?vik??
(1)It is subject to the following linear constraints:Constraint 1: Uniqueness.
For each span i, wemust assign one and only one label z, where z ?
{H,D, T,N}.
?zxiz = 1Constraint 2: Non-overlapping.
If two spans i andj overlap, then at most one of the spans can beassigned to a non-NONE entity label: H,D, T .
?z 6=Nxiz +?z 6=Nxjz ?
1Constraint 3: Consistency between the opinion-arg and opinion-implicit-arg classifiers.
For anopinion candidate i, if it is predicted to have animplicit argument in relation k, vik = 1, then noargument candidate should form a relation with i.If vik = 0, then there exists some argument can-didate j ?
Ak such that uij = 1.
We introducetwo auxiliary binary variables aik and bik to limitthe maximum number of relations associated witheach opinion candidate to be less than or equal tothree5.
When vik = 1, aik and bik have to be 0.?j?Akuij = 1?
vik + aik + bikaik ?
1?
vik, bik ?
1?
vikConstraint 4: Consistency between opinion-argclassifier and opinion entity extractor.
Supposean argument candidate j in relation k is assignedan argument label by the entity extractor, that isxjz = 1 (z = T for IS-ABOUT relation and z = Hfor IS-FROM relation), then there exists some opin-ion candidates that associate with j.
Similar toconstraint 3, we introduce auxiliary binary vari-ables cj and dj to enforce that an argument j linksto at most three opinion expressions.
If xjz = 0,then no relations should be extracted for j.?i?Ouij = xjz + cjk + djkcjk ?
xjz, djk ?
xjzConstraint 5: Consistency between the opinion-implicit-arg classifier and opinion entity extractor.When an opinion candidate i is predicted to asso-ciate with an implicit argument in relation k, thatis vik = 1, then we allow xiD to be either 1 or0 depending on the confidence of labeling i as anopinion expression.
When vik = 0, there exisitssome opinion argument associated with the opin-ion candidate, and we enforce xiD = 1, whichmeans the entity extractor agrees to label i as anopinion expression.vik + xiD ?
1Note that in our ILP formulation, the labelassignment for a candidate span involves onemultiple-choice decision among different opinionentity labels and the ?NONE?
entity label.
Thescores of different label assignments are compara-ble for the same span since they come from oneentity extraction model.
This makes our ILP for-mulation advantageous over the ILP formulationproposed in Choi et al (2006), which needs m bi-nary decisions for a candidate span, wherem is thenumber of types of opinion entities, and the scorefor each possible label assignment is obtained by5It is possible to add more auxiliary variables to allowmore than three arguments to link to an opinion expression,but this rarely happens in our experiments.
For the IS-FROMrelation, we set aik = 0, bik = 0 since an opinion expressionusually has only one holder.1644the sum of raw scores from m independent extrac-tion models.
This design choice also allows usto easily deal with multiple types of opinion ar-guments and opinion relations.4 ExperimentsFor evaluation, we used version 2.0 of the MPQAcorpus (Wiebe et al, 2005; Wilson, 2008), awidely used data set for fine-grained opinion anal-ysis.6 We considered the subset of 482 docu-ments7 that contain attitude and target annotations.There are a total of 9,471 sentences with opinion-related labels at the phrase level.
We set aside 132documents as a development set and use 350 doc-uments as the evaluation set.
All experiments em-ploy 10-fold cross validation on the evaluation set;the average over the 10 runs is reported.Our gold standard opinion expressions, opiniontargets and opinion holders correspond to the di-rect subjective annotations, target annotations andagent annotations, respectively.
The IS-FROM re-lation is obtained from the agent attribute of eachopinion expression.
The IS-ABOUT relation is ob-tained from the attitude annotations: each opinionexpression is annotated with attitude frames andeach attitude frame is associated with a list of tar-gets.
The relations may overlap: for example, inthe following sentence, the target of relation 1 con-tains relation 2.
[John]H1 is happyO1 because [[he]H2lovesO2 [being at Enderly Park]T2]T1 .We discard relations that contain sub-relations be-cause we believe that identifying the sub-relationsusually is sufficient to recover the discarded rela-tions.
(Prediction of overlapping relations is con-sidered as future work.)
In the example above, wewill identify (loves, being at Enderly Park) as anIS-ABOUT relation and happy as an opinion ex-pression associated with an implicit target.
Table 1shows some statistics of the corpus.We adopted the evaluation metrics for entity andrelation extraction from Choi et al (2006), whichinclude precision, recall, and F1-measure accord-ing to overlap and exact matching metrics.8 We6Available at http://www.cs.pitt.edu/mpqa/.7349 news articles from the original MPQA corpus, 84Wall Street Journal articles (Xbank), and 48 articles from theAmerican National Corpus.8Overlap matching considers two spans to match if theyoverlap, while exact matching requires two spans to be ex-actly the same.Opinion Target HolderTotalNum 5849 4676 4244Opinion-arg Relations Implicit RelationsIS-ABOUT 4823 1302IS-FROM 4662 1187Table 1: Data Statistics of the MPQA Corpus.will focus our discussion on results obtained us-ing overlap matching, since the exact boundariesof opinion entities are hard to define even for hu-man annotators (Wiebe et al, 2005).We trained CRFs for opinion entity identifica-tion using the following features: indicators forwords, POS tags, and lexicon features (the sub-jectivity strength of the word in the SubjectivityLexicon).
All features are computed for the cur-rent token and tokens in a [?1,+1] window.
Weused L2-regularization; the regularization param-eter was tuned using the development set.
Wetrained the classifiers for relation extraction usingL1-regularized logistic regression with default pa-rameters using the LIBLINEAR (Fan et al, 2008)package.
For joint inference, we used GLPK9 toprovide the optimal ILP solution.
The parameter?
was tuned using the development set.4.1 Baseline MethodsWe compare our approach to several pipeline base-lines.
Each extracts opinion entities first usingthe same CRF employed in our approach, andthen predicts opinion relations on the opinion en-tity candidates obtained from the CRF prediction.Three relation extraction techniques were used inthe baselines:?
Adj: Inspired by the adjacency rule usedin Hu and Liu (2004), it links each argu-ment candidate to its nearest opinion candi-date.
Arguments that do not link to any opin-ion candidate are discarded.
This is also usedas a strong baseline in Choi et al (2006).?
Syn: Links pairs of opinion and argumentcandidates that present prominent syntacticpatterns.
(We consider the syntactic patternslisted in Section 3.2.1.)
Previous work alsodemonstrates the effectiveness of syntacticinformation in opinion extraction (Johanssonand Moschitti, 2012).9http://www.gnu.org/software/glpk/1645Opinion Expression Opinion Target Opinion HolderMethod P R F1 P R F1 P R F1CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28CRF+RE 83.02 48.99 61.62 85.07 22.01 34.97 78.13 40.40 53.26Joint-Model 71.16 77.85 74.35?
75.18 57.12 64.92??
67.01 66.46 66.73?
?CRF 66.60 52.57 58.76 44.44 29.60 35.54 65.18 44.24 52.71CRF+Adj 66.60 52.57 58.76 49.10 25.81 33.83 68.03 43.84 53.32CRF+Syn 66.60 52.57 58.76 50.26 18.41 26.94 74.60 37.98 50.33CRF+RE 69.27 40.09 50.79 60.45 15.37 24.51 75 38.79 51.13Joint-Model 57.39 62.40 59.79?
49.15 38.33 43.07??
62.73 62.22 62.47?
?Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap andthe bottom table uses exact).
Two-tailed t-test results are shown on F1 measure for our method compared to the other baselines(statistical significance is indicated with ?
(p < 0.05), ??
(p < 0.005)).IS-ABOUT IS-FROMMethod P R F1 P R F1CRF+Adj 73.65 37.34 49.55 70.22 41.58 52.23CRF+Syn 76.21 28.28 41.25 77.48 36.63 49.74CRF+RE 78.26 20.33 32.28 74.81 37.55 50.00CRF+Adj-merged-10-best 25.05 61.18 35.55 30.28 62.82 40.87CRF+Syn-merged-10-best 41.60 45.66 43.53 48.08 54.03 50.88CRF+RE-merged-10-best 51.60 33.09 40.32 47.73 54.40 50.84Joint-Model 64.38 51.20 57.04??
64.97 58.61 61.63?
?Table 3: Performance on opinion relation extraction using the overlap metric.?
RE: Predicts opinion relations by employ-ing the opinion-arg classifier and opinion-implicit-arg classifier.
First, the opinion-argclassifier identifies pairs of opinion and argu-ment candidates that form valid opinion rela-tions, and then the opinion-implicit-arg clas-sifier is used on the remaining opinion candi-dates to further identify opinion expressionswithout explicit arguments.We report results using opinion entity candi-dates from the best CRF output and from themerged 10-best CRF output.10 The motivation ofmerging the 10-best output is to increase recall forthe pipeline methods.5 ResultsTable 2 shows the results of opinion entity identi-fication using both overlap and exact metrics.
Wecompare our approach with the pipeline baselinesand CRF (the first step of the pipeline).
We cansee that our joint inference approach significantlyoutperforms all the baselines in F1 measure on ex-tracting all types of opinion entities.
In general,10It is similar to the merged 10-best baseline in Choi etal.
(2006).
If an entity Ei extracted by the ith-best sequenceoverlaps with an entity Ej extracted by the jth-best sequence,where i ?
j, then we discard Ej .
If Ei and Ej do not over-lap, then we consider both entities.by adding the relation extraction step, the pipelinebaselines are able to improve precision over theCRF but fail at recall.
CRF+Syn and CRF+Adjprovide the same performance as CRF, since therelation extraction step only affects the results ofopinion arguments.
By incorporating syntacticinformation, CRF+Syn provides better precisionthan CRF+Adj on extracting arguments at the ex-pense of recall.
This indicates that using simplesyntactic rules would mistakenly filter many cor-rect relations.
By using binary classifiers to pre-dict relations, CRF+RE produces high precisionon opinion and target extraction but also results invery low recall.
Using the exact metric, we ob-serve the same general trend in the results as theoverlap metric.
The scores are lower since themetric is much stricter.Table 3 shows the results of opinion relation ex-traction using the overlap metric.
We compare ourapproach with pipelined baselines in two settings:one employs relation extraction on 1-best outputof CRF (top half of table) and the other employsthe merged 10-best output of CRF (bottom half oftable).
We can see that in general, using merged10-best CRF outputs boosts the recall while sac-rificing precision.
This is expected since mergingthe 10-best CRF outputs favors candidates that are1646IS-ABOUT Relation Extraction IS-FROM Relation ExtractionMethod P R F1 P R F1ILP-W/O-ENTITY 49.10 40.48 44.38 44.77 58.24 50.63ILP-W-SINGLE-RE 63.88 49.35 55.68 53.64 65.02 58.78ILP-W/O-IMPLICIT-RE 62.00 44.73 51.97 73.23 51.28 60.32Joint-Model 64.38 51.20 57.04??
64.97 58.61 61.63?Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.believed to be more accurate by the CRF predictor.If CRF makes mistakes, the mistakes will propa-gate to the relation extraction step.
The poor per-formance on precision further confirms the errorpropagation problem in the pipeline approaches.In contrast, our joint-inference method success-fully boosts the recall while maintaining reason-able precision.
This demonstrates that joint infer-ence can effectively leverage the advantage of in-dividual predictors and limit error propagation.To demonstrate the effectiveness of differentpotentials in our joint inference model, we con-sider three variants of our ILP formulation thatomit some potentials in the joint inference: oneis ILP-W/O-ENTITY, which extracts opinion rela-tions without integrating information from opin-ion entity identification; one is ILP-W-SINGLE-RE,which focuses on extracting a single opinion re-lation and ignores the information from the otherrelation; the third one is ILP-W/O-IMPLICIT-RE,which omits the potential for opinion-implicit-argrelation and assumes every opinion expression islinked to an explicit argument.
The objective func-tion of ILP-W/O-ENTITY can be represented asargmaxu?k?i?O?j?Akrijuij (2)which is subject to constraints on uij to enforcerelations to not overlap and limit the maximumnumber of relations that can be extracted for eachopinion expression and each argument.
For ILP-W-SINGLE-RE, we simply remove the variables as-sociated with one opinion relation in the objectivefunction (1) and constraints.
The formulation ofILP-W/O-IMPLICIT-RE removes the variables as-sociated with potential ri in the objective functionand corresponding constraints.
It can be viewedas an extension to the ILP approach in Choi et al(2006) that includes opinion targets and uses sim-pler ILP formulation with only one parameter andfewer binary variables and constraints to represententity label assignments 11.11We compared the proposed ILP formulation with the ILPTable 4 shows the results of these methods onopinion relation extraction.
We can see that with-out the knowledge of the entity extractor, ILP-W/O-ENTITY provides poor performance on bothrelation extraction tasks.
This confirms the effec-tiveness of leveraging knowledge from entity ex-tractor and relation extractor.
The improvementyielded by our approach over ILP-W-SINGLE-REdemonstrates the benefit of jointly optimizing dif-ferent types of opinion relations.
Our approachalso outperforms ILP-W/O-IMPLICIT-RE, whichdoes not take into account implicit relations.
Theresults demonstrate that incorporating knowledgeof implicit opinion relations is important.6 DiscussionWe note that the joint inference model yields aclear improvement on recall but not on precisioncompared to the CRF-based baselines.
Analyz-ing the errors, we found that the joint model ex-tracts comparable number of opinion entities com-pared to the gold standard, while the CRF-basedbaselines extract significantly fewer opinion enti-ties (around 60% of the number of entities in thegold standard).
With more extracted opinion enti-ties, the precision is sacriced but recall is boostedsubstantially, and overall we see an increase inF-measure.
We also found that a good portionof errors were made because the generated candi-dates failed to cover the correct solutions.
Recallthat the joint model finds the global optimal solu-tion over a set of opinion entity and relation can-didates, which are obtained from the n-best CRFpredictions and constituents in the parse tree thatsatisfy certain syntactic patterns.
It is possiblethat the generated candidates do not contain thegold standard answers.
For example, our modelfailed to identify the IS-ABOUT relation (offers,general aid) from the following sentence Powellhad contacted ... and received offersO1 of [gen-formulation in Choi et al (2006) on extracting opinion hold-ers, opinion expressions and IS-FROM relations, and showedthat the proposed ILP formulation performs better on all threeextraction tasks.1647eral aid]T1 ... because both the CRF predictor andsyntactic heuristics fail to capture (offers, generalaid) as a potential relation candidate.
By applyingsimple heuristics such as treating all verbs or verbphrases as opinion candidates would not help be-cause it would introduce a large number of nega-tive candidates and lower the accuracy of relationextraction (only 52% of the opinion expressionsare verbs or verb phrases and 64% of the opiniontargets are noun or noun phrases in the corpus weused).
Therefore a more effective candidate gen-eration method is needed to allow more candidateswhile limiting the number of negative candidates.We also observed incorrect parsing to be a cause oferror.
We hope to study ways to account for sucherrors in our approach as future work.For computational time, our ILP formulationcan be solved very efficiently using advanced ILPsolvers.
In our experiment, using GLPK?s branch-and-cut solver took 0.2 seconds to produce opti-mal ILP solutions for 1000 sentences on a machinewith Intel Core 2 Duo CPU and 4GB RAM.7 ConclusionIn this paper we propose a joint inference ap-proach for extracting opinion-related entities andopinion relations.
We decompose the task intodifferent subproblems, and jointly optimize themusing constraints that aim to encourage their con-sistency and reduce prediction uncertainty.
Weshow that our approach can effectively integrateknowledge from different predictors and achievesignificant improvements in overall performancefor opinion extraction.
For future work, we plan toextend our model to handle more complex opinionrelations, e.g.
nesting or cross-sentential relations.This can be potentially addressed by incorporat-ing more powerful predictors and more complexlinguistic constraints.AcknowledgmentsThis work was supported in part by DARPA-BAA-12-47 DEFT grant 12475008 and NSF grant BCS-0904822.
We thank Igor Labutov for helpful dis-cussion and suggestions, Ainur Yessenalina forearly discussion of the work, as well as the reviewsfor helpful comments.ReferencesE.
Breck, Y. Choi, and C. Cardie.
2007.
Identifyingexpressions of opinion in context.
In Proceedings ofthe 20th international joint conference on Artificalintelligence, pages 2683?2688.
Morgan KaufmannPublishers Inc.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opin-ions with conditional random fields and extractionpatterns.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 355?362.Association for Computational Linguistics.Y.
Choi, E. Breck, and C. Cardie.
2006.
Joint ex-traction of entities and relations for opinion recog-nition.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 431?439.
Association for ComputationalLinguistics.D.
Das, A.F.T.
Martins, and N.A.
Smith.
2012.
Anexact dual decomposition algorithm for shallow se-mantic parsing with constraints.
Proceedings of*SEM.
[ii, 10, 50].Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.M.
Hu and B. Liu.
2004.
Mining opinion featuresin customer reviews.
In Proceedings of the Na-tional Conference on Artificial Intelligence, pages755?760.
Menlo Park, CA; Cambridge, MA; Lon-don; AAAI Press; MIT Press; 1999.Richard Johansson and Alessandro Moschitti.
2010a.Reranking models in fine-grained opinion analysis.In Proceedings of the 23rd International Conferenceon Computational Linguistics, pages 519?527.
As-sociation for Computational Linguistics.Richard Johansson and Alessandro Moschitti.
2010b.Syntactic and semantic structure for opinion ex-pression detection.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 67?76.
Association for Com-putational Linguistics.Richard Johansson and Alessandro Moschitti.
2012.Relational features in fine-grained opinion analysis.Soo-Min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text.
In Proceedings of the Work-shop on Sentiment and Subjectivity in Text, pages 1?8.
Association for Computational Linguistics.N.
Kobayashi, K. Inui, and Y. Matsumoto.
2007.Extracting aspect-evaluation and aspect-of relationsin opinion mining.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural1648Language Learning (EMNLP-CoNLL), pages 1065?1074.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.K.
Liu, L. Xu, and J. Zhao.
2012.
Opinion targetextraction using word-based translation model.
InProceedings of the conference on Empirical Meth-ods in Natural Language Processing.
Associationfor Computational Linguistics.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Now Pub.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The im-portance of syntactic parsing and inference in se-mantic role labeling.
Computational Linguistics,34(2):257?287.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2009.
Expanding domain sentiment lexicon throughdouble propagation.
In Proceedings of the 21stinternational jont conference on Artifical intelli-gence, pages 1199?1204.
Morgan Kaufmann Pub-lishers Inc.G.
Qiu, B. Liu, J. Bu, and C. Chen.
2011.
Opinionword expansion and target extraction through doublepropagation.
Computational linguistics, 37(1):9?27.R.
Quirk, S. Greenbaum, G. Leech, J. Svartvik, andD.
Crystal.
1985.
A comprehensive grammar ofthe English language, volume 397.
Cambridge UnivPress.D.
Roth and W. Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
Defense Technical Information Center.J.
Ruppenhofer, S. Somasundaran, and J. Wiebe.
2008.Finding the sources and targets of subjective expres-sions.
In Proceedings of LREC.Vivek Srikumar and Dan Roth.
2011.
A joint modelfor extended semantic role labeling.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 129?139.
Associationfor Computational Linguistics.V.
Stoyanov and C. Cardie.
2008.
Topic identificationfor fine-grained opinion analysis.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics-Volume 1, pages 817?824.
Asso-ciation for Computational Linguistics.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.Language Resources and Evaluation, 39(2):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational linguistics, 35(3):399?433.Theresa Wilson.
2008.
Fine-Grained SubjectivityAnalysis.
Ph.D. thesis, Ph.
D. thesis, University ofPittsburgh.
Intelligent Systems Program.Y.
Wu, Q. Zhang, X. Huang, and L. Wu.
2009.
Phrasedependency parsing for opinion mining.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing: Volume 3-Volume3, pages 1533?1541.
Association for ComputationalLinguistics.B.
Yang and C. Cardie.
2012.
Extracting opinionexpressions with semi-markov conditional randomfields.
In Proceedings of the conference on Empiri-cal Methods in Natural Language Processing.
Asso-ciation for Computational Linguistics.Lei Zhang, Bing Liu, Suk Hwan Lim, and EamonnO?Brien-Strain.
2010.
Extracting and ranking prod-uct features in opinion documents.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics: Posters, pages 1462?1470.
Asso-ciation for Computational Linguistics.1649
