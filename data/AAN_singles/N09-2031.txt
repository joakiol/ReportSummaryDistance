Proceedings of NAACL HLT 2009: Short Papers, pages 121?124,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExtracting Bilingual Dictionary from Comparable Corpora withDependency HeterogeneityKun Yu Junichi TsujiiGraduate School of Information Science and TechnologyThe University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, Japan{kunyu, tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper proposes an approach for bilingualdictionary extraction from comparable corpora.The proposed approach is based on the obser-vation that a word and its translation sharesimilar dependency relations.
Experimental re-sults using 250 randomly selected translationpairs prove that the proposed approach signifi-cantly outperforms the traditional context-based approach that uses bag-of-words aroundtranslation candidates.1 IntroductionBilingual dictionary plays an important role in manynatural language processing tasks.
For example, ma-chine translation uses bilingual dictionary to reinforceword and phrase alignment (Och and Ney, 2003), cross-language information retrieval uses bilingual dictionaryfor query translation (Grefenstette, 1998).
The directway of bilingual dictionary acquisition is aligning trans-lation candidates using parallel corpora (Wu, 1994).
Butfor some languages, collecting parallel corpora is noteasy.
Therefore, many researchers paid attention to bi-lingual dictionary extraction from comparable corpora(Fung, 2000; Chiao and Zweigenbaum, 2002; Daille andMorin, 2008; Robitaille et al, 2006; Morin et al, 2007;Otero, 2008), in which texts are not exact translation ofeach other but share common features.Context-based approach, which is based on the ob-servation that a term and its translation appear in similarlexical contexts (Daille and Morin, 2008), is the mostpopular approach for extracting bilingual dictionaryfrom comparable corpora and has shown its effective-ness in terminology extraction (Fung, 2000; Chiao andZweigenbaum, 2002; Robitaille et al, 2006; Morin et al,2007).
But it only concerns about the lexical contextaround translation candidates in a restricted window.Besides, in comparable corpora, some words may appearin similar context even if they are not translation of eachother.
For example, using a Chinese-English comparablecorpus from Wikipedia and following the definition in(Fung, 1995), we get context heterogeneity vector ofthree words (see Table 1).
The Euclidean distance be-tween the vector of  ????(economics)?
and ?econom-ics?
is 0.084.
But the Euclidean distance between thevector of  ?????
and ?medicine?
is 0.075.
In suchcase, the incorrect dictionary entry ???
?/medicine?will be extracted by context-based approach.Table 1.
Context heterogeneity vector of words.Word Context Heterogeneity Vector???
(economics) (0.185, 0.006)economics (0.101, 0.013)medicine (0.113,0.028)To solve this problem, we investigate a comparablecorpora from Wikipedia and find the following phe-nomenon: if we preprocessed the corpora with a de-pendency syntactic analyzer, a word in source languageshares similar head and modifiers with its translation intarget language, no matter whether they occur in similarcontext or not.
We call this phenomenon as dependencyheterogeneity.
Based on this observation, we propose anapproach to extract bilingual dictionary from compara-ble corpora.
Not like only using bag-of-words aroundtranslation candidates in context-based approach, theproposed approach utilizes the syntactic analysis ofcomparable corpora to recognize the meaning of transla-tion candidates.
Besides, the lexical information used inthe proposed approach does not restrict in a small win-dow, but comes from the entire sentence.We did experiments with 250 randomly selectedtranslation pairs.
Results show that compared with theapproach based on context heterogeneity, the proposedapproach improves the accuracy of dictionary extractionsignificantly.2 Related WorkIn previous work about dictionary extraction from com-parable corpora, using context similarity is the mostpopular one.At first, Fung (1995) utilized context heterogeneityfor bilingual dictionary extraction.
Our proposed ap-proach borrows Fung?s idea but extends context hetero-geneity to dependency heterogeneity, in order to utilizerich syntactic information other than bag-of-words.After that, researchers extended context heterogeneityvector to context vector with the aid of an existing bilin-gual dictionary (Fung, 2000; Chiao and Zweigenbaum,2002; Robitaille et al, 2006; Morin et al, 2007; Dailleand Morin, 2008).
In these works, dictionary extraction121is fulfilled by comparing the similarity between the con-text vectors of words in target language and the contextvectors of words in source language using an externaldictionary.
The main difference between these worksand our approach is still our usage of syntactic depend-ency other than bag-of-words.
In addition, except for amorphological analyzer and a dependency parser, ourapproach does not need other external resources, such asthe external dictionary.
Because of the well-developedmorphological and syntactic analysis research in recentyears, the requirement of analyzers will not bring toomuch burden to the proposed approach.Besides of using window-based contexts, there werealso some works utilizing syntactic information for bi-lingual dictionary extraction.
Otero (2007) extractedlexico-syntactic templates from parallel corpora first,and then used them as seeds to calculate similarity be-tween translation candidates.
Otero (2008) defined syn-tactic rules to get lexico-syntactic contexts of words, andthen used an external bilingual dictionary to fulfill simi-larity calculation between the lexico-syntactic contextvectors of translation candidates.
Our approach differsfrom these works in two ways: (1) both the above worksdefined syntactic rules or templates by hand to get syn-tactic information.
Our approach uses data-driven syn-tactic analyzers for acquiring dependency relationsautomatically.
Therefore, it is easier to adapt our ap-proach to other language pairs.
(2) the types of depend-encies used for similarity calculation in our approach aredifferent from Otero?s work.
Otero (2007; 2008) onlyconsidered about the modification dependency amongnouns, prepositions and verbs, such as the adjectivemodifier of nouns and the object of verbs.
But our ap-proach not only uses modifiers of translation candidates,but also considers about their heads.3 Dependency Heterogeneity of Words inComparable CorporaDependency heterogeneity means a word and its trans-lation share similar modifiers and head in comparablecorpora.
Namely, the modifiers and head of unrelatedwords are different even if they occur in similar context.Table 2.
Frequently used modifiers (words are not ranked).???
(economics) economics medicine?
?/micro keynesian physiology?
?/macro new Chinese?
?/computation institutional traditional?/new positive biology?
?/politics classical internal?
?/university labor science??
?/classicists development clinical?
?/development engineering veterinary?
?/theory finance western?
?/demonstration international agricultureFor example, Table 2 collects the most frequentlyused 10 modifiers of the words listed in Table 1.
Itshows there are 3 similar modifiers (italic words) be-tween ????(economics)?
and ?economics?.
But thereis no similar word between the modifiers of ????
?and that of ?medicine?.
Table 3 lists the most frequentlyused 10 heads (when a candidate word acts as subject)of the three words.
If excluding copula, ?????
and?economics?
share one similar head (italic words).
But?????
and ?medicine?
shares no similar head.Table 3.
Frequently used heads(the predicate of subject, words are not ranked).???
(economics) economics medicine?/is is is?
?/average has tends?
?/graduate was include?
?/admit emphasizes moved?/can non-rivaled means?
?/split became requires?
?/leave assume includes?/compare relies were?
?/become can has?
?/emphasize replaces may4 Bilingual Dictionary Extraction with De-pendency HeterogeneityBased on the observation of dependency heterogeneityin comparable corpora, we propose an approach to ex-tract bilingual dictionary using dependency heterogene-ity similarity.4.1 Comparable Corpora PreprocessingBefore calculating dependency heterogeneity similarity,we need to preprocess the comparable corpora.
In thiswork, we focus on Chinese-English bilingual dictionaryextraction for single-nouns.
Therefore, we first use aChinese morphological analyzer (Nakagawa and Uchi-moto, 2007) and an English pos-tagger (Tsuruoka et al,2005) to analyze the raw corpora.
Then we use Malt-Parser (Nivre et al, 2007) to get syntactic dependency ofboth the Chinese corpus and the English corpus.
Thedependency labels produced by MaltParser (e.g.
SUB)are used to decide the type of heads and modifiers.After that, the analyzed corpora are refined throughfollowing steps: (1) we use a stemmer1 to do stemmingfor the English corpus.
Considering that only nouns aretreated as translation candidates, we use stems for trans-lation candidate but keep the original form of their headsand modifiers in order to avoid excessive stemming.
(2)stop words are removed.
For English, we use the stopword list from (Fung, 1995).
For Chinese, we remove??(of)?
as stop word.
(3) we remove the dependenciesincluding punctuations and remove the sentences with1 http://search.cpan.org/~snowhare/Lingua-Stem-0.83/122more than k (set as 30 empirically) words from bothEnglish corpus and Chinese corpus, in order to reducethe effect of parsing error on dictionary extraction.4.2 Dependency Heterogeneity Vector CalculationEquation 1 shows the definition of dependency hetero-geneity vector of a word W. It includes four elements.Each element represents the heterogeneity of a depend-ency relation.
?NMOD?
(noun modifier), ?SUB?
(sub-ject) and ?OBJ?
(object) are the dependency labelsproduced by MaltParser.
(HNMODHead ,HSUBHead ,HOBJHead ,HNMODMod )  (1)HNMODHead (W ) = number of different heads of W with NMOD labeltotal number of heads of W with NMOD labelHSUBHead (W ) = number of different heads of W with SUB labeltotal number of heads of W with SUB labelHOBJHead (W ) = number of different heads of W with OBJ labeltotal number of heads of W with OBJ labelHNMODMod (W ) = number of different modifiers of W with NMOD labeltotal number of modifiers of W with NMOD label4.3 Bilingual Dictionary ExtractionAfter calculating dependency heterogeneity vector oftranslation candidates, bilingual dictionary entries areextracted according to the distance between the vector ofWs in source language and the vector of Wt in target lan-guage.
We use Euclidean distance (see equation 2) fordistance computation.
The smaller distance between thedependency heterogeneity vectors of Ws and Wt, themore likely they are translations of each other.DH (Ws,Wt ) = DNMODHead 2 + DSUBHead 2 + DOBJHead 2 + DNMODMod 2 (2)DNMODHead = HNMODHead(Ws)?HNMODHead(Wt )DSUBHead = HSUBHead (W s) ?
HSUBHead (W t )DOBJHead = HOBJHead (Ws)?HOBJHead (Wt )DNMODMod = HNMODMod (Ws) ?HNMODMod (Wt )For example, following above definitions, we get de-pendency heterogeneity vector of the words analyzedbefore (see Table 4).
The distances between these vec-tors are DH(??
?, economics) = 0.222,  DH(??
?,medicine) = 0.496.
It is clear that the distance betweenthe vector of ????(economics)?
and ?economics?
ismuch smaller than that between ?????
and ?medi-cine?.
Thus, the pair ????/economics?
is extractedsuccessfully.Table 4.
Dependency heterogeneity vector of words.Word Dependency Heterogeneity Vector???
(economics) (0.398, 0.677, 0.733, 0.471)economics (0.466, 0.500, 0.625, 0.432)medicine (0.748, 0.524, 0.542, 0.220)5 Results and Discussion5.1 Experimental SettingWe collect Chinese and English pages from Wikipedia2with inter-language link and use them as comparablecorpora.
After corpora preprocessing, we get 1,132,4922 http://download.wikimedia.orgEnglish sentences and 665,789 Chinese sentences fordependency heterogeneity vector learning.
To evaluatethe proposed approach, we randomly select 250 Chi-nese/English single-noun pairs from the aligned titles ofthe collected pages as testing data, and divide them into5 folders.
Accuracy (see equation 3) and MMR (Voor-hees, 1999) (see equation 4) are used as evaluation met-rics.
The average scores of both accuracy and MMRamong 5 folders are also calculated.Accuracy = tii=1N?
N  (3)ti = 1, if there exists correct translation in top n ranking0, otherwise?
?
?MMR = 1N1rankii=1N?
,     ranki = ri,  if ri < n0, otherwise?
?
?
(4)n means top n evaluation,ri means the rank of the correct translation in top n rankingN means the total number of words for evaluation5.2 Results of Bilingual Dictionary ExtractionTwo approaches were evaluated in this experiment.
Oneis the context heterogeneity approach proposed in (Fung,1995) (context for short).
The other is our proposed ap-proach (dependency for short).The average results of dictionary extraction are listedin Table 5.
It shows both the average accuracy and aver-age MMR of extracted dictionary entries were improvedsignificantly (McNemar?s test, p<0.05) by the proposedapproach.
Besides, the increase of top5 evaluation wasmuch higher than that of top10 evaluation, which meansthe proposed approach has more potential to extract pre-cise bilingual dictionary entries.Table 5.
Average results of dictionary extraction.context dependencyave.accu ave.MMR ave.accu ave.MMRTop5 0.132 0.064 0.208(?57.58%) 0.104(?62.50%)Top10 0.296 0.086 0.380(?28.38%) 0.128(?48.84%)5.3 Effect of Dependency Heterogeneity VectorDefinitionIn the proposed approach, a dependency heterogeneityvector is defined as the combination of head and modi-fier heterogeneities.
To see the effects of different de-pendency heterogeneity on dictionary extraction, weevaluated the proposed approach with different vectordefinitions, which areonly-head: (HNMODHead ,HSUBHead ,HOBJHead )only-mod: (HNMODMod )only-NMOD: (HNMODHead ,HNMODMod )Table 6.
Average results with different vector definitions.Top5 Top10ave.accu ave.MMR ave.accu ave.MMRcontext 0.132 0.064 0.296 0.086dependency 0.208 0.104 0.380 0.128only-mod 0.156 0.080 0.336 0.103only-head 0.176 0.077 0.336 0.098only-NMODs 0.200 0.094 0.364 0.115123The results are listed in Table 6.
It shows with anytypes of vector definitions, the proposed approach out-performed the context approach.
Besides, if comparingthe results of dependency, only-mod, and only-head, aconclusion can be drawn that head dependency hetero-geneities and modifier dependency heterogeneities gavesimilar contribution to the proposed approach.
At last,the difference between the results of dependency andonly-NMOD shows the head and modifier with NMODlabel contributed more to the proposed approach.5.4 DiscussionTo do detailed analysis, we collect the dictionary entriesthat are not extracted by context approach but extractedby the proposed approach (good for short), and the en-tries that are extracted by context approach but not ex-tracted by the proposed approach (bad for short) fromtop10 evaluation results with their occurrence time (seeTable 7).
If neglecting the entries ???/passports?
and??
?/shanghai?, we found that the proposed approachtended to extract correct bilingual dictionary entries ifboth the two words occurred frequently in the compara-ble corpora, but failed if one of them seldom appeared.Table 7.
Good and bad dictionary entries.Good BadChinese English Chinese English??
?/262 jew/122 ??
?/53 crucifixion/19?
?/568 velocity/175 ??
?/6 aquarium/31?
?/2298 history/2376 ??
?/47 mixture/179?
?/1775 organizations/2194 ?/17 brick/66?
?/1534 movement/1541 ?
?/23 quantification/31?
?/76 passports/80 ?
?/843 shanghai/1247But there are two exceptions: (1) although ???(shanghai)?
and ?shanghai?
appeared frequently, the pro-posed approach did not extract them correctly; (2) both???(passport)?
and ?passports?
occurred less than 100times, but they were recognized successfully by the pro-posed approach.
Analysis shows the cleanliness of thecomparable corpora is the most possible reason.
In theEnglish corpus we used for evaluation, many words areincorrectly combined with ?shanghai?
by ?br?
(i.e.
linebreak), such as ?airportbrshanghai?.
These errors af-fected the correctness of dependency heterogeneity vec-tor of ?shanghai?
greatly.
Compared with the dirtyresource of ?shanghai?, only base form and plural formof ?passport?
occur in the English corpus.
Therefore, thedependency heterogeneity vectors of ????
and ?pass-ports?
were precise and result in the successful extrac-tion of this dictionary entry.
We will clean the corpora tosolve this problem in our future work.6 Conclusion and Future WorkThis paper proposes an approach, which not uses thesimilarity of bag-of-words around translation candidatesbut considers about the similarity of syntactic dependen-cies, to extract bilingual dictionary from comparablecorpora.
Experimental results show that the proposedapproach outperformed the context-based approach sig-nificantly.
It not only validates the feasibility of the pro-posed approach, but also shows the effectiveness ofapplying syntactic analysis in real application.There are several future works under considerationincluding corpora cleaning, extending the proposed ap-proach from single-noun dictionary extraction to multi-words, and adapting the proposed approach to other lan-guage pairs.
Besides, because the proposed approach isbased on the syntactic analysis of sentences with nomore than k words (see Section 4.1), the parsing accu-racy and the setting of threshold k will affect the cor-rectness of dependency heterogeneity vector learning.We will try other thresholds and syntactic parsers to seetheir effects on dictionary extraction in the future.AcknowledgmentsThis research is sponsored by Microsoft Research AsiaWeb-scale Natural Language Processing Theme.ReferencesY.Chiao and P.Zweigenbaum.
2002.
Looking for Candidate Transla-tional Equivalents in Specialized, Comparable Corpora.
Proceed-ings of LREC 2002.B.Daille and E.Morin.
2008.
An Effective Compositional Model forLexical Alignment.
Proceedings of IJCNLP-08.P.Fung.
1995.
Compiling Bilingual Lexicon Entries from a Non-parallel English-Chinese Corpus.
Proceedings of the 3rd AnnualWorkshop on Very Large Corpora.
pp.
173-183.P.Fung.
2000.
A Statistical View on Bilingual Lexicon Extractionfrom Parallel Corpora to Non-parallel Corpora.
Parallel Text Proc-essing: Alignment and Use of Translation Corpora.
Kluwer Aca-demic Publishers.G.Grefenstette.
1998.
The Problem of Cross-language InformationRetrieval.
Cross-language Information Retrieval.
Kluwer Aca-demic Publishers.E.Morin et al.
2007.
Bilingual Terminology Mining ?
Using Brain,not Brawn Comparable Corpora.
Proceedings of ACL 2007.T.Nakagawa and K.Uchimoto.
2007.
A Hybrid Approach to WordSegmentation and POS Tagging.
Proceedings of ACL 2007.J.Nivre et al.
2007.
MaltParser: A Language-independent System forData-driven Dependency Parsing.
Natural Language Engineering.13(2): 95-135.F.Och and H.Ney.
2003.
A Systematic Comparison of Various Statis-tical Alignment Models.
Computational Linguistics, 29(1): 19-51.P.Otero.
2007.
Learning Bilingual Lexicons from Comparable Englishand Spanish Corpora.
Proceedings of MT Summit XI.
pp.
191-198.P.Otero.
2008.
Evaluating Two Different Methods for the Task ofExtracting Bilingual Lexicons from Comparable Corpora.
Proceed-ings of LREC 2008 Workshop on Comparable Corpora.
pp.
19-26.X.Robitaille et al.
2006.
Compiling French Japanese Terminologiesfrom the Web.
Proceedings of EACL 2006.Y.Tsuruoka et al.
2005.
Developing a Robust Part-of-speech Taggerfor Biomedical Text.
Advances in Informatics ?
10th PanhellenicConference on Informationcs.
LNCS 3746. pp.
382-392.E.M.Voorhees.
1999.
The TREC-8 Question Answering Track Report.Proceedings of the 8th Text Retrieval Conference.D.Wu.
1994.
Learning an English-Chinese Lexicon from a ParallelCorpus.
Proceedings of the 1st Conference of the Association forMachine Translation in the Americas.124
