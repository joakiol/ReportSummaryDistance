Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 871?879,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPMultilingual Spectral ClusteringUsing Document Similarity PropagationDani Yogatama and Kumiko Tanaka-IshiiGraduate School of Information Science and Technology, University of Tokyo13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japanyogatama@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jpAbstractWe present a novel approach for multilin-gual document clustering using only com-parable corpora to achieve cross-lingualsemantic interoperability.
The methodmodels document collections as weightedgraph, and supervisory information isgiven as sets of must-linked constraints fordocuments in different languages.
Recur-sive k-nearest neighbor similarity propa-gation is used to exploit the prior knowl-edge and merge two language spaces.Spectral method is applied to find the bestcuts of the graph.
Experimental resultsshow that using limited supervisory in-formation, our method achieves promis-ing clustering results.
Furthermore, sincethe method does not need any languagedependent information in the process, ouralgorithm can be applied to languages invarious alphabetical systems.1 IntroductionDocument clustering is unsupervised classifica-tion of text collections into distinct groups of sim-ilar documents.
It has been used in many in-formation retrieval tasks, including data organiza-tion (Siersdorfer and Sizov, 2004), language mod-eling (Liu and Croft, 2004), and improving per-formances of text categorization system (Aggar-wal et al, 1999).
Advance in internet technologyhas made the task of managing multilingual docu-ments an intriguing research area.
The growth ofinternet leads to the necessity of organizing docu-ments in various languages.
There exist thousandsof languages, not to mention countless minor ones.Creating document clustering model for each lan-guage is simply unfeasible.
We need methods todeal with text collections in diverse languages si-multaneously.Multilingual document clustering (MLDC) in-volves partitioning documents, written in morethan one languages, into sets of clusters.
Simi-lar documents, even if they are written in differ-ent languages, should be grouped together intoone cluster.
The major challenge of MLDC isachieving cross-lingual semantic interoperability.Most monolingual techniques will not work sincedocuments in different languages are mapped intodifferent spaces.
Spectral method such as LatentSemantic Analysis has been commonly appliedfor MLDC task.
However, current techniquesstrongly rely on the presence of common wordsbetween different languages.
This method wouldonly work if the languages are highly related, i.e.,languages that share the same root.
Therefore, weneed another method to improve the robustness ofMLDC model.In this paper, we focus on the problem of bridg-ing multilingual space for document clustering.We are given text documents in different lan-guages and asked to group them into clusters suchthat documents that belong to the same topic aregrouped together.
Traditional monolingual ap-proach is impracticable since it is unable to pre-dict how similar two multilingual documents are.They have two different spaces which make con-ventional cosine similarity irrelevant.
We try tosolve this problem utilizing prior knowledge inthe form of must-linked constraints, gathered fromcomparable corpora.
Propagation method is usedto guide the language-space merging process.
Ex-perimental results show that the approach givesencouraging clustering results.This paper is organized as follows.
In section 2,we review related work.
In section 3, we proposeour algorithm for multilingual document cluster-ing.
The experimental results are shown in section4.
Section 5 concludes with a summary.8712 Related WorkChen and Lin (2000) proposed methods to clus-ter multilingual documents using translation tech-nology, relying on cross-lingual dictionary andmachine-translation system.
Multilingual ontol-ogy, such as Eurovoc, is also popular for MLDC(Pouliquen et al, 2004).
However, such resourcesare scarce and expensive to build.
Several otherdrawbacks of using this technique include dictio-nary limitation and word ambiguity.More recently, parallel texts have been used toconnect document collections from different lan-guages (Wei et al, 2008).
This is done by collaps-ing columns in a term by document matrix that aretranslations of each other.
Nevertheless, buildingparallel texts is also expensive and requires a lot ofworks, hence shifting the paradigm of multilingualworks to comparable corpora.Comparable corpora are collections of texts indifferent languages regarding similar topics pro-duced at the same time.
The key difference be-tween comparable corpora and parallel texts is thatdocuments in comparable corpora are not neces-sarily translations of each other.
They are easierto be acquired, and do not need exhaustive worksto be prepared.
News agencies often give informa-tion in many different languages and can be goodsources for comparable corpora.
Terms in com-parable corpora, being about the same topic, upto some point explain the same concepts in differ-ent languages.
Pairing comparable corpora withspectral method such as Latent Semantic Analysishas become prevalent, e.g.
(Gliozzo and Strappar-ava, 2005).
They rely on the presence of commonwords and proper nouns among various languagesto build a language-independent space.
The per-formance of such method is highly dependent onthe languages being used.
Here, we present an-other approach to exploit knowledge in compa-rable corpora; using propagation method to aidspreading similarity between collections of docu-ments in different languages.Spectral clustering is the task of finding goodclusters by using information contained in theeigenvectors of a matrix derived from the data.It has been successfully applied in many applica-tions including information retrieval (Deerwesteret al, 2003) and computer vision (Meila and Shi,2000).
An in-depth analysis of spectral algo-rithm for clustering problems is given in (Ng etal., 2002).
Zhang and Mao (2008) used a relatedtechnique called Modularity Eigenmap to extractcommunity structure features from the documentnetwork to solve hypertext classification problem.Semi-supervised clustering enhances clusteringtask by incorporating prior knowledge to aid clus-tering process.
It allows user to guide the cluster-ing process by giving some feedback to the model.In traditional clustering algorithm, only unlabeleddata is used to find assignments of data pointsto clusters.
In semi-supervised clustering, priorknowledge is given to improve performance of thesystem.
The supervision is usually given as pairof must-linked constraints and cannot link con-straints, first introduced in (Wagstaff and Cardie,2000).
Kamvar et al (2003) proposed spectrallearning algorithm that can take supervisory infor-mation in the form of pairwise constraints or la-beled data.
Their algorithm is intended to be usedin monolingual context, while our algorithm is de-signed to work in multilingual context.3 Multilingual Spectral ClusteringThere have been several works on multilingualdocument clustering as mention previously in Sec-tion 2.
Our key contribution here is the propaga-tion method to make spectral clustering algorithmworks for multilingual problems.
The clusteringmodel exploits the supervisory information by de-tecting k nearest neighbors of the newly-linkeddocuments, and propagates document similarity tothese neighbors.
The model can be applied to anymultilingual text collections regardless of the lan-guages.
Overall algorithm is given in Section 3.1and the method to merge multilingual spaces bysimilarity propagation is given in Section 3.2.3.1 Spectral Clustering AlgorithmSpectral clustering tries to find good clusters byusing top eigenvectors of normalized data affin-ity matrix.
The document set is being modeled asundirected graph G(V,E,W ), where V , E, andW denote the graph vertex set, edge set, and tran-sition probability matrix, respectively.
In graphG, v ?
V represents a document, and weightwij?W represents transition probability betweendocument vito vj.
The transition probabilitiescan be interpreted as edge flows in Markov ran-dom walk over graph vertices (documents in col-lections).Algorithm to perform spectral clustering isgiven in Algorithm 1.
Let A be affinity matrix872where element Aijis cosine similarity betweendocument viand vj(Algorithm 1, line 1).
It isstraightforward that documents belonging to dif-ferent languages will have similarity zero.
Rareexception occurs when they have common wordsbecause the languages are related one another.As a consequence, the similarity matrix will havemany zeros.
Our model amplifies prior knowledgein the form of comparable corpora by perform-ing document similarity propagation, presented inSection 3.2 (Algorithm 1, line 4; Algorithm 2, ex-plained in Section 3.2).
After propagation, theaffinity matrix is post-processed (Algorithm 1, line6, explained in Section 3.2) before being trans-formed into transition probability matrix.The transformation can be done using any nor-malization for spectral methods.
Define N =D?1A, as in (Meila and Shi, 2001), where D is thediagonal matrix whose elements Dij=?jAij(Algorithm 1, line 7).
Alternatively, we can defineN = D?1/2AD?1/2(Ng et al, 2002), or N =(A + dmaxI ?
D)/dmax(Fiedler, 1975), wheredmaxis the maximum rowsum of A.
For our ex-periment, we use the first normalization method,though other methods can be applied as well.Meila and Shi (2001) show that probability tran-sition matrix N with t strong clusters will have tpiecewise constant eigenvectors.
They also sug-gest using these t eigenvectors in clustering pro-cess.
We use the information contains in t largesteigenvectors of N (Algorithm 1, line 8-11) andperform K-means clustering algorithm to find thedata clusters (Algorithm 1, line 12).3.2 Propagating Prior KnowledgeWe use information obtained from comparablecorpora to merge multilingual language spaces.Suppose we have text collections in L differentlanguages.
We combine this collections with com-parable corpora, also in L languages, that act asour supervisory information.
Comparable corporaare used to gather prior knowledge by makingmust-linked constraints for documents in differentlanguages that belong to the same topic in the cor-pora, propagating similarity to other documentswhile doing so.Initially, our affinity matrix A represents cosinesimilarity between all pairs of documents.
Aijisset to zero if j is not the top k nearest neighborsof i and likewise.
Next, set Aijand Ajito 1 ifdocument i and document j are different in lan-Algorithm 1 Multilingual Spectral ClusteringInput: Term by document matrix M , pairwiseconstraintsOutput: Document clusters1: Create graph affinity matrix A ?
Rn?nwhereeach element Aijrepresents the similarity be-tween document viand vj.2: for all pairwise constraints in comparable cor-pora do3: Aij?
1, Aji?
1.4: Recursive Propagation (A,S, ?, k, vi, vj).5: end for6: Post-process matrix A so that every value inA is greater than ?
and less than 1.7: Form a diagonal matrix D, where Dii=?jAij.
Normalize N = D?1A.8: Find x1, x2?
?
?
, xt, the t largest eigenvectorsof N.9: Form matrix X = [x1, x2, ?
?
?
, xt] ?
Rn?t.10: Normalize row X to be unit length.11: Project each document into eigen-spacespanned by the above t eigenvectors (by treat-ing each row of X as a point in Rt, row i rep-resents document vi).12: ApplyK-means algorithm in this space to finddocument clusters.guage and belong to the same topic in our com-parable corpora.
This will incorporate the must-linked constraint to our model.
We can also givesupervisory information for pairs of document inthe same language, but this is optional.
We also donot use cannot-linked constraints since the maingoal is to merge multilingual spaces.
In our exper-iment we show that using only must-linked con-straints with propagation is enough to achieve en-couraging clustering results.The supervisory information acquired fromcomparable corpora only connects two nodes inour graph.
Therefore, the number of edges be-tween documents in different languages is aboutas many as the number of must-linked constraintsgiven.
We argue that we need more edges betweenpairs of documents in different languages to getbetter results.We try to build more edges by propagating sim-ilarity to other documents that are most similar tothe newly-linked documents.
Figure 1 gives an il-lustration of edge-creation process when two mul-tilingual documents (nodes) are connected.
Sup-873yx1viyx2zx1vjzx2(a) Connect two nodesyx1viyx2zx1vjzx2(b) Effect on neighbor nodesFigure 1: Pairing two multilingual documents af-fect their neighbors.
viand vjare documents intwo different languages.
yxand zxare neighborsof viand vjrespectively.pose that we have six documents in two differ-ent languages.
Initially, documents are only con-nected with other documents that belong to thesame language.
The supervisory information tellsus that two multilingual documents viand vjshould be connected (Figure 1(a)).
We then buildan edge between these two documents.
Further-more, we also use this information to build edgesbetween viand neighbors of vjand likewise (Fig-ure 1(b)).This follows from the hypothesis that bringingtogether two documents should also bring otherdocuments that are similar to those two closer inour clustering space.
Klein et al (2002) statedthat a good clustering algorithm, besides satisfy-ing known constraints, should also be able to sat-isfy the implications of those constraints.
Here,we allow not only instance-level inductive impli-cations, but utilize it to get higher-level inductiveimplications.
In other words, we alter similarityspace so that it can detect other clusters by chang-ing the topology of the original space.The process is analogous to shortening the dis-tance between sets of documents in Euclideanspace.
In vector space model, two documents thatare close to each other have high similarity, andthus will belong to the same cluster.
Pairing twodocuments can be seen as setting the distance inthis space to 0, thus raising their similarity to 1.While doing so, each document would also drawsets of documents connected to it closer to the cen-tre of the merge, which is equivalent to increasingtheir similarities.Suppose we have document viand vj, and y andz are sets of their respective k nearest neighbors,where |y| = |z| = k. The propagation methodis a recursive algorithm with base S, the num-ber of desired level of propagation.
Recursive k-nearest neighbor makes decision to give high sim-ilarity between multilingual documents not onlydetermined by their similarity to the newly-linkeddocuments, but also their similarity to the k near-est neighbors of the respective document.
Severaldocuments are affected by a single supervisory in-formation.
This will prove useful when only lim-ited amount of supervisory information given.
Ituses document similarity matrix A, as defined inthe previous section.1.
For yx?
y we propagate ?Aviyxto Avjyx.Set Ayxvj= Avjyx(Algorithm 2, line 5-6).In other words, we propagate the similaritybetween document viand y nearest neighborsof vito document vj.2.
Similarly, for zx?
z we propagate ?Avjzxto Avizx.
Set Azxvi= Avizx(Algorithm 2,line 10-11).
In other words, we propagate thesimilarity between document vjand z nearestneighbors of vjto document vi.3.
Propagate higher order similarity to k nearestneighbors of y and z, discounting the similar-ity quadratically, until required level of prop-agation S is reached (Algorithm 2, line 7 and12).The coefficient ?
represents the degree of en-forcement that the documents similar to a docu-ment in one language, will also have high simi-larity with other document in other language thatis paired up with its ancestor.
On the other hand,k represents the number of documents that are af-fected by pairing up two multilingual documents.After propagation, similarity of documents thatfalls below some threshold ?
is set to zero (Al-gorithm 1, line 6).
This post-processing step isperformed to nullify insignificant similarity valuespropagated to a document.
Additionally, if thereexists similarity of documents that is higher thanone, it is set to one.874Algorithm 2 Recursive PropagationInput: Affinity matrix A, level of propagation S,?, number of nearest neighbors k, document viand vjOutput: Propagated affinity matrix1: if S = 0 then2: return3: else4: for all yx?
k-NN document vido5: Avjyx?
Avjyx+ ?Aviyx6: Ayxvj?
Avjyx7: Recursive Propagation (A,S ?
1,?2, k, yx, vj)8: end for9: for all zx?
k-NN document vjdo10: Set Avizx?
Avizx+ ?Avjzx11: Set Azxvi?
Avizx12: Recursive Propagation (A,S ?
1,?2, k, vi, zx)13: end for14: end if4 Performance EvaluationThe goals of empirical evaluation include (1) test-ing whether the propagation method can mergemultilingual space and produce acceptable clus-tering results; (2) comparing the performance tospectral clustering method without propagation.4.1 Data DescriptionWe tested our model using Reuters Corpus Vol-ume 2 (RCV2), a multilingual corpus contain-ing news in thirteen different languages.
For ourexperiment, three different languages: English,French, and Spanish; in six different topics: sci-ence, sports, disasters accidents, religion, health,and economy are used.
We discarded documentswith multiple category labels.We do not apply any language specific pre-processing method to the raw text data.
Mono-lingual TFIDF is used for feature weighting.
Alldocument vectors are then converted into unit vec-tor by dividing by its length.
Table 1 shows theaverage length of documents in our corpus.4.2 Evaluation MetricFor our experiment, we used Rand Index (RI)which is a common evaluation technique for clus-tering task where the true class of unlabeled dataEnglish French Spanish TotalScience 290.10 165.10 213.45 222.88Sports 182.55 156.83 189.75 176.37Disasters 154.29 175.89 165.31 165.16Religion 317.77 177.91 242.67 246.11Health 251.19 233.70 227.25 237.38Economy 266.89 192.55 306.11 255.08Total 243.79 183.61 224.09 217.16Table 1: Average number of words of documentsin the corpus.
Each language consists of 600 doc-uments, and each topic consists of 100 documents(per language).is known.
Rand Index measures the percentage ofdecisions that are correct, or simply the accuracyof the model.
Rand Index is defined as:RI =TP + TNTP + FP + TN + FNRand Index penalizes false positive and false neg-ative decisions during clustering.
It takes into ac-count decision that assign two similar documentsto one cluster (TP), two dissimilar documents todifferent clusters (TN), two similar documents todifferent clusters (FN), and two dissimilar docu-ments to one cluster (FP).
We do not include linkscreated by supervisory information when calculat-ing true positive decisions and only consider thenumber of free decisions made.We also used F?-measure, the weighted har-monic mean of precision (P) and recall (R).
F?-measure is defined as:F?=(?2+ 1)PR?2P +RP =TPTP + FPR =TPTP + FNLast, we used purity to evaluate the accuracy ofassignments.
Purity is defined as:Purity =1N?tmaxj|?t?
cj|whereN is the number of documents, t is the num-ber of clusters, j is the number of classes, ?tandcjare sets of documents in cluster t and class jrespectively.87500.20.40.60.810  0.2  0.4  0.6  0.8  1RandIndexProportion of supervisory informationWith propagationWithout propagationLSA(a) Rand Index for 6 topics00.20.40.60.810  0.2  0.4  0.6  0.8  1RandIndexProportion of supervisory informationWith propagationWithout propagationLSA(b) Rand Index for 4 topicsFigure 2: Rand Index on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4topics as the proportion of supervisory information increases.
k = 30, ?
= 0.03, ?
= 0.5, t = number oftopics, and S = 2.00.20.40.60.810  0.2  0.4  0.6  0.8  1PurityProportion of supervisory informationWith propagationWithout propagationLSA(a) Purity for 6 topics00.20.40.60.810  0.2  0.4  0.6  0.8  1PurityProportion of supervisory informationWith propagationWithout propagationLSA(b) Purity for 4 topicsFigure 3: Purity on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4 topicsas the proportion of supervisory information increases.
k = 30, ?
= 0.03, ?
= 0.5, t = number of topics,and S = 2.4.3 Experimental ResultsTo prove the effectiveness of our clustering algo-rithm, we performed the following experiments onour data set.
We first tested our algorithm on fourtopics, science, sports, religion, and economy.
Wethen tested our algorithm using all six topics toget an understanding of the performance of ourmodel in larger collections with more topics.
Weused subset of our data as supervisory informa-tion and built must-linked constraints from it.
Theproportion of supervisory information provided tothe system is given in x-axis (Figure 2 - Figure4.3).
0.2 here means 20% of documents in eachlanguage are taken to be used as prior knowledge.Since the number of documents in each languagefor our experiment is the same, we have the samenumbers of documents in subset of English col-lection, subset of French collection, and subset ofSpanish collection.
We also ensure there are samenumbers of documents for a particular topic in allthree languages.
We can build must-linked con-straints as follows.
For each document in the sub-set of English collection, we create must-linkedconstraints with one randomly selected documentfrom the subset of French collection and one ran-domly selected document from the subset of Span-ish collection that belong to the same topic with it.We then create must-linked constraint between therespective French and Spanish documents.
Theconstraints given to the algorithm are chosen sothat there are several links that connect every topicin every language.
Note that the class label in-87600.20.40.60.810  0.2  0.4  0.6  0.8  1F2-measureProportion of supervisory informationWith propagationWithout propagationLSA(a) F2-measure for 6 topics00.20.40.60.810  0.2  0.4  0.6  0.8  1F2-measureProportion of supervisory informationWith propagationWithout propagationLSA(b) F2-measure for 4 topicsFigure 4: F2-measure on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4topics as the proportion of supervisory information increases.
k = 30, ?
= 0.03, ?
= 0.5, t = number oftopics, and S = 2.formation is only used to build must-linked con-straints between documents, and we do not assignthe documents to a particular cluster.Figure 2 shows the Rand Index as proportionof supervisory information increases.
Figure 3and Figure 4.3 give purity and F2-measure forthe algorithm respectively.
To show the impor-tance of the propagation in multilingual space, wegive comparison with spectral clustering modelwithout propagation.
Three lines in Figure 2 toFigure 4.3 indicate: (1) results with propagation(solid line); (2) results without propagation (long-dashed line); and (3) results using Latent Se-mantic Analysis(LSA)-based method by exploit-ing common words between languages (short-dashed line).
For each figure, 6 plots are takenstarting from 0 in 0.2-point-increments.
We con-ducted the experiments three times for each pro-portion of supervisory information and use the av-erage values.
As we can see from Figure 2, Fig-ure 3, and Figure 4.3, the propagation method cansignificantly improve the performance of spectralclustering algorithm.
For 1800 documents in 6topics, we manage to achieve RI = 0.91, purity= 0.84, and F2-measure = 0.76 with only 20% ofdocuments (360 documents) used as supervisoryinformation.
Spectral clustering algorithm with-out propagation can only achieve 0.69, 0.30, 0.28for RI, purity, and F2-measure respectively.
Thepropagation method is highly effective when onlysmall amount of supervisory information given tothe algorithm.
Obviously, the more supervisory in-formation given, the better the performance is.
Asthe number of supervisory information increases,the difference of the model performance with andwithout propagation becomes smaller.
This isbecause there are already enough links betweenmultilingual documents, so we do not necessar-ily build more links through similarity propagationanymore.
However, even when there are alreadymany links, our model with propagation still out-performs the model without propagation.We compare the performance of our algorithmto LSA-based multilingual document clusteringmodel.
We performed LSA to the multilingualterm by document matrix.
We do not use paral-lel texts and only rely on common words acrosslanguages as well as must-linked constraints tobuild multilingual space.
The results show that ex-ploiting common words between languages aloneis not enough to build a good multilingual se-mantic space, justifying the usage of supervisoryinformation in multilingual document clusteringtask.
When supervisory information is introduced,our method achieves better results than LSA-basedmethod.
In general, the LSA-based method per-forms better than the model without propagation.We assess the sensitivity of our algorithm toparameter ?, the penalty for similarity propaga-tion.
We assess the sensitivity of our algorithmto parameter ?, the penalty for similarity prop-agation.
We tested our algorithm using various?, starting from 0 to 1 in 0.2-point-increments,while other parameters being held constant.
Fig-ure 5(a) shows that changing ?
to some extent af-fects the performance of the algorithm.
However,after some value of reasonable ?
is found, increas-ing ?
does not have significant impact on the per-87700.20.40.60.810  0.2  0.4  0.6  0.8  1RandIndex?
(a) Changing ?, k = 30, t = 600.20.40.60.810  20  40  60  80  100RandIndexk(b) Changing k, ?
= 0.5, t = 600.20.40.60.810  5  10  15  20RandIndext(c) Changing t, ?
= 0.5, k = 30Figure 5: Rand Index on the RCV2 task with 1800 documents and 6 topics as (a) ?
increases; (b)k increases; and (c) t increases.
?
= 0.03, S = 2, and 20% of documents are used as supervisoryinformation.formance of the algorithm.
We also tested our al-gorithm using various k, starting from 0 to 100in 20-point-increments.
Figure 5(b) reveals thatthe performances of the model with different k arecomparable, as long as k is not too small.
How-ever, using too large k will slightly decrease theperformance of the model.
Too many propaga-tions make several dissimilar documents receivehigh similarity value that cannot be nullified bythe post-processing step.
Last, we experimentedusing various t ranging from 2 to 20.
Figure 5(c)shows that the method performs best when t = 10,and for reasonable value of t the method achievescomparable performance.5 ConclusionWe present here a multilingual spectral cluster-ing model that is able to work irrespective of thelanguages being used.
The key component ofour model is the propagation algorithm to mergemultilingual spaces.
We tested our algorithmon Reuters RCV2 Corpus and compared the per-formance with spectral clustering model withoutpropagation.
Experimental results reveal that us-ing limited supervisory information, the algorithmachieves encouraging clustering results.ReferencesCharu C. Aggarwal, Stephen C. Gates and Philip S.Yu.
1999.
On The Merits of Building Catego-rization Systems by Supervised Clustering.
In Pro-ceedings of Conference on Knowledge Discovery inDatabases:352-356.Hsin-Hsi Chen and Chuan-Jie Lin.
2000.
A Mul-tilingual News Summarizer.
In Proceedings of18th International Conference on ComputationalLinguistics:159-165.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harsh-man.
1990.
Indexing by Latent Semantic Analy-sis.
Journal of the American Society of InformationScience:41(6):391-407.Miroslav Fiedler.
1975.
A Property of Eigenvectors ofNonnegative Symmetric Matrices and its Applica-tions to Graph Theory.
Czechoslovak MathematicalJournal, 25:619-672.878Alfio Gliozzo and Carlo Strapparava.
2005.
Cross lan-guage Text Categorization by acquiring MultilingualDomain Models from Comparable Corpora.
In Pro-ceedings of the ACL Workshop on Building and Us-ing Parallel Texts:9-16.Sepandar D. Kamvar, Dan Klein, and Christopher D.Manning.
2003.
Spectral Learning.
In Proceed-ings of the International Joint Conference on Artifi-cial Intelligence (IJCAI).Dan Klein, Sepandar D. Kamvar, and Christopher D.Manning.
2002.
From instance-level constraints tospace-level constraints: Making the most of priorknowledge in data clustering.
In The Nineteenth In-ternational Conference on Machine Learning.Xiaoyong Liu and W. Bruce Croft.
2004.
Cluster-based Retrieval using Language Models.
In Pro-ceedings of the 27th annual international ACM SI-GIR conference on Research and development in in-formation retrieval:186-193.Marinla Meil?a and Jianbo Shi.
2000.
Learning seg-mentation by random walks.
In Advances in NeuralInformation Processing Systems:873-879.Marinla Meil?a and Jianbo Shi.
2001.
A Random WalksView of Spectral Segmentation.
In AI and Statistics(AISTATS).Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.2002.
On Spectral Clustering: Analysis and an al-gorithm.
In Proceedings of Advances in Neural In-formation Processing Systems (NIPS 14).Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,Emilia K?asper, and Irina Temnikova.
2004.
Mul-tilingual and Cross-lingual News Topic Tracking.
InProceedings of the 20th International Conference onComputational Linguistics.Stefan Siersdorfer and Sergej Sizov.
2004.
RestrictiveClustering and Metaclustering for Self-OrganizingDocument.
In Proceedings of the 27th annual in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval.Kiri Wagstaff and Claire Cardie 2000.
Clusteringwith Instance-level Constraints.
In Proceedingsof the 17th International Conference on MachineLearning:1103-1110.Chih-Ping Wei, Christopher C. Yang, and Chia-MinLin.
2008.
A Latent Semantic Indexing Based Ap-proach to Multilingual Document Clustering.
In De-cision Support Systems, 45(3):606-620Dell Zhang and Robert Mao.
2008.
Extracting Com-munity Structure Features for Hypertext Classifi-cation.
In Proceedings of the 3rd IEEE Interna-tional Conference on Digital Information Manage-ment (ICDIM).879
