Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1256?1265,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsHashing-based Approaches to Spelling Correction of Personal NamesRaghavendra UdupaMicrosoft Research IndiaBangalore, Indiaraghavu@microsoft.comShaishav KumarMicrosoft Research IndiaBangalore, Indiav-shaisk@microsoft.comAbstractWe propose two hashing-based solutions tothe problem of fast and effective personalnames spelling correction in People Searchapplications.
The key idea behind our meth-ods is to learn hash functions that map similarnames to similar (and compact) binary code-words.
The two methods differ in the datathey use for learning the hash functions - thefirst method uses a set of names in a given lan-guage/script whereas the second uses a set ofbilingual names.
We show that both methodsgive excellent retrieval performance in com-parison to several baselines on two lists ofmisspelled personal names.
More over, themethod that uses bilingual data for learninghash functions gives the best performance.1 IntroductionOver the last few years, People Search has emergedas an important search service.
Unlike general WebSearch and Enterprise Search where users search forinformation on a wide range of topics including peo-ple, products, news, events, etc., People Search isabout people.
Hence, personal names are used pre-dominantly as queries in People Search.
As in gen-eral Web Search, a good percentage of queries inPeople Search is misspelled.
Naturally, spelling cor-rection of misspelled personal names plays a veryimportant role in not only reducing the time and ef-fort needed by users to find people they are search-ing for but also in ensuring good user experience.Spelling errors in personal names are of a differ-ent nature compared to those in general text.
Longbefore People Search became widely popular, re-searchers working on the problem of personal namematching had recognized the human tendency to beinexact in recollecting names from the memory andspecifying them.
A study of personal names inhospital databases found that only 39% of the er-rors in the names were single typographical errors(Friedman and Sideli, 1992)1.
Further, multiple andlong distance typographical errors (Gregzorz Kon-drak for Grzegorz Kondrak), phonetic errors (as inErik Bryl for Eric Brill), cognitive errors (as in Sil-via Cucerzan for Silviu Cucerzan) and word substi-tutions (as in Rob Moore for Bob Moore) are ob-served relatively more frequently in personal namescompared to general text.In addition to within-the-word errors, PeopleSearch queries are plagued by errors that are notusually seen in general text.
The study by Fried-man and Sideli discovered that 36% of the errorswere due to addition or deletion of a word (as inRicardo Baeza for Ricardo Baeza-Yates) (Friedmanand Sideli, 1992).
Although word addition and dele-tion generally do not come under the purview ofspelling correction, in People Search they are im-portant and need to be addressed.Standard approaches to general purpose spellingcorrection are not well-suited for correcting mis-spelled personal names.
As pointed out by(Cucerzan and Brill, 2004), these approaches ei-ther try to correct individual words (and will fail tocorrect Him Clijsters to Kim Clijsters) or employfeatures based on relatively wide context windows1In contrast, 80% of misspelled words in general text are dueto single typographical errors as found by (Damerau, 1964).1256which are not available for queries in Web Searchand People Search.
Spelling correction techniquesmeant for general purpose web-queries require largevolumes of training data in the form of query logsfor learning the error models (Cucerzan and Brill,2004), (Ahmad and Kondrak, 2005).
However,query logs are not available in some applications(e.g.
Email address book search).
Further, un-like general purpose web-queries where word orderoften matters, in People Search word order is lax(e.g.
I might search for either Kristina Toutanova orToutanova Kristina).
Therefore, spelling correctiontechniques that rely crucially on bigram and higherorder language models will fail on queries with a dif-ferent word order than what is observed in the querylog.Unlike general purpose Web Search where it isnot reasonable to assume the availability of a high-coverage trusted lexicon, People Search typicallyemploys large authoritative name directories.
Forinstance, if one is searching for a friend on Face-book, the correct spelling of the friend?s name existsin the Facebook people directory2 (assuming that thefriend is a registered user of Facebook at the time ofthe search).
Similarly, if one is searching for a con-tact in Enterprise address book, the correct spellingof the contact is part of the address book.
In fact,even in Web Search, broad-coverage name directo-ries are available in the form of Wikipedia, IMDB,etc.
The availability of large authoritative name di-rectories that serve as the source of trusted spellingsof names throws open the possibility of correctingmisspelled personal names with the help of namematching techniques (Pfeifer et al, 1996), (Chris-ten, 2006), (Navarro et al, 2003).
However, the bestof the name matching techniques can at best workwith a few thousand names to give acceptable re-sponse time and accuracy.
They do not scale up tothe needs of People Search applications where thedirectories can have millions of names.In this work, we develop hashing-based namesimilarity search techniques and employ them forspelling correction of personal names.
The motiva-tion for using hashing as a building block of spellingcorrection is the following: given a query, we wantto return the global best match in the name directory2http://www.facebook.com/directory/people/that exceeds a similarity threshold.
As matching thequery with the names in the directory is a time con-suming task especially for large name directories,we solve the search problem in two stages:?
NAME BUCKETING: For each token of thequery, we do an approximate nearest neighborsearch of the name tokens of the directory andproduce a list of candidates, i.e., tokens that areapproximate matches of the query token.
Usingthe list of candidate tokens, we extract the listof candidate names which contain at least oneapproximately matching token.?
NAME MATCHING: We do a rigorous match-ing of the query with candidate names.Clearly, our success in finding the right name sug-gestion for the query in the NAME MATCHINGstage depends crucially on our success in gettingthe right name suggestion in the list of candidatesproduced by the NAME BUCKETING stage search.Therefore, we need a name similarity search tech-nique that can ensure very high recall without pro-ducing too many candidates.
Hashing is best suitedfor this task of fast and approximate name match-ing.
We hash the query tokens as well as directorytokens into d bit binary codes.
With binary codes,finding approximate matches for a query token is aseasy as finding all the database tokens that are at aHamming distance of r or less from the query tokenin the binary code representation (Shakhnarovich etal., 2008), (Weiss et al, 2008).
When the binarycodes are compact, this search can be done in a frac-tion of a second on directories containing millionsof names on a simple processor.Our contributions are:?
We develop a novel data-driven technique forlearning hash functions for mapping similarnames to similar binary codes using a set ofnames in a given language/script (i.e.
monolin-gual data).
We formulate the problem of learn-ing hash functions as an optmization problemwhose relaxation can be solved as a generalizedEigenvalue problem.
(Section 2.1).?
We show that hash functions can also be learntusing bilingual data in the form of name equiv-alents in two languages.
We formulate the1257problem of learning hash functions as an opt-mization problem whose relaxation can besolved using Canonical Correlation Analysis.
(Section 2.2)?
We develop new similarity measures for match-ing names (Section 3.1).?
We evaluate the two methods systematicallyand compare our performance against multiplebaselines.
(Section 5).2 Learning Hash FunctionsIn this section, we develop two techniques for learn-ing hash functions using names as training data.
Inthe first approach, we use monolingual data consist-ing of names in a language whereas in the second weuse bilingual name pairs.
In both techniques, the keyidea is the same: we learn hash functions that mapsimilar names in the training data to similar code-words.2.1 M-HASH: Learning with MonolingualNames DataLet (s, s?)
be a pair of names and w (s, s?)
be theirsimilarity3.
We are given a set of name pairs T ={(s, s?)}
as the training data.
Let ?
(s) ?
Rd1 be thefeature representation of s. We want to learn a hashfunction f that maps each name to a d bit codeword:f : s 7?
{?1, 1}d. We also want the Hamming dis-tance of the codeword of s to the codeword of s?
besmall when w (s, s?)
is large.
Further, we want eachbit of the codewords to be either 1 or ?1 with equalprobablity and the successive bits of the codewordsto be uncorrelated.
Thus we arrive at the followingoptimization problem4:minimize :?(s,s?
)?Tw(s, s?)
?
?f (s)?
f(s?)??2s.t.
:?s:(s,s?
)?Tf (s) = 0?s:(s,s?
)?Tf (s) f (s)T = ?2Idf (s) , f(s?)?
{?1, 1}d3We used 1?
length normalized Edit Distance between sand s?
as w (s, s?
).4Note that the Hamming distance of a codeword y to anothercodeword y?
is 14 ?y ?
y?
?2.where Id is an identity matrix of size d?
d.Note that the second constraint helps us avoid thetrap of mapping all names to the same codeword andthereby making the Hamming error zero while satis-fying the first and last constraints.It can be shown that the above minimization prob-lem is NP-hard even for 1-bit codewords (Weiss etal., 2008).
Further, the optimal solution gives code-words only for the names in the training data.
As wewant f to be defined for all s, we address the out-of-sample extension problem by relaxing f as follows5:fR (s) = AT?
(s) =(aT1 ?
(s) , .
.
.
, aTd ?
(s))T(1)where A = [a1, .
.
.
, ad] ?
Rd1?d is a rank d matrix(d ?
d1).After the linear relaxation (Equation 1), the firstconstraint simply means that the data be centered,i.e., have zero mean.
We center ?
by subtracting themean of ?
from every ?
(s) ?
?
to get ?
?.Subsequent to the above relaxation, we get thefollowing optimization problem:minimize : Tr AT ??L?
?TA (2)s.t.
: (3)AT ???
?TA = ?2Id (4)whereL is the graph Laplacian for the similarity ma-trix W defined by the pairwise similarities w (s, s?
).The minimization problem can be transformedinto a generalized Eigenvalue problem and solvedefficiently using either Cholesky factorization or QZalgorithm (Golub and Van Loan, 1996):??L?
?TA = ????TA?
(5)where ?
is a d?
d diagonal matrix.OnceA has been estimated from the training data,the codeword of a name s can be produced by bina-rizing each coordinate of fR (s):f (s) =(sgn(aT1 ?
(s)), .
.
.
, sgn(aTd ?
(s)))T(6)where sgn(u) = 1 if u > 0 and?1 otherwise for allu ?
R.5In contrast to our approach, Spectral Hashing, a well-known hashing technique, makes the unrealistic assumptionthat the training data is sampled from a multidimensional uni-form distribution to address the out-of-sample extension prob-lem (Weiss et al, 2008).1258In the reminder of this work, we call the systemthat uses the hash function learnt from monolingualdata as M-HASH.2.2 B-HASH: Learning with Bilingual NamesDataLet (s, t) be a pair of name s and its transliterationequivalent t in a different language/script.
We aregiven the set T = {(s, t)} as the training data.
Let?
(s) ?
Rd1 (and resp.
?
(t) ?
Rd2) be the featurerepresentation of s (and resp.
t).
We want to learna pair of hash functions f, g that map names to d bitcodewords: f : s 7?
{?1, 1}d, g : t 7?
{?1, 1}d.We also want the Hamming distance of the code-word of a name to the codeword of its transliterationbe small.
As in Section 2.1, we want each bit of thecodewords to be either 1 or?1 with equal probablityand the successive bits of the codewords to be uncor-related.
Thus we arrive at the following optimizationproblem:minimize :?
(s,t)?T?f (s)?
g (t)?2s.t.
:?s:(s,t)?Tf (s) = 0?t:(s,t)?Tg (t) = 0?s:(s,t)?Tf (s) f (s)T = ?2Id?t:(s,t)?Sg (t) g (t)T = ?2Idf (s) , g (t) ?
{?1, 1}dwhere Id is an identity matrix of size d?
d.As we want f (and resp.
g) to be defined for all s(and resp.
t), we relax f (and resp.
g) as follows:fR (s) = AT?
(s) (7)gR (t) = BT?
(s) (8)where A = [a1, .
.
.
, ad] ?
Rd1?d and B =[b1, .
.
.
, bd] ?
Rd2?d are rank d matrices.As before, we center ?
and ?
to get ??
and ??
re-spectively.
Thus, we get the following optimizationproblem:minimize : Tr H(A,B; ?
?, ??)(9)s.t.
: (10)AT ???
?TA = ?2Id (11)BT ???
?TB = ?2Id (12)where H(A,B; ?
?, ??
)=(AT ??
?BT ??
)(AT ??
?BT ??
)T.The minimization problem can be solved as a gen-eralized Eigenvalue problem:???
?TB = ????TA?
(13)???
?TA = ????TB?
(14)where ?
is a d ?
d diagonal matrix.
Further, Equa-tions 13 and 14 find the canonical coefficients of ?
?and ??
(Hardoon et al, 2004).As with monolingual learning, we get the code-word of s by binarizing the coordinates of fR (s)6:f (s) =(sgn(aT1 ?
(s)), .
.
.
, sgn(aTd ?
(s)))T(15)In the reminder of this work, we call the systemthat uses the hash function learnt from bilingual dataas B-HASH.3 Similarity ScoreIn this section, we develop new techniques for com-puting the similarity of names at token level as wellas a whole.
We will use these techniques in theNAME MATCHING stage of our algorithm (Sec-tion 4.2.1).3.1 Token-level SimilarityWe use a logistic function over multiple distancemeasures to compute the similarity between nametokens s and s?
:K(s, s?
)=11 + e?
?i ?idi(s,s?).
(16)While a variety of distance measures can beemployed in Equation 16, two obvious choices6As a biproduct of bilingual learning, we can hash names inthe second language using g:g (t) =(sgn(bT1 ?
(t)), .
.
.
, sgn(bTd ?
(t)))T1259are the normalized Damerau-Levenshtein edit dis-tance between s and s?
and the Hamming dis-tance between the codewords of s and s?
(=?f (s)?
f (s?)?).
In our experiments, we found thatthe continuous relaxation ?fR (s)?
fR (s?)?
wasbetter than ?f (s)?
f (s?)?
and hence we used itwith Damerau-Levenshtein edit distance.
We esti-mated ?1 and ?2 using a small held out set.3.2 Multi-token Name SimilarityLet Q = s1s2 .
.
.
sI and D = s?1s?2 .
.
.
s?J be twomulti-token names.
To compute the similarity be-tween Q and D, we first form a weighted bipartitegraph with a node for each si and a node for each s?jand set edge weight toK(si, s?j).
We then computethe weight (?max) of the maximum weighted match-ing7 in this graph.
The similarity between Q and Dis then computed asK (Q,D) =?max|I ?
J + 1|.
(17)4 Spelling Correction using HashingIn this section, we describe our algorithm forspelling correction using hashing as a buildingblock.4.1 Indexing the Name DirectoryGiven a name directory, we break each name into itsconstituent tokens and form a set of distinct name to-kens.
Using the name tokens and the original names,we build an inverted index which, for each name to-ken, lists all the names that have the token as a con-stituent.
Further, we hash each name token into a dbit codeword as described in Equation 6 (and resp.Equation 15) when using the hash function learnt onmonolingual data (and resp.
bilingual data) and storein a hash table.4.2 Querying the Name DirectoryQuerying is done in two stages:NAME BUCKETING and NAME MATCHING.7In practice, a maximal matching computed using a greedyapproach suffices since many of the edges in the bipartite graphhave low weight.4.2.1 Name BucketingGiven a query Q = s1s2 .
.
.
sI , we hash each siinto a codeword yi and retrieve all codewords in thehash table that are at a Hamming distance of r orless from yi.
We rank the name tokens thus retrievedusing the token level similarity score of Section 3.1and retain only the top 100.
Using the top tokens, weget al names which contain any of the name tokensas a constituent to form the pool of candidates C forthe NAME MATCHING stage.4.2.2 Name MatchingFirst we find the best match for a query Q in theset of candidates C as follows:D?
= argmaxD?CK (Q,D) .
(18)Next we suggest D?
as the correction for Q ifK (Q,D?)
exceeds a certain empirically determinedthreshold.5 Experiments and ResultsWe now discuss the experiments we conducted tostudy the retrieval performance of the two hashing-based approaches developed in the previous sec-tions.
Apart from evaluating the systems on test setsusing different name directories, we were interestedin comparing our systems with several baselines, un-derstanding the effect of some of the choices wemade (e.g.
training data size, conjugate language)and comparative analysis of retrieval performanceon queries of different complexity.5.1 Experimental SetupWe tested the proposed hashing-based spelling cor-rection algorithms on two test sets:?
DUMBTIONARY: 1231 misspellings of var-ious names from Dumbtionary8 and a namedirectory consisting of about 550, 000 namesgleaned from the English Wikipedia.
Each ofthe misspellings had a correct spelling in thename directory.?
INTRANET: 200 misspellings of employeestaken from the search logs of the intranetof a large organization and a name directory8http://www.dumbtionary.com1260consisting of about 150, 000 employee names.Each of the misspellings had a correct spellingin the name directory.Table 1 shows the average edit distance of a mis-spelling from the correct name.
Compared toDUMBTIONARY, the misspellings in INTRANETare more severe as the relatively high edit distanceindicates.
Thus, INTRANET represents very hardcases for spelling correction.Test Set Average Std.
Dev.DUMBTIONARY 1.39 0.76INTRANET 2.33 1.60Table 1: Edit distance of a misspelling from the correctname.5.1.1 TrainingFor M-HASH, we used 30,000 single tokennames in English (sampled from the list of namesin the Internet Movie Database9) as training dataand for B-HASH we used 14,941 parallel single to-ken names in English-Hindi 10.
Each name wasrepresented as a feature vector over character bi-grams.
Thus, the name token Klein has the bigrams{?k, kl, le, ei, in, n?}
as the features.We learnt the hash functions from the trainingdata by solving the generalized Eigenvalue problemsof Sections 2.1 and 2.2.
For both M-HASH and B-HASH we used the top 32 Eigenvectors to form thehash function resulting in a 32 bit representation forevery name token11.5.1.2 Performance MetricWe measured the performance of all the systemsusing Precision@1, the fraction of names for whicha correct spelling was suggested at Rank 1.5.1.3 BaselinesThe baselines are two popular search engines(S1 and S2), Double Metaphone (DM), a widely9http://www.imdb.com10We obtained the names from the organizersof NEWS2009 workshop (http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/pages/sharedtask.html).11We experimented with codewords of various lengths andfound that the 32 bit representation gave the best tradeoff be-tween retrieval accuracy and speed.used phonetic search algorithm (Philips, 2000) andBM25, a very popular Information Retrieval algo-rithm (Manning et al, 2008).
To use BM25 algo-rithm for spelling correction, we represented eachname as a bag of bigrams and set the parameters Kand b to 2 and 0.75 respectively.5.2 Results5.2.1 DUMBTIONARYTable 2 compares the results of the hashing-basedsystems with the baselines on DUMBTIONARY.
Asthe misspellings in DUMBTIONARY are relativelyeasier to correct, all the systems give reasonablygood retrieval results.
Nevertheless, the results ofM-HASH and B-HASH are substantially better thanthe baselines.
M-HASH reduced the error over thebest baseline (S1) by 13.04% whereas B-HASH re-duced by 46.17% (Table 6).M-HASH B-HASH S1 S2 DM BM2587.93 92.53 86.12 79.33 78.95 84.70Table 2: Precision@1 of the various systems on DUMB-TIONARY.To get a deeper understanding of the retrieval per-formance of the various systems, we studied queriesof varying complexity of misspelling.
Table 3 com-pares the results of our systems with S1 for queriesthat are at various edit distances from the correctnames.
We observe that M-HASH and B-HASH arebetter than S1 in dealing with relatively less severemisspellings.
More interestingly, B-HASH is con-sistently and significantly better than S1 even whenthe misspellings are severe.Distance M-HASH B-HASH S11 96.18 96.55 89.592 81.79 87.42 75.763 44.07 67.80 59.654 21.05 31.58 29.425 0.00 37.50 0.00Table 3: Precision@1 for queries at various edit distanceson DUMBTIONARY.5.2.2 INTRANETFor INTRANET, search engines could not be usedas baselines and therefore we compare our systems1261with Double Metaphone and BM25 in Table 4.
Weobserve that both M-HASH and B-HASH give sign-ficantly better retrieval results than the baselines.
M-HASH reduced the error by 36.20% over DoubleMetaphone whereas B-HASH reduced it by 51.73%.Relative to BM25, M-HASH reduced the error by31.87% whereas B-HASH reduced it by 48.44%.M-HASH B-HASH DM BM2570.65 77.79 54.00 56.92Table 4: Precision@1 of the various systems on IN-TRANET.Table 5 shows the results of our systems forqueries that are at various edit distances from thecorrect names.
We observe that the retrieval resultsfor each category of queries are consistent with theresults on DUMBTIONARY.
As before, B-HASHgives signficantly better results than M-HASH.Distance M-HASH B-HASH1 82.76 87.932 57.14 72.863 34.29 65.714 38.46 53.855 6.67 26.67Table 5: Precision@1 for queries at various edit distanceson INTRANET.Test Set M-HASH B-HASHDUMBTIONARY 13.04 46.17INTRANET 36.20 51.73Table 6: Percentage error reduction over the best base-line.5.2.3 Effect of Training Data SizeAs both M-HASH and B-HASH are data drivensystems, the effect of training data size on retrievalperformance is important to study.
Table 7 com-pares the results for systems trained with variousamounts of training data on DUMBTIONARY.
B-HASH trained with just 1000 name pairs gives95.5% of the performance of B-HASH trained with15000 name pairs.
Similarly, M-HASH trained with1000 names gives 98.5% of the performance ofM-HASH trained with 30000 name pairs.
This isprobably because the spelling mistakes in DUMB-TIONARY are relatively easy to correct.Table 8 shows the results on INTRANET.
We seethat increase in the size of training data brings sub-stantial returns for B-HASH.
In contrast, M-HASHgives the best results at 5000 and does not seem tobenefit from additional training data.Size M-HASH B-HASH1000 86.60 88.345000 87.36 91.1310000 86.96 92.5315000 87.19 92.2030000 87.93 -Table 7: Precision@1 on DUMBTIONARY as a functionof training data size.Size M-HASH B-HASH1000 66.04 66.035000 70.65 72.6710000 68.09 75.2615000 68.60 77.7930000 65.40 -Table 8: Precision@1 on INTRANET as a function oftraining data size.5.2.4 Effect of Conjugate LanguageIn Sections 5.2.1 and 5.2.2, we saw that bilingualdata gives substantially better results than monolin-gual data.
In the experiments with bilingual data,we used English-Hindi data for training B-HASH.A natural question to ask is what happens when weuse someother language, say Hebrew or Russian orTamil, instead of Hindi.
In other words, does theretrieval performance, on an average, vary substan-tially with the conjugate language?Table 9 compares the results on DUMB-TIONARY when B-HASH was trained usingEnglish-Hindi, English-Hebrew, English-Russian,and English-Tamil data.
We see that the retrievalresults are good despite the differences in the scriptand language.
Clearly, the source language (Englishin our experiments) benefits from being paired withany target language.
However, some languages seem1262to give substantially better results than others whenused as the conjugate language.
For instance, Hindias a conjugate for English seems to be better thanTamil.
At the time of writing this paper, we do notknow the reason for this behavior.
We believe that acombination of factors including feature representa-tion, training data, and language-specific confusionmatrix need to be studied in greater depth to say any-thing conclusively about conjugate languages.Conjugate DUMBTIONARY INTRANETHindi 92.53 77.79Hebrew 91.30 71.68Russian 89.42 64.94Tamil 90.48 69.12Table 9: Precision@1 of B-HASH for various conjugatelanguages.5.2.5 Error AnalysisWe looked at cases where either M-HASH orB-HASH (or both) failed to suggest the correctspelling.
It turns out that in the DUMBTIONARYtest set, for 81 misspelled names, both M-HASH andB-HASH failed to suggest the correct name at rank1.
Similarly, in the case of INTRANET test set, bothM-HASH and B-HASH failed to suggest the correctname at rank 1 for 47 queries.
This suggests thatqueries that are difficult for one system are also ingeneral difficult for the other system.
However, B-HASH was able to suggest correct names for someof the queries where M-HASH failed.
In fact, in theINTRANET test set, whenever B-HASH failed, M-HASH also failed.
And interestingly, in the DUMB-TIONARY test set, the average edit distance of thequery and the correct name for the cases where M-HASH failed to get the correct name in top 10 whileB-HASH got it at rank 1 was 2.96.
This could be be-cause M-HASH attempts to map names with smalleredit distances to similar codewords.Table 10 shows some interesting cases we foundduring error analysis.
For the first query, M-HASHsuggested the correct name whereas B-HASH didnot.
For the second query, both M-HASH and B-HASH suggested the correct name.
And for the thirdquery, B-HASH suggested the correct name whereasM-HASH did not.Query M-HASH B-HASHJohn Tiler John Tyler John TilleyDdear Dragba Didear Drogba Didear DrogbaJames Pol James Poe James PolkTable 10: Error Analysis.5.3 Query Response TimeThe average query response time is a measure ofthe speed of a system and is an important factorin real deployments of a Spelling Correction sys-tem.
Ideally, one would like the average query re-sponse time to be as small as possible.
However, inpractice, average query response time is not only afunction of the algorithm?s computational complex-ity but also the computational infrastructure support-ing the system.
In our expriments, we used a sin-gle threaded implementation of M-HASH and B-HASH on an Intel Xeon processor (2.86 GHz).
Ta-ble 11 shows the average query response time.
Wenote that M-HASH is substantially slower than B-HASH.
This is because the number of collisionsin the NAME BUCKETING stage is higher for M-HASH.We would like to point out that bothNAME BUCKETING and NAME MATCHINGstages can be multi-threaded on a multi-core ma-chine and the query response time can be decreasedby an order easily.
Further, the memory footprintof the system is very small and the codewordsrequire 4.1 MB for the employees name directory(150,000 names) and 13.8 MB for the Wikipedianame directory (550,000 names).Test Set MHASH BHASHDUMBTIONARY 190 87INTRANET 148 75Table 11: Average response time in milliseconds (singlethreaded system running on 2.86 GHz Intel Xeon Proces-sor).6 Related WorkSpelling correction of written text is a well stud-ied problem (Kukich, 1996), (Jurafsky and Mar-tin, 2008).
The first approach to spelling correc-1263tion made use of a lexicon to correct out-of-lexiconterms by finding the closest in-lexicon word (Dam-erau, 1964).
The similarity between a misspelledword and an in-lexicon word was measured usingEdit Distance (Jurafsky and Martin, 2008).
The nextclass of approaches applied the noisy channel modelto correct single word spelling errors (Kernighan etal., 1990), (Brill and Moore, 2000).
A major flaw ofsingle word spelling correction algorithms is they donot make use of the context of the word in correctingthe errors.
The next stream of approaches exploredways of exploiting the word?s context (Golding andRoth, 1996), (Cucerzan and Brill, 2004).
Recently,several works have leveraged the Web for improvedspelling correction (Chen et al, 2007),(Islam andInkpen, 2009), (Whitelaw et al, 2009).
Spelling cor-rection algorithms targeted for web-search querieshave been developed making use of query logs andclick-thru data (Cucerzan and Brill, 2004), (Ah-mad and Kondrak, 2005), (Sun et al, 2010).
Noneof these approaches focus exclusively on correctingname misspellings.Name matching techniques have been studied inthe context of database record deduplication, textmining, and information retrieval (Christen, 2006),(Pfeifer et al, 1996).
Most techniques use one ormore measures of phonetic similarity and/or stringsimilarity.
The popular phonetic similarity-basedtechniques are Soundex, Phonix, and Metaphone(Pfeifer et al, 1996).
Some of the string similarity-based techniques employ Damerau-Levenshtein editdistance, Jaro distance or Winkler distance (Chris-ten, 2006).
Data driven approaches for learning editdistance have also been proposed (Ristad and Yiani-los, 1996).
Most of these techniques either give poorretrieval performance on large name directories ordo not scale.Hashing techniques for similarity search is also awell studied problem (Shakhnarovich et al, 2008).Locality Sensitive Hashing (LSH) is a theoreticallygrounded data-oblivious approach for using randomprojections to define the hash functions for data ob-jects with a single view (Charikar, 2002), (Andoniand Indyk, 2006).
Although LSH guarantees thatasymptotically the Hamming distance between thecodewords approaches the Euclidean distance be-tween the data objects, it is known to produce longcodewords making it practically inefficient.
Re-cently data-aware approaches that employ MachineLearning techniques to learn hash functions havebeen proposed and shown to be a lot more effectivethan LSH on both synthetic and real data.
SemanticHashing employs Restricted Boltzmann Machine toproduce more compact codes than LSH (Salakhutdi-nov and Hinton, 2009).
Spectral Hashing formalizesthe requirements for a good code and relates them tothe problem of balanced graph partitioning which isknown to be NP hard (Weiss et al, 2008).
To givea practical algorithm for hashing, Spectral Hashingassumes that the data are sampled from a multidi-mensional uniform distribution and solves a relaxedpartitioning problem.7 ConclusionsWe developed two hashing-based techniques forspelling correction of person names in PeopleSearch applications.To the best of our knowledge,these are the first techniques that focus exclusivelyon correcting spelling mistakes in person names.Our approach has several advantages over otherspelling correction techniques.
Firstly, we do notsuggest incorrect suggestions for valid queries un-like (Cucerzan and Brill, 2004).
Further, as we sug-gest spellings from only authoritative name direc-tories, the suggestions are always well formed andcoherent.
Secondly, we do not require query logsand other resources that are not easily available un-like (Cucerzan and Brill, 2004), (Ahmad and Kon-drak, 2005).
Neither do we require pairs of mis-spelled names and their correct spellings for learn-ing the error model unlike (Brill and Moore, 2000)or large-coverage general purpose lexicon for unlike(Cucerzan and Brill, 2004) or pronunciation dictio-naries unlike (Toutanova and Moore, 2002).
Thirdly,we correct the query as a whole unlike (Ahmad andKondrak, 2005) and can handle word order changesunlike (Cucerzan and Brill, 2004).
Fourthly, wedo not iteratively process misspelled name unlike(Cucerzan and Brill, 2004).
Fifthly, we handle largename directories efficiently unlike the spectrum ofname matching techniques discussed in (Pfeifer etal., 1996).
Finally, our training data requirement isrelatively small.As future work, we would like to explore the pos-sibility of learning hash functions using 1) bilingual1264and monolingual data together and 2) multiple con-jugate languages.ReferencesFarooq Ahmad and Grzegorz Kondrak.
2005.
Learn-ing a spelling error model from search query logs.
InHLT ?05: Proceedings of the conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing, pages 955?962, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Alexandr Andoni and Piotr Indyk.
2006.
Near-optimalhashing algorithms for approximate nearest neighborin high dimensions.
In FOCS, pages 459?468.Rahul Bhagat and Eduard H. Hovy.
2007.
Phonetic mod-els for generating spelling variants.
In IJCAI, pages1570?1575.Mikhail Bilenko, Raymond J. Mooney, William W. Co-hen, Pradeep D. Ravikumar, and Stephen E. Fienberg.2003.
Adaptive name matching in information inte-gration.
IEEE Intelligent Systems, 18(5):16?23.E.
Brill and R. Moore.
2000.
An improved error modelfor noisy channel spelling correction.
In Proceedingsof ACL ?00, pages 286?293.Moses Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In STOC, pages 380?388.Qing Chen, Mu Li, and Ming Zhou.
2007.
Improvingquery spelling correction using web search results.
InEMNLP-CoNLL, pages 181?189.P.
Christen.
2006.
A comparison of personal namematching: techniques and practical issues.
Techni-cal Report TR-CS-06-02, Dept.
of Computer Science,ANU, Canberra.William W. Cohen, Pradeep D. Ravikumar, andStephen E. Fienberg.
2003.
A comparison of stringdistance metrics for name-matching tasks.
In IIWeb,pages 73?78.S Cucerzan and E. Brill.
2004.
Spelling correction as aniterative process that exploits the collective knowledgeof web users.
In Proceedings of EMNLP ?04, pages293?300.F.J.
Damerau.
1964.
A technique for computer detectionand correction of spelling errors.
Communications ofACM, 7(3):171?176.C.
Friedman and R. Sideli.
1992.
Tolerating spellingerrors during patient validation.
Computers andBiomedical Research, 25:486?509.Andrew R. Golding and Dan Roth.
1996.
Applying win-now to context-sensitive spelling correction.
CoRR,cmp-lg/9607024.Gene H. Golub and Charles F. Van Loan.
1996.
MatrixComputations.
Johns Hopkins University Press, Balti-more, MD, 3rd edition.David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-Taylor.
2004.
Canonical correlation analysis: Anoverview with application to learning methods.
Neu-ral Computation, 16(12):2639?2664.Aminul Islam and Diana Inkpen.
2009.
Real-wordspelling correction using google web 1tn-gram dataset.
In CIKM, pages 1689?1692.D.
Jurafsky and J.H.
Martin.
2008.
Speech and Lan-guage Processing.
Prentice-Hall.Mark D. Kernighan, Kenneth W. Church, and William A.Gale.
1990.
A spelling correction program based on anoisy channel model.
In COLING, pages 205?210.K.
Kukich.
1996.
Techniques for automatically correct-ing words in a text.
Computing Surveys, 24(4):377?439.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.G.
Navarro, R. Baeza-Yates, and J. Azevedo-Arcoverde.2003.
Matchsimile: a flexible approximate matchingtool for searching proper names.
Journal of the Amer-ican Society for Information Science and Technology,54(1):3?15.U.
Pfeifer, T. Poersch, and N. Fuhr.
1996.
Retrieval ef-fectiveness of proper name search methods.
Informa-tion Processing and Management, 32(6):667?679.L.
Philips.
2000.
The double metaphone search algo-rithm.
C/C++ Users Journal.Eric Sven Ristad and Peter N. Yianilos.
1996.
Learningstring edit distance.
CoRR, cmp-lg/9610005.Ruslan Salakhutdinov and Geoffrey E. Hinton.
2009.
Se-mantic hashing.
Int.
J. Approx.
Reasoning, 50(7):969?978.Gregory Shakhnarovich, Trevor Darrell, and Piotr In-dyk.
2008.
Nearest-neighbor methods in learningand vision.
IEEE Transactions on Neural Networks,19(2):377?377.Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.2010.
Learning phrase-based spelling error modelsfrom clickthrough data.
In Proceedings of ACL 2010.K.
Toutanova and R. Moore.
2002.
Pronounciation mod-eling for improved spelling correction.
In Proceedingsof ACL ?02, pages 141?151.Yair Weiss, Antonio B. Torralba, and Robert Fergus.2008.
Spectral hashing.
In NIPS, pages 1753?1760.Casey Whitelaw, Ben Hutchinson, Grace Chung, and GedEllis.
2009.
Using the web for language independentspellchecking and autocorrection.
In EMNLP, pages890?899.1265
