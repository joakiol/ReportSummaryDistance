Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 257?260,Prague, June 2007. c?2007 Association for Computational LinguisticsOE: WSD Using Optimal Ensembling (OE) MethodHarri M. T. SaarikoskiHelsinki UniversityLanguage Technology PhD ProgrammeF-00014 Helsinki, Finlandharri.saarikoski@helsinki.fiAbstractOptimal ensembling (OE) is a word sensedisambiguation  (WSD)  method  usingword-specific training factors (average pos-itive vs negative training per sense,  posexand negex) to predict best system (classifieralgorithm / applicable feature set) for giventarget  word.
Our  official  entry  (OE1)  inSenseval-4  Task  17  (coarse-grained  En-glish lexical sample task) contained manydesign flaws and thus  failed to  show thewhole  potential  of  the  method,  finishing-4.9% behind top system (+0.5 gain overbest  base  system).
A fixed system (OE2)finished  only  -3.4%  behind  (+2.0%  netgain).
All  our  systems  were  'closed',  i.e.used the official training data only (average56 training examples per each sense).
Wealso show that the official evaluation mea-sure  tends  to  favor  systems  that  do  wellwith high-trained words.1 IntroductionOptimal  ensembling  is  a  novel  method  forcombining  WSD  systems  and  obtaining  higherclassification  accuracy  (presented  more  fully  inSaarikoski  et  al.
2007).
The  essential  differencefrom other ensembling methods (such as varioustypes  of  voting  ensembles  and  cross-validationbased best machine selection) is that best machineis  predicted  using  factors  calculated  from words(e.g.
number of senses) and their training data (e.g.number  of  training  examples  per  sense).
Themethod  is  loosely  based  on  findings  of  systemperformance  differences  in  both  WSD (differentmachines by Yarowsky et al,  2002 and differentfeature  sets  by  Mihalcea,  2002)  and  otherclassification  tasks  such  as  text  categorization(Forman et al, 2004, Bay et al, 2002).2 MethodWe  first  describe  in  detail  the  two  selectionroutines in OE as deployed in this experiment.2.1 Machine (Mach) SelectionWe  selected  support  vector  machine  (SVM)(Vapnik, 1995) and Naive Bayes (NB) (John et al1995)  as  classifiers  for  our  base  systems  to  beoptimally ensembled.
This was mainly because oftheir  attested  strength  at  earlier  Sensevalevaluations  (Edmonds et al 2002, Mihalcea et al2004)  and mutual complementarity discovered byus  (Saarikoski  et  al.,  2007).
Original  batch  ofcandidate  machines  that  we  tested  for  OE usingSenseval-2  dataset  included  the  followingclassifiers:  Decision  Stump,  Decision  Tree  withvarious  values  of  confidence (c)  parameter  0.05,0.15,  0.25  and  instance-based  classifier  with  kvalues  ranging  from 1..15  at  intervals  of  two  1.After cross-validation runs against current dataset(see below), however, SVM and NB proved againto be overall strongest regardless of training input,so we built OE around those two classifiers.2.2 Feature Set (Fset) SelectionWe extracted  three  contextual  feature  sets  fromtraining data for all words to train the machines: 1-grams  (1g)  and  sequential  2-grams  both  fromwhole instance (2g) as well as part-of-speech tagsfrom local  1-word window around and includingtarget word (pos3).
We also used three 'multifsets'(1g-2g, 1g-pos3, 2g-pos3).1We used Weka implementations (J48, Ibk, SMO, DecisionStump, NaiveBayes) of these algorithms (Witten, 2005).2572.3 Best-System Prediction FactorsIn Figure 1, we quote prediction factors used forpredicting best system for some test words.Figure 1.
Prediction factors and OE1 accuracy forsome test words in Senseval-4 Task 17 (sorted byOE1 accuracy at the word).3 System DescriptionsWe designed and ran two systems:OE1 (official): For OE1, we used two machinesin  three  configurations  (SVMc=0.1,  SVMc=1.0,NB) trained on 3 feature sets, totalling at 3*3 = 9base  systems  (number  of  machines  *  number  offsets for each).
Selection of c(omplexity) parameterfor  SVM  was  based  on  previous  knowledge  ofperformance differences of c=0.1 and c=1.0 basedsystems  as  reported  in  Saarikoski  et  al.
(2007).This is based on accounts by e.g.
Vapnik (1995)that  lower  c  value makes  the classifier generate amore  complex  training  model  which  is  moresuitable  for  tougher  words  (lower  posex,  highernegex).We  learned  the  best-system  predictor  modelusing  performance  data  from  Senseval-4  10CVruns only.
For 70 words where two fsets performedwithin +/-5% of each other, we added the next bestfset into a 'multifset'.OE2 (unofficial): This system incorporated thefollowing fixes to OE1 (see Discussion below formotivations for these fixes): First, we significantlyreduced the base system grain.
We only used twomachines strongest in 10CV runs (SVMc=0.1 andNB)  and  these  machines  were  trained  with  fsetsfound best for those machines in 10CV runs: pos3for  both  machines,  SVMc=0.1  was  additionallytrained with 1g and NB with 2g respectively.
Thisresulted in a 2 * 2 = 4-system ensemble.
Best fsetwas still selected on the basis of 10CV runs.As training data for the best-machine predictor,we  used  the  performance  profiles  of  about  50systems (both our own and Senseval systems) runmainly against Senseval-2 English lexical sampledataset.
We  decided  to  use  only  two  predictionfactors (posex and negex,  see Figure 1) to predictbest  machine  for  each  word.
This  was  becausepreviously  we  had  found  these  two  machines(SVM and NB) particularly differing with regardto  the  combination or cross-section of  these  twofactors.
(For  illustration  of  the  predictor  modelwith  posex  and  negex  as  the  two  axes  anddiscussion of other possible factors, see Saarikoskiet.
al, 2007.
As to reasons for such a performancedifference  between  any  two  classificationmachines, see also Yarowsky et al, 2002).Difference  in  the  best-system  predictions  ofthese two systems (OE1 vs OE2) was substantial:33  words  fully  changed  machine  (from SVM toNB or vice versa), 40 words partially changed thesystem (change of SVM configuration or change offset from multifset to single fset).
Only 27 wordskept the same machine in same configuration andfset.
We  can  therefore  call  OE2  a  substantialrevision of OE1 (in effect a  rather total  departurefrom CV-based selection toward actual word factorbased optimal ensembling).In  both  OEs,  the  mach-fset  combinationpredicted to be the best for a word was run againstthe  test  instances  of  that  word  2.
In  case  of'multifsets', each single fset had equal probability-based vote in disambiguating the test instances of2 SyntaLex  code  (Mohammad  and  Pedersen,  2002,http://www.d.umn.edu/~tpederse/syntalex.html)  was  used  forextracting  n-grams  and  carrying  out  disambiguation.
BrillTagger (Brill, 1995) was used for extracting PoS tags.
Wekalibrary of classifiers (Witten, 2005) was used to run cross-vali-dations and best-system predictors.258that  word.
As  usual,  the  sense  with  highestprobability  was  chosen  as  answer  for  eachinstance.4 Test ResultsHere are the results:system name gross gain net gain accuracy3OE1 +3.0 (+7.8) +0.5 (+4.4) 83.8OE2 +2.3 (+7.0) +2.0 (+5.8) 85.3Table 1.
Results of OE systems.
In columns 2-3,macro  (micro)  averaged  per-word  gross  and  netgains calculated from actual  test  runs (not 10CVruns) are  reported.
Column  4 reports  the  officialmacro-averaged  accuracy  for  all  words  of  oursystems.
(Differences of the respective benefits ofthese  evaluation  measures  are  outlined  inDiscussion below and more generally in Sebastiani(2002).
Terms 'gross (or  potential)  gain'  and 'net(realized)  gain'  are  defined  in  Saarikoski  et  al.(2007).
).5 DiscussionWe now turn to analyze these results.
We can firstnote  that  results  are  largely  in  line  with  ourprevious  findings  with  OEs  and  other  types  ofensembles  (see  Saarikoski  et  al.,  2007).
In  whatfollows we attempt to account for the results: whyOE1 finished as much behind top system and alsowhy OE2 performed that  much better  than OE1.This first 'known issue' concerns both OEs:(1) Base system accuracy was low because wedid  not  use  strong  fsets: Our  official  entryfinished at 7th place in the evaluation, -4.9% behindtop system while the inofficial entry would havefinished in 5th place (-3.4% behind).
We attributethis  mainly  to  the  absence  of  more  advancedfeature  sets.
For  example,  we  did  not  employsyntactic  parse  features  (such  as  predicate-objectpairs) from which Yarowsky et al (2002) showed+2% gain.
We would  also  naturally  lose  to  anysystems using extra training or lexical knowledge(e.g.
2nd place  finisher  UBC-ALM,  at  86.9accuracy, used both semantic domains and SemCorcorpus).
But  without  knowing  how  much  extraknowledge  such  'open'  systems  used,  we  cannotsay by how much.3 Best base system in both OEs was NB-pos3 (83.3).Specifically in OE1 entry, there were two basicdesign flaws which we address next.
(2) Base system grain was too high to produceenough net gain: The base  system grain (18 basesystems) we attempted to predict in OE1 was fartoo  great  since  prediction  accuracy  rapidlydecreases  when  adding  new  systems.
The  grainwas also unnecessarily great, since the 4-grain weused for OE2 could harvest most of the gross gain(cf.
gross gains of the two systems in Table 1).
(3) Using 10CV runs uncritically for best fsetselection: This  was  ill-advised  because  of  manyreasons.
First, selecting best fset for WSD based onCV runs is known to be a difficult task (Mihalcea,2002).
Prediction accuracy for the three fsets weused for OE1 was 0.74, i.e.
for 26 words out of 100best  fset  was  mispredicted.
About  half  of  thesewere  cases  where  machine  was  mispredicted  aswell  and average loss tended to be even greater.Second,  multifsets could not be 10CV-tested withthe  Weka  machine-learning  toolkit  we  used(Witten,  2005).
Our  custom  resolution  to  thismultifset selection task was to select best and nextbest  fset.
This turned out  to produce many falsepredictions, some of which were quite substantial(> 10% loss to best fset).
For instance, at system.nwe lost  > 30% from selecting  NB-2g instead  ofactual  best  system  (NB-pos3).
Third,  only  aftersubmitting the entry, we also realized two strongestfsets  are not necessarily complementary (i.e.
thateach would contain relevant clues for different testinstances)  and  that  learning  machines  might  beconfused  (i.e.
could  not  effectively  carry  outfeature selection and weighting) by the profusionand heterogeneity of features in multifsets.
In fact,we found that omitting multifsets from  OE1 (i.e.having  3 single fsets with the same 3 machines =6-system OE) would have worked slightly betterthan OE1 (3*3=9): the accuracy rose from 83.8 to84.1.
Fourth, it was found previously (Saarikoski etal.,  2007)  that  CV-based  best  system  predictionscheme  tends  to  produce  less  gain  than  OE (cf.accuracy of OE1 < OE2 in Table 1).The  remaining  argument  discusses  Sensevalevaluation measure (applies to all OE systems):(4) Official evaluation measure is particularlyunfavorable  to  OE  systems:  Senseval  scoringscheme4 is calculated as the  number of  instancesdisambiguated correctly divided by number of all4 Documentation for scoring scheme can be found at:http://www.cse.unt.edu/~rada/senseval/senseval3/scoring/259instances  in  test  dataset.
This  measure  (termed'macro-averaged accuracy' in Sebastiani,  2002) isknown to upweigh classification cases (words) thathave more test instances.
While we recognize theusefulness of this measure, we calculated in Table1 the alternative measure (termed 'micro-averagedaccuracy' in Sebastiani, 2002).
It  differs from theformer  (defined by e.g.
Sebastiani,  2002) in thatall  words  are  treated  equally (i.e.
'normalized')regardless of number of test instances.
In addition,it has been Senseval practice (Edmonds et al 2002,Mihalcea et al 2004) that words with great numberof  test  instances  tend  to  have  an  equally  greatnumber  of  training  instances.
At  such  'easier'words,  system  performance  differences  (sysdiff)occur much less and since OE is based on locatingand making use of sysdiff, it cannot perform well.Therefore,  it  is  liable  to  lose  to  single-machinesystems with inherently stronger fsets (see point 1above).
For these reasons, the measures are verydifferent with the latter revealing the OE potentialmore appropriately.In fact, we estimate that only 40 out of the 100test words in this dataset show any kind of sysdiffbetween most participating systems (> 5% macro-averaged sysdiff per word).
Furthermore, only 20of  them  only  are  likely  to  produce  substantialsysdiff (> 10%).
For example, in our 10CV runs,we got 0.99 accuracies by all base systems for thevery highly trained word say.v with posex > 500.
Ifthere was a participating system that achieved 1.00in such a single high-train word (say.v), the hugenumber  of  test  instances  of  that  word  raised  itsmacro-averaged  accuracy,  winning  considerablyover systems performing well with low-train words(e.g.
propose.v with posex=11 and negex=24 andgrain=3 where  both  OE1 and OE2 performed at0.93  accuracy  owing  to  correct  best  systemchoice).
In other words, the official measure doesnot account for the finding (Yarowsky et al, 2002and  Saarikoski  et  al.,  2007)  that  systemsconsiderably  differ  precisely  in  terms  of  theirability  to  disambiguate  high/low-train  words(measured  by  posex/negex  factors).
Therefore,  itcan be said that the official measure fails to treatall systems equally.6 Conclusion and Further WorkSince OE is a generic method that can be appliedto any base systems, we believe it has a place inWSD  methodology.
With  remaining  openquestions  resolved  (optimizing  system  grain  tofeasible  prediction  accuracy,  discovering  morepredictive  factors  for  both  machines  and  fsets,understanding  how  the  evaluation  measurescomplete each other),  it  is  probable  that  OE canimprove  current  state  of  the  art  WSD  systems(especially  if  provided  with  stronger  while  stillcomplementary base systems).
Though OE systemsrun the risk that OE may in fact be inferior to itsbest base system, we would like to note that thusfar no OE of ours (around 10-15 different tests) hasfailed to produce net gain.ReferencesBay, S. D., and Pazzani, M. J. Characterizing model errors anddifferences.
In 17th International Conference on MachineLearning (2000)Brill,  E.  Transformation-Based  Error-Driven  Learning  andNatural  Language  Processing:  A Case  Study  in  Part  ofSpeech Tagging Computational Linguistics (1995)Edmonds,  P.,  and Kilgarriff,  A.
Introduction to  the  SpecialIssue on evaluating word sense disambiguation programs.Journal of Natural Language Engineering 8(4) (2002)Forman, G., and Cohen, I.
Learning from Little: Comparisonof  Classifiers  Given  Little  Training.
In  ECML,  15thEuropean Conference on Machine Learning and the  8thEuropean  Conference  on  Principles  and  Practice  ofKnowledge Discovery in Databases (2004)John, G. and Langley, P. Estimating Continuous Distributionsin  Bayesian  Classifiers.
Proceedings  of  the  EleventhConference  on  Uncertainty  in  Artificial  Intelligence.Morgan Kaufmann, San Mateo  (1995)Mihalcea, R. Word sense disambiguation with pattern learningand  automatic  feature  selection.
Journal  of  NaturalLanguage Engineering, 8(4) (2002)Mihalcea,  R.,  Kilgarriff,  A.  and  Chklovski,  T.  TheSENSEVAL-3 English lexical sample task.
Proceedings ofSENSEVAL-3 Workshop at ACL (2004)Mohammad, S. and Pedersen, T (2004).
Complementarity ofLexical  and  Simple  Syntactic  Features:  The  SyntalexApproach to Senseval-3.
Proceedings of Senseval-3Saarikoski,  H.,  Legrand,  S.,  Gelbukh,  A.
(2007)  Case-Sensitivity  of  Classifiers  for  WSD:  Complex  SystemsDisambiguate Tough Words Better.
In CICLING 2007 andLecture Notes in Computer Science, SpringerSebastiani,  F.  Machine  learning  in  automated  textcategorization,  ACM Computing  Surveys  (CSUR),  Vol.34 Issue 1 (2002) ACM Press, New York, NY, USA.Vapnik,  V.  N.  The  Nature  of  Statistical  Learning  Theory.Springer (1995)Witten, I., Frank, E. Data Mining: Practical Machine LearningTools  and  Techniques  (Second  Edition).
MorganKaufmann (2005).Yarowsky,  D.  and  Florian,  R.  Evaluating  sensedisambiguation across  diverse  parameter  spaces.
Journalof Natural Language Engineering, 8(4) (2002)  293-311.260
