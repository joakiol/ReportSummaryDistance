Vector-based Natural Language CallRoutingJenni fer  Chu-Carrol l*Lucent Technologies Bell LaboratoriesBob Carpenter*Lucent Technologies Bell LaboratoriesThis paper describes a domain-independent, automatically trained natural language call routerfor directing incoming calls in a call center.
Our call router directs customer calls based ontheir response to an open-ended How may I direct your call?
prompt.
Routing behavior istrained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques.
Terms consist of n-gram sequences ofmorphologicallyreduced content words, while documents representing routing destinations consist of weightedterm frequencies derived from calls to that destination in the training corpus.
Based on thestatistical discriminating power of the n-gram terms extracted from the caller's request, he calleris 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked adisambiguation question.
In the last case, the system dynamically generates queries tailored tothe caller's request and the destinations with which it is consistent, based on our extension ofthe vector model.
Evaluation of the call router performance over a financial services call centerusing both accurate transcriptions of calls and fairly noisy speech recognizer output demonstratedrobustness inthe face of speech recognition errors.
More specifically, using accurate transcriptionsof speech input, our system correctly routed 93.8% of the calls after redirecting 10.2% of all callsto a human operator.
Using speech recognizer output with a 23% error rate reduced the numberof correctly routed calls by 4%.1.
IntroductionThe call routing task is one of directing acustomer's call to an appropriate destinationwithin a call center or directly providing some simple information, such as currentloan rates, on the basis of some kind of interaction with the customer.
In currentsystems, such interaction is typically carried out via a touch-tone system with a rigidpredetermined navigational menu.
The primary disadvantages of navigating menusfor users are the time it takes to listen to all the options and the difficulty of matchingtheir goals to the given options.
These problems are compounded by the necessityof descending a nested hierarchy of choices to zero in on a particular activity.
Evenrequests with simple English phrasings uch as I want the balance on my car loan mayrequire users to navigate as many as four or five nested menus with four or fiveoptions each.
We describe an alternative to touch-tone menus that allows users tointeract with a call router in natural spoken English dialogues just as they would witha human operator.In a typical dialogue between a caller and a human operator, the operator respondsto a caller request by either outing the call to an appropriate destination, or queryingthe caller for further information to determine where the call should be routed.
Thus,* 600 Mountain Avenue, Murray Hill, NJ 07974.
E-mail: jencc@research.belMabs.comt 600 Mountain Avenue, Murray Hill, NJ 07974.
E-mail: carp@research.belMabs.com(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 3in developing an automatic all router, we select between these two options as well asa third option of sending the call to a human operator in situations where the routerrecognizes that to automatically handle the call is beyond its capabilities.
The rest ofthis paper provides both a description and an evaluation of an automatic all routerthat consists of 1) a routing module driven by a novel application of vector-basedinformation retrieval techniques, and 2) a disambiguation query generation modulethat utilizes the same vector representations a  the routing module and dynamicallygenerates queries tailored to the caller's request and the destinations with which it isconsistent, based on our extension of the vector model.
The overall call routing systemhas the following desirable characteristics: First, the training of the call router is domainindependent and fully automatic, 1 allowing the system to be easily ported to newdomains.
Second, the disambiguation module dynamically generates queries basedon caller requests and candidate destinations, allowing the system to tailor queries tospecific circumstances.
Third, the system is highly robust o speech recognition errors.Finally, the overall performance of the system is high, in particular when using noisyspeech recognizer output.
With transcription (perfect recognition), we redirect 10.2%of the calls to the operator, correctly routing 93.8% of the remainder either with orwithout disambiguation.
With spoken input processed automatically with recognitionperformance at a 23% word error rate, the percentage of correctly routed calls dropsby only 4%.2.
Related WorkCall routing is similar to text categorization i identifying which one of n topics (orin the case of call routing, destinations) most closely matches a caller's request.
Callrouting is distinguished from text categorization by requiring a single destination tobe selected, but allowing a request o be refined in an interactive dialogue.
The closestprevious work to ours is Ittner, Lewis, and Ahn (1995), in which noisy documentsproduced by optical character recognition are classified against multiple categories.
Weare further interested in carrying out the routing process using natural, conversationallanguage.The only work on natural anguage call routing to date that we are aware of is thatby Gorin and his colleagues (Gorin, Riccardi, and Wright 1997; Abella and Gorin 1997;Riccardi and Gorin 1998), who designed an automated system to route calls to AT&Toperators.
They select salient phrase fragments from caller requests (in response to thesystem's prompt of How may I help you ?
), such as made a long distance and the area code for,and sometimes including phrases that are not meaningful syntactic or semantic units,such as it on my credit.
These salient phrase fragments, which are incorporated intotheir finite-state language model for their speech recognizer, are then used to computelikely destinations, which they refer to as call types.
This is done by either computinga posteriori probabilities for all possible call types (Gorin 1996) or by passing theweighted fragments through a neural network classifier (Wright, Gorin, and Riccardi1997).
Abella and Gorin (1997) utilized the Boolean formula minimization algorithmfor combining the resulting set of call types based on a hand-coded hierarchy of calltypes.
This algorithm provides the basis for determining whether or not the goal of therequest can be uniquely identified, in order to select from a set of dialogue strategiesfor response generation.1 The training process i automatic except for minor editing of a standard stop list, which will bediscussed inSection 4.1.2, and for the mapping between n-gram morphologically reduced noun phrasesand their expanded forms in the disambiguation process, which will be discussed in Section 4.2.362Chu-Carroll and Carpenter Vector-based Natural Language Call Routing250200150100500 llIlllilllIE~ItilllllTFIH~ .
.
.
.
.
.20 40 60 80 100 120Number of Words in Initial User Ulterance120010006OO2OO0 .
.
.
.2 4 6 8 10 12 14 18 18Number of Conlent Words in Initial User Ultersnce(a) Total wordsFigure 1Histogram of call lengths.
(b) Content words3.
Corpus AnalysisTo examine human-human dialogue behavior, we analyzed a set of 4,497 transcribedtelephone calls involving actual customers interacting with human call operators ata large call center.
In the vast majority of these calls, the first customer utterancecontained between 1 and 20 words, while the longest first utterance had 131 words.However, these utterances included only a few content words ,  2 with almost all callscontaining fewer than 10 content words in the initial user utterance.
Figures l(a) andl(b) show histograms of call lengths based on total words and content words in theinitial user utterance in each call, respectively.Figure 2 shows the distribution of calls to the top 23 destinations on a log scalein our corpus.
3The perplexity of a probabil ity distribution provides a measure of thedifficulty of classification of samples drawn from that distribution.
Using the estimateof call distribution based on Figure 2, our task perplexity is 6.836.
4We further analyzed our corpus of calls along two dimensions: the semantics ofcaller requests and the dialogue actions for operator esponses.
The analysis of thesemantics of caller requests is intended to examine the ways in which users typicallyexpress their goal when prompted,  and is used to focus on an appropriate subset of theclasses of user utterances that the call router should handle automatically (as opposedto transferring to a human operator).
The analysis of the dialogue actions for operatorresponses, on the other hand, is intended to determine the types of responses the callrouter should be able to provide in response to user utterances in order to help designthe response generation component of the call router.
The analysis of the corpus alongboth dimensions was performed by the first author.3.1 Semantics of Caller RequestsIn our corpus, all callers respond to an initial open-ended prompt  of /ABC/ bankingservices call director; how may I direct your call?
Their responses varied greatly in their2 Content words are keywords automatically extracted from the training corpus that are consideredrelevant for routing purposes.
For details on how the list of content words is selected, see Section 4.1.2.3 These are destinations that received more than 10 calls in the corpus we analyzed.4 Recall that the entropy of a distribution p is the expected value of the log probability, given byH(p) = - Y'~x p(x) log 2 p(x).
The perplexity isgiven by 2 H(p) and can be thought of roughly as thenumber of equiprobable categories that would lead to the same classification difficulty.363Computational Linguistics Volume 25, Number 33.5~ 3o1mo 2.5T '5 2.
J  1.s II ~Destinations Figure 2Distribution of calls.degree of specificity.
We roughly classified the calls into the following three broadclasses:Destination Name, in which the caller explicitly specifies the name ofthe department towhich he wishes to be transferred.
The requesteddestination can form an answer to the operator's prompt by itself, as indeposit services, or be part of a complete sentence, as in I would like to speakto someone in auto leasing please.Activity, in which the caller provides a description of the activity hewishes to perform, and expects the operator to transfer his call to theappropriate department that handles the given activity.
Such descriptionsmay be ambiguous or unambiguous, depending on the level of detail thecaller provides, which in turn depends on the caller's understanding ofthe organization of the call center.
Because all transactions related tosavings accounts are handled by the deposit services department in thecall center we studied, the request I want to talk to someone about savingsaccounts will be routed to Deposit Services.
On the other hand, thesimilar request I want to talk to someone about car loans is ambiguousbetween Consumer Lending, which handles new car loans, and LoanServices, which handles existing car loans.
Queries can also beambiguous due to the caller's providing more than one activity, as in Ineed to get my checking account balance and then pay a car loan.Indirect Request, in which the caller describes his goal in a roundaboutway, often including irrelevant information.
This typically occurs withcallers who are unfamiliar with the call center organization, or those whohave difficulty concisely describing their goals.
An example of an actualindirect request is ah I'm calling "cuz ah a friend gave me this number and ahshe told me ah with this number I can buy some cars or whatever but she didn'tknow how to explain it to me so I just called you you know to get that information.Table I shows the distribution of caller requests in our corpus with respect to thesesemantic types.
Our analysis hows that in the vast majority of calls, the request wasbased on either destination name or activity.
Since in our corpus there are only 23 dis-364Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingTable 1Semantic types of caller requests.Destination Name Activity Indirect Request# of calls 949 3271 277% of all calls 21.1% 72.7% 6.2%/Table 2 ..-Call operator dialogue actions.Notification QueryNP Others# of calls 3,608 657 232% of all calls 80.2% 14.6% 5.2%tinct destinations, 5 and each destination only handles a fairly small number  (dozensto hundreds) of activities, requests based on destination ames and activities are ex-pected to be more predictable and thus more suitable for handling by an automaticcall router.
However, our system does not directly classify calls in terms of specificity;this classification was only intended to provide a sense of the distribution of callsreceived.3.2 Dialogue Actions for Operator ResponsesIn addition to analyzing how the callers phrased their requests in response to theoperator 's initial prompt,  we also analyzed how the operators responded to the callers'requests.
6 We found that in our corpus, the human operator either notifies the callerof a destination to which the call will be transferred, or queries the caller for furtherinformation, most frequently when the original request was ambiguous and, muchless often, when the original request was not heard or understood.Table 2 shows the frequency with which each dialogue action was employed byhuman operators in our corpus.
It shows that nearly 20% of all caller requests requirefurther disambiguation.
We further analyzed these calls that were not immediatelyrouted and noted that 75% of them involve underspecified noun phrases, such as re-questing car loans without specifying whether it is an existing car loan or a new carloan.
The remaining 25% mostly involve underspecif ied verb phrases, such as askingto transfer funds without specifying the accounts to and from which the transfer willtake place, or missing verb phrases, such as asking for direct deposit without specify-ing whether the caller wants to set up a direct deposit or change an existing directdeposit.Based on our analysis of operator esponses, we decided to first focus our routerresponses on notifying the caller of a selected destination in cases where the callerrequest is unambiguous,  and on formulating a query for noun phrase disambigua-tion in the case of noun phrase underspecification in the caller request.
For calls that5 Although the call center had nearly 100 departments, in our corpus of 4,500 calls, only 23 departmentsreceived more than 10 calls.
We chose to base our experiments on these 23 destinations.6 In most calls, we analyzed the utterances given in the operator's second turn in the dialogue.
However,in situations where the operator generates an acknowledgment, such as uh-huh, midway through thecaller's request, we analyzed utterances in the next operator turn.365Computational Linguistics Volume 25, Number 3CallerResponseDisambiguatingQueryCaller Request-\[ Routing Module I -I ~ didate DestinationsRouting l~_~a~o,;#oOf ~X'-~ 0Notification ~I Disambiguation Module Iential QueryYes / Ouerv ~ No Human~Figure 3Call router architecture.do not satisfy either criterion, the call router should s imply relay them to a humanoperator.
74.
Vector-based Call RoutingIn addition to notifying the caller of a selected destination or querying the caller forfurther information, an automatic all router should be able to identify when it isunable to handle a call and route the call to a human operator for further processing.The process of determining whether to route a call, generate a disambiguation query,or redirect the call to an operator is carried out by two modules in our system, therouting module  and the disambiguation module, as shown in Figure 3.
Given a callerrequest, the routing module selects a set of candidate destinations to which it believesthe call can reasonably be routed.
If there is exactly one such destination, the call isrouted to that destination and the caller notified; if there is no appropriate destination,the call is sent to an operator; and if there are multiple candidate destinations, the dis-ambiguation module is invoked.
In the last case, the disambiguation module attemptsto formulate a query that it believes will solicit relevant information from the caller toallow the revised request o be routed to a unique destination.
If such a query is suc-cessfully formulated, it is posed to the caller, and the system makes another attemptat routing the revised request, which includes the original request and the caller'sresponse to the fol low-up question; otherwise, the call is sent to a human operator.7 Note that the corpus analysis described in this section was conducted with the purpose of determiningguidelines for system design in order to achieve reasonable coverage of phenomena in actualhuman-human dialogues.
The call classification schemes presented in this section do not come intoplay in the actual training or testing of our system, nor do we discard any part of our training corpusas a result of this analysis.366Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingCredit Card ServioO0 ?Credit Card Fraudo Bank Hourso Deposit ServicesC: I want the balance on,,;/,o my car loan.J, ~  Loan Services~ Consumer Lendingo ABA RoutingFigure 4Two-dimensional vector epresentation for the routing module.Our approach to call routing is novel in its application of vector-based informa-tion retrieval techniques to the routing process, and in its extension of the vector-based representation for dynamically generating disambiguation queries (Chu-Carrolland Carpenter 1998).
The routing and disambiguation mechanisms are detailed in thefollowing sections.4.1 The Routing Module4.1.1 Vector Representation for the Routing Module.
In vector-based informationretrieval, the database contains a large collection of documents, each of which is rep-resented as a vector in n-dimensional space.
Given a query, a query vector is computedand compared to the existing document vectors, and those documents whose vectorsare similar to the query vector are returned.
We apply this technique to call routingby treating each destination as a document, and representing the destination as a vec-tor in n-dimensional space.
Given a caller request, an n-dimensional request vectoris computed.
The similarity between the request vector and each destination vectoris then computed and those destinations that are close to the request vector are thenselected as the candidate destinations.
This vector representation for destinations andquery is illustrated in a simplified two-dimensional space in Figure 4.In order to carry out call routing with the aforementioned vector representation,three issues must be addressed.
First, we must determine the vector representationfor each destination within the call center.
Once computed, these destination vec-tors should remain constant as long as the organization of the call center remains367Computational Linguistics Volume 25, Number 3Corpus ofTranscribed& RoutedCallsText for Filtered Text forABA Routing XNNN~ / ABA Routingi Morphological "Document Text for ~.
I Filtering & :,_ \] ~.
Filtered Text forConstruction Oep?s'* Svc / l  g:Ff W:gd Text for / ~k~x~NX Dep?sit SVCFilered T xtTouchline for TouchlineDocumentTerrn ~ Decomposition jLmxn ~ Term'D?cument Ve tors & Singular Value Term-Document Matrix| Matrix Construction VectorsFigure 5Training process for the routing module.TermExtraction/ SA~BAnR?T~tinrr~: f?r~__ Salient Terms forx Salilnt Terms~ Deposit Svcfor Touchlineunchanged.
8 Second, we must determine how a caller request will be mapped to thesame vector space for comparison with the destination vectors.
Finally, we must de-cide how the similarity between the request vector and each destination vector willbe measured in order to select candidate destinations.4.1.2 The Training Process.
The goal of the training phase of the call router is to de-termine the values of the destination vectors (and term vectors) that will subsequentlybe used in the routing process.
Our training process, depicted in Figure 5, requires acorpus of transcribed calls, each of which is routed to the appropriate destination.
9These routed calls are processed by five domain-independent procedures to obtain thedesired document (destination) and term vectors.Document Construction.
Since our goal is to represent each destination as an n-dimen-sional vector, we must create one (virtual) document per destination.
The document fora destination contains the raw text of the callers' contributions in all calls routed to thatdestination, since these are the utterances that provided vital information for routingpurposes.
For instance, the document for deposit services may contain utterances suchas I want to check the balance in my checking account and I would like to stop payment on acheck.
In our experiments, the corpus contains 3,753 calls routed to 23 destinations.
1?8 One may consider allowing the call router to constantly update the destination vectors as new data arebeing collected while the system is deployed.
We leave adding learning capabilities to the call routerfor future work.9 The transcription process can be carried out by humans or by an automatic speech recognizer.
In theexperiments reported in this paper, we used human transcriptions.10 These calls are a subset of the 4,500 calls used in our corpus analysis.
We included calls of all semantictypes, but excluded calls to destinations that were not represented bymore than 10 calls, as well asambiguous calls that were not resolved by the operator.368Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingMorphological Filtering and Stop Word Filtering.
For routing purposes, we are concernedwith the semantics of the words present in a document, but not with the morphologicalforms of the words themselves.
Thus we filter each (virtual) document, produced bythe document construction process, through the morphological  processor of the BellLabs Text-to-Speech synthesizer (Sproat 1998) to extract he root form of each word inthe corpus.
This process will reduce singulars, plurals, and gerunds to their root forms,such as reducing service, services, and servicing to the root service.
Also, the various verbforms are also reduced to their root forms, such as reducing oing, went, and gone to go.
11Next, the root forms of caller utterances are filtered through two lists, the ignorelist and the stop list, in order to build more accurate n-gram term models for sub-sequent processing.
The ignore list consists of noise words, which are common inspontaneous speech and can be removed without altering the meaning of an utter-ance, such as um and uh.
These words sometimes get in the way of proper n-gramextraction, as in I'd like to speak to someone about a car uh loan.
When the noise worduh is filtered out of the utterance, we can then properly extract he bigram car+loan.The stop list enumerates words that are ubiquitous and therefore do not contributeto discriminating between destinations, such as the, be, for, and morning.
We modif iedthe standard stop list distributed with the SMART information retrieval system (Salton1971) to include domain-specific terms and proper names that occurred in our trainingcorpus.
12 Note that when a word on the ignore list is removed from an utterance, itallows words preceding and succeeding the removed word to form n-grams, such ascar+loan in the example above.
On the other hand, when a stop word is removed froman utterance, a placeholder is inserted into the utterance to prevent he words preced-ing and following the removed stop word from forming n-grams.
For instance, afterstop word filtering, the caller utterance I want to check on an account becomes (sw) (sw)(sw) check (sw) (sw) account, resulting in the two unigrams check and account.
Withoutthe placeholders, we would extract he bigram check+account, just as if the caller hadused the term checking account in the utterance.In our experiments, the ignore list contains 25 words, which are variations ofcommon transcriptions of speech disfluencies, such as ah, aah, and ahh.
The stop listcontains over 1,200 words, including function words, proper names, greetings, etc.Term Extraction.
The output of the filtering processes is a set of documents, one foreach destination, containing the root forms of the content words extracted from theraw texts originally in each document.
In order to capture word co-occurrence, n-gramterms are extracted from the filtered texts.
First, a list of n-gram terms and their countsare generated from all filtered texts.
Thresholds are then applied to the n-gram countsto select as salient erms those n-gram terms that occurred sufficiently frequently.
Next,these salient terms are used to reduce the filtered text for each document o a bag ofsalient terms, i.e., a collection of n-gram terms along with their respective counts.Note that when an n-gram term is extracted, all of the lower order k-grams, where1<k<n,  are also extracted.
For instance, the word sequence checking account balance willresult in the tr igram check+account+balance, as well as the bigrams check+account andaccount+balance and the unigrams check, account, and balance.11 Not surprisingly, confusion among morphological variants was a source of substantial error from therecognizer.
Details can be found in Reichl et al (1998).12 The idea of a standard stop list in the information retrieval literature is to eliminate terms that do notcontribute to discriminating among documents.
We extend this notion to our application to includeadditional proper names uch as Alaska nd Houston, as well as domain- or application-specific termssuch as bye and cadet.
The modification to the standard stop list is performed by manually examiningthe unigram terms extracted from the training corpus.369Computational Linguistics Volume 25, Number 3In our experiments, we selected as salient terms unigrams that occurred at leasttwice and bigrams and trigrams that occurred at least three times.
This resulted in62 trigrams, 275 bigrams, and 420 unigrams.
In our training corpus, no four-gramoccurred three times.
Manual examination of these n-gram terms indicates that almostall of the selected salient terms are relevant for routing purposes.
13Term-Document Matrix Construction.
Once the bag of salient terms for each destinationis constructed, it is very straightforward toconstruct an m x n term-document frequencymatrix A, where m is the number of salient terms, n is the number of destinations,and an element at,d represents the number of times the term t occurred in calls todestination d. This number indicates the degree of association between term t anddestination d, and our underlying assumption is that if a term occurred frequently incalls to a destination in our training corpus, then occurrence of that term in a caller'srequest indicates that the call should be routed to that destination.In the term-document frequency matrix A, a row At is an n-dimensional vectorrepresenting the term t, while a column Ad is an m-dimensional vector epresenting thedestination d. However, by using the raw frequency counts as the elements of the ma-trix, more weight is given to terms that occurred more often in the training corpus thanto those that occurred less frequently.
For instance, a unigram term such as account,which occurs frequently in calls to multiple destinations will have greater frequencycounts than say, the trigram term social+security+number.
As a result, when the twovectors representing account and social+security+number are combined, as will be donein the routing process, the term vector for account contributes more to the combinedvector than that for social+security+number.
In order to balance the contribution of eachterm, the term-document frequency matrix is normalized so that each term vector isof unit length (later weightings do not preserve this normalization, though).
Let B bethe result of normalizing the term-document frequency matrix, whose elements aregiven as follows:at,dBt'd = iX-" A 2 "~1/2\z--.,l<eKn t,eJOur second weighting is based on the notion that a term that only occurs in a fewdocuments i more important in routing than a term that occurs in many documents.For instance, the term stop+payment, which occurred only in calls to deposit services,should be more important in discriminating among destinations than check, whichoccurred in many destinations.
Thus, we adopted the Inverse-document frequency(IDF) weighting scheme (Sparck Jones 1972) whereby a term is weighted inversely tothe number of documents in which it occurs.
This score is given by:nIDF(t) = log  2 d(t)where t is a term, n is the number of documents in the corpus, and d(t) is the number ofdocuments containing the term t. If t only occurred in one document, IDF(t) = log 2 n; ift occurred in every document, IDF(t) = log 2 1 -- 0.
Thus, using this weighting scheme,terms that occur in every document will be eliminated.
14We weight the matrix B by13 It would have been possible to hand-edit he set of n-gram terms at this point to remove unwantedterms.
The results we report in this paper use the automatically selected terms without anyhand-editing.14 To preserve all terms, we could have used a common variant of the IDF weighting where IDF(t) =nq-e log2 ~(t) for some nonnegative ?.370Chu-Carroll and Carpenter Vector-based Natural Language Call Routingmult iplying each row t by IDF(t) to arrive at the matrix C:Ct, d = IDF(t) ?
Bt,dSingular Value Decomposition a d Vector Representation.
I  the weighted term-documentfrequency matrix C, terms are represented as n-dimensional vectors (in our system,n = 23), and destinations are represented as m-dimensional vectors (in our system, m= 757).
In order to provide a uni form representation of term and document vectorsand to reduce the dimensionality of the document vectors, we applied the singularvalue decomposit ion to the m x n matrix C (Deerwester et al 1990) to obtain: 15C= U.S .V  T,where.2.3.U is an m x m orthonormal matrix;V is an n x n orthonormal matrix; andS is an m x n positive matrix whose nonzero values are Sl,1,..., Sr,r,where r is the rank of C, and they are arranged in descending orderS1,1 ~ S2,2 ~ ' ' '  ~ Sr, r ~ O.Figure 6 illustrates the results of singular value decomposit ion according to theabove equation.
The shaded portions of the matrices are what we use as the basis forour term and document vector representations, as follows:...Ur is an m x r matrix, in which each row forms the basis of our termvector representation;Vr is an n x r matrix, in which each row forms the basis of our documentvector representation; andSr is an r x r positive diagonal matrix whose values are used forappropriate scaling in the term and document vector representations.The actual representations of the term and document vectors are Ur and VF scaled(or not) by elements in St, depending on whether the representation is intended forcomparisons between terms, between documents, or between a term and a docu-ment.
For instance, since the similarity between two documents can be measured bythe dot product between vectors representing the two documents (Salton 1971), andC T ?
C contains the dot products of all pairwise column vectors in the weighted term-document frequency matrix C, the similarity between the ith and jth documents can15 Our original intent was to apply singular value decomposition a d to reduce the dimensionality of theresulting vectors below the rank of the original matrix in order to carry out latent semantic indexing(Deerwester tal.
1990).
Benefits cited for latent semantic indexing include the clustering of"synonyms" leading to improved recall.
In the end, dimensionality reduction degraded performancefor our data.
However, our method is not equivalent to the standard approach in vector-basedinformation retrieval, which simply uses the rows or columns of the term-document matrix (see Salton\[1971\] for definitions of the standard case).
The difference arises through the cosine measure ofsimilarity, which is operating over different spaces in the standard vector case and with the result ofSVD in our case.
Although we did not run the experiments, we believe similar results would beobtained by using cosine to compare rows or columns of the term-document matrix directly.371Computational Linguistics Volume 25, Number 3X XCm?n Um?m Sm?n T VnxnFigure 6Singular value decomposition.simply be recovered by element (C  T ?
C)/j.
Since U is orthonormal, S is a diagonalmatrix, we have:C T ?
C = (U-S"  vT) T" (U 'S"  V T)= V.S  T .U  T -U .S .V  T= V.S .S .V  T: (V 'S ) "  (V 'S )  wBecause only the first r diagonal elements of S are nonzero, we have:(W. S).
(V. S) T = (W r .
Sr) .
(V  r ?
Sr) TThe above equations suggest that scaling the vectors Vr with elements in Sr, i.e., repre-senting documents as row vectors in Vr'Sr, facilitates comparisons between documents.The same reasoning holds for representing terms as row vectors in Ur" Sr for compar-isons between terms, although in this particular application, we are not interested interm-term comparisons.To measure the degree of association between a term and a document, we look upan element in the weighted term-document frequency matrix.
Because S is a diagonalmatrix with only the first r elements nonzero, we have:C ~- U .S .
V T= U- (V -S)  w= Ur"  (Vr 'S r )  wTherefore, representing terms simply by row vectors in U r and documents by rowvectors in Vr'Sr allows us to make comparisons between documents, as well as betweenterms and documents.4.1.3 Call Routing.
As discussed earlier, two subprocesses need to be carried outduring the call routing process.
First, a pseudodocument vector must be constructed torepresent the caller's request in order to facilitate the comparisons between the requestand each document vector.
Second, a method for comparison must be established tomeasure the similarity between the pseudodocument vector and the document vectorsin  Vr ?
Sr, and a threshold must be determined to allow for the selection of candidatedestinations.372Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingPseudodocument Generation.
Given a caller utterance (either in text form from a key-board interface or as the output from an automatic speech recognizer), we first performthe morphological nd stop word filtering and the term extraction procedures as inthe training process to extract he relevant n-gram terms from the utterance.
Sincehigher-level n-gram terms are, in general, better indicators of potential destinations,we further allow trigrams to contribute more to constructing the pseudodocumentthan bigrams, which in turn contribute more than unigrams.
Thus we assign a weightw3 to trigrams, w2 to bigrams, and wl to unigrams, 16and each extracted n-gram term isthen weighted appropriately to create a bag of terms in which each extracted n-gramterm occurs Wn times.
As a result, when we construct a pseudodocument from the bagof terms, we get the effect of weighting each n-gram term by Wn.Given the extracted n-gram terms, we can present he request as an m x 1 vectorQ where each element Qi in the vector represents the number of times the ith termoccurred in the bag of terms.
The vector Q is then added as an additional column vectorin our original weighted term-document frequency vector C, as shown in Figure 7, andwe want to find the new corresponding column vector in V, Vq, that represents hepseudodocument i  the reduced r-dimensional space.
Since U is orthonormal nd Sis a diagonal matrix, we can solve for Vq by settingQ-_  U .
S .
Vq TBecause we want a representation f the query in the document space, we transposeQ, to yield:QT = Vq .
S .
U TFinally, multiplying both sides on the right by U, we have:QT .
u = Vq .
S .
UT .
U= vq .sFinally, note that for our query representation in the document space, we have QT.
U ----QT .
Ur and Vq .
S = Vq ?
Sr. Vq ?
Sr is a pseudodocument representation for the callerutterance in r-dimensional space, and is scaled appropriately for comparison betweendocuments.
This vector epresentation is simply obtained by multiplying QT and Ur,or equivalently, summing the vector representing each term in the bag of n-gramterms.Candidate Destination Selection.
Once the pseudodocument vector epresenting the callerrequest is computed, we measure the similarity between each document vector in VF' Srand the pseudodocument vector.
There are a number of ways one may measure thesimilarity between two vectors, such as using the cosine score between the vectors, theEuclidean distance between the vectors, the Manhattan distance between the vectors,etc.
We follow the standard technique adopted in the information retrieval communityand select he cosine score as the basis for our similarity measure.
The cosine scorebetween two n-dimensional vectors x and y is given as follows:cos(x, y) = x. yTV/E l ( iKn  x2 " ~--~X(i(n y216 In  our  sys tem,  w l  = 1; w 2 = 2; and  w3 = 4.373Computational Linguistics Volume 25, Number 3?
XC m?
(n+l) U m?rFigure 7Pseudodocument generation..TS r:~r V(n+l)?rUsing cosine reduces the contribution ofeach vector to its angle by normalizing forlength.
Thus the key in maximizing cosine between two vectors is to have them pointin the same direction.
However, although the raw vector cosine scores give some indi-cation of the closeness of a request o a destination, we noted that the absolute valueof such closeness does not translate directly into the likelihood for correct routing.Instead, some destinations may require a higher cosine value, i.e., a closer degree ofsimilarity, than others in order for a request to be correctly associated with those des-tinations.
We applied the technique of logistic regression (see Lewis and Gale \[1994\])in order to transform the cosine score for each destination using a sigmoid functionspecifically fitted for that destination.
This allows us to obtain a score that representsthe router's confidence that the call should be routed to that destination.From each call in the training data, we generate, for each destination, a cosinevalue/routing value pair, where the cosine value is that between the destination vec-tor and the request vector, and the routing value is 1 if the call was routed to thatdestination i the training data and 0 otherwise.
Thus, for each destination, we have aset of cosine value/routing value pairs equal to the number of calls in the training data.The subset of these value pairs whose routing value is I will be equal to the number ofcalls routed to that destination i the training set.
Then, we used least squared error tofit a sigmoid function, 1/(1 + e-(ax+b)), to the set of cosine value/routing value pairs.
17Assuming  da and db are the coefficients of the fitted sigmoid function for destinationd, we have the following confidence function for a destination d and cosine value x:Conf(da, db, x) = 1/(1 + e -(dax+db))Thus the score given a request and a destination, where d is the vector correspondingto destination d,and r is the vector corresponding to the request is Conf(da, db, cos(r, d)).To obtain a preliminary evaluation of the effectiveness of cosine vs. confidencescores, we tested routing performance on transcriptions of 307 unseen unambiguousrequests.
In each case, we selected the destination with the highest cosine/confidencescore to be the target destination.
Using raw cosine scores, 92.2% of the calls are routedto the correct destination.
On the other hand, using sigmoid confidence fitting, 93.5%of the calls are correctly routed.
This yields an error reduction rate of 16.7%, illustratingthe advantage oftransforming the raw cosine scores to more uniform confidence scoresthat allow for more accurate comparisons between destinations.17 Maximum likelihood fitting is often used rather than least squared error in logistic regression.374Chu-Carroll and Carpenter Vector-based Natural Language Call Routing.36I0.80.60.40.2Upperbound :0 ' ' ' ' ' ' L?wc'rb?undb ' ~0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thresho ldFigure 8Router performance vs. confidence threshold.We can compute the kappa statistic for the pure routing component of our sys-tem using the accuracies given above.
18 Recall that kappa is defined by (a -e ) / (1  -e )where a is the system's accuracy and e is the expected agreement by chance.
Select-ing destinations from the prior distribution and guessing destinations using the samedistribution leads to a chance performance ofe = Y~a P(d) 2 = 0.2858 where the summa-tion is over all destinations d and P(d) is the percentage of calls routed to destinationd.
The resulting kappa score is (0.935 - 0.2858)/(1 - 0.2858) = 0.909.Once we have obtained a confidence value for each destination, the final step inthe routing process is to compare the confidence values to a predetermined thresholdand return those destinations whose confidence values are greater than the thresholdas candidate destinations.
To determine the optimal value for this threshold, we rana series of experiments o compute the upper bound and lower bound of the router'sperformance by varying the threshold from 0 to 0.9 at 0.1 intervals.
The lower boundrepresents the percentage of calls that are routed correctly, while the upper boundindicates that percentage of calls that have the potential to be routed correctly afterdisambiguation (see Section 5 for details on upper bound and lower bound measures).Figure 8 illustrates the results of this set of experiments and shows that a thresholdof 0.2 yields optimal performance.
Thus we adopt 0.2 as our confidence threshold forselecting candidate destinations in the rest of our discussion.4.1.4 Call Routing Example.
To illustrate the call routing process with an example,suppose the caller responds to the operator's prompt with I am calling to apply for a newcar loan.
First the caller's utterance is passed through morphological filtering to obtainthe root forms of the words in the utterance, resulting in I am call to apply for a newcar loan.
Next, words on the stop list are removed and replaced with a placeholder,resulting in (sw I (sw I call (sw I apply (sw I (sw I new car loan.
From the filtered "utter-ance, the router extracts the salient n-gram terms to form a bag of terms as follows:new+car+loan, ew+car, car+loan, call, apply, new, car, and loan.
A request vector is thencomputed by taking the weighted sum of the term vectors representing the salientn-gram terms, and the cosine value between this request vector and each destinationvector is computed.
The cosine value for each destination is subsequently transformed18 See Siegel and Castellan (1988), and Carletta (1996) for a definition or discussion of the kappa statistic,and Walker et al (1998) for an application of the kappa statistic to dialogue valuation.375Computational Linguistics Volume 25, Number 31.
Consumer Lending 0.9792.
Loan Services 0.2603.
Home Loans 0.0774.
Collateral Control 0.0695.
Operator 0.038(a) Cosine ScoresFigure 9Ranking of candidate destinations.1.
Consumer Lending 0.9132.
Deposit Services 0.0703.
PC Banking 0.0494.
Loan Services 0.0355.
Auto Leasing 0.032(b) Confidence ScoresL ?
a n ~ c :  Car loans please.~ Consumer LendingFigure 10Two-dimensional vector epresentation for the disambiguation module.using the destination-specific sigmoid function to obtain a confidence score for eachdestination.
Figures 9(a) and 9(b) show the cosine scores and the confidence scoresfor the top five destinations, respectively.
Given a confidence threshold of 0.2, theonly candidate destination selected is Consumer Lending.
Thus, the caller's request isrouted unambiguously to that destination.4.2 The Disambiguation Module4.2.1 Vector Representation for the Disambiguation Module.
When the routing mod-ule returns more than one candidate destination, the disambiguation module is in-voked.
The disambiguation module attempts to formulate an appropriate query tosolicit further information from the caller to determine a unique destination to whichthe call should be routed.
As discussed earlier, this occurs when two or more destina-tion vectors are close to the request vector, as illustrated in reduced two-dimensionalspace in Figure 10.
In the example, the caller's request car loans please is ambiguoussince the caller does not specify whether he is interested in existing or new car loans.Therefore, the vector epresentation forthe request falls between the vectors represent-ing the two candidate destinations, Consumer Lending and Loan Services, and is closeto both of them.
The goal of the disambiguation process is to solicit an n-gram termfrom the caller so that when the vector epresenting this new n-gram term is added tothe original request vector, the refined request vector will be unambiguously routedto one of the two candidate destinations.
In terms of our vector representation, thismeans that our goal is to find term vectors that are close to the differences betweenthe candidate destination vectors and the request vector, i.e., the highlighted vectorsin Figure 10.
These difference vectors, which are simply the pairwise differences of el-ements in each vector and are dynamically generated from the destination and requestvectors, form the basis from which the disambiguation queries will be generated.376Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingAll n-gram terms& vectorsDifference vectorsTerm SelectionIi Select Terms withSelect Close Terms :' Select Relevant Terms Disambiguating PowerI .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.iWH-questionFigure 11The disambiguation process.?
: YesYN-question coperator01SelectedTerms4.2.2 Query Formulation.
Our disambiguation module selects a subset of the salientn-gram terms from which the query will be generated.
The subset of n-gram termsare those related to the original query that can likely be used to disambiguate amongthe candidate destinations.
They are chosen by filtering all n-gram terms based on thefollowing three criteria, as shown in Figure 11:Closeness Since the goal of the disambiguation process is to solicit termswhose corresponding vectors are close to the difference vectors, the firststep in the term selection process is to compare ach n-gram term vectorwith the difference vectors and select those n-gram term vectors that areclose to the difference vectors by the cosine measure.
Since both thedestination vectors and the request vector are scaled fordocument-document comparison in Vr ?
Sr space, the difference vectorsare also represented in Vr ?
SF space.
As discussed in Section 4.1.2,documents represented in Vr ?
Sr space are suitable for comparison withterms represented in Ur space.
In our system, for each difference vector,we compute the cosine score between the difference vector and eachterm vector, and select the 30 terms with the highest cosine scores as theset of close terms.
The reasons for selecting a threshold on the number  ofterms instead of on the cosine score are twofold.
First, in situationswhere many term vectors are close to the difference vector, we avoidgenerating an overly large set of close terms but instead focus on asmaller set of most promising terms.
Second, in situations where fewterm vectors are close to the difference vector, we still select a set of closeterms in the hope that they may contribute to formulating a reasonablequery, instead of giving up on the disambiguation process outright.Relevance From the set of close terms, we select a set of relevant erms,which are terms that further specify a term in the original request.
If aterm in the set of close terms can be combined with a term in theoriginal request o form a valid n-gram term, then the resulting n-gramterm is considered a relevant erm.
For instance, if car+loan is a term inthe original request, then both new and new+car would produce the377Computational Linguistics Volume 25, Number 3relevant erm new+car+loan.
This mechanism for selecting relevant ermsallows us to focus on selecting n-gram terms for noun phrasedisambiguation by eliminating close terms that are semantically relatedto underspecif ied n-gram noun phrases in the original request but do notcontribute to further disambiguating the noun phrases.D isambiguat ing power  The final criterion that we use for term selectionis to restrict attention to relevant erms that can be added to the originalrequest o result in an unambiguous routing decision using the routingmechanism described in Section 4.1.3.
In other words, we augment hebag of n-gram terms extracted from the original request with eachrelevant erm, and the routing module is invoked to determine if thisaugmented set of n-gram terms can be unambiguously  routed to aunique destination.
The set of relevant erms with disambiguating powerthen forms the set of selected terms from which the system's query willbe formulated.
If none of the relevant erms satisfy this criterion, then weinclude all relevant erms.
Thus, instead of giving up the disambiguationprocess when no one term is predicted to resolve the ambiguity, thesystem poses a question to solicit information from the caller to movethe original request one step toward being an unambiguous request.After the first disambiguation query is answered, the systemsubsequently selects a new set of terms from the refined, though stillambiguous, request and formulates a fol low-up disambiguation query.
19The result of this selection process is a finite set of terms that are relevant o theoriginal ambiguous request and, when added to it, are likely to resolve the ambiguity.The actual query is formulated based on the number  of terms in this set as well asfeatures of the selected terms.
As shown in Figure 11, if the three selection criteria ruledout all n-gram terms, then the call is sent to a human operator for further processing.If there is only one selected term, then a yes-no question is formulated based on thisterm.
If there is more than one selected term in the set, and a significant numberof these terms share a common headword, 2?
X, the system generalizes the query toask the wh-question For what type of X?
Otherwise, a yes-no question is formed basedon the term in the selected set that occurred most frequently in the training data,based on the heuristic that a more common term is more likely to be relevant hanan obscure term.
21 A third alternative would be to ask a disjunctive question, but wehave not yet explored this possibility.
Figure 3 shows that after the system poses itsquery, it attempts to route the refined request, which is the caller's original requestplus the response to the disambiguation query posed by the system.
In the case of wh-questions, n-gram terms are extracted from the response.
For instance, if the systemasks For what type of loan?
and the user responds It's a car loan, then the b igram car+loan19 Although it captures asimilar property of each term, this criterion is computationally much moreexpensive than the closeness criterion.
Thus, we adopt he closeness criterion to select a fixed numberof candidate terms and then apply the more expensive, but more accurate, criterion to the muchsmaller set of candidate terms.20 In our implemented system, this path is selected if 1) there are five or less selected terms and they allshare a common headword, or 2) there are more than five terms and at least five of them share acommon headword.21 The generation of natural anguage queries is based on templates for wh-questions and yes-noquestions.
The generation process consults amanually constructed mapping between -grammorphologically reduced noun phrases and their expanded forms.
For instance, itmaps the n-gramterm exist + car + loan to an existing car loan.
This mapping is the only manual effort needed to port thedisambiguation module to a new domain.378Chu-Carroll and Carpenter Vector-based Natural Language Call Routingis extracted from the response.
In the case of yes-no questions, the system determineswhether a yes or no answer is given.
22 In the case of a yes response, the term selectedto formulate the disambiguation query is considered the caller's response, while inthe case of a no response, the response is treated as in responses to wh-questions.
Forinstance, if the user says yes in response to the system's query Is this an existing carloan?, then the tr igram term exist+car+loan i the system's query is considered theuser 's  response.Note that our disambiguation mechanism, like our training process for basic rout-ing, is domain- independent (except for the manual  construction of a mapping betweenn-gram noun phrases and their expanded forms).
It utilizes the set of n-gram terms, aswell as term and document vectors that were obtained by the training of the call router.Thus, the call router can be ported to a new task with only very minor domain-specificwork on the disambiguation module.4.2.3 Disambiguation Example.
To illustrate the disambiguation module of our callrouter, consider the request Loans please.
This request is ambiguous because the callcenter we studied handles mortgage loans separately from all other types of loans,and for all other loans, existing loans and new loans are also handled by differentdepartments.Given this request, the call router first performs morphological, ignore word, andstop word filterings on the input, resulting in the filtered utterance of loan Iswl.
N-gram terms are then extracted from the filtered utterance, resulting in the unigramterm loan.
Next, the router computes a pseudodocument  vector that represents thecaller's request, which is compared in turn with the destination vectors.
The cosinevalues between the request vector and each destination vector are then mapped intoconfidence values.
Using a confidence threshold of 0.2, we have two candidate des-tinations, Loan Services and Consumer Lending; thus the disambiguation module isinvoked.Our disambiguation module first selects from all n-gram terms those whose termvectors are close to the difference vectors, i.e., the differences between each candidatedestination vector and the request vector.
This results in a list of 60 close terms, thevast majority of which are semantically close to loan, such as auto+loan, payoff, and owe.Next, the relevant erms are constructed from the set of close terms by selecting thoseclose terms that form a valid n-gram term with loan.
This results in a list of 27 relevantterms, including auto+loan and loan+payoff, but excluding owe, since neither loan+owenor owe+loan constitutes a valid bigram.
The third step is to select hose relevant ermswith disambiguating power, resulting in 18 disambiguating terms.
Since 11 of theseterms share a head noun loan, a wh-question is generated based on this headword,resulting in the query For what type of loan?Suppose in response to the system's query, the user answers Car loan.
The routerthen adds the new bigram car+loan and the two unigrams car and loan to the orig-inal request and attempts to route the refined request.
This refined request is againambiguous between Loan Services and Consumer Lending because the caller did notspecify whether it was an existing or new car loan.
Again, the disambiguation moduleselects the close, relevant, and disambiguating terms, resulting in a unique tr igramexist+car+loan.
Thus, the system generates the yes-no question Is this about an existing22 In our current system, aresponse is considered a yes response only if it explicitly contains the wordyes.
However, as discussed in Green and Carberry, (1994) and Hockey et al (1997), responses to yes-noquestions may not explicitly contain a yes or no term.
We leave incorporating a more sophisticatedresponse understanding model, such as Green and Carberry (1994), into our system for future work.379Computational Linguistics Volume 25, Number 3car loan?
23 If the user responds yes, then the tr igram exist+car+loan is added to therefined request and the call unambiguously  routed to Loan Services; if the user saysNo, it's a new car loan, then the tr igram new+car+loan is extracted from the response andthe call routed to Consumer Lending.
245.
Evaluation of the Call Router5.1 Routing Module PerformanceWe performed an evaluation of the routing module of our call router on a set of 389calls disjoint from the training corpus.
Of the 389 requests, 307 were unambiguous androuted to their correct destinations, and 82 were ambiguous and annotated with a listof potential destinations.
Unfortunately, in this test set, only the caller's utterance inresponse to the system's initial p rompt  of How may I direct your call?
was recorded andtranscribed; thus we have no information about where the ambiguous calls should berouted after disambiguation.
We evaluated the routing module performance on bothtranscriptions of caller utterances as well as output of the Bell Labs Automatic SpeechRecognizer (Reichl et al 1998) based on speech input of caller utterances (Carpenterand Chu-Carroll 1998).5.1.1 Term Extraction Performance.
Since the vector representation for caller requestsis computed based on the term vectors representing the n-gram terms extracted fromthe requests, the performance of our call router is directly tied to the the accuracy ofterms extracted from each caller utterance.
Given the set of n-gram terms obtained fromthe training process, the accuracy of extraction of such terms based on transcriptionsof caller utterances i 100%.
However,  when using the output of an automatic speechrecognizer as input to our call router, deletions of terms present in the caller's requestas well as insertions of terms that did not occur in the request affect the term extractionaccuracy and thus the routing performance.We evaluated the output of the automatic speech recognizer based on both wordaccuracy and term accuracy, as shown in Table 3.
25 Word accuracy is measured bytaking into account all words in the transcript and in the recognized string.
Two setsof results are given for word accuracy, one based on raw forms of words and the otherbased on comparisons of the root forms of words, i.e., after both the transcript and therecognized string are sent through the morphological  filter.
Term accuracy is measuredby taking into account only the set of actual /recognized words that contribute torouting performance, i.e., after both the transcript and the recognized string are sentthrough the term extraction process.For each evaluation dimension, we measured the recognizer performance by calcu-lating the precision and recall.
Precision is the percentage of words / te rms in the recog-nizer output that are actually in the transcription, i.e., percentage of found words / te rms23 Recall that our current system uses simple template filling for response generation by utilizingmanually constructed mappings from n-gram terms to their inflected forms, such as from exist+car+loanto an existing car loan.24 The current implementation f the system requires that the user specify the correct answer whenproviding a no answer to a yes-no question, in order for the call to be properly disambiguated.
However,it is possible that a system may attempt to disambiguate given a simple no answer by considering then-gram term being queried (exist+car+loan in the above example) as a negative feature, subtracting itsvector epresentation from the query, and attempting to route the resulting vector epresentation.25 In computing the precision and recall figures, we did not take into account multiple occurrences of thesame word.
In other words, we consider aword in the recognized string correct if the word occurs inthe transcribed text.
For comparison purposes, the standard speech recognition accuracy on raw ASRoutput is 69.94%.380Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingTable 3Word accuracy vs. term accuracy on ASR output.Word Accuracy Term AccuracyRaw Rooted Unigram Bigram TrigramPrecision 78.6% 79.8% 93 .7% 96.5% 98.5%Recall 76.0% 77.2% 88.4% 85.5% 83.6%that are correct, while recall is the percentage ofwords/terms in the transcription thatare correctly returned by the recognizer, i.e., percentage of actual words/terms thatare found.
Table 3 shows that using the root forms of words results in a 1% absoluteimprovement (approximately 5% error reduction) in both precision and recall overusing the raw forms of words.A comparison of the rooted word accuracy and the unigram accuracy shows thatthe recognizer performs much better on content words than on all words combined.Furthermore, comparisons among term accuracies for various n-gram terms show thatas n increases, precision increases while recall decreases.
This is because finding acorrect rigram requires that all three unigrams that make up the trigram be correctlyrecognized in order, hence the low recall.
On the other hand, this same feature makes itless likely for the recognizer to postulate a trigram by chance, hence the high precision.An overall observation i the results presented in Table 3 is that the speech recognizermisses between 12-17% of the n-gram terms used by the call router, and introducesan extra 1-6% of n-gram terms that should not have existed.
In the next section,we show how these deletions and insertions of n-gram terms affect he call router'sperformance.5.1.2 Destination Selection Performance.
In evaluating the performance ofthe routingmodule, we compare the list of candidate destinations with the manually annotatedcorrect destination(s) for each call.
The routing decision for each call is classified intoone of eight classes, as shown in Figure 12.
For instance, class 2a contains those callsthat 1) are actually unambiguous, 2) are considered ambiguous by the router, and3) have the potential to be routed to the correct destination, i.e., the correct destinationis one of the candidate destinations.
On the other hand, class 3b contains those callsthat 1) are actually ambiguous, 2) are considered unambiguous by the router, and 3)are routed to a destination that is not one of the potential destinations.We evaluated the router's performance on three subsets of our test data: unam-biguous requests alone, ambiguous requests alone, and all requests combined.
Foreach set of data, we calculated a lower bound performance, which measures the per-centage of calls that are correctly routed, and an upper bound performance, whichmeasures the percentage of calls that are either correctly routed or have the potentialto be correctly routed.
Table 4 shows how the upper bounds and lower bounds arecomputed based on the classification i Figure 12 for each of the three data sets.
Forinstance, for unambiguous requests (classes 1 and 2), the lower bound is the numberof calls actually routed to the correct destination (class la) divided by the number oftotal unambiguous requests, while the upper bound is the number of calls actuallyrouted to the correct destination (class la) plus the number of calls that the routerfinds to be ambiguous between the correct destination and some other destination(s)(class 2a), divided by the number of unambiguous requests.
The calls in 2a are con-381Computational Linguistics Volume 25, Number 3Is request actually unambiguous?
yes//Is call routed by router?ye /  NN~oLIs call routed by router?yes//// NXx~ocorrect?
contains correct?
one of possible?
overlaps with possible?yes/// XN~o yes/// NN~o yes/// NN~o yes/// NN~ola lb 2a 2b 3a 3b 4a 4bFigure 12Classification of routing module outcome.Table 4Calculation of upper bounds and lower bounds.Unambiguous Requests Ambiguous Requests All RequestsLower bound la/(1+2) 4a/(3+4) (la+4a)/allUpper bound (la+2a)/(1+2) (3a+4a)/(3+4) (la+2a+3a+4a)/allTable 5Routing results with threshold = 0.2.Class a Class b Class a Class bClass 1 246 5 Class 1 239 9Class 2 51 5 Class 2 49 10Class 3 33 1 Class 3 30 3Class 4 48 0 Class 4 42 7(a) Results on transcriptions (b) Results on ASR outputsidered potentially correct because it is likely that the call will be routed to the correctdestination after disambiguation.Tables 5(a) and 5(b) show the number of calls in our testing corpus that fell into theclasses illustrated in Figure 12 based on transcriptions ofcaller requests and the outputof an automatic speech recognizer, espectively.
Tables 6(a) and 6(b) show the upperbound and lower bound performance for the three test sets based on the results inTables 5(a) and (b), as well as the evaluation mechanism in Table 4.
These results howthat the system's overall performance in the case of perfect recognition falls somewherebetween 75.6% and 97.2%, while the performance using our current automatic speechrecognizer (ASR) output falls between 72.2% and 92.5%.
The actual performance ofthe system is determined by two factors: 1) the performance of the disambiguationmodule, which determines the correct routing rate of the unambiguous calls that areconsidered ambiguous by the router (class 2a, 16.6% of all unambiguous calls withtranscription and 15.9% with ASR output), and 2) the percentage of calls that wererouted correctly out of the ambiguous calls that were considered unambiguous bythe router (class 3a, 40.4% of all ambiguous calls with transcription and 36.6% withASR output).
Note that the performance figures given in Tables 6(a) and 6(b) are382Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingTable 6Router performance with threshold = 0.2.Unambiguous Requests Ambiguous Requests All RequestsLower bound 80.1% 58.5% 75.6%Upper bound 96.7% 98.8% 97.2%(a) Performance on transcriptionsUnambiguous Requests Ambiguous Requests All RequestsLower bound 77.9% 51.2% 72.2%Upper bound 93.8% 87.8% 92.5%(b) Performance on ASR outputbased on 100% automatic routing.
In the next section, we discuss the performance ofthe disambiguation module, which determines the overall system performance, andshow how allowing calls to be redirected to human operators affects the system'sperformance.5.2 Disambiguation Module PerformanceTo evaluate our disambiguation module, we needed dialogues that satisfy two criteria.First, the caller's first utterance must be ambiguous.
Second, the operator must haveasked a follow-up question to disambiguate he request and have subsequently routedthe call to the appropriate destination.
We used 157 calls that met these two criteriaas our test set for the disambiguation module.
Note that this test set is disjoint fromthe test set used in the evaluation of the call router, since none of the calls in that setsatisfied the second criterion (those calls were not recorded or transcribed beyond thecaller's response to the operator's prompt).
Furthermore, for this test set, we only hadaccess to the transcriptions of the calls but not the original speech files.For each ambiguous call, the first caller utterance was given to the router as input.The outcome of the router was classified as follows:Unambiguous if the call was routed to the selected estination.
Thisrouting was considered correct if the selected estination was the sameas the actual destination and incorrect otherwise.Ambiguous if the router attempted to initiate disambiguation.
Theoutcome of the routing of these calls was determined as follows:Correct if a disambiguation query was generated which, whenanswered, led to the correct destination.
26Incorrect if a disambiguation query was generated which, whenanswered, could not lead to a correct destination.26 Since our corpus consists of human-human dialogues, we do not have human responses tothe exactdisambiguation questions that our system generates.
We consider adisambiguation query correct if itattempts o solicit he same type of information as the human operator, egardless ofsyntacticphrasing, and if answered based on the user's response to the human operator's question, led to thecorrect destination.383Computational Linguistics Volume 25, Number 3Table 7Performance of disambiguation module on ambiguouscalls.Routed As Unambiguous Routed As AmbiguousCorrect Incorrect Correct Incorrect Reject40 12 60 3 42Reject if the router could not form a sensible query or wasunable to gather sufficient information from the user after itsqueries and routed the call to a human operator.Table 7 shows the number  of calls that fall into each of the five categories.
Outof the 157 calls, the router automatical ly routed 115 either with or without disam-biguation (73.2%).
Furthermore, 87.0% of these automatically routed calls were sentto the correct destination.
Notice that out of the 52 ambiguous calls that the routerconsidered unambiguous,  40 were routed correctly (76.9%).
This is because our sta-tistically trained call router is able to distinguish between cases where a semanticallyambiguous request is equally likely to be routed to two or more destinations, andsituations where the likelihood of one potential destination overwhelms that of theother(s).
In the latter case, the router routes the call to the most likely destination in-stead of initiating disambiguation, which has been shown to be an effective strategy;not surprisingly, human operators are also prone to guess the destination based onlikelihood and route calls without disambiguation.5.3 Overall PerformanceOur final evaluation of the overall performance of the call router is calculated byapplying the results for evaluating the disambiguation module in Section 5.2 to theresults for the routing module in Section 5.1.
Tables 8(a) and 8(b) show the percentageof calls that will be correctly routed, incorrectly routed, and rejected, if we apply theperformance of the disambiguation module (Table 7) to the calls that fall into each classin the evaluation of the routing module (Table 5).
27 For instance, the performance oftranscribed class 2 calls (unambiguous calls that the router considered ambiguous) iscomputed as follows:Correct percentageIncorrect percentage= correct~tota l= (51,60/105)/389= 7.5%= incor rect~tota l= (5 + 51 ?
3/105)/389= 1.7%27 Note that the results in Table 8(b) are only a rough upper bound for the system's overall performanceon recognizer output, since the performance ofthe disambiguation module presented in Table 7 isevaluated on transcribed texts (because we were not able to obtain any speech data that were recordedand transcribed beyond the caller's initial response to the system's prompt).
In reality, the insertionsand deletions of n-gram terms in the recognizer output may lead to some inappropriatedisambiguation queries or more rejections to human operators.
In addition, users may provide usefulinformation ot solicited by the system's query.384Chu-Carroll and Carpenter Vector-based Natural Language Call RoutingTable 8Overall performance of call router.Correct Incorrect RejectClass 1 63.2% 1.3% 0%Class 2 7.5% 1.7% 5.3%Class 3 6.5% 2.2% 0%Class 4 7.0% 0.4% 4.9%Total 84.2% 5.6% 10.2%(a) Performance on transcriptionsCorrect Incorrect RejectClass 1 61.4% 2.3% 0%Class 2 7.2% 2.9% 5.0%Class 3 5.9% 2.6% 0%Class 4 6.3% 2.1% 4.3%Total 80.8% 9.9% 9.3%(b) Performance on ASR outputRejected percentage = rejected~total= (51,42/105)/389= 5.3%The results in Table 8(a) show that, with perfect recognition, our call router sends84.2% of all calls in our test set to the correct destination either with or withoutdisambiguation, sends 5.6% of all calls to the incorrect destination, and redirects 10.2%of the calls to a human operator.
In other words, our system attempts to automaticallyhandle 89.8% of the calls, of which 93.8% are routed to their correct destinations.
Whenspeech recognition errors are introduced to the routing module, the percentage of callscorrectly routed decreases while that of calls incorrectly routed increases.
However,it is interesting to note that the rejection rate decreases, indicating that the systemattempted to handle a larger portion of calls automatically.5.4 Performance Comparison with Existing SystemsAs discussed in Section 2, Gorin and his colleagues have experimented with variousmethodologies for relating caller utterances with call types (destinations).
Their systemperformance is evaluated by comparing the most likely destination returned by theircall type classifier given the first caller utterance with a manual ly  annotated list ofdestinations labeled based again on the first caller utterance.
A call is consideredcorrectly classified if the destination returned by their classifier is present in the list ofpossible destinations.
In other words, their evaluation scheme is similar to our methodfor computing the upper  bound performance of our router discussed in Section 5.1.2.We evaluated our router using their evaluation scheme with a rejection thresholdof 0.2 on both transcriptions and recognition output on our original set of 389 callsused in evaluating the routing module.
Table 9 shows a comparison of our system'sperformance and the best-performing version of their system as reported in Wright,Gorin, and Riccardi (1997); henceforth WGR97YWithout other measures of task complexity, it is impossible to directly compare ourresults with those of WGR97.
In several respects, their task is substantially differentthan ours.
Their task is simpler in that there are fewer possible activities that a callermight request and fewer overall destinations; but it is more complex in that vocabulary28 Wright, Gorin, and Riccardi (1997) presents ystem performance in the form of a rejection rate vs.correct classification rate graph, with rejection rate ranging between 10-55% and correct classificationrate ranging between 63-94%.
We report on two sets of results from their graph in Table 9, one with thelowest rejection rate and one that they chose to emphasize in their paper.385Computational Linguistics Volume 25, Number 3Table 9Evaluation of our system and WGR97.# of Destinations On Transcription On ASR OutputRejection Rate Correct Rate Rejection Rate Correct RateOur system 23 0% 94% 3% 92%WGR97 14 10% 84% 12% 78%WGR97 14 40% 94% 40% 83%items like cities are far more open-ended.
Furthermore, it appears that they have manymore instances of callers requesting services from more than one destination.Comparison with human operators was not possible for our task as their routingaccuracy has not been evaluated.
Our transcriptions clearly indicate that they all makea substantial number of routing errors (5-10% or more), with a large degree of variationamong operators.6.
Future WorkIn our current system, we perform morphological reduction context-independentlywithout regard to word class.
Ideally, we would have distinguished the uses of theword check as a verb from its uses as a noun, requiring both training and run-timecategory disambiguation.We are also interested in further clustering words that are similar in meaning, suchas car, auto, and automobile, ven though they are not related by regular morphologicalprocesses.
For our application, digits or sequences of digits might be conflated into asingle term, as might states, car makes and models, and so on.
This kind of application-specific lexical clustering, whether done by hand or with the help of resources such asthesauri or semantic networks, should improve performance by overcoming inherentdata sparseness problems.
Classes might also prove helpful in dealing with changingitems such as movie titles.
In our earlier experiments, we used latent semantic analysis(Deerwester et al 1990) for dimensionality reduction in an attempt to automaticallycluster words that are semantically similar.
This involved selecting dimensionalityk, which is less than the rank r of the original term-document matrix.
We foundperformance degrades for any k < r.In the current version of our system, the interface between the automatic speechrecognizer and the call router is the top hypothesis of the speech recognizer for thespeech input.
As reported in Table 3, this top hypothesis has an approximately 10%error rate on salient unigrams.
One way to improve this error rate is to allow thespeech recognizer to produce an n-best list of the top n recognition hypotheses oreven a probabilistic word graph rather than a single best hypothesis.
The n-gramterms can then be extracted from the graph in a straightforward manner and weightedaccording to their scores from the recognizer.
Our prediction is that this will leadto increased recall, with perhaps a slight degradation i precision.
However, sinceincreased recall will, at the very least, increase the chance that the disambiguationmodule can formulate reasonable queries, we expect he system's overall performanceto improve as a result.386Chu-Carroll and Carpenter Vector-based Natural Language Call Routing7.
ConclusionsWe described and evaluated a domain-independent, automatically trained call routerthat takes one of three actions in response to a caller's request.
It can route the call toa destination within the call center, attempt o dynamically formulate a disambigua-tion query, or route the call to a human operator.
The routing module selects a set ofcandidate destinations based on n-gram terms extracted from the caller's request anda vector-based comparison between these n-gram terms and each possible destination.If disambiguation is necessary, a yes-no question or a wh-question is dynamically gen-erated from among n-gram terms automatically extracted from the training data basedon closeness, relevance, and disambiguating power.
This query formulation processallows the system to tailor the disambiguating query to the caller's original requestand the candidate destinations.We have further demonstrated the effectiveness of our call router by evaluatingthe call router on both transcriptions of caller requests and the output of an automaticspeech recognizer on these requests.
When the input to the call router is free of recog-nition errors, our system correctly routes 93.8% of the calls after redirecting 10.2% ofall calls to a human operator.
When using the output of a speech recognizer with anapproximately 23% word error rate, the rejection rate drops to 9.3%, the upper boundof the router performance drops from 97.2% to 92.5%, and the lower bound of theperformance drops from 75.6% to 72.2%, illustrating the robustness of our call routerin the face of speech recognition errors.AcknowledgmentsWe would like to thank ChristerSamuelsson, Chin-Hui Lee, Wu Chou, RonHartung, and Jim Hieronymus for helpfuldiscussions, Wolfgang Reichl for providingus with speech recognition results, Jan vanSanten for help with the statistics, as well asChristine Nakatani, Diane Litman, and thethree anonymous reviewers for their helpfulcomments on an earlier draft of thispaper.ReferencesAbella, Alicia and Allen L. Gorin.
1997.Generating semantically consistent inputsto a dialog manager.
In Proceedings ofthe5th European Conference on SpeechCommunication a d Technology,pages 1,879-1,882.Carletta, Jean C. 1996.
Assessing thereliability of subjective codings.Computational Linguistics, 22(2):249-254.Carpenter, Bob and Jennifer Chu-Carroll.1998.
Natural anguage call routing: Arobust, self-organizing approach.
InProceedings ofthe Fifth InternationalConference on Spoken Language Processing,pages 2,059-2,062.Chu-Carroll, Jennifer and Bob Carpenter.1998.
Dialogue management ivector-based call routing.
In Proceedings ofthe 36th Annual Meeting, pages 256-262.Association for ComputationalLinguistics.Deerwester, Scott, Susan T. Dumais, GeorgeW.
Furnas, Thomas K. Landauer, andRichard Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of theAmerican Society for Information Science,41(6):391-407.Gorin, Allen L. 1996.
Processing of semanticinformation i  fluently spoken language.In Proceedings ofthe International Conferenceon Spoken Language Processing,pages 1,001-1,004.Gorin, Allen L., G. Riccardi, and J. H.Wright.
1997.
How may I help you?Speech Communication, 23:113-127.Green, Nancy and Sandra Carberry.
1994.
Ahybrid reasoning model for indirectanswers.
In Proceedings ofthe 32nd AnnualMeeting, pages 58-65.
Association forComputational Linguistics.Hockey, Beth Ann, Deborah Rossen-Knill,Beverly Spejewski, Matthew Stone, andStephen Isard.
1997.
Can you predictresponses to yes/no questions?
Yes, no,and stuff.
In Proceedings ofthe 5th EuropeanConference on Speech Communication a dTechnology, pages 2,267-2,270.Ittner, David J., David D. Lewis, and DavidD.
Ahn.
1995.
Text categorization f lowquality images.
In Symposium on DocumentAnalysis and Information Retrieval,pages 301-315, Las Vegas.387Computational Linguistics Volume 25, Number 3Lewis, David D. and William A. Gale.
1994.A sequential lgorithm for training textclassifiers.
In W. Bruce Croft and C. J. vanRijsbergen, editors, ACM-SIGIR Conferenceon Research and Development i  InformationRetrieval, pages 3-12, London.Springer-Verlag.Reichl, Wolfgang, Bob Carpenter, JenniferChu-Carroll, and Wu Chou.
1998.Language modeling for content selectionin human-computer dialogues.
InProceedings ofthe Fifth InternationalConference on Spoken Language Processing,pages 2,315-2,318.Riccardi, G. and A. L. Gorin.
1998.Stochastic language models for speechrecognition and understanding.
InProceedings in the Fifth InternationalConference on Spoken Language Processing,pages 2,087-2,090.Salton, Gerald.
1971.
The SMART RetrievalSystem.
Prentice Hall, Inc.Siegel, Sidney.
and N. John Castellan, Jr.1988.
Nonparametric Statistics for theBehavioral Sciences.
McGraw-Hill.Sparck Jones, Karen.
1972.
A statisticalinterpretation f term specificity and itsapplication in retrieval.
Journal ofDocumentation, 28(1):11-20.Sproat, Richard, editor.
1998.
MultilingualText-to-Speech Synthesis: The Bell LabsApproach.
Kluwer, Boston, MA.Walker, Marilyn A., Diane J. Litman,Candace A. Kamm, and Alicia Abella.1998.
Evaluating spoken dialogue agentswith PARADISE: Two case studies.Computer Speech and Language,12(3):317-347.Wright, J. H., A. L. Gorin, and G. Riccardi1997.
Automatic acquisition of salientgrammar fragments for call-typeclassification.
In Proceedings ofthe 5thEuropean Conference on SpeechCommunication a d Technology,pages 1,419-1,422.388
