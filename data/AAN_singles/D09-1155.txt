Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1493?1502,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPTowards Discipline-Independent Argumentative Zoning:Evidence from Chemistry and Computational LinguisticsSimone TeufelComputer LaboratoryCambridge Universitysht25@cl.cam.ac.ukAdvaith SiddharthanComputer LaboratoryCambridge Universityas372@cl.cam.ac.ukColin BatchelorRoyal Society of ChemistryCambridge, UKbatchelorc@rsc.orgAbstractArgumentative Zoning (AZ) is an anal-ysis of the argumentative and rhetoricalstructure of a scientific paper.
It has beenshown to be reliably used by independenthuman coders, and has proven useful forvarious information access tasks.
Annota-tion experiments have however so far beenrestricted to one discipline, computationallinguistics (CL).
Here, we present a moreinformative AZ scheme with 15 categoriesin place of the original 7, and show thatit can be applied to the life sciences aswell as to CL.
We use a domain expertto encode basic knowledge about the sub-ject (such as terminology and domain spe-cific rules for individual categories) as partof the annotation guidelines.
Our resultsshow that non-expert human coders canthen use these guidelines to reliably an-notate this scheme in two domains, chem-istry and computational linguistics.1 IntroductionTeufel et al (1999) define the task of Argumenta-tive Zoning (AZ) as a sentence-by-sentence clas-sification with mutually exclusive categories fromthe annotation scheme given in Fig.
1.
The reason-ing behind the categories is inspired by the notionof a knowledge claim (Myers, 1992; Luukkonen,1992): the act of writing a paper corresponds toan attempt of claiming ownership for a new pieceof knowledge, which is to be integrated into therepository of scientific knowledge in the authors?field by the process of peer review and publica-tion.
In the cause of this process, the authorshave to convince the reviewers that the knowledgeclaim of the paper is valid (Swales, 1990; Hy-land, 1998).
What AZ aims to model, then, aresome of the relevant stages in this argument.
Wedivide the paper into zones, OTHER, OWN andBACKGROUND.
These are defined on the basisof who owns the knowledge claim in the corre-sponding segment.
There are also two categorieswhich are defined by their relationship to existingwork, BASIS and CONTRAST.
That means thatparts of the AZ scheme are similar to citation func-tion classification schemes from the area of cita-tion content analysis (Garfield, 1965; Weinstock,1971; Spiegel-Ru?sing, 1977), and to automaticcitation function classification (Nanba and Oku-mura, 1999; Garzone and Mercer, 2000; Teufelet al, 2006).
The remaining categories, AIM andTEXTUAL, fulfil different rhetorical functions forthe presentation of the paper.
AIM points out thepaper?s main knowledge claim, a rhetorical movewhich may be repeated in the conclusion and theintroduction.
TEXTUAL explains the physical lo-cation of information, e.g., by giving a sectionoverview or presenting a summary of a subsec-tion.
On the basis of human-annotated trainingmaterial, AZ can be automatically classified usingsupervised machine learning.Category DescriptionAIM Statement of research goal.BACKGROUND Description of generally acceptedbackground knowledge.BASIS Existing KC provides basis for newKC.CONTRAST An existing KC is contrasted, com-pared, or presented as weak.OTHER Description of existing KC.OWN Description of any other aspect ofnew KC.TEXTUAL Indication of paper?s textualstructure.Figure 1: AZ Annotation Scheme (Teufel et al1999).Rhetorical information marking is useful for1493many novel information access tasks.
For in-stance, information retrieval can profit fromrhetorical information in the form of paradigmshift statements (Chichester et al, 2005), as paperscontaining such statements have a high impact inan area.
75% of the ?Faculty of 1000 Biology?papers (which are chosen by experts for their spe-cial importance) contain paradigm shift sentences(Agnes Sandor, personal communication).AZ annotation allows the construction of multi-and single document summaries which concen-trate on differences and similarities to related(cited) work.
AZ can also be used for search ina data base of scientific articles, in particular forenhanced citation indexing.
This has been pre-viously explored in a task-based evaluation, wereusers were asked to list positive and negative cita-tions they would expect in a paper, given a shortextract (Teufel, 2001).
In that task, AZ-based ex-tracts outperformed other document surrogates.Feltrim et al (2005) present a writing supportsystem which analyses students?
drafts of sum-maries for their PhD theses, performs an AZ anal-ysis on them and critiques the rhetorical structureof the students?
draft on the basis of it.The definition of the AZ categories is basedon rhetorical principles and should be decidable,in principle, without specific domain knowledgeabout what is discussed in detail in the paper.
Wepresent here the first evidence that AZ categoriescan be reliably recognised across scientific disci-plines, using chemistry and computational linguis-tics as our model disciplines for these experiments.The categories just introduced are abstract anddepend on the annotators?
interpretation of arhetorical argument.
This means that there isno guarantee that several independent annotatorswould annotate similarly.
It is therefore crucialthat all annotations at a high level of interpreta-tion are backed up by human annotation with morethan one annotator.
However, annotations of cita-tion function classification typically use only theuntested annotation of a single human annotatoras gold standard, who is typically the designer of ascheme (Spiegel-Ru?sing, 1977; Weinstock, 1971;Nanba and Okumura, 1999; Garzone and Mercer,2000).
Teufel et al (2006) are the only exceptionwho test their citation function scheme using mod-ern corpus-linguistic annotation methodology.A study of human agreement on AZ annotationexists (Teufel et al, 1999), but this uses articlesfrom only one discipline, namely computationallinguistics.
In this paper, we use a similar method-ology to Teufel et al, but with data from two disci-plines.
The preliminary conclusion from these ex-periments is that annotation with chemistry papershas resulted in higher agreement than annotationwith computational linguistics papers.We extend the AZ annotation scheme to makefurther distinctions, as will be discussed in sec-tion 2.
We also created an environment in whichdomain knowledge that an annotator might haveabout the science in a paper is systematically dis-regarded.
We will describe how this was done insection 3, and then present the annotation experi-ment itself in section 4.2 Changes to the AZ SchemeArgumentative Zoning II (AZ-II) is a new annota-tion scheme, which is an elaboration of the orig-inal AZ scheme.
It is presented in Fig.
2.
Ourannotation guidelines are 111 sides of A4 and con-tain a decision tree, detailed description of the se-mantics of the 15 categories, 75 rules for pairwisedistinction of the categories and copious examplesfrom both chemistry and computational linguis-tics.
During guideline development, 70 chemistrypapers and 20 CL papers were used, which are dis-tinct from the ones used for annotation.
It took 3months part-time-work to prepare the guidelinesfor CL, and substantially less time to adapt themfor chemistry.
We have made them available atwww.cl.cam.ac.uk/research/nl/sciborg.The differences between the original AZ andAZ-II are as follows:?
Category AIM remained the same.?
Category BACKGROUND was renamedCO GRO, or common ground.?
Category OTHER was split into other peo-ple?s work (OTHR) and the authors?
own pre-vious work (PREV OWN).?
Category BASIS was split into usage (USE)and support (SUPPORT).?
Category CONTRAST was split into neu-tral comparison (CODI), contradiction(ANTISUPP), and a category combiningresearch gaps with criticism (GAP WEAK).?
Category OWN was split into description ofmethod (OWN MTHD), results (OWN RES)and conclusions (OWN CONC), and a cate-gory which specifies recoverable errors madeby the authors (OWN FAIL).1494Category Description Category DescriptionAIM Statement of specific research goal, orhypothesis of current paperOWN CONC Findings, conclusions (non-measurable)of own workNOV ADV Novelty or advantage of own approach CODI Comparison, contrast, difference toother solution (neutral)CO GRO No knowledge claim is raised (or knowl-edge claim not significant for the paper)GAP WEAK Lack of solution in field, problem withother solutionsOTHR Knowledge claim (significant for paper)held by somebody else.
Neutral descrip-tionANTISUPP Clash with somebody else?s results ortheory; superiority of own workPREV OWN Knowledge claim (significant) held byauthors in a previous paper.
Neutral de-scription.SUPPORT Other work supports current work or issupported by current workOWN MTHD New Knowledge claim, own work:methodsUSE Other work is used in own workOWN FAIL A solution/method/experiment in the pa-per that did not workFUT Statements/suggestions about futurework (own or general)OWN RES Measurable/objective outcome of ownworkFigure 2: AZ-II Annotation Scheme.?
Category TEXTUAL was discontinued, be-cause it is less informative than the other cat-egories.?
Two new categories were introduced,NOV ADV (advantages of the new knowl-edge claim) and FUT (declaration oflimitations or future work).Our AZ-II categories are more fine-grained thanthe original AZ categories.
The reasons for this aretwofold: To bring AZ closer to contemporary cita-tion function schemes, and to incorporate distinc-tions recently found useful by other researchers.For instance, Chichester et al (2005) argue thatANTISUPP is particularly important.
The finergrain in AZ-II has been accomplished purely bysplitting existing AZ categories; hence, the coarserAZ categories are recoverable (with the exceptionof the TEXTUAL category).
Annotation examplesare given in the appendix.As in AZ, citations are an important but not nec-essarily decisive cue for a sentence to belong toa particular zone.
The guidelines mention cita-tions as one factor in deciding whether a knowl-edge claim holds, and citations occur in severalexamples, so it is likely that the presence of ci-tations would have influenced annotators in theirdecision.Of the changes, the distinction which is likelyto have the greatest impact on the annotation isthe split of OWN according to the stage of the au-thors?
problem solving process ?
into methods, re-sults, conclusion or local failure.
In most life sci-ences, descriptions of research as a problem solv-ing process are a dominant phenomenon, wherebyproblem-solving descriptions can be of differinglength and embeddedness.
For instance, in syn-thetic chemistry, the starting compound for themain synthesis in the paper may first have to besynthesised itself (if it is not commercially avail-able, for instance).
In that case, arriving at thecompound is an intermediate, smaller problem-solving process which enables the larger problem-solving process that represents the new KC.The original AZ scheme didn?t mark the dis-tinction, possibly because it is not as easily ob-servable in CL as it is in the life sciences, andbecause problem-solving stages were not part ofthe main analytic interest of AZ, which focusedon how scientific argumentation is related to de-scriptions of own and other work.
Also, neither ofthe traditional AZ applications (summarisation orcitation indexing) had any direct use for the subdi-vided categories.
But in the life sciences, thereare applications which would make use of sucha subdivision.
For instance, in chemistry thereis a niche for search applications which guidesearchers directly to the method and/or result sec-tions in papers.
Specifically, the OWN FAIL cat-egory is motivated by the failure?and?recoverysearch.
In text, OWN FAILmarks cases where theauthors helpfully mention in passing steps whichwere found not to work during a long syntheticprocedure (often the ?total synthesis?
of a com-pound which is found in nature).
Such cases hap-pen frequently, and are generally followed by a?recovery?
statement which explains how the prob-lem can be avoided.
Another possible applica-tion that calls for a subdivision is Feltrim et al?s1495(2005) rhetorical writing system for novice writ-ers.
It trains novices in writing rhetorically well-formed abstracts and therefore must have a way ofdistinguishing, for instance, between methods andresults.Note that several of the applications based onAZ and AZ-II in general rely on the rare categoriesmuch more than they rely on the more frequentcategories.
OWN FAIL is an example of a rare butimportant category, and so is AIM, which is centralto summarisation applications.
The comparativeand contrastive categories CODI ANTISUPP andGAP WEAK, on the other hand, are particularlyuseful to citation-based search applications.Other AZ-like schemes for scientific discoursecreated for the biomedical domain (Mizuta andCollier, 2004) and for computer science (Feltrimet al, 2005) also made the decision to subdivideOWN, in similar ways to how we propose here.The current work, however, is the first experimen-tal proof that humans can make this distinction ?and the others encoded in AZ-II ?
reliably, and intwo quite distinct disciplines.3 Discipline-Independent Non-ExpertAnnotationAn important principle of AZ is that its categoriescan be decided without domain knowledge.
Thisrule is anchored in the guidelines: when choosinga category, no reasoning about the scientific factsis allowed.
The avoidance of domain-knowledgehas its motivation in a strategy for a hypotheti-cal automatic text-understanding system for unre-stricted texts.
Given the state of the art in text pro-cessing and knowledge representation, text under-standing systems should in our opinion use gen-eral, rhetorical, and logical aspects of the text,rather than attempting to recognise or represent thescientific knowledge contained in the text.
Whatthe human annotation ?
the gold standard ?
shouldthen do is to simulate the best possible output thatsuch a system could theoretically create.Annotators may use only general, rhetorical orlinguistic knowledge; knowledge which is sharedby all proficient speakers of a language.
Theguidelines spell out what is meant by these generalprinciples.
For instance, one can use lexical andsyntactic parallelism in a text to infer that the au-thors were setting up a comparison between them-selves and some other approach.There is, however, a problem with annotator ex-pertise and with the exact implementation of the?no domain knowledge?
principle.
This problemdoes not become apparent until one starts work-ing with disciplines where at least some of the an-notators or guideline developers are not domainexperts (chemistry, in our case).
Domain expertsnaturally use scientific knowledge and inferencewhen they make annotation decisions.
It wouldbe unrealistic to expect them to be able to disre-gard their domain knowledge simply because theywere instructed to do so.
Additionally, when allannotators/scheme developers are domain experts,it is hard to even notice the cases where they ?ac-cidentally?
use domain knowledge during anno-tation.
We therefore artificially created a situa-tion where all annotators are ?semi-informed non-experts?, which forces them to comply with theprinciple, namely by the following rules:Justification: Annotators have to justify all an-notation decisions by pointing to some text-basedevidence, and by giving the section heading in theguidelines that describes the particular reason forassigning the category.
General discipline-specificknowledge an annotator may happen to have is ex-cluded as justification.
Annotators?
justificationshave to be typed into the annotation tool and areopen to challenge during the training phase.
Muchof the allowable justification comes in the formof general and linguistic principles, e.g., an ex-plicit cue phrase, the title, or the structural simi-larity of textual strings.
For instance, annotatorsare allowed to infer that process-VPs in the titleare likely to be the contribution (knowledge of theactual concrete contribution of a paper is a require-ment for annotation of AIM).Discipline-specific Generics: The guidelinescontain a section with high-level facts about thegeneral research practices in the discipline.
Thesegenerics constitute the only scientific knowledgewhich is acceptable as a justification, and areaimed to help non-expert annotators recognisehow a paper might relate to already establishedscientific knowledge, so that they will be ableto avoid common mistakes about the knowledgeclaim status of a certain fact.
For instance, the bet-ter they are able to distinguish what is commonlyknown from what is newly claimed by the authors,the more consistent their annotation will be.Annotation with expert-trained non-expert an-notators means that a domain expert must be avail-able initially, during the development of the anno-1496tation scheme and the guidelines, either as a co-developer or as an informant.
The domain expert?sjob is to describe scientific knowledge in that do-main in a general way, in as far as it is neces-sary for the scheme?s distinctions, and to write thedomain-specific rules for the individual categories,including the choice of example sentences.
Thismeans that the guidelines are split into a domain-general and a domain-specific part.The discipline-specific generics in chemistrycome in the form of a ?chemistry primer?, a 10-page collection of high-level scientific domainknowledge.
It contains: a glossary of words a non-chemist would not have heard about or would notnecessarily recognise as chemical terminology; alist of possible types of experiments performedin chemistry; a list of commonly used machin-ery; a list of non-obvious negative characterisa-tions of experiments and compounds (?sluggish?,?inert?
); and a list of possible types of knowledgeclaims.
For instance, in chemistry each chemi-cal substance mentioned can have in principle aknowledge claim associated with its discovery orinvention ?
with the exception of water, rock salt,the metals known in prehistory and a few others.If a compound or process is however considered tobe so commonly used that it is in the ?general do-main?
(e.g., ?the Stern?Volmer equation?
or ?theGrignard reaction?
), it is no longer associated withsomebody?s knowledge claim, and as a result itsusage is not to be marked with category USE.Descriptions of individual categories can havedomain-specific subsections, as well as the gen-eral ones.
For instance, if the text states that theauthors could not replicate a published result, theguidelines describe the cases when this is the au-thors?
fault (OWN FAIL) in contrast to the caseswhere this is an indirect accusation of the previ-ous experiment (ANTISUPP).Another potentially unclear distinction isbetween results (OWN RES) and conclusions(OWN CONC).
The difference is defined onthe basis of how much reasoning is necessaryto be able to make the statement concerned.
Ifall the authors did was to read a measurementoff an instrument, the label OWN RES applies.Reasoning points to OWN CONC; it is some-times linguistically marked (?therefore?, ?weconclude?, ?this means that?
), but in many cases,domain knowledge may be required to decidewhether reasoning was necessary to make acertain statement.
Possible OWN RES statements,according to the chemistry primer, include: state-ments of simple numerical result; descriptions ofgraphs; descriptions of atoms?
positions in three-dimensional space; statements of trends, unlessa reason for these results is given; comparisonsof results of more than one experiment, unless areason for these results is given.The chemistry primer also lists phenomenawhich in a typical experiment would be read offchemical machinery (e.g., ?Stark effect?).
This listgives the non-expert annotator an objective crite-rion to answer the question how likely it is that acertain statement by the authors was arrived at byinference.
We also found that our list of phenom-ena which can be read off machinery, which wascompiled from the first 30 papers, generalised wellto the other 40 papers considered.The chemistry primer is not an attempt to sum-marise all methods and experimentation types inchemistry; this would be impossible to do, cer-tainly in a few pages.
Rather, it tries to answermany of the high-level questions a non-expertwould have to an expert, in the framework of AZ.This methodology allows to hire expert andnon-expert annotators and bring them in line witheach other.
We believe it could be expanded rel-atively easily into many other disciplines, usingdomain experts which create similar primers forgenetics, experimental physics, cell biology, butre-using the bulk of the guidelines.4 Annotation ExperimentsThe annotators were the co-developers of the an-notation scheme and the authors of this paper.Whereas all three annotators have good back-ground knowledge in CL, the largest difference be-tween them concerns their expertise in chemistry:Annotator A is a PhD-level chemist, Annotator Bhas two years?
of undergraduate training in chem-istry and can therefore be considered a chemicalsemi-expert, and Annotator C has no specialisedchemistry knowledge.As agreement measure we choose the Kappacoefficient ?
(Fleiss, 1971; Siegel and Castellan,1988), the agreement measure predominantly usedin natural language processing research (Carletta,1996).
?
corrects raw agreement P (A) for agree-ment by chance P (E):?
=P (A)?P (E)1?P (E)1497No matter how many items or annotators, orhow the categories are distributed, ?
= 0 whenthere is no agreement other than what would beexpected by chance, and ?
= 1 when agreementis perfect.
If two annotators agree less than ex-pected by chance, ?
can also be negative.
Chanceagreement P (E) is defined as the level of agree-ment which would be reached by random anno-tation using the same distribution of categories asthe real annotators.
All work done here is reportedin terms of Fleiss?
?.
1 ?
is also designed to ab-stract over the number of annotators as its formularelies on the proportion of expected vs. observedpairwise agreements possible in a pool.
That is,?
for k annotators will be an average of the val-ues of ?
taking all possible m-tuples of annota-tors from the annotator pool (with m < k).
As aside effect of its definition of random agreement,?
treats agreement in a rare category as more sur-prising, and rewards such agreement more than anagreement in a frequent category.
This is a desir-able property, because we are more interested inthe performance of the rare rhetorical categoriesthan we are in the performance of the more fre-quent zone categories.4.1 DataFor chemistry, 30 random-sampled papers fromjournals published between 2004 and 2007 by theRoyal Society of Chemistry were used for anno-tation2.
The papers cover all areas of chemistryand some areas close to chemistry, such as climatemodelling, process engineering, and a double-blind medical trial.
The data used for the exper-iment contains a total of 3745 sentences.For computational linguistics, 9 papers were an-notated, with a total of 1629 sentences.
The paperswere published between 1998 and 2001 at ACL,EACL or EMNLP conferences, and were takenfrom the Computation and Language archive.Both chemistry and CL papers were automaticallysentence-split, with manual correction of errors;acknowledgement sections were disregarded.
A1Artstein and Poesio (2008) observe that there are severalversion of ?
which differ in how P (E) is calculated.
In par-ticular, Fleiss?
(1971) ?
calculates P (E) as the average ob-served distribution of all annotators, whereas Cohen?s (1960)?
calculates P (E) only on the basis of the other annotator(s).2100 papers across a spread of disciplines from the Jan-uary 2004 issues of the RSC were selected blindly (but withan attempt to cover most areas of chemistry).
30 out of thesewere random sampled for annotation; the rest were used forannotation development.Category Chem CL Category Chem CLOWN MTHD 25.4 55.6 SUPPORT 1.5 0.7OWN RES 24.0 5.6 GAP WEAK 1.1 1.0OWN CONC 15.1 10.7 FUT 1.0 1.4OTHR 8.3 10.0 NOV ADV 1.0 0.8USE 7.9 2.7 CODI 0.8 1.2CO GRO 6.7 5.7 OWN FAIL 0.8 0.1PREV OWN 3.4 1.7 ANTISUPP 0.5 0.6AIM 2.3 1.8Figure 3: Frequency of AZ-II Categories (in %).web-based annotation tool was used for guidelinedefinition and for annotation.Our choice of which data sets to use was ef-fected by the relative length of papers more thanby the journal/conference distinction.
Averagearticle length between chemistry journal articles(3650 words/paper) and CL conference articles(4219 words/paper) is comparable, so conferencearticles in CL seem a much better choice for com-parative work than journal publications, which areoften very long in CL.
Additionally, conferenceshave a high profile in CL, and we found the con-ference publications to be of high editorial quality.We are nevertheless interested in the structure oflonger journal articles, and plan to investigate CLjournals in the future.The annotations were done using a web-basedannotation tool.
Every sentence is assigned a cat-egory.
No communication between the annotatorswas allowed.4.2 ResultsThe inter-annotator agreement for chemistrywas ?
= 0.71 (N=3745,n=15,k=3).
For CL,the inter-annotator agreement was ?
= 0.65(N=1629,n=15,k=3).
For comparison, theinter-annotator agreement for the original, CL-specific AZ with 7 categories was ?
= 0.71(N=3420,n=7,k=3).
Given the subjective natureof the task and the fact that AZ-II introduces ad-ditional distinctions, the AZ-II agreement can beconsidered acceptable for CL and relatively highfor chemistry.
Additionally, chemistry annota-tion used one non-expert annotator, who had nochemistry-specific domain knowledge apart fromthat in the chemistry primer.The distribution of categories for the two disci-plines is given in Fig.
3.
As expected, there is alarge discrepancy in frequency between the (rare)rhetorical categories and the (much more fre-quent) zone categories OWN MTHD, OWN RES,1498OWN CONC, OTHR and CO GRO.
For supervisedlearning, too few examples of any category can bea problem.
There are methods which attempt to re-duce the annotation effort by using a trained clas-sifier to suggest possible cases to a human.
How-ever, the classifier can only find examples similarto the ones that have already been manually clas-sified, when the real problem is a recall-problem,i.e., the challenge is to find more new examples inthe multitude of possible sentences.
To solve thisin a fundamentally sound way, there seems to beno other way than to annotate more texts, at thecost of more human effort.If we consider the differences across disci-plines, the most striking ones concern the distri-bution of OWN MTHD, which is more than twiceas common in CL (56% v. 25%), and OWN RES,which is far more common in chemistry overall(24% v 5.6%).
Usage of other people?s knowl-edge claims or materials also seems to be morecommon in chemistry, or at least more explicitlyexpressed (7.9% vs 2.7%).
With respect to theshorter, rarer categories, there is a marked dif-ference in OWN FAIL (0.1% in CL, but 0.8% inchemistry3 and SUPPORT, which is more commonin chemistry (1.5% vs 0.7%).
However, this effectis not present for ANTISUPP (contradiction of re-sults), the ?reverse?
category to SUPPORT, (0.6%in CL vs 0.5% in chemistry).As far as the chemistry annotation is con-cerned, it is interesting to find out whether Annota-tor A was influenced during annotation by domainknowledge which Annotator C did not have, andAnnotator B had to a lower degree4.
We there-fore calculated pairwise agreement, which was?AC= 0.66, ?BC= 0.73 and ?AB= 0.73 (all:N=3745,n=15,k=2).
That means that the largestdisagreements were between the non-expert (C)and the expert (A), though the differences aremodest.
This might point to the fact that Anno-tators A and B might have used a certain amountof domain-knowledge which the chemistry primerin the guidelines does not yet, but should, cover.In an attempt to determine how well cate-gories are defined, we first consider the binary dis-3These are not large differences in absolute terms ?
55items identified as OWN FAIL by at least one annotator inchemistry, vs. 7 such items in CL, the relative difference islarge and confirms that in chemistry papers, particularly de-scriptions of synthesis procedures, OWN FAIL cases appearrelatively frequently.4This question does not arise in the case of CL, as all an-notators can be considered experts in this respect.tinction between zone categories (OWN MTHD,OWN RES, OWN CONC, OWN FAIL, OTHR,PREV OWN and CO GRO) and rhetorical cate-gories (the other 8).
This shows an inter-annotatoragreement of ?binary= 0.78 (N=3745, n=2, k=3)for chemistry and ?binary= 0.65 (N=1629, n=2,k=3) for CL, indicating that annotators find it rel-atively easy (chemistry) or at least not more dif-ficult than the overall distinction (CL) to distin-guish these two types of categories.
We next per-form Krippendorff?s (1980) category distinctions(Fig.
4).
Here, all categories apart from the onediagnosed are collapsed, and what is reported isthe difference of inter-annotator agreement whencompared to the overall distinctiveness (?=0.71for chemistry, ?=0.65 for CL).
Where the differ-ence is positive, the annotators could distinguishthe given category better than they could distin-guish all categories, and where they are negative,correspondingly worse.5The results confirm that categories USE, AIM,OWN MTHD, OWN RES and FUT are particularlywell distinguished in both disciplines.
This is apositive result, as these categories are importantfor several types of searches.
In these cases theguidelines seem to fully suffice for their descrip-tion, but then again good performance of AIM,FUT and USE is not that surprising, as they aresignalled clearly by linguistic and non-linguisticcues.
However, there are three categories withparticularly low distinguishability in both disci-plines: ANTISUPP, OWN FAIL and PREV OWN.As ANTISUPP and OWN FAIL are crucial for theenvisaged downstream tasks, the problems withtheir definition should be identified and fixed.
Weare in the process of systematically troubleshoot-ing the guidelines for those categories.The table also shows that category definitionhas discipline-specific problems.
For instance,we believe that the fact that distinctiveness forOWN FAIL is so bad for CL must be due to thefact that we only encountered very few potentialOWN FAIL cases in this domain.
The definitionof the categories SUPPORT and NOV ADV alsoseem to be substantially more confusing for CLthan for chemistry.
However, CODI is a categorywhich shows average distinctiveness for CL, butmuch worse distinctiveness for chemistry.
We be-lieve this is due to the fact that comparisons of5All ?
values for chemistry were measured with N=3745,n=2, k=3; for CL with N=1629, n=2, k=3.1499methods and approaches are more common in CLand are clearly expressed, whereas in chemistrythe objects that are involved in comparisons aremore varied and at a lower grade of abstraction(e.g., compounds, properties of compounds, coef-ficients, etc.
), which obviously has a negative ef-fect on the distinctiveness of this category.Category Chem CL Category Chem CLUSE +0.12 +0.00 NOV ADV -0.07 -0.23AIM +0.09 +0.08 OWN CONC -0.08 -0.13OWN MTHD +0.05 +0.05 GAP WEAK -0.08 -0.16OWN RES +0.02 +0.04 PREV OWN -0.11 -0.15FUT +0.01 +0.06 OWN FAIL -0.19 -0.43CO GRO -0.01 -0.03 ANTISUPP -0.35 -0.32SUPPORT -0.04 -0.12 CODI -0.36 +0.00OTHR -0.06 +0.07Figure 4: Krippendorff?s Diagnostics for CategoryDistinction (?, relative to Overall Distinctiveness).We also provide a direct comparison of our an-notation results with those from the original AZscheme.
Comparisons between two similar anno-tation schemes can be made by collapsing thosecategories in each scheme which are not distin-guished in the other scheme.
Such a comparisoncan of course only ever approximate the smallestcommon denominator between two schemes.The AZ-II categories were collapsed into a setof six categories that closely resemble AZ cate-gories, as described in section 2 (with OWN simu-lated by the union of OWN FAIL, OWN MTHD,OWN RES, OWN CONC, FUT, and NOV ADV).This created a 6-category AZ annotation.As TEXTUAL is not marked up in AZ-II, theoriginal AZ annotation was also collapsed, by in-corporating TEXTUAL examples into OWN.
Thetwo 6-pronged AZ-annotations are now more di-rectly comparable.
Inter-annotator agreement forthe collapsed AZ-II showed ?
= 0.75 (N=3745,n=6, k=3).
This compares favourably to the col-lapsed AZ?s agreement of ?
= 0.71 (N=3420, n=6,k=3); but when comparing the raw numerical re-sults one should consider that different data fromdifferent disciplines is used (chemistry in AZ-II,CL in AZ).These results should be interpreted as a pos-itive result for the domain-independence of AZ,and also for the feasibility of using trained non-experts as annotators.
The additional work thatwent into the guidelines has produced annotationof a high consistency, even though AZ-II providesmore distinctions (15 categories vs. 7 in AZ).There is also the faint possibility that discourseannotation of chemistry is intrinsically easier thandiscourse annotation of CL, because it is a moreestablished discipline and not despite of it.
Forinstance, it is likely that the problem-solving cat-egories OWN FAIL, OWN MTHD, OWN RES andOWN CONC are easier to describe in a disciplinewith an established methodology (such as chem-istry), than they are in a younger, developing dis-cipline such as computational linguistics.5 ConclusionArgumentative Zoning is an analysis of the rhetor-ical progression of the scientific argument in a pa-per.
In this paper, we have made the followingcontributions to this analysis:?
We have presented a more informativescheme, which additionally recognises thestructure of an experiment in terms of prob-lem solving (method ?
results ?
conclusions)and makes more fine-grained distinctions insome of the sentiment-inspired relational cat-egories (e.g., criticism and comparisons toother approaches).?
We introduced an annotation methodologywhich attempts to systematically exclude theuse of annotators?
extraneous domain knowl-edge from the annotation.?
We have experimentally shown that humancoders can independently annotate this newAZ scheme in two distinct disciplines.
Ourresults show inter-annotator agreements of?=0.65 and ?=0.71 for computational lin-guistics and chemistry, respectively.Overall, the outcome of this work indicatesthat the phenomena described in AZ can be de-fined in a domain-independent way.
The experi-ment also tested how realistic the ?expert-trainednon-expert?
approach to domain-knowledge freeannotation is.
The fact that the agreement be-tween three annotators (an expert, a semi-expert,and a non-expert) is acceptable overall vindicateour task definition as domain-knowledge free (us-ing the tools of justification and domain-specificgeneric knowledge).
However, the agreements in-volving the semi-expert are higher than the agree-ment between expert and non-expert.
This prob-ably means that the chemistry generics were notfully adequate to ensure that the non-expert un-derstood enough of the chemistry to achieve thehighest-possible agreement.1500The automation of AZ-annotation is underway.This requires adaptation of the high-level featuresused in AZ (Teufel and Moens, 2002) to chemistry.We are also preparing an annotation experimentwith naive annotators.
Another research avenueis the expansion of the guidelines to other disci-plines such as bio-medicine, and to longer journalarticles, e.g., in computational linguistics.6 AcknowledgementsThis work was funded by EPSRC project Sciborg(EP/C010035/1).Appendix: Annotation Examples6AIM We now describe in this paper a synthetic route for thefunctionalisation of the framework of mesoporous organosil-ica by free phosphine oxide ligands, which can act as a tem-plate for the introduction of lanthanide ions.
(b514878b)AIM The aim of this paper is to examine the role that train-ing plays in the tagging process .
.
.
(9410012)NOV ADV Moreover, the simplicity and ease of applicationof the electrochemical method [...] should also be emphasisedand makes it an interesting and valuable synthetic tool.
(b513402a)NOV ADV Other than the economic factor, an important ad-vantage of combining morphological analysis and error detec-tion/correction is the way the lexical tree associated with theanalysis can be used to determine correction possibilities.
(9504024)CO GRO A wide range of organosulfur compounds are bi-ologically active and some find commercial application asfungicides and bactericides1?4 .
(b514441h)CO GRO It has often been stated that discourse is an inher-ently collaborative process .
.
.
(9504007)OTHR In their system, antibody immobilized on a solid sub-strate reacts with antigen, which binds with another antibodylabelled with peroxidase.
(b313094k)OTHR But in Moortgat?s mixed system all the different re-source management modes of the different systems are left in-tact in the combination and can be exploited in different partsof the grammar.
(9605016)PREV OWN As a program aimed at the applications ofimines(2a,g,5) we have studied the formation of carbanionsfrom imines and their subsequent reactions.
(b200198e)PREV OWN Earlier work of the author (Feldweg 1993;Feldweg 1995a) within the framework of a project on corpusbased development of lexical knowledge bases (ELWIS) hasproduced LIKELY .
.
.
(9502038)OWN MTHD In order for it to be useful for our purposes,the following extensions must be made: (0102021)OWN MTHD On the other hand, a tertiary amide can be anexcellent linking functional group.
(b201987f)6Corpus examples are taken from our chemistry and CLdata sets; indicated by their respective file numbers.OWN FAIL Initial attempts to improve the dehydration of 4via chemical or thermal means were unsuccessful; similarly,attempts to couple the chlorosilane (Me3Si)2 (Me2ClSi)CHwith Ag2O failed.
(b510692c)OWN FAIL When the ABL algorithms try to learn with twocompletely distinct sentences, nothing can be learned.
(0104006)OWN RES While the acid 1a readily coupled to the olefin,the corresponding boronic ester was surprisingly inert underthe reaction conditions.
(b311492a)OWN RES All the curves have a generally upward trend butalways lie far below backoff (51% error rate).
(0001012)OWN CONC It is unlikely that every VOC emit ted by plantsserves an ecological or physiological role .
.
.
(b507589k)OWN CONC Unless grammar size takes on proportionatelymuch more significance for such longer inputs, which seemsimplausible, it appears that in fact the major problems do notlie in the area of grammar size, but in input length.
(9405033)GAP WEAK Various methods of preparation have been de-veloped, but they often suffer from low yield and tediousseparation.
[16,17,28,31] (b200888m)GAP WEAK Here, we will produce experimental evidencesuggesting that this simple model leads to serious overesti-mates of system error rates.
.
.
(9407009)CODI However, the measured values of the dielectric con-stant (?
= 310) are lower than the values reported by Ganguliand coworkers(21) for BSTO pellets sintered at 1100 degC .
.
.
(b506578j)CODI Unlike most research in pragmatics that focuses oncertain types of presuppositions or implicatures, we provide aglobal framework in which one can express all these types ofpragmatic inferences.
(9504017)SUPPORT This is in line with the findings of Martin and Illasfor inorganic solids (84,85) .
(b515732c)SUPPORT Work similar to that described here has been car-ried out by Merialdo (1994), with broadly similar conclusions.
(9410012)USE The diamine 10 was prepared following a previouslypublished procedure(4d) .
(b110865b)USE We use the framework for the allocation and transferof control of Whittaker and Stenton (1988).
(9504007)FUT Our further efforts are directed towards the abovegoal,.
.
.
and overcoming limitations pertaining to the electron-poor arylboronic acids.
(b311492a)FUT An important area for future research is to developprincipled methods for identifying distinct speaker strategiespertaining to how they signal segments.
(9505025)ANTISUPP Although purification of 8b to a de of 95percenthas been reported elsewhere[31], in our hands it was alwaysobtained as a mixture of the two [EQN]-diastereomers.
(b310767a)ANTISUPP This result challenges the claims of recent dis-course theories (Grosz and Sidner 1986, Reichman 1985)which argue for a the close relation between cue words anddiscourse structure.
(9504006)1501ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
Computational Lin-guistics, 34(4):555?596.Jean Carletta.
1996.
Assessing agreement on classificationtasks: The kappa statistic.
Computational Linguistics,22(2):249?254.Christine Chichester, Frdrique Lisacek, Aaron Kaplan, andAgnes Sandor.
2005.
Discovering paradigm shift patternsin biomedical abstracts: application to neurodegenerativediseases.
In Proceedings of First International Sympo-sium on Semantic Mining in Biomedicine.Jacob Cohen.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and Psychological Measurement,20:37?46.V.
Feltrim, Simone Teufel, Gracas Nunes, and S. Alu-sio.
2005.
Argumentative zoning applied to critiquingnovices?
scientific abstracts.
In Janyce Wiebe JamesG.
Shanahan, Yan Qu, editor, Computing Attitude and Af-fect in Text: Theory and Applications, pages 233?245.Springer, Dordrecht, The Netherlands.J.
L. Fleiss.
1971.
Measuring nominal scale agreementamong many raters.
Psychological Bulletin, 76:378?381.Eugene Garfield.
1965.
Can citation indexing be automated?In M. et al Stevens, editor, Statistical Association Meth-ods for Mechanical Documentation (NBS Misc.
Pub.
269).National Bureau of Standards, Washington.Mark Garzone and Robert E. Mercer.
2000.
Towards an au-tomated citation classifier.
In Proceedings of the 13th Bi-ennial Conference of the CSCI/SCEIO (AI-2000), pages337?346.Ken Hyland.
1998.
Persuasion and context: The pragmat-ics of academic metadiscourse.
Journal of Pragmatics,30(4):437?455.Klaus Krippendorff.
1980.
Content Analysis: An Introduc-tion to its Methodology.
Sage Publications, Beverly Hills,CA.Terttu Luukkonen.
1992.
Is scientists?
publishing behaviourreward-seeking?
Scientometrics, 24:297?319.Yoko Mizuta and Nigel Collier.
2004.
An annotation schemefor rhetorical analysis of biology articles.
In Proceedingsof LREC?2004.Greg Myers.
1992.
In this paper we report...?speech actsand scientific facts.
Journal of Pragmatics, 17(4):295?313.Hidetsugu Nanba and Manabu Okumura.
1999.
Towardsmulti-paper summarization using reference information.In Proceedings of the XXth International Joint Conferenceon Artificial Intelligence (IJCAI-99), pages 926?931.Sidney Siegel and N. John Jr. Castellan.
1988.
Nonparamet-ric Statistics for the Behavioral Sciences.
McGraw-Hill,Berkeley, CA, 2nd edition.Ina Spiegel-Ru?sing.
1977.
Bibliometric and content analy-sis.
Social Studies of Science, 7:97?113.John Swales, 1990.
Genre Analysis: English in Academicand Research Settings.
Chapter 7: Research articles inEnglish, pages 110?176.
Cambridge University Press,Cambridge, UK.Simone Teufel and Marc Moens.
2002.
Summarising scien-tific articles ?
experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28(4):409?446.Simone Teufel, Jean Carletta, and Marc Moens.
1999.
Anannotation scheme for discourse-level argumentation inresearch articles.
In Proceedings of the 9th Meeting of theEuropean Chapter of the Association for ComputationalLinguistics (EACL-99), pages 110?117, Bergen, Norway.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006.Automatic classification of citation function.
In Proceed-ings of EMNLP-06.Simone Teufel.
2001.
Task-based evaluation of summaryquality: Describing relationships between scientific pa-pers.
In Proceedings of NAACL-01 Workshop ?AutomaticText Summarization?, Pittsburgh, PA.Melvin Weinstock.
1971.
Citation indexes.
In Encyclopediaof Library and Information Science, volume 5, pages 16?40.
Dekker, New York, NY.1502
