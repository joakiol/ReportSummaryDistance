Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293?301,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsResults of the WMT14 Metrics Shared TaskMatou?s Mach?a?cek and Ond?rej BojarCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied Linguisticsmachacekmatous@gmail.com and bojar@ufal.mff.cuni.czAbstractThis paper presents the results of theWMT14 Metrics Shared Task.
We askedparticipants of this task to score theoutputs of the MT systems involved inWMT14 Shared Translation Task.
We col-lected scores of 23 metrics from 12 re-search groups.
In addition to that we com-puted scores of 6 standard metrics (BLEU,NIST, WER, PER, TER and CDER) asbaselines.
The collected scores were eval-uated in terms of system level correlation(how well each metric?s scores correlatewith WMT14 official manual ranking ofsystems) and in terms of segment levelcorrelation (how often a metric agrees withhumans in comparing two translations of aparticular sentence).1 IntroductionAutomatic machine translation metrics play a veryimportant role in the development of MT systemsand their evaluation.
There are many differentmetrics of diverse nature and one would like toassess their quality.
For this reason, the Met-rics Shared Task is held annually at the Workshopof Statistical Machine Translation1, starting withKoehn and Monz (2006) and following up to Bo-jar et al.
(2014).In this task, we asked metrics developers toscore the outputs of WMT14 Shared TranslationTask (Bojar et al., 2014).
We have collected thecomputed metrics?
scores and use them to evalu-ate quality of the metrics.The systems?
outputs, human judgements andevaluated metrics are described in Section 2.
Thequality of the metrics in terms of system level cor-relation is reported in Section 3.
Segment levelcorrelation with a detailed discussion and a slight1http://www.statmt.org/wmt13change in the calculation compared to the previousyear is reported in Section 4.2 DataWe used the translations of MT systems involvedin WMT14 Shared Translation Task together withreference translations as the test set for the Met-rics Task.
This dataset consists of 110 systems?outputs and 10 reference translations in 10 trans-lation directions (English from and into Czech,French, German, Hindi and Russian).
For most ofthe translation directions each system?s output andthe reference translation contain 3003 sentences.For more details please see the WMT14 overviewpaper (Bojar et al., 2014).2.1 Manual MT Quality JudgementsDuring the WMT14 Translation Task, a large scalemanual annotation was conducted to compare thesystems.
We used these collected human judge-ments for the evalution of the automatic metrics.The participants in the manual annotation wereasked to evaluate system outputs by ranking trans-lated sentences relative to each other.
For eachsource segment that was included in the procedure,the annotator was shown the outputs of five sys-tems to which he or she was supposed to assignranks.
Ties were allowed.These collected rank labels for each five-tupleof systems were then interpreted as 10 pairwisecomparisons of systems and used to assign eachsystem a score that reflects how high that systemwas usually ranked by the annotators.
Please seethe WMT14 overview paper for details on how thisscore is computed.
You can also find inter- andintra-annotator agreement estimates there.2.2 Participants of the Metrics Shared TaskTable 1 lists the participants of WMT14 SharedMetrics Task, along with their metrics.
We have293Metric ParticipantAPAC Hokkai-Gakuen University (Echizen?ya, 2014)BEER ILLC ?
University of Amsterdam (Stanojevic and Sima?an, 2014)RED-* Dublin City University (Wu and Yu, 2014)DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014)ELEXR University of Tehran (Mahmoudi et al., 2013)LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014)METEOR Carnegie Mellon University (Denkowski and Lavie, 2014)AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014)PARMESAN Charles University in Prague (Baran?c?
?kov?a, 2014)TBLEU Charles University in Prague (Libovick?y and Pecina, 2014)UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014)VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014)Table 1: Participants of WMT14 Metrics Shared Taskcollected 23 metrics from a total of 12 researchgroups.In addition to that we have computed the fol-lowing two groups of standard metrics as base-lines:?
Mteval.
The metrics BLEU (Papineniet al., 2002) and NIST (Dodding-ton, 2002) were computed using thescript mteval-v13a.pl2which isused in the OpenMT Evaluation Cam-paign and includes its own tokeniza-tion.
We run mteval with the flag--international-tokenizationsince it performs slightly better (Mach?a?cekand Bojar, 2013).?
Moses Scorer.
The metrics TER (Snoveret al., 2006), WER, PER and CDER (Leuschet al., 2006) were computed using the Mosesscorer which is used in Moses model opti-mization.
To tokenize the sentences we usedthe standard tokenizer script as available inMoses toolkit.We have normalized all metrics?
scores suchthat better translations get higher scores.3 System-Level Metric AnalysisWhile the Spearman?s ?
correlation coefficientwas used as the main measure of system-level met-rics?
quality in the past, we have decided to usePearson correlation coefficient as the main mea-sure this year.
At the end of this section we givereasons for this change.We use the following formula to compute thePearson?s r for each metric and translation direc-tion:2http://www.itl.nist.gov/iad/mig//tools/r =?ni=1(Hi??H)(Mi??M)??ni=1(Hi??H)2??ni=1(Mi?
?M)2(1)where H is the vector of human scores of all sys-tems translating in the given direction, M is thevector of the corresponding scores as predicted bythe given metric.
?H and?M are their means re-spectively.Since we have normalized all metrics such thatbetter translations get higher score, we considermetrics with values of Pearson?s r closer to 1 asbetter.You can find the system-level correlations fortranslations into English in Table 2 and for trans-lations out of English in Table 3.
Each row in thetables contains correlations of a metric in each ofthe examined translation directions.
The metricsare sorted by average Pearson correlation coeffi-cient across translation directions.
The best resultsin each direction are in bold.The reported empirical confidence intervals ofsystem level correlations were obtained throughbootstrap resampling of 1000 samples (confidencelevel of 95 %).As in previous years, a lot of metrics outper-formed BLEU in system level correlation.
Ininto-English directions, metric DISCOTK-PARTY-TUNED has the highest correlation in two lan-guage directions and it is also the best correlatedmetric on average according to both Pearson andSpearman?s coefficients.
The second best corre-lated metric on average (according to Pearson) isLAYERED which is also the single best metricin Hindi-to-English direction.
Metrics REDSYSand REDSYSSENT are quite unstable, they winin French-to-English and Czech-to-English direc-tions respectively but they perform very poorly in294other directions.Except METEOR, none of the participants tookpart in the last year metrics task.
We can there-fore compare current and last year results onlyfor METEOR and baseline metrics.
METEOR, thelast year winner, performs generally well in somedirections but it horribly suffers when evaluatingtranslations from non-Latin script (Russian and es-pecially Hindi).
For the baseline metrics the re-sults are quite similar across the years.
In bothyears BLEU performs best among baseline met-rics, closely followed by CDER.
NIST is in themiddle of the list in both years.
The remainingbaseline metrics TER, WER and PER performmuch worse.The results into German are markedly lowerand have broader confidence intervals than the re-sults in other directions.
This could be explainedby a very high number (18) of participating sys-tems of similar quality.
Both human judgementsand automatic metrics are negatively affected bythese circumstances.
To preserve the reliability ofoverall metrics?
performance across languages, wedecided to exclude English-to-German directionfrom the average Pearson and Spearman?s corre-lation coefficients.In other out-of-English directions, the best cor-related metric on average according to Pearson co-efficient is NIST, even though it does not win inany single direction.
CDER is the second best ac-cording to Pearson and the best metric accordingto Spearman?s.
Again it does not win in any singledirection.
The metrics PER and WER are quiteunstable.
Each of them wins in two directions butperforms very badly in others.Compared to the last year results, the order ofmetrics participating in both years is quite simi-lar: NIST and CDER performed very well bothyears, followed by BLEU.
The metrics TER andWER are again at the end of the list.
An interest-ing change is that PER perform much better thisyear.3.1 Reasons for Pearson correlationcoefficientIn the translation task, there are often similar sys-tems with human scores very close to each other.
Itcan therefore easily happen that even a good met-ric compares two similar systems differently fromhumans.
We believe that the penalty incurred bythe metric for such a swap should somehow reflectthat the systems were hard to separate.Since the Spearman?s ?
converts both humanand metric scores to ranks and therefore disregardsthe absolute differences in the scores, it does ex-actly what we feel is not fair.
The Pearson corre-lation coefficient does not suffer from this prob-lem.
We are aware of the fact that Pearson cor-relation coefficient also reflects whether the rela-tion between manual and automatic scores is lin-ear (as opposed to e.g.
quadratic).
We don?t thinkthis would be negatively affecting any of the met-rics since overall, the systems are of a comparablequality and the metrics are likely to behave lin-early in this small range of scores.Moreover, the general agreement to adopt Pear-son instead of Spearman?s correlation coefficientwas already apparent during the WMT12 work-shop.
This change just did not get through forWMT13.4 Segment-Level Metric AnalysisWe measure the quality of metrics?
segment-levelscores using Kendall?s ?
rank correlation coeffi-cient.
In this type of evaluation, a metric is ex-pected to predict the result of the manual pairwisecomparison of two systems.
Note that the goldentruth is obtained from a compact annotation of fivesystems at once, while an experiment with text-to-speech evaluation techniques by Vazquez-Alvarezand Huckvale (2002) suggests that a genuine pair-wise comparison is likely to lead to more stableresults.In the past, slightly different variations ofKendall?s ?
computation were used in the MetricsTasks.
Also some of the participants have noticeda problem with ties in the WMT13 method.
There-fore, we discuss several possible variants in detailin this paper.4.1 Notation for Kendall?s ?
computationThe basic formula for Kendall?s ?
is:?
=|Concordant| ?
|Discordant||Concordant|+ |Discordant|(2)where Concordant is the set of all human com-parisons for which a given metric suggests thesame order andDiscordant is the set of all humancomparisons for which a given metric disagrees.In the original Kendall?s ?
, comparisons with hu-man or metric ties are considered neither concor-dant nor discordant.
However in the past, Metrics295CorrelationcoefficientPearsonCorrelationCoefficientSpearman?sDirectionfr-ende-enhi-encs-enru-enAverageAverageConsideredSystems8139513DISCOTK-PARTY-TUNED.977?.009.943?.020.956?.007.975?.031.870?.022.944?.018.912?.043LAYERED.973?.009.893?.026.976?.006.941?.045.854?.023.927?.022.894?.047DISCOTK-PARTY.970?.010.921?.024.862?.015.983?.025.856?.023.918?.019.856?.046UPC-STOUT.968?.010.915?.025.898?.013.948?.040.837?.024.913?.022o.901?.045VERTA-W.959?.011.867?.029.920?.011.934?.050.848?.024.906?.025.868?.045VERTA-EQ.959?.011.854?.031.927?.010.938?.048.842?.024.904?.025.857?.046TBLEU.952?.012.832?.034.954?.007.957?.040.803?.027.900?.024.841?.056BLEUNRC.953?.012.823?.035.959?.007.946?.044.787?.028.894?.025o.855?.056BLEU.952?.012.832?.034.956?.007.909?.054.789?.027.888?.027.833?.058UPC-IPA.966?.010.895?.027.914?.010.824?.073.812?.026.882?.029o.858?.044CDER.954?.012.823?.034.826?.016.965?.035.802?.027.874?.025.807?.050APAC.963?.010.817?.034.790?.016.982?.026.816?.026.874?.022.807?.049REDSYS.981?.008.898?.026.676?.022.989?.021.814?.026.872?.021.786?.047REDSYSSENT.980?.008.910?.024.644?.023.993?.018.807?.027.867?.020.771?.043NIST.955?.011.811?.035.784?.016.983?.025.800?.027.867?.023o.824?.055DISCOTK-LIGHT.965?.011.935?.022.557?.025.954?.038.791?.027.840?.024.774?.046METEOR.975?.009.927?.022.457?.027.980?.029.805?.026.829?.023o.788?.046TER.952?.012.775?.038.618?.021.976?.031.809?.027.826?.026.746?.057WER.952?.012.762?.038.610?.021.974?.033.809?.027.821?.026.736?.058AMBER.948?.012.910?.026.506?.026.744?.095.797?.027.781?.037.728?.051PER.946?.013.867?.031.411?.025.883?.063.799?.028.781?.032.698?.047ELEXR.971?.009.857?.031.535?.026.945?.044?.404?.045.581?.031.652?.046Table2:System-levelcorrelationsofautomaticevaluationmetricsandtheofficialWMThumanscoreswhentranslatingintoEnglish.Thesymbol?o?indicateswheretheSpearman?s?averageisoutofsequencecomparedtothemainPearsonaverage.296CorrelationcoefficientPearsonCorrelationCoefficientSpearman?sDirectionen-fren-hien-csen-ruAverageen-deAverageConsideredSystems131210918(excl.en-de)NIST.941?.022.981?.006.985?.006.927?.012.959?.012.200?.046.850?.030CDER.949?.020.949?.010.982?.006.938?.011.955?.012.278?.045.840?.036AMBER.928?.023.990?.004.972?.008.926?.012.954?.012.241?.045.817?.041METEOR.941?.021.975?.007.976?.007.923?.013.954?.012.263?.045.806?.039BLEU.937?.022.973?.007.976?.007.915?.013.950?.012.216?.046o.809?.036PER.936?.023.931?.011.988?.005.941?.011.949?.013.190?.047o.823?.037APAC.950?.020.940?.011.973?.008.929?.012.948?.013.346?.044.799?.041TBLEU.932?.023.968?.008.973?.008.912?.013.946?.013.239?.046o.805?.039BLEUNRC.933?.022.971?.007.974?.008.901?.014.945?.013.205?.046o.809?.039ELEXR.885?.029.962?.009.979?.007.938?.011.941?.014.260?.044.768?.036TER.954?.019.829?.017.978?.007.931?.012.923?.014.324?.045.745?.035WER.960?.018.516?.026.976?.007.932?.011.846?.016.357?.045.696?.037PARMESANn/an/a.962?.009n/a.962?.009n/a.915?.048UPC-IPA.940?.021n/a.969?.008.921?.013.943?.014.285?.045.785?.050REDSYSSENT.941?.021n/an/an/a.941?.021.208?.045o.962?.038REDSYS.940?.021n/an/an/a.940?.021.208?.045.962?.038UPC-STOUT.940?.021n/a.938?.011.919?.013.933?.015.301?.044.713?.040Table3:System-levelcorrelationsofautomaticevaluationmetricsandtheofficialWMThumanscoreswhentranslatingoutofEnglish.Thesymbol?o?indicateswheretheSpearman?s?averageisoutofsequencecomparedtothemainPearsonaverage.297Tasks (Callison-Burch et al.
(2012) and earlier),comparisons with human ties were considered asdiscordant.To easily see which pairs are counted as concor-dant and which as discordant, we have developedthe following tabular notation.
This is for examplethe WMT12 method:MetricWMT12 < = >Human< 1 -1 -1= X X X> -1 -1 1Given such a matrix Ch,mwhere h,m ?
{<,=, >}3and a metric we compute the Kendall?s ?
thefollowing way:We insert each extracted human pairwise com-parison into exactly one of the nine sets Sh,mac-cording to human and metric ranks.
For examplethe set S<,>contains all comparisons where theleft-hand system was ranked better than right-handsystem by humans and it was ranked the other wayround by the metric in question.To compute the numerator of Kendall?s ?
, wetake the coefficients from the matrix Ch,m, usethem to multiply the sizes of the correspondingsets Sh,mand then sum them up.
We do not in-clude sets for which the value of Ch,mis X. Tocompute the denominator of Kendall?s ?
, we sim-ply sum the sizes of all the sets Sh,mexcept thosewhere Ch,m= X.
To define it formally:?
=?h,m?{<,=,>}Ch,m6=XCh,m|Sh,m|?h,m?
{<,=,>}Ch,m6=X|Sh,m|(3)4.2 Discussion on Kendall?s ?
computationIn 2013, we thought that metric ties should not bepenalized and we decided to excluded them likethe human ties.
We will denote this method asWMT13:MetricWMT13 < = >Human< 1 X -1= X X X> -1 X 1It turned out, however, that it was not a good idea:metrics could game the scoring by avoiding hard3Here the relation < always means ?is better than?
evenfor metrics where the better system receives a higher score.cases and assigning lots of ties.
A natural solutionis to count the metrics ties also in denominator toavoid the problem.
We will denote this variant asWMT14:MetricWMT14 < = >Human< 1 0 -1= X X X> -1 0 1The WMT14 variant does not allow for gamingthe scoring like the WMT13 variant does.
Com-pared to WMT12 method, WMT14 does not pe-nalize ties.We were also considering to get human ties in-volved.
The most natural variant would be the fol-lowing variant denoted as HTIES:MetricHTIES < = >Human< 1 0 -1= 0 1 0> -1 0 1Unfortunately this method allows for gaming thescoring as well.
The least risky choice for metricsin hard cases would be to assign a tie because itcannot worsen the Kendall?s ?
and there is quite ahigh chance that the human rank is also a tie.
Met-rics could be therefore tuned to predict ties oftenbut such metrics are not very useful.
For example,the simplistic metric which assigns the same scoreto all candidates (and therefore all pairs would betied by the metric) would get the score equal tothe proportion of ties in all human comparisons.
Itwould become one of the best performing metricsin WMT13 even though it is not informative at all.We have decided to use WMT14 variant as themain evaluation measure this year, however, weare also reporting average scores computed byother variants.4.3 Kendall?s ?
resultsThe final Kendall?s ?
results are shown in Table 4for directions into English and in Table 5 for di-rections out of English.
Each row in the tablescontains correlations of a metric in given direc-tions.
The metrics are sorted by average corre-lation across translation directions.
The highestcorrelation in each column is in bold.
The ta-bles also contain average Kendall?s ?
computed byother variants including the variant WMT13 usedlast year.
Metrics which did not compute scores inall directions are at the bottom of the tables.
The298Directionfr-ende-enhi-encs-enru-enAvgAveragesofothervariantsofKendall?s?Extracted-pairs2609025260209002113034460WMT12WMT13HTIESDISCOTK-PARTY-TUNED.433?.012.380?.013.434?.013.328?.015.355?.011.386?.013.386?.013.386?.013.306?.010BEER.417?.013.337?.014.438?.013.284?.016.333?.011.362?.013.358?.013.363?.013o.318?.011REDCOMBSENT.406?.012.338?.014.417?.013.284?.015.336?.011.356?.013.346?.013.360?.013.317?.011REDCOMBSYSSENT.408?.012.338?.014.416?.013.282?.014.336?.011.356?.013.346?.013.359?.013.316?.010METEOR.406?.012.334?.014.420?.013.282?.015.329?.010.354?.013.341?.013.359?.013o.317?.010REDSYSSENT.404?.012.338?.014.386?.014.283?.015.321?.010.346?.013.335?.013.350?.013.309?.010REDSENT.403?.012.336?.014.383?.014.283?.015.323?.011.345?.013.334?.013.349?.013.308?.010UPC-IPA.412?.012.340?.014.368?.014.274?.015.316?.011.342?.013o.340?.014.343?.014.300?.011UPC-STOUT.403?.012.345?.014.352?.014.275?.015.317?.011.338?.013.336?.013.339?.013.294?.011VERTA-W.399?.013.321?.015.386?.014.263?.015.315?.011.337?.014.320?.014o.342?.014o.304?.011VERTA-EQ.407?.013.315?.014.384?.013.263?.015.312?.011.336?.013o.323?.013.341?.013.302?.011DISCOTK-PARTY.395?.013.334?.014.362?.013.264?.016.305?.011.332?.013o.332?.013.332?.013.263?.011AMBER.367?.013.313?.014.362?.013.246?.016.294?.011.316?.013.302?.013.321?.014o.286?.011BLEUNRC.382?.013.272?.014.322?.014.226?.016.269?.011.294?.013.267?.014.303?.014.271?.011SENTBLEU.378?.013.271?.014.300?.013.213?.016.263?.011.285?.013.258?.014.293?.014.264?.011APAC.364?.012.271?.014.288?.014.198?.016.276?.011.279?.013.243?.014.290?.014.261?.011DISCOTK-LIGHT.311?.014.224?.015.238?.013.187?.016.209?.011.234?.014.234?.014.234?.014.184?.011DISCOTK-LIGHT-KOOL.005?.001.001?.000.000?.000.002?.001.001?.000.002?.001?.996?.001o.676?.256o.211?.005Table4:Segment-levelKendall?s?correlationsofautomaticevaluationmetricsandtheofficialWMThumanjudgementswhentranslatingintoEnglish.ThelastthreecolumnscontainaverageKendall?s?computedbyothervariants.Thesymbol?o?indicateswheretheaveragesofothervariantsareoutofsequencecomparedtotheWMT14variant.Directionen-fren-deen-hien-csen-ruAvgAveragesofothervariantsofKendall?s?Extracted-pairs3335054660281205590028960WMT12WMT13HTIESBEER.292?.012.268?.009.250?.013.344?.009.440?.013.319?.011.314?.011.320?.011.272?.009METEOR.280?.012.238?.009.264?.012.318?.009.427?.012.306?.011.283?.011.313?.011o.273?.008AMBER.264?.012.227?.009.286?.012.302?.009.397?.013.295?.011.269?.011.303?.011.266?.009BLEUNRC.261?.012.202?.009.234?.013.297?.009.391?.012.277?.011.235?.011.289?.011.256?.009APAC.253?.012.210?.008.203?.012.292?.009.388?.013.269?.011.217?.011.285?.011.252?.008SENTBLEU.256?.012.191?.009.227?.012.290?.009.381?.013.269?.011o.232?.011.280?.011.246?.009UPC-STOUT.279?.011.234?.008n/a.282?.009.425?.013.305?.011.300?.010.306?.011.256?.008UPC-IPA.264?.012.227?.009n/a.298?.009.426?.013.304?.011.292?.011o.308?.011o.259?.008REDSENT.293?.012.242?.009n/an/an/a.267?.010.246?.010.273?.011.257?.008REDCOMBSYSSENT.291?.012.244?.009n/an/an/a.267?.010o.249?.010.272?.010.256?.008REDCOMBSENT.290?.012.242?.009n/an/an/a.266?.010.248?.010.271?.011.256?.008REDSYSSENT.290?.012.239?.008n/an/an/a.264?.010.235?.010o.273?.010o.257?.008Table5:Segment-levelKendall?s?correlationsofautomaticevaluationmetricsandtheofficialWMThumanjudgementswhentranslatingoutofEnglish.ThelastthreecolumnscontainaverageKendall?s?computedbyothervariants.Thesymbol?o?indicateswheretheaveragesofothervariantsareoutofsequencecomparedtotheWMT14variant.299possible values of ?
range between -1 (a metric al-ways predicted a different order than humans did)and 1 (a metric always predicted the same order ashumans).
Metrics with a higher ?
are better.We also computed empirical confidence inter-vals of Kendall?s ?
using bootstrap resampling.We varied the ?golden truth?
by sampling fromhuman judgments.
We have generated 1000 newsets and report the average of the upper and lower2.5 % empirical bound, which corresponds to the95 % confidence interval.In directions into English (Table 4), thestrongest correlated segment-level metric on av-erage is DISCOTK-PARTY-TUNED followed byBEER.
Unlike the system level correlation, theresults are much more stable here.
DISCOTK-PARTY-TUNED has the highest correlation in 4 of5 language directions.
Generally, the ranking ofmetrics is almost the same in each direction.The only two metrics which also participatedin last year metrics task are METEOR and SENT-BLEU.
In both years, METEOR performed quitewell unlike SENTBLEU which was outperformedby most of the metrics.The metric DISCOTK-LIGHT-KOOL is worthmentioning.
It is deliberately designed to assignthe same score for all systems for most of thesegments.
It obtained scores very close to zero(i.e.
totally uninformative) in WMT14 variant.
InWMT13 thought it reached the highest score.In directions out of English (Table 5), the met-ric with highest correlation on average across alldirections is BEER, followed by METEOR.5 ConclusionIn this paper, we summarized the results of theWMT14 Metrics Shared Task, which assesses thequality of various automatic machine translationmetrics.
As in previous years, human judgementscollected in WMT14 serve as the golden truth andwe check how well the metrics predict the judge-ments at the level of individual sentences as wellas at the level of the whole test set (system-level).This year, neither the system-level nor thesegment-level scores are directly comparable tothe previous years.
The system-level scores are af-fected by the change of the underlying interpreta-tion of the collected judgements in the main trans-lation task evaluation as well as our choice of Pear-son coefficient instead of Spearman?s rank corre-lation.
The segment-level scores are affected bythe different handling of ties this year.
Despitesomewhat sacrificing the year-to-year comparabil-ity, we believe all changes are towards a fairerevaluation and thus better in the long term.As in previous years, segment-level correlationsare much lower than system-level ones, reachingat most Kendall?s ?
of 0.45 for the best performingmetric in its best language pair.
So there is quitesome research work to be done.
We are happyto see that many new metrics emerged this year,which also underlines the importance of the Met-rics Shared Task.AcknowledgementsThis work was supported by the grant FP7-ICT-2011-7-288487 (MosesCore) of the EuropeanUnion.
We are grateful to Jacob Devlin and alsoPreslav Nakov for pointing out the issue of reward-ing ties and for further discussion.ReferencesBaran?c?
?kov?a, P. (2014).
Parmesan: ImprovingMeteor by More Fine-grained Paraphrasing.
InProceedings of the Ninth Workshop on Statisti-cal Machine Translation, Baltimore, USA.
As-sociation for Computational Linguistics.Bojar, O., Buck, C., Federmann, C., Haddow, B.,Koehn, P., Leveling, J., Monz, C., Pecina, P.,Post, M., Saint-Amand, H., Soricut, R., Specia,L., and Tamchyna, A.
(2014).
Findings of the2014 workshop on statistical machine transla-tion.
In Proceedings of the Ninth Workshop onStatistical Machine Translation.Callison-Burch, C., Koehn, P., Monz, C., Post, M.,Soricut, R., and Specia, L. (2012).
Findings ofthe 2012 workshop on statistical machine trans-lation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 10?51, Montr?eal, Canada.
Association for Compu-tational Linguistics.Chen, B. and Cherry, C. (2014).
A System-atic Comparison of Smoothing Techniques forSentence-Level BLEU.
In Proceedings of theNinth Workshop on Statistical Machine Trans-lation, Baltimore, USA.
Association for Com-putational Linguistics.Comelles, E. and Atserias, J.
(2014).
VERTa par-ticipation in the WMT14 Metrics Task.
In Pro-ceedings of the Ninth Workshop on Statistical300Machine Translation, Baltimore, USA.
Associ-ation for Computational Linguistics.Denkowski, M. and Lavie, A.
(2014).
Meteor Uni-versal: Language Specific Translation Evalua-tion for Any Target Language.
In Proceedingsof the Ninth Workshop on Statistical MachineTranslation, Baltimore, USA.
Association forComputational Linguistics.Doddington, G. (2002).
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human Lan-guage Technology Research, HLT ?02, pages138?145, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Echizen?ya, H. (2014).
Application of Prize basedon Sentence Length in Chunk-based AutomaticEvaluation of Machine Translation.
In Proceed-ings of the Ninth Workshop on Statistical Ma-chine Translation, Baltimore, USA.
Associationfor Computational Linguistics.Gautam, S. and Bhattacharyya, P. (2014).
LAY-ERED: Description of Metric for MachineTranslation Evaluation in WMT14 MetricsTask.
In Proceedings of the Ninth Workshopon Statistical Machine Translation, Baltimore,USA.
Association for Computational Linguis-tics.Gonz`alez, M., Barr?on-Cede?no, A., and M`arquez,L.
(2014).
IPA and STOUT: Leveraging Lin-guistic and Source-based Features for MachineTranslation Evaluation.
In Proceedings of theNinth Workshop on Statistical Machine Trans-lation, Baltimore, USA.
Association for Com-putational Linguistics.Guzman, F., Joty, S., M`arquez, L., and Nakov, P.(2014).
DiscoTK: Using Discourse Structurefor Machine Translation Evaluation.
In Pro-ceedings of the Ninth Workshop on StatisticalMachine Translation, Baltimore, USA.
Associ-ation for Computational Linguistics.Koehn, P. and Monz, C. (2006).
Manual and au-tomatic evaluation of machine translation be-tween european languages.
In Proceedings onthe Workshop on Statistical Machine Transla-tion, pages 102?121, New York City.
Associa-tion for Computational Linguistics.Leusch, G., Ueffing, N., and Ney, H. (2006).CDER: Efficient MT Evaluation Using BlockMovements.
In In Proceedings of EACL, pages241?248.Libovick?y, J. and Pecina, P. (2014).
TolerantBLEU: a Submission to the WMT14 MetricsTask.
In Proceedings of the Ninth Workshopon Statistical Machine Translation, Baltimore,USA.
Association for Computational Linguis-tics.Mach?a?cek, M. and Bojar, O.
(2013).
Results of theWMT13 Metrics Shared Task.
In Proceedingsof the Eighth Workshop on Statistical MachineTranslation, pages 45?51, Sofia, Bulgaria.
As-sociation for Computational Linguistics.Mahmoudi, A., Faili, H., Dehghan, M., andMaleki, J.
(2013).
ELEXR: Automatic Evalu-ation of Machine Translation Using Lexical Re-lationships.
In Castro, F., Gelbukh, A., andGonz?alez, M., editors, Advances in Artificial In-telligence and Its Applications, volume 8265 ofLecture Notes in Computer Science, pages 394?405.
Springer Berlin Heidelberg.Papineni, K., Roukos, S., Ward, T., and jing Zhu,W.
(2002).
BLEU: a method for automatic eval-uation of machine translation.
pages 311?318.Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,and Makhoul, J.
(2006).
A study of translationedit rate with targeted human annotation.
In InProceedings of Association for Machine Trans-lation in the Americas, pages 223?231.Stanojevic, M. and Sima?an, K. (2014).
BEER:A Smooth Sentence Level Evaluation Metricwith Rich Ingredients.
In Proceedings of theNinth Workshop on Statistical Machine Trans-lation, Baltimore, USA.
Association for Com-putational Linguistics.Vazquez-Alvarez, Y. and Huckvale, M. (2002).The reliability of the itu-t p.85 standard forthe evaluation of text-to-speech systems.
InHansen, J. H. L. and Pellom, B. L., editors, IN-TERSPEECH.
ISCA.Wu, X. and Yu, H. (2014).
RED, The DCU Sub-mission of Metrics Tasks.
In Proceedings of theNinth Workshop on Statistical Machine Trans-lation, Baltimore, USA.
Association for Com-putational Linguistics.301
