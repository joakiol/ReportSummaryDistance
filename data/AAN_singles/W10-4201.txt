Comparing Rating Scales and Preference Judgementsin Language EvaluationAnja Belz Eric KowNatural Language Technology GroupSchool of Computing, Mathematical and Information SciencesUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukAbstractRating-scale evaluations are common inNLP, but are problematic for a range ofreasons, e.g.
they can be unintuitive forevaluators, inter-evaluator agreement andself-consistency tend to be low, and theparametric statistics commonly applied tothe results are not generally consideredappropriate for ordinal data.
In this pa-per, we compare rating scales with an al-ternative evaluation paradigm, preference-strength judgement experiments (PJEs),where evaluators have the simpler task ofdeciding which of two texts is better interms of a given quality criterion.
Wepresent three pairs of evaluation experi-ments assessing text fluency and clarityfor different data sets, where one of eachpair of experiments is a rating-scale ex-periment, and the other is a PJE.
We findthe PJE versions of the experiments havebetter evaluator self-consistency and inter-evaluator agreement, and a larger propor-tion of variation accounted for by systemdifferences, resulting in a larger number ofsignificant differences being found.1 IntroductionRating-scale evaluations, where human evaluatorsassess system outputs by selecting a score on a dis-crete scale, are the most common form of human-assessed evaluation in NLP.
Results are typicallypresented in rank tables of means for each systemaccompanied by means-based measures of statisti-cal significance of the differences between systemscores.NLP system evaluation tends to involve sets ofsystems, rather than single ones (evaluations tendto at least incorporate a baseline or, more rarely, atopline system).
The aim of system evaluation isto gain some insight into which systems are bet-ter than which others, in other words, the aim isinherently relative.
Yet NLP system evaluation ex-periments have generally preferred rating scale ex-periments where evaluators assess each system?squality in isolation, in absolute terms.Such rating scales are not very intuitive to use;deciding whether a text deserves a 5, a 4 or a 3etc.
can be difficult.
Furthermore, evaluators mayascribe different meanings to scores and the dis-tances between them.
Individual evaluators havedifferent tendencies in using rating scales, e.g.what is known as ?end-aversion?
tendency wherecertain individuals tend to stay away from the ex-treme ends of scales; other examples are positiveskew and acquiescence bias, where individualsmake disproportionately many positive or agree-ing judgements; see e.g.
Choi and Pak, (2005).It is not surprising then that stable averages ofquality judgements, let alne high levels of agree-ment, are hard to achieve, as has been observed forMT (Turian et al, 2003; Lin and Och, 2004), textsummarisation (Trang Dang, 2006), and languagegeneration (Belz and Reiter, 2006).
It has evenbeen demonstrated that increasing the number ofevaluators and/or data can have no stabilising ef-fect at all on means (DUC literature).The result of a rating scale experiment is ordi-nal data (sets of scores selected from the discreterating scale).
The means-based ranks and statisti-cal significance tests that are commonly presentedwith the results of RSEs are not generally consid-ered appropriate for ordinal data in the statisticsliterature (Siegel, 1957).
At the least, ?a test on themeans imposes the requirement that the measuresmust be additive, i.e.
numerical?
(Siegel, 1957, p.14).
Parametric statistics are more powerful thannon-parametric alternatives, because they make anumber of strong assumptions (including that thedata is numerical).
If the assumptions are violatedthen the risks is that the significance of results isoverestimated.In this paper we explore an alternative evalua-tion paradigm, Preference-strength Judgement Ex-periments (PJEs).
Binary preference judgementshave been used in NLP system evaluation (Reiter etal., 2005), but to our knowledge this is the first sys-tematic investigation of preference-strength judge-ments where evaluators express, in addition totheir preference (which system do you prefer?
),also the strength of their preference (how stronglydo you prefer the system you prefer?).
It seemsintuitively convincing that it should be easier todecide which of two texts is clearer than to de-cide whether a text?s clarity deserves a 1, 2, 3, 4 or5.
However, it is less clear whether evaluators arealso able to express the strength of their preferencein a consistent fashion, resulting not only in goodself-consistency, but also in good agreement withother evaluators.We present three pairs of directly comparableRSE and PJE evaluations, and investigate how theycompare in terms of (i) the amount of variation ac-counted for by differences between systems (themore the better), relative to the amount of varia-tion accounted for by other factors such as evalu-ator and arbitrary text properties (the less the bet-ter); (ii) inter-evaluator agreement, (iii) evaluatorself-consistency, (iv) the number of significant dif-ferences identified, and (v) experimental cost.2 Overview of ExperimentsIn the following three sections we present the de-sign and results of three pairs of evaluations.
Eachpair consists of a rating-scale experiment (RSE)and a preference-strength judgement experiment(PJE) that differ only in the rating method they em-ploy (relative ratings in the PJE and absolute rat-ings in the RSE).1 In other words, they involve thesame set of system outputs, the same instructionsand method of presentating system outputs.
Eachpair is for a different data domain and system task,the first for generating chains of references to peo-ple in Wikipedia articles (Section 3); the secondfor weather forecast text generation (Section 4);and the third for generating descriptions of imagesof furniture and faces (Section 5).All experiments use a Repeated Latin Squares1We are currently preparing an open-source release of theRSE/PJE toolkit we have developed for implementing the ex-periments described in this paper which automatically gen-erates an experiment, including webpages, given some user-specified parameters and the data to be evaluated.Figure 1: Standardised 1?5 rating scale represen-tation for Fluency and Clarity criteria.design which ensures that each subject sees thesame number of outputs from each system andfor each test set item.
Following detailed instruc-tions, subjects first do 2 or 3 practice examples,followed by the texts to be evaluated, in an orderrandomised for each subject.
Subjects carry outthe evaluation over the internet, at a time and placeof their choosing.
They are allowed to interruptand resume (but are discouraged from doing so).There are subtle differences between the threeexperiment pairs, and for ease of comparison weprovide an overview of the six experiments we in-vestigate in this paper in Table 1.
Each of the as-pects of experimental design and execution shownin this table is explained and described in more de-tail in the relevant subsection below, but some ofthe important differences are highlighted here.In GREC-NEG PJE, each system is comparedwith only one other comparisor system (a human-authored topline), whereas in the other two PJE ex-periments, each system is compared with all othersystems for each test data set item.In the two versions of the METEO evaluation,evaluators were not drawn from the same cohort ofpeople, whereas in the other two evaluation pairsthey were drawn from the same cohort.
GREC-NEG RSE and METEO RSE used radio buttons (asshown in Figure 1) as the rating-scale evaluationmechanism whereas in TUNA RSE it was an un-marked slider bar.
While slightly different nameswere used for the evaluation criteria in two ofthe evaluation pairs, Fluency/Readability were ex-plained in very similar terms (does it read well?
),and Adequacy in TUNA was explained in terms ofclarity of reference (is it clear which entity the de-scription refers to?
), so there are in fact just twoevaluation criteria (albeit with different names).Where we use preference-strength judgements,Data set GREC-NEG METEO TUNAType RSE PJE RSE PJE RSE PJECriteria names Fluency, Clarity Readability, Clarity Fluency, AdequacyEvaluator type linguistics students uni staff ling stud linguistics studentsNum evaluators 10 10 22 22 8 28Comparisor(s) ?
human topline ?
all systems ?
all systemsTest set size 30 22 112N trials 300 300 484 1210 896 3136Rating tool radio buttons slider radio buttons slider slider bar sliderRange 1?5 ?10.0.. + 10.0 1?7 ?50.0.. + 50.0 0?100 ?50.0.. + 50.0Numbers visible?
yes no yes no no noTable 1: Overview of experiments with details of design and execution.
(Comparisor(s) = the othersystems against which each system is evaluated.
)the evaluation mechanism is implemented usingslider bars as shown at the bottom of Figure 2which map to a scale ?X.. + X.
The evalua-tor?s task is to express their preference in terms ofeach quality criterion by moving the pointers onthe sliders.
Moving the pointer to the left meansexpressing a preference for the text on the left,moving it to the right means preferring the text onthe right; the further to the left/right the slider ismoved, the stronger the preference.
It was not ev-ident to the evaluators that sliders were associatedwith numerical values.
Slider pointers started outin the middle of the scale (the position correspond-ing to no preference).
If they wanted to leave thepointer in the middle (i.e.
if they had no prefer-ence for either of the two texts), evaluators had tocheck a box to confirm their rating (to avoid evalu-ators accidentally not rating a text and leaving thepointer in the default position).3 GREC-NEG RSE/PJE: Named entityreference chains3.1 Data and generic designIn our first pair of experiments we used system andhuman outputs for the GREC-NEG task of selectingreferring expressions for people in discourse con-text.
The GREC-NEG data2 consists of introductionsections from Wikipedia articles about people inwhich all mentions of people have been annotatedby marking up the word strings that function asreferential expressions (REs) and annotating themwith coreference information as well as syntacticand semantic features.
The following is an exam-ple of an annotated RE from the corpus:<REF ENTITY="0" MENTION="1" SEMCAT="person" SYNCAT="np"SYNFUNC="subj"><REFEX ENTITY="0" REG08-TYPE="name"2The GREC-NEG data and documen-tation is available for download fromhttp://www.nltg.brighton.ac.uk/home/Anja.BelzCASE="plain">Sir Alexander Fleming</REFEX> </REF>(6 August 1881 - 11 March 1955) was a Scottish biol-ogist and pharmacologist.This data was used in the GREC-NEG?09shared-task competition (Belz et al, 2009), wherethe task was to create systems which automaticallyselect suitable REs for all references to all personentities in a text.The evaluation experiments use Clarity and Flu-ency as quality criteria which were explained inthe introduction as follows (the wording of the firstis from DUC):1.
Referential Clarity: It should be easy to identify whothe referring expressions are referring to.
If a personis mentioned, it should be clear what their role in thestory is.
So, a reference would be unclear if a personis referenced, but their identity or relation to the storyremains unclear.2.
Fluency: A referring expression should ?read well?,i.e.
it should be written in good, clear English, and theuse of titles and names should seem natural.
Note thatthe Fluency criterion is independent of the ReferentialClarity criterion: a reference can be perfectly clear, yetnot be fluent.The evaluations involved outputs for 30 randomlyselected items from the test set from 5 of the 6systems which participated in GREC-NEG?10, thefour baseline systems developed by the organisers,and the original corpus texts (10 systems in total).3.2 Preference judgement experimentThe human-assessed intrinsic evaluation inGREC?09 was designed as a preference-judgementtest where subjects expressed their preference, interms of the two criteria, for either the originalWikipedia text (human-authored ?topline?)
orthe version of it with system-selected referringexpressions in it.
There were three 10x10 LatinSquares, and a total of 300 trials (with twojudgements in each, one for Fluency and one forClarity) in this evaluation.
The subjects were 10Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.native speakers of English recruited from cohortsof students currently completing a linguistics-related degree at Kings College London andUniversity College London.Figure 2 shows what subjects saw during theevaluation of an individual text pair.
The place(left/right) of the original Wikipedia article wasrandomly determined for each individual evalua-tion of a text pair.
People references are high-lighted in yellow/orange, those that are identicalin both texts are yellow, those that are different areorange.3 The sliders are the standardised designdescribed in the preceding section.3.3 Rating scale experimentOur new experiment used our standardised radiobutton design for a 1?5 rating scale as shown inFigure 1.
We used the same Latin Squares designas for the PJE version, and recruited 10 differentevaluators from the same student cohorts at KingsCollege London and University College London.Evaluators saw just one text in each trial, with thepeople references highlighted in yellow.3.4 Results and comparative analysisMeasures comparing the results from the two ver-sions of the GREC-NEG evaluation are shown inTable 2.
The first row for each experiment type3When viewed in black and white, the orange highlightsare the slighly darker ones.Type Measure Clarity FluencyRSE F(9,290) 10.975** 35.998**N sig diffs 19/45 27/45K?s W (inter) .543** .760**avg W (intra) .5275 .7192( Text F(29,270) 2.512** 1.825** )( Evaluator F(9,290) 3.998** .630 )PJE F(9,290) 29.539** 26.596**N sig diffs 26/45 24/45K?s W (inter) .717** .725**avg W (intra) .6909 .7125( Text F(29,270) .910 1.237 )( Evaluator F(9,290) 1.237 4.145** )Table 2: GREC-NEG RSE/PJE: Results of analy-ses looking at effect of System.shows the F ratio as determined by a one-wayANOVA with the evaluation criterion in questionas the dependent variable and System as the fac-tor.
F is the ratio of between-groups variabilityover within-group (or residual) variability, i.e.
thelarger the value of F, the more of the variability ob-served in the data is accounted for by the groupingfactor, here System, relative to what variability re-mains within the groups.The second row shows the number of signifi-cant differences out of the possible total, as deter-mined by a Tukey?s HSD analysis.
Kendall?s W(interpretable as a coefficient of concordance) isa commonly used measure of the agreement be-tween judges and is based on mean rank.
It rangesfrom 0 to 1, and the closer to 1 it is the greater theagreement.
The fourth row (K?s W, inter) showsthe standard W measure, estimating the degree towhich the evaluators agreed.
The 5th row (K?s W,intra) shows the average W for repeated ratingsby the same judge, i.e.
it is a measure of the av-erage self-consistency achieved by the evaluators.Finally, in the last two rows we give F-ratios forText (test data set item) and Evaluator, estimatingthe effect these two have independently of System.The F ratios and numbers of significant differ-ences are very similar in the PJE version, but verydissimilar in the RSE version of this experiment.For Fluency, F is greater in the RSE version thanin the PJE version where there appear to be big-ger differences between scores assigned by evalua-tors.
However, Kendall?s W shows that in terms ofmean score ranks, the evaluators agreed to a simi-lar extent in both experiment versions.Clarity in the RSE version has lower valuesacross the board than the rest of Table 2: it ac-counts for less of the variation, has fewer signifi-cant differences and lower levels of inter-evaluatoragreement and self-consistency.
If the results fromthe PJE version were not also available one mightbe inclined to conclude that there was not as muchdifference between systems in terms of Clarity asthere was in terms of Fluency.
However, becauseFluency and Clarity have a similarly strong effectin GREC-NEG PJE, it looks instead as though theevaluators found it harder to apply the Clarity cri-terion in GREC-NEG RSE than Fluency in GREC-NEG RSE, and than Clarity in GREC-NEG PJE.One way of interpreting this is that it is possibleto achieve the same good levels of inter-evaluatorand intra-evaluator variation for the Clarity crite-rion as for Fluency (both as defined and appliedwithin the context of this specific experiment), andthat it is therefore worrying that the RSE versiondoes not achieve it.4 METEO RSE/PJE: Weather forecasts4.1 DataOur second pair of evaluations used the Prodigy-METEO4 version (Belz, 2009) of the SUMTIME-METEO corpus (Sripada et al, 2002) which con-tains system outputs and the pairs of wind forecast4The Prodigy-METEO corpus is freely available here:http://www.nltg.brighton.ac.uk/home/Anja.Belztexts and wind data the systems were trained on,e.g.
:Data: 1 SSW 16 20 - - 0600 2 SSE - - -- NOTIME 3 VAR 04 08 - - 2400Text: SSW 16-20 GRADUALLY BACKING SSETHEN FALLING VARIABLE 4-8 BYLATE EVENINGThe input vector is a sequence of 7-tuples?i, d, smin, smax, gmin, gmax, t?
where i is the tu-ple?s ID, d is the wind direction, smin and smax arethe minimum and maximum wind speeds, gminand gmax are the minimum and maximum gustspeeds, and t is a time stamp (indicating for whattime of the day the data is valid).
The wind fore-cast texts were taken from comprehensive mar-itime weather forecasts produced by the profes-sional meteorologists employed by a commercialweather forecasting company for clients who runoffshore oilrigs.There were two evaluation criteria; Clarity wasexplained as indicating how understandable a fore-cast was, and Readability as indicating how fluentand readable it was.
The experiment involved 22forecast dates and outputs from the 10 systems de-scribed in (Belz and Kow, 2009) (also included inthe corpus release) for those dates (as well as thecorresponding forecasts in the corpus) in the eval-uation, i.e.
a total of 242 forecast texts.4.2 Rating scale experimentWe used the results of a previous experiment (Belzand Kow, 2009) in which participants were askedto rate forecast texts for Clarity and Readability,each on a scale of 1?7.The 22 participants were all University ofBrighton staff whose first language was Englishand who had no experience of NLP.
While ear-lier experiments used master mariners as well aslay-people in a similar evaluation (Belz and Re-iter, 2006), these experiments also demonstratedthat the correlation between the ratings by expertevaluators and lay-people is very strong in the ME-TEO domain (Pearson?s r = 0.845).We used a single 22 (evaluators) by 22 (test dataitems) Latin Square; there were 484 trials in thisexperiment.4.3 Preference judgement experimentOur new experiment used our standardised pref-erence strength sliders (bottom of Figure 2).
Werecruited 22 different evaluators from among stu-dents currently completing or recently havingType Measure Clarity ReadabilityRSE F(10,473) 23.507** 24.351**N sig diffs 24/55 23/55K?s W .497** .533**( Text F(21,462) 1.467 1.961** )( Evaluator F(21,462) 4.832** 4.824** )PJE F(10,1865) 45.081** 41.318**N sig diffs 34/55 32/55K?s W .626** .542**( Text F(21,916) 1.436 1.573 )( Evaluator F(21,921) .794 1.057 )Table 3: METEO RSE/PJE: Results of analyseslooking at effect of System.completed a linguistics-related degree at Oxford,KCL, UCL, Sussex and Brighton.We had at our disposal 11 METEO systems, sothere were(112)= 55 system combinations to eval-uate on the 22 test data items.
We decided on adesign of ten 11 ?
11 Latin Squares to accommo-date the 55 system pairings, so there was a total of1210 trials in this experiment.4.4 Results and comparative analysisTable 3 shows the same types of comparative mea-sures as in the previous section.
Note that theself-consistency measure is missing, because forMETEO-PJE we do not have multiple scores for thesame pair of systems by the same evaluator.For the METEO task, the relative amount vari-ation in Clarity and Radability accounted for bySystem is similar in the RSE, and again similar inthe PJE.
However, F ratios and numbers of signifi-cant differences found are higher in the latter thanin the RSE.
The inter-evaluator agreement measurealso has higher values for both Clarity and Read-ability in the PJE, although the difference is muchmore pronounced in the case of Clarity.In the RSE version, Evaluator has a small butsignificant effect on both Clarity and Readability,which disappears in the PJE version.
Similarly, asmall effect of Text (date of weather forecast inthis data set) on Fluency in the RSE version disap-pears in the PJE version.5 RSE/PJE Pair 2: Descriptions offurniture items and faces5.1 Data and generic designIn our third pair of evaluations, we used the sys-tem outputs from the TUNA?09 shared-task com-petition (Gatt et al, 2009).5 The TUNA data isa collection of images of domain entities pairedwith descriptions of entities.
Each pair consists ofseven entity images where one is highlighted (by ared box surrounding it), paired with a descriptionof the highlighted entity, e.g.
:the small blue fanThe descriptions were collected in an online ex-periment with anonymous participants, and thenannotated for semantic content.
In TUNA?09, thetask for participating systems was to generate de-scriptions of the highlighted entities given seman-tic representations of all seven entities.
In the eval-uation experiments, evaluators were asked to givetwo ratings in answer to the following questions(the first for Adequacy, the second for Fluency):1.
How clear is this description?
Try to imagine someonewho could see the same grid with the same pictures, butdidn?t know which of the pictures was the target.
Howeasily would they be able to find it, based on the phrasegiven?2.
How fluent is this description?
Here your task is tojudge how well the phrase reads.
Is it good, clear En-glish?Participants were shown a system output, to-gether with its corresponding domain, displayedas the set of corresponding images on the screen.The intended (target) referent was highlighted bya red frame surrounding it on the screen.Following detailed instructions, subjects didtwo practice examples, followed by the 112 testitems in random order.There were 8 ?systems?
in the TUNA evalua-tions: the descriptions produced by the 6 systemsand two sets of humans-authored descriptions.5.2 Rating scale experimentThe rating scale experiment that was part of theTUNA?09 evaluations had a design of fourteen 8 ?8 squares, and a total of 896 trials.5The TUNA?09 data and documen-tation is available for download fromhttp://www.nltg.brighton.ac.uk/home/Anja.BelzType Measure Adequacy FluencyRSE F(7,888) 6.371** 17.207**N sig diffs 7/28 15/28K?s W .471** .676**( Text1 F(111,784) 1.519** 1.091 )( Text2 F(14,881) 8.992** 4.694** )( Evaluator F(7,888) 13.136** 17.479** )PJE F(7,6264) 46.503** 89.236**N sig diffs 19/28 22/28K?s W .573** .654**( Text1 F(111,3024) .746 .921 )( Text2 F(14,3121) .856 .853 )( Evaluator F(27,3108) 1.3 1.638* )Table 4: TUNA RSE/PJE: Results of analyseslooking at effect of System.Subjects were asked to give their judgments forClarity and Fluency for each item by manipulatinga slider.
The slider pointer was placed in the centerat the beginning of each trial.
The position of theslider selected by the subject mapped to an integervalue between 1 and 100.
However, the scale wasnot visible to participants who knew only that oneend of the scale corresponded to the worst possiblescore and the opposite end to the best.Eight native speakers of English were recruitedfor this experiment from among post-graduatestudents currently doing a Masters degree in alinguistics-related subject at UCL, Sussex andBrighton universities.5.3 Preference judgement experimentOur new experiment used our standardised pref-erence strength sliders (bottom of Figure 2).
Toaccommodate all pairwise comparisons as well asall test set items, we used a design of four 28?
28 Latin Squares, and recruited 28 evaluatorsfrom among students currently completing, or re-cently having completed, a degree in a linguistics-related subject at Oxford, KCL, UCL, Sussex andBrighton universities.
There were 3,136 trials inthis version of the experiment.5.4 Results and comparative analysisTable 4 shows the same measures as we reportedfor the other two experiment pairs above.
Thepicture is somewhat similar in that the measureshave better values for PJE version except for theinter-evaluator agreement (Kendall?s W) for Flu-ency which is slightly higher for the RSE version.For the TUNA dataset, we look at two Text factors.Text2 refers to different sets of entities used in tri-als; there are 15 different ones.
Text1 refers to setsof entities and their specific distribution over thevisual display grid in trials (see the figure in Sec-tion 5.1); there are 112 different combinations ofentity set and grid locations.The most striking aspect of the results in Table 4is the effect of Evaluator in the RSE version whichappears to account for more variability in the dataeven than System (relative to other factors).
Infact, in the case of Adequacy, even Text2 causesmore variation than System.
In contrast, in the PJEversion, by far the biggest cause of variability isSystem (for both criteria), and the F ratios for Textand Evaluators are not significant except for Eval-uator on Fluency (weakly significant at .05).On the face of it, the variation between evalua-tors in the RSE version as evidenced by the F ra-tio is worrying.
However, Kendall?s W shows thatin terms of mean rank, evaluators actually agreedsimilarly well on Fluency in both RSE and PJE.The F measure is based on mean scores whereas Wis based on mean score ranks, so there was morevariation in the absolute scores than in the ranks.The reason is likely to be connected to the wayratings were expressed by evaluators in the TUNA-RSE experiment: recall that evaluators had the taskof moving the pointer to the place on the sliderbar that they felt corresponded to the quality oftext being evaluated.
As no numbers were visi-ble, the only information evaluators had to go onwas which was the ?worse?
end and which was the?better?
end of the slider.
It seems that differentevaluators used this evaluation tool in very differ-ent ways (accounting for the variation in absolutescores), but were able to apply their way of usingthe tool reasonably consistently to different texts(so that they were able to achieve reasonably goodagreement with the other evaluators in terms ofrelative scores).6 DiscussionWe have looked at a range of aspects of evalu-ation experiments: the effect of the factors Sys-tem, Text and Evaluator on evaluation scores; thenumber of significant differences between systemsfound; self-consistency; and inter-evaluator agree-ment (as described by F ratios obtained in one-wayANOVAs for Evaluator, as well as by Kendall?s Wmeasuring inter-evaluator agreement).The results are unambiguous as far as theClarity criterion (called Adequacy in TUNA) isconcerned: in all three experiment pairs, thepreference-strength judgement (PSE) version hada greater effect of System, a smaller effect ofText and Evaluator, more significant pairwise dif-ferences, better inter-evaluator agreement, and(where we were able to measure it) better self-consistency.The same is true for Readability in METEO andFluency in TUNA, in the latter case except for Wwhich is slightly lower in TUNA-PJE than TUNA-RSE.
However, Readability in GREC-NEG bucksthe trend: here, all measures are worse in thePJE version than in the RSE version (although forthe W measures, the differences are small).
Partof the reason for this may be that in GREC-NEGPJE each system was only compared to one sin-gle other ?system?, the (human-authored) originalWikipedia texts.If we see less effect of Clarity than of Fluencyin an experiment (as in GREC-NEG RSE and TUNARSE), then we might want to conclude that sys-tems differed less in terms of Clarity than in termsof Fluency.
However, the real explanation maybe that evaluators simply found it harder to applythe Clarity criterion than the Fluency criterion ina given evaluation set-up.
The fact that the differ-ence in effect between Fluency and Clarity virtu-ally disappears in GREC-NEG PJE makes this themore likely explanation at least for the GREC-NEGevaluations.Parametric statistics are more powerful thannon-parametric ones because of the strong as-sumptions they make about the nature of the data.Roughly speaking, they are more likely to uncoversignificant differences.
Where the assumptions areviolated, the risk is that significance is overesti-mated (the likelihood that null hypotheses are in-correctly rejected increases).
One might considerusing a slider mapping to a continuous scale in-stead of a multiple-choice rating form in order toovercome this problem, but the evidence from theTUNA RSE evaluation appears to be that this canresult in unacceptably large variation in how indi-vidual evaluators apply the scale to assign absolutescores.What seems to make the difference in terms ofease of application of evaluation criteria and re-duction of undesirable effects is not the use of con-tinuous scales (as e.g.
implemented in slider bars),but the comparative element, where pairs of sys-tems are compared and one is selected as better interms of a given criterion than the other.It makes sense intuitively that deciding whichof two texts is clearer should be an easier task thandeciding whether a system is a 5, 4, 3 or 1 in termsof its clarity.
PJEs enabled evaluators to apply theClarity criterion to determine ranks more consis-tently in all three experiment pairs.However, it was an open question whether eval-uators would also be able to express the strengthof their preference consistently.
From the resultswe report here it seems clear that this is indeed thecase: the System F ratios which look at absolutescores (in the PJEs quantifying the strength of apreference) are higher, and the Evaluator F ratioslower, in all but one of the experiments.While there were the same number of trialsin the two GREC-NEG evaluations, there were2.5 times as many trials in METEO-PJE than inMETEO-RSE, and 3.5 times as many trials inTUNA-PJE than in TUNA-RSE.
The increase in tri-als is counter-balanced to some extent by the factthat evaluators tend to give relative judgementsfar more quickly than absolute judgements, butclearly there is an increase in cost associated withincluding all system pairings in a PJE.
If this costgrows unacceptably large, a subset of systems hasto be selected as reference systems.7 Concluding RemarksOur aim in the research presented in this paperwas to investigate how rating-scale experimentscompare to preference-strength judgement experi-ments in the evaluation of automatically generatedlanguage.
We find that preference-strength judge-ment evaluations generally have a greater rela-tive effect of System (the factor actually under in-vestigation), a smaller relative effect of Text andEvaluator (whose effect should be small), a largernumber of significant pairwise differences be-tween systems, better inter-evaluator agreement,and (where we were able to measure it) better eval-uator self-consistency.ReferencesAnja Belz and Eric Kow.
2009.
System building costvs.
output quality in data-to-text generation.
In Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation, pages 16?24.A.
Belz and E. Reiter.
2006.
Comparing automatic andhuman evaluation of NLG systems.
In Proceedingsof EACL?06, pages 313?320.Anja Belz, Eric Kow, and Jette Viethen.
2009.
TheGREC named entity generation challenge 2009:Overview and evaluation results.
In Proceedings ofthe ACL-IJCNLP?09 Workshop on Language Gen-eration and Summarisation (UCNLG+Sum), pages88?98.A.
Belz.
2009.
Prodigy-METEO: Pre-alpha releasenotes (Nov 2009).
Technical Report NLTG-09-01,Natural Language Technology Group, CMIS, Uni-versity of Brighton.Bernard Choi and Anita Pak.
2005.
A catalog of bi-ases in questionnaires.
Preventing Chronic Disease,2(1):A13.A.
Gatt, A. Belz, and E. Kow.
2009.
The TUNA Chal-lenge 2009: Overview and evaluation results.
InProceedings of the 12th European Workshop on Nat-ural Language Generation (ENLG?09), pages 198?206.C.-Y.
Lin and F. J. Och.
2004.
ORANGE: A methodfor evaluating automatic evaluation metrics for ma-chine translation.
In Proceedings of the 20th Inter-national Conference on Computational Linguistics(COLING?04), pages 501?507, Geneva.E.
Reiter, S. Sripada, J.
Hunter, and J. Yu.
2005.Choosing words in computer-generated weatherforecasts.
Artificial Intelligence, 167:137?169.Sidney Siegel.
1957.
Non-parametric statistics.
TheAmerican Statistician, 11(3):13?19.S.
Sripada, E. Reiter, J.
Hunter, and J. Yu.
2002.SUMTIME-METEO: A parallel corpus of naturallyoccurring forecast texts and weather data.
TechnicalReport AUCS/TR0201, Computing Science Depart-ment, University of Aberdeen.H.
Trang Dang.
2006.
DUC 2005: Evaluationof question-focused summarization systems.
InProceedings of the COLING-ACL?06 Workshop onTask-Focused Summarization and Question Answer-ing, pages 48?55.J.
Turian, L. Shen, and I. D. Melamed.
2003.
Evalu-ation of machine translation and its evaluation.
InProceedings of MT Summmit IX, pages 386?393,New Orleans.
