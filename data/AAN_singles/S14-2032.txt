Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 203?207,Dublin, Ireland, August 23-24, 2014.COMMIT-P1WP3: A Co-occurrence Based Approachto Aspect-Level Sentiment AnalysisKim Schouten1,2schouten@ese.eur.nlFlavius Frasincar1frasincar@ese.eur.nl1Econometric Institute, Erasmus University Rotterdam, The Netherlands2Erasmus Studio, Erasmus University Rotterdam, The NetherlandsFranciska de Jong2fdejong@ese.eur.nlAbstractIn this paper, the crucial ingredients forour submission to SemEval-2014 Task 4?Aspect Level Sentiment Analysis?
arediscussed.
We present a simple aspect de-tection algorithm, a co-occurrence basedmethod for category detection and a dic-tionary based sentiment classification al-gorithm.
The dictionary for the latter isbased on co-occurrences as well.
The fail-ure analysis and related work section focusmainly on the category detection methodas it is most distinctive for our work.1 IntroductionIn recent years, sentiment analysis has taken flightand is now actively used, on the Web and be-yond (Liu, 2012).
To provide users of sentimenttools with more detailed and useful information, anumber of innovations have been introduced, andamong others a switch from document-level sen-timent analysis towards fine-grained, aspect-levelsentiment analysis can be seen (Feldman, 2013).In line with the many challenges associated withthis, SemEval-2014 Task 4 ?Aspect Level Senti-ment Analysis?
(Pontiki et al., 2014) is split intofour sub-tasks: Aspect Detection, Aspect Senti-ment Classification, Category Detection, and Cat-egory Sentiment Classification.The main focus of this paper is on the categorydetection algorithm we developed, but a methodfor aspect detection and a sentiment classifica-tion algorithm (both for aspects and categories) arealso included.
The aspect detection algorithm willbe presented first, followed by the category de-tection algorithm and the sentiment classificationThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/method.
Next, the benchmark results for all algo-rithms are presented, plus a discussion and failureanalysis of the category detection method.
Finally,conclusions are drawn and some suggestions forfuture work are presented.2 Related WorkBecause the focus of this paper lies on the cat-egory detection method, only for that method ashort snippet of related work is given.
That algo-rithm, being an adapted version of Schouten andFrasincar (2014), is inspired by the work of Zhangand Zhu (2013) and Hai et al (2011).
In theseworks, a co-occurrence matrix is created betweenwords in the sentence and aspects in order to findimplicit aspects (i.e., aspects that are not literallymentioned, as opposed to the explicit aspects usedin this task).While implicit aspects are similar to aspect cat-egories to some extent, these methods do not workwhen a fixed, limited set of possible aspect cat-egories is used that is, most importantly, not asubset of the set of aspects.
The above meth-ods could never, for instance, identify the ?anec-dotes/miscellaneous?
category, as this word neverappears as an aspect in the data set.
This is themain reason why we have chosen to count the co-occurrences between words and the annotated as-pect categories.3 Aspect Detection MethodIn the work reported here, the aspect detectionmethod plays the role of a baseline method ratherthan a full-fledged algorithm.
In its most basicform, it annotates all noun phrases as aspects.However, by using the training set to count howoften each word appears within an aspect, a sim-ple probability can be computed representing thechance that this word is an aspect word or not.This probability is used to filter the set of nounphrases, such that only noun phrases remain that203have at least one word for which the aspect proba-bility ?
0.05 and for those noun phrases, all lead-ing words in the noun phrase with a probabilitybelow 0.05 are removed.
This will remove wordslike determiners from the initial noun phrase, asthose are not included in the aspect term.
Becausethis filtering is strict, the result is a typical highprecision, low recall algorithm for aspect detec-tion.4 Category Detection MethodTo find the aspect categories, the co-occurrencebased algorithm from Schouten and Frasin-car (2014) is used and improved upon.
The cen-tral construct in this algorithm is a co-occurrencematrix that captures the frequency of the co-occurrences between words (i.e., the lemmas ofthe words) in the sentence and the annotated as-pect category.
This gives a mapping from words toaspect categories.
When processing an unlabelledsentence, a score is computed for each aspect cat-egory as shown in Eq.
1.scoreai=1vv?j=1ci,joj, (1)where v is the number of words in the sentence,aiis the ith aspect category in the list of possibleaspect categories for which the score is computed,j represents the jth word in the sentence, ci,jis theco-occurrence frequency of aspect category i andlemma j in the data set, and ojis the frequency oflemma j in the data set.Whereas in Schouten and Frasincar (2014), thehighest scoring category was chosen on the con-dition that its score exceeded a threshold, ourmethod is now able to choose more than one as-pect category per sentence.
This is done by train-ing a separate threshold for each of the five aspectcategories using all training data.
When the scorefor some aspect category is higher than its associ-ated threshold (i.e., scoreai> thresholdai), thesentence is annotated as having that aspect cate-gory.Since we assume the five threshold values to beindependent of each other, a simple linear searchis performed separately for all five of them to findthe optimal threshold value by optimizing F1(cf.Sec.
6).
As a default option, the fifth category(?anecdotes/miscellaneous?)
is associated to anysentence for which none of the five categories ex-ceeded their threshold.
The trained threshold val-ues for the five categories are:ambience price food service misc0.042 0.024 0.211 0.071 0.143The pseudocode for the creation of the co-occurrence matrix can be found in Algorithm 1,and Algorithm 2 describes the process of annotat-ing a sentence with aspect categories.Algorithm 1 Aspect category detection training.Initialize set of word lemmas with frequenciesOInitialize set of aspect categories AInitialize co-occurrence matrix Cfor sentence s ?
training data dofor word w ?
s doO(w) = O(w) + 1end forfor aspect category a ?
s doadd a to Afor word w ?
s doC(w, a) = C(w, a) + 1end forend forend forfor aspect category a in A dothresholda=0bestF1= 0for t = 0 to 1 step 0.001 doExecute Algorithm 2 on training dataCompute F1if F1> bestF1then thresholda= tend ifend forend for5 Sentiment Classification MethodFor sentiment classification, a method is devel-oped that first creates a sentiment lexicon basedon the aspect sentiment annotation.
That lexiconis then consequently used to determine the senti-ment of both aspects and categories that have nosentiment annotation.
The intuition behind thismethod is that a lexicon should cover domain-specific words and expressions in order to be ef-fective.
To avoid creating such a sentiment lexi-con manually, the aspect sentiment annotations areleveraged to create one automatically.
The idea isthat words that often appear close to positive or204Algorithm 2 Aspect category detection execution.for sentence s ?
test data dofor aspect category a ?
A doscore = 0for word w ?
s doif O(w) > 0 thenscore = score+C(w, a)/O(w)end ifend forscore = score/ length(s)if score > thresholdathenAssign aspect category a to send ifend forif s has no assigned aspect categories thenAssign ?anecdotes/miscellaneous?
to send ifend fornegative aspects are likely to have the same polar-ity.
Since sentiment is also carried by expressions,rather than single words only, the constructed sen-timent lexicon has entries for encountered uni-grams, bigrams, and trigrams.
In each sentence,the distance between each n-gram and each aspectis computed and the sentiment of the aspect, dis-counted by the computed distance, is added to thesentiment value of the n-gram, as shown in Eq.
2.sentimentg=1freqg?
?s?Sgp ?
torder(g)?
?a?Aspolaritya(distanceg,a)m,(2)where g is the n-gram (i.e., word unigram, bigram,or trigram), freqgis the frequency of n-gram gin the data set, s is a sentence in Sg, which isthe set of sentences that contain n-gram g, p is aparameter to correct for the overall positivity ofthe data set, t is a parameter that corrects for therelative influence of the type of n-gram (i.e., dif-ferent values are used for t1, t2, and t3), a is anaspect in As, which is the set of aspects in sen-tence s, polarityais 1 when aspect a is positiveand ?1 when a is negative, and m is a parame-ter that determines how strong the discounting bythe distance should be.
The distance computedis the shortest word distance between the aspectand the n-gram (i.e., both an n-gram and an as-pect can consist of multiple words, in which casethe closest two are used to compute the distance).Note that essentially, dictionary creation is basedon how often an n-gram co-occurs with positiveor negative aspects.
In our submitted run on therestaurant data, we set torder(g)to 1, 5, and 4 forunigrams, bigrams, and trigrams, respectively, andp = 2 and for the laptop data we set torder(g)to 1,0, and 3 for the n-grams and p = 1.
In both cases,m was kept at 1.
These values were determined bymanual experimentation.To compute the sentiment of an aspect, the sen-timent value of each n-gram is divided by the dis-tance between that n-gram and the aspect, com-puted in a similar fashion as in the above formula)and summed up, as shown in Eq.
3.sentimenta,sa=?g?sasentimentg(min distanceg,a)m, (3)where, in addition to the definitions in the previ-ous equation, g is an n-gram in sa, which is thesentence in which aspect a occurs.
Note that foreach occurrence of a term, its sentiment value isadded to the total score.
If the result is above zero,the class will be ?positive?, and if the result is be-low zero, the class will be ?negative?.
In the rareevent of the sentiment score being exactly zero,the ?neutral?
class is assigned.For category sentiment classification, the for-mula of Eq.
3 remains the same, except that thedistance term min distancemg,ais set to 1, sinceaspect categories pertain to the whole sentence in-stead of having specific offsets.6 EvaluationAll three algorithms presented above were evalu-ated through a submission in the SemEval-2014Task 4 ?Aspect Level Sentiment Analysis?.
Twodata sets have been used, one consisting of sen-tences from restaurant reviews, the other consist-ing of sentences from laptop reviews.
Both setshave been annotated with aspects and aspect senti-ment, but only the restaurant set is also annotatedwith aspect categories and their associated senti-ment class.
Both data sets are split into a trainingset of roughly 3000 sentences and a test set of 800sentences.All sentences in the data set have been pre-processed by a tokenizer, a Part-of-Speech tagger,and a lemmatizer.
These tasks were performed by205Table 1: Official results for both algorithms.aspect detection (subtask 1)precision recall F1laptop 0.836 0.148 0.252restaurant 0.909 0.388 0.544category detection (subtask 3)precision recall F1restaurant 0.633 0.558 0.593aspect sentiment classification (subtask 2)laptop accuracy 0.570restaurant accuracy 0.660category sentiment classification (subtask 4)restaurant accuracy 0.677the Stanford CoreNLP framework1.
Furthermore,the OpenNLP2chunker was used to provide basicphrase chunking in order to retrieve noun phrasesfor instance.The official scores, as computed by the task or-ganizers are shown in Table 1.
Note that the senti-ment classification algorithm is used for subtasks 2and 4, so two scores are reported, and that subtasks3 and 4 can only be performed with the restaurantdata set.As the performance of the category detectionmethod was lower than anticipated, a failure anal-ysis has been performed.
This led to the observa-tion that overfitting is one of major factors in ex-plaining the lower performance .
This is shown inFigure 1, in which one can easily notice the greatdifference in in-sample performance, and the per-formance on unseen data.
Notice that by using 10-fold cross-validation, better results are achievedthan on the official test set.
This indicates thatthere are factors other than overfitting that influ-ence the performance.Interestingly, especially recall is influenced bythe overfitting problem: precision is almost thesame for the 10-fold cross-validation and evenwith the in-sample performance it increases onlya little bit.
To gain more insight into the differencein recall, a graph showing the relative contributionto false negatives of the five categories is shown inFigure 2.
For completeness, the same graph but forfalse positives is also shown, together with the fre-quency distribution of the categories in both train-ing and test set.Immediately visible is the effect of defaulting to1http://nlp.stanford.edu/software/corenlp.shtml2https://opennlp.apache.org/0.70.750.80.850.9precisionrecallF1-measure0.50.550.60.650.7officialresults10-foldcrossvalidationontraining datain-sample result:test ontrainingdatatraining datadataFigure 1: Performance measure of category detec-tion on different parts of data.the ?anecdotes/miscellaneous?
when no category isassigned to that sentence: many false positives aregenerated by this rule, but there are almost no falsenegatives for this category.
Note that without thisdefault, F1-measure would drop by roughly 3 per-centage points.Also notable is the difference between the in-sample bar and the official results bar: two cat-egories, namely ?anecdotes/miscellaneous?
and?food?
show large differences in contribution tofalse positives and false negatives.
The algo-rithm finds fewer ?food?
categories in the testset, than in the training set, while for ?anec-dotes/miscellaneous?, the reverse is the case.
Thiscan at least be partly explained by the change indata statistics: in the training set, 33% of the an-notated categories are ?food?
and 30% are ?anec-dotes/miscellaneous?, whereas in the test set, thesenumbers are 40% and 22%, respectively.
Withmuch more sentences having the ?food?
category,false positives will be lower but false negativeswill be higher.
For ?anecdotes/miscellaneous?, thereverse is true: with less sentences in the test sethaving this category, false positives will by higher,but false negatives will be lower, a change rein-forced by ?anecdotes/miscellaneous?
being the de-fault.Two factors remain that might have negativelyimpacted the performance of the algorithm.
Thefirst is that in the restaurant set, many words ap-pear only once (e.g., dishes, ingredients), andwhen words do not appear in the training set, noco-occurrence with any category can be recorded.This primarily affects recall.
The second is thatthe category thresholds, while working well on thetraining set, do not seem to generalize well to the2060%20%40%60%80%100%trainingtestCategory Distributionfoodpriceserviceambienceanecdotes/misc0%20%40%60%80%100%official10-foldin-sampleContribution to FalseNegatives0%20%40%60%80%100%official10-foldin-sampleContribution to False Positivestrainingtestfoodpriceserviceambienceanecdotes/miscofficial10-foldin-sampleofficial10-foldin-sampleFigure 2: The frequency distribution of each category and its relative contribution to the total number offalse negatives (left graph) and false positives (right graph).
The middle graph shows the change in thedistribution of categories in the training and test set.test set.
Testing the algorithm with one thresholdfor all five categories, while showing a sharply de-creased in-sample performance, yields an out-of-sample F1-measure that is only slightly lower thanF1-measure with different thresholds.7 ConclusionIn this paper the main ingredients for our submis-sion to SemEval-2014 Task 4 ?Aspect Level Sen-timent Analysis?
are discussed: a simple aspectdetection method, a co-occurrence based methodfor category detection, and a dictionary based sen-timent classification algorithm.
Since the categorydetection algorithm did not perform as expected, afailure analysis has been performed, while for theothers this was less necessary as they performedroughly as expected.The failure analysis provides several startingpoints for future research.
First, it would be in-teresting to determine the exact nature of the de-pendency between category performance and cat-egory frequency, as discussed above, and to re-move this dependency, since it is not guaranteed inreal-life scenarios that the frequency distributionof the training set is the same as the set of instancesan algorithm will encounter when in use.
Fur-thermore, training five separate category thresh-old, while good for performance in general, alsoaggravates the problem of overfitting.
Hence, im-proving the generalization of the algorithm, andthe thresholds in particular, is important.
Last,a method to deal with very low frequency wordscould prove useful as well.AcknowledgmentThe authors are partially supported by the Dutchnational program COMMIT.ReferencesRonen Feldman.
2013.
Techniques and Applicationsfor Sentiment Analysis.
Communications of theACM, 56(4):82?89.Zhen Hai, Kuiyu Chang, and J. Kim.
2011.
ImplicitFeature Identification via Co-occurrence Associa-tion Rule Mining.
In Proceedings of the 12th In-ternational Conference on Computational Linguis-tics and Intelligent Text processing (CICLing 2011),volume 6608, pages 393?404.
Springer.Bing Liu.
2012.
Sentiment Analysis and OpinionMining, volume 16 of Synthesis Lectures on HumanLanguage Technologies.
Morgan & Claypool.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
SemEval-2014 Task 4:Aspect Based Sentiment Analysis.
In Proceedingsof the International Workshop on Semantic Evalua-tion (SemEval 2014).Kim Schouten and Flavius Frasincar.
2014.
Find-ing Implicit Features in Consumer Reviews for Sen-timent Analysis.
In Proceedings of the 14th In-ternational Conference on Web Engineering (ICWE2014), pages 130?144.
Springer.Yu Zhang and Weixiang Zhu.
2013.
ExtractingImplicit Features in Online Customer Reviews forOpinion Mining.
In Proceedings of the 22nd Inter-national Conference onWorldWideWeb Companion(WWW 2013 Companion), pages 103?104.
Interna-tional World Wide Web Conferences Steering Com-mittee.207
