Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344?351,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGLEU: Automatic Evaluation of Sentence-Level FluencyAndrew Mutton?
Mark Dras?
Stephen Wan?,?
Robert Dale?
?Centre for Language Technology ?Information and Communication TechnologiesMacquarie University CSIRONSW 2109 Australia NSW 2109 Australiamadras@ics.mq.edu.auAbstractIn evaluating the output of language tech-nology applications?MT, natural languagegeneration, summarisation?automatic eval-uation techniques generally conflate mea-surement of faithfulness to source contentwith fluency of the resulting text.
In thispaper we develop an automatic evaluationmetric to estimate fluency alone, by examin-ing the use of parser outputs as metrics, andshow that they correlate with human judge-ments of generated text fluency.
We then de-velop a machine learner based on these, andshow that this performs better than the indi-vidual parser metrics, approaching a lowerbound on human performance.
We finallylook at different language models for gener-ating sentences, and show that while individ-ual parser metrics can be ?fooled?
dependingon generation method, the machine learnerprovides a consistent estimator of fluency.1 IntroductionIntrinsic evaluation of the output of many languagetechnologies can be characterised as having at leasttwo aspects: how well the generated text reflectsthe source data, whether it be text in another lan-guage for machine translation (MT), a natural lan-guage generation (NLG) input representation, a doc-ument to be summarised, and so on; and how well itconforms to normal human language usage.
Thesetwo aspects are often made explicit in approachesto creating the text.
For example, in statistical MTthe translation model and the language model aretreated separately, characterised as faithfulness andfluency respectively (as in the treatment in Jurafskyand Martin (2000)).
Similarly, the ultrasummarisa-tion model of Witbrock and Mittal (1999) consistsof a content model, modelling the probability that aword in the source text will be in the summary, anda language model.Evaluation methods can be said to fall into two cate-gories: a comparison to gold reference, or an appealto human judgements.
Automatic evaluation meth-ods carrying out a comparison to gold reference tendto conflate the two aspects of faithfulness and flu-ency in giving a goodness score for generated out-put.
BLEU (Papineni et al, 2002) is a canonical ex-ample: in matching n-grams in a candidate transla-tion text with those in a reference text, the metricmeasures faithfulness by counting the matches, andfluency by implicitly using the reference n-grams asa language model.
Often we are interested in know-ing the quality of the two aspects separately; manyhuman judgement frameworks ask specifically forseparate judgements on elements of the task that cor-respond to faithfulness and to fluency.
In addition,the need for reference texts for an evaluation metriccan be problematic, and intuitively seems unneces-sary for characterising an aspect of text quality thatis not related to its content source but to the use oflanguage itself.
It is a goal of this paper to providean automatic evaluation method for fluency alone,without the use of a reference text.One might consider using a metric based on lan-guage model probabilities for sentences: in eval-344uating a language model on (already existing) testdata, a higher probability for a sentence (and lowerperplexity over a whole test corpus) indicates bet-ter language modelling; perhaps a higher probabilitymight indicate a better sentence.
However, here weare looking at generated sentences, which have beengenerated using their own language model, ratherthan human-authored sentences already existing ina test corpus; and so it is not obvious what languagemodel would be an objective assessment of sentencenaturalness.
In the case of evaluating a single sys-tem, using the language model that generated thesentence will only confirm that the sentence doesfit the language model; in situations such as com-paring two systems which each generate text usinga different language model, it is not obvious thatthere is a principled way of deciding on a fair lan-guage model.
Quite a different idea was suggestedin Wan et al (2005), of using the grammatical judge-ment of a parser to assess fluency, giving a measureindependent of the language model used to gener-ate the text.
The idea is that, assuming the parserhas been trained on an appropriate corpus, the poorperformance of the parser on one sentence relativeto another might be an indicator of some degree ofungrammaticality and possibly disfluency.
In thatwork, however, correlation with human judgementswas left uninvestigated.The goal of this paper is to take this idea and de-velop it.
In Section 2 we look at some related workon metrics, in particular for NLG.
In Section 3, weverify whether parser outputs can be used as esti-mators of generated sentence fluency by correlatingthem with human judgements.
In Section 4, we pro-pose an SVM-based metric using parser outputs asfeatures, and compare its correlation against humanjudgements with that of the individual parsers.
InSection 5, we investigate the effects on the variousmetrics from different types of language model forthe generated text.
Then in Section 6 we conclude.2 Related WorkIn terms of human evaluation, there is no uniformview on what constitutes the notion of fluency, or itsrelationship to grammaticality or similar concepts.We mention a few examples here to illustrate therange of usage.
In MT, the 2005 NIST MT Evalu-ation Plan uses guidelines1 for judges to assess ?ad-equacy?
and ?fluency?
on 5 point scales, where theyare asked to provide intuitive reactions rather thanpondering their decisions; for fluency, the scale de-scriptions are fairly vague (5: flawless English; 4:good English; 3: non-native English; 2: disfluentEnglish; 1: incomprehensible) and instructions areshort, with some examples provided in appendices.Zajic et al (2002) use similar scales for summari-sation.
By contrast, Pan and Shaw (2004), for theirNLG system SEGUE tied the notion of fluency moretightly to grammaticality, giving two human evalu-ators three grade options: good, minor grammaticalerror, major grammatical/pragmatic error.
As a fur-ther contrast, the analysis of Coch (1996) was verycomprehensive and fine-grained, in a comparison ofthree text-production techniques: he used 14 humanjudges, each judging 60 letters (20 per generationsystem), and required them to assess the letters forcorrect spelling, good grammar, rhythm and flow,appropriateness of tone, and several other specificcharacteristics of good text.In terms of automatic evaluation, we are not awareof any technique that measures only fluency or sim-ilar characteristics, ignoring content, apart from thatof Wan et al (2005).
Even in NLG, where, given thevariability of the input representations (and hencedifficulty in verifying faithfulness), it might be ex-pected that such measures would be available, theavailable metrics still conflate content and form.For example, the metrics proposed in Bangalore etal.
(2000), such as Simple Accuracy and GenerationAccuracy, measure changes with respect to a refer-ence string based on the idea of string-edit distance.Similarly, BLEU has been used in NLG, for exampleby Langkilde-Geary (2002).3 Parsers as EvaluatorsThere are three parts to verifying the usefulness ofparsers as evaluators: choosing the parsers and themetrics derived from them; generating some textsfor human and parser evaluation; and, the key part,getting human judgements on these texts and corre-lating them with parser metrics.1http://projects.ldc.upenn.edu/TIDES/Translation/TranAssessSpec.pdf3453.1 The ParsersIn testing the idea of using parsers to judge fluency,we use three parsers, from which we derive fourparser metrics, to investigate the general applicabil-ity of the idea.
Those chosen were the Connexorparser,2 the Collins parser (Collins, 1999), and theLink Grammar parser (Grinberg et al, 1995).
Eachproduces output that can be taken as representingdegree of ungrammaticality, although this output isquite different for each.Connexor is a commercially available dependencyparser that returns head?dependant relations as wellas stemming information, part of speech, and so on.In the case of an ungrammatical sentence, Connexorreturns tree fragments, where these fragments aredefined by transitive head?dependant relations: forexample, for the sentence Everybody likes big cakesdo it returns fragments for Everybody likes big cakesand for do.
We expect that the number of fragmentsshould correlate inversely with the quality of a sen-tence.
For a metric, we normalise this number bythe largest number of fragments for a given data set.
(Normalisation matters most for the machine learnerin Section 4.
)The Collins parser is a statistical chart parser thataims to maximise the probability of a parse using dy-namic programming.
The parse tree produced is an-notated with log probabilities, including one for thewhole tree.
In the case of ungrammatical sentences,the parser will assign a low probability to any parse,including the most likely one.
We expect that thelog probability (becoming more negative as the sen-tence is less likely) should correlate positively withthe quality of a sentence.
For a metric, we normalisethis by the most negative value for a given data set.Like Connexor, the Link Grammar parser returns in-formation about word relationships, forming links,with the proviso that links cannot cross and that ina grammatical sentence all links are indirectly con-nected.
For an ungrammatical sentence, the parserwill delete words until it can produce a parse; thenumber it deletes is called the ?null count?.
We ex-pect that this should correlate inversely with sen-tence quality.
For a metric, we normalise this bythe sentence length.
In addition, the parser produces2http://www.connexor.comanother variable possibly of interest.
In generatinga parse, the parser produces many candidates andrules some out by a posteriori constraints on validparses.
In its output the parser returns the number ofinvalid parses.
For an ungrammatical sentence, thisnumber may be higher; however, there may also bemore parses.
For a metric, we normalise this by thetotal number of parses found for the sentence.
Thereis no strong intuition about the direction of correla-tion here, but we investigate it in any case.3.2 Text Generation MethodTo test whether these parsers are able to discriminatesentence-length texts of varying degrees of fluency,we need first to generate texts that we expect will bediscriminable in fluency quality ranging from goodto very poor.
Below we describe our method for gen-erating text, and then our preliminary check on thediscriminability of the data before giving them to hu-man judges.Our approach to generating ?sentences?
of a fixedlength is to take word sequences of different lengthsfrom a corpus and glue them together probabilisti-cally: the intuition is that a few longer sequencesglued together will be more fluent than many shortersequences.
More precisely, to generate a sentence oflength n, we take sequences of length l (such that ldivides n), with sequence i of the form wi,1 .
.
.
wi,l,where wi, is a word or punctuation mark.
We startby selecting sequence 1, first by randomly choos-ing its first word according to the unigram probabil-ity P (w1,1), and then the sequence uniformly ran-domly over all sequences of length l starting withw1,1; we select subsequent sequences j (2 ?
j ?n/l) randomly according to the bigram probabilityP (wj,1 |wj?1,l).
Taking as our corpus the Reuterscorpus,3 for length n = 24, we generate sentencesfor sequence sizes l = 1, 2, 4, 8, 24 as in Figure 1.So, for instance, the sequence-size 8 example wasconstructed by stringing together the three consecu-tive sequences of length 8 (There .
.
.
to; be .
.
.
have;to .
.
.
.)
taken from the corpus.These examples, and others generated, appear tobe of variable quality in accordance with our intu-ition.
However, to confirm this prior to testing them3http://trec.nist.gov/data/reuters/reuters.html346Extracted (Sequence-size 24)Ginebra face Formula Shell in a sudden-death playoff on Sun-day to decide who will face Alaska in a best-of-seven series forthe title.Sequence-size 8There is some thinking in the government to be nearly as dra-matic as some people have to be slaughtered to eradicate theepidemic.Sequence-size 4Most of Banharn?s move comes after it can still be averted thecrash if it should again become a police statement said.Sequence-size 2Massey said in line with losses, Nordbanken is well-placed tobenefit abuse was loaded with Czech prime minister AndrisShkele, said.Sequence-size 1The war we?re here in a spokesman Jeff Sluman 86 percent jumpthat Spain to what was booked, express also said.Figure 1: Sample sentences from the first trialDescription CorrelationSmall 0.10 to 0.29Medium 0.30 to 0.49Large 0.50 to 1.00Table 1: Correlation coefficient interpretationout for discriminability in a human trial, we wantedsee whether they are discriminable by some methodother than our own judgement.
We used the parsersdescribed in Section 3.1, in the hope of finding anon-zero correlation between the parser outputs andthe sequence lengths.Regarding the interpretation of the absolute value of(Pearson?s) correlation coefficients, both here and inthe rest of the paper, we adopt Cohen?s scale (Co-hen, 1988) for use in human judgements, given inTable 1; we use this as most of this work is to do withhuman judgements of fluency.
For data, we gener-ated 1000 sentences of length 24 for each sequencelength l = 1, 2, 3, 4, 6, 8, 24, giving 7000 sentencesin total.
The correlations with the four parser out-puts are as in Table 2, with the medium correlationsfor Collins and Link Grammar (nulled tokens) indi-cating that the sentences are indeed discriminable tosome extent, and hence the approach is likely to beuseful for generating sentences for human trials.3.3 Human JudgementsThe next step is then to obtain a set of human judge-ments for this data.
Human judges can only be ex-pected to judge a reasonably sized amount of data,Metric Corr.Collins Parser 0.3101Connexor -0.2332Link Grammar Nulled Tokens -0.3204Link Grammar Invalid Parses 0.1776GLEU 0.4144Table 2: Parser vs sequence size for original data setso we first reduced the set of sequence sizes to bejudged.
To do this we determined for the 7000generated sentences the scores according to the (ar-bitrarily chosen) Collins parser, and calculated themeans for each sequence size and the 95% confi-dence intervals around these means.
We then chosea subset of sequence sizes such that the confidenceintervals did not overlap: 1, 2, 4, 8, 24; the idea wasthat this would be likely to give maximally discrim-inable sentences.
For each of these sequences sizes,we chose randomly 10 sentences from the initial set,giving a set for human judgement of size 50.The judges consisted of twenty volunteers, all nativeEnglish speakers without explicit linguistic training.We gave them general guidelines about what consti-tuted fluency, mentioning that they should considergrammaticality but deliberately not giving detailedinstructions on the manner for doing this, as we wereinterested in the level of agreement of intuitive un-derstanding of fluency.
We instructed them also thatthey should evaluate the sentence without consider-ing its content, using Colourless green ideas sleepfuriously as an example of a nonsensical but per-fectly fluent sentence.
The judges were then pre-sented with the 50 sentences in random order, andasked to score the sentences according to their ownscale, as in magnitude estimation (Bard et al, 1996);these scores were then normalised in the range [0,1].Some judges noted that the task was difficult be-cause of its subjectivity.
Notwithstanding this sub-jectivity and variation in their approach to the task,the pairwise correlations between judges were high,as indicated by the maximum, minimum and meanvalues in Table 3, indicating that our assumptionthat humans had an intuitive notion of fluencyand needed only minimal instruction was justified.Looking at mean scores for each sequence size,judges generally also ranked sentences by sequencesize; see Figure 2.
Comparing human judgement347Statistic Corr.Maximum correlation 0.8749Minimum correlation 0.4710Mean correlation 0.7040Standard deviation 0.0813Table 3: Data on correlation between humansFigure 2: Mean scores for human judgescorrelations against sequence size with the same cor-relations for the parser metrics (as for Table 2, but onthe human trial data) gives Table 4, indicating thathumans can also discriminate the different generatedsentence types, in fact (not surprisingly) better thanthe automatic metrics.Now, having both human judgement scores of somereliability for sentences, and scoring metrics fromthree parsers, we give correlations in Table 5.
GivenCohen?s interpretation, the Collins and Link Gram-mar (nulled tokens) metrics show moderate correla-tion, the Connexor metric almost so; the Link Gram-mar (invalid parses) metric correlation is by far theweakest.
The consistency and magnitude of the firstthree parser metrics, however, lends support to theidea of Wan et al (2005) to use something like theseas indicators of generated sentence fluency.
The aimof the next section is to build a better predictor thanthe individual parser metrics alone.Metric Corr.Humans 0.6529Collins Parser 0.4057Connexor -0.3804Link Grammar Nulled Tokens -0.3310Link Grammar Invalid Parses 0.1619GLEU 0.4606Table 4: Correlation with sequence size for humantrial data setMetric Corr.Collins Parser 0.3057Connexor -0.3445Link-Grammar Nulled Tokens -0.2939Link Grammar Invalid Parses 0.1854GLEU 0.4014Table 5: Correlation between metrics and humanevaluators4 An SVM-Based MetricIn MT, one problem with most metrics like BLEUis that they are intended to apply only to document-length texts, and any application to individual sen-tences is inaccurate and correlates poorly withhuman judgements.
A neat solution to poorsentence-level evaluation proposed by Kulesza andShieber (2004) is to use a Support Vector Machine,using features such as word error rate, to estimatesentence-level translation quality.
The two main in-sights in applying SVMs here are, first, noting thathuman translations are generally good and machinetranslations poor, that binary training data can becreated by taking the human translations as posi-tive training instances and machine translations asnegative ones; and second, that a non-binary metricof translation goodness can be derived by the dis-tance from a test instance to the support vectors.
Inan empirical evaluation, Kulesza and Shieber foundthat their SVM gave a correlation of 0.37, whichwas an improvement of around half the gap betweenthe BLEU correlations with the human judgements(0.25) and the lowest pairwise human inter-judgecorrelation (0.46) (Turian et al, 2003).We take a similar approach here, using as featuresthe four parser metrics described in Section 3.
Wetrained an SVM,4 taking as positive training datathe 1000 instances of sentences of sequence length24 (i.e.
sentences extracted from the corpus) andas negative training data the 1000 sentences of se-quence length 1.
We call this learner GLEU.5As a check on the ability of the GLEU SVM to dis-tinguish these ?positive?
sentences from ?negative?ones, we evaluated its classification accuracy on a(new) test set of size 300, split evenly between sen-tences of sequence length 24 and sequence length 1.4We used the package SVM-light (Joachims, 1999).5For GrammaticaLity Evaluation Utility.348This gave 81%, against a random baseline of 50%,indicating that the SVM can classify satisfactorily.We now move from looking at classification accu-racy to the main purpose of the SVM, using distancefrom support vector as a metric.
Results are givenfor correlation of GLEU against sequence sizes forall data (Table 2) and for the human trial data set(Table 4); and also for correlation of GLEU againstthe human judges?
scores (Table 5).
This last indi-cates that GLEU correlates better with human judge-ments than any of the parsers individually, and iswell within the ?moderate?
range for correlation in-terpretation.
In particular, for the GLEU?human cor-relation, the score of 0.4014 is approaching the min-imum pairwise human correlation of 0.4710.5 Different Text Generation MethodsThe method used to generate text in Section 3.2 isa variation of the standard n-gram language model.A question that arises is: Are any of the metrics de-fined above strongly influenced by the type of lan-guage model used to generate the text?
It may be thecase, for example, that a parser implementation usesits own language model that predisposes it to favoura similar model in the text generation process.
Thisis a phenomenon seen in MT, where BLEU seems tofavour text that has been produced using a similarstatistical n-gram language model over other sym-bolic models (Callison-Burch et al, 2006).Our previous approach used only sequences ofwords concatenated together.
To define some newmethods for generating text, we introduced varyingamounts of structure into the generation process.5.1 Structural Generation MethodsPoStag In the first of these, we constructed arough approximation of typical sentence grammarstructure by taking bigrams over part-of-speechtags.6 Then, given a string of PoS tags of lengthn, t1 .
.
.
tn, we start by assigning the probabilitiesfor the word in position 1, w1, according to the con-ditional probability P (w1 | t1).
Then, for position j(2 ?
j ?
n), we assign to candidate words the valueP (wj | tj)?P (wj |wj?1) to score word sequences.6We used the supertagger of Bangalore and Joshi (1999).So, for example, we might generate the PoS tag tem-plate Det NN Adj Adv, take all the words corre-sponding to each of these parts of speech, and com-bine bigram word sequence probability with the con-ditional probability of words with respect to theseparts of speech.
We then use a Viterbi-style algo-rithm to find the most likely word sequence.In this model we violate the Markov assumption ofindependence in much the same way as Witbrockand Mittal (1999) in their combination of contentand language model probabilities, by backtrackingat every state in order to discourage repeated wordsand avoid loops.Supertag This is a variant of the approach above,but using supertags (Bangalore and Joshi, 1999) in-stead of PoS tags.
The idea is that the supertagsmight give a more fine-grained definition of struc-ture, using partial trees rather than parts of speech.CFG We extracted a CFG from the ?10% of thePenn Treebank found in the NLTK-lite corpora.7This CFG was then augmented with productions de-rived from the PoS-tagged data used above.
We thengenerated a template of length n pre-terminal cate-gories using this CFG.
To avoid loops we biased theselection towards terminals over non-terminals.5.2 Human JudgementsWe generated sentences according to a mix of theinitial method of Section 3.2, for calibration, andthe new methods above.
We again used a sentencelength of 24, and sequence lengths for the initialmethod of l = 1, 8, 24.
A sample of sentences gen-erated for each of these six types is in Figure 3.For our data, we generated 1000 sentences per gen-eration method, giving a corpus of 6000 sentences.For the human judgements we also again took 10sentences per generation method, giving 60 sen-tences in total.
The same judges were given the sameinstructions as previously.Before correlating the human judges?
scores andthe parser outputs, it is interesting to look at howeach parser treats the sentence generation methods,and how this compares with human ratings (Ta-ble 6).
In particular, note that the Collins parser ratesthe PoStag- and Supertag-generated sentences more7http://nltk.sourceforge.net349Extracted (Sequence-size 24)After a near three-hour meeting and last-minute talks with Pres-ident Lennart Meri, the Reform Party council voted overwhelm-ingly to leave the government.Sequence-size 8If Denmark is closely linked to the Euro Disney reported a netprofit of 85 million note: the figures were rounded off.Sequence-size 1Israelis there would seek approval for all-party peace now com-plain that this year, which shows demand following year and 56billion pounds.POS-tag, Viterbi-mappedHe said earlier the 9 years and holding company?s government,including 69.62 points as a number of last year but market.Supertag, Viterbi-mappedThat 97 saying he said in its shares of the market 74.53 percent,adding to allow foreign exchange: I think people.Context-Free GrammarThe production moderated Chernomyrdin which leveled gov-ernment back near own 52 over every a current at from the saidby later the other.Figure 3: Sample sentences from the second trialsent.
type s-24 s-8 s-1 PoS sup.
CFGCollins 0.52 0.48 0.41 0.60 0.57 0.36Connexor 0.12 0.16 0.24 0.26 0.25 0.43LG (null) 0.02 0.06 0.10 0.09 0.11 0.18LG (invalid) 0.78 0.67 0.56 0.62 0.66 0.53GLEU 1.07 0.32 -0.96 0.28 -0.06 -2.48Human 0.93 0.67 0.44 0.39 0.44 0.31Table 6: Mean normalised scores per sentence typehighly even than real sentences (in bold).
Theseare the two methods that use the Viterbi-style algo-rithm, suggesting that this probability maximisationhas fooled the Collins parser.
The pairwise correla-tion between judges was around the same on averageas in Section 3.3, but with wider variation (Table 7).The main results, determining the correlation of thevarious parser metrics plus GLEU against the newdata, are in Table 8.
This confirms the very vari-able performance of the Collins parser, which hasdropped significantly.
GLEU performs quite consis-tently here, this time a little behind the Link Gram-mar (nulled tokens) result, but still with a bettercorrelation with human judgement than at least twoStatistic Corr.Maximum correlation 0.9048Minimum correlation 0.3318Mean correlation 0.7250Standard deviation 0.0980Table 7: Data on correlation between humansMetric Corr.Collins Parser 0.1898Connexor -0.3632Link-Grammar Nulled Tokens -0.4803Link Grammar Invalid Parses 0.1774GLEU 0.4738Table 8: Correlation between parsers and humanevaluators on new human trial dataMetric Corr.Collins Parser 0.2313Connexor -0.2042Link-Grammar Nulled Tokens -0.1289Link Grammar Invalid Parses -0.0084GLEU 0.4312Table 9: Correlation between parsers and humanevaluators on all human trial datajudges with each other.
(Note also that the GLEUSVM was not retrained on the new sentence types.
)Looking at all the data together, however, is whereGLEU particularly displays its consistency.
Aggre-gating the old human trial data (Section 3.3) and thenew data, and determining correlations against themetrics, we get the data in Table 9.
Again the SVM?sperformance is consistent, but is now almost twiceas high as its nearest alternative, Collins.5.3 DiscussionIn general, there is at least one parser that correlatesquite well with the human judges for each sentencetype.
With well-structured sentences, the probabilis-tic Collins parser performs best; on sentences thatare generated by a poor probabilistic model lead-ing to poor structure, Link Grammar (nulled tokens)performs best.
This supports the use of a machinelearner taking as features outputs from several parsertypes; empirically this is confirmed by the large ad-vantage GLEU has on overall data (Table 9).The generated text itself from the Viterbi-based gen-erators as implemented here is quite disappoint-ing, given an expectation that introducing structurewould make sentences more natural and hence leadto a range of sentence qualities.
In hindsight, thisis not so surprising; in generating the structure tem-plate, only sequences (over tags) of size 1 were used,which is perhaps why the human judges deemedthem fairly close to sentences generated by the origi-350nal method using sequence size 1, the poorest of thatinitial data set.6 ConclusionIn this paper we have investigated a new approach toevaluating the fluency of individual generated sen-tences.
The notion of what constitutes fluency isan imprecise one, but trials with human judges haveshown that even if it cannot be exactly defined, oreven articulated by the judges, there is a high levelof agreement about what is fluent and what is not.Given this data, metrics derived from parser out-puts have been found useful for measuring fluency,correlating up to moderately well with these humanjudgements.
A better approach is to combine thesein a machine learner, as in our SVM GLEU, whichoutperforms individual parser metrics.
Interestingly,we have found that the parser metrics can be fooledby the method of sentence generation; GLEU, how-ever, gives a consistent estimate of fluency regard-less of generation type; and, across all types of gen-erated sentences examined in this paper, is superiorto individual parser metrics by a large margin.This all suggests that the approach has promise, butit needs to be developed further for pratical use.
TheSVM presented in this paper has only four features;more features, and in particular a wider range ofparsers, should raise correlations.
In terms of thedata, we looked only at sentences generated withseveral parameters fixed, such as sentence length,due to our limited pool of judges.
In future we wouldlike to examine the space of sentence types morefully.
In particular, we will look at predicting the flu-ency of near-human quality sentences.
More gener-ally, we would like to look also at how the approachof this paper would relate to a perplexity-based met-ric; how it compares against BLEU or similar mea-sures as a predictor of fluency in a context where ref-erence sentences are available; and whether GLEUmight be useful in applications such as reranking ofcandidate sentences in MT.AcknowledgementsWe thank Ben Hutchinson and Mirella Lapata for discussions,and Srinivas Bangalore for the TAG supertagger.
The sec-ond author acknowledges the support of ARC Discovery GrantDP0558852.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertagging:An approach to almost parsing.
Computational Linguistics,25(2):237?265.Srinivas Bangalore, Owen Rambow, and Steve Whittaker.2000.
Evaluation metrics for generation.
In Proceedings of theFirst International Natural Language Generation Conference(INLG2000), Mitzpe Ramon, Israel.E.
Bard, D. Robertson, and A. Sorace.
1996.
Magnitude esti-mation and linguistic acceptability.
Language, 72(1):32?68.Chris Callison-Burch, Miles Osborne, and Philipp Koehn.2006.
Re-evaluating the Role of Bleu in Machine TranslationResearch.
In Proceedings of EACL, pages 249?256.Jose?
Coch.
1996.
Evaluating and comparing three text-production strategies.
In Proceedings of the 16th InternationalConference on Computational Linguistics (COLING?96), pages249?254.J.
Cohen.
1988.
Statistical power analysis for the behavioralsciences.
Erlbaum, Hillsdale, NJ, US.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University of Penn-sylvania.Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995.
Arobus parsing algorithm for link grammars.
In Proceedings ofthe Fourth International Workshop on Parsing Technologies.Thorsten Joachims.
1999.
Making Large-Scale SVM LearningPractical.
MIT Press.Daniel Jurafsky and James Martin.
2000.
Speech and Lan-guage Processing: An Introduction to Natural Languge Pro-cessing, Computational Linguistics, and Speech Recognition.Prentice-Hall.Alex Kulesza and Stuart Shieber.
2004.
A learning approach toimproving sentence-level MT evaluation.
In Proceedings of the10th International Conference on Theoretical and Methodolog-ical Issues in Machine Translation, Baltimore, MD, US.Irene Langkilde-Geary.
2002.
An empirical verification of cov-erage and correctness for a general-purpose sentence generator.In Proceedings of the International Natural Language Genera-tion Conference (INLG) 2002, pages 17?24.Shimei Pan and James Shaw.
2004.
Segue: A hybrid case-based surface natural language generator.
In Proceedings ofthe International Conference on Natural Language Generation(INLG) 2004, pages 130?140.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
Technical Report RC22176, IBM.Joseph Turian, Luke Shen, and I. Dan Melamed.
2003.
Evalua-tion of Machine Translation and its evaluation.
In Proceedingsof MT Summit IX, pages 23?28.Stephen Wan, Robert Dale, Mark Dras, and Ce?cile Paris.
2005.Searching for grammaticality: Propagating dependencies in theViterbi algorithm.
In Proceedings of the 10th European NaturalLanguage Processing Wworkshop, Aberdeen, UK.Michael Witbrock and Vibhu Mittal.
1999.
Ultra-summarization: A statistical approach to generating highly con-densed non-executive summaries.
In Proceedings of the 22ndInternational Conference on Research and Development in In-formation Retrieval (SIGIR?99).David Zajic, Bonnie Dorr, and Richard Schwartz.
2002.
Au-tomatic headline generation for newspaper stories.
In Pro-ceedings of the ACL-2002 Workshop on Text Summarization(DUC2002), pages 78?85.351
