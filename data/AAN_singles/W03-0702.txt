Conceptual Language Models for Dialog systemsRenato De MoriLIA CNRS  BP 122884911 Avignon Cedex 9 - Francerenato.demori, @lia.univ-avignon.frFrederic B?chetLIA CNRS  BP 122884911 Avignon Cedex 9 - Francefrederic.bechet, @lia.univ-avignon.fr12IntroductionThe purpose of computer speech understanding is tofind conceptual representations from signs coded intothe speech signal.Contrary to speech interpretation by humans in whichthe same discourse may be interpreted differently bydifferent subjects, for practical applications of computerunderstanding the result of interpretation should beunique for a given signal.
Usually it is represented by anobject which is an instance of class corresponding to asemantic structure which can be fairly complex even ifit is built with instances of conceptual constituents be-longing to a small set of major ontological categories.The mapping process that leads to a semantic interpreta-tion can be derived manually because human interpreta-tion of sentences can be completely explained with alogical formalism or it can be inferred by machinelearning algorithms in order to ensure a large coverageof possible sentence patterns.
Theories and practicalimplementations of these approaches are proposed in[1],[2].Limitations of coverage in the manual approach and inprecision of machine learning can be reduced by makingmanually a detailed analysis of a limited number of ex-amples and generalizing each analysis with automaticmethods.
In particular, a well structured lexicon can bevery useful, in which the meaning of words is repre-sented together with suggestions of possible syntacticand conceptual structures.Word associations found with networks of word rela-tions [3] can also be useful for suggesting compositionsof semantic constituents into conceptual structures.Thus, given an observed example, other examples canbe manually derived and generalized automatically.Computer understanding of a spoken sentences is prob-lem solving activity whose central engine is a searchprocess involving various types of models.Searching for concepts can be combined with searchingfor words.
This suggests that statistical language models(LMs) could be adapted based on expectations of con-cepts predicted by a system belief.
With this perspec-tive, it is important to notice that, while the observationof only certain words may be sufficient for hypothesiz-ing a conceptual structure, complete details of wordphrases expressing a conceptual structure have to beknown in order to adapt a generic LM to the expectationof such a structure.This paper introduces a search method and a learningparadigm based on the just introduced considerations.The search engine built with this method finds the bestcommon path between the system knowledge repre-sented by the composition of Stochastic Finite StateTransducers (SFST) and a Stochastic Finite StateAutomaton (SFSA) representing the lattice of word hy-potheses generated by an Automatic Speech Recogni-tion System (ASR).Hypothesis evaluation and searchLet a dialogue system have a belief which generatesexpectations B about conceptual structures.Expectation uncertainty is represented by a probabilitydistribution P(B) which is non-zero for a set of concep-tual structures expected at a given time.
Thus for a gen-eral concept structure ?
and a description Y of thespeech signal, one gets:)B,Y,(Pmax)B,Y,(P)Y,(P)Y,(Pmaxarg)Y|(Pmaxarg*BB????=??=?=???
{ })B,,W(P)W|Y(Pmaxarg*)B,,W(P)W|Y(Pmax)B,Y,,W(P)B,Y,(PB,W,WW???????=??
(1))B(P)B|W(P)BW|(P)B,W,(P ?=?A general concept structure ?
can be represented as astring of parenthesized terminals and non-terminals.These expressions can be decomposed into chunks.
Asentence may contain only one or more chunks of anincomplete structure, Thus, a system should be able togenerate interpretation hypotheses about parts of a con-ceptual structure.
In this case, symbol ?
makes refer-ence only to a set of components.Probability P(?|BW) can be simply set equal to 0 for aconceptual structure which cannot be inferred from W.If the conceptual structure is part of the expectations ofsystem beliefs and can be inferred unambiguously fromW, then P(?|BW) as in many practical applications in-cluding the one considered in this paper, then P(?|BW).Otherwise, let [  be the sequence of con-cept symbols corresponding to the preterminal symbolsin ?.
Probability P(?|BW) can be expressed as follows:]c....c...c1 ??
{ }?==??=?
????
?21111BW]c...c[|cP)BW|c(P)BW|]c....c...c([P)BW|(P(2)At least, for some values of ?
the probability { }BW]c...c[|cP 11 ???
is one for a class of applica-tions.Let ?
be the set of conceptual components, chunks ofthem or conceptual structures known to the system.
Ex-pectations derived from the system belief can begrouped into a set B1.
Let B2 the complement of B1w.r.t.
?
and F be a filler structure representing all theconceptual structures not in the application or just ig-nored by ignorance of the system knowledge.
B1, B2and F are the possible values for B in the (1) and theirprobabilities P(B) can be established subjectively or byevaluating counts for user responses consistent with thebelief, consistent with the application but not with thebelief and inconsistent with the application knowledge.Probability P(W|B) is that of an LM which is adapted tothe system belief.
It can be obtained with an LM built inthe following way.Each conceptual structure or part of it ?
is representedby a finite-state network N(?
).All the networks corresponding to structures in B1 areconnected in parallel in a single structure with associ-ated a probability P(B1).
A similar structure is built forthe automata corresponding to structures in B2.
A fillerF is also considered containing a network derived by atrigram LM.
A network N(?)
is obtained by the con-catenation of finite-state automata C(?)
inferred withthe procedure described in the next section representingchunks of knowledge with fillers F. These automataoutput components of conceptual structures.A search is performed by finding the most likely com-mon path in the network and in the automaton derivedfrom a lattice of word hypotheses generated by thespeech recognizer with the generic trigram LM.
Systembelief make vary the topology of the network by dy-namically changing the composition of  sets B1 and B2.Network recompilation can be avoided by just puttingall the N(?)
in parallel and dynamically assigning eachnetwork of B1 a probability :1B)1B(P)](N[P =?
(3)where 1B  indicates the number of elements in B1.Probabilities of networks in B2 are assigned in a similarway.A word sequence W always corresponds to a path in Fand may correspond to one or more conceptual struc-tures represented by paths in networks in B1 and B2.
Inthe second case, the likelihood of W in F will be muchlower than the likelihood in B1 or B2 because phrasesrecognized by the chunk automata of the network areboosted as it will be shown later.
Thus the best path forW, in this case, will go through a network whose auto-mata produce as output the components of a conceptualstructure.3 Knowledge inferenceUsually, when an application is developed, an evensmall training corpus is available.Semantic categories and functions are manually derivedfor an application.
They can be modified when the ap-plication is deployed in order to correct errors or addmissing constituents.A number of words in the lexicon have lexical entriescontaining their syntactic category, syntactic constructswhich can appear in the same sentence, semantic fea-tures and constructs they can be part of.
When one ofthese words is encountered in the training corpus, it isconsidered as a trigger for the semantic categories con-tained in its lexical entry.
The association betweenwords and semantic features is part of the semanticknowledge of the system.The presence of a category in the sentence under analy-sis can be verified manually or by deriving it from theparse tree of the sentence.
As lexical entries, grammarsand rules for deriving semantic structures from parsetrees may be imprecise or incomplete, a single examplecan be carefully examined and validated manually.Once a single example is available with a detailed syn-tactic and semantic analysis, it can be generalized.
Asentence may contain a complete or partial semanticstructure or just one component concept.
Let ?
representsuch a semantic interpretation.
Furthermore, each struc-ture may correspond to a pattern made of phrases andfillers of the sentence represented by a sequence ofwords W. Semantic Classification Trees (SCT) pro-posed in [1] can be used for automatically deriving sen-tence patterns corresponding to conceptual structures.The purpose of learning is to build or modify a SFSTthat accepts a sequence of words and output a semanticinterpretation ?.The initial analysis of an example starts by using a tag-ger for replacing words with their preterminal syntacticcategories.Then, semantic tags are automatically associated withsequences of syntactic tags manually or using the se-mantic knowledge.
A tag expression made of syntacticand semantic tags is obtained in this way as a represen-tation for of ?.
As a by-product, expressions for theconstituents of and components of  ?
are built andadded to the semantic knowledge.Generalization of the example uses a phrase generator toproduce sequences of words from the tag expression.These sequences of words enrich the finite state transla-tor which has to map word sequences into the concep-tual structure ?.Further generalization can be obtained by inferringsynonyms with a WordNet.
If generalization has pro-vided erroneous sequences of words, these sequencescan be removed  by manual inspection or when it is ob-served that the system has made an interpretation errorbecause of them.
With a similar procedure, new se-quences of words can be added to the automaton for ?.Once it has been found that a word (noun or verb) con-tributed to hypothesize a concept in the semantic struc-ture, the concept is added as semantic feature in thelexical entry of the word.In summary learning of semantic knowledge follows thefollowing steps:1 Set the semantic categories for the application.2 Set the lexical entries for the words that are semanti-cally relevant for the application.3 For every analyzed sentence?
if semantic interpretation is correct   then donothing,?
if a phrase is misplaced in the representation ofa semantic structure then remove it,?
if a phrase is missed in the representation of asemantic structure, but the corresponding tagexpressions is present in the semantic knowl-edge, then the phrase is added to the corre-sponding SFST,?
if the tag expression does not exist in the se-mantic knowledge, then it is built and se-quences of words are generated from it withthe above outlined generalization procedure.A set of SFST is built in this way.
They are added to theLM to provide concept specific components and to pro-duce semantic interpretations at the same time with atranslation process.References[1] Kuhn R.  and De Mori R. (1995).
The Application ofSemantic Classification Trees to Natural Language Un-derstanding.
IEEE Trans.
on Pattern Analysis and Ma-chine Intelligence, 17 : 449-460.
[2] Pieraccini R., Levin E., and Lee C.-H. (1991).
Sto-chastic Representation of Conceptual Structure in theATIS Task.
Proceedings of the, 1991 Speech and Natu-ral Language Workshop, 121-124, Morgan Kaufmannpubl, Los Altos, CA.
[3] Vossen P. Diez-Orzas P. and Peters W., (1997)The multilingual design of EuroWordnet.Proc ACL/EACL workshop on automatic informationextraction and building of lexical semantic resources forNLP applications, Madrid, 1997.
