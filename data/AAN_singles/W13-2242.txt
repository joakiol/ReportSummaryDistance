Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 343?351,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsReferential Translation Machines for Quality EstimationErgun Bic?iciCentre for Next Generation Localisation,Dublin City University, Dublin, Ireland.ergun.bicici@computing.dcu.ieAbstractWe introduce referential translation ma-chines (RTM) for quality estimation oftranslation outputs.
RTMs are a computa-tional model for identifying the translationacts between any two data sets with re-spect to a reference corpus selected in thesame domain, which can be used for esti-mating the quality of translation outputs,judging the semantic similarity betweentext, and evaluating the quality of studentanswers.
RTMs achieve top performancein automatic, accurate, and language inde-pendent prediction of sentence-level andword-level statistical machine translation(SMT) quality.
RTMs remove the need toaccess any SMT system specific informa-tion or prior knowledge of the training dataor models used when generating the trans-lations.
We develop novel techniques forsolving all subtasks in the WMT13 qual-ity estimation (QE) task (QET 2013) basedon individual RTM models.
Our resultsachieve improvements over last year?s QEtask results (QET 2012), as well as ourprevious results, provide new features andtechniques for QE, and rank 1st or 2nd inall of the subtasks.1 IntroductionQuality Estimation Task (QET) (Callison-Burch etal., 2012; Callison-Burch et al 2013) aims to de-velop quality indicators for translations and pre-dictors without access to the references.
Predic-tion of translation quality is important because theexpected translation performance can help in esti-mating the effort required for correcting the trans-lations during post-editing by human translators.Bicici et al(2013) develop the Machine Trans-lation Performance Predictor (MTPP), a state-of-the-art, language independent, and SMT systemextrinsic machine translation performance predic-tor, which achieves better performance than thecompetitive QET baseline system (Callison-Burchet al 2012) by just looking at the test source sen-tences and becomes the 2nd overall after also look-ing at the translation outputs in QET 2012.In this work, we introduce referential translationmachines (RTM) for quality estimation of transla-tion outputs, which is a computational model foridentifying the acts of translation for translatingbetween any given two data sets with respect toa reference corpus selected in the same domain.RTMs reduce our dependence on any task depen-dent resource.
In particular, we do not use thebaseline software or the SMT resources providedwith the QET 2013 challenge.
We believe havingaccess to glass-box features such as the phrase ta-ble or the n-best lists is not realistic especially foruse-cases where translations may be provided bydifferent MT vendors (not necessarily from SMTproducts) or by human translators.
Even the priorknowledge of the training corpora used for build-ing the SMT models or any other model used whengenerating the translations diverges from the goalof independent and unbiased prediction of trans-lation quality.
Our results show that we do notneed to use any SMT system dependent informa-tion to achieve the top performance when predict-ing translation output quality.2 Referential Translation Machine(RTM)Referential translation machines (RTMs) providea computational model for quality and seman-tic similarity judgments using retrieval of rele-vant training data (Bic?ici and Yuret, 2011a; Bic?ici,2011) as interpretants for reaching shared seman-tics (Bic?ici, 2008).
RTMs achieve very good per-formance in judging the semantic similarity ofsentences (Bic?ici and van Genabith, 2013a) andwe can also use RTMs to automatically assess the343correctness of student answers to obtain better re-sults (Bic?ici and van Genabith, 2013b) than thestate-of-the-art (Dzikovska et al 2012).RTM is a computational model for identifyingthe acts of translation for translating between anygiven two data sets with respect to a reference cor-pus selected in the same domain.
RTM can beused for predicting the quality of translation out-puts.
An RTM model is based on the selection ofcommon training data relevant and close to boththe training set and the test set of the task wherethe selected relevant set of instances are called theinterpretants.
Interpretants allow shared semanticsto be possible by behaving as a reference point forsimilarity judgments and providing the context.
Insemiotics, an interpretant I interprets the signsused to refer to the real objects (Bic?ici, 2008).RTMs provide a model for computational seman-tics using interpretants as a reference accordingto which semantic judgments with translation actsare made.
Each RTM model is a data translationmodel between the instances in the training setand the test set.
We use the FDA (Feature De-cay Algorithms) instance selection model for se-lecting the interpretants (Bic?ici and Yuret, 2011a)from a given corpus, which can be monolingualwhen modeling paraphrasing acts, in which casethe MTPP model (Section 2.1) is built using theinterpretants themselves as both the source and thetarget side of the parallel corpus.
RTMs map thetraining and test data to a space where translationacts can be identified.
We view that acts of transla-tion are ubiquitously used during communication:Every act of communication is an act oftranslation (Bliss, 2012).Translation need not be between different lan-guages and paraphrasing or communication alsocontain acts of translation.
When creating sen-tences, we use our background knowledge andtranslate information content according to the cur-rent context.
Given a training set train, a testset test, and some monolingual corpus C, prefer-ably in the same domain as the training and testsets, the RTM steps are:1.
T = train ?
test.2.
select(T, C)?
I3.
MTPP(I,train)?
Ftrain4.
MTPP(I,test)?
Ftest5.
learn(M,Ftrain)?M6.
predict(M,Ftest)?
q?Step 2 selects the interpretants, I, relevant to theinstances in the combined training and test data.Steps 3 and 4 use I to map train and testto a new space where similarities between transla-tion acts can be derived more easily.
Step 5 trainsa learning model M over the training features,Ftrain, and Step 6 obtains the predictions.
RTMrelies on the representativeness of I as a mediumfor building translation models for translating be-tween train and test.Our encouraging results in the QET challengeprovides a greater understanding of the acts oftranslation we ubiquitously use when communi-cating and how they can be used to predict theperformance of translation, judging the semanticsimilarity between text, and evaluating the qual-ity of student answers.
RTM and MTPP modelsare not data or language specific and their mod-eling power and good performance are applicableacross different domains and tasks.
RTM expandsthe applicability of MTPP by making it feasiblewhen making monolingual quality and similarityjudgments and it enhances the computational scal-ability by building models over smaller but morerelevant training data as interpretants.2.1 The Machine Translation PerformancePredictor (MTPP)In machine translation (MT), pairs of source andtarget sentences are used for training statisticalMT (SMT) models.
SMT system performance isaffected by the amount of training data used aswell as the closeness of the test set to the trainingset.
MTPP (Bic?ici et al 2013) is a state-of-the-art and top performing machine translation per-formance predictor, which uses machine learningmodels over features measuring how well the testset matches the training set to predict the qualityof a translation without using a reference trans-lation.
MTPP measures the coverage of individ-ual test sentence features and syntactic structuresfound in the training set and derives feature func-tions measuring the closeness of test sentences tothe available training data, the difficulty of trans-lating the sentence, and the presence of acts oftranslation for data transformation.2.2 MTPP Features for Translation ActsMTPP uses n-gram features defined over text orcommon cover link (CCL) (Seginer, 2007) struc-tures as the basic units of information over whichsimilarity calculations are made.
Unsupervised344parsing with CCL extracts links from base wordsto head words, resulting in structures represent-ing the grammatical information instantiated in thetraining and test data.
Feature functions use statis-tics involving the training set and the test sen-tences to determine their closeness.
Since they arelanguage independent, MTPP allows quality esti-mation to be performed extrinsically.We extend MTPP (Bic?ici et al 2013) in itslearning module, the features included, and theirrepresentations.
Categories for the 308 features(S for source, T for target) used are listed belowwhere the number of features are given in {#} andthe detailed descriptions for some of the featuresare presented in (Bic?ici et al 2013).?
Coverage {110}: Measures the degree towhich the test features are found in the train-ing set for both S ({56}) and T ({54}).?
Synthetic Translation Performance {6}: Cal-culates translation scores achievable accord-ing to the n-gram coverage.?
Length {7}: Calculates the number of wordsand characters for S and T and their averagetoken lengths and their ratios.?
Feature Vector Similarity {16}: Calculatessimilarities between vector representations.?
Perplexity {90}: Measures the fluency ofthe sentences according to language models(LM).
We use both forward ({30}) and back-ward ({15}) LM features for S and T.?
Entropy {9}: Calculates the distributionalsimilarity of test sentences to the training setover top N retrieved sentences.?
Retrieval Closeness {24}: Measures the de-gree to which sentences close to the test setare found in the selected training set, I, us-ing FDA (Bic?ici and Yuret, 2011a).?
Diversity {6}: Measures the diversity of co-occurring features in the training set.?
IBM1 Translation Probability {16}: Cal-culates the translation probability of testsentences using the selected training set,I, (Brown et al 1993).?
IBM2 Alignment Features {11}: Calculatesthe sum of the entropy of the distribution ofalignment probabilities for S (?s?S ?p log pfor p = p(t|s) where s and t are tokens) andT, their average for S and T, the number of en-tries with p ?
0.2 and p ?
0.01, the entropyof the word alignment between S and T andits average, and word alignment log probabil-ity and its value in terms of bits per word.?
Minimum Bayes Retrieval Risk {4}: Calcu-lates the translation probability for the trans-lation having the minimum Bayes risk amongthe retrieved training instances.?
Sentence Translation Performance {3}: Cal-culates translation scores obtained accord-ing to q(T,R) using BLEU (Papineni etal., 2002), NIST (Doddington, 2002), orF1 (Bic?ici and Yuret, 2011b) for q.?
Character n-grams {4}: Calculates cosinebetween character n-grams (for n=2,3,4,5)obtained for S and T (Ba?r et al 2012).?
LIX {2}: Calculates the LIX readabilityscore (Wikipedia, 2013; Bjo?rnsson, 1968) forS and T. 1For retrieval closeness, we use FDA insteadof dice for sentence selection.
We also improveFDA?s instance selection score by scaling with thelength of the sentence (Bic?ici and Yuret, 2011a).IBM2 alignments and their probabilities are ob-tained by first obtaining IBM1 alignments andprobabilities, which become the starting point forthe IBM2 model.
Both models are trained for 25to 75 iterations or until convergence.3 Quality Estimation Task ResultsWe participate in all of the four challenges of thequality estimation task (QET) (Callison-Burchet al 2013), which include English to Spanish(en-es) and German to English translation direc-tions.
There are two main categories of chal-lenges: sentence-level prediction (Task 1.
*) andword-level prediction (Task 2).
Task 1.1 is aboutpredicting post-editing effort (PEE), Task 1.2 isabout ranking translations from different systems,Task 1.3 is about predicting post-editing time(PET), and Task 2 is about binary or multi-classclassification of word-level quality.For each task, we develop RTM mod-els using the parallel corpora and the LMcorpora distributed by the translation task(WMT13) (Callison-Burch et al 2013) and theLM corpora provided by LDC for English andSpanish 2.
The parallel corpora contain 4.3Msentences for de-en with 106M words for de and111M words for en and 15M sentences for en-eswith 406M words for en and 455M words for1LIX=AB + C 100A , where A is the number of words, C iswords longer than 6 characters, B is words that start or endwith any of ?.
?, ?
:?, ?!
?, ???
similar to (Hagstro?m, 2012).2English Gigaword 5th, Spanish Gigaword 3rd edition.345es.
We do not use any resources provided byQET including data, software, or baseline featuressince they are SMT system dependent or languagespecific.
Instance selection for the training set andthe language model (LM) corpus is handled by aparallel implementation of FDA (Bic?ici, 2013).We tokenize and true-case all of the corpora.
Thetrue-caser is trained on all of the training corpususing Moses (Koehn et al 2007).
We prepare thecorpora by following this procedure: tokenize ?train the true-caser ?
true-case.
Table 1 lists thestatistics of the data used in the training and testsets for the tasks.Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3 & 2Trainsents 2254 32730 22338 803words 63K (en) 762K (de) 528K (en) 18K (en)67K (es) 786K (en) 559K (es) 20K (es)Test sents 500 1810 1315 284Table 1: Data statistics for different tasks.
Thenumber of words is listed after tokenization.Since we do not know the best training setsize that will maximize the performance, we relyon previous SMT experiments (Bic?ici and Yuret,2011a; Bic?ici and Yuret, 2011b) and quality es-timation challenges (Bic?ici and van Genabith,2013a; Bic?ici and van Genabith, 2013b) to selectthe proper training set size.
For each training andtest sentence provided in each subtask, we choosebetween 65 and 600 sentences from the paralleltraining corpora to be added to the training set,which creates roughly 400K sentences for train-ing.
We add the selected training set to the 8 mil-lion sentences selected for each LM corpus.
Thestatistics of the training data selected by the par-allel FDA and used as interpretants in the RTMmodels is given in Table 2.Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3 2sents 406K 318K 299K 398K 397Kwords 6.3M (en) 4.8M (de) 4.3M (en) 6.6M (en) 6.6M (en)6.9M (es) 4.9M (en) 4.6M (es) 7.2M (es) 7.2M (es)Table 2: Statistics of the training data used as in-terpretants in the RTM models in thousands (K) ofsentences or millions (M) of words.3.1 EvaluationIn this section, we describe the metrics we use toevaluate the learning performance.
Let yi repre-sent the actual target value for instance i, y?
themean of the actual target values, y?i the value es-timated by the learning model, and ?
?y the mean ofthe estimated target values, then we use the fol-lowing metrics to evaluate the learning models:?
Mean Absolute Error (MAE): |?| =?ni=1 |y?i?yi|n?
Relative Absolute Error (RAE) : ?
?|| =?ni=1 |y?i?yi|?ni=1 |y??yi|?
Root Mean Squared Error: RMSE =??ni=1(y?i?yi)2n?
DeltaAvg: ??
(V, S) =1|S|/2?1?|S|/2n=2(?n?1k=1?s?
?ki=1 qiV (s)|?ki=1 qi|)?
Correlation: r =?ni=1(y?i???y)(yi?y?)??ni=1(y?i???y)2??ni=1(yi?y?
)2DeltaAvg (Callison-Burch et al 2012) calculatesthe average quality difference between the scoresfor the top n ?
1 quartiles and the overall qualityfor the test set.
Relative absolute error measuresthe error relative to the error when predicting theactual mean.
We use the coefficient of determina-tion, R2 = 1 ?
?ni=1(y?i ?
yi)2/?ni=1(y?
?
yi)2,during optimization where the models areregression based and higher R2 values are better.3.2 Task 1: Sentence-level Prediction ofQualityIn this subsection, we develop techniques for theprediction of quality at the sentence-level.
We firstdiscuss the learning models we use and how weoptimize them and then provide the results for theindividual subtasks and the settings used.3.2.1 Learning Models and OptimizationThe learning models we use for predicting thetranslation quality include the ridge regression(RR) and support vector regression (SVR) withRBF (radial basis functions) kernel (Smola andScho?lkopf, 2004).
Both of these models learna regression function using the features to esti-mate a numerical target value such as the HTERscore, the F1 score (Bic?ici and Yuret, 2011b), orthe PET score.
We also use these learning modelsafter a feature subset selection with recursive fea-ture elimination (RFE) (Guyon et al 2002) or adimensionality reduction and mapping step usingpartial least squares (PLS) (Specia et al 2009),both of which are described in (Bic?ici et al 2013).The learning parameters that govern the behaviorof RR and SVR are the regularization ?
for RR andthe C, ?, and ?
parameters for SVR.
We optimize346the learning parameters, the number of featuresto select, and the number of dimensions used forPLS.
More detailed description of the optimiza-tion process is given in (Bic?ici et al 2013).
Inour submissions, we only used the results we ob-tained from SVR and SVR after PLS (SVRPLS)since they perform the best during training.Optimization can be a challenge for SVR due tothe large number of parameter settings to search.In this work, we decrease the search space by se-lecting ?
close to the theoretically optimal values.We select ?
close to the standard deviation of thenoise in the training set since the optimal valuefor ?
is shown to have linear dependence to thenoise level for different noise models (Smola et al1998).
We use RMSE of RR on the training set asan estimate for the noise level (?
of noise) and thefollowing formulas to obtain the ?
with ?
= 3:?
= ??
?lnnn (1)and the C (Cherkassky and Ma, 2004; Chal-imourda et al 2004):C = max(|y?
+ 3?y|, |y?
?
3?y|) (2)Since the C obtained could be low (Chalimourdaet al 2004), we use a range of C values in ad-dition to the obtained C value including C valueswith a couple of ?y values larger.Table 3 lists the RMSE of the RR model on thetraining set and the corresponding ?
and C val-ues for different subtasks.
We also present the op-timized parameter values for SVR and SVRPLS.Table 3 shows that, empirically, Equation 1 andEquation 2 gives results close to the best parame-ters found after optimization.Task 1.1 1.2 (de-en) 1.2 (en-es) 1.3RMSE RR .1397 .1169 .1569 68.06?
.0245 .0062 .01 18.64C .8398 .8713 1.02 371.28C?
(SVR) .8398 .5 .5 100?
(SVR) .0005 .001 .0001 .0005C?
(SVRPLS) 1.5 .8713 1.02 100?
(SVRPLS) .0001 .0001 .0001 .001# dim (SVRPLS) 60 60 60 60Table 3: Optimal parameters predicted by Equa-tion 1 and Equation 2 and the optimized parame-ter values, C?
and ?
for SVR and SVRPLS and thenumber of dimensions (# dim) for SVRPLS.3.2.2 Task 1.1: Scoring and Ranking forPost-Editing EffortTask 1.1 involves the prediction of the case insen-sitive translation edit rate (TER) scores obtainedby TERp (Snover et al 2009) and their ranking.In contrast, we derive features over sentences thatare true-cased.
We obtain the rankings by sortingaccording to the predicted TER scores.Task 1.1 R2 r RMSE MAE RAERR 0.3510 0.5965 0.1393 0.1086 0.7888RR PLS 0.4232 0.6509 0.1313 0.1023 0.7430SVR 0.4394 0.6647 0.1295 0.0967 0.7023SVR PLS 0.4305 0.6569 0.1305 0.1003 0.7284Table 4: Task1.1 results on the training set.Table 4 presents the learning performance onthe training set using the optimized parameters.We are able to significantly improve the resultswhen compared with the QET 2012 (Callison-Burch et al 2012) and our previous results (Bic?iciet al 2013) especially in terms of MAE and RAE.The results on the test set are given in Table 5.Rank lists the overall ranking in the task.
RTMswith SVR PLS learning is able to achieve the toprank in this task.Ranking DeltaAvg r RankCNGL SVRPLS 11.09 0.55 1CNGL SVR 9.88 0.51 4Scoring MAE RMSE RankCNGL SVRPLS 13.26 16.82 3CNGL SVR 13.85 17.28 8Table 5: Task1.1 results on the test set.3.2.3 Task 1.2: Ranking Translations fromDifferent SystemsTask 1.2 involves the prediction of the rankingamong up to 5 translation outputs produced by dif-ferent MT systems.
Evaluation is done againstthe human rankings using the Kendall?s ?
corre-lation (Callison-Burch et al 2013): ?
= (c ?d)/n(n?1)2 = c?dc+d where a pair is concordant, c, ifthe ordering agrees, discordant, d, if their orderingdisagrees, and neither concordant nor discordant iftheir rankings are equal.We use sentence-level F1 scores (Bic?ici andYuret, 2011b) as the target to predict.
We useF1 because it can be easily interpreted and it cor-relates well with human judgments (more thanTER) (Bic?ici and Yuret, 2011b; Callison-Burch etal., 2011).
We also found that the ?
of the rank-ings obtained according to the F1 score over the347training set (0.2040) is better than BLEU (Pap-ineni et al 2002) (0.1780) and NIST (Dodding-ton, 2002) (0.1907) for de-en.
Table 6 presents thelearning performance on the training set using theoptimized parameters.
Learning F1 becomes aneasier task than learning TER as observed from theresults but we have significantly more training in-stances.
We use the SVR model for predicting theF1 scores on the training set and the test set.
MAEis a more important performance metric here sincewe want to be as precise as possible when predict-ing the actual performance.Task 1.2 R2 r RMSE MAE RAEde-en RR 0.6320 0.7953 0.1169 0.0733 0.5535SVR 0.7528 0.8692 0.0958 0.0463 0.3494en-es RR 0.5101 0.7146 0.1569 0.1047 0.6323SVR 0.4819 0.7018 0.1613 0.0973 0.5873Table 6: Task1.2 results on the training set.Our next goal is to learn a threshold for judg-ing if two translations are equal over the predictedF1 scores.
This threshold is used to determinewhether we need to alter the ranking.
We tryto mimic the human decision process when de-termining whether two translations are equivalent.On some occasions where the sentences are closeenough, humans give them equal ranking.
Thisis also related to the granularity of the differencesvisible with a 1 to 5 ranking schema.We compared different threshold formulationsand used the following condition in our submis-sions to decide whether the ranking of item i in aset S of translations, i ?
S, should be different:?j 6=iF1(j)?
F1(i)|j ?
i| /|S| > t, (3)where t is the optimized threshold minimizing thefollowing loss for n training instances:n?i=1?
(f(t, qi), ri) (4)where f(t, qi) is a function returning rankingsbased on the threshold t and the quality scores forinstance i, qi and ?
(rj , ri) calculates the ?
scorebased on the rankings rj and ri.For both de-en and en-es subtasks, we found thethresholds obtained to be very similar or the same.The optimized values are given in Table 7.
On thetest set, we used the same threshold, t = 0.00275for both de-en and en-es, which is a little higherthan the optimal t to prevent overfitting.Task 1.2 ?
t # same # allde-en .2339 .00013 236 25644.2287 .00275 494en-es .2801 .00073 136 17752.2764 .00275 233Table 7: Task1.2 optimized thresholds and thecorresponding comparisons that were found to beequal (# same) over all comparisons (# all).We believe that human judgments of linguis-tic equality and the corresponding thresholds welearned in this work can be useful for developingbetter automatic evaluation metrics and can im-prove the correlation of the scores obtained withhuman judgments (as we did here).
The results onthe test set are given in Table 8.
We are also ableto achieve the top ranking in this task.Ties penalized model ?
Rankde-en CNGL SVRPLS F1 0.17 3CNGL SVR F1 0.17 4en-es CNGL SVRPLS F1 0.15 1CNGL SVR F1 0.13 2Ties ignored model ?
Rankde-en CNGL SVRPLS F1 0.17 3CNGL SVR F1 0.17 4en-es CNGL SVRPLS F1 0.16 2CNGL SVR F1 0.13 3Table 8: Task1.2 results on the test set.3.2.4 Task 1.3: Predicting Post-Editing TimeTask 1.3 involves the prediction of the post-editingtime (PET) for a translator to post-edit the MT out-put.
Table 9 presents the learning performance onthe training set using the optimized parameters.Task 1.3 R2 r RMSE MAE RAERR 0.4463 0.6702 68.0628 39.5250 0.6694RR PLS 0.5917 0.7716 58.4464 35.8759 0.6076SVR 0.4062 0.6753 70.4853 36.5132 0.6184SVR PLS 0.5316 0.7604 62.6031 33.5490 0.5682Table 9: Task1.3 results on the training set.The results on the test set are given in Table 10.We are able to become the 2nd best system accord-ing to MAE in this task.3.3 Task 2: Word-level Prediction of QualityIn this subsection, we develop a learning model,global linear models with dynamic learning rate(GLMd), for the prediction of quality at the word-level where the word-level quality is a binary (K:keep, C: change) or multi-class classification prob-lem (K: keep, S: substitute, D: delete).
We firstdiscuss the GLMd learning model, then we present348Task 1.3 MAE RankCNGL SVR 49.2121 3CNGL SVRPLS 49.6161 4RMSE RankCNGL SVRPLS 86.6175 4CNGL SVR 90.3650 7Table 10: Task1.3 results on the test set.the word-level features we use, and then presentour results on the test set.3.3.1 Global Linear Models with DynamicLearning (GLMd)Collins (2002) develops global learning models(GLM), which rely on Viterbi decoding, percep-tron learning, and flexible feature definitions.
Weextend the GLM framework by parallel percep-tron training (McDonald et al 2010) and dynamiclearning with adaptive weight updates in the per-ceptron learning algorithm:w = w + ?
(?
(xi, yi)?
?
(xi, y?))
, (5)where ?
returns a global representation for in-stance i and the weights are updated by ?
=exp(log10(3?1/0)) with ?1 and 0 representingthe error of the previous and first iteration respec-tively.
?
decays the amount of the change duringweight updates at later stages and prevents largefluctuations with updates.
We used both the GLMmodel and the GMLd models in our submissions.3.3.2 Word-level FeaturesWe introduce a number of novel features for theprediction of word-level translation quality.
Inbroad categories, these word-level features are:?
CCL: Uses CCL links.?
Word context: Surrounding words.?
Word alignments: Alignments, their probabili-ties, source and target word contexts.?
Length: Word lengths, n-grams over them.?
Location: Location of the words.?
Prefix and Suffix: Word prefixes, suffixes.?
Form: Capital, contains digit or punctuation.We found that CCL links are the most discrimi-native feature among these.
In total, we used 511Kfeatures for binary and 637K for multi-class clas-sification.
The learning curve is given in Figure 1.The results on the test set are given in Table 11.P, R, and A stand for precision, recall, and accu-racy respectively.
We are able to become the 2ndaccording to A in this task.Figure 1: Learning curve with the parallel GLMand GLMd models.Binary A P R F1 Rank (A)CNGL dGLM .7146 .7392 .9261 .8222 2CNGL GLM .7010 .7554 .8581 .8035 5Multi-class A RankCNGL dGLM .7162 3CNGL GLM .7116 4Table 11: Task 2 results on the test set.4 ContributionsReferential translation machines achieve top per-formance in automatic, accurate, and language in-dependent prediction of sentence-level and word-level statistical machine translation (SMT) qual-ity.
RTMs remove the need to access any SMTsystem specific information or prior knowledge ofthe training data or models used when generatingthe translations.
We develop novel techniques forsolving all subtasks in the quality estimation (QE)task (QET 2013) based on individual RTM mod-els.
Our results achieve improvements over lastyear?s QE task results (QET 2012), as well as ourprevious results, provide new features and tech-niques for QE, and rank 1st or 2nd in all of thesubtasks.AcknowledgmentsThis work is supported in part by SFI(07/CE/I1142) as part of the Centre for NextGeneration Localisation (www.cngl.ie) at DublinCity University and in part by the European Com-mission through the QTLaunchPad FP7 project(No: 296347).
We also thank the SFI/HEA IrishCentre for High-End Computing (ICHEC) for theprovision of computational facilities and support.349ReferencesDaniel Ba?r, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In *SEM 2012: The First JointConference on Lexical and Computational Seman-tics ?
Volume 1: Proceedings of the main conferenceand the shared task, and Volume 2: Proceedings ofthe Sixth International Workshop on Semantic Eval-uation (SemEval 2012), pages 435?440, Montre?al,Canada, 7-8 June.
Association for ComputationalLinguistics.Ergun Bic?ici and Josef van Genabith.
2013a.
CNGL-CORE: Referential translation machines for measur-ing semantic similarity.
In *SEM 2013: The SecondJoint Conference on Lexical and Computational Se-mantics, Atlanta, Georgia, USA, 13-14 June.
Asso-ciation for Computational Linguistics.Ergun Bic?ici and Josef van Genabith.
2013b.
CNGL:Grading student answers by acts of translation.
In*SEM 2013: The Second Joint Conference on Lex-ical and Computational Semantics and Proceedingsof the Seventh International Workshop on SemanticEvaluation (SemEval 2013), Atlanta, Georgia, USA,14-15 June.
Association for Computational Linguis-tics.Ergun Bic?ici and Deniz Yuret.
2011a.
Instance se-lection for machine translation using feature decayalgorithms.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 272?283,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Ergun Bic?ici and Deniz Yuret.
2011b.
RegMT systemfor machine translation, system combination, andevaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 323?329,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Ergun Bic?ici, Declan Groves, and Josef van Genabith.2013.
Predicting sentence translation quality usingextrinsic and language independent features.
Ma-chine Translation.Ergun Bic?ici.
2011.
The Regression Model of MachineTranslation.
Ph.D. thesis, Koc?
University.
Supervi-sor: Deniz Yuret.Ergun Bic?ici.
2013.
Feature decay algorithms for fastdeployment of accurate statistical machine transla-tion systems.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Ergun Bic?ici.
2008.
Consensus ontologies in sociallyinteracting multiagent systems.
Journal of Multia-gent and Grid Systems.Carl Hugo Bjo?rnsson.
1968.
La?sbarhet.
Liber.Chris Bliss.
2012.
Comedy is transla-tion, February.
http://www.ted.com/talks/chris bliss comedy is translation.html.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263?311, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omer F. Zaidan.
2011.
Findings of the 2011Workshop on Statistical Machine Translation.
InProceedings of the Sixth Workshop on StatisticalMachine Translation, Edinburgh, England, July.
As-sociation for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montre?al, Canada, June.
Association forComputational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2013.Findings of the 2013 workshop on statistical ma-chine translation.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, pages10?51.
Association for Computational Linguistics,August.Athanassia Chalimourda, Bernhard Scho?lkopf, andAlex J. Smola.
2004.
Experimentally optimal?
in support vector regression for different noisemodels and parameter settings.
Neural Networks,17(1):127?141, January.Vladimir Cherkassky and Yunqian Ma.
2004.
Practicalselection of svm parameters and noise estimation forsvm regression.
Neural Netw., 17(1):113?126, Jan-uary.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing - Volume 10, EMNLP?02, pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, pages 138?145, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Myroslava O. Dzikovska, Rodney D. Nielsen, andChris Brew.
2012.
Towards effective tutorial feed-back for explanation questions: A dataset and base-lines.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association for350Computational Linguistics: Human Language Tech-nologies, pages 200?210, Montre?al, Canada, June.Association for Computational Linguistics.Isabelle Guyon, Jason Weston, Stephen Barnhill, andVladimir Vapnik.
2002.
Gene selection for cancerclassification using support vector machines.
Ma-chine Learning, 46(1-3):389?422.Kenth Hagstro?m.
2012.
Swedish readabil-ity calculator.
https://github.com/keha76/Swedish-Readability-Calculator.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of the Assoc.
for Compu-tational Linguistics, pages 177?180, Prague, CzechRepublic, June.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 456?464, Los Angeles, California,June.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Yoav Seginer.
2007.
Learning Syntactic Structure.Ph.D.
thesis, Universiteit van Amsterdam.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tu-torial on support vector regression.
Statistics andComputing, 14(3):199?222, August.A.
J. Smola, N. Murata, B. Scho?lkopf, and K.-R.Mu?ller.
1998.
Asymptotically optimal choice of?-loss for support vector machines.
In L. Niklas-son, M. Boden, and T. Ziemke, editors, Proceedingsof the International Conference on Artificial NeuralNetworks, Perspectives in Neural Computing, pages105?110, Berlin.
Springer.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy,or hter?
: exploring different human judgmentswith a tunable mt metric.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 259?268, Stroudsburg, PA,USA.
Association for Computational Linguistics.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In Proceedings of the 13th Annual Con-ference of the European Association for MachineTranslation (EAMT), pages 28?35, Barcelona, May.EAMT.Wikipedia.
2013.
Lix.http://en.wikipedia.org/wiki/LIX.351
