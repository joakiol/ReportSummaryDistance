Robust, Finite-State Parsing for Spoken Language UnderstandingEdward  C. Ka iserCenter for Spoken Language UnderstandingOregon Graduate Inst i tutePO Box 91000 Port land OR 97291kaiser?cse, ogi.
eduAbst rac tHuman understanding of spoken language ap-pears to integrate the use of contextual ex-pectations with acoustic level perception in atightly-coupled, sequential fashion.
Yet com-puter speech understanding systems typicallypass the transcript produced by a speech rec-ognizer into a natural anguage parser with nointegration of acoustic and grammatical con-straints.
One reason for this is the complex-ity of implementing that integration.
To ad-dress this issue we have created a robust, se-mantic parser as a single finite-state machine(FSM).
As such, its run-time action is less com-plex than other robust parsers that are basedon either chart or generalized left-right (GLR)architectures.
Therefore, we believe it is ul-timately more amenable to direct integrationwith a speech decoder.1 In t roduct ionAn important goal in speech processing is to ex-tract meaningful information: in this, the taskis understanding rather than transcription.
Forextracting meaning from spontaneous speechfull coverage grammars tend to be too brittle.In the 1992 DARPA ATIS task competition,CMU's Phoenix parser was the best scoring sys-tem (Issar and Ward, 1993).
Phoenix operatesin a loosely-coupled architecture on the 1-besttranscript produced by the recognizer.
Concep-tually it is a semantic ase-frame parser (Hayeset al, 1986).
As such, it allows slots within aparticular ease-frame to be filled in any order,and allows out-of-grammar words between slotsto be skipped over.
Thus it can return partialparses - -  as frames in which only some of theavailable slots have been filled.Humans appear to perform robust under-standing in a tightly-coupled fashion.
Theybuild incremental, partial analyses of an ut-terance as it is being spoken, in a way thathelps them to meaningfully interpret he acous-tic evidence.
To move toward machine under-standing systems that tightly-couple acousticfeatures and structural knowledge, researcherslike Pereira and Wright (1997) have argued forthe use of finite-state acceptors (FSAs) as anefficient means of integrating structural knowl-edge into the recognition process for limited do-main tasks.We have constructed a parser for spontaneousspeech that is at once both robust and finite-state.
It is called PROFER, for Predictive, RO-bust, Finite-state parsER.
Currently PROFERaccepts a transcript as input.
We are modifyingit to accept a word-graph as input.
Our aim isto incorporate PROFER directly into a recog-nizer.For example, using a grammar that defines e-quences of numbers (each of which is less thanten thousand and greater than ninety-nine andcontains the word "hundred"), inputs like thefollowing string can be robustly parsed by PRO-FER:Input:first I've got twenty ahhh thirty yaaaaaathirty ohh wait no twenty twenty ninehundred two errr three ahhh four and thentwo hundred ninety uhhhhh let me be surehere yaaaa ninety seven and last is fiveoh seven uhhh I mean sixParse-tree:\[fsType:numher_type,hundred_fs:\[decade:\[twenty,nine\],hundred,four\],hundred_fs:\[two,hundred,decade:\[ninety,seven\]\],hundred_fs:\[five,hundred,six\]\]573There are two characteristically "robust" ac-tions that are illustrated by this example.?
For each "slot" (i.e., "As" element) filled inthe parse-tree's case-frame structure, therewere several words both before and afterthe required word, hundred, that had to beskipped-over.
This aspect of robust parsingis akin to phrase-spotting.?
In mapping the words, "five oh seven uhhhI mean six," the parser had to choose alater-in-the-input parse (i.e., "\[five, hun-dred, six\]") over a heuristically equivalentearlier-in-the-input parse (i.e., "\[five, hun-dred, seven\]").
This aspect of robust pars-ing is akin to dynamic programming (i.e.,finding all possible start and end pointsfor all possible patterns and choosing thebest).2 Robust Finite-state ParsingCMU's Phoenix system is implemented as a re-cursive transition network (RTN).
This is sim-ilar to Abney's system of finite-state-cascades(1996).
Both parsers have a "stratal" system oflevels.
Both are robust in the sense of skippingover out-of-grammar areas, and building upstructural islands of certainty.
And both can befairly described as run-time chart-parsers.
How-ever, Abney's ystem inserts bracketing and tag-ging information by means of cascaded trans-ducers, whereas Phoenix accomplishes the samething by storing state information in the chartedges themselves - - thus using the chart edgeslike tokens.
PROFER is similar to Phoenix inthis regard.Phoenix performs a depth-first search over itstextual input, while Abney's "chunking" and"attaching" parsers perform best-first searches(1991).
However, the demands of a tightly-coupled, real-time system argue for a breadth-first search-strategy, which in turn argues forthe use of a finite-state parser, as an efficientmeans of supporting such a search strategy.PROFER is a strictly sequential, breadth-firstparser.PROFER uses a regular grammar formalismfor defining the patterns that it will parse fromthe input, as illustrated in Figures 1 and 2.Net name tags correspond to bracketed (i.e.,"tagged") elements in the output.
Aside from.
.
.
.
.
.
.
.
.
.
.
.
.
l ~ .~?
:3  ? "
"  7 .
.
.
.
.
.
.
.
.
"; ::::::::::::::::::::: ................ : ................................ ........... , ................... .
.
.
.
.
.
.
.
.
.
.
.  '
i .
.
.
.rip.gin ','~i ~.
\])~.
'.,i~ : : : i i~ \ ] ) ; ;~ .
:  .I rewrite patterns \]!
!Figure 1: Formalismnet names, a grammar definition can also con-tain non-terminal rewrite names and terminals.Terminals are directly matched against input?Non-terminal rewrite names group together sev-eral rewrite patterns (see Figure 2), just as netnames can be used to do, but rewrite names donot appear in the output.Each individual rewrite pattern defines a"conjunction" of particular terms or sub-patterns that can be mapped from the inputinto the non-terminal t the head of the patternblock, as illustrated in (Figure 1).
Whereas, thelist of patterns within a block represents a "dis-junction" (Figure 2).~i iii !i~agt ,a  , '~ i  \ [ id \ ]................................................. .. ~ ~ .. .
.
.
.
~ .
~:~:~(two) "\]ii~i : .
: : : : i~~ ii;i; ~|\[ii::: i ~  : \ ]  ........... ; .......................................................................................... .
.
.
.
.
.
.
.
.
.
{~!
i i : :~ i \ ]Figure 2: FormalismSince not all Context-Free Grammar (CFG)expressions can be translated into regular ex-pressions, as illustrated in Figure 3, some re-strictions are necessary to rule out the possibil-ity of "center-embedding" (see the right-mostblock in Figure 3).
The restriction is that nei-ther a net name nor a rewrite name can appearin one of its own descendant blocks of rewritepatterns.Even with this restriction it is still possibleto define regular grammars that allow for self-574Figure 3: Context-Free translations toembedding to any finite depth, by copying thenet or rewrite definition and giving it a uniquename for each level of self-embedding desired.For example, both grammars illustrated in Fig-ure 4 can robustly parse inputs that containsome number of a's followed by a matchingnumber of b's up to the level of embedding de-fined, which in both of these cases is four deep.EXAMPLE: nets EXAMPLE: rewrites[se] [ser](a [se_one] b) (a SE_ONE b)(a b) (a b)[se_one] SE_0NE(a [se_t~o] b) (a SE_TWO b)(a b) (a b)[se_two] SE_TWO(a [se_three] b) (a SE_THREE b)(a b) (a b)[se_three] SE_THREE(a b) (a b)INPUT : INPUT:a c a b d e  b ac  abd  ebPARSE: PARSE:se :  [a,se_one: [a,b] ,b] set: [a,a,b,b]Figure 4: Finite self-embedding.3 The  Power  of  Regu lar  GrammarsTomita (1986) has argued that context-freegrammars (CFGs) are over-powered for natu-ral language.
Chart parsers are designed todeal with the worst case of very-deep or infi-nite self-embedding allowed by CFGs.
How-ever, in natural language this worst case doesnot occur.
Thus, broad coverage GeneralizedLeft-Right (GLR) parsers based on Tomita's al-gorithm, which ignore the worst case scenario,case-flame style regular expressions.are in practice more efficient and faster thancomparable chart-parsers (Briscoe and Carroll,1993).PROFER explicitly disallows the worst caseof center-self-embedding that Tomita's GLR de-sign allows - -  but ignores.
Aside from infinitecenter-self-embedding, a regular grammar for-malism like PROFER's can be used to defineevery pattern in natural anguage definable bya GLR parser.4 The  Compi la t ion  ProcessThe following small grammar will serve as thebasis for a high-level description of the compi-lation process.
[s](n Iv] n)(p Iv] p)Iv](v)In Kaiser et al (1999) the relationship be-tween PROFER's compilation process and thatof both Pereira and Wright's (1997) FSAs andCMU's Phoenix system has been described.Here we wish to describe what happens dur-ing PROFER's compilation stage in terms ofthe Left-Right parsing notions of item-set for-mation and reduction.As compilation begins the FSM always startsat state 0:0 (i.e., net 0, start state 0) and tra-verses an arc labeled by the top-level net nameto the 0:1 state (i.e., net 0, final state 1), as il-lustrated in Figure 5.
This initial arc is then re-written by each of its rewr i te  pat terns  (Fig-ure 5).As each new net within the grammar descrip-tion is encountered it receives a unique net-IDnumber, the compilation descends recursivelyinto that new sub-net (Figure 5), reads in its575?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, ?Figure 5: Definition expansion.grammar description file, and compiles it.
Sincerewrite names are unique only within the net inwhich they appear, they can be processed iter-atively during compilation, whereas net namesmust be processed recursively within the scopeof the entire grammar's definition to allow forre-use.As each element within a rewr i te  pat ternis encountered a structure describing its exactcontext is filled in.
All terminals that appearin the same context are grouped together as a"context-group" or simply "context."
So arcs inthe final FSM are traversed by "contexts" notterminals.When a net name itself traverses an arc it isglued into place contextually with e arcs (i.e.,NULL arcs) (Figure 6).
Since net names, likeany other pattern element, are wrapped insideof a context structure before being situated inthe FSM, the same net name can be re-usedinside of many different contexts, as in Figure 6.Figure 6: Contextualizing sub-nets.As the end of each net  definition file isreached, all of its NULL arcs are removed.
Eachinitial state of a sub-net is assumed into its par-ent state - -  which is equivalent to item-set for-mation in that parent state (Figure 7 left-side).Each final state of a sub-net is erased, and itsincoming arcs are rerouted to its terminal par-ent's state, thus performing a reduction (Fig-ure 7 right-side).Figure 7: Removing NULL arcs.5 The  Pars ing  ProcessAt run-time, the parse proceeds in a strictlybreadth-first manner (Figure 8,(Kaiser et al,1999)).
Each destination state within a parseis named by a hash-table key string com-posed of a sequence of "net:state" combina-tions that uniquely identify the location of thatstate within the FSM (see Figure 8).
These"net:state" names effectively represent a snap-shot of the stack-configuration that would beseen in a parallel GLR parser.PROFER deals with ambiguity by "split-ting" the branches of its graph-structured stack(as is done in a Generalized Left-Right parser(Tomita, 1986)).
Each node within the graph-structured stack holds a "token" that recordsthe information needed to build a bracketedparse-tree for any given branch.When partial-paths converge on the samestate within the FSM they are scored heuris-tically, and all but the set of highest scoringpartial paths are pruned away.
Currently theheuristics favor interpretations that cover themost input with the fewest slots.
Command lineparameters can be used to refine the heuristics,so that certain kinds of structures be either min-imized or maximized over the parse.Robustness within this scheme is achieved byallowing multiple paths to be propagated in par-allel across the input space.
And as each such576..... - !ITFigure 8: The parsing process.partial-path is extended, it is allowed to skip-over terms in the input that are not licensed bythe grammar.
This allows all possible start andend times of all possible patterns to be consid-ered.6 D iscuss ionMany researchers have looked at ways to im-prove corpus-based language modeling tech-niques.
One way is to parse the training setwith a structural parser, build statistical mod-els of the occurrence of structural elements, andthen use these statistics to build or augment ann-gram language model.Gillet and Ward (1998) have reported reduc-tions in perplexity using a stochastic ontext-free grammar (SCFG) defining both simple se-mantic "classes" like dates and times, and de-generate classes for each individual vocabularyword.
Thus, in building up class statistics over acorpus parsed with their grammar they are ableto capture both the traditional n-gram word se-quences plus statistics about semantic lass se-quences.Briscoe has pointed out that using stochas-tic context-free grammars (SCFGs) as the ba-sis for language modeling, " .
.
.means that in-formation about the probability of a rule apply-ing at a particular point in a parse derivation islost" (1993).
For this reason Briscoe developeda GLR parser as a more "natural way to obtaina finite-state representation .
.
. "
on which thestatistics of individual "reduce" actions couldbe determined.
Since PROFER's state nameseffectively represent the stack-configurations ofa parallel GLR parser it also offers the ability toperform the full-context statistical parsing thatBriscoe has called for.Chelba and Jelinek (1999) use a struc-tural language model (SLM) to incorporate thelonger-range structural knowledge representedin statistics about sequences of phrase-head-word/non-terminal-tag elements exposed by atree-adjoining rammar.
Unlike SCFGs theirstatistics are specific to the structural contextin which head-words occur.
They have shownboth reduced perplexity and improved word er-ror rate (WER) over a conventional tri-gramsystem.One can also reduce complexity and improveword-error ates by widening the speech recog-nition problem to include modeling not onlythe word sequence, but the word/part-of-speech(POS) sequence.
Heeman and Allen (1997) hasshown that doing so also aids in identifyingspeech repairs and intonational boundaries inspontaneous speech.However, all of these approaches rely oncorpus-based language modeling, which is alarge and expensive task.
In many practical usesof spoken language technology, like using simplestructured ialogues for class room instruction(as can be done with the CSLU toolkit (Suttonet al, 1998)), corpus-based language modelingmay not be a practical possibility.In structured dialogues one approach canbe to completely constrain recognition by theknown expectations at a given state.
Indeed,the CSLU toolkit provides a generic recognizer,which accepts a set of vocabulary and word se-quences defined by a regular grammar on a per-state basis.
Within this framework the task ofa recognizer is to choose the best phonetic paththrough the finite-state machine defined by theregular grammar.
Out-of-vocabulary words areaccounted for by a general purpose "garbage"phoneme model (Schalkwyk et al, 1996).We experimented with using PROFER in thesame way; however, our initial attempts to doso did not work well.
The amount of informa-tion carried in PROFER's token's (to allow forbracketing and heuristic scoring of the seman-tic hypotheses) requires tructures that are anorder of magnitude larger than the tokens ina typical acoustic recognizer.
When these largetokens are applied at the phonetic-level so many577are needed that a memory space explosion oc-curs.
This suggests to us that there must be twolevels of tokens: small, quickly manipulated to-kens at the acoustic level (i.e., lexical level), andlarger, less-frequently used tokens at the struc-tural level (i.e., syntactic, semantic, pragmaticlevel).7 Future  WorkIn the MINDS system Young et al (1989) re-ported reduced word error rates and large re-ductions in perplexity by using a dialogue struc-ture that could track the active goals, topicsand user knowledge possible in a given dialoguestate, and use that knowledge to dynamicallycreate a semantic case-frame network, whosetransitions could in turn be used to constrainthe word sequences allowed by the recognizer.Our research aim is to maximize the effective-ness of this approach.
Therefore, we hope to:?
expand the scope of PROFER's  structuraldefinitions to include not only word pat-terns, but intonation and stress patterns aswell, and?
consider how build to general languagemodels that complement the use of the cat-egorial constraints PROFER can impose(i.e., syllable-level modeling, intonationalboundary modeling, or speech repair mod-eling).Our immediate fforts are focused on consider-ing how to modify PROFER to accept a word-graph as input - -  at first as part of a loosely--coupled system, and then later as part of anintegrated system in which the elements of theword-graph are evaluated against he structuralconstraints as they are created.8 ConclusionWe have presented our finite-state, robustparser, PROFER, described some of its work-ings, and discussed the advantages it may offerfor moving towards a tight integration of robustnatural language processing with a speech de-coder - -  those advantages being: its efficiencyas an FSM and the possibility that it may pro-vide a useful level of constraint to a recognizerindependent of a large, task-specific languagemodel.9 AcknowledgementsThe author was funded by the Intel ResearchCouncil, the NSF (Grant No.
9354959), andthe CSLU member consortium.
We also wishto thank Peter Heeman and Michael Johnstonfor valuable discussions and support.Re ferencess.
Abney.
1991.
Parsing by chunks.
In R. Berwick, S.Abney, and C. Termy, editors, Principle.Based Pars-ing.
Kluwer Academic Publishers.S.
Abney.
1996.
Partial parsing via finite-state cas-cades.
In Proceedings o/ the ESSLLI '96 Robust Pars-ing Workshop.T.
Briscoe and J. Carroll.
1993.
Generalized probabilis-tic LR parsing of natural anguage (corpora) withunification-based grammars.
Computational Linguis-tics, 19(1):25-59.C.
Chelba and F. Jelinek.
1999.
Recognition perfor-mance of a structured language model.
In The Pro-ceedings o/ Eurospeech '99 (to appear), September.J.
Gillet and W. Ward.
1998.
A language model combin-ing trigrams and stochastic context-free grammars.
InProceedings of ICSLP '98, volume 6, pgs 2319-2322.P.
J. Hayes, A. G. Hauptmann, J. G. Carbonell, and M.Tomita.
1986.
Parsing spoken language: a semanticcaseframe approach.
In l l th International Con\]erenceon Computational Linguistics, Proceedings of Coling'86, pages 587-592.P.
A. Heeman and J. F. Allen.
1997.
Intonational bound-aries, speech repairs, and discourse markers: Model-ing spoken dialog.
In Proceedings o~ the 35th AnnualMeeting o\] the Association \]or Computational Lin-guistics, pages 254-261.S.
Issar and W. Ward.
1993.
Cmu's robust spoken lan-guage understanding system.
In Eurospeech '93, pages2147-2150.E.
Kaiser, M. Johnston, and P. Heeman.
1999.
Profer:Predictive, robust finite-state parsing for spoken lan-guage.
In Proceedings o/ ICASSP '99.F.
C. N. Pereira nd R. N. Wright.
1997.
Finite-state ap-proximations of phrase-structure grammars.
In Em-manuel Roche and Yves Schabes, editors, Finite-StateLanguage Processing, pages 149-173.
The MIT Press.J.
Schalkwyk, L. D. Colton, and M. Fanty.
1996.
TheCSLU-sh toolkit for automatic speech recognition:Technical report no.
CSLU-011-96, August.S.
Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Ver-meulen, M. Macon, Y. Yan, E. Kaiser, B. Rundle,K.
Shobaki, P. Hosom, A. Kain, J. Wouters, M. Mas-saro, and M. Cohen.
1998.
Universal speech tools:the cslu toolkit".
In Proceedings of ICSLP '98, pages3221-3224, Nov..M. Tomita.
1986.
Efficient Parsing/or Natural Lan-guage: A Fast Algorithm \]or Practical Systems.Kluwer Academic Publishers.S.
R. Young, A. G. Hauptmann, W. H. Ward, E. T.Smith, and P. Werner.
1989.
High level knowledgesources in usable speech recognition systems.
Com-munications o\] the ACM, 32(2):183-194, February.578
