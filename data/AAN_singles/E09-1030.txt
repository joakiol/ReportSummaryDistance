Proceedings of the 12th Conference of the European Chapter of the ACL, pages 255?263,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsReconstructing false start errors in spontaneous speech textErin FitzgeraldJohns Hopkins UniversityBaltimore, MD, USAerinf@jhu.eduKeith HallGoogle, Inc.Zurich, Switzerlandkbhall@google.comFrederick JelinekJohns Hopkins UniversityBaltimore, MD, USAjelinek@jhu.eduAbstractThis paper presents a conditional ran-dom field-based approach for identifyingspeaker-produced disfluencies (i.e.
if andwhere they occur) in spontaneous speechtranscripts.
We emphasize false start re-gions, which are often missed in cur-rent disfluency identification approachesas they lack lexical or structural similar-ity to the speech immediately following.We find that combining lexical, syntac-tic, and language model-related featureswith the output of a state-of-the-art disflu-ency identification system improves over-all word-level identification of these andother errors.
Improvements are reinforcedunder a stricter evaluation metric requiringexact matches between cleaned sentencesannotator-produced reconstructions, andaltogether show promise for general re-construction efforts.1 IntroductionThe output of an automatic speech recognition(ASR) system is often not what is required for sub-sequent processing, in part because speakers them-selves often make mistakes (e.g.
stuttering, self-correcting, or using filler words).
A cleaner speechtranscript would allow for more accurate languageprocessing as needed for natural language process-ing tasks such as machine translation and conver-sation summarization which often assume a gram-matical sentence as input.A system would accomplish reconstruction ofits spontaneous speech input if its output wereto represent, in flawless, fluent, and content-preserving text, the message that the speaker in-tended to convey.
Such a system could also be ap-plied not only to spontaneous English speech, butto correct common mistakes made by non-nativespeakers (Lee and Seneff, 2006), and possibly ex-tended to non-English speaker errors.A key motivation for this work is the hope that acleaner, reconstructed speech transcript will allowfor simpler and more accurate human and natu-ral language processing, as needed for applicationslike machine translation, question answering, textsummarization, and paraphrasing which often as-sume a grammatical sentence as input.
This ben-efit has been directly demonstrated for statisticalmachine translation (SMT).
Rao et al (2007) gaveevidence that simple disfluency removal from tran-scripts can improve BLEU (a standard SMT eval-uation metric) up to 8% for sentences with disflu-encies.
The presence of disfluencies were found tohurt SMT in two ways: making utterances longerwithout adding semantic content (and sometimesadding false content) and exacerbating the datamismatch between the spontaneous input and theclean text training data.While full speech reconstruction would likelyrequire a range of string transformations and po-tentially deep syntactic and semantic analysis ofthe errorful text (Fitzgerald, 2009), in this workwe will first attempt to resolve less complex errors,corrected by deletion alone, in a given manually-transcribed utterance.We build on efforts from (Johnson et al, 2004),aiming to improve overall recall ?
especially offalse start or non-copy errors ?
while concurrentlymaintaining or improving precision.1.1 Error classes in spontaneous speechCommon simple disfluencies in sentence-like ut-terances (SUs) include filler words (i.e.
?um?, ?ah?,and discourse markers like ?you know?
), as well asspeaker edits consisting of a reparandum, an inter-ruption point (IP), an optional interregnum (like ?Imean?
), and a repair region (Shriberg, 1994), asseen in Figure 1.255[that?s]?
??
?reparandumIP???
?+ {uh}????interregnumthat?s?
??
?repaira reliefFigure 1: Typical edit region structure.
In theseand other examples, reparandum regions are inbrackets (?
[?, ?]?
), interregna are in braces (?{?,?}?
), and interruption points are marked by ?+?.These reparanda, or edit regions, can be classifiedinto three main groups:1.
In a repetition (above), the repair phrase isapproximately identical to the reparandum.2.
In a revision, the repair phrase alters reparan-dum words to correct the previously statedthought.EX1: but [when he] + {i mean} when she put itthat wayEX2: it helps people [that are going to quit] + thatwould be quitting anyway3.
In a restart fragment (also called a falsestart), an utterance is aborted and thenrestarted with a new train of thought.EX3: and [i think he?s] + he tells me he?s glad hehas one of thoseEX4: [amazon was incorporated by] {uh} well ionly knew two people thereIn simple cleanup (a precursor to full speech re-construction), all detected filler words are deleted,and the reparanda and interregna are deleted whilethe repair region is left intact.
This is a strong ini-tial step for speech reconstruction, though morecomplex and less deterministic changes are of-ten required for generating fluent and grammaticalspeech text.In some cases, such as the repetitions men-tioned above, simple cleanup is adequate for re-construction.
However, simply deleting the identi-fied reparandum regions is not always optimal.
Wewould like to consider preserving these fragments(for false starts in particular) if1.
the fragment contains content words, and2.
its information content is distinct from that insurrounding utterances.In the first restart fragment example (EX3 in Sec-tion 1.1), the reparandum introduces no new ac-tive verbs or new content, and thus can be safelydeleted.
The second example (EX4) howeverdemonstrates a case when the reparandum may beconsidered to have unique and preservable con-tent of its own.
Future work should address howto most appropriately reconstruct speech in thisand similar cases; this initial work will for riskinformation loss as we identify and delete thesereparandum regions.1.2 Related WorkStochastic approaches for simple disfluency de-tection use features such as lexical form, acousticcues, and rule-based knowledge.
Most state-of-the-art methods for edit region detection such as(Johnson and Charniak, 2004; Zhang and Weng,2005; Liu et al, 2004; Honal and Schultz, 2005)model speech disfluencies as a noisy channelmodel.
In a noisy channel model we assume thatan unknown but fluent string F has passed througha disfluency-adding channel to produce the ob-served disfluent string D, and we then aim to re-cover the most likely input string F?
, defined asF?
= argmaxFP (F |D)= argmaxFP (D|F )P (F )where P (F ) represents a language model defin-ing a probability distribution over fluent ?source?strings F , and P (D|F ) is the channel model defin-ing a conditional probability distribution of ob-served sentences D which may contain the typesof construction errors described in the previoussubsection.
The final output is a word-level tag-ging of the error condition of each word in the se-quence, as seen in line 2 of Figure 2.The Johnson and Charniak (2004) approach,referred to in this document as JC04, combinesthe noisy channel paradigm with a tree-adjoininggrammar (TAG) to capture approximately re-peated elements.
The TAG approach models thecrossed word dependencies observed when thereparandum incorporates the same or very similarwords in roughly the same word order, which JC04refer to as a rough copy.
Our version of this sys-tem does not use external features such as prosodicclasses, as they use in Johnson et al (2004), butotherwise appears to produce comparable resultsto those reported.While much progress has been made in sim-ple disfluency detection in the last decade, eventop-performing systems continue to be ineffec-tive at identifying words in reparanda.
To bet-ter understand these problems and identify areas256Label % of words Precision Recall F-scoreFillers 5.6% 64% 59% 61%Edit (reparandum) 7.8% 85% 68% 75%Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.Label % of edits RecallRough copy (RC) edits 58.8% 84.8%Non-copy (NC) edits 41.2% 43.2%Total edits 100.0% 67.6%Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.1 he that ?s uh that ?s a relief2 E E E FL - - - -3 NC RC RC FL - - - -Figure 2: Example of word class and refined wordclass labels, where - denotes a non-error, FL de-notes a filler, E generally denotes reparanda, andRC and NC indicate rough copy and non-copyspeaker errors, respectively.for improvement, we used the top-performing1JC04 noisy channel TAG edit detector to produceedit detection analyses on the test segment of theSpontaneous Speech Reconstruction (SSR) corpus(Fitzgerald and Jelinek, 2008).
Table 1 demon-strates the performance of this system for detect-ing filled pause fillers, discourse marker fillers,and edit words.
The results of a more granularanalysis compared to a hand-refined reference (asshown in line 3 of Figure 2) are shown in Table 2.The reader will recall that precision P is definedas P = |correct||correct|+|false| and recall R =|correct||correct|+|miss| .We denote the harmonic mean of P and R as F-score F and calculate it F = 21/P+1/R .As expected given the assumptions of the TAGapproach, JC04 identifies repetitions and mostrevisions in the SSR data, but less success-fully labels false starts and other speaker self-interruptions which do not have a cross-serial cor-relations.
These non-copy errors (with a recall ofonly 43.2%), are hurting the overall edit detectionrecall score.
Precision (and thus F-score) cannotbe calculated for the experiment in Table 2; sincethe JC04 does not explicitly label edits as roughcopies or non-copies, we have no way of knowingwhether words falsely labeled as edits would have1As determined in the RT04 EARS Metadata ExtractionTaskbeen considered as false RCs or false NCs.
Thiswill unfortunately hinder us from using JC04 as adirect baseline comparison in our work targetingfalse starts; however, we consider these results tobe further motivation for the work.Surveying these results, we conclude that thereis still much room for improvement in thefield of simple disfluency identification, espe-cially the cases of detecting non-copy reparandumand learning how and where to implement non-deletion reconstruction changes.2 Approach2.1 DataWe conducted our experiments on the recently re-leased Spontaneous Speech Reconstruction (SSR)corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisherconversational telephone speech (CTS) data (Cieriet al, 2004).
Advantages of the SSR data include?
aligned parallel original and cleaned sen-tences?
several levels of error annotations, allowingfor a coarse-to-fine reconstruction approach?
multiple annotations per sentence reflectingthe occasional ambiguity of correctionsAs reconstructions are sometimes non-deterministic (illustrated in EX6 in Section1.1), the SSR provides two manual reconstruc-tions for each utterance in the data.
We usethese dual annotations to learn complementaryapproaches in training and to allow for moreaccurate evaluation.The SSR corpus does not explicitly label allreparandum-like regions, as defined in Section 1.1,but only those which annotators selected to delete.257Thus, for these experiments we must implicitlyattempt to replicate annotator decisions regardingwhether or not to delete reparandum regions whenlabeling them as such.
Fortunately, we expect thisto have a negligible effect here as we will empha-size utterances which do not require more complexreconstructions in this work.The Spontaneous Speech Reconstruction cor-pus is partitioned into three subcorpora: 17,162training sentences (119,693 words), 2,191 sen-tences (14,861 words) in the development set, and2,288 sentences (15,382 words) in the test set.
Ap-proximately 17% of the total utterances contain areparandum-type error.The output of the JC04 model ((Johnson andCharniak, 2004) is included as a feature and usedas an approximate baseline in the following exper-iments.
The training of the TAG model within thissystem requires a very specific data format, so thissystem is trained not with SSR but with Switch-board (SWBD) (Godfrey et al, 1992) data as de-scribed in (Johnson and Charniak, 2004).
Key dif-ferences in these corpora, besides the form of theirannotations, include:?
SSR aims to correct speech output, whileSWBD edit annotation aims to identifyreparandum structures specifically.
Thus, asmentioned, SSR only marks those reparandawhich annotators believe must be deletedto generate a grammatical and content-preserving reconstruction.?
SSR considers some phenomena such asleading conjunctions (?and i did?
?
?i did?)
tobe fillers, while SWBD does not.?
SSR includes more complex error identifi-cation and correction, though these effectsshould be negligible in the experimentalsetup presented herein.While we hope to adapt the trained JC04 modelto SSR data in the future, for now these differencein task, evaluation, and training data will preventdirect comparison between JC04 and our results.2.2 Conditional random fieldsConditional random fields (Lafferty et al, 2001),or CRFs, are undirected graphical models whoseprediction of a hidden variable sequence Y isglobally conditioned on a given observation se-quence X , as shown in Figure 3.
Each observedFigure 3: Illustration of a conditional randomfield.
For this work, x represents observable in-puts for each word as described in Section 3.1 andy represents the error class of each word (Section3.2).state xi ?
X is composed of the correspondingword wi and a set of additional features Fi, de-tailed in Section 3.1.The conditional probability of this model can berepresented asp?
(Y |X) =1Z?
(X)exp(?k?kFk(X,Y )) (1)where Z?
(X) is a global normalization factor and?
= (?1 .
.
.
?K) are model parameters related toeach feature function Fk(X,Y ).CRFs have been widely applied to tasks innatural language processing, especially those in-volving tagging words with labels such as part-of-speech tagging and shallow parsing (Sha andPereira, 2003), as well as sentence boundarydetection (Liu et al, 2005; Liu et al, 2004).These models have the advantage that they modelsequential context (like hidden Markov models(HMMs)) but are discriminative rather than gen-erative and have a less restricted feature set.
Ad-ditionally, as compared to HMMs, CRFs offerconditional (versus joint) likelihood, and directlymaximizes posterior label probabilities P (E|O).We used the GRMM package (Sutton, 2006) toimplement our CRF models, each using a zero-mean Gaussian prior to reduce over-fitting ourmodel.
No feature reduction is employed, exceptwhere indicated.3 Word-Level ID Experiments3.1 Feature functionsWe aim to train our CRF model with sets offeatures with orthogonal analyses of the errorfultext, integrating knowledge from multiple sources.While we anticipate that repetitions and otherrough copies will be identified primarily by lexical258and local context features, this will not necessarilyhelp for false starts with little or no lexical overlapbetween reparandum and repair.
To catch these er-rors, we add both language model features (trainedwith the SRILM toolkit (Stolcke, 2002) on SWBDdata with EDITED reparandum nodes removed),and syntactic features to our model.
We also in-cluded the output of the JC04 system ?
which hadgenerally high precision on the SSR data ?
in thehopes of building on these results.Altogether, the following features F were ex-tracted for each observation xi.?
Lexical features, including?
the lexical item and part-of-speech(POS) for tokens ti and ti+1,?
distance from previous token to the nextmatching word/POS,?
whether previous token is partial wordand the distance to the next word withsame start, and?
the token?s (normalized) position withinthe sentence.?
JC04-edit: whether previous, next, or cur-rent word is identified by the JC04 system asan edit and/or a filler (fillers are classified asdescribed in (Johnson et al, 2004)).?
Language model features: the unigram logprobability of the next word (or POS) tokenp(t), the token log probability conditioned onits multi-token history h (p(t|h))2, and thelog ratio of the two (log p(t|h)p(t) ) to serve asan approximation for mutual information be-tween the token and its history, as defined be-low.I(t;h) =?h,tp(h, t) logp(h, t)p(h)p(t)=?h,tp(h, t)[logp(t|h)p(t)]This aims to capture unexpected n-gramsproduced by the juxtaposition of the reparan-dum and the repair.
The mutual informationfeature aims to identify when common wordsare seen in uncommon context (or, alterna-tively, penalize rare n-grams normalized forrare words).2In our model, word historys h encompassed the previoustwo words (a 3-gram model) and POS history encompassedthe previous four POS labels (a 5-gram model)?
Non-terminal (NT) ancestors: Given an au-tomatically produced parse of the utterance(using the Charniak (1999) parser trained onSwitchboard (SWBD) (Godfrey et al, 1992)CTS data), we determined for each word allNT phrases just completed (if any), all NTphrases about to start to its right (if any), andall NT constituents for which the word is in-cluded.
(Ferreira and Bailey, 2004) and others havefound that false starts and repeats tend to endat certain points of phrases, which we alsofound to be generally true for the annotateddata.Note that the syntactic and POS features weused are extracted from the output of an automaticparser.
While we do not expect the parser to al-ways be accurate, especially when parsing errorfultext, we hope that the parser will at least be con-sistent in the types of structures it assigns to par-ticular error phenomena.
We use these features inthe hope of taking advantage of that consistency.3.2 Experimental setupIn these experiments, we attempt to label thefollowing word-boundary classes as annotated inSSR corpus:?
fillers (FL), including filled pauses and dis-course markers (?5.6% of words)?
rough copy (RC) edit (reparandum incor-porates the same or very similar words inroughly the same word order, including repe-titions and some revisions) (?4.6% of words)?
non-copy (NC) edit (a speaker error where thereparandum has no lexical or structural re-lationship to the repair region following, asseen in restart fragments and some revisions)(?3.2% of words)Other labels annotated in the SSR corpus (suchas insertions and word reorderings), have been ig-nored for these error tagging experiments.We approach our training of CRFs in severalways, detailed in Table 3.
In half of our exper-iments (#1, 3, and 4), we trained a single modelto predict all three annotated classes (as definedat the beginning of Section 3.3), and in the otherhalf (#2, 5, and 6), we trained the model to predictNCs only, NCs and FLs, RCs only, or RCs and FLs(as FLs often serve as interregnum, we predict thatthese will be a valuable cue for other edits).259Setup Train data Test data Classes trained per model#1 Full train Full test FL + RC + NC#2 Full train Full test {RC,NC}, FL+{RC,NC}#3 Errorful SUs Errorful SUs FL + RC + NC#4 Errorful SUs Full test FL + RC + NC#5 Errorful SUs Errorful SUs {RC,NC}, FL+{RC,NC}#6 Errorful SUs Full test {RC,NC}, FL+{RC,NC}Table 3: Overview of experimental setups for word-level error predictions.We varied the subcorpus utterances used intraining.
In some experiments (#1 and 2) wetrained with the entire training set3, including sen-tences without speaker errors, and in others (#3-6)we trained only on those sentences containing therelevant deletion errors (and no additionally com-plex errors) to produce a densely errorful train-ing set.
Likewise, in some experiments we pro-duced output only for those test sentences whichwe knew to contain simple errors (#3 and 5).
Thiswas meant to emulate the ideal condition wherewe could perfectly predict which sentences con-tain errors before identifying where exactly thoseerrors occurred.The JC04-edit feature was included to help usbuild on previous efforts for error classification.To confirm that the model is not simply replicatingthese results and is indeed learning on its own withthe other features detailed, we also trained modelswithout this JC04-edit feature.3.3 Evaluation of word-level experiments3.3.1 Word class evaluationWe first evaluate edit detection accuracy on a per-word basis.
To evaluate our progress identify-ing word-level error classes, we calculate preci-sion, recall and F-scores for each labeled class c ineach experimental scenario.
As usual, these met-rics are calculated as ratios of correct, false, andmissed predictions.
However, to take advantage ofthe double reconstruction annotations provided inSSR (and more importantly, in recognition of theoccasional ambiguities of reconstruction) wemod-3Using both annotated SSR reference reconstructions foreach utteranceified these calculations slightly as shown below.corr(c) =?i:cwi=c?
(cwi = cg1,i or cwi = cg2,i)false(c) =?i:cwi=c?
(cwi 6= cg1,i and cwi 6= cg2,i)miss(c) =?i:cg1,i=c?
(cwi 6= cg1,i)where cwi is the hypothesized class forwi and cg1,iand cg2,i are the two reference classes.Setup Class labeled FL RC NCTrain and test on all SUs in the subcorpus#1 FL+RC+NC 71.0 80.3 47.4#2 NC - - 42.5#2 NC+FL 70.8 - 47.5#2 RC - 84.2 -#2 RC+FL 67.8 84.7 -Train and test on errorful SUs#3 FL+RC+NC 91.6 84.1 52.2#4 FL+RC+NC 44.1 69.3 31.6#5 NC - - 73.8#6 w/ full test - - 39.2#5 NC+FL 90.7 - 69.8#6 w/ full test 50.1 - 38.5#5 RC - 88.7 -#6 w/ full test - 75.0 -#5 RC+FL 92.3 87.4 -#6 w/ full test 62.3 73.9 -Table 4: Word-level error prediction F1-score re-sults: Data variation.
The first column identifieswhich data setup was used for each experiment(Table 3).
The highest performing result for eachclass in the first set of experiments has been high-lighted.Analysis: Experimental results can be seen inTables 4 and 5.
Table 4 shows the impact of260Features FL RC NCJC04 only 56.6 69.9-81.9 1.6-21.0lexical only 56.5 72.7 33.4LM only 0.0 15.0 0.0NT bounds only 44.1 35.9 11.5All but JC04 58.5 79.3 33.1All but lexical 66.9 76.0 19.6All but LM 67.9 83.1 41.0All but NT bounds 61.8 79.4 33.6All 71.0 80.3 47.4Table 5: Word-level error prediction F-score re-sults: Feature variation.
All models were trainedwith experimental setup #1 and with the set of fea-tures identified.training models for individual features and of con-straining training data to contain only those ut-terances known to contain errors.
It also demon-strates the potential impact on error classificationafter prefiltering test data to those SUs with er-rors.
Table 5 demonstrates the contribution of eachgroup of features to our CRF models.Our results demonstrate the impact of varyingour training data and the number of label classestrained for.
We see in Table 4 from setup #5 exper-iments that training and testing on error-containingutterances led to a dramatic improvement in F1-score.
On the other hand, our results for experi-ments using setup #6 (where training data was fil-tered to contain errorful data but test data was fullypreserved) are consistently worse than those of ei-ther setup #2 (where both train and test data wasuntouched) or setup #5 (where both train and testdata were prefiltered).
The output appears to suf-fer from sample bias, as the prior of an error oc-curring in training is much higher than in testing.This demonstrates that a densely errorful trainingset alne cannot improve our results when testingdata conditions do not match training data condi-tions.
However, efforts to identify errorful sen-tences before determining where errors occur inthose sentences may be worthwhile in preventingfalse positives in error-less utterances.We next consider the impact of the four featuregroups on our prediction results.
The CRF modelappears competitive even without the advantageof building on JC04 results, as seen in Table 54.4JC04 results are shown as a range for the reasons given inSection 1.2: since JC04 does not on its own predict whetheran ?edit?
is a rough copy or non-copy, it is impossible to cal-Interestingly and encouragingly, the NT boundsfeatures which indicate the linguistic phrase struc-tures beginning and ending at each word accord-ing to an automatic parse were also found to behighly contribututive for both fillers and non-copyidentification.
We believe that further pursuit ofsyntactic features, especially those which can takeadvantage of the context-free weakness of statisti-cal parsers like (Charniak, 1999) will be promisingin future research.It was unexpected that NC classification wouldbe so sensitive to the loss of lexical features whileRC labeling was generally resilient to the drop-ping of any feature group.
We hypothesize thatfor rough copies, the information lost from the re-moval of the lexical items might have been com-pensated for by the JC04 features as JC04 per-formed most strongly on this error type.
Thisshould be further investigated in the future.3.3.2 Strict evaluation: SU matchingDepending on the downstream task of speech re-construction, it could be imperative not only toidentify many of the errors in a given spoken ut-terance, but indeed to identify all errors (and onlythose errors), yielding the precise cleaned sentencethat a human annotator might provide.In these experiments we apply simple cleanup(as described in Section 1.1) to both JC04 out-put and the predicted output for each experimentalsetup in Table 3, deleting words when their rightboundary class is a filled pause, rough copy ornon-copy.Taking advantage of the dual annotations foreach sentence in the SSR corpus, we can reportboth single-reference and double-reference eval-uation.
Thus, we judge that if a hypothesizedcleaned sentence exactly matches either referencesentence cleaned in the same manner, we count thecleaned utterance as correct and otherwise assignno credit.Analysis: We see the outcome of this set of ex-periments in Table 6.
While the unfiltered test setsof JC04-1, setup #1 and setup #2 appear to havemuch higher sentence-level cleanup accuracy thanthe other experiments, we recall that this is natu-ral also due to the fact that the majority of thesesentences should not be cleaned at all, besidesculate precision and thus F1 score precisely.
Instead, here weshow the resultant F1 for the best case and worst case preci-sion range.261Setup Classes deleted # SUs # SUs which match gold % accuracyBaseline only filled pauses 2288 1800 78.7%JC04-1 E+FL 2288 1858 81.2%CRF-#1 RC, NC, and FL 2288 1922 84.0%CRF-#2?
{RC,NC} 2288 1901 83.1%Baseline only filled pauses 281 5 1.8%JC04-2 E+FL 281 126 44.8%CRF-#3 RC, NC, and FL 281 156 55.5%CRF-#5?
{RC,NC} 281 132 47.0%Table 6: Word-level error predictions: exact SU match results.
JC04-2 was run only on test sentencesknown to contain some error to match the conditions of Setup #3 and #5 (from Table 3).
For the baselines,we delete only filled pause filler words like ?eh?
and ?um?.occasional minor filled pause deletions.
Look-ing specifically on cleanup results for sentencesknown to contain at least one error, we see, onceagain, that our system outperforms our baselineJC04 system at this task.4 DiscussionOur first goal in this work was to focus on an areaof disfluency detection currently weak in otherstate-of-the-art speaker error detection systems ?false starts ?
while producing comparable classi-fication on repetition and revision speaker errors.Secondly, we attempted to quantify how far delet-ing identified edits (both RC and NC) and filledpauses could bring us to full reconstruction ofthese sentences.We?ve shown in Section 3 that by training andtesting on data prefiltered to include only utter-ances with errors, we can dramatically improveour results, not only by improving identificationof errors but presumably by reducing the risk offalsely predicting errors.
We would like to furtherinvestigate to understand how well we can auto-matically identify errorful spoken utterances in acorpus.5 Future WorkThis work has shown both achievable and demon-strably feasible improvements in the area of iden-tifying and cleaning simple speaker errors.
We be-lieve that improved sentence-level identification oferrorful utterances will help to improve our word-level error identification and overall reconstructionaccuracy; we will continue to research these areasin the future.
We intend to build on these efforts,adding prosodic and other features to our CRF andmaximum entropy models,In addition, as we improve the word-level clas-sification of rough copies and non-copies, we willbegin to move forward to better identify morecomplex speaker errors such as missing argu-ments, misordered or redundant phrases.
We willalso work to apply these results directly to the out-put of a speech recognition system instead of totranscripts alone.AcknowledgmentsThe authors thank our anonymous reviewers fortheir valuable comments.
Support for this workwas provided by NSF PIRE Grant No.
OISE-0530118.
Any opinions, findings, conclusions,or recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the supporting agency.ReferencesJ.
Kathryn Bock.
1982.
Toward a cognitive psy-chology of syntax: Information processing contri-butions to sentence formulation.
Psychological Re-view, 89(1):1?47, January.Eugene Charniak.
1999.
A maximum-entropy-inspired parser.
In Meeting of the North AmericanAssociation for Computational Linguistics.Christopher Cieri, Stephanie Strassel, MohamedMaamouri, Shudong Huang, James Fiumara, DavidGraff, Kevin Walker, and Mark Liberman.
2004.Linguistic resource creation and distribution forEARS.
In Rich Transcription Fall Workshop.Fernanda Ferreira and Karl G. D. Bailey.
2004.
Disflu-encies and human language comprehension.
Trendsin Cognitive Science, 8(5):231?237, May.262Erin Fitzgerald and Frederick Jelinek.
2008.
Linguis-tic resources for reconstructing spontaneous speechtext.
In Proceedings of the Language Resources andEvaluation Conference, May.Erin Fitzgerald.
2009.
Reconstructing SpontaneousSpeech.
Ph.D. thesis, The Johns Hopkins University.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
SWITCHBOARD: Telephone speechcorpus for research and development.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech, and Signal Processing, pages 517?520,San Francisco.Matthias Honal and Tanja Schultz.
2005.
Au-tomatic disfluency removal on recognized spon-taneous speech ?
rapid adaptation to speaker-dependent disfluenices.
In Proceedings of the IEEEInternational Conference on Acoustics, Speech, andSignal Processing.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy channel model of speech repairs.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics.Mark Johnson, Eugene Charniak, and Matthew Lease.2004.
An improved model for recognizing disfluen-cies in conversational speech.
In Rich TranscriptionFall Workshop.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc.
18th International Conf.
onMachine Learning, pages 282?289.
Morgan Kauf-mann, San Francisco, CA.John Lee and Stephanie Seneff.
2006.
Automaticgrammar correction for second-language learners.In Proceedings of the International Conference onSpoken Language Processing.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-bara Peskin, and Mary Harper.
2004.
The ICSI/UWRT04 structural metadata extraction system.
In RichTranscription Fall Workshop.Yang Liu, Andreas Stolcke, Elizabeth Shriberg, andMary Harper.
2005.
Using conditional randomfields for sentence boundary detection in speech.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics, pages 451?458,Ann Arbor, MI.Sharath Rao, Ian Lane, and Tanja Schultz.
2007.
Im-proving spoken language translation by automaticdisfluency removal: Evidence from conversationalspeech transcripts.
In Machine Translation SummitXI, Copenhagen, Denmark, October.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In HLT-NAACL.Elizabeth Shriberg.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. thesis, University ofCalifornia, Berkeley.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the IEEEInternational Conference on Acoustics, Speech, andSignal Processing, Denver, CO, September.Charles Sutton.
2006.
GRMM: A graphical modelstoolkit.
http://mallet.cs.umass.edu.Qi Zhang and Fuliang Weng.
2005.
Exploring fea-tures for identifying edited regions in disfluent sen-tences.
In Proceedings of the International Work-shop on Parsing Techniques, pages 179?185.263
