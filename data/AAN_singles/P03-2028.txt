Spoken Interactive ODQA System: SPIQAChiori Hori, Takaaki Hori, Hajime Tsukada,Hideki Isozaki, Yutaka Sasaki and Eisaku MaedaNTT Communication Science LaboratoriesNippon Telegraph and Telephone Corporation2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, JapanAbstractWe have been investigating an interactiveapproach for Open-domain QA (ODQA)and have constructed a spoken interactiveODQA system, SPIQA.
The system de-rives disambiguating queries (DQs) thatdraw out additional information.
To testthe efficiency of additional information re-quested by the DQs, the system recon-structs the user?s initial question by com-bining the addition information with ques-tion.
The combination is then used for an-swer extraction.
Experimental results re-vealed the potential of the generated DQs.1 IntroductionOpen-domain QA (ODQA), which extracts answersfrom large text corpora, such as newspaper texts, hasbeen intensively investigated in the Text REtrievalConference (TREC).
ODQA systems return an ac-tual answer in response to a question written in anatural language.
However, the information in thefirst question input by a user is not usually sufficientto yield the desired answer.
Interactions for col-lecting additional information to accomplish QA areneeded.
To construct more precise and user-friendlyODQA systems, a speech interface is used for theinteraction between human beings and machines.Our goal is to construct a spoken interactiveODQA system that includes an automatic speechrecognition (ASR) system and an ODQA system.To clarify the problems presented in building sucha system, the QA systems constructed so far havebeen classified into a number of groups, dependingon their target domains, interfaces, and interactionsto draw out additional information from users to ac-complish set tasks, as is shown in Table 1.
In thistable, text and speech denote text input and speechinput, respectively.
The term ?addition?
representsadditional information queried by the QA systems.This additional information is separate to that de-rived from the user?s initial questions.Table 1: Domain and data structure for QA systemstarget domain specific opendata structure knowledge DB unstructured textwithout addition CHAT-80 SAIQAtextwith addition MYCIN (SPIQA?
)without addition Harpy VAQAspeechwith addition JUPITER (SPIQA?)?
SPIQA is our system.To construct spoken interactive ODQA systems,the following problems must be overcome: 1.
Sys-tem queries for additional information to extract an-swers and effective interaction strategies using suchqueries cannot be prepared before the user inputs thequestion.
2.
Recognition errors degrade the perfor-mance of QA systems.
Some information indispens-able for extracting answers is deleted or substitutedwith other words.Our spoken interactive ODQA system, SPIQA,copes with the first problem by adopting disam-biguating users?
questions using system queries.
Inaddition, a speech summarization technique is ap-plied to handle recognition errors.2 Spoken Interactive QA system: SPIQAFigure 1 shows the components of our system, andthe data that flows through it.
This system com-prises an ASR system (SOLON), a screening filterthat uses a summarization method, and ODQA en-gine (SAIQA) for a Japanese newspaper text corpus,a Deriving Disambiguating Queries (DDQ) module,and a Text-to-Speech Synthesis (TTS) engine (Fi-nalFluet).ASRTTSScreeningfilterODQA engine(SAIQA)DDQmoduleAnswerderived?Answersentence generatorQuestionreconstructorNoYesAdditionalinfo.
New questionFirstquestionQuestion/Additional info.User Answer/DDQ speechAnswersentenceDDQsentenceRecognitionresultAnswerFigure 1: Components and data flow in SPIQA.ASR systemOur ASR system is based on the Weighted Finite-State Transducers (WFST) approach that is becom-ing a promising alternative formulation for the tra-ditional decoding approach.
The WFST approachoffers a unified framework representing variousknowledge sources in addition to producing an op-timized search network of HMM states.
We com-bined cross-word triphones and trigrams into a sin-gle WFST and applied a one-pass search algorithmto it.Screening filterTo alleviate degradation of the QA?s perfor-mance by recognition errors, fillers, word fragments,and other distractors in the transcribed question, ascreening filter that removes these redundant andirrelevant information and extracts meaningful in-formation is required.
The speech summarizationapproach (C. Hori et.
al., 2003) is applied to thescreening process, wherein a set of words maximiz-ing a summarization score that indicates the appro-priateness of summarization is extracted automati-cally from a transcribed question, and these wordsare then concatenated together.
The extraction pro-cess is performed using a Dynamic Programming(DP) technique.ODQA engineThe ODQA engine, SAIQA, has four compo-nents: question analysis, text retrieval, answer hy-pothesis extraction, and answer selection.DDQ moduleWhen the ODQA engine cannot extract an appro-priate answer to a user?s question, the question isconsidered to be ?ambiguous.?
To disambiguate theinitial questions, the DDQ module automatically de-rives disambiguating queries (DQs) that require in-formation indispensable for answer extraction.
Thesituations in which a question is considered ambigu-ous are those when users?
questions exclude indis-pensable information or indispensable informationis lost through ASR errors.
These instances of miss-ing information should be compensated for by theusers.To disambiguate a question, ambiguous phraseswithin it should be identified.
The ambiguity ofeach phrase can be measured by using the struc-tural ambiguity and generality score for the phrase.The structural ambiguity is based on the dependencystructure of the sentence; phrase that is not modifiedby other phrases is considered to be highly ambigu-ous.
Figure 2 has an example of a dependency struc-ture, where the question is separated into phrases.Each arrow represents the dependency between twophrases.
In this example, ?the World Cup?
has noWhich  country won the  world  cupin Southeast Asia ?Figure 2: Example of dependency structure.modifiers and needs more information to be identi-fied.
?Southeast Asia?
also has no modifiers.
How-ever, since ?the World Cup?appears more frequentlythan ?Southeast Asia?
in the retrieved corpus, ?theWorld Cup?
is more difficult to identify.
In otherwords, words that frequently occur in a corpus rarelyhelp to extract answers in ODQA systems.
There-fore, it is adequate for the DDQ module to generatequestions relating to ?World Cup?
in this example,such as ?What kind of World Cup??
, ?What yearwas the World Cup held?
?.The structural ambiguity of the n-th phrase is de-fined asAD(Pn) = log{1 ?
?Ni=1:i=nD(Pi, Pn)},where the complete question is separated into Nphrases, and D(Pi, Pn) is the probability that phrasePnwill be modified by phrase Pi, which can be cal-culated using Stochastic Dependency Context-FreeGrammar (SDCFG) (C. Hori et.
al., 2003).Using this SDCFG, only the number of non-terminal symbols is determined and all combina-tions of rules are applied recursively.
The non-terminal symbol has no specific function, such asa noun phrase.
All the probabilities of rules arestochastically estimated based on data.
Probabilitiesfor frequently used rules become greater, and thosefor rarely used rules become smaller.
Even thoughtranscription results given by a speech recognizer areill-formed, the dependency structure can be robustlyestimated by our SDCFG.The generality score is defined asAG(Pn) =?w?Pn:w=cont log P (w),where P (w) is the unigram probability of w basedon the corpus to be retrieved.
Thus, ?w = cont?means that w is a content word such as a noun, verbor adjective.We generate the DQs using templates of interrog-ative sentences.
These templates contain an inter-rogative and a phrase taken from the user?s question,i.e., ?What kind of * ?
?, ?What year was * held?
?and ?Where is * ?
?.The DDQ module selects the best DQ based on itslinguistic appropriateness and the ambiguity of thephrase.
The linguistic appropriateness of DQs canbe measured by using a language model, N-gram.Let Smnbe a DQ generated by inserting the n-thphrase into the m-th template.
The DDQ moduleselects the DQ that maximizes the DQ score:H(Smn) = ?LL(Smn)+?DAD(Pn)+?GAG(Pn),where L(?)
is a linguistic score such as the loga-rithm for trigram probability, and ?L, ?D, and ?Gare weighting factors to balance the scores.Hence, the module can generate a sentence thatis linguistically appropriate and asks the user to dis-ambiguate the most ambiguous phrase in his or herquestion.3 Evaluation ExperimentsQuestions consisting of 69 sentences read aloud byseven male speakers were transcribed by our ASRsystem.
The question transcriptions were processedwith a screening filter and input into the ODQAengine.
Each question consisted of about 19 mor-phemes on average.
The sentences were grammat-ically correct, formally structured, and had enoughinformation for the ODQA engine to extract the cor-rect answers.
The mean word recognition accuracyobtained by the ASR system was 76%.3.1 Screening filterScreening was performed by removing recognitionerrors using a confidence measure as a threshold andthen summarizing it within an 80% to 100% com-paction ratio.
In this summarization technique, theword significance and linguistic score for summa-rization were calculated using text from Mainichinewspapers published from 1994 to 2001, compris-ing 13.6M sentences with 232M words.
The SD-CFG for the word concatenation score was calcu-lated using the manually parsed corpus of Mainichinewspapers published from 1996 to 1998, consist-ing of approximately 4M sentences with 68M words.The number of non-terminal symbols was 100.
Theposterior probability of each transcribed word in aword graph obtained by ASR was used as the confi-dence score.3.2 DDQ moduleThe word generality score AGwas computed usingthe same Mainichi newspaper text described above,while the SDCFG for the dependency ambiguityscore ADfor each phrase was the same as that usedin (C. Hori et.
al., 2003).
Eighty-two types of inter-rogative sentences were created as disambiguatingqueries for each noun and noun-phrase in each ques-tion and evaluated by the DDQ module.
The linguis-tic score L indicating the appropriateness of inter-rogative sentences was calculated using 1000 ques-tions and newspaper text extracted for three years.The structural ambiguity score ADwas calculatedbased on the SDCFG, which was used for the screen-ing filter.3.3 Evaluation methodThe DQs generated by the DDQ module were eval-uated in comparison with manual disambiguationqueries.
Although the questions read by the sevenspeakers had sufficient information to extract ex-act answers, some recognition errors resulted in aloss of information that was indispensable for ob-taining the correct answers.
The manual DQs weremade by five subjects based on a comparison ofthe original written questions and the transcriptionresults given by the ASR system.
The automaticDQs were categorized into two classes: APPRO-PRIATE when they had the same meaning as atleast one of the five manual DQs, and INAPPRO-PRIATE when there was no match.
The QA per-formance in using recognized (REC) and screenedquestions (SCRN) were evaluated by MRR (MeanReciprocal Rank) (http://trec.nist.gov/data/qa.html).SCRN was compared with the transcribed questionthat just had recognition errors removed (DEL).
Inaddition, the questions reconstructed manually bymerging these questions and additional informationrequested the DQs generated by using SCRN, (DQ)were also evaluated.
The additional information wasextracted from the original users?
question withoutrecognition errors.
In this study, adding informationby using the DQs was performed only once.3.4 Evaluation resultsTable 2 shows the evaluation results in terms ofthe appropriateness of the DQs and the QA-systemMRRs.
The results indicate that roughly 50% of theDQs generated by the DDQ module based on thescreened results were APPROPRIATE.
The MRRfor manual transcription (TRS) with no recognitionerrors was 0.43.
In addition, we could improve theMRR from 0.25 (REC) to 0.28 (DQ) by using theDQs only once.
Experimental results revealed thepotential of the generated DQs in compensating forthe degradation of the QA performance due to recog-nition errors.4 ConclusionThe proposed spoken interactive ODQA system,SPIQA copes with missing information by adopt-ing disambiguation of users?
questions by systemqueries.
In addition, a speech summarization tech-nique was applied for handling recognition errors.Although adding information was performed usingDQs only once, experimental results revealed thepotential of the generated DQs to acquire indispens-able information that was lacking for extracting an-swers.
In addition, the screening filter helped to gen-erate the appropriate DQs.
Future research will in-Table 2: Evaluation results of disambiguatingqueries generated by the DDQ module.Word MRR w/o IN-SPKacc.
REC DEL SCRN DQ errors APP APPA 70% 0.19 0.16 0.17 0.23 4 32 33B 76% 0.31 0.24 0.29 0.31 8 36 25C 79% 0.26 0.18 0.26 0.30 10 34 25D 73% 0.27 0.21 0.24 0.30 4 35 30E 78% 0.24 0.21 0.24 0.27 7 31 31F 80% 0.28 0.25 0.30 0.33 8 34 27G 74% 0.22 0.19 0.19 0.22 3 35 31AVG 76% 0.25 0.21 0.24 0.28 9% 49% 42%An integer without a % other than MRRs indicates number ofsentences.
Word acc.
:word accuracy, SPK:speaker, AVG: aver-aged values, w/o errors: transcribed sentences without recog-nition errors, APP: appropriate DQs and InAPP: inappropriateDQs.clude an evaluation of the appropriateness of DQsderived repeatedly to obtain the final answers.
Inaddition, the interaction strategy automatically gen-erated by the DDQ module should be evaluated interms of how much the DQs improve QA?s total per-formance.ReferencesF.
Pereira et.
al., ?Definite Clause Grammars for LanguageAnalysis ?a Survey of the Formalism and a Comparison withAugmented Transition Networks,?
Artificial Intelligence, 13:231-278, 1980.E.
H. Shortliffe, ?Computer-Based Medical Consultations:MYCIN,?
Elsevier/North Holland, New York NY, 1976.B.
Lowerre et.
al., ?The Harpy speech understanding system,?W.
A. Lea (Ed.
), Trends in Speech recognition, pp.
340, Pren-tice Hall.L.
D. Erman et.
al., ?The Hearsay-II Speech-UnderstandingSystem: Integrating Knowledge to Resolve Uncertainty,?ACM computing Survays, Vol.
12, No.
2, pp.
213 ?
253,1980.V.
Zue, et al, ?JUPITER: A Telephone-Based ConversationalInterface for Weather Information,?
IEEE Transactions onSpeech and Audio Processing, Vol.
8, No.
1, 2000.S.
Harabagiu et.
al., ?Open-Domain Voice-Activated Ques-tion Answering,?
COLING2002, Vol.I, pp.
321?327, Taipei,2002.C.
Hori et.
al., ?A Statistical Approach for Automatic SpeechSummarization,?
EURASIP Journal on Applied Signal Pro-cessing (EURASIP), pp128?139, 2003.Y.
Sasaki et.
al., ?NTT?s QA Systems for NTCIR QAC-1,?Working Notes of the Third NTCIR Workshop Meeting,pp.63?70, 2002.
