Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 151?156,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsHedge Classification with Syntactic Dependency Features based on anEnsemble ClassifierYi Zheng, Qifeng Dai, Qiming Luo, Enhong ChenDepartment of Computer Science,University of Science and Technology of China, Hefei, China.
{xiaoe, dqf2008}@mail.ustc.edu.cn{luoq, cheneh}@ustc.edu.cnAbstractWe present our CoNLL-2010 Shared Tasksystem in the paper.
The system operates inthree steps: sequence labeling, syntactic de-pendency parsing, and classification.
We haveparticipated in the Shared Task 1.
Our experi-mental results measured by the in-domain andcross-domain F-scores on the biological do-main are 81.11% and 67.99%, and on theWikipedia domain 55.48% and 55.41%.1 IntroductionThe goals of the Shared Task (Farkas et al, 2010)are: (1) learning to detect sentences containinguncertainty and (2) learning to resolve the in-sentence scope of hedge cues.
We have partici-pated in the in-domain and cross-domain chal-lenges of Task 1.
Specifically, the aim of Task 1is to identify sentences in texts that contain unre-liable or uncertain information, and it is formu-lated as a binary classification problem.Similar to Morante et al (2009), we use theBIO-cue labels for all tokens in a sentence topredict whether a token is the first one of a hedgecue (B-cue), inside a hedge cue (I-cue), or out-side of a hedge cue (O-cue).
Thus we formulatethe problem at the token level, and our task is tolabel tokens in every sentence with BIO-cue.
Fi-nally, sentences that contain at least one B-cue orI-cue are considered as uncertain.Our system operates in three steps: sequencelabeling, syntactic dependency parsing, and clas-sification.
Sequence labeling is a preprocessingstep for splitting sentence into tokens and obtain-ing features of tokens.
Then a syntactic depend-ency parser is applied to obtain the dependencyinformation of tokens.
Finally, we employ anensemble classifier based on combining CRF(conditional random field) and MaxEnt (maxi-mum entropy) classifiers to label each token withthe BIO-cue.Our experiments are conducted on two train-ing data sets: one is the abstracts and full articlesfrom BioScope (biomedical domain) corpus(Vincze et al, 2008)1, the other one is paragraphsfrom Wikipedia possibly containing weasel in-formation.
Both training data sets have been an-notated manually for hedge/weasel cues.
Theannotation of weasel/hedge cues is carried out atthe phrase level.
Sentences containing at leastone hedge/weasel cue are considered as uncertain,while sentences with no hedge/weasel cues areconsidered as factual.
The results show that em-ploying the ensemble classifier outperforms thesingle classifier system on the Wikipedia data set,and using the syntactic dependency informationin the feature set outperform the system withoutsyntactic dependency information on the biologi-cal data set (in-domain).In related work, Szarvas (2008) extended themethodology of Medlock and Briscoe (2007),and presented a hedge detection method in bio-medical texts with a weakly supervised selectionof keywords.
Ganter and Strube (2009) proposedan approach for automatic detection of sentencescontaining linguistic hedges using Wikipediaweasel tags and syntactic patterns.The remainder of this paper is organized asfollows.
Section 2 presents the technical detailsof our system.
Section 3 presents experimentalresults and performance analysis.
Section 4 pre-sents our discussion of the experiments.
Section5 concludes the paper and proposes future work.2 System DescriptionThis section describes the implementation of oursystem.2.1 Information Flow of Our SystemCommon classification systems consist of twosteps: feature set construction and classification.The feature set construction process of our sys-1http://www.inf.u-szeged.hu/rgai/bioscope151tem consists of sequence labeling and syntacticdependency parsing.
Figure 1 shows the maininformation flow of our system.Figure 1: The main information flow of our sys-tem2.2 Sequence labelingThe sequence labeling step consists of the fol-lowing consecutive stages: (1) tokenizing, (2)chunking, (3) POS-tagging, (4) lemmatizing.Firstly, the PTBTokenizer2 is employed to splitsentence into tokens.
Then, tokens are labeledwith BIO-tags by the OpenNLP 3  chunker.
Fi-nally, Stanford Parser4 is used to obtain the POSand lemma of tokens.2.3 Syntactic Dependency ParsingIn the syntactic dependency parsing stage, weuse the Stanford Parser again to obtain depend-ency information of tokens.
Based on the Stan-ford typed dependencies manual (Marneffe andManning 2008), we have decided to obtain thetree dependencies structure.
During the processof parsing, we found that the parser may fail due2a tokenizer from Stanford Parser.3http://www.opennlp.org/4http://nlp.stanford.edu/software/lex-parser.shtmlto either empty sentences or very long sentences.To deal with very long sentences, we decided toallocate more memory.
To deal with empty sen-tences, we decided to simply label them as cer-tain ones because there are only a few emptysentences in the training and test data sets and wecould ignore their influence.2.4 FeaturesAfter sequence labeling and syntactic depend-ency parsing, we obtain candidate features.
Inour system, all the features belong to the follow-ing five categories: (1) token features, (2) de-pendency features, (3) neighbor features, (4) datafeatures, (5) bigram and trigram features.Token features of the current token are listedbelow:?
token: the current token.?
index: index of the current token in the sen-tence?
pos: POS of the current token.?
lemma: lemma of the current token.?
chunk: BIO-chunk tags of the current token.Dependency features of the current token arelisted below:?
parent_index: the index of the parent tokenof the current token.?
parent_token: the parent token of the currenttoken.?
parent_lemma: the lemma of the parent tokenof the current token.?
parent_pos: the POS of the parent token ofthe current token.?
parent_relation: the dependency relation ofthe current token and its parent token.Neighbor features of the current token includetoken, lemma, pos, chunk tag of three tokens tothe right and three to the left.Data features of current token are listed below:?
type: indicating documentPart 5  type of thesentence which contains the current token,such as Text, SectionTitle and so on.?
domain: distinguishing the Wikipedia andbiological domain.?
abstract_article: indicating document type ofthe sentence which contains the current token,abstract or article.5documentPart, SectionTitle, Text and so on are tagsin the training and test data sets.CRF  MaxEntStartMergingEndTokenizingChunkingPOS-taggingLemmatizingSyntacticDependencyParsing152We empirically selected some bigram featuresand trigram features as listed below:?
left_token_2+left_token_1?
left_token_1+token?
token+right_token_1?
right_token_1+right_token_2?
left_token_2+left_token_1+token?
left_token_1+token+right_token_1?
token+right_token_1+right_token_2These are the complete set of features for oursystem.
If the value of a feature is empty, we setit to a default value.
In the ensemble classifier,we have selected different features for each indi-vidual classifier.
Details of this are described inthe next subsection.2.5 ClassificationIn our system, we have combined CRF++6 andOpenNLP MaxEnt7 classifiers into an ensembleclassifier.
The set of features for each classifierare shown in the column named ?system?
of Ta-ble 6.
And the two classifiers are used in trainingand prediction separately, based on their individ-ual set of features.
Then we merge the results inthis way: for each token, if the two predictionsfor it are both O-cue, then we label the tokenwith an O-cue; otherwise, we label the tokenwith a B-cue (one of the predictions is B-cue) oran I-cue (no B-cue in the predictions).
The moti-vation of the ensemble classifier approach isbased on the observation of our internal experi-ments using 10-fold cross validation, which wedescribe in Section 3.
In addition, the parametersof OpenNLP MaxEnt classifier are all set to de-fault values (number of iterations is 100, cutoff is0 and without smoothing).
For CRF++, we onlyset the option ?-f?
as 3 and the option ?-c?
as 1.5,and the others are set to default values.3 Experimental ResultsWe have participated in four subtasks, biologicalin-domain challenge (Bio-in-domain), biologicalcross-domain challenge (Bio-cross-domain),Wikipedia in-domain challenge (Wiki-in-domain) and Wikipedia cross-domain challenge(Wiki-cross-domain).
In all the experiments, TP,FP, FN and F-Score for the uncertainty class areused as the performance measures.
We have6http://crfpp.sourceforge.net/7http://maxent.sourceforge.net/tested our system with the test data set and ob-tained official results as shown in Table 1.
Inaddition, we have performed several internal ex-periments on the training data set and severalexperiments on the test data set, which we de-scribe in the next two subsections.
The featuresets used for each subtask in our system areshown in Table 6, where each column denotes afeature set named after the title of the column(?System?, ?dep?, ?).
Actually, for differentsubtasks, we make use of the same feature setnamed ?system?.SubTask TP FP FN F-ScoreBio-in-domain 717 261 73 81.11Bio-cross-domain 566 309 224 67.99Wiki-in-domain 974 303 1260 55.48Wiki-cross-domain 991 352 1243 55.41Table 1: Official results of our system.3.1 Internal ExperimentsInitially we only used a single classifier insteadof an ensemble classifier.
We performed 10-foldcross validation experiments on the training dataset at the sentence level with different featuresets.
The results of these experiments are shownin Table 2 and Table 3.In internal experiments, we mainly focus onthe results of different models and different fea-ture sets.
In Table 2 and Table 3, CRF and ME(MaxEnt) indicate the two classifiers; ENSMBstands for the ensemble classifier obtained bycombining CRF and MaxEnt classifiers; the threewords ?dep?, ?neighbor?
and ?together?
indicatethe feature sets for different experiments shownin Table 6, and ?together?
is the union set of?dep?
and ?neighbor?.The results of ME and CRF experiments (thirdcolumn of Table 2 and Table 3) show that theindividual classifier wrongly predicts many un-certain sentences as certain ones.
The number ofsuch errors is much greater than the number oferrors of predicting certain ones as uncertain.
Inother words, FN is greater than FP in our ex-periments and the recall ratio is very low, espe-cially for the Wikipedia data set.153Biological in-domain Biological cross-domain Experiment TP FP FN F-Score TP FP FN F-ScoreME-dep 244 28 34 88.73 220 24 58 84.29CRF-dep 244 20 34 90.04 230 19 48 87.29ENSMB-dep 248 32 30 88.89 235 28 43 86.88ME-neighbor 229 14 49 87.91 211 12 67 84.23CRF-neighbor 244 16 34 90.71 228 21 50 86.53ENSMB-neighbor 247 22 31 90.31 241 26 37 88.44ME-together 234 11 44 89.48 205 12 73 82.83CRF-together 247 13 31 91.82 234 21 44 87.80ENSMB-together 253 17 25 92.36 242 26 36 88.64Table 2: Results of internal experiments on the biological training data set.Wikipedia in-domain Wikipedia cross-domain Experiment TP FP FN F-Score TP FP FN F-ScoreME-dep 131 91 117 55.74 145 108 103 57.88CRF-dep 108 51 140 53.07 115 60 133 54.37ENSMB-dep 148 103 100 59.32 153 119 95 58.85ME-neighbor 106 52 142 52.22 130 77 118 57.14CRF-neighbor 123 44 125 59.28 123 72 125 55.53ENSMB-neighbor 145 71 103 62.50 154 116 94 59.46ME-together 100 57 148 49.38 117 69 131 53.92CRF-together 125 54 123 58.55 127 67 121 57.47ENSMB-together 141 83 107 59.75 146 104 102 58.63Table 3: Results of internal experiments on the Wikipedia training data set.Biological in-domain Biological cross-domain Experiment TP FP FN F-Score TP FP FN F-ScoreSystem-ME 650 159 140 81.30 518 265 272 65.86System-CRF 700 197 90 82.99 464 97 326 68.69System-ENSMB 717 261 73 81.11 566 309 224 67.99Table 4: Results of additional experiment of biological test data set.Wikipedia in-domain Wikipedia cross-domain Experiment TP FP FN F-Score TP FP FN F-ScoreSystem-ME 794 235 1440 48.67 798 284 1436 48.13System-CRF 721 112 1513 47.02 747 153 1487 47.67System-ENSMB 974 303 1260 55.48 991 352 1243 55.41Table 5: Results of additional experiment of Wikipedia test data set.Based on this analysis, we propose an ensem-ble classifier approach to decrease FN in order toimprove the recall ratio.
The results of the en-semble classifier show that: along with the de-creasing of FN, FP and TP are both increasing.Although the recall ratio increases, the precisionratio decreases at the same time.
Therefore, theensemble classifier approach is a trade-off be-tween precision and recall.
For data sets with lowrecall ratio, such as Wikipedia, the ensembleclassifier outperforms each single classifier interms of F-score, just as the ME, CRF andENSMB experiments show in Table 2 and Table3.In addition, we have performed simple featureselection in the internal experiments.
The com-parison of ?dep?, ?neighbor?
and ?together?
ex-periments shown in Table 2 demonstrates thatthe dependency and neighbor features are bothbeneficial only for the biological in-domain ex-periment.
This may be because that sentences ofthe biological data are more regular than those ofthe Wikipedia data.3.2 Additional experiments on test data setWe have also performed experiments on the testdata set, and the results are shown in Table 4 andTable 5.
With the same set of features of our sys-154tem as shown in Table 6, we have performedthree experiments: System-ME (ME denotesMaxEnt classifier), System-CRF (CRF denotesCRF classifier) and System-ENSMB (ENSMBdenotes ensemble classifier), where ?System?denotes the feature set in Table 6.
The meaningsof these words are similar to internal experiments.As Table 4 and Table 5 show, for the Wikipe-dia test data set, the ensemble classifier outper-forms each single classifier in terms of F-scoreby improving the recall ratio with a larger extentthan the extent of the decreasing of the precisionratio.
For the biological test data set, the ensem-ble classifier outperforms System-ME but under-performs System-CRF.
This may be due to therelatively high values of the precision and recallratios already obtained by each single classifier.4 DiscussionThe features in our experiments are selected em-pirically, and the performance of our systemcould be improved with more elaborate featureselection.
From the experimental results, we ob-serve that there are still many uncertain sen-tences predicted as certain ones.
This indicatesthat the ability of learning uncertain informationwith the current classifiers and feature sets needsto be improved.
We had the plan of exploring theensemble classifier by combining CRF, MaxEntand SVM (Support Vector Machine), but it wasgiven up due to limited time.
In addition, wewere not able to complete experiments withMaxEnt classifier based on bigram and trigramfeatures due to limited time.
Actually only twolabels I and O are needed for Task 1.
We havenot done the experiments with only I and O la-bels, and we plan to do it in the future.According to our observation, the low F-scoreon the Wikipedia data set is due to many uncer-tain phrases.
By contrast, for the biological dataset, the uncertain information consists of mostlysingle words rather than phrases.
It is difficult fora classifier to learn uncertain information con-sisting of 3 words or more.
As we have observed,these uncertain phrases follow several patterns.A hybrid approach based on rule-based and sta-tistical approaches to recognize them seems to bea promising.5 Conclusion and Future WorkOur CoNLL-2010 Shared Task system operatesin three steps: sequence labeling, syntactic de-pendency parsing, and classification.
The resultsshow that employing the ensemble classifier out-performs each single classifier for the Wikipediadata set, and using the syntactic dependency in-formation in the feature set outperform the sys-tem without syntactic dependency informationfor the biological data set (in-domain).
Our finalsystem achieves promising results.
Due to lim-ited time, we have only performed simple featureselection empirically.
In the future, we plan toexplore more elaborate feature selection and ex-plore ensemble classifier by combining moreclassifiers.AcknowledgmentsThe work was supported by the National NaturalScience Foundation of China (No.60775037), theNational High Technology Research and Devel-opment Program of China (863 Program,No.2009 AA01Z123), and Specialized ResearchFund for the Doctoral Program of Higher Educa-tion (No.20093402110017).ReferencesRich?rd Farkas, Veronika Vincze, Gy?rgy M?ra,J?nos Csirik, and Gy?rgy Szarvas.
2010.
TheCoNLL-2010 Shared Task: Learning to DetectHedges and their Scope in Natural Language Text.In Proceedings of CoNLL-2010: Shared Task,pages 1?12.Viola Ganter, and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009 Con-ference Short Papers, pages 173?176.Marie-Catherine de Marneffe, and Christopher D.Manning.
2008.
Stanford typed dependenciesmanual, September 2008.Ben Medlock, and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proc.
of ACL 2007, pages 992?999.Roser Morante, and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProc.
of the BioNLP 2009 Workshop, pages28?36.Gy?rgy Szarvas.
2008.
Hedge classification in bio-medical texts with a weakly supervised selection ofkeywords.
In Proc.
of ACL 2008, pages 281?289.Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra, and J?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for un-certainty, negation and their scopes.
BMC Bioin-formatics, 9(Suppl 11):S9.155Feature System Dep Neighbor Togethertoken mc mc mc mcindex m m m mpos mc mc mc mclemma mc mc mc mcchunkmc mc mcparent_index mc mc  mcparent_tokenmc  mcparent_lemma mc mc  mcparent_relation mc mc  mcparent_pos mc mc  mcleft_token_1 c  c cleft_lemma_1 mc  mc mcleft_pos_1 mc  mc mcleft_chunk_1mc mcleft_token_2 c  c cleft_lemma_2 c  mc mcleft_pos_2 mc  mc mcleft_chunk_2mc mcleft_token_3left_lemma_3 mc  m mleft_pos_3 mc  m mleft_chunk_3m mright_token_1 c  c cright_lemma_1 mc  mc mcright _pos_1 mc  mc mcright _chunk_1mc mcright_token_2 c  c cright _lemma_2 mc  mc mcright _pos_2 c  mc mcright _chunk_2mc mcright_token_3right _lemma_3 c  m mright _pos_3 mc  m mright _chunk_3m mtype m mc mc mcdomain m mc mc mcabstract_article m mc mc mcleft_token_2+left_token_1 c  c cleft_token_1+token c  c ctoken+right_token_1 c  c cright_token_1+right_token_2 c  c cleft_token_2+left_token_1+token c  c cleft_token_1+token+right_token_1 c  c ctoken+right_token_1+right_token_2 c  c cTable 6: Features selected for different experiments.
The symbol m indicates MaxEnt classifier and c indicatesCRF classifier.156
