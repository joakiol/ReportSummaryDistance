Speaker Adaptation from Limited Trainingin the BBN BYBLOS Speech Recognition SystemFrancis KubalaMing-Whel Feng, John Makhoul, Richard SchwartzBBN Systems and Technologies Corporation10 Moulton St., Cambridge, Ma.
02238Abst rac tThe BBN BYBLOS continuous peech recognition system has been used to develop a methodof speaker adaptation from limited training.
The key step in the method is the estimation of aprobabilistic spectral mapping between a prototype speaker, for whom there exists a well-trainedspeaker-dependent hidden Markov model (HMM), and a target speaker for whom there is only asmall amount of training speech available.
The mapping defines a set of transformation matriceswhich are used to modify the parameters of the prototype model.
The resulting transformed modelis then used as an approximation to a well-trained model for the target speaker.
We review thetechniques employed to accomplish this transformation a d present experimental results conductedon the DARPA Resource Management database.1.
In t roduct ionSoon after a speech recognition system begins operation, small amounts of new speech databecome available to the system as spoken utterances are successfully transcribed to text.
This datais of potentially great value to the system because it contains detailed information on the currentstate of the speaker and the environment.
The purpose of rapid speaker adaptation is to utilizesuch small samples of speech to improve the recognition performance of the system.Speaker adaptation offers other benefits as well.
For applications which cannot olerate the ini-tial training expense of high performance speaker-dependent models, adaptation can trade-off peakperformance for rapid training of the system.
For typical experimental systems being investigatedtoday on a 1000-word continuous speech task domain, speaker-dependent training uses 30 minutesof speech (600 sentences), while the adaptation methods described here use only 2 minutes (40sentences).For applications in which an initial speaker-independent model fails to perform adequately dueto a change in the environment or the task domain not represented in the training data, adaptationcan utilize an economical initial model generated from the speaker-dependent training of a singleprototype speaker.
Again, looking at typical systems today, speaker-independent models train on 31/2 hours of speech (4200 sentences), while adaptation can use a speaker-dependent model trainedfrom 30 minutes (600 sentences).In this paper, we describe the speaker adaptive capabilities of the BBN BYBLOS continuousspeech recognition system.
Our basic approach to the problem is described first in section 2.
Twomethods for estimating the speaker transformation are described in section 3.
In section 4 wepresent our latest results on a standard testbed atabase.i003.
Methods  For  Comput ing  the  Trans format ionIn 1987 \[5\] we reported a new algorithm for estimating a probabilistic spectral mapping betweentwo speakers.
The transformation i this method is equivalent to expanding the HMM of theprototype, replacing each state by N states and connecting them in parallel by N transitions.
Thetransition probabilities on each of these paths axe then p(kils, ), which are the original prototypeprobabilities for each spectrum, i given the state, ~.
The pdf at each new state on these paths isp(k~lk~, ?
(~)) which corresponds to one row of the transformation matrix, T?
(s).Since the conditional probability, i~(k~18 ) in equation (3) is computed by the expanded HMM,the familiar forward-backward procedure can be used to estimate T~b(s ).
The target speech is firstquantized by the prototype codebook and is automatically aligned against the prototype model.This method worked very well with low perplexity grammars but performance degraded unaccept-ably as the perplexity of the grammar increased.We found that cross-speaker quantization was a significant factor in the performance degrada-tion.
Also, the transformed pdfs were excessively smooth.
We think that the original models, whichhave been smoothed appropriately for the prototype by interpolating context-dependent phonememodels, may not be specific enough to preserve important detail under the transformation.To overcome these problems, we investigated a text-dependent procedure which is described in\[2\].
In this method we constrain the prototype and target speaker to say a common set of trainingsentences.
A class labeling, ?
(s), is derived for each frame of prototype speech by using the proto-type HMM to perform recognition while constrained to the correct word sequence.
Matching pairsof utterances are time-aligned using a DTW procedure on the parameters of the training speech.This alignment of the speech frames defines a set of spectral co-occurrence triplets, {(k~, ki, ?
(s))},for all i, j ,  which can be counted to estimate the elements of each matrix T?
(a) directly.In this method the target speech is quantized by a codebook derived from the target's owntraining data thereby eliminating the cross-speaker quantization problem.
The smoothing problemis overcome by using the prototype speech itself as the prototype model while estimating thetransformation.We found that the second method outperformed the first using 30 seconds of training speechand an artificial grammar of perplexity .60.
This remained true even after controlling for thequantization problem of the first method by adapting the prototype codebook after the manner of\[6\].Several enhancements have been made to the DTW-based method.
As described in \[3\], we in-troduced an iterative normalization procedure which modifies the speech parameters ofone speakerby shifting them toward the other speaker.
A VQ codebook partitions the speech of one speakerinto groups of spectra which quantize to a common VQ codeword.
The DTW alignment maps thepartition onto corresponding groups of spectra for the other speaker.
The shift is then determinedby the difference vector between the means of these corresponding groups of spectra.
Each itera-tion of aligning and shifting reduces the mean-squared rror of the aligned speech parameters untilconvergence.More recently, we have used additional features in the DTW to improve the alignment betweenutterances, and additional codebooks in the HMM to improve the prototype model.i012.
Bas ic  Approach  to Speaker  Adapt ionWe view the problem of speaker adaptation as one of modeling the difference between twospeakers.
One of the speakers, who we call the prototype, is represented by a speaker-dependentHMM trained from large amounts of speech data.
The other speaker, called the target, is repre-sented by only a small sample of speech.
If the difference between the speakers can be successfullymodeled, then one strategy for speaker adaptation is to make the prototype speaker look like thetarget speaker.
This can be accomphshed by finding a transformation which can be apphed to theprototype HMM that makes it into a good model of the target speech.The difference between speakers is a complex one, involving the interaction of spectral, artic-ulatory, phonological, and dialectal influences.
A non-parametric probabihstic mapping betweenthe VQ spectra of the two speakers has appropriate properties for such a problem.
A probabilistictransformation can capture the many-to-many mapping typical of the differences between speak-ers and it can be made robust even when estimated from sparse data.
Non-parametricity makesfew constraining assumptions about the data under transformation.
Mapping VQ spectra betweenspeakers constrains the transformation to dimensionswhich an be estimated reasonably from thelimited training data.We begin with high performance speaker-dependent phonetic models which have been trainedfrom a large sample of speech from the prototype speaker.
The speaker-dependent training proce-dure in the BYBLOS system has been described in \[1\].
For each state of the prototype HMM, wehave a discrete probability density function (pdf) represented here as a row vector:p(s) = \[p(kt\]s),p(k2Is), ...,p(kNls)\] (1)where p(kils) is the probability of the VQ label ki at state s of the prototype HMM model, and Nis the size of the VQ codebook.The elements of the desired transformed pdf, p'(s), can be computed from:Np(k~ls ) = ~p(Ikils)p(k~lki, s) (2)i=1Since we have insufficient data to estimate a separate transformation for each state we approximatep'(s) by:N(k ls) : p(k,I,)p(k lk,, (3)i=1where ?
(s) specifies an equivalence class defined on the states s.For each of the classes, ?
(s), the set of conditional probabihties, {p(k~,ki, ?
(s))}, for an i and jform an N ?
N matrix, T?
(,), which cart be interpreted as a probabilistic transformation matrix fromone speaker's spectral space to another's.
We can then rewrite the computation of the transformedpdf, p'(s), as the product of the prototype row vector, p(s), and the matrix, T?(?
):p'(s) = p(s) x T4,(.
); Tij?
(,) = p(k;lki, ep(s)) (4)There are many ways to estimate T?(s).
We describe next two procedures that we have inves-tigated.1024.
Experimental Resu l tsThe DARPA Resource Management database \[4\] defines a protocol for evaluating speaker adap-tive recognition systems which is constrained to use 12 sentences common to all speakers in thedatabase.
To avoid problems due to unobserved spectra, we have chosen to develop our speakeradaptation methods on a larger training set, which restricts us to the speaker-dependent portionof the database for performance evaluation.This segment of the database includes training and test data for 12 speakers ampled fromrepresentative dialects of the United States.
We have used the first 40 utterances (2 minutes ofspeech) of the designated training material for our limited training sample.
Two development testsets have been defined by the National Institute of Standards and Technology (NIST).
These testsets consist of 25 utterances for each speaker.
Each test set is drawn from different sentence textsand includes about 200 word tokens.For all of our experiments, we have used one male prototype speaker originally from the NewYork area.
30 minutes of speech (600 sentences) were recorded at BBN in a normal office environ-ment and used to train the prototype HMM.
The speech is sampled at 20 kHz and analysed into14 mel-frequency epstral coefficients at a frame rate of 10 ms. 14 cepstral derivatives, computedas a linear regression over 5 adjacent frames, are derived from the original coefficients.
The trans-formation matrices are made to be phoneme-dependent by defining the equivalence classes, ~b(s),over the 61 phonemes in the lexicon.Experiment Features Normalized Codebooks % Word Error123456141428282828NOYESNOYESNOYES17.814.715.313.210.89.8Table 1: Comparison of speaker adaptation results averaged over 8 speakers for the Word-Pairgrammar and the Oct. '87 test set.We have performed our development work on 8 speakers using the test set designated by NISTas Oct. '87 test.
The results of this work, using the standard word-pair grammar, are summarizedin Table 1, where:% Word Error = 100 ?
\[(substitutions + deletions + insertions) / number of word tokens\]For each experiment we show the number of features used in the DTW alignment, whether theiterative normalization procedure was used, and the number of codebooks used in recognition.Using experiment (1) as a baseline, the table shows a 45% decrease overall in word error ratefor using all three improvements ogether.
Comparing experiments using 14 features with theircounterparts u ing 28 features hows that the contribution due to the differential features i roughlya 10% - 14% reduction in error rate.
A similar comparison for using/not-using the normalization103reveals a 9% - 17% reduction.
Finally, using the second codebook reduces the error rate by 26% -29%.It should be mentioned that the 40 sentences used for training in these experiments are drawnequaUy from 6 recording sessions separated by several days.
Furthermore, the test data is fromanother session altogether.
For the adaptation methods described here, it is reasonable to assumethat the training data would be recorded in a single session and only a few minutes before thetransformed models were ready for use.
This means that the adaptation training and test datashould realistically come from the same recording session.
From earlier published experiments usingsingle-session training and test, we believe the multi-session material accounts for about I/5 of thetotal word error for the experiments reported here.SpeakerDAS (F)DMS (F)DTD (F)TABPGI-ICMR (F)ttXS (F)DTBERSRKMJWSBEF~vLeWordSubstitutions Deletions Insertions Correct2.02.24.32.23.92.63.27.58.57.29.08.45.11.52.80.43.42.01.70.52.61.41.94.55.82.40.00.00.40.00.01.73.20.01.92.90.00.90.996.695.095.394.494.195.796.489.990.190.986.585.892.6Word SentenceError Error3.5 16.05.0 20.05.1 32.05.6 32.05.9 32.06.0 36.06.9 32.010.1 48.011.8 52.012.0 48.013.5 52.015.1 56.08.4 38.0Table 2: Recognition performance by speaker for the Word-Pair grammar and the May '88 test set.We evaluated the three improvements o the system by testing on new data designated asthe May 88 test set which is defined for 12 speakers.
For this experiment, we added 2 features,normalized energy and differential energy, and an additional codebook for the energy features.
Allparameters for this experiment were fixed prior to testing.
The results shown in Table 2 wereobtained from the first run of the system on the May '88 test data.
All entries in the table arepercentages, where:% Word Correct = 100 x \[1 - (substitutions + deletions) / number of word tokens\]% Sentence Error = 100 x \[number of sentences with any error  / number of sentences\]and % Word Error is defined as in Table 1.The speakers in Table 2 are ordered from the top by increasing word error rate.
It is evidentfrom the table that the speakers cluster into two distinct performance groups.
It is remarkablethat all 5 female speakers are included in the higher performance group despite the fact that theprototype is male.
The ordering of speakers hown here is not predicted by their speaker-dependent104performance or by subjective listening.The average word error rate of 8.4% for this test set is comparable to previously reported resultsfrom speaker-independent systems on this identical test set.
Using training from 105 speakers (4200sentences), the word error rates for the Sphinx system of CMU was 8.9% and for the Lincoln Labssystem; 10.1%.
New results from these systems, on different est data but from the same 12speakers, are reported elsewhere in these proceedings.5.
Conc lus ionThree improvements o the DTW-based speaker adaptation method have been combined toachieve a 45% overall reduction in recognition word error rate on development test data.
The largestsingle improvement was due to the addition of a codebook derived from a set of cepstral derivativefeatures.
This improvement does not affect the estimation of the between-speaker transformation.This suggests that further improvements o the speaker-dependent prototype model can lead tosignificant improvements in the adapted model's performance.The performance of the system on new evaluation test data was 8.4% word error averaged over12 speakers, using the standard word-pair grammar.
The system used a total of 600 sentencesfrom a single prototype speaker and and a training sample of 40 sentences from each of the 12 testspeakers.
The performance of the system is comparable to several speaker-independent systemstrained on 4200 sentences from 105 speakers, and tested on the same data.
This result suggeststhat speaker adaptation may be the most cost-effective solution for applications which must bebrought up quickly and must accommodate changing task domains or test conditions.AcknowledgementThis work was supported by the Defense Advanced Research Projects Agency and monitoredby the Space and Naval Warfare Systems Command under Contract No.
N00039-85-C-0423.References\[1\] Chow, Y., M. Dunham, O. Kimball, M. Krasner, F. Kubala, J. Makhoul, P. Price, S. Roucos,and R. Schwartz (1987) "BYBLOS: The BBN Continuous Speech Recognition System," IEEEICASSP-87, paper 3.7.1.\[2\] Feng, M., F. Kubala, R. Schwartz, J. Makhoul (1988) "Improved Speaker Adaptation UsingText Dependent Spectral Mappings," IEEE ICASSP-88, paper $3.9.\[3\] Feng, M., R. Schwartz, F. Kubala, J. Makhoul (1989) "Iterative Normalization for Speaker-Adaptive Training in Continuous Speech Recognition," IEEE ICASSP-89, To be published.\[4\] Price, P., W. Fisher, J. Bernstein, and D. PaUett (1988) "The DARPA 1000-Word ResourceManagement Database for Continuous Speech Recognition," IEEE ICASSP-88, paper $13.21.\[5\] Schwartz, R., Y. Chow, F. Kubala (1987) "Rapid Speaker Adaptation using a ProbabilisticSpectral Mapping," IEEE ICASSP-87, paper 15.3.1.\[6\] Shikano, K., K. Lee, R. Reddy (1986) "Speaker Adaptation Through Vector Quantization,"IEEE \[CASSP-86, paper 49.5.105
