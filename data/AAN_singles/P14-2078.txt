Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 476?481,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsMutual Disambiguation for Entity LinkingEric ChartonPolytechnique Montr?ealMontr?eal, QC, Canadaeric.charton@polymtl.caMarie-Jean MeursConcordia UniversityMontr?eal, QC, Canadamarie-jean.meurs@concordia.caLudovic Jean-LouisPolytechnique Montr?ealludovic.jean-louis@polymtl.caMichel GagnonPolytechnique Montr?ealmichel.gagnon@polymtl.caAbstractThe disambiguation algorithm presented inthis paper is implemented in SemLinker, anentity linking system.
First, named entitiesare linked to candidate Wikipedia pages bya generic annotation engine.
Then, the al-gorithm re-ranks candidate links according tomutual relations between all the named enti-ties found in the document.
The evaluationis based on experiments conducted on the testcorpus of the TAC-KBP 2012 entity linkingtask.1 IntroductionThe Entity Linking (EL) task consists in linkingname mentions of named entities (NEs) found in adocument to their corresponding entities in a ref-erence Knowledge Base (KB).
These NEs can beof type person (PER), organization (ORG), etc.,and they are usually represented in the KB by aUniform Resource Identifier (URI).
Dealing withambiguity is one of the key difficulties in this task,since mentions are often highly polysemous, andpotentially related to many different KB entries.Various approaches have been proposed to solvethe named entity disambiguation (NED) problem.Most of them involve the use of surface forms ex-tracted from Wikipedia.
Surface forms consist ofa word or a group of words that match lexical unitslike Paris or New York City.
They are used asmatching sequences to locate corresponding can-didate entries in the KB, and then to disambiguatethose candidates using similarity measures.The NED problem is related to the Word SenseDisambiguation (WSD) problem (Navigli, 2009),and is often more challenging since mentions ofNEs can be highly ambiguous.
For instance,names of places can be very common as is Paris,which refers to 26 different places in Wikipedia.Hence, systems that attempt to address the NEDproblem must include disambiguation resources.In the context of the Named Entity Recognition(NER) task, such resources can be generic andgenerative.
This generative approach does not ap-ply to the EL task where each entity to be linked toa semantic description has a specific word context,marker of its exact identity.One of the classical approach to conduct thedisambiguation process in NED applications is toconsider the context of the mention to be mapped,and compare this context with contextual informa-tion about the potential target entities (see for in-stance the KIM system (Popov et al, 2003)).
Thisis usually done using similarity measures (such ascosine similarity, weighted Jaccard distance, KLdivergence...) that evaluate the distance betweena bag of words related to a candidate annotation,and the words surrounding the entity to annotatein the text.In more recent approaches, it is suggested thatannotation processes based on similarity distancemeasures can be improved by making use of otherannotations present in the same document.
Suchtechniques are referred to as semantic related-ness (Strube and Ponzetto, 2006), collective dis-ambiguation (Hoffart et al, 2011b), or joint dis-ambiguation (Fahrni et al, 2012).
The idea is toevaluate in a set of candidate links which one isthe most likely to be correct by taking the otherlinks contained in the document into account.
Forexample, if a NE describes a city name like Paris,it is more probable that the correct link for thiscity name designates Paris (France) rather thanParis (Texas) if a neighbor entity offers candidatelinks semantically related to Paris (France) likethe Seine river or the Champs-Elys?ees.
Such tech-niques mostly involve exploration of graphs result-ing of all the candidate annotations proposed for agiven document, and try to rank the best candi-dates for each annotation using an ontology.
Theontology (like YAGO or DBPedia) provides a pre-476existing set of potential relations between the enti-ties to link (like for instance, in our previous exam-ple, Paris (France) has river Seine) that willbe used to rank the best candidates according totheir mutual presence in the document.In this paper we explore the capabilities of a dis-ambiguation algorithm using all the available an-notation layers of NEs to improve their links.
Thepaper makes the following novel propositions: 1)the ontology used to evaluate the relatedness ofcandidates is replaced by internal links and cate-gories from the Wikipedia corpus; 2) the coher-ence of entities is improved prior to the calcula-tion of semantic relatedness using a co-referenceresolution algorithm, and a NE label correctionmethod; 3) the proposed method is robust enoughto improve the performance of existing entity link-ing annotation engines, which are capable of pro-viding a set of ranked candidates for each annota-tion in a document.This paper is organized as follows.
Section 2describes related works.
The proposed method ispresented in Section 3 where we explain how ourSemLinker system prepares documents that con-tain mentions to disambiguate, then we detail thedisambiguation algorithm.
The evaluation of thecomplete system is provided in Section 4.
Finally,we discuss the obtained results, and conclude.2 Related WorkEntity annotation and linking in natural languagetext has been extensively studied in NLP research.A strong effort has been conducted recently by theTAC-KBP evaluation task (Ji et al, 2010) to cre-ate standardized corpus, and annotation standardsbased on Wikipedia for evaluation and comparisonof EL systems.
In this paper, we consider the TAC-KBP framework.
We describe below some recentapproaches proposed for solving the EL task.2.1 Wikipedia-based Disambiguation MethodsThe use of Wikipedia for explicit disambiguationdates back to (Bunescu and Pasca, 2006) who builta system that compared the context of a mentionto the Wikipedia categories of an entity candidate.Lately, (Cucerzan, 2007; Milne and Witten, 2008;Nguyen and Cao, 2008) extended this frameworkby using richer features for similarity comparison.Some authors like Milne and Witten (2008) uti-lized machine learning methods rather than a sim-ilarity function to map mentions to entities.
Theyalso introduced the notion of semantic relatedness.Alternative propositions were suggested in otherworks like (Han and Zhao, 2009) that consideredthe relatedness of common noun phrases in a men-tion context with Wikipedia article names.
Whileall these approaches focus on semantic relation be-tween entities, their potential is limited by the sep-arate mapping of candidate links for each mention.2.2 Semantic Web Compliant MethodsMore recently, several systems have beenlaunched as web services dedicated to EL tasks.Most of them are compliant with new emergentsemantic web standards like LinkedData network.DBPedia Spotlight (Mendes et al, 2011) is asystem that finds mentions of DBpedia (Aueret al, 2007) resources in a textual document.Wikimeta (Charton and Gagnon, 2012) is anothersystem relying on DBpedia.
It uses bags of wordsto disambiguate semantic entities according toa cosine similarity algorithm.
Those systemshave been compared with commercial oneslike AlchemyAPI, Zemanta, or Open Calaisin (Gangemi, 2013).
The study showed thatthey perform differently on various essentialaspects of EL tasks (mention detection, linking,disambiguation).
This suggests a wide range ofpotential improvements on many aspects of theEL task.
Only some of these systems introducethe semantic relatedness in their methods likethe AIDA (Hoffart et al, 2011b) system.
Itproposes a disambiguation method that combinespopularity-based priors, similarity measures, andcoherence.
It relies on the Wikipedia-derivedYAGO2 (Hoffart et al, 2011a) knowledge base.3 Proposed AlgorithmWe propose a mutual disambiguation algorithmthat improves the accuracy of entity links in a doc-ument by using successive corrections applied toan annotation object representing this document.The annotation object is composed of informationextracted from the document along with linguisticand semantic annotations as described hereafter.3.1 Annotation ObjectDocuments are processed by an annotator capableof producing POS tags for each word, as well asspans, NE surface forms, NE labels and rankedcandidate Wikipedia URIs for each candidate NE.For each document D, this knowledge is gathered477in an array called annotation object, which has ini-tially one row per document lexical unit.
Since thesystem focuses on NEs, rows with lexical unitsthat do not belong to a NE SF are dropped fromthe annotation object, and NE SF are refined as de-scribed in (Charton et al, 2014).
When NE SF arespanned over several rows, these rows are mergedinto a single one.
Thus, we consider an annotationobject AD, which is an array with a row for eachNE, and columns storing related knowledge.If n NEs were annotated in D, then ADhas nrows.
If l candidate URIs are provided for eachNE, then ADhas (l + 4) columns cu,u?
{1,l+4}.Columns c1to clstore Wikipedia URIs associatedwith NEs, ordered by decreasing values of likeli-hood.
Column cl+1stores the offset of the NEs,cl+2stores their surface forms, cl+3stores the NElabels (PER, ORG, ...), and cl+4stores the (vec-tors of) POS tags associated with the NE surfaceforms.
ADcontains all the available knowledgeabout the NEs found inD.
Before being processedby the disambiguation module,ADis dynamicallyupdated by correction processes.3.2 Named Entity Label CorrectionTo support the correction process based on co-reference chains, the system tries to correct NElabels for all the NEs listed in the annotation ob-ject.
The NE label correction process assigns thesame NE label to all the NEs associated with thesame first rank URI.
For all the rows inAD, sets ofrows with identical first rank URIs are considered.Then, for each set, NE labels are counted per type,and all the rows in a same set are updated with themost frequent NE label found in the set, i.e.
all theNEs in this set are tagged with this label.3.3 Correction Based on Co-reference ChainsFirst rank candidate URIs are corrected by a pro-cess that relies on co-reference chains found inthe document.
The co-reference detection is con-ducted using the information recorded in the anno-tation object.
Among the NEs present in the docu-ment, the ones that co-refer are identified and clus-tered by logical rules applied to the content of theannotation object.
When a co-reference chain ofNEs is detected, the system assigns the same URIto all the members of the chain.
This URI is se-lected through a decision process that gives moreweight to longer surface forms and frequent URIs.The following example illustrates an applicationof this correction process:Three sentences are extracted from a documentabout Paris, the French capital.
NEs are indicatedin brackets, first rank URIs and surface forms areadded below the content of each sentence.- [Paris] is famous around the world.URI1: http://en.wikipedia.org/wiki/Paris HiltonNE surface form: Paris- The [city of Paris] attracts millions of tourists.URI1: http://en.wikipedia.org/wiki/ParisNE surface form: city of Paris- The [capital of France] is easy to reach by train.URI1: http://en.wikipedia.org/wiki/ParisNE surface form: capital of FranceThe three NEs found in these sentences com-pose a co-reference chain.
The second NE hasa longer surface form than the first one, andits associated first rank URI is the most fre-quent.
Hence, the co-reference correction pro-cess will assign the right URI to the first NE(URI1: http://en.wikipedia.org/wiki/Paris), whichwas wrongly linked to the actress Paris Hilton.3.4 Mutual Disambiguation ProcessThe extraction of an accurate link is a process oc-curring after the URI annotation of NEs in thewhole document.
The system makes use of allthe semantic content stored in ADto locally im-prove the precision of each URI annotation in thedocument.
The Mutual Disambiguation Process(MDP) relies on the graph of all the relations (in-ternal links, categories) between Wikipedia con-tent related to the document annotations.A basic example of semantic relatedness thatshould be captured is explained hereafter.
Let usconsider the mention IBM in a given document.Candidate NE annotations for this mention can beInternational Business Machine or InternationalBrotherhood of Magicians.
But if the IBM men-tion co-occurs with a Thomas Watson, Jr mentionin the document, there will probably be more linksbetween the International Business Machine andThomas Watson, Jr related Wikipedia pages thanbetween the International Brotherhood of Magi-cians and Thomas Watson, Jr related Wikipediapages.
The purpose of the MDP is to capture thissemantic relatedness information contained in thegraph of links extracted from Wikipedia pages re-lated to each candidate annotation.In MDP, for each Wikipedia URI candidate an-notation, all the internal links and categories con-tained in the source Wikipedia document related478to this URI are collected.
This information will beused to calculate a weight for each of the l can-didate URI annotations of each mention.
For agiven NE, this weight is expected to measure themutual relations of a candidate annotation with allthe other candidate annotations of NEs in the doc-ument.
The input of the MDP is an annotationobject ADwith n rows, obtained as explained inSection 3.1.
For all i ?
[[1, n]], k ?
[[1, l]], we buildthe set Ski, composed of the Wikipedia URIs andcategories contained in the source Wikipedia doc-ument related to the URI stored in AD[i][k] thatwe will refer to as URIkito ease the reading.Scoring:For all i, j ?
[[1, n]], k ?
[[1, l]], we want to cal-culate the weight of mutual relations between thecandidate URIkiand all the first rank candidatesURI1jfor j 6= i.
The calculation combines twoscores that we called direct semantic relation score(dsr score) and common semantic relation score(csr score):- the dsr score for URIkisums up the number ofoccurrences of URIkiin S1jfor all j ?
[[1, n]]?
{i}.- the csr score for URIkisums up the number ofcommon URIs and categories between Skiand S1jfor all j ?
[[1, n]]?
{i}.We assumed the dsr score was much moresemantically significant than the csr score, andtranslated this assumption in the weight calcula-tion by introducing two correction parameters ?and ?
used in the final scoring calculation.Re-ranking:For all i ?
[[1, n]], for each set of URIs {URIki, k ?
[[1, l]]}, the re-ranking process is conducted ac-cording to the following steps:For all i ?
I ,1.
?k ?
[[1, l]], calculate dsr score(URIki)2.
?k ?
[[1, l]], calculate csr score(URIki)3.
?k ?
[[1, l]], calculatemutual relation score(URIki) =?.dsr score(URIki)+?.csr score(URIki)4. re-order {URIki, k ?
[[1, l]]}, bydecreasing order of mutual relation score.In the following, we detail the MDP in the con-text of a toy example to illustrate how it works.The document contains two sentences, NE men-tions are in bold:IBM has 12 research laboratoriesworldwide.
Thomas J. Watson, Jr.became president of the company.For the first NE mention [IBM], ADcontainstwo candidate URIs identifying two different re-sources:[IBM] URI11?
International Brotherhood of MagiciansURI21?
International Business Machines CorporationFor the second NE mention [Thomas J.Watson, Jr.], ADcontains the following can-didate URI, which is ranked first:[Thomas J. Watson, Jr.] URI12?
Thomas Watson, Jr.S11gathers URIs and categories contained in theInternational Brotherhood of Magicians Wikipediapage.
S21is associated to the International BusinessMachines Corporation, and S12to the Thomas Watson,Jr.
page.
dsr score(URI11) sums up the number ofoccurrences of URI11in S1jfor all j ?
[[1, n]]?
{1}.Hence, in the current example, dsr score(URI11) isthe number of occurrences of URI11in S12, namelythe number of times the International Brotherhoodof Magicians are cited in the Thomas Watson, Jr.page.
Similarly, dsr score(URI21) is equal to thenumber of times the International Business MachinesCorporation is cited in the Thomas Watson, Jr. page.csr score(URI11) sums up the number of commonURIs and categories between S11and S12, i.e.
thenumber of URIs and categories appearing in bothInternational Brotherhood of Magicians and ThomasWatson, Jr. pages.
csr score(URI21) counts thenumber of URIs and categories appearing in bothInternational Business Machines Corporation andThomas Watson, Jr. pages.After calculation, we have:mutual relation score(URI11) < mutual relation score(URI21)The candidate URIs for [IBM] are re-rankedaccordingly, and International Business MachinesCorporation becomes its first rank candidate.4 Experiments and ResultsSemLinker has been evaluated on the TAC-KBP2012 EL task (Charton et al, 2013).
In this task,mentions of entities found in a document collec-tion must be linked to entities in a reference KB, orto new named entities discovered in the collection.The document collection built for KBP 2012 con-tains a combination of newswire articles (News),479SemLinker TAC-KBP2012 systemsmodules no disambiguation MDP only all modules 1st2nd3rdmedianCategory B3+P B3+R B3+F1 B3+P B3+R B3+F1 B3+P B3+R B3+F1 B3+F1 B3+F1 B3+F1 B3+F1Overall 0.620 0.633 0.626 0.675 0.681 0.678 0.694 0.695 0.695 0.730 0.699 0.689 0.536PER 0.771 0.791 0.781 0.785 0.795 0.790 0.828 0.838 0.833 0.809 0.840 0.714 0.645ORG 0.600 0.571 0.585 0.622 0.578 0.599 0.621 0.569 0.594 0.715 0.615 0.717 0.485GPE 0.412 0.465 0.437 0.570 0.628 0.598 0.574 0.626 0.599 0.627 0.579 0.614 0.428News 0.663 0.691 0.677 0.728 0.748 0.738 0.750 0.767 0.758 0.782 0.759 0.710 0.574Web 0.536 0.520 0.528 0.572 0.550 0.561 0.585 0.556 0.570 0.630 0.580 0.508 0.491Table 1: SemLinker results on the TAC-KBP 2012 test corpus with/out disambiguation modules, andthree best results and median from TAC-KBP 2012 systems.posts to blogs and newsgroups (Web).
Given aquery that consists of a document with a specifiedname mention of an entity, the task is to determinethe correct node in the reference KB for the entity,adding a new node for the entity if it is not alreadyin the reference KB.
Entities can be of type person(PER), organization (ORG), or geopolitical entity(GPE).
The reference knowledge base is derivedfrom an October 2008 dump of English Wikipedia,which includes 818,741 nodes.
Table 2 provides abreakdown of the queries per categories of entities,and per type of documents.Category All PER ORG GPE News Web# queries 2226 918 706 602 1471 755Table 2: Breakdown of the TAC-KBP 2012 testcorpus queries according to entity types, and doc-ument categories.A complete description of these linguistic re-sources can be found in (Ellis et al, 2011).
Forthe sake of reproducibility, we applied the KBPscoring metric (B3+ F ) described in (TAC-KBP,2012), and we used the KBP scorer1.The evaluated system makes use of theWikimeta annotation engine.
The maximum num-ber of candidate URIs is l = 15.
The MDP correc-tion parameters ?
and ?
described in Section 3.4have been experimentally set to ?
= 10, ?
= 2.Table 1 presents the results obtained by the sys-tem in three configurations.
In the first column,the system is evaluated without the disambigua-tion module.
In the second column, we appliedthe MDP without correction processes.
The sys-tem with the complete disambiguation module ob-tained the results provided in the third column.The three best results and the median from TAC-KBP 2012 systems are shown in the remainingcolumns for the sake of comparison.1http://www.nist.gov/tac/2013/KBP/EntityLinking/tools.htmlWe observe that the complete algorithm (co-references, named entity labels and MDP) pro-vides the best results on PER NE links.
On GPEand ORG entities, the simple application of MDPwithout prior corrections obtains the best results.A slight loss of accuracy is observed on ORG NEswhen the MDP is applied with corrections.
Forthose three categories of entities, we show that thecomplete system improves the performance of asimple algorithm using distance measures.
Resultson categories News and Web show that the bestperformance on the whole KBP corpus (withoutdistinction of NE categories) is obtained with thecomplete algorithm.5 ConclusionThe presented system provides a robust seman-tic disambiguation method, based on mutual re-lation of entities inside a document, using a stan-dard annotation engine.
It uses co-reference, NEnormalization methods, and Wikipedia internallinks as mutual disambiguation resource to im-prove the annotations.
We show that our propo-sition improves the performance of a standard an-notation engine applied to the TAC-KBP evalua-tion framework.
SemLinker is fully implemented,and publicly released as an open source toolkit(http://code.google.com/p/semlinker).
Ithas been deployed in the TAC-KBP 2013 evalu-ation campaign.
Our future work will integrateother annotation engines in the system architecturein a collaborative approach.AcknowledgmentsThis research was supported as part of Dr EricCharton?s Mitacs Elevate Grant sponsored by3CE.
Participation of Dr Marie-Jean Meurs wassupported by the Genozymes Project funded byGenome Canada & G?enome Qu?ebec.
The Con-cordia Tsang Lab provided computing resources.480ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.2007.
Dbpedia: A nucleus for a web of open data.In The semantic web, pages 722?735.
Springer.Razvan C. Bunescu and Marius Pasca.
2006.
Us-ing encyclopedic knowledge for named entity dis-ambiguation.
In Proceedings of the Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL).
ACL.Eric Charton and Michel Gagnon.
2012.
A disam-biguation resource extracted from Wikipedia for se-mantic annotation.
In Proceedings of LREC 2012.Eric Charton, Marie-Jean Meurs, Ludovic Jean-Louis,and Michel Gagnon.
2013.
SemLinker systemfor KBP2013: A disambiguation algorithm basedon mutual relations of semantic annotations insidea document.
In Text Analysis Conference KBP.U.S.
National Institute of Standards and Technology(NIST).Eric Charton, Marie-Jean Meurs, Ludovic Jean-Louis,and Michel Gagnon.
2014.
Improving Entity Link-ing using Surface Form Refinement.
In Proceedingsof LREC 2014.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on wikipedia data.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing EMNLP-CoNLL.
ACL.Joe Ellis, Xuansong Li, Kira Griffitt, Stephanie MStrassel, and Jonathan Wright.
2011.
Linguistic re-sources for 2012 knowledge base population evalu-ations.
In Proceedings of TAC-KBP 2012.Angela Fahrni, Thierry G?ockel, and Michael Strube.2012.
Hitsmonolingual and cross-lingual entitylinking system at tac 2012: A joint approach.
InTAC (Text Analysis Conference) 2012 Workshop.Aldo Gangemi.
2013.
A Comparison of KnowledgeExtraction Tools for the Semantic Web.
In The 10thExtended Semantic Web Conference (ESWC) 2013.Xianpei Han and Jun Zhao.
2009.
Named entitydisambiguation by leveraging wikipedia semanticknowledge.
In Proceedings of the Conference onInformation and Knowledge Management (CIKM).ACM.Johannes Hoffart, Fabian M Suchanek, KlausBerberich, Edwin Lewis-Kelham, Gerard De Melo,and Gerhard Weikum.
2011a.
Yago2: exploring andquerying world knowledge in time, space, context,and many languages.
In Proceedings of the 20thinternational conference companion on World wideweb, pages 229?232.
ACM.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-iol, Bilyana Taneva, Stefan Thater, and GerhardWeikum.
2011b.
Robust disambiguation of namedentities in text.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 782?792.
Association for ComputationalLinguistics.Heng Ji, Ralph Grishman, HT Dang, and K Griffitt.2010.
Overview of the TAC 2010 knowledge basepopulation track.
Proceedings of TAC 2010.Pablo N Mendes, Max Jakob, Andr?es Garc?
?a-Silva, andChristian Bizer.
2011.
DBpedia Spotlight: Shed-ding Light on the Web of Documents.
In The 7thInternational Conference on Semantic Systems (I-Semantics) 2011, pages 1?8.David N. Milne and Ian H. Witten.
2008.
Named en-tity disambiguation by leveraging wikipedia seman-tic knowledge.
In Proceedings of the Conference onInformation and Knowledge Management (CIKM).ACM.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys (CSUR), 41(2):10.Hien T. Nguyen and Tru H. Cao.
2008.
Namedentity disambiguation on an ontology enriched bywikipedia.
In Research, Innovation and Vision forthe Future, 2008.
RIVF 2008.
IEEE InternationalConference on, pages 247?254.
IEEE.Borislav Popov, Atanas Kiryakov, Angel Kirilov, Dimi-tar Manov, Damyan Ognyanoff, and Miroslav Gora-nov. 2003.
KIM ?
Semantic annotation platform.Lecture Notes in Computer Science, pages 834?849.Michael Strube and Simone Paolo Ponzetto.
2006.WikiRelate!
Computing Semantic Relatedness Us-ing Wikipedia.
In AAAI, volume 6, pages 1419?1424.TAC-KBP.
2012.
Proposed Task Description forKnowledge-Base Population at TAC 2012.
In Pro-ceedings of TAC-KBP 2012.
National Institute ofStandards and Technology.481
