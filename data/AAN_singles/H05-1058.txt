Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 459?466, Vancouver, October 2005. c?2005 Association for Computational LinguisticsPart-of-Speech Tagging using Virtual Evidence and Negative TrainingSheila M. Reynolds and Jeff A. BilmesDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195-2500{sheila,bilmes}@ee.washington.eduAbstractWe present a part-of-speech tagger whichintroduces two new concepts: virtual evi-dence in the form of an ?observed child?node, and negative training data to learnthe conditional probabilities for the ob-served child.
Associated with each wordis a flexible feature-set which can in-clude binary flags, neighboring words, etc.The conditional probability of Tag givenWord + Features is implemented usinga factored language-model with back-offto avoid data sparsity problems.
Thismodel remains within the framework ofDynamic Bayesian Networks (DBNs) andis conditionally-structured, but resolvesthe label bias problem inherent in the con-ditional Markov model (CMM).1 IntroductionA common sequence-labeling task in natural lan-guage processing involves assigning a part-of-speech (POS) tag to each word in the input text.Previous authors have used numerous HMM-basedmodels (Banko and Moore, 2004; Collins, 2002;Lee et al, 2000; Thede and Harper, 1999) andother types of networks including maximum entropymodels (Ratnaparkhi, 1996), conditional Markovmodels (Klein and Manning, 2002; McCallum etal., 2000), conditional random fields (CRF) (Laf-ferty et al, 2001), and cyclic dependency networks(Toutanova et al, 2003).
All of these models makeuse of varying amounts of contextual information.In this paper, we present a new model which re-mains within the well understood framework of Dy-namic Bayesian Networks (DBNs), and we showthat it produces state-of-the-art results when ap-plied to the POS-tagging task.
This new model isconditionally-structured and, through the use of vir-tual evidence (Pearl, 1988; Bilmes, 2004), resolvesthe explaining-away problems (often described aslabel or observation bias) inherent in the CMM.This paper is organized as follows.
In sec-tion 2 we discuss the differences between a hiddenMarkov model (HMM) and the corresponding con-ditional Markov model (CMM).
In section 3 we de-scribe our observed-child model (OCM), introduc-ing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of nega-tive training data.
In section 4 we discuss our exper-iments and results, including a comparison of threesimple first-order models and state-of-the-art resultsfrom our feature-rich second-order OCM.For clarity, the comparisons and derivations insections 2 and 3 are done for first-order models us-ing a single binary feature.
The same ideas are thengeneralized to a higher order model with more fea-tures (including adjacent words).2 Generative vs.
Conditional ModelsIn this section we discuss the tradeoffs between thegenerative hidden Markov model (HMM) and theconditional Markov model (CMM).
For pedagogicalreasons, the figures and equations are for first ordermodels with a single word-feature.The HMM shown in Figure 1 includes a single459feature (the binary flag isCap) in addition to theword itself.
Each observation, oi = (wi, fi), is aword-feature pair.
Let o = {oi} be the observationsequence and s = {si} be the associated tag (state)sequence.
The HMM1 factorizes the joint probabil-ity distribution over these two sequences as:P (s,o) =?iP (si|si?1)P (wi|si)P (fi|si)TagWordisCapFigure 1: First order HMM.A similar model often used for sequence label-ing tasks is the conditional Markov model (CMM)which reverses the arrows between the words andthe tags (Figure 2), and factorizes as:P (s,o) =?iP (si|si?1, wi, fi)P (wi)P (fi)TagWordisCapFigure 2: First order CMM.Because the words and features are observed, thismodel does not require that we compute the proba-bility of the evidence, P (o), when finding the opti-mal tag sequence.
The tag-sequence s which max-imizes the joint probability P (s,o) is the same onethat maximizes the conditional probability P (s|o).The CMM, therefore, does not require that we modelthe language, allowing us to focus on modeling theconditional probability of the tags given the words.The HMM has its advantages as well, principallythat it is easier to train than the CMM because it1In this HMM, Word and isCap are independent given Tag,but this need not be true in general.factorizes the joint probability into simpler com-ponents.
The tables required for P (si|si?1) andP (oi|si) are significantly smaller than the one forP (si|si?1, oi) which may be difficult to estimate dueto either data sparsity or normalization issues.
Onepotential disadvantage of the HMM is that when it istrained using a maximum likelihood procedure, it isnot necessarily encouraged to optimally classify tagsdue to its generative nature.
One solution is to trainthe HMM using a discriminative procedure.
Anotheroption is to use entirely different models.A key disadvantage of the CMM is that itmakes critical statements about independence thatthe HMM does not: the converging arrows at eachtag put the parent nodes (the previous tag and thecurrent observation) into causal competition and asa result the model states that the previous tag is inde-pendent of the current observation.
In other words,all states (tags) are independent of future observa-tions (words).
The CMM thus incorporates a strongdirectional bias which does not exist in the HMM.One way to eliminate this bias is to use a CRF(Lafferty et al, 2001; McCallum, 2003), where fac-tors over neighboring tags may use features fromanywhere in the observation sequence.
The CRFis discriminative and avoids label/observation biasby using a model that is constrained only in thatthe conditional distribution factorizes over an undi-rected Markov chain.
However, most popular train-ing procedures for a CRF are time-consuming andcomplex processes.3 Using Virtual EvidenceOur goals in this work are to: 1) keep the discrimi-native nature of the CMM to the extent possible; 2)avoid label and observation bias issues; and 3) stayentirely within the DBN framework where trainingis relatively simple.
We thus propose a new solu-tion to the problem, which retains the discrimina-tive conditional form of ?tag given word?
from theCMM, but avoids label bias by temporally linkingadjacent tags in a new way.
Specifically, we employvirtual evidence in the form of a binary observedchild node, ci, between adjacent tags (Figure 3) ora windowed sequence of tags.
During decoding, thisnode will always be observed to be equal to 1 (one).Intuitively, this binary variable acts as an indicator of460TagWordisCapCFigure 3: First order observed-child model (OCM)with the tags connected in pairs.tag-pair consistency.
When the tag pairs are consis-tent (as they are in real text), we should have a highconditional probability that ci = 1; and when thetag pairs are not consistent, the conditional probabil-ity that ci = 1 should be low.
With this conditionaldistribution, observing ci = 1 during decoding ex-presses a preference for consistent tag pairs.The presence of this observed-child node resultsin a term in the factorization of the joint probabilitydistribution that couples its parents:P (c, s,o) ?
?iP (ci|si?1, si)P (si|wi, fi)where ci is the observed-child node of tags si?1 andsi, and we omit the probability of the observations,P (wi, fi) which do not affect the final choice of s.By the rules of d-separation (Pearl, 1988), the ex-istence of ci defined in this way means that the par-ents (the adjacent tags) are not conditionally inde-pendent given the child.
This link between adja-cent tags through an observed-child node allows fora probabilistic relationship to exist between the ad-jacent tags.
Thus, future words can influence tags,which is not true for the CMM.
Whether or not arelationship between tags will actually be learned,however, will critically depend on how the model istrained.
In a graphical model, it is the lack of anedge that ensures some form of independence; thepresence of an edge (or a path made up of two ormore edges) does not necessarily ensure the reverse.3.1 TrainingThe introduction of virtual evidence into a graph-ical model requires that careful thought be givento the training process.
If we were to na?
?vely addci = 1 to all samples of the training data, the modelwould learn that ci is constant rather than random,and therefore that it is independent of its parents,si?1 and si.
In other words, this na?
?vely-trainedmodel would assume that P (ci = 1|si?1, si) =1 ?
(si?1, si), and when used to tag the sentencesin the test-set (also labeled with ci = 1), it wouldmaximize this simplified joint probability in whichthe relationship between si?1 and si has been lost:P (c, s,o) ?
?iP (si|wi, fi)In order to induce and thereby have the modellearn the relationship between the adjacent tags si?1and si, the training has to be modified to includesamples that are labeled with ci = 0.
The proba-bility table P (ci = 1|si?1, si) should favor common(consistent) tag-pairs with high probabilities, whilediscouraging rare tag-pairs with low probabilities.Although all observations (in both training andtest sets) are labeled with ci = 1, we hypothesizean alternate set of observations labeled with ci = 0.This alternate set will be the source of the negativetraining data 2.
It is a set of nonsensical sentenceswith the same distribution over individual tags, i.e.the same P (si), but in this set adjacent tags are in-dependent.
We denote the total number of trainingsamples by M .
This is divided into positive train-ing samples, M1, and negative training samples, M0,with M1+M0 = M .
The ratio of the amount of pos-itive to negative training data should be the same asthe ratio of our prior beliefs about tag-pair consis-tency, namely the ratio of P (ci = 1) to P (ci = 0).With no evidence to support that one is more likelythan the other, one option is to use the strategy of?assuming the least?
and use a maximum entropyprior, setting M0 = M1.
More flexibly, we can de-fine n to be the ratio of the two so that M0 = n ?M1.Now we derive a method for training the condi-tional probability table P (ci|si?1, si) in terms of thepointwise mutual information between the adjacenttags si?1 and si.
We first rewrite the conditionalprobability (henceforth abbreviated as p) as:p = P (ci = 1|si?1, si) =P (ci = 1, si?1, si)P (si?1, si)If the probabilities are maximum likelihood (ML)estimates derived from counts on the training data,we can equivalently write:p = N(ci = 1, si?1, si)N(si?1, si)2This use of implied negative training data is similar to the?neighborhood?
concept described in (Smith and Eisner, 2005)461where N(?)
is the count function.Expanding the denominator into two terms:p = N(ci = 1, si?1, si)N(ci = 1, si?1, si) + N(ci = 0, si?1, si)Without any negative training data (labeled withci = 0), this ratio would always evaluate to 1, and noprobabilistic relationship between si?1 and si wouldbe learned.From the start, we have implicitly postulated a re-lationship between adjacent tags.
We now formallystate two hypotheses: H1 that there is a relationshipbetween adjacent tags which can be described bysome joint probability distribution P (si?1, si), andthe null hypothesis, H0, that there is no such rela-tionship, i.e.
si?1 and si are independent:PH1 = P (si?1, si)PH0 = P (si?1)P (si)Now we can express the counts as follows:N(ci = 1, si?1, si) = M1 ?
P (si?1, si)N(ci = 0, si?1, si) = M0 ?
P (si?1)P (si)where M1 is the total number of tokens in the (posi-tive) training data, and M0 is the total number of to-kens in the induced negative training data.
We sub-stitute M0 with n ?
M1 for the reasons mentionedearlier, and simplify to obtain:p = P (si?1, si)P (si?1, si) + nP (si?1)P (si)which can be simplified to obtain:p = 11 + n[P (si?1,si)P (si?1)P (si)]?1The ratio of probabilities in the denominator is theratio used in computing the pointwise mutual infor-mation between si?1 and si.
This ratio, which wewill call ?, is also the likelihood ratio between thetwo previously stated hypotheses.
Finally, we writethe conditional probability as a function of ?
:P (ci = 1|si?1, si) =11 + n?
?1 =??
+ nwhere ?
= PH1PH0= P (si?1, si)P (si?1)P (si)= P (si|si?1)P (si)The conditional probability, P (ci = 1|si?1, si) is amapping g(?)
from ?
?
[0,?)
to p ?
[0, 1).Beginning with (Church and Hanks, 1989), nu-merous authors have used the pointwise mutual in-formation between pairs of words to analyze wordco-locations and associations.
This ratio tells uswhether si?1 and si co-occur more or less often thanwould be expected by chance alone.Consider, for example, the tags DT (determiner)and NN (noun), and the four possible ordered tag-pairs.
The probabilities P (si) and P (si|si?1) de-rived from the training data (see section 4.1), thelikelihood ratio score ?, the conditional probabilityp = P (ci = 1|si?1, si), and the occurrence countsN are shown in Table 1.
As expected, the sequenceDT-NN (e.g.
the surplus) occurs very often and getsa high score, while DT-DT (e.g.
this a) and NN-DT (e.g.
surplus the) occur infrequently and get lowscores.
The sequence NN-NN (e.g.
trade surplus)gets a neutral score (?
?
1) indicating that if the pre-ceding word is a noun, the likelihood that the currentword is a noun is nearly equal to the likelihood thatany randomly chosen word is a noun.We present two methods for inducing the negativetraining counts that are required to train the condi-tional probability table for P (ci|si?1, si).In the first method, we generate ?nonsense?
sen-tences by randomly scrambling each sentence in thetraining-set n times, using a uniform distributionover all possible permutations.
This results in nnegative training sentences for each positive trainingsentence and therefore M0 = n?M1.
Effectively, theratio of priors on ci is now:P (ci = 1)P (ci = 0)= M1M0= 1nThe conditional probability table P (ci|si?1, si) issi?1-si P (si) P (si|si?1) ?
p NDT-NN 0.129 0.4905 3.80 0.79 37301NN-NN 0.129 0.1270 0.98 0.49 15571NN-DT 0.080 0.0071 0.09 0.08 870DT-DT 0.080 0.0018 0.02 0.02 134Table 1: Sample likelihood ratio scores (?
), proba-bilities, p (for n = 1), and counts for four tag-pairs.462then trained using all n+1 versions of each sentence,thus inducing the desired dependence between si?1and si.
The method of scrambling sentences n-timesonly approximates the theory described above be-cause it is performed on a sentence-by-sentence ba-sis rather than across the entire training set.
Also, theresulting negative training data represents only n re-alizations of a random process, so the total numberof samples may not be large enough to approximatethe underlying distribution.In the second method, rather than generate thenegative training data in the form of scrambled sen-tences, we compute the negative-training counts di-rectly, based on the positive unigram counts and thehypotheses presented in section 3.1.
For example,the negative bigram counts are a function of themarginal probability of each tag, P (si):N(ci = 0, si?1, si) = nM1 ?
P (si?1)P (si)Negative unigram and trigram counts are computedin a similar fashion, and then the conditional proba-bility table is derived as a smoothed back-off modeldirectly from the combined sets of counts.These two methods are conceptually similar butmay exhibit subtle differences: one is randomizingat the sentence level while the other operates overthe entire training set and does not have the samesensitivity to small values of n.4 Experiments and ResultsIn this section we describe our experiments and theresults obtained.
Sections 4.1 and 4.2 describe thedata sets and features.
Section 4.3 presents compar-isons between several simple models using just thetags, the words, and a single binary feature for eachword.
Section 4.4 presents results from a feature-rich second-order observed-child model in whichtags are linked in groups of three.All training of language models is done using theSRILM toolkit (Stolcke, 2002) with the FLM exten-sions (Bilmes and Kirchhoff, 2003), and the imple-mentation and testing of the various graphical mod-els is carried out with the help of the graphical mod-els toolkit (GMTK) (Bilmes and Zweig, 2002).4.1 Data SetsThe data used for these experiments is the WallStreet Journal data from Penn Treebank III (Mar-cus et al, 1994).
We extracted tagged sentencesfrom the parse trees and divided the data into train-ing (sections 0-18), development (sections 19-21),and test (sections 22-24) sets as in (Toutanova et al,2003).
Except for the final results for the feature-rich model, all results are on the development set.4.2 FeaturesThe tagged sentences extracted from the Penn Tree-bank are pre-processed to generate appropriately-formatted training data for the SRILM toolkit, aswell as the vocabulary and observation files to beused during testing.The pre-processing includes building a dictionarybased on the training data.
All words containinguppercase letters are converted to lowercase beforebeing written to the dictionary.
Words that occurrarely are excluded from the dictionary and are in-stead mapped to a single out-of-vocabulary word.This is based on the idea from (Ratnaparkhi, 1996)that rare words in the training set are similar to un-known words in the test set, and can be used to learnhow to tag the unknown words that will be encoun-tered during testing.
In this work, rare words arethose that occurr fewer than 5 times.
The dictio-nary also includes special begin-sentence and end-sentence words, as well as punctuation marks, re-sulting in a total of 10,824 words.
A list of the 45tags found in the training data is also created, andis similarly augmented with special begin-sentenceand end-sentence tags, for a total of 47 distinct tags.Each word has associated with it a set of features.During training, these features are used to learn asmoothed back-off model for P (si|wi, fi) (where fiis a vector of features associated with word wi).The following five binary flags, taken from(Toutanova et al, 2003), are derived from the cur-rent word wi and used as features :?
is-capitalized (refers to the first letter only);?
has-digits (word contains one or more digits);?
is-hyphenated (word contains ?-?);?
is-all-caps (all letters are capitalized);?
is-conjunction (true if is-all-caps, has-digits,and is-hyphenated are all true, for exampleCFC-12 or F/A-18).Prefixes and suffixes are also known to be infor-mative and so we add a prefix-feature and a suffix-463feature to our set.
Previous work used all possibleprefixes and suffixes ranging in length from 1 to kcharacters, with k = 4 (Ratnaparkhi, 1996), andk = 10 (Toutanova et al, 2003).
This method re-sults in very long lists of thousands of suffixes andprefixes.
In this work, we instead analyzed the rarewords in the training data to generate shorter lists ofinformative prefixes and suffixes, with lengths be-tween 1 and 7 characters.
Each prefix/suffix wasscored based on the number of times it appearedwith a particular tag, and all prefixes/suffixes thatscored above 20 (an arbitrarily chosen threshold)were kept.
This process resulted in two separatelists: one with 377 prefixes, and the other with 704suffixes.
Each word is then assigned a single pre-fix feature and a single suffix feature from theselists (which both include an entry for ?unknown?
).When assigning prefix and suffix features to the rarewords (in the training data) or the unknown words(in the test data), we assume that the longest string isthe most informative.
(This may not necessarily betrue: for example, although the suffix ing is certainlymore informative than g, it is less clear whether ulat-ing would be more or less informative than ing.
)We also include the two adjacent words as fea-tures of the current word.
Our model provides greatflexibility in the choice of features to be includedin the current word?s feature-set.
This feature-set isnot limited to binary flags and indeed can includeanything that can be extracted from the observa-tion sequence in the pre-processing stage.
By usinga smoothed back-off model, issues related to data-sparsity and over-fitting are avoided.4.3 First Order Model ComparisonsIn this section we compare results obtained fromthree first-order models: HMM, CMM, and OCM,using a Na?
?ve Bayes (NB) model as a baseline.
TheNa?
?ve Bayes model is a zeroth-order model with noconnection between adjacent tags, while the first-order models connect adjacent tags in pairs.
(Notethat the HMM in this case is just a ?temporal?
NBsince given the tag, the features are independent.)
Inthese experiments, the only feature used is the is-capitalized flag (the most informative of the binaryflags tested).
The results are shown in Table 2.The conditional probability tables (CPTs) forthe CMM and the OCM were generated using themodel token known-w. unk.-w.type accur.
accur.
accur.Na?
?ve Bayes 90.56% 93.83% 43.4%OCMn=0 90.89% 94.07% 45.2%CMM 93.23% 95.69% 57.9%OCMn=1 93.94% 96.39% 58.6%HMM 94.30% 96.53% 62.3%OCMn=4 94.42% 96.63% 62.7%Table 2: Scores for first order models.factored language model (FLM) extensions to theSRILM toolkit, wth generalized parallel backoffand Witten-Bell smoothing.
(Modified Kneser-Neysmoothing could not be applied because some of therequired low-order meta-counts needed by the dis-count estimator were zero.)
The negative trainingdata for the OCM was generated using the scram-ble method, with values of n as in the table.
Whenno negative training data is used (n = 0), the CPTfor the observed-child shows a very weak depen-dence on the specific tag-pair (si?1, si): the proba-bility values in the tag-bigram model range only be-tween 0.89 and 1.
This weak dependence results inperformance comparable to that of the Na?
?ve Bayesmodel.
That there is any dependence at all is due tothe smoothing since ci = 0 is never observed in thetraining data.
With negative training data (n = 4),there is a much stronger dependence on the tag-pair,and the values for P (ci = 1|si?1, si) range between0.0002 and 1.We found experimentally that the OCM reachedpeak performance with n = 4 and that for larger nthe performance stayed relatively constant: the vari-ation for values of n up to 14 was only 0.05%.4.4 Feature-Rich Second-Order OCMIn this section we describe the results obtained froma more complex second order OCM with the addi-tional word features described in section 4.2.This model is illustrated in Figure 4 which, forclarity highlights the details only for one (tag,word)pair.
The observed-child node, ci, now has three par-ents: the tags si?1, si, and si+1.
Each tag, si, inturn has K + 1 parents: the current word, wi, anda set of K features (shown bundled together).
Themodel switches between the two feature bundles as464model description token known-word unknown-wordaccuracy accuracy accuracyOCM-I, scramble, n = 4 96.39% 96.87% 89.5%OCM-I, computed counts, n = 4 96.41% 96.90% 89.3%OCM-I, computed counts, n = 1 96.41% 96.92% 89.0%OCM-II, computed counts, n = 1 96.64% 97.12% 89.5%OCM-II, as above, on test-set 96.77% 97.25% 90.0%Table 3: Tagging accuracy using the feature-rich 2nd order observed-child model.illustrated, based on the current word.
For knownwords, a small set of features is used, while a muchlarger set of features is used for unknown words.This switching increases the speed of the model atno cost: the additional features increase the taggingaccuracy for unknown words but are redundant forknown words.This model factorizes the joint probability as:P (c, s,o) ?
?iP (ci|si?1, si, si+1)P (si|wi, fi)where fi is the appropriate feature bundle for wordwi, depending on whether wi is known or unknown.cisi-1 si si+1wiknown-word featuresunknown-word featuresFigure 4: Second order OCM with tags connected intriples and switching sets of word features.Two sets of experiments were performed usingtwo models, which we will refer to as OCM-I andOCM-II.
Both of these are second order models(connecting tags in triples), but with different setsof features.
In model OCM-I, the only feature usedfor known words is the is-capitalized flag used insection 4.3.
The unknown words use a total of sevenfeatures: suffix, prefix, and all five of the binary flagsdescribed in section 4.2.
Model OCM-II adds the ad-jacent words (wi?1 and wi+1) to the feature-set forboth known and unknown words.As seen above, the model factorizes the jointprobability into two conditional probability terms.Each of these CPTs is implemented as a smoothed,factored-language back-off model.The observed-child CPT uses generalized back-off, combining at run-time the results of backing offfrom each of the three parents if the specific tag-triple is not found in the table.
The tag CPT useslinear backoff, dropping the adjacent words first.The backoff order for the other features was cho-sen based on experiments to determine the relativeinformation content of each feature.
This resultedin the following backoff order: prefix, has-digit, is-conjunction, is-all-caps, is-hyphenated, suffix, is-capitalized, word (where the least informative fea-ture, prefix, is the first feature to be dropped).Results from these experiments are shown in Ta-ble 3.
Except for the last line, which reports resultson the test set, all results are on the developmentset.
The first three lines show results obtained fromOCM-I (without adjacent word features).
The twomethods of generating negative training data yieldnearly identical results, showing that they are com-parable.
Comparing rows 2 and 3 in the table we seethat the computed-counts method is relatively insen-sitive to the value of n (for n ?
1).OCM-II, which uses the adjacent words as fea-tures for both known and unknown words furtherimproves overall accuracy, and produces state-of-the-art results.
The token-level accuracy result ob-tained from the OCM-II model on the developmentset (96.64%) can be directly compared to an accu-racy of 96.57% reported in (Toutanova et al, 2003)for a cyclic dependency network using similar wordfeatures and the same three tag context.4655 ConclusionsIn this paper, we have introduced two new conceptsto the problem of part-of-speech tagging: virtual evi-dence and negative training data.
We have moreovershown that this new model can produce state-of-the-art results on this NLP task with appropriately cho-sen features.
The model stays entirely within themathematically formal language of Bayesian net-works, and even though it is conditional in nature,the model does not suffer from label or observa-tion (or directional) bias.
Staying within this frame-work has other advantages as well, including thatthe training procedures remain within the relativelysimple maximum likelihood framework, albeit withappropriate smoothing.
We believe that this modelholds great promise for other NLP tasks as well as inother applications of machine-learning such as com-putational biology.
In particular the way it factor-izes the joint probability into a ?horizontal?
com-ponent which connects various nodes to the virtual-evidence node, and a ?vertical?
component (usedhere to link a tag to a set of observations), providesgreat simplicity, flexibility, and power.6 AcknowledgementsThe authors would like to thank the anonymous re-viewers for their constructive comments.
SheilaReynolds is supported by an NDSEG fellowship.ReferencesMichele Banko and Robert C. Moore.
2004.
Part ofSpeech Tagging in Context.
Proceedings of COLING.Jeff Bilmes.
2004.
On Soft Evidence in Bayesian Net-works.
Tech.
Rep. UWEETR-2004-0016, U. Wash-ington Dept.
of Electrical Engineering, 2004.Jeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and generalized parallel backoff.
Pro-ceedings of HLT-NAACL: Short Papers, 4-6.Jeff Bilmes and Geoffrey Zweig.
2002.
The graphi-cal models toolkit: An open source software systemfor speech and time-series processing.
Proceedings ofICASSP, vol4, 3916-3919.Kenneth W. Church and Patrick Hanks.
1989.
Word As-sociation Norms, Mutual Information, and Lexicogra-phy.
Proceedings of ACL, 76-83.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
Proc.
EMNLP.Dan Klein and Christopher D. Manning.
2002.
Condi-tional Structure versus Conditional Estimation in NLPModels.
Proceedings of EMNLP, 9-16.John Lafferty, Andrew McCallum and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
Pro-ceedings of ICML, 282-289.Sang-Zoo Lee, Jun-ichi Tsujii and Hae-Chang Rim.2000.
Part-of-Speech Tagging Based on HiddenMarkov Model Assuming Joint Independence.
Pro-ceedings of 38th ACL, 263-269.Mitchell P. Marcus, Beatrice Santorini and Mary A.Marcinkiewicz.
1994.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19:313-330.Andrew McCallum.
2003.
Efficiently Inducing Featuresof Conditional Random Fields.
Proceedings of UAI.Andrew McCallum, Dayne Freitag and Fernando Pereira.2000.
Maximum-Entropy Markov Models for Infor-mation Extraction and Segmentation.
Proc.
17th In-ternational Conf.
on Machine Learning, 591-598.Judea Pearl.
1988.
Probabilistic Reasoning in Intelli-gent Systems: Networks of Plausible Inference.
Mor-gan Kaufmann.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
EMNLP 1, 133-142.Noah A. Smith and Jason Eisner 2005.
Contrastive Es-timation: Training Log-Linear Models on UnlabeledData.
Proceedings of ACL.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
Proc.
ICASSP, vol 2, 901-904.Scott M. Thede and Mary P. Harper.
1999.
A Second-Order Hidden Markov Model for Part-of-Speech Tag-ging.
Proceedings of 37th ACL, 175-182.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.Proceedings of HLT-NAACL, 252-259.466
