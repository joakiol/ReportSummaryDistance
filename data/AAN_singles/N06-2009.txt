Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?36,New York, June 2006. c?2006 Association for Computational LinguisticsAnswering the Question You Wish They Had Asked:The Impact of Paraphrasing for Question AnsweringPablo Ariel DuboueIBM T.J. Watson Research Center19 Skyline DriveHawthorne, NY 10532, USAduboue@us.ibm.comJennifer Chu-CarrollIBM T.J. Watson Research Center19 Skyline DriveHawthorne, NY 10532, USAjencc@us.ibm.comAbstractState-of-the-art Question Answering (QA)systems are very sensitive to variationsin the phrasing of an information need.Finding the preferred language for sucha need is a valuable task.
We investi-gate that claim by adopting a simple MT-based paraphrasing technique and evalu-ating QA system performance on para-phrased questions.
We found a potentialincrease of 35% in MRR with respect tothe original question.1 IntroductionIn a typical Question Answering system, an inputquestion is analyzed to formulate a query to re-trieve relevant documents from a target corpus (Chu-Carroll et al, 2006; Harabagiu et al, 2006; Sunet al, 2006).
This analysis of the input questionaffects the subset of documents that will be exam-ined and ultimately plays a key role in determiningthe answers the system chooses to produce.
How-ever, most existing QA systems, whether they adoptknowledge-based, statistical, or hybrid methods, arevery sensitive to small variations in the questionform, often yielding substantially different answersfor questions that are semantically equivalent.
Forexample, our system?s answer to ?Who invented thetelephone??
is ?Alexander Graham Bell;?
how-ever, its top answer to a paraphrase of the abovequestion ?Who is credited with the invention of thetelephone??
is ?Gutenberg,?
who is credited withthe invention of the printing press, while ?AlexanderGraham Bell,?
who is credited with the invention ofthe telephone, appears in rank four.To demonstrate the ubiquity of this phenomenon,we asked the aforementioned two questions to sev-eral QA systems on the web, including LCC?s Pow-erAnswer system,1 MIT?s START system,2 Answer-Bus,3 and Ask Jeeves.4 All systems exhibited dif-ferent behavior for the two phrasings of the ques-tion, ranging from minor variations in documentspresented to justify an answer, to major differencessuch as the presence of correct answers in the answerlist.
For some systems, the more complex questionform posed sufficient difficulty that they chose notto answer it.In this paper we focus on investigating a high riskbut potentially high payoff approach, that of improv-ing system performance by replacing the user ques-tion with a paraphrased version of it.
To obtain can-didate paraphrases, we adopt a simple yet powerfultechnique based on machine translation, which wedescribe in the next section.
Our experimental re-sults show that we can potentially achieve a 35% rel-ative improvement in system performance if we havean oracle that always picks the optimal paraphrasefor each question.
Our ultimate goal is to automat-ically select from the set of candidates a high po-tential paraphrase using a component trained againstthe QA system.
In Section 3, we present our ini-tial approach to paraphrase selection which showsthat, despite the tremendous odds against selectingperformance-improving paraphrases, our conserva-tive selection algorithm resulted in marginal im-provement in system performance.1http://www.languagecomputer.com/demos2http://start.csail.mit.edu3http://www.answerbus.com4http://www.ask.com33(A)What toxins are mosthazardous to expectantmothers?en?it Che tossine sono pi?
peri-colose alle donne incinte?it?enWhich toxins are moredangerous to the preg-nant women?
(B)Find out about India?snuclear weapons pro-gram.en?esDescubra sobre el pro-grama de las armas nu-cleares de la India.es?enDiscover on the programof the nuclear weaponsof India.Figure 1: Example of lexical and syntactical paraphrases via MT-paraphrasing using Babelfish.2 MT-Based Automatic ParaphrasingTo measure the impact of paraphrases on QA sys-tems, we seek to adopt a methodology by whichparaphrases can be automatically generated from auser question.
Inspired by the use of parallel trans-lations to mine paraphrasing lexicons (Barzilay andMcKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we lever-age existing machine translation systems to generatesemantically equivalent, albeit lexically and syntac-tically distinct, questions.Figure 1 (A) illustrates how MT-based paraphras-ing captures lexical paraphrasing, ranging from ob-taining simple synonyms such as hazardous anddangerous to deriving more complex equivalentphrases such as expectant mother and pregnantwoman.
In addition to lexical paraphrasing, sometwo-way translations achieve structural paraphras-ing, as illustrated by the example in Figure 1 (B).Using multiple MT engines can help paraphrasediversity.
For example, in Figure 1 (B), if we use the@promt translator5 for English-to-Spanish transla-tion and Babelfish6 for Spanish-to-English transla-tion, we get ?Find out on the nuclear armamentprogram of India?
where both lexical and struc-tural paraphrasings are observed.The motivation of generating an array of lexicallyand structurally distinct paraphrases is that some ofthese paraphrases may better match the processingcapabilities of the underlying QA system than theoriginal question and are thus more likely to pro-duce correct answers.
Our observation is that whilethe paraphrase set contains valuable performance-improving phrasings, it also includes a large num-ber of ungrammatical sentences which need to be fil-5http://www.online-translator.com6http://babelfish.altavista.comQ&ASystemQuestionAnswer ListParaphraseSelectionFeatureExtractorparaphrase...paraphraseparaphraseparaphrase...paraphraseparaphraseparaphraseMTParaphraserFigure 2: System Architecture.tered out to reduce negative impact on performance.3 Using Automatic Paraphrasing inQuestion AnsweringWe use a generic architecture (Figure 2) that treatsa QA system as a black box that is invoked after aparaphrase generation module, a feature extractionmodule, and a paraphrase selection module are exe-cuted.
The preprocessing modules identifies a para-phrase of the original question, which could be thequestion itself, to send as input to the QA system.A key advantage of treating the core QA system asa black box is that the preprocessing modules canbe easily applied to improve the performance of anyQA system.7We described the paraphrase generation modulein the previous section and will discuss the remain-ing two modules below.Feature Extraction Module.
For each possibleparaphrase, we compare it against the original ques-tion and compute the features shown in Table 1.These are a subset of the features that we have ex-perimented with and have found to be meaningfulfor the task.
All of these features are required in or-7In our earlier experiments, we adopted an approach thatcombines answers to all paraphrases through voting.
These ex-periments proved unsuccessful: in most cases, the answer to theoriginal question was amplified, both when right and wrong.34Feature Description IntuitionSumIDFThe sum of the IDF scores for all terms inthe original question and the paraphrase.Paraphrases with more informative terms forthe corpus at hand should be preferred.Lengths Number of query terms for each of the para-phrase and the original question.We expect QA systems to prefer shorter para-phrases.CosineDistanceThe distance between the vectors of bothquestions, IDF-weighted.Certain paraphrases diverge too much from theoriginal.AnswerTypesWhether answer types, as predicted by ourquestion analyzer, are the same or overlap.Choosing a paraphrase that does not share ananswer type with the original question is risky.Table 1: Our features, computed for each paraphrase by comparing it against the original question.der not to lower the performance with respect to theoriginal question.
They are ordered by their relativecontributions to the error rate reduction.Paraphrase Selection Module.
To select a para-phrase, we used JRip, the Java re-implementation ofripper (Cohen, 1996), a supervised rule learner inthe Weka toolkit (Witten and Frank, 2000).We initially formulated paraphrase selection as athree-way classification problem, with an attempt tolabel each paraphrase as being ?worse,?
the ?same,?or ?better?
than the original question.
Our objectivewas to replace the original question with a para-phrase labeled ?better.?
However, the priors forthese classes are roughly 30% for ?worse,?
65% for?same,?
and 5% for ?better?.
Our empirical evi-dence shows that successfully pinpointing a ?better?paraphrase improves, on average, the reciprocal rankfor a question by 0.5, while erroneously picking a?worse?
paraphrase results in a 0.75 decrease.
Thatis to say, errors are 1.5 times more costly than suc-cesses (and five times more likely).
This scenariostrongly suggests that a high precision algorithm iscritical for this component to be effective.To increase precision, we took two steps.
First,we trained a cascade of two binary classifiers.
Thefirst one classifies ?worse?
versus ?same or better,?with a bias for ?worse.?
The second classifier hasclasses ?worse or same?
versus ?better,?
now with abias towards ?better.?
The second step is to constrainthe confidence of the classifier and only accept para-phrases where the second classifier has a 100% con-fidence.
These steps are necessary to avoid decreas-ing performance with respect to the original ques-tion, as we will show in the next section.4 Experimental ResultsWe trained the paraphrase selection module us-ing our QA system, PIQUANT (Chu-Carroll et al,2006).
Our target corpus is the AQUAINT corpus,employed in the TREC QA track since 2002.As for MT engines, we employed Babelfishand Google MT,8 rule-based systems developed bySYSTRAN and Google, respectively.
We adopteddifferent MT engines based on the hypothesis thatdifferences in their translation rules will improve theeffectiveness of the paraphrasing module.To measure performance, we trained and tested bycross-validation over 712 questions from the TREC9 and 10 datasets.
We paraphrased the questions us-ing the four possible combinations of MT engineswith up to 11 intermediate languages, obtaining atotal of 15,802 paraphrases.
These questions werethen fed to our system and evaluated per TREC an-swer key.
We obtained a baseline MRR (top fiveanswers) of 0.345 running over the original ques-tions.
An oracle run, in which the best paraphrase(or the original question) is always picked wouldyield a MRR of 0.48.
This potential increase is sub-stantial, taking into account that a 35% improve-ment separated the tenth participant from the sec-ond in TREC-9.
Our three-fold cross validation us-ing the features and algorithm described in Section 3yielded a MRR of 0.347.
Over 712 questions, it re-placed 14, two of which improved performance, therest stayed the same.
On the other hand, randomselection of paraphrases decreased performance to0.156, clearly showing the importance of selecting agood paraphrase.8http://translate.google.com355 Related WorkMost of the work in QA and paraphrasing focusedon folding paraphrasing knowledge into the questionanalyzer or the answer locator (Rinaldi et al, 2003;Tomuro, 2003).
Our work, on the contrary, focuseson question paraphrasing as an external component,independent of the QA system architecture.Some authors (Dumais et al, 2002; Echihabi etal., 2004) considered the query sent to a search en-gine as a ?paraphrase?
of the original natural lan-guage question.
For instance, Echihabi et al (2004)presented a large number of ?reformulations?
thattransformed the query into assertions that couldmatch the answers in text.
Here we understand aquestion paraphrase as a reformulation that is itselfa question, not a search engine query.Other efforts in using paraphrasing for QA(Duclaye et al, 2003) focused on using the Webto obtain different verbalizations for a seed relation(e.g., Author/Book); however, they have yet to applytheir learned paraphrases to QA.Recently, there has been work on identifying para-phrases equivalence classes for log analysis (Hed-strom, 2005).
Hedstrom used a vector model fromInformation Retrieval that inspired our cosine mea-sure feature described in Section 3.6 ConclusionsThe work presented here makes contributions atthree different levels.
First, we have shown that po-tential impact of paraphrasing with respect to QAperformance is significant.
Replacing a questionwith a more felicitously worded question can poten-tially result in a 35% performance increase.Second, we performed our experiments by tap-ping into a readily available paraphrase resource:MT engines.
Our results speak of the usefulness ofthe approach in producing paraphrases.
This tech-nique of obtaining a large, although low quality,set of paraphrases can be easily employed by otherNLP practitioners wishing to investigate the impactof paraphrasing on their own problems.Third, we have shown that the task of selecting abetter phrasing is amenable to learning, though morework is required to achieve its full potential.
In thatrespect, the features and architecture discussed inSection 3 are a necessary first step in that direction.In future work, we are interested in developingeffective filtering techniques to reduce our candidateset to a small number of high precision paraphrases,in experimenting with state-of-the-art paraphrasers,and in using paraphrasing to improve the stability ofthe QA system.AcknowledgmentsThe authors would like to thank Nelson Correa andAnnie Ying for helpful discussions and comments.This work was supported in part by the DisruptiveTechnology Office (DTO)?s Advanced Question An-swering for Intelligence (AQUAINT) Program un-der contract number H98230-04-C-1577.ReferencesRegina Barzilay and Kathleen R. McKeown.
2001.
Extracting paraphrases from aparallel corpus.
In Proceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL-EACL 2001), Toulouse, France, July.Jennifer Chu-Carroll, Pablo A. Duboue, John M. Prager, and Krzysztof Czuba.2006.
IBM?s piquant II in TREC 2005.
In E. M. Voorhees and Lori P. Buck-land, editors, Proceedings of the Fourthteen Text REtrieval Conference Pro-ceedings (TREC 2005), Gaithersburg, MD, USA.William Cohen.
1996.
Learning trees and rules with set-valued features.
InProceedings of the 14th joint American Association for Artificial Intelligenceand IAAI Conference (AAAI/IAAI-96), pages 709?716.
American Associationfor Artificial Intelligence.Mona Diab.
2000.
An unsupervised method for word sense tagging using parallelcorpora: A preliminary investigation.
In Special Interest Group in LexicalSemantics (SIGLEX) Workshop, Association for Computational Linguistics,Hong Kong, China, October.Florence Duclaye, Francois Yvon, and Olivier Collin.
2003.
Learning para-phrases to improve a question-answering system.
In EACL 2003, 11th Con-ference of the European Chapter of the Association for Computational Lin-guistics, Workshop in NLP for QA, Budapest, Hungary, April.S.
Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002.
Web question answering:is more always better?
In Proc.
SIGIR ?02, pages 291?298, New York, NY,USA.
ACM Press.A.
Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran.2004.
Multiple-engine question answering in textmap.
In Proc.
TREC 2003.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.
2006.Employing two question answering systems.
In Proc.
TREC 2005.Anna Hedstrom.
2005.
Question categorization for a question answering systemusing a vector space model.
Master?s thesis, Department of Linguistics andPhilology (Language Technology Programme) Uppsala University, Uppsala,Sweden.Fabio Rinaldi, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Moll?.2003.
Exploiting paraphrases in a question answering system.
In Proceedingsof the Second International Workshop on Paraphrasing, pages 25?32, July.R.
Sun, J. Jiang, Y.F.
Tan, H. Cui, T.-S. Chua, and M.-Y.
Kan. 2006.
Usingsyntactic and semantic relation analysis in question answering.
In Proc.
TREC2005.Noriko Tomuro.
2003.
Interrogative reformulation patterns and acquisition ofquestion paraphrases.
In Proceedings of the Second International Workshopon Paraphrasing, pages 33?40, July.Ian H. Witten and Eibe Frank.
2000.
Data Mining: Practical Machine LearningTools and Techniques with Java Implementations.
Morgan Kaufmann Pub-lishers.36
