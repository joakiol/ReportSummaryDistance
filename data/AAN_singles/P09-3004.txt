Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27?35,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPParaphrase Recognition Using Machine Learning to Combine SimilarityMeasuresProdromos MalakasiotisDepartment of InformaticsAthens University of Economics and BusinessPatission 76, GR-104 34 Athens, GreeceAbstractThis paper presents three methods that canbe used to recognize paraphrases.
Theyall employ string similarity measures ap-plied to shallow abstractions of the inputsentences, and a Maximum Entropy clas-sifier to learn how to combine the result-ing features.
Two of the methods also ex-ploit WordNet to detect synonyms and oneof them also exploits a dependency parser.We experiment on two datasets, the MSRparaphrasing corpus and a dataset that weautomatically created from the MTC cor-pus.
Our system achieves state of the artor better results.1 IntroductionRecognizing or generating semantically equiva-lent phrases is of significant importance in manynatural language applications.
In question answer-ing, for example, a question may be phrased dif-ferently than in a document collection (e.g., ?Whois the author of War and Peace??
vs. ?Leo Tol-stoy is the writer of War and Peace.?
), and takingsuch variations into account can improve systemperformance significantly (Harabagiu et al, 2003;Harabagiu and Hickl, 2006).
A paraphrase gener-ator, meaning a module that produces new phrasesor patterns that are semantically equivalent (or al-most equivalant) to a given input phrase or pattern(e.g., ?X is the writer of Y ?
?
?X wrote Y ?
?
?Y was written by X??
?X is the author of Y ?,or ?X produces Y ?
?
?X manufactures Y ?
?
?X is the manufacturer of Y ?)
can be used to pro-duce alternative phrasings of the question, beforematching it against a document collection.Unlike paraphrase generators, paraphrase rec-ognizers decide whether or not two given phrases(or patterns) are paraphrases, possibly by general-izing over many different training pairs of phrases.Paraphrase recognizers can be embedded in para-phrase generators to filter out erroneous generatedparaphrases; but they are also useful on their own.In question answering, for example, they can beused to check if a pattern extracted from the ques-tion (possibly by replacing named entities by theirsemantic categories and turning the question intoa statement) matches any patterns extracted fromcandidate answers.
As a further example, in textsummarization, especially multi-document sum-marization, a paraphrase recognizer can be usedto check if a sentence is a paraphrase of any othersentence already present in a partially constructedsummary.Note that, although ?paraphrasing?
and ?textualentailment?
are sometimes used as synonyms, weuse the former to refer to methods that generateor recognize semantically equivalent (or almostequivalent) phrases or patterns, whereas in textualentailment (Dagan et al, 2006; Bar-Haim et al,2006; Giampiccolo et al, 2007) the expressions orpatterns are not necessarily semantically equiva-lent; it suffices if one entails the other, even if thereverse direction does not hold.
For example, ?Ywas written by X?
textually entails ?Y is the workof X?, but the reverse direction does not neces-sarily hold (e.g., if Y is a statue); hence, the twosentences are not paraphrases.In this paper, we focus on paraphrase recogni-tion.
We propose three methods that employ stringsimilarity measures, which are applied to severalabstractions of a pair of input phrases (e.g., thephrases themselves, their stems, POS tags).
Thescores returned by the similarity measures are usedas features in a Maximum Entropy (ME) classifier(Jaynes, 1957; Good, 1963), which learns to sepa-rate true paraphrase pairs from false ones.
Two ofour methods also exploit WordNet to detect syn-onyms, and one of them uses additional featuresto measure similarities of grammatical relations27obtained by a dependency parser.1Our experi-ments were conducted on two datasets: the pub-licly available Microsoft Research Paraphrasingcorpus (Dolan et al, 2004) and a dataset that weconstructed from the MTC corpus.2The experi-mental results show that our methods perform verywell.
Even the simplest one manages to achievestate of the art results, even though it uses fewerlinguistic resources than other reported systems.The other two, more elaborate methods performeven better.Section 2 presents the three methods, and sec-tion 3 our experiments.
Section 4 covers relatedwork.
Section 5 concludes and proposes furtherwork.2 The three methodsThe main idea underlying our methods is that bycapturing similarities at various shallow abstrac-tions of the input (e.g., the original sentences, thestems of their words, their POS tags), we can rec-ognize paraphrases and textual entailment reason-ably well, provided that we learn to assign ap-propriate weights to the resulting features.
Fur-ther improvements are possible by recognizingsynonyms and by employing similarity measuresthat operate on the output of dependency grammarparsers.2.1 Method 1 (INIT)During training, the first method, called INIT, isgiven a set {?S1,1, S1,2, y1?
, .
.
.
, ?Sn,1, Sn,2, yn?
},where Si,1and Si,2are sentences (more gener-ally, phrases), yi= 1 (positive class) if thetwo sentences are paraphrases, and yi= ?1(negative class) otherwise.
Each pair of sen-tences ?Si,1, Si,2?
is converted to a feature vec-tor ~vi, whose values are scores returned by sim-ilarity measures that indicate how similar Si,1and Si,2are at various levels of abstraction.The vectors and the corresponding categories{?~v1, yi?
, .
.
.
, ?
~vn, yn?}
are given as input to theME classifier, which learns how to classify newvectors ~v, corresponding to unseen pairs of sen-tences ?S1, S2?.We use nine string similarity measures: Leven-shtein distance (edit distance), Jaro-Winkler dis-tance, Manhattan distance, Euclidean distance, co-1We use Stanford University?s ME classifier and parser;see http://nlp.stanford.edu/.2The corpus is available by the LDC, Catalogue NumberLDC2002T01, ISBN 1-58563-217-1.sine similarity, n-gram distance (with n = 3),matching coefficient, Dice coefficient, and Jac-card coefficient.
To save space, we do not repeatthe definitions of the similarity measures here,since they are readily available in the literatureand they are also summarized in our previous work(Malakasiotis and Androutsopoulos, 2007).For each pair of input strings ?S1, S2?, we formten new pairs of strings?s11, s12?, .
.
.
,?s101, s102?corresponding to ten different levels of abstractionof S1and S2, and we apply the nine similaritymeasures to the ten new pairs, resulting in a to-tal of 90 measurements.
These measurements arethen included as features in the vector ~v that cor-responds to ?S1, S2?.
The?si1, si2?pairs are:?s11, s12?
: two strings consisting of the original tokens of S1and S2, respectively, with the original order of the to-kens maintained;3?s21, s22?
: as in the previous case, but now the tokens arereplaced by their stems;?s31, s32?
: as in the previous case, but now the tokens arereplaced by their part-of-speech (POS) tags;?s41, s42?
: as in the previous case, but now the tokens arereplaced by their soundex codes;4?s51, s52?
: two strings consisting of only the nouns of S1andS2, as identified by a POS-tagger, with the original or-der of the nouns maintained;?s61, s62?
: as in the previous case, but now with nouns re-placed by their stems;?s71, s72?
: as in the previous case, but now with nouns re-placed by their soundex codes;?s81, s82?
: two strings consisting of only the verbs of S1andS2, as identified by a POS-tagger, with the original or-der of the verbs maintained;?s91, s92?
: as in the previous case, but now with verbs re-placed by their stems;?s101, s102?
: as in the previous case, but now with verbs re-placed by their soundex codes.Note that the similarities are measured in termsof tokens, not characters.
For instance, the editdistance of S1and S2is the minimum number ofoperations needed to transform S1to S2, where anoperation is an insertion, deletion or substitutionof a single token.
Moreover, we use high-level3We use Stanford University?s tokenizer and POS-tagger,and Porter?s stemmer.4Soundex is an algorithm intended to map English namesto alphanumeric codes, so that names with the same pronun-ciations receive the same codes, despite spelling differences;see http://en.wikipedia.org/wiki/Soundex.28POS tags only, i.e., we do not consider the num-ber of nouns, the voice of verbs etc.
; this increasesthe similarity of positive?s31, s32?pairs.A common problem is that the string similar-ity measures may be misled by differences in thelengths of S1and S2.
This is illustrated in the fol-lowing examples, where the underlined part of S1is much more similar to S2than the entire S1.S1: While Bolton apparently fell and was immobilized,Selenski used the mattress to scale a 10-foot, razor-wirefence, Fischi said.S2: After the other inmate fell, Selenski used the mattressto scale a 10-foot, razor-wire fence, Fischi said.To address this problem, when we consider apair of strings ?s1, s2?, if s1is longer than s2, weobtain all of the substrings s?1of s1that have thesame length as s2.
Then, for each s?1, we computethe nine values fj(s?1, s2), where fj(1 ?
j ?
9)are the string similarity measures.
Finally, we lo-cate the s?1with the best average similarity (overall similarity measures) to s2, namely s??1:s?
?1= argmaxs?110?j=1fj(s?1, s2)and we keep the nine fj(s?
?1, s2) values and theiraverage as ten additional measurements.
Simi-larly, if s2is longer than s1, we keep the ninefj(s1, s?
?2) values and their average.
This processis applied to pairs?s11, s12?, .
.
.
,?s41, s42?, wherelarge length differences are more likely to appear,adding 40 more measurements (features) to thevector ~v of each ?S1, S2?
pair of input strings.The measurements discussed above provide 130numeric features.5To those, we add two Booleanfeatures indicating the existence or absence ofnegation in S1or S2, respectively; negation is de-tected by looking for words like ?not?, ?won?t?etc.
Finally, we add a length ratio feature, de-fined asmin(LS1,LS2)max(LS1,LS2), where LS1and LS2are thelengths, in tokens, of S1and S2.
Hence, there is atotal of 133 available features in INIT.2.2 Method 2 (INIT+WN)Paraphrasing may involve using synonyms whichcannot be detected by the features we have con-sidered so far.
In the following pair of sentences,for example, ?dispatched?
is used as a synonym5All feature values are normalized in [?1, 1].
We use ourown implementation of the string similarity measures.of ?sent?
; treating the two verbs as the same to-ken during the calculation of the string similaritymeasures would yield a higher similarity.
The sec-ond method, called INIT+WN, treats words fromS1and S2that are synonyms as identical; other-wise the method is the same as INIT.S1: Fewer than a dozen FBI agents were dispatched to se-cure and analyze evidence.S2: Fewer than a dozen FBI agents will be sent to Iraq tosecure and analyze evidence of the bombing.2.3 Method 3 (INIT+WN+DEP)The features of the previous two methods op-erate at the lexical level.
The third method,called INIT+WN+DEP, adds features that operateon the grammatical relations (dependencies) a de-pendency grammar parser returns for S1and S2.We use three measures to calculate similarity atthe level of grammatical relations, namely S1de-pendency recall (R1), S2dependency recall (R2)and their F -measure (FR1,R2), defined below:R1=|common dependencies||S1dependencies|R2=|common dependencies||S2dependencies|FR1,R2=2?R1?R2R1+R2The following two examples illustrate the use-fulness of dependency similarity measures in de-tecting paraphrases.
In the first example S1and S2are not paraphrases and the scores are low, while inthe second example where S1and S2have almostidentical meanings, the scores are much higher.Figures 1 and 2 lists the grammatical relations (de-pendencies) of the two sentences with the commonones shown in bold.Example 1:S1: Gyorgy Heizler, head of the local disaster unit, said thecoach was carrying 38 passengers.S2: The head of the local disaster unit, Gyorgy Heizler, saidthe coach driver had failed to heed red stop lights.R1= 0.43, R2= 0.32, FR1,R2= 0.36Example 2:S1: Amrozi accused his brother, whom he called ?the wit-ness?, of deliberately distorting his evidence.S2: Referring to him as only ?the witness?, Amrozi accusedhis brother of deliberately distorting his evidence.R1= 0.69, R2= 0.6, FR1,R2= 0.6429Grammatical relations of S1 Grammatical relations of S2mod(Heizler-2, Gyorgy-1) mod(head-2, The-1)arg(said-11, Heizler-2) arg(said-12, head-2)mod(head-2, of-3) mod(Heizler-2, head-4)mod(head-4, of-5) mod(unit-7, the-4)mod(unit-9, the-6) mod(unit-7, local-5)mod(unit-9, local-7) mod(unit-7, disaster-6)mod(unit-9, disaster-8) arg(of-3, unit-7)arg(of-5, unit-9) mod(Heizler-10, Gyorgy-9)mod(coach-13, the-12) mod(unit-7, Heizler-10)arg(carrying-15, coach-13) mod(driver-15, the-13)aux(carrying-15, was-14) mod(driver-15, coach-14)arg(said-11, carrying-15) arg(failed-17, driver-15)mod(passengers-17, 38-16) aux(failed-17, had-16)arg(said-12, failed-17) arg(carrying-15, passengers-17)aux(heed-19, to-18)arg(failed-17, heed-19)mod(lights-22, red-20)mod(lights-22, stop-21)arg(heed-19, lights-22)Figure 1: Grammatical relations of example 1.Grammatical relations of S1 Grammatical relations of S2arg(accused-2, Amrozi-1) dep(accused-12, Referring-1)mod(brother-4, his-3) mod(Referring-1, to-2)arg(accused-2, brother-4) arg(to-2, him-3)arg(called-8, whom-6) cc(him-3, as-4)arg(called-8, he-7) dep(as-4, only-5)mod(witness-8, the-7) mod(brother-4, called-8)mod(witness-11, the-10) conj(him-3, witness-8)arg(accused-12, Amrozi-11) dep(called-8, witness-11)mod(brother-4, of-14) mod(brother-14, his-13)mod(distorting-16, deliberately-15) arg(accused-12, brother-14)arg(of-14, distorting-16) mod(brother-14, of-15)mod(evidence-18, his-17) mod(distorting-17, deliberately-16)arg(distorting-16, evidence-18) arg(of-15, distorting-17)mod(evidence-19, his-18)arg(distorting-17, evidence-19)Figure 2: Grammatical relations of example 2.30As with POS-tags, we use only the highest levelof the tags of the grammatical relations, which in-creases the similarity of positive pairs of S1andS2.
For the same reason, we ignore the direction-ality of the dependency arcs which we have foundto improve the results.
INIT+WN+DEP employs atotal of 136 features.2.4 Feature selectionLarger feature sets do not necessarily lead to im-proved classification performance.
Despite seem-ing useful, some features may in fact be too noisyor irrelevant, increasing the risk of overfitting thetraining data.
Some features may also be redun-dant, given other features; thus, feature selectionmethods that consider the value of each feature onits own (e.g., information gain) may lead to sub-optimal feature sets.Finding the best subset of a set of available fea-tures is a search space problem for which severalmethods have been proposed (Guyon et al, 2006).We have experimented with a wrapper approach,whereby each feature subset is evaluated accord-ing to the predictive power of a classifier (treatedas a black box) that uses the subset; in our exper-iments, the predictive power was measured as F -measure (defined below, not to be confused withFR1,R2).
More precisely, during feature selection,for each feature subset we performed 10-fold crossvalidation on the training data to evaluate its pre-dictive power.
After feature selection, the classi-fier was trained on all the training data, and it wasevaluated on separate test data.With large feature sets, an exhaustive searchover all subsets is intractable.
Instead, we ex-perimented with forward hill-climbing and beamsearch (Guyon et al, 2006).
Forward hill-climbingstarts with an empty feature set, to which it addsfeatures, one at a time, by preferring to add at eachstep the feature that leads to the highest predic-tive power.
Forward beam search is similar, exceptthat the search frontier contains the k best exam-ined states (feature subsets) at each time; we usedk = 10.
For k = 1, beam search reduces to hill-climbing.3 ExperimentsWe now present our experiments, starting from adescription of the datasets used.3.1 DatasetsWe mainly used the Microsoft Research (MSR)Paraphrasing Corpus (Dolan et al, 2004), whichconsists of 5,801 pairs of sentences.
Each pairis manually annotated by two human judges as atrue or false paraphrase; a third judge resolved dis-agreements.
The data are split into 4,076 trainingpairs and 1,725 testing pairs.We have experimented with a dataset we createdfrom the MTC corpus.
MTC is a corpus containingnews articles in Mandarin Chinese; for each article11 English translations (by different translators)are also provided.
We considered the translationsof the same Chinese sentence as paraphrases.
Weobtained all the possible paraphrase pairs and weadded an equal number of randomly selected nonparaphrase pairs, which contained sentences thatwere not translations of the same sentence.
In thisway, we constructed a dataset containing 82,260pairs of sentences.
The dataset was then split intraining (70%) and test (30%) parts, with an equalnumber of positive and negative pairs in each part.3.2 Evaluation measures and baselineWe used four evaluation measures, namely accu-racy (correctly classified pairs over all pairs), pre-cision (P , pairs correctly classified in the positiveclass over all pairs classified in the positive class),recall (R, pairs correctly classified in the positiveclass over all true positive pairs), and F -measure(with equal weight on precision and recall, definedas2?P ?RP+R).
These measures are not to be confusedwith the R1, R2, and FR1,R2of section 2.3 whichare used as features.A reasonable baseline method (BASE) is to usejust the edit distance similarity measure and athreshold in order to decide whether two phrasesare paraphrases or not.
The threshold is chosenusing a grid search utility and 10-fold cross vali-dation on the training data.
More precisely, in afirst step we search the range [-1, 1] with a stepof 0.1.6In each step, we perform 10-fold crossvalidation and the value that achieves the best F -measure is our initial threshold, th, for the secondstep.
In the second step, we perform the same pro-cedure in the range [th - 0.1, th + 0.1] and with astep of 0.001.6Recall that we normalize similarity in [-1, 1].313.3 Experimental resultsWith both datasets, we experimented with a Max-imum Entropy (ME) classifier.
However, prelim-inary results (see table 1) showed that our MTCdataset is very easy.
BASE achieves approximately95% in accuracy and F -measure, and an approx-imate performance of 99.5% in all measures (ac-curacy, precision, recall, F -measure) is achievedby using ME and only some of the features ofINIT (we use 36 features corresponding to pairs?s11, s12?,?s21, s22?,?s31, s32?,?s41, s42?plus the twonegation features).
Therefore, we did not experi-ment with the MTC dataset any further.Table 2 (upper part) lists the results of our ex-periments on the MSR corpus.
We optionally per-formed feature selection with both forward hill-climbing (FHC) and forward beam search (FBS).All of our methods clearly perform better thanBASE.
As one might expect, there is a lot of re-dundancy in the complete feature set.
Hence, thetwo feature selection methods (FHC and FBS) leadto competitive results with much fewer features (7and 10, respectively, instead of 136).
However,feature selection deteriorates performance, espe-cially accuracy, i.e., the full feature set is better,despite its redundancy.
Table 2 also includes allother reported results for the MSR corpus that weare aware of; we are not aware of the exact numberof features used by the other researchers.It is noteworthy that INIT achieves state of theart performance, even though the other approachesuse many more linguistic resources.
For example,Wan et al?s approach (Wan et al, 2006), whichachieved the best previously reported results, issimilar to ours, in that it also trains a classifier withsimilarity measures; but some of Wan et al?s mea-sures require a dependency grammar parser, unlikeINIT.
More precisely, for each pair of sentences,Wan et al construct a feature vector with valuesthat measure lexical and dependency similarities.The measures are: word overlap, length difference(in words), BLEU (Papineni et al, 2002), depen-dency relation overlap (i.e., R1and R2but notFR1,R2), and dependency tree edit distance.
Themeasures are also applied on sequences containingthe lemmatized words of the original sentences,similarly to one of our levels of abstraction.
Inter-estingly, INIT achieves the same (and slightly bet-ter) accuracy as Wan et al?s system, without em-ploying any parsing.
Our more enhanced methods,INIT+WN and INIT+WN+DEP, achieve even betterresults.Zhang and Patrick (2005) use a dependencygrammar parser to convert passive voice phrasesto active voice ones.
They also use a preprocess-ing stage to generalize the pairs of sentences.
Thepreprocessing replaces dates, times, percentages,etc.
with generic tags, something that we have alsodone in the MSR corpus, but it also replaces wordsand phrases indicating future actions (e.g., ?plansto?, ?be expected to?)
with the word ?will?
; thelatter is an example of further preprocessing thatcould be added to our system.
After the prepro-cessing, Zhang and Patrick create for each sen-tence pair a feature vector whose values measurethe lexical similarity between the two sentences;they appear to be using the maximum number ofconsecutive common words, the number of com-mon words, edit distance (in words), and modi-fied n-gram precision, a measure similar to BLEU.The produced vectors are then used to train a de-cision tree classifier.
Hence, Zhang and Patrick?sapproach is similar to ours, but we use more anddifferent similarity measures and several levels ofabstraction of the two sentences.
We also use ME,along with a wrapper approach to feature selec-tion, rather than decision tree induction and its em-bedded information gain-based feature selection.Furthermore, all of our methods, even INIT whichemploys no parsing at all, achieve better resultscompared to Zhang and Patrick?s.Qiu et al (2006) first convert the sentences intotuples using parsing and semantic role labeling.They then match similar tuples across the two sen-tences, and use an SVM (Vapnik, 1998) classifier todecide whether or not the tuples that have not beenmatched are important or not.
If not, the sentencesare paraphrases.
Despite using a parser and a se-mantic role identifier, Qiu et al?s system performsworse than our methods.Finally, Finch et al?s system (2005) achievedthe second best overall results by employing POStagging, synonymy resolution, and an SVM.
In-terestingly, the features of the SVM correspondto machine translation evaluation metrics, ratherthan string similarity measures, unlike our system.We plan to examine further how the features ofFinch et al and other ideas from machine trans-lation can be embedded in our system, althoughINIT+WN+DEP outperforms Finch et al?s system.Interestingly, even when not using more resourcesthan Finch et al as in methods INIT and INIT+WN32method features accuracy precision recall F -measureBASE ?
95.30 98.16 92.32 95.15INIT?
38 99.62 99.50 99.75 99.62Table 1: Results (%) of our methods on our MTC dataset.method features accuracy precision recall F -measureBASE 1 69.04 72.42 86.31 78.76INIT 133 75.19 78.51 86.31 82.23INIT+WN 133 75.48 78.91 86.14 82.37INIT+WN+DEP 136 76.17 79.35 86.75 82.88INIT+WN+DEP + FHC 7 73.86 75.14 90.67 82.18INIT+WN+DEP + FBS 10 73.68 73.68 93.98 82.61Finch et al ?
74.96 76.58 89.80 82.66Qiu et al ?
72.00 72.50 93.40 81.60Wan et al ?
75.00 77.00 90.00 83.00Zhang & Patrick ?
71.90 74.30 88.20 80.70Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus.we achieve similar or better accuracy results.4 Related workWe have already made the distinction betweenparaphrase (and textual entailment) generators vs.recognizers, and we have pointed out that rec-ognizers can be embedded in generators as fil-ters.
The latter is particularly useful in bootstrap-ping paraphrase generation approaches (Riloffand Jones, 1999; Barzilay and McKeown, 2001;Ravichandran and Hovy, 2001; Ravichandran etal., 2003; Duclaye et al, 2003; Szpektor et al,2004), which are typically given seed pairs ofnamed entities for which a particular relationholds; the system locates in a document collec-tion (or the entire Web) contexts were the seedscooccur, and uses the contexts as patterns that canexpress the relation; the patterns are then used tolocate new named entities that satisfy the relation,and a new iteration begins.
A paraphrase recog-nizer could be used to filter out erroneous gener-ated paraphrases between iterations.Another well known paraphrase generator is Linand Pantel?s (2001) DIRT, which produces slottedsemantically equivalent patterns (e.g., ?X is thewriter of Y ?
?
?X wrote Y ?
?
?Y was writ-ten by X?
?
?X is the author of Y ?
), basedon the assumption that different paths of depen-dency trees (obtained from a corpus) that occurfrequently with the same words (slot fillers) attheir ends are often paraphrases.
An extension ofDIRT, named LEDIR, has also been proposed (Bha-gat et al, 2007) to recognize directional textualentailment rules (e.g., ?Y was written by X?
?
?Y is the work of X?).
Ibrahim et al?s (2003)method is similar to DIRT, but it uses only de-pendency grammar paths from aligned sentences(from a parallel corpus) that share compatible an-chors (e.g., identical strings, or entity names of thesame semantic category).
Shinyama and Sekine(2003) adopt a very similar approach.In another generation approach, Barzilay andLee (2002; 2003) look for pairs of slotted wordlattices that share many common slot fillers; thelattices are generated by applying a multiple-sequence alignment algorithm to a corpus of mul-tiple news articles about the same events.
Finally,Pang et al (2003) create finite state automata bymerging parse trees of aligned sentences from aparallel corpus; in each automaton, different pathsrepresent paraphrases.
Again, a paraphrase recog-nizer could be embedded in all of these methods,to filter out erroneous generated patterns.5 Conclusions and further workWe have presented three methods (INIT, INIT+WN,INIT+WN+DEP) that recognize paraphrases givenpairs of sentences.
These methods employ ninestring similarity measures applied to ten shallowabstractions of the input sentences.
Moreover,INIT+WN and INIT+WN+DEP exploit WordNet forsynonymy resolution, and INIT+WN+DEP uses ad-ditional features that measure grammatical rela-tion similarity.
Supervised machine learning isused to learn how to combine the resulting fea-tures.
We experimented with a Maximum Entropyclassifier on two datasets; the publicly availableMSR corpus and one that we constructed from the33MTC corpus.
However, the latter was found to bevery easy, and consequently we mainly focused onthe MSR corpus.On the MSR corpus, all of our methods achievedsimilar or better performance than the sate of theart, even INIT, despite the fact that it uses fewerlinguistic resources.
Hence, INIT may have prac-tical advantages in less spoken languages, whichhave limited resources.
The most elaborate ofour methods, INIT+WN+DEP, achieved the best re-sults, but it requires WordNet and a reliable depen-dency grammar parser.
Feature selection experi-ments indicate that there is significant redundancyin our feature set, though the full feature set leadsto better performance than the subsets producedby feature selection.
Further improvements maybe possible by including in our system additionalfeatures, such as BLEU scores or features for wordalignment.Our long-term goal is to embed our recognizerin a bootstrapping paraphrase generator, to filterout erroneous paraphrases between bootstrappingiterations.
We hope that our recognizer will be ad-equate for this purpose, possibly in combinationwith a human in the loop, who will inspect para-phrases the recognizer is uncertain of.AcknowledgementsThis work was funded by the Greek PENED 2003programme, which is co-funded by the EuropeanUnion (80%), and the Greek General Secretariatfor Research and Technology (20%).ReferencesR.
Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-ampiccolo, B. Magnini, and I. Szpektor.
2006.
The2nd PASCAL recognising textual entailment chal-lenge.
In Proceedings of the 2nd PASCAL Chal-lenges Workshop on Recognising Textual Entail-ment, Venice, Italy.R.
Barzilay and L. Lee.
2002.
Bootstrapping lexi-cal choice via multiple-sequence alignment.
In Pro-ceedings of EMNLP, pages 164?171, Philadelphia,PA.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase:an unsupervised approach using multiple-sequencealignment.
In Proceedings of HLT-NAACL, pages16?23, Edmonton, Canada.R.
Barzilay and K. McKeown.
2001.
Extracting para-phrases from a parallel corpus.
In Proceedings ofACL/EACL, pages 50?57, Toulouse, France.R.
Bhagat, P. Pantel, and E. Hovy.
2007.
LEDIR:An unsupervised algorithm for learning directional-ity of inference rules.
In Proceedings of the EMNLP-CONLL, pages 161?170.I.
Dagan, O. Glickman, and B. Magnini.
2006.
ThePASCAL recognising textual entailment challenge.In Qui?nonero-Candela et al, editor, LNAI, volume3904, pages 177?190.
Springer-Verlag.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsu-pervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In Pro-ceedings of COLING, page 350, Morristown, NJ.F.
Duclaye, F. Yvon, and O. Collin.
2003.
Learningparaphrases to improve a question-answering sys-tem.
In Proceedings of the EACL Workshop on Nat-ural Language Processing for Question AnsweringSystems, pages 35?41, Budapest, Hungary.A.
Finch, Y. S. Hwang, and E. Sumita.
2005.
Usingmachine translation evaluation techniques to deter-mine sentence-level semantic equivalence.
In Pro-ceedings of the 3rd International Workshop on Para-phrasing, Jeju Island, Korea.D.
Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.2007.
The third Pascal recognizing textual entail-ment challenge.
In Proceedings of the ACL-PascalWorkshop on Textual Entailment and Paraphrasing,pages 1?9, Prague, Czech Republic.I.
J.
Good.
1963.
Maximum entropy for hypothesisformulation, especially for multidimentional conti-gency tables.
Annals of Mathematical Statistics,34:911?934.I.M.
Guyon, S.R.
Gunn, M. Nikravesh, and L. Zadeh,editors.
2006.
Feature Extraction, Foundations andApplications.
Springer.S.
Harabagiu and A. Hickl.
2006.
Methods for usingtextual entailment in open-domain question answer-ing.
In Proceedings of COLING-ACL, pages 905?912, Sydney, Australia.S.M.
Harabagiu, S.J.
Maiorano, and M.A.
Pasca.2003.
Open-domain textual question answer-ing techniques.
Natural Language Engineering,9(3):231?267.A.
Ibrahim, B. Katz, and J. Lin.
2003.
Extract-ing structural paraphrases from aligned monolingualcorpora.
In Proceedings of the ACL Workshop onParaphrasing, pages 57?64, Sapporo, Japan.E.
T. Jaynes.
1957.
Information theory and statisticalmechanics.
Physical Review, 106:620?630.D.
Lin and P. Pantel.
2001.
Discovery of inferencerules for question answering.
Natural Language En-gineering, 7:343?360.34P.
Malakasiotis and I. Androutsopoulos.
2007.
Learn-ing textual entailment using svms and string similar-ity measures.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing,pages 42?47, Prague, June.
Association for Compu-tational Linguistics.B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-based alignment of multiple translations: extractingparaphrases and generating new sentences.
In Pro-ceedings of HLT-NAACL, pages 102?109, Edmon-ton, Canada.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL, pages 311?318,Philadelphia, Pennsylvania.L.
Qiu, M. Y. Kan, and T.S.
Chua.
2006.
Paraphraserecognition via dissimilarity significance classifica-tion.
In Proceedings of EMNLP, pages 18?26, Syd-ney, Australia.D.
Ravichandran and E. Hovy.
2001.
Learning surfacetext patterns for a question answering system.
InProceedings of ACL, pages 41?47, Philadelphia, PA.D.
Ravichandran, A. Ittycheriah, and S. Roukos.
2003.Automatic derivation of surface text patterns for amaximum entropy based question answering sys-tem.
In Proceedings of HLT-NAACL, pages 85?87,Edmonton, Canada.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of AAAI, pages 474?479, Orlando,FL.Y.
Shinyama and S. Sekine.
2003.
Paraphrase ac-quisition for information extraction.
In Proceed-ings of the ACL Workshop on Paraphrasing, Sap-poro, Japan.I.
Szpektor, H. Tanev, I. Dagan, and B. Coppola.
2004.Scaling Web-based acquisition of entailment rela-tions.
In Proceedings of EMNLP, Barcelona, Spain.V.
Vapnik.
1998.
Statistical learning theory.
JohnWiley.S.
Wan, M. Dras, R. Dale, and C. Paris.
2006.
Us-ing dependency-based features to take the ?para-farce?
out of paraphrase.
In Proceedings of the Aus-tralasian Language Technology Workshop, pages131?138, Sydney, Australia.Y.
Zhang and J. Patrick.
2005.
Paraphrase identifi-cation by text canonicalization.
In Proceedings ofthe Australasian Language Technology Workshop,pages 160?166, Sydney, Australia.35
