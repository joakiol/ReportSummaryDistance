Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 8?12,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsHybrid Selection of Language Model Training Data Using LinguisticInformation and PerplexityAntonio ToralSchool of ComputingDublin City UniversityDublin, Irelandatoral@computing.dcu.ieAbstractWe explore the selection of training datafor language models using perplexity.
Weintroduce three novel models that makeuse of linguistic information and evaluatethem on three different corpora and twolanguages.
In four out of the six scenar-ios a linguistically motivated method out-performs the purely statistical state-of-the-art approach.
Finally, a method whichcombines surface forms and the linguisti-cally motivated methods outperforms thebaseline in all the scenarios, selecting datawhose perplexity is between 3.49% and8.17% (depending on the corpus and lan-guage) lower than that of the baseline.1 IntroductionLanguage models (LMs) are a fundamental piecein statistical applications that produce natural lan-guage text, such as machine translation and speechrecognition.
In order to perform optimally, a LMshould be trained on data from the same domainas the data that it will be applied to.
This poses aproblem, because in the majority of applications,the amount of domain-specific data is limited.A popular strand of research in recent years totackle this problem is that of training data selec-tion.
Given a limited domain-specific corpus anda larger non-domain-specific corpus, the task con-sists on finding suitable data for the specific do-main in the non-domain-specific corpus.
The un-derlying assumption is that a non-domain-specificcorpus, if broad enough, contains sentences sim-ilar to a domain-specific corpus, which therefore,would be useful for training models for that do-main.This paper focuses on the approach that usesperplexity for the selection of training data.
Thefirst works in this regard (Gao et al 2002; Linet al 1997) use the perplexity according to adomain-specific LM to rank the text segments (e.g.sentences) of non-domain-specific corpora.
Thetext segments with perplexity less than a giventhreshold are selected.A more recent method, which can be consid-ered the state-of-the-art, is Moore-Lewis (Mooreand Lewis, 2010).
It considers not only the cross-entropy1 according to the domain-specific LM butalso the cross-entropy according to a LM builton a random subset (equal in size to the domain-specific corpus) of the non-domain-specific cor-pus.
The additional use of a LM from the non-domain-specific corpus allows to select a subsetof the non-domain-specific corpus which is bet-ter (the perplexity of a test set of the specific do-main has lower perplexity on a LM trained onthis subset) and smaller compared to the previ-ous approaches.
The experiment was carried outfor English, using Europarl (Koehn, 2005) as thedomain-specific corpus and LDC Gigaword2 asthe non-domain-specific one.In this paper we study whether the use of twotypes of linguistic knowledge (lemmas and namedentities) can contribute to obtain better resultswithin the perplexity-based approach.2 MethodologyWe explore the use of linguistic information forthe selection of data to train domain-specific LMsfrom non-domain-specific corpora.
Our hypothe-sis is that ranking by perplexity on n-grams thatrepresent linguistic patterns (rather than n-gramsthat represent surface forms) captures additionalinformation, and thus may select valuable data thatis not selected according solely to surface forms.We use two types of linguistic information at1note that using cross-entropy is equivalent to using per-plexity since they are monotonically related.2http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2007T078word level: lemmas and named entity categories.We experiment with the following models:?
Forms (hereafter f), uses surface forms.
Thismodel replicates the Moore-Lewis approachand is to be considered the baseline.?
Forms and named entities (hereafter fn), usessurface forms, with the exception of any worddetected as a named entity, which is substi-tuted by its type (e.g.
person, organisation).?
Lemmas (hereafter l), uses lemmas.?
Lemmas and named entities (hereafter ln),uses lemmas, with the exception of any worddetected as a named entity, which is substi-tuted by its type.A sample sentence, according to each of thesemodels, follows:f: I declare resumed the session of theEuropean Parliamentfn: I declare resumed the session of theNP00O00l: i declare resume the session of theeuropean_parliamentln: i declare resume the session of theNP00O00Table 1 shows the number of n-grams on LMsbuilt on the English side of News Commentary v8(hereafter NC) for each of the models.
Regarding1-grams, compared to f, the substitution of namedentities by their categories (fn) results in smallervocabulary size (-24.79%).
Similarly, the vocabu-lary is reduced for the models l (-8.39%) and ln (-44.18%).
Although not a result in itself, this mightbe an indication that using linguistically motivatedmodels could be useful to deal with data sparsity.n f fn l ln1 65076 48945 59619 363262 981077 847720 835825 7021183 2624800 2382629 2447759 22127094 3633724 3412719 3523888 33253115 3929751 3780064 3856917 3749813Table 1: Number of n-grams in LMs built usingthe different modelsOur procedure follows that of the Moore-Lewismethod.
We build LMs for the domain-specificcorpus and for a random subset of the non-domain-specific corpus of the same size (numberof sentences) of the domain-specific corpus.
Eachsentence s in the non-domain-specific corpus isthen scored according to equation 1 where PPI(s)is the perplexity of s according to the domain-specific LM and PPO(s) is the perplexity of s ac-cording to the non-domain-specific LM.score(s) = PPI(s)?
PPO(s) (1)We build LMs for the domain-specific and non-domain-specific corpora using the four modelspreviously introduced.
Then we rank the sen-tences of the non-domain-specific corpus for eachof these models and keep the highest ranked sen-tences according to a threshold.
Finally, we build aLM on the set of sentences selected3 and computethe perplexity of the test set on this LM.We also investigate the combination of the fourmodels.
The procedure is fairly straightforward:given the sentences selected by all the models fora given threshold, we iterate through these sen-tences following the ranking order and keeping allthe distinct sentences selected until we obtain a setof sentences whose size is the one indicated by thethreshold.
I.e.
we add to our distinct set of sen-tences first the top ranked sentence by each of themethods, then the sentence ranked second by eachmethod, and so on.3 Experiments3.1 SettingWe use corpora from the translation task atWMT13.4 Our domain-specific corpus is NC, andwe carry out experiments with three non-domain-specific corpora: a subset of Common Crawl5(hereafter CC), Europarl version 7 (hereafter EU),and United Nations (Eisele and Chen, 2010) (here-after UN).
We use the test data from WMT12(newstest2012) as our test set.
We carry out ex-periments on two languages for which these cor-pora are available: English (referred to as ?en?
intables) and Spanish (?es?
in tables).We test the methods on three very different non-domain-specific corpora, both in terms of the top-ics that they cover (text crawled from web in CC,parliamentary speeches in EU and official docu-ments from United Nations in UN) and their size3For the linguistic methods we replace the sentences se-lected (which contain lemmas and/or named entities) with thecorresponding sentences in the original corpus (containingonly word forms).4http://www.statmt.org/wmt13/translation-task.html5http://commoncrawl.org/9(around 2 million sentences both for CC and EU,and around 11 million for UN).
This can be con-sidered as a contribution of this paper since pre-vious works such as Moore and Lewis (2010)and, more recently, Axelrod et al(2011) test theMoore-Lewis method on only one non-domain-specific corpus: LDC Gigaword and an unpub-lished general-domain corpus, respectively.All the LMs are built with IRSTLM5.80.01 (Federico et al 2008), use up to 5-gramsand are smoothed using a simplified version ofthe improved Kneser-Ney method (Chen andGoodman, 1996).
For lemmatisation and namedentity recognition we use Freeling 3.0 (Padro?
andStanilovsky, 2012).
The corpora are tokenisedand truecased using scripts from the Mosestoolkit (Koehn et al 2007).3.2 Experiments with Different ModelsFigures 1, 2 and 3 show the perplexities obtainedby each method on different subsets selected fromthe English corpora CC, EU and UN, respectively.We obtain these subsets according to differentthresholds, i.e.
percentages of sentences selectedfrom the non-domain-specific corpus.
These arethe first 164 ranked sentences,132 ,116 ,18 ,14 ,12 and1.6 Corresponding figures for Spanish are omitteddue to the limited space available and also becausethe trends in those figures are very similar.64 32 16 8 4 2 1600650700750800850900950100010501100ffnllnSize 1/xPerplexityFigure 1: Results of the different methods on CCIn all the figures, the results are very similar re-gardless of the use of lemmas.
The use of namedentities, however, produces substantially differentresults.
The models that do not use named entitycategories obtain the best results for lower thresh-olds (up to 1/32 for CC, and up to 1/16 both for6An additional threshold, 1128 , is used for the United Na-tions corpus64 32 16 8 4 2 11000110012001300140015001600779ff9nlSiz1e/ixPfi lrpFigure 2: Results of the different methods on EU643 21 84 62 3 1 4 605566556855675569556055ffnllnSize 6/xPerplexityFigure 3: Results of the different methods on UNEU and UN).
If the best perplexity is obtainedwith a lower threshold than this (the case of EU,1/32, and UN, 1/64), then methods that do notuse named entities obtain the best result.
How-ever, if the optimal perplexity is obtained with ahigher threshold (the case of CC, 1/2), then usingnamed entities yields the best result.Table 2 presents the results for each model.
Foreach scenario (corpus and language combination),we show the threshold for which the best result isobtained (column best).
The perplexity obtainedon data selected by each model is shown in thesubsequent columns.
For the linguistic methods,we also show the comparison of their performanceto the baseline (as percentages, columns diff).
Theperplexity when using the full corpus is shown(column full) together with the comparison of thisresult to the best method (last column diff).The results, as previously seen in Figures 1, 2and 3, differ with respect to the corpus but followsimilar trends across languages.
For CC we obtainthe best results using named entities.
The modelln obtains the best result for English (5.54% lower10corpus best f fn diff l diff ln diff full diffcc en 1/2 660.77 625.62 -5.32 660.58 -0.03 624.19 -5.54 638.24 -2.20eu en 1/32 1072.98 1151.13 7.28 1085.66 1.18 1170.00 9.04 1462.61 -26.64un en 1/64 984.08 1127.55 14.58 979.06 -0.51 1121.45 13.96 1939.44 -49.52cc es 1/2 499.22 480.17 -3.82 498.93 -0.06 480.45 -3.76 481.96 -0.37eu es 1/16 788.62 813.32 3.13 801.50 1.63 825.13 4.63 960.06 -17.86un es 1/32 725.93 773.89 6.61 723.37 -0.35 771.25 6.24 1339.78 -46.01Table 2: Results for the different modelsperplexity than the baseline), while the model fnobtains the best result for Spanish (3.82%), al-though in both cases the difference between thesetwo models is rather small.For the other corpora, the best results are ob-tained without named entities.
In the case of EU,the baseline obtains the best result, although themodel l is not very far (1.18% higher perplexityfor English and 1.63% for Spanish).
This trendis reversed for UN, the model l obtaining the bestscores but close to the baseline (-0.51%, -0.35%).3.3 Experiments with the Combination ofModelsTable 3 shows the perplexities obtained by themethod that combines the four models (columncomb) for the threshold that yielded the best re-sult in each scenario (see Table 2), compares theseresults (column diff) to those obtained by the base-line (column f) and shows the percentage of sen-tences that this method inspected from the sen-tences selected by the individual methods (columnperc).corpus f comb diff perccc en 660.77 613.83 -7.10 76.90eu en 1072.98 1035.51 -3.49 70.51un en 984.08 908.47 -7.68 74.58cc es 499.22 478.87 -4.08 74.61eu es 788.62 748.22 -5.12 68.05un es 725.93 666.62 -8.17 74.32Table 3: Results of the combination methodThe combination method outperforms the base-line and any of the individual linguistic modelsin all the scenarios.
The perplexity obtained bycombining the models is substantially lower thanthat obtained by the baseline (ranging from 3.49%to 8.17%).
In all the scenarios, the combinationmethod takes its sentences from roughly the top70% sentences ranked by the individual methods.4 Conclusions and Future WorkThis paper has explored the use of linguistic infor-mation (lemmas and named entities) for the taskof training data selection for LMs.
We have intro-duced three linguistically motivated models, andcompared them to the state-of-the-art method forperplexity-based data selection across three dif-ferent corpora and two languages.
In four outof these six scenarios a linguistically motivatedmethod outperforms the state-of-the-art approach.We have also presented a method which com-bines surface forms and the three linguisticallymotivated methods.
This combination outper-forms the baseline in all the scenarios, select-ing data whose perplexity is between 3.49% and8.17% (depending on the corpus and language)lower than that of the baseline.Regarding future work, we have several plans.One interesting experiment would be to applythese models to a morphologically-rich language,to check if, as hypothesised, these models deal bet-ter with sparse data.Another strand regards the application of thesemodels to filter parallel corpora, e.g.
following theextension of the Moore-Lewis method (Axelrod etal., 2011) or in combination with other methodswhich are deemed to be more suitable for paralleldata, e.g.
(Mansour et al 2011).We have used one type of linguistic informa-tion in each LM, but another possibility is to com-bine different pieces of linguistic information ina single LM, e.g.
following a hybrid LM thatuses words and tags, depending of the frequencyof each type (Ruiz et al 2012).Given the fact that the best result is obtainedwith different models depending on the corpus, itwould be worth to investigate whether given a newcorpus, one could predict the best method to be ap-plied and the threshold for which one could expectto obtain the minimum perplexity.11AcknowledgmentsWe would like to thank Raphae?l Rubino for in-sightful conversations.
The research leading tothese results has received funding from the Eu-ropean Union Seventh Framework ProgrammeFP7/2007-2013 under grant agreements PIAP-GA-2012-324414 and FP7-ICT-2011-296347.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,USA.
Association for Computational Linguistics.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meet-ing on Association for Computational Linguistics,ACL ?96, pages 310?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Andreas Eisele and Yu Chen.
2010.
Multiun: Amultilingual corpus from united nation documents.In Nicoletta Calzolari, Khalid Choukri, Bente Mae-gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,Mike Rosner, and Daniel Tapias, editors, LREC.
Eu-ropean Language Resources Association.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In INTER-SPEECH, pages 1618?1621.
ISCA.Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu Lee.
2002.
Toward a unified approach to sta-tistical language modeling for chinese.
1(1):3?33,March.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit, pages 79?86, Phuket, Thailand.
AAMT, AAMT.Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-Jiann Chen, and Lin-Shan Lee.
1997.
Chinese lan-guage model adaptation based on document clas-sification and multiple domain-specific languagemodels.
In George Kokkinakis, Nikos Fakotakis,and Evangelos Dermatas, editors, EUROSPEECH.ISCA.Saab Mansour, Joern Wuebker, and Hermann Ney.2011.
Combining translation and language modelscoring for domain-specific data filtering.
In In-ternational Workshop on Spoken Language Trans-lation, pages 222?229, San Francisco, California,USA, December.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Llu?
?s Padro?
and Evgeny Stanilovsky.
2012.
Freeling3.0: Towards wider multilinguality.
In Proceedingsof the Language Resources and Evaluation Confer-ence (LREC 2012), Istanbul, Turkey, May.
ELRA.Nick Ruiz, Arianna Bisazza, Roldano Cattoni, andMarcello Federico.
2012.
FBK?s Machine Trans-lation Systems for IWSLT 2012?s TED Lectures.
InProceedings of the 9th International Workshop onSpoken Language Translation (IWSLT).12
