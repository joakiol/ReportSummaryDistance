A Comparative Study of Mixture Models for Automatic Topic Segmentationof Multiparty DialoguesMaria GeorgesculISSCO/TIM, ETIUniversity of Genevamaria.georgescul@eti.unige.chAlexander ClarkDepartment of Computer ScienceRoyal Holloway University of Londonalexc@cs.rhul.ac.ukSusan ArmstrongISSCO/TIM, ETIUniversity of Genevasusan.armstrong@issco.unige.chAbstractIn this article we address the task of auto-matic text structuring into linear and non-overlapping thematic episodes at a coarselevel of granularity.
In particular, wedeal with topic segmentation on multi-partymeeting recording transcripts, which posespecific challenges for topic segmentationmodels.
We present a comparative studyof two probabilistic mixture models.
Basedon lexical features, we use these models inparallel in order to generate a low dimen-sional input representation for topic segmen-tation.
Our experiments demonstrate that inthis manner important information is cap-tured from the data through less features.1 IntroductionSome of the earliest research related to the prob-lem of text segmentation into thematic episodes usedthe word distribution as an intrinsic feature of texts(Morris and Hirst, 1991).
The studies of (Reynar,1994; Hearst, 1997; Choi, 2000) continued in thisvein.
While having quite different emphasis at dif-ferent levels of detail (basically from the point ofview of the employed term weighting and/or theadopted inter-block similarity measure), these stud-ies analyzed the word distribution inside the textsthrough the instrumentality of merely one feature,i.e.
the one-dimensional inter-block similarity.More recent work use techniques from graph the-ory (Malioutov and Barzilay, 2006) and machinelearning (Galley et al, 2003; Georgescul et al,2006; Purver et al, 2006) in order to find patternsin vocabulary use.We investigate new approaches for topic segmen-tation on corpora containing multi-party dialogues,which currently represents a relatively less exploreddomain.
Compared to other types of audio content(e.g.
broadcast news recordings), meeting record-ings are less structured, often exhibiting a high de-gree of participants spontaneity and there may beoverlap in finishing one topic while introducing an-other.
Moreover while ending the discussion on acertain topic, there can be numerous new attemptsto introduce a new topic before it becomes the fo-cus of the dialogue.
Therefore, the task of automatictopic segmentation of meeting recordings is moredifficult and requires a more refined analysis.
(Gal-ley et al, 2003; Georgescul et al, 2007) dealt withthe problem of topic segmentation of multiparty di-alogues by combining various features based on cuephrases, syntactic and prosodic information.
In thisarticle, our investigation is based on using merelylexical features.We study mixture models in order to group thewords co-occurring in texts into a small numberof semantic concepts in an automatic unsupervisedway.
The intuition behind these models is that atext document has an underlying structure of ?la-tent?
topics, which is hidden.
In order to revealthese latent topics, the basic assumption made is thatwords related to a semantic concept tend to occur inthe proximity of each other.
The notion of proxim-ity between semantically related words can vary forvarious tasks.
For instance, bigrams can be consid-ered to capture correlation between words at a very925short distance.
At the other extreme, in the domainof document classification, it is often assumed thatthe whole document is concerned with one specifictopic and in this sense all words in a document areconsidered to be semantically related.
We considerfor our application that words occurring in the samethematic episode are semantically related.In the following, the major issues we will discussinclude the formulations of two probabilistic mix-ture approaches, their methodology, aspects of theirimplementation and the results obtained when ap-plied in the topic segmentation context.
Section 2presents our approach on using probabilistic mix-ture models for topic segmentation and shows com-parisons between these techniques.
In Section 3 wediscuss our empirical evaluation of these models fortopic segmentation.
Finally, some conclusions aredrawn in Section 4.2 Probabilistic Mixture ModelsThe probabilistic latent models described in the fol-lowing exploit hierarchical Bayesian frameworks.Based on prior distributions of word rate variabilityacquired from a training corpus, we will compute adensity function to further analyze the text content inorder to perform topic segmentation at a coarse levelof granularity.
In this model, we will be workingwith ?blocks?
of text which consist of a fixed num-ber of consecutive utterances.In the following two subsections, we use the fol-lowing notation:?
We consider a text corpus B = {b1, b2, ..., bM}containing M blocks of text with words froma vocabulary W = {w1, w2, ..., wN}.
M isa constant scalar representing the number ofblocks of text.
N is a constant scalar represent-ing the number of terms in vocabulary W .?
We pre-process the data by eliminating con-tent free words such as articles, prepositionsand auxiliary verbs.
Then, we proceed by lem-matizing the remaining words and by adopt-ing a bag-of-words representation.
Next,we summarize the data in a matrix F =(f(bi, wi,j))(i,j)?M?N , where f(bi, wi,j) de-notes the log.entropy weighted frequency ofword wi,j in block bi.?
Each occurrence of a word in a block oftext is considered as representing an ob-servation (wm,n, bm), i.e.
a realization froman underlying sequence of random variables(Wm,n, Bm)1?m?M1?n?N .
wm,n denotes the termindicator for the n-th word in the m-th blockof text.?
Each pair (wm,n, bm) is associated with a dis-crete hidden random variable Zm,n over somefinite set Z ={z1, z2, ..., zK}.
K is a constantscalar representing the number of mixture com-ponents to generate.?
We denote by P (zm,n = zk) or simply byP (zk) the probability that the k-th topic hasbeen sampled for the n-th word in the m-thblock of text.2.1 Aspect Model for Dyadic Data (AMDD)In this section we describe how we apply latent mod-eling for dyadic data (Hofmann, 2001) to text repre-sentation for topic segmentation.2.1.1 Model Settingn,mwn,mzmbMblock  platen,mwn,mzmbblock  plateMword  plateNword  plateN1) Asymmetric PLSA parameterization 2) Symmetric PLSA parameterizationFigure 1: Graphical model representation of the as-pect model.We express the joint or conditional probabilityof words and blocks of text, by assuming that thechoice of a word during the generation of a blockof text is independent of the block itself, given some(unobserved) hidden variable, also called latent vari-able or aspect.The graphical representation of the AMDD datageneration process is illustrated in Figure 1 by using926the plate notation.
That is, the ovals (i.e.
the nodesof the graph) represent probabilistic variables.
Thedouble ovals around the variables wm,n and bm de-note observed variables.
zm,n is the mixture indi-cator, the hidden variable, that chooses the topic forthe n-th word in the m-th block of text.
Arrows in-dicate conditional dependencies between variables.For instance, the wm,n variable in the word spaceand the bm variable in the block space have no di-rect dependencies, i.e.
it is assumed that the choiceof words in the generation of a block of text is in-dependent of the block given a hidden variable.
Theboxes represent ?plates?, i.e.
replicates of samplingsteps with the variable in the lower left corner re-ferring to the number of samples.
For instance, the?word plate?
in Figure 1 illustrates N independentlyand identically distributed repeated trials of the ran-dom variable wm,n.According to the topology of the asymmetricAMDD Bayesian network from Figure 1, we canspecify the joint distribution of a word wm,n, a latenttopic zk and a block of text bm: P (wm,n, zk, bm) =P (bm) ?
P (zk|bm) ?
P (wm,n|zk).
The joint distribu-tion of a block of text bm and a word wm,n is thus:P (bm, wm,n) =K?k=1P (wm,n, zk, bm) = P (bm)?
?Kk=1 P (zk|bm)?
??
?mixing proportions?
P (wm,n|zk)?
??
?mixture components(1)Equation 1 describes a special case of a finite mix-ture model, i.e.
it uses a convex combination of a setof component distributions to model the observeddata.
That is, each word in a block of text is seenas a sample from a mixture model, where mixturecomponents are multinomials P (wm,n|zk) and themixing proportions are P (zk|bm).2.1.2 Inferring and Employing the AMDDModelThe Expectation-Maximization (EM) algorithm isthe most popular method to estimate the parametersfor mixture models to fit a training corpus.
TheEM algorithm for AMDD is based on iterativelymaximizing the log-likelihood function: LPLSA =?Mm=1?Nn=1f(bm, wm,n) ?
logP (wm,n, bm).
How-ever, the EM algorithm for AMDD is prone to over-fitting since the number of parameters to be esti-mated grows linearly with the number of blocks oftext.
In order to avoid this problem, we employedthe tempered version of the EM algorithm that hasbeen proposed by Hofmann (2001).We use the density estimation method in AMDDto reduce the dimension of the blocks-by-wordsspace.
Thus, instead of using the words as ba-sic units for each block of text representation, weemploy a ?topic?
basis, assuming that a few top-ics will capture more information than the entirehuge amount of words in the vocabulary.
Thus,the m-th block of text is represented by the vector(P (z1|bm), P (z2|bm), ..., P (zk|bm)).
Then, we usethese posterior probabilities as a threshold to iden-tify the boundaries of thematic episodes via sup-port vector classification (Georgescul et al, 2006).That is, we consider the topic segmentation task as abinary-classification problem, where each utteranceshould be classified as marking the presence or theabsence of a topic shift in the dialogue.2.2 Latent Dirichlet Allocation (LDA)Latent Dirichlet Allocation (Blei et al, 2003) canbe seen as an extension of AMDD by defining aprobabilistic mixture model that includes Dirichlet-distributed priors over the masses of the multinomi-als P (w|z) and P (z|b).2.2.1 Model SettingIn order to describe the formal setting of LDA inour context, we use the following notation in addi-tion to those given at the beginning of Section 2:?
~?m is a parameter notation for P (z|b = bm),the topic mixture proportion for the m-th blockof text;?
~?
is a hyperparameter (a vector of dimensionK) on the mixing proportions ~?m;?
?
={~?m}Mm=1is a matrix (of dimensionM ?
K), composed by placing the vectors~?1, ~?2, ..., ~?M as column components;?
~?k is a parameter notation for P (w|zk), themixture component for topic k;?
~?
is a hyperparameter (a vector of dimensionN ) on the mixture components ~?k ;927?
?
= {~?k}Kk=1 is a matrix of dimensionK ?
N composed by placing the vectors~?1, ~?2, ..., ~?K as column components;?
Nm denotes the length of the m-th block of textand is modeled with a Poisson distribution withconstant parameter ?
;word plate???
?topic plateKn,mw  ?k?n,mzNmm?
?Mblock  plateFigure 2: Graphical model representation of latentDirichlet alocation.LDA generates a stream of observable wordswm,n partitioned into blocks of text ~bm as shownby the graphical model in Figure 2.
The Bayesiannetwork can be interpreted as follows: the variables?, ?
and z are the three sets of latent variables thatwe would like to infer.
The plate surrounding ~?k il-lustrates the repeated sampling of word distributionsfor each topic zk until K topics have been generated.The plate surrounding ~?m illustrates the sampling ofa distribution over topics for each block b for a to-tal of M blocks of text.
The inner plate over zm,nand wm,n illustrates the repeated sampling of topicsand words until Nm words have been generated fora block~bm.Each block of text is first generated by drawinga topic proportion ~?m, i.e.
by picking a distributionover topics from a Dirichlet distribution.
For eachword wm,n from a block of text~bm, a topic indicatork is sampled for zm,n according to the block-specificmixture proportion ~?m.
That is, ~?m determinesP (zm,n).
The topic probabilities ~?k are also sam-pled from a Dirichlet distribution.
The words in eachblock of text are then generated by using the corre-sponding topic-specific term distribution ~?zm,n .Given the graphical representation of LDA illus-trated in Figure 2, we can write the joint distributionof a word wm,n and a topic zk as:P (wm,n, zk|~?m,?)
= P (zk|~?m) ?
P (wm,n|~?k).Summing over k, we obtain the marginal distribu-tion:P (wm,n|~?m,?)
=?Kk=1???
P (zk|~?m)?
??
?mixture proportion?
P (wm,n|~?k)?
??
?mixture component??
?.Hence, similarly to AMDD (see Equation 1), theLDA model assumes that a word wm,n is generatedfrom a random mixture over topics.
Topic proba-bilities are conditioned on the block of text a wordbelongs to.
Moreover LDA leaves flexibility toassign a different topic to every observed word anda different proportion of topics for every block oftext.The joint distribution of a block of text ~bmand the latent variables of the model ~zm, ~?m,?, given the hyperparameters ~?, ~?
is furtherspecified by: P (~bm, ~zm, ~?m,?|~?, ~?)
=topic plate?
??
?P (?|~?)
?P (~?m|~?)
?Nm?n=1word plate?
??
?P (zm,n|~?m) ?
P (wm,n|~?zm,n)?
??
?block plate.Therefore, the likelihood of a block~bm is derivedas the marginal distribution obtained by summingover the zm,n and integrating out the distributions~?m and ?.2.2.2 Inferring and Employing the LDA ModelSince the integral involved in computing the like-lihood of a block ~bm is computationally intractable,several methods for approximating this posteriorhave been proposed, including variational expecta-tion maximization (Blei et al, 2003) and Markovchain Monte Carlo methods (Griffiths and Steyvers,2004).We follow an approach based on Gibbs samplingas proposed in (Griffiths and Steyvers, 2004).
Asthe convergence criteria for the Markov chain, we928check how well the parameters cluster semanticallyrelated blocks of text in a training corpus and thenwe use these values as estimates for comparable set-tings.The LDA model provides a soft clustering of theblocks of text, by associating them to topics.
Weexploit this clustering information, by using the dis-tribution of topics over blocks of text to furthermeasure the inter-blocks similarity.
As in Section2.1.2, the last step of our system consists in em-ploying binary support vector classification to iden-tify the boundaries of thematic episodes in the text.That is, we consider as input features for supportvector learning the component values of the vector(?m,z1 , ?m,z2 , ..., ?m,zk).3 ExperimentsIn order to evaluate the performance of AMDD andLDA for our task of topic segmentation, in our ex-periments we used the transcripts of ICSI-MR cor-pus (Janin et al, 2004), which consists of 75 meet-ing recordings.
A subset of 25 meetings, which aretranscribed by humans and annotated with thematicboundaries (Galley et al, 2003), has been kept fortesting purposes and support vector machine train-ing.
The transcripts of the remaining 50 meetingshave been used for the unsupervised inference ofour latent models.
The fitting phase of the mix-ture models rely on the same data set that have beenpre-processed by tokenization, elimination of stop-words and lemmatization.Once the models?
parameters are learned, the in-put data representation is projected into the lowerdimension latent semantic space.
The evaluationphase consists in checking the performance of eachmodel for predicting thematic boundaries.
That is,we check the performance of the models for predict-ing thematic boundaries on the same test set.
Thesize of a block of text during the testing phase hasbeen set to one, i.e.
each utterance has been consid-ered as a block of text.Figure 3 compares the performance obtained forvarious k values, i.e.
various dimensions of the latentsemantic space, or equivalently different numbers oflatent topics.
We have chosen k={50, ...400} usingincremental steps of 50.The performance of each latent model is mea-0.00000.10000.20000.30000.40000.50000.60000.70000.80000.900050100150200250300350400Latent space dimensionAccuracyPLSA LDAFigure 3: Results of applying the mixture models fortopic segmentation.sured by the accuracy Acc = 1 ?
Pk, where Pkdenotes the error measure proposed by (Beefermanet al, 1999).
Note that the Pk error allows for aslight variation in where the hypothesized thematicboundaries are placed.
That is, wrong hypothesizedthematic boundaries occurring in the proximity ofa reference boundary (i.e.
in a fixed-size interval oftext) are tolerated.
As proposed by (Beeferman etal., 1999), we set up the size of this interval to halfof the average number of words per segment in thegold standard segmentation.As we observe from Figure 3, LDA and AMDDachieved rather comparable thematic segmenta-tion accuracy.
While LDA steadily outperformedAMDD, the results do not show a notable advan-tage of LDA over AMDD.
In contrast, AMDD hasbetter performances for less dimensionality reduc-tion.
That is, the LDA performance curve goes downwhen the number of latent topics exceeds over 300.LDA LCSeg SVMsPk error rate 21% 32 % 22%Table 1: Comparative performance results.In Table 1, we provide the best results obtainedon ICSI data via LDA modeling.
We also reproducethe results reported on in the literature by (Galleyet al, 2003) and (Georgescul et al, 2006), whenthe evaluation of their systems was also done onICSI data.
The LCSeg system proposed by (Gal-ley et al, 2003) is based on exploiting merely lex-ical features.
Improved performance results have929been obtained by (Galley et al, 2003) when extranon-lexical features have been adopted in a decisiontree classifier.
The system proposed by (Georges-cul et al, 2006) is based on support vector machines(SVMs) and is labeled in the table as SVMs.
Weobserve from the table that our approach based oncombining LDA modeling with SVM classificationoutperforms LCSeg and performs comparably to thesystem of Georgescul et al (2006).
Thus, our exper-iments show that the LDA word density estimationapproach does capture important information fromthe data through 90% less features than a bag-of-words representation.4 ConclusionsWith the goal of performing linear topic segmen-tation by exploiting word distributions in the inputtext, the focus of this article was on both comparingtheoretical aspects and experimental results of twoprobabilistic mixture models.
The algorithms areapplied to a meeting transcription data set and arefound to provide an appropriate method for reduc-ing the size of the data representation, by perform-ing comparably to previous state-of-the-art methodsfor topic segmentation.ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical Models for Text Segmentation.
Ma-chine Learning, 34:177?210.
Special Issue on NaturalLanguage Learning.David M. Blei, Andrew Y. Ng, and Michael Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, pages 993?1022.Freddy Choi.
2000.
Advances in Domain Indepen-dent Linear Text Segmentation.
In Proceedings of the1st Conference of the North American Chapter of theAssociation for Computational Linguistics (NAACL),Seattle, USA.Michael Galley, Kathleen McKeown, Eric Fosler-Luissier, and Hongyan Jing.
2003.
Discourse Seg-mentation of Multi-Party Conversation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 562?569,Sapporo, Japan.Maria Georgescul, Alexander Clark, and Susan Arm-strong.
2006.
Word Distributions for Thematic Seg-mentation in a Support Vector Machine Approach.
InProceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL), pages 101?108,New York City, USA.Maria Georgescul, Alexander Clark, and Susan Arm-strong.
2007.
Exploiting Structural Meeting-SpecificFeatures for Topic Segmentation.
In Actes de la14e`me Confe?rence sur le Traitement Automatique desLangues Naturelles (TALN), pages 15?24, Toulouse,France.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing Scientific Topics.
In Proceedings of the NationalAcademy of Sciences, volume 101, pages 5228?5235.Marti Hearst.
1997.
TextTiling: Segmenting Text intoMulti-Paragraph Subtopic Passages.
ComputationalLinguistics, 23(1):33?64.Thomas Hofmann.
2001.
Unsupervised Learning byProbabilistic Latent Semantic Analysis.
MachineLearning, 42:177?196.Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,Jane Edwards, Javier Macias-Guarasa, Nelson Mor-gan, Barbara Peskin, Elizabeth Shriberg, AndreasStolcke, Chuck Wooters, and Britta Wrede.
2004.The ICSI Meeting Project: Resources and Research.In Proceedings of the International Conference onAcoustics, Speech and Signal Processing (ICASSP),Meeting Recognition Workshop, Montreal, Quebec,Canada.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics (COL-ING/ACL), pages 25?32, Sydney, Australia.Jane Morris and Graeme Hirst.
1991.
Lexical Cohe-sion Computed by Thesaural Relations as an Indicatorof the Structure of Text.
Computational Linguistics,17(1):21?48.Matthew Purver, Konrad P. Ko?rding, Thomas L. Grif-fiths, and Joshua B. Tenenbaum.
2006.
UnsupervisedTopic Modelling for Multi-Party Spoken Discourse.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING/ACL), pages 17?24, Sydney, Australia.Jeffrey Reynar.
1994.
An Automatic Method of FindingTopic Boundaries.
In Proceedings of the 32nd AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 331?333, Las Cruces, New Mexico,USA.930
