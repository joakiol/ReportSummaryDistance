Adaptive Chinese Word Segmentation1Jianfeng Gao*, Andi Wu*, Mu Li*, Chang-Ning Huang*, Hongqiao Li**, Xinsong Xia$, Haowei Qin&*Microsoft Research.
{jfgao, andiwu, muli, cnhuang}@microsoft.com**Beijing Institute of Technology, Beijing.
lhqtxm@bit.edu.cn$Peking University, Beijing.
xia_xinsong@founder.com&Shanghai Jiaotong university, Shanghai.
haoweiqin@sjtu.edu.cn1This work was done while Hongqiao Li, Xinsong Xia and Haowei Qin were visiting Microsoft Research (MSR) Asia.
We thankXiaodan Zhu for his early contribution, and the three reviewers, one of whom alerted us the related work of (Uchimoto et al, 2001).AbstractThis paper presents a Chinese word segmen-tation system which can adapt to differentdomains and standards.
We first present a sta-tistical framework where domain-specificwords are identified in a unified approach toword segmentation based on linear models.We explore several features and describe howto create training data by sampling.
We thendescribe a transformation-based learningmethod used to adapt our system to differentword segmentation standards.
Evaluation ofthe proposed system on five test sets with dif-ferent standards shows that the systemachieves state- of-the-art performance on all ofthem.1 IntroductionChinese word segmentation has been a long-standing research topic in Chinese language proc-essing.
Recent development in this field shows that,in addition to ambiguity resolution and unknownword detection, the usefulness of a Chinese wordsegmenter also depends crucially on its ability toadapt to different domains of texts and differentsegmentation standards.The need of adaptation involves two researchissues that we will address in this paper.
The first isnew word detection.
Different domains/applicationsmay have different vocabularies which contain newwords/terms that are not available in a generaldictionary.
In this paper, new words refer to OOVwords other than named entities, factoids and mor-phologically derived words.
These words aremostly domain specific terms (e.g.
???
?cellular?
)and time-sensitive political, social or cultural terms(e.g.
??
?Three Links?, ??
?SARS?
).The second issue concerns the customizabledisplay of word segmentation.
Different ChineseNLP-enabled applications may have different re-quirements that call for different granularities ofword segmentation.
For example, speech recogni-tion systems prefer ?longer words?
to achievehigher accuracy whereas information retrievalsystems prefer ?shorter words?
to obtain higherrecall rates, etc.
(Wu, 2003).
Given a word seg-mentation specification (or standard) and/or someapplication data used as training data, a segmenterwith customizable display should be able to providealternative segmentation units according to thespecification which is either pre-defined or impliedin the data.In this paper, we first present a statisticalframework for Chinese word segmentation, wherevarious problems of word segmentation are solvedsimultaneously in a unified approach.
Our ap-proach is based on linear models where componentmodels are inspired by the source-channel modelsof Chinese sentence generation.
We then describe indetail how the new word identification (NWI)problem is handled in this framework.
We exploreseveral features and describe how to create trainingdata by sampling.
We evaluate the performance ofour segmentation system using an annotated test set,where new words are simulated by sampling.
Wethen describe a transformation-based learning (TBL,Brill, 1995) method that is used to adapt our systemto different segmentation standards.
We comparethe adaptive system to other state-of-the-art systemsusing four test sets in the SIGHAN?s First Interna-tional Chinese Word Segmentation Bakeoff, each ofwhich is constructed according to a different seg-mentation standard.
The performance of our systemis comparable to the best systems reported on allfour test sets.
It demonstrates the possibility ofhaving a single adaptive Chinese word segmenterthat is capable of supporting multiple user applica-tions.Word Class2 Model Feature Functions, f(S,W)Context Model Word class based trigram, P(W).
-log(P(W))Lexical Word (LW) --- 1 if S forms a word lexicon entry, 0 otherwise.Morphological Word (MW) --- 1 if S forms a morph lexicon entry, 0 otherwise.Named Entity (NE) Character/word bigram, P(S|NE).
-log(P(S|NE))Factoid (FT) --- 1 if S can be parsed using a factoid grammar, 0 otherwiseNew Word (NW) --- Score of SVM classifierFigure 1: Context model, word classes, and class models, and feature functions.2In our system, we define three types of named entity: person name (PN), location name (LN), organization (ON) and translit-eration name (TN); ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone number,and WWW; and five types of morphologically derived words (MDW): affixation, reduplication, merging, head particle and split.2 Chinese Word Segmentation withLinear ModelsLet S be a Chinese sentence which is a characterstring.
For all possible word segmentations W, wewill choose the most likely one W* which achievesthe highest conditional probability P(W|S): W* =argmaxw P(W|S).
According to Bayes?
decision ruleand dropping the constant denominator, we canequivalently perform the following maximization:)|()(maxarg* WSPWPWW=.
(1)Equation (1) represents a source-channel approachto Chinese word segmentation.
This approachmodels the generation process of a Chinese sen-tence: first, the speaker selects a sequence of con-cepts W to output, according to the probabilitydistribution P(W); then he attempts to express eachconcept by choosing a sequence of characters,according to the probability distribution P(S|W).We define word class as a group of words thatare supposed to be generated according to the samedistribution (or in the same manner).
For instance,all Chinese person names form a word class.
Wethen have multiple channel models, each for oneword class.
Since a channel model estimates thelikelihood that a character string is generated givena word class, it is also referred to as class model.Similarly, source model is referred to as contextmodel because it indicates the likelihood that a wordclass occurs in a context.
We have only one contextmodel which is a word-class-based trigram model.Figure 1 shows word classes and class models thatwe used in our system.
We notice that differentclass models are constructed in different ways (e.g.name entity models are n-gram models trained oncorpora whereas factoid models use derivation rulesand have binary values).
The dynamic value rangesof different class models can be so different that it isimproper to combine all models through simplemultiplication as Equation (1).In this study we use linear models.
The methodis derived from linear discriminant functions widelyused for pattern classification (Duda et al, 2001),and has been recently introduced into NLP tasks byCollins and Duffy (2001).
It is also related to log-linear models for machine translation (Och, 2003).In this framework, we have a set of M+1 featurefunctions fi(S,W), i = 0,?,M.
They are derived fromthe context model (i.e.
f0(W)) and M class models,each for one word class, as shown in Figure 1: Forprobabilistic models such as the context model orperson name model, the feature functions are de-fined as the negative logarithm of the correspondingprobabilistic models.
For each feature function,there is a model parameter ?i.
The best word seg-mentation W* is determined by the decision rule as?===MiiiWMWWSfWSScoreW00* ),(maxarg),,(maxarg ??
(2)Below we describe how to optimize ?s.
Ourmethod is a discriminative approach inspired by theMinimum Error Rate Training method proposed inOch (2003).
Assume that we can measure thenumber of segmentation errors in W by comparing itwith a reference segmentation R using a functionEr(R,W).
The training criterion is to minimize thecount of errors over the training data as?=RWSMMSWRErM,,11^)),(,(minarg1??
?, (3)where W is detected by Equation (2).
However, wecannot apply standard gradient descent to optimizeInitialization: ?0=?, ?i=1, i = 1,?,M.For t = 1 ?
T,  j = 1 ?
NWj = argmax ?
?i fi(Sj,W)For i = 1?
M?i = ?i + ?
(Score(?,S,W)-Score(?,S,R))(fi(R) - fi(W)),where ?={?0, ?1,?,?M} and ?
=0.001.Figure 2: The training algorithm for model parametersmodel parameters according to Equation (3) be-cause the gradient cannot be computed explicitly(i.e., Er is not differentiable), and there are manylocal minima in the error surface.
We then use avariation called stochastic gradient descent (orunthresholded perceptron, Mitchell, 1997).
Asshown in Figure 2, the algorithm takes T passes overthe training set (i.e.
N sentences).
All parameters areinitially set to be 1, except for the context modelparameter ?0 which is set to be a constant ?
duringtraining, and is estimated separately on held-outdata.
Class model parameters are updated in a sim-ple additive fashion.
Notice that Score(?,S,W) is notless than Score(?,S,R).
Intuitively the updated ruleincreases the parameter values for word classeswhose models were ?underestimated?
(i.e.
expectedfeature value f(W) is less than observed featurevalue f(R)), and decreases the parameter valueswhose models were ?overestimated?
(i.e.
f(W) islarger than f(R)).
Although the method cannotguarantee a global optimal solution, it is chosen forour modeling because of its efficiency and the bestresults achieved in our experiments.Given the linear models, the procedure of wordsegmentation in our system is as follows: First, allword candidates (lexical words and OOV words ofcertain types) are generated, each with its wordclass tag and class model score.
Second, Viterbisearch is used to select the best W according toEquation (2).
Since the resulting W* is a sequence ofsegmented words that are either lexical words orOOV words with certain types (e.g.
person name,morphological words, new words) we then have asystem that can perform word segmentation andOOV word detection simultaneously in a unifiedapproach.
Most previous works treat OOV worddetection as a separate step after word segmentation.Compared to these approaches, our method avoidsthe error propagation problem and can incorporate avariety of knowledge to achieve a globally optimalsolution.
The superiority of the unified approachhas been demonstrated empirically in Gao et al(2003), and will also be discussed in Section 5.3 New Word IdentificationNew words in this section refer to OOV words thatare neither recognized as named entities or factoidsnor derived by morphological rules.
These wordsare mostly domain specific and/or time-sensitive.The identification of such new words has not beenstudied extensively before.
It is an important issuethat would have substantial impact on the per-formance of word segmentation.
For example,approximately 30% of OOV words in theSIGHAN?s PK corpus (see Table 1) are new wordsof this type.
There has been previous work on de-tecting Chinese new words from a large corpus inan off-line manner and updating the dictionarybefore word segmentation.
However, our approachis able to detect new words on-line, i.e.
to spot newwords in a sentence on the fly during the process ofword segmentation where widely-used statisticalfeatures such as mutual information or term fre-quency are not available.For brevity of discussion, we will focus on theidentification of 2-character new words, denoted asNW_11.
Other types of new words such as NW_21(a 2-character word followed with a character) andNW_12 can be detected similarly (e.g.
by viewingthe 2-character word as an inseparable unit, like acharacter).
Below, we shall describe the class modeland context model for NWI, and the creation oftraining data by sampling.3.1 Class ModelWe use a classifier (SVM in our experiments) toestimate the likelihood of two adjacent characters toform a new word.
Of the great number of featureswe experimented, three linguistically-motivatedfeatures are chosen due to their effectiveness andavailability for on-line detection.
They are Inde-pendent Word Probability (IWP), Anti-Word Pair(AWP), and Word Formation Analogy (WFA).Below we describe each feature in turn.
In Section3.2, we shall describe the way the training data (newword list) for the classifier is created by sampling.IWP is a real valued feature.
Most Chinesecharacters can be used either as independent wordsor component parts of multi-character words, orboth.
The IWP of a single character is the likelihoodfor this character to appear as an independent wordin texts (Wu and Jiang, 2000):)() ,()(xCWxCxIWP = .
(4)where C(x, W) is the number of occurrences of thecharacter x as an independent word in training data,and C(x) is the total number of x in training data.
Weassume that the IWP of a character string is theproduct of the IWPs of the component characters.Intuitively, the lower the IWP value, the more likelythe character string forms a new word.
In our im-plementation, the training data is word-segmented.AWP is a binary feature derived from IWP.
Forexample, the value of AWP of an NW_11 candidateab is defined as: AWP(ab)=1 if IWP(a)>?
or IWP(b)>?, 0 otherwise.
?
?
[0, 1] is a pre-set threshold.Intuitively, if one of the component characters isvery likely to be an independent word, it is unlikelyto be able to form a word with any other characters.While IWP considers all component characters in anew word candidate, AWP only considers the onewith the maximal IWP value.WFA is a binary feature.
Given a character pair(x, y), a character (or a multi-character string) z iscalled the common stem of (x, y) if at least one of thefollowing two conditions hold: (1) character stringsxz and yz are lexical words (i.e.
x and y as prefixes);and (2) character strings zx and zy are lexical words(i.e.
x and y as suffixes).
We then collect a list ofsuch character pairs, called affix pairs, of which thenumber of common stems is larger than a pre-setthreshold.
The value of WFA for a given NW_11candidate ab is defined as: WFA(ab) = 1 if thereexist an affix pair (a, x) (or (b, x)) and the string xb(or ax) is a lexical word, 0 otherwise.
For example,given an NW_11 candidate ??
(xia4-gang3, ?out ofwork?
), we have WFA(??)
= 1 because (?, ?)
isan affix pair (they have 32 common stems such as _?,  ?,  ?,  ?,  ?,  ?,  ?)
and ??
(shang4-gang3, ?take over a shift?)
is a lexical word.3.2 Context ModelThe motivations of using context model for NWIare two-fold.
The first is to capture useful contex-tual information.
For example, new words are morelikely to be nouns than pronouns, and the POStagging is context-sensitive.
The second is moreimportant.
As described in Section 2, with a contextmodel, NWI can be performed simultaneously withother word segmentation tasks (e.g.
: word break,named entity recognition and morphological analy-sis) in a unified approach.However, it is difficult to develop a trainingcorpus where new words are annotated because ?weusually do not know what we don?t know?.
Oursolution is Monte Carlo simulation.
We sample a setof new words from our dictionary according to thedistribution ?
the probability that any lexical wordw would be a new word P(NW|w).
We then generatea new-word-annotated corpus from a word-seg-mented text corpus.Now we describe the way P(NW|w) is estimated.It is reasonable to assume that new words are thosewords whose probability to appear in a new docu-ment is lower than general lexical words.
Let Pi(k)be the probability of word wi that occurs k times in adocument.
In our experiments, we assume thatP(NW|wi) can be approximated by the probability ofwi occurring less than K times in a new document:?
?=?10)()|(Kkii kPwNWP , (5)where the constant K is dependent on the size of thedocument: The larger the document, the larger thevalue.
Pi(k) can be estimated using several termdistribution models (see Chapter 15.3 in Manningand Sch?tze, 1999).
Following the empirical studyin (Gao and Lee, 2000), we use K-Mixture (Katz,1996) which estimate Pi(k) askki kP )1(1)1()( 0, +++?= ??????
, (6)where ?k,0=1 if  k=0, 0 otherwise.
?
and ?
are pa-rameters that can be fit using the observed mean ?and the observed inverse document frequency IDFas follow:Ncf=?
,dfNIDF log= ,dfdfcfIDF ?=?
?= 12??
, and ???
= ,where cf is the total number of occurrence of wordwi in training data, df is the number of documents intraining data that wi occurs in, and N is the totalnumber of documents.
In our implementation, thetraining data contain approximately 40 thousanddocuments that have been balanced among domain,style and time.4 Adaptation to Different StandardsThe word segmentation standard (or standard forbrevity) varies from system to system because thereis no commonly accepted definition of ChineseCondition: ?Affixation?
Condition: ?Date?
Condition: ?PersonName?Actions: Insert a boundarybetween ?Prefix?
and ?Stem?
?Actions: Insert a boundary between?Year?
and ?Mon?
?Actions: Insert a boundary be-tween ?FamilyName?
and ?Given-Name?
?Figure 3: Word internal structure and class-type transformation templates.words and different applications may have differentrequirements that call for different granularities ofword segmentation.It is ideal to develop a single word segmentationsystem that is able to adapt to different standards.We consider the following standard adaptationparadigm.
Suppose we have a ?general?
standardpre-defined by ourselves.
We have also created alarge amount of training data which are segmentedaccording to this general standard.
We then developa generic word segmenter, i.e.
the system describedin Sections 2 and 3.
Whenever we deploy the seg-menter for any application, we need to customizethe output of the segmenter according to an appli-cation-specific standard, which is not always ex-plicitly defined.
However, it is often implicitlydefined in a given amount of application data(called adaptation data) from which the specificstandard can be partially learned.In our system, the standard adaptation is con-ducted by a postprocessor which performs an or-dered list of transformations on the output of thegeneric segmenter ?
removing extraneous wordboundaries, and inserting new boundaries ?
toobtain a word segmentation that meets a differentstandard.The method we use is transformation-basedlearning (Brill, 1995), which requires an initialsegmentation, a goal segmentation into which wewish to transform the initial segmentation and aspace of allowable transformations (i.e.
transfor-mation templates).
Under the abovementionedadaptation paradigm, the initial segmentation is theoutput of the generic segmenter.
The goal segmen-tation is adaptation data.
The transformation tem-plates can make reference to words (i.e.
lexicalizedtemplates) as well as some pre-defined types (i.e.class-type based templates), as described below.We notice that most variability in word seg-mentation across different standards comes fromthose words that are not typically stored in thedictionary.
Those words are dynamic in nature andare usually formed through productive morpho-logical processes.
In this study, we focus on threecategories: morphologically derived words (MDW),named entities (NE) and factoids.For each word class that belongs to these cate-gories2, we define an internal structure similar to(Wu, 2003).
The structure is a tree with ?word class?as the root, and ?component types?
as the othernodes.
There are 30 component types.
As shown inFigure 3, the word class Affixation has threecomponent types: Prefix, Stem and Suffix.Similarly, PersonName has two component typesand Date has nine ?
3 as non-terminals and 6 asterminals.
These internal structures are assigned towords by the generic segmenter at run time.The transformation templates for words of theabove three categories are of the form:Condition: word classActions:z Insert ?
place a new boundarybetween two component types.z Delete ?
remove an existingboundary between two componenttypes.Since the application of the transformations de-rived from the above templates are conditioned onword class and make reference to component types,we call the templates class-type transformationtemplates.
Some examples are shown in Figure 3.In addition, we also use lexicalized transforma-tion templates as:z Insert ?
place a new boundarybetween two lemmas.Mon DayPre_Y Pre_MDig_M Dig_DYearDatePersonNameFamilyName GivenNameAffixationPrefix Stem SuffixPre_DDig_Yz Delete ?
remove an existingboundary between two lemmas.Here, lemmas refer to those basic lexical wordsthat cannot be formed by any productive morpho-logical process.
They are mostly single characters,bi-character words, and 4-character idioms.In short, our adaptive Chinese word segmenterconsists of two components: (1) a generic seg-menter that is capable of adapting to the vocabu-laries of different domains and (2) a set of outputadaptors, learned from application data, for adapt-ing to different ?application-specific?
standards5 EvaluationWe evaluated the proposed adaptive word seg-mentation system (henceforth AWS) using fivedifferent standards.
The training and test corpora ofthese standards are detailed in Table 1, where MSRis defined by ourselves, and the other four are stan-dards used in SIGHAN?s First International Chi-nese Word Segmentation Bakeoff (Bakeoff test setsfor brevity, see Sproat and Emperson (2003) fordetails).Corpus Abbrev.
# Tr.
Word # Te.
Word?General?
standard  MSR 20M 226KBeijing University PK 1.1M 17KU.
Penn ChineseTreebankCTB 250K 40KHong Kong City U. HK 240K 35KAcademia Sinica AS 5.8M 12KTable 1: standards and corpora.MSR is used as the general standard in our ex-periments, on the basis of which the generic seg-menter has been developed.
The training and testcorpora were annotated manually, where there isonly one allowable word segmentation for eachsentence.
The training corpus contains approxi-mately 35 million Chinese characters from variousdomains of text such as newspapers, novels, maga-zines etc.
90% of the training corpus are used forcontext model training, and 10% are held-out datafor model parameter training as shown in Figure 2.The NE class models, as shown in Figure 1, weretrained on the corresponding NE lists that werecollected separately.
The test set contains a total of225,734 tokens, including 205,162 lexi-con/morph-lexicon words, 3,703 PNs, 5,287 LNs,3,822 ONs, and 4,152 factoids.
In Section 5.1, wewill describe some simulated test sets that are de-rived from the MSR test set by sampling NWs froma 98,686-entry dictionary.The four Bakeoff standards are used as ?specific?standards into which we wish to adapt the generalstandard.
We notice in Table 1 that the sizes ofadaptation data sets (i.e.
training corpora of the fourBakeoff standards) are much smaller than that of theMSR training set.
The experimental setting turnsout to be a good simulation of the adaptation para-digm described in Section 4.The performance of word segmentation ismeasured through test precision (P), test recall (R),F score (which is defined as 2PR/(P+R)), the OOVrate for the test corpus (on Bakeoff corpora, OOV isdefined as the set of words in the test corpus notoccurring in the training corpus.
), the recall onOOV words (Roov), and the recall on in-vocabulary(Riv) words.
We also tested the statistical signifi-cance of results, using the criterion proposed bySproat and Emperson (2003), and all results re-ported in this section are significantly differentfrom each other.5.1 NWI ResultsThis section discusses two factors that we believehave the most impact on the performance of NWI.First, we compare methods where we use the NWIcomponent (i.e.
an SVM classifier) as a post-processor versus as a feature function in the linearmodels of Equation (2).
Second, we compare dif-ferent sampling methods of creating simulatedtraining data for context model.
Which samplingmethod is best depends on the nature of P(NW|w).As described in Section 3.2, P(NW|w) is unknownand has to be approximated by Pi(k) in our study, soit is expected that the closer P(NW|w) and Pi(k) are,the better the resulting context model.
We comparethree estimates of Pi(k) in Equation (5) using termmodels based on Uniform, Possion, and K- Mixturedistributions, respectively.Table 2 shows the results of the generic seg-menter on three test sets that are derived from theMSR test set using the above three different sam-pling methods, respectively.
For all three distribu-tions, unified approaches (i.e.
using NWI compo-nent as a feature function) outperform consecutiveapproaches (i.e.
using NWI component as a post-processor).
This demonstrates empirically thebenefits of using context model for NWI and theunified approach to Chinese word segmentation, asdescribed in 3.2.
We also perform NWI on BakeoffAWS w/o NW AWS w/ NW (post-processor) AWS w/ NW (unified approach)word segmentation word segmentation NW word segmentation NW  # of NWP% R% P% R% P% R% P% R% P% R%Uniform 5,682 92.6 94.5 94.7 95.2 64.1 66.8 95.1 95.5 68.1 78.4Poisson 3,862 93.4 95.6 94.5 95.9 61.4 45.6 95.0 95.7 57.2 60.6K-Mixture 2,915 94.7 96.4 95.1 96.2 44.1 41.5 95.6 96.2 46.2 60.4Table 2: NWI results on MSR test set, NWI as post-processor versus unified approachPK CTBP R F OOV Roov Riv P R F OOV Roov Riv1.
AWS w/o adaptation .824 .854 .839 .069 .320 .861 .799 .818 .809 .181 .624 .8612.
AWS .952 .959 .955 .069 .781 .972 .895 .914 .904 .181 .746 .9503.
AWS w/o NWI .949 .963 .956 .069 .741 .980 .875 .910 .892 .181 .690 .9594.
FMM w/ adaptation .913 .946 .929 .069 .524 .977 .805 .874 .838 .181 .521 .9525.
Rank 1 in Bakeoff .956 .963 .959 .069 .799 .975 .907 .916 .912 .181 .766 .9496.
Rank 2 in Bakeoff .943 .963 .953 .069 .743 .980 .891 .911 .901 .181 .736 .949Table 3: Comparison scores for PK open and CTB open.HK ASP R F OOV Roov Riv P R F OOV Roov Riv1.
AWS w/o adaptation .819 .822 .820 .071 .593 .840 .832 .838 .835 .021 .405 .8472.
AWS .948 .960 .954 .071 .746 .977 .955 .961 .958 .021 .584 .9693.
AWS w/o NWI .937 .958 .947 .071 .694 .978 .958 .943 .951 .021 .436 .9694.
FMM w/ adaptation .818 .823 .821 .071 .591 .841 .930 .947 .939 .021 .160 .9645.
Rank 1 in Bakeoff .954 .958 .956 .071 .788 .971 .894 .915 .904 .021 .426 .9266.
Rank 2 in Bakeoff .863 .909 .886 .071 .579 .935 .853 .892 .872 .021 .236 .906Table 4: Comparison scores for HK open and AS open.test sets.
As shown in Tables 3 and 4 (Rows 2 and 3),the use of NW functions (via the unified approach)substantially improves the word segmentation per-formance.We find in our experiments that NWs sampledby Possion and K-Mixture are mostly specific andtime-sensitive terms, in agreement with our intui-tion, while NWs sampled by Uniform include morecommon words and lemmas that are easier to detect.Consequently, by Uniform sampling, the P/R ofNWI is the highest but the P/R of the overall wordsegmentation is the lowest, as shown in Table 2.Notice that the three sampling methods are notcomparable in terms of P/R of NWI in Table 2because of different sampling result in different setsof new words in the test set.
We then perform NWIon Bakeoff test sets where the sets of new words areless dependent on specific sampling methods.
Theresults however do not give a clear indication whichsampling method is the best because the test sets aretoo small to show the difference.
We then leave it tofuture work a thorough empirical comparisonamong different sampling methods.5.2 Standard Adaptation ResultsThe results of standard adaptation on four Bakeofftest sets are shown in Tables 3 and 4.
A set oftransformations for each standard is learnt usingTBL from the corresponding Bakeoff training set.For each test set, we report results using our systemwith and without standard adaptation (Rows 1 and2).
It turns out that performance improves dra-matically across the board in all four test sets.For comparison, we also include in each tablethe results of using the forward maximum matching(FMM) greedy segmenter as a generic segmenter(Row 4), and the top 2 scores (sorted by F) that arereported in SIGHAN?s First International ChineseWord Segmentation Bakeoff (Rows 5 and 6).
Wecan see that with adaptation, our generic segmentercan achieve state-of-the-art performance on dif-ferent standards, showing its superiority over othersystems.
For example, there is no single segmenterin SIGHAN?s Bakeoff, which achieved top-2 ranksin all four test sets (Sproat and Emperson, 2003).We notice in Table 3 and 4 that the quality ofadaptation seems to depend largely upon the size ofadaptation data: we outperformed the best bakeoffsystems in the AS set because the size of the adap-tation data is big while we are worse in the CTB setbecause of the small size of the adaptation data.
Toverify our speculation, we evaluated the adaptationresults using subsets of the AS training set of dif-ferent sizes, and observed the same trend.
However,even with a much smaller adaptation data set (e.g.250K), we still outperform the best bakeoff results.6 Related WorkMany methods of Chinese word segmentation havebeen proposed (See Wu and Tseng, 1993; Sproatand Shih, 2001 for reviews).
However, it is difficultto compare systems due to the fact that there is nowidely accepted standard.
There has been less workon dealing with NWI and standard adaptation.All feature functions in Figure 1, except the NWfunction, are derived from models presented in(Gao et al, 2003).
The linear models are similar towhat was presented in Collins and Duffy (2001).
Analternative to linear models is the log-linear modelssuggested by Och (2003).
See Collins (2002) for acomparison of these approaches.The features for NWI were studied in Wu &Jiang (2000) and Li et al (2004).
The use of sam-pling was proposed in Della Pietra et al (1997) andRosenfeld et al (2001).
There is also a related workon this line in Japanese (Uchimoto et al, 2001).A detailed discussion on differences among thefour Bakeoff standards is presented in Wu (2003),which also proposes an adaptive system where thedisplay of the output can be customized by users.The method described in Section 4 can be viewed asan improved version in that the transformations arelearnt automatically from adaptation data.
The useof TBL for Chinese word segmentation was firstsuggested in Palmer (1997).7 ConclusionThis paper presents a statistical approach to adap-tive Chinese word segmentation based on linearmodels and TBL.
The system has two components:A generic segmenter that can adapt to the vocabu-laries of different domains, and a set of outputadaptors, learned from application data, for adapt-ing to different ?application-specific?
standards.We evaluate our system on five test sets, each cor-responding to a different standard.
We achievestate-of-the-art performance on all test sets.ReferencesBrill, Eric.
1995.
Transformation-based error-drivenlearning and natural language processing: a case studyin Part-of-Speech tagging.
In: Computational Linguis-tics, 21(4).Collins, Michael and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In: Advances in NeuralInformation Processing Systems (NLPS 14).Collins, Michael.
2002.
Parameter estimation for statis-tical parsing models: theory and practice of distribu-tion-free methods.
To appear.Della Pietra, S., Della Pietra, V., and Lafferty, J.
1997.Inducing features of random fields.
In: IEEE Transac-tions on Pattern Analysis and Machine Intelligence, 19,380-393.Duda, Richard O, Hart, Peter E. and Stork, David G.2001.
Pattern classification.
John Wiley & Sons, Inc.Gao, Jianfeng and Kai-Fu Lee.
2000.
Distribution basedpruning of backoff language models.
In: ACL2000.Gao, Jianfeng, Mu Li and Chang-Ning Huang.
2003.Improved source-channel model for Chinese wordsegmentation.
In: ACL2003.Katz, S. M. 1996.
Distribution of content words andphrases in text and language modeling, In: NaturalLanguage Engineering, 1996(2): 15-59Li, Hongqiao, Chang-Ning Huang, Jianfeng Gao andXiaozhong Fan.
2004.
The use of SVM for Chinesenew word identification.
In: IJCNLP2004.Manning, C. D. and H. Sch?tze, 1999.
Foundations ofStatistical Natural Language Processing.
The MITPress.Mitchell, Tom M. 1997.
Machine learning.
TheMcGraw-Hill Companies, Inc.Och, Franz.
2003.
Minimum error rate training in statis-tical machine translation.
In: ACL2003.Palmer, D. 1997.
A trainable rule-based algorithm forword segmentation.
In: ACL '97.Rosenfeld, R., S. F. Chen and X. Zhu.
2001.
Wholesentence exponential language models: a vehicle forlinguistic statistical integration.
In: Computer Speechand Language, 15 (1).Sproat, Richard and Chilin Shih.
2002.
Corpus-basedmethods in Chinese morphology and phonology.
In:COLING 2002.Sproat, Richard and Tom Emerson.
2003.
The firstinternational Chinese word segmentation bakeoff.
In:SIGHAN 2003.Uchimoto, K., S. Sekine and H. Isahara.
2001.
Theunknown word problem: a morphological analysis ofJapanese using maximum entropy aided by a diction-ary.
In: EMNLP2001.Wu, Andi and Zixin Jiang.
2000.
Statistically-enhancednew word identification in a rule-based Chinese system.In: Proc of the 2rd ACL Chinese Processing Workshop.Wu, Andi.
2003.
Customizable segmentation of mor-phologically derived words in Chinese.
In: Interna-tional Journal of Computational Linguistics and Chi-nese Language Processing, 8(1): 1-27.Wu, Zimin and Gwyneth Tseng.
1993.
Chinese textsegmentation for text retrieval achievements and prob-lems.
In: JASIS, 44(9): 532-542.
