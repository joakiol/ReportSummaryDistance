Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1003?1013,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsStructural Representations for Learning Relations between Pairs of TextsSimone Filice and Giovanni Da San Martino and Alessandro MoschittiALT, Qatar Computing Research Institute, Hamad Bin Khalifa University{sfilice,gmartino,amoschitti}@qf.org.qaAbstractThis paper studies the use of structuralrepresentations for learning relations be-tween pairs of short texts (e.g., sentencesor paragraphs) of the kind: the secondtext answers to, or conveys exactly thesame information of, or is implied by, thefirst text.
Engineering effective featuresthat can capture syntactic and semantic re-lations between the constituents compos-ing the target text pairs is rather complex.Thus, we define syntactic and semanticstructures representing the text pairs andthen apply graph and tree kernels to themfor automatically engineering features inSupport Vector Machines.
We carry outan extensive comparative analysis of state-of-the-art models for this type of relationallearning.
Our findings allow for achiev-ing the highest accuracy in two differ-ent and important related tasks, i.e., Para-phrasing Identification and Textual Entail-ment Recognition.1 IntroductionAdvanced NLP systems, e.g., IBM Watson system(Ferrucci et al, 2010), are the result of effectiveuse of syntactic/semantic information along withrelational learning (RL) methods.
This researcharea is rather vast including, extraction of syntac-tic relations, e.g., (Nastase et al, 2013), predicaterelations, e.g., Semantic Role Labeling (Carrerasand M`arquez, 2005) or FrameNet parsing (Gildeaand Jurafsky, 2002) and relation extraction be-tween named entities, e.g., (Mintz et al, 2009).Although extremely interesting, the abovemethods target relations only between text con-stituents whereas the final goal of an intelligentsystem would be to interpret the semantics oflarger pieces of text, e.g., sentences or para-graphs.
This line of research relates to threebroad fields, namely, Question Answering (QA)(Voorhees and Tice, 1999), Paraphrasing Identifi-cation (PI) (Dolan et al, 2004) and Recognitionof Textual Entailments (RTE) (Giampiccolo et al,2007).
More generally, RL from text can be deniedas follows: given two text fragments, the maingoal is to derive relations between them, e.g., ei-ther if the second fragment answers the question,or conveys exactly the same information or is im-plied by the first text fragment.
For example, thefollowing two sentences:- License revenue slid 21 percent, however, to$107.6 million.- License sales, a key measure of demand, fell 21percent to $107.6 million.express exactly the same meaning, whereas thenext one:- She was transferred again to Navy when theAmerican Civil War began, 1861.implies:- The American Civil War started in 1861.Automatic learning a model for deriving the re-lations above is rather complex as any of the textconstituents, e.g., License revenue, a key measureof demand, in the two sentences plays an importantrole.
Therefore, a suitable approach should ex-ploit representations that can structure the two sen-tences and put their constituents in relation.
Sincethe dependencies between constituents can be anexponential number and representing structures inlearning algorithms is rather challenging, auto-matic feature engineering through kernel methods(Shawe-Taylor and Cristianini, 2004; Moschitti,2006) can be a promising direction.In particular, in (Zanzotto and Moschitti, 2006),we represented the two evaluating sentences forthe RTE task with syntactic structures and then ap-plied tree kernels to them.
The resulting systemwas very accurate but, unfortunately, it could notscale to large datasets as it is based on a compu-1003tationally exponential algorithm.
This prevents itsapplication to PI tasks, which typically require alarge dataset to train the related systems.In this paper, we carry out an extensive exper-imentation using different kernels based on treesand graphs and their combinations with the aim ofassessing the best model for relation learning be-tween two entire sentences (or even paragraphs).More in detail, (i) we design many models for RLcombining state-of-the-art tree kernels and graphkernels and apply them to innovative computa-tional structures.
These innovative combinationsuse for the fist time semantic/syntactic tree ker-nels and graph kernels for the tackled tasks.
(ii)Our kernels provide effective and efficient solu-tions, which solve the previous scalability problemand, at the same time, exceed the state of the arton both RTE and PI.
Finally, our study suggestsresearch directions for designing effective graphkernels for RL.2 Related WorkIn this paper, we apply kernel methods, which en-able an efficient comparison of structures in huge,possibly infinite, feature spaces.
While for trees, acomparison using all possible subtrees is possible,designing kernel functions for graphs with suchproperty is an NP-Hard problem (i.e., it shows thesame complexity of the graph isomorphism prob-lem) (Gartner et al, 2003).
Thus most kernelsfor graphs only associate specific types of sub-structures with features, such as paths (Borgwardtand Kriegel, 2005; Heinonen et al, 2012), walks(Kashima et al, 2003; Vishwanathan et al, 2006)and tree structures (Cilia and Moschitti, 2007;Mah?e and Vert, 2008; Shervashidze et al, 2011;Da San Martino et al, 2012).We exploit structural kernels for PI, whose taskis to evaluate whether a given pair of sentences isin the paraphrase class or not, (see for example(Dolan et al, 2004)).
Paraphrases can be seen asa restatement of a text in another form that pre-serves the original meaning.
This task has a pri-mary importance in many other NLP and IR taskssuch as Machine Translation, Plagiarism Detec-tion and QA.
Several approaches have been pro-posed, e.g., (Socher et al, 2011) apply a recursiveauto encoder with dynamic pooling, and (Madnaniet al, 2012) use eight machine translation metricsto achieve the state of the art.
To our knowledge noprevious model based on kernel methods has beenapplied before: with such methods, we outperformthe state of the art in PI.A description of RTE can be found in (Giampic-colo et al, 2007): it is defined as a directionalrelation extraction between two text fragments,called text and hypothesis.
The implication is sup-posed to be detectable only based on the text con-tent.
Its applications are in QA, Information Ex-traction, Summarization and Machine translation.One of the most performing approaches of RTE 3was (Iftene and Balahur-Dobrescu, 2007), whichlargely relies on external resources (i.e., WordNet,Wikipedia, acronyms dictionaries) and a base ofknowledge developed ad hoc for the dataset.
In(Zanzotto and Moschitti, 2006), we designed aninteresting but computationally expensive modelusing simple syntactic tree kernels.
In this pa-per, we develop models that do not use externalresources but, at the same time, are efficient andapproach the state of the art in RTE.3 Structural kernelsKernel Machines carry out learning and classifi-cation by only relying on the inner product be-tween instances.
This can be efficiently and im-plicitly computed by kernel functions by exploit-ing the following dual formulation of the model(hyperplane):?i=1..lyi?i?
(oi) ?
?
(o) + b = 0,where yiare the example labels, ?ithe supportvector coefficients, oiand o are two objects, ?
isa mapping from the objects to feature vectors ~xiand ?
(oi) ?
?
(o) = K(oi, o) is the kernel func-tion implicitly defining such mapping.
In caseof structural kernels, K maps objects in substruc-tures, thus determining their size and shape.
Giventwo structures S1and S2, our general definition ofstructural kernels is the following:K(S1, S2) =?s1?S1,s2?S2,si?Skiso(s1, s2), (1)where siare substructures of Si, S is the set of ad-missible substructures, and kisodetermines if thetwo substructures are isomorphic, i.e., it outputs 1if s1and s2are isomorphic and 0 otherwise.In the following, we also provide a morecomputational-oriented definition of structuralkernels to more easily describe those we use in ourwork:Let the set S = {s1, s2, .
.
.
, s|S|} be the substruc-ture space and ?i(n) be an indicator function,equal to 1 if the target siis rooted at node n and1004equal to 0 otherwise.
A structural-kernel functionover S1and S2isK(S1, S2) =?n1?NS1?n2?NS2?
(n1, n2), (2)where NS1and NS2are the sets of the S1?s andS2?s nodes, respectively and?
(n1, n2) =|S|?i=1?i(n1)?i(n2).
(3)The latter is equal to the number of commonsubstructures rooted in the n1and n2nodes.In order to have a similarity score between 0and 1, a normalization in the kernel space, i.e.,K(S1,S2)?K(S1,S1)?K(S2,S2)is usually applied.
From apractical computation viewpoint, it is convenientto divide structural kernels in two classes of algo-rithms working either on trees or graphs.3.1 The Partial Tree Kernel (PTK)PTK (Moschitti, 2006) generalizes a large classof tree kernels as it computes one of the mostgeneral tree substructure spaces.
Given two treesS1and S2, PTK considers any connected subsetof nodes as possible feature of the substructurespace, and counts how many of them are sharedby S1and S2.
Its computation is carried out byEq.
2 using the following ?PTKfunction:if the labels of n1and n2are different?PTK(n1, n2) = 0; else ?PTK(n1, n2) =?
(?2+?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1?PTK(cn1(~I1j), cn2(~I2j)))where ?, ?
?
[0, 1] are two decay factors,~I1and~I2are two sequences of indices, which index sub-sequences of children u,~I = (i1, ..., i|u|), in se-quences of children s, 1 ?
i1< ... < i|u|?
|s|,i.e., such that u = si1..si|u|, and d(~I) = i|u|?i1+ 1 is the distance between the first and lastchild.
The PTK computational complexity isO(p?2|NS1||NS2|) (Moschitti, 2006), where p isthe largest subsequence of children that we wantto consider and ?
is the maximal outdegree ob-served in the two trees.
However the average run-ning time tends to be linear for natural languagesyntactic trees (Moschitti, 2006).3.2 Smoothed Partial Tree Kernel (SPTK)Constraining the application of lexical simi-larity to words embedded in similar structuresprovides clear advantages over all-vs-all wordssimilarity, which tends to semantically di-verge.
Indeed, syntax provides the necessaryrestrictions to compute an effective semanticsimilarity.
SPTK (Croce et al, 2011) gen-eralizes PTK by enabling node similarityduring substructure matching.
More formally,SPTK is computed by Eq.
2 using the following?SPTK(n1, n2) =?|S|i,j=1?i(n1)?j(n2)?
(si, sj),where ?
is a similarity between structures1.
Therecursive definition of ?SPTKis the following:1. if n1and n2are leaves ?SPTK(n1, n2) =???
(n1, n2);2. else ?SPTK(n1, n2) = ??
(n1, n2)?(?2+?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1??
(cn1(~I1j), cn2(~I2j))),where ?
is any similarity between nodes, e.g., be-tween their lexical labels, and the other variablesare the same of PTK.
The worst case complexityof SPTK is identical to PTK and in practice isnot higher than O(|NS1||NS2|).3.3 Neighborhood Subgraph PairwiseDistance Kernel (NSPDK)When general subgraphs are used as features in akernel computation, eq.
1 and 2 become computa-tionally intractable (Gartner et al, 2003).
To solvethis problem, we need to restrict the set of consid-ered substructures S. (Costa and De Grave, 2010)defined NSPDK such that the feature space isonly constituted by pairs of subgraphs (substruc-tures) that are (i) centered in two nodes n1and n2such that their distance is not more than D; and(ii) constituted by all nodes (and their edges) atan exact distance h from n1or n2, where the dis-tance between two nodes is defined as the num-ber of edges in the shortest path connecting them.More formally, let G, NGand EGbe a graph andits set of nodes and edges, respectively, the sub-structure space S = SG(H,D) used byNSPDKin eqs 2 and 3 is:{(?h(n), ?h(n?))
: 1 ?
h ?
H,n, n??
NG,d(n, n?)
?
D},where ?h(n) returns the subgraph obtained by ex-ecuting h steps of a breadth-first visit ofG startingfrom node n and d(n, n?)
is the distance betweentwo nodes in the graph.
Note that (i) any feature1Note that this generalizes Eq.
3.1005of the space is basically a pair of substructures;and (ii) there is currently no efficient (implicit) for-mulation for computing such kernel.
In contrast,whenH andD are limited, it is simple to computethe space SG(H,D) explicitly.
In such case, thecomplexity of the kernel is given by the substruc-ture extraction step, which is O(|NG| ?
h?
log ?
).3.4 Kernel CombinationsPrevious sections have shown three different ker-nels.
Among them, NSPDK is actually an ex-plicit kernel, where the features are automaticallyextracted with a procedure.
In NLP, features areoften manually defined by domain experts, whoknow the linguistic phenomena involved in thetask.
When available, such features are importantas they encode some of the background knowledgeon the task.
Therefore, combining different featurespaces is typically very useful.
Fortunately, ker-nel methods enable an easy integration of differentkernels or feature spaces, i.e., the kernel sum pro-duces the joint feature space and it is still a validkernel.
In the next section, we show representa-tions of text, i.e., structures and features, specificto PI and RTE.4 Representations for RL from textThe kernels described in the previous section canbe applied to generic trees and graphs.
Auto-matic feature engineering using structural kernelsrequires the design of structures for representingdata examples that are specific to the learning taskwe want to tackle.
In our case, we focus on RL,which consists in deriving the semantic relationbetween two entire pieces of text.
We focus ontwo well-understood relations, namely, paraphras-ing and textual implications.
The tasks are simplydefined as: given two texts a1and a2, automati-cally classify if (i) a1is a paraphrase of a2and/or(ii) a1implies a2.
Although the two tasks are lin-guistically and conceptually rather different, theycan be modeled in a similar way from a shallowrepresentation viewpoint.
This is exactly the per-spective we would like to keep for showing the ad-vantage of using kernel methods.
Therefore, in thefollowing, we define sentence representations thatcan be suitably used for both tasks and then werely on structural kernels and the adopted learningalgorithm for exploring the substructures relevantto the different tasks.4.1 Tree RepresentationsAn intuitive understanding of our target taskssuggests that syntactic information is essential toachieve high accuracy.
Therefore, we considerthe syntactic parse trees of the pair of sentencesinvolved in the evaluation.
For example, Fig.
1shows the syntactic constituency trees of thesentences reported in the introduction (thesedo not include the green label REL and thedashed edges).
Given two pairs of sentences,pa= ?a1, a2?
and pb= ?b1, b2?, an initial kernelfor learning the tasks, can be the simple treekernel sum, e.g., PTK(a1, b1) + PTK(a2, b2)as was defined in (Moschitti, 2008).
This kernelworks in the space of the union of the sets of allsubtrees from the upper and lower trees, e.g.
:a1:{[PP [TO [to::t]][NP [QP [$[$::$]][QP [CD [107.6::c]]]]]], [PP[TO][NP [QP [$][QP [CD [107]]]]]], [PP[TO][NP [QP [QP [CD]]]]], [PP [NP [QP[QP]]]], ...}?a2:{[NP [NP [DT [a::d]] [JJ [key::j]NN]][PP]], [NP [NP [DT] [JJ NN]][PP]], [NP[NP [JJ NN]][PP]], [NP [NP [NN]][PP]],[NP [NP [JJ]][PP]], ...}However, such features cannot capture the rela-tions between the constituents (or semantic lexicalunits) from the two trees.
In contrast, these are es-sential to learn the relation between the two entiresentences2.To overcome this problem, in (Zanzotto andMoschitti, 2006), we proposed the use of place-holders for RTE: the main idea was to annotate thematches between the constituents of the two sen-tences, e.g., 107.6 millions, on both trees.
Thisway the tree fragments in the generated kernelspace contained an index capturing the correspon-dences between a1and a2.
The critical drawbackof this approach is that other pairs, e.g., pb, willhave in general different indices, making the rep-resentation very sparse.
Alternatively, we experi-mented with models that select the best match be-tween all possible placeholder assignments acrossthe two pairs.
Although we obtained a good im-provement, such solution required an exponentialcomputational time and the selection of the max2Of course assuming that text meaning is compositional.1006ROOTSNP-RELJJ-RELlicense::jNNrevenue::nVPVBDslide::vNP-RELCD-REL21::cNN-RELpercent::n,,::,ADVBRBhowever::r,,::,PPTOto::tNPQP-REL$-REL$::$QP-RELCD-REL107.6::cCD-RELmillion::c..::.ROOTSNPNP-RELJJ-RELlicense::jNNSsale::n,,::,NPNPDTa::dJJkey::jNNmeasure::nPPINof::iNPNNdemand::n,,::,VPVBDfall::vNP-RELCD-REL21::cNN-RELpercent::nPPTOto:tNPQP-REL$-REL$::$QP-RELCD-REL107.6::cCD-RELmillion::c..::.Figure 1: Text representations for PI and RTE: (i) pair of trees, a1(upper) and a2(lower), (ii) combinedin a graph with dashed edges, and (iii) labelled with the tag REL (in green).
The nodes highlighted inyellow constitute a feature for the NSPDK kernel (h = 1, D = 3) centered at the nodes ADVB andNP-REL.assignment made our similarity function a non-valid kernel.Thus, for this paper, we prefer to rely on a morerecent solution we proposed for passage rerankingin the QA domain (Severyn and Moschitti, 2012;Severyn et al, 2013a; Severyn et al, 2013b), andfor Answer Selection (Severyn and Moschitti,2013).
It consists in simply labeling matchingnodes with a special tag, e.g., REL, whichindicates the correspondences between words.REL is attached to the father and grandfathernodes of the matching words.
Fig.
1 showsseveral green REL tags attached to the usualPOS-tag and constituent node labels of the parsetrees.
For example, the lemma license is matchedby the two sentences, thus both its father, JJ,and its grandfather, NP, nodes are marked withREL.
Thanks to such relational labeling thesimple kernel, PTK(a1, b1) + PTK(a2, b2), cangenerate relational features from a1, e.g., [NP[NP-REL [JJ-REL] NN]][PP]], [NP [NP-REL[NN]][PP]], [NP [NP-REL [JJ-REL]][PP]],...If such features are matched in b1, they providethe fuzzy information: there should be a matchsimilar to [NP [NP-REL [JJ-REL]] also betweena2and b2.
This kind of matches establishes a sortof relational pair features.It should be noted that we proposed morecomplex REL tagging policies for Passage Re-ranking, exploiting additional resources such asLinked Open Data or WordNet (Tymoshenko etal., 2014).
Another interesting application of thisRL framework is the Machine Translation Evalua-tion (Guzm?an et al, 2014).
Finally, we used a sim-ilar model for translating questions to SQL queriesin (Giordani and Moschitti, 2012).4.2 Graph RepresentationsThe relational tree representation can capture re-lational features but the use of the same RELtag for any match between the two trees preventsto deterministically establish the correspondencesbetween nodes.
For exactly representing suchmatches (without incurring in non-valid kernelsor sparsity problems), a graph representation isneeded.
If we connect matching nodes (or alsonodes labelled as REL) in Fig.
1 (see dashedlines), we obtain a relational graph.
Substructuresof such graph clearly indicate how constituents,e.g., NPs, VPs, PPs, from one sentence map intothe other sentence.
If such mappings observedin a pair of paraphrase sentences are matchedin another sentence pair, there may be evidencethat also the second pair contains paraphrase sen-1007tences.Unfortunately, the kernel computing the spaceof all substructures of a graph (even if only con-sidering connected nodes) is an intractable prob-lem as mentioned in Sec.
3.3.
Thus, we opt for theuse of NSPDK, which generates specific pairsof structures.
Intuitively, the latter can capture re-lational features between constituents of the twotrees.
Figure 1 shows an example of features gen-erated by the NSPDK with parameters H =1, D = 3 (the substructures are highlighted inyellow), i.e., [ADVB [VP] [RB]], [NP-REL [VP][CD-REL] [NN-REL]].4.3 Basic FeaturesIn addition to structural representations, we alsouse typical features for capturing the degrees ofsimilarity between two sentences.
In contrast,with the previous kernels these similarities arecomputed intra-pair, e.g., between a1and a2.
Notethat any similarity measure generates only one fea-ture.
Their description follows:?
Syntactic similarities, which apply the cosinefunction to vectors of n-grams (with n = 1, 2, 3, 4)of word lemmas and part-of-speech tags.?
Kernel similarities, which use PTK or SPTKapplied to the sentences within the pair.We also used similarity features from theDKPro of the UKP Lab (B?ar et al, 2012), testedin the Semantic Textual Similarity (STS) task:?
Longest common substring measure and Longestcommon subsequence measure, which determinethe length of the longest substring shared by twotext segments.?
Running-Karp-Rabin Greedy String Tiling pro-vides a similarity between two sentences by count-ing the number of shuffles in their subparts.?
Resnik similarity based on the WordNet hierar-chy.?
Explicit Semantic Analysis (ESA) similar-ity (Gabrilovich and Markovitch, 2007) repre-sents documents as weighted vectors of con-cepts learned from Wikipedia, WordNet and Wik-tionary.?
Lexical Substitution (Szarvas et al, 2013):a supervised word sense disambiguation systemis used to substitute a wide selection of high-frequency English nouns with generalizations,then Resnik and ESA features are computed on thetransformed text.4.4 Combined representationsAs mentioned in Sec.
3.4, we can combine ker-nels for engineering new features.
LetK be PTKor SPTK, given two pairs of sentences pa=?a1, a2?
and pb= ?b1, b2?, we build the followingkernel combinations for the RTE task:(i) K+(pa, pb) = K(a1, b1) +K(a2, b2), whichsimply sums the similarities between the firsttwo sentences and the second two sentenceswhose implication has to be derived.
(ii) An alternative kernel combines the twosimilarity scores above with the product:K?
(pa, pb) = K(a1, b1) ?K(a2, b2).
(iii) The symmetry of the PI task requires differ-ent kernels.
The most intuitive applies Kbetween all member combinations and sumall contributions: all+K(pa, pb)=K(a1, b1) +K(a2, b2) +K(a1, b2) +K(a2, b1).
(iv) It is also possible to combine pairs ofcorresponding kernels with the product:all?K(pa, pb) = K(a1, b1)K(a2, b2) +K(a1, b2)K(a2, b1).
(v) An alternative kernel selects only the best be-tween the two products above: MK(pa, pb) =max(K(a1, b1)K(a2, b2),K(a1, b2)K(a2, b1)).This is motivated by the observation thatbefore measuring the similarity betweentwo pairs, we need to establish whichaiis more similar to bj.
However, themax operator causes MKnot to be avalid kernel function, thus we substi-tute it with a softmax function, whichis a valid kernel, i.e., SMK(pa, pb) = soft-max(K(a1, b1)K(a2, b2),K(a1, b2)K(a2, b1)),where softmax(x1, x2) =1clog(ecx1+ ecx2)(c=100 was accurate enough).The linear kernel (LK) over the basic features(described previously) and/or NSPDK can be ofcourse added to all the above kernels.5 Experiments5.1 SetupMSR Paraphrasing: we used the Microsoft Re-search Paraphrase Corpus (Dolan et al, 2004) con-sisting of 4,076 sentence pairs in the training setand 1,725 sentence pairs in test set, with a distri-bution of about 66% between positive and negative1008Vs Test 5 Fold Cross ValidationKernel Acc (%) P R F1 Acc (%) P R F1withoutRELtaggingLK 75.88 0.784 0.881 0.829 75.54 ?
0.45 0.786 ?
0.009 0.876 ?
0.019 0.828 ?
0.004GK 72.81 0.720 0.967 0.825 72.49 ?
1.22 0.723 ?
0.014 0.957 ?
0.011 0.824 ?
0.008SMPTK72.06 0.722 0.943 0.818 72.04 ?
1.08 0.725 ?
0.009 0.940 ?
0.017 0.819 ?
0.009SMSPTKLSA72.12 0.722 0.943 0.818 72.56 ?
1.10 0.731 ?
0.010 0.937 ?
0.017 0.821 ?
0.009SMSPTKW2V71.88 0.719 0.946 0.817 72.23 ?
1.07 0.727 ?
0.009 0.938 ?
0.017 0.820 ?
0.009all?PTK71.42 0.718 0.939 0.814 71.57 ?
0.86 0.724 ?
0.007 0.933 ?
0.015 0.815 ?
0.008all?SPTKLSA72.29 0.725 0.941 0.819 72.06 ?
0.62 0.730 ?
0.007 0.928 ?
0.014 0.817 ?
0.006all?SPTKW2V71.59 0.717 0.947 0.816 71.61 ?
0.76 0.725 ?
0.008 0.931 ?
0.013 0.815 ?
0.007all+PTK70.78 0.716 0.930 0.809 70.76 ?
0.91 0.720 ?
0.008 0.924 ?
0.017 0.809 ?
0.009all+SPTKLSA71.48 0.720 0.934 0.813 71.42 ?
0.91 0.727 ?
0.008 0.920 ?
0.020 0.812 ?
0.009all+SPTKW2V70.72 0.714 0.935 0.809 71.19 ?
1.22 0.723 ?
0.010 0.927 ?
0.018 0.812 ?
0.011MPTK72.17 0.725 0.935 0.817 72.31 ?
0.67 0.731 ?
0.007 0.930 ?
0.015 0.819 ?
0.007MSPTKLSA72.00 0.725 0.934 0.816 72.32 ?
0.44 0.732 ?
0.006 0.927 ?
0.014 0.818 ?
0.005MSPTKW2V71.71 0.722 0.933 0.814 71.99 ?
0.96 0.730 ?
0.008 0.926 ?
0.016 0.816 ?
0.008withRELtaggingGK 75.07 0.752 0.933 0.833 74.69 ?
2.52 0.749 ?
0.029 0.940 ?
0.008 0.834 ?
0.018SMPTK76.17 0.765 0.927 0.838 75.42 ?
0.86 0.771 ?
0.007 0.903 ?
0.012 0.832 ?
0.008SMSPTKLSA76.52 0.767 0.929 0.840 75.62 ?
0.90 0.772 ?
0.007 0.905 ?
0.013 0.833 ?
0.007SMSPTKW2V76.35 0.766 0.929 0.839 75.64 ?
0.77 0.771 ?
0.004 0.907 ?
0.012 0.833 ?
0.007all?PTK75.36 0.767 0.905 0.830 74.76 ?
0.71 0.769 ?
0.006 0.892 ?
0.016 0.826 ?
0.008all?SPTKLSA75.65 0.770 0.903 0.831 74.83 ?
0.92 0.771 ?
0.009 0.891 ?
0.011 0.826 ?
0.008all?SPTKW2V75.88 0.772 0.905 0.833 75.26 ?
0.81 0.771 ?
0.008 0.898 ?
0.011 0.830 ?
0.008all+PTK74.49 0.762 0.895 0.824 73.99 ?
1.04 0.767 ?
0.010 0.880 ?
0.013 0.820 ?
0.009all+SPTKLSA75.07 0.767 0.899 0.827 73.87 ?
0.85 0.766 ?
0.009 0.880 ?
0.010 0.819 ?
0.007all+SPTKW2V75.42 0.772 0.894 0.829 74.16 ?
0.75 0.768 ?
0.008 0.882 ?
0.012 0.821 ?
0.007GK+SMSPTKW2V76.70 0.782 0.901 0.837 76.12 ?
0.96 0.787 ?
0.008 0.885 ?
0.015 0.833 ?
0.009LK+GK 78.67 0.802 0.902 0.849 77.85 ?
1.00 0.804 ?
0.008 0.886 ?
0.015 0.843 ?
0.009LK+SMSPTKW2V77.74 0.794 0.899 0.843 77.52 ?
1.41 0.802 ?
0.011 0.885 ?
0.016 0.841 ?
0.011LK+GK+SMSPTKW2V79.13 0.807 0.901 0.852 78.11 ?
0.94 0.811 ?
0.005 0.879 ?
0.016 0.844 ?
0.009(Socher et al, 2011) 76.8 ?
?
0.836 ?
?
?
?
(Madnani et al, 2012) 77.4 ?
?
0.841 ?
?
?
?Table 1: Results on Paraphrasing Identificationexamples.
These pairs were extracted from top-ically similar Web news articles, applying someheuristics that select potential paraphrases to beannotated by human experts.RTE-3.
We adopted the RTE-3 dataset (Giampic-colo et al, 2007), which is composed by 800 text-hypothesis pairs in both the training and test sets,collected by human annotators.
The distributionof the examples among the positive and negativeclasses is balanced.5.1.1 Models and ParameterizationWe train our classifiers with the C-SVM learningalgorithm (Chang and Lin, 2011) within KeLP3, aKernel-based Machine Learning platform that im-plements tree kernels.
In both tasks, we appliedthe kernels described in Sec.
4, where the trees aregenerated with the Stanford parser4.SPTK uses a node similarity function?
(n1, n2) implemented as follows: if n1and n2are two identical syntactic nodes ?
= 1.
If n1and n2are two lexical nodes with the same POStag, their similarity is evaluated computing thecosine similarity of their corresponding vectors ina wordspace.
In all the other cases ?
= 0.
Wegenerated two different wordspaces.
The first is3https://github.com/SAG-KeLP4http://nlp.stanford.edu/software/corenlp.shtmla co-occurrence LSA embedding as described in(Croce and Previtali, 2010).
The second space isderived by applying a skip-gram model (Mikolovet al, 2013) with the word2vec tool5.
SPTKusing the LSA will be referred to as SPTKLSA,while when adopting word2vec it will be indicatedwith SPTKW2V.
We used default parametersboth for PTK and SPTK whereas we selectedh and D parameters of NSPDK that obtainedthe best average accuracy using a 5-fold crossvalidation on the training set.5.1.2 Performance measuresThe two considered tasks are binary classificationproblems thus we used Accuracy, Precision, Re-call and F1.
The adopted corpora have a prede-fined split between training and test sets thus wetested our models according to such settings forexactly comparing with previous work.
Addition-ally, to better assess our results, we performed a 5-fold cross validation on the complete datasets.
Incase of PI, the same sentence can appear in mul-tiple pairs thus we distributed the pairs such thatthe same sentence can only appear in one fold at atime.5https://code.google.com/p/word2vec/1009Vs Test 5 Fold Cross ValidationKernel Acc (%) P R F1 Acc (%) P R F1withoutRELtaggingLK 62 0.608 0.729 0.663 62.94 ?
5.68 0.635 ?
0.057 0.679 ?
0.083 0.652 ?
0.049GK 55.375 0.555 0.651 0.599 55.63 ?
1.81 0.564 ?
0.022 0.612 ?
0.087 0.584 ?
0.032PTK+56.13 0.560 0.676 0.612 54.13 ?
3.26 0.547 ?
0.024 0.637 ?
0.051 0.587 ?
0.027SPTK+LSA56.88 0.566 0.683 0.619 53.63 ?
2.50 0.543 ?
0.024 0.622 ?
0.060 0.578 ?
0.027SPTK+W2V56.63 0.563 0.683 0.617 54.06 ?
2.34 0.546 ?
0.022 0.634 ?
0.060 0.585 ?
0.026PTK?55.88 0.558 0.671 0.609 52.81 ?
1.99 0.535 ?
0.025 0.623 ?
0.055 0.574 ?
0.028SPTK?LSA56.25 0.561 0.671 0.611 53.56 ?
2.09 0.543 ?
0.022 0.616 ?
0.065 0.576 ?
0.026SPTK?W2V55.25 0.554 0.646 0.597 52.50 ?
1.77 0.533 ?
0.027 0.619 ?
0.071 0.571 ?
0.034withRELtaggingGK 61.63 0.603 0.734 0.662 59.81 ?
3.84 0.599 ?
0.037 0.678 ?
0.071 0.634 ?
0.026PTK+66.00 0.627 0.829 0.714 67.75 ?
7.17 0.655 ?
0.067 0.817 ?
0.038 0.725 ?
0.046SPTK+LSA65.38 0.622 0.824 0.709 67.81 ?
7.30 0.656 ?
0.069 0.816 ?
0.036 0.725 ?
0.047SPTK+W2V66.38 0.629 0.837 0.718 68.00 ?
7.15 0.658 ?
0.068 0.816 ?
0.039 0.726 ?
0.046PTK?66.13 0.629 0.827 0.714 67.75 ?
7.37 0.658 ?
0.071 0.804 ?
0.038 0.722 ?
0.049SPTK?LSA66.00 0.629 0.822 0.712 68.00 ?
7.62 0.661 ?
0.074 0.808 ?
0.039 0.725 ?
0.049SPTK?W2V67.00 0.636 0.834 0.722 67.69 ?
6.95 0.658 ?
0.069 0.804 ?
0.040 0.722 ?
0.043GK+SPTK?W2V66.38 0.634 0.815 0.713 66.00 ?
6.79 0.648 ?
0.069 0.769 ?
0.034 0.701 ?
0.044LK+GK 62.25 0.609 0.737 0.667 62.06 ?
5.49 0.620 ?
0.051 0.702 ?
0.053 0.656 ?
0.036LK+SPTK?W2V66.13 0.628 0.829 0.715 68.25 ?
7.54 0.663 ?
0.076 0.816 ?
0.032 0.728 ?
0.047LK+GK+SPTK?W2V66.00 0.633 0.800 0.707 66.31 ?
7.35 0.652 ?
0.075 0.770 ?
0.053 0.703 ?
0.052(Zanzotto et al, 2009) 66.75 0.667 ?
?
?
?
?
?
(Iftene and Balahur-Dobrescu, 2007) 69.13 ?
?
?
?
?
?
?Table 2: Results on Textual Entailment Recognition5.2 Results on PIThe results are reported in Table 1.
The first col-umn shows the use of the relational tag REL inthe structures (discussed in Sec.
4.1).
The secondcolumn indicates the kernel models described insections 3 and 4 as well as the combination of thebest models.
Columns 3-6 report Accuracy, Pre-cision, Recall and F1 derived on the fixed test set,whereas the remaining columns regard the resultsobtained with cross validation.
We note that:First, when REL information is not used in thestructures, the linear kernel (LK) on basic fea-tures outperforms all the structural kernels, whichall perform similarly.
The best structural kernel isthe graph kernel, NSPDK (GK in short).
Thisis not surprising as without REL, GK is the onlykernel that can express relational features.Second, SPTK is only slightly better thanPTK.
The reason is mainly due to the ap-proach used for building the dataset: potentialparaphrases are retrieved applying some heuristicsmostly based on the lexical overlap between sen-tences.
Thus, in most cases, the lexical similarityused in SPTK is not needed as hard matches oc-cur between the words of the sentences.Third, when REL is used on the structures, allkernels reach or outperform the F1 (official mea-sure of the challenge) of LK.
The relational struc-tures seem to drastically reduce the inconsistentmatching between positive and negative examples,reflecting in remarkable increasing in Precision.
Inparticular, SMSPTKLSAachieves the state of theart6, i.e., 84.1 (Madnani et al, 2012).Next, combining our best models produces asignificant improvement of the state of the art, e.g.,LK+GK+SMSPTKW2Voutperforms the resultin (Madnani et al, 2012) by 1.7% in accuracy and1.1 points in F1.Finally, the cross-validation experiments con-firm the system behavior observed on the fixedtest set.
The Std.
Dev.
(specified after the ?
sign)shows that in most cases the system differences aresignificant.5.3 Results on RTEWe used the same experimental settings performedfor PI to carry out the experiments on RTE.
Theresults are shown in Table 2 structured in the sameway as the previous table.
We note that:(i) Findings similar to PI are obtained.
(ii) Again the relational structures (using REL)provide a remarkable improvement in Ac-curacy (RTE challenge measure), allowingtree kernels to compete with the state of theart.
This is an impressive result consider-ing that our models do not use any exter-nal resource, e.g., as in (Iftene and Balahur-Dobrescu, 2007).
(iii) This time, SPTK?W2Vimproves onPTK by1 absolute percent point.6The performance of the several best systems improvedby our models are nicely summarized at http://aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art)1010(iv) The kernel combinations are not more effec-tive than SPTK alone.Finally, the cross-fold validation experiments con-firm the fixed-test set results.6 Discussion and ConclusionsIn this paper, we have engineered and studiedseveral models for relation learning.
We utilizedstate-of-the-art kernels for structures and creatednew ones by combining kernels together.
Addi-tionally, we provide a novel definition of effectiveand computationally feasible structural kernels.Most importantly, we have designed novel com-putational structures for trees and graphs, whichare for the first time tested in NLP tasks.
Our ker-nels are computationally efficient thus solving oneof the most important problems of previous work.We empirically tested our kernels on two of themost representative tasks of RL from text, namely,PI and RTE.
The extensive experimentation us-ing many kernel models also combined with tradi-tional feature vector approaches sheds some lighton how engineering effective graph and tree ker-nels for learning from pairs of entire text frag-ments.
In particular, our best models significantlyoutperform the state of the art in PI and the bestkernel model for RTE 3, with Accuracy close tothe one of the best system of RTE 3.It should be stressed that the design of previousstate-of-the-art models involved the use of severalresources, annotation and heavy manually engi-neering of specific rules and features: this makesthe portability of such systems on other domainsand tasks extremely difficult.
Moreover the un-availability of the used resources and the opacityof the used rules have also made such systems verydifficult to replicate.On the contrary, the models we propose enableresearchers to:(i) build their system without the use of spe-cific resources.
We use a standard syntacticparser, and for some models we use well-known and available corpora for automati-cally learning similarities with word embed-ding algorithms; and(ii) reuse our work for different (similar) tasks(see paraphrasing) and data.The simplicity and portability of our system is asignificant contribution to a very complex researcharea such as RL from two entire pieces of text.Our study has indeed shown that our kernelmodels, which are very simple to be implemented,reach the state of the art and can be used with largedatasets.Furthermore, it should be noted that our mod-els outperform the best tree kernel approach of theRTE challenges (Zanzotto and Moschitti, 2006)and also its extension that we proposed in (Zan-zotto et al, 2009).
These systems are also adapt-able and easy to replicate, but they are subject toan exponential computational complexity and canthus only be used on very small datasets (e.g., theycannot be applied to the MSR Paraphrase corpus).In contrast, the model we proposed in this papercan be used on large datasets, because its kernelcomplexity is about linear (on average).We believe that disseminating these findingsto the research community is very important, asit will foster research on RL, e.g., on RTE, us-ing structural kernel methods.
Such research hashad a sudden stop as the RTE data in the latestchallenges increased from 800 instances to sev-eral thousands and no tree kernel model has beenenough accurate to replace our computational ex-pensive models (Zanzotto et al, 2009).In the future, it would be interesting defininggraph kernels that can combine more than two sub-structures.
Another possible extension regards theuse of node similarity in graph kernels.
Addition-ally, we would like to test our models on otherRTE challenges and on several QA datasets, whichfor space constraints we could not do in this work.AcknowledgmentsThis research is part of the Interactive sYstemsfor Answer Search (IYAS) project, conducted bythe Arabic Language Technologies (ALT) groupat Qatar Computing Research Institute (QCRI)within the Hamad Bin Khalifa University andQatar Foundation.ReferencesDaniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proc.
of SemEval ?12.
ACL.Karsten M Borgwardt and Hans-Peter Kriegel.
2005.Shortest-Path Kernels on Graphs.
ICDM, 0:74?81.Xavier Carreras and Llu?
?s M`arquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
In Proc.
of CONLL ?05, USA.1011Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Elisa Cilia and Alessandro Moschitti.
2007.
Ad-vanced tree-based kernels for protein classification.In AI*IA 2007: Artificial Intelligence and Human-Oriented Computing, 10th Congress of the ItalianAssociation for Artificial Intelligence, Rome, Italy,September 10-13, 2007, Proceedings, pages 218?229.Fabrizio Costa and Kurt De Grave.
2010.
Fastneighborhood subgraph pairwise distance kernel.
InICML, number v.Danilo Croce and Daniele Previtali.
2010.
Mani-fold learning for the semi-supervised induction offramenet predicates: An empirical investigation.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured lexical similarity via con-volution kernels on dependency trees.
In Proceed-ings EMNLP.Giovanni Da San Martino, Nicol`o Navarin, andAlessandro Sperduti.
2012.
A tree-based kernel forgraphs.
In Proceedings of the Twelfth SIAM Interna-tional Conference on Data Mining, Anaheim, Cal-ifornia, USA, April 26-28, 2012., pages 975?986.SIAM / Omnipress.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProc.
of COLING ?04, Stroudsburg, PA, USA.David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya A Kalyanpur,Adam Lally, J William Murdock, Eric Nyberg, JohnPrager, et al 2010.
Building watson: An overviewof the deepqa project.
AI magazine, 31(3):59?79.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proc.
of IJCAI-07, pages 1606?1611.Thomas Gartner, P Flach, and S Wrobel.
2003.
OnGraph Kernels : Hardness Results and Efficient Al-ternatives.
LNCS, pages 129?143.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recognizingtextual entailment challenge.
In Proc.
of the ACL-PASCAL RTE ?07 Workshop, pages 1?9.
ACL.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28:245?288.Alessandra Giordani and Alessandro Moschitti.
2012.Translating questions to sql queries with genera-tive parsers discriminatively reranked.
In COLING(Posters), pages 401?410.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez,Alessandro Moschitti, Preslav Nakov, and MassimoNicosia.
2014.
Learning to differentiate better fromworse translations.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 214?220, Doha, Qatar,October.
Association for Computational Linguistics.M Heinonen, N V?alim?aki, V M?akinen, and J Rousu.2012.
Efficient Path Kernels for Reaction FunctionPrediction.
Bioinformatics Models, Methods and Al-gorithms.Adrian Iftene and Alexandra Balahur-Dobrescu.
2007.Hypothesis transformation and semantic variabilityrules used in recognizing textual entailment.
In RTEWorkshop, Prague.Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.2003.
Marginalized kernels between labeled graphs.In ICML, pages 321?328.
AAAI Press.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metrics forparaphrase identification.
In Proceedings of NAACLHLT ?12.
ACL.Pierre Mah?e and Jean-Philippe Vert.
2008.
Graph ker-nels based on tree patterns for molecules.
MachineLearning, 75(1):3?35, October.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In ACL-AFNLP.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Proc.
of ECML?06, pages 318?329.Alessandro Moschitti.
2008.
Kernel methods, syn-tax and semantics for relational text categorization.In Proceedings of the 17th ACM Conference on In-formation and Knowledge Management, CIKM ?08,pages 253?262, New York, NY, USA.
ACM.Vivi Nastase, Preslav Nakov, Diarmuid Saghdha, andStan Szpakowicz.
2013.
Semantic Relations Be-tween Nominals.
Morgan & Claypool Publishers.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In SIGIR.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In EMNLP, pages 458?467.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013a.
Building structures from clas-sifiers for passage reranking.
In Proceedings of1012the 22Nd ACM International Conference on Con-ference on Information &#38; Knowledge Manage-ment, CIKM ?13, pages 969?978, New York, NY,USA.
ACM.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013b.
Learning adaptable patternsfor passage reranking.
Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning, pages 75?83.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press, USA.Nino Shervashidze, Pascal Schweitzer, Erik Jan vanLeeuwen, Kurt Mehlhorn, and Karsten M Borg-wardt.
2011.
Weisfeiler-Lehman Graph Kernels.JMLR, 12:2539?2561.Richard Socher, Eric H. Huang, Jeffrey Pennin,Christopher D Manning, and Andrew Y. Ng.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In NIPS 24.Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych.2013.
Supervised all-words lexical substitution us-ing delexicalized features.
In NAACL.Kateryna Tymoshenko, Alessandro Moschitti, and Ali-aksei Severyn.
2014.
Encoding semantic resourcesin syntactic structures for passage reranking.
EACL2014, page 664.S.
V. N. Vishwanathan, Karsten M Borgwardt, andNicol N Schraudolph.
2006.
Fast Computation ofGraph Kernels.
In NIPS.E.
Voorhees and D. Tice, 1999.
The TREC-8 QuestionAnswering Track Evaluation.Fabio Massimo Zanzotto and Alessandro Moschitti.2006.
Automatic learning of textual entailmentswith cross-pair similarities.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 401?408,Sydney, Australia, July.
Association for Computa-tional Linguistics.Fabio massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learn-ing approach to textual entailment recognition.
Nat.Lang.
Eng., 15(4):551?582, October.1013
