Joining Statistics with NLP for Text CategorizationPau l  S. J acobsArtificial Intell igence LaboratoryGE Research and Development  CenterSchenectady,  NY 12301 USApsjacobs@crd.ge.comAutomatic news categorization systems have pro-duced high accuracy, consistency, and flexibility usingsome natural language processing techniques.
Theseknowledge-based categorization methods are more pow-erful and accurate than statistical techniques.
However,the phrasal pre-processing and pattern matching methodsthat seem to work for categorization have the disadvan-tage of requiring a fair amount of knowledge-encodingby human beings.
In addition, they work much better atcertain tasks, such as identifying major events in texts,than at others, such as determining what sort of businessor product is involved in a news event.Statistical methods for categorization, on the otherhand, are easy to implement and require little or no hu-man customization.
But they don't offer any of the ben-efits of natural anguage processing, such as the ability toidentify relationships and enforce linguistic constraints.Our approach has been to use statistics in the knowl-edge acquisition component of a linguistic pattern-basedcategorization system, using statistical methods, for ex-ample, to associate words with industries and identifyphrases that information about businesses or products.Instead of replacing knowledge-based methods with statis-tics, statistical training replaces knowledge ngineering.This has resulted in high accuracy, shorter customiza-tion time, and good prospects for the application of thestatistical methods to problems in lexical acquisition.1 In t roduct ionText categorization is an excellent application domainfor natural language processing systems.
First, it is atask in which NLP techniques have born fruit, producinghigh accuracy along with other benefits \[Hayes and We-instein, 1990; Kuhns, 1990; Tong et al, 1986\].
Second, itprovides an easy way of measuring success, by comparingsystem responses with "expert" category assignments.Third, it is a ripe domain for exploring statistical meth-ods for automated knowledge acquisition.
Publishedwork on text categorization has focused on the first itemabove, arguing convincingly for knowledge-based tech-niques and their accuracy, but has not yet opened theway for the investigation of category assignment as away of testing NLP methods or on the prospects for ac-quisition.
This work focuses on combining statistics andNLP in a knowledge-based categorization system, usingstatistics as way of augmenting hand-coded knowledge.The context of this research is a commercially-developed system \[Rau and Jacobs, 1991\] that automat-ically assigns categories to news stories for "custom clip-ping" and other markets.
Like Construe/TIS \[Hayes andWeinstein, 1990\], the work derives from, and coordinateswith, NLP efforts, but the system primarily uses a lexico-semantic pattern matcher for categorization \[Jacobs etal., 1991\].
Categorization tasks vary greatly in difficulty,but the recall and precision results produced in our testsare similar to those reported by other systems, with cov-erage of over 90% on topic assignment and performancebetter than human indexers on most aspects of the task.Figure 1 shows a typical example of a news story, withassociated human-assigned categories.
Retrieval is per-formed by matching a desired set of categories (termeda query or profile) against those assigned in the textdatabase.
Our system, known as NLDB, mimics thesecategory assignments, extracting company names \[Rau,1991\], topics or subject indicators, industries, and others(including, for example, stock exchanges and geographicregions).
The program also incorporates portions of theSCISOR system \[Jacobs and Rau, 1990\], which can fillcertain other fields, such as the target and suitor of atakeover.This sort of system has a simple appeal: the "answers"(the set of category assignments) are usually clear-cut,yet they clearly require some detailed content analysis.On the other hand, the technologies that could con-tribute to this analysis are bafflingly complex, from dis-course methods that distinguish topics from backgroundevents to word sense techniques that help to distinguish,for example, COMMUNICATIONS from BROADCASTING andHEALTH CARE from PHARMACEUTICALS.Figure 2 shows the complete list of industry and topicassignments currently in use to categorize texts in theNLDB system.The development of this system has advanced thestate of the art in practical NLP by proving the util-ity of statistical training methods on a knowledge-basedNLP task.
Feeding in large volumes of texts with hu-man answers has found new ways around old problem~,in knowledge acquisition.
This paper explains the re-lationship between problems in NLP and performanc?in categorization and describes a statistical method folautomatically creating lexico-semantic patterns for cat-egorization.178Companies Industries Topics OtherARGOSYSTEMS AVIATION BUSINESS CORPORATEARGOSYSTEMS INC DEFENSE CONTRACTING CONTRACT NEWSGRIDBOEING ELECTRONICS NYASEBOEING CO(BA) OTCUTL USAUTL CORP(UTLC)BOEING'S ARGOSYSTEMS SUBSIDIARY TO MAKE TENDER OFFER FOR ALL UTL CORE SHARESSEATTLE (JULY 31) PR NEWSWIRE - The Boeing Co. (NYSE: BA) has announced its agreement tocause its whol-ly-owned subsidiary ARGOSystems of Sunnyvale, Calif., to make a cash tender offer at $4.75 per share for all shares ofUTL Corp. (NASDAQ: UTLC) of Dallas.The transaction is valued at approximately $20 million.
The boards of ARGOSystems and UTL have approved thetransaction.The tender offer will commence no later than Aug. 6.
Upon completion of the tender offer, the agreement calls for amerger in which the remaining UTL stockholders will also receive $4.75 in cash per share.
The tender offer is subject tocertain conditions including the tender of at least a majority of the outstanding UTL shares.UTL Corp. designs, develops, manufactures and markets electronic warfare systems used for reconnaissance and sur.veillance.
The systems provide information on the location and identification ofradar and communications emitters ....Figure 1: Input Text and Assigned Categories2 Overa l l  Resu l t sFigure 3 summarizes the overall results of this experi-ment, including the results of assigning topic categories(the task generally reported in this sort of work) and in-dustry categories with statistics only, natural anguage,and the combination; and the overall effect of the com-bined approach.Recall here has essentially the same meaning as ininformation retrieval; i.e.
the percentage of human-assigned categories that the system also produced.
Pre-cision is the percentage of system-assigned categoriesthat also appeared in the human indices.
These statis-tics make the dubious (and often incorrect) assumptionthat the human-assigned categories are always correct.In Figure 1, for example, NLDB included AEROSPACE onthe industry list--This hurts precision because it is notincluded on the human list.The major achievement here is that the combinationof statistical analysis and natural anguage based cate-gorization is considerably better than either alone.
Thesystem uses statistical methods where they do better (i.e.industries) and NLP where it does better (i.e.
topics),and shows that combined NLP and statistics can be bet-ter than either technique alone within a particular task.The results tend to understate he real impact of thiscombination, in part because of the large differences indifficulty among sets of categories, and in part becauseof the portion of the human-assigned categories that areincorrect.
In analyzing sample texts where the humancategories differ from the automatically-assigned cate-gories, we have found that the system tends to be cor-rect about as often as the human indexer, with manycases so difficult to judge that multiple independent as-sessments differ.
Since this means that the system recallagainst he human is higher than human recall againstthe system, the results indicate that the system's overallperformance is better than human performance.
How-ever, for the purpose of this experiment, we use theseresults to compare different system configurations, andnot to illustrate an absolute measure of accuracy.3 Categor i za t ion  ~ Natura l  LanguageWhile it is easy to attain a certain level of accuracy intext categorization using a single layer of techniques andto combine all texts and all categories in evaluating theresults, this aggregation of results obscures many of thereal problems where NLP and categorization come to-gether.
Each type of category can highlight a differentset of NLP issues, and different texts reveal different pro-cessing problems.
For example, the problem of mistakinga totally irrelevant text for a text of a particular categoryis very different from the subtle task of distinguishingtexts about one category from another.
Similarly, NLPresults in processing texts within a category are quitedifferent from results in assigning texts to categories, forreasons that will be explained below.The best way to evaluate NLP techniques, therefore,is within the context of a more precise task than cate-gorization ill general, and as a complement tostatisticalmethods.
Even in component tasks where pure statis-tical methods tend to outperform pure NLP methods,NLP can play an important role in improving results,and statistics can play a role in improving NLP.179Industry Segmentsadvertising electronics photographyaerospace entertainment plasticsagriculture environmental + services ' precious +metalsautos financial + services publishingaviation food railroads banking forestry +productsbeverages freight real + estatebiotechnology health + care restaurantsbroadcasting industrial + products retailbuilding + material insurance rubberbusiness + services machinery ship buildingchemicals metals telecommunicationscomputers mining textilesconstruction nuclear + energy tobaccoconsumer + products office + equipment toysdefense +contracting personal +care + products travel services educational + services petroleum + productselectronic + publishing pharmaceuticals trucksutilitiesSubject Indicatorsair + force depression moneyantitrust divestiture nasd haltappointment dividend nasd resumebankruptcy earnings navyboycott economy new productbudget electionbusiness executive change newscabinet expansion newsbriefcapitol export prime + ratecareer government public offeringchg-naq import recessioncommodity inflation refinancingcongress insider + trading resignationcontract joint venture restructuringcorporate labor socialismcoup lawsuitcrime layoff spacedebt legislation strikedeficit market taxesdemocracy merger trademilitary unemploymentFigure 2: Keywords for Industry and Topic SegmentsCategorization TaskTopic assignment (NL)Topic assignment (Stats)Topic assic~nment (Stats + NL /Industry assignment (NL)Industry assignment (Stats}Industry assignment (Stats + NL)Recall.94.73.95.34.64.67Precision.61.79.65.18.50.46 All categories (NL) .74All categories (Stats + NL) .79 .64Figure 3: Overall Results3.1 Word  Weights  for Indust ry  Ass ignmentFor example, in the news categorization task describedearlier, natural language delivered the weakest perfor-mance relative to statistics on the assignment of indus-try categories.
Performance on topic assignment wasgenerally much higher using natural anguage, and com-pany name extraction and variation was handled usinga separate mechanism \[Rau, 1991\].
The two most ob-vious differences between topic assignment and industryassignment are:1.
It is generally easy to determine where the topic ofa story is expressed, either in the first sentence orby spotting certain words and phrases that are goodindicators, while the industries involved can appearalmost anywhere in a story,2.
The breadth of language that expresses topic ismuch narrower than the breadth required to handlethe different industries; for example, it is quite easyto identify texts dealing with bankruptcies, lawsuits,and mergers using a few key words and phrases (vo-cabularies of no more than 20 or 30 words per topic):but a single industry can be indicated by any num-ber of words or expressions, including names of spe-cific customers, products, or devices.For these reasons, statistics have the upper hand inthe identification of industries, and the first pass at theNLDB system used linguistic methods for topic assign-ment and statistical methods for industries.
This wa~unsatisfying, because we quickly came across errors inthe results that might have been prevented using simpleNLP methods, as well as places where the results couldhave helped to augment or correct linguistic knowledgeThe statistical methods, which will be described laterinvolve weighting individual words and phrases accord-ing to their value in distinguishing industries.
The most180obvious errors resulting from these methods were findinga good indicator in the wrong place (either in irrelevanttext or in background text), and finding a good indica-tor used in a different way.
These errors pervade the re-sults of statistical categorization, with the most obviousproblems coming when the results clearly derived fromthe misinterpretation f individual words and phrases.For example, the word brewing occurred 14 times in asample of about 11,000 news stories.
In 9 out of those 14cases, or 64%, the story was correctly categorized underthe industry BEVERAGES, which includes less than 1% ofthe stories.
By most any statistical metric, brewing isa strong indicator for BEVERAGES (better, in fact, thanbeer and beverages, although not quite as good as Pepsi).However, the statistical categorization method failed, forexample, on the following text, incorrectly assigning thetext to BEVERAGES:The issue first surfaced Monday whenDawes complained there is a "black hole" of in-formation about how Richards deposited statemoney while the S&L  crisis was brewing IThe word gas is not quite as good an indicator asbrewing--in 55% of occurrences it indicates PETROLEUPRODUCTS, and 11% of the time UTILITIES, with scat-tered other interpretations.
But gas is much more fre-quent than brewing, occurring 835 times in the sametraining sample where brewing occurred 14 times.
So,in terms of overall performance, knowing when gas is agood indicator of an industry can make more of a differ-ence.
The problem is with texts such as the following:Of  55 check-ups of the 17 patients, mild di-arrhea was reported during 2 percent of check-ups, nausea or vomiting in about 3 percent anda moderate increase in intestinal gas in about10 percent.While the problem with brewing above can easily besolved by using any simple method of filtering out ir-relevant ext (the sentence appears in the middle of astory about a political campaign), this is not the casewith gas.
The gas example, like many similar errors, ap-pears in relevant text describing the health effects of brancereal, which could be correctly categorized as HEALTHCARE and FOOD.Note also that it is difficult to compensate for errorsin individual word weights by using combined statisticalweights for categorization.
This point will he discussedmore in Section 5, but the main problem is that contentwords like patients, nausea and check-ups imply don'thave enough information content o act as good discrim-inators compared to gas.3.2 Recal l  and  Prec is ionWhile it is easy to spot places where statistics tend tointroduce erroneous categories, thus lowering precisionin examples uch as brewing and gas above, it is harderto understand why statistical methods also fail to pro-duce enough information to assign a category, thus pro-ducing low recall.
Since the task of assigning industriesaitalics addeddepends more on what businesses companies are in thanwhat an individual story is about, the information aboutthe industry is often localized, perhaps even in a singlemention of a company or product.
For example, thefollowing is a typical, though difficult, story about anexecutive change:.... James W. Nelson has been named vicepresident-manufacturing and distribution forthe household products group at Lehn & FinkProducts, maker of such well-known brands asLysol, Love My Carpet, Resolve, Chubs, Mop& Glo, Ogilvie, Minwax and Thompson's.Lehn & Fink Products, headquartered inMontvale, is a leading international marketerof household and do-it-yourself products.The human categorizer assigned the story to thecategories of CONSUNER PRODUCTS and PERSONAL CAREPRODUCTS, although the latter is probably an error.While CONSUMER PRODUCTS is the strongest category in-dicated statistically, from words such as household andbrands, it is still weakly indicated; in fact, it would bedifficult to get a statistical measure to admit CONSUMERPRODUCTS without also including RETAIL, BEVERAGES,BUILDING MATERIAL, and even TEXTILES, which areloosely coupled with terms such as brands, do-it-yourselfand carpet.
It is also quite difficult, because of the highindependent frequencies of the words, to identify house-hold products as a collocation or combination that shouldbe considered.The key to getting good recall and precision on textssuch as these is to consider the weights of the individualwords and phrases to determine what industries couldbe involved, but to use the structure of the texts tohelp determine where the industry information mightbe.
Phrases like X is a leading marketer of Y or Xis the maker of Y appear throughout news stories, andare sure indicators of industry information, even thoughthey do not point to any particular industry.
Linguisticapproaches probably won't help to guess that Love MyCarpet is a consumer product, but they can help to de-termine that the industry discriminators lie in the textfollowing patterns uch as X is the maker of Y. Statisticscan then guess the industries associated with Y.The NLP method used in NLDB associates categorieswith linguistic patterns.
We will next describe the pat-tern language, then explain how statistical methods canautomatically add simple patterns.4 Lex ico -Semant ic  Pat ternsIn SCISOR \[Jacobs and Rau, 1990\], MUC \[Jacobs etal., 1991; Krupka et al, 1991\], and other applications,we have found that lexically-driven pre-processing servesas a complement to parsing and semantic interpretation,both in identifying portions of relevant ext and in mark-ing the input text to make it easier to process.
Ourlexico-semantic pattern rules are quite similar to thosein CONSTRUE/TIS \[Hayes and Weinstein, 1990\], asso-ciating each pattern with an action rule that can ma-nipulate text or activate or de-activate a category.
This181type of knowledge structure has proven effective for topicidentification as well as other forms of pre-processing.Because the pattern matcher is designed as an efficient"trigger" mechanism and an aid in parsing, the patternsare mostly simple combinations of lexical categories.
Thepatterns largely adopt the language of regular expres-sions, including the following terms and operators:?
Lexical features that can be tested in a pattern?
Logical combination of lexical feature tests- -OR,AND , and NOT?
Wild cards?
Variable assignment (?X = )?
Grouping operators- -  <> for grouping, D for dis-junctive grouping?
Repet i t ion--  * for 0 or more, + for 1 or more?
Range- -  aNfor 0 toN,  -?-Nfor l toN?
Optional Const i tuents--  {} for optionalWhile this pattern language provides a tool for rec-ognizing linguistic constructs, most patterns for cate-gorization are simple lexical items, semantic ategories,or combinations.
For example, the word root dividendand the phrase holders of record are good indicators of aDIV IDEND story.Most topics have rules that include such sure-fire sin-gle words and phrases, along with some more complexpatterns.
For example, the following two rules help torecognize stories about mergers and acquisitions:(or tender merger) offer => C-TAKEOVER ;? "
C1  .
.
.
announced .
.
.
acquisition of ... C2J?C l=~cname~ * $announce-verb  * $merger-verb?
5 \[of with\] $ ?C2=~cname}=> (C-TAKEOVER (r-agent ?Cl) (r-target ?C2))In addition to helping to catch a broader ange of con-structs that indicate takeovers, the more complex pat-terns like the second one above can make preliminary as-signments of roles, which can greatly speed and aid pars-ing in systems that perform both parsing and categoriza-tion.
The ability to construct and add these more sophis-ticated patterns by hand is a major advantage, which ac-counts for much of the benefit of knowledge-based meth-ods over statistical means.
However, the more simplepatterns are required, the more labor-intensive this pro-cess can be, and the more manual tuning must be donein order to get accurate results.Statistical methods have the advantage of buildingrules automatically from a training set.
But, in orderto get the benefit of statistics, the methods used mustadd knowledge in the same form as the knowledge-basedrules, and must produce a clear result that is accessiblefor knowledge engineering.
In other words, the statis-tical methods themselves must be an aid rather than areplacement for knowledge acquisition.
The next sectiondescribes how this is accomplished.5 Stat is t ics  and  AcquisitionThe strategies for statistical training described here alluse  a "training" set of 11,500 news stories, includingabout 3,000,000 words, with human-assigned categoriesassigned to each story.
The "test" set used was anew sample of one day's news, or 700 stories including200,000 words.Many statistical methods in information retrieval useprobahilistic weights of individual terms \[Salton andMcGill, 1983\], where a term can be a single word, root,or combination of words.
The techniques we exploredinclude various weighting schemes, with the end goal be-ing to use heavily-weighted terms as the building blocksfor patterns.
A term can be weighted with respect o itsoverall relevance, or with respect o its ability to deter-mine a particular category.
The "pure" statistical resultsreported earlier summed the weights of all terms in a textwith respect o each industry category.Automating the process of acquiring lexico-semanticpatterns poses a number of distinct problems.
First,there has to be some means of distinguishing where in-dustry information might appear in a text.
Second, sin-gle words should he distinguished from phrases in certaincases where the individual words might be misleading.Third, the statistical methods must produce individualactions, not weights that must be combined to derive thefinal answer or answers.
Fourth, there has to be somegood way of determining when the statistical results werebound to introduce errors.All of these requirements come together to assure thatthe statistical methods can be used to improve existingsets of pattern-action rules, that manual methods willnot counteract he results of acquisition, and that thestatistical and NLP methods can interact gracefully.5.1 Ident i fy ing  Re levant  TextMany of the errors with statistical methods, especiallywhen single words and phrases are used for categoriza-tion, come from unusual occurrences in background orirrelevant exts, such as the brewing example earlier.
Asimilar case is the word Yankee which is an excellent in-dicator of NUCLEAR ENERGY (because of the New Hamp-shire Yankee Power Plant), except in an isolated clusterof articles about George Steinbrenner, the owner of theNew York Yankees.We tried two methods of correcting for such problems.First, we noted that most industry information is con-tained in the headline, first, and last paragraphs of texts,and tested using only these paragraphs for categoriza-tion.
Second, we tried a simple filter to score how mucheach paragraph was like a first or last paragraph, usingthe following calculation for the relevance weight of aterm:1000 d log 2 bWhere b is number of occurrences of each term in thefirst and last paragraphs of text, and d is the ratio oIoccurrences in these texts to all occurrences, minus aconstant.The following are some of the noteworthy results otthese tests:182?
A separate relevance filter produced some improve-ment over simple term weighting, about 3 points inprecision, and a larger improvement, about 5 points,when only "in-or-out" patterns were used.
This wasabove and beyond a comparable gain from consid-ering only headline, first, and last paragraphs.?
Almost all combinations of looking at first, last, andadditional relevant paragraphs ended up with aboutthe same results.
In other words, looking at onlyparagraphs with high relevance scores that were alsoat the beginning or end of a story produced aboutthe same results, combining recall and precision, aslooking at highly relevant paragraphs in addition tothe first and last paragraphs.
We settled on usingonly first and last paragraphs with relevant scoresbecause this simply minimizes the amount of textthat must be processed.?
Using only first and last paragraphs for training, aswell as categorizing, produced no improvement inresults.
We think that this is because the improve-ment from having a more accurate training sampleis neutralized by having less text to train on.
Thissuggests using a still larger training sample.These tests showed that a simple relevance filter pro-duced a consistent, small, advantage in accuracy overusing the whole text of each story.
However, surpris-ingly, it was hard to see any gain from considering themiddle parts of stories where those parts had high rel-evance scores.
This suggests that further work couldproduce better indicators of relevance that get industryinformation out of the middle parts without introducingmore extraneous categories.5.2 Co l locat ions  and  NamesStatistical methods looking for sure-fire indicators must,by their nature, consider overwhelming statistical evi-dence even when these indicators are relatively infre-quent.
Unfortunately, even when a word or phrase cor-relates with an industry 100% of the time in a sample,this does not mean that it will be a sure indicator in anew sample.
This is especially a problem with propernames and names of locations, but is also an issue withwords that occur frequently in collocations.Treating words as individual indicators when they re-ally are part of a name or collocation can hurt precisionby increasing the chance that the single word will appearindependently.
It can also hurt recall, because the com-bined evidence derived from a name or collocation can bestrong even when the individual words contribute little.As evidence of the problem with proper names, thewords Flint (a city in Michigan where General Motorsproduces cars and trucks), Donahue (the name of a pop-ular daytime TV show), and Warner (a communicationsconglomerate) are all good statistical indicators.
In fact,in the training sample of about a month's worth of news,Donahue was an indicator of BROADCASTING 18 out of 18times, making it a better indicator even than Pepsi.
Butin the one-day test sample, Donahue occurred only once,in a story about the president of Nike (the athletic shoecompany).
Flint occurred in a story about an art displayin the Michigan town, and Warner as a name unrelatedto the communications company.While accurate recognition of proper names is neces-sary for good precision, it is also a requirement for recall.A company may be involved in industries that are diffi-cult to get from either the parts of the name or the sur-rounding context.
For example, household is a weak indi-cator for CONSUMER PRODUCTS, but the company House-hold International is in the IIDUSTRIAL PRODUCTS cat-egory.
Similarly, Digital Equipment is in a different in-dustry from Digital Communications.
Since these namesare so important for industry assignment, we found thatthe best results came from handling all proper namesseparately and being much more lenient about when toadmit an industry name based on the name of a companythan for individual words and phrases.With other compounds, there were again problemswith both recall and precision.
Recall problems camefrom common words that have a special meaning whenconjoined, such as real estate--both real and estate fre-quently occur, and have no significance with respect oindustry, but real estate is a good indicator of the REALESTATE category.
Television is a weak indicator of severalindustry categories, but television viewers and televisionnetworks are strong indicators.We took a simple approach to handling such col-locations, by computing mutual information statistics\[Church et al, 1989\] for bigrams (two-word sequences)in the training corpus, and treating combinations witha high degree of mutual information as if they were sin-gle terms.
So real estate would be treated as an atom,the individual words not being considered for catego-rization.
This yielded fair results, but probably is not asgood a method as looking for discriminators specificallywhen the individual words are indicators of multiple cat-egories.The following are some of the key results from separateprocessing of names and collocations:?
Company name extraction was the best contribu-tor to accuracy, with a 10% improvement in re-call and no significant loss of precision over treat-ing company names as any other word.
This is anespecially compelling result because the individualcomponents of company names are often themselvesgood indicators, and because the size of the train-ing set is not nearly large enough to cover manyof the companies that occur in a given test (hencethe training data inherently miss a fair number ofcompanies).?
The use of bigrams yielded about a 6% gain in pre-cision with no real effect on recall.
We expect thatthe number of bigrams was not adequate to have amajor positive impact on recall, while the method ofignoring the individual component words can neu-tralize some of the positive effect.5.3 Set t ing  Word  Thresho ldsKnowledge-based methods aim at high-accuracy individ-ual patterns.
It is hard to balance these against weightedterms, so it is best to tune the statistics to identify sim-ple indicators rather than weights to be combined with183other weights.
This way, a rule can combine hand-codedknowledge with automatically-acquired data by lookingfor industry information in a particular place and get-ting the industry from a single indicator in that place,as in the company manufactures satellites.Because work in information retrieval \[Salton andMcGill, 1983\] has suggested that combinations ofweighted terms could be more accurate than single in-or-out assignments, we compared a number of differentweighting methods with a number of different methodsfor discriminating key indicators.
We found that, in gen-eral, the combination of weighted terms produced betterresults than simply taking the union of the industriesactivated by the "best" terms.
However, the results forthe best discrete assignment of industries to individualterms were very close to those of the weighted terms, andthe benefits of this approach--including the ability tointegrate statistics with knowledge-based methods, theidentification of important ambiguities in word mean-ing, and speed and simplicity--suggest that statisticalthresholds for individual terms, without any combiningof weights, is a good approach.We had to devise a statistical means of distinguish-ing only those terms that were very good indicators bythemselves of a particular category, without having tocompute a score for each paragraph.
This would nothave worked without the pre-processing of relevant ext,name and collocation extraction.
In fact, the perfor-mance of categorization using single in-or-out erms wasmore than 10% lower in precision than the combina-tion of weights without the pre-processing, but aboutthe same with the combined method.
The apparent ex-planation for this is that most of the error introducedin using single terms as discriminators comes from theconfusion of terms either in special combinations or inirrelevant ext.The following is the formula for weighting terms thatare individual indicators of a particular category:(200d)(log2 b)(log~ r)where b is number of times a term appears is a storyabout a particular category, d is the difference betweenthat number and the overall percentage of words in textsof that category, and r is the ratio of combined proba-bilities to the product of independent probabilities, themutual information statistic.With a threshold of 100, this weight identifies termsthat are good independent indicators of each topic.These terms can then be used automatically, either bythemselves or in combination with other terms, to createpatterns in the knowledge base.5.4 Combin ing  In format ionThe statistical pre-processing methods and calculationsof relevance weights and weights for category indicatorslay the groundwork for automatically constructing lin-guistic patterns for categorization.
Because these indi-vidual discriminators can be combined with hand-codedknowledge, the statistical recognition of these terms issufficient to augment a knowledge base automatically,and the combination of the hand-coded rules with thestatistical patterns is better than either alone (althoughit could always be argued that, with a little more work,the same rules could have been hand-coded).These results are not completely satisfying.
The sta-tistical acquisition method uses only a fraction of thepower of the pattern language, and the error rate of thesystem could still be reduced.
We tried three differenttypes of methods-finding exceptions, co-indicators, andmeta-indicators-to try to improve results using combina-tions rather than single term rules.
These three methodsare described as follows:Except ions :  Exceptions spot secondary terms thatwould override a category indicated by a "good"term, for example, if oil appears in a text aboutthe automobile industry rather than petroleum, itmight appear near motor or engine.Co- lnd icators :  Co-indicators spot combinations ofwords, where at least one was a "good" term, wherethe combination was a much better indicator thatthe single term.Meta - lnd icators :  Meta-indicators spot terms such asproduces and manufactures, which are not them-selves indicators of a particular industry, but oftenappear near terms that indicate an industry.For this process, we compiled a set of tables cover-ing, for each good discriminator, the words that ap-peared as neighbors of that term along with the num-ber of times those words appeared in texts about the"right" category vs. texts not about the category.
Thiswas a computation-intensive process for 3 million words,so much so that we had to reduce the size of the tableby considering only terms with moderate frequency--thehighest frequency terms are "stop" words, and the lowestfrequency terms are not good discriminators.The following are the major results of this analysis:?
Using exception lists to try to correct for precisionproblems turned out to be of surprisingly little value(about 2% precision).
While it is possible that dif-ferent methods would yield better results, it seemsthat the data on exceptions are just too sparse--itis much easier to get good data on positive examplesfrom the training set than negative xamples.?
Using co-indicators, or second order relations, ap-peared to be much more promising than exceptions.Like the use of bigrams, this produced only a smalleffect (about 2% in both recall and precision), butany technique that improves recall without a loss ofprecision is worth exploring.?
The use of recta-indicators, while also not produc-ing a major effect on results, looks like the mostpromising method.
Like co-indicators, these meta-indicators rely mainly on positive examples from thetext, but they have the additional advantage of be-ing able to use much larger volumes of data.6 Fer t i le  Areas  fo r  Future  ResearchWhile the improvements in overall system performanceon this task came as a result of many months of en-gineering and experiments, the most promising aspect184of this evaluation is the prospect for new areas wherestatistics can help natural anguage and vice versa.
Wehave identified three critical research areas that are likelyto improve both system performance and general NLPperformance.The first major area of research is in discourse, or textstructure analysis.
This experiment showed that the firstand last paragraphs of a news story give much more accu-rate information than others, and that a general assess-ment of relevance of a paragraph serves as a good filterfor the extraction of information from that paragraph.However, this simple filter falls far short of really identi-fying where the information in a story lies.
For example,in many stories the "last" paragraph really comes in themiddle of a text, with some additional material comingat the end because of incidental information or strangeediting.
Similarly, in more complicated texts, there canbe more than one introductory paragraph, multiple con-eluding paragraphs, and other critical information in themiddle.
Statistical techniques are a very weak means ofguessing this type of structural information.
We expectthat we can improve the results slightly with some moresophisticated discourse analysis; and, perhaps more im-portantly, this type of evaluation can measure how wella structural theory of text can perform.The second area to explore is developing eneralizedpatterns from detailed statistical analysis.
The first-order logarithmic measures used for acquisition here areoverly simplistic, and assume that relationships betweenwords and categories are basically independent.
This as-sumption is false, because the words are a manifestationof concepts and linguistic relations in the text that thestatistics are ignoring.
We are investigating a varietyof more complex means, including multivariate discrimi-nant analysis, that help to determine when, for example,the effect of combining satellite with weapon is really aneffect of combining satellite with any military concept.The third, more linguistic, area is in identifying the-matic roles.
In meta-indicators described earlier, we arereally identifying potential function words in the text,such as produces or manufactures.
Since the use of theseterms in category assignment really assumes that anycompany or industry appearing around them is involvedin the function or operation, it should be possible to usemore detailed parsing to check whether this assumptionfits linguistically, and to use the statistical analysis toacquire functional or thematic relations that can help inmore detailed analysis.7 Summary and ConclusionThis paper has addressed the area where statistical andlinguistic analysis come together with an applicationfocus - -  the assignment of categories to news stories,particularly the identification of topics and industries.The experiments reported here show that using statis-tical methods to acquire simple lexical patterns helpsknowledge-based processing and leads to a substantialimprovement in overall system performance.
In addition,this method promises to ease the burden of hand-codingknowledge for each application, by automatically identi-fying the significant erms and combinations of terms touse knowledge-intensive NLP applications.Re ferences\[Church et al, 1989\] K. Church, W. Gale, P. Hanks, andD.
Hindle.
Parsing, word associations, and predicate-argument relations.
In Proceedings of the Interna-tional Workshop on Parsing Technologies, CarnegieMellon University, 1989.\[Hayes and Weinstein, 1990\] Philip J. Hayes andSteven P. Weinstein.
CONSTRUE/TIS: A system forcontent-based indexing of a database of news stories.In Proceedings of the Second Annual Conference onInnovative Applications of Artificial Intelligence, May1990.\[Jacobs and Rau, 1990\] Paul Jacobs and Lisa Ran.SCISOR: Extracting information from on-line news.Communications of the Association for ComputingMachinery, 33(11):88-97, November 1990.\[Jacobs et al, 1991\] Paul S. Jacobs, George R. Krupka,and Lisa F. Rau.
Lexico-semantic pattern matchingas a companion to parsing in text understanding.
In,Fourth DARPA Speech and Natural Language Work-shop, pages 337-342, San Mateo, CA, February 1991.Morgan-Kaufmann.\[Krupka et al, 1991\] George R. Krupka, Paul S. Jacobs,Lisa F. Rau, and Lucja Iwafiska.
Description of theGE NLToolset system as used for MUC-3.
In Proceed-ings of the Third Message Understanding Conference(MUC-3), San Mateo, CA, May 1991.
Morgan Kauf-mann Publishers.\[Kuhns, 1990\] Robert Kuhns.
News analysis: A nat-ural language application to text processing.
InPaul S. Jacobs, editor, Text-Based Intelligent Sys-tems: Current Research in Text Analysis, InformationExtraction, and Retrieval, pages 147-150.
September1990.
GE Research and Development Center ReportCRD90/198.\[Ran and Jacobs, 1991\] Lisa F. Rau and Paul S. Jacobs.Creating segmented atabases from free text for textretrieval.
In Proceedings of the 14th InternationalConference on Research and Development in Infor-mation Retrieval, pages 337-346, October 1991.\[Ran, 1991\] Lisa F. Ran.
Extracting company namesfrom text.
In Tim Finin, editor, Sixth IEEE Con-ference on Artificial Intelligence Applications.
IEEEComputer Society Press, Miami Beach, Florida,February 1991.\[Salton and McGill, 1983\] G. Salton and M. McGill.An Introduction to Modern Information Retrieval.McGraw-Hill, New York, 1983.\[Tong et al, 1986\] Richard M. Tong, L. A. Appelbaum,V.
N. Askman, and J. F. Cunningham.
RUBRIC III:An object-oriented expert system for information re-trieval.
In Proceedings of the 2nd Annual IEEE Sym-posium on Expert Systems in Government, pages 106-115, Washington, DC., October 1986.
IEEE ComputerSociety Press.185
