Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 5?8,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMultilingual, Efficient and Easy NLP Processing with IXA PipelineRodrigo AgerriIXA NLP GroupUniv.
of the Basque CountryUPV/EHUDonostia San-Sebasti?anrodrigo.agerri@ehu.esJosu BermudezDeusto Institute of TechnologyDeustotechUniv.
of DeustoBilbaojosu.bermudez@deusto.esGerman RigauIXA NLP GroupUniv.
of the Basque CountryUPV/EHUDonostia-San Sebasti?angerman.rigau@ehu.esAbstractIXA pipeline is a modular set of Natural Lan-guage Processing tools (or pipes) which pro-vide easy access to NLP technology.
It aims atlowering the barriers of using NLP technologyboth for research purposes and for small indus-trial developers and SMEs by offering robustand efficient linguistic annotation to both re-searchers and non-NLP experts.
IXA pipelinecan be used ?as is?
or exploit its modularityto pick and change different components.
Thispaper describes the general data-centric archi-tecture of IXA pipeline and presents competi-tive results in several NLP annotations for En-glish and Spanish.1 IntroductionMany Natural Language Processing (NLP) applica-tions demand some basic linguistic processing (Tok-enization, Part of Speech (POS) tagging, Named EntityRecognition and Classification (NER), Syntactic Pars-ing, Coreference Resolution, etc.)
to be able to furtherundertake more complex tasks.
Generally, NLP anno-tation is required to be as accurate and efficient as pos-sible and existing tools, quite righly, have mostly fo-cused on performance.
However, this generally meansthat NLP suites and tools usually require researchersto do complex compilation/installation/configuration inorder to use such tools.
At the same time, in the indus-try, there are currently many Small and Medium Enter-prises (SMEs) offering services that one way or anotherdepend on NLP annotations.In both cases, in research and industry, acquiring, de-ploying or developing such base qualifying technolo-gies is an expensive undertaking that redirects theiroriginal central focus: In research, much time is spentin the preliminaries of a particular research experimenttrying to obtain the required basic linguistic annota-tion, whereas in an industrial environment SMEs seetheir already limited resources taken away from of-fering products and services that the market demands.IXA pipeline provides ready to use modules to per-form efficient and accurate linguistic annotation to al-low users to focus on their original, central task.
Whendesigning the architecture, we took several decisionswith respect to what IXA pipeline had to be:Simple and ready to use: Every module of the IXApipeline can be up an running after two simple steps.Portable: The modules come with ?all batteries in-cluded?
which means that no classpath configurationsor installing of any third-party dependencies is re-quired.
The modules will will run on any platform aslong as a JVM 1.7+ and/or Python 2.7 are available.Modular: Unlike other NLP toolkits, which often arebuilt in a monolithic architecture, IXA pipeline is builtin a data centric architecture so that modules can bepicked and changed (even from other NLP toolkits).The modules behave like Unix pipes, they all take stan-dard input, do some annotation, and produce standardoutput which in turn is the input for the next module.The data-centric architecture of IXA pipeline meansthat any module is highly independent and can there-fore be used with other tools from other toolkits if re-quired.Efficient: Piping the tokenizer (250K words per sec-ond) POS tagger and lemmatizer all in one processannotates over 5K words/second.
The NERC mod-ule annotates over 5K words/second.
In a multi-coremachine, these times are dramatically reduced due tomulti-threading.Multilingual: Currently we offer NLP annotations forboth English and Spanish, but other languages are be-ing included in the pipeline.
Tokenization alreadyworks for several languages, including Dutch, French,Italian, German, Spanish and English.Accurate: For example, POS tagging and NERC forEnglish and Spanish are comparable with other stateof the art systems, as it is the coreference resolutionmodule for English.Apache License 2.0: IXA Pipeline is licensed underthe Apache License 2.0, an open-source license that fa-cilitates source code use, distribution and integration,also for commercial purposes.1Next section describes the IXA pipeline architecture,section 3 the modules so far developed.
Wheneveravailable, we also present empirical evaluation.
Sec-tion 4 describes the various ways of using the tools.Finally, section 5 discusses some concluding remarks.1http://www.apache.org/licenses/LICENSE-2.0.html52 ArchitectureIXA pipeline is primarily conceived as a set ofready to use tools that can provide efficient andaccurate linguistic annotation without any installa-tion/configuration/compilation effort.
As in Unix-likeoperative systems, IXA pipeline consists of a set of pro-cesses chained by their standard streams, in a way thatthe output of each process feeds directly as input to thenext one.
The Unix pipeline metaphor has been ap-plied for NLP tools by adopting a very simple and wellknown data centric architecture, in which every mod-ule/pipe is interchangeable for another one as long as ittakes and produces the required data format.The data format in which both the input and output ofthe modules needs to be formatted to represent and fil-ter linguistic annotations is KAF (Bosma et al., 2009).KAF is a language neutral annotation format represent-ing both morpho-syntactic and semantic annotation in astructured format.
KAF was originally designed in theKyoto European project2, but it has since been in con-tinuous development3.
Our Java modules all use kaflib4library for easy integration.Every module in the IXA pipeline, except the coref-erence resolution, is implemented in Java, and re-quires Java JDK1.7+ to compile.
The integration ofthe Java modules in the IXA pipeline is performed us-ing Maven5.
Maven is used to take care of classpathsconfigurations and third-party tool dependencies.
Thismeans that the binaries produced and distributed willwork off-the-self.
The coreference module uses pip6to provide an easy, one step installation.
If the sourcecode of an ixa-pipe-$module is cloned from the remoterepository, one command to compile and have ready thetools will suffice.Some modules in IXA pipeline provide linguistic an-notation based on probabilistic supervised approachessuch as POS tagging, NER and Syntactic Parsing.
IXApipeline uses two well known machine learning algo-rithms, namely, Maximum Entropy and the Percep-tron.
Both Perceptron (Collins, 2002; Collins, 2003)and Maximum Entropy models (Ratnaparkhi, 1999) areadaptable algorithms which have been successfully ap-plied to NLP tasks such as POS tagging, NER andParsing with state of the art results.
To avoid dupli-cation of efforts, IXA pipeline uses the already avail-able open-source Apache OpenNLP API7to train POS,NER and parsing probabilistic models using these twoapproaches.2http://kyoto-project.eu3http://www.opener-project.org/kaf/4https://github.com/ixa-ehu/kaflib5http://maven.apache.org/6https://pypi.python.org/pypi/pip7http://opennlp.apache.org3 PipesIXA pipeline currently provides the following linguis-tic annotations: Sentence segmentation, tokenization,Part of Speech (POS) tagging, Lemmatization, NamedEntity Recognition and Classification (NER), Con-stituent Parsing and Coreference Resolution.
Everymodule works for English and Spanish and is imple-mented in Java/Maven as described above.
The onlyexception is the coreference resolution module, whichcurrently is available in Python 2.7 and for English only(Spanish version will comme soon).
We will now de-scribe which annotation services are provided by eachmodule of the pipeline.3.1 ixa-pipe-tokThis module provides rule-based Sentence Segmenta-tion and Tokenization for French, Dutch, English, Ital-ian and Spanish.
It produces tokenized and segmentedtext in KAF, running text and CoNLL formats.
Therules are originally based on the Stanford English To-kenizer8, but with substantial modifications and addi-tions.
These include tokenization for other languagessuch as French and Italian, normalization accordingthe Spanish Ancora Corpus (Taul?e et al., 2008), para-graph treatment, and more comprehensive gazeteersof non breaking prefixes.
The tokenizer depends ona JFlex9specification file which compiles in secondsand performs at a very reasonable speed (around 250Kword/second, and much quicker with Java multithread-ing).3.2 ixa-pipe-posixa-pipe-pos provides POS tagging and lemmatizationfor English and Spanish.
We have obtained the bestresults so far with the same featureset as in Collins?s(2002) paper.
Perceptron models for English have beentrained and evaluated on the WSJ treebank using theusual partitions (e.g., as explained in Toutanova et al.(2003).
We currently obtain a performance of 97.07%vs 97.24% obtained by Toutanova et al., (2003)).
ForSpanish, Maximum Entropy models have been trainedand evaluated using the Ancora corpus; it was ran-domly divided in 90% for training and 10% for test-ing.
This corresponds to 440K words used for train-ing and 70K words for testing.
We obtain a perfor-mance of 98.88% (the corpus partitions are availablefor reproducibility).
Gim?enez and Marquez (2004) re-port 98.86%, although they train and test on a differentsubset of the Ancora corpus.Lemmatization is currently performed via 3 differentdictionary lookup methods: (i) Simple Lemmatizer: Itis based on HashMap lookups on a plain text dictionary.Currently we use dictionaries from the LanguageToolproject10under their distribution licenses.
The English8http://www-nlp.stanford.edu/software/tokenizer.shtml9http://jflex.de/10http://languagetool.org/6dictionary contains 300K lemmas whereas the Spanishprovides over 600K; (ii) Morfologik-stemming11: TheMorfologik library provides routines to produce binarydictionaries, from dictionaries such as the one used bythe Simple Lemmatizer above, as finite state automata.This method is convenient whenever lookups on verylarge dictionaries are required because it reduces thememory footprint to 10% of the memory required forthe equivalent plain text dictionary; and (iii) We alsoprovide lemmatization by lookup in WordNet-3.0 (Fell-baum and Miller, 1998) via the JWNL API12.
Note thatthis method is only available for English.3.3 ixa-pipe-nercMost of the NER systems nowdays consist of languageindependent systems (sometimes enriched with gaze-teers) based on automatic learning of statistical mod-els.
ixa-pipe-nerc provides Named Entity Recogni-tion (NER) for English and Spanish.
The named en-tity types are based on the CONLL 200213and 200314tasks which were focused on language-independent su-pervised named entity recognition (NER) for four typesof named entities: persons, locations, organizations andnames of miscellaneous entities that do not belong tothe previous three groups.
We currently provide twovery fast language independent models using a rathersimple baseline featureset (e.g., similar to that of Cur-ran and Clark (2003), except POS tag features).For English, perceptron models have been trainedusing CoNLL 2003 dataset.
We currenly obtain 84.80F1 which is coherent with other results reported withthese features (Clark and Curran, 2003; Ratinov andRoth, 2009).
The best Stanford NER model reportedon this dataset achieves 86.86 F1 (Finkel et al., 2005),whereas the best system on this dataset achieves 90.80F1 (Ratinov and Roth, 2009), using non local featuresand substantial external knowledge.For Spanish we currently obtain best results train-ing Maximum Entropy models on the CoNLL 2002dataset.
Our best model obtains 79.92 F1 vs 81.39F1 (Carreras et al., 2002), the best result so far on thisdataset.
Their result uses external knowledge and with-out it, their system obtains 79.28 F1.3.4 ixa-pipe-parseixa-pipe-parse provides statistical constituent parsingfor English and Spanish.
Maximum Entropy modelsare trained to build shift reduce bottom up parsers (Rat-naparkhi, 1999) as provided by the Apache OpenNLPAPI.
Parsing models for English have been trained us-ing the Penn treebank and for Spanish using the Ancoracorpus (Taul?e et al., 2008).Furthermore, ixa-pipe-parse provides two methodsof HeadWord finders: one based on Collins?
head rules11https://github.com/morfologik/morfologik-stemming12http://jwordnet.sourceforge.net/13http://www.clips.ua.ac.be/conll2002/ner/14http://www.clips.ua.ac.be/conll2003/ner/as defined in his PhD thesis (1999), and another onebased on Stanford?s parser Semantic Head Rules15.The latter are a modification of Collins?
head rules ac-cording to lexical and semantic criteria.
These headrules are particularly useful for the Coreference reso-lution module and for projecting the constituents intodependency graphs.As far as we know, and although previous ap-proaches exist (Cowan and Collins, 2005), ixa-pipe-parse provides the first publicly available statisticalparser for Spanish.3.5 Coreference ResolutionThe module of coreference resolution included in theIXA pipeline is loosely based on the Stanford MultiSieve Pass system (Lee et al., 2013).
The module takesevery linguistic information it requires from the KAFlayers annotated by all the previously described mod-ules.
The system consists of a number of rule-basedsieves.
Each sieve pass is applied in a deterministicmanner, reusing the information generated by the pre-vious sieve and the mention processing.
The order inwhich the sieves are applied favours a highest precisionapproach and aims at improving the recall with the sub-sequent application of each of the sieve passes.
Thisis illustrated by the evaluation results of the CoNLL2011 Coreference Evaluation task (Lee et al., 2013), inwhich the Stanford?s system obtained the best results.So far we have evaluated our module on the CoNLL2011 testset and we are a 5% behind the Stanford?s sys-tem (52.8 vs 57.6 CoNLL F1), the best on that task (Leeet al., 2013).
It is interesting that in our current imple-mentation, mention-based metrics are favoured (CEAFand B3).
Still, note that these results are comparablewith the results obtained by the best CoNLL 2011 par-ticipants.
Currently the module performs coreferenceresolution only for English, although a Spanish versionwill be coming soon.4 Related WorkOther NLP toolkits exist providing similar or more ex-tensive functionalities than the IXA pipeline tools, al-though not many of them provide multilingual support.GATE (Cunningham, 2002) is an extensive frameworksupporting annotation of text.
GATE has some capacityfor wrapping Apache UIMA components16, so shouldbe able to manage distributed NLP components.
How-ever, GATE is a very large and complex system, with acorresponding steep learning curve.Freeling (Padr?o and Stanilovsky, 2012) providesmultilingual processing for a number of languages,incluing Spanish and English.
As opposed to IXApipeline, Freeling is a monolithic toolkit written in C++which needs to be compiled natively.
The Stanford15http://www-nlp.stanford.edu/software/lex-parser.shtml16http://uima.apache.org/7CoreNLP17is a monolithic suite, which makes it dif-ficult to integrate other tools in its chain.IXA pipeline tools can easily be used piping the in-put with the output of another too, and it is also pos-sible to easily replace or extend the toolchain with athird-party tool.
IXA pipeline is already being used todo extensive parallel processing in the FP7 Europeanprojects OpeNER18and NewsReader19.5 Conclusion and Future WorkIXA pipeline provides a simple, efficient, accurate andready to use set of NLP tools.
Its modularity and datacentric architecture makes it flexible to pick and changeor integrate new linguistic annotators.
Currently we of-fer linguistic annotation for English and Spanish, butmore languages are being integrated.
Furthermore,other annotations such as Semantic Role Labelling andNamed Entity Disambiguation are being included inthe pipeline following the same principles.Additionally, current integrated modules are be-ing improved: both on the quality and variety ofthe probabilistic models, and on specific issues suchas lemmatization, and treatment of time expressions.Finally, we are adding server-mode execution intothe pipeline to provide faster processing.
IXApipeline is publicly available under Apache 2.0 license:http://adimen.si.ehu.es/web/ixa-pipes.AcknowledgementsTThis work has been supported by the OpeNER FP7project under Grant No.
296451, the FP7 NewsReaderproject, Grant No.
316404, and by the SKATER Span-ish MICINN project No TIN2012-38584-C06-01.
Thework of Josu Bermudez on coreference resolution issupported by a PhD Grant of the University of Deusto(http://www.deusto.es).ReferencesWauter Bosma, Piek Vossen, Aitor Soroa, GermanRigau, Maurizio Tesconi, Andrea Marchetti, Mon-ica Monachini, and Carlo Aliprandi.
2009.
Kaf: ageneric semantic annotation format.
In Proceedingsof the GL2009 Workshop on Semantic Annotation.X.
Carreras, L. Marquez, and L. Padro.
2002.
Namedentity extraction using AdaBoost.
In proceedingsof the 6th conference on Natural language learning-Volume 20, pages 1?4.Stephen Clark and James Curran.
2003.
Language In-dependent NER using a Maximum Entropy Tagger.In Proceedings of the Seventh Conference on Nat-ural Language Learning (CoNLL-03), pages 164?167, Edmonton, Canada.17http://nlp.stanford.edu/software/corenlp.shtml18http://www.opener-project.org19http://www.newsreader-project.euMichael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
Computational lin-guistics, 29(4):589?637.Brooke Cowan and Michael Collins.
2005.
Mor-phology and reranking for the statistical parsing ofspanish.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 795?802.Association for Computational Linguistics.Hamish Cunningham.
2002.
Gate, a general architec-ture for text engineering.
Computers and the Hu-manities, 36(2):223?254.C.
Fellbaum and G. Miller, editors.
1998.
Wordnet: AnElectronic Lexical Database.
MIT Press, Cambridge(MA).J.
R. Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by gibbs sampling.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, pages 363?370.Jes?us Gim?enez and Lluis Marquez.
2004.
Svmtool: Ageneral pos tagger generator based on support vectormachines.
In In Proceedings of the 4th InternationalConference on Language Resources and Evaluation.Citeseer.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, pages 1?54, January.Llu?
?s Padr?o and Evgeny Stanilovsky.
2012.
Freeling3.0: Towards wider multilinguality.
In Proceedingsof the Language Resources and Evaluation Confer-ence (LREC 2012), Istanbul, Turkey, May.
ELRA.L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning, page 147155.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
Machinelearning, 34(1-3):151?175.Mariona Taul?e, Maria Ant`onia Mart?
?, and Marta Re-casens.
2008.
Ancora: Multilevel annotated corporafor catalan and spanish.
In LREC.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of HLT-NAACL, pages 252?259.8
