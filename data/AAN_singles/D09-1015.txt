Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 141?150,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPNested Named Entity RecognitionJenny Rose Finkel and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{jrfinkel|manning}@cs.stanford.eduAbstractMany named entities contain other namedentities inside them.
Despite this fact, thefield of named entity recognition has al-most entirely ignored nested named en-tity recognition, but due to technological,rather than ideological reasons.
In this pa-per, we present a new technique for rec-ognizing nested named entities, by usinga discriminative constituency parser.
Totrain the model, we transform each sen-tence into a tree, with constituents for eachnamed entity (and no other syntactic struc-ture).
We present results on both news-paper and biomedical corpora which con-tain nested named entities.
In three outof four sets of experiments, our modeloutperforms a standard semi-CRF on themore traditional top-level entities.
At thesame time, we improve the overall F-scoreby up to 30% over the flat model, which isunable to recover any nested entities.1 IntroductionNamed entity recognition is the task of finding en-tities, such as people and organizations, in text.Frequently, entities are nested within each other,such as Bank of China and University of Wash-ington, both organizations with nested locations.Nested entities are also common in biomedicaldata, where different biological entities of inter-est are often composed of one another.
In theGENIA corpus (Ohta et al, 2002), which is la-beled with entity types such as protein and DNA,roughly 17% of entities are embedded within an-other entity.
In the AnCora corpus of Spanish andCatalan newspaper text (Mart??
et al, 2007), nearlyhalf of the entities are embedded.
However, workon named entity recognition (NER) has almost en-tirely ignored nested entities and instead chosen tofocus on the outermost entities.We believe this has largely been for practical,not ideological, reasons.
Most corpus designershave chosen to skirt the issue entirely, and haveannotated only the topmost entities.
The widelyused CoNLL (Sang and Meulder, 2003), MUC-6,and MUC-7 NER corpora, composed of Americanand British newswire, are all flatly annotated.
TheGENIA corpus contains nested entities, but theJNLPBA 2004 shared task (Collier et al, 2004),which utilized the corpus, removed all embeddedentities for the evaluation.
To our knowledge, theonly shared task which has included nested enti-ties is the SemEval 2007 Task 9 (Ma?rquez et al,2007b), which used a subset of the AnCora corpus.However, in that task all entities corresponded toparticular parts of speech or noun phrases in theprovided syntactic structure, and no participant di-rectly addressed the nested nature of the data.Another reason for the lack of focus on nestedNER is technological.
The NER task arose in thecontext of the MUC workshops, as small chunkswhich could be identified by finite state modelsor gazetteers.
This then led to the widespreaduse of sequence models, first hidden Markov mod-els, then conditional Markov models (Borthwick,1999), and, more recently, linear chain conditionalrandom fields (CRFs) (Lafferty et al, 2001).
Allof these models suffer from an inability to modelnested entities.In this paper we present a novel solution to theproblem of nested named entity recognition.
Ourmodel explicitly represents the nested structure,allowing entities to be influenced not just by thelabels of the words surrounding them, as in a CRF,but also by the entities contained in them, and inwhich they are contained.
We represent each sen-tence as a parse tree, with the words as leaves, andwith phrases corresponding to each entity (and anode which joins the entire sentence).
Our treeslook just like syntactic constituency trees, such asthose in the Penn TreeBank (Marcus et al, 1993),141ROOTPROTPROTNNPEBP2PROTNNalphaNNA1,,PROTNNalphaNNB1,,CCandPROTNNalphaNNB2NNSproteinsVBDboundDTtheDNAPROTNNPEBP2NNsiteINwithinDTtheDNANNmousePROTNNGM-CSFNNpromoter..Figure 1: An example of our tree representation over nested named entities.
The sentence is from theGENIA corpus.
PROT is short for PROTEIN.but they tend to be much flatter.
This model allowsus to include parts of speech in the tree, and there-fore to jointly model the named entities and thepart of speech tags.
Once we have converted oursentences into parse trees, we train a discrimina-tive constituency parser similar to that of (Finkelet al, 2008).
We found that on top-level enti-ties, our model does just as well as more conven-tional methods.
When evaluating on all entitiesour model does well, with F-scores ranging fromslightly worse than performance on top-level only,to substantially better than top-level only.2 Related WorkThere is a large body of work on named en-tity recognition, but very little of it addressesnested entities.
Early work on the GENIA cor-pus (Kazama et al, 2002; Tsuruoka and Tsujii,2003) only worked on the innermost entities.
Thiswas soon followed by several attempts at nestedNER in GENIA (Shen et al, 2003; Zhang etal., 2004; Zhou et al, 2004) which built hiddenMarkov models over the innermost named enti-ties, and then used a rule-based post-processingstep to identify the named entities containing theinnermost entities.
Zhou (2006) used a more elab-orate model for the innermost entities, but thenused the same rule-based post-processing methodon the output to identify non-innermost entities.Gu (2006) focused only on proteins and DNA, bybuilding separate binary SVM classifiers for inner-most and outermost entities for those two classes.Several techniques for nested NER in GENIAwhere presented in (Alex et al, 2007).
Their firstapproach was to layer CRFs, using the output ofone as the input to the next.
For inside-out lay-ering, the first CRF would identify the innermostentities, the next layer would be over the wordsand the innermost entities to identify second-levelentities, etc.
For outside-in layering the first CRFwould identify outermost entities, and then succes-sive CRFs would identify increasingly nested en-tities.
They also tried a cascaded approach, withseparate CRFs for each entity type.
The CRFswould be applied in a specified order, and theneach CRF could utilize features derived from theoutput of previously applied CRFs.
This techniquehas the problem that it cannot identify nested en-tities of the same type; this happens frequently inthe data, such as the nested proteins at the begin-ning of the sentence in Figure 1.
They also tried ajoint labeling approach, where they trained a sin-gle CRF, but the label set was significantly ex-panded so that a single label would include all ofthe entities for a particular word.
Their best resultswhere from the cascaded approach.Byrne (2007) took a different approach, on his-torical archive text.
She modified the data by con-catenating adjacent tokens (up to length six) intopotential entities, and then labeled each concate-nated string using the C&C tagger (Curran andClark, 1999).
When labeling a string, the ?previ-ous?
string was the one-token-shorter string con-taining all but the last token of the current string.For single tokens the ?previous?
token was thelongest concatenation starting one token earlier.SemEval 2007 Task 9 (Ma?rquez et al, 2007b)included a nested NER component, as well asnoun sense disambiguation and semantic role la-beling.
However, the parts of speech and syn-tactic tree were given as part of the input, andnamed entities were specified as corresponding tonoun phrases in the tree, or particular parts ofspeech.
This restriction substantially changes thetask.
Two groups participated in the shared task,but only one (Ma?rquez et al, 2007a) worked onthe named entity component.
They used a multi-label AdaBoost.MH algorithm, over phrases in the142DNAparent=ROOTNNparent=DNA,grandparent=ROOTmouse@DNAparent=ROOT,prev=NN,first=PROTPROTparent=DNA,grandparent=ROOTNNparent=PROT,grandparent=DNAGM-CSFNNparent=DNA,grandparent=ROOTpromoterFigure 2: An example of a subtree after it has been annotated and binarized.
Features are computed overthis representation.
An @ indicates a chart parser active state (incomplete constituent).parse tree which, based on their labels, could po-tentially be entities.Finally, McDonald et al (2005) presented atechnique for labeling potentially overlapping seg-ments of text, based on a large margin, multilabelclassification algorithm.
Their method could beused for nested named entity recognition, but theexperiments they performed were on joint (flat)NER and noun phrase chunking.3 Nested Named Entity Recognition asParsingOur model is quite simple ?
we represent each sen-tence as a constituency tree, with each named en-tity corresponding to a phrase in the tree, alongwith a root node which connects the entire sen-tence.
No additional syntactic structure is rep-resented.
We also model the parts of speech aspreterminals, and the words themselves as theleaves.
See Figure 1 for an example of a namedentity tree.
Each node is then annotated with bothits parent and grandparent labels, which allowsthe model to learn how entities nest.
We bina-rize our trees in a right-branching manner, andthen build features over the labels, unary rules,and binary rules.
We also use first-order horizon-tal Markovization, which allows us to retain someinformation about the previous node in the bina-rized rule.
See Figure 2 for an example of an an-notated and binarized subtree.
Once each sentencehas been converted into a tree, we train a discrimi-native constituency parser, based on (Finkel et al,2008).It is worth noting that if you use our model ondata which does not have any nested entities, thenit is precisely equivalent to a semi-CRF (Sarawagiand Cohen, 2004; Andrew, 2006), but with nolength restriction on entities.
Like a semi-CRF, weare able to define features over entire entities ofarbitrary length, instead of just over a small, fixedwindow of words like a regular linear chain CRF.We model part of speech tags jointly with thenamed entities, though the model also works with-out them.
We determine the possible part ofspeech tags based on distributional similarity clus-ters.
We used Alexander Clarke?s software,1 basedon (Clark, 2003), to cluster the words, and thenallow each word to be labeled with any part ofspeech tag seen in the data with any other wordin the same cluster.
Because the parts of speechare annotated with the parent (and grandparent)labels, they determine what, if any, entity typesa word can be labeled with.
Many words, such asverbs, cannot be labeled with any entities.
We alsolimit our grammar based on the rules observed inthe data.
The rules whose children include part ofspeech tags restrict the possible pairs of adjacenttags.
Interestingly, the restrictions imposed by thisjoint modeling (both observed word/tag pairs andobserved rules) actually result in much faster infer-ence (and therefore faster train and test times) thana model over named entities alone.
This is differ-ent from most work on joint modeling of multiplelevels of annotation, which usually results in sig-nificantly slower inference.3.1 Discriminative Constituency ParsingWe train our nested NER model using the sametechnique as the discriminatively trained, condi-tional random field-based, CRF-CFG parser of(Finkel et al, 2008).
The parser is similar to a1http://www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html143Local Features Pairwise Featureslabeli distsimi + distsimi?1 + labeli labeli?1 + labeliwordi + labeli shapei + shapei+1 + labeli wordi + labeli?1 + labeliwordi?1 + labeli shapei?1 + shapei + labeli wordi?1 + labeli?1 + labeliwordi+1 + labeli wordi?1 + shapei + labeli wordi+1 + labeli?1 + labelidistsimi + labeli shapei + wordi+1 + labeli distsimi + labeli?1 + labelidistsimi?1 + labeli words in a 5 word window distsimi?1 + labeli?1 + labelidistsimi+1 + labeli prefixes up to length 6 distsimi+1 + labeli?1 + labelishapei + labeli suffixes up to length 6 distsimi?1 + distsimi + labeli?1 + labelishapei?1 + labeli shapei + labeli?1 + labelishapei+1 + labeli shapei?1 + labeli?1 + labelishapei+1 + labeli?1 + labelishapei?1 + shapei + labeli?1 + labelishapei?1 + shapei+1 + labeli?1 + labeliTable 1: The local and pairwise NER features used in all of our experiments.
Consult the text for a fulldescription of all features, which includes feature classes not in this table.chart-based PCFG parser, except that instead ofputting probabilities over rules, it puts clique po-tentials over local subtrees.
These unnormalizedpotentials know what span (and split) the rule isover, and arbitrary features can be defined over thelocal subtree, the span/split and the words of thesentence.
The inside-outside algorithm is run overthe clique potentials to produce the partial deriva-tives and normalizing constant which are neces-sary for optimizing the log likelihood.
Optimiza-tion is done by stochastic gradient descent.The only real drawback to our model is run-time.
The algorithm is O(n3) in sentence length.Training on all of GENIA took approximately 23hours for the nested model and 16 hours for thesemi-CRF.
A semi-CRF with an entity length re-striction, or a regular CRF, would both have beenfaster.
At runtime, the nested model for GENIAtagged about 38 words per second, while the semi-CRF tagged 45 words per second.
For compar-ison, a first-order linear chain CRF trained withsimilar features on the same data can tag about4,000 words per second.4 FeaturesWhen designing features, we first made ones sim-ilar to the features typically designed for a first-order CRF, and then added features which are notpossible in a CRF, but are possible in our enhancedrepresentation.
This includes features over entireentities, features which directly model nested en-tities, and joint features over entities and parts ofspeech.
When features are computed over eachlabel, unary rule, and binary rule, the feature func-tion is aware of the rule span and split.Each word is labeled with its distributional sim-ilarity cluster (distsim), and a string indicatingorthographic information (shape) (Finkel et al,2005).
Subscripts represent word position in thesentence.
In addition to those below, we includefeatures for each fully annotated label and rule.Local named entity features.
Local named en-tity features are over the label for a single word.They are equivalent to the local features in a linearchain CRF.
However, unlike in a linear chain CRF,if a word belongs to multiple entities then the localfeatures are computed for each entity.
Local fea-tures are also computed for words not contained inany entity.
Local features are in Table 1.Pairwise named entity features.
Pairwise fea-tures are over the labels for adjacent words, andare equivalent to the edge features in a linear chainCRF.
They can occur when pairs of words havethe same label, or over entity boundaries wherethe words have different labels.
Like with the lo-cal features, if a pair of words are contained in, orstraddle the border of, multiple entities, then thefeatures are repeated for each.
The pairwise fea-tures we use are shown in Table 1.Embedded named entity features.
Embeddednamed entity features occur in binary rules whereone entity is the child of another entity.
For ourembedded features, we replicated the pairwise fea-tures, except that the embedded named entity wastreated as one of the words, where the ?word?
(and other annotations) were indicative of the typeof entity, and not the actual string that is the en-tity.
For instance, in the subtree in Figure 2, wewould compute wordi+labeli?1+labeli as PROT-DNA-DNA for i = 18 (the index of the word GM-CSF).
The normal pairwise feature at the same po-144GENIA ?
Testing on All EntitiesNested NER Model Semi-CRF Model# Test (train on all entities) (train on top-level entities)Entities Precision Recall F1 Precision Recall F1Protein 3034 79.04 69.22 73.80 78.63 64.04 70.59DNA 1222 69.61 61.29 65.19 71.62 57.61 63.85RNA 103 86.08 66.02 74.73 79.27 63.11 70.27Cell Line 444 73.82 56.53 64.03 76.59 59.68 67.09Cell Type 599 68.77 65.44 67.07 72.12 59.60 65.27Overall 5402 75.39 65.90 70.33 76.17 61.72 68.19Table 2: Named entity results on GENIA, evaluating on all entities.GENIA ?
Testing on Top-level Entities OnlyNested NER Model Semi-CRF Model# Test (train on all entities) (train on top-level entities)Entities Precision Recall F1 Precision Recall F1Protein 2592 78.24 72.42 75.22 76.16 72.61 74.34DNA 1129 70.40 64.66 67.41 71.21 62.00 66.29RNA 103 86.08 66.02 74.73 79.27 63.11 70.27Cell Line 420 75.54 58.81 66.13 76.59 63.10 69.19Cell Type 537 69.36 70.39 69.87 71.11 65.55 68.22Overall 4781 75.22 69.02 71.99 74.57 68.27 71.28Table 3: Named entity results on GENIA, evaluating on only top-level entities.sition would be GM-CSF-DNA-DNA.Whole entity features.
We had four whole en-tity features: the entire phrase; the preceding andfollowing word; the preceding and following dis-tributional similarity tags; and the preceding dis-tributional similarity tag with the following word.Local part of speech features.
We used thesame POS features as (Finkel et al, 2008).Joint named entity and part of speech features.For the joint features we replicated the POS fea-tures, but included the parent of the POS, whicheither is the innermost entity type, or would indi-cate that the word is not in any entities.5 ExperimentsWe performed two sets of experiments, the first setover biomedical data, and the second over Spanishand Catalan newspaper text.
We designed our ex-periments to show that our model works just aswell on outermost entities, the typical NER task,and also works well on nested entities.5.1 GENIA Experiments5.1.1 DataWe performed experiments on the GENIA v.3.02corpus (Ohta et al, 2002).
This corpus contains2000 Medline abstracts (?500k words), annotatedwith 36 different kinds of biological entities, andwith parts of speech.
Previous NER work usingthis corpus has employed 10-fold cross-validationfor evaluation.
We wanted to explore differentmodel variations (e.g., level of Markovization, anddifferent sets of distributional similarity cluster-ings) and feature sets, so we needed to set asidea development set.
We split the data by puttingthe first 90% of sentences into the training set, andthe remaining 10% into the test set.
This is theexact same split used to evaluate part of speechtagging in (Tsuruoka et al, 2005).
For develop-ment we used the first half of the data to train, andthe next quarter of the data to test.2 We made thesame modifications to the label set as the organiz-ers of the JNLPBA 2004 shared task (Collier etal., 2004).
They collapsed all DNA subtypes intoDNA; all RNA subtypes into RNA; all protein sub-types into protein; kept cell line and cell type; andremoved all other entities.
However, they also re-moved all embedded entities, while we kept them.As discussed in Section 3, we annotated eachword with a distributional similarity cluster.
Weused 200 clusters, trained using 200 million wordsfrom PubMed abstracts.
During development, wefound that fewer clusters resulted in slower infer-2This split may seem strange: we had originally intendeda 50/25/25 train/dev/test split, until we found the previouslyused 90/10 split.145JNLPBA 2004 ?
Testing on Top-level Entities OnlyNested NER Model Semi-CRF Model Zhou & Su (2004)# Test (train on all entities) (train on top-level entities)Entities Precision Recall F1 Precision Recall F1 Precision Recall F1Protein 4944 66.98 74.58 70.57 68.15 62.68 65.30 69.01 79.24 73.77DNA 1030 62.96 66.50 64.68 65.45 52.23 58.10 66.84 73.11 69.83RNA 115 63.06 60.87 61.95 64.55 61.74 63.11 64.66 63.56 64.10Cell line 487 49.92 60.78 54.81 49.61 52.16 50.85 53.85 65.80 59.23Cell type 1858 75.12 65.34 69.89 73.29 55.81 63.37 78.06 72.41 75.13Overall 8434 66.78 70.57 68.62 67.50 59.27 63.12 69.42 75.99 72.55Table 4: Named entity results on the JNLPBA 2004 shared task data.
Zhou and Su (2004) was the bestsystem at the shared task, and is still state-of-the-art on the dataset.ROOTSPAAtAQdobledoubleNCpartidomatchFC,,ORGANIZATIONDAeltheORGANIZATIONNPBarc?aBarc?aVSesisDAeltheAQfavoritofavoriteFE?
?FC,,VMafirmastatesPERSONPERSONNPMakaayMakaayPERSONFC,,NCdelanteroattackerSPdelofORGANIZATIONNPDeportivoDeportivoFP..Figure 3: An example sentence from the AnCora corpus, along with its English translation.ence with no improvement in performance.5.1.2 Experimental SetupWe ran several sets of experiments, varying be-tween all entities, or just top-level entities, fortraining and testing.
As discussed in Section 3, ifwe train on just top-level entities then the model isequivalent to a semi-CRF.
Semi-CRFs are state-of-the-art and provide a good baseline for per-formance on just the top-level entities.
Semi-CRFs are strictly better than regular, linear chainCRFs, because they can use all of the features andstrucutre of a linear chain CRF, but also utilizewhole-entity features (Andrew, 2006).
We alsoevaluated the semi-CRF model on all entities.
Thismay seem like an unfair evaluation, because thesemi-CRF has no way of recovering the nested en-tities, but we wanted to illustrate just how muchinformation is lost when using a flat representa-tion.5.1.3 ResultsOur named entity results when evaluating on allentities are shown in Table 2 and when evaluat-ing on only top-level entities are shown in Table 3.Our nested model outperforms the flat semi-CRFon both top-level entities and all entities.While not our main focus, we also evaluatedour models on parts of speech.
The model trainedon just top level entities achieved POS accuracyof 97.37%, and the one trained on all entitiesachieved 97.25% accuracy.
The GENIA tagger(Tsuruoka et al, 2005) achieves 98.49% accuracyusing the same train/test split.5.1.4 Additional JNLPBA 2004 ExperimentsBecause we could not compare our results on theNER portion of the GENIA corpus with any otherwork, we also evaluated on the JNLPBA corpus.This corpus was used in a shared task for theBioNLP workshop at Coling in 2004 (Collier etal., 2004).
They used the entire GENIA corpus fortraining, and modified the label set as discussed inSection 5.1.1.
They also removed all embeddedentities, and kept only the top-level ones.
Theythen annotated new data for the test set.
Thisdataset has no nested entities, but because thetraining data is GENIA we can still train our modelon the data annotated with nested entities, and thenevaluate on their test data by ignoring all embed-ded entities found by our named entity recognizer.146AnCora Spanish ?
Testing on All EntitiesNested NER Model Semi-CRF Model# Test (train on all entities) (train on top-level entities)Entities Precision Recall F1 Precision Recall F1Person 1778 65.29 78.91 71.45 75.10 32.73 45.59Organization 2137 86.43 56.90 68.62 47.02 26.20 33.65Location 1050 78.66 46.00 58.05 84.94 13.43 23.19Date 568 87.13 83.45 85.25 79.43 29.23 42.73Number 991 81.51 80.52 81.02 66.27 28.15 39.52Other 512 17.90 64.65 28.04 10.77 16.60 13.07Overall 7036 62.38 66.87 64.55 51.06 25.77 34.25Table 5: Named entity results on the Spanish portion of AnCora, evaluating on all entities.AnCora Spanish ?
Testing on Top-level Entities OnlyNested NER Model Semi-CRF Model# Test (train on all entities) (train on top-level entities)Entities Precision Recall F1 Precision Recall F1Person 1050 57.42 66.67 61.70 71.23 52.57 60.49Organization 1060 77.38 40.66 53.31 44.33 49.81 46.91Location 279 72.49 36.04 48.15 79.52 24.40 37.34Date 290 72.29 57.59 64.11 71.77 51.72 60.12Number 519 57.17 49.90 53.29 54.87 44.51 49.15Other 541 11.30 38.35 17.46 9.51 26.88 14.04Overall 3739 50.57 49.72 50.14 46.07 44.61 45.76Table 6: Named entity results on the Spanish portion of AnCora, evaluating on only top-level entities.This experiment allows us to show that our namedentity recognizer works well on top-level entities,by comparing it with prior work.
Our model alsoproduces part of speech tags, but the test data isnot annotated with POS tags, so we cannot showPOS tagging results on this dataset.One difficulty we had with the JNLPBA exper-iments was with tokenization.
The version of GE-NIA distributed for the shared task is tokenizeddifferently from the original GENIA corpus, butwe needed to train on the original corpus as it isthe only version with nested entities.
We tried ourbest to retokenize the original corpus to match thedistributed data, but did not have complete suc-cess.
It is worth noting that the data is actually to-kenized in a manner which allows a small amountof ?cheating.?
Normally, hyphenated words, suchas LPS-induced, are tokenized as one word.
How-ever, if the portion of the word before the hyphenis in an entity, and the part after is not, such asBCR-induced, then the word is split into two to-kens: BCR and -induced.
Therefore, when a wordstarts with a hyphen it is a strong indicator that theprior word and it span the right boundary of an en-tity.
Because the train and test data for the sharedtask do not contain nested entities, fewer wordsare split in this manner than in the original data.We did not intentionally exploit this fact in ourfeature design, but it is probable that some of ourorthographic features ?learned?
this fact anyway.This probably harmed our results overall, becausesome hyphenated words, which straddled bound-aries in nested entities and would have been splitin the original corpus (and were split in our train-ing data), were not split in the test data, prohibitingour model from properly identifying them.For this experiment, we retrained our model onthe entire, retokenized, GENIA corpus.
We alsoretrained the distributional similarity model on theretokenized data.
Once again, we trained onemodel on the nested data, and one on just the top-level entities, so that we can compare performanceof both models on the top-level entities.
Our fullresults are shown in Table 4, along with the cur-rent state-of-the-art (Zhou and Su, 2004).
Besidesthe tokenization issues harming our performance,Zhou and Su (2004) also employed clever post-processing to improve their results.5.2 AnCora Experiments5.2.1 DataWe performed experiments on the NER portionof AnCora (Mart??
et al, 2007).
This corpus hasSpanish and Catalan portions, and we evaluatedon both.
The data is also annotated with partsof speech, parse trees, semantic roles and word147AnCora Catalan ?
Testing on All EntitiesNested NER Model Semi-CRF Model# Test (train all entities) (train top-level entities only)Entities Precision Recall F1 Precision Recall F1Person 1303 89.01 50.35 64.31 70.08 46.20 55.69Organization 1781 68.95 83.77 75.64 65.32 41.77 50.96Location 1282 76.78 72.46 74.56 75.49 36.04 48.79Date 606 84.27 81.35 82.79 70.87 38.94 50.27Number 1128 86.55 83.87 85.19 75.74 38.74 51.26Other 596 85.48 8.89 16.11 64.91 6.21 11.33Overall 6696 78.09 68.23 72.83 70.39 37.60 49.02Table 7: Named entity results on the Catalan portion of AnCora, evaluating on all entities.AnCora Catalan ?
Testing on Top-level Entities OnlyNested NER Model Semi-CRF Model# Test (train all entities) (train top-level entities only)Entities Precision Recall F1 Precision Recall F1Person 801 67.44 47.32 55.61 62.63 67.17 64.82Organization 899 52.21 74.86 61.52 57.68 73.08 64.47Location 659 54.86 67.68 60.60 62.42 57.97 60.11Date 296 62.54 66.55 64.48 59.46 66.89 62.96Number 528 62.35 70.27 66.07 63.08 68.94 65.88Other 342 49.12 8.19 14.04 45.61 7.60 13.03Overall 3525 57.67 59.40 58.52 60.53 61.42 60.97Table 8: Named entity results on the Catalan portion of AnCora, evaluating on only top-level entities.senses.
The corpus annotators made a distinctionbetween strong and weak entities.
They definestrong named entities as ?a word, a number, a date,or a string of words that refer to a single individualentity in the real world.?
If a strong NE containsmultiple words, it is collapsed into a single token.Weak named entities, ?consist of a noun phrase,being it simple or complex?
and must contain astrong entity.
Figure 3 shows an example from thecorpus with both strong and weak entities.
Theentity types present are person, location, organi-zation, date, number, and other.
Weak entities arevery prevalent; 47.1% of entities are embedded.For Spanish, files starting with 7?9 were the testset, 5?6 were the development test set, and the re-mainder were the development train set.
For Cata-lan, files starting with 8?9 were the test set, 6?7were the development test set, and the remainderwere the development train set.
For both, the de-velopment train and test sets were combined toform the final train set.
We removed sentenceslonger than 80 words.
Spanish has 15,591 train-ing sentences, and Catalan has 14,906.5.2.2 Experimental SetupThe parts of speech provided in the data includedetailed morphological information, using a sim-ilar annotation scheme to the Prague TreeBank(Hana and Hanova?, 2002).
There are around 250possible tags, and experiments on the developmentdata with the full tagset where unsuccessful.
Weremoved all but the first two characters of eachPOS tag, resulting in a set of 57 tags which moreclosely resembles that of the Penn TreeBank (Mar-cus et al, 1993).
All reported results use our mod-ified version of the POS tag set.We took only the words as input, none of theextra annotations.
For both languages we trained a200 cluster distributional similarity model over thewords in the corpus.
We performed the same setof experiments on AnCora as we did on GENIA.5.2.3 Results and DiscussionThe full results for Spanish when testing on all en-tities are shown in Table 5, and for only top-levelentities are shown in Table 6.
For part of speechtagging, the nested model achieved 95.93% accu-racy, compared with 95.60% for the flatly trainedmodel.
The full results for Catalan when testing onall entities are shown in Table 7, and for only top-level entities are shown in Table 8.
POS taggingresults were even closer on Catalan: 96.62% forthe nested model, and 96.59% for the flat model.It is not surprising that the models trained onall entities do significantly better than the flatlytrained models when testing on all entities.
The148story is a little less clear when testing on just top-level entities.
In this case, the nested model does4.38% better than the flat model on the Spanishdata, but 2.45% worse on the Catalan data.
Theoverall picture is the same as for GENIA: model-ing the nested entities does not, on average, reduceperformance on the top-level entities, but a nestedentity model does substantially better when evalu-ated on all entities.6 ConclusionsWe presented a discriminative parsing-basedmethod for nested named entity recognition,which does well on both top-level and nested enti-ties.
The only real drawback to our method is thatit is slower than common flat techniques.
Whilemost NER corpus designers have defenestratedembedded entities, we hope that in the future thiswill not continue, as large amounts of informationare lost due to this design decision.AcknowledgementsThanks to Mihai Surdeanu for help with the An-Cora data.
The first author was supported bya Stanford Graduate Fellowship.
This paper isbased on work funded in part by the Defense Ad-vanced Research Projects Agency through IBM.The content does not necessarily reflect the viewsof the U.S. Government, and no official endorse-ment should be inferred.ReferencesBeatrice Alex, Barry Haddow, and Claire Grover.
2007.Recognising nested named entities in biomedical text.
InBioNLP Workshop at ACL 2007, pages 65?72.Galen Andrew.
2006.
A hybrid markov/semi-markov con-ditional random field for sequence segmentation.
In Pro-ceedings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2006).A.
Borthwick.
1999.
A Maximum Entropy Approach toNamed Entity Recognition.
Ph.D. thesis, New York Uni-versity.Kate Byrne.
2007.
Nested named entity recognition in his-torical archive text.
In ICSC ?07: Proceedings of the Inter-national Conference on Semantic Computing, pages 589?596.Alexander Clark.
2003.
Combining distributional and mor-phological information for part of speech induction.
InProceedings of the tenth Annual Meeting of the EuropeanAssociation for Computational Linguistics (EACL), pages59?66.Nigel Collier, J. Kim, Y. Tateisi, T. Ohta, and Y. Tsuruoka, ed-itors.
2004.
Proceedings of the International Joint Work-shop on NLP in Biomedicine and its Applications.J.
R. Curran and S. Clark.
1999.
Language independent NERusing a maximum entropy tagger.
In CoNLL 1999, pages164?167.Jenny Finkel, Shipra Dingare, Christopher Manning, Malv-ina Nissim, Beatrice Alex, and Claire Grover.
2005.
Ex-ploring the boundaries: Gene and protein identification inbiomedical text.
In BMC Bioinformatics 6 (Suppl.
1).Jenny Rose Finkel, Alex Kleeman, and Christopher D. Man-ning.
2008.
Efficient, feature-based conditional randomfield parsing.
In ACL/HLT-2008.Baohua Gu.
2006.
Recognizing nested named entities in GE-NIA corpus.
In BioNLP Workshop at HLT-NAACL 2006,pages 112?113.Jir???
Hana and Hana Hanova?.
2002.
Manual for morpholog-ical annotation.
Technical Report TR-2002-14, UK MFFCKL.Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, andJun?ichi Tsujii.
2002.
Tuning support vector machinesfor biomedical named entity recognition.
In Proceedingsof the Workshop on Natural Language Processing in theBiomedical Domain (ACL 2002).John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic mod-els for segmenting and labeling sequence data.
In ICML2001, pages 282?289.
Morgan Kaufmann, San Francisco,CA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.L.
Ma?rquez, L. Padre`, M. Surdeanu, and L. Villarejo.
2007a.UPC: Experiments with joint learning within semeval task9.
In Proceedings of the 4th International Workshop onSemantic Evaluations (SemEval-2007).L.
Ma?rquez, L. Villarejo, M.A.
Mart?
?, and M. Taule`.
2007b.Semeval-2007 task 09: Multilevel semantic annotation ofCatalan and Spanish.
In Proceedings of the 4th Inter-national Workshop on Semantic Evaluations (SemEval-2007).M.A.
Mart?
?, M. Taule`, M. Bertran, and L. Ma?rquez.
2007.Ancora: Multilingual and multilevel annotated corpora.MS, Universitat de Barcelona.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Flexible text segmentation with structured mul-tilabel classification.
In HLT ?05: Proceedings of theconference on Human Language Technology and Empiri-cal Methods in Natural Language Processing, pages 987?994.Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim.
2002.
TheGENIA corpus: an annotated research abstract corpus inmolecular biology domain.
In Proceedings of the secondinternational conference on Human Language TechnologyResearch, pages 82?86.149Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: Language-independent named entity recognition.
In Proceedings ofCoNLL-2003.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markovconditional random fields for information extraction.
In InAdvances in Neural Information Processing Systems 17,pages 1185?1192.Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim Tan.
2003.
Effective adaptation of a hidden markovmodel-based named entity recognizer for biomedical do-main.
In Proceedings of the ACL 2003 workshop on Nat-ural language processing in biomedicine.
Association forComputational Linguistics (ACL 2003).Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2003.
Boost-ing precision and recall of dictionary-based protein namerecognition.
In Proceedings of the ACL-03 Workshop onNatural Language Processing in Biomedicine, pages 41?48.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, TomokoOhta, John McNaught, Sophia Ananiadou, and Jun?ichiTsujii.
2005.
Developing a robust part-of-speech tag-ger for biomedical text.
In Advances in Informatics -10th Panhellenic Conference on Informatics, LNCS 3746,pages 382?392.Jie Zhang, Dan Shen, Guodong Zhou, Jian Su, and Chew-LimTan.
2004.
Enhancing HMM-based biomedical namedentity recognition by studying special phenomena.
Jour-nal of Biomedical Informatics, 37(6):411?422.GuoDong Zhou and Jian Su.
2004.
Exploring deepknowledge resources in biomedical name recognition.In Joint Workshop on Natural Language Processing inBiomedicine and Its Applications at Coling 2004.Guodong Zhou, Jie Zhang, Jian Su, Dan Shen, and ChewlimTan.
2004.
Recognizing names in biomedical texts: amachine learning approach.
Bioinformatics, 20(7):1178?1190.Guodong Zhou.
2006.
Recognizing names in biomedicaltexts using mutual information independence model andSVM plus sigmoid.
International Journal of Medical In-formatics, 75:456?467.150
