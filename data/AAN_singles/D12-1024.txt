Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 256?265, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsFramework of Automatic Text SummarizationUsing Reinforcement LearningSeonggi RyangGraduate School of InformationScience and TechnologyUniversity of Tokyosryang@is.s.u-tokyo.ac.jpTakeshi AbekawaNational Institute of Informaticsabekawa@nii.ac.jpAbstractWe present a new approach to the problemof automatic text summarization called Au-tomatic Summarization using ReinforcementLearning (ASRL) in this paper, which modelsthe process of constructing a summary withinthe framework of reinforcement learning andattempts to optimize the given score functionwith the given feature representation of a sum-mary.
We demonstrate that the method of re-inforcement learning can be adapted to auto-matic summarization problems naturally andsimply, and other summarizing techniques,such as sentence compression, can be easilyadapted as actions of the framework.The experimental results indicated ASRL wassuperior to the best performing method inDUC2004 and comparable to the state of theart ILP-style method, in terms of ROUGEscores.
The results also revealed ASRL cansearch for sub-optimal solutions efficientlyunder conditions for effectively selecting fea-tures and the score function.1 IntroductionAutomatic text summarization aims to automaticallyproduce a short and well-organized summary of sin-gle or multiple documents (Mani, 2001).
Automaticsummarization, especially multi-document summa-rization, has been an increasingly important task inrecent years, because of the exponential explosionof available information.
The brief summary thatthe summarization system produces allows readersto quickly and easily understand the content of orig-inal documents without having to read each individ-ual document, and it should be helpful for dealingwith enormous amounts of information.The extractive approach to automatic summariza-tion is a popular and well-known approach in thisfield, which creates a summary by directly selectingsome textual units (e.g., words and sentences) fromthe original documents, because it is difficult to gen-uinely evaluate and guarantee the linguistic qualityof the produced summary.One of the most well-known extractive ap-proaches is maximal marginal relevance (MMR),which scores each textual unit and extracts the unitthat has the highest score in terms of the MMR cri-teria (Goldstein et al2000).
Greedy MMR-stylealgorithms are widely used; however, they cannottake into account the whole quality of the sum-mary due to their greediness, although a summaryshould convey all the information in given docu-ments.
Global inference algorithms for the extrac-tive approach have been researched widely in recentyears (Filatova and Hatzivassiloglou, 2004; McDon-ald, 2007; Takamura and Okumura, 2009) to con-sider whether the summary is ?good?
as a whole.These algorithms formulate the problem as integerlinear programming (ILP) to optimize the score:however, as ILP is non-deterministic polynomial-time hard (NP-hard), the time complexity is verylarge.
Consequently, we need some more efficientalgorithm for calculations.We present a new approach to the problem of au-tomatic text summarization called Automatic Sum-marization using Reinforcement Learning (ASRL),which models the process of construction of a sum-mary within the framework of reinforcement learn-256ing and attempts to optimize the given score functionwith the given feature representation of a summary.We demonstrate that the method of reinforcementlearning can be adapted to problems with automaticsummarization naturally and simply, and other sum-marizing techniques, such as sentence compression,can be easily adapted as actions of the framework,which should be helpful to enhance the quality ofthe summary that is produced.
This is the first paperutilizing reinforcement learning for problems withautomatic summarization of text.We evaluated ASRL with the DUC2004 summa-rization task 2, and the experimental results revealedASRL is superior to the best method of performancein DUC2004 and comparable with the state of theart ILP-style method, based on maximum coveragewith the knapsack constraint problem, in terms ofROUGE scores with experimental settings.
We alsoevaluated ASRL in terms of optimality and execu-tion time.
The experimental results indicated ASRLcan search the state space efficiently for some sub-optimal solutions under the condition of effectivelyselecting features and the score function, and pro-duce a summary whose score denotes the expecta-tion of the score of the same features?
states.
Theevaluation of the quality of a produced summaryonly depends on the given score function, and there-fore it is easy to adapt the new method of evaluationwithout having to modify the structure of the frame-work.2 Formulation of Extractive ApproachWe first focus on the extractive approach, which isdirectly used to produce a summary by extractingsome textual units, by avoiding the difficulty of hav-ing to consider the genuine linguistic quality of asummary.The given document (or documents) in extractivesummarization approaches is reduced to the set oftextual units: D = {x1, x2, ?
?
?
, xn}, where n isthe size of the set, and xi denotes individual textualunits.
Note that any textual unit is permitted, suchas character, word, sentence, phrase, and concep-tual unit.
If we determine a sentence is a textual unitto be extracted, the formulated problem is a problemof extracting sentences from the source document,which is one of the most popular settings for sum-marization tasks.Next, we define the score function, score(S), forany subset of the document: S ?
D. Subset S is oneof the summaries of the given document.
The aim ofthis summarization problem is to find the summarythat maximizes this function when the score functionis given.
The score function is typically defined bytaking into consideration the tradeoff between rele-vance and redundancy.Then, we define length function L(S), which in-dicates the length of summary S. The length isalso arbitrary, which can be based on the character,word, and sentence.
We assume the limitation ofsummary length K is given in summarization tasks.Finally, we define the extractive approach of theautomatic summarization problem as:S?
= argmaxS?Dscore(S) (1)s.t.
L(S) ?
K.3 MotivationWe can regard the extractive approach as a searchproblem.
It is extremely difficult to solve this searchproblem because the final result of evaluation givenby the given score function is not available until itfinishes, and we therefore need to try all combina-tions of textual units.
Consequently, the score func-tion, which denotes some criterion for the qualityof a summary, tends to be determined so that thefunction can be decomposed to components and itis solved with global inference algorithms, such asILP.
However, both decomposing the score func-tion properly and utilizing the evaluation of half-wayprocess of searches are generally difficult.
For ex-ample, let us assume that we design the score func-tion by using some complex semantic considerationsto take into account the readability of a summary,and the score is efficiently calculated if the wholesummary is given.
Then, formulating the problemas a global inference problem and solving it withmethods of integer linear programming might gen-erally be difficult, because of the complex compo-sition of the score function, despite the ease withwhich the whole summary is evaluated.
The read-ability score might be based on extremely complexcalculations of dependency relations, or a great dealof external knowledge the summarizer cannot know257merely from the source documents.
In fact, it is idealthat we can only directly utilize the score function,in the sense that we do not have to consider the de-composed form of the given score function.We need to consider the problem with automaticsummarization to be the same as that with reinforce-ment learning to handle these problems.
Reinforce-ment learning is one of the solutions to three prob-lems.?
The learning of the agent only depends on thereward provided by the environment.?
Furthermore, the reward is delayed, in the sensethat the agent cannot immediately know the ac-tual evaluation of the executed action.?
The agent only estimates the value of thestate with the information on rewards, withoutknowledge of the actual form of the score func-tion, to maximize future rewards.We suggest the formulation of the problem as wehave just described will enable us to freely designthe score function without limitations and expandthe capabilities of automatic summarization.4 Models of Extractive Approach forReinforcement Learning4.1 Reinforcement LearningReinforcement learning is a powerful method ofsolving planning problems, especially problems for-mulated as Markov decision processes (MDPs) (Sut-ton and Barto, 1998).
The agent of reinforcementlearning repeats three steps until terminated at eachepisode in the learning process.1.
The agent observes current state s from the en-vironment, contained in state space S .2.
Next, it determines and executes next action aaccording to current policy ?.
Action a is con-tained in the action space limited by the currentstate: A(s), which is a subset of whole actionspace A =?s?S A(s).
Policy ?
is the strat-egy for selecting action, represented as a con-ditional distribution of actions: p(a|s).3.
It then observes next state s?
and receives re-ward r from the environment.The aim of reinforcement learning is to find optimalpolicy ??
only with information on sample trajecto-ries and to reward the experienced agent.We describe how to adapt the extractive approachto the problem of reinforcement learning in the sec-tions that follow.4.2 StateA state denotes a summary.
We represent state sas a tuple of summary S (a set of textual units) andadditional state variables: s = (S,A, f).
We assumes has the history of actionsA that the agent executedto achieve this state.
Additionally, s has the binarystate variable, f ?
{0, 1}, which denotes whether sis a terminal state or not.
Initial state s0 is (?, ?, 0).We assume the d-dimensional feature representa-tion of state s: ?
(s) ?
Rd, which only depends onthe feature of summary ??
(S) ?
Rd?1.
Given ??
(S),we define the features as:?
(s) ={(??
(S), 0)T (L(S) ?
K)(0, 1)T (K < L(S)) .
(2)This definition denotes that summaries that violatethe length limitation are shrunk to a single feature,(0, 1)T, which means it is not a summary.Note the features of the state only depend on thefeatures of the summary, not on the executed actionsto achieve the state.
Unlike naive search methods,this property has the potential for different states tobe represented as the same vector, which has thesame features.
The agent, however, should search asmany possible states as it can.
Therefore, the gen-eralization function of the feature representation isof utmost importance.
The accurate selection of fea-tures contributes to reducing the search space andprovides efficient learning as will be discussed later.4.3 ActionAn action denotes a transition operation that pro-duces a new state from a current state.
We assumedall actions were deterministic in this study.
We de-fine inserti(1 ?
i ?
n) actions, each of whichinserts textual unit xi to the current state unless thestate is terminated, as described in the following di-258agram:st at st+1??StAt0??inserti??????
?St ?
{xi}At ?
{inserti}0??
.
(3)In addition to insertion actions, we define finishthat terminates the current episode in reinforcementlearning:st at st+1??StAt0??finish??????
?StAt ?
{finish}1??
(4)Note that ft = 1 means state st is a terminal state.Then, the whole action set, A, is defined byinserti and finish:A = {insert1, insert2, ?
?
?
, insertn,finish}.
(5)We can calculate the available actions limited bystate st:A(st) ={A\At (L(St) ?
K){finish} (K < L(St)).
(6)This definition means that the agent may execute oneof the actions that have not yet been executed in thisepisode, and it has no choice but to finish if the sum-mary of the current state already violates length lim-itations.4.4 RewardThe agent receives a reward from the environmentas some kind of criterion of how good the action theagent executed was.
If the current state is st, theagent executes at, and the state makes a transitioninto st+1; then, the agent receives the reward, rt+1:rt+1=??
?score(St) (at = finish, L(St) ?
K)?Rpenalty (at = finish,K < L(St))0 (otherwise), (7)where Rpenalty > 0.The agent can receive the score awarded by thegiven score function if and only if the executed ac-tion is finish and the summary length is appropri-ate.
If the summary length is inappropriate but theexecuted action is finish, the environment awardsa penalty to the agent.
The most important point ofthis definition is that the agent receives nothing un-der the condition where the next state is not termi-nated.
In this sense, the reward is delayed.
Due tothis definition, maximizing the expectation of futurerewards is equivalent to maximizing the given scorefunction, and we do not need to consider the decom-posed form of the score function, i.e., we only needto consider the final score of the whole summary.4.5 Value Function ApproximationOur aim is to find the optimal policy.
This isachieved by obtaining the optimal state value func-tion, V ?
(s), because if we obtain this, the greedypolicy is optimal, which determines the action so asto maximize the state value after the transition oc-curred.
Therefore, our aim is equivalent to findingV ?(s).
Let us try to estimate the state value func-tion with parameter ?
?
Rd:V (s) = ?T?(s).
(8)We can also represent and estimate the action valuefunction, Q(s, a), by using V (s):Q(s, a) = r + ?V (s?
), (9)where the execution of a causes the state transitionfrom s to s?
and the agent receives reward r, and?
(0 ?
?
?
1) is the discount rate.
Note that allactions are deterministic in this study.By using these value functions, we define thepolicy as the conditional distribution, p(a|s; ?, ?
),which is parameterized by ?
and a temperature pa-rameter ?
:p(a|s; ?, ?)
= eQ(s,a)/??a?
eQ(s,a?)/?
.
(10)Temperature ?
decreases as learning progresses,which causes the policy to be greedier.
This softmaxselection strategy is called Boltzmann selection.4.6 Learning AlgorithmThe goal of learning is to estimate ?.
We use theTD (?)
algorithm with function approximation (Sut-ton and Barto, 1998).
Algorithm 1 represents thewhole system of our method, called Automatic Sum-marization using Reinforcement Learning (ASRL)259Algorithm 1 ASRLInput: document D = {x1, x2, ?
?
?
, xn},score function score(S)1: initialize ?
= 02: for k = 1 to N do3: s?
(?, ?, 0)// initial state4: e = 05: while s is not terminated do6: a ?
p(a|s; ?, ?k)// selects action with current policy7: (s?, r)?
execute(s, a)// observes next state and receive reward8: ?
?
r + ??T?(s?)?
?T?
(s)// calculates TD-error9: e?
?
?e + ?
(s)// updates the eligibility trace10: ?
?
?
+ ?k?e// learning with current learning rate11: s?
s?12: end while13: end for14: s?
(?, ?, 0)15: while s is not terminated do16: a?
maxa Q(s, a)// selects action greedilywith the learned policy17: (s?, r)?
execute(s, a)18: s?
s?19: end while20: return the summary of sin this paper.
N is the number of learning episodes,and e(?
Rd) and ?
(0 ?
?
?
1) correspond to the el-igibility trace and the trace decay parameter.
The el-igibility trace, e, conveys all information on the fea-tures of states that the agent previously experienced,with previously decaying influences of features dueto decay parameter ?
and discount rate ?
(Line 9).Line 1 initializes parameter ?
to start up its learn-ing.
The following procedures from Lines 2 to 13learn ?
with the TD (?)
algorithm, by using infor-mation on actual interactions with the environment.Learning rate ?k and temperature parameter ?k de-cay as the learning episode progresses.
The bestsummary with the obtained policy is calculated insteps from Lines 14 to 19.
If the agent can estimate?
properly, greedy output is the optimal solution.5 Models of Combined Approach forReinforcement LearningWe formulated the extractive approach as a problemwith reinforcement learning in the previous section.In fact, we can also formulate a more general modelof summarization, since evaluation only depends onthe final state and it is not actually very important toregard the given documents as a set of textual unitscontained in the original documents.We explain how to take into account other meth-ods within the ASRL framework by modifying themodels in this section, with an example of sentencecompression.
We assume that we have a method ofsentence compression, comp(x), and that a textualunit to be extracted is a sentence.
What we have todo is to only simply modify the definitions of thestate and action.
Note that this is just one exampleof the combined method.
Even other summarizationsystems can be similarly adapted to ASRL.5.1 StateWe do not want to execute sentence compressiontwice, so we have to modify the state variables toconvey the information: s = (S,A, c, f), wherec ?
{0, 1}, and S,A, and f are the same definitionsas previously described.5.2 ActionWe add deterministic action comp toA, which pro-duces the new summary constructed by compressingthe last inserted sentence of the current summary:st at st+1????StAt00????comp????????
?St\{xc} ?
{comp(xc)}At ?
{comp}10???
?,(11)where xc is the last sentence that is inserted into St.Next, we modify inserti and finish:st at st+1????StAtct0????inserti????????
?St ?
{xi}At ?
{inserti}00???
?,(12)260st at st+1????StAtct0????finish????????
?StAt ?
{finish}ct1????.
(13)Note comp ?
A(st) may be executed if and only ifct = 0. inserti resets c to 0.6 ExperimentsWe conducted three experiments in this study.
First,we evaluated our method with ROUGE metrics(Lin, 2004), in terms of ROUGE-1, ROUGE-2, andROUGE-L. Second, we conducted an experiment onmeasuring the optimization capabilities of ASRL,with the scores we obtained and the execution time.Third, we evaluated ASRL taking into considerationsentence compression by using a very naive method,in terms of ROUGE-1, ROUGE-2, and ROUGE-3.6.1 Experimental SettingsWe used sentences as textual units for the extrac-tive approach in this research.
Each sentence anddocument were represented as a bag-of-words vec-tor with tf*idf values, with stopwords removed.
Alltokens were stemmed by using Porter?s stemmer(Porter, 1980).We experimented with our proposed method onthe dataset of DUC2004 task2.
This is a multi-document summarization task that contains 50 docu-ment clusters, each of which has 10 documents.
Weset up the length limitation to 665 bytes, used in theevaluation of DUC2004.We set up the parameters of ASRL where thenumber of episodes N = 300, the training rate?k = 0.001 ?
101/(100 + k1.1), and the tempera-ture ?k = 1.0 ?
0.987k?1 where k was the number ofepisodes that decayed as learning progressed.
Bothdiscount rate ?
and trace decay parameter ?
werefixed to 1 for episodic tasks.
The penalty, Rpenalty,was fixed to 1.We used the following score function in thisstudy:score(S) =?xi?S?sRel(xi)??xi,xj?S,i<j(1?
?s)Red(xi, xj), (14)whereRel(xi) = Sim(xi, D) + Pos(xi)?1 (15)Red(xi, xj) = Sim(xi, xj).
(16)?s is the parameter for the trade-off betweenrelevance and redundancy, Sim(xi, D) andSim(xi, xj) correspond to the cosine similaritiesbetween sentence xi and the sentence set of thegiven original documents D, and between sentencexi and sentence xj .
Pos(xi) is the position ofthe occurrence of xi when we index sentences ineach document from top to bottom with one origin.This score function was determined by referenceto McDonald (2007).
We set ?s = 0.9 in thisexperiment.We designed ??
(S), i.e., the vector representationof a summary, to adapt it to the summarization prob-lem as follows.?
Coverage of important words: The elementsare the top 100 words in terms of the tf*idf ofthe given document with binary representation.?
Coverage ratio: This is calculated by countingup the number of top 100 elements included inthe summary.?
Redundancy ratio: This is calculated bycounting up the number of elements that exces-sively cover the top 100 elements.?
Length ratio: This is the ratio between thelength of the summary and length limitationK.?
Position: This feature takes into considerationthe position of sentence occurrences.
It is cal-culated with?x?S Pos(x)?1.Consequently, ??
(S) is a 104-dimensional vector.We executed ASRL 10 times with the settings pre-viously described and used all the results for evalu-ation.We used the dataset of DUC2003, which is a simi-lar task that contains 30 document clusters and eachcluster had 10 documents, to determine ?k and ?s.We determined the parameters so that they wouldconverge properly and become close to the opti-mal solutions calculated by ILP, under the condi-tions that the described feature representation andthe score function were given.261ROUGE-1 ROUGE-2 ROUGE-LASRL 0.39013 0.09479 0.33769MCKP 0.39033 0.09613 0.34225PEER65 0.38279 0.09217 0.33099ILP 0.34712 0.07528 0.31241GREEDY 0.30618 0.06400 0.27507Table 1: Results of ROUGE evaluation compared withother peers in DUC2004.
Scores for ILP and GREEDYhave statistically significant differences from scores ofASRL.6.2 EvaluationWe compared ASRL with four other conventionalmethods.?
GREEDY: This method is a simple greedy al-gorithm, which repeats the selection of the sen-tence with the highest score of the remainingsentences by using an MMR-like method ofscoring as follows:x = argmaxx?D\S[?sRel(x)?(1?
?s)maxxi?SRed(x, xi)], (17)where S is the current summary.?
ILP: This indicates the method proposed byMcDonald (2007) for maximizing the scorefunction (14) with integer linear programming.?
PEER65: This is the best performing system intask 2 of the DUC2004 competition in terms ofROUGE-1 proposed by Conroy et al2004).?
MCKP: This method was proposed by Taka-mura and Okamura (2009).
MCKP defines anautomatic summarization problem as a maxi-mum coverage problem with a knapsack con-straint, which uses conceptual units (Filatovaand Hatzivassiloglou, 2004), and composes themeaning of sentences, as textual units and at-tempts to cover as many units as possible underthe knapsack constraint.7 Results7.1 Evaluation with ROUGEWe evaluated our method of ASRL with ROUGE,in terms of ROUGE-1, ROUGE-2, and ROUGE-L.ROUGE-1 ROUGE-2 ROUGE-LASRL.0 0.39274 0.09537 0.34010ASRL.1 0.39243 0.09683 0.33855ASRL.2 0.39241 0.09597 0.34070ASRL.3 0.39190 0.09580 0.33898ASRL.4 0.39054 0.09579 0.33663ASRL.5 0.38911 0.09395 0.33551ASRL.6 0.38866 0.09392 0.33701ASRL.7 0.38854 0.09338 0.33661ASRL.8 0.38821 0.09363 0.33833ASRL.9 0.38532 0.09281 0.33321Table 2: Results of ROGUE evaluation for each ASRLpeer of 10 results in DUC2004.
ASRL did not convergewith stable solution with these experimental settings be-cause of property of randomness.The experimental results are summarized in Tables1 and 2.
Table 1 lists the results for the comparisonand Table 2 lists all the results for ASRL peers.The results imply ASRL is superior to PEER65,ILP, and GREEDY, and comparable to MCKP withthese experimental settings in terms of ROUGEmet-rics.
Note that ASRL is a kind of approximatemethod, because actions are selected probabilisti-cally and the method of reinforcement learning oc-casionally converges with some sub-optimal solu-tion.
This can be expected from Table 2, which in-dicates the results vary although each ASRL solu-tion converged with some solution.
However, in thisexperiment, ASRL achieved higher ROUGE scoresthan ILP, which achieved optimal solutions.
Thisseems to have been caused by the properties of thefeatures, which we will discuss later.
It seems thisfeature representation is useful for efficiently search-ing the feature space.
The method of mapping a stateto features is, however, approximate in the sense thatsome states will shrink to the same feature vector,and ASRL therefore has no tendency to convergewith some stable solution.7.2 Evaluation of Optimization CapabilitiesSince we proposed our method as an approach to ap-proximate optimization, there was the possibility ofconvergence with some sub-optimal solution as pre-viously discussed.
We also evaluated our approachfrom the point of view of the obtained scores and theexecution time to confirm whether our method had2620 50 100 150 200 250 300Episode?0.50.00.51.01.52.02.53.0AverageofScoresASRLGREEDYILPFigure 1: Average score for each episode in ASRL inDUC2004.
Horizontal lines indicate scores of summariesobtained with ILP and GREEDY.optimization capabilities.The experimental results are plotted in Figures1 and 2.
Figure 1 plots the average for the re-wards (i.e., scores) that the agent obtained for eachepisode.
The horizontal line for ILP is the averagefor the optimal scores of (14).
The score in ASRLincreases as the number of episodes increases, andovertakes the score of GREEDY at some episode.The agent attempts to come close to the optimalscore line of ILP but seems to fail, and finally con-verges to some local optimal solution.
We shouldincrease the number of episodes, adjust parameters?
and ?
, and select more appropriate features forthe state to improve the optimization capabilities ofASRL.Figure 2 plots the execution time for each peer.The horizontal axis is the number of textual units,i.e., the number of sentences in this experiment.
Thevertical axis is the execution time taken by the task.The plots of ASRL and ILP fit a linear function forthe former and an exponential function for the lat-ter.
The experimental results indicate that while theexecution time for ILP tends to increase exponen-tially, that for ASRL increases linearly.
The timecomplexity of ASRL is linear with respect to thenumber of actions because the agent has to selectthe next action from the available actions for eachepisode, whose time complexity is naively O(|A|).As inserti actions are dominant in the extractive100 200 300 400 500 600 700The number of textual units0100020003000400050006000Executiontime(sec)ASRLASRL(fit)ILPILP(fit)Figure 2: Execution time on number of textual units foreach problem in DUC2004.
Plot of ASRL is fitted to lin-ear function and that of ILP is fitted to exponential func-tion.approach, the execution time increases linearly withrespect to the number of textual units.
However, ILPhas to take into account the combinations of textualunits, whose number increases exponentially.In conclusion, both the experimental results in-dicate that ASRL efficiently calculated a summarythat was sub-optimal, but that was of relatively high-quality in terms of ROUGE metrics, with the exper-imental settings we used.7.3 Evaluation of Effects of SentenceCompressionWe also evaluated the combined approach with sen-tence compression.
We evaluated the method de-scribed in Section 5 called ASRLC in this experi-ment for the sake of convenience.
We used a verynaive method of sentence compression for this ex-periment, which compressed a sentence to only im-portant words, i.e., selecting word order by usingthe tf*idf score to compress the length to abouthalf.
This method of compression did not take intoconsideration either readability or linguistic quality.Note we wanted to confirm what effect the othermethods would have, and we expected this to im-prove the ROUGE-1 score.
We used the ROUGE-3score in this evaluation instead of ROUGE-L, to con-firm whether naive sentence compression occurred.The experimental results are summarized in Ta-263ROUGE-1 ROUGE-2 ROUGE-3ASRL 0.39013 0.09479 0.03435ASRLC 0.39141 0.09259 0.03239Table 3: Evaluation of combined methods.ble 3, which indicates ROUGE-1 increases butROUGE-2 and ROUGE-3 decrease as expected.
Thevariations, however, are small.
This phenomenonwas reported by Lin (2003) in that the effectivenessof sentence compression by local optimization at thesentence level was insufficient.
Therefore, we wouldhave to consider the range of applications with thecombined method.8 Discussion8.1 Local Optimality of ASRLWe will discuss why ASRL seems to converge withsome ?good?
local optimum with the described ex-perimental settings in this section.Since our model of the state value function wassimply linear and our parameter estimation was im-plemented by TD (?
), which is a simple methodin RL, it seems simply employing more efficientor state-of-the-art reinforcement learning methodsmay improve the performance of ASRL, such asGTD and GTD2 (Sutton et al2009b; Sutton et al2009a).
These methods basically only contribute tofaster convergence, and the score that they will con-verge to might not differ significantly.
As a result, itwould not matter much which method was used foroptimization.The main point of this problem is modeling thefeature representation of states, and this causessub-optimality.
The vector representation of statesshrinks the different states to a single representation,i.e., the agent regards states whose features are simi-lar to be similar states.
Due to this property, the pol-icy of reinforcement learning is learned to maximizethe expected score of each feature vector, whichincludes many states.
Such sub-optimality aver-agely balanced by the feature representation raisesthe possibility of achieving states that have a high-quality summary with a low score, since we do nothave a genuine score function.Thus, the most important thing in our methodis to intentionally design the features of states andthe score function, so that the agent can generalizestates, while taking into consideration truly-essentialfeatures for the required summarization.
It would beuseful if the forms of features and the score functioncould be arbitrarily designed by the user becausethere is the capability of obtaining a high-qualitysummaries.8.2 Potential of Combined MethodOther useful methods, even other summarizationsystems, can easily be adapted to ASRL as was de-scribed in Section 5.
The experimental results re-vealed that sentence compression has some effect.In fact, all operations that produce a new summaryfrom an old summary can be used, i.e., even othersummarizing methods can be employed for an ac-tion.
We assumed a general combined method mayhave a great deal of potential to enhance the qualityof summaries.8.3 Can We Obtain ?a Global Policy?
?We formulated each summarization task as a rein-forcement learning task in this paper, i.e., whereeach learned policy differs.
As this may be a littleunnatural, we wanted to obtain a single learned pol-icy, i.e., a global policy.However, we assessed that we cannot achieve aglobal policy with these feature and score functionsettings because the best vector, which is the fea-ture representation of the summary that achieves anoptimal score under the current settings, seems tovary for each cluster, even if the domain of the clus-ters is the same (e.g., a news domain).
Having saidthat, we simultaneously surmised that we could ob-tain a global policy if we could obtain a highly gen-eral, crucial, and efficient feature representation of asummary.
We also think a global policy is essentialin terms of reinforcement learning and we intend toattempt to achieve this in future work.9 ConclusionWe presented a new approach to the problem ofautomatic text summarization called ASRL in thispaper, which models the process of constructinga summary with the framework of reinforcementlearning and attempts to optimize the given scorefunction with the given feature representation.264The experimental results demonstrated ASRLtends to converge sub-optimally, and excessively de-pends on the formulation of features and the scorefunction.
Although it is difficult, we believe this for-mulation would enable us to improve the quality ofsummaries by designing them freely.We intend to employ the ROUGE score as thescore function in future work, and obtain the param-eters of the state value function.
Using these results,we will attempt to obtain a single learned policy byemploying the ROUGE score or human evaluationsas rewards.
We also intend to consider efficient fea-tures and a score to achieve stable convergence.
Inaddition, we plan to use other methods of functionapproximation, such as RBF networks.ReferencesJ.M.
Conroy, J.D.
Schlesinger, J. Goldstein, and D.P.
O ?leary.
2004.
Left-brain/right-brain multi-documentsummarization.
In Proceedings of the Document Un-derstanding Conference (DUC 2004).E.
Filatova and V. Hatzivassiloglou.
2004.
A formalmodel for information selection in multi-sentence textextraction.
In Proceedings of the 20th internationalconference on Computational Linguistics, page 397.Association for Computational Linguistics.J.
Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.2000.
Multi-document summarization by sentenceextraction.
In Proceedings of the 2000 NAACL-ANLPWorkshop on Automatic summarization-Volume4, pages 40?48.
Association for Computational Lin-guistics.C.Y.
Lin.
2003.
Improving summarization performanceby sentence compression: a pilot study.
In Proceed-ings of the sixth international workshop on Informa-tion retrieval with Asian languages-Volume 11, pages1?8.
Association for Computational Linguistics.C.Y.
Lin.
2004.
Rouge: A package for automatic eval-uation of summaries.
In Proceedings of the workshopon text summarization branches out (WAS 2004), vol-ume 16.I.
Mani.
2001.
Automatic summarization, volume 3.John Benjamins Pub Co.R.
McDonald.
2007.
A study of global inference algo-rithms in multi-document summarization.
Advancesin Information Retrieval, pages 557?564.MF Porter.
1980.
An algorithm for suffix stripping.Program: electronic library and information systems,14(3):130?137.R.S.
Sutton and A.G. Barto.
1998.
Reinforcement learn-ing: An introduction, volume 1.
Cambridge UnivPress.R.S.
Sutton, H.R.
Maei, D. Precup, S. Bhatnagar,D.
Silver, C. Szepesva?ri, and E. Wiewiora.
2009a.Fast gradient-descent methods for temporal-differencelearning with linear function approximation.
In Pro-ceedings of the 26th Annual International Conferenceon Machine Learning, pages 993?1000.
ACM.R.S.
Sutton, C. Szepesva?ri, and H.R.
Maei.
2009b.
Aconvergent o (n) algorithm for off-policy temporal-difference learning with linear function approxima-tion.H.
Takamura and M. Okumura.
2009.
Text summariza-tion model based on maximum coverage problem andits variant.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 781?789.
Association forComputational Linguistics.265
