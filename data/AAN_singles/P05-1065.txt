Proceedings of the 43rd Annual Meeting of the ACL, pages 523?530,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsReading Level Assessment Using Support Vector Machines andStatistical Language ModelsSarah E. SchwarmDept.
of Computer Science and EngineeringUniversity of WashingtonSeattle, WA 98195-2350sarahs@cs.washington.eduMari OstendorfDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195-2500mo@ee.washington.eduAbstractReading proficiency is a fundamen-tal component of language competency.However, finding topical texts at an appro-priate reading level for foreign and sec-ond language learners is a challenge forteachers.
This task can be addressed withnatural language processing technology toassess reading level.
Existing measuresof reading level are not well suited tothis task, but previous work and our ownpilot experiments have shown the bene-fit of using statistical language models.In this paper, we also use support vectormachines to combine features from tradi-tional reading level measures, statisticallanguage models, and other language pro-cessing tools to produce a better methodof assessing reading level.1 IntroductionThe U.S. educational system is faced with the chal-lenging task of educating growing numbers of stu-dents for whom English is a second language (U.S.Dept.
of Education, 2003).
In the 2001-2002 schoolyear, Washington state had 72,215 students (7.2% ofall students) in state programs for Limited EnglishProficient (LEP) students (Bylsma et al, 2003).
Inthe same year, one quarter of all public school stu-dents in California and one in seven students inTexas were classified as LEP (U.S. Dept.
of Edu-cation, 2004).
Reading is a critical part of languageand educational development, but finding appropri-ate reading material for LEP students is often diffi-cult.
To meet the needs of their students, bilingualeducation instructors seek out ?high interest level?texts at low reading levels, e.g.
texts at a first or sec-ond grade reading level that support the fifth gradescience curriculum.
Teachers need to find materialat a variety of levels, since students need differenttexts to read independently and with help from theteacher.
Finding reading materials that fulfill theserequirements is difficult and time-consuming, andteachers are often forced to rewrite texts themselvesto suit the varied needs of their students.Natural language processing (NLP) technology isan ideal resource for automating the task of selectingappropriate reading material for bilingual students.Information retrieval systems successfully find top-ical materials and even answer complex queries intext databases and on the World Wide Web.
How-ever, an effective automated way to assess the read-ing level of the retrieved text is still needed.
Inthis work, we develop a method of reading level as-sessment that uses support vector machines (SVMs)to combine features from statistical language mod-els (LMs), parse trees, and other traditional featuresused in reading level assessment.The results presented here on reading level as-sessment are part of a larger project to developteacher-support tools for bilingual education instruc-tors.
The larger project will include a text simpli-fication system, adapting paraphrasing and summa-rization techniques.
Coupled with an informationretrieval system, these tools will be used to selectand simplify reading material in multiple languagesfor use by language learners.
In addition to studentsin bilingual education, these tools will also be use-ful for those with reading-related learning disabili-523ties and adult literacy students.
In both of these sit-uations, as in the bilingual education case, the stu-dent?s reading level does not match his/her intellec-tual level and interests.The remainder of the paper is organized as fol-lows.
Section 2 describes related work on readinglevel assessment.
Section 3 describes the corporaused in our work.
In Section 4 we present our ap-proach to the task, and Section 5 contains experi-mental results.
Section 6 provides a summary anddescription of future work.2 Reading Level AssessmentThis section highlights examples and features ofsome commonly used measures of reading level anddiscusses current research on the topic of readinglevel assessment using NLP techniques.Many traditional methods of reading level assess-ment focus on simple approximations of syntacticcomplexity such as sentence length.
The widely-used Flesch-Kincaid Grade Level index is based onthe average number of syllables per word and theaverage sentence length in a passage of text (Kin-caid et al, 1975) (as cited in (Collins-Thompsonand Callan, 2004)).
Similarly, the Gunning Fog in-dex is based on the average number of words persentence and the percentage of words with three ormore syllables (Gunning, 1952).
These methods arequick and easy to calculate but have drawbacks: sen-tence length is not an accurate measure of syntacticcomplexity, and syllable count does not necessar-ily indicate the difficulty of a word.
Additionally,a student may be familiar with a few complex words(e.g.
dinosaur names) but unable to understand com-plex syntactic constructions.Other measures of readability focus on seman-tics, which is usually approximated by word fre-quency with respect to a reference list or corpus.The Dale-Chall formula uses a combination of av-erage sentence length and percentage of words noton a list of 3000 ?easy?
words (Chall and Dale,1995).
The Lexile framework combines measuresof semantics, represented by word frequency counts,and syntax, represented by sentence length (Stenner,1996).
These measures are inadequate for our task;in many cases, teachers want materials with moredifficult, topic-specific words but simple structure.Measures of reading level based on word lists do notcapture this information.In addition to the traditional reading level metrics,researchers at Carnegie Mellon University have ap-plied probabilistic language modeling techniques tothis task.
Si and Callan (2001) conducted prelimi-nary work to classify science web pages using uni-gram models.
More recently, Collins-Thompson andCallan manually collected a corpus of web pagesranked by grade level and observed that vocabularywords are not distributed evenly across grade lev-els.
They developed a ?smoothed unigram?
clas-sifier to better capture the variance in word usageacross grade levels (Collins-Thompson and Callan,2004).
On web text, their classifier outperformedseveral other measures of semantic difficulty: thefraction of unknown words in the text, the numberof distinct types per 100 token passage, the mean logfrequency of the text relative to a large corpus, andthe Flesch-Kincaid measure.
The traditional mea-sures performed better on some commercial corpora,but these corpora were calibrated using similar mea-sures, so this is not a fair comparison.
More impor-tantly, the smoothed unigram measure worked betteron the web corpus, especially on short passages.
Thesmoothed unigram classifier is also more generaliz-able, since it can be trained on any collection of data.Traditional measures such as Dale-Chall and Lexileare based on static word lists.Although the smoothed unigram classifier outper-forms other vocabulary-based semantic measures, itdoes not capture syntactic information.
We believethat higher order n-gram models or class n-grammodels can achieve better performance by captur-ing both semantic and syntactic information.
This isparticularly important for the tasks we are interestedin, when the vocabulary (i.e.
topic) and grade levelare not necessarily well-matched.3 CorporaOur work is currently focused on a corpus obtainedfrom Weekly Reader, an educational newspaper withversions targeted at different grade levels (WeeklyReader, 2004).
These data include a variety of la-beled non-fiction topics, including science, history,and current events.
Our corpus consists of articlesfrom the second, third, fourth, and fifth grade edi-524Grade Num Articles Num Words2 351 71.5k3 589 444k4 766 927k5 691 1MTable 1: Distribution of articles and words in theWeekly Reader corpus.Corpus Num Articles Num WordsBritannica 115 277kB.
Elementary 115 74kCNN 111 51kCNN Abridged 111 37kTable 2: Distribution of articles and words in theBritannica and CNN corpora.tions of the newspaper.
We design classifiers to dis-tinguish each of these four categories.
This cor-pus contains just under 2400 articles, distributed asshown in Table 1.Additionally, we have two corpora consisting ofarticles for adults and corresponding simplified ver-sions for children or other language learners.
Barzi-lay and Elhadad (2003) have allowed us to use theircorpus from Encyclopedia Britannica, which con-tains articles from the full version of the encyclope-dia and corresponding articles from Britannica El-ementary, a new version targeted at children.
TheWestern/Pacific Literacy Network?s (2004) web sitehas an archive of CNN news stories and abridgedversions which we have also received permission touse.
Although these corpora do not provide an ex-plicit grade-level ranking for each article, broad cat-egories are distinguished.
We use these data as asupplement to the Weekly Reader corpus for learn-ing models to distinguish broad reading level classesthan can serve to provide features for more detailedclassification.
Table 2 shows the size of the supple-mental corpora.4 ApproachExisting reading level measures are inadequate dueto their reliance on vocabulary lists and/or a superfi-cial representation of syntax.
Our approach uses n-gram language models as a low-cost automatic ap-proximation of both syntactic and semantic analy-sis.
Statistical language models (LMs) are used suc-cessfully in this way in other areas of NLP such asspeech recognition and machine translation.
We alsouse a standard statistical parser (Charniak, 2000) toprovide syntactic analysis.In practice, a teacher is likely to be looking fortexts at a particular level rather than classifying agroup of texts into a variety of categories.
Thuswe construct one classifier per category which de-cides whether a document belongs in that categoryor not, rather than constructing a classifier whichranks documents into different categories relative toeach other.4.1 Statistical Language ModelsStatistical LMs predict the probability that a partic-ular word sequence will occur.
The most commonlyused statistical language model is the n-gram model,which assumes that the word sequence is an (n?1)thorder Markov process.
For example, for the com-mon trigram model where n = 3, the probability ofsequence w is:P (w) = P (w1)P (w2|w1)m?i=3P (wi|wi?1, wi?2).
(1)The parameters of the model are estimated using amaximum likelihood estimate based on the observedfrequency in a training corpus and smoothed usingmodified Kneser-Ney smoothing (Chen and Good-man, 1999).
We used the SRI Language ModelingToolkit (Stolcke, 2002) for language model training.Our first set of classifiers consists of one n-gramlanguage model per class c in the set of possibleclasses C. For each text document t, we can cal-culate the likelihood ratio between the probabilitygiven by the model for class c and the probabilitiesgiven by the other models for the other classes:LR = P (t|c)P (c)?c?
6=c P (t|c?
)P (c?
)(2)where we assume uniform prior probabilities P (c).The resulting value can be compared to an empiri-cally chosen threshold to determine if the documentis in class c or not.
For each class c, a languagemodel is estimated from a corpus of training texts.525In addition to using the likelihood ratio for classi-fication, we can use scores from language models asfeatures in another classifier (e.g.
an SVM).
For ex-ample, perplexity (PP ) is an information-theoreticmeasure often used to assess language models:PP = 2H(t|c), (3)where H(t|c) is the entropy relative to class c of alength m word sequence t = w1, ..., wm, defined asH(t|c) = ?
1m log2 P (t|c).
(4)Low perplexity indicates a better match between thetest data and the model, corresponding to a higherprobability P (t|c).
Perplexity scores are used as fea-tures in the SVM model described in Section 4.3.The likelihood ratio described above could also beused as a feature, but we achieved better results us-ing perplexity.4.2 Feature SelectionFeature selection is a common part of classifierdesign for many classification problems; however,there are mixed results in the literature on featureselection for text classification tasks.
In Collins-Thompson and Callan?s work (2004) on readabil-ity assessment, LM smoothing techniques are moreeffective than other forms of explicit feature selec-tion.
However, feature selection proves to be impor-tant in other text classification work, e.g.
Lee andMyaeng?s (2002) genre and subject detection workand Boulis and Ostendorf?s (2005) work on featureselection for topic classification.For our LM classifiers, we followed Boulis andOstendorf?s (2005) approach for feature selectionand ranked words by their ability to discriminatebetween classes.
Given P (c|w), the probability ofclass c given word w, estimated empirically fromthe training set, we sorted words based on their in-formation gain (IG).
Information gain measures thedifference in entropy when w is and is not includedas a feature.IG(w) = ?
?c?CP (c) log P (c)+ P (w)?c?CP (c|w) log P (c|w)+ P (w?
)?c?CP (c|w?)
log P (c|w?).
(5)The most discriminative words are selected as fea-tures by plotting the sorted IG values and keepingonly those words below the ?knee?
in the curve, asdetermined by manual inspection of the graph.
In anearly experiment, we replaced all remaining wordswith a single ?unknown?
tag.
This did not resultin an effective classifier, so in later experiments theremaining words were replaced with a small set ofgeneral tags.
Motivated by our goal of represent-ing syntax, we used part-of-speech (POS) tags as la-beled by a maximum entropy tagger (Ratnaparkhi,1996).
These tags allow the model to represent pat-terns in the text at a higher level than that of individ-ual words, using sequences of POS tags to capturerough syntactic information.
The resulting vocabu-lary consisted of 276 words and 56 POS tags.4.3 Support Vector MachinesSupport vector machines (SVMs) are a machinelearning technique used in a variety of text classi-fication problems.
SVMs are based on the principleof structural risk minimization.
Viewing the data aspoints in a high-dimensional feature space, the goalis to fit a hyperplane between the positive and neg-ative examples so as to maximize the distance be-tween the data points and the plane.
SVMs were in-troduced by Vapnik (1995) and were popularized inthe area of text classification by Joachims (1998a).The unit of classification in this work is a singlearticle.
Our SVM classifiers for reading level use thefollowing features:?
Average sentence length?
Average number of syllables per word?
Flesch-Kincaid score?
6 out-of-vocabulary (OOV) rate scores.?
Parse features (per sentence):?
Average parse tree height?
Average number of noun phrases?
Average number of verb phrases?
Average number of ?SBAR?s.1?
12 language model perplexity scoresThe OOV scores are relative to the most common100, 200 and 500 words in the lowest grade level1SBAR is defined in the Penn Treebank tag set as a ?clauseintroduced by a (possibly empty) subordinating conjunction.?
Itis an indicator of sentence complexity.526(grade 2) 2.
For each article, we calculated the per-centage of a) all word instances (tokens) and b) allunique words (types) not on these lists, resulting inthree token OOV rate features and three type OOVrate features per article.The parse features are generated using the Char-niak parser (Charniak, 2000) trained on the standardWall Street Journal Treebank corpus.
We chose touse this standard data set as we do not have anydomain-specific treebank data for training a parser.Although clearly there is a difference between newstext for adults and news articles intended for chil-dren, inspection of some of the resulting parsesshowed good accuracy.Ideally, the language model scores would be forLMs from domain-specific training data (i.e.
moreWeekly Reader data.)
However, our corpus is lim-ited and preliminary experiments in which the train-ing data was split for LM and SVM training wereunsuccessful due to the small size of the resultingdata sets.
Thus we made use of the Britannica andCNN articles to train models of three n-gram or-ders on ?child?
text and ?adult?
text.
This resultedin 12 LM perplexity features per article based ontrigram, bigram and unigram LMs trained on Bri-tannica (adult), Britannica Elementary, CNN (adult)and CNN abridged text.For training SVMs, we used the SVMlight toolkitdeveloped by Joachims (1998b).
Using developmentdata, we selected the radial basis function kerneland tuned parameters using cross validation and gridsearch as described in (Hsu et al, 2003).5 Experiments5.1 Test Data and Evaluation CriteriaWe divide the Weekly Reader corpus described inSection 3 into separate training, development, andtest sets.
The number of articles in each set is shownin Table 3.
The development data is used as a testset for comparing classifiers, tuning parameters, etc,and the results presented in this section are based onthe test set.We present results in three different formats.
Foranalyzing our binary classifiers, we use DetectionError Tradeoff (DET) curves and precision/recall2These lists are chosen from the full vocabulary indepen-dently of the feature selection for LMs described above.Grade Training Dev/Test2 315 183 529 304 690 385 623 34Table 3: Number of articles in the Weekly Readercorpus as divided into training, development and testsets.
The dev and test sets are the same size and eachconsist of approximately 5% of the data for eachgrade level.measures.
For comparison to other methods, e.g.Flesch-Kincaid and Lexile, which are not binaryclassifiers, we consider the percentage of articleswhich are misclassified by more than one gradelevel.Detection Error Tradeoff curves show the tradeoffbetween misses and false alarms for different thresh-old values for the classifiers.
?Misses?
are positiveexamples of a class that are misclassified as neg-ative examples; ?false alarms?
are negative exam-ples misclassified as positive.
DET curves have beenused in other detection tasks in language processing,e.g.
Martin et al (1997).
We use these curves to vi-sualize the tradeoff between the two types of errors,and select the minimum cost operating point in or-der to get a threshold for precision and recall calcu-lations.
The minimum cost operating point dependson the relative costs of misses and false alarms; itis conceivable that one type of error might be moreserious than the other.
After consultation with teach-ers (future users of our system), we concluded thatthere are pros and cons to each side, so for the pur-pose of this analysis we weighted the two types oferrors equally.
In this work, the minimum cost op-erating point is selected by averaging the percent-ages of misses and false alarms at each point andchoosing the point with the lowest average.
Unlessotherwise noted, errors reported are associated withthese actual operating points, which may not lie onthe convex hull of the DET curve.Precision and recall are often used to assess in-formation retrieval systems, and our task is similar.Precision indicates the percentage of the retrieveddocuments that are relevant, in this case the per-centage of detected documents that match the target527grade level.
Recall indicates the percentage of thetotal number of relevant documents in the data setthat are retrieved, in this case the percentage of thetotal number of documents from the target level thatare detected.5.2 Language Model Classifier1   2     5    10    20    40    60    80    90    125102040608090False Alarm probability (in %)Missprobability(in%)grade 2grade 3grade 4grade 5Figure 1: DET curves (test set) for classifiers basedon trigram language models.Figure 1 shows DET curves for the trigram LM-based classifiers.
The minimum cost error rates forthese classifiers, indicated by large dots in the plot,are in the range of 33-43%, with only one over 40%.The curves for bigram and unigram models havesimilar shapes, but the trigram models outperformthe lower-order models.
Error rates for the bigrammodels range from 37-45% and the unigram mod-els have error rates in the 39-49% range, with all butone over 40%.
Although our training corpus is smallthe feature selection described in Section 4.2 allowsus to use these higher-order trigram models.5.3 Support Vector Machine ClassifierBy combining language model scores with other fea-tures in an SVM framework, we achieve our bestresults.
Figures 2 and 3 show DET curves for thisset of classifiers on the development set and testset, respectively.
The grade 2 and 5 classifiers havethe best performance, probably because grade 3 and4 must be distinguished from other classes at bothhigher and lower levels.
Using threshold values se-lected based on minimum cost on the development1   2     5    10    20    40    60    80    90    125102040608090False Alarm probability (in %)Missprobability(in%)grade 2grade 3grade 4grade 5Figure 2: DET curves (development set) for SVMclassifiers with LM features.1   2     5    10    20    40    60    80    90    125102040608090False Alarm probability (in %)Missprobability(in%)grade 2grade 3grade 4grade 5Figure 3: DET curves (test set) for SVM classifierswith LM features.set, indicated by large dots on the plot, we calcu-lated precision and recall on the test set.
Results arepresented in Table 4.
The grade 3 classifier has highrecall but relatively low precision; the grade 4 classi-fier does better on precision and reasonably well onrecall.
Since the minimum cost operating points donot correspond to the equal error rate (i.e.
equal per-centage of misses and false alarms) there is variationin the precision-recall tradeoff for the different gradelevel classifiers.
For example, for class 3, the oper-ating point corresponds to a high probability of falsealarms and a lower probability of misses, which re-sults in low precision and high recall.
For operatingpoints chosen on the convex hull of the DET curves,the equal error rate ranges from 12-25% for the dif-528Grade Precision Recall2 38% 61%3 38% 87%4 70% 60%5 75% 79%Table 4: Precision and recall on test set for SVM-based classifiers.Grade ErrorsFlesch-Kincaid Lexile SVM2 78% 33% 5.5%3 67% 27% 3.3%4 74% 26% 13%5 59% 24% 21%Table 5: Percentage of articles which are misclassi-fied by more than one grade level.ferent grade levels.We investigated the contribution of individual fea-tures to the overall performance of the SVM clas-sifier and found that no features stood out as mostimportant, and performance was degraded when anyparticular features were removed.5.4 ComparisonWe also compared error rates for the best per-forming SVM classifier with two traditional read-ing level measures, Flesch-Kincaid and Lexile.
TheFlesch-Kincaid Grade Level index is a commonlyused measure of reading level based on the averagenumber of syllables per word and average sentencelength.
The Flesch-Kincaid score for a document isintended to directly correspond with its grade level.We chose the Lexile measure as an example of areading level classifier based on word lists.3 Lexilescores do not correlate directly to numeric grade lev-els, however a mapping of ranges of Lexile scores totheir corresponding grade levels is available on theLexile web site (Lexile, 2005).For each of these three classifiers, Table 5 showsthe percentage of articles which are misclassified bymore than one grade level.
Flesch-Kincaid performspoorly, as expected since its only features are sen-3Other classifiers such as Dale-Chall do not have automaticsoftware available.tence length and average syllable count.
Althoughthis index is commonly used, perhaps due to its sim-plicity, it is not accurate enough for the intendedapplication.
Our SVM classifier also outperformsthe Lexile metric.
Lexile is a more general measurewhile our classifier is trained on this particular do-main, so the better performance of our model is notentirely surprising.
Importantly, however, our clas-sifier is easily tuned to any corpus of interest.To test our classifier on data outside the WeeklyReader corpus, we downloaded 10 randomly se-lected newspaper articles from the ?Kidspost?
edi-tion of The Washington Post (2005).
?Kidspost?
isintended for grades 3-8.
We found that our SVMclassifier, trained on the Weekly Reader corpus, clas-sified four of these articles as grade 4 and seven ar-ticles as grade 5 (with one overlap with grade 4).These results indicate that our classifier can gener-alize to other data sets.
Since there was no trainingdata corresponding to higher reading levels, the bestperformance we can expect for adult-level newspa-per articles is for our classifiers to mark them as thehighest grade level, which is indeed what happenedfor 10 randomly chosen articles from standard edi-tion of The Washington Post.6 Conclusions and Future WorkStatistical LMs were used to classify texts basedon reading level, with trigram models being no-ticeably more accurate than bigrams and unigrams.Combining information from statistical LMs withother features using support vector machines pro-vided the best results.
Future work includes testingadditional classifier features, e.g.
parser likelihoodscores and features obtained using a syntax-basedlanguage model such as Chelba and Jelinek (2000)or Roark (2001).
Further experiments are plannedon the generalizability of our classifier to text fromother sources (e.g.
newspaper articles, web pages);to accomplish this we will add higher level text asnegative training data.
We also plan to test thesetechniques on languages other than English, and in-corporate them with an information retrieval systemto create a tool that may be used by teachers to helpselect reading material for their students.529AcknowledgmentsThis material is based upon work supported by the National Sci-ence Foundation under Grant No.
IIS-0326276.
Any opinions,findings, and conclusions or recommendations expressed in thismaterial are those of the authors and do not necessarily reflectthe views of the National Science Foundation.Thank you to Paul Heavenridge (Literacyworks), the WeeklyReader Corporation, Regina Barzilay (MIT) and Noemie El-hadad (Columbia University) for sharing their data and corpora.ReferencesR.
Barzilay and N. Elhadad.
Sentence alignment for monolin-gual comparable corpora.
In Proc.
of EMNLP, pages 25?32,2003.C.
Boulis and M. Ostendorf.
Text classification by aug-menting the bag-of-words representation with redundancy-compensated bigrams.
Workshop on Feature Selection inData Mining, in conjunction with SIAM conference on DataMining, 2005.P.
Bylsma, L. Ireland, and H. Malagon.
Educating English Lan-guage Learners in Washington State.
Office of the Superin-tendent of Public Instruction, Olympia, WA, 2003.J.S.
Chall and E. Dale.
Readability revisited: the new Dale-Chall readability formula.
Brookline Books, Cambridge,Mass., 1995.E.
Charniak.
A maximum-entropy-inspired parser.
In Proc.
ofNAACL, pages 132?139, 2000.C.
Chelba and F. Jelinek.
Structured Language Modeling.Computer Speech and Language, 14(4):283-332, 2000.S.
Chen and J. Goodman.
An empirical study of smoothingtechniques for language modeling.
Computer Speech andLanguage, 13(4):359?393, 1999.K.
Collins-Thompson and J. Callan.
A language model-ing approach to predicting reading difficulty.
In Proc.
ofHLT/NAACL, pages 193?200, 2004.R.
Gunning.
The technique of clear writing.
McGraw-Hill,New York, 1952.C.-W. Hsu et al A practical guide to support vector classi-fication.
http://www.csie.ntu.edu.tw/?cjlin/papers/guide/guide.pdf, 2003.
Accessed 11/2004.T.
Joachims.
Text categorization with support vector machines:learning with many relevant features.
In Proc.
of the Eu-ropean Conference on Machine Learning, pages 137?142,1998a.T.
Joachims.
Making large-scale support vector machine learn-ing practical.
In Advances in Kernel Methods: Support Vec-tor Machines.
B. Scho?lkopf, C. Burges, A. Smola, eds.
MITPress, Cambridge, MA, 1998b.J.P.
Kincaid, Jr., R.P.
Fishburne, R.L.
Rodgers, andB.S.
Chisson.
Derivation of new readability formulas forNavy enlisted personnel.
Research Branch Report 8-75, U.S.Naval Air Station, Memphis, 1975.Y.-B.
Lee and S.H.
Myaeng.
Text genre classification withgenre-revealing and subject-revealing features.
In Proc.
ofSIGIR, pages 145?150, 2002.The Lexile framework for reading.
http://www.lexile.com, 2005.
Accessed April 15, 2005.A.
Martin, G. Doddington, T. Kamm, M. Ordowski, andM.
Przybocki.
The DET curve in assessment of detectiontask performance.
Proc.
of Eurospeech, v. 4, pp.
1895-1898,1997.A.
Ratnaparkhi.
A maximum entropy part-of-speech tagger.
InProc.
of EMNLP, pages 133?141, 1996.B.
Roark.
Probabilistic top-down parsing and language model-ing.
Computational Linguistics, 27(2):249-276, 2001.L.
Si and J.P. Callan.
A statistical model for scientific readabil-ity.
In Proc.
of CIKM, pages 574?576, 2001.A.J.
Stenner.
Measuring reading comprehension with the Lex-ile framework.
Presented at the Fourth North American Con-ference on Adolescent/Adult Literacy, 1996.A.
Stolcke.
SRILM - an extensible language modeling toolkit.Proc.
ICSLP, v. 2, pp.
901-904, 2002.U.S.
Department of Education, National Center for Ed-ucational Statistics.
The condition of education.http://nces.ed.gov/programs/coe/2003/section1/indicator04.asp, 2003.
Accessed June18, 2004.U.S.
Department of Education, National Center for EducationalStatistics.
NCES fast facts: Bilingual education/LimitedEnglish Proficient students.
http://nces.ed.gov/fastfacts/display.asp?id=96, 2003.
AccessedJune 18, 2004.V.
Vapnik.
The Nature of Statistical Learning Theory.
Springer,New York, 1995.The Washington Post.
http://www.washingtonpost.com, 2005.
Accessed April 20, 2005.Weekly Reader.
http://www.weeklyreader.com,2004.
Accessed July, 2004.Western/Pacific Literacy Network / Literacyworks.
CNNSF learning resources.
http://literacynet.org/cnnsf/, 2004.
Accessed June 15, 2004.530
