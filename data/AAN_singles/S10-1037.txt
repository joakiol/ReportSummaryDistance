Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 174?177,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsBUAP: An Unsupervised Approach to Automatic Keyphrase Extractionfrom Scientific ArticlesRoberto Ortiz, David Pinto, Mireya TovarFaculty of Computer Science, BUAPPuebla, Mexicokorn resorte2003@hotmail.com,{dpinto, mtovar}@cs.buap.mxHe?ctor Jime?nez-SalazarInformation Technologies Dept., UAMDF, Mexicohgimenezs@gmail.comAbstractIn this paper, it is presented an unsuper-vised approach to automatically discoverthe latent keyphrases contained in scien-tific articles.
The proposed technique isconstructed on the basis of the combi-nation of two techniques: maximal fre-quent sequences and pageranking.
Weevaluated the obtained results by usingmicro-averaged precision, recall and F-scores with respect to two different goldstandards: 1) reader?s keyphrases, and 2)a combined set of author?s and reader?skeyphrases.
The obtained results werealso compared against three different base-lines: one unsupervised (TF-IDF based)and two supervised (Na?
?ve Bayes andMaximum Entropy).1 IntroductionThe task of automatic keyphrase extraction hasbeen studied for several years.
Firstly, as semanticmetadata useful for tasks such as summarization(Barzilay and Elhadad, 1997; Lawrie et al, 2001;DAvanzo and Magnini, 2005), but later rec-ognizing the impact that good keyphraseswould have on the quality of various Nat-ural Language Processing (NLP) applica-tions (Frank et al, 1999; Witten et al, 1999;Turney, 1999; Barker and Corrnacchia, 2000;Medelyan and Witten, 2008).
Thus, the selectionof important, topical phrases from within thebody of a document may be used in order toimprove the performance of systems dealingwith different NLP problems such as, clustering,question-answering, named entity recognition,information retrieval, etc.In general, a keyphrase may be considered asa sequence of one or more words that capture themain topic of the document, as that keyphrase isexpected to represent one of the key ideas ex-pressed by the document author.
Following thepreviously mentioned hypothesis, we may take ad-vantage of two different techniques of text analy-sis: maximal frequent sequences to extract a se-quence of one or more words from a given text,and pageranking, expecting to extract those wordsequences that represent the key ideas of the au-thor.The interest on extracting high qualitykeyphrases from raw text has motivated forums,such as SemEval, where different systems mayevaluate their performances.
The purpose ofSemEval is to evaluate semantic analysis systems.In particular, in this paper we are reporting theresults obtained in Task #5 of SemEval-2 2010,which has been named: ?Automatic KeyphraseExtraction from Scientific Articles?.
We focusedthis paper on the description of our approach and,therefore, we do not describe into detail the tasknor the dataset used.
For more information aboutthis information read the ?Task #5 Descriptionpaper?, also published in this proceedings volume(Nam Kim et al, 2010).The rest of this paper is structured as follows.Section 2 describes into detail the components ofthe proposed approach.
In Section 3 it is shownthe performance of the presented system.
Finally,in Section 4 a discussion of findings and furtherwork is given.2 Description of the approachThe approach presented in this paper relies on thecombination of two different techniques for select-ing the most prominent terms of a given text: max-imal frequent sequences and pageranking.
In Fig-ure 1 we may see this two step approach, wherewe are considering a sequence to be equivalent toan n-gram.
The complete description of the pro-cedure is given as follows.We select maximal frequent sequences which174we consider to be candidate keyphrases and, there-after, we ranking them in order to determine whichones are the most importants (according to thepageranking algorithm).
In the following subsec-tions we give a brief description of these two tech-niques.
Afterwards, we provide an algorithm ofthe presented approach.Figure 1: Two step approach of BUAP Team at theTask #5 of SemEval-22.1 Maximal Frequent SequencesDefinition: If a sequence p is a subsequence of qand the number of elements in p is equal to n, thenthe p is called an n-gram in q.Definition: A sequence p = a1?
?
?
akis a sub-sequence of a sequence q if all the items aioccurin q and they occur in the same order as in p. Ifa sequence p is a subsequence of a sequence q wesay that p occurs in q.Definition: A sequence p is frequent in S if p isa subsequence of at least ?
documents in S where?
is a given frequency threshold.
Only one oc-currence of sequence in the document is counted.Several occurrences within one document do notmake the sequence more frequent.Definition: A sequence p is a maximal frequentsequence in S if there does not exists any sequenceq in S such that p is a subsequence of q and p isfrequent in S.2.2 PageRankingThe algorithm of PageRanking was defined byBrin and Page in (Brin and Page, 1998).
It is agraph-based algorithm used for ranking webpages.The algorithm considers input and output links ofeach page in order to construct a graph, whereeach vertex is a webpage and each edge may bethe input or output links for this webpage.
Theydenote as In(Vi) the set of input links of webpageVi, and Out(Vi) their output links.
The algorithmproposed to rank each webpage based on the vot-ing or recommendation of other webpages.
Thehigher the number of votes that are cast for a ver-tex, the higher the importance of the vertex.
More-over, the importance of the vertex casting the votedetermines how important the vote itself is, andthis information is also taken into account by theranking model.Although this algoritm has been initially pro-posed for webpages ranking, it has been also usedfor other NLP applications which may model theircorresponding problem in a graph structure.
Eq.
(1) is the formula proposed by Brin and Page.S(Vi) = (1 ?
d) + d ?
?j?In(Vi)1|Out(Vj)|S(Vj)(1)where d is a damping factor that can be set be-tween 0 and 1, which has the role of integrat-ing into the model the probability of jumpingfrom a given vertex to another random vertexin the graph.
This factor is usually set to 0.85(Brin and Page, 1998).There are some other propossals, like the onepresented in (Mihalcea and Tarau, 2004), where atextranking algorithm is presented.
The authorsconsider a weighted version of PageRank andpresent some applications to NLP using unigrams.They also construct multi-word terms by exploringthe conections among ranked words in the graph.Our algorithm differs from textranking in that weuse MFS for feeding the PageRanking algorithm.2.3 AlgorithmThe complete algoritmic description of the pre-sented approach is given in Algorithm 1.
Read-ers and writers keyphrases may be quite dif-ferent.
In particular, writers usually introduceacronyms in their text, but they use the completeor expanded representation of these acronymsfor their keyphrases.
Therefore, we have in-cluded a module (Extract Acronyms) for ex-tracting both, acronyms with their correspondingexpanded version, which are used afterwards asoutput of our system.
We have preprocessed thedataset removing stopwords and punctuation sym-bols.
Lemmatization (TreeTagger1) and stemming(Porter Stemmer (Porter, 1980)) were also appliedin some stages of preprocessing.The Maximal Freq Sequences module ex-tracts maximal frequent sequences of words andwe feed the PageRaking module (PageRanking)1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/175with all these sequences for determining the mostimportant ones.
We use the structure of the sci-entific articles in order to determine in and outlinks of the sequences found.
In fact, we use aneighborhood criterion (a pair of MFS in the samesentence) for determining the links between thoseMFS?s.
Once the ranking is calculated, we may se-lect those sequences of a given length (unigrams,bigrams and trigrams) as output of our system.
Wealso return a maximum of three acronyms, andtheir associated multiterm phrases (MultiTerm),as candidate keyphrases.
Determining the lengthand quantity of the sequences (n-grams) was ex-perimentally deduced from the training corpus.Algorithm 1: Algorithm of the Two Step ap-proach for the Task #5 at SemEval-2Input: A document set: D = {d1, d2, ?
?
?
}Output: A set K = {K1,K2, ?
?
? }
ofkeyphrases for each document di:Ki= {ki,1, ki,2, ?
?
?
}foreach di?
D do1AcronymSet = Extract Acronyms(di);2d1i= Pre Processing(di);3MFS = Maximal Freq Sequences(d1i);4CK = PageRanking(d1i, MFS);5CU = Top Nine Unigrams(CK);6CT = Top Three Trigrams(CK);7Ki= CT ;8NU = 0;9Acronyms = 0;10foreach unigram ?
CU do11if unigram ?
AcronymSet then12if Acronyms < 3 then13Ki= Ki?
{unigram};14EA = MultiTerm(unigram);15Ki= Ki?
{EA};16Acronyms++;17end18else19Ki= Ki?
{unigram};20NU++;21end22end23N = (15?
(2?Acronyms+|CT |+NU));24CB = Top N Bigrams(CK, N );25Ki= Ki?CB;26end27return K = {K1,K2, ?
?
?
}28In this edition of the Task #5 of SemEval-22010, we tested three different runs, which werenamed: BUAP ?
1, BUAP ?
2 and BUAP ?
3.Definition and differences among the three runsare given in Table 3.The results obtained with each run, togetherwith three different baselines are given in the fol-lowing section.3 Experimental resultsIn all tables, P , R, F mean micro-averaged pre-cision, recall and F -scores.
For baselines, therewere provided 1,2,-3 grams as candidates andTFIDF as features.
In Table 2, TFIDF is anunsupervised method to rank the candidates basedon TFIDF scores.
NB and ME are super-vised methods using Na?
?ve Bayes and maximumentropy in WEKA.
In second column, R meansto use the reader-assigned keyword set as gold-standard data and C means to use both author-assigned and reader-assigned keyword sets as an-swers.Notice from Tables 2 and 3 that we outper-formed all the baselines for the Top 15 candidates.However, the Top 10 candidates were only outper-formed by the Reader-Assigned keyphrases found.This implies that the Writer keyphrases we ob-tained were not of as good as the Reader ones.
Aswe mentioned, readers and writers assign differentkeywords.
The former write keyphrases based onthe lecture done, by the latter has a wider contextand their keyphrases used to be more complex.
Weplan to investigate this issue in the future.4 ConclusionsWe have presented an approach based on the ex-traction of maximal frequent sequences which arethen ranked by using the pageranking algorithm.Three different runs were tested, modifying thepreprocessing stage and the number of bigramsgiven as output.
We did not see an improve-ment when we used lemmatization of the docu-ments.
The run which obtained the best resultswas ranking by the organizer according to the top15 best keyphrases, however, we may see that ourruns need to be analysed more into detail in orderto provide a re-ranking procedure for the best 15keyphrases found.
This procedure may improvethe top 5 candidates precision.176Run name DescriptionBUAP ?
1 : This run is exactly the one described in Algorithm 1.BUAP ?
2 : Same as BUAP ?
1 but lemmatization was applied a priori and stemming at the end.BUAP ?
3 : Same as BUAP ?
2 but output twice the number of bigrams.Table 1: Description of the three runs submitted to the Task #5 of SemEval-2 2010Method by top 5 candidates top 10 candidates top 15 candidatesP R F P R F P R FTF ?
IDF R 17.80% 7.39% 10.44% 13.90% 11.54% 12.61% 11.60% 14.45% 12.87%C 22.00% 7.50% 11.19% 17.70% 12.07% 14.35% 14.93% 15.28% 15.10%NB R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%ME R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%Table 2: BaselinesMethod by top 5 candidates top 10 candidates top 15 candidatesP R F P R F P R FBUAP ?
1 R 10.40% 4.32% 6.10% 13.90% 11.54% 12.61% 14.93% 18.60% 16.56%C 13.60% 4.64% 6.92% 17.60% 12.01% 14.28% 19.00% 19.44% 19.22%BUAP ?
2 R 10.40% 4.32% 6.10% 13.80% 11.46% 12.52% 14.67% 18.27% 16.27%C 14.40% 4.91% 7.32% 17.80% 12.14% 14.44% 18.73% 19.17% 18.95%BUAP ?
3 R 10.40% 4.32% 6.10% 12.10% 10.05% 10.98% 12.33% 15.37% 13.68%C 14.40% 4.91% 7.32% 15.60% 10.64% 12.65% 15.67% 16.03% 15.85%Table 3: The three different runs submitted to the competitionAcknowledgmentsThis work has been partially supported by CONA-CYT (Project #106625) and PROMEP (Grant#103.5/09/4213).References[Barker and Corrnacchia2000] K. Barker and N. Cor-rnacchia.
2000.
Using noun phrase heads to extractdocument keyphrases.
In 13th Biennial Conferenceof the Canadian Society on Computational Studiesof Intelligence: Advances in Artificial Intelligence.
[Barzilay and Elhadad1997] R. Barzilay and M. El-hadad.
1997.
Using lexical chains for text sum-marization.
In ACL/EACL 1997 Workshop on Intel-ligent Scalable Text Summarization, pages 10?17.
[Brin and Page1998] S. Brin and L. Page.
1998.
Theanatomy of a large-scale hypertextual web searchengine.
In COMPUTER NETWORKS AND ISDNSYSTEMS, pages 107?117.
Elsevier Science Pub-lishers B. V.[DAvanzo and Magnini2005] E. DAvanzo andB.
Magnini.
2005.
A keyphrase-based approachto summarization:the lake system.
In DocumentUnderstanding Conferences (DUC-2005).
[Frank et al1999] E. Frank, G.W.
Paynter, I. Witten,C.
Gutwin, and C.G.
Nevill-Manning.
1999.
Do-main specific keyphrase extraction.
In 16th Interna-tional Joint Conference on AI, pages 668?673.
[Lawrie et al2001] D. Lawrie, W. B. Croft, andA.
Rosenberg.
2001.
Finding topic words for hi-erarchical summarization.
In SIGIR 2001.
[Medelyan and Witten2008] O. Medelyan and I. H.Witten.
2008.
Domain independent automatickeyphrase indexing with small training sets.
Jour-nal of American Society for Information Science andTechnology, 59(7):1026?1040.
[Mihalcea and Tarau2004] R. Mihalcea and P. Tarau.2004.
Textrank: Bringing order into texts.
InEMNLP 2004, ACL, pages 404?411.
[Nam Kim et al2010] S. Nam Kim, O. Medelyan, andM.Y.
Kan. 2010.
Semeval-2010 task5: Auto-matic keyphrase extraction from scientific articles.In Proceedings of the Fifth International Workshopon Semantic Evaluations (SemEval-2010).
Associa-tion for Computational Linguistics.
[Porter1980] M. F. Porter.
1980.
An algorithm for suf-fix stripping.
Program, 14(3).
[Turney1999] P. Turney.
1999.
Learning to extractkeyphrases from text.
Technical Report ERB-1057.
(NRC #41622), National Research Council, Institutefor Information Technology.
[Witten et al1999] I. Witten, G. Paynter, E. Frank,C.
Gutwin, and G. Nevill-Manning.
1999.Kea:practical automatic key phrase extraction.
Infourth ACM conference on Digital libraries, pages254?256.177
