Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 520?529,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsInformation Structure Prediction for Visual-world Referring ExpressionsMicha ElsnerDepartment of Linguistics,The Ohio State Universitymelsner@ling.osu.eduHannah RohdeLinguistics & English Language,University of Edinburghhannah.rohde@ed.ac.ukAlasdair D. F. ClarkeSchool of Informatics,University of Edinburgha.clarke@ed.ac.ukAbstractWe investigate the order of mention forobjects in relational descriptions in visualscenes.
Existing work in the visual do-main focuses on content selection for textgeneration and relies primarily on tem-plates to generate surface realizations fromunderlying content choices.
In contrast,we seek to clarify the influence of visualperception on the linguistic form (as op-posed to the content) of descriptions, mod-eling the variation in and constraints onthe surface orderings in a description.
Wefind previously-unknown effects of the vi-sual characteristics of objects; specifically,when a relational description involves a vi-sually salient object, that object is morelikely to be mentioned first.
We conducta detailed analysis of these patterns usinglogistic regression, and also train and eval-uate a classifier.
Our methods yield signif-icant improvement in classification accu-racy over a naive baseline.1 IntroductionVisual-world referring expression generation(REG) is the task of instructing a listener howto find an object (the target) in a visual scene.In complicated scenes, people often producerelational descriptions, in which the target objectis described relative to another (a landmark)(Viethen and Dale, 2008).
While existing REGsystems can generate relational descriptions,they tend to focus on content selection (that is,choosing an appropriate set of landmarks foreach object).
Surface realization (turning theselected content into a string of words) is handledby simple heuristics, such as sets of templates.Complex descriptions, however, have a non-trivialinformation structure?
objects are not mentionedin an arbitrary order.
Numerous studies innon-visual domains show that English speakersfavor constructions that place familiar (given)information before unfamiliar (new) (Bresnan etal., 2007; Ward and Birner, 2001; Prince, 1981).We show that this pattern also holds for visual-world referring expressions (REs), and moreover,that objects with sufficient visual prominence aretreated as given.
Thus, we argue that the conceptof salience used in surface realization shouldincorporate metrics from visual perception.In this study, we create a model of informationordering in complex relational descriptions.
Us-ing a discriminative classifier, we learn to predictthe information structuring strategies used in ourcorpus.
We compare these strategies to the typicalgiven/new pattern of English discourse.
Experi-ments on a corpus of descriptions of cartoon peo-ple in the childrens?
book ?Where?s Wally?
(Hand-ford, 1987), corpus described in (Clarke et al.,2013), show that our approach significantly out-performs a naive baseline, improving especiallyon prediction of non-canonical orderings.This study has three main contributions.
First,it demonstrates that humans use sophisticated in-formation ordering strategies for REG, and there-fore that the template strategies used in previouswork do not adequately model human production.Second, it makes a practical proposal for an im-proved model which is capable of predicting theseorderings; while this model is not a full-scale sur-face realizer, we view it as an important interme-diate step towards one.
Finally, it makes a the-oretical contribution: By linking the informationstructures observed in the data to the existing re-520search on salience and information structure, weshow that visually prominent objects are treatedas part of common ground despite the lack of pre-vious mention.2 Related workComputational models of REG (Krahmer and vanDeemter, 2012) focus mainly on content selection:Given a list of objects in the scene and their visualattributes, such models decide what information toinclude in a description so as to specify the tar-get object.
Early systems (with the exception ofDale and Haddock (1991)) did not produce rela-tional descriptions.
Nor did these systems modelthe visual salience of the objects or attributes un-der discussion.Later models (Kelleher et al., 2005; Kelleherand Kruijff, 2006; Duckham et al., 2010) intro-duce simple models of visual salience, promptedby psycholinguistic research which shows that ob-jects are more likely to be selected as landmarkswhen they are easy for an observer to find (Beunand Cremers, 1998).
Clarke et al.
(2013) extendthese results with a more complicated model ofvisual salience (Torralba et al., 2006).
Fang et al.
(2013) similarly note that generated REs shouldavoid information that is perceptually expensive toobtain.
However, these results focus on content se-lection rather than surface realization.In comparison to selection, surface realizationfor REG has received little attention.
Many re-searchers do not even perform realization, but sim-ply compare their systems?
selected content withthe gold standard under metrics like the Dice co-efficient.
The TUNA challenges (Gatt et al., 2008;Gatt et al., 2009; Gatt and Belz, 2010) are an ex-ception; participants were required to provide sur-face realizations, which were evaluated via NIST,BLEU and string edit distance.
Many partici-pants used a template-based realizer written byIrene Langkilde-Geary, which imposes a fixed or-dering on attributes like ?size?
and ?color?
buthas no provisions for relational descriptions.
Afew participants created their own realizers.
Brug-man et al.
(2009) describe a system with multi-ple hand-written templates.
Di Fabbrizio et al.
(2008) propose several learning-based systems;the most effective were a dependency-based ap-proach which learned precedence relationships be-tween pairs of words, and a template-based ap-proach which learned global orderings over sets ofattributes.
Neither approach is designed to handlerelational descriptions, nor do they incorporate vi-sual information.
Duan et al.
(2013), also studyingthe Wally corpus, demonstrates that visual featuresaffect determiner choice for NPs, but do not studyinformation structure.Several studies give basic principles for infor-mation structure in English discourse.
Prince(1981) introduces the key distinctions betweendiscourse-old and new entities (previously men-tioned vs not mentioned) and hearer-old and newentities (familiar to the listener vs not familiar).Clark and Wilkes-Gibbs (1986) extends the latterdistinction to a notion of common ground; entitiesin the common ground are familiar to both par-ticipants in the discourse, and each participant isin turn aware of the other?s familiarity.
As notedby Prince (1981) and expanded on by Ward andBirner (2001) and in Centering Theory (Grosz etal., 1995), the first element in an English sentenceis generally reserved for old information, whilenew information is usually placed at the end.
Forinstance, see these (contrived) examples:(1) a. Obama adopted a dog named Bo.b.
#A dog named Bo was adopted byObama.Ex.
(1-a) demonstrates the standard order (un-der the assumption that Obama is familiar to areader of this paper while Bo may not be).
(1-b)violates the ordering principles and is likely tobe judged less felicitous.
Importantly, Obama ishearer-old not because of a preceding discoursemention but due to (assumed) general knowl-edge; it is an unused (Prince, 1981), or existential(Bean and Riloff, 1999) entity.
General knowl-edge shared by speakers of a community is oneway in which an entity enters the common ground.Along with this shared socio-cultural background,speakers may also share physical co-presence andlinguistic co-presence (Clark, 1996).
They can in-dicate salient entities, individuals, or entire eventsby engaging their listener in joint attention viapointing or gaze cueing (Baldwin, 1995; Carpen-ter et al., 1998); in this paper, we demonstrate thatvisual prominence is also sufficient.Maienborn (2001) explicitly suggests that thistopic-comment structure principle is the motiva-tion for the frequent appearance of locative modi-fiers in clause-initial position; however, she givesno felicity conditions on when this leftward move-ment is expected.
Since most of the modifiers in521this study are locatives, our data should be taken asendorsing this theoretical position, but supplyingfelicity conditions in terms of common ground.These principles have been applied to compu-tational surface realization in non-visual domains(Webber, 2004; Nakatsu and White, 2010, andothers).
Freer-word-order languages such as Ger-man also have predictable information structureswhich have been employed in surface realizationsystems, but these require a different structuralanalysis than in English (Zarrie?
et al., 2012; Fil-ippova and Strube, 2007).3 Information structures in our corpusIn this section, we define the particular orderingstrategies which we investigate in the rest of thepaper.
We begin by defining some terms: A re-lational description includes two objects, the an-chor, which is the object being located, and thelandmark, an object which is mentioned to makeit easier to locate the anchor.
The anchor may bethe target of the entire expression, or it may in turnserve as a landmark in another relational descrip-tion (as in ?the man next to the horse next to thebuilding?
where ?horse?
serves as both a landmarkfor ?man?
and an anchor for ?building?.1TheREs in this corpus reflect the variation in the wayspeakers constructed their descriptions: Some pro-duced multiple complete sentences; others usedabbreviated language and compacted their expres-sion into a single sentence or phrase.
In this pa-per we use the term ?ordering?
to refer to speak-ers?
decisions of whether to precede or postpose areference to one object relative to their referenceto another.
In this way, the ?syntax?
of the de-scription is built out of references to particular ob-jects (the noun phrases) and the relationships be-tween those references.
Note that the referencesmay consist of a short phrase (?the man with thesword?)
or an entire clause (?he is standing andholding a sword?
)In our corpus, speakers use three primary strate-gies to order anchors and landmarks, exemplifiedby the following REs from our corpus (shown withbold for text describing the anchor and italics fortext for landmarks):(2) Near the hut that is burning, there is a manholding a lit torch in one hand, and asword in the other.1In our examples below, the anchor is the target of theoverall expression, i.e., the intended referent in the REG task.
(3) Man closest to the rear tyre of the van.
(4) There is a person standing in the waterwearing a blue shirt and yellow hatEx.
(2) places the landmark so that it precedesthe anchor; Ex.
(3) shows the landmark follow-ing it.
Ex.
(4) shows a more complex structure,which we refer to as interleaved, where informa-tion about the anchor is given in multiple phrasesand the landmark phrase appears between them.2(These orders are determined with respect to thefirst mention of the landmark.)
We denote theseordering strategies as PRECEDE, FOLLOW and IN-TER respectively.We also distinguish between landmarks whichare only mentioned in relation to an anchor andthose which are first introduced in a non-relativeconstruction such as ?look at the X?
or ?there?s anX?
:(5) There is a horse rearing up on its hind legs.Behind the horse is a man laying down onhis back completely flat and straight.Since these constructions establish the existenceof a landmark without immediately incorporatingit into the description, we denote these as ESTAB-LISH constructions.Finally, our annotation scheme distinguishesbetween genuine landmarks (visible objects orgroups of objects in the scene) and image regionslike ?the left?
or ?bottom center?
:(6) Bottom center, man looking left4 DatasetWe use a collection of referring expressionselicited on Mechanical Turk, previously describedin (Clarke et al., 2013).3The dataset containsdescriptions of targets in 11 images from thechildrens?
book Where?s Wally4(Handford, 1987;Handford, 1988); in each image, 16 people weredesignated as targets.
Each participant saw eachscene only once.
An example scene is shown inFigure 1.
The participant was instructed to type adescription of the person in the red box so that an-other person viewing the same scene (but withoutthe box) would be able to find them; to make sure2This structure is not syntactically discontinuous, but vi-sually it is; if the listener wants to confirm these details visu-ally, they must first look at the person, then look away at thewater and then look back at the person.3Via http://datashare.is.ed.ac.uk/handle/10283/3364Published in the USA as Where?s Waldo.522this was clear, as part of the study instructions,they completed a few visual searches based on textdescriptions.
The image in the figure also containsa black box (not part of the initial stimulus), whichthe annotator has added to designate the landmarkobject ?burning hut?).
The dataset contains 1672descriptions, contributed by 152 different partici-pants (152 participants ?
11 scenes).The REs are annotated for visual and linguisticcontent.
The annotation scheme indicates whichsubstrings of the RE describe the target object, an-other mentioned object or an image region.
Ref-erences to parts or attributes of objects are nottreated as separate objects; ?a man holding torchand sword?
in Figure 1 is a single object.
Thementioned objects are linked to bounding boxes(or for very large objects, bounding polygons) inthe image.For each mention of a non-target object, the an-notation indicates whether it is part of a relationaldescription of a specific anchor, and if so which; ifit is not, it receives an ESTABLISH tag.
These an-notations are used to determine the ordering strate-gies used in this study.
In some cases, the linkagebetween objects is implicit:(7) ...there are 4 men smoking... the man youare looking for is the one [=of the 4 men]leaning against a crateIn the above RE, 4 men is first introduced in anESTABLISH construction.
The word ?one?
refersimplicitly to part of this set of men, so the annota-tor marks a relational link from ?4 men?
to ?one?.In our analysis in this study, we treat the entity?crates?
as anchored to the target (?one?)
on thebasis of this implicit link (so that this is an instanceof the PRECEDE-ESTABLISH pattern), but we donot treat the hidden link itself as a mention or tryto predict its nonexistent ?position?
in the string.5 Distribution of ordering strategiesWe first describe the distribution of these strate-gies across the corpus as a whole.
As shown in Ta-ble 1, landmarks are ordered about equally to theFOLLOW or PRECEDE of the objects they help tolocate.
Regions, on the other hand, prefer the PRE-CEDE ordering.
The INTER ordering is less com-mon, but still quite well-represented.
The ESTAB-LISH construction (initial ?there is?
or ?look at?
)occurs only with PRECEDE ordering, and indeedcan be viewed as a syntactic strategy for achievingsuch an order.
We will explain these characteristicThe <targ>man</targ> just to the leftof the <lmark rel=?targ?
obj=?imgID?>burning hut</lmark> <targ>holding atorch and a sword</targ>.Figure 1: Example scene (red box indicates tar-get) with annotated referring expression.
Words in<targ> tags describe the target.
A single land-mark (the burning hut, indicated by the rel at-tribute) is mentioned in a relational descriptionwhose anchor is the target; the annotator hasmarked it with a black box.patterns in linguistic terms in Section 7.As in most discourse tasks (Ford and Olson,1975; Pechmann, 2009), speakers display a fairamount of variability.
To measure this, we exam-ine each anchor/landmark pair which is mentionedby more than one speaker, and compute how oftenthese speakers use the same strategy.
There are664 such pairs,5appearing a total of 2361 timesin the corpus.6Of these, 66% agree on the direc-tional strategy.7Separately, 14% of the expres-sions use an ESTABLISH construction, and 43% ofthese are agreed on by the majority.
(The remain-ing variation could in principle have two sources:The content of the expression as a whole could af-fect the realization of a particular pair of objects,or individual speakers might simply differ in theirusage patterns.)
Nonetheless, there is a good dealof regularity in speakers?
decisions.
In the rest ofthe paper, we attempt to model and predict thisregularity.5286 of these pairs are mentioned by exactly two speakers.6This is more than the total number of referring expres-sions in the corpus, because many of the REs contain multiplepairs of entities.7If strategies were assigned randomly using the overallmarginals, we would expect only 34% agreement.
Using thismethod of calculating chance agreement, we would obtain aCohen?s ?
of .48.523PRECEDE INTER FOLLOWRegion 60 (440) 21 (160) 19 (138)L-mark 38 (977) 25 (632) 37 (945)ESTABLISH NON-EST.PRECEDE landmark 51 (495) 49 (482)Table 1: Distribution of ordering strategies for alllandmarks and regions in the corpus: % (count).An additional 24 landmarks occur with no associ-ated anchor (and therefore no discernible order).6 Visual and non-visual informationSince visual properties are known to affect land-mark selection (Kelleher et al., 2005; Viethen andDale, 2008), we expect them to influence informa-tion structure as well.
Our system uses three visualproperties to predict information structure; we se-lect properties that are known from previous workto help predict whether a landmark will be men-tioned.
These properties are the area of the an-chor and landmark, the distance between them(Golland et al., 2010, among others) and their cen-trality (centr.)
(distance from the center of thescreen) (Kelleher et al., 2005).8These propertiesare all indicators of visual salience (Toet, 2011),the property which makes objects in a scene easyto find quickly (Wolfe, 2012) and tends to drawinitial gaze fixations (Itti and Koch, 2000).
Wealso include indicators for whether the anchor isthe target object, and whether the landmark is animage region (reg) (see section 3).In addition, we give a few non-visual featuresderived from the content structure.
These includethe number of dependents (landmarks which re-late to each object in the description) and the num-ber of descendants (the direct dependents, theirdependents and so forth).
When the speaker hasto arrange a large number of landmarks, they tendto vary the ordering more, because of heavy-shifteffects (White and Rajkumar, 2012) and the diffi-culty of preposing more than one constituent.7 Regression analysisTo gain some insight into the influence of differ-ent features, we conduct a logistic regression anal-ysis.
For each pair of (anchor, landmark) occur-8Following Clarke et al.
(2013), we attempted to alsomeasuring distinctiveness from the background using a per-ceptual model of visual salience (Torralba et al., 2006).
Al-though this measure is effective in predicting landmark selec-tion, it proves uninformative here for predicting informationstructure, yielding no significant effects in any analyses.ring in a relational description, we attempt to pre-dict the manner of realization (direction and ES-TABLISH).
We performed a logistic regression foreach class (one-vs-all); thus there are four regres-sors in total, making 0-1 predictions for PRECEDE,PRECEDE-ESTABLISH, INTER and FOLLOW.Because their distributions are heavily skewed,area is transformed to square root area and dis-tance/centrality values are log-transformed as inClarke et al.
(2013).9Features are scaled to zeromean and unit variance.
Finally, centrality valuesare negated so that higher values indicate morecentral objects; this is for ease of interpretation.We fit models using random intercepts for speakerand image using the LME4 package (Bates et al.,2011), then removed all fixed effects which werenever significant for any class and reran the anal-ysis until a minimal model was reached (Crawley,2007).
This minimization removed the number ofdescendants features (but kept number of directdependents).
Table 2 shows the significant coef-ficients, standard deviations and Z-scores.
(Notethat as the regressions are separate, the coefficientsare comparable reading down columns, but notacross rows).The regression analysis shows that as landmarksget larger, they are more likely to be realized withthe PRECEDE (?
= 3.27) or INTER (?
= 1.28)strategies (but not PRECEDE-ESTABLISH) and lesslikely (?
= ?3.76) to be placed following.
(Thisdoes not appear to be the case for landmarks thatare central; these are slightly more likely to beordered FOLLOW (?
= .81).)
The PRECEDE-ESTABLISH construction is neither favored nordisfavored by landmark area.
It does, however,have a strong preference for landmarks with manydependents (?
= 2.38), since these are more nat-urally realized in the clause-final position intro-duced by a ?There is X?-type construction.
In con-trast, landmarks with many dependents disfavorthe INTER strategy (?
= ?1.07), since this wouldrequire placing a heavy NP in a central rather thanrightward position.There are also a few effects of visual featuresof the anchor objects.
Larger anchors (which areeasier to see in their own right) prefer landmarksto FOLLOW (?
= .35).
This presumably reflectsthe fact that, since the listener is more likely tosee them quickly, such anchors are more often re-9We use these continuous values in our analysis; our clas-sifier model (below) uses discretized area, distance and cen-trality.524Feature PRECEDE Z PREC.-EST.
Z INTER Z FOLLOW Zintercept -4.18 ?
.37 -11.2 -2.66 ?
.50 -5.3 -2.51 ?
.32 -7.7 2.72 ?
.32 8.5anch area -.27 ?
.06 -4.6 -.19 ?
.09 -2.2 - - .35 ?
.05 6.9anch centr .11 ?
.05 2.0 - - - - - -anch deps - - -.74 ?
.12 -6.2 .22 ?
.06 3.6 - -anch=targ .30 ?
.13 2.3 - - .55 ?
.14 4.0 -.71 ?
.13 -5.7distance - - -.24 ?
.09 -2.6 - - - -lmk=reg 11.46 ?
1.35 8.5 - - 3.01 ?
1.19 2.5 -12.62 ?
1.17 -10.8lmk area 3.27 ?
.38 8.7 - - 1.28 ?
.32 4.0 -3.76 ?
.32 -11.7lmk centr - - - - - - .81 ?
.32 2.6lmk deps - - 2.38 ?
.14 16.9 -1.07 ?
.13 -8.3 -1.37 ?
.12 -11.5Table 2: Regression coefficients, standard deviations and Z-scores from one-vs-all logistic regressionswith direction/ESTABLISH status as output variable.
Only effects significant at p < .05 level are shown;other effects are displayed as -.alized at the start of an expression.
(Clarke et al.
(2013) show that they have fewer landmarks over-all.)
Again, the effect of centrality is counterin-tuitive, but weak (?
= .81).
Anchors with moredependents are slightly more likely to use the IN-TER slot (?
= .22), suggesting that the variousdependents are spread syntactically throughout theexpression.Although distance and centrality are weak in-dicators in this dataset, area shows strong effectswhich support our conclusion that visual saliencebehaves like discourse salience.
The standard in-formation order of English clauses places given in-formation first and new information later (Prince,1981).
Thus, we observe that the non-right or-ders are used for larger objects, which is what wewould expect if their visual perceptibility is suffi-cient to place them in common ground despite thelack of a previous mention.10On the other hand,the FOLLOW order is used for smaller objects thatcannot be assumed to be part of common ground(and are therefore treated as new).The use of ESTABLISH constructions for mid-sized objects also makes sense on theoreticalgrounds.
ESTABLISH constructions are a wayof achieving the PRECEDE information structure,which places the landmark first?
and this makessense primarily if the landmark is reasonablysalient, since otherwise it will not be found anyfaster than the target.
On the other hand, mostof the constructions we discuss as ESTABLISH,10Prince (1981) discusses other discourse-new items thatare nonetheless treated as familiar, like ?The FBI?, under thename unused (that is, available, but not previously in use inthe discourse).such as existential ?there is?, require their objectto be discourse-new (Ward and Birner, 1995); itwould be infelicitous to start a description by stat-ing the existence of something already in the com-mon ground ?there is a sky, and it is blue.
.
.
?Thus, it makes sense that neither large or smallobjects favor the use of this construction; it can beused to foreground an object which is not salientenough to be assumed in common ground, but issalient enough to find without a great deal of vi-sual search.8 Information structure predictionIn this section, we experiment with an idealizedversion of the information structuring task.
Weprovide our system with gold standard contentselection?
we know which objects will be men-tioned, and if they serve as landmarks, we knowthe anchor they describe.
However, we do notknow which information strategies will be used toorder them; our task is to predict this.
In doingso, we are working with an idealized version ofthe standard generation pipeline, which often op-erates as a two-stage process, with content selec-tion followed by surface realization.
Informationstructure prediction is intermediate between thesetwo stages; once we have decided which objectsto mention (or in concert), we would like to de-cide what order to mention them in.We set up the prediction task as in the pre-vious section: Given an anchor/landmark pair,our system must decide what direction and ES-TABLISH status to assign it.
However, here weevaluate the system as a classifier.
We treat an-chor/landmark pair as independent from the others525Feat type # featurestype (targ/lmark/region) of anchor 3type (targ/lmark/region) of dep 3quartile of anchor area 4quartile of lmark area 4quartile of anchor?
lmark dist 4quartile of dist anchor?
screen ctr 4quartile of dist lmark?
screen ctr 4# direct dependents of anchor 6# descendents of anchor 6Table 3: Feature templates and number of instan-tiations in our discriminative system.
(including other pairs from the same description);during development, we investigated a parser-likestructured classifier based on (Socher et al., 2011;Salakhutdinov and Hinton, 2009) that jointly clas-sified all the relational descriptions in a single ut-terance at once, but results did not improve overthe classifier system, perhaps because on averagethe trees are fairly shallow.8.1 Discriminative comparisonWe train a discriminative multilabel classifier us-ing maximum entropy.11We predict EST-DIR pairsgiven a set of discrete features shown in Table3.
This setup differs slightly from the previoussection (which used one-vs-all); we are attempt-ing to conform to the standard practices of psy-cholinguistics and computational linguistics re-spectively.
Area, salience, distance to center andinter-object distance values are discretized by de-termining in which quartile of the training set eachvalue falls (lowest 25%, mid-low, mid-high, high-est 25%).
Our initial model used continuous val-ues as in the previous section, but results weresomewhat poorer, suggesting some of these fea-tures may have nonlinear effects.8.2 ExperimentsWe hold out three images (vikings, airport,blackandwhite) as a development set.
In test, weexclude these 3 documents and use the other 8for evaluation.
In both development and test, weconduct experiments by crossvalidation, testing onone document at a time and training on the otherten.1211Learned using the Theano neural-network package(Bergstra et al., 2010) and stochastic gradient descent codefrom deeplearning.net/tutorial (Bengio, 2009).12This means we always use 10 of the 11 documents fortraining, whether in dev or test, but we didn?t do error anal-We report two trivial baseline strategies, alllandmarks following (the best baseline for over-all accuracy) and all landmarks preceding (the bestbaseline for predicting the direction, but not asgood overall because the PRECEDE predictions aresplit between ESTABLISH and not ESTABLISH).Our preliminary analysis shows that regions havea strong tendency to precede their anchors, so wealso report results for a baseline using this pat-tern (regions preceding, everything else follow-ing).
We believe this baseline pattern is the onewhich would be learned as a template by previ-ous systems like Di Fabbrizio et al.
(2008), sincethis system can learn relationships between broadtypes of entities (target, landmark and region) butdoes not use visual features of the actual entitiesin the scene to make any finer distinctions.We also provide two ?inter-subject?
oraclescores intended to estimate the performance ceil-ing imposed by human variability.
This oracleassigns each anchor/landmark pair the directionand ESTABLISH status assigned by the majorityof speakers who mentioned that pair.
The ?mul-tiple mentions?
estimate of agreement is the onementioned in Section 5; it was based only on pairsmentioned by multiple speakers.
The ?all?
esti-mate is based on all objects; it is higher because,for pairs mentioned by only one speaker, it is bydefinition perfect.
Our system?s use of the num-ber of descendants feature is not captured by thisoracle?
these features capture information abouta particular speaker?s content plan beyond theirdecision to mention a particular pair?
but we sus-pect that the oracle?s performance will nonethelessbe hard for any practical system to beat.We report gross accuracy (correctly predictingboth DIR and ESTABLISH) for relational pairs (Ta-ble 5), and also decompose by direction (Table 4)and ESTABLISH status (Table 6).The baseline correctly predicts 43% of pairs,implying that this pattern (regions precede, land-marks follow) covers a bit under half the data.
Theclassifier improves this to 52%.
When predictingthe direction alone, the best baseline (PRECEDE)scores 42%; the classifier scores 57%.
All sys-tem scores are significantly better than the base-line (sign test on pairs, p < 0.01).
In predictionsof ESTABLISH tags, our result is a 60% f-score,which is indistinguishable from the lower boundysis on the training examples.
Data size does appear to mat-ter; training on 8 documents at a time and testing on 3 yieldspoorer results.526System PRECEDE INTER FOLLOW Dir AccPrec Rec F Prec Rec F Prec Rec FFollow 0 0 0 0 0 0 32 100 49 32Precede 44 100 62 0 0 0 0 0 0 44Regions precede 61 32 42 0 0 0 37 87 52 42Discr 66 69 68 39 23 29 53 65 58 57Inter-subj (multiple mentions) 77 61 68 54 62 58 67 76 71 66Inter-subj (all) 84 75 79 65 69 67 74 83 78 76Table 4: Direction scores (p/r/f per direction and total pair directions correctly predicted) in 2382 pairsin test set.
Overall accuracy differences between system and baselines are significant (p < .01).System Pair accuracyFollow 36Precede 29Regions precede 43Discr 52Inter-subj (mult) 64Inter-subj (all) 74Table 5: Gross accuracy (%) for 2382 test pairs.System ESTABLISHPrec Rec FFollow 0 0 0Precede 0 0 0Regions precede 0 0 0Discr 55 67 60Inter-subj (mult) 68 43 53Inter-subj (all) 82 66 73Table 6: ESTABLISH scores (p/r/f for EST=TRUE)in 2382 pairs in test set.estimate of interannotator agreement.9 ConclusionsThe results of this study show that the informationstructure of relational descriptions is highly vari-able, and depends on notions of salience and com-mon ground that are difficult to capture with tem-plates or simple case-based rules.
This suggeststhat the question of realization for visual-word re-ferring expressions may need to be reopened.
Adata-driven approach not only allows better pre-diction of which strategy will be used (reducingerror by 9% absolute, 16% relative) but also en-ables us to analyze the pattern and conclude thatthe visual salience of an object acts in the sameway as discourse salience.Several open questions remain.
One is the fail-ure of the Torralba et al.
(2006) visual distinctive-ness model to make any difference: Is this actuallya perceptual fact, or does it merely demonstratethat the model is not as predictive of human atten-tional patterns as we would like?
More importantis the question of what lies behind the substantialvariations we observe across individuals.
Thesemay reflect truly different strategies; for instance,some speakers may generate REs incrementally asthey scan the image (Pechmann, 2009) while oth-ers perform a more complete scan before begin-ning (Gatt et al., 2012).
We suspect answeringthis question is beyond the scope of corpus stud-ies, and intend to investigate via psycholinguisticexperiments using an eyetracker.Another question is to what extend the patternswe observe are intended to facilitate listeners?
vi-sual search (an audience design hypothesis) ver-sus speakers?
efficient construction of utterances.This study focused on predicting speaker behavior,while acknowledging that the utterances speakersproduce are not always optimal for listeners (Belzand Gatt, 2008).
However, we suspect that in thiscase, putting easy-to-see objects early really doeshelp listeners; we are currently planning percep-tion experiments to test this hypothesis.Finally, we intend to incorporate the visual fea-tures used in this study into a full-scale realizationsystem.
This will enable us to create more human-like REs for visual domains.
Such REs can be in-corporated into natural language systems for a va-riety of interactive visual-world tasks.AcknowledgementsThe third author was supported by EPSRC grantEP/H050442/1 and ERC grant 203427 ?Syn-chronous Linguistic and Visual Processing?.
Wealso thank Marie-Catherine de Marneffe, CraigeRoberts, the OSU Pragmatics group and ouranonymous reviewers for their helpful comments.527ReferencesD.
A. Baldwin.
1995.
Understanding the link betweenjoint attention and language.
In Joint attention: itsorigins and role in development.
Lawrence ErlbaumAssoc., Hillsdale, NJ.D.
Bates, M. Maechler, and B. Bolker.
2011.lme4: Linear mixed-effects models using s4classes.
Comprehensive R Archive Network:cran.r-project.org.David L. Bean and Ellen Riloff.
1999.
Corpus-basedidentification of non-anaphoric noun phrases.
InProceedings of the 37th annual meeting of the As-sociation for Computational Linguistics (ACL?99),pages 373?380, Morristown, NJ, USA.
Associationfor Computational Linguistics.Anja Belz and Albert Gatt.
2008.
Intrinsic vs. ex-trinsic evaluation measures for referring expressiongeneration.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguisticson Human Language Technologies: Short Papers,pages 197?200.
Association for Computational Lin-guistics.Yoshua Bengio.
2009.
Learning deep architectures forAI.
Foundations and Trends in Machine Learning,2(1):1?127.
Also published as a book.
Now Pub-lishers, 2009.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference(SciPy), June.
Oral Presentation.Robbert-Jan Beun and Anita H.M. Cremers.
1998.Object reference in a shared domain of conversation.Pragmatics and Cognition, 6(1-2):121?152.Joan Bresnan, Anna Cueni, Tatiana Nikitina, andR.
Harald Baayen.
2007.
Predicting the dative al-ternation.
Cognitive Foundations of Interpretation,pages 69?94.Ivo Brugman, Mari?et Theune, Emiel Krahmer, andJette Viethen.
2009.
Realizing the costs: template-based surface realisation in the graph approach toreferring expression generation.
In Proceedings ofthe 12th European Workshop on Natural LanguageGeneration, ENLG ?09, pages 183?184, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.M.
Carpenter, K. Nagell, and M. Tomasello.
1998.
So-cial cognition, joint attention, and communicativecompetence from 9 to 15 months of age.
Mono-graphs of the Society for Research in Child Devel-opment, 63(4).Herbert H. Clark and Deanna Wilkes-Gibbs.
1986.Referring as a collaborative process.
Cognition,22(1):1?39.Herbert H. Clark.
1996.
Using language.
CambridgeUniversity Press, Cambridge.Alasdair D. F. Clarke, Micha Elsner, and Hannah Ro-hde.
2013.
Where?s Wally: The influence ofvisual salience on referring expression generation.Frontiers in Psychology (Perception Science), Issueon Scene Understanding: Behavioral and computa-tional perspectives.Michael Crawley.
2007.
The R Book.
Wiley-Blackwell, Hoboken, NJ.Robert Dale and Nicholas J. Haddock.
1991.
Gen-erating referring expressions involving relations.
InEACL, pages 161?166.Giuseppe Di Fabbrizio, Amanda J. Stent, and SrinivasBangalore.
2008.
Referring expression generationusing speaker-based attribute selection and trainablerealization (ATTR).
In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion (INLG), Salt Fork, OH.Manjuan Duan, Micha Elsner, and Marie-Catherinede Marneffe.
2013.
Visual and linguistic predictorsfor the definiteness of referring expressions.
In Pro-ceedings of the 17th Workshop on the Semantics andPragmatics of Dialogue (SemDial), Amsterdam.Matt Duckham, Stephan Winter, and Michelle Robin-son.
2010.
Including landmarks in routing instruc-tions.
Journal of Location Based Services, 4(1):28?52.Rui Fang, Changsong Liu, Lanbo She, and Joyce Y.Chai.
2013.
Towards situated dialogue: Revisitingreferring expression generation.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 392?402, Seattle,Washington, USA, October.
Association for Com-putational Linguistics.Katja Filippova and Michael Strube.
2007.
Generat-ing constituent order in German clauses.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 320?327,Prague, Czech Republic, June.
Association for Com-putational Linguistics.William Ford and David Olson.
1975.
The elabora-tion of the noun phrase in children?s description ofobjects.
Journal of Experimental Child Psychology,19:371?382.Albert Gatt and Anja Belz.
2010.
Introducing sharedtask evaluation to NLG: The TUNA shared taskevaluation challenges.
In E. Krahmer and M. The-une, editors, Empirical Methods in Natural Lan-guage Generation.
Springer, Berlin and Heidelberg.Albert Gatt, Anja Belz, and Eric Kow.
2008.
TheTUNA-REG challenge 2008: Overview and eval-uation results.
In Proceedings of the 5th Interna-tional Conference on Natural Language Generation(INLG), Salt Fork, OH.528Albert Gatt, Anja Belz, and Eric Kow.
2009.
TheTUNA-REG challenge 2009: Overview and eval-uation results.
In Proceedings of the 12th Euro-pean Workshop on Natural Language Generation(ENLG), Athens.A.
Gatt, E. Krahmer, R. P. G. van Gompel, and K. vanDeemter.
2012.
Does domain size impact speechonset time during reference production?
In Pro-ceedings of the 34th Annual Meeting of the Cog-nitive Science Society, pages 1584?1589, Sapporo,Japan.Dave Golland, Percy Liang, and Dan Klein.
2010.A game-theoretic approach to generating spatial de-scriptions.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 410?419, Cambridge, MA, October.Association for Computational Linguistics.Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-stein.
1995.
Centering: A framework for model-ing the local coherence of discourse.
ComputationalLinguistics, 21(2):203?225.M.
Handford.
1987.
Where?s Wally?
Walker Books,London, 3 edition.M.
Handford.
1988.
Where?s Wally Now?
WalkerBooks, London, 4 edition.L.
Itti and C. Koch.
2000.
A saliency-based searchmechanism for overt and covert shifts of visual at-tention.
Vision research, 40(10-12):1489?1506.John D. Kelleher and Geert-Jan M. Kruijff.
2006.
In-cremental generation of spatial referring expressionsin situated dialog.
In ACL.J.
Kelleher, F. Costello, and J. van Genabith.
2005.Dynamically structuring, updating and interrelatingrepresentations of visual and linguistic discoursecontext.
Artificial Intelligence, 167(12):62 ?
102.Connecting Language to the World.Emiel Krahmer and Kees van Deemter.
2012.
Com-putational generation of referring expressions: Asurvey.
Computational Linguistics, 38(1):173?218,March.Claudia Maienborn.
2001.
On the position and inter-pretation of locative modifiers.
Natural LanguageSemantics, 9(2):191?240.Crystal Nakatsu and Michael White.
2010.
Generat-ing with discourse combinatory categorial grammar.Linguistic Issues in Language Technology, 4(1).T.
Pechmann.
2009.
Incremental speech produc-tion and referential overspecification.
Linguistics,27(1):89?110.Ellen Prince.
1981.
Toward a taxonomy of given-newinformation.
In Peter Cole, editor, Radical Prag-matics, pages 223?255.
Academic Press, New York.Ruslan Salakhutdinov and Geoffrey Hinton.
2009.Replicated softmax: an undirected topic model.
InY.
Bengio, D. Schuurmans, J. Lafferty, C. K. I.Williams, and A. Culotta, editors, Advances in Neu-ral Information Processing Systems 22, pages 1607?1614.Richard Socher, Cliff C. Lin, Andrew Y. Ng, andChristopher D. Manning.
2011.
Parsing naturalscenes and natural language with recursive neuralnetworks.
In Proceedings of the 26th InternationalConference on Machine Learning (ICML).A.
Toet.
2011.
Computational versus psychophysi-cal bottom-up image saliency: A comparative eval-uation study.
Pattern Analysis and Machine Intelli-gence, IEEE Transactions on, 33(11):2131 ?2146.A.
Torralba, A. Oliva, M. Castelhano, and J. M. Hen-derson.
2006.
Contextual guidance of attention innatural scenes: The role of global features on objectsearch.
Psychological Review, 113:766?786.Jette Viethen and Robert Dale.
2008.
The use of spa-tial relations in referring expressions.
In Proceed-ings of the 5th International Conference on NaturalLanguage Generation, Salt Fork, Ohio, USA.Gregory Ward and Betty Birner.
1995.
Definitenessand the English existential.
Language, 71(4):722?742, December.Gregory Ward and Betty Birner.
2001.
Discourse andinformation structure.
In Deborah Schiffrin, Debo-rah Tannen, and Heidi Hamilton, editors, Handbookof discourse analysis, pages 119?137.
Basil Black-well, Oxford.Bonnie L. Webber.
2004.
D-ltag: extending lexical-ized tag to discourse.
Cognitive Science, 28(5):751?779.Michael White and Rajakrishnan Rajkumar.
2012.Minimal dependency length in realization ranking.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 244?255, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Jeremy M. Wolfe.
2012.
Visual search.
In P. Todd,T.
Holls, and T. Robbins, editors, Cognitive Search:Evolution, Algorithms and the Brain, pages 159 ?175.
MIT Press, Cambridge, MA, USA.Sina Zarrie?, Aoife Cahill, and Jonas Kuhn.
2012.To what extent does sentence-internal realisation re-flect discourse context?
a study on word order.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 767?776, Avignon, France, April.Association for Computational Linguistics.529
