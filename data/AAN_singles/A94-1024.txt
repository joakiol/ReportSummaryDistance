Tagging and Morphological Disambiguation of Turkish TextKemal  Oflazer and i l ker  Kuru i i zDepar tment  of  Computer  Eng ineer ing  and  In format ion  Sc ienceB i lkent  Un ivers i tyB i lkent ,  Ankara ,  TURKEY{ko ,kuruoz}~cs .b i l kent .edu .
t rAbst rac tAutomatic text tagging is an importantcomponent in higher level analysis of textcorpora, and its output can be used inmany natural anguage processing applica-tions.
In languages like Turkish or Finnish,with agglutinative morphology, morpholog-ical disambiguation is a very crucial pro-cess in tagging, as the structures of manylexical forms are morphologically ambigu-ous.
This paper describes a POS tagger forTurkish text based on a full-scale two-levelspecification of Turkish morphology that isbased on a lexicon of about 24,000 rootwords.
This is augmented with a multi-word and idiomatic construct recognizer,and most importantly morphological dis-ambiguator based on local neighborhoodconstraints, heuristics and limited amountof statistical information.
The tagger alsohas functionality for statistics compilationand fine tuning of the morphological an-alyzer, such as logging erroneous morpho-logical parses, commonly used roots, etc.Preliminary results indicate that the tag-ger can tag about 98-99% of the texts ac-curately with very minimal user interven-tion.
Furthermore for sentences morpho-logically disambiguated with the tagger, anLFG parser developed for Turkish, gener-ates, on the average, 50% less ambiguousparses and parses almost 2.5 times faster.The tagging functionality is not specific toTurkish, and can be applied to any lan-guage with a proper morphological nalysisinterface.1 In t roduct ionAs a part of large scale project on natural anguageprocessing for Turkish, we have undertaken the de-velopment of a number of tools for analyzing Turk-ish text.
This paper describes one such tool - a texttagger for Turkish.
The tagger is based on a fullscale two-level morphological specification of Turk-ish (Oflazer, 1993), implemented on the PC-KIMMOenvironment (Antworth, 1990).
In this paper, we de-scribe the functionality and the performance of ourtagger along with various techniques that we haveemployed to deal with various sources of ambigui-ties.2 Tagg ing  TextAutomatic text tagging is an important step in dis-covering the linguistic structure of large text cor-pora.
Basic tagging involves annotating the wordsin a given text with various pieces of information,such as part-of-speech and other lexical features.Part-of-speech tagging facilitates higher-level analy-sis, such as parsing, essentially by performing a cer-tain amount of ambiguity resolution using relativelycheaper methods.The most important functionality of a tagger isthe resolution of the structure and parts-of-speech ofthe lexical items in the text.
This, however, is not avery trivial task since many words are in general am-biguous in their part-of-speech for various reasons.In English, for example a word such as make canbe verb or a noun.
In Turkish, even though thereare ambiguities of such sort, the agglutinative na-ture of the language usually helps resolution of suchambiguities due to morphotactical restrictions.
Onthe other hand, this very nature introduces anotherkind of ambiguity, where a lexical form can be mor-phologically interpreted in many ways.
For example,the word evin, can be broken down as: 1ev in  POS English1.
N(ev)+2SG-POSS N (your) house2.
N(ev)+GEN N of the house3.
N(evin) N wheat germIf, however, the local context is considered, it maybe possible to resolve the ambiguity as in:1 Output of the morphological nalyzer is edited forclarity.144sen-in ev-in ..PN(you)+GEN N(ev)+2SG-POSSyour house.. ev in  kapl-sl ..N(ev)+GEN N(door)+3SG-POSSdoor of the houseusing genitive-possessive agreement constraints.As a more complex case we can give the following:allnml?1 ADJ(al)+2SG-POSS+NtoV0+NARR+3SG 2(V) (it) was your red (one)2 ADJ(al)+GEN+NtoV0+NARR+3SG(V) (it) belongs to the red (one)a N(ahn) +NtoV0 +NARR+3SG(V) (it) was a forehead4 V(al) +PASS+VtoAdj (mis)(ADJ) (a) taken (object)5 V(al)+PASS+NARR+3SG(v) ( it)  was taken6 V(ahn)+VtoAdj(mis)(ADJ) (an) offended (person)7 V(ahn)+NARR+3SG(V) (s/he) was offendedIt is in general rather hard to select one of theseinterpretations without doing substantial nalysis ofthe local context, and even then one can not fullyresolve such (usually semantic) ambiguities.An additional problem that can be off-loaded tothe tagger is the recognition of multi-word or id-iomatic constructs.
In Turkish, which abounds withsuch forms, such a recognizer can recognize thesevery productive multi-word constructs, likeko~-a ko~-arun+OPT+3SG run+OPT+3SGyap-ar yap-ma-zdo+AOR+3SG do+NEG+AOR+3SGwhere both components are verbal but the com-pound construct is a manner or temporal adverb.This relieves the parser from dealing with them atthe syntactic level.
Furthermore, it is also possibleto recognize various proper nouns with this func-tionality.
Such help from a tagging functionalitywould simplify the development of parsers for Turk-ish (Demir, 1993; Giing6rdii, 1993).Researchers have used a number of different ap-proaches for building text taggers.
Karlsson (Karls-son, 1990) has used a rule-based approach wherethe central idea is to maximize the use of mor-phological information.
Local constraints expressedas rules basically discard many alternative parseswhenever possible.
Brill (Brill, 1992) has designeda rule-based tagger for English.
The tagger worksby automatically recognizing rules and remedyingits weaknesses, thereby incrementally improving itsperformance.
More recently, there has been a rule-2In Turkish, all adjectives can be used as nouns, hencewith very minor differences adjectives have the samemorphotactics a nouns.based approach implemented with finite-state ma-chines (Koskenniemi et al, 1992; Voutilainen andTapanainen, 1993).A completely different approach to tagging usesstatistical methods, (e.g., (Church, 1988; Cutting etal., 1993)).
These systems essentially train a statis-tical model using a previously hand-tagged corpusand provide the capability of resolving ambiguity onthe basis of most likely interpretation.
The modelsthat have been widely used assume that the part-of-speech of a word depends on the categories of the twopreceding words.
However, the applicability of suchapproaches to word-order free languages remains tobe seen.2.1 An  exampleWe can describe the process of tagging by showingthe analysis for the sentence:iflen d6ner dfnmez evimizin yak~nmda bulunanderin gb'lde yiizerek gev~emek en biiyiik zevkimdi.
(Relaxing by swimming the deep lake near ourhouse, as soon as I return from work was my greatestpleasure.
)which we assume has been processed by the morpho-logical analyzer with the following output:i~ten POS1.
N(i~)+ABL N+dfner1.
N(d6ner) N2.
V(dfn)+AOR+3SG V+" 3.
V(d6n)+VtoAdj(er) ADJd fnmez1.
V(dfn)+NEG+AOR+3SG V+2.
V(dfn)+Vtohdj(mez) ADJevimizin1.
N(ev)+IPL-POSS+GEN N+yakmmda1.
AD J(yahn)+3SG-POSS+LOC N+2.
AD J(yakin)+2SG-POSS+LOC Nbulunan1.
V(bul)+PASS+VtoAD J(yan) ADJ2.
V(bulun)+VtoADJ(yan) ADJ+derin1.
N(deri)+2SG-POSS N2.
ADJ(derin) ADJ+3.
V(der)+IMP+2PL V4.
V(de)+VtoAD J(er)+2SG-POSS N5.
V(de)+VtoADJ(er)+GEN Ngf lde1.
N(g61)+LOC N+yfizerek1.
V(yfiz)+VtoADV(yerek) ADV+gev~emek1.
V(gev~e)+VtoINF(mak) V+e l l1.
N(en) N2.
ADV(en) ADV+bfiyfik1.
ADJ(biiyfik) ADJ+zevkimdi1.
N(zevk)+ISG-POSS+ V+NtoV()+PAST+3SG145Although there are a number of choices for tagsfor the lexical items in the sentence, almost all ex-cept one set of choices give rise to ungrammatical orimplausible sentence structures.
3 There are numberof points that are of interest here:?
the construct d6ner d6nmez formed by twotensed verbs, is actually a temporal adverbmeaning ... as soon as .. return(s), hence thesetwo lexical items can be coalesced into a singlelexical item and tagged as a temporal adverb.?
The second person singular possessive interpre-tation of yahmnda is not possible since thisword forms a simple compound noun phrasewith the previous lexical item and the third per-son singular possessive morpheme functions asthe compound marker, agreeing with the agree-ment of the previous genitive case-marked form.?
The word derin (deep) is the modifier of a sim-ple compound noun derin g61 (deep lake) hencethe second choice can safely be selected.
Theverbal root in the third interpretation is veryunlikely to be used in text, let alne in sec-ond person imperative form.
The fourth andthe fifth interpretations are not very plausibleeither.
The first interpretation (meaning yourskin) may be a possible choice but can be dis-carded in the middle of a longer compound nounphrase.?
The word en preceding an adjective indicatesa superlative construction and hence the nounreading can be discarded.3 The Tagging ToolThe tagging tool that we have developed integratesthe following functionality with a user interface, asshown in Figure 1, implemented under X-windows.It can be used interactively, though user interactionis very rare and (optionally) occurs only when thedisambiguation can not be done by the tagger.1.
Morphological analysis with error logging,2.
Multi-word and idiomatic construct recogni-tion,3.
Morphological disambiguation by using con-straints, heuristics and certain statistics,4.
Root and lexical form statistics compilation,The second and the third functionalities are imple-mented by a rule-base subsystem which allows oneto write rules of the following form:C1 : A1 ; C2 : A2 ; ?
?
?
Cn : An.where each Ci is a set of constraints on a lexical form,and the corresponding Ai is an action to be executedon the set of parses associated with that lexical form,only when all the condilions are sa~isJied.3The correct choices of tags are marked with +.The conditions refer to any available morpholog-ical or positional feature associated with a lexicalform such as:?
Absolute or relative lexical position (e.g., sen-tence initial or final, or 1 after the current word,etc.)?
root and final POS category,?
derivation type,?
case, agreement (number and person), and cer-tain semantic markers, for nominal forms,?
aspect and tense, subcategorization require-ments, verbal voice, modality,and sense for ver-bal forms?
subcategorization requirements for postposi-tions.Conditions may refer to absolute feature values orvariables (as in Prolog, denoted by the prefix _ in thefollowing examples) which are then used to link con-ditions.
All occurrences of a variable have to unifyfor the match to be considered successful.
This fea-ture is powerful and and lets us specify in a rathergeneral way, (possibly long distance) feature con-straints in complex NPs, PPs and VPs.
This is apart of our approach that distinguishes it from otherconstraint-based approaches.The actions are of the following types:?
Null action: Nothing is done on the matchingparse.?
De lete :  Removes the matching parse if morethan one parse for the lexical form are still inthe set associated with the lexical form.?
Output :  Removes all but the matching parsefrom the set effectively tagging the lexical formwith the matching parse.?
Compose:  Composes a new parse from variousmatching parses, for multi-word constructs.These rules are ordered, and applied in the givenorder and actions licensed by any matching rule areapplied.
One rule formalism is used to encode bothmulti-word constructs and constraints.3.1 The  Mu l t i -word  Const ruct  P rocessorAs mentioned before, tagging text on lexical item ba-sis may generate spurious or incorrect results whenmultiple lexical items act as single syntactic or se-mantic entity.
For example, in the sentence $irin mi~irin bir kSpek ko~a ko~a geldi (A very cute dog camerunning) the fragment ~irin mi ~irin constitutes aduplicated emphatic adjective in which there is anembedded question suffix mi (written separately in7hrkish), 4 and the fragment ko~a ko~a is a dupli-cated verbal construction, which has the grammat-ical role of manner adverb in the sentence, though4If, however, the adjective ~irin was not repeated,then we would have a question formation.146(3o)  v .T lgged Fi le show, l .
tx t  ~ Stat i s t i cs  f i l e  Global ( Tagging Rules v)  ( ~: aoGrusu 3 - mlsZrZn \[4 CONS Cla Cla C93aJ : C'mZgZr+flHn" CC*CAT* N)C*R* "mZsZr ' )c*~g*  3SG)C*CASE* GEN)))?
1 - anavmtsnI \[3 CONS c93a csa\] : ( '~avt tan+sH"  ( ( 'CAP  N)C'R" =en lvat~' )C*AGR*  3SG)C*POSS - / kr l  s to f  3$G) (*CASE* NC$O ) ) kolomb \] - gOster l len  \[2 CONS C83b\] : ( 'gCster+Hl+yAn" CC*CAT ~ V) (*R*  "g0ster ')C*VOZCE* PASS)C*COHV" ADS ~11~ I r I  - o la rak  \[1\]  : ( 'o l+yArAk"  ( ( * tA le"  V) (*R-  "ol")( ' !
;UBCAT* NOO('CONV* N)V "yarak ' ) ( '$UB*  A ' r~) )"yan')C-AGR* 3S;G)(*C/LSE- NOl4)))kez - Far \[5 CONS C93a cg3a Cg3a C22ad : ( "  e r "  ((*CAT* N)(*R* "yer ' )C*AGg* 3SG)(*CASE* NOH)))burada - * \ [1\ ]  : ( ' . "
((*CAT* PUNCT~C*R* " . '
) )~gormUS -.kt.~(+AGR - b u  UnPJJ \[23$G)(.CAsE.CGN5 CgSb~ NO~)): ( 'bugUnkU" ((*CAT* N)C*R* "bugUN " ) (*5UB*  TEHP)(*CONV o AD~~rtka  - adZyla \[1\]  : C'adZeylA" ((*CAT* N)(*R* "~d/name')C*POS$* 3SG)C*CASE* INS) ) )yor l t le r~n~ - k~re ~pler  \[RULE M27\] : ((*CAT- N)(*R* "kareytp ler ' ) ( *SUB*  PROP))ad~mdan - .
\[1~ : ( ' . "
((*CAT* PUNCT)C*R* " . '
) ) )  sa an en a~yXga - dah l  \[1\]  : (?dsha" ((*CAT* AOV)(eR- "dahB=)(*SUg * CONPARATZVE)(*SUB* TENP)))-- doGrusu \[2 CONS C83b\] : ('doGrU+SH" ((*CAT" &DY)(*R* "doGru')C*SUg* QUAL)C*~R* 3SG)(*POSS*gOre 3SG) (*CASE* N(X,O ) )  ~1mZs\]:r -- .
\ [1\]  : ( ' . "
( ( *cKf*  PUNCT)C*R* " . '
) ) )  \] bunde~ -- k r l s to f  kolomb \[RULE 1427\] : ((*CAT" N)(*R* "k r ts to f  kolon~?
)(*SUB = PROP))Skip Parses Less T i l l .
2 l~'1"~"rl TO= Processed ~d 11~ ~ + C,t,t~,,J,:-<.t,J~,,)St i r  0.0~ lu le .
5.7% Cons.
58.5% Usmr 0.0% Unmmb.
35 ,5x  Amb.
0.0%Noofp I~eS:@ 7(13 .2~)  1 19(35.0%) 2 16(30.2X) 3 7(13.2~;) 4 8(15 .1~)  5mndmore  1(1.9%)word Parsed mlslrl ( ~ )  { ~  ~ Stlt.
\ [ \ ]  off  Rules ~+ On Cons.
~ On5 '  " ; ;  ,~ .
~- - ; ; ;~  ~ .
.
.
.
.
.
"(( .
.
.
.
~ , ' .
.  "
.
, , l~(  .
.
.
.
.
.
.
~(  .
.
.
.
.
.
35?
)( .
.
.
.
.
.
.
.
)))\ [ \ ]  \[2\] f " l  (O.SS)('mlstr+yH'((-CAT~EO(*R,-mI$1r')(*AGR, gS(~(eCASE, ACC~)))User ~ On(0  lg~,  Ilkar KURUOZ Bilkent U nlvmrsltyFigure 1: User interface of tagging toolboth of the constituent forms are verbal construc-tions.
The purpose of the multi-word construct pro-cessor is to detect and tag such productive con-structs in addition to various other semantically co-alesced forms such as proper nouns, etc.The following is a set of multi-word constructs forTurkish that we handle in our tagger.
This list isnot meant to be comprehensive, and new constructspecifications can easily be added.
It is conceivablethat such a functionality can be used in almost anylanguage.1.
duplicated optative and 3SG verbal forms func-tioning as manner adverb, e.g., ko~a ko~a, aoristverbal forms with root duplications and sensenegation functioning as temporal adverbs, e.g.,yapar yapmaz, and duplicated verbal and de-rived adverbial forms with the same verbal rootacting as temporal adverbs, e.g., gitti gideli,2.
duplicated compound nominal form construc-tions that act as adjectives, e.g., giizeller giizeli,and emphatic adjectival forms involving thequestion suffix, e.g., giizel mi giizel,3.
adjective or noun duplications that act as man-ner adverbs, e.g., hzzh hzzh, ev ev,4.
idiomatic word sequences with specific usagewhose semantics i  not compositional, e.g., yamszra, hi~ olmazsa, and idiomatic forms which arenever used singularly, e.g., giiriil giiriil,5.
proper nouns, e.g., J immy Carter, TopkapzSarayz (Topkapl Palace).6. compound verb formations which are formed bya lexically adjacent, direct or oblique object anda verb, which for the purposes of syntactic anal-ysis, may be considered as single lexical item.We can give the following example for specifyinga multi-word construct: 5Lex=_Nl, Rootf_Rl, Cat=V, AspectfAOR, Agrf3SG,SensefPOS : ;Lex=_N2, Root=_Rl, Cat=V, AspectfAOR, Agrf3SG,Sense = NEG:Composef((*CAT* ADV) (*R* "_NI _N2 (_R1) ")(*SUB* TEMP) ).This rule would match any adjacent verbal lexicalforms with the same root, both with the aorist as-pect, and 3SG agreement.
The first verb has to bepositive and the second one negated.
When found,a composite lexical form with an temporal adverbpart-of-speech, is then generated.
The original ver-bal root may be recovered from the root of the com-posed form for any subcategorization checks, at thesyntactic level.3.2 Us ing  constraints for morphologicalambigui ty  resolutionMorphological analysis does not have access to syn-tactic context, so when the morphological structure5The output of the morphological nalyzer is actuallya feature-value list in the standard LISP format.147of a lexical form has several distinct analyses, itis not possible to disambiguate such cases exceptmaybe by using root usage frequencies.
For disam-biguation one may have to use information providedby sentential position and the local morphosyntac-tic context.
Voutilainen and Heikkila (Voutilainen etal., 1992) have proposed a constraint grammar ap-proach where one specifies constraints on the localcontext of a word to disambiguate among multiplereadings of a word.
Their approach has, however,been applied to English where morphological infor-mation has rather little use in such resolution.In our tagger, constraints are applied on eachword, and check if the forms within a specified neigh-borhood of the word satisfy certain morphosyntacticor positional restrictions, and/or agreements.
Ourconstraint pattern specification is very similar tomulti-word construct specification.
Use of variables,operators and actions, are same except hat the com-pose actions does not make sense here.
The follow-ing is an example constraint hat is used to selectthe postpositional reading of certain word when it ispreceded by a yet unresolved nominal form with acertain case.
The only requirement is that the caseof the nominal form agrees with the case subcatego-rization requirement of the following postposition.
(LP = 0 refers to current word, LP = 1 refers tonext word.
)LP  = O, Case  = _C  : Output;LP  = I, Cat  = POSTP ,  Subcat  = _C  : Output.When a match is found, the matching parses fromboth words are selected and the others are discarded.This one constraint disambiguates almost all of thepostpositions and their arguments, the exceptionsbeing nominal words which semantically convey theinformation provided by the case (such as words in-dicating direction, which may be used as if they havea dative case).Finally the following example constraint deletesthe sentence final adjectival readings derived fromverbs, effectively preferring the verbal reading (asTurkish is a SOV language.
)Cat  = V, F ina lcat  = ADJ ,  SP  = END : De le te .4 Per fo rmance  o f  the  TaggerWe have performed some preliminary experimentsto assess the effectiveness of our tagger.
We haveused about 250 constraints for Turkish.
Some ofthese constraints are very general as the postpositionrule above, while some are geared towards recogni-tion of NP's of various sorts and a small number ap-ply certain syntactic heuristics.
In this section, wesummarize our preliminary results.
Table 1 presentssome preliminary results about the our tagging ex-periments.Although the texts that we have experimentedwith are rather small, the results indicate that ourapproach is effective in disambiguating morpholog-ical structures, and hence POS, with minimal userintervention.
Currently, the speed of the tagger islimited by essentially that of the morphological na-lyzer, but we have ported the morphological nalyzerto the XEROX TWOL system developed by Kart-tunen and Beesley (Karttunen and Beesley, 1992).This system can analyze Turkish word forms atabout 1000 forms/see on SparcStation 10's.
We in-tend to integrate this to our tagger soon, improvingits speed performance considerably.We have tested the impact of morphological dis-ambiguation on the performance of a LFG parserdeveloped for Turkish (GiingSrdii, 1993; GiingSrdiiand Oflazer, 1994).
The input to the parser was dis-ambiguated using the tool developed and the resultswere compared to the case when the parser had toconsider all possible morphological ambiguities it-self.
For a set of 80 sentences considered, it can beseen that (Table 2), morphological disambiguationenables almost a factor of two reduction in the av-erage number of parses generated and over a factorof two speed-up in time.5 Conc lus ionsThis paper has presented an overview of a tool fortagging text along with various issues that havecome up in disambiguating morphological parses ofTurkish words.
We have noted that the use of con-straints is very effective in morphological disam-biguation.
Preliminary results indicate that the tag-ger can tag about 98-99% of the texts accuratelywith very minimal user intervention, though it isconceivable that it may do worse on more substantialtext - but there is certainly room for improvement inthe mechanisms provided.
The tool also provides forrecognition of multi-word constructs that behave asa single syntactic and semantic entity in higher levelanalysis, and the compilation of information for fine-tuning of the morphological nalyzer and the taggeritself.
We, however, feel that our approach does notdeal satisfactorily with most aspects of word-orderfreeness.
We are currently working on an extensionwhereby the rules do not apply immediately but voteon their preferences and a final global vote tally de-termines the assignments.6 AcknowledgmentThis research was supported in part by a NATO Sci-ence for Stability Program Grant, TU-LANGUAGE.ReferencesE.
L. Antworth.
1990.
PC-KIMMO: A Two-levelProcessor for Morphological Analysis.
Summer In-stitute of Linguistics, Dallas, Texas.148Table h Statistics on texts tagged, and tagging and disambiguation resultsText Words Morphological Parse Distribution0 1 2 3 4 >51 468 7.3% 28.7% 41.1% 11.1% 7.1% 4.7%2 573 1.0% 30.2% 37.3% 13.1% 11.1% 7.3%3 533 3.8% 24.8% 38.1% 19.1% 9.2 % 5.0%4 7004 3.9% 17.2% 41.5% 15.6% 11.7% 10.1%Note: Words with zero parses are proper names which are not in the lexicon of the morpholoText % Correctly % Tagged % Correctly Automatic Disambiguation byTagged by Tagged Multi-word ConstraintsAutomatically User Total Rules1 98.5 1.0 99.1 10.1 67.72 98.5 0.3 98.8 7.5 74.43 97.8 1.1 98.9 3.1 74.54 95.4 1.7 97.1 4.2 76.4:ical analyzer.Table 2: Impact of disambiguation on parsing performanceNo disambiguation With disambiguation RatiosAvg.
Length Avg.
Avg.
Avg.
Avg.
(words) parses time (sec) parses time (sec) parses speed-up5.7 5.78 29.11 3.30 11.91 1.97 2.38Note: The ratios are the averages of the sentence by sentence ratios.E.
Brill.
1992.
A simple rule-based part-of-speechtagger.
In Proceedings of the Third Conference onApplied Computational Linguistics, Trento, Italy.K.
W. Church.
1988.
A stochastic parts programand noun phrase parser for unrestricted text.
InProceedings of the Second Conference on AppliedNatural Language Processing (ACL), pages 136-143.D.
Cutting, J. Kupiec, J. Pedersen, and P. Sibun.1993.
A practical part-of-speech tagger.
Technicalreport, Xerox Palo Alto Research Center.C.
Demir.
1993.
An ATN grammar for Turkish.Master's thesis, Department of Computer Engi-neering and Information Sciences, Bilkent Univer-sity, Ankara, Turkey, July.Z.
GfingSrdfi and K. Oflazer.
1994.
Parsing Turkishusing the Lexieal-Functional Grammar formalism.In Proceedings of COLING-94, the 15th Interna-tional Conference on Computational Linguistics,Kyoto, Japan.Z.
Gfing6rdfi.
1993.
A Lexical-Functional Gram-mar for Turkish.
Master's thesis, Department ofComputer Engineering and Information Sciences,Bilkent University, Ankara, Turkey, July.F.
Karlsson.
1990.
Constraint grammar as a frame-work for parsing running text.
In Proceedings ofCOLING-90, the 13th International Conferenceon Computational Linguistics, volume 3, pages168-173, Helsinki, Finland.L.
Karttunen and K. R. Beesley.
1992.
Two-levelrule compiler.
Technical Report, XEROX PaloAlto Research Center.K.
Koskenniemi, P. Tapanainen, and A. Voutilainen.1992.
Compiling and using finite-state syntacticrules.
In Proceedings of COLING-92, the 14thInternational Conference on Computational Lin-guistics, volume 1, pages 156-162, Nantes, France.K.
Oflazer.
1993.
Two-level description of Turkishmorphology.
In Proceedings of the Sixth Confer-ence of the European Chapter of the Associationfor Computational Linguistics, April.
A full ver-sion appears in Literary and Linguistic Comput-ing, Vol.9 No.2, 1994.A.
Voutilainen and P. Tapanainen.
1993.
Ambiguityresolution in a reductionistic parser.
In Proceed-ings of EACL'93, Utrecht, Holland.A.
Voutilainen, J. Heikkila, and A. Anttila.
1992.Constraint Grammar of English.
University ofHelsinki.149
