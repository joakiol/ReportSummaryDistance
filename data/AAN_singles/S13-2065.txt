Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 395?401, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsKLUE: Simple and robust methods for polarity classificationThomas Proisl and Paul Greiner and Stefan Evert and Besim KabashiFriedrich-Alexander-Universit?t Erlangen-N?rnbergDepartment Germanistik und KomparatistikProfessur f?r KorpuslinguistikBismarckstr.
691054 Erlangen, Germany{thomas.proisl,paul.greiner,stefan.evert,besim.kabashi}@fau.deAbstractThis paper describes our approach to theSemEval-2013 task on ?Sentiment Analysisin Twitter?.
We use simple bag-of-words mod-els, a freely available sentiment dictionary auto-matically extended with distributionally similarterms, as well as lists of emoticons and inter-net slang abbreviations in conjunction with fastand robust machine learning algorithms.
Theresulting system is resource-lean, making it rel-atively independent of a specific language.
De-spite its simplicity, the system achieves compet-itive accuracies of 0.70?0.72 in detecting thesentiment of text messages.
We also apply ourapproach to the task of detecting the context-dependent sentiment of individual words andphrases within a message.1 IntroductionThe SemEval-2013 task on ?Sentiment Analysis inTwitter?
(Wilson et al 2013) focuses on polarity clas-sification, i. e. the problem of determining whethera textual unit, e. g. a document, paragraph, sentenceor phrase, expresses a positive, negative or neutralsentiment (for a review of research topics and re-cent developments in the field of sentiment analysissee Liu (2012)).
There are two subtasks: in task B,?Message Polarity Classification?, whole messageshave to be classified as being of positive, negativeor neutral sentiment; in task A, ?Contextual PolarityDisambiguation?, a marked instance of a word orphrase has to be classified in the context of a wholemessage.The training data for task B consist of approxi-mately 10 200 manually annotated Twitter messages,the training data for task A of approximately 9 500marked instances in approximately 6 300 Twitter mes-sages.1 The test data consist of in-domain Twit-ter messages (3 813 messages for task B and 4 435marked instances in 2 826 messages for task A) andout-of-domain SMS text messages (2 094 messagesfor task B, 2 334 marked instances in 1 437 messagesfor task A).
The distribution of messages and markedinstances over sentiment categories in the trainingand test sets is shown in Tab.
1.pos neg neu totaltrain-B 3 783 1 600 4 832 10 215test-B Twitter 1 572 601 1 640 3 813test-B SMS 492 394 1 208 2 094train-A 5 862 3 166 463 9 491test-A Twitter 2 734 1 541 160 4 435test-A SMS 1 071 1 104 159 2 334Table 1: The data sets for both tasksThe main focus of the current paper lies on experi-menting with resource-lean and robust methods fortask B, the classification of whole messages.
We do,however, apply our approach also to task A.2 Features used for polarity classificationOur general approach is quite simple: we extractfeature vectors from the training data (based on the1These figures indicate the amount of training data we wereactually able to use.
Due to Twitter?s licensing conditions, thetraining data could only be made available as a collection of IDs.Even when using the official Twitter API for collecting the actualmessages rather than the screen-scraping approach suggested bythe task organizers, ca.
10% of the data were not (or no longer)available.395original messages and a small number of additionalresources) and feed them into fast and robust super-vised machine learning algorithms implemented inthe Python machine learning library scikit-learn (Pe-dregosa et al 2011).
For task B, the features arecomputed on the basis of the whole message; for taskA, we use essentially the same features, but computethem once for the marked word or phrase and oncefor the rest of the message.
All the features we useare described in some more detail in the followingsubsections.2.1 Bag of wordsWe experimented with three different sets of bag-of-words features: unigrams, unigrams and bigrams, andan extended unigram model that includes a simpletreatment of negation.
For all three models we simplyuse the word frequencies as feature weights.Our preprocessing pipeline starts with a simplepreliminary tokenization step (lowercasing the wholemessage and splitting it on whitespace).
In the re-sulting list of tokens, all user IDs and web URLs arereplaced with placeholders.2 Any remaining punctu-ation is stripped from the tokens and empty tokensare deleted.
In the extended unigram model, up tothree tokens following a negation marker are thenprefixed with not_ (fewer tokens if another negationmarker or the end of the message is reached).
Finallyall words are stemmed using the Snowball stemmer.3For a token unigram or bigram to be included inthe bag of words models, it has to occur in at leastfive messages.As an additional feature we include the total num-ber of tokens per message.2.2 Features based on a sentiment dictionaryWidely-used algorithms such as SentiStrength (Thel-wall et al 2010) rely heavily on dictionaries contain-ing sentiment ratings of words and/or phrases.
Weuse features based on an extended version of AFINN-111 (Nielsen, 2011).4The AFINN sentiment dictionary contains senti-ment ratings ranging from ?5 (very negative) to 52The regular expression for matching web URLs hasbeen taken from http://daringfireball.net/2010/07/improved_regex_for_matching_urls.3http://snowball.tartarus.org/4http://www2.imm.dtu.dk/pubdb/p.php?6010(very positive) for 2 476 word forms.
In order to ob-tain a better coverage, we extended the dictionarywith distributionally similar words.
For this pur-pose, large-vocabulary distributional semantic mod-els (DSM) were constructed from a version of theEnglish Wikipedia5 and the Google Web 1T 5-Gramsdatabase (Brants and Franz, 2006).
The WikipediaDSM consists of 122 281 case-folded word formsas target terms and 30 484 mid-frequency contentwords (lemmatised) as feature terms; the Web1T5DSM of 241 583 case-folded word forms as targetterms and 100 063 case-folded word forms as fea-ture terms.
Both DSMs use a context window of twowords to the left and right, and were reduced to 300latent dimensions using randomized singular valuedecomposition (Halko et al 2009).For each AFINN entry, the 30 nearest neighboursaccording to each DSM were considered as exten-sion candidates.
Sentiment ratings for the new candi-dates were computed by averaging over the 30 near-est neighbours of the respective candidate term (withscores set to 0 for all neighbours not listed in AFINN),and rescaling to the range [?5,5].6 After some ini-tial experiments, only candidates with a computedrating ?
?2.5 or ?
2.5 were retained, resulting in anextended dictionary of 2 820 word forms.As with the bag of words model, we make use ofa simple heuristic treatment of negation: following anegation marker, the polarity of the next sentiment-carrying token up to a distance of at most four tokensis multiplied by ?1.The sentiment dictionary is used to extract fourfeatures: I) the number of tokens that express a posi-tive sentiment, II) the number of tokens that expressa negative sentiment, III) the total number of tokensthat express a sentiment according to our sentimentdictionary and IV) the arithmetic mean of all the sen-timent scores from the sentiment dictionary in themessage.5We used the pre-processed and linguistically annotatedWackypedia corpus available from http://wacky.sslmit.unibo.it/.6Scaling coefficients were determined by regression on ex-tension candidates that were already listed in AFINN.3962.3 Features based on emoticons and internetslang abbreviationsIn addition to the sentiment dictionary we use a listof 212 emoticons and 95 internet slang abbreviationsfrom Wikipedia.
We manually classified these 307emotion markers as negative (?1), neutral (0) or pos-itive (1).The extracted features based on this list are similarto the ones based on the sentiment dictionary.
We useI) the number of positive emotion markers, II) thenumber of negative emotion markers, III) the totalnumber of emotion markers and IV) the arithmeticmean of all the emotion markers in the message.3 ExperimentsIn this section we evaluate different classifiers (multi-nomial Naive Bayes,7 Linear SVM8 and MaximumEntropy9) and various combinations of features onthe gold test sets.
We vary the bag-of-words model(bow), the use of AFINN (sent), our extensions tothe sentiment dictionary (ext) and the list of emotionmarkers (emo).
To present as clear a picture of theclassifiers?
performances as possible, we report F-scores for each of the three classes, the weighted av-erage of all three F-scores (Fw), the (unweighted) av-erage of the positive and negative F-scores (Fpos+neg;this is the value shown in the official task results andused for ranking systems), as well as accuracy.Results for submitted systems are typeset in italics,the best results in each column are typeset in boldfont.3.1 Task B: Message Polarity ClassificationExperiments with just a simple unigram bag-of-words model show that for both the Twitter (Tab.
3)and the SMS data (Tab.
4) the Maximum Entropyclassifier outperforms multinomial Naive Bayes andLinear SVM by a considerable margin.
For compar-ison, we also include some weak baselines (Tab.
2).The random baselines classify messages randomly,107We always use the default setting alpha = 1.0.8In all experiments, we use the following parameters:penalty = ?l1?, dual = False, C = 1.0.9We use the following parameter settings in our experiments:penalty = ?l1?, C = 1.0.10randomuniform assumes a uniform probability distribution(all categories have equal probabilities), randomweighted haslearned the probability distribution from the training data,the majority baselines simply assign all messagesto the most frequent category in the training data.11As one would expect, all three learning algorithmsare vastly superior to those baselines.
Using bothunigrams and bigrams in the bag-of-words modelimproves classifier peformance; so does the extendedunigram model with negations.For the Twitter data, adding the sentiment dictio-nary, the dictionary extensions and the list of emo-tion markers further improves classifier performance,with the best results being achieved by a combina-tion of all these features with a uni- and bigram bag-of-words model.
The best combination of featureswould have been the fourth best system out of 35constrained systems (sixth best out of all 51 systems),one rank higher than our task submission.12For the SMS data, adding the sentiment dictio-nary and the dictionary extensions seems to improvethe official score Fpos+neg, but slightly decreasesweighted average F-score and accuracy.
This mightbe due to the greater orthographical variation in SMStexts.
Emotion markers seem to be a much bettersentiment indicator in the SMS data.
But while justcombining the list of emotion markers with the ex-tended unigram bag-of-words model leads to the bestweighted average F-score and accuracy, Fpos+neg isbest when a combination of all features is used.
Thisis also the system we submitted, being the third bestsystem (out of 44) for that task.3.2 Task A: Contextual PolarityDisambiguationThe results for task A are similar to those for taskB in that Maximum Entropy is the best classifier forthe unigram bag-of-words model for both the Twitter(Tab.
5) and the SMS data (Tab.
6).
Adding negationtreatment to the bag-of-words model increases classi-fier performance, as do the inclusion of AFINN andthe use of emotion markers.
Interestingly, extend-ing the sentiment dictionary based on distributionalsimilarity leads to slightly worse results.
Therefore,randomweighted,binary uses the same probability distribution butclassifies messages only as either positive or negative.11majority classifies all messages as neutral, as this is the mostfrequent category in the training data, majoritybinary does binaryclassification and thus classifies all messages as positive.12Evaluation results for all SemEval-2013 tasks are avail-able online: http://www.cs.york.ac.uk/semeval-2013/index.php?id=evaluation-results.397classifier Fpos Fneg Fneu Fw Fpos+neg Accrandomuniform 0.3666 0.2128 0.3745 0.3458 0.2897 0.3318randomweighted 0.3912 0.1681 0.4521 0.3820 0.2796 0.3835randomweighted,binary 0.5186 0.2042 0.000 0.2460 0.3614 0.3349majority 0.0000 0.0000 0.6015 0.2587 0.0000 0.4301majoritybinary 0.5838 0.0000 0.0000 0.2407 0.2919 0.4123Table 2: Some weak baselines for task B, Twitter test setclassifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg AccMultin.
NB uni - - - 0.6355 0.5093 0.6898 0.6390 0.5724 0.6423LinearSVM uni - - - 0.6412 0.4884 0.6876 0.6371 0.5648 0.6418MaxEnt uni - - - 0.6705 0.5109 0.7212 0.6671 0.5907 0.6761MaxEnt uni+bi - - - 0.6845 0.5192 0.7257 0.6762 0.6019 0.6845MaxEnt unineg - - - 0.6797 0.5284 0.7242 0.6750 0.6041 0.6824MaxEnt unineg + - - 0.6860 0.5661 0.7284 0.6854 0.6261 0.6911MaxEnt unineg - - + 0.6807 0.5393 0.7229 0.6766 0.6100 0.6835MaxEnt unineg + + - 0.6841 0.5529 0.7258 0.6814 0.6185 0.6874MaxEnt unineg + + + 0.6963 0.5650 0.7325 0.6912 0.6306 0.6968MaxEnt unineg + - + 0.6952 0.5753 0.7338 0.6929 0.6353 0.6984MaxEnt uni+bi + - + 0.7034 0.5706 0.7358 0.6964 0.6370 0.7018MaxEnt uni+bi + + + 0.7052 0.5720 0.7371 0.6979 0.6386 0.7031MaxEnt - + + + 0.6920 0.3532 0.6533 0.6220 0.5226 0.6370Table 3: Evaluation results for task B on the Twitter test setclassifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg AccMultin.
NB uni - - - 0.4918 0.4773 0.5541 0.5250 0.4845 0.5153LinearSVM uni - - - 0.5833 0.5046 0.7229 0.6490 0.5440 0.6442MaxEnt uni - - - 0.6260 0.5015 0.7903 0.6974 0.5638 0.7015MaxEnt uni+bi - - - 0.6003 0.5380 0.7658 0.6840 0.5692 0.6829MaxEnt unineg - - - 0.6528 0.5412 0.7884 0.7100 0.5970 0.7125MaxEnt unineg + - - 0.6399 0.5955 0.7744 0.7092 0.6177 0.7073MaxEnt unineg - - + 0.6596 0.5507 0.8033 0.7220 0.6052 0.7259MaxEnt unineg + + - 0.6374 0.5905 0.7731 0.7068 0.6140 0.7049MaxEnt unineg + + + 0.6506 0.5900 0.7903 0.7198 0.6203 0.7197MaxEnt unineg + - + 0.6556 0.5833 0.7908 0.7200 0.6195 0.7202MaxEnt uni+bi + - + 0.6318 0.5896 0.7750 0.7064 0.6107 0.7044MaxEnt uni+bi + + + 0.6341 0.5783 0.7746 0.7047 0.6062 0.7030MaxEnt - + + + 0.5961 0.3421 0.7179 0.6186 0.4691 0.6342Table 4: Evaluation results for task B on the SMS test set398classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg AccMultin.
NB uni - - - 0.7799 0.6164 0.0498 0.6967 0.6981 0.7067LinearSVM uni - - - 0.7759 0.6046 0.0576 0.6905 0.6902 0.6949MaxEnt uni - - - 0.7974 0.6155 0.0110 0.7059 0.7065 0.7218MaxEnt uni+bi - - - 0.8071 0.6320 0.0222 0.7179 0.7195 0.7335MaxEnt unineg - - - 0.8058 0.6380 0.0110 0.7188 0.7219 0.7342MaxEnt unineg + - - 0.8160 0.6610 0.0317 0.7339 0.7385 0.7479MaxEnt unineg + + - 0.8153 0.6583 0.0316 0.7325 0.7368 0.7466MaxEnt unineg + + + 0.8141 0.6608 0.0330 0.7326 0.7374 0.7468MaxEnt unineg + - + 0.8153 0.6664 0.0331 0.7353 0.7409 0.7493Table 5: Evaluation results for task A on the Twitter test setclassifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg AccMultin.
NB uni - - - 0.6766 0.6657 0.0213 0.6268 0.6712 0.6452LinearSVM uni - - - 0.6628 0.6533 0.0365 0.6157 0.6581 0.6290MaxEnt uni - - - 0.6829 0.6630 0.0117 0.6277 0.6729 0.6491MaxEnt uni+bi - - - 0.6825 0.6504 0.0230 0.6224 0.6665 0.6435MaxEnt unineg - - - 0.7008 0.6770 0.0120 0.6427 0.6889 0.6654MaxEnt unineg + - - 0.7127 0.6962 0.0238 0.6579 0.7044 0.6804MaxEnt unineg + + - 0.7108 0.6954 0.0238 0.6568 0.7031 0.6791MaxEnt unineg + + + 0.7090 0.7017 0.0237 0.6589 0.7054 0.6808MaxEnt unineg + - + 0.7114 0.7034 0.0238 0.6608 0.7074 0.6829Table 6: Evaluation results for task A on the SMS test setwe could have improved upon our task submissionby excluding the sentiment dictionary extensions ?however, the gains are very small and the system?sranks would still be the same (17/28 for the Twitterdata, 16/26 for the SMS data).4 Discussion4.1 Error analysis4.1.1 Task B: Message Polarity ClassificationThe most prominent problem, according to the con-fusion matrix in Tab.
7, is that a lot of negative mes-sages are classified as neutral; the same problemexists to a lesser extent for positive messages.A qualitative analysis of mis-classified messagesfor which the MaxEnt classifier indicated high con-fidence suggests that the human annotators did notclearly distinguish between sentiment expressed bythe authors of messages and their own response tomessage content.
For example, the messages shownpredictedpos neg neugoldpos 979 352 70 40 523 100neg 70 47 287 213 244 134neu 191 191 58 75 1391 942Table 7: Task B, confusion matrix for tweets/SMSin (1) and (2) report a negative and positive event,respectively, in a neutral way and should thereforebe annotated with neutral sentiment.
However, in thetest data they are labelled as negative and positive bythe human annotators.
(1) MT @LccSy #Syria, Deir Ezzor | Marba?eh:Aerial shelling dropped explosive barrels onresidential buildings in the town.
Tue, 23October.399(2) European Exchanges open with a slightrise: (AGI) Rome, October 24 - Euro-pean Exchanges opened with a slight ris...http://t.co/mAljf6eTThis problem is probably a major factor in the mis-classification of many negative and positive messagesas neutral.
In order to better reproduce the humanannotations, the system would additionally have todecide whether a reported event is of a negative, pos-itive or neutral nature per se ?
a quite different taskthat would require external training data and worldknowledge.An analysis of mis-classified positive messagesfurther suggests that certain punctuation marks, espe-cially multiple exclamation marks, might be usefulas additional features.4.1.2 Task A: Contextual PolarityDisambiguationThe confusion matrix in Tab.
8 shows that mes-sages marked as negative in the test data often mis-classified as positive and vice versa, while neutralinstances are overwhelmingly classified as positiveor negative.
This suggests that for the classifiers weuse, there might be too few neutral instances in thetraining data (cf.
Tab.
1).predictedpos neg neugoldpos 2329 826 397 239 8 6neg 550 341 980 761 11 2neu 109 92 48 65 3 2Table 8: Task A, confusion matrix for tweets/SMS4.2 Conclusion and future workWe use a resource-lean approach, relying only onthree external resources: a stemmer, a relativelysmall sentiment dictionary and an even smaller listof emotion markers.
Stemmers are already avail-able for many languages and both kinds of lexicalresources can be gathered relatively easily for otherlanguages.
The list of emotion markers should applyto most languages.
This makes our whole system rel-atively language-independent, provided that a similaramount of manually labelled training data is avail-able.13 In fact, the learning curve for our system(Fig.
1) suggests that even as few as 3 000?3 500labelled messages might be sufficient.
The similar0.00.20.40.60.81.0Amount of training dataScore500 1500 2500 3500 4500 5500 6500 7500 8500 9500Fpos+negAccuracyFigure 1: Learning curve of our system for the ?MessagePolarity Classification?
task, evaluated on the Twitter dataevaluation results for the Twitter and the SMS datashow that not relying on Twitter-specific features likehashtags pays off: by making our system as genericas possible, it is robust, not overfitted to the trainingdata, and generalizes well to other types of data.
Themethods discussed in the current paper are particu-larly well suited to the ?Message Polarity Classifica-tion?
task, our system ranking amongst the best.
Itturns out, however, that simply applying the same ap-proach to the ?Contextual Polarity Disambiguation?task yields only mediocre results.In the future, we would like to experiment with acouple of additional features.
Determining the near-est neighbors of a message based on Latent SemanticAnalysis might be a useful addition, as might be theuse of part-of-speech tags created by an in-domainPOS tagger (Gimpel et al 2011)14.
We would alsolike to find out whether a heuristic treatment of inten-sifiers and detensifiers, the normalization of characterrepetitions, or the inclusion of some punctuation-based features could further improve classifier per-formance.13For task B, even the extended unigram bag-of-words modelby itself, without any additional resources, would have per-formed quite well as the 9th best constrained system on theTwitter test set (13th best system overall) and the 5th best systemon the SMS test set.14http://www.ark.cs.cmu.edu/TweetNLP/400ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Linguistic Data Consortium, Philadelphia,PA.Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-panjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: Annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics, pages 42?47, Portland,Oregon.
Association for Computational Linguistics.N.
Halko, P. G. Martinsson, and J.
A. Tropp.
2009.
Find-ing structure with randomness: Stochastic algorithmsfor constructing approximate matrix decompositions.Technical Report 2009-05, ACM, California Instituteof Technology, September.Bing Liu.
2012.
Sentiment Analysis and Opinion Mining.Synthesis Lectures on Human Language Technologies.Morgan & Claypool.Finn ?rup Nielsen.
2011.
A new ANEW: Evaluation ofa word list for sentiment analysis in microblogs.
InProceedings of the ESWC2011 Workshop on ?MakingSense of Microposts?
: Big things come in small pack-ages, number 718 in CEUR Workshop Proceedings,pages 93?98, Heraklion.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Mike Thelwall, Kevan Buckley, Georgios Paltoglou,Di Cai, and Arvid Kappas.
2010.
Sentiment in shortstrength detection informal text.
Journal of the Amer-ican Society for Information Science and Technology,61(12):2544?2558.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.SemEval-2013 task 2: Sentiment analysis in Twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013).
Association forComputational Linguistics.401
