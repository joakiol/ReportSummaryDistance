Proceedings of the 6th Workshop on Statistical Machine Translation, pages 316?322,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsShallow Semantic Trees for SMTWilker Aziz, Miguel Rios and Lucia SpeciaResearch Group in Computational LinguisticsUniversity of WolverhamptonStafford Street, Wolverhampton, WV1 1SB, UK{w.aziz, m.rios, l.specia}@wlv.ac.ukAbstractWe present a translation model enriched withshallow syntactic and semantic informationabout the source language.
Base-phrase la-bels and semantic role labels are incorporatedinto an hierarchical model by creating shal-low semantic ?trees?.
Results show an in-crease in performance of up to 6% in BLEUscores for English-Spanish translation over astandard phrase-based SMT baseline.1 IntroductionThe use of semantic information to improve Statis-tical Machine Translation (SMT) is a very recent re-search topic that has been attracting significant at-tention.
In this paper we describe our participationin the shared translation task of the 6th Workshop onStatistical Machine Translation (WMT) with a sys-tem that incorporates shallow syntactic and semanticinformation into hierarchical SMT models.The system is based on the Moses toolkit (Hoanget al, 2009; Koehn et al, 2007) using hierarchi-cal models informed with shallow syntactic (chunks)and semantic (semantic role labels) information forthe source language.
The toolkit SENNA (Collobertet al, 2011) is used to provide base-phrases (chunks)and semantic role labels.Experiments with English-Spanish and English-German news datasets show promising results andhighlight important issues about the use of seman-tic information in hierarchical models as well as anumber of possible directions for further research.The remaining of the paper is organized as fol-lows: Section 2 presents related work; Section 3 de-scribes the method; Section 4 presents the results ob-tained for the English-Spanish and English-Germantranslation tasks; and Section 5 brings some conclu-sions and directions for further research.2 Related WorkIn hierarchical SMT (Chiang, 2005), a SynchronousContext Free Grammar (SCFG) is learned from aparallel corpus.The model capitalizes on the recur-sive nature of language replacing sub-phrases byan unlabeled nonterminal.
Hierarchical models areknown to produce high coverage rules, once they areonly constrained by the word alignment.
Neverthe-less the lack of specialized vocabulary also leads tospurious ambiguity (Chiang, 2005).Syntax-based models are hierarchical modelswhose rules are constrained by syntactic informa-tion.The syntactic constraints have an impact inthe rule extraction process, reducing drastically thenumber of rules available to the system.
While thismay be helpful to reduce ambiguity, it can lead topoorer performance (Ambati and Lavie, 2008).Motivated by the fact that syntactically constrain-ing a hierarchical model can decrease translationquality, some attempts to overcome the problemsat rule extraction time have been made.
Venugopaland Zollmann (2006) propose a heuristic method torelax parse trees known as Syntax Augmented Ma-chine Translation (SAMT).
Significant gains are ob-tained by grouping nonterminals under categorieswhen they do not span across syntactic constituents.Hoang and Koehn (2010) propose a soft syntax-based model which combines the precision of asyntax-constrained model with the coverage of an316unconstrained hierarchical model.
Instead of hav-ing heuristic strategies to combine nonterminals in aparse tree, whenever a rule cannot be retrieved be-cause it does not span a constituent, the extractionprocedure falls back to the hierarchical approach, re-trieving a rule with unlabeled nonterminals.
Perfor-mance gains are reported over standard hierarchicalmodels using both full parse trees and shallow syn-tax.Moving beyond syntactic information, some at-tempts have recently been made to add semantic an-notations to SMT.
Wu and Fung (2009) present atwo-pass model to incorporate semantic informationto the phrase-based SMT pipeline.
The method per-forms conventional translation in a first step, fol-lowed by a constituent reordering step seeking tomaximize the cross-lingual match of the semanticrole labels of the translation and source sentences.Liu and Gildea (2010) add features extracted fromthe source sentences annotated with semantic rolelabels in a tree-to-string SMT model.
They mod-ify a syntax-based SMT system in order to penal-ize/reward role reordering and role deletion.
Theinput sentence is parsed for semantic roles and theroles are then projected onto the target side usingword alignment information at decoding time.
Theyassume that a one-to-one mapping between sourceand target roles is desirable.Baker et al (2010) propose to graft semantic in-formation, namely named entities and modalities, tosyntactic tags in a syntax-based model.
The vocab-ulary of nonterminals is specialized using the se-mantic categories, for instance, a noun phrase (NP)whose head is a geopolitical entity (GPE) will betagged as NPGPE, making the rule table less am-biguous.Similar to (Baker et al, 2010) we specialize a vo-cabulary of syntactic nonterminals with semantic in-formation, however we use shallow syntax (base-phrases) and semantic role labels instead of con-stituent parse and named entities.
The resulting shal-low trees are relaxed following SAMT (Venugopaland Zollmann, 2006).
Different from previous workwe add the semantic knowledge at the level of thecorpus annotation.
As a consequence, instead of bi-asing deletion and reordering through additional fea-tures (Liu and Gildea, 2010), we learn hierarchicalrules that encode those phenomena, taking also intoaccount the semantic role of base-phrases.3 Proposed MethodThe proposed method is based on an extension of thehierarchical models in Moses using source languageinformation.
Our submission included systems fortwo language pairs: English-Spanish (en-es) andEnglish-German (en-de) and was constrained to us-ing data provided by WMT11.
Phrase and rule ex-traction were performed using the entire en-es anden-de portions of Europarl.
Model parameters weretuned using the news-test2008 dataset.
Three 5-gram Spanish and German language models weretrained using SRILM1 with the News Commentaries(?
160K sentences), Europarl (?
2M sentences)and News (?
5M sentences) corpora.
These modelswere interpolated using scripts provided in Moses(Koehn and Schroeder, 2007).At pre-processing stage, sentences longer than 80tokens were filtered from the training/developmentcorpus.
The parallel corpus was then tokenized andtruecased.
Additionally, for en-de, compound split-ting of the German side of the corpus was performedusing a frequency based method described in (Koehnand Knight, 2003).
This method helps alleviate spar-sity, reducing the size of the vocabulary by decom-posing compounds into their base words.
Recas-ing and detokenization, along with compound merg-ing of the translations into German, were handledat post-processing stage.
Compound merging wasperformed by finding the most likely sequences ofwords to be merged into previously seen compounds(Stymne, 2009).3.1 Source Language AnnotationFor rule extraction, training and test, the English sideof the corpus was annotated with Semantic Role La-bels (SRL) using the toolkit SENNA2, which alsooutputs POS and base-phrase (without prepositionalattachment) tags.
The resulting source language an-notation was used to produce trees in order to builda tree-to-string model in Moses.1http://www.speech.sri.com/projects/srilm/2http://ml.nec-labs.com/senna/317SNP VP NP PP NP O O NP VP NP ADVPPRP VBZ TO VB DT NN TO NN PUNC CC PRP VBZ RB VBD WDT RBhe intends to donate this money to charity , but he has not decided which yetFigure 1: Example of POS tags and base-phrase annotation.
Base-phrases: noun-phrase (NP), verb-phrase(VP), prepositional-phrase (PP), adverbial-phrase (ADVP), outside-of-a-phrase (O)In order to derive trees for the source side of thecorpus from this annotation, a new level is created toadd the POS tags for each word form.
Syntactic tagsare then added by grouping words and POS tags intobase phrases using linguistic information as givenby SENNA.
Figure 1 shows an example of an inputsentence annotated with POS and base-phrase infor-mation.
Additionally, SRLs are used to enrich thePOS and base-phrase annotation levels.
Semanticroles are assigned to each predicate independently.As a consequence, the resulting annotation cannotbe considered a tree and there is not an obvious hi-erarchy of predicates in a sentence.
For example,Figure 2 shows the SRL annotation for the examplein Figure 1.
[A0 He] [T intends] [A1 to donate this money to charity],but he has not decided which yet[A0 He] intends to [T donate] [A1 this money] [A2 tocharity], but he has not decided which yetHe intends to donate this money to charity, but [A0 he]has [AM-NEG not] [T decided] [A1 which] [AM-TMPyet]Figure 2: SRL for sentence in Figure 1Arguments of a single predicate never overlap,however in longer sentences, the occurrence of mul-tiple verbs increases the chances that arguments ofdifferent predicates overlap, that is, the argument ofa verb might contain or even coincide with the argu-ment of another verb and depending on the verb theargument role might change.
For example, in Fig-ure 2: i) He is both the agent of intend and donate;ii) this money is the donated thing and also part ofthe chunk which express the intention (to donate thismoney to charity).
In a different example we can seethat arguments might overlap and their roles changecompletely depending on their target predicates (e.gin I gave you something to eat, you is the recipientof the verb give and the agent of the verb eat).
Forthis reason, why semantic role labels are usually an-notated individually in different structures, as shownin Figure 2, each annotation focusing on a single tar-get verb.
In order to convert the predicates and argu-ments of a sentence into a single tree, we enrich thePOS-tags and base-phrase annotation as follows:?
Semantic labels are directly grafted to the base-phrase annotation whenever possible, that is,if a predicate argument coincides with a sin-gle base-phrase, the base-phrase type is spe-cialized with the argument role.
In Figure 3,the noun-phrase (NP) the money is specializedinto NP:A1:donate, since that single NP is theargument A1 of donate.?
If a predicate argument groups multiple base-phrases, the semantic label applies to a node ina new level of the tree subsuming all these base-phrases.
In Figure 3, the base-phrases to (PP)and charity (NP) are grouped by A2:donate.?
We add the labels sequentially from the short-est chunks to the largest ones.
If two la-bels spanning the same number of tokens: i)overlap completely, we merge them so thatno hierarchy is imposed between their targets(e.g.
in Figure 3, the noun-phrase He is spe-cialized into NP:A0:donate,intend); ii) over-lap partially, we merge them so that the re-sulting label will compete against other labelsin a different length category.
If a label span-ning a larger chunk overlaps partially with alabel spanning a shorter chunk, or contains it,we stack them in a way that the first subsumesthe second (e.g in Figure 3, A1:intend sub-sumes VP:T:donate, NP:A1:donate,intend andA2:donate).?
Verb phrases might get split if they containmultiple target predicates (e.g.
in Figure 3,the VP intends to donate is split into two verb-318phrases, each specialized with its own role la-bel).?
Finally, tags are lexicalized, that is, semanticlabels are composed by their type (e.g.
A0) andtarget predicate lemma (verb).Figure 3 shows and example of how semantic la-bels are combined with shallow syntax in order toproduce the input tree for the sentence in Figure1.
The argument A1 of intend subsumes the targetverb donate and its arguments A1 and A2; A2:donategroups base-phrases so as to attach the preposition tothe noun phrase.Finally, following the method for syntactic treesby Venugopal and Zollmann (2006), the input treesare relaxed in order to alleviate the impact of thelinguistic constraints on rule extraction.
We relaxtrees3 by combining any pairs of neighboring nodes.For example, NP:A0:donate,intend+VP:T:intendand NP:A1:donate+A2:donate are created for thetree in Figure 3.4 ResultsAs a baseline to compare against our proposed ap-proach (srl), we took a phrase-based SMT system(pb) built using the Moses toolkit with the samedatasets and training conditions described in Sec-tion 3.
The results are reported in terms of standardBLEU (Papineni et al, 2002) (and its case sensitiveversion, BLEU-c) and tested for statistical signifi-cance using an approximate randomization test (Rie-zler and Maxwell, 2005) with 100 iterations.In addition, we included an intermediate modelbetween these two: a hierarchical model in-formed with source-language base-phrase informa-tion (chunk).
For the English-Spanish task we alsobuilt a purely hierarchical model (hier) using Mosesand the same datasets and training conditions.
Forthe English-German task, hierarchical models havenot been shown to outperform standard phrase-basedmodels in previous work (Koehn et al, 2010).Table 1 shows the performance achieved for theEnglish-Spanish translation task test set, where (srl)is our official submission.
One can notice a signifi-cant gain in performance (up to 6% BLEU) in usingtree-based models (with or without source language3Using the Moses implementation relax-parse for SAMT 2annotation) as opposed to using standard phrase-based models.Model BLEU BLEU-cpb 0.2429 0.2340srl 0.2901 0.2805hier 0.3029 0.2933chunk 0.3034 0.2935Table 1: English-Spanish experiments - differencesbetween all pairs of models are statistically signifi-cant with 99% confidence, except for the pair (hier,chunk)The purely hierarchical approach performs aswell as our linguistically informed tree-based mod-els (chunk and srl).
On the one hand this findingis somewhat disappointing as we expected that tree-based models would benefit from linguistic annota-tion.
On the other hand it shows that the linguisticannotation yields a significant reduction in the num-ber of unnecessary productions: the linguistically in-formed models are much smaller than hier (Table5), but perform just as well.
Whether the linguisticannotation significantly helps make the productionsless ambiguous or not is still a question to be ad-dressed in further experimentation.Table 2 shows the performance achieved for theEnglish-German translation task test set.
These re-sults indicate that the linguistic information did notlead to any significant gains in terms of automaticmetrics.
An in-depth comparative analysis based ona manual inspection of the translations remains to bedone.Model BLEU BLEU-cpb 0.1398 0.1360srl 0.1381 0.1344chunk 0.1403 0.1367Table 2: English-German experiments - differencesbetween pairs of models are not statistically signifi-cantIn Table 3 we also show the impact of three com-pound merging strategies as post-processing for en-de: i) no compound merging (nm), ii) frequency-based compound merging (fb), and iii) frequency-319SNP:A0:donate,intendPRPHeVP:T:intendVBZintendsA1:intendVP:T:donateTOtoVBdonateNP:A1:donateDTthisNNmoneyA2:donatePPTOtoNPNNcharity...Figure 3: Tree for example in Figure 1based compound merging constrained by POS4(cfb).
Applying both frequency-based compoundmerging strategies (Stymne, 2009) resulted in sig-nificant improvements of nearly 0.5% in BLEU.Model BLEU BLEU-cnm 0.1334 0.1298fb 0.1369 0.1332cfb 0.1381 0.1344Table 3: English-German compound merging - dif-ferences between all pairs of models are statisticallysignificant with 99% confidenceAnother somewhat disappoint result is the perfor-mance of srl when compared to chunk.
We believethe main reason why the chunk models outperformthe srl models is data sparsity.
The semantic infor-mation, and particularly the way it was used in thispaper, with lexicalized roles, led to a very sparsemodel.
As an attempt to make the srl model lesssparse, we tested a version of this model withoutlexicalizing the semantic tags, in other words, us-ing the semantic role labels only, for example, A1instead of A1:intend in Figure 3.
Table 4 shows thatmodels with lexicalized semantic roles (lex) consis-tently outperform the alternative version (non lex),although the differences were only statistically sig-nificant for the en-de dataset.
One reason for thatmay be that non-lexicalized rules do not help mak-4POS tagging was performed using the TreeTagger toolkit:http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ing the chunk rules less ambiguous.Model BLEU BLEU-cen-esnon lex 0.2891 0.2795en-eslex 0.2901 0.2805en-denon lex 0.1319 0.1284en-delex 0.1381 0.1344Table 4: Alternative model with non-lexicalized tags- differences are statistically significant with 99%confidence for en-de onlyTable 5 shows how the additional annotation con-strains the rule extraction (for the en-es dataset).
Theunconstrained model hier presents the largest ruletable, followed by the chunk model, which is onlyconstrained by syntactic information.
The modelsenriched with semantic labels, both the lexicalizedor non-lexicalized versions, contain a comparablenumber of rules.
They are at least half the size ofthe chunk model and about 9 times smaller than thehier model.
However, the number of nonterminalsin the lexicalized models highlights the sparsity ofsuch models.Model Rules Nonterminalshier 962,996,167 1chunk 235,910,731 3,390srlnon lex 92,512,493 44,095srllex 117,563,878 3,350,145Table 5: Statistics from the rule tableIn order to exemplify the importance of having320some form of lexicalized information as part of thesemantic models, Figure 4 shows two predicateswhich present different semantic roles, even thoughthey have nearly the same shallow syntactic struc-ture.
In this case, unless lexicalized, rules map-ping semantic roles into base-phrases become am-biguous.
Besides, the same role might appear sev-eral times in the same sentence (Figure 2).
In thiscase, if the semantic roles are not annotated withtheir target lemma, they bring additional confusion.Therefore, the model needs the lexical informationto distinguish role deletion and reordering phenom-ena across predicates.Figure 4: Different SRL for similar chunks[NP:A0 I] [VP:T gave] [NP:A2 you] [NP:A1 a car][NP:A0 I] [VP:T dropped] [NP:A1 the glass] [AM-LOC[PP on] [NP the floor]]In WMT11?s official manual evaluation, our sys-tem submissions (srl) were ranked 10th out of 15systems in the English-Spanish task, and 18th outof 22 systems participating in the English-Germantask.
For detailed results refer to the overview paperof the Shared Translation Task of the Sixth Work-shop on Machine Translation (WMT11).5 ConclusionsWe have presented an effort towards using shal-low syntactic and semantic information for SMT.The model based on shallow syntactic information(chunk annotation) has significantly outperformed abaseline phrase-based model and performed as wellas a hierarchical phrase-based model with a signifi-cantly smaller number of translation rules.While annotating base-phrases with semantic la-bels is intuitively a promising research direction, thecurrent model suffers from sparsity and representa-tion issues resulting from the fact that multiple pred-icates share arguments within a given sentence.
Asa consequence, shallow semantics has not yet shownimprovements with respect to the chunk-based mod-els.In future work, we will address the sparsity is-sues in the lexicalized semantic models by cluster-ing predicates in a way that semantic roles can bespecialized with semantic categories, instead of theverb lemmas.ReferencesVamshi Ambati and Alon Lavie.
2008.
Improving syntaxdriven translation models by re-structuring divergentand non-isomorphic parse tree structures.
In The EightConference of the Association for Machine Translationin the Americas (AMTA).Kathryn Baker, Michael Bloodgood, Chris Callison-burch, Bonnie J. Dorr, Nathaniel W. Filardo, LoriLevin, Scott Miller, and Christine Piatko.
2010.Semantically-informed syntactic machine translation:A tree-grafting approach.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceeding ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 263?270.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.arXiv:1103.0398v1.Hieu Hoang and Philipp Koehn.
2010.
Improved trans-lation with source syntax labels.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 409?417.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.
Aunified framework for phrase-based, hierarchical, andsyntax-based statistical machine translation.
In Pro-ceedings of International Workshop on Spoken Lan-guage Translation, pages 152 ?
159.Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In Proceedings of thetenth conference on European chapter of the Associ-ation for Computational Linguistics - Volume 1, pages187?193.Philipp Koehn and Josh Schroeder.
2007.
Experiments indomain adaptation for statistical machine translation.In Proceedings of the Second Workshop on StatisticalMachine Translation, pages 224?227.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-lison Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In 45th An-nual Meeting of the Association for ComputationalLinguistics.Philipp Koehn, Barry Haddow, Philip Williams, and HieuHoang.
2010.
More linguistic annotation for statis-tical machine translation.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation andMetricsMATR, pages 115?120.321Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In Proceedings of the23rd International Conference on Computational Lin-guistics, pages 716?724.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318.Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor mt.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, Work-shop in Intrinsic and Extrinsic Evaluation Measuresfor MT and Summarization.Sara Stymne.
2009.
A comparison of merging strate-gies for translation of german compounds.
In Proceed-ings of the 12th Conference of the European Chapterof the Association for Computational Linguistics: Stu-dent Research Workshop, pages 61?69.Ashish Venugopal and Andreas Zollmann.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation, pages 138?141.Dekai Wu and Pascale Fung.
2009.
Semantic roles forsmt: a hybrid two-pass model.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, Companion Vol-ume: Short Papers, pages 13?16.322
