c?
2004 Association for Computational LinguisticsCorMet: A Computational, Corpus-BasedConventional Metaphor Extraction SystemZachary J. Mason?Brandeis UniversityCorMet is a corpus-based system for discovering metaphorical mappings between concepts.
Itdoes this by finding systematic variations in domain-specific selectional preferences, which areinferred from large, dynamically mined Internet corpora.Metaphors transfer structure from a source domain to a target domain, making some conceptsin the target domain metaphorically equivalent to concepts in the source domain.
The verbs thatselect for a concept in the source domain tend to select for its metaphorical equivalent in thetarget domain.
This regularity, detectable with a shallow linguistic analysis, is used to find themetaphorical interconcept mappings, which can then be used to infer the existence of higher-levelconventional metaphors.Most other computational metaphor systems use small, hand-coded semantic knowledge basesand work on a few examples.
Although CorMet?s only knowledge base is WordNet (Fellbaum 1998)it can find the mappings constituting many conventional metaphors and in some cases recognizesentences instantiating those mappings.
CorMet is tested on its ability to find a subset of theMaster Metaphor List (Lakoff, Espenson, and Schwartz 1991).1.
IntroductionLakoff (1993) argues that rather than being a rare form of creative language, somemetaphors are ubiquitous, highly structured, and relevant to cognition.
To date, therehas been no robust, broadly applicable computational metaphor interpretation system,a gap this article is intended to take a first step toward filling.Most computational models of metaphor depend on hand-coded knowledge basesand work on a few examples.
CorMet is designed to work on a larger class ofmetaphors by extracting knowledge from large corpora without drawing on any hand-coded knowledge sources besides WordNet.A method for computationally interpreting metaphorical language would be use-ful for NLP.
Although metaphorical word senses can be cataloged and treated as justanother part of the lexicon, this kind of representation ignores regularities in polysemy.A conventional metaphor may have a very large number of linguistic manifestations,which makes it useful to model the metaphor?s underlying mechanisms.
CorMet isnot capable of interpreting any manifestation of conventional metaphor but is a steptoward such a system.CorMet analyzes large corpora of domain-specific documents and learns the selec-tional preferences of the characteristic verbs of each domain.
A selectional preferenceis a verb?s predilection for a particular type of argument in a particular role.
For in-stance, the object of the verb pour is generally a liquid.
Any noun that pour takes as an?
Computer Science Department, Waltham, MA 02134.
E-mail: zmason@amazon.com.24Computational Linguistics Volume 30, Number 1an object is likely to be intended as a liquid, either metaphorically or literally.
CorMetfinds conventional metaphors by finding systematic differences in selectional prefer-ences between domains.
For instance, if CorMet were to find a sentence like Fundspoured into his bank account in a document from the FINANCE domain, it could inferthat in that domain, pour has a selection preference for financial assets in its subject.By comparing this selectional preference with pour?s selectional preferences in the LABdomain, CorMet can infer a metaphorical mapping from money to liquids.
By findingsets of co-occuring interconcept mappings (like the above mapping and a mappingfrom investments to containers, for instance), Cormet can articulate the higher-orderstructure of conceptual metaphors.
Note that Cormet is designed to detect higher-order conceptual metaphors by finding some of the sentences embodying some of theinterconcept mappings constituting the metaphor of interest but is not designed to bea tool for reliably detecting all instances of a particular metaphor.CorMet?s domain-specific corpora are obtained from the Internet.
In this context,a domain is a set of related concepts, and a domain-specific corpus is a set of docu-ments relevant to those concepts.
CorMet?s input parameters are two domains betweenwhich to search for interconcept mappings and, for each domain, a set of characteristickeywords.CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff,Espenson, and Schwartz 1991), a manually compiled catalog of metaphor.
CorMetworks on domains that are specific and concrete (e.g., the domain of finance, but notthat of actions).
CorMet?s discrimination is relatively coarse: It measures trends inselectional preferences across many documents, so common mappings are discernible.CorMet considers the selectional preferences only of verbs, on the theory that they aregenerally more selectively restrictive than nouns or adjectives.It is worth noting that WordNet, CorMet?s primary knowledge source, implicitlyencodes some of the metaphors CorMet is intended to find; Peters and Peters (2000) useWordNet to find many artifact/cognition metaphors.
Also, WordNet enumerates somemetaphorical senses of some verbs.
CorMet does not use any of WordNet?s informationabout verbs and ignores regularities in the distribution of noun homonyms that couldbe used to find some metaphors.The article is organized as follows: Section 2 describes the mechanisms by whichconventional metaphors are detected.
Section 3 walks through CorMet?s process intwo examples.
Section 4 describes how the system?s performance is evaluated againstthe Master Metaphor List (Lakoff, Espenson, and Schwartz 1991), and Section 5 coversselect related work.2.
The Metaphor Extraction Engine2.1 Searching the Net for Domain CorporaIdeally, CorMet could draw on a large quantity of manually vetted, highly represen-tative domain-specific documents.
The precompiled corpora available on-line (Kucera1992; Marcus, Santorini, and Marcinkiewicz 1993) do not span enough subjects.
Otheron-line data sources include the Internet?s hierarchically structured indices, such asYahoo?s ontology (www.yahoo.com) and Google?s (www.google.com).
Each index en-try contains a small number of high-quality links to relevant Web pages, but this is nothelpful, because CorMet requires many documents, and those documents need not beof more than moderate quality.
Searching the Internet for domain-specific text seemsto be the only way to obtain sufficiently large, diverse corpora.CorMet obtains documents by submitting queries to the Google search engine.There are two types of queries: one to fetch any domain-specific documents and an-25Mason CorMetother to fetch domain-specific documents that contain a particular verb.
The first kindof query consists of a conjunction of from two to five randomly selected domain key-words.
Domain keywords are words characteristic of a domain, supplied by the useras an input.
For the FINANCE domain, a reasonable set of keywords is stocks, bonds,NASDAQ, Dow, investment, finance.
Each query incorporates only a few keywords inorder to maximize the number of distinct possible queries.Queries for domain-specific documents containing a particular verb are composedof a conjunction of domain-specific terms and a disjunction of forms of the verb thatare more likely to be verbs than other parts of speech.
For the verb attack, for in-stance, acceptable forms are attacked and attacking, but not attack and attacks, which aremore likely to be nouns.
The syntactic categories in which a word form appears aredetermined by reference to WordNet.Some queries for the verb attack in the FINANCE domain are:1.
(attacked OR attacking) AND (bonds AND Dow AND investment)2.
(attacked OR attacking) AND (NASDAQ AND investment AND finance)3.
(attacked OR attacking) AND (stocks AND bonds AND NASDAQ)4.
(attacked OR attacking) AND (stocks AND NASDAQ AND Dow)Queries return links to up to 10,000 documents, of which CorMet fetches and analyzesno more than 3,000.
In the 13 domains studied, about 75% of these documents arerelevant to the domain of interest (as measured through a randomly chosen, hand-evaluated sample of 100 documents per domain), so the noise is substantial.
Thedocuments are processed to remove embedded scripts and HTML tags.The mined documents are parsed with the apple pie parser (Sekine and Grishman1995).
Case frames are extracted from parsed sentences using templates; for instance,(S (NP & OBJ) (VP (were | was | got | get) (VP WORDFORM-PASSIVE)) is used to extractroles for passive, agentless sentences (where WORDFORM-PASSIVE is replaced by apassive form of the verb under analysis).2.2 Finding Characteristic PredicatesLearning the selectional preferences for a verb in a domain is expensive in terms oftime, so it is useful to find a small set of important verbs in each domain.
CorMetseeks information about verbs typical of a domain, because these verbs are morelikely to figure in metaphors in which that domain is the metaphor?s source.
Besiege,for instance, is characteristic of the MILITARY domain and appears in many instancesof the MILITARY ?
MEDICINE mapping, such as The antigens besieged the virus.To find domain-characteristic verbs, CorMet dynamically obtains a large sampleof domain-relevant documents, decomposes them into a bag-of-words representation,stems the words with an implementation of the Porter (1980) stemmer, and finds theratio of occurrences of each word stem to the total number of stems in the domaincorpus.
The frequency of each stem in the corpus is compared to its frequency ingeneral English (as recorded in an English-language frequency dictionary [Kilgarriff2003]).The 400 verb stems with the highest relative frequency (computed as a ratio of thestem?s frequency in the domain to its frequency in the English frequency dictionary) areconsidered characteristic.
CorMet treats any word form that may be a verb (accordingto WordNet) as though it is a verb, which biases CorMet toward verbs with commonnominal homonyms.
Word stems that have high relative frequency in more than one26Computational Linguistics Volume 30, Number 1Table 1Characteristic stems for LAB and FINANCE domains.Rank LAB FINANCE1 oxidiz amortiz2 sulfat arbitrag3 fluorin labor4 vapor overvalu5 titrat outsourc6 adsorb escrow7 electropl repurchas8 valenc refinanc9 atomiz forecast10 anneal invest11 sinter discount12 substitu stock13 compound certify14 hydrat bank15 frit credit16 ionize yield17 deactiv bond18 intermix rate19 halogen reinvest20 solubl leveragdomain, like e-mail and download, are eliminated on the suspicion that they are morecharacteristic of documents on the Internet in general than of a substantive domain.Table 1 lists the 20 highest-scoring stems in the LAB and FINANCE domains.2.3 Selectional Preference LearningThere are three constraints on CorMet?s selectional-preference-learning algorithm.
First,it must tolerate noise, because complex sentences are often misparsed, and the caseframe extractor is error prone.
Second, it should be able to work around WordNet?slacunae.
Finally, there should be a reasonable metric for comparing the similarity be-tween selectional preferences.CorMet first uses the selectional-preference-learning algorithm described in Resnik(1993), then clustering over the results.
Resnik?s algorithm takes a set of words ob-served in a case slot (e.g., the subject of pour or the indirect object of give) and findsthe WordNet nodes that best characterize the selectional preferences of that slot.
(Notethat WordNet nodes are treated as categories subcategorizing their descendants.)
Acase slot has a preference for a WordNet node to the extent that that node, or oneof its descendants, is more likely to appear in that case slot than it is to appear atrandom.An overall measure of the choosiness of a case slot is selectional-preferencestrength, SR(p), defined as the relative entropy of the posterior probability P(c|p) andthe prior probability P(c) (where P(c) is the a priori probability of the appearance of aWordNet node c, or one of its descendants, and P(c|p) is the probability of that nodeor one of its descendants appearing in a case slot p.) Recall that the relative entropy oftwo distributions X and Y, D(X||Y), is the inefficiency incurred by using an encodingoptimal for Y to encode X.SR(p) = D(P(c|p)||P(c)) =?cP(c|p) log P(c|p)P(c)27Mason CorMetThe degree to which a case slot selects for a particular node is measured by se-lectional association.
In effect, the selectional associations divide up the selectionalpreference strength for a case slot among that slot?s possible fillers.
Selectional associ-ation is defined as?R(p, c) =1SR(p)P(c|p) log P(c|p)P(c)To compute ?R(p, c), what is needed is a distribution over word classes, but whatis observed in the corpus is a distribution over word forms.
Resnik?s algorithm worksaround this problem by approximating a word class distribution from the word formdistribution.
For each word form observed filling a case slot, credit is divided evenlyamong all of that word form?s possible senses (and their ancestors in WordNet).
Al-though Resnik?s algorithm makes no explicit attempt at sense disambiguation, greateractivation tends to accumulate in those nodes that best characterize a predicate?s se-lectional preferences.CorMet uses Resnik?s algorithm to learn domain-specific selection preferences.
Itoften finds different selectional preferences for predicates whose preferences should,intuitively, be the same.
In the MILITARY domain, the object of assault selects stronglyfor fortification but not social group, whereas the selectional preferences for the objectof attack are the opposite.
Taking the cosine of the selectional preferences of these twocase slots (one of many possible similarity metrics) gives a surprisingly low score.
Inorder to facilitate more accurate judgments of selectional-preference similarity, CorMetfinds clusters of WordNet nodes that, although not as accurate, allow more meaningfulcomparisons of selectional preferences.Clusters are built using the nearest-neighbor clustering algorithm (Jain, Murty, andFlynn 1999).
A predicate?s selectional preferences are represented as vectors whose nthelement represents the selectional association of the nth WordNet node for that predi-cate.
The similarity function used is the dot product of the two selectional-preferencevectors.
Empirically, the level of granularity obtained by running nearest-neighborclustering twice (i.e., clustering over the sets of nodes constituting selectional pref-erences, then clustering over the clusters) produces the most conceptually coherentclusters.There are typically fewer than 100 second-order clusters (i.e., clusters of clusters)per domain.
In the LAB domain there are 54 second-order clusters, and in the FINANCEdomain there are 67.
The time complexity of searching for metaphorical interconceptmappings between two domains is proportional to the number of pairs of salientdomain objects, so it is more efficient to search over pairs of salient clusters than overthe more numerous individual salient nodes.Table 2 shows a MILITARY cluster.
These clusters are helpful for finding verbswith similar, but not identical, selectional preferences.
Although attack, for instance,does not select for fortification, it does select for other elements of fortification?s cluster,such as building and defensive structure.The fundamental limitation of WordNet with respect to selectional-preferencelearning is that it fails to exhaust all possible lexical relationships.
WordNet can hardlybe blamed: The task of recording all possible relationships between all English words isprohibitively large, if not infinite.
Nevertheless, there are many words that intuitivelyshould have a common parent but do not.
For instance, liquid body substance and watershould both be hyponyms of liquid, but in WordNet their shallowest common ancestoris substance.
One of the descendants of substance is solid, so there is no single node thatrepresents all liquids.Li and Abe (1998) describe another method of corpus-driven selectional-preferencelearning that finds a tree cut of WordNet for each case slot.
A tree cut is a set of28Computational Linguistics Volume 30, Number 1Table 2The elements of a cluster of WordNet nodes characteristic of the MILITARY domain.penal institution-1 fortification-1correctional institution-1 defensive structure-1institution-2 housing-1structure-1 room-1establishment-4 prison-1building-1 tower-1area-3nodes that specifies a partition of the ontology?s leaf nodes, where a node stands forall the leaf nodes descended from it.
The method chooses among possible tree cutsaccording to minimum-description-length criteria.
The description length of a tree cutrepresentation is the sum of the size of the tree cut itself (i.e., the minimum number ofnodes specifying the partition) and the space required for representing the observeddata with that tree cut.
For CorMet?s purposes, the problem with this approach is thatit is difficult to find clusters of (possibly hypernymically related) nodes representing aselectional preference using its results (because the tree cut includes exactly one nodeon each path from each leaf node to the root).
There are similar objections to similarapproaches such as that of Carroll and McCarthy (2000).2.4 PolarityPolarity is a measure of the directionality and magnitude of structure transfer betweentwo concepts or two domains.
Nonzero polarity exists when language characteristicof a concept from one domain is used in a different domain of a different concept.The kind of characteristic language CorMet can detect is limited to verbal selectionalpreferences.Say CorMet is searching for a mapping between the concepts liquids (characteristicof the LAB domain) and assets (characteristic of the FINANCE domain), as illustratedin Figure 1.
There are verbs in LAB that strongly select for liquids, such as pour, flow,and freeze.
In FINANCE, these verbs select for assets.
In FINANCE there are verbs thatstrongly select for assets such as spend, invest, and tax.
In the LAB domain, these verbsselect for nothing in particular.
This suggests that liquid is the source concept and assetis the target concept, which implies that LAB and FINANCE are the source and targetdomains, respectively.
CorMet computes the overall polarity between two domains (asopposed to between two concepts) by summing over the polarity between each pairof high-salience concepts from the two domains of interest.Interconcept polarity is defined as follows: Let ?
be the set of case slots in domainX with the strongest selectional preference for the node cluster A.
Let ?
be the set ofcase slots in domain Y with the strongest selectional preferences for the node clusterB.
The degree of structure flow from A in X to B in Y is computed as the degreeto which the predicates ?
select for the nodes B in Y, or selection strength(Y ,?,B).Structure flow in the opposite direction is selection strength(X,?, A).
The definitionof selection strength(Domain, case slots, node cluster) is the average of the selectional-preference strengths of the predicates in case slots for the nodes in node cluster inDomain.
The polarity for ?
and ?
is the difference in the two quantities.
If the po-larity is near zero, there is not much structure flow and no evidence for a metaphoricmapping.In some cases a difference in selectional preferences between domains does notindicate the presence of a metaphor.
To take a fictitious but illustrative example, say29Mason CorMetFigure 1Asymmetric structure transfer between LAB and FINANCE.
Predicates from LAB that select forliquids are transferred to FINANCE and select for money.
On the other hand, predicates fromFINANCE that select for money are transferred to LAB and do not select for liquids.that in the LAB domain the subject of sit has a preference for chemists whereas in theFINANCE domain it has a preference for investment bankers.
The difference in selec-tional preferences is caused by the fact that chemists are the kind of person more likelyto appear in LAB documents and investment bankers in FINANCE ones.
Instances likethis are easy to filter out because their polarity is zero.A verb is treated as characteristic of a domain X if it is at least twice as frequentin the domain corpus as it is in general English and it is at least one and a half timesas frequent in domain X as in the contrasting domain Y (these ratios were chosenempirically).
Pour, for instance, occurs three times as often in FINANCE and twenty-three times as often in LAB as it does in general English.
Since it is nearly eighttimes as frequent in LAB as in FINANCE, it is considered characteristic of the former.This heuristic resolves the confusion than can be caused by the ubiquity of certainconventional metaphors?the high density of metaphorical uses of pour in FINANCEcould otherwise make it seem as though pour is characteristic of that domain.A verb with weak selectional preferences (e.g., exist) is a bad choice for a char-acteristic predicate even if it occurs disproportionately often in a domain.
Highlyselective verbs are more useful because violations of their selectional preferences aremore informative.
For this reason a predicate?s salience to a domain is defined as itsselectional-preference strength times the ratio of its frequency in the domain to itsfrequency in English.Literal and metaphorical selectional preferences may coexist in the same domain.Consider the selectional preferences of pour in the chemical and financial domains.
Inthe LAB domain, pour is mostly used literally: People pour liquids.
There are occasional30Computational Linguistics Volume 30, Number 1metaphorical uses (e.g., Funding is pouring into basic proteomics research), but the literalsense is more common.
In FINANCE, pour is mostly used metaphorically, althoughthere are occasionally literal uses (e.g., Today oil poured into the new Turkmenistan pipeline).Algorithms 1?3 show pseudocode for finding metaphoric mappings between con-cepts.Algorithm 1: Find Inter Concept Mappings(domain1, domain2)comment: Find mappings from concepts in domain1 to concepts in domain2 or viceversaDomain 1 Clusters ?
Get Best Clusters(domain1)Domain 2 Clusters ?
Get Best Clusters(domain2)for each Concept 1 ?
Domain 1 Clustersdo??????????????????????
?for each Concept 2 ?
Domain 2 Clustersdo??????????????????
?Polarity Score ?
Detect Inter Concept Mapping(Concept 1, Concept 2,domain1, domain2)if Polarity score > NOISE THRESHOLDthen output mapping(Concept 1 ?
Concept 2)if Polarity score < ?NOISE THRESHOLDthen output mapping(Concept 2 ?
Concept 1)Algorithm 2: Detect Inter Concept Mapping(Concept 1, Concept 2, domain1, do-main2)polarity from 1 to 2 ?
Inter Concept Polarity(Concept 1, Concept 2, domain1,domain2)polarity from 2 to 1 ?
Inter Concept Polarity(Concept 2, Concept 1, domain2,domain1)if absolute value(polarity from 1 to 2 ?
polarity from 2 to 1) < C1then return (0);if polarity from 1 to 2 > C2 and polarity from 2 to 1 > C2then return (0);return (polarity from 1 to 2 ?
polarity from 2 to 1)Algorithm 3: Inter Concept Polarity(Concept A, Concept B, domain A, domain B)polarity ?
0domain A predicates ?
get predicates selecting for concept(Concept A,domain A)for each Predicate from A ?
domain A predicatesdo {polarity ?
polarity + Selection strength(Predicate from A, Concept B,domain B)return (polarity)31Mason CorMet2.5 SystematicityAccording to the thematic-relation hypothesis (Grubner 1976), many domains are con-ceived of in terms of physical objects moving along paths between locations in space.In the money domain, assets are mapped to objects and asset holders are mapped tolocations.
In the idea domain, ideas are mapped to objects, minds are mapped to loca-tions, and communications are mapped to paths.
Axioms of inference from the targetdomain usually become available for reasoning about the source domain, unless thereis an aspect of the source domain that specifically contradicts them.
For instance, inthe domain of material objects, a thing moved from point X to point Y is no longer atX, but in the idea domain, it exists at both locations.Thematically related metaphors may consistently co-occur in the same sentences.For example, the metaphors LIQUID ?
MONEY and CONTAINERS ?
INSTITUTIONSoften co-occur, as in the sentence Capital flowed into the new company.
Conversely, co-occurring metaphors are often components of a single metaphorical conceptualization.A metaphorical mapping is therefore more credible when it is a component of a systemof mappings.In CorMet, systematicity measures a metaphorical mapping?s tendency to co-occurwith other mappings.
The systematicity score for a mapping X is defined as the numberof strong, distinct mappings co-occurring with X.
This measure goes only a little waytoward capturing the extent to which a metaphor exhibits the structure describedin the thematic-relations hypothesis, but extending CorMet to find the entities thatcorrespond to objects, locations, and paths is beyond the scope of this article.2.6 Confidence RatingCorMet computes a confidence measure for each metaphor it discovers.
Confidenceis a function of three things.
The more verbs mediating a metaphor (as attack andassault mediate ENEMY ?
DISEASE in The antigen attacked the virus and Chemotherapyassaults the tumor), the more credible it is.
Strongly unidirectional structure flow fromsource domain to target makes a mapping more credible.
Finally, a mapping is morelikely to be correct if it systematically co-occurs with other mappings.
The confidencemeasure should not be interpreted as a probability of correctness: The data available forcalibrating such a distribution are inadequate.
The weights of each factor, empiricallyassigned plausible values, are given in Table 3.The confidence measure is intended to wrap all the available evidence about ametaphor?s credibility into one number.
A principled way of doing this is desirable,but unfortunately there are not enough data to make meaningful use of machine-learning techniques to find the best set of components and weights.
There is substantialarbitrariness in the confidence rating: The components used and the weights they areTable 3Factors used in evaluating a mapping M and their weights.Component Weight|supporting predicates(M)|max num of support preds in domain0.25polarity(M)max polarity in domain0.5|co occurring mappings(M)|max number of cooccurring mappings0.2532Computational Linguistics Volume 30, Number 1Table 4Characteristic keywords for LAB and FINANCE domains.LAB beaker experiment cylinder chemical precipitate mixturereaction valence molarity pressureFINANCE money stocks bonds equity trading inflation arbitragecapital investment marketassigned could easily be different and are best considered guesses that give reasonableresults.3.
Two ExamplesThis section provides a walk-through of the derivation and analysis of the conceptmapping LIQUID ?
MONEY and components of the interconcept mapping WAR ?MEDICINE.
In the interests of brevity only representative samples of CorMet?s dataare shown.
See Mason (2002) for a more detailed account.3.1 LIQUID ?
MONEYCorMet?s inputs are two domain sets of characteristic keywords for each domain (Ta-ble 4).
The keywords must characterize a cluster in the space of Internet documents,but CorMet is relatively insensitive to the particular keywords.It is difficult to find keywords characterizing a cluster centering on money alone,so keywords for a more general domain, FINANCE, are provided.
It is also difficultto characterize a cluster of documents mostly about liquids.
Chemical-engineeringarticles and hydrographic encyclopedias tend to pertain to the highly technical aspectsof liquids instead of their everyday behavior.
Documents related to laboratory workare targeted on the theory that most references to liquids in a corpus dedicated to themanipulation and transformation of different states of matter are likely to be literal andwill not necessarily be highly technical.
Tables 5 and 6 show the top 20 characteristicverbs for LAB and FINANCE, respectively.CorMet finds the selectional preferences of all of the characteristic predicates?
caseslots.
A sample of the selectional preferences of the top 20 verbs in LAB and FINANCEare shown in Tables 7 and 8, respectively.
The leftmost columns of these two tables havethe (stemmed form of the) characteristic verb and the thematic role characterized.
Theright-hand sides have clusters of characteristic nodes.
The numbers associated withthe nodes are the bits of uncertainty about the identity of a word x resolved by thefact that x fills the given case slot, or P(x ?
N)?
P(x ?
N|case slot(x)) (where x ?
Nis read as x is N or a hyponym of N).All of the 400 possible mappings between the top 20 concepts (clusters) from thetwo domains are examined.
Each possible mapping is evaluated in terms of polarity,the number of frames instantiating the mapping, and the systematic co-occurrence ofthat mapping with different, highly salient mappings.
The best mappings for LAB ?FINANCE are shown in Table 9.Mappings are expressed in abbreviated form for clarity, with only the most rec-ognizable (if not necessarily the most salient) node of each concept displayed.
Theforemost mapping characterizes money in terms of liquid, the mapping for which thetwo domains were selected.
The second represents a somewhat less intuitive mappingfrom liquids to institutions.
This metaphor is driven primarily by institutions?
capacity33Mason CorMetTable 5Characteristic verbs 1?20 of the LAB domain.Rank Stem Ratio of frequencies Frequency in domain Frequency in English1 oxidiz 3,073.608 0.0003 1.0e?072 sulfat 2,301.591 0.0003 1.3e?073 fluorin 1,452.467 0.0001 1.0e?074 vapor 1,325.237 0.0007 5.2e?075 titrat 831.007 0.0006 8.3e?076 adsorb 433.721 5.6e?05 1.2e?077 electropl 392.986 3.1e?05 7.9e?088 valenc 349.522 0.0004 1.4e?069 atomiz 324.696 1.9e?05 5.9e?0810 anneal 312.406 8.1e?05 2.5e?0711 sinter 264.322 3.6e?05 1.3e?0712 substitu 251.511 3.7e?05 1.4e?0713 compound 99.632 0.002 2.0e?0514 hydrat 238.017 0.0001 6.5e?0715 frit 237.08 1.6e?05 6.9e?0816 ionize 221.372 9.2e?05 4.1e?0717 deactiv 207.629 1.4e?05 6.9e?0818 intermix 84.18 5.0e?06 5.9e?0819 halogen 195.701 0.0001 6.9e?0720 solubl 192.204 0.0007 4.1e?06Table 6Characteristic verbs 1?20 of the FINANCE domain.Rank Stem Ratio of frequencies Freqency in domain Frequency in English1 amortiz 807.531 5.6e?05 6.9e?082 arbitrag 305.836 0.0006 2.0e?063 labor 302.797 0.0004 1.6e-064 overvalu 296.945 4.7e?05 1.5e?075 outsourc 260.625 2.8e?05 1.0e?076 escrow 248.192 2.9e?05 1.1e?077 repurchas 241.309 9.4e?05 3.8e?078 refinanc 213.369 3.4e?05 1.5e?079 forecast 27.007 0.0004 1.4e?0510 invest 72.604 0.0019 2.7e?0511 discount 22.59 0.0005 2.2e?0512 stock 70.172 0.0067 9.5e?0513 certify 21.08 5.7e-05 2.7e?0614 bank 20.624 0.0045 0.000215 credit 20.432 0.0016 7.9e?0516 yield 56.144 0.001 1.8e?0517 bond 122.467 0.0045 3.7e?0518 rate 17.563 0.0055 0.000319 reinvest 104.197 0.0001 1.1e?0620 leverag 100.576 0.0002 2.2e?0634Computational Linguistics Volume 30, Number 1Table 7Sample selectional preferences for LAB verbs.substance-1 0.0116vapor obj liquid-1 0.0478fluid-1 0.0473metallic element-1 0.0217anneal with substance-1 0.0101chemical element-1 0.0112substance-1 0.0123compound subj compound-2 0.036organic compound-1 0.0431matter-3 0.0145adsorb obj substance-1 0.014physical object-1 0.0087hydrat subj substance-1 0.0181compound-2 0.0401Table 8Sample selectional preferences for FINANCE verbs.income-1 0.0118financial gain-1 0.0114security-8 0.0069currency-1 0.034sum-1 0.0136invest obj transferred property-1 0.0036fund-1 0.008asset-1 0.1183gain-4 0.0113medium of exchange-1 0.0415money-1 0.0375cost-1 0.0269financial loss-1 0.0263discount obj transferred property-1 0.0237loss-2 0.0262outgo-1 0.0269cost-1 0.0211financial loss-1 0.0206credit subj transferred property-1 0.0182loss-2 0.0205outgo-1 0.0211Table 9Mappings LAB ?
FINANCE.Mapping Frames Polarity Systematicity Final scoreliquid-1 ?
income-1 61 11.8 2 .56liquid-1 ?
institution-1 59 3.83 2 .55container-1 ?
institution-1 11 3.16 1 .35liquid-1 ?
information-1 56 4.29 2 .5435Mason CorMetto dissolve.
Of course, this mapping is incorrect insofar as solids undergo dissolution,not liquids.
CorMet made this mistake because of faulty thematic-role identification;it frequently failed to distinguish between the different thematic roles played by thesubjects in sentences like The company dissolved and The acid dissolved the compound.
Thethird mapping characterizes communication as a liquid.
This was not the mapping theauthor had in mind when he chose the domains, but it is intuitively plausible: Onespeaks of information flowing as readily as of money flowing.
That this mapping ap-pears in a search not targeted to it reflects this metaphor?s strength.
It also illustrates asource of error in inferring the existence of conventional metaphors between domainsfrom the existence of interconcept mappings.
The fourth mapping is from contain-ers to organizations.
This mapping complements the first one: As liquids flow intocontainers, so money flows into organizations.
Another good mapping, not presenthere, is money flows into equities and investments.
CorMet misses this mapping because,at the level of concepts, money and equities are conflated.
This happens because theyare near relatives in the WordNet ontology and because there is very high overlapbetween the predicates selecting for them.Compare the mappings CorMet derived with the Master Metaphor List?s (Lakoff,Espenson, and Schwartz 1991) characterization of the MONEY IS A LIQUID metaphor:1.
Cash is a Liquid.
(a) liquid assets(b) currency(c) liquidating assets(d) My money is all dried up(e) He?s just sponging off you (absorbing cash)(f) He?s solvent/insolvent2.
Gain/Loss is Movement of a Liquid.
(a) cash flow(b) influx and outflux of money(c) Don?t pour your money down the drain3.
Money Which Cannot be Accessed is Frozen(a) frozen assets(b) price freeze4.
Control in Financial Situation is Control in Liquid(a) keep your head above water, financially(b) get in over your head(c) stay afloat(d) the business went under/sunk(e) drowning in debtsThe Master Metaphor List also describes INVESTMENTS ARE CONTAINERS FORMONEY, as exemplified in the following:1.
Put your money in bonds.2.
The bottom of the economy dropped out.36Computational Linguistics Volume 30, Number 1Table 10A sample of frames from FINANCE instantiating liquid ?
income.vb subj obj into from withdissolv stakespour investors cashpour investors cashpour profits marketpour Cash sharespour Earningspour stake brandpour cashpour flight money stockspour investors stockscool stockscool Reserve economyevapor profitevapor mortgagesevapor profit turnpump Reserve reservespump stocks themvapor stockvapor profitsmelt profit nothingmelt stocks3.
I?m down to my bottom dollar.4.
This is an airtight investment.CorMet has found mappings that can reasonably be construed as corresponding tothese metaphors.
Compare the mappings from the Master Metaphor List with framesmined by this system and identified as instantiating liquid?
income, shown in Table 10.It is important to note that although CorMet can list the case frames that have driventhe derivation of a particular high-level mapping, it is designed to discover high-level mappings, not interpret or even recognize particular instances of metaphoricallanguage.
Just as in the Master Metaphor List, there are frames in the CorMet listingin which money and equities are characterized as liquids, are moved as liquids (i.e.,pouring earnings and pumping reserves) and change state as liquids (i.e., meltingstocks, dissolving stakes, evaporating profits, frozen money).3.2 MILITARY ?
MEDICINEThis subsection describes the search for mappings between the MEDICINE and MIL-ITARY domains.
The domain keywords for MEDICINE and MILITARY are shown inTable 11.
The characteristic verbs of the MILITARY and MEDICINE domains are givenin Tables 12 and 13, respectively.
Their selectional preferences are given in Tables 14and 15, respectively.The highest-quality mappings between the MILITARY and MEDICINE domains areshown in Table 16.
This pair of domains produces more mappings than the the LABand FINANCE pair.
Many source concepts from the MILITARY domain are mappedto body parts.
The heterogeneity of the source concepts seems to be driven by theheterogeneity of possible military targets.
Similarly, many source concepts are mappedto drugs.
The case frames supporting this mapping suggest that this is because of37Mason CorMetTable 11Characteristic keywords for the MEDICINE and MILITARY domains.MEDICINE doctor surgeon hospital operate pharmaceutical medicinerecuperate organ tissue bacteria virus diagnose cancersickness nurse researchMILITARY army navy soldier battle war attack bombing destructioninfantry tactics siege invasion troops barracksTable 12Characteristic verbs for MILITARY.Rank Stem Ratio of frequencies Frequency in domain Frequency in English0 nuke 372.494 2.2e?05 5.9e?081 harbor 714.253 0.0004 6.9e?072 strafe 156.471 5.1e?05 3.2e?073 honor 626.577 0.0003 4.7e?074 combat 105.121 0.001 9.6e?065 torpedo 96.519 0.0002 2.0e?066 stonewal 382.93 3.0e?05 7.9e?087 bombard 54.602 0.0002 5.1e?068 skirmish 56.105 0.0001 2.4e?069 bomb 49.341 0.0019 3.9e?0510 favor 169.023 0.0001 6.5e?0711 envision 158.417 1.4e?05 8.9e?0812 attack 31.661 0.0034 0.000113 cannonad 117.742 1.0e?05 8.9e?0814 rearm 115.601 1.2e?05 1.0e?0715 sieg 107.732 0.0008 7.8e?0616 raid 20.817 0.0004 2.1e?0517 highlight 77.358 0.0014 1.9e?0518 enlist 74.138 0.0002 3.4e?0619 infest 17.725 1.3e?05 7.4e?07the heterogeneity of military aggressors (fortifications do not generally fall into thiscategory; this mapping is an error caused by the frame extractor?s frequent confusionof subject and object).
These mappings can be interpreted as indicating that thingsthat are attacked map to body parts and things that attack map to drugs.The mapping fortification?
illness represents the mapping of targetable strongholdsto disease.
Illnesses are conceived of as fortifications besieged by treatment.Compare this with the Master Metaphor List?s characterization of TREATING ILL-NESS IS FIGHTING A WAR:1.
The Disease is an Enemy.2.
The Body is a Battleground.
(a) The body is not immune to invasion.
(b) The disease infiltrates your body and takes over.3.
Infection is an Attack by the Disease.
(a) His body was under siege by AIDS.
(b) He was attacked by an unknown virus.
(c) The virus began an attack on the organ systems.38Computational Linguistics Volume 30, Number 1Table 13Characteristic verbs for MEDICINE.Rank Stem Ratio of frequencies Frequency in domain Frequency in English1 immuniz 304.704 0.0001 4.0e?072 diaper 110.023 2.6e?05 2.3e?073 detoxify 106.181 2.0e?05 1.8e?074 oxidiz 104.006 1.1e?05 1.0e?075 pasteur 102.149 3.5e?05 3.4e?076 palpat 89.38 1.4e?05 1.5e?077 misdiagnos 87.394 7.8e?06 8.9e?088 metastas 87.049 4.0e?05 4.5e?079 expector 86.826 8.6e?06 9.9e?0810 implant 85.263 0.0001 2.3e?0611 decoct 82.996 6.6e?06 7.9e?0812 vaccin 81.157 0.0007 8.8e?0613 transplant 78.7 0.0005 7.1e?0614 labor 77.016 0.0001 1.6e?0615 infect 69.575 0.0003 5.4e?0616 deactiv 67.126 4.6e?06 6.9e?0817 detox 63.417 7.6e?06 1.1e?0718 recuper 62.588 7.3e?05 1.1e?0619 heal 61.753 0.0005 8.9e?0620 clot 58.416 7.4e?05 1.2e?06Table 14Selectional preferences for MILITARY verbs.social group-1 0.005combat subj body-3 0.0123gathering-1 0.0053combat obj military unit-1 0.0156social group-1 0.01unit-3 0.0135enlist subj military unit-1 0.0603social group-1 0.0475military unit-1 0.0164social group-1 0.0131military unit-1 0.0368military unit-1 0.0397muster subj company-6 0.0101gathering-1 0.0013unit-3 0.0196social gathering-1 0.0049force-4 0.0052district-1 0.0046seat-5 0.0046region-3 0.0022bomb subj administrative district-1 0.0051country-1 0.0021capital-3 0.0046city-2 0.0073national capital-1 0.005639Mason CorMetTable 15Selectional preferences for MEDICINE verbs.descendant-1 0.0246child-2 0.0198immuniz subj relative-1 0.0137offspring-1 0.0193child-4 0.0246oxidiz subj food-1 0.0513substance-1 0.0158organ-1 0.0204gland-1 0.0303implant subj body part-1 0.0238tissue-1 0.0151part-7 0.0225Table 16Mappings MILITARY ?
MEDICINE.Mapping Frames Polarity Systematicity Final scoremilitary unit-1 ?
body part-1 285 65.55 33 0.95fortification-1 ?
body part-1 298 55.12 33 0.88vehicle-1 ?
body part-1 238 35.2 32 0.67military action-1 ?
body part-1 207 35 25 0.6region-3 ?
body part-1 57 30.9 5 0.31skilled worker-1 ?
body part-1 127 17.3 11 0.31military unit-1 ?
drug-1 84 51.77 28 0.64vehicle-1 ?
drug-1 63 35.7 28 0.5military action-1 ?
drug-1 71 30.91 27 0.47fortification-1 ?
drug-1 67 24.64 22 0.38weaponry-1 ?
drug-1 58 10.8 24 0.28military action-1 ?
medical care-1 71 28.21 20 0.4fortification-1 ?
medical care-1 78 16.37 20 0.32weaponry-1 ?
medical care-1 48 9.64 20 0.24fortification-1 ?
illness-1 243 .21 38 .454.
Medicine is a Weapon.
(a) The so-called cure is no magic bullet.5.
Medical Procedures are Attacks by the Patient.
(a) The doctors tried to wipe out the infection.6.
The Immune System is a Defense.
(a) The body normally has its own defenses.7.
Winning the War is being Cured of the Disease.
(a) Beating measles takes patience.8.
Being Defeated is Dying.
(a) The patient finally gave up the battle.40Computational Linguistics Volume 30, Number 1Table 17Selected frames supporting {fortification, vehicle, military action, region, skilled worker} ?
body part.vb subj obj into from withattack system receptorsattack pain jointsattack immunosuppressants kidneybesieg flood abdomenbesieg scars thighdestroy organs bacteriadestroy Microtubules agentsdestroy gangliondestroy therapy tissuedestroy cancer bonedestroy virus liverdestroy internist stomachtarget organtarget vaccine intestinesCorMet?s results can reasonably be interpreted as matching all of the mappings fromthe Master Metaphor List except winning-is-a-cure and defeat-is-dying.
CorMet?s fail-ure to find this mapping is caused by the fact that win, lose, and their synonyms donot have high salience in the MILITARY domain, which may be a reflection of theubiquity of win and lose outside of that domain.Table 17 shows sample frames from which the body part ?
{fortification, vehicle,military action, region, skilled worker} mapping was derived.4.
Testing against the Master Metaphor ListThis section describes the evaluation of CorMet against a gold standard, specifically,by determining how many of the metaphors in a subset of the Master MetaphorList (Lakoff, Espenson, and Schwartz 1991) can be discovered by CorMet given acharacterization of the relevant source and target domains.
The final evaluation of thecorrespondence between the mappings CorMet discovers and the Master MetaphorList entry is necessarily done by hand.
This is a highly subjective method of evaluation;a formal, objective evaluation of correctness would be preferable, but at present nosuch metric is available.The Master Metaphor List is the basis for evaluation because it is composed ofmanually verified metaphors common in English.
The test set is restricted to thoseelements of the Master Metaphor List with concrete source and target domains.
Thisrequirement excludes many important conventional metaphors, such as EVENTS AREACTIONS.
About a fifth of the Master Metaphor List meets this constraint.
This fractionis surprisingly small: It turns out that the bulk of the Master Metaphor List consistsof subtle refinements of a few highly abstract metaphors.
The concept pairs and cor-responding domain pairs for the target metaphors in the Master Metaphor List aregiven in Table 18.A mapping discovered by CorMet is considered correct if submappings specifiedin the Master Metaphor List are nearly all present with high salience and incorrectsubmappings are present with comparatively low salience.
The mappings discoveredthat best represent the targeted metaphors are shown in Table 19.Some of these test cases are marked successes.
For instance, ECONOMIC HARMIS PHYSICAL INJURY seems to be captured by the mapping from the loss-3 cluster to41Mason CorMetTable 18Master Metaphor List mappings and the domain pairs in which they are sought.Master Metaphor List mapping DomainsTheories are Fortifications Theory & ArchitectureEmotion is a Fluid Emotion & LabPeople are Containers for Emotions Emotion & LabLove is War Love & MilitaryEffects of Humor are Injuries Humor & MilitaryTreating Illness is Fighting a War Medicine & MilitaryLove is a Journey Love & JourneyEconomic Harm is Physical Injury Finance & MedicineMachines are People Mechanical & BodyMoney is a Liquid Finance & LabInvestments are Containers for Money Finance & LabBodies are Buildings Body & ArchitectureSociety is a Body Society & BodyTable 19Best mappings for domain pairs.Master Metaphor List mapping Empirical mapping ScoreFortifications ?
Theories none 0Fluid ?
Emotion liquid-1 ?
feeling-1 .25Containers for Emotions ?
People container-1 ?
person-1 .13War ?
Love feeling-1 ?
military unit-1 .34Injuries ?
Effects of Humor weapon-1 ?
joke-1 .18Fighting a War ?
Treating Illness military action-1 ?
medical care-1 .4Journey ?
Love travel-1 ?
feeling-1 .17Physical Injury ?
Economic Harm harm-1 ?
loss-3 .20Machines ?
People none 0Liquid ?
Money liquid-1 ?
income-1 .56Containers for Money ?
Investments container-1 ?
institution-1 .35Buildings ?
Bodies none 0Body ?
Society body part-1 ?
organization-1 .14the harm-1 cluster.
CorMet found reasonable mappings in 10 of 13 cases attempted.This implies 77% accuracy, although in light of the small test and the subjectivity ofjudgment, this number must not be taken too seriously.Some test cases were disappointing.
CorMet found no mapping between THE-ORY and ARCHITECTURE.
This seems to be an artifact of the low-quality corporaobtained for these domains.
The documents intended to be relevant to architecturewere often about zoning or building policy, not the structure of buildings.
For theory,many documents were calls for papers or about university department policy.
It isunsurprising that there are no particular mappings between two sets of miscellaneousadministrative and policy documents.
The weakness of the ARCHITECTURE corpusalso prevented CorMet from discovering any BODY ?
ARCHITECTURE mappings.Accuracy could be improved by refining the process by which domain-specific cor-pora are obtained to eliminate administrative documents or by requiring documentsto have a higher density of domain-relevant terms.Is it meaningful when CorMet finds a mapping, or will it find a mapping betweenany pair of domains?
To answer this question, CorMet was made to search for42Computational Linguistics Volume 30, Number 1Table 20Arbitrarily selected domains and the mapping strengths between them.Domain 1 Domain2 PolarityMedicine Plants 0Military Society 0Medicine Society 0Finance Body 0Lab Theory 0Society Journey 0mappings between randomly selected pairs of domains.
Table 20 lists a set of arbi-trarily selected domain pairs and the strength of the polarization between them.
In allcases, the polarization is zero.
This can be interpreted as an encouraging lack of falsepositives.
Another perspective is that CorMet should have found mappings betweensome of these pairs, such as MEDICINE and SOCIETY, on the theory that societies canbe said to sicken, die, or heal.
Although this is certainly a valid conventional metaphor,it seems to be less prominent than those metaphors that CorMet did discover.5.
Related WorkTwo of the most broadly effective computational models of metaphor are Fass (1991)and Martin (1990), in both of which metaphors are detected through selectional-preference violations and interpreted using an ontology.
They are distinguished fromCorMet in that they work on both novel and conventional metaphors and rely ondeclarative hand-coded knowledge bases.Fass (1991) describes Met*, a system for interpreting nonliteral language that buildson Wilks (1975) and Wilks (1978).
Met* discriminates among metonymic, metaphorical,literal, and anomalous language.
It is a component of collative semantics, a semanticsfor natural language processing that has been implemented in the program meta5(Fass, 1986, 1987, 1988).
Met* treats metonymy as a way of referring to one thingby means of another and metaphor as a way of revealing an interesting relationshipbetween two entities.In Met*, a verb?s selectional preferences are represented as a vector of types.
Theverb drink?s preference for an animal subject and a liquid object are represented as(animal, drink, liquid).
Metaphorical interpretations are made by finding a sense vectorin Met*?s knowledge base whose elements are hypernyms of both the preferred argu-ment types and the actual arguments.
For example, the car drinks gasoline maps to thevector (car, drink, gasoline).
But car is not a hypernym of animal, so Met* searches for ametaphorical interpretation, coming up with (thing, use, energy source).Martin (1990) describes the Metaphor Interpretation, Denotation, and AcquisitionSystem (MIDAS), a computational model of metaphor interpretation.
MIDAS has beenintegrated with the Unix Consultant (UC), a program that answers English questionsabout using Unix.
UC tries to find a literal answer to each question with which itis presented.
If violations of literal selectional preference make this impossible, UCcalls on MIDAS to search its hierarchical library of conventional metaphors for onethat explains the anomaly.
If no such metaphor is found, MIDAS tries to generalizea known conventional metaphor by abstracting its components to the most-specificsenses that encompass the question?s anomalous language.
MIDAS then records the43Mason CorMetmost concrete metaphor descended from the new, general metaphor that provides anexplanation for the query?s language.MIDAS is driven by the idea that novel metaphors are derived from known, exist-ing ones.
The hierarchical structure of conventional metaphor is a regularity not cap-tured by other computational approaches.
Although MIDAS can quickly understandnovel metaphors that are the descendants of metaphors in its memory, it cannot inter-pret compound metaphors or detect intermetaphor relationships besides inheritance.INVESTMENTS ?
CONTAINERS and MONEY ?
WATER, for instance, are clearly re-lated, but not in a way that MIDAS can represent.
Since not all novel metaphors aredescendants of common conventional metaphors, MIDAS?s coverage is limited.MetaBank (Martin 1994) is an empirically derived knowledge base of conventionalmetaphors designed for use in natural language applications.
MetaBank starts witha knowledge base of metaphors based on the Master Metaphor List.
MetaBank cansearch a corpus for one metaphor or scan a large corpus for any metaphorical content.The search for a target metaphor is accomplished by choosing a set of probe wordsassociated with that metaphor and finding sentences with those words, which are thenmanually sorted as literal, examples of the target metaphor, examples of a differentmetaphor, unsystematic homonyms, or something else.
MetaBank compiles statisticson the frequency of conventional metaphors and the usefulness of the probe words.MetaBank has been used to study container metaphors in a corpus of UNIX-relatede-mail and to study metaphor distributions in the Wall Street Journal.Peters and Peters (2000) mine WordNet for patterns of systematic polysemy byfinding pairs of WordNet nodes at a relatively high level in the ontology (but stillbelow the root nodes) whose descendants share a set of common word forms.
Thenodes publication and publisher, for instance, have paper, newspaper, and magazine ascommon descendants.
This is a metonymic relationship; the system can also capturemetaphoric relationships, as in the nodes supporting structure and theory, among whosecommon descendants are (for example) framework, foundation, and base.
Peters andPeters?
system found many metaphoric relationships between node pairs that weredescendants of the unique beginners artifact and cognition.Goatly (1997) describes a set of linguistic cues of metaphoricality beyondselectional-preference violations, such as metaphorically speaking and, surprisingly,literally.
These cues are generally ambiguous (except for metaphorically speaking) butcould usefully be incorporated into computational approaches to metaphor.6.
ConclusionCorMet embodies a method for semiautomatically finding metaphoric mappings be-tween concepts, which can then be used to infer conventionally metaphoric relation-ships between domains.
It can sometimes identify metaphoric language, if it manifestsas a common selectional-preference gradient between domains, but is far from beingable to recognize metaphoric language in general.
CorMet differs from other compu-tational approaches to metaphor in requiring no manually compiled knowledge basebesides WordNet.
It has successfully found some of the conventional metaphors onthe Master Metaphor List.CorMet uses gradients in selectional preferences learned from dynamically mined,domain-specific corpora to identify metaphoric mappings between concepts.
It is rea-sonably accurate despite the noisiness of many of its components.
CorMet demon-strates the viability of a computational, corpus-based approach to conventional meta-phor but requires more work before it can constitute a viable NLP tool.44Computational Linguistics Volume 30, Number 1ReferencesCarroll, J., and D. McCarthy.
2000.
Wordsense disambiguation using automaticallyacquired verbal preferences.
Computersand the Humanities, 34(1?2).Cho, See-Young.
1993.
Metaphor andcultural coherence.
In Proceedings of the27th Conference on Cross-Language Studiesand Contrastive Linguistics.Fass, Dan.
1986.
Collative semantics: Anapproach to coherence.
Memorandum inComputer and Cognitive ScienceMCCS-86-56, New Mexico StateUniversity, New Mexico.Fass, Dan.
1987.
Collative semantics: Anoverview of the current meta5 program.Memorandum in Computer andCognitive Science MCCS-87-112, NewMexico State University, NM.Fass, Dan.
1988.
Collative semantics: Asemantics for natural languageprocessing.
Memorandum in Computerand Cognitive Science MCCS-88-118, NewMexico State University, NM.Fass, Dan.
1991.
Met: A method fordiscriminating metonymy and metaphorby computer.
Computational Linguistics,17(1):49?90.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Goatly, Andrew.
1997.
The Language ofMetaphors.
Routledge, London.Gruber, Jeffrey.
1976.
Lexical Structures inSyntax and Semantics.
Amsterdam,North-Holland.Jain, Anil K., M. Narasimha Murty, andPatrick J. Flynn.
1999.
Data clustering: Areview.
ACM Computing Surveys,31(3):264?323.Kilgarriff, Adam.
2003.
BNC wordfrequency list.
Available online athttp://www.itri.brighton.ac.uk/Adam.Kilgarriff/bnc-readme.html.Kucera, Henry.
1992.
Brown corpus.
InS.
Shapiro, editor, Encyclopedia of ArtificialIntelligence, volume 1.
Wiley, New York,pages 128?130.Lakoff, George.
1993.
The contemporarytheory of metaphor.
In Andrew Ortony,editor, Metaphor and Thought.
CambridgeUniversity Press, Cambridge.Lakoff, George, Jane Espenson, andAlan Schwartz.
1991.
The mastermetaphor list.
Draft 2nd ed.
TechnicalReport, University of California atBerkeley.Lenat, Douglas.
1995.
Cyc: A large-scaleinvestment in knowledge infrastructure.In Communications of the ACM, 38:11.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDI principle.
Computational Linguistics,24(2):217?244.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.Martin, James.
1990.
A Computational Modelof Metaphor Interpretation.
Academic Press.Martin, James.
1994.
Metabank: Aknowledge base of metaphoric languageconventions.
Computational Intelligence,10(2):134?149.Mason, Zachary.
2002.
A Computational,Corpus-Based Metaphor Extraction System.Ph.D.
thesis, Brandeis University.Peters, Winn and Ivonne Peters.
2000.Lexicalised systematic polysemy inWordNet.
In Proceedings of the SecondInternational Conference on LanguageResources and Evaluation, Athens.Porter, Martin F. 1980.
An algorithm forsuffix stripping.
Program, 14(3):130?137.Resnik, Philip.
1993.
Selection andInformation: A Class Based Approach toLexical Relationships.
Ph.D. thesis,University of Pennsylvania.Sekine, Satoshi and Ralph Grishman.
1995.A corpus-based probabilistic grammarwith only two non-terminals.
InProceedings of the Fourth InternationalWorkshop on Parsing Technology, Prague,Czech Republic.Wilks, Yorick.
1975.
A preferential,pattern-seeking, semantics for naturallanguage inference.
Artificial Intelligence,6:53?74.Wilks, Yorick.
1978.
Making preferencesmore active.
Artificial Intelligence,11(3):197?223.
