Automatic Compensation for Parser Figure-of-Merit Flaws*Don Blaheta and Eugene Charniak{dpb,  ec}@cs ,  b rown,  eduDepartment ofComputer ScienceBox 1910 / 115 Waterman St.--4th floorBrown UniversityProvidence, RI 02912AbstractBest-first chart parsing utilises a figure ofmerit (FOM) to efficiently guide a parse byfirst attending to those edges judged better.In the past it has usually been static; thispaper will show that with some extra infor-mation, a parser can compensate for FOMflaws which otherwise slow it down.
Our re-sults are faster than the prior best by a fac-tor of 2.5; and the speedup is won with nosignificant decrease in parser accuracy.1 IntroductionSentence parsing is a task which is tra-ditionMly rather computationally intensive.The best known practical methods are stillroughly cubic in the length of the sentence--less than ideM when deMing with nontriviMsentences of 30 or 40 words in length, as fre-quently found in the Penn Wall Street Jour-nal treebank corpus.Fortunately, there is now a body of litera-ture on methods to reduce parse time so thatthe exhaustive limit is never eached in prac-tice.
1 For much of the work, the chosen ve-hicle is chart parsing.
In this technique, theparser begins at the word or tag level anduses the rules of a context-free grammar tobuild larger and larger constituents.
Com-pleted constituents are stored in the cellsof a chart according to their location and* This research was funded in part by NSF GrantIRI-9319516 and ONR Grant N0014-96-1-0549.IAn exhaustive parse always "overgenerates" be-cause the grammar contains thousands of extremelyrarely applied rules; these are (correctly) rejectedeven by the simplest parsers, eventuMly, but it wouldbe better to avoid them entirely.length.
Incomplete constituents ("edges")are stored in an agenda.
The exhaustionof the agenda definitively marks the comple-tion of the parsing algorithm, but the parseneedn't ake that long; Mready in the earlywork on chart parsing, (Kay, 1970) suggeststhat by ordering the agenda one can finda parse without resorting to an exhaustivesearch.
The introduction of statistical pars-ing brought with an obvious tactic for rank-ing the agenda: (Bobrow, 1990) and (Chi-trao and Grishman, 1990) first used proba-bilistic context free grammars (PCFGs) togenerate probabilities for use in a figure ofmerit (FOM).
Later work introduced otherFOMs formed from PCFG data (Kochmanand Kupin, 1991); (Magerman and Marcus,1991); and (Miller and Fox, 1994).More recently, we have seen parse timeslowered by several orders of magnitude.
The(Caraballo and Charniak, 1998) article con-siders a number of different figures of meritfor ordering the agenda, and ultimately rec-ommends one that reduces the number ofedges required for a full parse into the thou-sands.
(Goldwater et al, 1998) (henceforth\[Gold98\]) introduces an edge-based tech-nique, (instead of constituent-based), whichdrops the average dge count into the hun-dreds.However, if we establish "perfection" asthe minimum number of edges needed togenerate the correct parse 47.5 edges on av-erage in our corpus--we can hope for stillmore improvement.
This paper looks at twonew figures of merit, both of which take the\[Gold98\] figure (of "independent" merit) asa starting point in cMculating a new figure513of merit for each edge, taking into accountsome additional information.
Our work fur-ther lowers the average dge count, bringingit from the hundreds into the dozens.2 Figure of independent merit(Caraballo and Charniak, 1998) and\[Gold98\] use a figure which indicates themerit of a given constituent or edge, relativeonly to itself and its children but indepen-dent of the progress of the parse we willcall this the edge's independent merit (IM).The philosophical backing for this figure isthat we would like to rank an edge based onthe valueP(N~,kIto,n ) , (1)where N~, k represents an edge of type i (NP,S, etc.
), which encompasses words j throughk -  1 of the sentence, and t0,~ represents all npart-of-speech tags, from 0 to n - 1.
(As inthe previous research, we simplify by look-ing at a tag stream, ignoring lexical infor-mation.)
Given a few basic independence as-sumptions (Caraballo and Charniak, 1998),this value can be calculated asi i fl( N ,k)P(NJ'k\]t?
'~) = P(to,n) , (2)with fl and a representing the well-known"inside" and "outside" probability functions:fl(Nj, k) = P(tj,klNj,,) (3)a(N ,k) = P(tod, N ,k, tk,n).
(4)Unfortunately, the outside probability is notcalculable until after a parse is completed.Thus, the IM is an approximation; if we can-not calculate the full outside probability (theprobability of this constituent occurring withall the other tags in the sentence), we canat least calculate the probability of this con-stituent occurring with the previous and sub-sequent tag.
This approximation, asgiven in(Caraballo and Charniak, 1998), isP(Nj, kltj-1)/3(N~,k)P(tklNj, k)P(tj,klt~-1)P(tklt~-l) (5)Of the five values required, P(N~.,kltj) ,P(tkltk_l), and P(tklN~,k) can be observeddirectly from the training data; the insideprobability isestimated using the most prob-able parse for Nj, k, and the tag sequenceprobability is estimated using a bitag ap-proximation.Two different probability distributions areused in this estimate, and the PCFG prob-abilities in the numerator tend to be a bitlower than the brag probabilities in the de-nominator; this is more of a factor in largerconstituents, o the figure tends to favourthe smaller ones.
To adjust the distribu-tions to counteract this effect, we will usea normalisation constant 7?
as in \[Gold98\].Effectively, the inside probability fl is mul-tiplied by r/k-j , preventing the discrepancyand hence the preference for shorter edges.In this paper we will use r /= 1.3 throughout;this is the factor by which the two distribu-tions differ, and was also empirically shownto be the best tradeoff between number of ?popped edges and accuracy (in \[Gold98\]).3 Finding FOM flawsClearly, any improvement to be had wouldneed to come through eliminating the in-correct edges before they are popped fromthe agenda--that is, improving the figure ofmerit.
We observed that the FOMs usedtended to cause the algorithm to spend toomuch time in one area of a sentence, gener-ating multiple parses for the same substring,before it would generate ven one parse foranother area.
The reason for that is that thefigures of independent merit are frequentlygood as relative measures for ranking differ-ent parses of the same sectio.n of the sen-tence, but not so good as absolute measuresfor ranking parses of different substrings.For instance, if the word "there" as anNP in "there's a hole in the bucket" hada low probability, it would tend to hold upthe parsing of a sentence; since the bi-tagprobability of "there" occurring at the be-ginning of a sentence is very high, the de-nominator of the IM would overbalance thenumerator.
(Note that this is a contrived514example--the actual problem cases are moreobscure.)
Of course, a different figure of in-dependent merit might have different char-acteristics, but with many of them there willbe cases where the figure is flawed, causinga single, vital edge to remain on the agendawhile the parser 'thrashes' around in otherparts of the sentence with higher IM values.We could characterise this observation asfollows:Pos tu la te  1 The longer an edge stays in theagenda without any competitors, the morelikely it is to be correct (even if it has a lowfigure of independent merit).A better figure, then, would take into ac-count whether a given piece of text had al-ready been parsed or not.
We took two ap-proaches to finding such a figure.4 Compensat ing  fo r  f laws4.1 Exper iment  1: Table lookupIn one approach to the problem, we triedto start our program with no extra informa-tion and train it statistically to counter theproblem mentioned in the previous section.There are four values mentioned in Postu-late 1: correctness, time (amount of workdone), number of competitors, and figure ofindependent merit.
We defined them as fol-lows:Correctness .
The obvious definition is thatan edge N~, k is correct if a constituentNj, k appears in the parse given in thetreebank.
There is an unobvious butunfortunate consequence of choosingthis definition, however; in many cases(especially with larger constituents),the "correct" rule appears just once inthe entire corpus, and is thus consid-ered too unlikely to be chosen by theparser as correct.
If the "correct" parsewere never achieved, we wouldn't haveany statistic at all as to the likelihood ofthe first, second, or third competitor be-ing better than the others.
If we define"correct" for the purpose of statistics-gathering as "in the MAP parse", theproblem is diminished.
Both defini-tions were tried for gathering statis-tics, though of course only the first wasused for measuring accuracy of outputparses.Work .
Here, the most logical measure foramount of work done is the numberof edges popped off the agenda.
Weuse it both because it is convenientlyprocessor-independent and because itoffers us a tangible measure of perfec-tion (47.5 edges--the average number ofedges in the correct parse of a sentence).Compet i to rsh ip .
At the most basic level,the competitors of a given edge Nj, kwould be all those edges N~, n such thatm _< j and n > k. Initially we only con-sidered an edge a 'competitor' if it metthis definition and were already in thechart; later we tried considering an edgeto be a competitor if it had a higher in-.dependent merit, no matter whether itbe in the agenda or the chart.
We alsotried a hybrid of the two.Mer i t .
The independent merit of an edge isdefined in section 2.
Unlike earlier work,which used what we call "IndependentMerit" as the FOM for parsing, we usethis figure as just one of many sourcesof information about a given edge.Given our postulate, the ideal figure ofmerit would beP( correct lW, C, IM) .
(6)We can save information about this proba-bility for each edge in every parse; but tobe useful in a statistical model, the IM mustfirst be discretised, and all three prior statis-tics need to be grouped, to avoid sparse dataproblems.
We bucketed all three logarithmi-cally, with bases 4, 2, and 10, respectively.This gives us the following approximation:P( correct I\[log 4W J, \[log 2CJ ,  \[log10 IMJ).
(7)To somewhat counteract he effect of dis-cretising the IM figure, each time we needed515FOM = P(correct\]\[log 4 WJ ,  \[log2CJ, \[logao IM\])(\[logmI\]Y -lOgloI\]k 0+ P (correct l \[log4 WJ, \[log2 CJ, \[log o IM\]) (loglo IM-  \[log o IMJ) (8)to calculate a figure of merit, we looked upthe table entry on either side of the IM andinterpolated.
Thus the actual value used as afigure of merit was that given in equation (8).Each trial consisted of a training run anda testing run.
The training runs consisted ofusing a grammar induced on treebank sec-tions 2-21 to run the edge-based best-firstalgorithm (with the IM alone as figure ofmerit) on section 24, collecting the statis-tics along the way.
It seems relatively obvi-ous that each edge should be counted whenit is created.
But our postulate involvesedges which have stayed on the agenda fora long time without accumulating competi-tors; thus we wanted to update our countswhen an edge happened to get more com-petitors, and as time passed.
Whenever thenumber of edges popped crossed into a newlogarithmic bucket (i.e.
whenever it passeda power of four), we re-counted every edgein the agenda in that new bucket.
In ad-dition, when the number of competitors of agiven edge passed a bucket boundary (powerof two), that edge would be re-counted.
Inthis manner, we had a count of exactly howmany edges--correct or not--had a given IMand a given number of competitors at a givenpoint in the parse.Already at this stage we found strong evi-dence for our postulate.
We were paying par-ticular attention to those edges with a lowIM and zero competitors, because those werethe edges that were causing problems whenthe parser ignored them.
When, consideringthis subset of edges, we looked at a graph ofthe percentage of edges in the agenda whichwere correct, we saw an increase of orders ofmagnitude as work increased--see Figure 1.For the testing runs, then, we used as fig-ure of merit the value in expression 8.
Asidefrom that change, we used the same edge-based best-first parsing algorithm as before.The test runs were all made on treebank sec-0.120.10.08G,~ O.Oe01~0 0.04=o0.02.
\[ IoglolM J = -4.
L IoglolM J = -5?
\[ IoglolM J = -6L IoglolM J = -7o L IoglolM J = -8,.~ ~ 2'.s ?
~.5 ~ ~.slog4 edges popped 4.5Figure 1: Zero competitors, low IM--Proportion of agenda edges correct vs. worktion 22, with all sentences longer than 40words thrown out; thus our results can bedirectly compared to those in the previouswork.We made several trials, using different def-initions of 'correct' and 'competitor', as de-scribed above.
Some performed much bet-ter than others, as seen in Table 1, whichgives our results, both in terms of accuracyand speed, as compared to the best previousresult, given in \[Gold98\].
The trial descrip-tions refer back to the multiple definitionsgiven for 'correct' and 'competitor' at thebeginning of this section.
While our bestspeed improvement (48.6% of the previousminimum) was achieved with the first run,it is associated with a significant loss in ac-curacy.
Our best results overall, listed inthe last row of the table, let us cut the edgecount by almost half while reducing labelledprecision/recall by only 0.24%.4.2 Experiment 2: Demerit ingWe hoped, however, that we might be ableto find a way to simplify the algorithm suchthat it would be easier to implement and/or516Table 1: Performance of various statistical schemataTrial description\[Gold98\] standardCorrect, Chart competitorsCorrect, higher-merit competitorsCorrect, Chart or higher-meritMAP, higher-merit competitorsLabelled Labelled Change in Edges PercentPrecision Recall LP/LR avg.
popped 2 of std.75.814% 73.334% 229.7374.982% 72.920% -.623% 111.59 48.6%75.588% 73.190% -.185% 135.23 58.9%75.433% 73.152% -.282% 128.94 56.1%75.365% 73.220% -.239% 120.47 52.4%.
.
.
.
.
, .
- " ' " " " " ' " i " " ' " ' " .
: ,?
. '
" " ' "  .
.
.
i .
i " ' " ' .
.0 5 6-5 3 4log m IM - ,5 o log 2 competitorsFigure 2: Stats at 64-255 edges poppedline is not parallel to the competitor axis,but rather angled so that the low-IM low-competitor items pass the scan before thehigh-IM high-competitor items.
This can besimulated by multiplying each edge's inde-pendent merit by a demeriting factor 5 percompetitor (thus a total of 5c).
Its exactvalue would determine the steepness of thescan line.Each trial consisted of one run, an edge-based best-first parse of treebank section 22(with sentences longer than 40 words thrownout, as before), using the new figure of merit:k- j  i i i~, ~ ) .
(9)faster to run, without sacrificing accuracy.To that end, we looked over the data, view-ing it as (among other things) a series of"planes" seen by setting the amount of workconstant (see Figure 2).
Viewed like this, theoriginal algorithm behaves like a scan line,parallel to the competitor axis, scanning forthe one edge with the highest figure of (in-dependent) merit.
However, one look at fig-ure 2 dramatically confirms our postulatethat an edge with zero competitors can havean IM orders of magnitude lower than anedge with many competitors, and still bemore likely to be correct.
Effectively, then,under the table lookup algorithm, the scan2previous work has shown that the parser per-forms better if it runs slightly past the first parse;so for every run referenced in this paper, the parserwas allowed to run to first parse plus a tenth.
Allreported final counts for popped edges are thus 1.1times the count at first parse.This idea works extremely well.
It is, pre-dictably, easier to implement; somewhat sur-prisingly, though, it actually performs bet-ter than the method it approximates.
When5 = .7, for instance, the accuracy loss is only.28%, comparable to the table lookup result,but the number of edges popped drops tojust 91.23, or 39.7% of the prior result foundin \[Gold98\].
Using other demeriting factorsgives similarly dramatic decreases in edgecount, with varying effects on accuracy--seeFigures 3 and 4.It is not immediately clear as to why de-meriting improves performance so dramat-ically over the table lookup method.
Onepossibility is that the statistical method runsinto too many sparse data problems aroundthe fringe of the data set--were we able touse a larger data set, we might see the statis-tics approach the curve defined by the de-meriting.
Another is that the bucketing istoo coarse, although the interpolation along5172~, -0  t8oCL10076.576)75 .5C~"~ 74.57472.801, o12 o13 o.,' o15 o15 0.7 o15 015demeriting factorFigure 3: Edges popped vs. 5O.0labelled recallo0 0 000 0 0 0 00 00 00 0 0X K XX X NX X X  ~ X X X X X  x X0'., o~ 013 oi, 0'.5 015 o'., 015 oi,demeriting factor 8Figure 4: Precision and recall vs. 5the independent merit axis would seem tomitigate that problem.5 Conclus ionIn the prior work, we see the average dgecost of a chart parse reduced from 170,000or so down to 229.7.
This paper gives a sim-ple modification to the \[Gold98\] algorithmthat further reduces this count to just over90 edges, less than two times the perfectminimum number of edges.
In addition tospeeding up tag-stream parsers, it seems rea-sonable to assume that the demeriting sys-tem would work in other classes of parserssuch as the lexicalised model of (Charniak,1997)--as long as the parsing technique hassome sort of demeritable ranking system, orat least some way of paying less attentionto already-filled positions, the kernel of thesystem should be applicable.
Furthermore,because of its ease of implementation, westrongly recommend the demeriting systemto those working with best-first parsing.ReferencesRobert J. Bobrow.
1990.
Statistical agendaparsing.
In DARPA Speech and LanguageWorkshop, pages 222-224.Sharon Carabal\]o and Eugene Charniak.1998.
New figures of merit for best-first probabilistic hart parsing.
Compu-tational Linguistics, 24(2):275-298, June.Eugene Charniak.
1997.
Statistical pars-ing with a context-free grammar and wordstatistics.
In Proceedings of the FourteenthNational Conference on Artificial Intelli-gence, pages 598-603, Menlo Park.
AAAIPress/MIT Press.Mahesh V. Chitrao and Ralph Grishman.1990.
Statistical parsing of messages.
InDARPA Speech and Language Workshop,pages 263-266.Sharon Goldwater, Eugene Charniak, andMark Johnson.
1998.
Best-first edge-based chart parsing.
In 6th Annual Work-shop for Very Large Corpora, pages 127-133.Martin Kay.
1970.
Algorithm schemata nddata structures in syntactic processing.
InBarbara J. Grosz, Karen Sparck Jones,and Bonne Lynn  Weber, editors, Readingsin Natural Language Processing, pages 35-70.
Morgan Kaufmann, Los Altos, CA.Fred Kochman and Joseph Kupin.
1991.Calculating the probability of a partialparse of a sentence.
In DARPA Speech andLanguage Workshop, pages 273-240.David M. Magerman and Mitchell P. Mar-cus.
1991.
Parsing the voyager domainusing pearl.
In DARPA Speech and Lan-guage Workshop, pages 231-236.Scott Miller and Heidi Fox.
1994.
Auto-matic grammar acquisition.
In Proceed-ings of the Human Language TechnologyWorkshop, pages 268-271.518
