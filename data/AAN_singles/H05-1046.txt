Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 363?370, Vancouver, October 2005. c?2005 Association for Computational LinguisticsDisambiguating Toponyms in NewsEric Garbin Inderjeet ManiDepartment of Linguistics Department of LinguisticsGeorgetown University Georgetown UniversityWashington, DC 20057, USA Washington, DC 20057, USAegarbin@cox.net im5@georgetown.eduAbstractThis research is aimed at the problem ofdisambiguating toponyms (place names)in terms of a classification derived bymerging information from two publiclyavailable gazetteers.
To establish the dif-ficulty of the problem, we measured thedegree of ambiguity, with respect to agazetteer, for toponyms in news.
Wefound that 67.82% of the toponyms foundin a corpus that were ambiguous in a gaz-etteer lacked a local discriminator in thetext.
Given the scarcity of human-annotated data, our method used unsuper-vised machine learning to develop disam-biguation rules.
Toponyms wereautomatically tagged with informationabout them found in a gazetteer.
Atoponym that was ambiguous in the gazet-teer was automatically disambiguatedbased on preference heuristics.
Thisautomatically tagged data was used totrain a machine learner, which disambigu-ated toponyms in a human-annotatednews corpus at 78.5% accuracy.1 IntroductionPlace names, or toponyms, are ubiquitous in natu-ral language texts.
In many applications, includingGeographic Information Systems (GIS), it is nec-essary to interpret a given toponym mention as aparticular entity in a geographical database or gaz-etteer.
Thus the mention ?Washington?
in ?He vis-ited Washington last year?
will need to beinterpreted as a reference to either the city Wash-ington, DC or the U.S. state of Washington, and?Berlin?
in ?Berlin is cold in the winter?
couldmean Berlin, New Hampshire or Berlin, Germany,among other possibilities.
While there has been aconsiderable body of work distinguishing betweena toponym and other kinds of names (e.g., personnames), there has been relatively little work onresolving which place and what kind of place givena classification of kinds of places in a gazetteer.Disambiguated toponyms can be used in a GIS tohighlight a position on a map corresponding to thecoordinates of the place, or to draw a polygon rep-resenting the boundary.In this paper, we describe a corpus-based methodfor disambiguating toponyms.
To establish the dif-ficulty of the problem, we began by quantifyingthe degree of ambiguity of toponyms in a corpuswith respect to a U.S. gazetteer.
We then carriedout a corpus-based investigation of features thatcould help disambiguate toponyms.
Given thescarcity of human-annotated data, our method usedunsupervised machine learning to develop disam-biguation rules.
Toponyms were automaticallytagged with information about them found in agazetteer.
A toponym that was ambiguous in thegazetteer was automatically disambiguated basedon preference heuristics.
This automatically taggeddata was used to train the machine learner.
Wecompared this method with a supervised machinelearning approach trained on a corpus annotatedand disambiguated by hand.Our investigation targeted toponyms that namecities, towns, counties, states, countries or nationalcapitals.
We sought to classify each toponym as anational capital, a civil political/administrativeregion, or a populated place (administration un-specified).
In the vector model of GIS, the type ofplace crucially determines the geometry chosen torepresent it (e.g., point, line or polygon) as well asany reasoning about geographical inclusion.
Theclass of the toponym can be useful in ?grounding?the toponym to latitude and longitude coordinates,363EntryNumberToponymU.S.CountyU.S.
State Lat-Long(dddmmss)Elevation (ft.above sealevel)Class110 ActonMiddlesex Massachu-setts422906N-0712600W260Ppl (popu-lated place)111 Acton Yellow-stoneMontana 455550N-1084048W3816 Ppl112 Acton Los Ange-lesCalifornia 342812N-1181145W2720 PplTable 1.
Example GNIS entries for an ambiguous toponymbut it can also go beyond grounding to support spa-tial reasoning.
For example, if the province ismerely grounded as a point in the data model (e.g.,if the gazetteer states that the centroid of a prov-ince is located at a particular latitude-longitude)then without the class information, the inclusion ofa city within a province can?t be established.
Also,resolving multiple cities or a unique capital to apolitical region mentioned in the text can be a use-ful adjunct to a map that lacks political boundariesor whose boundaries are dated.It is worth noting that our classification is morefine-grained than efforts like the EDT task inAutomatic Content Extraction1 program (Mitchelland Strassel 2002), which distinguishes betweentoponyms that are a Facility ?Alfredo Kraus Audi-torium?, a Location ?the Hudson River?, and Geo-Political Entities that include territories ?U.S.heartland?, and metonymic or other derivativeplace references ?Russians?, ?China (offered)?,?the U.S.
company?, etc.
Our classification, beinggazetteer based, is more suited to GIS-based appli-cations.2 Quantifying Toponym Ambiguity2.1 DataWe used a month?s worth of articles from the NewYork Times (September 2001), part of the EnglishGigaword (LDC 2003).
This corpus consisted of7,739 documents and, after SGML stripping, 6.51million word tokens with a total size of 36.4MB).We tagged the corpus using a list of place namesfrom the USGS Concise Gazetteer (GNIS).
Theresulting corpus is called MAC1, for ?MachineAnnotated Corpus 1?.
GNIS covers cities, states,1 www.ldc.upenn.edu/Projects/ACE/and counties in the U.S., which are classified as?civil?
and ?populated place?
geographical enti-ties.
A geographical entity is an entity on theEarth?s surface that can be represented by somegeometric specification in a GIS; for example, as apoint, line or polygon.
GNIS also covers 53 othertypes of geo-entities, e.g., ?valley,?
?summit?,?water?
and ?park.?
GNIS has 37,479 entries, with27,649 distinct toponyms, of which 13,860toponyms had multiple entries in the GNIS (i.e.,were ambiguous according to GNIS).
Table 1shows the entries in GNIS for an ambiguoustoponym.2.2 AnalysisLet E be a set of elements, and let F be a set of fea-tures.
We define a feature g in F to be a disam-biguator for E iff for all pairs <ex, ey> in E X E,g(ex) ?
g(ey) and neither g(ex) nor g(ey) are null-valued.
As an example, consider the GNIS gazet-teer in Table 1, let F = {U.S. County, U.S. State,Lat-Long, and Elevation}.
We can see that eachfeature in F is a disambiguator for the set of entriesE = {110, 111, 112}.Let us now characterize the mapping betweentexts and gazetteers.
A string s1 in a text is said tobe a discriminator within a window w for anotherstring s2 no more than w words away if s1 matchesa disambiguator d for s2 in a gazetteer.
For exam-ple, ?MT?
is a  discriminator within a window 5for the toponym ?Acton?
in ?Acton, MT,?
since?MT?
occurs within a ?5-word window of ?Acton?and matches, via an abbreviation, ?Montana?, thevalue of a GNIS disambiguator U.S. State (here thetokenized words are ?Acton?, ?,?, and ?MT?
).A trie-based lexical lookup tool (called LexScan)was used to match each toponym in GNIS againstthe corpus MAC1.
Of the 27,649 distinct toponyms364in GNIS, only 4553 were found in the corpus (notethat GNIS has only U.S. toponyms).
Of the 4553toponyms, 2911 (63.94%) were ?bare?
toponyms,lacking a local discriminator within a ?5-wordwindow that could resolve the name.Of the 13,860 toponyms that were ambiguousaccording to GNIS, 1827 of them were found inMAC1, of which only 588 had discriminatorswithin a ?5-word window (i.e., discriminatorswhich matched gazetteer features that disambigu-ated the toponym).
Thus, 67.82% of the 1827toponyms found in MAC1 that were ambiguous inGNIS lacked a discriminator.This 67.82% proportion is only an estimate oftrue toponym ambiguity, even for the sampleMAC1.
There are several sources of error in thisestimate: (i) World cities, capitals and countrieswere not yet considered, since GNIS only coveredU.S.
toponyms.
(ii) In general, a single feature(e.g., County, or State) may not be sufficient todisambiguate a set of entries.
It is of course possi-ble for two different places named by a commontoponym to be located in the same county in thesame state.
However, there were no toponyms withthis property in GNIS.
(iii) A string in MAC1tagged by GNIS lexical lookup as a toponym maynot have been a place name at all (e.g., ?Lord Ac-ton lived ??).
Of the toponyms that were spurious,most were judged by us to be common words andperson names.
This should not be surprising, as5341 toponyms in GNIS are also person namesaccording to the U.S. Census Bureau2 (iv) LexScanwasn?t perfect, for the following reasons.
First, itsought only exact matches.
Second, the matchingrelied on expansion of standard abbreviations.
Dueto non-standard abbreviations, the number of trueU.S.
toponyms in the corpus likely exceeded 4553.Third, the matches were all case-sensitive: whilecase-insensitivity caused numerous spuriousmatches, case-sensitivity missed a more predict-able set, i.e.
all-caps dateline toponyms or lower-case toponyms in Internet addresses.Note that the 67.82% proportion is just an esti-mate of local ambiguity.
Of course, there are oftennon-local discriminators (outside the ?5-wordwindows); for example, an initial place name ref-erence could have a local discriminator, with sub-2 www.census.gov/genealogy/www/freqnames.htmlsequent references in the article lacking local dis-criminators while being coreferential with the ini-tial reference.
To estimate this, we selected caseswhere a toponym was discriminated on its firstmention.
In those cases, we counted the number oftimes the toponym was repeated in the samedocument without the discriminator.
We found that73% of the repetitions lacked a local discriminator,suggesting an important role for coreference (seeSections 4 and 5).3 Knowledge Sources for Automatic Dis-ambiguationTo prepare a toponym disambiguator, we requireda gazetteer as well as corpora for training and test-ing it.3.1 GazetteerTo obtain a gazetteer that covered worldwideinformation, we harvested countries, country capi-tals, and populous world cities from two websitesATLAS3 and GAZ4, to form a consolidated gazet-teer (WAG) with four features G1,..,G4 based ongeographical inclusion, and three classes, as shownin Table 2.
As an example, an entry for Aberdeencould be the following feature vector: G1=UnitedStates, G2=Maryland, G3=Harford County,G4=Aberdeen, CLASS=ppl.We now briefly discuss the merging of ATLASand GAZ to produce WAG.
ATLAS provided asimple list of countries and their capitals.
GAZrecorded the country as well as the population of700 cities of at least 500,000 people.
If a city wasin both sources, we allowed two entries but or-dered them in WAG to make the more specifictype (e.g.
?capital?)
the default sense, the one thatLexScan would use.
Accents and diacritics werestripped from WAG toponyms by hand, and aliaseswere associated with standard forms.
Finally, wemerged GNIS state names with these, as well asabbreviations discovered by our abbreviation ex-pander.3.2 CorporaWe selected a corpus consisting of 15,587 articlesfrom the complete Gigaword Agence France3 .
www.worldatlas.com4 www.worldgazetteer.com365Presse, May 2002.
LexScan was used to tag, in-sensitive to case, all WAG toponyms found in thiscorpus, with the attributes in Table 2.
If there weremultiple entries in WAG for a toponym, LexScanonly tagged the preferred sense, discussed below.The resulting tagged corpus, called MAC-DEV,Tag At-tributeDescriptionCLASS Civil (Political Region or Administrative Area, e.g.
Country, Province, County), Ppl(Populated Place, e.g.
City, Town), Cap (Country Capital, Provincial Capital, or CountySeat)G1 CountryG2 Province (State) or Country-CapitalG3 County or Independent CityG4 City, Town (Within County)Table 2: WAG Gazetteer AttributesCorpus Size Use How AnnotatedMAC1 6.51 million words with61,720 place names (4553distinct) from GNISAmbiguity Study (Gigaword NYT Sept.2001) (Section 2)LexScan of allsenses, no attributesmarkedMAC-DEV5.47 million words with124,175 place names(1229 distinct) fromWAGDevelopment Corpus (Gigaword AFPMay 2002) (Section 4)LexScan using at-tributes from WAG,with heuristic pref-erenceMAC-ML6.21 million words with181,866 place names(1322 distinct) fromWAGMachine Learning Corpus (Gigaword APWorldwide January 2002) (Section 5)LexScan using at-tributes from WAG,with heuristic pref-erenceHAC 83,872 words with 1275place names (435 distinct)from WAG.Human Annotated Corpus (from Time-Bank 1.2,  and Gigaword NYT Sept. 2001and June 2002) (Section 5)LexScan usingWAG, with attrib-utes and sense beingmanually correctedTable 3.
Summary of CorporaTerm foundwith CapT-testCivilT-testPplTerm foundwith PplT-testCivilT-testCapTerm foundwith CivilT-testPplT-testCap?stock?
4 4 ?winter?
3.61 3.61 ?air?
3.16 3.16?exchange?
4.24 4.24 ?telephone?
3.16 3.16 ?base?
3.16 3.16?embassy?
3.61 3.61 ?port?
3.46 3.46 ?accuses?
3.61 3.61?capital?
1.4 2.2 ?midfielder?
3.46 3.46 ?northern?
5.57 5.57?airport?
3.32 3.32 ?city?
1.19 1.19 ?airlines?
4.8 4.8?summit?
4 4 ?near?
2.77 3.83 ?invaded?
3.32 3.32?lower?
3.16 3.16 ?times?
3.16 3.16 ?southern?
3.87 6.71?visit?
4.61 4.69 ?southern?
3.87 3.87 ?friendly?
4 4?conference?
4.24 4.24 ?yen?
4 0.56 ?state-run?
3.32 3.32?agreement?
3.16 3.16 ?attack?
0.18 3.87 ?border?
7.48 7.48Table 4.
Top 10 terms disambiguating toponym classeswas used as a development corpus for featureexploration.
To disambiguate the sense for atoponym that was ambiguous in WAG, we usedtwo preference heuristics.
First, we searched366MAC1 for two dozen highly frequent ambiguoustoponym strings (e.g., ?Washington?, etc.
), andobserved by inspection which sense predomi-nated in MAC1, preferring the predominantsense for each of these frequently mentionedtoponyms.
For example, in MAC1, ?Washing-ton?
was predominantly a Capital.
Second, fortoponyms outside this most frequent set, weused the following specificity-based preference:Cap.
> Ppl > Civil.
In other words, we preferthe more specific sense; since there are a smallernumber of Capitals than Populated places, weprefer Capitals to Populated Places.For machine learning, we used the GigawordAssociated Press Worldwide January 2002(15,999 articles), tagged in the same way byLexScan as MAC-DEV was.
This set was calledMAC-ML.
Thus, MAC1, MAC-DEV, andMAC-ML were all generated automatically,without human supervision.For a blind test corpus with human annotation,we opportunistically sampled three corpora:MAC1, TimeBank 1.25 and the June 2002 NewYork Times from the English Gigaword, withthe first author tagging a random 28, 88, and 49documents respectively from each.
Each tag inthe resulting human annotated corpus (HAC)had the WAG attributes from Table 2 with man-ual correction of all the WAG attributes.
Asummary of the corpora, their source, and anno-tation status is shown in Table 3.4 Feature ExplorationWe used the tagged toponyms in MAC-DEV toexplore useful features for disambiguating theclasses of toponyms.
We identified single-wordterms that co-occurred significantly with classeswithin a k-word window (we tried k= ?3, andk=?20).
These terms were scored for pointwisemutual information (MI) with the classes.
Termswith average tf.idf of less than 4 in the collectionwere filtered out as these tended to be personalpronouns, articles and prepositions.To identify which terms helped select for par-ticular classes of toponyms, the set of 48 termswhose MI scores were above a threshold (-11,chosen by inspection) were filtered using thestudent?s t-statistic, based on an idea in (Church5 www.timeml.organd Hanks 1991).
The t-statistic was used tocompare the distribution of the term with oneclass of toponym to its distribution with otherclasses to assess whether the underlying distri-butions were significantly different with at least95% confidence.
The results are shown in Table4, where scores for a term that occurred jointlyin a window with at least one other class labelare shown in bold.
A t-score > 1.645 is a signifi-cant difference with 95% confidence.
However,because joint evidence was scarce, we eventu-ally chose not to eliminate Table 4 terms such as?city?
(t =1.19) as features for machine learning.Some of the terms were significant disambigua-tors between only one pair of classes, e.g.
?yen,??attack,?
and ?capital,?
but we kept them on thatbasis.FeatureNameDescriptionAbbrev Value is true iff thetoponym  is abbreviated.AllCaps Value is true iff thetoponym is all capital let-ters.Left/RightPos{1,.., k}Values are the orderedtokens up to k positions tothe left/rightWkContext Value is the set of MIcollocated terms found inwindows of ?
k tokens (tothe left and right)TagDis-courseValue is the set ofCLASS values representedby all toponyms from thedocument:  e.g., the set{civil, capital, ppl}CorefClass Value is the CLASS ifany for a prior mention ofa toponym in the docu-ment, or noneTable 5.
Features for Machine LearningBased on the discovered terms in experimentswith different window sizes, and an examinationof MAC1 and MAC-DEV, we identified a finalset of features that, it seemed, might be usefulfor machine learning experiments.
These areshown in Table 5.
The features Abbrev and All-caps describe evidence internal to the toponym:367an abbreviation may indicate a state (Mass.
),territory (N.S.W.
), country (U.K.), or some othercivil place; an all-caps toponym might be a capi-tal or ppl in a dateline.
The feature sets LeftPosand RightPos target the ?k positions in eachwindow as ordered tokens, but note that onlywindows with a MI term are considered.
Thedomain of WkContext is the window of ?k to-kens around a toponym that contains a MI collo-cated term.We now turn to the global discourse-level fea-tures.
The domain for TagDiscourse is the wholedocument, which is evaluated for the set oftoponym classes present: this information mayreflect the discourse topic, e.g.
a discussion ofU.S.
sports teams will favor mentions of citiesover states or capitals.
The feature CorefClassimplements a one sense per discourse strategy,motivated by our earlier observation (from Sec-tion 2) that 73% of subsequent mentions of atoponym that was discriminated on first mentionwere expressed without a local discriminator.5 Machine LearningThe features in Table 5 were used to code fea-ture vectors for a statistical classifier.
The resultsare shown in Table 6.
As an example, when theRipper classifier (Cohen 1996) was trained onMAC-ML with a window of k= ?3 word tokens,the predictive accuracy when tested using cross-validation MAC-ML was 88.39% ?0.24 (where0.24 is the standard deviation across 10 folds).Accuracy on Test SetWindow = ?3 Window = ?20TrainingSetTest Set PredictiveAccuracyRecall,  Preci-sion, F-measurePredictiveAccuracyRecall, Precision,F-measureMAC-ML  MAC-ML(cross-validation)88.39 ?0.24 (Civ.65.0)Cap r70 p88 f78Civ.
r94 p90 f92Ppl r87 p82 f84Avg.
r84 p87 f8580.97 ?0.33 (Civ.57.1)Cap r61 p77 f68Civ.
r83 p86 f84Ppl r81 p72 f76Avg.
r75 p78 f76MAC-DEV  MAC-DEV(cross-validation)87.08 ?0.28 (Civ.57.8)Cap r74 p87 f80Civ.
r93 p88 f91Ppl r82 p80 f81Avg.
r83 p85 f8481.36 ?0.59 (Civ.59.3)Cap r49 p78 f60Civ.
r92 p81 f86Ppl r56 p70 f59Avg.
r66 p77 f68MAC-DEV HAC 68.66 (Civ.59.7)Cap r50 p71 f59Civ.
r93 p70 f80Ppl r24 p57 f33Avg.
r56 p66 f5765.33(Civ.
50.7)Cap r100 p100f100Civ.
r84 p62 f71Ppl r43 p71 f54Avg.
r76 p78 f75HACHAC(cross-validation)77.5 ?
2.94(Ppl 72.9)Cap r70 p97 f68Civ.
r34 p94 f49Ppl r98 p64 f77Avg.
r67 p85 f6573.12 ?3.09 (Ppl51.3)Cap r17 p90 f20Civ.
r63 p76 f68Ppl r84 p73 f77Avg.
r54 p79 f55MAC-DEV+MAC-MLMAC-DEV+MAC-ML (cross-validation)86.76 ?0.18 (Civ.60.7)Cap r70 p89 f78Civ.
r94 r88 f91Ppl r81 p80 f80Avg.
r82 p86 f8379.70 ?0.30 (Civ.59.7)Cap r56 p73 f63Civ.
r83 p86 f84Ppl r80 p68 f73Avg.
r73 p76 f73MAC-DEV+MAC-MLHAC 73.07 (Civ.51.7)Cap r71 p83 f77Civ.
r91 p69 f79Ppl r45 f81 f58Avg.
r69 p78 f7178.30(Civ.
50)Cap r100 p63 f77Civ.
r91 p75 f82Ppl r63 p88 f73Avg.
r85 p75 f77Table 6.
Machine Learning Accuracy368The majority class (Civil) had the predictive accu-racy shown in parentheses.
(When tested on a dif-ferent set from the training set, cross-validationwasn?t used).
Ripper reports a confusion matrix foreach class; Recall, Precision, and F-measure forthese classes are shown, along with their averageacross classes.In all cases, Ripper is significantly better in pre-dictive accuracy than the majority class.
Whentesting using cross-validation on the same ma-chine-annotated corpus as the classifier was trainedon, performance is comparable across corpora, andis in the high 80%, e.g., 88.39 on MAC-ML(k=?3).
Performance drops substantially when wetrain on machine-annotated corpora but test on thehuman-annotated corpus (HAC) (the unsupervisedapproach), or when we both train and test on HAC(the supervised approach).
The noise in the auto-generated classes in the machine-annotated corpusis a likely cause for the lower accuracy of the un-supervised approach.
The poor performance of thesupervised approach can be attributed to the lack ofhuman-annotated training data: HAC is a small,83,872-word corpus.Rule Description(Window = ?3)Coverageof Examplesin Testing(Accuracy)If not AllCaps(P) and  Right-Pos1(P,?SINGLE_QUOTE?
)and Civil ?
TagDiscourse ThenCivil(P).5/67(100%)If not AllCaps(P) and  Left-Pos1(P, southern) and Civil ?TagDiscourse Then Civil(P).13/67(100%)Table 7.
Sample Rules Learnt by RipperTagDiscourse was a critical feature; ignoring itduring learning dropped the accuracy nearly 9 per-centage points.
This indicates that prior mention ofa class increases the likelihood of that class.
(Notethat when inducing a rule involving a set-valuedfeature, Ripper tests whether an element is a mem-ber of that set-valued feature, selecting the test thatmaximizes information gain for a set of examples.
)Increasing the window size only lowered accuracywhen tested on the same corpus (using cross-validation); for example, an increase from ?3words to ?20 words (intervening sizes are notshown for reasons of space) lowered the PA by 5.7percentage points on MAC-DEV.
However, in-creasing the training set size was effective, and thisincrease was more substantial for larger windowsizes: combining MAC-ML with MAC-DEV im-proved accuracy on HAC by about 4.5% for k= ?3,but an increase of 13% was seen for k =?20.
Inaddition, F-measure for the classes was steady orincreased.
As Table 6 shows, this was largely dueto the increase in recall on the non-majorityclasses.
The best performance when training Rip-per on the machine-annotated MAC-DEV+MAC-ML and testing on the human-annotated corpusHAC was 78.30.Another learner we tried, the SMO support-vector machine from WEKA (Witten and Frank2005), was marginally better, showing 81.0 predic-tive accuracy training and testing on MAC-DEV+MAC-ML (ten-fold cross-validation, k=?20)and 78.5 predictive accuracy training on MAC-DEV+MAC-ML and testing on HAC (k=?20).Ripper rules are of course more transparent: exam-ple rules learned from MAC-DEV are shown inTable 7, along with their coverage of feature vec-tors and accuracy on the test set HAC.6 Related WorkWork related to toponym tagging has includedharvesting of gazetteers from the Web (Uryupina2003), hand-coded rules to place name disam-biguation, e.g., (Li et al 2003) (Zong et al 2005),and machine learning approaches to the problem,e.g., (Smith and Mann 2003).
There has of coursebeen a large amount of work on the more generalproblem of word-sense disambiguation, e.g.,(Yarowsky 1995) (Kilgarriff and Edmonds 2002).We discuss the most relevant work here.While (Uryupina 2003) uses machine learning toinduce gazetteers from the Internet, we merelydownload and merge information from two popularWeb gazetteers.
(Li et al 2003) use a statisticalapproach to tag place names as a LOCation class.They then use a heuristic approach to locationnormalization, based on a combination of hand-coded pattern-matching rules as well as discoursefeatures based on co-occurring toponyms (e.g., adocument with ?Buffalo?, ?Albany?
and ?Roches-ter?
will likely have those toponyms disambiguatedto New York state).
Our TagDiscourse feature ismore coarse-grained.
Finally, they assume onesense per discourse in their rules, whereas we use it369as a feature CorefClass for use in learning.
Overall,our approach is based on unsupervised machinelearning, rather than hand-coded rules for locationnormalization.
(Smith and Mann 2003) use a ?minimally super-vised?
method that exploits as training datatoponyms that are found locally disambiguated,e.g., ?Nashville, Tenn.?
; their disambiguation taskis to identify the state or country associated withthe toponym in test data that has those disambigua-tors stripped off.
Although they report 87.38% ac-curacy on news, they address an easier problemthan ours, since: (i) our earlier local ambiguity es-timate suggests that as many as two-thirds of thegazetteer-ambiguous toponyms may be excludedfrom their test on news, as they would lack localdiscriminators (ii) the classes our tagger uses (Ta-ble 3) are more fine-grained.
Finally, they use onesense per discourse as a bootstrapping strategy toexpand the machine-annotated data, whereas in ourcase CorefClass is used as a feature.Our approach is distinct from other work in thatit firstly, attempts to quantify toponym ambiguity,and secondly, it uses an unsupervised approachbased on learning from noisy machine-annotatedcorpora using publicly available gazetteers.7 ConclusionThis research provides a measure of the degree ofof ambiguity with respect to a gazetteer fortoponyms in news.
It has developed a toponymdisambiguator that, when trained on entirely ma-chine annotated corpora that avail of easily avail-able Internet gazetteers, disambiguates toponymsin a human-annotated corpus at 78.5% accuracy.Our current project includes integrating our dis-ambiguator with other gazetteers and with a geo-visualization system.
We will also study the effectof other window sizes and the combination of thisunsupervised approach with minimally-supervisedapproaches such as (Brill 1995) (Smith and Mann2003).
To help mitigate against data sparseness, wewill cluster terms based on stemming and semanticsimilarity.The resources and tools developed here may beobtained freely by contacting the authors.ReferencesEric Brill.
1995.
Unsupervised learning of disambigua-tion rules for part of speech tagging.
ACL ThirdWorkshop on Very Large Corpora, Somerset, NJ, p.1-13.Ken Church, Patrick Hanks, Don Hindle, and WilliamGale.
1991.
Using Statistics in Lexical Analysis.
InU.
Zernik (ed), Lexical Acquisition: Using On-lineResources to Build a Lexicon, Erlbaum, p. 115-164.William Cohen.
1996.
Learning Trees and Rules withSet-valued Features.
Proceedings of AAAI 1996,Portland, Oregon, p. 709-716.Adam Kilgarriff and Philip Edmonds.
2002.
Introduc-tion to the Special Issue on Evaluating Word SenseDisambiguation Systems.
Journal of Natural Lan-guage Engineering 8 (4).Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li.2003.
A hybrid approach to geographical referencesin information extraction.
HLT-NAACL 2003 Work-shop: Analysis of Geographic References, Edmonton,Alberta, Canada.LDC.
2003.
Linguistic Data Consortium: English Giga-wordwww.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2003T05Alexis Mitchell and Stephanie Strassel.
2002.
CorpusDevelopment for the ACE (Automatic Content Ex-traction) Program.
Linguistic Data Consortiumwww.ldc.upenn.edu/Projects/LDC_Institute/Mitchell/ACE_LDC_06272002.pptDavid Smith and Gideon Mann.
2003.
Bootstrappingtoponym classifiers.
HLT-NAACL 2003 Workshop:Analysis of Geographic References, p. 45-49, Ed-monton, Alberta, Canada.Ian Witten and Eibe Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques, 2nd Edi-tion.
Morgan Kaufmann, San Francisco.Olga Uryupina.
2003.
Semi-supervised learning of geo-graphical gazetteers from the internet.
HLT-NAACL2003 Workshop: Analysis of Geographic References,Edmonton, Alberta, Canada.David Yarowsky.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
Pro-ceedings of ACL 1995, Cambridge, Massachusetts.Wenbo Zong, Dan Wu, Aixin Sun, Ee-Peng Lim, andDion H. Goh.
2005.
On Assigning Place Names toGeography Related Web Pages.
Joint Conference onDigital Libraries (JCDL2005), Denver, Colorado.370
