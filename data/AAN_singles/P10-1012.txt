Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 108?117,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAutomatic Evaluation Method for Machine Translation usingNoun-Phrase ChunkingHiroshi Echizen-yaHokkai-Gakuen UniversityS 26-Jo, W 11-chome, Chuo-ku,Sapporo, 064-0926 Japanechi@eli.hokkai-s-u.ac.jpKenji ArakiHokkaido UniversityN 14-Jo, W 9-Chome, Kita-ku,Sapporo, 060-0814 Japanaraki@media.eng.hokudai.ac.jpAbstractAs described in this paper, we proposea new automatic evaluation method formachine translation using noun-phrasechunking.
Our method correctly deter-mines the matching words between twosentences using corresponding nounphrases.
Moreover, our method deter-mines the similarity between two sen-tences in terms of the noun-phrase or-der of appearance.
Evaluation experi-ments were conducted to calculate thecorrelation among human judgments,along with the scores produced us-ing automatic evaluation methods forMT outputs obtained from the 12 ma-chine translation systems in NTCIR-7.
Experimental results show thatour method obtained the highest cor-relations among the methods in bothsentence-level adequacy and fluency.1 IntroductionHigh-quality automatic evaluation has be-come increasingly important as various ma-chine translation systems have developed.
Thescores of some automatic evaluation meth-ods can obtain high correlation with humanjudgment in document-level automatic evalua-tion(Coughlin, 2007).
However, sentence-levelautomatic evaluation is insufficient.
A greatgap exists between language processing of au-tomatic evaluation and the processing by hu-mans.
Therefore, in recent years, various au-tomatic evaluation methods particularly ad-dressing sentence-level automatic evaluationshave been proposed.
Methods based on wordstrings (e.g., BLEU(Papineni et al, 2002),NIST(NIST, 2002), METEOR(Banerjee andLavie., 2005), ROUGE-L(Lin and Och, 2004),and IMPACT(Echizen-ya and Araki, 2007))calculate matching scores using only commonwords between MT outputs and referencesfrom bilingual humans.
However, these meth-ods cannot determine the correct word corre-spondences sufficiently because they fail to fo-cus solely on phrase correspondences.
More-over, various methods using syntactic analyt-ical tools(Pozar and Charniak, 2006; Muttonet al, 2007; Mehay and Brew, 2007) are pro-posed to address the sentence structure.
Nev-ertheless, those methods depend strongly onthe quality of the syntactic analytical tools.As described herein, for use with MT sys-tems, we propose a new automatic evaluationmethod using noun-phrase chunking to obtainhigher sentence-level correlations.
Using nounphrases produced by chunking, our methodyields the correct word correspondences anddetermines the similarity between two sen-tences in terms of the noun phrase order of ap-pearance.
Evaluation experiments using MToutputs obtained by 12 machine translationsystems in NTCIR-7(Fujii et al, 2008) demon-strate that the scores obtained using our sys-tem yield the highest correlation with the hu-man judgments among the automatic evalua-tion methods in both sentence-level adequacyand fluency.
Moreover, the differences be-tween correlation coefficients obtained usingour method and other methods are statisti-cally significant at the 5% or lower signifi-cance level for adequacy.
Results confirmedthat our method using noun-phrase chunkingis effective for automatic evaluation for ma-chine translation.2 Automatic Evaluation Methodusing Noun-Phrase ChunkingThe system based on our method has four pro-cesses.
First, the system determines the corre-108spondences of noun phrases between MT out-puts and references using chunking.
Secondly,the system calculates word-level scores basedon the correct matched words using the deter-mined correspondences of noun phrases.
Next,the system calculates phrase-level scores basedon the noun-phrase order of appearance.
Thesystem calculates the final scores combiningword-level scores and phrase-level scores.2.1 Correspondence of Noun Phrasesby ChunkingThe system obtains the noun phrases fromeach sentence by chunking.
It then determinescorresponding noun phrases between MT out-puts and references calculating the similarityfor two noun phrases by the PER score(Su etal., 1992).
In that case, PER scores of twokinds are calculated.
One is the ratio of thenumber of match words between an MT out-put and reference for the number of all wordsof the MT output.
The other is the ratio of thenumber of match words between the MT out-put and reference for the number of all wordsof the reference.
The similarity is obtained asan F -measure between two PER scores.
Thehigh score represents that the similarity be-tween two noun phrases is high.
Figure 1presents an example of the determination ofthe corresponding noun phrases.MT output :in general , [NP the amount ] of [NP the crowning fall ]is large like [NP the end ] .Reference :generally , the closer [NP it ] is to [NP the end part ] ,the larger [NP the amount ] of [NP crowning drop ] is .
(1) Use of noun phrase chunkingMT output :in general , [NP the amount ] of [NP the crowning fall ]is large like [NP the end ] .Reference :generally , the closer [NP it ] is to [NP the end part ] ,the larger [NP the amount ] of [NP crowning drop ] is .
(2) Determination of corresponding noun phrases1.00000.37140.7429Figure 1: Example of determination of corre-sponding noun phrases.In Fig.
1, ?the amount?, ?the crowning fall?and ?the end?
are obtained as noun phrasesin MT output by chunking, and ?it?, ?the endpart?, ?the amount?
and ?crowning drop?
areobtained in the reference by chunking.
Next,the system determines the corresponding nounphrases from these noun phrases between theMT output and reference.
The score between?the end?
and ?the end part?
is the highestamong the scores between ?the end?
in theMT output and ?it?, ?the end part?, ?theamount?, and ?crowning drop?
in the refer-ence.
Moreover, the score between ?the endpart?
and ?the end?
is the highest among thescores between ?the end part?
in referenceand ?the amount?, ?the crowning fall?, ?theend?
in the MT output.
Consequently, ?theend?
and ?the end part?
are selected as nounphrases with the highest mutual scores: ?theend?
and ?the end part?
are determined as onecorresponding noun phrase.
In Fig.
1, ?theamount?
in the MT output and ?the amount?in reference, and ?the crowning fall?
in theMT output and ?crowning drop?
in the ref-erence also are determined as the respectivecorresponding noun phrases.
The noun phrasefor which the score between it and other nounphrases is 0.0 (e.g., ?it?
in reference) has nocorresponding noun phrase.
The use of thenoun phrases is effective because the frequencyof the noun phrases is higher than those ofother phrases.
The verb phrases are not usedfor this study, but they can also be generatedby chunking.
It is difficult to determine thecorresponding verb phrases correctly becausethe words in each verb phrase are often fewerthan the noun phrases.2.2 Word-level ScoreThe system calculates the word-level scoresbetween MT output and reference using thecorresponding noun phrases.
First, the sys-tem determines the common words based onLongest Common Subsequence (LCS).
Thesystem selects only one LCS route when sev-eral LCS routes exist.
In such cases, the sys-tem calculates the Route Score (RS) using thefollowing Eqs.
(1) and (2):RS =?c?LCS(?w?cweight(w))?
(1)109weight(w) =??????
?words in corresponding2 noun phrasewords in non1 corresponding noun phrase(2)In Eq.
(1), ?
is a parameter for lengthweighting of common parts; it is greater than1.0.
Figure 2 portrays an example of deter-mination of the common parts.
In the firstprocess of Fig.
2, LCS is 7.
In this example,several LCS routes exist.
The system selectsthe LCS route which has ?,?, ?the amountof?, ?crowning?, ?is?, and ?.?
as the com-mon parts.
The common part is the partfor which the common words appear contin-uously.
In contrast, IMPACT selects a differ-ent LCS route that includes ?, the?, ?amountof?, ?crowning?, ?is?, and ?.?
as the com-mon parts.
In IMPACT, using no analyticalknowledge, the LCS route is determined usingthe information of the number of words in thecommon parts and the position of the com-mon parts.
The RS for LCS route selectedusing our method is 32 (= 12.0 + (2 + 2 +1)2.0 + 22.0 + 12.0 + 12.0) when ?
is 2.0.
TheRS for LCS route selected by IMPACT is 19(= (1 + 1)2.0 + (2 + 1)2.0 + 22.0 + 12.0 + 12.0).In the LCS route selected by IMPACT, theweight of ?the?
in the common part ?, the?is 1 because ?the?
in the reference is not in-cluded in the corresponding noun phrase.
Inthe LCS route selected using our method, theweight of ?the?
in ?the amount of?
is 2 because?the?
in MT output and ?the?
in the referenceare included in the corresponding noun phrase?NP1?.
Therefore, the system based on ourmethod can select the correct LCS route.Moreover, the word-level score is calculatedusing the common parts in the selected LCSroute as the following Eqs.
(3), (4), and (5).Rwd =??
?RNi=0(?i?c?LCS length(c)?)m???1?
(3)Pwd =??
?RNi=0(?i?c?LCS length(c)?)n???1?
(4)MT output :in general , [NP1 the amount ] of [NP2 the crowning fall ]is large like [NP3 the end ] .Reference :generally , the closer [NP it ] is to [NP3 the end part ] , thelarger [NP1 the amount ] of [NP2 crowning drop ] is .
(1) First process for determination of common parts :LCS = 7(2) Second process for determination of common parts :LCS=3Our methodMT output :in general , [NP1 the amount ] of [NP2 the crowning fall ]is large like [NP3 the end ] .Reference :generally , the closer [NP it ] is to [NP3 the end part ] , thelarger [NP1 the amount ] of [NP2 crowning drop ] is .Our methodMT output :in general , [NP1 the amount ] of [NP2 the crowning fall ]is large like [NP3 the end ] .Reference :generally , the closer [NP it ] is to [NP3 the end part ] , thelarger [NP1 the amount ] of [NP2 crowning drop ] is .IMPACT12.0 (2+2+1)2.0 22.012.012.0(1+1)2.0(2+1)2.0 22.0 12.012.0Figure 2: Example of common-part determi-nation.scorewd =(1 + ?2)RwdPwdRwd + ?2Pwd(5)Equation (3) represents recall and Eq.
(4)represents precision.
Therein, m signifies theword number of the reference in Eq.
(3), andn stands for the word number of the MT out-put in Eq.
(4).
Here, RN denotes the repe-tition number of the determination process ofthe LCS route, and i, which has initial value 0,is the counter for RN .
In Eqs.
(3) and (4), ?is a parameter for the repetition process of thedetermination of LCS route, and is less than1.0.
Therefore, Rwd and Pwd becomes smallas the appearance order of the common partsbetween MT output and reference is different.Moreover, length(c) represents the number ofwords in each common part; ?
is a param-eter related to the length weight of commonparts, as in Eq.
(1).
In this case, the weightof each common word in the common part is1.
The system calculates scorewd as the word-level score in Eq.
(5).
In Eq.
(5), ?
is deter-mined as Pwd/Rwd.
The scorewd is between0.0 and 1.0.110In the first process of Fig.
2,?i?c?LCS length(c)?
is 13.0 (=0.50 ?
(12.0 + 32.0 + 12.0 + 12.0 + 12.0)) when ?
and?
are 0.5 and 2.0, respectively.
In this case,the counter i is 0.
Moreover, in the secondprocess of Fig.
2, ?i?c?LCS length(c)?
is 2.5(=0.51?
(12.0 +22.0)) using two common parts?the?
and ?the end?, except the commonparts determined using the first process.In Fig.
2, RN is 1 because the systemfinishes calculating ?i?c?LCS length(c)?when counter i became 1: this means thatall common parts were processed untilthe second process.
As a result, Rwd is0.1969 (=?
(13.0 + 2.5)/202.0 =?0.0388),and Pwd is 0.2625 (=?
(13.0 + 2.5)/152.0 =?0.0689).
Consequently, scorewd is 0.2164(= (1+1.33322)?0.1969?0.26250.1969+1.33322?0.2625 ).
In this case, ?becomes 1.3332 (=0.26250.1969).
The system candetermine the matching words correctly usingthe corresponding noun phrases between theMT output and the reference.The system calculates scorewd multi usingRwd multi and Pwd multi which are, respec-tively, maximum Rwd and Pwd when multiplereferences are used as the following Eqs.
(6),(7) and (8).
In Eq.
(8), ?
is determined asPwd multi/Rwd multi.
The scorewd multi is be-tween 0.0 and 1.0.Rwd multi =maxuj=1???????????????
(RN?i=0(?i ?c?LCSlength(c)?))jm?j???????1?????????
(6)Pwd multi =maxuj=1???????????????
(RN?i=0(?i ?c?LCSlength(c)?))jn?j???????1?????????
(7)scorewd multi =(1 + ?2Rwd multi)Pwd multiRwd multi + ?2Pwd multi(8)2.3 Phrase-level ScoreThe system calculates the phrase-level scoreusing the noun phrases obtained by chunking.First, the system extracts only noun phrasesfrom sentences.
Then it generalizes each nounphrase as each word.
Figure 3 presents exam-ples of generalization by noun phrases.MT output :in general , [NP1 the amount ] of [NP2 the crowning fall ]is large like [NP3 the end ] .Reference :generally , the closer [NP it ] is to [NP3 the end part ] ,the larger [NP1 the amount ] of [NP2 crowning drop ] is .
(1) Corresponding noun phrases(2) Generalization by noun phrasesMT output :NP1  NP2  NP3Reference :NP  NP3   NP1   NP2Figure 3: Example of generalization by nounphrases.Figure 3 presents three corresponding nounphrases between the MT output and the refer-ence.
The noun phrase ?it?, which has no cor-responding noun phrase, is expressed as ?NP?in the reference.
Consequently, the MT outputis generalized as ?NP1 NP2 NP3?
; the refer-ence is generalized as ?NP NP3 NP1 NP2?.Subsequently, the system obtains the phrase-level score between the generalized MT outputand reference as the following Eqs.
(9), (10),and (11).Rnp =???
?RNi=0(?i?cnpp?LCS length(cnpp)?
)(mcnp ?
?mno cnp)????1?
(9)Pnp =???
?RNi=0(?i?cnpp?LCS length(cnpp)?
)(ncnp ?
?nno cnp)????1?
(10)111Table 1: Machine translation system types.System No.
1 System No.
2 System No.
3 System No.
4 System No.
5 System No.
6Type SMT SMT RBMT SMT SMT SMTSystem No.
7 System No.
8 System No.
9 System No.
10 System No.
11 System No.
12Type SMT SMT EBMT SMT SMT RBMTscorenp =(1 + ?2)RnpPnpRnp + ?2Pnp(11)In Eqs.
(9) and (10), cnpp denotes thecommon noun phrase parts; mcnp and ncnprespectively signify the quantities of commonnoun phrases in the reference and MT output.Moreover, mno cnp and nno cnp are the quanti-ties of noun phrases except the common nounphrases in the reference and MT output.
Thevalues of mno cnp and nno cnp are processedas 1 when no non-corresponding noun phrasesexist.
The square root used for mno cnp andnno cnp is to decrease the weight of the non-corresponding noun phrases.
In Eq.
(11), ?
isdetermined as Pnp/Rnp.
In Fig.
3, Rnp andPnp are 0.7071 (=?1?22.0+0.5?12.0(3?1)2.0) when ?
is0.5 and ?
is 2.0.
Therefore, scorenp is 0.7071.The system obtains scorenp multi calculat-ing the average of scorenp when multiple ref-erences are used as the following Eq.
(12).scorenp multi =?uj=0 (scorenp)ju(12)2.4 Final ScoreThe system calculates the final score by com-bining the word-level score and the phrase-level score as shown in the following Eq.
(13).score =scorewd + ?
?
scorenp1 + ?
(13)Therein, ?
represents a parameter for theweight of scorenp: it is between 0.0 and 1.0.The ratio of scorewd to scorenp is 1:1 when ?
is1.0.
Moreover, scorewd multi and scorenp multiare used for Eq.
(13) in multiple references.In Figs.
2 and 3, the final score betweenthe MT output and the reference is 0.4185(=0.2164+0.7?0.70711+0.7 ) when ?
is 0.7.
The systemcan realize high-quality automatic evaluationusing both word-level information and phrase-level information.3 Experiments3.1 Experimental ProcedureWe calculated the correlation between thescores obtained using our method and scoresproduced by human judgment.
The systembased on our method obtained the evaluationscores for 1,200 English output sentences re-lated to the patent sentences.
These Englishoutput sentences are sentences that 12 ma-chine translation systems in NTCIR-7 trans-lated from 100 Japanese sentences.
Moreover,the number of references to each English sen-tence in 100 English sentences is four.
Thesereferences were obtained from four bilingualhumans.
Table 1 presents types of the 12 ma-chine translation systems.Moreover, three human judges evaluated1,200 English output sentences from the per-spective of adequacy and fluency on a scale of1?5.
We used the median value in the evalua-tion results of three human judges as the finalscores of 1?5.
We calculated Pearson?s correla-tion efficient and Spearman?s rank correlationefficient between the scores obtained using ourmethod and the scores by human judgments interms of sentence-level adequacy and fluency.Additionally, we calculated the correlationsbetween the scores using seven other methodsand the scores by human judgments to com-pare our method with other automatic evalua-tion methods.
The other seven methods wereIMPACT, ROUGE-L, BLEU1, NIST, NMG-WN(Ehara, 2007; Echizen-ya et al, 2009),METEOR2, and WER(Leusch et al, 2003).Using our method, 0.1 was used as the value ofthe parameter ?
in Eqs.
(3)-(10) and 1.1 wasused as the value of the parameter ?
in Eqs.(1)?(10).
Moreover, 0.3 was used as the valueof the parameter ?
in Eq.
(13).
These val-1BLEU was improved to perform sentence-levelevaluation: the maximum N value between MT outputand reference is used(Echizen-ya et al, 2009).2The matching modules of METEOR are the exactand stemmed matching module, and a WordNet-basedsynonym-matching module.112Table 2: Pearson?s correlation coefficient for sentence-level adequacy.No.
1 No.
2 No.
3 No.
4 No.
5 No.
6 No.
7Our method 0.7862 0.4989 0.5970 0.5713 0.6581 0.6779 0.7682IMPACT 0.7639 0.4487 0.5980 0.5371 0.6371 0.6255 0.7249ROUGE-L 0.7597 0.4264 0.6111 0.5229 0.6183 0.5927 0.7079BLEU 0.6473 0.2463 0.4230 0.4336 0.3727 0.4124 0.5340NIST 0.5135 0.2756 0.4142 0.3086 0.2553 0.2300 0.3628NMG-WN 0.7010 0.3432 0.6067 0.4719 0.5441 0.5885 0.5906METEOR 0.4509 0.0892 0.3907 0.2781 0.3120 0.2744 0.3937WER 0.7464 0.4114 0.5519 0.5185 0.5461 0.5970 0.6902Our method II 0.7870 0.5066 0.5967 0.5191 0.6529 0.6635 0.7698BLEU with our method 0.7244 0.3935 0.5148 0.5231 0.4882 0.5554 0.6459No.
8 No.
9 No.
10 No.
11 No.
12 Avg.
AllOur method 0.7664 0.7208 0.6355 0.7781 0.5707 0.6691 0.6846IMPACT 0.7007 0.7125 0.5981 0.7621 0.5345 0.6369 0.6574ROUGE-L 0.6834 0.7042 0.5691 0.7480 0.5293 0.6228 0.6529BLEU 0.5188 0.5884 0.3697 0.5459 0.4357 0.4607 0.4722NIST 0.4218 0.4092 0.1721 0.3521 0.4769 0.3493 0.3326NMG-WN 0.6658 0.6068 0.6116 0.6770 0.5740 0.5818 0.5669METEOR 0.3881 0.4947 0.3127 0.2987 0.4162 0.3416 0.2958WER 0.6656 0.6570 0.5740 0.7491 0.5301 0.6031 0.5205Our method II 0.7676 0.7217 0.6343 0.7917 0.5474 0.6632 0.6774BLEU with our method 0.6395 0.6696 0.5139 0.6611 0.5079 0.5698 0.5790ues of the parameter are determined using En-glish sentences from Reuters articles(Utiyamaand Isahara, 2003).
Moreover, we obtainedthe noun phrases using a shallow parser(Shaand Pereira, 2003) as the chunking tool.
Werevised some erroneous results that were ob-tained using the chunking tool.3.2 Experimental ResultsAs described in this paper, we performed com-parison experiments using our method andseven other methods.
Tables 2 and 3 respec-tively show Pearson?s correlation coefficient forsentence-level adequacy and fluency.
Tables 4and 5 respectively show Spearman?s rank cor-relation coefficient for sentence-level adequacyand fluency.
In Tables 2?5, bold typefacesignifies the maximum correlation coefficientsamong eight automatic evaluation methods.Underlining in our method signifies that thedifferences between correlation coefficients ob-tained using our method and IMPACT arestatistically significant at the 5% significancelevel.
Moreover, ?Avg.?
signifies the aver-age of the correlation coefficients obtained by12 machine translation systems in respectiveautomatic evaluation methods, and ?All?
arethe correlation coefficients using the scores of1,200 output sentences obtained using the 12machine translation systems.3.3 DiscussionIn Tables 2?5, the ?Avg.?
score of our methodis shown to be higher than those of other meth-ods.
Especially in terms of the sentence-leveladequacy shown in Tables 2 and 4, ?Avg.
?of our method is about 0.03 higher than thatof IMPACT.
Moreover, in system No.
8 and?All?
of Tables 2 and 4, the differences be-tween correlation coefficients obtained usingour method and IMPACT are statistically sig-nificant at the 5% significance level.Moreover, we investigated the correlation ofmachine translation systems of every type.
Ta-ble 6 shows ?All?
of Pearson?s correlation co-efficient and Spearman?s rank correlation coef-ficient in SMT (i.e., system Nos.
1?2, systemNos.
4?8 and system Nos.
10?11) and RBMT(i.e., system Nos.
3 and 12).
The scores of900 output sentences obtained by 9 machine113Table 3: Pearson?s correlation coefficient for sentence-level fluency.No.
1 No.
2 No.
3 No.
4 No.
5 No.
6 No.
7Our method 0.5853 0.3782 0.5689 0.4673 0.5739 0.5344 0.7193IMPACT 0.5581 0.3407 0.5821 0.4586 0.5768 0.4852 0.6896ROUGE-L 0.5551 0.3056 0.5925 0.4391 0.5666 0.4475 0.6756BLEU 0.4793 0.0963 0.4488 0.3033 0.4690 0.3602 0.5272NIST 0.4139 0.0257 0.4987 0.1682 0.3923 0.2236 0.3749NMG-WN 0.5782 0.3090 0.5434 0.4680 0.5070 0.5234 0.5363METEOR 0.4050 0.1405 0.4420 0.1825 0.4259 0.2336 0.4873WER 0.5143 0.3031 0.5220 0.4262 0.4936 0.4405 0.6351Our method II 0.5831 0.3689 0.5753 0.3991 0.5610 0.5445 0.7186BLEU with our method 0.5425 0.2304 0.5115 0.3770 0.5358 0.4741 0.6142No.
8 No.
9 No.
10 No.
11 No.
12 Avg.
AllOur method 0.5796 0.6424 0.3241 0.5920 0.4321 0.5331 0.5574IMPACT 0.5612 0.6320 0.3492 0.6034 0.4166 0.5211 0.5469ROUGE-L 0.5414 0.6347 0.3231 0.5889 0.4127 0.5069 0.5387BLEU 0.5040 0.5521 0.2134 0.4783 0.4078 0.4033 0.4278NIST 0.3682 0.3811 0.1682 0.3116 0.4484 0.3146 0.3142NMG-WN 0.5526 0.5799 0.4509 0.6308 0.4124 0.5007 0.5074METEOR 0.2511 0.4153 0.1376 0.3351 0.2902 0.3122 0.2933WER 0.5492 0.6421 0.3962 0.6228 0.4063 0.4960 0.4478Our method II 0.5774 0.6486 0.3428 0.5975 0.4197 0.5280 0.5519BLEU with our method 0.5660 0.6247 0.2536 0.5495 0.4550 0.4770 0.5014translation systems in SMT and the scores of200 output sentences obtained by 2 machinetranslation systems in RBMT are used respec-tively.
However, EBMT is not included in Ta-ble 6 because EBMT is only system No.
9.In Table 6, our method obtained the highestcorrelation among the eight methods, exceptin terms of the adequacy of RBMT in Pear-son?s correlation coefficient.
The differencesbetween correlation coefficients obtained us-ing our method and IMPACT are statisticallysignificant at the 5% significance level for ad-equacy of SMT.To confirm the effectiveness of noun-phrasechunking, we performed the experiment usinga system combining BLEU with our method.In this case, BLEU scores were used as scorewdin Eq.
(13).
This experimental result is shownas ?BLEU with our method?
in Tables 2?5.
Inthe results of ?BLEU with our method?
in Ta-bles 2?5, underlining signifies that the differ-ences between correlation coefficients obtainedusing BLEU with our method and BLEU aloneare statistically significant at the 5% signif-icance level.
The coefficients of correlationfor BLEU with our method are higher thanthose of BLEU in any machine translation sys-tem, ?Avg.?
and ?All?
in Tables 2?5.
More-over, for sentence-level adequacy, BLEU withour method is significantly better than BLEUin almost all machine translation systems and?All?
in Tables 2 and 4.
These results indicatethat our method using noun-phrase chunkingis effective for some methods and that it isstatistically significant in each machine trans-lation system, not only ?All?, which has largesentences.Subsequently, we investigated the precisionof the determination process of the corre-sponding noun phrases described in section2.1: in the results of system No.
1, we cal-culated the precision as the ratio of the num-ber of the correct corresponding noun phrasesfor the number of all noun-phrase correspon-dences obtained using the system based on ourmethod.
Results show that the precision was93.4%, demonstrating that our method can de-termine the corresponding noun phrases cor-rectly.Moreover, we investigated the relation be-114Table 4: Spearman?s rank correlation coefficient for sentence-level adequacy.No.
1 No.
2 No.
3 No.
4 No.
5 No.
6 No.
7Our method 0.7456 0.5049 0.5837 0.5146 0.6514 0.6557 0.6746IMPACT 0.7336 0.4881 0.5992 0.4741 0.6382 0.5841 0.6409ROUGE-L 0.7304 0.4822 0.6092 0.4572 0.6135 0.5365 0.6368BLEU 0.5525 0.2206 0.4327 0.3449 0.3230 0.2805 0.4375NIST 0.5032 0.2438 0.4218 0.2489 0.2342 0.1534 0.3529NMG-WN 0.7541 0.3829 0.5579 0.4472 0.5560 0.5828 0.6263METEOR 0.4409 0.1509 0.4018 0.2580 0.3085 0.1991 0.4115WER 0.6566 0.4147 0.5478 0.4272 0.5524 0.4884 0.5539Our method II 0.7478 0.4972 0.5817 0.4892 0.6437 0.6428 0.6707BLEU with our method 0.6644 0.3926 0.5065 0.4522 0.4639 0.4715 0.5460No.
8 No.
9 No.
10 No.
11 No.
12 Avg.
AllOur method 0.7298 0.7258 0.5961 0.7633 0.6078 0.6461 0.6763IMPACT 0.6703 0.7067 0.5617 0.7411 0.5583 0.6164 0.6515ROUGE-L 0.6603 0.6983 0.5340 0.7280 0.5281 0.6012 0.6435BLEU 0.4571 0.5827 0.3220 0.4987 0.4302 0.4069 0.4227NIST 0.4255 0.4424 0.1313 0.2950 0.4785 0.3276 0.3062NMG-WN 0.6863 0.6524 0.6412 0.7015 0.5728 0.5968 0.5836METEOR 0.4242 0.4776 0.3335 0.2861 0.4455 0.3448 0.2887WER 0.6234 0.6480 0.5463 0.7131 0.5684 0.5617 0.4797Our method II 0.7287 0.7255 0.5936 0.7761 0.5798 0.6397 0.6699BLEU with our method 0.5850 0.6757 0.4596 0.6272 0.5452 0.5325 0.5474tween the correlation obtained by our methodand the quality of chunking.
In ?Our method?shown in Tables 2?5, noun phrases for whichsome erroneous results obtained using thechunking tool were revised.
?Our method II?of Tables 2?5 used noun phrases that weregiven as results obtained using the chunk-ing tool.
Underlining in ?Our method II?
ofTables 2?5 signifies that the differences be-tween correlation coefficients obtained usingour method II and IMPACT are statisticallysignificant at the 5% significance level.
Fun-damentally, in both ?Avg.?
and ?All?
of Ta-bles 2?5, the correlation coefficients of ourmethod II without the revised noun phrasesare lower than those of our method using therevised noun phrases.
However, the differencebetween our method and our method II in?Avg.?
and ?All?
of Tables 2?5 is not large.The performance of the chunking tool has nogreat influence on the results of our methodbecause scorewd in Eqs.
(3), (4), and (5) donot depend strongly on the performance ofthe chunking tool.
For example, in sentencesshown in Fig.
2, all common parts are thesame as the common parts of Fig.
2 when ?thecrowning fall?
in the MT output and ?crown-ing drop?
in the reference are not determinedas the noun phrases.
Other common parts aredetermined correctly because the weight of thecommon part ?the amount of?
is higher thanthose of other common parts by Eqs.
(1) and(2).
Consequently, the determination of thecommon parts except ?the amount of?
is notdifficult.In other language sentences, we already per-formed the experiments using Japanese sen-tences from Reuters articles(Oyamada et al,2010).
Results show that the correlation co-efficients of IMPACT with our method, forwhich IMPACT scores were used as scorewd inEq.
(13), were highest among some methods.Therefore, our method might not be language-dependent.
Nevertheless, experiments usingvarious language data are necessary to eluci-date this point.4 ConclusionAs described herein, we proposed a new auto-matic evaluation method for machine transla-115Table 5: Spearman?s rank correlation coefficient for sentence-level fluency.No.
1 No.
2 No.
3 No.
4 No.
5 No.
6 No.
7Our method 0.5697 0.3299 0.5446 0.4199 0.5733 0.5060 0.6459IMPACT 0.5481 0.3285 0.5572 0.3976 0.5960 0.4317 0.6334ROUGE-L 0.5470 0.3041 0.5646 0.3661 0.5638 0.3879 0.6255BLEU 0.4157 0.0559 0.4286 0.2018 0.4475 0.2569 0.4909NIST 0.4209 0.0185 0.4559 0.1093 0.3186 0.1898 0.3634NMG-WN 0.5569 0.3461 0.5381 0.4300 0.5052 0.5264 0.5328METEOR 0.4608 0.1429 0.4438 0.1783 0.4073 0.1596 0.4821WER 0.4469 0.2395 0.5087 0.3292 0.4995 0.3482 0.5637Our method II 0.5659 0.3216 0.5484 0.3773 0.5638 0.5211 0.6343BLEU with our method 0.5188 0.1534 0.4793 0.3005 0.5255 0.3942 0.5676No.
8 No.
9 No.
10 No.
11 No.
12 Avg.
AllOur method 0.5646 0.6617 0.3319 0.6256 0.4485 0.5185 0.5556IMPACT 0.5471 0.6454 0.3222 0.6319 0.4358 0.5062 0.5489ROUGE-L 0.5246 0.6428 0.2949 0.6159 0.3928 0.4858 0.5359BLEU 0.4882 0.5419 0.1407 0.4740 0.4176 0.3633 0.3971NIST 0.4150 0.4193 0.0889 0.3006 0.4752 0.2980 0.2994NMG-WN 0.5684 0.5850 0.4451 0.6502 0.4387 0.5102 0.5156METEOR 0.2911 0.4267 0.1735 0.3264 0.3512 0.3158 0.2886WER 0.5320 0.6505 0.3828 0.6501 0.4003 0.4626 0.4193Our method II 0.5609 0.6687 0.3629 0.6223 0.4384 0.5155 0.5531BLEU with our method 0.5470 0.6213 0.2184 0.5808 0.4870 0.4495 0.4825Table 6: Correlation coefficient for SMT and RBMT.Pearson?s correlation coefficient Spearman?s rank correlation coefficientAdequacy Fluency Adequacy FluencySMT RBMT SMT RBMT SMT RBMT SMT RBMTOur method 0.7054 0.5840 0.5477 0.5016 0.6710 0.5961 0.5254 0.5003IMPACT 0.6721 0.5650 0.5364 0.4960 0.6397 0.5811 0.5162 0.4951ROUGE-L 0.6560 0.5691 0.5179 0.4988 0.6225 0.5701 0.4942 0.4783NMG-WN 0.5958 0.5850 0.5201 0.4732 0.6129 0.5755 0.5238 0.4959tion.
Our method calculates the scores for MToutputs using noun-phrase chunking.
Conse-quently, the system obtains scores using thecorrectly matched words and phrase-level in-formation based on the corresponding nounphrases.
Experimental results demonstratethat our method yields the highest correlationamong eight methods in terms of sentence-level adequacy and fluency.Future studies will improve our method,enabling it to achieve high correlation insentence-level fluency.
Future studies will alsoinclude experiments using data of various lan-guages.AcknowledgementsThis work was done as research under theAAMT/JAPIO Special Interest Group onPatent Translation.
The Japan Patent In-formation Organization (JAPIO) and the Na-tional Institute of Informatics (NII) providedcorpora used in this work.
The author grate-fully acknowledges JAPIO and NII for theirsupport.
Moreover, this work was partiallysupported by Grants from the High-Tech Re-search Center of Hokkai-Gakuen Universityand the Kayamori Foundation of Informa-tional Science Advancement.116ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
ME-TEOR: An Automatic Metric for MT Eval-uation with Improved Correlation with Hu-man Judgments.
In Proc.
of ACL Workshopon Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summariza-tion, 65?72.Deborah Coughlin.
2003.
Correlating Automatedand Human Assessments of Machine TranslationQuality.
In Proc.
of MT Summit IX, 63?70.Hiroshi Echizen-ya and Kenji Araki.
2007.
Auto-matic Evaluation of Machine Translation basedon Recursive Acquisition of an Intuitive Com-mon Parts Continuum.
In Proc.
of MT SummitXII, 151?158.Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shi-mohata, Atsushi Fujii, Masao Utiyama, MikioYamamoto, Takehito Utsuro and Noriko Kando.2009.
Meta-Evaluation of Automatic EvaluationMethods for Machine Translation using PatentTranslation Data in NTCIR-7.
In Proc.
of the3rd Workshop on Patent Translation, 9?16.Terumasa Ehara.
2007.
Rule Based MachineTranslation Combined with Statistical Post Ed-itor for Japanese to English Patent Transla-tion.
In Proc.
of MT Summit XII Workshopon Patent Translation, 13?18.Atsushi Fujii, Masao Utiyama, Mikio Yamamotoand Takehito Utsuro.
2008.
Overview of thePatent Translation Task at the NTCIR-7 Work-shop.
In Proc.
of 7th NTCIR Workshop Meetingon Evaluation of Information Access Technolo-gies: Information Retrieval, Question Answer-ing and Cross-lingual Information Access, 389?400.Gregor Leusch, Nicola Ueffing and Hermann Ney.2003.
A Novel String-to-String Distance Mea-sure with Applications to Machine TranslationEvaluation.
In Proc.
of MT Summit IX, 240?247.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic Evaluation of Machine Translation Qual-ity Using Longest Common Subsequence andSkip-Bigram Statistics.
In Proc.
of ACL?04,606?613.Dennis N. Mehay and Chris Brew.
2007.BLEUA?TRE: Flattening Syntactic Dependen-cies for MT Evaluation.
In Proc.
of MT SummitXII, 122?131.Andrew Mutton, Mark Dras, Stephen Wan andRobert Dale.
2007.
GLEU: Automatic Eval-uation of Sentence-Level Fluency.
In Proc.
ofACL?07, 344?351.NIST.
2002.
Automatic Evaluationof Machine Translation Quality Us-ing N-gram Co-Occurrence Statistics.http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.Takashi Oyamada, Hiroshi Echizen-ya and KenjiAraki.
2010.
Automatic Evaluation of MachineTranslation Using both Words Information andComprehensive Phrases Information.
In IPSJSIG Technical Report, Vol.2010-NL-195, No.
3(in Japanese).Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu.
2002.
BLEU: a Method for Au-tomatic Evaluation of Machine Translation.
InProc.
of ACL?02, 311?318.Michael Pozar and Eugene Charniak.
2006.
Bllip:An Improved Evaluation Metric for MachineTranslation.
Brown University Master Thesis.Fei Sha and Fernando Pereira.
2003.
Shallow Pars-ing with Conditional Random Fields.
In Proc.of HLT-NAACL 2003, 134?141.Keh-Yih Su, Ming-Wen Wu and Jing-Shin Chang.1992.
A New Quantitative Quality Measure forMachine Translation Systems.
In Proc.
of GOL-ING?92, 433?439.Masao Utiyama and Hitoshi Isahara.
2003.
Re-liable Measures for Aligning Japanese?EnglishNews Articles and Sentences.
In Proc.
of theACL?03, pp.72?79.117
