Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 341?343,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsEngagement-based Multi-party Dialog with a Humanoid RobotDavid Klotz and Johannes Wienke and Julia Peltason and Britta Wrede and Sebastian WredeApplied Informatics GroupBielefeld University{dklotz, jwienke, jpeltaso, bwrede, swrede}@techfak.uni-bielefeld.deVasil Khalidov and Jean-Marc OdobezIDIAP Research Institute{vasil.khalidov, odobez}@idiap.chAbstractWhen a robot is situated in an environmentcontaining multiple possible interaction part-ners, it has to make decisions about when toengage specific users and how to detect andreact appropriately to actions of the users thatmight signal the intention to interact.In this demonstration we present the integra-tion of an engagement model in an existing di-alog system based on interaction patterns.
Asa sample scenario, this enables the humanoidrobot Nao to play a quiz game with multipleparticipants.1 IntroductionGiving robotic systems the ability to join in conver-sation with one or multiple users poses many newchallenges for the development of appropriate dia-log systems and models.
When a dialog system issituated in the real, physical world and used in moreopen settings, more effort needs to be spent on estab-lishing and maintaining clear communication chan-nels between the system and its users.
E.g.
the sys-tem first needs to detect that there are potential userswith whom interacting would be possible, it needs todecide if a detected person wants to interact with thesystem at all and it needs to make decisions whenand how it should try to start an interaction with thatperson.Bohus and Horvitz (2009) have developed amodel for representing the current relation of a userwith such a system (their engagement state) and de-termining if they want to be involved in an interac-tion with the system (using explicit engagement ac-tions and the more abstract engagement intention).Each user can be engaged in specific interactions(denoting different ?basic unit[s] of sustained, in-teractive problem-solving?)
and there can be multi-ple such interactions, each with potentially differentusers.This demonstration shows how an engagementmodel inspired by these ideas was integrated intoan existing dialog system and how it helps in real-izing interactive scenarios with a robot that incorpo-rate cues for the dialog from the system?s environ-ment.
Section 3 gives more details about this modeland how it is used by the dialog.2 ScenarioAs a scenario for this demonstration we chose a sim-ple quiz game involving the robot Nao as a host play-ing with one or multiple human users.
At first, therobot waits until one of the human interaction part-ners approaches.
When the person opens the interac-tion (i.e.
by greeting the robot), the system respondswith an appropriate greeting.
While the person con-tinues to show the intention to interact with the robot(determined by the process described in section 3.1),the robot will ask questions randomly chosen froma predefined set and will try to judge if the personanswered them correctly.When another person enters the robot?s field ofview, the system also tries to determine if they havethe intention to interact with it.
If that is the case, thesystem suspends the current interaction with the firstperson and actively tries to engage the second per-son, encouraging him or her to join the ongoing quizgame.
The prospective new player can then choose341Figure 1: Two persons interacting with the developed system.to join or decline the request.As long as one of the engaged participants showsthe intention to interact, the robot continues to askquestions which all participants can try to answer.The quiz game is stopped either by an explicit re-quest of one the users or after all participants haveleft the scene.This scenario serves as a good testbed for the in-tegration of different cues for the engagement modeland how that model affects the actions taken by thedialog system.
The right-hand side of figure 1 showstwo people interacting with the robot during the quizgame.3 System OverviewFigure 2 shows an overview of the different com-ponents involved in the demonstrated system.
Thisincludes components for the perception (e.g.
access-ing images from the robot?s camera and audio fromits microphones), for generating actions (e.g.
usingthe robot?s text-to-speech system), the dialog systemitself and a memory system for connecting these di-verse components.The dialog system used for this demonstrationis called PaMini, which is short for ?Pattern-basedMixed-Initiative human-robot Interaction?
and isdescribed in more detail by Peltason and Wrede(2010).
This dialog system was modified in Klotz(2010) with a model of engagement based on theideas presented by Bohus and Horvitz (2009).
Inour adaptation of this model, there are extensionpoints for integrating different sources of informa-tion about the user?s engagement intentions and ac-tions, described in the following section.3.1 Determining the User?s Actions &IntentionFor determining the user?s actions (e.g.
if the userexplicitly wants to start an interaction with the sys-tem), this demonstration uses a set of possible utter-ances which are simply matched against the resultsof a speech recognition module.To get an estimation of the user?s intention to in-teract, the image from the robot?s camera is first usedto detect the faces of users and to estimate their cur-rent visual focus of attention.
A module based ona framework by Ba and Odobez (2009) is used todetermine probabilities that the user is looking ateach of a pre-defined list of possible focus targets,including the robot itself and other users visible inthe scene.
The upper left of figure 1 shows a visu-alization of this module?s output.
Nao denotes therobot as the focus target with the highest probabil-ity, while the designation UN is short for the ?unfo-cused?
target.This list of probabilities is then stored in a mem-342Figure 2: Components of the developed system.ory system developed by Wienke and Wrede (2011).The memory system provides temporal query capa-bilities which are finally used to guess a user?s cur-rent intention of interacting with the robot based onthe history of the probabilities that the robot was theuser?s current visual focus of attention target.
Thisresult is also stored in the memory system togetherwill all other information known about a user.3.2 Engagement Cues for the DialogThe dialog system receives the information about theuser?s state and intention from the memory systemand uses it in several rules for controlling its own en-gagement actions.
The intention is e.g.
used to deter-mine if there is a new user that should be persuadedto join the quiz game described in section 2 and ifany of the users still shows interest so that a newquestion should be asked.
The general state of thedetected users is also used e.g.
to observe when theusers leave the robot?s field of view for a longer pe-riod of time which causes the dialog system to closeits current interaction.4 ConclusionWe have shown how an existing dialog system thatwas enhanced using an explicit model of engage-ment can be used to realize interactive scenarioswith a robot that is situated in the physical world.An estimation of the user?s current visual focus ofattention is used to gauge their intention to engagethe robot in conversation.A video recording of two people interacting withthe developed system is available online at http://youtu.be/pWZLVF2Xa8gAcknowledgmentsThis work was done in the HUMAVIPS project,funded by the European Commission SeventhFramework Programme, Theme Cognitive Systemsand Robotics, Grant agreement no.
247525.ReferencesS.
Ba and J.-M. Odobez.
2009.
Recognizing Visual Fo-cus of Attention from Head Pose in Natural Meetings.IEEE Trans.
on System, Man and Cybernetics: part B,Cybernetics, 39:16?34.Dan Bohus and Eric Horvitz.
2009.
Models for multi-party engagement in open-world dialog.
In Proceed-ings of the SIGDIAL 2009 Conference, pages 225?234,London, UK.
Association for Computational Linguis-tics.David Klotz.
2010.
Modeling engagement in a multi-party human-robot dialog.
Master?s thesis, BielefeldUniversity.Julia Peltason and Britta Wrede.
2010.
Modelinghuman-robot interaction based on generic interactionpatterns.
In AAAI Fall Symposium: Dialog withRobots, Arlington, VA, USA.Johannes Wienke and Sebastian Wrede.
2011.
A spatio-temporal working memory for multi-level data fusion.In Proc.
of IEEE/RSJ International Conference on In-telligent Robots and Systems.
submitted.343
