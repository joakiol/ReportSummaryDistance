Alternative Approaches for Generating Bodies of Grammar RulesGabriel Infante-Lopez and Maarten de RijkeInformatics Institute, University of Amsterdam{infante,mdr}@science.uva.nlAbstractWe compare two approaches for describing and gen-erating bodies of rules used for natural languageparsing.
In today?s parsers rule bodies do not ex-ist a priori but are generated on the fly, usually withmethods based on n-grams, which are one particu-lar way of inducing probabilistic regular languages.We compare two approaches for inducing such lan-guages.
One is based on n-grams, the other on min-imization of the Kullback-Leibler divergence.
Theinferred regular languages are used for generatingbodies of rules inside a parsing procedure.
We com-pare the two approaches along two dimensions: thequality of the probabilistic regular language theyproduce, and the performance of the parser theywere used to build.
The second approach outper-forms the first one along both dimensions.1 IntroductionN -grams have had a big impact on the state of theart in natural language parsing.
They are centralto many parsing models (Charniak, 1997; Collins,1997, 2000; Eisner, 1996), and despite their sim-plicity n-gram models have been very successful.Modeling with n-grams is an induction task (Gold,1967).
Given a sample set of strings, the task is toguess the grammar that produced that sample.
Usu-ally, the grammar is not be chosen from an arbitraryset of possible grammars, but from a given class.Hence, grammar induction consists of two parts:choosing the class of languages amongst which tosearch and designing the procedure for performingthe search.
By using n-grams for grammar induc-tion one addresses the two parts in one go.
In par-ticular, the use of n-grams implies that the solu-tion will be searched for in the class of probabilis-tic regular languages, since n-grams induce prob-abilistic automata and, consequently, probabilisticregular languages.
However, the class of probabilis-tic regular languages induced using n-grams is aproper subclass of the class of all probabilistic reg-ular languages; n-grams are incapable of capturinglong-distance relations between words.
At the tech-nical level the restricted nature of n-grams is wit-nessed by the special structure of the automata in-duced from them, as we will see in Section 4.2.N -grams are not the only way to induce regularlanguages, and not the most powerful way to do so.There is a variety of general methods capable of in-ducing all regular languages (Denis, 2001; Carrascoand Oncina, 1994; Thollard et al, 2000).
What istheir relevance for natural language parsing?
Re-call that regular languages are used for describingthe bodies of rules in a grammar.
Consequently, thequality and expressive power of the resulting gram-mar is tied to the quality and expressive power of theregular languages used to describe them.
And thequality and expressive power of the latter, in turn,are influenced directly by the method used to inducethem.
These observations give rise to a natural ques-tion: can we gain anything in parsing from usinggeneral methods for inducing regular languages in-stead of methods based on n-grams?
Specifically,can we describe the bodies of grammatical rulesmore accurately and more concisely by using gen-eral methods for inducing regular languages?In the context of natural language parsing wepresent an empirical comparison between algo-rithms for inducing regular languages using n-grams on the one hand, and more general algorithmsfor learning the general class of regular language onthe other hand.
We proceed as follows.
We gen-erate our training data from the Wall Street JournalSection of the Penn Tree Bank (PTB), by transform-ing it to projective dependency structures, following(Collins, 1996), and extracting rules from the result.These rules are used as training material for the ruleinduction algorithms we consider.
The automataproduced this way are then used to build grammarswhich, in turn, are used for parsing.We are interested in two different aspects of theuse of probabilistic regular languages for naturallanguage parsing: the quality of the induced au-tomata and the performance of the resulting parsers.For evaluation purposes, we use two different met-rics: perplexity for the first aspect and percentageof correct attachments for the second.
The main re-sults of the paper are that, measured in terms of per-plexity, the automata induced by algorithms otherthan n-grams describe the rule bodies better thanautomata induced using n-gram-based algorithms,and that, moreover, the gain in automata qualityis reflected by an improvement in parsing perfor-mance.
We also find that the parsing performanceof both methods (n-grams vs. general automata) canbe substantially improved by splitting the trainingmaterial into POS categories.
As a side product,we find empirical evidence to suggest that the effec-tiveness of rule lexicalization techniques (Collins,1997; Sima?an, 2000) and parent annotation tech-niques (Klein and Manning, 2003) is due to the factthat both lead to a reduction in perplexity in the au-tomata induced from training corpora.Section 2 surveys our experiments, and later sec-tions provide details of the various aspects.
Sec-tion 3 offers details on our grammatical frame-work, PCW-grammars, on transforming automatato PCW-grammars, and on parsing with PCW-grammars.
Section 4 explains the starting point ofthis process: learning automata, and Section 5 re-ports on parsing experiments.
We discuss relatedwork in Section 6 and conclude in Section 7.2 OverviewWe want to build grammars using different algo-rithms for inducing their rules.
Our main questionis aimed at understanding how different algorithmsfor inducing regular languages impact the parsingperformance with those grammars.
A second issuethat we want to explore is how the grammars per-form when the quality of the training material is im-proved, that is, when the training material is sep-arated into part of speech (POS) categories beforethe regular language learning algorithms are run.We first transform the PTB into projective depen-dencies structures following (Collins, 1996).
Fromthe resulting tree bank we delete all lexical informa-tion except POS tags.
Every POS in a tree belongingto the tree-bank has associated to it two different,possibly empty, sequences of right and left depen-dents, respectively.
We extract all these sequencesfor all trees, producing two different sets containingright and left sequences of dependents respectively.These two sets form the training material used forbuilding four different grammars.
The four gram-mars differ along two dimensions: the number ofautomata used for building them and the algorithmused for inducing the automata.
As to the latter di-mension, in Section 4 we use two algorithms: theMinimum Discriminative Information (MDI) algo-rithm, and a bigram-based algorithm.
As to the for-mer dimension, two of the grammars are built us-ing only two different automata, each of which isbuilt using the two sample set generated from thePTB.
The other two grammars were built using twoautomata per POS, exploiting a split of the train-ing samples into multiple samples, two samples perPOS, to be precise, each containing only those sam-ples where the POS appeared as the head.The grammars built from the induced automataare so-called PCW-grammars (see Section 3), a for-malism based on probabilistic context free gram-mars (PCFGs); as we will see in Section 3, inferringthem from automata is almost immediate.3 Grammatical FrameworkWe briefly detail the grammars we work with(PCW-grammars), how automata give rise to thesegrammars, and how we parse using them.3.1 PCW-GrammarsWe need a grammatical framework that modelsrule bodies as instances of a regular language andthat allows us to transform automata to gram-mars as directly as possible.
We decided to em-bed them in the general grammatical framework ofCW-grammars (Infante-Lopez and de Rijke, 2003):based on PCFGs, they have a clear and well-understood mathematical background and we do notneed to implement ad-hoc parsing algorithms.A probabilistic constrained W-grammar (PCW-grammar) consists of two different sets of PCF-likerules called pseudo-rules and meta-rules respec-tively and three pairwise disjoint sets of symbols:variables, non-terminals and terminals.
Pseudo-rules and meta-rules provide mechanisms for build-ing ?real?
rewrite rules.
We use ?
w=?
?
to indicatethat ?
should be rewritten as ?.
In the case of PCW-grammars, rewrite rules are built by first selecting apseudo-rule, and then using meta-rules for instanti-ating all the variables in the body of the pseudo-rule.To illustrate these concepts, we provide an exam-ple.
Let W = (V,NT, T, S, m?
?, s??)
be a CW-grammar such that the set of variable, non-terminalsmeta-rules pseudo-rulesAdj m?
?0.5 AdjAdj S s?
?1 AdjNounAdj m?
?0.5 Adj Adj s?
?0.1 bigNoun s?
?1 ball...and terminals are defined as follows: V = {Adj },NT = {S, Adj , Noun}, T = {ball , big , fat ,red , green , .
.
.}.
As usual, the numbers attachedto the arrows indicate the probabilities of the rules.The rules defined by W have the following shape:S w=?
Adj ?
Noun .
Suppose now that we want tobuild the rule S w=?
Adj Adj Noun .
We take thepseudo-rule S s?
?1 Adj Noun and instantiate thevariable Adj with Adj Adj to get the desired rule.The probability for it is 1 ?
0.5 ?
0.5, that is, theprobability of the derivation for Adj Adj times theprobability of the pseudo-rule used.
Trees for thisparticular grammar are flat, with a main node S andall the adjectives in it as daughters.
An examplederivation is given in Figure 1(a).3.2 From Automata to GrammarsNow that we have introduced PCW-grammars, wedescribe how we build them from the automatathat we are going to induce in Section 4.
Sincewe will induce two families of automata (?Many-Automata?
where we use two automata per POS,and ?One-Automaton?
where we use only two au-tomata to fit every POS), we need to describe twoautomata-to-grammar transformations.Let?s start with the case where we build two au-tomata per POS.
Let w be a POS in the PTB; let AwLand AwR be the two automata associated to it.
Let GwLand GwR be the PCFGs equivalent to AwL and AwR, re-spectively, following (Abney et al, 1999), and letSwL and SwR be the starting symbols of GwL and GwR,respectively.
We build our final grammar G withstarting symbol S, by defining its meta-rules as thedisjoint union of all rules in GwL and GwR (for all POSw), its set of pseudo-rules as the union of the sets{W s?
?1 SwLwSwR and Ss?
?1 SwLwSwR}, whereW is a unique new variable symbol associated to w.When we use two automata for all parts ofspeech, the grammar is defined as follows.
Let ALand AR be the two automata learned.
Let GL andGR be the PCFGs equivalent to AL and AR, and letSL and SR be the starting symbols of GL and GR,respectively.
Fix a POS w in the PTB.
Since the au-tomata are deterministic, there exist states SwL andSwR that are reachable from SL and SR, respectively,by following the arc labeled with w. Define a gram-mar as in the previous case.
Its starting symbol is S,its set of meta-rules is the disjoint union of all rulesin GwL and GwR (for all POS w), its set of pseudo-rules is {W s?
?1 SwLwSwR , Ss?
?1 SwLwSwR :w is a POS in the PTB and W is a unique new vari-able symbol associated to w}.3.3 Parsing PCW-GrammarsParsing PCW-grammars requires two steps: ageneration-rule step followed by a tree-buildingstep.
We now explain how these two steps can becarried out in one go.
Parsing with PCW-grammarscan be viewed as parsing with PCF grammars.
Themain difference is that in PCW-parsing derivationsfor variables remain hidden in the final tree.
To clar-ify this, consider the trees depicted in Figure 1; thetree in part (a) is the CW-tree corresponding to theword red big green ball, and the tree in part (b) isthe same tree but now the instantiations of the meta-rules that were used have been made visible.SAdjredAdjbigAdjgreenNounballSAdj 1Adj 1Adj 1AdjredAdjbigAdjgreenNounball(a) (b)Figure 1: (a) A tree generated by W .
(b) The sametree with meta-rule derivations made visible.To adapt a PCFG to parse CW-grammars, weneed to define a PCF grammar for a given PCW-grammar by adding the two sets of rules while mak-ing sure that all meta-rules have been marked some-how.
In Figure 1(b) the head symbols of meta-ruleshave been marked with the superscript 1.
After pars-ing the sentence with the PCF parser, all markedrules should be collapsed as shown in part (a).4 Building AutomataThe four grammars we intend to induce are com-pletely defined once the underlying automata havebeen built.
We now explain how we build those au-tomata from the training material.
We start by de-tailing how the material is generated.4.1 Building the Sample SetsWe transform the PTB, sections 2?22, to depen-dency structures, as suggested by (Collins, 1999).All sentences containing CC tags are filtered out,following (Eisner, 1996).
We also eliminate allword information, leaving only POS tags.
For eachresulting dependency tree we extract a sample set ofright and left sequences of dependents as shown inFigure 2.
From the tree we generate a sample setwith all right sequences of dependents {, , }, andanother with all left sequences {, , red big green}.The sample set used for automata induction is theunion of all individual tree sample sets.4.2 Learning Probabilistic AutomataProbabilistic deterministic finite state automata(PDFA) inference is the problem of inducing astochastic regular grammar from a sample set ofstrings belonging to an unknown regular language.The most direct approach for solving the task is bySJJjjredJJjjbigJJjjgreennnballballgreenbigred(a) (b)jj jj nnleft right left right left right    red big green (c)Figure 2: (a), (b) Dependency representations ofFigure 1.
(c) Sample instances extracted from thistree.using n-grams.
The n-gram induction algorithmadds a state to the resulting automaton for each se-quence of symbols of length n it has seen in thetraining material; it also adds an arc between statesa?
and ?b labeled b, if the sequence a?b appearsin the training set.
The probability assigned to thearc (a?, ?b) is proportional to the number of timesthe sequence a?b appears in the training set.
For theremainder, we take n-grams to be bigrams.There are other approaches to inducing regulargrammars besides ones based on n-grams.
The firstalgorithm to learn PDFAs was ALERGIA (Carrascoand Oncina, 1994); it learns cyclic automata withthe so-called state-merging method.
The MinimumDiscrimination Information (MDI) algorithm (Thol-lard et al, 2000) improves over ALERGIA and usesKullback-Leibler divergence for deciding when tomerge states.
We opted for the MDI algorithm asan alternative to n-gram based induction algorithms,mainly because their working principles are rad-ically different from the n-gram-based algorithm.The MDI algorithm first builds an automaton thatonly accepts the strings in the sample set by merg-ing common prefixes, thus producing a tree-shapedautomaton in which each transition has a probabilityproportional to the number of times it is used whilegenerating the positive sample.The MDI algorithm traverses the lattice of allpossible partitions for this general automaton, at-tempting to merge states that satisfy a trade-off thatcan be specified by the user.
Specifically, assumethat A1 is a temporary solution of the algorithmand that A2 is a tentative new solution derived fromA1.
?
(A1, A2) = D(A0||A2) ?
D(A0||A1) de-notes the divergence increment while going fromA1 to A2, where D(A0||Ai) is the Kullback-Leiblerdivergence or relative entropy between the twodistributions generated by the corresponding au-tomata (Cover and Thomas, 1991).
The new solu-tion A2 is compatible with the training data if thedivergence increment relative to the size reduction,that is, the reduction of the number of states, is smallenough.
Formally, let alha denote a compatibil-ity threshold; then the compatibility is satisfied if?
(A1,A2)|A1|?|A2| < alpha.
For this learning algorithm,alpha is the unique parameter; we tuned it to getbetter quality automata.4.3 Optimizing AutomataWe use three measures to evaluate the quality ofa probabilistic automaton (and set the value ofalpha optimally).
The first, called test sampleperplexity (PP), is based on the per symbol log-likelihood of strings x belonging to a test sam-ple according to the distribution defined by the au-tomaton.
Formally, LL = ?
1|S|?x?S log (P (x)),where P (x) is the probability assigned to the stringx by the automata.
The perplexity PP is defined asPP = 2LL.
The minimal perplexity PP = 1 isreached when the next symbol is always predictedwith probability 1 from the current state, whilePP = |?| corresponds to uniformly guessing froman alphabet of size |?|.The second measure we used to evaluate the qual-ity of an automaton is the number of missed samples(MS).
A missed sample is a string in the test sam-ple that the automaton failed to accept.
One suchinstance suffices to have PP undefined (LL infinite).Since an undefined value of PP only witnesses thepresence of at least one MS we decided to count thenumber of MS separately, and compute PP withouttaking MS into account.
This choice leads to a moreaccurate value of PP, while, moreover, the value ofMS provides us with information about the general-ization capacity of automata: the lower the value ofMS, the larger the generalization capacities of theautomaton.
The usual way to circumvent undefinedperplexity is to smooth the resulting automaton withunigrams, thus increasing the generalization capac-ity of the automaton, which is usually paid for withan increase in perplexity.
We decided not to useany smoothing techniques as we want to comparebigram-based automata with MDI-based automatain the cleanest possible way.
The PP and MS mea-sures are relative to a test sample; we transformedsection 00 of the PTB to obtain one.11If smoothing techniques are used for optimizing automatabased on n-grams, they should also be used for optimizingMDI-based automata.
A fair experiment for comparing thetwo automata-learning algorithms using smoothing techniqueswould consist of first building two pairs of automata.
The firstpair would consist of the unigram-based automaton togetherThe third measure we used to evaluate the qualityof automata concerns the size of the automata.
Wecompute NumEdges and NumStates (the number ofedges and the number of states of the automaton).We used PP, US, NumEdges, and NumStates tocompare automata.
We say that one automaton is ofa better quality than another if the values of the 4indicators are lower for the first than for the sec-ond.
Our aim is to find a value of alpha thatproduces an automaton of better quality than thebigram-based counterpart.
By exhaustive search,using all training data, we determined the optimalvalue of alpha.
We selected the value of alphafor which the MDI-based automaton outperformsthe bigram-based one.2We exemplify our procedure by considering au-tomata for the ?One-Automaton?
setting (where weused the same automata for all parts of speech).
InFigure 3 we plot all values of PP and MS computedfor different values of alpha, for each training set(i.e., left and right).
From the plots we can identifyvalues of alpha that produce automata having bet-ter values of PP and MS than the bigram-based ones.All such alphas are the ones inside the markedareas; automata induced using those alphas pos-sess a lower value of PP as well as a smaller num-ber of MS, as required.
Based on these explorationsMDI BigramsRight Left Right LeftNumEdges 268 328 20519 16473NumStates 12 15 844 755Table 1: Automata sizes for the ?One-Automaton?case, with alpha = 0.0001.we selected alpha = 0.0001 for building the au-tomata used for grammar induction in the ?One-Automaton?
case.
Besides having lower values ofPP and MS, the resulting automata are smaller thanthe bigram based automata (Table 1).
MDI com-presses information better; the values in the tableswith an MDI-based automaton outperforming the unigram-based one.
The second one, a bigram-based automata togetherwith an MDI-based automata outperforming the bigram-basedone.
Second, the two n-gram based automata smoothed into asingle automaton have to be compared against the two MDI-based automata smoothed into a single automaton.
It wouldbe hard to determine whether the differences between the finalautomata are due to smoothing procedure or to the algorithmsused for creating the initial automata.
By leaving smoothingout of the picture, we obtain a clearer understanding of the dif-ferences between the two automata induction algorithms.2An equivalent value of alpha can be obtained indepen-dently of the performance of the bigram-based automata bydefining a measure that combines PP and MS.
This measureshould reach its maximum when PP and MS reach their mini-mums.suggest that MDI finds more regularities in the sam-ple set than the bigram-based algorithm.To determine optimal values for the ?Many-Automata?
case (where we learned two automatafor each POS) we used the same procedure asfor the ?One-Automaton?
case, but now for ev-ery individual POS.
Because of space constraintswe are not able to reproduce analogues of Fig-ure 3 and Table 1 for all parts of speech.
Figure 4contains representative plots; the remaining plotsare available online at http://www.science.uva.nl/?infante/POS.Besides allowing us to find the optimal alphas,the plots provide us with a great deal of informa-tion.
For instance, there are two remarkable thingsin the plots for VBP (Figure 4, second row).
First,it is one of the few examples where the bigram-based algorithm performs better than the MDI al-gorithm.
Second, the values of PP in this plot arerelatively high and unstable compared to other POSplots.
Lower perplexity usually implies better qual-ity automata, and as we will see in the next section,better automata produce better parsers.
How can weobtain lower PP values for the VBP automata?
Theclass of words tagged with VBP harbors many dif-ferent behaviors, which is not surprising, given thatverbs can differ widely in terms of, e.g., their sub-categorization frames.
One way to decrease the PPvalues is to split the class of words tagged with VBPinto multiple, more homogeneous classes.
Notefrom Figures 3 and 4 that splitting the original sam-ple sets into POS-dependent sets produces a hugedecrease on PP.
One attempt to implement this ideais lexicalization: increasing the information in thePOS tag by adding the lemma to it (Collins, 1997;Sima?an, 2000).
Lexicalization splits the class ofverbs into a family of singletons producing more ho-mogeneous classes, as desired.
A different approach(Klein and Manning, 2003) consists in adding headinformation to dependents; words tagged with VBPare then split into classes according to the words thatdominate them in the training corpus.Some POS present very high perplexities, buttags such as DT present a PP close to 1 (and 0 MS)for all values of alpha.
Hence, there is no needto introduce further distinctions in DT, doing so willnot increase the quality of the automata but will in-crease their number; splitting techniques are boundto add noise to the resulting grammars.
The plotsalso indicate that the bigram-based algorithm cap-tures them as well as the MDI algorithm.In Figure 4, third row, we see that the MDI-basedautomata and the bigram-based automata achievethe same value of PP (close to 5) for NN, but05101520255e-05  0.0001  0.00015  0.0002  0.00025  0.0003  0.00035  0.0004AlphaUnique Automaton - Left SideMDI Perplex.
(PP)Bigram Perplex.
(PP)MDI Missed Samples (MS)Bigram Missed Samples (MS)0510152025305e-05  0.0001  0.00015  0.0002  0.00025  0.0003  0.00035  0.0004AlphaUnique Automaton - Right SideMDI Perplex.
(PP)Bigram Perplex.
(PP)MDI Missed Samples (MS)Bigram Missed Samples (MS)Figure 3: Values of PP and MS for automata used in building One-Automaton grammars.
(X-axis): alpha.
(Y-axis): missed samples (MS) and perplexity (PP).
The two constant lines represent the values of PP andMS for the bigram-based automata.34567890.0e+002.0e-054.0e-056.0e-058.0e-051.0e-041.2e-041.4e-041.6e-041.8e-042.0e-04AlphaVBP - LeftSideMDI Perplex.
(PP)Bigram Perplex.
(PP)MDI Missed Samples (MS)Bigram Missed Samples (MS)34567890.0e+002.0e-054.0e-056.0e-058.0e-051.0e-041.2e-041.4e-041.6e-041.8e-042.0e-04AlphaVBP - LeftSideMDI Perplex.
(PP)Bigram Perplex.
(PP)MDI Missed Samples (MS)Bigram Missed Samples (MS)0510152025300.0e+002.0e-054.0e-056.0e-058.0e-051.0e-041.2e-041.4e-041.6e-041.8e-042.0e-04AlphaNN - LeftSideMDI Perplex.
(PP)Bigram Perplex.
(PP)MDI Missed Samples (MS)Bigram Missed Samples (MS)0510152025300.0e+002.0e-054.0e-056.0e-058.0e-051.0e-041.2e-041.4e-041.6e-041.8e-042.0e-04AlphaNN - RightSideMDI Perplex.
(PP)Bigram Perplex.
(PP)MDI Missed Samples (MS)Bigram Missed Samples (MS)Figure 4: Values of PP and MS for automata for ad-hoc automatathe MDI misses fewer examples for alphas big-ger than 1.4e ?
04.
As pointed out, we built theOne-Automaton-MDI using alpha = 0.0001 andeven though the method allows us to fine-tune eachalpha in the Many-Automata-MDI grammar, weused a fixed alpha = 0.0002 for all parts of speech,which, for most parts of speech, produces better au-tomata than bigrams.
Table 2 lists the sizes of theautomata.
The differences between MDI-based andbigram-based automata are not as dramatic as inthe ?One-Automaton?
case (Table 1), but the formeragain have consistently lower NumEdges and Num-States values, for all parts of speech, even wherebigram-based automata have a lower perplexity.MDI BigramsPOS Right Left Right LeftDT NumEdges 21 14 35 39NumStates 4 3 25 17VBP NumEdges 300 204 2596 1311NumStates 50 45 250 149NN NumEdges 104 111 3827 4709NumStates 6 4 284 326Table 2: Automata sizes for the three parts of speechin the ?Many-Automata?
case, with alpha =0.0002 for parts of speech.5 Parsing the PTBWe have observed remarkable differences in qualitybetween MDI-based and bigram-based automata.Next, we present the parsing scores, and discuss themeaning of the measures observed for automata inthe context of the grammars they produce.
The mea-sure that translates directly from automata to gram-mars is automaton size.
Since each automaton istransformed into a PCFG, the number of rules inthe resulting grammar is proportional to the numberof arcs in the automaton, and the number of non-terminals is proportional to the number of states.From Table 3 we see that MDI compresses informa-tion better: the sizes of the grammars produced bythe MDI-based automata are an order of magnitudesmaller that those produced using bigram-based au-tomata.
Moreover, the ?One-Automaton?
versionssubstantially reduce the size of the resulting gram-mars; this is obviously due to the fact that all POSshare the same underlying automaton so that infor-mation does not need to be duplicated across partsof speech.
To understand the meaning of PP andOne Automaton Many AutomataMDI Bigram MDI Bigram702 38670 5316 68394Table 3: Number of rules in the grammars built.MS in the context of grammars it helps to think ofPCW-parsing as a two-phase procedure.
The firstphase consists of creating the rules that will be usedin the second phase.
And the second phase con-sists in using the rules created in the first phase as aPCFG and parsing the sentence using a PCF parser.Since regular expressions are used to build rules, thevalues of PP and MS quantify the quality of the setof rules built for the second phase: MS gives us ameasure of the number rule bodies that should becreated but that will not be created, and, hence, itgives us a measure of the number of ?correct?
treesthat will not be produced.
PP tells us how uncertainthe first phase is about producing rules.Finally, we report on the parsing accuracy.
Weuse two measures, the first one (%Words) was pro-posed by Lin (1995) and was the one reported in(Eisner, 1996).
Lin?s measure computes the frac-tion of words that have been attached to the rightword.
The second one (%POS) marks as correct aword attachment if, and only if, the POS tag of thehead is the same as that of the right head, i.e., theword was attached to the correct word-class, eventhough the word is not the correct one in the sen-tence.
Clearly, the second measure is always higherthan the first one.
The two measures try to cap-ture the performance of the PCW-parser in the twophases described above: (%POS) tries to capturethe performance in the first phase, and (%Words) inthe second phase.
The measures reported in Table 4are the mean values of (%POS) and (%Words) com-puted over all sentences in section 23 having lengthat most 20.
We parsed only those sentences becausethe resulting grammars for bigrams are too big:parsing all sentences without any serious pruningtechniques was simply not feasible.
From Table 4MDI Bigrams%Words %POS %Words %POSOne-Aut.
0.69 0.73 0.59 0.63Many-Aut.
0.85 0.88 0.73 0.76Table 4: Parsing results for the PTBwe see that the grammars induced with MDI out-perform the grammars created with bigrams.
More-over, the grammar using different automata per POSoutperforms the ones built using only a single au-tomaton per side (left or right).
The results suggestthat an increase in quality of the automata has a di-rect impact on the parsing performance.6 Related Work and DiscussionModeling rule bodies is a key component of parsers.N -grams have been used extensively for this pur-pose (Collins 1996, 1997; Eisner, 1996).
In theseformalisms the generative process is not consideredin terms of probabilistic regular languages.
Con-sidering them as such (like we do) has two ad-vantages.
First, a vast area of research for induc-ing regular languages (Carrasco and Oncina, 1994;Thollard et al, 2000; Dupont and Chase, 1998)comes in sight.
Second, the parsing device itself canbe viewed under a unifying grammatical paradigmlike PCW-grammars (Chastellier and Colmerauer,1969; Infante-Lopez and de Rijke, 2003).
As PCW-grammars are PCFGs plus post tree transformations,properties of PCFGs hold for them too (Booth andThompson, 1973).In our comparison we optimized the value ofalpha, but we did not optimize the n-grams, asdoing so would mean two different things.
First,smoothing techniques would have to be used tocombine different order n-grams.
To be fair, wewould also have to smooth different MDI-based au-tomata, which would leave us in the same point.Second, the degree of the n-gram.
We opted forn = 2 as it seems the right balance of informative-ness and generalization.
N -grams are used to modelsequences of arguments, and these hardly ever havelength > 3, making higher degrees useless.
To makea fair comparison for the Many-Automata grammarswe did not tune the MDI-based automata individu-ally, but we picked a unique alpha.MDI presents a way to compact rule informa-tion on the PTB; of course, other approaches exists.In particular, Krotov et al (1998) try to induce aCW-grammar from the PTB with the underlying as-sumption that some derivations that were supposedto be hidden were left visible.
The attempt to usealgorithms other than n-grams-based for inducingof regular languages in the context of grammar in-duction is not new; for example, Kruijff (2003) usesprofile hidden models in an attempt to quantify freeorder variations across languages; we are not awareof evaluations of his grammars as parsing devices.7 Conclusions and Future WorkOur experiments support two kinds of conclusions.First, modeling rules with algorithms other thann-grams not only produces smaller grammars butalso better performing ones.
Second, the proce-dure used for optimizing alpha reveals that somePOS behave almost deterministically for selectingtheir arguments, while others do not.
These find-ings suggests that splitting classes that behave non-deterministically into homogeneous ones could im-prove the quality of the inferred automata.
We sawthat lexicalization and head-annotation seem to at-tack this problem.
Obvious questions for futurework arise: Are these two techniques the best way tosplit non-homogeneous classes into homogeneousones?
Is there an optimal splitting?AcknowledgmentsWe thank our referees for valuable comments.
Bothauthors were supported by the Netherlands Organi-zation for Scientific Research (NWO) under projectnumber 220-80-001.
De Rijke was also supportedby grants from NWO, under project numbers 365-20-005, 612.069.006, 612.000.106, 612.000.207,and 612.066.302.ReferencesS.
Abney, D. McAllester, and F. Pereira.
1999.
Relatingprobabilistic grammars and automata.
In Proc.
37thAnnual Meeting of the ACL, pages 542?549.T.
Booth and R. Thompson.
1973.
Applying probabilitymeasures to abstract languages.
IEEE Transaction onComputers, C-33(5):442?450.R.
Carrasco and J. Oncina.
1994.
Learning stochasticregular grammars by means of state merging method.In Proc.
ICGI-94, Springer, pages 139?150.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proc.
14th Nat.Conf.
on Artificial Intelligence, pages 598?603.G.
Chastellier and A. Colmerauer.
1969.
W-grammar.In Proc.
1969 24th National Conf., pages 511?518.M.
Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proc.
34th AnnualMeeting of the ACL, pages 184?191.M.
Collins.
1997.
Three generative, lexicalized modelsfor statistical parsing.
In Proc.
35th Annual Meetingof the ACL and 8th Conf.
of the EACL, pages 16?23.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania, PA.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proc.
ICML-2000, Stanford, Ca.T.
Cover and J. Thomas.
1991.
Elements of InformationTheory.
Jonh Wiley and Sons, New York.F.
Denis.
2001.
Learning regular languages from simplepositive examples.
Machine Learning, 44(1/2):37?66.P.
Dupont and L. Chase.
1998.
Using symbol cluster-ing to improve probabilistic automaton inference.
InProc.
ICGI-98, pages 232?243.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
COLING-96, pages 340?245, Copenhagen, Denmark.J.
Eisner.
2000.
Bilexical grammars and their cubic-timeparsing algorithms.
In Advances in Probabilistic andOther Parsing Technologies, pages 29?62.
Kluwer.E.
M. Gold.
1967.
Language identification in the limit.Information and Control, 10:447?474.G.
Infante-Lopez and M. de Rijke.
2003.
Natural lan-guage parsing with W-grammars.
In Proc.
CLIN2003.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Proc.
41st Annual Meeting of the ACL.A.
Krotov, M. Hepple, R.J. Gaizauskas, and Y. Wilks.1998.
Compacting the Penn Treebank grammar.
InProc.
COLING-ACL, pages 699?703.G.
Kruijff.
2003.
3-phase grammar learning.
In Proc.Workshop on Ideas and Strategies for MultilingualGrammar Development.D.
Lin.
1995.
A dependency-based method for evaluat-ing broad-coverage parsers.
In Proc.
IJCAI-95.K.
Sima?an.
2000.
Tree-gram Parsing: Lexical Depen-dencies and Structual Relations.
In Proc.
38th AnnualMeeting of the ACL, pages 53?60, Hong Kong, China.F.
Thollard, P. Dupont, and C. de la Higuera.
2000.Probabilistic DFA inference using kullback-leibler di-vergence and minimality.
In Proc.
ICML 2000.
