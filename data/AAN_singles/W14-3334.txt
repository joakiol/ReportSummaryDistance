Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275?286,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsEstimating Word Alignment Quality for SMT Reordering TasksSara Stymne J?org Tiedemann Joakim NivreUppsala UniversityDepartment of Linguistics and Philologyfirstname.lastname@lingfil.uu.seAbstractPrevious studies of the effect of wordalignment on translation quality in SMTgenerally explore link level metrics onlyand mostly do not show any clear connec-tions between alignment and SMT qual-ity.
In this paper, we specifically inves-tigate the impact of word alignment ontwo pre-reordering tasks in translation, us-ing a wider range of quality indicatorsthan previously done.
Experiments onGerman?English translation show that re-ordering may require alignment modelsdifferent from those used by the core trans-lation system.
Sparse alignments withhigh precision on the link level, for trans-lation units, and on the subset of cross-ing links, like intersected HMM models,are preferred.
Unlike SMT performancethe desired alignment characteristics aresimilar for small and large training datafor the pre-reordering tasks.
Moreover,we confirm previous research showing thatthe fuzzy reordering score is a useful andcheap proxy for performance on SMT re-ordering tasks.1 IntroductionWord alignment is a key component in all state-of-the-art statistical machine translation (SMT) sys-tems, and there has been some work exploring theconnection between word alignment quality andtranslation quality (Och and Ney, 2003; Fraser andMarcu, 2007; Lambert et al., 2012).
The standardway to evaluate word alignments in this context isby using metrics like alignment error rate (AER)and F-measure on the link level, and the generalconclusion appears to be that translation qualitybenefits from alignments with high recall (ratherthan precision), at least for large training data.
Al-though many other ways of measuring alignmentquality have been proposed, such as working ontranslation units (Ahrenberg et al., 2000; Ayan andDorr, 2006; S?gaard and Kuhn, 2009) or using linkdegree and related measures (Ahrenberg, 2010),these methods have not been used to study the re-lation between alignment and translation quality,with the exception of Lambert et al.
(2012).Word alignment is also used for many othertasks besides translation, including term bankcreation (Merkel and Foo, 2007), cross-lingualannotation projection for part-of-speech tagging(Yarowsky et al., 2001), semantic roles (Pado andLapata, 2005), pronoun anaphora (Postolache etal., 2006), and cross-lingual clustering (T?ackstr?omet al., 2012).
Even within SMT itself, there aretasks such as reordering that often make crucialuse of word alignments.
For instance, source lan-guage reordering commonly relies on rules learntautomatically from word-aligned data (e.g., Xiaand McCord (2004)).
As far as we know, no onehas studied the impact of alignment quality onthese additional tasks, and it seems to be tacitlyassumed that alignments that are good for transla-tion are also good for other tasks.In this paper we set out to explore the impactof alignment quality on two pre-reordering tasksfor SMT.
In doing so, we employ a wider range ofquality indicators than is customary, and for refer-ence these indicators are used also to assess over-all translation quality.
To allow an in-depth explo-ration of the connections between several aspectsof word alignment and reordering, we limit ourstudy to one language pair, German?English.
Wethink this is a suitable language pair for studyingreordering since it has both short range and longrange reorderings.
Our main focus is on using rel-atively large training data, 2M sentences, but wealso report results with small training data, 170Ksentences.
The main conclusion of our study isthat alignments that are optimal for translation arenot necessarily optimal for reordering, where pre-275cision is of greater importance than recall.
ForSMT the best alignments are different dependingon corpus size, but for the reordering tasks resultsare stable across training data size.In section 2 we discuss previous work relatedto word alignment and SMT.
In section 3, we in-troduce the word alignment quality indicators weuse, and show experimental results for a numberof alignment systems on an SMT task.
In sec-tion 4, we turn to reordering for SMT and usethe same quality indicators to study the impact ofalignment quality on reordering quality.
In section5 we briefly describe results using small trainingdata.
In section 6, we conclude and suggest direc-tions for future work.2 Word Alignment and SMTWord alignment is the task of relating wordsin one language to words in the translation inanother language, see an example in Figure 1.Word alignment models can be learnt automati-cally from large corpora of sentence aligned data.Brown et al.
(1993) proposed the so-called IBMmodels, which are still widely used.
These fivemodels estimate alignments from corpora usingthe expectation-maximization algorithm, and eachmodel adds some complexity.
Model 4 is com-monly used in SMT systems.
There have beenmany later suggestions of alternatives to thesemodels.
These are often alternatives to model 2,such as the HMM model (Vogel et al., 1996) andfast align (Dyer et al., 2013).All these generative models produce directionalalignments where one word in the source can belinked to many target words (1?m links) but notvice versa.
It is generally desirable to also allown?1 and n?m links, and to achieve this it is com-mon practice to perform word alignment in bothdirections and to symmetrize them using someheuristic.
A number of common symmetrizationstrategies are described in Table 1 (Koehn et al.,2005).
There are also other alternatives, such asthe refined method (Och and Ney, 2003), or linkdeletion from the union (Fossum et al., 2008).There is also a wide range of alternative ap-proaches to word alignment.
For example, variousdiscriminative models have been proposed in theliterature (Liu et al., 2005; Moore, 2005; Taskaret al., 2005).
Their advantage is that they mayintegrate a wide range of features that may leadto improved alignment quality.
However, most ofSymmetrization Descriptionint: intersection ATS?ASTuni: union ATS?ASTgd: grow-diag intersection plus adjacent linksfrom the union if both linkedwords are unalignedgdf: grow-diag-final gd with links from the unionadded in a final step if eitherlinked word is unalignedgdfa:grow-diag-final-andgd with links from the unionadded in a final step if both linkedwords are unalignedTable 1: Symmetrization strategies for word align-ments ATSand ASTin two directionsthese models require external tools (for creatinglinguistic features) and manually aligned trainingdata, which we do not have for our data sets (be-sides the data we need for evaluation).
Investigat-ing these types of models are outside the scope ofour current work.Word alignments are used as an importantknowledge source for training SMT systems.
Inword-based SMT, the parameters of the gener-ative word alignment models are essentially thetranslation model of the system.
In phrase-basedSMT (PBSMT) (Koehn et al., 2003), which isamong the state-of-the-art systems today, wordalignments are used as a basis for extractingphrases and estimating phrase alignment probabil-ities.
Similarly, word alignments are also used forestimating rule probabilities in various kinds of hi-erarchical and syntactic SMT (Chiang, 2007; Ya-mada and Knight, 2002; Galley et al., 2004).Intrinsic evaluation of word alignment is gener-ally based on a comparison to a gold standard ofhuman alignments.
Based on the gold standard,metrics like precision, recall and F-measure canbe calculated for each alignment link, see Eqs.
1?2, where A are hypothesized alignment links andG are gold standard links.
Another common met-ric is alignment error rate (AER) (Och and Ney,2000), which is based on a distinction betweensure, S, and possible, P , links in the gold stan-dard.
1?AER is identical to balanced F-measurewhen the gold standard does not make a distinc-tion between S and P.Precision(A,G) =|G ?A||A|(1)Recall(A,G) =|G ?A||G|(2)AER = 1?|P ?A|+ |S ?A||S|+ |A|(3)276Crossing = 8SKDT =?8/66 ?
0.656 1?1 links3 multi links0 null linksFigure 1: An example alignment illustrating n?1, 1?m and crossing links.The relation between word alignment qual-ity and PBSMT has been studied by some re-searchers.
Och and Ney (2000) looked at the im-pact of IBM and HMM models on the alignmenttemplate approach (Och et al., 1999) in terms ofAER.
They found that AER correlates with humanevaluation of sentence level quality, but not withword error rate.
Fraser and Marcu (2007) foundthat there is no correlation between AER and Bleu(Papineni et al., 2002), especially not when the P -set is large.
They found that a balanced F-measureis a better indicator of Bleu, but that a weightedF-measure is even better (see Eq.
4) mostly witha higher weight for recall than for precision.
Thisweight, however, needs to be optimized for eachdata set, language pair, and gold standard align-ment separately.F(A,G, ?)
=(?Precision(A,G)+1?
?Recall(A,G))?1(4)Ayan and Dorr (2006) on the other hand foundsome evidence for the importance of precisionover recall.
However, they used much smallertraining data than Fraser and Marcu (2007).
Theyalso suggested using a measure called consistentphrase error-rate (CPER), but found that it washard to assess the impact of alignment on MT, bothwith AER and CPER.
Lambert et al.
(2012) per-formed a study where they investigated the effectof word alignment on MT using a large number ofword alignment indicators.
They found that therewas a difference between large and small datasetsin that alignment precision was more importantwith small data sets, and recall more importantwith large data sets.
Overall they did not find anyindicator that was significant over two languagepairs and different corpus sizes.
There were moresignificant indicators for large datasets, however.Most researchers who propose new alignmentmodels perform both a gold standard evalua-tion and an SMT evaluation (Liang et al., 2006;Ganchev et al., 2008; Junczys-Dowmunt and Sza?,2012; Dyer et al., 2013).
The relation between thetwo types of evaluation is often quite weak.
Sev-eral of these studies only show AER on their goldstandard, despite its well-known shortcomings.Even though many studies have shown somerelation between translation quality and AER orweighted F-measure, it has rarely been investi-gated thoroughly in its own right, and, as far as weare aware, not for other tasks than SMT.
Further-more, most of these studies considers nothing elsebut link level agreement.
In this paper we take abroader view on alignment quality and explore theeffect of other types of quality indicators as well.3 Word Alignment Quality IndicatorsWe investigate four groups of quality indicators.The first group is the classic group where met-rics are calculated on the alignment link level,which has been used in several studies.
In ourexperiments we use a gold standard that does notmake use of distinctions between sure and possiblelinks, as suggested by Fraser and Marcu (2007).With this, we can calculate the standard metricsP(recision) R(ecall) and F(-measure).
We willmainly use balanced F-measure, but occasionallyalso report weighted F-measure.
As noted before,1?AER is equivalent to balanced F when onlysure links are used, and will thus not be reportedseparately.S?gaard and Kuhn (2009) and S?gaard and Wu(2009) suggested working on the translation unit(TU) level, instead of the link level.
A translationunit, or cept (Goutte et al., 2004), is defined asa maximally connected subgraph of an alignment.In Figure 1, the twelve links form nine translationunits.
S?gaard and Wu (2009) suggest the metricTUER, translation unit error rate, shown in Eq.
5,where AUare hypothesized translation units, andGUare gold standard translation units.1They useTUER to establish lower bounds for the cover-age of alignments from different formalisms, notto evaluate SMT.
While they only use TUER, it1TUER is similar to CPER (Ayan and Dorr, 2006), whichmeasures the error rate of extracted phrases.
Due to howphrase extraction handle null links, there are differences,however.277is also possible to define Precision, Recall and F-measure over translation units in the same way asfor alignment links.
We will use these three mea-sures to get a broader picture of TUs in alignmentevaluation.
Also in this case, 1?TUER is equiva-lent to F-measure.TUER(A,G) = 1?2|AU?GU||AU|+ |GU|(5)The TU metrics are quite strict, since they re-quire exact matching of TUs.
Tiedemann (2005)suggested the MWU metrics for word alignmentevaluation, which also consider partial matchesof annotated multi-word units, which is a similarconcept to TUs.
In those metrics, precision andrecall grow proportionally to the number of cor-rectly aligned words within translation units.
Pro-posed links are in this way scored according totheir overlap with translation units in the gold stan-dard.
Precision and recall are defined in Eqs.
6?7,where overlap(XU, Y ) is the number of sourceand target words in XUthat overlap with transla-tion units in Y normalized by the size of XU(interms of source and target words).
Note, that TUsneed to overlap in source and target.
Otherwise,their overlap will be counted as zero.PMWU=?AU?Aoverlap(AU, G)|A|(6)RMWU=?GU?Goverlap(GU, A)|G|(7)There have also been attempts at classifyingalignments in other ways, not related to a goldstandard.
Ahrenberg (2010) proposed severalways to categorize human alignments, includinglink degree, reordering of links, and structural cor-respondence.
He used these indicators to profilehand-aligned corpora from different domains.
Wewill not use structural correspondence, which re-quires a dependency parser, and which we believeis error prone when performed automatically.
Wewill use what we call link degree, i.e., how manyalignment links each word obtains.
Ahrenberg(2010) used a fine-grained scheme of the percent-age for different degrees, including isomorphism1?1, deletion 0?1, reduction m?1, and paraphrasem?n.
Similar link degree classes were used byLambert et al.
(2012).
In this work we will re-duce these classes into three: 1?1 links, null links,which combine the 0?1 and 1?0 cases, and multilinks where there are many words on at least oneside.Ahrenberg (2010) also proposed to measure re-orderings.
He does this by calculating the percent-age of links with crossings of different lengths.
Todefine this he only considers adjacent links in thesource using the distance between correspondingtarget words, which means that his metric becomesa directional measure.
Reorderings of alignmentswas also used by Genzel (2010), who used cross-ing score, the number of crossing links, to rankreordering rules.
This is non-directional and sim-pler to calculate than Ahrenberg (2010)?s metrics,and implicitly covers length since a long distancereordering leads to a higher number of pairwisecrossing links.
Birch and Osborne (2011) sug-gest using squared Kendall ?
distance (SKTD), seeEq.
8, where n is the number of links, as a basisof LR-score, an MT metric that takes reorderinginto account.
They found that squaring ?
betterexplained reordering, than using only ?
.
In thisstudy we will use both, crossing score and SKTD.Figure 1 shows these scores for an example sen-tence.
These two measures only tell us how muchreordering there is.
To quantify this relative to thegold standard we also report the absolute differ-ence between the number of gold standard cross-ings and system crossings, which we call Crossd-iff.
To account for the quality of crossings, to someextent, we will also report precision, recall, and F-measure for the subset of translation units that areinvolved in a crossing.SKTD =?|crossing link pairs|(n2?
n)/2(8)3.1 Alignment ExperimentsWe perform all our experiments for German?English.
The alignment indicators are calculatedon a corpus of 987 hand aligned sentences (Padoand Lapata, 2005).
The gold standard containsexplicit null links, which the symmetrized auto-matic alignments do not.
To allow a straightfor-ward comparison we consistently remove all nulllinks when comparing system alignments to thegold standard.For creating the automatic alignments we usedGIZA++ (Och and Ney, 2003) to compute direc-tional alignments for model 2?4 and the HMMmodel, and fast align (fa) (Dyer et al., 2013) asnewer alternatives to model 2.
These models re-quire large amounts of data to be estimated reli-ably.
To achieve this we concatenated the goldstandard with the large SMT training data (see278AlignmentlinksTranslationunitsMWULinkdegreeLinkcrossingsTotalPRFTotalPRFPRF1-1nullmultiTotalSKTDPRFCrossdiffgold22629???17068??????.542.328.13030163.292??
?02-int15362.850.577.68715362.701.631.664.849.712.774.500.500.00010064.267.551.463.503200993-int16573.860.630.72716573.707.686.697.857.776.814.561.439.00012682.274.553.521.537174814-int16529.903.660.76316529.743.720.731.901.813.855.559.441.00011229.251.663.522.58418934HMM-int14871.922.606.73114871.768.669.715.920.750.827.476.524.0008077.221.709.417.52522086fa-int15997.857.606.71015997.696.652.673.854.742.794.531.469.0009724.246.568.471.515204392-gd22882.702.710.70616511.599.579.589.806.827.816.524.289.18621823.270.446.444.44583403-gd21961.757.734.74517644.650.672.661.817.855.836.608.270.12221886.278.492.523.50782774-gd22754.768.772.77017611.670.692.681.839.886.862.605.247.14821966.259.583.517.5488197HMM-gd19430.812.698.75115831.709.658.682.878.820.848.499.407.09414334.231.621.411.49515829fa-gd23148.702.719.71017043.589.588.588.802.839.820.548.258.19418578.242.454.447.450115852-gdfa23840.687.724.70517469.575.588.582.780.841.809.590.216.19425616.279.419.473.44467183-gdfa23049.736.749.74218732.621.681.650.786.870.826.684.188.12827119.294.451.561.50045474-gdfa23704.751.787.76918561.645.701.672.813.901.855.673.172.15426977.275.529.562.5453044HMM-gdfa20554.799.726.76116955.685.681.683.857.851.854.565.337.09817399.246.584.475.52412764fa-gdfa23717.693.726.71017612.575.594.584.785.846.815.587.214.19920384.247.439.465.45297792-gdf29050.591.758.66417089.511.512.512.761.876.814.625.002.37359592.338.321.438.370294293-gdf26575.660.775.71318354.588.632.609.778.891.831.712.064.22550834.344.387.552.455206714-gdf26529.693.812.74818269.628.673.650.810.922.862.706.070.22347216.322.459.585.51417053HMM-gdf23886.725.765.74416660.651.635.643.851.887.869.579.251.16936881.309.473.499.4866718fa-gdf26724.633.748.68617454.524.536.530.769.865.814.589.101.31034309.379.351.445.39241462-uni30712.566.769.65215864.503.468.485.774.869.818.584.002.41371223.349.305.396.345410603-uni28093.636.789.70417391.592.603.597.791.889.837.684.067.24961823.355.381.523.441316604-uni27920.670.827.74017411.636.649.642.826.921.871.682.074.24457408.333.456.564.50427245HMM-uni24712.707.772.73815980.649.608.628.857.881.869.561.260.18042264.319.459.475.46712101fa-uni27951.612.756.67616385.512.491.504.781.867.822.548.111.34638285.396.336.407.3688122Table2:Valuesforalignmentqualityindicatorsforthedifferentalignments,where2?4,HMM,andfaarealignmentmodels,andsymmetrizationstrategiesrefertoTable1279Section 3.2) of 2M sentences during alignment.For symmetrization we used all methods in Table1, as implemented in the Moses toolkit (Koehn etal., 2007) and in fast align (Dyer et al., 2013).Based on the automatically aligned gold stan-dard, we calculated all alignment indicators for allsettings.
The complete results can be found inTable 2, where we have ordered the symmetriza-tion methods with the most sparse, intersection, ontop.
Overall we can see that while several of thealignment methods create a much higher numberof alignment links than the gold standard, they donot produce many more translation units.
This isvery interesting and indicates why link level statis-tics may not be accurate enough to predict the per-formance of certain downstream applications.
Asexpected, the metric scores for translation unitsare lower than for link level metrics.
This ispartly due to the fact that these measures do notcount any partially correct links; the MWU met-rics which considers partial matches often havehigher scores than link level metrics.
Anotherfinding is that the number of crossings vary a lotwith more than twice as many as the reference formodel2+union, and less than three times as manyfor HMM+intersection.
The HMM and fa modelshave fewer reorderings than the IBM models.We are now interested in the relation betweenalignment evaluation on the link level and on thetranslation unit level, which has not been thor-oughly investigated before.
Table 3 shows the cor-relations between the various metrics.
Both preci-sion and F-measure at the link level have signifi-cant correlations to all TU metrics.
Link level re-call, on the other hand, is significantly negativelycorrelated with TU precision, but not significantlycorrelated to any other TU metric, not even TU re-call.
Link level precision is thus highly importantfor matching translation units.
We can also notehere that while there is a trade-off between preci-sion and recall on link level, this is not the case fortranslation units, which can have both high pre-cision and high recall.
The same is not true forMWU, that allows partial matching, where we alsosee at least some precision/recall trade-off.3.2 SMT ExperimentsFor reference, we first study the impact of align-ment on SMT performance.
Our SMT systemis a standard PBSMT system trained on WMT13Translation unitLink level ?
P R FP .95 .77 .90R ?.57 ?.22 ?.42F .70 .90 .83Table 3: Pearson correlations between gold stan-dard word alignment evaluation on the link leveland on translation unit level.
Significant correla-tions are marked with bold (< 0.01).data.2We trained a German?English system on2M sentences from Europarl and News Commen-tary.
We used the target side of the parallel corpusand the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model.
For training the translationmodel and for decoding we used the Moses toolkit(Koehn et al., 2007).
We applied a standard featureset consisting of a language model feature, fourtranslation model features, word penalty, phrasepenalty, and distortion cost.
For tuning we usedminimum error-rate training (Och, 2003).
In or-der to minimize the risk of tuning influencing theresults, we used a fixed set of weights for eachexperiment, tuned on a model 4+gdfa alignment.3For tuning we used newstest2009 with 2525 sen-tences, and for testing we used newstest2013 with3000 sentences.
Evaluation was performed usingthe Bleu metric (Papineni et al., 2002).
The samesystem setup was used for the SMT systems withreordering.Table 4 shows the results on the SMT task.Model 3 and 4 with gd/gdfa symmetrization yieldthe highest scores.
There is a larger difference be-tween systems with different symmetrization thanbetween systems with different alignment models.The sparse intersection symmetrization gives thepoorest results.
The top row in Table 5 showscorrelations between Bleu and all word alignmentquality indicators.
There are significant correla-tions with link level recall.
A weighted link levelF-measure with ?
= 0.3 gives a significant corre-lation of .72, which confirms the results of Fraserand Marcu (2007).
There are no significant corre-lations with the TU metrics but a positive correla-tion with the number of TUs.
For the MWU met-rics the correlations are similar to the link level,2http://www.statmt.org/wmt13/translation-task.html3This could have disfavored the other alignments, so wealso performed control experiments where we ran separatetunings for each alignment.
While the absolute results variedsomewhat, the correlations with alignment indicators werestable.280m2 m3 m4 HMM fainter 18.1 19.1 19.3 18.8 18.9gd 20.4 20.9 20.9 20.5 20.6gdfa 20.4 20.7 20.8 20.5 20.5gdf 19.4 19.7 20.1 19.9 20.0union 19.2 19.6 19.8 19.7 20.0Table 4: Baseline Bleu scores for different sym-metrization heuristicssuggesting that they measure similar things.
Intu-itively it seems important for SMT to match fulltranslation units, but it might be the case that thephrase extraction strategy is robust as long as thereare partial matches.
There are no significant cor-relations with link degree or link crossings, ex-cept a negative correlation with Crossdiff, whichmeans that it is good to have a similar number ofcrossings as the baseline.
These results confirmresults from previous studies that link level mea-sures, especially recall and weighted F-measureshow some correlation with SMT quality whereasprecision does not.4 Reordering Tasks for SMTReordering is an important part of any SMT sys-tem.
One way to address it is to add reorder-ing models to standard PBSMT systems, for in-stance lexicalized reordering models (Koehn et al.,2005), or to directly model reordering in hierarchi-cal (Chiang, 2007) or syntactic translation models(Yamada and Knight, 2002).
Another type of ap-proach is preordering, where the source side is re-ordered to mimic the target side before translation.There have also been approaches where reorderingis modeled as part of the evaluation of MT systems(Birch and Osborne, 2011).We can distinguish two main types of ap-proaches to preordering in SMT, either by usinghand-written rules, which often operate on syn-tactic trees (Collins et al., 2005), or by reorderingrules that are learnt automatically based on a wordaligned corpus (Xia and McCord, 2004).
The lat-ter approach is of interest to us, since it is basedon word alignments.There has been much work on automatic learn-ing of reordering rules, which can be based on dif-ferent levels of annotation, such as part-of-speechtags (Rottmann and Vogel, 2007; Niehues andKolss, 2009; Genzel, 2010), chunks (Zhang etal., 2007) or parse trees (Xia and McCord, 2004).In general, all these approaches lead to improve-ments of translation quality.
The reordering isalways applied on the translation input.
It canalso be applied on the source side of the train-ing corpora, which sometimes improves the results(Rottmann and Vogel, 2007), but sometimes doesnot make a difference (Stymne, 2012).
When pre-ordering is performed on the translation input, itcan be presented to the decoder as a 1-best reorder-ing (Xia and McCord, 2004), as an n-best list (Liet al., 2007), or as a lattice of possible reorderings(Rottmann and Vogel, 2007; Zhang et al., 2007).In the preordering studies cited above it is oftennot even stated which alignment model was used.A few authors mention the alignment tool that hasbeen applied but no comparison between differentalignment models is performed in any of the pa-pers we are aware of.
Li et al.
(2007), for exam-ple, simply state that they used GIZA++ and gdfsymmetrization and that they removed less proba-ble multi links.
Lerner and Petrov (2013) use theintersection of HMM alignments and claims thatmodel 4 did not add much value.
Genzel (2010)did mention that using a standard model 4 wasnot successful for his rule learning approach.
In-stead he used filtered model-1-alignments, whichhe claims was more successful.
However, thereare no further analyses or comparisons betweenthe alignments reported in any of these papers.Another type of approach to reordering is toonly reorder the data in order to improve wordalignments, and to restore the original word or-der before training the SMT system.
This typeof approach has the advantage that no modifica-tions are needed for the translation input.
This ap-proach has also been used both with hand-writtenrules (Carpuat et al., 2010; Stymne et al., 2010)and with rules based on initial word alignments onnon-reordered texts (Holmqvist et al., 2009).
Forthe latter approach a small study of the effect of gdand gdfa symmetrizations was presented, whichonly showed small variations in quality scores(Holmqvist et al., 2012).Below we present the two tasks that we studyin this paper: part-of-speech-based reordering forcreating input lattices for SMT and alignment-based reordering for improving phrase-tables.
Weevaluate the performance of these tasks in rela-tion to the use of different alignment models andsymmetrization heuristics.
For these tasks we aremainly interested in the full translation task, forwhich we report Bleu scores.
In addition we alsoshow fuzzy reordering score (FRS), which focuses281Alignment links Translation units MWUTotal P R F Total P R F P R FSMT, Bleu .33 ?.25 .56 .46 .65 ?.20 .16 ?.02 ?.29 .59 .44POSReo, FRS ?.80 .87 ?.49 .75 ?.23 .90 .81 .89 .82 ?.45 .22POSReo, Bleu ?.64 .74 ?.27 .85 .05 .80 .80 .86 .67 ?.23 .35AlignReo, FRS ?.77 .88 ?.43 .84 ?.11 .90 .88 .92 .81 ?.37 .31AlignReo, Bleu ?.81 .83 ?.58 .61 ?.24 .75 .64 .72 .71 ?.53 .04Link degree Link crossings1-1 null multi Total SKTD P R F CrossdiffSMT, Bleu .33 ?.30 .21 ?.05 ?.14 ?.09 .25 .07 ?.63POSReo, FRS ?.41 .84 ?.89 ?.81 ?.70 .90 .21 .86 ?.41POSReo, Bleu ?.17 .66 ?.80 ?.71 ?.60 .79 .42 .89 ?.49AlignReo, FRS ?.32 .77 ?.86 ?.80 ?.73 .94 .27 .92 ?.38AlignReo, Bleu ?.57 .83 ?.79 ?.93 ?.91 .86 ?.07 .69 ?.52Table 5: Pearson correlations between different alignment characteristics and scores for the translationand reordering tasks.
Significant correlations are marked with bold (< 0.01).only on the reordering component (Talbot et al.,2011).
It compares a system reordering to a refer-ence reordering, by measuring how many chunksthat have to be moved to get an identical word or-der, see Eq.
9, where C is the number of con-tiguously aligned chunks, and M the number ofwords.
To find the reference ordering we applythe method of Holmqvist et al.
(2009), describedin Section 4.2, to the gold standard alignment.FRS = 1?C ?
1M ?
1(9)4.1 Part-of-Speech-Based ReorderingOur first reordering task is a part-of-speech-basedpreordering method described by Rottmann andVogel (2007) and Niehues and Kolss (2009),which was successfully used for German?Englishtranslation.
Rules are learnt from a word alignedPOS-tagged corpus.
Based on the alignments, tagpatterns are identified that give rise to specific re-orderings.
These patterns are then scored basedon relative frequency.4The rules are then appliedto the translation input to create a reordering lat-tice, with normalized edge scores based on rulescores.
In our experiments we only use rules witha score higher than 0.2, to limit the size of the lat-tices.
For calculating FRS, we pick the highestscoring 1-best word order from the lattices.We learn rules from our entire SMT trainingcorpus varying alignment models and symmetriza-tion.
To investigate only the effect of word align-ment for creating reordering rules, we do not4Note that we do not use words (Rottmann and Vogel,2007) or wild cards (Niehues and Kolss, 2009) in our rules.m2 m3 m4 HMM fainter .577 .575 .581 .596 .567gd .555 .559 .570 .589 .546gdfa .540 .540 .559 .579 .539gdf .439 .499 .542 .560 .495union .442 .492 .544 .563 .486Table 6: Fuzzy reordering scores for part-of-speech-based reordering for different alignmentsm2 m3 m4 HMM fainter 21.4 21.6 21.8 21.6 21.6gd 21.5 21.6 21.6 21.7 21.5gdfa 21.4 21.5 21.7 21.7 21.4gdf 20.3 21.0 21.4 21.5 21.0union 20.3 21.5 21.6 21.5 20.8Table 7: Bleu scores for part-of-speech-based re-ordering for different alignmentschange the SMT system, which is trained basedon model 4+gdfa alignments.
The only thing thatvaries for the translation task is thus the input lat-tice given to this SMT system.The results are shown in Tables 6 and 7.
MostBleu scores are better than using the same SMTsystem without preordering, with a Bleu score of20.8.
The results on FRS and Bleu are highly cor-related at .94, despite the fact that we use a latticeas SMT input, and the 1-best order for FRS.
Forboth metrics sparse symmetrization like intersec-tion and gd performs best.
Model 4 and HMMperform best with similar Bleu scores, but FRS isbetter for the HMM model.Table 5 shows the correlations with the wordalignment indicators, in the rows labeled POSReo.There are strong correlations with all TU metrics,contrary to the SMT task.
There are also signifi-cant correlations with link level precision and bal-282anced F-measure.
The correlation with weightedlink level F-measure is even higher, .91 for ?
=0.6.
This is an indication that this algorithm ismore sensitive to precision than the SMT task.
Asfor the SMT task, the correlation patterns are simi-lar for the MWU metrics as for link level.
For linkdegree, null alignments are correlated, but there isa negative correlation for multi links.
The correla-tions with the number of crossings and SKTD arenegative, which means that it is better to have alow number of crossings.
This may seem counter-intuitive, but note in Table 1 that many alignmentshave a much higher number of crossings than thebaseline.
The precision of the crossing links ishighly correlated with performance on this task,while the recall is not.
This tells us that it is impor-tant that the crossings we find in the alignment aregood, but that it is less important that we find allcrossings.
This makes sense since the rule learnercan then learn at least a subset of all existing cross-ings well.4.2 Reordering for AlignmentIn our second reordering task we investigatealignment-based reordering for improving phrase-tables (Holmqvist et al., 2009; Holmqvist et al.,2012).
This strategy first performs a word align-ment, based on which the source text is reorderedto remove all crossings.
A second alignment istrained on the reordered data, which is then re-stored to the original order before training thefull SMT system.
In Holmqvist et al.
(2012) itwas shown that this strategy leads to improve-ments in link level recall and F-measure as wellas small translation improvements for English?Swedish.
It also led to small improvements forGerman?English translation.Similar to the previous experiments, we nowvary alignment models and symmetrization thatare used for reordering during the first step.
Thesecond step is kept the same using model 4+gdfain order to focus on the reordering step in our com-parisons.
Tables 8 and 9 show the results of theseexperiments.
In this case the reordering strat-egy was not successful, always producing lowerBleu scores than the baseline of 20.8.
However,there are some interesting differences in these out-comes.
On this task as well, FRS and Bleu scoresare highly correlated at .89, which was expected,since this method directly uses the reordered datato train phrase tables.
For the best systems, them2 m3 m4 HMM fainter .583 .604 .669 .654 .598gd .548 .583 .646 .642 .561gdfa .532 .564 .633 .645 .553gdf .422 .482 .571 .574 .474union .395 .455 .552 .545 .452Table 8: Fuzzy reordering scores for alignment-based reordering for different alignmentsm2 m3 m4 HMM fainter 19.5 19.5 19.9 20.2 19.4gd 19.3 19.5 19.8 20.2 19.3gdfa 19.1 19.2 19.6 20.0 19.2gdf 18.3 18.2 18.6 19.0 18.9union 17.4 17.8 18.4 18.8 18.8Table 9: Bleu scores for alignment-based reorder-ing for different alignmentsFRS scores are higher than for the previous task,see Table 6, which shows that reordering directlybased on alignments is easier than learning and ap-plying rules based on them, given suitable align-ments.
On this task, again, the sparser alignmentsare the most successful on both tasks.
Here, how-ever, the HMM model gives the best Bleu scores,and similar FRS scores to model 4.Table 5 shows the correlations with the wordalignment indicators, in the rows labeled Align-Reo.
The correlation patterns are very similarto the previous task.
A few more indicators aresignificantly negatively correlated with alignment-based reordering than with the other reorderingtasks and metrics.
The performance on our tworeordering tasks are significantly correlated at .76.Again alignments with good scores on TU met-rics, link level precision and crossing link preci-sion are preferable.
For this task, the best correla-tion with weighted link level F-measure is .86 for?
= 0.8.
Again, we thus see that sparse align-ments with high precision on all measures includ-ing the crossing subset, are important.5 Small Training DataSince previous work has suggested that trainingdata size influences the relation between align-ment and SMT quality for small and large trainingdata (Lambert et al., 2012), we investigated this is-sue also for our reordering tasks.
We repeated allour experiments on a small dataset, only the NewsCommentary data from WMT13, with 170K sen-tences.
Due to space constraints we cannot showall results in the paper, but the main findings are283summarized in this section.To acquire alignment results we realigned thegold standard concatenated with the smaller data,to reflect the actual quality of alignment with asmall dataset.
As expected the quality scores tendto be lower with less data.
Overall the same sys-tems tend to perform good on each metric with thesmall and large data, even though there is somevariation in the ranking between systems.
On theSMT task as well, the Bleu scores are lower, asexpected.
In this case fast align is doing best fol-lowed by model 4 and 3.
The best symmetrizationis again gd and gdfa.
There are also some differ-ences in the correlation profile.
Link recall andnumber of translation units are no longer signifi-cantly correlated, whereas the number of crossingsand SKTD are.
The highest correlation for linklevel F-measure is .60 for balanced F-measure,showing that precision is equally important to re-call with less data.For the reordering tasks the scores are againlower.
The POS-based reorderings again help overthe baseline SMT, whereas the alignment-basedreordering leads to slightly lower scores.
The cor-relation profile look exactly the same for Bleufor POS-based reordering.
FRS for both tasksand Bleu for alignment-based reordering have thesame correlation profiles as Bleu for alignment-based reordering on large data.
There are thusvery small differences in the word alignment qual-ity indicators that are relevant with large and smalltraining data, while there are some differences onthe SMT task.
For weighted link level F-measure,the highest correlations are found with ?
= 0.6?0.7 on the different metrics, again showing thatprecision is more important than recall.
For FRSon both tasks and Bleu for alignment-based re-ordering, model4 and HMM with intersection andgd still perform best.
For Bleu for POS-based re-ordering, gdfa and model 3 also give good results.6 Conclusion and Future WorkWe have shown that the best combination of align-ment and symmetrization models for SMT are notthe best models for reordering tasks in our ex-perimental setting.
For SMT, high recall is moreimportant than precision with large training data,while precision and recall are of equal impor-tance with small training data.
This finding sup-ports previous research (Fraser and Marcu, 2007;Lambert et al., 2012).
Translation unit metricsare not predictive of SMT performance.
For thelarge data condition model 3 and 4 with gd andgdfa symmetrization gave the best results, whereasfast align with gd and gdfa was best with smalltraining data.For the two preordering tasks we investigated,however, link level weighted F-measure that gavemore weight to precision was important, as well asall TU metrics.
It was also important to have highprecision for the crossing subset of TUs.
Hence,it is more important to reliably find some cross-ings than to find all crossings.
This make sensesince the extracted rules or performed reorderingsare likely good in such cases, even if we are notable to find all possible reorderings.
In conclu-sion, based on this study, we recommend intersec-tion symmetrization with model 4 and HMM forSMT reordering tasks.We have studied two relatively different re-ordering tasks with two training data sizes, butfound that they to a large extent prefer the sametypes of alignments.
Moreover, the results onthese two reordering tasks correlates strongly withFRS, which is much cheaper to calculate thanSMT metrics that may even require retraining offull SMT systems.
This is consistent with Tal-bot et al.
(2011) who suggested FRS for preorder-ing tasks.
We thus would encourage developersof alignment methods to not only give results forSMT, but also for FRS, as a proxy for reorderingtasks.
Furthermore, it is also useful to give resultson TU metrics in addition to link level metrics tocomplement the evaluation.In this paper, we have looked at existing genera-tive alignment and symmetrization models.
In fu-ture work, we would also like to investigate othermodels, including the removal of low-confidencelinks, which has previously been proposed for pre-reordering (Li et al., 2007; Genzel, 2010).
Giventhe results, it also seems motivated to developor adapt the existing models in general, to bet-ter fit the properties of specific auxiliary tasks.Furthermore, we need to validate our findings onother language pairs, especially for non-relatedlanguages with even more diverse word order.AcknowledgmentsThis work was supported by the Swedish strategicresearch programme eSSENCE.284ReferencesLars Ahrenberg, Magnus Merkel, Anna S?agvall Hein,and J?org Tiedemann.
2000.
Evaluation of wordalignment systems.
In Proceedings of LREC, vol-ume III, pages 1255?1261, Athens, Greece.Lars Ahrenberg.
2010.
Alignment-based profiling ofEuroparl data in an English-Swedish parallel corpus.In Proceedings of LREC, pages 3398?3404, Valetta,Malta.Necip Fazil Ayan and Bonnie J. Dorr.
2006.
Goingbeyond AER: An extensive analysis of word align-ments and their impact on MT.
In Proceedings ofColing and ACL, pages 9?16, Sydney, Australia.Alexandra Birch and Miles Osborne.
2011.
Reorder-ing metrics for MT.
In Proceedings of ACL, pages1027?1035, Portland, Oregon, USA.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Marine Carpuat, Yuval Marton, and Nizar Habash.2010.
Improving Arabic-to-English statistical ma-chine translation by reordering post-verbal subjectsfor alignment.
In Proceedings of ACL, Short Papers,pages 178?183, Uppsala, Sweden.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):202?228.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan, USA.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM model 2.
In Proceedings of NAACL,pages 644?648, Atlanta, Georgia, USA.Victoria Fossum, Kevin Knight, and Steven Abney.2008.
Using syntax to improve word alignment pre-cision for syntax-based machine translation.
In Pro-ceedings of WMT, pages 44?52, Columbus, Ohio.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine trans-lation.
Computational Linguistics, 33(3):293?303.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of NAACL, pages 273?280, Boston,Massachusetts, USA.Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.2008.
Better alignments = better translations?
InProceedings of ACL, pages 986?993, Columbus,Ohio, USA.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine trans-lation.
In Proceedings of Coling, pages 376?384,Beijing, China.Cyril Goutte, Kenji Yamada, and Eric Gaussier.
2004.Aligning words using matrix factorisation.
In Pro-ceedings of ACL, pages 502?509, Barcelona, Spain.Maria Holmqvist, Sara Stymne, Jody Foo, and LarsAhrenberg.
2009.
Improving alignment for SMTby reordering and augmenting the training corpus.In Proceedings of WMT, pages 120?124, Athens,Greece.Maria Holmqvist, Sara Stymne, Lars Ahrenberg, andMagnus Merkel.
2012.
Alignment-based reorderingfor SMT.
In Proceedings of LREC, Istanbul, Turkey.Marcin Junczys-Dowmunt and Arkadiusz Sza?.
2012.SyMGiza++: Symmetrized word alignment modelsfor statistical machine translation.
In InternationalJoint Conference of Security and Intelligent Infor-mation Systems, pages 379?390, Warsaw, Poland.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL, pages 48?54, Edmonton, Al-berta, Canada.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descrip-tion for the 2005 IWSLT speech translation evalu-ation.
In Proceedings of the International Workshopon Spoken Language Translation, Pittsburgh, Penn-sylvania, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL, Demonstration Session, pages177?180, Prague, Czech Republic.Patrik Lambert, Simon Petitrenaud, Yanjun Ma, andAndy Way.
2012.
What types of word alignmentimprove statistical machine translation?
MachineTranslation, 26(4):289?323.Uri Lerner and Slav Petrov.
2013.
Source-side clas-sifier preordering for machine translation.
In Pro-ceedings of EMNLP, pages 513?523, Seattle, Wash-ington, USA.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, and Yi Guan.
2007.
A probabilistic ap-proach to syntax-based reordering for statistical ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the ACL, pages 720?727, Prague, CzechRepublic.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of NAACL,pages 104?111, New York City, New York, USA.285Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedings ofACL, pages 459?466, Ann Arbor, Michigan, USA.Magnus Merkel and Jody Foo.
2007.
Terminologyextraction and term ranking for standardizing termbanks.
In Proceedings of the 16th Nordic Confer-ence on Computational Linguistics, pages 349?354,Tartu, Estonia.Robert C. Moore.
2005.
A discriminative frameworkfor bilingual word alignment.
In Proceedings ofHLT and EMNLP, pages 81?88, Vancouver, BritishColumbia, Canada.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Pro-ceedings of WMT, pages 206?214, Athens, Greece.Franz Josef Och and Hermann Ney.
2000.
A com-parison of alignment models for statistical machinetranslation.
In Proceedings of Coling, pages 1086?1090, Saarbr?ucken, Germany.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for sta-tistical machine translation.
In Proceedings of theJoint Conference of EMNLP and Very Large Cor-pora, pages 20?28, College Park, Maryland, USA.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167, Sapporo, Japan.Sebastian Pado and Mirella Lapata.
2005.
Cross-linguistic projection of role-semantic information.In Proceedings of HLT and EMNLP, pages 859?866,Vancouver, British Columbia, Canada.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318, Philadelphia, Pennsylvania,USA.Oana Postolache, Dan Cristea, and Constantin Or?asan.a.
2006.
Transferring coreference chains throughword alignment.
In Proceedings of LREC, pages889?892, Genoa, Italy.Kay Rottmann and Stephan Vogel.
2007.
Word re-ordering in statistical machine translation with aPOS-based distortion model.
In Proceedings ofthe 11th International Conference on Theoreticaland Methodological Issues in Machine Translation,pages 171?180, Sk?ovde, Sweden.Anders S?gaard and Jonas Kuhn.
2009.
Empiricallower bounds on alignment error rates in syntax-based machine translation.
In Proceedings of theThird Workshop on Syntax and Structure in Statis-tical Translation, pages 19?27, Boulder, Colorado,USA.Anders S?gaard and Dekai Wu.
2009.
Empirical lowerbounds on translation unit error rate for the full classof inversion transduction grammars.
In Proceedingsof 11th International Conference on Parsing Tech-nologies, pages 33?36, Paris, France.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,pages 901?904, Denver, Colorado, USA.Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.2010.
Vs and OOVs: Two problems for translationbetween German and English.
In Proceedings ofWMT and MetricsMATR, pages 183?188, Uppsala,Sweden.Sara Stymne.
2012.
Clustered word classes for pre-ordering in statistical machine translation.
In Pro-ceedings of ROBUS-UNSUP 2012: Joint Workshopon Unsupervised and Semi-Supervised Learning inNLP, pages 28?34, Avignon, France.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proceedings ofNAACL, pages 477?487, Montr?eal, Quebec, Canada.David Talbot, Hideto Kazawa, Hiroshi Ichikawa, JasonKatz-Brown, Masakazu Seno, and Franz Och.
2011.A lightweight evaluation framework for machinetranslation reordering.
In Proceedings of WMT,pages 12?21, Edinburgh, Scotland.Ben Taskar, Lacoste-Julien Simon, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In Proceedings of HLT and EMNLP,pages 73?80, Vancouver, British Columbia, Canada.J?org Tiedemann.
2005.
Optimisation of wordalignment clues.
Natural Language Engineering,11(03):279?293.
Special Issue on Parallel Texts.Stephan Vogel, Hermann Ney, and Christoph Tillman.1996.
HMM-based word alignment in statisticaltranslation.
In Proceedings of Coling, pages 836?841, Copenhagen, Denmark.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proceedings of Coling, pages508?514, Geneva, Switzerland.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
In Proceedings of ACL,pages 303?310, Philadelphia, Pennsylvania, USA.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Proceedings of the First International Conferenceon Human Language Technology, pages 1?8, SanDiego, California, USA.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Improved chunk-level reordering for statistical ma-chine translation.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation,pages 21?28, Trento, Italy.286
