Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsExtracting Biomedical Events and Modifications Using SubgraphMatching with Noisy Training DataAndrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,Haibin Liu?, W. John Wilbur?
and Karin Verspoor??
NICTA Victoria Research Laboratory, University of Melbourne, Australia{andrew.mackinlay, david.martinez}@nicta.com.au{antonio.jimeno, karin.verspoor}@nicta.com.au?
National Center for Biotechnology Information, Bethesda, MD, USAhaibin.liu@nih.gov, wilbur@ncbi.nlm.nih.govAbstractThe Genia Event (GE) extraction task ofthe BioNLP Shared Task addresses the ex-traction of biomedical events from the nat-ural language text of the published litera-ture.
In our submission, we modified anexisting system for learning of event pat-terns via dependency parse subgraphs toutilise a more accurate parser and signifi-cantly more, but noisier, training data.
Weexplore the impact of these two aspects ofthe system and conclude that the change inparser limits recall to an extent that cannotbe offset by the large quantities of trainingdata.
However, our extensions of the sys-tem to extract modification events showspromise.1 IntroductionIn this paper, we describe our submission to theGenia Event (GE) information extraction subtaskof the BioNLP Shared Task.
This task requires thedevelopment of systems that are capable of iden-tifying bio-molecular events as those events areexpressed in full-text publications.
The task rep-resents an important contribution to the broaderproblem of converting unstructured informationcaptured in the biomedical literature into struc-tured information that can be used to index andanalyse bio-molecular relationships.This year?s task builds on previous instantia-tions of this task (Kim et al 2009; Kim et al2012), with only minor changes in the task defini-tion introduced for 2011.
The task organisers pro-vided full text publications annotated with men-tions of biological entities including proteins andgenes, and asked participants to provide annota-tions of simple events including gene expression,binding, localization, and protein modification, aswell as higher-order regulation events (e.g., pos-itive regulation of gene expression).
In our sub-mission, we built on a system originally developedfor the BioNLP-ST 2011 (Liu et al 2011) and ex-tended in more recent work (Liu et al 2013a; Liuet al 2013b).
This system learns to recognise sub-graphs of syntactic dependency parse graphs thatexpress a given bio-molecular event, and matchesthose subgraphs to new text using an algorithmcalled Approximate Subgraph Matching.Due to the method?s fundamental dependencyon the syntactic dependency parse of the text, inthis work we set out to explore the impact ofsubstituting the previously employed dependencyparsers with a different parser which has beendemonstrated to achieve higher performance thanother commonly used parsers for full-text biomed-ical literature (Verspoor et al 2012).In addition, we aimed to address the relativelylower recall of the method through incorporationof large quantities of external training data, ac-quired through integration of previously automat-ically extracted bio-molecular events available ina web repository of such extracted events, EVEX(Van Landeghem et al 2011; Van Landeghemet al 2012), and additional bio-molecular eventsgenerated from a large sample of full text pub-lications using one of the state-of-the-art eventextraction systems, TEES (Bjo?rne and Salakoski,2011).
Since the performance of the subgraphmatching method, as an instance-based learningstrategy (Alpaydin, 2004), is dependent on havinggood training examples that express the events in arange of syntactic structures, the motivation under-lying this was to increase the amount of trainingdata available to the system, even if that data wasderived from a less-than-perfect source.
The aug-mentation of training corpora with external unla-belled data that is automatically processed to gen-erate additional labels has been explored for re-training the same system, in an approach known asself-training.
This approach has been shown to be35very effective for improving parsing performance(McClosky et al 2006; McClosky and Charniak,2008).
Self-training of the TEES system has beenpreviously explored (Bjorne et al 2012), withsomewhat mixed results, but with evidence sug-gesting it could be useful with an appropriate strat-egy for selecting training examples.
Here, ratherthan training our system with its own output overexternal data, we explore a semi-supervised learn-ing approach in which we train our system with theoutputs of a different system (TEES) over externaldata.2 Methodology2.1 Base Event Extraction SystemThe event extraction algorithm is essentially thesame as the one used in Liu et al(2013b).
A fullerdescription can be found there, but we summarisethe most important aspects of it here.2.1.1 Event Extraction with ASMThe principal method used in event extraction isApproximate Subgraph Matching, or ASM (Liu etal., 2013a).
Broadly, we learn subgraph patternsfrom the event structures in the training data, andthen apply them by looking for matches with thepatterns of the learned rules, using ASM to allowfor non-exact matches of the patterns.The first stage in this is learning the rules whichlink subgraphs to associated patterns.
The inputis a set of dependency-parsed articles (the setupis described in ?2.1.2), and a set of gold-standardannotations of proteins and events in the sharedtask format.
Using the standoff annotations in thetraining data, every protein and trigger is mappedto one or more nodes in the corresponding depen-dency graphs.
In addition, the textual content ofevery protein is replaced with a generic string en-abling abstraction over individual protein names.Then, for each event annotation in the trainingdata, we retrieve the nodes from the graph corre-sponding to the associated trigger and protein en-tities.
We determine the shortest path (or paths, incase of a tie) connecting the graph trigger to eachof the event argument nodes.
For arguments whichare themselves events (e.g., for regulatory events),the node corresponding to the trigger of the eventargument is used instead of a protein node.
Wherethere are multiple arguments, we take the union ofthe shortest paths to each individual argument.This path is then used as the pattern compo-nent of an event rule.
The rule also consists of anevent type, and a mapping from event argumentsto nodes from the pattern graph, or to an eventtype/node pair for nested event arguments.
Af-ter processing all training documents, we get onthe order of a few thousand rules; this can be de-creased slightly by removing rules with subgraphsthat are isomorphic to those of other rules.In principle, this set of rules could then be di-rectly applied to the test documents, by searchingfor any matching subgraphs.
However, in practicedoing so leads to very low recall, since the pat-terns are not general enough to get a broad range ofmatches on new data.
We can alleviate this by re-laxing the strictness of the subgraph matching pro-cess.
Most basically, we relax node matching.
In-stead of requiring an exact match between both thetoken and the part-of-speech of the nodes of thesentence graph and those from the rule subgraph,we also allow a match on the basis of the lemma(according to BioLemmatizer (Liu et al 2012)),and a coarse-grained POS-tag (where there is onlyone POS-tag for nouns, verbs and adjectives).More importantly, we also relax the require-ments on how closely the graphs must match, byusing ASM.
ASM defines distances measures be-tween subgraphs, based on structure, edge labelsand edge directions, and uses a set of specifiedweights to combine them into an overall subgraphdistance.
We have a pre-configured set of distancethresholds for each event type, and for each sen-tence/rule pairing, we extract events for any ruleswith subgraphs under the given threshold.The problem with this approximate matching isthat some rules now match too broadly, and pre-cision is reduced.
This is mitigated by addingan iterative optimisation phase.
In each iteration,we run the event extraction using the current ruleset over some dataset ?
usually the training set,or a subset of it.
We check the contribution ofeach rule in terms of postulated events and actualevents which match the gold standard.
If the ra-tio of matched to postulated events is too low (forthe work reported here, the threshold is 0.25), therule is discarded.
This process is repeated until nomore rules are discarded.
This can take multipleiterations since the rules are interdependent due tothe presence of nested event arguments.The optimisation step is by far the most time-consuming step of our process, especially for thelarge rule sets produced in some configurations.36We were able to improve optimisation times some-what by parallelising the event extraction, andtemporarily removing documents with long ex-traction times from the optimisation process un-til as late as possible, but it remained the primarybottleneck in our experimentation.2.1.2 Parsing PipelineIn our parsing pipeline, we first split sentencesusing the JULIE Sentence Boundary Detector, orJSBD (Tomanek et al 2007).
We then parseusing a version of clearnlp1 (Choi and McCal-lum, 2013), a successor to ClearParser (Choi andPalmer, 2011), which was shown to have state-of-the-art performance over the CRAFT corpusof full-text biomedical articles (Verspoor et al2012).
We use dependency and POS-tagging mod-els trained on the CRAFT corpus (except wherenoted); these pre-trained models are provided withclearnlp.
Our fork of clearnlp integrates to-ken span marking into the parsing process, so thedependency nodes can easily be matched to thestandoff annotations provided with the shared taskdata.
This pipeline is not dependent on any pre-annotated data, so can thus be trivially applied toextra data not provided as part of the shared task.In addition the parsing is fast, requiring roughly 46wall-clock seconds (processing serially) to parsethe 5059 sentences from the training and develop-ment sets of the 2013 GE task ?
an average of 9 msper sentence.
The ability to apply the same pars-ing configuration to new text was useful for addingextra training data, as discussed in ?2.2.The usage of clearnlp as the parser is the pri-mary point of difference between our system andthat of Liu et al(2013b), who use the Charniak-Johnson parser with the McClosky biomedicalmodel (CJM; McClosky and Charniak (2008)), al-though there are other minor differences in tokeni-sation and sentence splitting.
We expected that thehigher accuracy of clearnlp over biomedical textwould translate into increased accuracy of eventdetection in the shared task; we consider this ques-tion in some detail below.2.2 Adding Noisy Training DataOne of the limitations of the ASM approach is thatthe high precision comes at the cost of lower re-call.
Our hypothesis is that adding extra traininginstances, even if some are errors, will raise re-call and improve overall performance.
We utilised1https://code.google.com/p/clearnlp/two sources of automatically-annotated data: theEVEX database, and running an automatic eventannotator over documents from PubMed Central(PMC) and MEDLINE.To test our hypothesis, we utilise one of thebest performing automatic event extractors in pre-vious BioNLP tasks: TEES (Turku Event Extrac-tion System)2 (Bjo?rne et al 2011).
We expand ourpool of training examples by adding the highest-confidence events TEES identifies in unlabelledtext.
We explored different approaches to rankingevents based on classifier confidence empirically.TEES relies on multi-class SVMs both for trig-ger and event classification, and produces confi-dence scores for each prediction.
We exploredranking events according to: (i) score of the trig-ger prediction, (ii) score of the event-type predic-tion, and (iii) sum of trigger and event type predic-tions.
We also compared the performance whenselecting the top-k events overall, versus choos-ing the top-k events for each event type.
We alsotested adding as many instances per event-type asthere were in the manually-annotated dataset, withdifferent multiplying factors.
Finally, we evalu-ated the effect of using different splits of the datafor the evaluation and optimisation steps of ASM.This is the full list of parameters that we testedover held-out data:?
Original confidence scores: we ranked eventsaccording to the three SVM scores mentionedabove: trigger prediction, event-type predic-tion, and combined.?
Overall top-k: we selected the top 1,000,5,000, 10,000, 20,000, 30,000, 40,000, and50,000 for the different experimental runs.?
Top-k per type: for each event type, we se-lected the top 400, 1,000, and 2,000.?
Training bias per type: we add as many in-stances from EVEX per type as there are inthe manually annotated data.
We experimentwith adding up to 6 times as many as in man-ually annotated data.?
Training/optimisation split: we combinemanually and automatically annotated datafor training.
For optimisation we testeddifferent options: manually annotated only,manual + automatic, manual + top-100events, and manual + top-1000 events.2http://jbjorne.github.com/TEES/37We did not explore all these settings exhaus-tively due to time constraints, and we report herethe most promising settings.
It is worth mention-ing that most of the configurations contributed toimprove the baseline performance.
We only ob-served drops when using automatically-annotateddata in the optimisation step.2.2.1 Data from EVEXConveniently, the developers of TEES have re-leased the output of their tool over the full 2009collection of MEDLINE, consisting of abstracts ofbiomedical articles, in a collection known as theEVEX dataset.
We used the full EVEX dataset asprovided by the University of Turku, and exploreddifferent ways of ranking the full list of events asdescribed above.2.2.2 Data from TEESTo augment the training data, we annotated twodata sets with TEES based on MEDLINE andPubMed Central (PMC).
The developers of TEESreleased a trained model for the GE 2013 trainingdata that we utilised.Due to the long pre-processing time of TEES,which includes gene named entity recognition,part-of-speech tagging and parsing, we used theEVEX pre-processed MEDLINE, which requiredsome adaptation of the EVEX XML to the XMLformat accepted by TEES.
Once this adaptationwas finished, the files were processed by TEES.Then, we have selected articles from PMC us-ing a query containing specific MeSH headingsrelated to the GE task and limiting the result toonly the Open Access part of PMC.
From the al-most 600k articles from the PMC Open Access set,we reduced the total number of articles to around155k.
The PMC query is the following:(Genetic Phenomena[MH] OR MetabolicPhenomena[MH] OR Cell PhysiologicalPhenomena[MH] OR BiochemicalProcesses[MH]) AND open access[filter]Furthermore, the articles were split into sectionsand specific sections from the full text like Intro-duction, Background and Methods were removedto reduce the quantity of text to be annotated byTEES.
The PMC files produced by this filteringwere processed by TEES on the NICTA cluster.2.3 Modification DetectionTo evaluate the utility of ASM for a diverse rangeof tasks, we also applied it to the task of detect-ing modification (SPECULATION or NEGATION)NEGATION cues?
Basic: not, no, never, nor, only, neither, fail, cease,stop, terminate, end, lacking, missing, absent, absence,failure, negative, unlikely, without, lack, unable?
Data-derived: any, prevention, prevent, disrupt, dis-ruptionSPECULATION cues:?
Basic: analysis, whether, may, should, can, could, un-certain, questionable, possible, likely, probable, prob-ably, possibly, conceivable, conceivably, perhaps, ad-dress, analyze, analyse, assess, ask, compare, consider,enquire, evaluate, examine, experiment, explore, inves-tigate, test, research, study, speculate?
Data-derived: measure, measurement, suggest, sug-gestion, value, quantify, quantification, determine, de-termination, detect, detection, calculate, calculationTable 1: Modification cuesof events.
In event detection, triggers are explic-itly annotated, so the linguistic cue which indi-cates that an event is occurring is easy to identify.As described in Section 3.2, these triggers are im-portant for learning event patterns.The event extraction method is based on pathsbetween dependency graph nodes, so it is neces-sary to have at least two relevant graph nodes be-fore we can determine a path between them.
Forlearning modification rules, one graph node is thetrigger of the event which is subjec to modifica-tion.
However here we needed a method to deter-mine another node in the sentence which providedevidence that NEGATION or SPECULATION wasoccurring, and could thus form an endpoint for asemantically relevant graph pattern.
To achievethis, we specified a set cue lemmas for NEGATIONand SPECULATION.
The basic set of cue lemmascame from a variety of sources.
Some were man-ually specified and some were derived from previ-ous work on modification detection (Cohen et al2011; MacKinlay et al 2012).
We manually ex-panded this cue list to include obvious derivationalvariants.
This gave us a basic set of 34 SPECULA-TION and 21 NEGATION cues.We also used a data-driven strategy to find ad-ditional lemmas indicative of modification.
Weadapted the method of Rayson and Garside (2000)which uses log-likelihood for finding words thatcharacterise differences between corpora.
Here,the ?corpora?
are sentences attached to all eventsin the training set, and sentences attached to eventswhich are subject to NEGATION or SPECULATION(treated separately).
We build a frequency distri-bution over lemmas in each set of sentences, andcalculate the log-likelihood for all lemmas, us-38ing the observed frequency from the modificationevents and the expected frequency over all events.Sorting by decreasing log-likelihood, we get alist of lemmas which are most strongly associatedwith NEGATION or SPECULATION.
We manuallyexamined the highest-ranked lemmas from thesetwo lists and noted lemmas which may occur,according to human judgment, in phrases whichwould denote the relevant modification type.
Wefound seven extra SPECULATION cues and threeextra NEGATION cues.
Expanding with morpho-logical variants as described above yielded 47SPECULATION cues and 26 NEGATION cues to-tal.
These cues are shown, divided into basic anddata-derived, in Table 1.For every node N with a lemma in the appro-priate set of cue lemmas, we create a rule basedon the shortest path between the cue lemma nodeN and the event trigger node.
The trigger lem-mas are replaced with generic lemmas which onlyreflect the POS-tag of the trigger, to broaden therange of possible matches.
Each rule thus consistsof the POS-tag of an event trigger, and a subgraphpattern including the abstracted event trigger node.At modification detection time, the rules are ap-plied in a similar way to the event rules.
Afterdetecting events, we look for matches of each ex-tracted event with every modification rule.
A ruleR is considered to match if the event trigger nodePOS tag matches the POS tag of the rule, and thesubgraph pattern of the rule matches the graph ofthe sentence, including a node corresponding tothe event trigger node.
If R is found to matchfor a given event and sentence, any events whichhave the trigger defined in the rule are marked asSPECULATION or NEGATION as appropriate.
Asin event extraction, we use ASM to allow a loosermatch between graphs, but initial experimentationshowed that increasing the match thresholds be-yond a relatively small distance was detrimental.We have not yet added an optimisation phase formodification, which might allow larger ASM dis-tance threshold to have more benefit.3 ResultsWe present our results over development data,and the official test.
We report the ApproximateSpan/Approximate Recursive metric in all our ta-bles, for easy comparison of scores.
We describethe data split used for development, explain ourevent extraction results, and finally describe ourperformance in modification detection.3.1 Data division for developmentIn the data provided by the task organisers, thesplit of data between training and developmentsets, with 249 and 222 article sections respec-tively, was fairly even.
If we had used such a split,we would have had an unfeasibly small amountof data to train from during development, andpossible unexpected effects when we sharply in-creased the amount of training data for runningover the held-out test set.
We instead used ourown data set split during development, poolingthe provided training and development sets, andrandomly selecting six PMC articles (PMC IDs2626671, 2674207, 3062687, 3148254, 3333881and 3359311) for the development set, with theremainder available for training.
We respected ar-ticle boundaries in the new split to avoid trainingand testing on sentences taken from different sec-tions of the same article.
Results over the devel-opment set reported in this section are over thisdata split.
We will refer to our training subset asGE13tr, and to the testing subset as GE13dev.For our runs over the official test of this chal-lenge, we merged all the manually annotated datafrom 2013 to be used as training.
We also per-formed some experiments with adding the exam-ples from the 2011 GE task to our training data.3.2 Event ExtractionFor our first experiment, we evaluated the contri-bution of the automatically annotated data over us-ing GE13tr data only.
We performed a set of ex-periments to explore the parameters described inSection 2.2 over two sources of extra examples:EVEX and TEES.Using EVEX data in training resulted in clearimprovements in performance when only manu-ally annotated data was consulted for optimisa-tion.
The increase was mainly due to the betterrecall, with small variations in precision over thebaseline for the majority of experiments.
Our bestrun over the GE13dev data followed this setting:rank events according to trigger scores, include alltop-30000 events (without considering the types ofthe events), and use only manually annotated datafor the optimisation step.
Other settings also per-formed well, as we will see below.For TEES, we selected noisy examples fromMEDLINE and PMC to be used as additional39System Prec.
Rec.
F-sc.GE13tr 60.40 27.02 37.34+TEES 59.27 29.89 39.74+TEES +EVEX (top5k) 46.93 30.78 37.18+TEES +EVEX (top20k) 56.32 31.90 40.73+TEES +EVEX (top30k) 55.34 32.48 40.93+TEES +EVEX (pt1k) 58.54 30.96 40.50+TEES +EVEX (trx4) 57.83 31.23 40.56Table 2: Impact of adding extra training data to theASM method.
top5k,20k,30k: using the top 5,000,20,000, and 30,000 events.
pt1k: using the top1,000 events per event-type.
trx4: following thetraining bias of events, with a multiplying factorof four.
For TEES we always use the top 10,000events.
Evaluated over GE13dev.training data.
Initial results showed that when us-ing only MEDLINE annotated data in the train-ing step, the performance decreased compared tonot using any additional data.
This might havebeen due to differences between the EVEX pre-processed data that we used and what TEES wasexpecting, so the MEDLINE set was not consid-ered for further experimentation.
Using PMC ar-ticles annotated with TEES in the training step se-lected by the evidence score of TEES shows an in-crease of recall while slightly decreasing the pre-cision, which was expected.
We selected the top10000 events from the PMC set based on the evi-dence score as additional training data.Table 2 summarises the results of combin-ing different settings of EVEX with TEES.
Weachieve a considerable boost in recall, at the costof precision for most configurations.
The only set-ting where there is a slight drop in F-score is theexperiment with only 5000 events from EVEX; inthe remaining runs we are able to alleviate the dropin precision, and improve the F-score.
Consider-ing the addition of top-events according to theirtype, the increment in recall is slightly lower, butthese runs are able to reach similar F-score to thebest ones, using less training data.
Results withTEES might be slightly overoptimistic since thePMC annotation is based on a TEES model trainedon the 2013 GE data and our configurations areevaluated on a subset of this data.For our next experiment, we tested the contribu-tion of adding the dataset from the 2011 GE taskto the training dataset.
We use this data both inthe training and optimisation steps.
The results areTrain Prec.
Rec.
F-sc.GE13tr 60.40 27.02 37.34+GE11 53.41 32.62 40.50Table 3: Adding GE11 data to the training and op-timisation steps.
Evaluated over GE13dev.Parser Train Prec.
Rec.
F-sc.clearnlpGE13 60.40 27.02 37.34+GE11 53.41 32.62 40.50CJMGE13 60.96 33.11 42.91+GE11 64.11 38.93 48.44Table 4: Performance depending on the appliedparsing pipeline (clearnlp for this work againstthe CJM pipeline of Liu et al(2013b)) overGE13dev.
For each run, the available data wasused both in training and optimisation.given in Table 3, where we can observe a boost inrecall at the cost of precision.
Overall, the im-proved F-score suggests that this dataset wouldmake a useful contribution to the system.We also compared our system to that of Liuet al(2013b), where the primary difference(although not the only difference, as noted in?2.1.2) is the use of clearnlp instead of the CJM(Charniak-Johnson/McClosky) pipeline.
It is thussomewhat surprising to see in Table 4 that theCJM pipeline outperforms our clearnlp pipelineby 5.5?8% in F-score, depending on the train-ing data.
For the smaller GE13-only training set,the gap is smaller, and the precision figures arein fact comparable.
However, the recall is uni-formly lower, suggesting that the rules learnedfrom clearnlp parses are for some reason less gen-erally applicable.
Another interesting differenceis that our clearnlp pipeline gets a smaller benefitfrom the addition of the GE11 training data.
Weconsider possible reasons for this in ?4.1.Table 5 contains the evaluation of different ex-periments on the official test data.
We tested thebaseline system using the training and develop-ment data from 2011 and 2013 GE tasks and theaddition of TEES and EVEX data.
The additionaldata improves the recall slightly compared to notusing it, while, as expected, it decreases the pre-cision.
Table 5 also shows the results for our of-ficial submission (+TEES+EVEX sub), which dueto time constraints was a combination of the opti-mised rules of different data splits and has a lower40Train Prec.
Rec.
F-sc.GE11, GE13 65.71 32.57 43.55+TEES+EVEX 63.67 33.50 43.91+TEES+EVEX * 50.68 36.99 42.77Table 5: Test set results, always optimised overgold data only.
* denotes the official submission.performance compared to the other results.3.3 Modification DetectionWe show results for selected modification detec-tion experiments in Table 6.
In all cases we usedall of the available gold training data from theGE11 and GE13 datasets.
To assess the impact ofmodification cues, we show results using the basicset as well as with the addition of the data-derivedcues.
It has often been noted (MacKinlay et al2012; Cohen et al 2011) that modification detec-tion accuracy is strongly dependent on the qualityof the upstream event annotation, so we provide anoracle evaluation, using gold-standard event anno-tations rather than automatic output.The performance over the automatically-annotated runs is respectable, given that the recallis fundamentally limited by the recall of the inputevent annotations, which is only around 30% forthe configurations shown.
With the oracle eventannotations, the results improve substantially,with considerable gains in precision, and recallincreasing by a factor of 4?6.
This boost in recallin particular is more than we would naively expectfrom the roughly threefold increase in recall overthe events.
It seems that many of the modificationrules we learned were even more effective overevents which our pipeline was unable to detect.The modification rules were learned from oracleevent data, but this does not fully explain thediscrepancy.
Regardless, our algorithm for mod-ification detection showed excellent performanceover the oracle annotations.
Over the 2009 versionof the BioNLP shared task data, MacKinlay et al(2012) report F-scores of 54.6% for NEGATIONand 51.7% for SPECULATION.
These are notdirectly comparable with those in Table 6, butrunning our newer algorithm over the same 2009data gives F-scores of 84.2% for NEGATION and69.1% for SPECULATION.For the official run, which conflates eventextraction and modification detection accuracy,our system was ranked third for NEGATION andSPECULATION out of the three competing teams,although the other teams had event extraction F-scores of roughly 8% higher than our system.
ForSPECULATION, our system had the highest preci-sion of 34.15%, while the F-score of 20.22% wasclose to the best result of 23.92%.
Our NEGA-TION detection was less competitive, with an F-score of 20.94% ?
roughly 6% lower than the otherteams.
We cannot extrapolate directly from the or-acle evaluation in Table 6, but it seems to indicatethat an increase in event extraction accuracy wouldhave flow-on benefits in modification detection.4 Discussion4.1 Detrimental Effects of Parser ChoiceThe biggest surprise here was that clearnlp, amore accurate dependency parser for the biomed-ical domain, as evaluated on the CRAFT tree-bank, gave a substantially lower event extrac-tion F-score than the CJM parser.
To determinewhether preprocessing caused the differences, wereplaced the existing modules (sentence-splittingfrom JSBD and tokenisation/POS-tagging fromclearnlp) with the BioC-derived versions from theCJM pipeline, but this yielded only an insignifi-cant decrease in accuracy.Over the same training data, the optimised rulesfrom CJM have an average of 2.6 nodes per sub-graph path, compared to 3.9 nodes per path usingclearnlp.
A longer path is less likely to matchthan a shorter path, so this may help to explainthe lower generalisability of the clearnlp-derivedrules.
While it is possible for a longer subgraphto match just as generally, if the test sentencesare parsed consistently, in general there are morenodes and edges which can fail to match due to mi-nor surface variations.
One way to mitigate this isto raise the ASM distance thresholds to compen-sate for this; preliminary experiments suggest itwould provide a small (?
1%) boost in F-score butthis would not close the gap between the parsers.Both parsers produce outputs with StanfordDependency labels (de Marneffe and Manning,2008), so we might naively expect similar graphtopology and subgraph pattern lengths.
However,the CJM pipeline produces graphs in the ?CCpro-cessed?
SD format, which are simpler and denser.If a node N has a link to a node O with a conjunc-tion link to another node P (from e.g.
and), an ex-tra link with the same label is added directly fromN to P in the CCprocessed format.
This means41NEGATION SPECULATIONEval Events (F-sc) Cues P / R / F P / R / FDevGE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83TestGE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing theF-score of the configuration), as well as using oracle event annotations from the gold standard, over ourdevelopment set and the official test set.
Rules are learned from GE13+GE11 gold data (excluding anytest data).
Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or theaugmented set with data-driven additions (47 SPEC/26 NEG).
* denotes the official submission.there are more direct links in the graph, match-ing the semantics more closely.
The shortest pathfromN to P is now direct, instead of viaO, whichcould enable the CJM pipeline to produce moregeneral rules.To evaluate how much this detrimentally af-fects the clearnlp pipeline, as a post hoc in-vestigation, we implemented a conversion mod-ule.
Using Stanford Dependency parser code,we replicated the CCprocessed conversion on theclearnlp graphs, reducing the average subgraphpattern length to 2.8, and slightly improving ac-curacy.
Over our development set, compared tothe results in Table 3 it gave a 0.7% absolute F-score boost over using GE13 training-data only,and 1.1% over using GE11 and GE13 training data(in both cases improving recall).
Over the testset, the improvement was greater, with a P/R/Fof 35.66/64.99/46.05, a 2.5% increase in F-scorecompared to the results in Table 5 and only 2.9%less than the official Liu et al(2012) submission.Clearly some of the inter-parser discrepanciesare due to surface features and post-processing,and as noted above, we can also achieve small im-provements by relaxing ASM thresholds, so someproblems may be caused by the default parametersbeing suboptimal for the parser.
However, the ac-curacy is still lower where we would expect it tobe higher, and this remaining discrepancy is diffi-cult to explain without performing a detailed erroranalysis, which we leave for future work.4.2 Effect of additional dataOur initial intuition that using additional noisytraining data during the training of the systemwould improve the performance is supported bythe results in Table 2.
Table 3 shows that us-ing a larger set of manually annotated data basedon 2011 GE task data also improves performance.However, these tables also indicate that addingmanually annotated data produces an increase inperformance comparable to adding the noisy data,despite its smaller size, and when using this man-ually annotated set together with the noisy data,the improvement resulting from the noisy data issmaller (Table 5).
Noisy data was only used dur-ing training, which limits its effectiveness?anyrule extracted from automatically acquired anno-tations that are not seen during optimisation of therule set will have a lower weight.
On the otherhand, we found that using noisy data for optimi-sation seemed to decrease performance.
Together,these results suggest that studying strategies, pos-sibly self-training, for selection of events from thenoisy data to be used during rule set optimisationin the ASM method are warranted.5 ConclusionUsing additional training data, whether manuallyannotated or noisy, improves the performance ofour baseline event extraction system.
The gainsthat we achieved by adding training data, however,were outweighed by a loss of performance due toour parser substitution, with longer dependencysubgraphs limiting rule generalisability the mostlikely explanation.
Our experiments demonstratethat while a given parser might be ?better?
in oneevaluation context, that advantage may not trans-late to improved performance in a downstreamtask that depends strongly on the parser output.We presented an extension of the subgraph match-ing methodology to extract modification eventswhich, when based on a good core event extrac-tion system, shows very promising results.42AcknowledgmentsNICTA is funded by the Australian Governmentas represented by the Department of Broadband,Communications and the Digital Economy andthe Australian Research Council through the ICTCentre of Excellence program.
This research wassupported in part by the Intramural Research Pro-gram of the NIH, NLM.ReferencesEthem Alpaydin.
2004.
Introduction to MachineLearning.
MIT Press.Jari Bjo?rne and T. Salakoski.
2011.
Generalizingbiomedical event extraction.
In Proceedings ofBioNLP Shared Task 2011 Workshop, pages 183?191.Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2011.
Ex-tracting contextualized complex biological eventswith rich graph-based features sets.
ComputationalIntelligence, 27(4):541?557.Jari Bjorne, Filip Ginter, and Tapio Salakoski.
2012.University of turku in the bionlp?11 shared task.BMC Bioinformatics, 13(Suppl 11):S4.Jinho D. Choi and Andrew McCallum.
2013.Transition-based dependency parsing with selec-tional branching.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, Sofia, Bulgaria.Jinho D. Choi and Martha Palmer.
2011.
Getting themost out of transition-based dependency parsing.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 687?692, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.K.B.
Cohen, K. Verspoor, H.L.
Johnson, C. Roeder,P.V.
Ogren, W.A.
Baumgartner, E. White, H. Tip-ney, and L. Hunter.
2011.
High-precision biologicalevent extraction: Effects of system and data.
Com-putational Intelligence, 27(4):681701, November.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In CrossParser ?08: Coling 2008: Pro-ceedings of the workshop on Cross-Framework andCross-Domain Parser Evaluation, pages 1?8, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.J.D.
Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-jii.
2009.
Overview of bionlp?09 shared task onevent extraction.
Proceedings of Natural LanguageProcessing in Biomedicine (BioNLP) NAACL 2009Workshop, pages 1?9.Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-jii, Toshihisa Takagi, and Akinori Yonezawa.
2012.The genia event and protein coreference tasks ofthe bionlp shared task 2011.
BMC Bioinformatics,13(Suppl 11):S1.H.
Liu, R. Komandur, and K. Verspoor.
2011.
Fromgraphs to events: A subgraph matching approach forinformation eextraction from biomedical text.
ACLHLT 2011, page 164.Haibin Liu, Tom Christiansen, William Baumgartner,and Karin Verspoor.
2012.
Biolemmatizer: alemmatization tool for morphological processing ofbiomedical text.
Journal of Biomedical Semantics,3(1):3.Haibin Liu, Lawrence Hunter, Vlado Keselj, and KarinVerspoor.
2013a.
Approximate subgraph matching-based literature mining for biomedical events and re-lations.
PLoS ONE, 8(4):e60954, 04.Haibin Liu, Karin Verspoor, Don Comeau, AndrewMacKinlay, and W. John Wilbur.
2013b.
General-izing an approximate subgraph matching-based sys-tem to extract events in molecular biology and can-cer genetics.
In Proceedings of the 2013 BioNLPWorkshop Companion Volume for the Shared Task.Andrew MacKinlay, David Martinez, and Timo-thy Baldwin.
2012.
Detecting modification ofbiomedical events using a deep parsing approach.BMC Medical Informatics and Decision Making,12(Suppl 1):S4.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings ofthe Association for Computational Linguistics (ACL2008, short papers).David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language Technologyconference of the North American chapter of theACL, pages 152?159.Paul Rayson and Roger Garside.
2000.
Comparingcorpora using frequency profiling.
In The Workshopon Comparing Corpora, pages 1?6, Hong Kong,China, October.
Association for Computational Lin-guistics.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
Sentence and token splitting based on con-ditional random fields.
In Proceedings of the 10thConference of the Pacific Association for Compu-tational Linguistics, pages 49?57, Melbourne, Aus-tralia.S.
Van Landeghem, F. Ginter, Y.
Van de Peer, andT.
Salakoski.
2011.
EVEX: A pubmed-scale re-source for homology-based generalization of textmining predictions.
In Proceedings of BioNLP 2011Workshop, pages 28?37.43S.
Van Landeghem, K. Hakala, S. Ro?nnqvist,T.
Salakoski, Y.
Van de Peer, and F. Ginter.
2012.Exploring biomolecular literature with EVEX: Con-necting genes through events, homology and indirectassociations.
Advances in Bioinformatics, Specialissue Literature-Mining Solutions for Life ScienceResearch:ID 582765.Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-franchi, Colin Warner, Helen L. Johnson, ChristopheRoeder, Jinho D. Choi, Christopher Funk, YuriyMalenkiy, Miriam Eckert, Nianwen Xue, WilliamA.
Baumgartner Jr., Michael Bada, Martha Palmer, ,and Lawrence E. Hunter.
2012.
A corpus of full-textjournal articles is a robust evaluation tool for reveal-ing differences in performance of biomedical naturallanguage processing tools.
BMC Bioinformatics.44
