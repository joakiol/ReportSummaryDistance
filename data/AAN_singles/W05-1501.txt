Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 1?10,Vancouver, October 2005. c?2005 Association for Computational LinguisticsEfficient and robust LFG parsing: SXLFGPierre Boullier and Beno?t SagotINRIA-Rocquencourt, Projet Atoll,Domaine de Voluceau, Rocquencourt B.P.
10578 153 Le Chesnay Cedex, France{pierre.boullier, benoit.sagot}@inria.frAbstractIn this paper, we introduce a new parser,called SXLFG, based on the Lexical-Functional Grammars formalism (LFG).We describe the underlying context-freeparser and how functional structures areefficiently computed on top of the CFGshared forest thanks to computation shar-ing, lazy evaluation, and compact datarepresentation.
We then present vari-ous error recovery techniques we imple-mented in order to build a robust parser.Finally, we offer concrete results whenSXLFG is used with an existing gram-mar for French.
We show that our parseris both efficient and robust, although thegrammar is very ambiguous.1 IntroductionIn order to tackle the algorithmic difficulties ofparsers when applied to real-life corpora, it is nowa-days usual to apply robust and efficient methodssuch as Markovian techniques or finite automata.These methods are perfectly suited for a large num-ber of applications that do not rely on a complex rep-resentation of the sentence.
However, the descriptiveexpressivity of resulting analyses is far below whatis needed to represent, e.g., phrases or long-distancedependencies in a way that is consistent with seri-ous linguistic definitions of these concepts.
For thisreason, we designed a parser that is compatible witha linguistic theory, namely LFG, as well as robustand efficient despite the high variability of languageproduction.Developing a new parser for LFG (Lexical-Functional Grammars, see, e.g., (Kaplan, 1989)) isnot in itself very original.
Several LFG parsers al-ready exist, including those of (Andrews, 1990) or(Briffault et al, 1997).
However, the most famousLFG system is undoubtedly the Xerox LinguisticsEnvironment (XLE) project which is the successorof the Grammars Writer?s Workbench (Kaplan andMaxwell, 1994; Riezler et al, 2002; Kaplan et al,2004).
XLE is a large project which concentrates alot of linguistic and computational technology, relieson a similar point of view on the balance betweenshallow and deep parsing, and has been successfullyused to parse large unrestricted corpora.Nevertheless, these parsers do not always use inthe most extensive way all existing algorithmic tech-niques of computation sharing and compact infor-mation representation that make it possible to writean efficient LFG parser, despite the fact that the LFGformalism, as many other formalisms relying on uni-fication, is NP-hard.
Of course our purpose is not tomake a new XLE system but to study how robust-ness and efficiency can be reached in LFG parsingon raw text.Building constituent structures (c-structures) doesnot raise any particular problem in theory,1 be-cause they are described in LFG by a context-freegrammar (CFG), called (CF) backbone in this pa-per.
Indeed, general parsing algorithms for CFGsare well-known (Earley, GLR,.
.
.
).
On the otherhand, the efficient construction of functional struc-tures (f-structures) is much more problematic.
Thefirst choice that a parser designer must face is thatof when f-structures are processed: either during CF1In practice, the availability of a good parser is sometimesless straightforward.1parsing (interleaved method) or in a second phase(two-pass computation).
The second choice is be-tween f-structures evaluation on single individual[sub-]parses ([sub-]trees) or on a complete represen-tation of all parses.
We choose to process all phrasalconstraints by a CF parser which produces a sharedforest2 of polynomial size in polynomial time.
Sec-ond, this shared forest is used, as a whole, to de-cide which functional constraints to process.
Forambiguous CF backbones, this two pass computa-tion is more efficient than interleaving phrasal andfunctional constraints.3 Another advantage of thistwo pass vision is that the CF parser may be easilyreplaced by another one.
It may also be replacedby a more powerful parser.4 We choose to evalu-ate functional constraints directly on the shared for-est since it has been proven (See (Maxwell and Ka-plan, 1993)), as one can easily expect, that tech-niques which evaluate functional constraints on anenumeration of the resulting phrase-structure treesare a computational disaster.
This article exploresthe computation of f-structures directly (without un-folding) on shared forests.
We will see how, in somecases, our parser allows to deal with potential com-binatorial explosion.
Moreover, at all levels, errorrecovering mechanisms turn our system into a robustparser.Our parser, called SXLFG, has been evaluatedwith two large-coverage grammars for French, oncorpora of various genres.
In the last section of thispaper, we present quantitative results of SXLFG us-ing one of these grammars on a general journalisticcorpus.2 The SXLFG parser: plain parsingThis section describes the parsing process for fullygrammatical sentences.
Error recovery mechanisms,that are used when this is not the case, are describedin the next section.2Informally, a shared forest is a structure which can repre-sent a set of parses (even an unbounded number) in a way thatshares all common sub-parses.3This fact can be easily understood by considering that func-tional constraints may be constructed in exponential time ona sub-forest that may well be discarded later on by (future)phrasal constraints.4For example, we next plan to use an RCG backbone (see(Boullier, 2004) for an introduction to RCGs), with the func-tional constraints being evaluated on the shared forest output byan RCG parser.2.1 Architecture overviewThe core of SXLFG is a general CF parser that pro-cesses the CF backbone of the LFG.
It is an Earley-like parser that relies on an underlying left-cornerautomaton and is an evolution of (Boullier, 2003).The set of analyses produced by this parser is rep-resented by a shared parse forest.
In fact, this parseforest may itself be seen as a CFG whose produc-tions are instantiated productions of the backbone.5The evaluation of the functional equations is per-formed during a bottom-up left-to-right walk in thisforest.
A disambiguation module, which discardsunselected f-structures, may be invoked on any nodeof the forest including of course its root node.The input of the parser is a word lattice (all wordsbeing known by the lexicon, including special wordsrepresenting unknown tokens of the raw text).
Thislattice is converted by the lexer in a lexeme lattice(a lexeme being here a CFG terminal symbol asso-ciated with underspecified f-structures).2.2 The context-free parserThe evolutions of the Earley parser compared to thatdescribed in (Boullier, 2003) are of two kinds: it ac-cepts lattices (or DAGs) as input and it has syntac-tic error recovery mechanisms.
This second pointwill be examined in section 3.1.
Dealing with DAGsas input does not require, at least from a theoreti-cal standpoint, considerable changes in the Earleyalgorithm.6 Since the Earley parser is guided bya left-corner finite automaton that defines a regularsuper-set of the CF backbone language, this automa-ton also deals with DAGs as input (this correspondsto an intersection of two finite automata).5If A is a non-terminal symbol of the backbone, Aijis an in-stantiated non-terminal symbol if and only if Aij+?Gai+1.
.
.
ajwhere w = a1.
.
.
anis the input string and+?Gthe transitiveclosure of the derives relation.6If i is a node of the DAG and if we have a transition on theterminal t to the node j (without any loss in generality, we cansuppose that j > i) and if the Earley item [A ?
?.t?, k] isan element of the table T [i], then we can add to the table T [j]the item [A ?
?t.
?, k] if it is not already there.
One musttake care to begin a PREDICTOR phase in a T [j] table only ifall Earley phases (PREDICTOR, COMPLETOR and SCANNER)have already been performed in all tables T [i], i < j.22.3 F-Structures computationAs noted in (Kaplan and Bresnan, 1982), if the num-ber of CF parses (c-structures) grows exponentiallyw.r.t.
the length of the input, it takes exponentialtime to build and check the associated f-structures.Our experience shows that the CF backbone for largeLFGs may be highly ambiguous (cf.
Section 4).This means that (full) parsing of long sentenceswould be intractable.
Although in CF parsing an ex-ponential (or even unbounded) number of parse treescan be computed and packed in polynomial time ina shared forest, the same result cannot be achievedwith f-structures for several reasons.7 However, thisintractable behavior (and many others) may well notoccur in practical NL applications, or some tech-niques (See Section 2.4) may be applied to restrictthis combinatorial explosion.Efficient computation of unification-based struc-tures on a shared forest is still a evolving researchfield.
However, this problem is simplified if struc-tures are monotonic, as is the case in LFG.
In sucha case the support (i.e., the shared forest) does notneed to be modified during the functional equationresolution.
If we adopt a bottom-up left-to-righttraversal strategy in the shared forest, informationin f-structures is cumulated in a synthesized way.This means that the evaluation of a sub-forest8 isonly performed once, even when this sub-forest isshared by several parent nodes.
In fact, the effectof a complete functional evaluation is to associateto each node of the parse forest a set of partial f-structures which only depends upon the descendantsof that node (excluding its parent or sister nodes).The result of our LFG parser is the set of (com-plete and consistent, if possible) main f-structures(i.e., the f-structures associated to the root of theshared forest), or, when a partial analysis occurs,7As an example, it is possible, in LFG, to define f-structureswhich encode individual parses.
If a polynomial sized sharedforest represents an exponential number of parses, the numberof different f-structures associated to the root of the shared for-est would be that exponential number of parses.
In other words,there are cases where no computational sharing of f-structuresis possible.8If the CF backbone G is cyclic (i.e., ?A s.t.
A +?GA), theforest may be a general graph, and not only a DAG.
Though ourCF parser supports this case, we exclude it in SXLFG.
Of coursethis (little) restriction does not mean that cyclic f-structures arealso prohibited.
SXLFG does support cyclic f-structures, whichcan be an elegant way to represent some linguistic relations.the sets of (partial) f-structures which are associatedwith maximal internal nodes).
Such sets of (partialor not) f-structures could be factored out in a singlef-structure containing disjunctive values, as in XLE.We decided not to use these complex disjunctive val-ues, except for some atomic types, but rather to asso-ciate to any (partial) f-structure a unique identifier:two identical f-structures will always have the sameidentifier throughout the whole process.
Experi-ments (not reported here) show that this strategy isworth using and that the total number of f-structuresbuilt during a complete parse remains very tractable,except maybe in some pathological cases.As in XLE, we use a lazy copying strategy duringunification.
When two f-structures are unified, weonly copy their common parts which are needed tocheck whether these f-structures are unifiable.
Thisrestricts the quantity of copies between two daughternodes to the parts where they interact.
Of course,the original daughter f-structures are left untouched(and thus can be reused in another context).2.4 Internal and global disambiguationApplications of parsing systems often need a dis-ambiguated result, thus calling for disambiguationtechniques to be applied on the ambiguous outputof parsers such as SXLFG.
In our case, this im-plies developing disambiguation procedures in orderto choose the most likely one(s) amongst the main f-structures.
Afterwards, the shared forest is pruned,retaining only c-structures that are compatible withthe chosen main f-structure(s).On the other hand, on any internal node of the for-est, a possibly huge number of f-structures may becomputed.
If nothing is done, these numerous struc-tures may lead to a combinatorial explosion that pre-vents parsing from terminating in a reasonable time.Therefore, it seems sensible to allow the grammardesigner to point out in his or her grammar a set ofnon-terminal symbols that have a linguistic propertyof (quasi-)saturation, making it possible to applyon them disambiguation techniques.9 Hence, somenon-terminals of the CF backbone that correspond9Such an approach is indeed more satisfying than a blindskimming that stops the full processing of the sentence when-ever the amount of time or memory spent on a sentence exceedsa user-specified limit, replacing it by a partial processing thatperforms a bounded amount of work on each remaining non-terminal (Riezler et al, 2002; Kaplan et al, 2004).3to linguistically saturated phrases may be associ-ated with an ordered list of disambiguation meth-ods, each of these non-terminals having its own list.This allows for swift filtering out on relevant internalnodes of f-structures that could arguably only leadto inconsistent and/or incomplete main f-structures,or that would be discarded later on by applying thesame method on the main f-structures.
Concomi-tantly, this leads to a significant improvement ofparsing times.
This view is a generalization of theclassical disambiguation method described above,since the pruning of f-structures (and incidentallyof the forest itself) is not reserved any more to theaxiom of the CF backbone.
We call global disam-biguation the pruning of the main f-structures, andinternal disambiguation the same process appliedon internal nodes of the forest.
It must be noticedthat neither disambiguation type necessarily leadsto a unique f-structure.
Disambiguation is merelya shortcut for partial or total disambiguation.Disambiguation methods are generally dividedinto probabilistic and rule-based techniques.
Ourparsing architecture allows for implementing bothkinds of methods, provided the computations canbe performed on f-structures.
It allows to asso-ciate a weight with all f-structures of a given in-stantiated non-terminal.10 Applying a disambigua-tion rule consists in eliminating of all f-structuresthat are not optimal according to this rule.
Each op-tional rule is applied in a cascading fashion (one canchange the order, or even not apply them at all).After this disambiguation mechanism on f-structures, the shared forest (that represent c-structures) is filtered out so as to correspond exactlyto the f-structure(s) that have been kept.
In partic-ular, if the disambiguation is complete (only one f-structure has been kept), this filtering yields in gen-eral a unique c-structure (a tree).10See (Kinyon, 2000) for an argumentation on the impor-tance of performing disambiguation on structures such as TAGderivation trees or LFG f-structures and not constituent(-like)structures.3 Techniques for robust parsing3.1 Error recovery in the CF parserThe detection of an error in the Earley parser11 canbe caused by two different phenomena: the CF back-bone has not a large enough coverage or the input isnot in its language.
Of course, although the parsercannot make the difference between both causes,parser and grammar developers must deal with themdifferently.
In both cases, the parser has to be ableto perform recovery so as to resume parsing as wellas, if possible, to correctly parse valid portions ofincorrect inputs, while preserving a sensible relationbetween these valid portions.
Dealing with errorsin parsers is a field of research that has been mostlyaddressed in the deterministic case and rarely in thecase of general CF parsers.We have implemented two recovery strategies inour Earley parser, that are tried one after the other.The first strategy is called forward recovery, thesecond one backward recovery.12 Both generate ashared forest, as in the regular case.The mechanism is the following.
If, at a certainpoint, the parsing is blocked, we then jump forwarda certain amount of terminal symbols so as to be ableto resume parsing.
Formally, in an Earley parserwhose input is a DAG, an error is detected when,whatever the active table T [j], items of the formI = [A ?
?.t?, i] in this table are such that in theDAG there is no out-transition on t from node j. Wesay that a recovery is possible in k on ?
if in the suf-fix ?
= ?1X?2there exists a derived phrase fromthe symbol X which starts with a terminal symbolr and if there exists a node k in the DAG, k ?
j,with an out-transition on r. If it is the case and ifthis possible recovery is selected, we put the item[A ?
?t?1.X?2, i] in table T [k].
This will ensure11Let us recall here that the Earley algorithm, like the GLRalgorithm, has the valid prefix property.
This is still true whenthe input is a DAG.12The combination of these two recovery techniques leads toa more general algorithm than the skipping of the GLR* algo-rithm (Lavie and Tomita, 1993).
Indeed, we can not only skipterminals, but in fact replace any invalid prefix by a valid pre-fix (of a right sentential form) with an increased span.
In otherwords, both terminals and non-terminals may be skipped, in-serted or changed, following the heuristics described later on.However, in (Lavie and Tomita, 1993), considering only theskipping of terminal symbols was fully justified since their aimwas to parse spontaneous speech, full of noise and irrelevancesthat surround the meaningful words of the utterance.4S2SNPNpnJeanVvessayePVPprepdeVP?spunct...Figure 1: Simplified constituents structure for in-complete sentence Jean essaye de... (?Jean triesto...?).
The derivation of VP in the empty string isthe result of a forward recovery, and will lead toan incomplete functional structure (no ?pred?
in thesub-structure corresponding to node VP).at least one valid transition from T [k] on r. The ef-fect of such a recovery is to assume that betweenthe nodes j and k in the DAG there is a path that is aphrase generated by t?1.
We select only nodes k thatare as close as possible to j.
This economy principleallows to skip (without analysis) the smallest pos-sible number of terminal symbols, and leads prettyoften to no skipping, thus deriving ?1into the emptystring and producing a recovery in k = j.
This re-covery mechanism allows the parsing process to goforward, hence the name forward recovery.If this strategy fails, we make use of backwardrecovery.13 Instead of trying to apply the currentitem, we jump backward over terminal symbols thathave already been recognized by the current item,until we find its calling items, items on which wetry to perform a forward recovery at turn.
In caseof failure, we can go up recursively until we suc-ceed.
Indeed, success is guaranteed, but in the worstcase it is obtained only at the axiom.
In this ex-treme case, the shared forest that is produced is onlya single production that says that the input DAGis derivable from the axiom.
We call this situationtrivial recovery.
Formally, let us come back to theitem I = [A ?
?.t?, i] of table T [j].
We knowthat there exists in table T [i] an item J of the form[B ?
?.A?, h] on which we can hazard a forward13This second strategy could be also used before or even inparallel with the forward recovery.recovery in l on ?, where i ?
j ?
l. If this fails, wego on coming back further and further in the past,until we reach the initial node of the DAG and theroot item [S?
?
.S$, 0] of table T [0] ($ is the end-of-sentence mark and S?
the super-axiom).
Sinceany input ends with an $ mark, this strategy alwayssucceeds, leading in the worst case to trivial recov-ery.An example of an analysis produced is shown inFigure 1: in this case, no out-transition on spunct isavailable after having recognized prep.
Hence a for-ward recovery is performed that inserts an ?empty?VP after the prep, so as to build a valid parse.3.2 Inconsistent or partial f-structuresThe computation of f-structures fails if and onlyif no consistent and complete main f-structure isfound.
This occurs because unification constraintsspecified by functional equations could not havebeen verified or because resulting f-structures areinconsistent or incomplete.
Without entering intodetails, inconsistency mostly occurs because sub-categorization constraints have failed.??????????????????????????
?pred = ?essayer <subj, de-vcomp>?, v[2..3]subj =??
?pred = ?Jean <(subj)>?, pn[1..2]det = +hum = +Aij={R1829, R17726, R17028}??
?F68de-vcomp =????
?pred = ?de <obj|...>?, prep[3..4]vcomp =[subj = []F68Aij={R16284}2]F69pcase = deAij= {}2????
?F70number = sgperson = 3mode = indicativetense = presentAij={R13033, R11948, R13449}??????????????????????????
?Figure 2: Simplified incomplete functional structurefor incomplete sentence Jean essaye de... (?Jeantries to...?).
Sub-structure identifiers are indicated assubscripts (like F70).
In the grammar, a rule can tellthe parser to store the current instantiated productionin the special field Aijof its associated left-handside structure.
Hence, atoms of the form Rqprep-resent instantiated production, thus allowing to linksub-structures to non-terminals of the c-structure.5A first failure leads to a second evaluation of f-structures on the shared forest, during which consis-tency and completeness checks are relaxed (an ex-ample thereof is given in Figure 2).
In case of suc-cess, we obtain inconsistent or incomplete main f-structures.
Of course, this second attempt can alsofail.
We then look in the shared forest for a set ofmaximal nodes that have f-structures (possibly in-complete or inconsistent) and whose mother nodeshave no f-structures.
They correspond to partialdisjoint analyses.
The disambiguation process pre-sented in section 2.4 applies to all maximal nodes.3.3 Over-segmentation of unparsable sentencesDespite all these recovery techniques, parsing some-times fails, and no analysis is produced.
This canoccur because a time-out given as parameter has ex-pired before the end of the process, or because theEarley parser performed a trivial recovery (becauseof the insufficient coverage of the grammar, or be-cause the input sentence is simply too far from beingcorrect: grammatical errors, incomplete sentences,too noisy sentences, .
.
.
).For this reason, we developed a layer over SXLFGthat performs an over-segmentation of ungrammat-ical sentences.
The idea is that it happens fre-quently that portions of the input sentence are ana-lyzable as sentences, although the full input sentenceis not.
Therefore, we split in segments unparsablesentences (level 1 segmentation); then, if needed,we split anew unparsable level 1 segments14 (level2 segmentation), and so on with 5 segmentation lev-els.15 Such a technique supposes that the grammarrecognizes both chunks (which is linguistically jus-tified, e.g., in order to parse nominal sentences) andisolated terminals (which is linguistically less rele-vant).
In a way, it is a generalization of the use of aFRAGMENT grammar as described in (Riezler et al,2002; Kaplan et al, 2004).14A sentence can be split into two level 1 segments, the firstone being parsable.
Then only the second one will be over-segmented anew into level 2 segments.
And only unparsablelevel 2 segments will be over-segmented, and so on.15The last segmentation level segments the input string intoisolated terminals, in order to guarantee that any input is parsed,and in particular not to abandon parsing on sentences in whichsome level 1 or 2 segments are parsable, but in which some partsare only parsable at level 5.4 Quantitative results4.1 Grammar, disambiguation rules, lexiconTo evaluate the SXLFG parser, we used our systemwith a grammar for French that is an adaptation of anLFG grammar originally developed by Cl?ment forhis XLFG system (Cl?ment and Kinyon, 2001).
Inits current state, the grammar has a relatively largecoverage.
Amongst complex phenomena coveredby this grammar are coordinations (without ellip-sis), juxtapositions (of sentences or phrases), inter-rogatives, post-verbal subjects and double subjects(Pierre dort-il ?
), all kinds of verbal kernels (in-cluding clitics, auxiliaries, passive, negation), com-pletives (subcategorized or adjuncts), infinitives (in-cluding raising verbs and all three kinds of controlverbs), relatives or indirect interrogatives, includingwhen arbitrarily long-distance dependencies are in-volved.
However, comparatives, clefts and ellipticalcoordinations are not specifically covered, inter alia.Moreover, we have realized that the CF backbone istoo ambiguous (see below).Besides the grammar itself, we developed a set ofdisambiguation heuristics.
Following on this point(Cl?ment and Kinyon, 2001), we use a set of rulesthat is an adaptation and extension of the three sim-ple principles they describe and that are appliedon f-structures, rather than a stochastic model.16Our rules are based on linguistic considerations andcan filter out functional structures associated to agiven node of the forest.
This includes two specialrules that eliminate inconsistent and incomplete f-structures either in all cases or when consistent andcomplete structures exist (these rules are not appliedduring the second pass, if any).
As explained above,some non-terminals of the CF backbone, that corre-spond to linguistically saturated phrases, have beenassociated with an ordered list of these rules, each ofthese non-terminal having its own list.17.16As sketched before, this could be easily done by defining arule that uses a stochastic model to compute a weight for eachf-structure (see e.g., (Miyao and Tsujii, 2002)) and retains onlythose with the heaviest weights (Riezler et al, 2002; Kaplanet al, 2004).
However, our experiments show that structuralrules can be discriminative enough to enable efficient parsing,without the need for statistical data that have to be acquired onannotated corpora that are rare and costly, in particular if theconsidered language is not English.17Our rules, in their order of application on main f-structures,i.e.
on the axiom of the backbone, are the following (note that601002003004005006007008000  20  40  60  80  100Number of sentencesinthesameclass(lengthbetween10i and10(i+1)-1)Sentence length (number of transitions in the corresponding word lattice)Figure 3: Repartition of sentences of the test corpusw.r.t.
their length.
We show the cardinal of classesof sentences of length 10i to 10(i + 1) ?
1, plottedwith a centered x-coordinate (10(i + 1/2)).The lexicon we used is the latest version ofLefff (Lexique des formes fl?chies du fran?ais18),which contains morphosyntactic and syntactic infor-mation for more than 600,000 entries correspondingto approximately 400,000 different tokens (words orcomponents of multi-word units).The purpose of this paper is not however to val-idate the grammar and these disambiguation rules,since the grammar has only the role of enabling eval-uation of parsing techniques developed in the currentwork.4.2 ResultsAs for any parser, the evaluation of SXLFG has beencarried out by testing it in a real-life situation.
Weused the previously cited grammar on a raw journal-istic corpus of 3293 sentences, not filtered and pro-when used on other non-terminal symbols than the axiom, somerules may not be applied, or in a different order):Rule 1: Filter out inconsistent and incomplete structures, ifthere is at least one consistent and complete structure.Rule 2: Prefer analyses that maximize the sum of the weightsof involved lexemes; amongst lexical entries that have aweight higher than normal are multi-word units.Rule 3: Prefer nominal groups with a determiner.Rule 4: Prefer arguments to modifiers, and auxiliary-participle relations to arguments (the computation isperformed recursively on all (sub-)structures).Rule 5: Prefer closer arguments (same remark).Rule 6: Prefer deeper structures.Rule 7: Order structures according to the mode of verbs (werecursively prefer structures with indicative verbs, sub-junctive verbs, and so on).Rule 8: Order according to the category of adverb governors.Rule 9: Choose one analysis at random (to guarantee that theoutput is a unique analysis).18Lexicon of French inflected forms11000001e+101e+151e+201e+251e+300  20  40  60  80  100Number of treesintheCFGparseforest(logscale)Sentence length (number of transitions in the corresponding word lattice)Median number of treesNumber of trees at percentile rank 90Number of trees at percentile rank 10Figure 4: CFG ambiguity (medians are computed onclasses of sentences of length 10i to 10(i+1)?1 andplotted with a centered x-coordinate (10(i + 1/2)).cessed by the SXPipe pre-parsing system describedin (Sagot and Boullier, 2005).
The repartition of sen-tences w.r.t.
their length is plotted in Figure 3.In all Figures, the x-coordinate is bounded so asto show results only on statistically significant data,although we parse all sentences, the longest one be-ing of length 156.However, in order to evaluate the performance ofour parser, we had to get rid of, as much as possible,the influence of the grammar and the corpus in thequantitative results.
Indeed, the performance of theSXLFG parser does not depend on the quality andthe ambiguity of the grammar, which is an input forSXLFG.
On the contrary, our aim is to develop aparser which is as efficient and robust as possiblegiven the input grammar, and in spite of its (possiblyhuge) ambiguity and of its (possibly poor) intrinsiccoverage.4.2.1 CFG parser evaluationTherefore, Figure 4 demonstrates the level of am-biguity of the CF backbone by showing the mediannumber of CF parses given the number of transitionsin the lattice representing the sentence.
Althoughthe number of trees is excessively high, Figure 5shows the efficiency of our CF parser19 (the max-imum number of trees reached in our corpus is ashigh as 9.12 1038 for a sentence of length 140, which19Our experiments have been performed on a AMD Athlon2100+ (1.7 GHz).701002003004005000  20  40  60  80  100CFGparsingtime(milliseconds)Sentence length (number of transitions in the corresponding word lattice)Median CFG parsing timeCFG parsing time at percentile rank 90CFG parsing time at percentile rank 10Figure 5: CF parsing time (same remark as for Fig.
4).is parsed in only 0.75 s).
Moreover, the error re-covery algorithms described in section 3.1 are suc-cessful in most cases where the CF backbone doesnot recognize the input sentences: out of the 3292sentences, 364 are not recognized (11.1%), and theparser proposes a non-trivial recovery for all but 13(96.3%).
We shall see later the relevance of the pro-posed recovered forests.
We should however noticethat the ambiguity of forests is significantly higherin case of error recovery.4.2.2 Evaluation of f-structures computationAlthough the CF backbone is massively ambigu-ous, results show that our f-structures evaluationsystem is pretty efficient.
Indeed, with a timeout of20 seconds, it takes only 6 301 seconds to parse thewhole corpus, and only 5, 7% of sentences reach thetimeout before producing a parse.
These results canbe compared to the result with the same grammar onthe same corpus, but without internal disambigua-tion (see 2.4), which is 30 490 seconds and 41.2%of sentences reaching the timeout.The coverage of the grammar on our corpus withinternal disambiguation is 57.6%, the coverage be-ing defined as the proportion of sentences for whicha consistent and complete main f-structure is outputby the parser.
This includes cases where the sen-tence was agrammatical w.r.t.
the CF backbone, butfor which the forest produced by the error recov-ery techniques made it possible to compute a consis-tent and complete main f-structure (this concerns 86sentences, i.e., 2.6% of all sentences, and 24.5% ofall agrammatical sentences w.r.t.
the backbone; thisshows that CF error recovery gives relevant results).The comparison with the results with the samegrammar but without internal disambiguation is in-teresting (see Table 1): in this case, the high propor-tion of sentences that reach the timeout before beingparsed leads to a coverage as low as 40.2%.
Amidthe sentences covered by such a system, 94.6% arealso covered by the full-featured parser (with inter-nal disambiguation), which means that only 72 sen-tences covered by the grammar are lost because ofthe internal disambiguation.
This should be com-pared with the 645 sentences that are not parsed be-cause of the timeout when internal disambiguationis disabled, but that are covered by the grammar andcorrectly parsed if internal disambiguation is used:the risk that is taken by pruning f-structures duringthe parsing process is much smaller than the benefitit gives, both in terms of coverage and parsing time.Since we do not want the ambiguity of the CFbackbone to influence our results, Figure 6 plots thetotal parsing time, including the evaluation of fea-tures structures, against the number of trees pro-duced by the CF parser.8Results With internal Without internaldisambiguation disambiguationTotal number of sentences 3293Recognized by the backbone 2929 88.9%CF parsing with non-trivial recovery 351 10.6%CF parsing with trivial recovery 13 0.4%Consistent and complete main f-structure 1896 57.6% 1323 40.2%Inconsistent and incomplete main f-structure 734 22.3% 316 9.6%Partial f-structures 455 13.8% 278 8.4%No f-structure 6 0.2% 6 0.2%No result (trivial recovery) 13 0.4% 13 0.4%Timeout (20 s) 189 5.7% 1357 40.2%Table 1: Coverage results with and without internal ranking, with the same grammar and corpus.101001000100001  100000  1e+10  1e+15  1e+20Total parsingtime(milliseconds)Number of trees in the forestMedian total parsing timeTotal parsing time at percentile rank 90Total parsing time at percentile rank 10Figure 6: Total parsing time w.r.t.
the number of trees in the forest produced by the CF backbone (mediansare computed on classes of sentences whose number of trees lies between 102i and 102i+2 ?
1 and plottedwith a centered x-coordinate (102i+1)).95 ConclusionThis paper shows several important results.It shows that wide-coverage unification-basedgrammars can be used to define natural languagesand that their parsers can, in practice, analyze rawtext.It shows techniques that allow to compute fea-ture structures efficiently on a massively ambiguousshared forest.It also shows that error recovery is worth doingboth at the phrasal and functional levels.
We haveshown that a non-negligible portion of input textsthat are not in the backbone language can neverthe-less, after CF error recovery, be qualified as validsentences for the functional level.Moreover, the various robustness techniques thatare applied at the functional level allow to gather(partial) useful information.
Note that these ro-bust techniques, which do not alter the overall ef-ficiency of SXLFG, apply in the two cases of incom-plete grammar (lack of covering) and agrammati-cal phrases (w.r.t.
the current definition), though itseems to be more effective in this latter case.ReferencesAvery Andrews.
1990.
Functional closure in LFG.
Tech-nical report, The Australian National University.Pierre Boullier.
2003.
Guided Earley parsing.
In Pro-ceedings of the 8th International Workshop on ParsingTechnologies (IWPT?03), pages 43?54, Nancy, France,April.Pierre Boullier.
2004.
Range concatenation grammars.In New developments in parsing technology, pages269?289.
Kluwer Academic Publishers.Xavier Briffault, Karim Chibout, G?rard Sabah, andJ?r?me Vapillon.
1997.
An object-oriented lin-guistic engineering environment using LFG (Lexical-Functional Grammar) and CG (Conceptual Graphs).In Proceedings of Computational Environments forGrammar Development and Linguistic Engineering,ACL?97 Workshop.Lionel Cl?ment and Alexandra Kinyon.
2001.
XLFG ?an LFG parsing scheme for French.
In Proceedings ofLFG?01, Hong Kong.Ronald Kaplan and Joan Bresnan.
1982.
Lexical-functional grammar: a formal system for grammaticalrepresentation.
In J. Bresnan, editor, The Mental Rep-resentation of Grammatical Relations, pages 173?281.MIT Press, Cambridge, MA.Ronald M. Kaplan and John T. Maxwell.
1994.
Gram-mar writer?s workbench, version 2.0.
Technical report,Xerox Corporation.Ronald Kaplan, Stefan Riezler, Tracey King, JohnMaxwell, Alex Vasserman, and Richard Crouch.2004.
Speed and accuracy in shallow and deepstochastic parsing.
In Proceedings of HLT/NAACL,Boston, Massachusetts.Ronald Kaplan.
1989.
The formal architecture of lexicalfunctionnal grammar.
Journal of Informations Scienceand Engineering.Alexandra Kinyon.
2000.
Are structural principles use-ful for automatic disambiguation ?
In Proceedingsof in COGSCI?00, Philadelphia, Pennsylvania, UnitedStates.Alon Lavie and Masaru Tomita.
1993.
GLR* ?
an effi-cient noise-skipping parsing algorithm for context-freegrammars.
In Proceedings of the Third InternationalWorkshop on Parsing Technologies, pages 123?134,Tilburg, Netherlands and Durbuy, Belgium.John Maxwell and Ronald Kaplan.
1993.
The interfacebetween phrasal and functional constraints.
Computa-tional Linguistics, 19(4):571?589.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In Proceedings ofHLT, San Diego, California.Stefan Riezler, Tracey King, Ronald Kaplan, RichardCrouch, John Maxwell, and Mark Johnson.
2002.Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimationtechniques.
In Proceedings of the Annual Meeting ofthe ACL, University of Pennsylvania.Beno?t Sagot and Pierre Boullier.
2005.
From raw cor-pus to word lattices: robust pre-parsing processing.
InProceedings of L&TC 2005, Poznan?, Pologne.10
