Proceedings of the 43rd Annual Meeting of the ACL, pages 346?353,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsLearning Stochastic OT Grammars: A Bayesian approachusing Data Augmentation and Gibbs SamplingYing Lin?Department of LinguisticsUniversity of California, Los AngelesLos Angeles, CA 90095yinglin@ucla.eduAbstractStochastic Optimality Theory (Boersma,1997) is a widely-used model in linguis-tics that did not have a theoretically soundlearning method previously.
In this pa-per, a Markov chain Monte-Carlo methodis proposed for learning Stochastic OTGrammars.
Following a Bayesian frame-work, the goal is finding the posterior dis-tribution of the grammar given the rela-tive frequencies of input-output pairs.
TheData Augmentation algorithm allows oneto simulate a joint posterior distribution byiterating two conditional sampling steps.This Gibbs sampler constructs a Markovchain that converges to the joint distribu-tion, and the target posterior can be de-rived as its marginal distribution.1 IntroductionOptimality Theory (Prince and Smolensky, 1993)is a linguistic theory that dominates the field ofphonology, and some areas of morphology and syn-tax.
The standard version of OT contains the follow-ing assumptions:?
A grammar is a set of ordered constraints ({Ci :i = 1, ?
?
?
, N}, >);?
Each constraint Ci is a function: ??
?
{0, 1, ?
?
?
}, where ??
is the set of strings in thelanguage;?The author thanks Bruce Hayes, Ed Stabler, Yingnian Wu,Colin Wilson, and anonymous reviewers for their comments.?
Each underlying form u corresponds to a setof candidates GEN(u).
To obtain the uniquesurface form, the candidate set is successivelyfiltered according to the order of constraints, sothat only the most harmonic candidates remainafter each filtering.
If only 1 candidate is leftin the candidate set, it is chosen as the optimaloutput.The popularity of OT is partly due to learning al-gorithms that induce constraint ranking from data.However, most of such algorithms cannot be ap-plied to noisy learning data.
Stochastic OptimalityTheory (Boersma, 1997) is a variant of OptimalityTheory that tries to quantitatively predict linguis-tic variation.
As a popular model among linguiststhat are more engaged with empirical data than withformalisms, Stochastic OT has been used in a largebody of linguistics literature.In Stochastic OT, constraints are regarded asindependent normal distributions with unknownmeans and fixed variance.
As a result, the stochasticconstraint hierarchy generates systematic linguisticvariation.
For example, consider a grammar with3 constraints, C1 ?
N(?1, ?2), C2 ?
N(?2, ?2),C3 ?
N(?3, ?2), and 2 competing candidates for agiven input x:p(.)
C1 C2 C3x ?
y1 .77 0 0 1x ?
y2 .23 1 1 0Table 1: A Stochastic OT grammarwith 1 input and 2 outputs346The probabilities p(.)
are obtained by repeatedlysampling the 3 normal distributions, generating thewinning candidate according to the ordering of con-straints, and counting the relative frequencies in theoutcome.
As a result, the grammar will assign non-zero probabilities to a given set of outputs, as shownabove.The learning problem of Stochastic OT involvesfitting a grammar G ?
RN to a set of candidateswith frequency counts in a corpus.
For example,if the learning data is the above table, we need tofind an estimate of G = (?1, ?2, ?3)1 so that thefollowing ordering relations hold with certain prob-abilities:max{C1, C2} > C3; with probability .77max{C1, C2} < C3; with probability .23 (1)The current method for fitting Stochastic OT mod-els, used by many linguists, is the Gradual Learn-ing Algorithm (GLA) (Boersma and Hayes, 2001).GLA looks for the correct ranking values by usingthe following heuristic, which resembles gradientdescent.
First, an input-output pair is sampled fromthe data; second, an ordering of the constraints issampled from the grammar and used to generate anoutput; and finally, the means of the constraints areupdated so as to minimize the error.
The updatingis done by adding or subtracting a ?plasticity?
valuethat goes to zero over time.
The intuition behindGLA is that it does ?frequency matching?, i.e.
look-ing for a better match between the output frequen-cies of the grammar and those in the data.As it turns out, GLA does not work in all cases2,and its lack of formal foundations has been ques-tioned by a number of researchers (Keller andAsudeh, 2002; Goldwater and Johnson, 2003).However, considering the broad range of linguisticdata that has been analyzed with Stochastic OT, itseems unadvisable to reject this model because ofthe absence of theoretically sound learning meth-ods.
Rather, a general solution is needed to eval-uate Stochastic OT as a model for linguistic varia-tion.
In this paper, I introduce an algorithm for learn-ing Stochastic OT grammars using Markov chainMonte-Carlo methods.
Within a Bayesian frame-1Up to translation by an additive constant.2Two examples included in the experiment section.
See 6.3.work, the learning problem is formalized as find-ing the posterior distribution of ranking values (G)given the information on constraint interaction basedon input-output pairs (D).
The posterior contains allthe information needed for linguists?
use: for exam-ple, if there is a grammar that will generate the exactfrequencies as in the data, such a grammar will ap-pear as a mode of the posterior.In computation, the posterior distribution is sim-ulated with MCMC methods because the likeli-hood function has a complex form, thus makinga maximum-likelihood approach hard to perform.Such problems are avoided by using the Data Aug-mentation algorithm (Tanner and Wong, 1987) tomake computation feasible: to simulate the pos-terior distribution G ?
p(G|D), we augment theparameter space and simulate a joint distribution(G, Y ) ?
p(G,Y |D).
It turns out that by settingY as the value of constraints that observe the de-sired ordering, simulating from p(G,Y |D) can beachieved with a Gibbs sampler, which constructs aMarkov chain that converges to the joint posteriordistribution (Geman and Geman, 1984; Gelfand andSmith, 1990).
I will also discuss some issues relatedto efficiency in implementation.2 The difficulty of a maximum-likelihoodapproachNaturally, one may consider ?frequency matching?as estimating the grammar based on the maximum-likelihood criterion.
Given a set of constraints andcandidates, the data may be compiled in the form of(1), on which the likelihood calculation is based.
Asan example, given the grammar and data set in Table1, the likelihood of d=?max{C1, C2} > C3?
canbe written as P (d|?1, ?2, ?3)=1?
?
0???
0?
?12pi?2 exp{?
~fxy ??
?~fTxy2}dx dywhere ~fxy = (x?
?1 + ?3, y ?
?2 + ?3), and ?is the identity covariance matrix.
The integral signfollows from the fact that both C1 ?
C2, C2 ?
C3are normal, since each constraint is independentlynormally distributed.If we treat each data as independently generatedby the grammar, then the likelihood will be a prod-uct of such integrals (multiple integrals if many con-straints are interacting).
One may attempt to max-imize such a likelihood function using numerical347methods3, yet it appears to be desirable to avoid like-lihood calculations altogether.3 The missing data scheme for learningStochastic OT grammarsThe Bayesian approach tries to explore p(G|D),the posterior distribution.
Notice if we take theusual approach by using the relationship p(G|D) ?p(D|G) ?
p(G), we will encounter the same prob-lem as in Section 2.
Therefore we need a feasibleway of sampling p(G|D) without having to derivethe closed-form of p(D|G).The key idea here is the so-called ?missing data?scheme in Bayesian statistics: in a complex model-fitting problem, the computation can sometimes begreatly simplified if we treat part of the unknownparameters as data and fit the model in successivestages.
To apply this idea, one needs to observe thatStochastic OT grammars are learned from ordinaldata, as seen in (1).
In other words, only one as-pect of the structure generated by those normal dis-tributions ?
the ordering of constraints ?
is usedto generate outputs.This observation points to the possibility oftreating the sample values of constraints ~y =(y1, y2, ?
?
?
, yN ) that satisfy the ordering relationsas missing data.
It is appropriate to refer to themas ?missing?
because a language learner obviouslycannot observe real numbers from the constraints,which are postulated by linguistic theory.
Whenthe observed data are augmented with missing dataand become a complete data model, computation be-comes significantly simpler.
This type of idea is of-ficially known as Data Augmentation (Tanner andWong, 1987).
More specifically, we also make thefollowing intuitive observations:?
The complete data model consists of 3 randomvariables: the observed ordering relations D,the grammar G, and the missing samples ofconstraint values Y that generate the orderingD.?
G and Y are interdependent:?
For each fixed d, values of Y that respect dcan be obtained easily once G is given: wejust sample from p(Y |G) and only keep3Notice even computing the gradient is non-trivial.those that observe d. Then we let d varywith its frequency in the data, and obtaina sample of p(Y |G,D);?
Once we have the values of Y that respectthe ranking relations D, G becomes in-dependent of D. Thus, sampling G fromp(G|Y,D) becomes the same as samplingfrom p(G|Y ).4 Gibbs sampler for the joint posterior ?p(G, Y |D)The interdependence of G and Y helps design iter-ative algorithms for sampling p(G, Y |D).
In thiscase, since each step samples from a conditionaldistribution (p(G|Y,D) or p(Y |G,D)), they can becombined to form a Gibbs sampler (Geman and Ge-man, 1984).
In the same order as described in Sec-tion 3, the two conditional sampling steps are imple-mented as follows:1.
Sample an ordering relation d according tothe prior p(D), which is simply normalizedfrequency counts; sample a vector of con-straint values y = {y1, ?
?
?
, yN} from the nor-mal distributions N(?
(t)1 , ?2), ?
?
?
, N(?
(t)N , ?2)such that y observes the ordering in d;2.
Repeat Step 1 and obtain M samples of miss-ing data: y1, ?
?
?
, yM ; sample ?
(t+1)i fromN(?j yji /M, ?2/M).The grammar G = (?1, ?
?
?
, ?N ), and the su-perscript (t) represents a sample of G in iterationt.
As explained in 3, Step 1 samples missing datafrom p(Y |G,D), and Step 2 is equivalent to sam-pling from p(G|Y,D), by the conditional indepen-dence of G and D given Y .
The normal posteriordistribution N(?j yji /M, ?2/M) is derived by us-ing p(G|Y ) ?
p(Y |G)p(G), where p(Y |G) is nor-mal, and p(G) ?
N(?0, ?0) is chosen to be an non-informative prior with ?0 ?
?.M (the number of missing data) is not a crucialparameter.
In our experiments, M is set to the totalnumber of observed forms4.
Although it may seemthat ?2/M is small for a large M and does not play4Other choices of M , e.g.
M = 1, lead to more or less thesame running time.348a significant role in the sampling of ?
(t+1)i , the vari-ance of the sampling distribution is a necessary in-gredient of the Gibbs sampler5.Under fairly general conditions (Geman and Ge-man, 1984), the Gibbs sampler iterates these twosteps until it converges to a unique stationary dis-tribution.
In practice, convergence can be monitoredby calculating cross-sample statistics from multipleMarkov chains with different starting points (Gel-man and Rubin, 1992).
After the simulation isstopped at convergence, we will have obtained aperfect sample of p(G,Y |D).
These samples canbe used to derive our target distribution p(G|D) bysimply keeping all the G components, since p(G|D)is a marginal distribution of p(G,Y |D).
Thus, thesampling-based approach gives us the advantage ofdoing inference without performing any integration.5 Computational issues in implementationIn this section, I will sketch some key steps in theimplementation of the Gibbs sampler.
Particular at-tention is paid to sampling p(Y |G,D), since a directimplementation may require an unrealistic runningtime.5.1 Computing p(D) from linguistic dataThe prior probability p(D) determines the numberof samples (missing data) that are drawn under eachordering relation.
The following example illustrateshow the ordering D and p(D) are calculated fromdata collected in a linguistic analysis.
Consider adata set that contains 2 inputs and a few outputs,each associated with an observed frequency in thelexicon:C1 C2 C3 C4 C5 Freq.x1 y11 0 1 0 1 0 4y12 1 0 0 0 0 3y13 0 1 1 0 1 0y14 0 0 1 0 0 0x2 y21 1 1 0 0 0 3y22 0 0 1 1 1 0Table 2: A Stochastic OT grammar with 2 inputsThe three ordering relations (corresponding to 3attested outputs) and p(D) are computed as follows:5As required by the proof in (Geman and Geman, 1984).Ordering Relation D p(D)??
?C1>max{C2, C4}max{C3, C5}>C4C3>max{C2, C4}.4??
?max{C2, C4}>C1max{C2, C3, C5}>C1C3>C1.3max{C3, C4, C5} > max{C1, C2} .3Table 3: The ordering relations D and p(D)computed from Table 2.Here each ordering relation has several conjuncts,and the number of conjuncts is equal to the numberof competing candidates for each given input.
Theseconjuncts need to hold simultaneously because eachwinning candidate needs to be more harmonic thanall other competing candidates.
The probabilitiesp(D) are obtained by normalizing the frequencies ofthe surface forms in the original data.
This will havethe consequence of placing more weight on lexicalitems that occur frequently in the corpus.5.2 Sampling p(Y |G,D) under complexordering relationsA direct implementation p(Y |G, d) is straightfor-ward: 1) first obtain N samples from N Gaussiandistributions; 2) check each conjunct to see if theordering relation is satisfied.
If so, then keep thesample; if not, discard the sample and try again.However, this can be highly inefficient in manycases.
For example, if m constraints appear in theordering relation d and the sample is rejected, theN ?m random numbers for constraints not appear-ing in d are also discarded.
When d has several con-juncts, the chance of rejecting samples for irrelevantconstraints is even greater.In order to save the generated randomnumbers, the vector Y can be decom-posed into its 1-dimensional components(Y1, Y2, ?
?
?
, YN ).
The problem then becomessampling p(Y1, ?
?
?
, YN |G,D).
Again, we may useconditional sampling to draw yi one at a time: wekeep yj 6=i and d fixed6, and draw yi so that d holdsfor y.
There are now two cases: if d holds regardlessof yi, then any sample from N(?
(t)i , ?2) will do;otherwise, we will need to draw yi from a truncated6Here we use yj 6=i for all components of y except the i-thdimension.349normal distribution.To illustrate this idea, consider an example usedearlier where d=?max{c1, c2} > c3?, and the ini-tial sample and parameters are (y(0)1 , y(0)2 , y(0)3 ) =(?
(0)1 , ?
(0)2 , ?
(0)3 ) = (1,?1, 0).Sampling dist.
Y1 Y2 Y3p(Y1|?1, Y1 > y3) 2.3799 -1.0000 0p(Y2|?2) 2.3799 -0.7591 0p(Y3|?3, Y3 < y1) 2.3799 -0.7591 -1.0328p(Y1|?1) -1.4823 -0.7591 -1.0328p(Y2|?2, Y2 > y3) -1.4823 2.1772 -1.0328p(Y3|?3, Y3 < y2) -1.4823 2.1772 1.0107Table 4: Conditional sampling steps forp(Y |G, d) = p(Y1, Y2, Y3|?1, ?2, ?3, d)Notice that in each step, the sampling density iseither just a normal, or a truncated normal distribu-tion.
This is because we only need to make sure thatd will continue to hold for the next sample y(t+1),which differs from y(t) by just 1 constraint.In our experiment, sampling from truncated nor-mal distributions is realized by using the idea of re-jection sampling: to sample from a truncated nor-mal7 pic(x) = 1Z(c) ?N(?, ?)
?I{x>c}, we first find anenvelope density function g(x) that is easy to sam-ple directly, such that pic(x) is uniformly bounded byM ?
g(x) for some constant M that does not dependon x.
It can be shown that once each sample x fromg(x) is rejected with probability r(x) = 1?
pic(x)M ?g(x) ,the resulting histogram will provide a perfect samplefor pic(x).
In the current work, the exponential dis-tribution g(x) = ?
exp {?
?x} is used as the enve-lope, with the following choices for ?
and the rejec-tion ratio r(x), which have been optimized to lowerthe rejection rate:?
= c+?c+ 4?22?2r(x) = exp{(x+ c)22 + ?0(x+ c)?
?2?202}Putting these ideas together, the final version ofGibbs sampler is constructed by implementing Step1 in Section 4 as a sequence of conditional sam-pling steps for p(Yi|Yj 6=i, d), and combining them7Notice the truncated distribution needs to be re-normalizedin order to be a proper density.with the sampling of p(G|Y,D).
Notice the order inwhich Yi is updated is fixed, which makes our imple-mentation an instance of the systematic-scan Gibbssampler (Liu, 2001).
This implementation may beimproved even further by utilizing the structure ofthe ordering relation d, and optimizing the order inwhich Yi is updated.5.3 Model identifiabilityIdentifiability is related to the uniqueness of solu-tion in model fitting.
Given N constraints, a gram-mar G ?
RN is not identifiable because G + Cwill have the same behavior as G for any constantC = (c0, ?
?
?
, c0).
To remove translation invariance,in Step 2 the average ranking value is subtractedfrom G, such that ?i ?i = 0.Another problem related to identifiability ariseswhen the data contains the so-called ?categoricaldomination?, i.e., there may be data of the follow-ing form:c1 > c2 with probability 1.In theory, the mode of the posterior tends to infin-ity and the Gibbs sampler will not converge.
Sincehaving categorical dominance relations is a com-mon practice in linguistics, we avoid this problemby truncating the posterior distribution8 by I|?|<K ,where K is chosen to be a positive number largeenough to ensure that the model be identifiable.
Therole of truncation/renormalization may be seen as astrong prior that makes the model identifiable on abounded set.A third problem related to identifiability occurswhen the posterior has multiple modes, which sug-gests that multiple grammars may generate the sameoutput frequencies.
This situation is common whenthe grammar contains interactions between manyconstraints, and greedy algorithms like GLA tend tofind one of the many solutions.
In this case, onecan either introduce extra ordering relations or useinformative priors to sample p(G|Y ), so that the in-ference on the posterior can be done with a relativelysmall number of samples.5.4 Posterior inferenceOnce the Gibbs sampler has converged to its station-ary distribution, we can use the samples to make var-8The implementation of sampling from truncated normals isthe same as described in 5.2.350ious inferences on the posterior.
In the experimentsreported in this paper, we are primarily interested inthe mode of the posterior marginal9 p(?i|D), wherei = 1, ?
?
?
, N .
In cases where the posterior marginalis symmetric and uni-modal, its mode can be esti-mated by the sample median.In real linguistic applications, the posteriormarginal may be a skewed distribution, and manymodes may appear in the histogram.
In these cases,more sophisticated non-parametric methods, such askernel density estimation, can be used to estimatethe modes.
To reduce the computation in identifyingmultiple modes, a mixture approximation (by EMalgorithm or its relatives) may be necessary.6 Experiments6.1 Ilokano reduplicationThe following Ilokano grammar and data set, usedin (Boersma and Hayes, 2001), illustrate a complextype of constraint interaction: the interaction be-tween the three constraints: ?COMPLEX-ONSET,ALIGN, and IDENTBR([long]) cannot be factoredinto interactions between 2 constraints.
For anygiven candidate to be optimal, the constraint thatprefers such a candidate must simultaneously dom-inate the other two constraints.
Hence it is not im-mediately clear whether there is a grammar that willassign equal probability to the 3 candidates./HRED-bwaja/ p(.)
?C-ONS AL IBRbu:.bwa.ja .33 1 0 1bwaj.bwa.ja .33 2 0 0bub.wa.ja .33 0 1 0Table 5: Data for Ilokano reduplication.Since it does not address the problem of identifi-ability, the GLA does not always converge on thisdata set, and the returned grammar does not alwaysfit the input frequencies exactly, depending on thechoice of parameters10.In comparison, the Gibbs sampler convergesquickly11, regardless of the parameters.
The resultsuggests the existence of a unique grammar that will9Note G = (?1, ?
?
?
, ?N ), and p(?i|D) is a marginal ofp(G|D).10B &H reported results of averaging many runs of the algo-rithm.
Yet there appears to be significant randomness in eachrun of the algorithm.11Within 1000 iterations.assign equal probabilities to the 3 candidates.
Theposterior samples and histograms are displayed inFigure 1.
Using the median of the marginal posteri-ors, the estimated grammar generates an exact fit tothe frequencies in the input data.0 200 400 600 800 1000?2?1.5?1?0.500.511.522.5?2 ?1 0 1 2050100150200250300350Figure 1: Posterior marginal samples and histograms forExperiment 2.6.2 Spanish diminutive suffixationThe second experiment uses linguistic data on Span-ish diminutives and the analysis proposed in (Arbisi-Kelm, 2002).
There are 3 base forms, each as-sociated with 2 diminutive suffixes.
The gram-mar consists of 4 constraints: ALIGN(TE,Word,R),MAX-OO(V), DEP-IO and BaseTooLittle.
The datapresents the problem of learning from noise, sinceno Stochastic OT grammar can provide an exact fitto the data: the candidate [ubita] violates an extraconstraint compared to [liri.ito], and [ubasita] vio-lates the same constraint as [liryosito].
Yet unlike[lityosito], [ubasita] is not observed.Input Output Freq.
A M D B/uba/ [ubita] 10 0 1 0 1[ubasita] 0 1 0 0 0/mar/ [marEsito] 5 0 0 1 0[marsito] 5 0 0 0 1/liryo/ [liri.ito] 9 0 1 0 0[liryosito] 1 1 0 0 0Table 6: Data for Spanish diminutive suffixation.In the results found by GLA, [marEsito] alwayshas a lower frequency than [marsito] (See Table 7).This is not accidental.
Instead it reveals a problem-atic use of heuristics in GLA12: since the constraintB is violated by [ubita], it is always demoted when-ever the underlying form /uba/ is encountered dur-ing learning.
Therefore, even though the expected12Thanks to Bruce Hayes for pointing out this problem.351model assigns equal values to ?3 and ?4 (corre-sponding to D and B, respectively), ?3 is alwaysless than ?4, simply because there is more chanceof penalizing D rather than B.
This problem arisesprecisely because of the heuristic (i.e.
demotingthe constraint that prefers the wrong candidate) thatGLA uses to find the target grammar.The Gibbs sampler, on the other hand, does notdepend on heuristic rules in its search.
Since modesof the posterior p(?3|D) and p(?4|D) reside in neg-ative infinity, the posterior is truncated by I?i<K ,with K = 6, based on the discussion in 5.3.
Re-sults of the Gibbs sampler and two runs of GLA13are reported in Table 7.Input Output Obs Gibbs GLA1 GLA2/uba/ [ubita] 100% 95% 96% 96%[ubasita] 0% 5% 4% 4%/mar/ [marEsito] 50% 50% 38% 45%[marsito] 50% 50% 62% 55%/liryo/ [liri.ito] 90% 95% 96% 91.4%[liryosito] 10% 5% 4% 8.6%Table 7: Comparison of Gibbs sampler and GLA7 A comparison with Max-Ent modelsPreviously, problems with the GLA14 have inspiredother OT-like models of linguistic variation.
Onesuch proposal suggests using the more well-knownMaximum Entropy model (Goldwater and Johnson,2003).
In Max-Ent models, a grammar G is alsoparameterized by a real vector of weights w =(w1, ?
?
?
, wN ), but the conditional likelihood of anoutput y given an input x is given by:p(y|x) = exp{?i wifi(y, x)}?z exp{?i wifi(z, x)}(2)where fi(y, x) is the violation each constraint as-signs to the input-output pair (x, y).Clearly, Max-Ent is a rather different type ofmodel from Stochastic OT, not only in the useof constraint ordering, but also in the objectivefunction (conditional likelihood rather than likeli-hood/posterior).
However, it may be of interest tocompare these two types of models.
Using the same13The two runs here both use 0.002 and 0.0001 as the finalplasticity.
The initial plasticity and the iterations are set to 2and 1.0e7.
Slightly better fits can be found by tuning these pa-rameters, but the observation remains the same.14See (Keller and Asudeh, 2002) for a summary.data as in 6.2, results of fitting Max-Ent (using con-jugate gradient descent) and Stochastic OT (usingGibbs sampler) are reported in Table 8:Input Output Obs SOT ME MEsm/uba/ [ubita] 100% 95% 100% 97.5%[ubasita] 0% 5% 0% 2.5%/mar/ [marEsito] 50% 50% 50% 48.8%[marsito] 50% 50% 50% 51.2%/liryo/ [liri.ito] 90% 95% 90% 91.4%[liryosito] 10% 5% 10% 8.6%Table 8: Comparison of Max-Ent and Stochastic OT modelsIt can be seen that the Max-Ent model, in the ab-sence of a smoothing prior, fits the data perfectly byassigning positive weights to constraints B and D. Aless exact fit (denoted by MEsm) is obtained whenthe smoothing Gaussian prior is used with ?i = 0,?2i = 1.
But as observed in 6.2, an exact fit is im-possible to obtain using Stochastic OT, due to thedifference in the way variation is generated by themodels.
Thus it may be seen that Max-Ent is a morepowerful class of models than Stochastic OT, thoughit is not clear how the Max-Ent model?s descriptivepower is related to generative linguistic theories likephonology.Although the abundance of well-behaved opti-mization algorithms has been pointed out in favorof Max-Ent models, it is the author?s hope that theMCMC approach also gives Stochastic OT a sim-ilar underpinning.
However, complex StochasticOT models often bring worries about identifiability,whereas the convexity property of Max-Ent may beviewed as an advantage15.8 DiscussionFrom a non-Bayesian perspective, the MCMC-basedapproach can be seen as a randomized strategy forlearning a grammar.
Computing resources make itpossible to explore the entire space of grammars anddiscover where good hypotheses are likely to occur.In this paper, we have focused on the frequently vis-ited areas of the hypothesis space.It is worth pointing out that the Graduate LearningAlgorithm can also be seen from this perspective.An examination of the GLA shows that when theplasticity term is fixed, parameters found by GLAalso form a Markov chain G(t) ?
RN , t = 1, 2, ?
?
?
.Therefore, assuming the model is identifiable, it15Concerns about identifiability appear much more fre-quently in statistics than in linguistics.352seems possible to use GLA in the same way as theMCMC methods: rather than forcing it to stop, wecan run GLA until it reaches stationary distribution,if it exists.However, it is difficult to interpret the resultsfound by this ?random walk-GLA?
approach: thestationary distribution of GLA may not be the targetdistribution ?
the posterior p(G|D).
To constructa Markov chain that converges to p(G|D), one mayconsider turning GLA into a real MCMC algorithmby designing reversible jumps, or the Metropolis al-gorithm.
But this may not be easy, due to the diffi-culty in likelihood evaluation (including likelihoodratio) discussed in Section 2.In contrast, our algorithm provides a general solu-tion to the problem of learning Stochastic OT gram-mars.
Instead of looking for a Markov chain in RN ,we go to a higher dimensional space RN ?RN , us-ing the idea of data augmentation.
By taking advan-tage of the interdependence of G and Y , the Gibbssampler provides a Markov chain that converges top(G,Y |D), which allows us to return to the originalsubspace and derive p(G|D) ?
the target distribu-tion.
Interestingly, by adding more parameters, thecomputation becomes simpler.9 Future workThis work can be extended in two directions.
First,it would be interesting to consider other types ofOT grammars, in connection with the linguistics lit-erature.
For example, the variances of the normaldistribution are fixed in the current paper, but theymay also be treated as unknown parameters (Nagyand Reynolds, 1997).
Moreover, constraints may beparameterized as mixture distributions, which rep-resent other approaches to using OT for modelinglinguistic variation (Anttila, 1997).The second direction is to introduce informativepriors motivated by linguistic theories.
It is foundthrough experimentation that for more sophisticatedgrammars, identifiability often becomes an issue:some constraints may have multiple modes in theirposterior marginal, and it is difficult to extract modesin high dimensions16.
Therefore, use of priors isneeded in order to make more reliable inferences.
Inaddition, priors also have a linguistic appeal, since16Notice that posterior marginals do not provide enough in-formation for modes of the joint distribution.current research on the ?initial bias?
in language ac-quisition can be formulated as priors (e.g.
Faithful-ness Low (Hayes, 2004)) from a Bayesian perspec-tive.Implementing these extensions will merely in-volve modifying p(G|Y,D), which we leave for fu-ture work.ReferencesAnttila, A.
(1997).
Variation in Finnish Phonology and Mor-phology.
PhD thesis, Stanford University.Arbisi-Kelm, T. (2002).
An analysis of variability in Spanishdiminutive formation.
Master?s thesis, UCLA, Los Angeles.Boersma, P. (1997).
How we learn variation, optionality, prob-ability.
In Proceedings of the Institute of Phonetic Sciences21, pages 43?58, Amsterdam.
University of Amsterdam.Boersma, P. and Hayes, B. P. (2001).
Empirical tests of theGradual Learning Algorithm.
Linguistic Inquiry, 32:45?86.Gelfand, A. and Smith, A.
(1990).
Sampling-based approachesto calculating marginal densities.
Journal of the AmericanStatistical Association, 85(410).Gelman, A. and Rubin, D. B.
(1992).
Inference from iterativesimulation using multiple sequences.
Statistical Science,7:457?472.Geman, S. and Geman, D. (1984).
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration of images.IEEE Trans.
on Pattern Analysis and Machine Intelligence,6(6):721?741.Goldwater, S. and Johnson, M. (2003).
Learning OT constraintrankings using a Maximum Entropy model.
In Spenader,J., editor, Proceedings of the Workshop on Variation withinOptimality Theory, Stockholm.Hayes, B. P. (2004).
Phonological acquisition in optimality the-ory: The early stages.
In Kager, R., Pater, J., and Zonneveld,W., editors, Fixing Priorities: Constraints in PhonologicalAcquisition.
Cambridge University Press.Keller, F. and Asudeh, A.
(2002).
Probabilistic learningalgorithms and Optimality Theory.
Linguistic Inquiry,33(2):225?244.Liu, J. S. (2001).
Monte Carlo Strategies in Scientific Com-puting.
Number 33 in Springer Statistics Series.
Springer-Verlag, Berlin.Nagy, N. and Reynolds, B.
(1997).
Optimality theory and vari-able word-final deletion in Faetar.
Language Variation andChange, 9.Prince, A. and Smolensky, P. (1993).
Optimality Theory: Con-straint Interaction in Generative Grammar.
Forthcoming.Tanner, M. and Wong, W. H. (1987).
The calculation of poste-rior distributions by data augmentation.
Journal of the Amer-ican Statistical Association, 82(398).353
