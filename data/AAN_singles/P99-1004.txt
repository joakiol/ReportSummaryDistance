Measures of Distr ibutional SimilarityL i l l ian LeeDepartment  of Computer  ScienceCornell UniversityIthaca, NY 14853-7501llee@cs, cornell, eduAbst rac tWe study distributional similarity measures forthe purpose of improving probability estima-tion for unseen cooccurrences.
Our contribu-tions are three-fold: an empirical comparisonof a broad range of measures; a classificationof similarity functions based on the informationthat they incorporate; and the introduction ofa novel function that is superior at evaluatingpotential proxy distributions.1 IntroductionAn inherent problem for statistical methods innatural language processing is that of sparsedata - -  the inaccurate representation i anytraining corpus of the probability of low fre-quency events.
In particular, reasonable ventsthat happen to not occur in the training set maymistakenly be assigned a probability of zero.These unseen events generally make up a sub-stantial portion of novel data; for example, Es-sen and Steinbiss (1992) report that 12% of thetest-set bigrams in a 75%-25% split of one mil-lion words did not occur in the training parti-tion.We consider here the question of how to es-timate the conditional cooccurrence probabilityP(v\[n) of an unseen word pair (n, v) drawn fromsome finite set N x V. Two state-of-the-arttechnologies are Katz's (1987) backoff methodand Jelinek and Mercer's (1980) interpolationmethod.
Both use P(v) to estimate P(v\[n)when (n, v) is unseen, essentially ignoring theidentity of n.An alternative approach is distance-weightedaveraging, which arrives at an estimate for un-seen cooccurrences by combining estimates for25cooccurrences involving similar words: 1/P(v\[n) ---- ~-~mES(n) sim(n, m)P(v\[m)~-\]mES(n) sim(n, m) , (1)where S(n) is a set of candidate similar wordsand sim(n, m) is a function of the similaritybetween n and m. We focus on distributionalrather than semantic similarity (e.g., Resnik(1995)) because the goal of distance-weightedaveraging is to smooth probability distributions- -  although the words "chance" and "probabil-ity" are synonyms, the former may not be agood model for predicting what cooccurrencesthe latter is likely to participate in.There are many plausible measures of distri-butional similarity.
In previous work (Daganet al, 1999), we compared the performance ofthree different functions: the Jensen-Shannondivergence (total divergence to the average), theL1 norm, and the confusion probability.
Ourexperiments on a frequency-controlled pseu-doword disambiguation task showed that usingany of the three in a distance-weighted aver-aging scheme yielded large improvements overKatz's backoff smoothing method in predictingunseen coocurrences.
Furthermore, by using arestricted version of model (1) that stripped in-comparable parameters, we were able to empir-ically demonstrate hat the confusion probabil-ity is fundamentally worse at selecting usefulsimilar words.
D. Lin also found that the choiceof similarity function can affect the quality ofautomatically-constructed thesauri to a statis-tically significant degree (1998a) and the abilityto determine common morphological roots by asmuch as 49% in precision (1998b).1The term "similarity-based", which we have usedpreviously, has been applied to describe other modelsas well (L. Lee, 1997; Karov and Edelman, 1998).These empirical results indicate that investi-gating different similarity measures can lead toimproved natural anguage processing.
On theother hand, while there have been many sim-ilarity measures proposed and analyzed in theinformation retrieval literature (Jones and Fur-nas, 1987), there has been some doubt expressedin that community that the choice of similaritymetric has any practical impact:Several authors have pointed out thatthe difference in retrieval performanceachieved by different measures of asso-ciation is insignificant, providing thatthese are appropriately normalised.
(van Rijsbergen, 1979, pg.
38)But no contradiction arises because, as van Rijs-bergen continues, "one would expect this sincemost measures incorporate the same informa-tion".
In the language-modeling domain, thereis currently no agreed-upon best similarity met-ric because there is no agreement on what the"same in fo rmat ion" -  the key data that a sim-ilarity function should incorporate - -  is.The overall goal of the work described herewas to discover these key characteristics.
Tothis end, we first compared a number of com-mon similarity measures, evaluating them in aparameter-free way on a decision task.
Whengrouped by average performance, they fell intoseveral coherent classes, which corresponded tothe extent to which the functions focused onthe intersection of the supports (regions of posi-tive probability) of the distributions.
Using thisinsight, we developed an information-theoreticmetric, the skew divergence, which incorporatesthe support-intersection data in an asymmetricfashion.
This function yielded the best perfor-mance overall: an average rror rate reductionof 4% (significant at the .01 level) with respectto the Jensen-Shannon divergence, the best pre-dictor of unseen events in our earlier experi-ments (Dagan et al, 1999).Our contributions are thus three-fold: an em-pirical comparison of a broad range of similaritymetrics using an evaluation methodology thatfactors out inessential degrees of freedom; a pro-posal, building on this comparison, of a charac-teristic for classifying similarity functions; andthe introduction of a new similarity metric in-corporating this characteristic that is superiorat evaluating potential proxy distributions.2{}2 Distributional Similarity FunctionsIn this section, we describe the seven distri-butional similarity functions we initally evalu-ated.
2 For concreteness, we choose N and Vto be the set of nouns and the set of transitiveverbs, respectively; a cooccurrence pair (n, v)results when n appears as the head noun of thedirect object of v. We use P to denote probabil-ities assigned by a base language model (in ourexperiments, we simply used unsmoothed rel-ative frequencies derived from training corpuscounts).Let n and m be two nouns whose distribu-tional similarity is to be determined; for nota-tional simplicity, we write q(v) for P(vln ) andr(v) for P(vlm), their respective conditionalverb cooccurrence probabilities.Figure 1 lists several familiar functions.
Thecosine metric and Jaccard's coefficient are com-monly used in information retrieval as measuresof association (Salton and McGill, 1983).
Notethat Jaccard's coefficient differs from all theother measures we consider in that it is essen-tially combinatorial, being based only on thesizes of the supports of q, r, and q ?
r ratherthan the actual values of the distributions.Previously, we found the Jensen-Shannon di-vergence (Rao, 1982; J. Lin, 1991) to be a usefulmeasure of the distance between distributions:JS(q,r)=-~l \ [D(q  aVgq,r)+D(r aVgq,r) \]The function D is the KL divergence, whichmeasures the (always nonnegative) average in-efficiency in using one distribution to code foranother (Cover and Thomas, 1991):(v)D(pl(V) IIp2(V)) = EP l (V) log  P l  p2(v) "VThe function avga, r denotes the average distri-bution avgq,r(V ) --= (q(v)+r(v))/2; observe thatits use ensures that the Jensen-Shannon diver-gence is always defined.
In contrast, D(qllr ) isundefined if q is not absolutely continuous withrespect to r (i.e., the support of q is not a subsetof the support of r).2Strictly speaking, some of these funct ions are dissim-ilarity measures, but  each such funct ion f can be recastas a similarity funct ion via the s imple t rans format ionC - f ,  where C is an  appropr iate  constant .
Whether  wemean f or C - f should be clear from context.Euclidean distanceL1 normcosineJaccard's coefficientL2(q,r) =Ll(q,r) =cos(q, r) =Jac(q, r) =~v (q(v) - r(v)) 2Iq(v) - r(v)lV~-~v q(v)r(v)X/~-~v q(v) 2 V/Y~-v r(v) 2I{v : q(v) > 0 and r(v) > 0}lI{v I q(v) > 0 or r(v) > O}lFigure 1: Well-known functionsThe confusion probability has been used byseveral authors to smooth word cooccurrenceprobabilities (Sugawara et al, 1985; Essen andSteinbiss, 1992; Grishman and Sterling, 1993);it measures the degree to which word m canbe substituted into the contexts in which n ap-pears.
If the base language model probabili-ties obey certain Bayesian consistency condi-tions (Dagan et al, 1999), as is the case forrelative frequencies, then we may write the con-fusion probability as follows:P(m)conf(q, r, P(m) ) = E q(v)r(v) -p-~(v) "VNote that it incorporates unigram probabilitiesas well as the two distributions q and r.Finally, Kendall's % which appears in workon clustering similar adjectives (Hatzivassilo-glou and McKeown, 1993; Hatzivassiloglou,1996), is a nonparametric measure of the as-sociation between random variables (Gibbons,1993).
In our context, it looks for correlationbetween the behavior of q and r on pairs ofverbs.
Three versions exist; we use the simplest,Ta, here:r(q,r) = E sign \[(q(vl) - q(v2))(r(vl) - r(v2))\]v,,v  2(l t)where sign(x) is 1 for positive arguments, -1for negative arguments, and 0 at 0.
The intu-ition behind Kendall's T is as follows.
Assumeall verbs have distinct conditional probabilities.If sorting the verbs by the likelihoods assignedby q yields exactly the same ordering as thatwhich results from ranking them according tor, then T(q, r) = 1; if it yields exactly the op-posite ordering, then T(q, r) -- -1.
We treat avalue of -1  as indicating extreme dissimilarity.
3It is worth noting at this point that thereare several well-known measures from the NLPliterature that we have omitted from our ex-periments.
Arguably the most widely used isthe mutual information (Hindle, 1990; Churchand Hanks, 1990; Dagan et al, 1995; Luk,1995; D. Lin, 1998a).
It does not apply inthe present setting because it does not mea-sure the similarity between two arbitrary prob-ability distributions (in our case, P(VIn ) andP(VIm)) , but rather the similarity betweena joint distribution P(X1,X2) and the cor-responding product distribution P(X1)P(X2).Hamming-type metrics (Cardie, 1993; Zavreland Daelemans, 1997) are intended for datawith symbolic features, since they count featurelabel mismatches, whereas we are dealing fea-ture Values that are probabilities.
Variations ofthe value difference metric (Stanfill and Waltz,1986) have been employed for supervised isam-biguation (Ng and H.B.
Lee, 1996; Ng, 1997);but it is not reasonable in language modeling toexpect raining data tagged with correct prob-abilities.
The Dice coej~cient (Smadja et al,1996; D. Lin, 1998a, 1998b) is monotonic in Jac-card's coefficient (van Rijsbergen, 1979), so itsinclusion in our experiments would be redun-dant.
Finally, we did not use the KL divergencebecause it requires a smoothed base languagemodel.SZero would also be a reasonable choice, since it in-dicates zero correlation between q and r. However, itwould then not be clear how to average in the estimatesof negatively correlated words in equation (1).273 Empi r i ca l  Compar i sonWe evaluated the similarity functions intro-duced in the previous section on a binary dec-ision task, using the same experimental frame-work as in our previous preliminary compari-son (Dagan et al, 1999).
That is, the dataconsisted of the verb-object cooccurrence pairsin the 1988 Associated Press newswire involv-ing the 1000 most frequent nouns, extractedvia Church's (1988) and Yarowsky's process-ing tools.
587,833 (80%) of the pairs servedas a training set from which to calculate baseprobabilities.
From the other 20%, we pre-pared test sets as follows: after discarding pairsoccurring in the training data (after all, thepoint of similarity-based estimation is to dealwith unseen pairs), we split the remaining pairsinto five partitions, and replaced each noun-verb pair (n, vl) with a noun-verb-verb triple(n, vl, v2) such that P(v2) ~ P(vl).
The taskfor the language model under evaluation wasto reconstruct which of (n, vl) and (n, v2) wasthe original cooccurrence.
Note that by con-struction, (n, Vl) was always the correct answer,and furthermore, methods relying solely on uni-gram frequencies would perform no better thanchance.
Test-set performance was measured bythe error rate, defined asT (# of incorrect choices + (# of ties)/2),where T is the number of test triple tokens inthe set, and a tie results when both alternativesare deemed equally likely by the language modelin question.To perform the evaluation, we incorporatedeach similarity function into a decision rule asfollows.
For a given similarity measure f andneighborhood size k, let 3f, k(n) denote the kmost similar words to n according to f. Wedefine the evidence according to f for the cooc-currence ( n, v~) asEf, k(n, vi) = \[(m E SLk(n) : P(vilm) > l }l ?Then, the decision rule was to choose the alter-native with the greatest evidence.The reason we used a restricted version of thedistance-weighted averaging model was that wesought to discover fundamental differences inbehavior.
Because we have a binary decisiontask, Ef,k(n, vl) simply counts the number of knearest neighbors to n that make the right de-cision.
If we have two functions f and g suchthat Ef,k(n, Vl) > Eg,k(n, vi), then the k mostsimilar words according to f are on the wholebetter predictors than the k most similar wordsaccording to g; hence, f induces an inherentlybetter similarity ranking for distance-weightedaveraging.
The difficulty with using the fullmodel (Equation (1)) for comparison purposesis that fundamental differences can be obscuredby issues of weighting.
For example, supposethe probability estimate ~v(2  -L l (q ,  r)).
r(v)(suitably normalized) performed poorly.
Wewould not be able to tell whether the causewas an inherent deficiency in the L1 norm orjust a poor choice of weight function - -  per-haps (2 -  Ll(q,r)) 2 would have yielded betterestimates.Figure 2 shows how the average error ratevaries with k for the seven similarity metricsintroduced above.
As previously mentioned, asteeper slope indicates a better similarity rank-ing.All the curves have a generally upward trendbut always lie far below backoff (51% errorrate).
They meet at k = 1000 because Sf, looo(n)is always the set of all nouns.
We see that thefunctions fall into four groups: (1) the L2 norm;(2) Kendall's T; (3) the confusion probabilityand the cosine metric; and (4) the L1 norm,Jensen-Shannon divergence, and Jaccard's co-efficient.We can account for the similar performanceof various metrics by analyzing how they incor-porate information from the intersection of thesupports of q and r. (Recall that we are usingq and r for the conditional verb cooccurrrenceprobabilities of two nouns n and m.) Considerthe following supports (illustrated in Figure 3):Vq = {veV : q (v )>O}= {v ?V : r (v )>0}Yqr = {v ?
V : q(v)r(v) > 0} = Yq nWe can rewrite the similarity functions fromSection 2 in terms of these sets, making use ofthe identities ~-~veyq\yq~ q(v) + ~veyq~ q(v) =~'~-v~U~\Vq~ r(v) + ~v~Vq~ r(v) = 1.
Table 1 liststhese alternative forms in order of performance.280.40.380.360.34~ 0.320 .3 - -0.280.26100Error rates (averages and ranges)I i i I iI.,2-*.--Jag~200 300 400 500 600 700 800 900 1000kFigure 2: Similarity metric performance.
Errorbars denote the range of error rates over the fivetest sets.
Backoff's average rror rate was 51%.L2(q, r ).
2(l l)= , /Eq(v )2 -2Eq(v) r (v )+ Er (v )  2V.
.
vq~ v~= 2 IVq~l IV \ (vq u V~)l - 2 IVq \ Vail Iv~ \Vq~l+ E E sign\[(q(vl) - q(v2))(r (v l )  - r(v2))\]Vl E(VqA Vr) v2EYq~,+ E E s ign \ [ (q (v l ) -q (v2) ) ( r (v l ) - r (v2) ) \ ]Vl eVqr v2EVqUVrconf(q, r, P(m))cos(q, r)= P(ra)  Y\] q (v ) r (v ) /P (v )v e Vq~= E q(v)r (v) (  E q(v) 2 E r(v)2) -1/2v~ Vqr ve Vq v~ VrL l (q , r )JS (q ,  r)Jac(q, r)= 2-- E ( Iq (v ) - r (v ) l -q (v ) - r (v ) )vE Vqr= log2 + 1 E (h(q(v) + r(v))  - h(q(v))  - h( r (v) ) )  ,v ~ Vq~= IV~l/IV~ u v~lh( x ) = -x  log xTable 1: Similarity functions, written in terms of sums over supports and grouped by averageperformance.
\ denotes et difference; A denotes ymmetric set difference.We see that for the non-combinatorial functions,the groups correspond to the degree to whichthe measures rely on the verbs in Vat.
TheJensen-Shannon divergence and the L1 normcan be computed simply by knowing the val-ues of q and r on Vqr.
For the cosine and theconfusion probability, the distribution values onVqr are key, but other information is also incor-porated.
The statistic Ta takes into account allverbs, including those that occur neither with29vFigure 3: Supports on Vn nor m. Finally, the Euclidean distance isquadratic in verbs outside Vat; indeed, Kaufmanand Rousseeuw (1990) note that it is "extremelysensitive to the effect of one or more outliers"(pg.
117).The superior performance of Jac(q, r) seemsto underscore the importance of the set Vqr.Jaccard's coefficient ignores the values of q andr on Vqr; but we see that simply knowing thesize of Vqr relative to the supports of q and rleads to good rankings.4 The  Skew D ivergenceBased on the results just described, it appearsthat it is desirable to have a similarity func-tion that focuses on the verbs that cooccur withboth of the nouns being compared.
However,we can make a further observation: with theexception of the confusion probability, all thefunctions we compared are symmetric, that is,f(q, r) -= f(r, q).
But the substitutability ofone word for another need not symmetric.
Forinstance, "fruit" may be the best possible ap-proximation to "apple", but the distribution of"apple" may not be a suitable proxy for the dis-tribution of "fruit".aIn accordance with this insight, we developeda novel asymmetric generalization of the KL di-vergence, the a-skew divergence:sa(q,r) = D(r \[\[a'q + (1 - a ) - r )for 0 <_ a < 1.
It can easily be shown that sadepends only on the verbs in Vat.
Note that ata -- 1, the skew divergence is exactly the KL di-vergence, and su2 is twice one of the summandsof JS  (note that it is still asymmetric).40n a related note, an anonymous reviewer cited thefollowing example from the psychology literature: we cansay Smith's lecture is like a sleeping pill, but "not theother way round".30We can think of a as a degree of confidencein the empirical distribution q; or, equivalently,(1 - a) can be thought of as controlling theamount by which one smooths q by r. Thus,we can view the skew divergence as an approx-imation to the KL divergence to be used whensparse data problems would cause the lattermeasure to be undefined.Figure 4 shows the performance of sa fora = .99.
It performs better than all the otherfunctions; the difference with respect to Jac-card's coefficient is statistically significant, ac-cording to the paired t-test, at all k (exceptk = 1000), with significance level .01 at all kexcept 100, 400, and 1000.5 D iscuss ionIn this paper, we empirically evaluated a num-ber of distributional similarity measures, includ-ing the skew divergence, and analyzed their in-formation sources.
We observed that the abilityof a similarity function f(q, r) to select usefulnearest neighbors appears to be correlated withits focus on the intersection Vqr of the supportsof q and r. This is of interest from a computa-tional point of view because Vqr tends to be arelatively small subset of V, the set of all verbs.Furthermore, it suggests downplaying the role ofnegative information, which is encoded by verbsappearing with exactly one noun, although theJaccard coefficient does take this type of infor-mation into account.Our explicit division of V-space into vari-ous support regions has been implicitly con-sidered in other work.
Smadja et al (1996)observe that for two potential mutual transla-tions X and Y, the fact that X occurs withtranslation Y indicates association; X's occur-ring with a translation other than Y decreasesone's belief in their association; but the absenceof both X and Y yields no information.
Inessence, Smadja et al argue that informationfrom the union of supports, rather than the justthe intersection, is important.
D. Lin (1997;1998a) takes an axiomatic approach to deter-mining the characteristics of a good similaritymeasure.
Starting with a formalization (basedon certain assumptions) ofthe intuition that thesimilarity between two events depends on boththeir commonality and their differences, he de-rives a unique similarity function schema.
The0.40.38 I0.36 \[0.340.320.30.280.26 ?- 100Error rates (averages and ranges)L1JS~0 300 ~0 ~0 600 700 800 ~0 1000kFigure 4: Performance of the skew divergence with respect o the best functions from Figure 2.definition of commonality is left to the user (sev-eral different definitions are proposed for differ-ent tasks).We view the empirical approach taken in thispaper as complementary to Lin's.
That is, weare working in the context of a particular appli-cation, and, while we have no mathematical cer-tainty of the importance of the "common sup-port" information, we did not assume it a priori;rather, we let the performance data guide ourthinking.Finally, we observe that the skew metricseems quite promising.
We conjecture that ap-propriate values for a may inversely correspondto the degree of sparseness in the data, andintend in the future to test this conjecture onlarger-scale prediction tasks.
We also plan toevaluate skewed versions of the Jensen-Shannondivergence proposed by Rao (1982) and J. Lin(1991).6 AcknowledgementsThanks to Claire Cardie, Jon Kleinberg, Fer-nando Pereira, and Stuart Shieber for helpfuldiscussions, the anonymous reviewers for theirinsightful comments, Fernando Pereira for ac-cess to computational resources at AT&T, andStuart Shieber for the opportunity to pursuethis work at Harvard University under NSFGrant No.
IRI9712068.ReferencesClaire Cardie.
1993.
A case-based approachto knowledge acquisition for domain-specificsentence analysis.
In 11th National Confer-ence on Artifical Intelligence, pages 798-803.Kenneth Ward Church and Patrick Hanks.1990.
Word association orms, mutual in-formation, and lexicography.
ComputationalLinguistics, 16(1):22-29.Kenneth W. Church.
1988.
A stochastic partsprogram and noun phrase parser for un-restricted text.
In Second Conference onApplied Natural Language Processing, pages136-143.Thomas M. Cover and Joy A. Thomas.
1991.Elements of Information Theory.
John Wiley.Ido Dagan, Shanl Marcus, and Shanl Marko-vitch.
1995.
Contextual word similarityand estimation from sparse data.
ComputerSpeech and Language, 9:123-152.Ido Dagan, Lillian Lee, and Fernando Pereira.1999.
Similarity-based models of cooccur-rence probabilities.
Machine Learning, 34(1-3) :43-69.Ute Essen and Volker Steinbiss.
1992.
Co-occurrence smoothing for stochastic languagemodeling.
In ICASSP 92, volume 1, pages161-164.Jean Dickinson Gibbons.
1993.
NonparametricMeasures of Association.
Sage University Pa-per series on Quantitative Applications in the31Social Sciences, 07-091.
Sage Publications.Ralph Grishman and John Sterling.
1993.Smoothing of automatically generated selec-tional constraints.
In Human Language Tech-nology: Proceedings of the ARPA Workshop,pages 254-259.Vasileios Hatzivassiloglou and Kathleen McKe-own.
1993.
Towards the automatic dentifica-tion of adjectival scales: Clustering of adjec-tives according to meaning.
In 31st AnnualMeeting of the ACL, pages 172-182.Vasileios Hatzivassiloglou.
1996.
Do we needlinguistics when we have statistics?
A com-parative analysis of the contributions of lin-guistic cues to a statistical word groupingsystem.
In Judith L. Klavans and PhilipResnik, editors, The Balancing Act, pages 67-94.
MIT Press.Don Hindle.
1990.
Noun classification frompredicate-argument structures.
In 28th An-nual Meeting of the A CL, pages 268-275.Frederick Jelinek and Robert L. Mercer.
1980.Interpolated estimation of Markov source pa-rameters from sparse data.
In Proceedingsof the Workshop on Pattern Recognition inPractice.William P. Jones and George W. Furnas.1987.
Pictures of relevance.
Journal of theAmerican Society for Information Science,38(6):420-442.Yael Karov and Shimon Edelman.
1998.Similarity-based word sense disambiguation.Computational Linguistics, 24(1):41-59.Slava M. Katz.
1987.
Estimation of probabili-ties from sparse data for the language modelcomponent of a speech recognizer.
IEEETransactions on Acoustics, Speech and SignalProcessing, ASSP-35(3):400--401, March.Leonard Kanfman and Peter J. Rousseeuw.1990.
Finding Groups in Data: An Intro-duction to Cluster Analysis.
John Wiley andSons.Lillian Lee.
1997.
Similarity-Based Approachesto Natural Language Processing.
Ph.D. the-sis, Harvard University.Dekang Lin.
1997.
Using syntactic dependencyas local context o resolve word sense ambi-guity.
In 35th Annual Meeting of the ACL,pages 64-71.Dekang Lin.
1998a.
Automatic retrieval and32clustering of similar words.
In COLING-A CL'98, pages 768-773.Dekang Lin.
1998b.
An information theoreticdefinition of similarity.
In Machine Learn-ing: Proceedings of the Fiftheenth Interna-tional Conference (ICML '98).Jianhua Lin.
1991.
Divergence measures basedon the Shannon entropy.
IEEE Transactionson Information Theory, 37(1):145-151.Alpha K. Luk.
1995.
Statistical sense disam-biguation with relatively small corpora usingdictionary definitions.
In 33rd Annual Meet-ing of the ACL, pages 181-188.Hwee Tou Ng and Hian Beng Lee.
1996.
Inte-grating multiple knowledge sources to disam-biguate word sense: An exemplar-based ap-proach.
In 3~th Annual Meeting of the ACL,pages 40--47.Hwee Tou Ng.
1997.
Exemplar-based wordsense disambiguation: Some recent improve-ments.
In Second Conference on Empiri-cal Methods in Natural Language Processing(EMNLP-2), pages 208-213.C.
Radhakrishna Rao.
1982.
Diversity: Itsmeasurement, decomposition, apportionmentand analysis.
SankyhZt: The Indian Journalof Statistics, 44(A):1-22.Philip Resnik.
1995.
Using information contentto evaluate semantic similarity in a taxonomy.In Proceedings of IJCAI-95, pages 448-453.Gerard Salton and Michael J. McGill.
1983.
In-troduction to Modern Information Retrieval.McGraw-Hill.Frank Smadja, Kathleen R. McKeown, andVasileios Hatzivassiloglou.
1996.
Translat-ing collocations for bilingual exicons: A sta-tistical approach.
Computational Linguistics,22(1):1-38.Craig Stanfill and David Waltz.
1986.
To-ward memory-based reasoning.
Communica-tions of the ACM, 29(12):1213-1228.K.
Sugawara, M. Nishimura, K. Toshioka,M.
Okochi, and T. Kaneko.
1985.
Isolatedword recognition using hidden Markov mod-els.
In ICASSP 85, pages 1-4.C.
J. van Rijsbergen.
1979.
Information Re-trieval.
Butterworths, econd edition.Jakub Zavrel and Walter Daelemans.
1997.Memory-based learning: Using similarity forsmoothing.
In 35th Annual Meeting of theA CL, pages 436-443.
