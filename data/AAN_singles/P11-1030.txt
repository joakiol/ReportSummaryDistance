Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 288?298,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLocal Histograms of Character N -grams for Authorship AttributionHugo Jair EscalanteGraduate Program in Systems Eng.Universidad Auto?noma de Nuevo Leo?n,San Nicola?s de los Garza, NL, 66450, Me?xicohugo.jair@gmail.comThamar SolorioDept.
of Computer and Information SciencesUniversity of Alabama at Birmingham,Birmingham, AL, 35294, USAsolorio@cis.uab.eduManuel Montes-y-Go?mezComputer Science Department, INAOE,Tonantzintla, Puebla, 72840, Me?xicoDepartment of Computer and Information Sciences,University of Alabama at Birmingham,Birmingham, AL, 35294, USAmmontesg@cis.uab.eduAbstractThis paper proposes the use of local his-tograms (LH) over character n-grams for au-thorship attribution (AA).
LHs are enrichedhistogram representations that preserve se-quential information in documents; they havebeen successfully used for text categorizationand document visualization using word his-tograms.
In this work we explore the suitabil-ity of LHs over n-grams at the character-levelfor AA.
We show that LHs are particularlyhelpful for AA, because they provide usefulinformation for uncovering, to some extent,the writing style of authors.
We report experi-mental results in AA data sets that confirm thatLHs over character n-grams are more help-ful for AA than the usual global histograms,yielding results far superior to state of the artapproaches.
We found that LHs are even moreadvantageous in challenging conditions, suchas having imbalanced and small training sets.Our results motivate further research on theuse of LHs for modeling the writing style ofauthors for related tasks, such as authorshipverification and plagiarism detection.1 IntroductionAuthorship attribution (AA) is the task of decidingwhom, from a set of candidates, is the author of agiven document (Houvardas and Stamatatos, 2006;Luyckx and Daelemans, 2010; Stamatatos, 2009b).There is a broad field of application for AA meth-ods, including spam filtering (de Vel et al, 2001),fraud detection, computer forensics (Lambers andVeenman, 2009), cyber bullying (Pillay and Solorio,2010) and plagiarism detection (Stamatatos, 2009a).Therefore, the development of automated AA tech-niques has received much attention recently (Sta-matatos, 2009b).
The AA problem can be natu-rally posed as one of single-label multiclass clas-sification, with as many classes as candidate au-thors.
However, unlike usual text categorizationtasks, where the core problem is modeling the the-matic content of documents (Sebastiani, 2002), thegoal in AA is modeling authors?
writing style (Sta-matatos, 2009b).
Hence, document representationsthat reveal information about writing style are re-quired to achieve good accuracy in AA.Word and character based representations havebeen used in AA with some success so far (Houvar-das and Stamatatos, 2006; Luyckx and Daelemans,2010; Plakias and Stamatatos, 2008b).
Such rep-resentations can capture style information throughword or character usage, but they lack sequential in-formation, which can reveal further stylistic infor-mation.
In this paper, we study the use of richerdocument representations for the AA task.
In partic-ular, we consider local histograms over n-grams atthe character-level obtained via the locally-weightedbag of words (LOWBOW) framework (Lebanon etal., 2007).Under LOWBOW, a document is represented by aset of local histograms, computed across the wholedocument but smoothed by kernels centered on dif-ferent document locations.
In this way, document288representations preserve both word/character usageand sequential information (i.e., information aboutthe positions in which words or characters occur),which can be more helpful for modeling the writ-ing style of authors.
We report experimental re-sults in an AA data set used in previous studies un-der several conditions (Houvardas and Stamatatos,2006; Plakias and Stamatatos, 2008b; Plakias andStamatatos, 2008a).
Results confirm that local his-tograms of character n-grams are more helpful forAA than the usual global histograms of words orcharacter n-grams (Luyckx and Daelemans, 2010);our results are superior to those reported in re-lated works.
We also show that local histogramsover character n-grams are more helpful than lo-cal histograms over words, as originally proposedby (Lebanon et al, 2007).
Further, we performedexperiments with imbalanced and small trainingsets (i.e., under a realistic AA setting) using theaforementioned representations.
We found that theLOWBOW-based representation resulted even moreadvantageous in these challenging conditions.
Thecontributions of this work are as follows:?
We show that the LOWBOW framework can behelpful for AA, giving evidence that sequential in-formation encoded in local histograms is useful formodeling the writing style of authors.?
We propose the use of local histograms overcharacter-level n-grams for AA.
We show thatcharacter-level representations, which have provedto be very effective for AA (Luyckx and Daelemans,2010), can be further improved by adopting a localhistogram formulation.
Also, we empirically showthat local histograms at the character-level are morehelpful than local histograms at the word-level forAA.?
We study several kernels for a support vector ma-chine AA classifier under the local histograms for-mulation.
Our study confirms that the diffusion ker-nel (Lafferty and Lebanon, 2005) is the most ef-fective among those we tried, although competitiveperformance can be obtained with simpler kernels.?
We report experimental results that are superior tostate of the art approaches (Plakias and Stamatatos,2008b; Plakias and Stamatatos, 2008a), with im-provements ranging from 2%?6% in balanced datasets and from 14%?
30% in imbalanced data sets.2 Related WorkAA can be faced as a multiclass classifica-tion task with as many classes as candidate au-thors.
Standard classification methods have beenapplied to this problem, including support vec-tor machine (SVM) classifiers (Houvardas and Sta-matatos, 2006) and variants thereon (Plakias andStamatatos, 2008b; Plakias and Stamatatos, 2008a),neural networks (Tearle et al, 2008), Bayesian clas-sifiers (Coyotl-Morales et al, 2006), decision treemethods (Koppel et al, 2009) and similarity basedtechniques (Keselj et al, 2003; Lambers and Veen-man, 2009; Stamatatos, 2009b; Koppel et al, 2009).In this work, we chose an SVM classifier as it hasreported acceptable performance in AA and becauseit will allow us to directly compare results with pre-vious work that has used this same classifier.A broad diversity of features has been used to rep-resent documents in AA (Stamatatos, 2009b).
How-ever, as in text categorization (Sebastiani, 2002),word-based and character-based features are amongthe most widely used features (Stamatatos, 2009b;Luyckx and Daelemans, 2010).
With respect toword-based features, word histograms (i.e., the bag-of-words paradigm) are the most frequently usedrepresentations in AA (Zhao and Zobel, 2005;Argamon and Levitan, 2005; Stamatatos, 2009b).Some researchers have gone a step further andhave attempted to capture sequential informationby using n-grams at the word-level (Peng et al,2004) or by discovering maximal frequent word se-quences (Coyotl-Morales et al, 2006).
Unfortu-nately, because of computational limitations, the lat-ter methods cannot discover enough sequential in-formation from documents (e.g., word n-grams areoften restricted to n ?
{1, 2, 3}, while full se-quential information would be obtained with n ?
{1 .
.
.
D} where D is the maximum number ofwords in a document).With respect to character-based features, n-gramsat the character level have been widely used in AAas well (Plakias and Stamatatos, 2008b; Peng etal., 2003; Luyckx and Daelemans, 2010).
Peng etal.
(2003) propose the use of language models at then-gram character-level for AA, whereas Keselj etal.
(2003) build author profiles based on a selectionof frequent n-grams for each author.
Stamatatos andco-workers have studied the impact of feature se-lection, with character n-grams, in AA (Houvardasand Stamatatos, 2006; Stamatatos, 2006a), ensem-ble learning with character n-grams (Stamatatos,2006b) and novel classification techniques based289on characters at the n-gram level (Plakias and Sta-matatos, 2008a).Acceptable performance in AA has been reportedwith character n-gram representations.
However,as with word-based features, character n-grams areunable to incorporate sequential information fromdocuments in their original form (in terms of thepositions in which the terms appear across a doc-ument).
We believe that sequential clues can behelpful for AA because different authors are ex-pected to use different character n-grams or wordsin different parts of the document.
Accordingly,in this work we adopt the popular character-basedand word-based representations, but we enrich themin a way that they incorporate sequential informa-tion via the LOWBOW framework.
Hence, the pro-posed features preserve sequential information be-sides capturing character and word usage informa-tion.
Our hypothesis is that the combination of se-quential and frequency information can be particu-larly helpful for AA.The LOWBOW framework has been mainly usedfor document visualization (Lebanon et al, 2007;Mao et al, 2007), where researchers have used in-formation derived from local histograms for dis-playing a 2D representation of document?s con-tent.
More recently, Chasanis et al (2009) usedthe LOWBOW framework for segmenting moviesinto chapters and scenes.
LOWBOW representa-tions have also been applied to discourse segmen-tation (AMIDA, 2007) and have been suggested fortext summarization (Das and Martins, 2007).
How-ever, to the best of our knowledge the use of theLOWBOW framework for AA has not been studiedelsewhere.
Actually, the only two references usingthis framework for text categorization are (Lebanonet al, 2007; AMIDA, 2007).
The latter can be due tothe fact that local histograms provide little gain overusual global histograms for thematic classificationtasks.
In this paper we show that LOWBOW rep-resentations provide important improvements overglobal histograms for AA; in particular, local his-tograms at the character-level achieve the highestperformance in our experiments.3 BackgroundThis section describes preliminary information ondocument representations and pattern classificationwith SVMs.3.1 Bag of words representationsIn the bag of words (BOW) representation, docu-ments are represented by histograms over the vo-cabulary1 that was used to generate a collection ofdocuments; that is, a document i is represented as:di = [xi,1, .
.
.
, xi,|V |] (1)where V is the vocabulary and |V | is the number ofelements in V , di,j = xi,j is a weight that denotesthe contribution of term j to the representation ofdocument i; usually xi,j is related to the occurrence(binary weighting) or the weighted frequency of oc-currence (e.g., the tf-idf weighting scheme) of theterm j in document i.3.2 Locally-weighted bag-of-wordsrepresentationInstead of using the BOW framework directly, weadopted the LOWBOW framework for documentrepresentation (Lebanon et al, 2007).
The underly-ing idea in LOWBOW is to compute several localhistograms per document, where these histogramsare smoothed by a kernel function, see Figure 1.The parameters of the kernel specify the position ofthe kernel in the document (i.e., where the local his-togram is centered) and its scale (i.e., to what extentit is smoothed).
In this way the sequential informa-tion in the document is preserved together with termusage statistics.Let Wi = {wi,1, .
.
.
, wi,Ni}, denote the terms(in order of appearance) in document i where Niis the number of terms that appear in document iand wi,j ?
V is the term appearing at positionj; let vi = {vi,1, .
.
.
, vi,Ni} be the set of indexesin the vocabulary V of the terms appearing in Wi,such that vi,j is the index in V of the term wi,j ;let t = [t1, .
.
.
, tNi ] be a set of (equally spaced)scalars that determine intervals, with 0 ?
tj ?
1 and?Nij=1 tj = 1, such that each tj can be associated toa position in Wi.
Given a kernel smoothing functionKs?,?
: [0, 1] ?
R with location parameter ?
andscale parameter ?, where ?kj=1 Ks?,?
(tj) = 1 and1In the following we will refer to arbitrary vocabularies,which can be formed with terms from either words or charactern-grams.290Figure 1: Diagram of the process for obtaining localhistograms.
Terms (wi) appearing in different posi-tions (1, .
.
.
, N ) of the document are weighted accordingto the locations (?1, .
.
.
, ?k) of the smoothing functionK?,?(x).
Then, the term position weighting is combinedwith term frequency weighting for obtaining local his-tograms over the terms in the vocabulary (1, .
.
.
, |V |).?
?
[0, 1].
The LOWBOW framework computes alocal histogram for each position ?j ?
{?1, .
.
.
, ?k}as follows:dlji,{vi,1,...,vi,Ni} = di,{vi,1,...,vi,Ni} ?Ks?j ,?
(t) (2)where dli,vj :vj 6?vi = const, a small constant value,and di,j is defined as above.
Hence, a set dl{1,...,k}iof k local histograms are computed for each doc-ument i.
Each histogram dlji carries informationabout the distribution of terms at a certain position?j of the document, where ?
determines how thenearby terms to ?j influence the local histogramj.
Thus, sequential information of the document isconsidered throughout these local histograms.
Notethat when ?
is small, most of the sequential informa-tion is preserved, as local histograms are calculatedat very local scales; whereas when ?
?
1, local his-tograms resemble the traditional BOW representa-tion.Under LOWBOW documents can be representedin two forms (Lebanon et al, 2007): as a single his-togram dLi = const ?
?kj=1 dlji (hereafter LOW-BOW histograms) or by the set of local histogramsitself dl{1,...,k}i .
We performed experiments withboth forms of representation and considered wordsand n-grams at the character-level as terms (c.f.
Sec-tion 5).
Regarding the smoothing function, we con-sidered the re-normalized Gaussian pdf restricted to[0, 1]:Ks?,?
(x) =??
?N (x;?,?)?
( 1???
)??(??? )
if x ?
[0, 1]0 otherwise(3)where ?
(x) is the cumulative distribution functionfor a Gaussian with mean 0 and standard deviation 1,evaluated at x, see (Lebanon et al, 2007) for furtherdetails.3.3 Support vector machinesSupport vector machines (SVMs) are pattern classi-fication methods that aim to find an optimal sepa-rating hyperplane between examples from two dif-ferent classes (Shawe-Taylor and Cristianini, 2004).Let {xi, yi}N be pairs of training patterns-outputs,where xi ?
Rd and y ?
{?1, 1}, with d the di-mensionality of the problem.
SVMs aim at learn-ing a mapping from training instances to outputs.This is done by considering a linear function of theform: f(x) = Wx + b, where parameters W and bare learned from training data.
The particular linearfunction considered by SVMs is as follows:f(x) =?i?iyiK(xi, x)?
b (4)that is, a linear function over (a subset of) trainingexamples, where ?i is the weight associated withtraining example i (those for which ?i > 0 are the socalled support vectors) and yi is the label associatedwith training example i, K(xi, xj) is a kernel2 func-tion that aims at mapping the input vectors, (xi, xj),into the so called feature space, and b is a biasterm.
Intuitively, K(xi, xj) evaluates how similarinstances xi and xj are, thus the particular choice ofkernel is problem dependent.
The parameters in ex-pression (4), namely ?
{1,...,N} and b, are learned byusing exact optimization techniques (Shawe-Taylorand Cristianini, 2004).2One should not confuse the kernel smoothing function,Ks?,?
(x), defined in Equation (3) with the Mercer kernel inEquation (4), as the former acts as a smoothing function andthe latter acts as a similarity function.2914 Authorship Attribution with LOWBOWRepresentationsFor AA we represent the training documents ofeach author using the framework described in Sec-tion 3.2, thus each document of each candidate au-thor is either a LOWBOW histogram or a bag of lo-cal histograms (BOLH).
Recall that LOWBOW his-tograms are an un-weighted sum of local histogramsand hence can be considered a summary of term us-age and sequential information; whereas the BOLHcan be seen as term occurrence frequencies acrossdifferent locations of the document.For both types of representations we consider anSVM classifier under the one-vs-all formulation forfacing the AA problem.
We consider SVM as baseclassifier because this method has proved to be veryeffective in a large number of applications, includingAA (Houvardas and Stamatatos, 2006; Plakias andStamatatos, 2008b; Plakias and Stamatatos, 2008a);further, since SVMs are kernel-based methods, theyallow us to use local histograms for AA by consid-ering kernels that work over sets of histograms.We build a multiclass SVM classifier by con-sidering the pairs of patterns-outputs associated todocuments-authors.
Where each pattern can be ei-ther a LOWBOW histogram or the set of local his-tograms associated with the corresponding docu-ment, and the output associated to each pattern isa categorical random variable (outputs) that asso-ciates the representation of each document to its cor-responding author y1,...,N ?
{1, .
.
.
, C}, with Cthe number of candidate authors.
For building themulticlass classifier we adopted the one-vs-all for-mulation, where C binary classifiers are built andwhere each classifier fi discriminates among exam-ples from class i (positive examples) and the restj : j ?
{1, .
.
.
, C}, j 6= i; despite being one of thesimplest formulations, this approach has shown toobtain comparable and even superior performance tothat obtained by more complex formulations (Rifkinand Klautau, 2004).For AA using LOWBOW histograms, we con-sider a linear kernel since it has been success-fully applied to a wide variety of problems (Shawe-Taylor and Cristianini, 2004), including AA (Hou-vardas and Stamatatos, 2006; Plakias and Sta-matatos, 2008b).
However, standard kernels can-not work for input spaces where each instance is de-scribed by a set of vectors.
Therefore, usual kernelsare not applicable for AA using BOLH.
Instead, werely on particular kernels defined for sets of vectorsrather than for a single vector.
Specifically, we con-sider kernels of the form (Rubner et al, 2001; Grau-man, 2006):K(P,Q) = exp (?
D(P,Q)2?)
(5)where D(P,Q) is the sum of the distances betweenthe elements of the bag of local histograms asso-ciated to author P and the elements of the bag ofhistograms associated with author Q; ?
is the scaleparameter of K. Let P = {p1, .
.
.
, pk} and Q ={q1, .
.
.
, qk} be the elements of the bags of localhistograms for instances P and Q, respectively, Ta-ble 1 presents the distance measures we consider forAA using local histograms.Kernel DistanceDiffusion D(P,Q) = ?kl=1 arccos(?
?pl ??ql?
)EMD D(P,Q) = EMD(P,Q)Eucidean D(P,Q) =?
?kl=1(pl ?
ql).2?2 D(P,Q) =?
?kl=1(pl?ql)2(pl+ql)Table 1: Distance functions used to calculate the kerneldefined in Equation (5).Diffusion, Euclidean, and ?2 kernels compare lo-cal histograms one to one, which means that the lo-cal histograms calculated at the same locations arecompared to each other.
We believe that for AAthis is advantageous as it is expected that an authoruses similar terms at similar locations of the docu-ment.
The Earth mover?s distance (EMD), on theother hand, is an estimate of the optimal cost in tak-ing local histograms from Q to local histograms inP (Rubner et al, 2001); that is, this measure com-putes the optimal matching distance between localhistograms from different authors that are not neces-sarily computed at similar locations.5 Experiments and ResultsFor our experiments we considered the data set usedin (Plakias and Stamatatos, 2008b; Plakias and Sta-matatos, 2008a).
This corpus is a subset of theRCV1 collection (Lewis et al, 2004) and comprises292documents authored by 10 authors.
All of the docu-ments belong to the same topic.
Since this data sethas predefined training and testing partitions, our re-sults are comparable to those obtained by other re-searchers.
There are 50 documents per author fortraining and 50 documents per author for testing.We performed experiments with LOWBOW3 rep-resentations at word and character-level.
For the ex-periments with words, we took the top 2,500 mostcommon words used across the training documentsand obtained LOWBOW representations.
We usedthis setting in agreement with previous work onAA (Houvardas and Stamatatos, 2006).
For ourcharacter n-gram experiments, we obtained LOW-BOW representations for character 3-grams (onlyn-grams of size n = 3 were used) consideringthe 2, 500 most common n-grams.
Again, this set-ting was adopted in agreement with previous workon AA with character n-grams (Houvardas andStamatatos, 2006; Plakias and Stamatatos, 2008b;Plakias and Stamatatos, 2008a; Luyckx and Daele-mans, 2010).
All our experiments use the SVM im-plementation provided by Canu et al (2005).5.1 Experimental settingsIn order to compare our methods to related workswe adopted the following experimental setting.
Weperform experiments using all of the training doc-uments per author, that is, a balanced corpus (wecall this setting BC).
Next we evaluate the perfor-mance of classifiers over reduced training sets.
Wetried balanced reduced data sets with: 1, 3, 5 and10 documents per author (we call this configura-tion RBC).
Also, we experimented with reduced-imbalanced data sets using the same imbalance ratesreported in (Plakias and Stamatatos, 2008b; Plakiasand Stamatatos, 2008a): we tried settings 2 ?
10,5?
10, and 10?
20, where, for example, setting 2-10 means that we use at least 2 and at most 10 doc-uments per author (we call this setting IRBC).
BCsetting represents the AA problem under ideal con-ditions, whereas settings RBC and IRBC aim at em-ulating a more realistic scenario, where limited sam-ple documents are available and the whole data set ishighly imbalanced (Plakias and Stamatatos, 2008b).3We used LOWBOW code of G. Lebanon and Y. Mao avail-able from http://www.cc.gatech.edu/?ymao8/lowbow.htm5.2 Experimental results in balanced dataWe first compare the performance of the LOWBOWhistogram representation to that of the traditionalBOW representation.
Table 2 shows the accuracy(i.e., percentage of documents in the test set thatwere associated to its correct author) for the BOWand LOWBOW histogram representations when us-ing words and character n-grams information.
ForLOWBOW histograms, we report results with threedifferent configurations for ?.
As in (Lebanon et al,2007), we consider uniformly distributed locationsand we varied the number of locations that were in-cluded in each setting.
We denote with k the numberof local histograms.
In preliminary experiments wetried several other values for k, although we foundthat representative results can be obtained with thevalues we considered here.Method Parameters Words CharactersBOW - 78.2% 75.0%LOWBOW k = 2;?
= 0.2 75.8% 72.0%LOWBOW k = 5;?
= 0.2 77.4% 75.2%LOWBOW k = 20;?
= 0.2 77.4% 75.0%Table 2: Authorship attribution accuracy for the BOWrepresentation and LOWBOW histograms.
Column 2shows the parameters we used for the LOWBOW his-tograms; columns 3 and 4 show results using words andcharacter n-grams, respectively.From Table 2 we can see that the BOW repre-sentation is very effective, outperforming most ofthe LOWBOW histogram configurations.
Despite asmall difference in performance, BOW is advanta-geous over LOWBOW histograms because it is sim-pler to compute and it does not rely on parameterselection.
Recall that the LOWBOW histogram rep-resentations are obtained by the combination of sev-eral local histograms calculated at different locationsof the document, hence, it seems that the raw sum oflocal histograms results in a loss of useful informa-tion for representing documents.
The worse perfor-mance was obtained when k = 2 local histogramsare considered (see row 3 in Table 2).
This re-sult is somewhat expected since the larger the num-ber of local histograms, the more LOWBOW his-tograms approach the BOW formulation (Lebanonet al, 2007).We now describe the AA performance obtainedwhen using the BOLH formulation; these results293are shown in Table 3.
Most of the results fromthis table are superior to those reported in Table 2,showing that bags of local histograms are a betterway to exploit the LOWBOW framework for AA.As expected, different kernels yield different results.However, the diffusion kernel outperformed most ofthe results obtained with other kernels; confirmingthe results obtained by other researchers (Lebanonet al, 2007; Lafferty and Lebanon, 2005).Kernel Euc.
Diffusion EMD ?2WordsSetting-1 78.6% 81.0% 75.0% 75.4%Setting-2 77.6% 82.0% 76.8% 77.2%Setting-3 79.2% 80.8% 77.0% 79.0%CharactersSetting-1 83.4% 82.8% 84.4% 83.8%Setting-2 83.4% 84.2% 82.2% 84.6%Setting-3 83.6% 86.4% 81.0% 85.2%Table 3: Authorship attribution accuracy when using bagsof local histograms and different kernels for word-basedand character-based representations.
The BC data set isused.
Settings 1, 2 and 3 correspond to k = 2, 5 and 20,respectively.On average, the worse kernel was that based onthe earth mover?s distance (EMD), suggesting thatthe comparison of local histograms at different loca-tions is not a fruitful approach (recall that this is theonly kernel that compares local histograms at differ-ent locations).
This result evidences that authors usesimilar word/character distributions at similar loca-tions when writing different documents.The best performance across settings and kernelswas obtained with the diffusion kernel (in bold, col-umn 3, row 9) (86.4%); that result is 8% higherthan that obtained with the BOW representation and9% better than the best configuration of LOWBOWhistograms, see Table 2.
Furthermore, that resultis more than 5% higher than the best reported re-sult in related work (80.8% as reported in (Plakiasand Stamatatos, 2008b)).
Therefore, the consid-ered local histogram representations over charactern-grams have proved to be very effective for AA.One should note that, in general, better per-formance was obtained when using character-levelrather than word-level information.
This confirmsthe results already reported by other researchersthat have used character-level and word-level infor-mation for AA (Houvardas and Stamatatos, 2006;Plakias and Stamatatos, 2008b; Plakias and Sta-matatos, 2008a; Peng et al, 2003).
We believe thiscan be attributed to the fact that character n-gramsprovide a representation for the document at a finergranularity, which can be better exploited with localhistogram representations.
Note that by considering3-grams, words of length up to three are incorpo-rated, and usually these words are function words(e.g., the, it, as, etc.
), which are known to be in-dicative of writing style.
Also, n-gram informationis more dense in documents than word-level infor-mation.
Hence, the local histograms are less sparsewhen using character-level information, which re-sults in better AA performance.True authorAC AS BL DL JM JG MM MD RS TN88 2 0 0 0 0 0 0 0 010 98 0 0 0 0 0 0 0 00 0 68 0 40 0 0 0 0 00 0 0 80 0 0 0 0 0 40 0 12 2 42 0 0 2 0 00 0 0 0 0 100 0 0 0 22 0 2 0 0 0 100 0 0 00 0 18 0 18 0 0 98 0 00 0 0 2 0 0 0 0 100 40 0 0 16 0 0 0 0 0 90Table 4: Confusion matrix (in terms of percentages) forthe best result in the BC corpus (i.e., last row, column 3in Table 3).
Columns show the true author for test docu-ments and rows show the authors predicted by the SVM.Table 4 shows the confusion matrix for the settingthat reached the best results (i.e., column 3, last rowin Table 3).
From this table we can see that 8 outof the 10 authors were recognized with an accuracyhigher or equal to 80%.
For these authors sequentialinformation seems to be particularly helpful.
How-ever, low recognition performance was obtained forauthors BL (B. K. Lim) and JM (J. MacArtney).The SVM with BOW representation of character n-grams achieved recognition rates of 40% and 50%for BL and JM respectively.
Thus, we can state thatsequential information was indeed helpful for mod-eling BL writing style (improvement of 28%), al-though it is an author that resulted very difficult tomodel.
On the other hand, local histograms were notvery useful for identifying documents written by JM(made it worse by ?8%).
The largest improvement(38%) of local histograms over the BOW formula-tion was obtained for author TN (T. Nissen).
This294result gives evidence that TN uses a similar distri-bution of words in similar locations across the doc-uments he writes.
These results are interesting, al-though we would like to perform a careful analysisof results in order to determine for what type of au-thors it would be beneficial to use local histograms,and what type of authors are better modeled with astandard BOW approach.5.3 Experimental results in imbalanced dataIn this section we report results with RBC andIRBC data sets, which aim to evaluate the perfor-mance of our methods in a realistic setting.
Forthese experiments we compare the performance ofthe BOW, LOWBOW histogram and BOLH repre-sentations; for the latter, we considered the best set-ting as reported in Table 3 (i.e., an SVM with dif-fusion kernel and k = 20).
Tables 5 and 6 showthe AA performances when using word and charac-ter information, respectively.We first analyze the results in the RBC data set(recall that for this data set we consider 1, 3, 5, 10,and 50, randomly selected documents per author).From Tables 5 and 6 we can see that BOW andLOWBOW histogram representations obtained sim-ilar performance to each other across the differenttraining set sizes, which agree with results in Table 2for the BC data sets.
The best performance acrossthe different configurations of the RBC data set wasobtained with the BOLH formulation (row 6 in Ta-bles 5 and 6).
The improvements of local histogramsover the BOW formulation vary across different set-tings and when using information at word-level andcharacter-level.
When using words (columns 2-6in Table 5) the differences in performance are of15.6%, 6.2%, 6.8%, 2.9%, 3.8% when using 1, 3, 5,10 and 50 documents per author, respectively.
Thus,it is evident that local histograms are more beneficialwhen less documents are considered.
Here, the lackof information is compensated by the availability ofseveral histograms per author.When using character n-grams (columns 2-6 inTable 6) the corresponding differences in perfor-mance are of 5.4%, 6.4%, 6.4%, 6% and 11.4%,when using 1, 3, 5, 10, and 50 documents per au-thor, respectively.
In this case, the larger improve-ment was obtained when 50 documents per authorare available; nevertheless, one should note that re-sults using character-level information are, in gen-eral, significantly better than those obtained withword-level information; hence, improvements areexpected to be smaller.When we compare the results of the BOLH for-mulation with the best reported results elsewhere(c.f.
last row 6 in Tables 5 and 6) (Plakias and Sta-matatos, 2008b), we found that the improvementsrange from 14% to 30.2% when using character n-grams and from 1.2% to 26% when using words.The differences in performance are larger when lessinformation is used (e.g., when 5 documents areused for training) and we believe the differenceswould be even larger if results for 1 and 3 documentswere available.
These are very positive results; forexample, we can obtain almost 71% of accuracy, us-ing local histograms of character n-grams when asingle document is available per author (recall thatwe have used all of the test samples for evaluatingthe performance of our methods).We now analyze the performance of the differentmethods when using the IRBC data set (columns 7-9 in Tables 5 and 6).
The same pattern as before canbe observed in experimental results for these datasets as well: BOW and LOWBOW histograms ob-tained comparable performance to each other andthe BOLH formulation performed the best.
TheBOLH formulation outperforms state of the art ap-proaches by a considerable margin that ranges from10% to 27%.
Again, better results were obtainedwhen using character n-grams for the local his-tograms.
With respect to RBC data sets, the BOLHat the character-level resulted very robust to the re-duction of training set size and the highly imbal-anced data.Summarizing, the results obtained in RBC andIRBC data sets show that the use of local histogramsis advantageous under challenging conditions.
AnSVM under the BOLH representation is less sen-sitive to the number of training examples availableand to the imbalance of data than an SVM usingthe BOW representation.
Our hypothesis for thisbehavior is that local histograms can be thought ofas expanding training instances, because for eachtraining instance in the BOW formulation we havek?training instances under BOLH.
The benefits ofsuch expansion become more notorious as the num-ber of available documents per author decreases.295WORDSData set Balanced ImbalancedSetting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20BOW 36.8% 57.1% 62.4% 69.9% 78.2% 62.3% 67.2% 71.2%LOWBOW 37.9% 55.6% 60.5% 69.3% 77.4% 61.1% 67.4% 71.5%Diffusion kernel 52.4% 63.3% 69.2% 72.8% 82.0% 66.6% 70.7% 74.1%Reference - - 53.4% 67.8% 80.8% 49.2% 59.8% 63.0%Table 5: AA accuracy in RBC (columns 2-6) and IRBC (columns 7-9) data sets when using words as terms.
We reportresults for the BOW, LOWBOW histogram and BOLH representations.
For reference (last row), we also include thebest result reported in (Plakias and Stamatatos, 2008b), when available, for each configuration.CHARACTER N-GRAMSData set Balanced ImbalancedSetting 1-doc 3-docs 5-docs 10-docs 50-docs 2-10 5-10 10-20BOW 65.3% 71.9% 74.2% 76.2% 75.0% 70.1% 73.4% 73.1%LOWBOW 61.9% 71.6% 74.5% 73.8% 75.0% 70.8% 72.8% 72.1%Diffusion kernel 70.7% 78.3% 80.6% 82.2% 86.4% 77.8% 80.5% 82.2%Reference - - 50.4% 67.8% 76.6% 49.2% 59.8% 63.0%Table 6: AA accuracy in the RBC and IRBC data sets when using character n-grams as terms.6 ConclusionsWe have described the use of local histograms (LH)over character n-grams for AA.
LHs are enrichedhistogram representations that preserve sequentialinformation in documents (in terms of the positionsof terms in documents); we explored the suitabil-ity of LHs over n-grams at the character-level forAA.
We showed evidence supporting our hypothe-sis that LHs are very helpful for AA; we believe thatthis is due to the fact that LOWBOW representationscan uncover, to some extent, the writing preferencesof authors.
Our experimental results showed thatLHs outperform traditional bag-of-words formula-tions and state of the art techniques in balanced,imbalanced, and reduced data sets.
The improve-ments were larger in reduced and imbalanced datasets, which is a very positive result as in real AAapplications one often faces highly imbalanced andsmall sample issues.
Our results are promising andmotivate further research on the use and extensionof the LOWBOW framework for related tasks (e.g.authorship verification and plagiarism detection).As future work we would like to explore the useof LOWBOW representations for profile-based AAand related tasks.
Also, we would like to developmodel selection strategies for learning what combi-nation of hyperparameters works better for modelingeach author.AcknowledgmentsWe thank E. Stamatatos for making his data setavailable.
Also, we are grateful for the thought-ful comments of L. A. Barro?n and those of theanonymous reviewers.
This work was partially sup-ported by CONACYT under project grants 61335,and CB-2009-134186, and by UAB faculty develop-ment grant 3110841.ReferencesAMIDA.
2007.
Augmented multi-party interactionwith distance access.
Available from http://www.amidaproject.org/, AMIDA Report.S.
Argamon and S. Levitan.
2005.
Measuring the useful-ness of function words for authorship attribution.
InProceedings of the Joint Conference of the Associationfor Computers and the Humanities and the Associationfor Literary and Linguistic Computing, Victoria, BC,Canada.S.
Canu, Y. Grandvalet, V. Guigue, and A. Rakotoma-monjy.
2005.
SVM and kernel methods Matlab tool-box.
Perception Systmes et Information, INSA deRouen, Rouen, France.V.
Chasanis, A. Kalogeratos, and A. Likas.
2009.
Moviesegmentation into scenes and chapters using locallyweighted bag of visual words.
In Proceedings of theACM International Conference on Image and VideoRetrieval, pages 35:1?35:7, Santorini, Fira, Greece.ACM Press.R.
M. Coyotl-Morales, L. Villasen?or-Pineda, M. Montes-y-Go?mez, and P. Rosso.
2006.
Authorship attribu-tion using word sequences.
In Proceedings of 11th296Iberoamerican Congress on Pattern Recognition, vol-ume 4225 of LNCS, pages 844?852, Cancun, Mexico.Springer.D.
Das and A. Martins.
2007.
A survey on au-tomatic text summarization.
Available from:http://www.cs.cmu.edu/?nasmith/LS2/das-martins.07.pdf, Literature Survey for theLanguage and Statistics II course at Carnegie MellonUniversity.O.
de Vel, A. Anderson, M. Corney, and G. Mohay.
2001.Multitopic email authorship attribution forensics.
InProceedings of the ACM Conference on Computer Se-curity - Workshop on Data Mining for Security Appli-cations, Philadelphia, PA, USA.K.
Grauman.
2006.
Matching Sets of Features for Ef-ficient Retrieval and Recognition.
Ph.D. thesis, Mas-sachusetts Institute of Technology.J.
Houvardas and E. Stamatatos.
2006.
N-gram fea-ture selection for author identification.
In Proceedingsof the 12th International Conference on Artificial In-telligence: Methodology, Systems, and Applications,volume 4183 of LNCS, pages 77?86, Varna, Bulgaria.Springer.V.
Keselj, F. Peng, N. Cercone, and C. Thomas.
2003.
N-gram-based author profiles for authorship attribution.In Proceedings of the Pacific Association for Compu-tational Linguistics, pages 255?264, Halifax, Canada.M.
Koppel, J. Schler, and S. Argamon.
2009.
Computa-tional methods in authorship attribution.
Journal of theAmerican Society for Information Science and Tech-nology, 60:9?26.J.
Lafferty and G. Lebanon.
2005.
Diffusion kernelson statistical manifolds.
Journal of Machine LearningResearch, 6:129?163.M.
Lambers and C. J. Veenman.
2009.
Forensic author-ship attribution using compression distances to pro-totypes.
In Computational Forensics, Lecture Notesin Computer Science, Volume 5718.
ISBN 978-3-642-03520-3.
Springer Berlin Heidelberg, 2009, p. 13, vol-ume 5718 of LNCS, pages 13?24.
Springer.G.
Lebanon, Y. Mao, and J. Dillon.
2007.
The locallyweighted bag of words framework for document rep-resentation.
Journal of Machine Learning Research,8:2405?2441.D.
Lewis, T. Yang, and F. Rose.
2004.
RCV1: A newbenchmark collection for text categorization research.Journal of Machine Learning Research, 5:361?397.K.
Luyckx and W. Daelemans.
2010.
The effect of au-thor set size and data size in authorship attribution.Literary and Linguistic Computing, pages 1?21, Au-gust.Y.
Mao, J. Dillon, and G. Lebanon.
2007.
Sequentialdocument visualization.
IEEE Transactions on Visu-alization and Computer Graphics, 13(6):1208?1215.F.
Peng, D. Shuurmans, V. Keselj, and S. Wang.
2003.Language independent authorship attribution usingcharacter level language models.
In Proceedings of the10th conference of the European chapter of the Associ-ation for Computational Linguistics, volume 1, pages267?274, Budapest, Hungary.F.
Peng, D. Shuurmans, and S. Wang.
2004.
Augmentingnaive Bayes classifiers with statistical language mod-els.
Information Retrieval Journal, 7(1):317?345.S.
R. Pillay and T. Solorio.
2010.
Authorship attributionof web forum posts.
In Proceedings of the eCrime Re-searchers Summit (eCrime), 2010, pages 1?7, Dallas,TX, USA.
IEEE.S.
Plakias and E. Stamatatos.
2008a.
Author identifi-cation using a tensor space representation.
In Pro-ceedings of the 18th European Conference on Artifi-cial Intelligence, volume 178, pages 833?834, Patras,Greece.
IOS Press.S.
Plakias and E. Stamatatos.
2008b.
Tensor space mod-els for authorship attribution.
In Proceedings of the 5thHellenic Conference on Artificial Intelligence: Theo-ries, Models and Applications, volume 5138 of LNCS,pages 239?249, Syros, Greece.
Springer.R.
Rifkin and A. Klautau.
2004.
In defense of one-vs-allclassification.
Journal of Machine Learning Research,5:101?141.Y.
Rubner, C. Tomasi, J. Leonidas, and J. Guibas.
2001.The earth mover?s distance as a metric for image re-trieval.
International Journal of Computer Vision,40(2):99?121.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Computing Surveys, 34(1):1?47.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.E.
Stamatatos.
2006a.
Authorship attribution based onfeature set subspacing ensembles.
International Jour-nal on Artificial Intelligence Tools, 15(5):823?838.E.
Stamatatos.
2006b.
Ensemble-based author identifi-cation using character n-grams.
In Proceedings of the3rd International Workshop on Text-based InformationRetrieval, pages 41?46, Riva del Garda, Italy.E.
Stamatatos.
2009a.
Intrinsic plagiarism detec-tion using character n-gram profiles.
In Proceed-ings of the 3rd International Workshop on UncoveringPlagiarism, Authorship, and Social Software Misuse,PAN?09, pages 38?46, Donostia-San Sebastian, Spain.E.
Stamatatos.
2009b.
A survey of modern authorshipattribution methods.
Journal of the American Societyfor Information Science and Technology, 60(3):538?556.M.
Tearle, K. Taylor, and H. Demuth.
2008.
Analgorithm for automated authorship attribution usingneural networks.
Literary and Linguist Computing,23(4):425?442.297Y.
Zhao and J. Zobel.
2005.
Effective and scalable au-thorship attribution using function words.
In Proceed-ings of 2nd Asian Information Retrieval Symposium,volume 3689 of LNCS, pages 174?189, Jeju Island,Korea.
Springer.298
