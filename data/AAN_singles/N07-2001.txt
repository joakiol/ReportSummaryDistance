Proceedings of NAACL HLT 2007, Companion Volume, pages 1?4,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsComparing User Simulation Models For Dialog Strategy LearningHua AiUniversity of PittsburghIntelligent Systems ProgramPittsburgh PA, 15260, USAhua@cs.pitt.eduJoel R. TetreaultUniversity of PittsburghLRDCPittsburgh PA, 15260, USAtetreaul@pitt.eduDiane J. LitmanUniversity of PittsburghDept.
of Computer ScienceLRDCPittsburgh PA, 15260, USAlitman@cs.pitt.eduAbstractThis paper explores what kind of user sim-ulation model is suitable for developinga training corpus for using Markov Deci-sion Processes (MDPs) to automaticallylearn dialog strategies.
Our results sug-gest that with sparse training data, a modelthat aims to randomly explore more dialogstate spaces with certain constraints actu-ally performs at the same or better than amore complex model that simulates real-istic user behaviors in a statistical way.1 IntroductionRecently, user simulation has been used in the de-velopment of spoken dialog systems.
In contrast toexperiments with human subjects, which are usuallyexpensive and time consuming, user simulation gen-erates a large corpus of user behaviors in a low-costand time-efficient manner.
For example, user sim-ulation has been used in evaluation of spoken dia-log systems (Lo?pez-Co?zar et al, 2003) and to learndialog strategies (Scheffler, 2002).
However, thesestudies do not systematically evaluate how helpful auser simulation is.
(Schatzmann et al, 2005) pro-pose a set of evaluation measures to assess the re-alness of the simulated corpora (i.e.
how similarare the simulated behaviors and human behaviors).Nevertheless, how realistic a simulated corpus needsto be for different tasks is still an open question.We hypothesize that for tasks like system eval-uation, a more realistic simulated corpus is prefer-able.
Since the system strategies are evaluated andadapted based on the analysis of these simulated dia-log behaviors, we would expect that these behaviorsare what we are going to see in the test phase whenthe systems interact with human users.
However,for automatically learning dialog strategies, it is notclear how realistic versus how exploratory (Singh etal., 2002) the training corpus should be.
A train-ing corpus needs to be exploratory with respect tothe chosen dialog system actions because if a cer-tain action is never tried at certain states, we willnot know the value of taking that action in that state.In (Singh et al, 2002), their system is designed torandomly choose one from the allowed actions withuniform probability in the training phase in order toexplore possible dialog state spaces.
In contrast,weuse user simulation to generate exploratory trainingdata because in the tutoring system we work with,reasonable tutor actions are largely restricted by stu-dent performance.
If certain student actions do notappear, this system would not be able to explore astate space randomly .This paper investigates what kind of user simula-tion is good for using Markov Decision Processes(MDPs) to learn dialog strategies.
In this study,we compare three simulation models which differ intheir efforts on modeling the dialog behaviors in atraining corpus versus exploring a potentially largerdialog space.
In addition, we look into the impact ofdifferent state space representations and different re-ward functions on the choice of simulation models.2 System and CorpusOur system is a speech-enabled Intelligent Tutor-ing System that helps students understand qualita-1tive physics questions.
The dialog policy was deter-ministic and hand-crafted in a finite state paradigm(Ai et al, 2006).
We collected 130 dialogs (1019student utterances) with 26 human subjects.
Cor-rectness (correct(c), incorrect(ic)) is automaticallyjudged by the system1 and kept in the system?s logs.Percent incorrectness (ic%) is also automaticallycalculated and logged.
Each student utterance wasmanually annotated for certainty (certain, uncer-tain, neutral, mixed) in a previous study2 based onboth lexical and prosodic information.
In this study,we use a two-way classification (certain(cert), not-certain(ncert)), where we collapse uncertain, neu-tral, and mixed to be ncert to balance our data.
Anexample of coded dialog between the tutor (T) and astudent (S) is given in Table 1.3 Experimental Setup3.1 Learning TaskOur current system can only respond to the cor-rectness of a student?s utterances; the system thusignores other underlying information, for exam-ple, certainty which is believed to provide use-ful information for the tutor.
In our corpus, thestrength of the tutor?s minimal feedback (defined be-low) is in fact strongly correlated with the percent-age of student certainty (chi-square test, p<0.01).Strong Feedback (SF) is when the tutor clearly stateswhether the student?s answer is correct or incor-rect (i.e., ?This is great!?
); Weak Feedback (WF)is when the tutor does not comment on the correct-ness of a student?s answer or gives slightly negativefeedback such as ?well?.
Our goal is to learn howto manipulate the strength of the tutor minimal feed-back in order to maximize student?s overall certaintyin the entire dialog.
We keep the other parts of thetutor feedback (e.g.
explanations, questions) so thesystem?s original design of maximizing the percent-age of student correct answers is utilized.3.2 Simulation ModelsAll three models we describe below are trained fromthe real corpus we collected.
We simulate on theword level because generating student?s dialog actsalone does not provide sufficient information for1Kappa of 0.79 is gained comparing to human judgements.2Kappa of 0.68 is gained in a preliminary agreement study.T1: Which law of motion would you use?S1: Newton?s second law?
[ic, ic%=1, ncert]T2: Well...
The best law to use is Newton?sthird law.
Do you recall what it says?S2: For every action there is an equal andopposite reaction?
[c, ic%=50%, ncert]Table 1: Sample coded dialog excerpt.our tutoring system to decide the next system?s ac-tion.
Thus, the output of the three models is a stu-dent utterance along with the student certainty (cert,ncert).
Since it is hard to generate a natural lan-guage utterance for each tutor?s question, we use thestudent answers in the real corpus as the candidateanswers for the simulated students (Ai et al, 2006).In addition, we simulate student certainty in a verysimple way: the simulation models output the cer-tainty originally associated with that utterance.Probabilistic Model (PM) is meant to capture re-alistic student behavior in a probabilistic way.
Givena certain tutor question along with a tutor feedback,it will first compute the probabilities of the fourtypes of student answers from the training corpus: cand cert, c and ncert, ic and cert, and ic and ncert.Then, following this distribution, the model selectsthe type of student answers to output, and then itpicks an utterance that satisfies the correctness andcertainty constraints of the chosen answer type fromthe candidate answer set and outputs that utterance.We implement a back-off mechanism to count pos-sible answers that do not appear in the real corpus.Total Random Model (TRM) ignores what thecurrent question is or what feedback is given.
It ran-domly picks one utterance from all the utterances inthe entire candidate answer set.
This model tries toexplore all the possible dialog states.Restricted Random Model (RRM) differs fromthe PM in that given a certain tutor question and atutor feedback, it chooses to give a c and cert, c andncert, ic and cert, or ic and ncert answer with equalprobability.
This model is a compromise betweenthe exploration of the dialog state space and the re-alness of generated user behaviors.3.3 MDP ConfigurationA MDP has four main components: states, actions,a policy, and a reward function.
In this study, the ac-tions allowed in each dialog state are SF and WF;2the policy we are trying to learn is in every statewhether the tutor should give SF and WF in orderto maximize the percent certainty in the dialog.Since different state space representations and re-ward functions have a strong impact on the MDPpolicy learning, we investigate different configura-tions to avoid possible bias introduced by certainconfigurations.
We use two state space representa-tions: SSR1 uses the correctness of current studentturn and percent incorrectness so far; and SSR2 addsin the certainty of the current student turn on top ofSSR1.
Two reward functions are investigated: inRF1, we assign +100 to every dialog that has a per-cent certainty higher than the median from the train-ing corpus, and -100 to every dialog that has a per-cent certainty below the median; in RF2, we assigndifferent rewards to every different dialog by multi-plying the percent certainty in that dialog with 100.Other MDP parameter settings are the same as de-scribed in (Tetreault et al, 2006).3.4 MethodologyWe first let the three simulation models interact withthe original system to generate different training cor-pora.
Then, we learn three MDP policies in a fixedconfiguration from the three training corpora sep-arately.
For each configuration, we run the sim-ulation models until we get enough training datasuch that the learned policies on that corpus do notchange anymore (40,000 dialogs are generated byeach model).
After that, the learned new policies areimplemented into the original system respectively 3.Finally, we use our most realistic model, the PM,to interact with each new system 500 times to eval-uate the new systems?
performances.
We use twoevaluation measures.
EM1 is the number of dialogsthat would be assigned +100 using the old mediansplit.
EM2 is the average of percent certainty in ev-ery single dialog from the newly generated corpus.A policy is considered better if it can improve thepercentage of certainty more than other policies, orhas more dialogs that will be assigned +100.
Thebaseline for EM1 is 250, since half of the 500 di-alogs would be assigned +100 using the old median3For example, the policy learned from the training corpusgenerated by the RRM with SSR1 and RF1 is: give SF whenthe current student answer is ic and ic%>50%, otherwise giveWF.split.
The baseline for EM2 is 35.21%, which isobtained by calculating the percent certainty in thecorpus generated by the 40,000 interactions betweenthe PM and the original system.4 Results and DiscussionTable 2 summarizes our results.
There are twocolumns under each ?state representation+rewardfunction?
configuration, presenting the results usingthe two evaluation approaches.
EM1 measures ex-actly what RF1 tries to optimize; while EM2 mea-sures exactly what RF2 tries to optimize.
However,we show the results evaluated by both EM1 andEM2 for all configurations since the two evaluationmeasures have their own practical values and canbe deployed under different design requirements.All results that significantly4 outperform the corre-sponding baselines are marked with ?.When evaluating using EM1, the RRM signifi-cantly4 outperforms the other two models in all con-figurations (in bold in Table 2).
Also, the PM per-forms better (but not statistically significantly) thanthe TRM.
When evaluating on EM2, the RRM sig-nificantly4 outperforms the other two when usingSSR1 and RF1 (in bold in Table 2).
In all otherconfigurations, the three models do not differ signif-icantly.
It is not surprising that the RRM outper-forms the PM in most of the cases even when wetest on the PM.
(Schatzmann et al, 2005) also ob-serve that a good model can still perform well whentested on a poor model.We suspect that the performance of the PM isharmed by the data sparsity issue in the real cor-pus that we trained the model on.
Consider the caseof SSR1: 25.8% of the potentially possible dialogstates do not exist in the real corpus.
Although weimplement a back-off mechanism, the PM will stillhave much less chance to transition to the states thatare not observed in the real corpus.
Thus, when welearn the MDP policy from the corpus generated bythis model, the actions to take in these less-likelystates are not fully learned.
In contrast, the RRMtransitions from one state to each of the next possiblestates with equal probability, which compensates forthe data sparsity problem.
We further examine theresults obtained using SSR1 and RF1 and evaluated4Using 2-sided t-test with Bonferroni correction, p<0.05.3Model Name SSR1+RF1 SSR2+RF1 SSR1+RF2 SSR2+RF2EM1 EM2 EM1 EM2 EM1 EM2 EM1 EM2Probabilistic Model 222 36.30% 217 37.63% 197 40.78%?
197 40.01%?Total Random Model 192 36.30% 211 38.57% 188 40.21%?
179 40.21%?Restricted Random Model 390?
46.11%?
368?
37.27% 309 40.21%?
301 40.21%?Table 2: Evaluation of the new policies trained with the three simulation modelsby EM1 to confirm our hypothesis.
When lookinginto the frequent states5, 70.1% of them are seen fre-quently in the training corpus generated by the PM,while 76.3% are seen frequently in the training cor-pus generated by the RRM.
A higher percentage in-dicates the policy might be better trained with moretraining instances.
This explains why the RRM out-performs the PM in this case.While the TRM also tries to explore dialog statespace, only 65.2% of the frequent states in testingphase are observed frequently in the training phase.This is because the Total Random Model answers90% of the questions incorrectly and often goesdeeply down the error-correction paths.
It does ex-plore some states that are at the end of the paths,but since these are the infrequent states in the testphase, exploring these states does not actually im-prove the model?s performance much.
On the otherhand, while the student correctness rate in the realcorpus is 60%, the RRM prevents itself from beingtrapped in the less-likely states on incorrect answerpaths by keeping its correctness rate to be 50%.Our results are preliminary but suggest interest-ing points in building simulation models: 1.
Whentrained from a sparse data set, it may be better touse a RRM than a more realistic PM or a more ex-ploratory TRM; 2.
State space representation maynot impact evaluation results as much as rewardfunctions and evaluation measures, since when us-ing RF2 and evaluating with EM2, the differenceswe see using RF1 or EM1 become less significant.In our future work, we are going to further investi-gate whether the trends shown in this paper general-ize to on-line MDP policy learning.
We also want toexplore other user simulations that are designed forsparse training data (Henderson et al, 2005).
More5We define frequent states to be those that comprise at least1% of the entire corpus.
These frequent states add up to morethan 80% of the training/testing corpus.
However, deciding thethreshold of the frequent states in training/testing is an openquestion.importantly, we are going to test the new policieswith the other simulations and human subjects tovalidate the learning process.AcknowledgementsNSF (0325054, 0328431) supports this research.The authors wish to thank Tomas Singliar for hisvaluable suggestions, Scott Silliman for his supporton building the simulation system, and the anony-mous reviewers for their insightful comments.ReferencesH.
Ai and D. Litman.
2006.
Comparing Real-Real,Simulated-Simulated, and Simulated-Real Spoken Di-alogue Corpora.
In Proc.
AAAI Workshop on Statis-tical and Empirical Approaches for SDS.J.
Henderson, O.Lemon, and K.Georgila.
2005.
Hybridreinforcement/supervised learning for dialogue poli-cies from COMMUNICATOR data.
In Proc.
IJCAIworkshop on Knowledge and Reasoning in PracticalDialogue Systems.R.
Lo?pez-Co?zar, A.
De la Torre, J. Segura, and A. Ru-bio.
2003.
Assessment of dialog systems by means ofa new simulation technique.
Speech Communication(40): 387-407.K.
Scheffler.
2002.
Automatic Design of Spoken DialogSystems.
Ph.D.
diss., Cambridge University.J.
Schatzmann, K. Georgila, and S. Young.
2005.
Quan-titative Evaluation of User Simulation Techniques forSpoken Dialog Systems.
In Proc.
of 6th SIGdial.J.
Schatzmann, M. N. Stuttle, K. Weilhammer andS.
Young.
2005.
Effects of the User Model onSimulation-based Learning of Dialogue Strategies.
InProc.
of ASRU05.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing Dialog Managment with ReinforcementLearning: Experiments with the NJFun System.
Jour-nal of Artificial Intelligence Research, (16):105-133.J.
Tetreault and D. Litman.
2006.
Comparing the Utilityof State Features in Spoken Dialogue Using Reinforce-ment Learning..
In Proc.
NAACL06.4
