On Statistical Parameter SettingDamir ?AVAR, Joshua HERRING,Toshikazu IKUTA, Paul RODRIGUESLinguistics Dept., Indiana UniversityBloomington, IN, 46405dcavar@indiana.eduGiancarlo SCHREMENTIComputer Science, Indiana UniversityBloomington, IN, 47405gischrem@indiana.eduAbstractWe present a model and an experimentalplatform of a bootstrapping approach tostatistical induction of natural languageproperties that is constraint based with votingcomponents.
The system is incremental andunsupervised.
In the following discussion wefocus on the components for morphologicalinduction.
We show that the much harderproblem of incremental unsupervisedmorphological induction can outperformcomparable all-at-once algorithms withrespect to precision.
We discuss how we usesuch systems to identify cues for induction ina cross-level architecture.1 IntroductionIn recent years there has been a growing amountof work focusing on the computational modelingof language processing and acquisition, implying acognitive and theoretical relevance both of themodels as such, as well as of the languageproperties extracted from raw linguistic data.1 Inthe computational linguistic literature severalattempts to induce grammar or linguisticknowledge from such data have shown that atdifferent levels a high amount of information canbe extracted, even with no or minimal supervision.Different approaches tried to show how variouspuzzles of language induction could be solved.From this perspective, language acquisition is theprocess of segmentation of non-discrete acousticinput, mapping of segments to symbolicrepresentations, mapping representations onhigher-level representations such as phonology,morphology and syntax, and even induction ofsemantic properties.
Due to space restrictions, wecannot discuss all these approaches in detail.
Wewill focus on the close domain of morphology.Approaches to the induction of morphology aspresented in e.g.
Schone and Jurafsky (2001) orGoldsmith (2001) show that the morphological1 See Batchelder (1998) for a discussion of theseaspects.properties of a small subset of languages can beinduced with high accuracy, most of the existingapproaches are motivated by applied orengineering concerns, and thus make assumptionsthat are less cognitively plausible: a.
Large corporaare processed all at once, though unsupervisedincremental induction of grammars is rather theapproach that would be relevant from apsycholinguistic perspective; b.
Arbitrary decisionsabout selections of sets of elements are made,based on frequency or frequency profile rank,2though such decisions should rather be derived oravoided in general.However, the most important aspects missing inthese approaches, however, are the link to differentlinguistic levels and the support of a generallearning model that makes predictions about howknowledge is induced on different linguistic levelsand what the dependencies between information atthese levels are.
Further, there is no study focusingon the type of supervision that might be necessaryfor the guidance of different algorithm typestowards grammars that resemble theoretical andempirical facts about language acquisition, andprocessing and the final knowledge of language.While many theoretical models of languageacquisition use innateness as a crutch to avoidoutstanding difficulties, both on the general andabstract level of I-language as well as the moredetailed level of E-language, (see, among others,Lightfoot (1999) and Fodor and Teller (2000),there is also significant research being done whichshows that children take advantage of statisticalregularities in the input for use in the language-learning task (see Batchelder (1997) and relatedreferences within).In language acquisition theories the dominantview is that knowledge of one linguistic level isbootstrapped from knowledge of one, or evenseveral different levels.
Just to mention suchapproaches: Grimshaw (1981), and Pinker (1984)2 Just to mention some of the arbitrary decisionsmade in various approaches, e.g.
Mintz (1996) selects asmall set of all words, the most frequent words, toinduce word types via clustering ; Schone and Jurafsky(2001) select words with frequency higher than 5 toinduce morphological segmentation.9assume that semantic properties are used tobootstrap syntactic knowledge, and Mazuka (1998)suggested that prosodic properties of languageestablish a bias for specific syntactic properties,e.g.
headedness or branching direction ofconstituents.
However, these approaches are basedon conceptual considerations and psycholinguistcempirical grounds, the formal models andcomputational experiments are missing.
It isunclear how the induction processes acrosslinguistic domains might work algorithmically, andthe quantitative experiments on large scale data aremissing.As for algorithmic approaches to cross-levelinduction, the best example of an initial attempt toexploit cues from one level to induce properties ofanother is presented in D?jean (1998), wheremorphological cues are identified for induction ofsyntactic structure.
Along these lines, we willargue for a model of statistical cue-based learning,introducing a view on bootstrapping as proposed inElghamry (2004), and Elghamry and ?avar (2004),that relies on identification of elementary cues inthe language input and incremental induction andfurther cue identification across all linguisticlevels.1.1 Cue-based learningPresupposing input driven learning, it has beenshown in the literature that initial segmenationsinto words (or word-like units) is possible withunsupervised methods (e.g.
Brent and Cartwright(1996)), that induction of morphology is possible(e.g.
Goldsmith (2001), Schone and Jurafsky(2001)) and even the induction of syntacticstructures (e.g.
Van Zaanen (2001)).
As mentionedearlier, the main drawback of these approaches isthe lack of incrementality, certain arbitrarydecisions about the properties of elements takeninto account, and the lack of integration into ageneral model of bootstrapping across linguisticlevels.As proposed in Elghamry (2004), cues areelementary language units that can be identified ateach linguistic level, dependent or independent ofprior induction processes.
That is, intrinsicproperties of elements like segments, syllables,morphemes, words, phrases etc.
are the onesavailable for induction procedures.
Intrinsicproperties are for example the frequency of theseunits, their size, and the number of other units theyare build of.
Extrinsic properties are taken intoaccount as well, where extrinsic stands fordistributional properties, the context, relations toother units of the same type on one, as well asacross linguistic levels.
In this model, extrinsic andintrinsic properties of elementary language unitsare the cues that are used for grammar inductiononly.As shown in Elghamry (2004) and Elghamry and?avar (2004), there are efficient ways to identify akernel set of such units in an unsupervised fashionwithout any arbitrary decision where to cut the setof elements and on the basis of what kind offeatures.
They present an algorithm that selects theset of kernel cues on the lexical and syntactic level,as the smallest set of words that co-occurs with allother words.
Using this set of words it is possibleto cluster the lexical inventory into open andclosed class words, as well as to identify thesubclasses of nouns and verbs in the open class.The direction of the selectional preferences of thelanguage is derived as an average of point-wiseMutual Information on each side of the identifiedcues and types, which is a self-supervision aspectthat biases the search direction for a specificlanguage.
This resulting information is understoodas derivation of secondary cues, which then can beused to induce selectional properties of verbs(frames), as shown in Elghamry (2004).The general claim thus is:?
Cues can be identified in an unsupervisedfashion in the input.?
These cues can be used to induce properties ofthe target grammar.?
These properties represent cues that can beused to induce further cues, and so on.The hypothesis is that this snowball effect canreduce the search space of the target grammarincrementally.
The main research questions arenow, to what extend do different algorithmsprovide cues for other linguistic levels and whatkind of information do they require as supervisionin the system, in order to gain the highest accuracyat each linguistic level, and how does the linguisticinformation of one level contribute to theinformation on another.In the following, the architectural considerationsof such a computational model are discussed,resulting in an example implementation that isapplied to morphology induction, wheremorphological properties are understood torepresent cues for lexical clustering as well assyntactic structure, and vice versa, similar to theideas formulated in D?jean (1998), among others.1.2 Incremental Induction ArchitectureThe basic architectural principle we presupposeis incrementality, where incrementally utterancesare processed.
The basic language unit is anutterance, with clear prosodic breaks before andafter.
The induction algorithm consumes suchutterances and breaks them into basic linguisticunits, generating for each step hypotheses about10the linguistic structure of each utterance, based onthe grammar built so far and statistical propertiesof the single linguistic units.
Here we presuppose asuccessful segmentation into words, i.e.
feedingthe system utterances with unambiguous wordboundaries.
We implemented the followingpipeline architecture:The GEN module consumes input and generateshypotheses about its structural descriptions (SD).EVAL consumes a set of SDs and selects the set ofbest SDs to be added to the knowledge base.
Theknowledge base is a component that not only storesSDs but also organizes them into optimalrepresentations, here morphology grammars.All three modules are modular, containing a setof algorithms that are organized in a specificfashion.
Our intention is to provide a generalplatform that can serve for the evaluation andcomparison of different approaches at every levelof the induction process.
Thus, the system isdesigned to be more general, applicable to theproblem of segmentation, as well as type andgrammar induction.We assume for the input to consist of analphabet: a non-empty set A of n symbols {s1, s2,...sn}.
A word w is a non-empty list of symbols w =[s1,s2,... sn], with s?A.
The corpus is a non-emptylist C of words C = [w1,w2,... wn].In the following, the individual modules for themorphology induction task are described in detail.1.2.1 GENFor the morphology task GEN is compiled from aset of basically two algorithms.
One algorithm is avariant of Alignment Based Learning (ABL), asdescribed in Van Zaanen (2001).The basic ideas in ABL go back to concepts ofsubstitutability and/or complementarity, asdiscussed in Harris (1961).
The concept ofsubstitutability generally applies to central part ofthe induction procedure itself, i.e.
substitutableelements (e.g.
substrings, words, structures) areassumed to be of the same type (represented e.g.with the same symbol).The advantage of ABL for grammar induction isits constraining characteristics with respect to theset of hypotheses about potential structuralproperties of a given input.
While a brute-forcemethod would generate all possible structuralrepresentations for the input in a first orderexplosion and subsequently filter out irrelevanthypotheses, ABL reduces the set of possible SDsfrom the outset to the ones that are motivated byprevious experience/input or a pre-existinggrammar.Such constraining characteristics make ABLattractive from a cognitive point of view, bothbecause hopefully the computational complexity isreduced on account of the smaller set of potentialhypotheses, and also because learning of newitems, rules, or structural properties is related to ageneral learning strategy and previous experienceonly.
The approaches that are based on a brute-force first order explosion of all possiblehypotheses with subsequent filtering of relevant orirrelevant structures are both memory-intensiveand require more computational effort.The algorithm is not supposed to make anyassumptions about types of morphemes.
There isno expectation, including use of notions like stem,prefix, or suffix.
We assume only linear sequences.The properties of single morphemes, being stemsor suffixes, should be a side effect of theirstatistical properties (including their frequency andco-occurrence patterns, as will be explained in thefollowing), and their alignment in the corpus, orrather within words.There are no rules about language built-in, suchas what a morpheme must contain or how frequentit should be.
All of this knowledge is inducedstatistically.In the ABL Hypotheses Generation, a givenword in the utterance is checked againstmorphemes in the grammar.
If an existingmorpheme LEX aligns with the input word INP, ahypothesis is generated suggesting amorphological boundary at the alignmentpositions:INP (speaks) + LEX (speak) = HYP [speak, s]Another design criterion for the algorithm iscomplete language independence.
It should be ableto identify morphological structures of Indo-European type of languages, as well asagglutinative languages (e.g.
Japanese andTurkish) and polysynthetic languages like someBantu dialects or American Indian languages.
Inorder to guarantee this behavior, we extended theAlignment Based hypothesis generation with apattern identifier that extracts patterns of charactersequences of the types:1.
A ?
B ?
A2.
A ?
B ?
A ?
B3.
A ?
B ?
A ?
CThis component is realized with cascadedregular expressions that are able to identify and11return the substrings that correspond to therepeating sequences.3All possible alignments for the existing grammarat the current state, are collected in a hypothesislist and sent to the EVAL component, described inthe following.
A hypothesis is defined as a tuple:H = <w, f, g>, with w the input word, f itsfrequency in C, and g a list of substrings thatrepresent a linear list of morphemes in w, g = [m1, m2, ... mn ].1.2.2 EVALEVAL is a voting based algorithm that subsumesa set of independent algorithms that judge the listof SDs from the GEN component, using statisticaland information theoretic criteria.
The specificalgorithms are grouped into memory and usabilityoriented constraints.Taken as a whole, the system assumes two (oftencompeting) cognitive considerations.
The first ofthese forms a class of what we term ?time-based?constraints on learning.
These constraints areconcerned with the processing time required of asystem to make sense of items in an input stream,whereby ?time?
is understood to mean the numberof steps required to generate or parse SDs ratherthan the actual temporal duration of the process.To that end, they seek to minimize the amount ofstructure assigned to an utterance, which is to saythey prefer to deal with as few rules as possible.The second of these cognitive considerations formsa class of ?memory-based?
constraints.
Here, weare talking about constraints that seek to minimizethe amount of memory space required to store anutterance by maximizing the efficiency of thestorage process.
In the specific case of our model,which deals with morphological structure, thismeans that the memory-based constraints searchthe input string for regularities (in the form ofrepeated substrings) that then need only be storedonce (as a pointer) rather than each time they arefound.
In the extreme case, the time-basedconstraints prefer storing the input ?as is?, withoutany processing at all, where the memory-basedconstraints prefer a rule for every character, as thiswould assign maximum structure to the input.Parsable information falls out of the tensionbetween these two conflicting constraints, whichcan then be applied to organize the input intopotential syntactic categories.
These can then be3 This addition might be understood to be a sort ofsupervision in the system.
However, as shown in recentresearch on human cognitive abilities, and especially onthe ability to identify patterns in the speech signal byvery young infants (Marcus et al 1999) shows that wecan assume such an ability to be part of the cognitiveabilities, maybe not even language specificused to set the parameters for the internal adultparsing system.Each algorithm is weighted.
In the currentimplementation these weights are set manually.
Infuture studies we hope to use the weighting forself-supervision.4 Each algorithm assigns anumerical rank to each hypothesis multiplied withthe corresponding weight, a real number between 0and 1.On the one hand, our main interest lies in thecomparison of the different algorithms and apossible interaction or dependency between them.Also, we expect the different algorithms to be ofvarying importance for different types oflanguages.Mutual Information (MI)For the purpose of this experiment we use avariant of standard Mutual Information (MI), seee.g.
MacKay (2003).
Information theory tells usthat the presence of a given morpheme restricts thepossibilities of the occurrence of morphemes to theleft and right, thus lowering the amount of bitsneeded to store its neighbors.
Thus we should beable to calculate the amount of bits needed by amorpheme to predict its right and left neighborsrespectively.
To calculate this, we have designed avariant of mutual information that is concernedwith a single direction of information.This is calculated in the following way.
Forevery morpheme y that occurs to the right of x wesum the point-wise MI between x and y, but werelativize the point-wise MI by the probability thaty follows x, given that x occurs.
This then gives usthe expectation of the amount of information that xtells us about which morpheme will be to its right.Note that p(<xy>) is the probability of the bigram<xy> occurring and is not equal to p(<yx>) whichis the probability of the bigram <yx> occurring.We calculate the MI on the right side of x?G by:p(< xy >| x)lg p(< xy >)p(x)p(y)y?
{<xY >}?and the MI on the left of x?G respectively by:p(< yx >| x)lg p(< yx >)p(y) p(x)y?
{<Yx>)?One way we use this as a metric, is by summingup the left and right MI for each morpheme in a4 One possible way to self-supervise the weights inthis architecture is by taking into account the revisionssubsequent components make when they optimize thegrammar.
If rules or hypotheses have to be removedfrom the grammar due to general optimizationconstraints on the grammars as such, the weight of theresponsible algorithm can be lowered, decreasing itsgeneral value in the system on the long run.
Therelevant evaluations with this approach are not yetfinished.12hypothesis.
We then look for the hypothesis thatresults in the maximal value of this sum.
Thetendency for this to favor hypotheses with manymorphemes is countered by our criterion offavoring hypotheses that have fewer morphemes,discussed later.Another way to use the left and right MI is injudging the quality of morpheme boundaries.
In agood boundary, the morpheme on the left sideshould have high right MI and the morpheme onthe right should have high left MI.
Unfortunately,MI is not reliable in the beginning because of thelow frequency of morphemes.
However, as thelexicon is extended during the induction procedure,reliable frequencies are bootstrapping thissegmentation evaluation.Minimum Description Length (DL)The principle of Minimum Description Length(MDL), as used in recent work on grammarinduction and unsupervised language acquisition,e.g.
Goldsmith (2001) and De Marcken (1996),explains the grammar induction process as aniterative minimization procedure of the grammarsize, where the smaller grammar corresponds to thebest grammar for the given data/corpus.The description length metric, as we use it here,tells us how many bits of information would berequired to store a word given a hypothesis of themorpheme boundaries, using the so far generatedgrammar.
For each morpheme in the hypothesisthat doesn't occur in the grammar we need to storethe string representing the morpheme.
Formorphemes that do occur in our grammar we justneed to store a pointer to that morphemes entry inthe grammar.
We use a simplified calculation,taken from Goldsmith (2001), of the cost of storinga string that takes the number of bits ofinformation required to store a letter of thealphabet and multiply it by the length of the string.lg(len(alphabet))* len(morpheme)We have two different methods of calculatingthe cost of the pointer.
The first assigns a variablethe cost based on the frequency of the morphemethat it is pointing to.
So first we calculate thefrequency rank of the morpheme being pointed to,(e.g.
the most frequent has rank 1, the second rank2, etc.).
We then calculate:floor(lg( freq_ rank) ?1)to get a number of bits similar to the way Morsecode assigns lengths to various letters.The second is simpler and only calculates theentropy of the grammar of morphemes and usesthis as the cost of all pointers to the grammar.
Theentropy equation is as follows:p(x)lg1p(x)x?G?The second equation doesn't give variablepointer lengths, but it is preferred since it doesn'tcarry the heavy computational burden ofcalculating the frequency rank.We calculate the description length for each GENhypothesis only,5 by summing up the cost of eachmorpheme in the hypothesis.
Those with lowdescription lengths are favored.Relative Entropy (RE)We are using RE as a measure for the cost ofadding a hypothesis to the existing grammar.
Welook for hypotheses that when added to thegrammar will result in a low divergence from theoriginal grammar.We calculate RE as a variant of the Kullback-Leibler Divergence, see MacKay (2003).
Givengrammar G1, the grammar generated so far, and G2the grammar with the extension generated for thenew input increment, P(X) is the probability massfunction (pmf) for grammar G2, and Q(X) the pmffor grammar G1:P(x)lgP(x)Q(x)x?X?Note that with every new iteration a new elementcan appear, that is not part of G1.
Our variant of REtakes this into account by calculating the costs forsuch a new element x to be the point-wise entropyof this element in P(X), summing up over all newelements:P(x)lg1P(x)x?X?These two sums then form the RE between theoriginal grammar and the new grammar with theaddition of the hypothesis.
Hypotheses with lowRE are favored.This metric behaves similarly to descriptionlength, that is discussed above, in that both arecalculating the distance between our originalgrammar and the grammar with the inclusion of thenew hypothesis.
The primary difference is RE alsotakes into account how the pmf differs in the twogrammars and that our variation punishes newmorphemes based upon their frequency relative tothe frequency of other morphemes.
Ourimplementation of MDL does not considerfrequency in this way, which is why we areincluding RE as an independent metric.Further MetricsIn addition to the mentioned metric, we take intoaccount the following criteria: a.
Frequency of5 We do not calculate the sizes of the grammars withand without the given hypothesis, just the amount eachgiven hypothesis would add to the grammar, favoringthe least increase of total grammar size.13morpheme boundaries; b.
Number of morphemeboundaries; c. Length of morphemes.The frequency of morpheme boundaries is givenby the number of hypotheses that contain thisboundary.
The basic intuition is that the higher thisnumber is, i.e.
the more alignments are found at acertain position within a word, the more likely thisposition represents a morpheme boundary.
Wefavor hypotheses with high values for thiscriterion.The number of morpheme boundaries indicateshow many morphemes the word was split into.
Toprevent the algorithm from degenerating into thestate where each letter is identified as a morpheme,we favor hypotheses with low number ofmorpheme boundaries.The length of the morphemes is also taken intoaccount.
We favor hypotheses with longmorphemes to prevent the same degenerate state asthe above criterion.1.2.3 Linguistic KnowledgeThe acquired lexicon is stored in a hypothesisspace which keeps track of the words from theinput and the corresponding hypotheses.
Thehypothesis space is defined as a list of hypotheses:Hypotheses space: S = [ H1, H2, ... Hn]Further, each morpheme that occurred in the SDsof words in the hypothesis space is kept with itsfrequency information, as well as bigrams thatconsist of morpheme pairs in the SDs and theirfrequency.6Similar to the specification of signatures inGoldsmith (2001), we list every morpheme withthe set of morphemes it co-occurs.
Signatures arelists of morphemes.
Grammar construction isperformed by replacement of morphemes with asymbol, if they have equal signatures.The hypothesis space is virtually divided intotwo sections, long term and short term storage.Long term storage is not revised further, in thecurrent version of the algorithm.
The short termstorage is cyclically cleaned up by eliminating thesignatures with a low likelihood, given the longterm storage.2 The experimental settingIn the following we discuss the experimentalsetting.
We used the Brown corpus,7 the child-6 Due to space restrictions we do not formalize thisfurther.
A complete documentation and the source codeis available at: http://jones.ling.indiana.edu/~abugi/.7 The Brown Corpus of Standard American English,consisting of 1,156,329 words from American textsprinted in 1961 organized into 59,503 utterances andcompiled by W.N.
Francis and H. Kucera at BrownUniversity.oriented speech portion of the CHILDES Petercorpus,8 and Caesar?s ?De Bello Gallico?
in Latin.9From the Brown corpus we used the files ck01 ?ck09, with an average number of 2000 words perchapter.
The total number of words in these files is18071.
The randomly selected portion of ?De BelloGallico?
contained 8300 words.
The randomlyselected portion of the Peter corpus contains 58057words.The system reads in each file and dumps loginformation during runtime that contains theinformation for online and offline evaluation, asdescribed below in detail.The gold standard for evaluation is based onhuman segmentation of the words in the respectivecorpora.
We create for every word a manualsegmentation for the given corpora, used for onlineevaluation of the system for accuracy of hypothesisgeneration during runtime.
Due to complicatedcases, where linguist are undecided about theaccurate morphological segmentation, a team of 5linguists was cooperating with this task.The offline evaluation is based on the grammarthat is generated and dumped during runtime aftereach input file is processed.
The grammar ismanually annotated by a team of linguists,indicating for each construction whether it wassegmented correctly and exhaustively.
Anadditional evaluation criterion was to markundecided cases, where even linguists do notagree.
This information was however not used inthe final evaluation.2.1 EvaluationWe used two methods to evaluate theperformance of the algorithm.
The first analyzesthe accuracy of the morphological rules producedby the algorithm after an increment of n words.The second looks at how accurately the algorithmparsed each word that it encountered as itprogressed through the corpus.The morphological rule analysis looks at eachgrammar rule generated by the algorithm andjudges it on the correctness of the rule and theresulting parse.
A grammar rule consists of a stemand the suffixes and prefixes that can be attachedto it, similar to the signatures used in Goldsmith(2001).
The grammar rule was then marked as towhether it consisted of legitimate suffixes andprefixes for that stem, and also as to whether the8 Documented in L. Bloom (1970) and available athttp://xml.talkbank.org:8888/talkbank/file/CHILDES/Eng-USA/Bloom70/Peter/.9 This was taken from the Gutenberg archive at:http://www.gutenberg.net/etext/10657.
The Gutenbergheader and footer were removed for the experimentalrun.14stem of the rule was a true stem, as opposed to astem plus another morpheme that wasn't identifiedby the algorithm.
The number of rules that werecorrect in these two categories were then summed,and precision and recall figures were calculated forthe trial.
The trials described in the graph belowwere run on three increasingly large portions of thegeneral fiction section of the Brown Corpus.
Thefirst trial was run on one randomly chosen chapter,the second trial on two chapters, and the third onthree chapters.
The graph shows the harmonicaverage (F-score) of precision and recall.The second analysis is conducted as thealgorithm is running and examines each parse thesystem produces.
The algorithm's parses arecompared with the ?correct?
morphological parseof the word using the following method to derive anumerical score for a particular parse.
The firstpart of the score is the distance in charactersbetween each morphological boundary in the twoparses, with a score of one point for each characterspace.
The second part is a penalty of two pointsfor each morphological boundary that occurs inone parse and not the other.
These scores wereexamined within a moving window of words thatprogressed through the corpus as the algorithm ran.The average scores of words in each such windowwere calculated as the window advanced.
Thepurpose of this method was to allow theperformance of the algorithm to be judged at agiven point without prior performance in thecorpus affecting the analysis of the currentwindow.
The following graph shows how theaverage performance of the windows of analyzedwords as the algorithm progresses through fiverandomly chosen chapters of general fiction in theBrown Corpus amounting to around 10,000 words.The window size for the following graph was set to40 words.The evaluations on Latin were based on theinitial 4000 words of ?De Bello Gallico?
in apretest.
In the very initial phase we reached aprecision of 99.5% and a recall of 13.2%.
This ishowever the preliminary result for the initial phaseonly.
We expect that for a larger corpus the recallwill increase much higher, given the richmorphology of Latin, potentially with negativeconsequences for precision.The results on the Peter corpus are shown in thefollowing table:After file precision recall01 .9957 .832601-03 .9968 .812101-05 .9972 .801901-07 .9911 .771001-09 .9912 .7666We notice a more or less stable precision valuewith decreasing recall, due to a higher number ofwords.
The Peter corpus contains also many veryspecific transcriptions and tokens that are indeedunique, thus it is rather surprising to get suchresults at all.
The following graphics shows the F-score for the Peter corpus:3 ConclusionThe evaluations on two related morphologysystems show that with a restrictive setting of theparameters in the described algorithm, approx 99%precision can be reached, with a recall higher than60% for the portion of the Brown corpus, and evenhigher for the Peter corpus.We are able to identify phases in the generationof rules that turn out to be for English: a. initiallyinflectional morphology on verbs, with the plural?s?
on nouns, and b. subsequently other types ofmorphemes.
We believe that this phenomenon ispurely driven by the frequency of thesemorphemes in the corpora.
In the manuallysegmented portion of the Brown corpus weidentified on the token level 11.3% inflectionalmorphemes, 6.4% derivational morphemes, and82.1% stems.
In average there are twice as manyinflectional morphemes in the corpus, thanderivational.Given a very strict parameters, focusing on thedescription length of the grammar, our systemwould need long time till it would discoverprefixes, not to mention infixes.
By relaxing theweight of description length we can inhibit the15generation and identification of prefixing rules,however, to the cost of precision.Given these results, the inflectional paradigmscan be claimed to be extractable even with anincremental approach.
As such, this means thatcentral parts of the lexicon can be induced veryearly along the time line.The existing signatures for each morpheme canbe used as simple clustering criteria.10 Clusteringwill separate dependent (affixes) from independentmorphemes (stems).
Their basic distinction is thataffixes will usually have a long signature, i.e.many elements they co-occur with, as well as ahigh frequency, while for stems the opposite istrue.11 Along these lines, morphemes with a similarsignature can be replaced by symbols, expressingthe same type information and compressing thegrammar further.
This type information, especiallyfor rare morphemes is essential in subsequentinduction of syntactic structure.
Due to spacelimitations, we cannot discuss in detail subsequentsteps in the cross-level induction procedures.Nevertheless, the model presented here provides animportant pointer to the mechanics of howgrammatical parameters might come to be set.Additionally, we provide a method by which totest the roles different statistical algorithms play inthis process.
By adjusting the weights of thecontributions made by various constraints, we canapproach an understanding of the optimal orderingof algorithms that play a role in the computationalframework of language acquisition.This is but a first step to what we hope willeventually finish a platform for a detailed study ofvarious induction algorithms and evaluationmetrics.ReferencesE.
O. Batchelder.
1997.
Computational evidence for theuse of frequency information in discovery of theinfant?s first lexicon.
PhD dissertation, CUNY.E.
O. Batchelder.
1998.
Can a computer really modelcognition?
A case study of six computational modelsof infant word discovery.
In M. A. Gernsbacher andS.
J. Derry, editors, Proceedings of the 20th AnnualConference of the Cognitive Science Society, pages120?125.
Lawrence Erlbaum, University ofWisconsin-Madison.L.
Bloom, L. Hood, and P. Lightbown.
1974.
Imitationin language development: If, when and why.Cognitive Psychology, 6, 380?420.10 Length of the signature and frequency of eachmorpheme are mapped on a feature vector.11 This way, similar to the clustering of words intoopen and closed class on the basis of feature vectors, asdescribed in Elghamry and ?avar (2004), themorphemes can be separated into open and closed class.M.R.
Brent and T.A.
Cartwright.
1996.
Distributionalregularity and phonotactic constraints are useful forsegmentation.
Cognition 61: 93-125.H.
D?jean.
1998.
Concepts et alorithmes pour lad?couverte des structures formelles des langues.Doctoral dissertation, Universit?
de Caen BasseNormandie.K.
Elghamry.
2004.
A generalized cue-based approachto the automatic acquisition of subcategorizationframes.
Doctoral dissertation, Indiana University.K.
Elghamry and D. ?avar.
2004.
Bootstrapping cuesfor cue-based bootstrapping.
Mscr.
IndianaUniversity.J.
Fodor and V. Teller.
2000.
Decoding syntacticparameters: The superparser as oracle.
Proceedings ofthe Twenty-Second Annual Conference of theCognitive Science Society, 136-141.J.
Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics 27(2): 153-198.Z.S.
Harris.
1961.
Structural linguistics.
University ofChicago Press.
Chicago.J.
Grimshaw.
1981.
Form, function, and the languageacquisition device.
In C.L.
Baker and J.J.
McCarthy(eds.
), The Logical Problem of Language Acquisition.Cambridge, MA: MIT Press.D.J.C.
MacKay.
2003.
Information Theory, Inference,and Learning Algorithms.
Cambridge: CambridgeUniversity Press.C.G.
de Marcken.
1996.
Unsupervised LanguageAcquisition.
Phd dissertation, MIT.G.F.
Marcus, S. Vijayan, S. Bandi Rao, and P.M.Vishton.
1999.
Rule-learning in seven-month-oldinfants.
Science 283:77-80.R.
Mazuka.
1998.
The Development of LanguageProcessing Strategies: A cross-linguistic studybetween Japanese and English.
Lawrence Erlbaum.T.H.
Mintz.
1996.
The roles of linguistic input andinnate mechanisms in children's acquisition ofgrammatical categories.
Unpublished doctoraldissertation, University of Rochester.S.
Pinker.
1984.
Language Learnability and LanguageDevelopment, Harvard University Press, Cambridge,MA.S.
Pinker.
1994.
The language instinct.
New York, NY:W. Morrow and Co.P.
Schone and D. Jurafsky.
2001.
Knowledge-FreeInduction of Inflectional Morphologies.
InProceedings of NAACL-2001.
Pittsburgh, PA, June2001.M.M.
Van Zaanen and Pieter Adriaans.
2001.Comparing two unsupervised grammar inductionsystems: Alignment-based learning vs. EMILE.
Tech.Rep.
TR2001.05, University of Leeds.M.M.
Van Zaanen.
2001.
Bootstrapping Structure intoLanguage: Alignment-Based Learning.
Doctoraldissertation, The University of Leeds.16
