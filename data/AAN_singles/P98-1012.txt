Entity-Based Cross-Document Coreferencing Using the VectorSpace ModelAmit  BaggaBox 90129Dept.
of Computer  ScienceDuke UniversityDurham, NC 27708-0129amit@cs.duke.eduBreck Ba ldwinInst i tute for Research in Cognit ive SciencesUniversity of Pennsylvania3401 Walnut St. 400CPhi ladelphia, PA 19104breck@unagi.cis.upenn.eduAbst ractCross-document coreference occurs when the sameperson, place, event, or concept is discussed in morethan one text source.
Computer recognition of thisphenomenon is important because it helps break"the document boundary" by allowing a user to ex-amine information about a particular entity frommultiple text sources at the same time.
In this paperwe describe a cross-document coreference r solutionalgorithm which uses the Vector Space Model to re-solve ambiguities between people having the samename.
In addition, we also describe a scoring algo-rithm for evaluating the cross-document coreferencechains produced by our system and we compare ouralgorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.1 In t roduct ionCross-document coreference occurs when the sameperson, place, event, or concept is discussed in morethan one text source.
Computer recognition of thisphenomenon is important because it helps break"the document boundary" by allowing a user to ex-amine information about a particular entity frommultiple text sources at the same time.
In partic-ular, resolving cross-document coreferences allowsa user to identify trends and dependencies acrossdocuments.
Cross-document coreference an also beused as the central tool for producing summariesfrom multiple documents, and for information fu-sion, both of which have been identified as advancedareas of research by the TIPSTER Phase III pro-gram.
Cross-document coreference was also identi-fied as one of the potential tasks for the Sixth Mes-sage Understanding Conference (MUC-6) but wasnot included as a formal task because it was consid-ered too ambitious (Grishman 94).In this paper we describe ahighly successful cross-document coreference resolution algorithm whichuses the Vector Space Model to resolve ambiguitiesbetween people having the same name.
In addition,we also describe a scoring algorithm for evaluatingthe cross-document coreference hains produced byour system and we compare our algorithm to thescoring algorithm used in the MUC-6 (within docu-ment) coreference task.2 Cross -Document  Core ference :  TheProb lemCross-document coreference is a distinct echnologyfrom Named Entity recognizers like IsoQuest's Ne-tOwl and IBM's Textract because it attempts todetermine whether name matches are actually thesame individual (not all John Smiths are the same).Neither NetOwl or Textract have mechanisms whichtry to keep same-named individuals distinct if theyare different people.Cross-document coreference also differs in sub-stantial ways from within-document coreference.Within a document there is a certain amount ofconsistency which cannot be expected across doc-uments.
In addition, the problems encountered dur-ing within document coreference are compoundedwhen looking for coreferences across documents be-cause the underlying principles of linguistics anddiscourse context no longer apply across docu-ments.
Because the underlying assumptions in cross-document coreference are so distinct, they requirenovel approaches.3 Arch i tec ture  and  the  Methodo logyFigure 1 shows the architecture of the cross-document system developed.
The system is builtupon the University of Pennsylvania's within docu-ment coreference system, CAMP, which participatedin the Seventh Message Understanding Conference(MUC-7) within document coreference task (MUC-7 1998).Our system takes as input the coreference pro-cessed documents output by CAMP.
It then passesthese documents through the SentenceExtractormodule which extracts, for each document, all thesentences relevant to a particular entity of interest.The VSM-Disambiguate module then uses a vectorspace model algorithm to compute similarities be-tween the sentences extracted for each pair of docu-ments.79Coreference Chains for doe.01~ Permlight Coreference System ~ SentenceExtractorCross-Document Coreference Chainsi iLi VSM- Disambiguat?summary'O1 \[summary.tin I ~;Figure 1: Architecture of the Cross-Document Coreference SystemJohn Perry, of Weston Golf Club, an-nounced his resignation yesterday.
He wasthe President of the Massachusetts GolfAssociation.
During his two years in of-rice, Perry guided the MGA into a closerrelationship with the Women's Golf Asso-ciation of Massachusetts.Oliver "Biff" Kelly of Weymouth suc-ceeds John Perry as president of the Mas-sachusetts Golf Association.
"We will havecontinued growth in the future," said Kelly,who will serve for two years.
"There's beena lot of changes and there will be continuedchanges as we head into the year 2000.
"Figure 2: Extract from doc.36?
,IFigure 3: Coreference Chains for doc.36Details about each of the main steps of the cross-document coreference algorithm are given below.?
First, for each article, CAMP is run on the ar-ticle.
It produces coreference chains for all theentities mentioned in the article.
For example,consider the two extracts in Figures 2 and 4.The coreference hains output by CAMP for thetwo extracts are shown in Figures 3 and 5.Figure 4: Extract from doc.38IIIIFigure 5: Coreference Chains for doc.38Next, for the coreference hain of interest withineach article (for example, the coreference chainthat contains "John Perry"), the Sentence Ex-tractor module extracts all the sentences thatcontain the noun phrases which form the corefoerence chain.
In other words, the SentenceEx-tractor module produces a "summary" of the ar-ticle with respect to the entity of interest.
Thesesummaries are a special case of the query sensi-tive techniques being developed at Penn using80CAMP.
Therefore, for doc.36 (Figure 2), sinceat least one of the three noun phrases ("JohnPerry," "he," and "Perry") in the coreferencechain of interest appears in each of the threesentences in the extract, the summary producedby SentenceExtractor is the extract itself.
Onthe other hand, the summary produced by Sen-tenceExtractor for the coreference chain of in-terest in doc.38 is only the first sentence of theextract because the only element of the corefer-ence chain appears in this sentence.
* For each article, the VSM-Disambiguate mod-ule uses the summary extracted by the Sen-tenceExtractor and computes its similarity withthe summaries extracted from each of the otherarticles.
Summaries having similarity above acertain threshold are considered to be regard-ing the same entity.4 Un ivers i ty  o f  Pennsy lvan ia ' sCAMP SystemThe University of Pennsylvania's CAMP system re-solves within document coreferences for several dif-ferent classes including pronouns, and proper names(Baldwin 95).
It ranked among the top systemsin the coreference task during the MUC-6 and theMUC-7 evaluations.The coreference chains output by CAMP enableus to gather all the information about the entity ofinterest in an article.
This information about theentity is gathered by the SentenceExtractor mod-ule and is used by the VSM-Disambiguate modulefor disambignation purposes.
Consider the extractfor doc.36 shown in Figure 2.
We are able to in-clude the fact that the John Perry mentioned in thisarticle was the president of the Massachusetts GolfAssociation only because CAMP recognized that the"he" in the second sentence is coreferent with "JohnPerry" in the first.
And it is this fact which actuallyhelps VSM-Disambignate decide that the two JohnPerrys in doc.36 and doc.38 are the same person.5 The  Vector  Space  Mode lThe vector space model used for disambignating en-tities across documents i the standard vector spacemodel used widely in information retrieval (Salton89).
In this model, each summary extracted by theSentenceExtractor module is stored as a vector ofterms.
The terms in the vector are in their mor-phological root form and are filtered for stop-words(words that have no information content like a, the,of, an, ... ).
If $1 and $2 are the vectors for the twosummaries extracted from documents D1 and D2,then their similarity is computed as:Sim(S1, $2) = E wlj x w2jcommon terms tjwhere tj is a term present in both $1 and $2, wlj isthe weight of the term tj in S1 and w~j is the weightof tj in $2.The weight of a term tj in the vector Si for asummary is given by:t f ?
logWi j  = 2_}_ .
.
.+2  Jsi~ + si2 sinwhere t f  is the frequency of the term tj in the sum-mary, N is the total number of documents in thecollection being examined, and df is the number ofdocuments in the collection that the term t j  occurs2 is the cosine normaliza- in.
~/si~ + si~ +.
.
.
+ Sintion factor and is equal to the Euclidean length ofthe vector Si.The VSM-Disambignate module, for each sum-mary Si, computes the similarity of that summarywith each of the other summaries.
If the similaritycomputed is above a pre-defined threshold, then theentity of interest in the two summaries are consid-ered to be coreferent.6 Exper imentsThe cross-document coreference system was testedon a highly ambiguous test set which consisted of197 articles from 1996 and 1997 editions of theNew York Times.
The sole criteria for includingan article in the test set was the presence or theabsence of a string in the article which matchedthe "/John.
*?Smith/" regular expression.
In otherwords, all of the articles either contained the nameJohn Smith or contained some variation with a mid-dle initial/name.
The system did not use any NewYork Times data for training purposes.
The answerkeys regarding the cross-document chains were man-ually created, but the scoring was completely auto-mated.6.1 Analysis of  the DataThere were 35 different John Smiths mentioned inthe articles.
Of these, 24 of them only had one ar-ticle which mentioned them.
The other 173 articleswere regarding the 11 remaining John Smiths.
Thebackground of these John Smiths , and the numberof articles pertaining to each, varied greatly.
De-scriptions of a few of the John Smiths are: Chairmanand CEO of General Motors, assistant track coach atUCLA, the legendary explorer, and the main charac-ter in Disney's Pocahontas, former president of theLabor Party of Britain.7 Scoring the OutputIn order to score the cross-document coreferencechains output by the system, we had to map thecross-document coreference scoring problem to awithin-document coreference scoring problem.
This81was done by creating a meta document consisting6f the file names of each of the documents that thesystem was run on.
Assuming that each of the docu-ments in the data set was about a single John Smith,the cross-document coreference chains produced bythe system could now be evaluated by scoring thecorresponding within-document coreference chainsin the meta document.We used two different scoring algorithms for scor-ing the output.
The first was the standard algorithmfor within-document coreference chains which wasused for the evaluation of the systems participatingin the MUC-6 and the MUC-7 coreference tasks.The shortcomings of the MUC scoring algorithmwhen used for the cross-document coreference taskforced us to develop a second algorithm.Details about both these algorithms follow.7.1 The  MUC Coreference ScoringA lgor i thm 1The MUC algorithm computes precision and recallstatistics by looking at the number of links identi-fied by a system compared to the links in an answerkey.
In the model-theoretic description of the al-gorithm that follows, the term "key" refers to themanually annotated coreference chains (the truth)while the term "response" refers to the coreferencechains output by a system.
An equivalence s t is thetransitive closure of a coreference chain.
The algo-rithm, developed by (Vilain 95), computes recall inthe following way.First, let S be an equivalence set generated by thekey, and let R1.. .
Rm be equivalence classes gener-ated by the response.
Then we define the followingfunctions over S:?
p(S) is a partition of S relative to the response.Each subset of S in the partition is formed byintersecting S and those response sets Ri thatoverlap S. Note that the equivalence classes de-fined by the response may include implicit sin-gleton sets - these correspond to elements thatare mentioned in the key but not in the re-sponse.
For example, say the key generates theequivalence class S = {A B C D}, and the re-sponse is simply <A-B>.
The relative partitionp(S) is then {A B} {C} and {D}.?
c(S) is the minimal number of "correct" linksnecessary to generate the equivalence class S. Itis clear that c(S) is one less than the cardinalityof S, i.e., c(S) = (IS\[ - 1) .?
m(S) is the number of "missing" links in theresponse relative to the key set S. As notedabove, this is the number of links necessary to1The exposition of this scorer has been taken nearly en-tirely from (Vilain 95).Figure 6: TruthFigure 7: Response: Example 1fully reunite any components of the p(S) parti-tion.
We note that this is simply one fewer thanthe number of elements in the partition, that is,m(S) = (Ip(S)l- I) .Looking in isolation at a single equivalence classin the key, the recall error for that class is just thenumber of missing links divided by the number ofm(S) correct links, i.e., c(S) ?c(S)-m(S) Recall in turn is c(S) , which equals( IS l -  1) - ( Ip (S ) l -  I)ISl- iThe whole expression can now be simplified toISl- Ip(S)IISl- 1Precision is computed by switching the roles of thekey and response in the above formulation.7.2 Shor tcomings  of  the MUC Scor ingAlgorithmWhile the (Vilain 95) provides intuitive results forcoreference scoring, it however does not work as wellin the context of evaluating cross document corefer-ence.
There are two main reasons.1.
The algorithm does not give any credit for sep-arating out singletons (entities that occur inchains consisting only of one element, the en-tity itself) from other chains which have beenidentified.
This follows from the convention in82Figure 8: Response: Example 2coreference annotation of not identifying thoseentities that are markable as possibly coreferentwith other entities in the text.
Rather, entitiesare only marked as being coreferent if they ac-tually are coreferent with other entities in thetext.
This shortcoming could be easily enoughovercome with different annotation conventionsand with minor changes to the algorithm, butit is worth noting.2.
All errors are considered to be equal.
The MUCscoring algorithm penalizes the precision num-bers equally for all types of errors.
It is our po-sition that, for certain tasks, some coreferenceerrors do more damage than others.Consider the following examples: suppose thetruth contains two large coreference chains andone small one (Figure 6), and suppose Figures 7and 8 show two different responses.
We will ex-plore two different precision errors.
The firsterror will connect one of the large coreferencechains with the small one (Figure 7).
The sec-ond error occurs when the two large coreferencechains are related by the errant coreferent link(Figure 8).
It is our position that the second er-ror is more damaging because, compared to thefirst error, the second error makes more entitiescoreferent that should not be.
This distinctionis not reflected in the (Vilain 95) scorer whichscores both responses as having a precision scoreof 90% (Figure 9).7.3 Our  B -CUBED Scoring A lgor i thm 2Imagine a scenario where a user recalls a collectionof articles about John Smith, finds a single arti-cle about the particular John Smith of interest andwants to see all the other articles about that indi-vidual.
In commercial systems with News data, pre-cision is typically the desired goal in such settings.As a result we wanted to model the accuracy of thesystem on a per-document basis and then build amore global score based on the sum of the user'sexperiences.2The main idea of this algorithm was initially put forth byAlan W. Biermann of Duke University.Consider the case where the user selects document6 in Figure 8.
This a good outcome with all therelevant documents being found by the system andno extraneous documents.
If the user selected oc-ument 1, then there are 5 irrelevant documents inthe systems output - precision is quite low then.The goal of our scoring algorithm then is to modelthe precision and recall on average when looking formore documents about the same person based onselecting a single document.Instead of looking at the links produced by a sys-tem, our algorithm looks at the presence/absenceof entities from the chains produced.
Therefore, wecompute the precision and recall numbers for eachentity in the document.
The numbers computedwith respect o each entity in the document are thencombined to produce final precision and recall num-bers for the entire output.For an entity, i, we define the precision and recallwith respect o that entity in Figure 10.The final precision and recall numbers are com-puted by the following two formulae:Final Precision =NZ wi * Precisionii=lNFinal Recall = E wi * Recall~i=lwhere N is the number of entities in the document,and wi is the weight assigned to entity i in the doc-ument.
For all the examples and the experiments inthis paper we assign equal weights to each entity i.e.wi = 1IN.
We have also looked at the possibilitiesof using other weighting schemes.
Further detailsabout the B-CUBED algorithm including a modeltheoretic version of the algorithm can be found in(Bagga 98a).Consider the response shown in Figure 7.
Usingthe B-CUBED algorithm, the precision for entity-6in the document equals 2/7 because the chain out-put for the entity contains 7 elements, 2 of which arecorrect, namely {6,7}.
The recall for entity-6, how-ever, is 2/2 because the chain output for the entityhas 2 correct elements in it and the "truth" chain forthe entity only contains those 2 elements.
Figure 9shows the final precision and recall numbers com-puted by the B-CUBED algorithm for the examplesshown in Figures 7 and 8.
The figure also shows theprecision and recall numbers for each entity (orderedby entity-numbers).7.4 Overcoming the Shor tcomings  of  theMUC A lgor i thmThe B-CUBED algorithm does overcome the the twomain shortcomings of the MUC scoring algorithmdiscussed earlier.
It implicitly overcomes the first83Output MUC Algorithm B-CUBED Algorithm (equal weights for every entity)P: 1-%(90%) P:~* \ [~+~+~+~+~+~+~+~+~+~+~+~\]=76%Example 1R:~(100%) R:~*\[~+~+~+~+~+~+~+~+~+~+5+51=I00%P: 9 (90%)  P:~* \ [5+~+~+~+~+~+~+-~+5+5+5+5\ ]=58%Example 2R:~(100%) R: 1*\[~+~+~+~+~+~+~+~+-~+~+~+~1=I00%Precisioni =Figure 9: Scores of Both Algorithms on the Examplesnumber of correct elements in the output chain containing entityiRecalli =number of elements in the output chain containing entityinumber of correct elements in the output chain containing entityinumber of elements in the truth chain containing entityi(t)(2)Figure 10: Definitions for Precision and Recall for an Entity ishortcoming ofthe MUC-6 algorithm by calculatingthe precision and recall numbers for each entity inthe document (irrespective of whether an entity ispart of a coreference chain).
Consider the responsesshown in Figures 7 and 8.
We had mentioned earlierthat the error of linking the the two large chains inthe second response ismore damaging than the errorof linking one of the large chains with the smallerchain in the first response.
Our scoring algorithmtakes this into account and computes a final preci-sion of 58% and 76% for the two responses respec-tively.
In comparison, the MUC algorithm computesa precision of 90% for both the responses (Figure 9).8 Resu l tsFigure 11 shows the precision, recall, and F-Measure(with equal weights for both precision and recall)using the B-CUBED scoring algorithm.
The VectorSpace Model in this case constructed the space ofterms only from the summaries extracted by Sen-tenceExtractor.
In comparison, Figure 12 showsthe results (using the B-CUBED scoring algorithm)when the vector space model constructed the spaceof terms from the articles input to the system (itstill used the summaries when computing the simi-laxity).
The importance of using CAMP to extractsummaries i verified by comparing the highest F-Measures achieved by the system for the two cases.The highest F-Measure for the former case is 84.6%while the highest F-Measure for the latter case is78.0%.
In comparison, for this task, named-entitytools like NetOwl and Textract would mark all theJohn Smiths the same.
Their performance using ourg100 =.
,90 "'t,,80605040 c3020 '100 =0 0.1Precision/Recall vs Threshold: ; ": "7 : ;" ;ur; Ig~Pr;c i ;s ionOur  A Ig :  Reca l l  -+- - -%,, Our  AIg :  F -Measure  -o - -\ \  '~~ ~',~"~ '~"  G" "E}" "{3"" B"  "D - ~3" "O" "~"  G" "E}" "El" "El"+ o "-+- -+-  -+ .
_+_ ..+_ _+_ _+_ _+_ .+_ _+_ .+~..~_I I I I I I I I0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thresho ldFigure 11: Precision, Recall, and F-Measure Us-ing the B-CUBED Algorithm With Training On theSummariesscoring algorithm is 23% precision, and 100% recall.Figures 13 and 14 show the precision, recall, andF-Measure calculated using the MUC scoring algo-rithm.
Also, the baseline case when all the JohnSmiths are considered tobe the same person achieves83% precision and 100% recall.
The high initial pre-cision is mainly due to the fact that the MUC algo-rithm assumes that all errors are equal.We have also tested our system on other classes ofcross-document coreference like names of companies,and events.
Details about these experiments can befound in (Bagga 98b).84Precision/Recall vs Threshold100"'~-, Our AIg: Precision8o .
: ,.70 I "~,"b "~'~'G"r~'-n~.
,0 L~'~"~~?~~~--~201000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 12: Precision, Recall, and F-Measure Usingthe B-CUBED Algorithm With Training On EntireArticlesPrecision/Recall vs Threshold100 / .
,  ~ ~ .
: - .
.
,  - : ; : ; : ~ : ~ : ; c ; : ; :~--o':~,-_ MUC AIo: Precision90 ~-=~-,,~, MUC AIg: Recall .
.
.
.
'~--o MUC AIg: F-Measure -o--7080 i ",, ""m..60 '" "D'-{~.
{3.. O~w *'O.~..~..\[~.G..O.~.t3.
050 "4"'-40 "+"+'~"+-~-~-~-,+,302010o ,.
, , , ,.
,.
, ,.
,0 01 0.2 0.3 0.4 05 06 0.7 08 0.9ThresholdFigure 13: Precision, Recall, and F-Measure Usingthe MUC Algorithm With Training On the Sum-maries9 Conc lus ionsAs a novel research problem, cross document coref-erence provides an different perspective from re-lated phenomenon like named entity recognition andwithin document coreference.
Our system takessummaries about an entity of interest and uses vari-ous information retrieval metrics to rank the similar-ity of the summaries.
We found it quite challengingto arrive at a scoring metric that satisfied our intu-itions about what was good system output v.s.
bad,but we have developed a scoring algorithm that is animprovement for this class of data over other withindocument coreference scoring algorithms.
Our re-sults are quite encouraging with potential perfor-mance being as good as 84.6% (F-Measure).8@.1009080706050403020:1000Precision/Recall vs Threshold- ' ~  MUC AIg: Precision2 ?
",:'Q MUC AIg: Recall -~-.MUC AIg: F-Measure -o--'~,- '" 'O .
.
0~ ""O.. 12;~"- "0- ~3.. 0 .
.0 .
.~"~'" k'-,,k .., W "(3..0..0.0..  0""4-'= +- - .~.
?, , , , ,.
, , ,.
,0,1 0.2 0.3 0.4 05 0.6 0.7 08 0.9ThresholdFigure 14: Precision, Recall, and F-Measure Usingthe MUC Algorithm With Training On Entire Arti-cles10 AcknowledgmentsThe first author was supported in part by a Fel-lowship from IBM Corporation, and in part by theInstitute for Research in Cognitive Science at theUniversity of Pennsylvania.Re ferencesBagga, Amit, and Breck Baldwin.
Algorithms forScoring Coreference Chains.
To appear at TheFirst International Conference on Language Re-sources and Evaluation Workshop on LinguisticsCoreference, May 1998.Bagga, Amit, and Breck Baldwin.
How Much Pro-cessing Is Required for Cross-Document Corefer-ence?
To appear at The First International Con-ferenee on Language Resources and Evaluation onLinguistics Coreferenee, May 1998.Baldwin, Breck, et el.
University of Pennsylva-nia: Description of the University of Pennsylva-nia System Used for MUC-6, Proceedings of theSixth Message Understanding Conference (MUC-6), pp.
177-191, November 1995.Grishman, Ralph.
Whither Written Language Eval-uation?, Proceedings of the Human LanguageTechnology Workshop, pp.
120-125, March 1994,San Francisco: Morgan Kaufmann.Proceedings of the Seventh Message UnderstandingConference (MUC-7), April 1998.Salton, Gerard.
Automatic Text Processing: TheTransformation, Analysis, and Retrieval of In-formation by Computer, 1989, Reading, MA:Addison-Wesley.Vilain, Marc, et el.
A Model-Theoretic CoreferenceScoring Scheme, Proceedings of the Sixth MessageUnderstanding Conference (MUC-6), pp.
45-52,November 1995, San Francisco: Morgan Kauf-mann.85
