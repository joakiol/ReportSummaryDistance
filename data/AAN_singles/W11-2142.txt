Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsJoint WMT Submission of the QUAERO Project?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart?RWTH Aachen University, Aachen, Germany?Karlsruhe Institute of Technology, Karlsruhe, Germany?LIMSI-CNRS, Orsay, France?SYSTRAN Software, Inc.?surname@cs.rwth-aachen.de?firstname.surname@kit.edu?firstname.lastname@limsi.fr ?surname@systran.frAbstractThis paper describes the joint QUAERO sub-mission to the WMT 2011 machine transla-tion evaluation.
Four groups (RWTH AachenUniversity, Karlsruhe Institute of Technol-ogy, LIMSI-CNRS, and SYSTRAN) of theQUAERO project submitted a joint translationfor the WMT German?English task.
Eachgroup translated the data sets with their ownsystems.
Then RWTH system combinationcombines these translations to a better one.
Inthis paper, we describe the single systems ofeach group.
Before we present the results ofthe system combination, we give a short de-scription of the RWTH Aachen system com-bination approach.1 OverviewQUAERO is a European research and develop-ment program with the goal of developing multi-media and multilingual indexing and managementtools for professional and general public applica-tions (http://www.quaero.org).
Research in machinetranslation is mainly assigned to the four groupsparticipating in this joint submission.
The aim ofthis WMT submission was to show the quality of ajoint translation by combining the knowledge of thefour project partners.
Each group develop and main-tain their own different machine translation system.These single systems differ not only in their generalapproach, but also in the preprocessing of trainingand test data.
To take the advantage of these dif-ferences of each translation system, we combinedall hypotheses of the different systems, using theRWTH system combination approach.1.1 Data SetsFor WMT 2011 each QUAERO partner trained theirsystems on the parallel Europarl and News Com-mentary corpora.
All single systems were tuned onthe newstest2009 dev set.
The newstest2008 dev setwas used to train the system combination parame-ters.
Finally the newstest2010 dev set was used tocompare the results of the different system combi-nation approaches and settings.2 Translation Systems2.1 RWTH Aachen Single SystemsFor the WMT 2011 evaluation the RWTH utilizedRWTH?s state-of-the-art phrase-based and hierar-chical translation systems.
GIZA++ (Och and Ney,2003) was employed to train word alignments, lan-guage models have been created with the SRILMtoolkit (Stolcke, 2002).2.1.1 Phrase-Based SystemThe phrase-based translation (PBT) system issimilar to the one described in Zens and Ney (2008).After phrase pair extraction from the word-alignedbilingual corpus, the translation probabilities are es-timated by relative frequencies.
The standard featureset alo includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fash-ion.
Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on theword graph.3582.1.2 Hierarchical SystemFor the hierarchical setups described in this pa-per, the open source Jane toolkit (Vilar et al, 2010)is employed.
Jane has been developed at RWTHand implements the hierarchical approach as intro-duced by Chiang (2007) with some state-of-the-artextensions.
In hierarchical phrase-based translation,a weighted synchronous context-free grammar is in-duced from parallel text.
In addition to contiguouslexical phrases, hierarchical phrases with up to twogaps are extracted.
The search is typically carriedout using the cube pruning algorithm (Huang andChiang, 2007).
The model weights are optimizedwith standard MERT (Och, 2003) on 100-best lists.2.1.3 Phrase Model TrainingFor some PBT systems a forced alignment pro-cedure was applied to train the phrase translationmodel as described in Wuebker et al (2010).
Amodified version of the translation decoder is usedto produce a phrase alignment on the bilingual train-ing data.
The phrase translation probabilities are es-timated from their relative frequencies in the phrase-aligned training data.
In addition to providing a sta-tistically well-founded phrase model, this has thebenefit of producing smaller phrase tables and thusallowing more rapid and less memory consumingexperiments with a better translation quality.2.1.4 Final SystemsFor the German?English task, RWTH conductedexperiments comparing the standard phrase extrac-tion with the phrase training technique described inSection 2.1.3.
Further experiments included the useof additional language model training data, rerank-ing of n-best lists generated by the phrase-based sys-tem, and different optimization criteria.A considerable increase in translation quality canbe achieved by application of German compoundsplitting (Koehn and Knight, 2003).
In comparisonto standard heuristic phrase extraction techniques,performing force alignment phrase training (FA)gives an improvement in BLEU on newstest2008and newstest2009, but a degradation in TER.
Theaddition of LDC Gigaword corpora (+GW) to thelanguage model training data shows improvementsin both BLEU and TER.
Reranking was done on1000-best lists generated by the the best availablesystem (PBT (FA)+GW).
Following models wereapplied: n-gram posteriors (Zens and Ney, 2006),sentence length model, a 6-gram LM and IBM-1 lex-icon models in both normal and inverse direction.These models are combined in a log-linear fashionand the scaling factors are tuned in the same man-ner as the baseline system (using TER?4BLEU onnewstest2009).The final table includes two identical Jane sys-tems which are optimized on different criteria.
Theone optimized on TER?BLEU yields a much lowerTER.2.2 Karlsruhe Institute of Technology SingleSystem2.2.1 PreprocessingWe preprocess the training data prior to trainingthe system, first by normalizing symbols such asquotes, dashes and apostrophes.
Then smart-casingof the first words of each sentence is performed.
Forthe German part of the training corpus we use thehunspell1 lexicon to learn a mapping from old Ger-man spelling to new German spelling to obtain a cor-pus with homogeneous spelling.
In addition, we per-form compound splitting as described in (Koehn andKnight, 2003).
Finally, we remove very long sen-tences, empty lines, and sentences that probably arenot parallel due to length mismatch.2.2.2 System OverviewThe KIT system uses an in-house phrase-baseddecoder (Vogel, 2003) to perform translation.
Op-timization with regard to the BLEU score is doneusing Minimum Error Rate Training as describedby Venugopal et al (2005).
The translation modelis trained on the Europarl and News CommentaryCorpus and the phrase table is based on a GIZA++Word Alignment.
We use two 4-gram SRI languagemodels, one trained on the News Shuffle corpus andone trained on the Gigaword corpus.
Reordering isperformed based on continuous and non-continuousPOS rules to cover short and long-range reorder-ings.
The long-range reordering rules were also ap-plied to the training corpus and phrase extractionwas performed on the resulting reordering lattices.Part-of-speech tags are obtained using the TreeTag-1http://hunspell.sourceforge.net/359ger (Schmid, 1994).
In addition, the system appliesa bilingual language model to extend the context ofsource language words available for translation.
Theindividual models are described briefly in the fol-lowing.2.2.3 POS-based Reordering ModelWe use a reordering model that is based on parts-of-speech (POS) and learn probabilistic rules fromthe POS tags of the words in the training corpus andthe alignment information.
In addition to continu-ous reordering rules that model short-range reorder-ing (Rottmann and Vogel, 2007), we apply non-continuous rules to address long-range reorderingsas typical for German-English translation (Niehuesand Kolss, 2009).
The reordering rules are appliedto the source sentences and the reordered sentencevariants as well as the original sequence are encodedin a word lattice which is used as input to the de-coder.2.2.4 Lattice Phrase ExtractionFor the test sentences, the POS-based reorderingallows us to change the word order in the source sen-tence so that the sentence can be translated more eas-ily.
If we apply this also to the training sentences, wewould be able to extract also phrase pairs for origi-nally discontinuous phrases and could apply themduring translation of reordered test sentences.Therefore, we build reordering lattices for alltraining sentences and then extract phrase pairs fromthe monotone source path as well as from the re-ordered paths.
To limit the number of extractedphrase pairs, we extract a source phrase only onceper sentence, even if it is found in different paths andwe only use long-range reordering rules to generatethe lattices for the training corpus.2.2.5 Bilingual Language ModelIn phrase-based systems the source sentence issegmented by the decoder during the search pro-cess.
This segmentation into phrases leads to theloss of context information at the phrase boundaries.The language model can make use of more targetside context.
To make also source language contextavailable we use a bilingual language model, an ad-ditional language model in the phrase-based systemin which each token consist of a target word and allsource words it is aligned to.
The bilingual tokensenter the translation process as an additional targetfactor.2.3 LIMSI-CNRS Single System2.3.1 System overviewThe LIMSI system is built with n-code2, an opensource statistical machine translation system basedon bilingual n-grams.2.3.2 n-code OverviewIn a nutshell, the translation model is im-plemented as a stochastic finite-state transducertrained using a n-gram model of (source,target)pairs (Casacuberta and Vidal, 2004).
Training thismodel requires to reorder source sentences so as tomatch the target word order.
This is performed by astochastic finite-state reordering model, which usespart-of-speech information3 to generalize reorderingpatterns beyond lexical regularities.In addition to the translation model, eleven fea-ture functions are combined: a target-languagemodel; four lexicon models; two lexicalized reorder-ing models (Tillmann, 2004) aiming at predictingthe orientation of the next translation unit; a weakdistance-based distortion model; and finally a word-bonus model and a tuple-bonus model which com-pensate for the system preference for short transla-tions.
The four lexicon models are similar to the onesuse in a standard phrase based system: two scorescorrespond to the relative frequencies of the tuplesand two lexical weights estimated from the automat-ically generated word alignments.
The weights asso-ciated to feature functions are optimally combinedusing a discriminative training framework (Och,2003), using the newstest2009 data as developmentset.The overall search is based on a beam-searchstrategy on top of a dynamic programming algo-rithm.
Reordering hypotheses are computed in apreprocessing step, making use of reordering rulesbuilt from the word reorderings introduced in the tu-ple extraction process.
The resulting reordering hy-potheses are passed to the decoder in the form ofword lattices (Crego and Marin?o, 2007).2http://www.limsi.fr/Individu/jmcrego/n-code3Part-of-speech information for English and German is com-puted using the TreeTagger.3602.3.3 Data PreprocessingBased on previous experiments which havedemonstrated that better normalization tools providebetter BLEU scores (K. Papineni and Zhu, 2002),all the English texts are tokenized and detokenizedwith in-house text processing tools (De?chelotte etal., 2008).
For German, the standard tokenizer sup-plied by evaluation organizers is used.2.3.4 Target n-gram Language ModelsThe English language model is trained assumingthat the test set consists in a selection of news textsdating from the end of 2010 to the beginning of2011.
This assumption is based on what was donefor the 2010 evaluation.
Thus, a development cor-pus is built in order to create a vocabulary and tooptimize the target language model.Development Set and Vocabulary In order tocover different period, two development sets areused.
The first one is newstest2008.
However, thiscorpus is two years older than the targeted time pe-riod.
Thus a second development corpus is gath-ered by randomly sampling bunches of 5 consecu-tive sentences from the provided news data of 2010and 2011.To estimate a LM, the English vocabulary is firstdefined by including all tokens observed in the Eu-roparl and news-commentary corpora.
This vocabu-lary is then expanded with all words that occur morethat 5 times in the French-English giga-corpus, andwith the most frequent proper names taken from themonolingual news data of 2010 and 2011.
This pro-cedure results in a vocabulary around 500k words.Language Model Training All the training dataallowed in the constrained task are divided into 9sets based on dates on genres.
On each set, astandard 4-gram LM is estimated from the 500kword vocabulary with in-house tools using abso-lute discounting interpolated with lower order mod-els (Kneser and Ney, 1995; Chen and Goodman,1998).All LMs except the one trained on the news cor-pora from 2010-2011 are first linearly interpolated.The associated coefficients are estimated so as tominimize the perplexity evaluated on the dev2010-2011.
The resulting LM and the 2010-2011 LM arefinally interpolated with newstest2008 as develop-ment data.
This two steps interpolation aims to avoidan overestimate of the weight associated to the 2010-2011 LM.2.4 SYSTRAN Software, Inc.
Single SystemThe data submitted by SYSTRAN were obtained bythe SYSTRAN baseline system in combination witha statistical post editing (SPE) component.The SYSTRAN system is traditionally classi-fied as a rule-based system.
However, over thedecades, its development has always been driven bypragmatic considerations, progressively integratingmany of the most efficient MT approaches and tech-niques.
Nowadays, the baseline engine can be con-sidered as a linguistic-oriented system making use ofdependency analysis, general transfer rules as wellas of large manually encoded dictionaries (100k ?800k entries per language pair).The basic setup of the SPE component is identi-cal to the one described in (L. Dugast and Koehn,2007).
A statistical translation model is trained onthe rule-based translation of the source and the tar-get side of the parallel corpus.
This is done sepa-rately for each parallel corpus.
Language models aretrained on each target half of the parallel corpora andalso on additional in-domain corpora.
Moreover, thefollowing measures ?
limiting unwanted statisticaleffects ?
were applied:?
Named entities are replaced by special tokenson both sides.
This usually improves wordalignment, since the vocabulary size is signif-icantly reduced.
In addition, entity translationis handled more reliably by the rule-based en-gine.?
The intersection of both vocabularies (i.e.
vo-cabularies of the rule-based output and the ref-erence translation) is used to produce an addi-tional parallel corpus (whose target is identicalto the source).
This was added to the paralleltext in order to improve word alignment.?
Singleton phrase pairs are deleted from thephrase table to avoid overfitting.?
Phrase pairs not containing the same numberof entities on the source and the target side arealso discarded.361?
Phrase pairs appearing less than 2 times werepruned.The SPE language model was trained 15Mphrases from the news/europarl corpora, providedas training data for WMT 2011.
Weights for theseseparate models were tuned by the MERT algorithmprovided in the Moses toolkit (P. Koehn et al, 2007),using the provided news development set.3 RWTH Aachen System CombinationSystem combination is used to produce consensustranslations from multiple hypotheses produced withdifferent translation engines that are better in termsof translation quality than any of the individual hy-potheses.
The basic concept of RWTH?s approachto machine translation system combination has beendescribed by Matusov et al (2006; 2008).
This ap-proach includes an enhanced alignment and reorder-ing framework.
A lattice is built from the input hy-potheses.
The translation with the best score withinthe lattice according to a couple of statistical mod-els is selected as consensus translation.
A deeperdescription will be also given in the WMT11 sys-tem combination paper of RWTH Aachen Univer-sity.
For this task only the A2L framework has beenused.4 ExperimentsWe tried different system combinations with differ-ent sets of single systems and different optimiza-tion criteria.
As RWTH has two different transla-tion systems, we put the output of both systems intosystem combination.
Although both systems havethe same preprocessing, their hypotheses differ.
Fi-nally, we added for both RWTH systems two addi-tional hypotheses to the system combination.
Thetwo hypotheses of Jane were optimized on differ-ent criteria.
The first hypothesis was optimized onBLEU and the second one on TER?BLEU.
The firstRWTH phrase-based hypothesis was generated withforce alignment, the second RWTH phrase-basedhypothesis is a reranked version of the first one asdescribed in 2.1.4.
Compared to the other systems,the system by SYSTRAN has a completely differentapproach (see section 2.4).
It is mainly based on arule-based system.
For the German?English pair,SYSTRAN achieves a lower BLEU score in eachtest set compared to the other groups.
But since theSYSTRAN system is very different to the others, westill obtain an improvement when we add it also tosystem combination.We obtain the best result from system combina-tion of all seven systems, optimizing the parameterson BLEU.
This system was the system we submittedto the WMT 2011 evaluation.For each dev set we obtain an improvement com-pared to the best single systems.
For newstest2008and newstest2009 we get an improvement of 0.5points in BLEU and 1.8 points in TER compared tothe best single system of Karlsruhe Institute of Tech-nology.
For newstest2010 we get an improvementof 1.8 points in BLEU and 2.7 points in TER com-pared to the best single system of RWTH.
The sys-tem combination weights optimized for the best runare listed in Table 2.
We see that although the singlesystem of SYSTRAN has the lowest BLEU scores,it gets the second highest system weight.
This highvalue shows the influence of a completely differentsystem.
On the other hand, all RWTH systems arevery similar, because of their same preprocessingand their small variations.
Therefor the system com-bination parameter of all four systems by themselvesare relatively small.
The summarized ?RWTH ap-proach?
system weight, though, is again on par withthe other systems.5 ConclusionThe four statistical machine translation systems ofKarlsruhe Institute of Technology, RWTH Aachenand LIMSI and the very structural approach of SYS-TRAN produce hypotheses with a huge variabilitycompared to the others.
Finally the RWTH Aachensystem combination combined all single system hy-potheses to one hypothesis with a higher BLEUcompared to each single system.
If the systemcombination implementation can handle enough sin-gle systems we would recommend to add all singlesystems to the system combination.
Although thesingle system of SYSTRAN has the lowest BLEUscores and the RWTH single systems are similar weachieved the best result in using all single systems.362newstest2008 newstest2009 newstest2010 descriptionBLEU TER BLEU TER BLEU TER22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN SoftwareTable 1: All systems for the WMT 2011 German?English translation task (truecase).
BLEU and TER results are inpercentage.
FA denotes systems with phrase training, +GW the use of LDC data for the language model.
sc denotessystem combination.system weightKarlsruhe Institute of Technology 0.350RWTH PBT (FA) rerank +GW 0.001RWTH PBT (FA) 0.046RWTH jane + GW BLEU opt 0.023RWTH jane + GW TER?BLEU opt 0.034Limsi-CNRS 0.219SYSTRAN Software 0.328Table 2: Optimized systems weights for each system of the best system combination result.AcknowledgmentsThis work was achieved as part of the QUAEROProgramme, funded by OSEO, French State agencyfor innovation.ReferencesF.
Casacuberta and E. Vidal.
2004.
Machine translationwith inferred stochastic finite-state transducers.
Com-putational Linguistics, 30(3):205?225.S.F.
Chen and J.T.
Goodman.
1998.
An empiricalstudy of smoothing techniques for language modeling.Technical Report TR-10-98, Computer Science Group,Harvard University.D.
Chiang.
2007.
Hierarchical Phrase-Based Transla-tion.
Computational Linguistics, 33(2):201?228.J.M.
Crego and J.B. Marin?o.
2007.
Improving statisticalMT by coupling reordering and decoding.
MachineTranslation, 20(3):199?215.D.
De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-vain, H. Meynard, and F. Yvon.
2008.
LIMSI?s statis-tical translation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.L.
Huang and D. Chiang.
2007.
Forest Rescoring: FasterDecoding with Integrated Language Models.
In Proc.Annual Meeting of the Association for ComputationalLinguistics, pages 144?151, Prague, Czech Republic,June.T.
Ward K. Papineni, S. Roukos and W. Zhu.
2002.
Bleu:363a method for automatic evaluation of machine transla-tion.
In ACL ?02: Proc.
of the 40th Annual Meetingon Association for Computational Linguistics, pages311?318.
Association for Computational Linguistics.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proceedings of the In-ternational Conference on Acoustics, Speech, and Sig-nal Processing, ICASSP?95, pages 181?184, Detroit,MI.P.
Koehn and K. Knight.
2003.
Empirical Methodsfor Compound Splitting.
In Proceedings of EuropeanChapter of the ACL (EACL 2009), pages 187?194.J.
Senellart L. Dugast and P. Koehn.
2007.
Statisticalpost-editing on systran?s rule-based translation system.In Proceedings of the Second Workshop on Statisti-cal Machine Translation, StatMT ?07, pages 220?223,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.E.
Matusov, N. Ueffing, and H. Ney.
2006.
ComputingConsensus Translation from Multiple Machine Trans-lation Systems Using Enhanced Hypotheses Align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40.E.
Matusov, G. Leusch, R.E.
Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,J.B.
Mari no, M. Paulik, S. Roukos, H. Schwenk, andH.
Ney.
2008.
System Combination for MachineTranslation of Spoken and Written Language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237.J.A.
Nelder and R. Mead.
1965.
The Downhill SimplexMethod.
Computer Journal, 7:308.J.
Niehues and M. Kolss.
2009.
A POS-Based Model forLong-Range Reorderings in SMT.
In Fourth Work-shop on Statistical Machine Translation (WMT 2009),Athens, Greece.F.J.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum Error Rate Training for Statis-tical Machine Translation.
In Proc.
Annual Meeting ofthe Association for Computational Linguistics, pages160?167, Sapporo, Japan, July.A.
Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin, andE.
Herbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proceedings of the45th Annual Meeting of the ACL on Interactive Posterand Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.K.
Rottmann and S. Vogel.
2007.
Word Reordering inStatistical Machine Translation with a POS-Based Dis-tortion Model.
In TMI, Sko?vde, Sweden.H.
Schmid.
1994.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
In International Conferenceon NewMethods in Language Processing, Manchester,UK.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Spoken LanguageProcessing, volume 2, pages 901?904, Denver, Col-orado, USA, September.C.
Tillmann.
2004.
A unigram orientation model for sta-tistical machine translation.
In Proceedings of HLT-NAACL 2004, pages 101?104.
Association for Com-putational Linguistics.A.
Venugopal, A. Zollman, and A. Waibel.
2005.
Train-ing and Evaluation Error Minimization Rules for Sta-tistical Machine Translation.
In Workshop on Data-drive Machine Translation and Beyond (WPT-05), AnnArbor, MI.D.
Vilar, S. Stein, M. Huck, and H. Ney.
2010.
Jane:Open Source Hierarchical Translation, Extended withReordering and Lexicon Models.
In ACL 2010 JointFifth Workshop on Statistical Machine Translation andMetrics MATR, pages 262?270, Uppsala, Sweden,July.S.
Vogel.
2003.
SMT Decoder Dissected: Word Re-ordering.
In Int.
Conf.
on Natural Language Process-ing and Knowledge Engineering, Beijing, China.J.
Wuebker, A. Mauser, and H. Ney.
2010.
TrainingPhrase Translation Models with Leaving-One-Out.
InProceedings of the 48th Annual Meeting of the Assoc.for Computational Linguistics, pages 475?484, Upp-sala, Sweden, July.R.
Zens and H. Ney.
2006.
N-gram Posterior Proba-bilities for Statistical Machine Translation.
In HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics AnnualMeeting (HLT-NAACL), Workshop on Statistical Ma-chine Translation, pages 72?77, New York City, June.R.
Zens and H. Ney.
2008.
Improvements in DynamicProgramming Beam Search for Phrase-based Statisti-cal Machine Translation.
In Proc.
of the Int.
Workshopon Spoken Language Translation (IWSLT), Honolulu,Hawaii, October.364
