Topic Detection Based on Dialogue HistoryTakayuki NAKATA, Takahiro IKEDA, Shinichi ANDO, Akitoshi OKUMURAMultimedia Research Laboratories, NEC Corporation4-1-1, Miyazaki, Miyamae-ku, Kawasaki, KANAGAWA, 216-8555, JAPANt-nakata@bk.jp.nec.com, t-ikeda@di.jp.nec.co.jp, s-ando@cw.jp.nec.com, a-okumura@bx.jp.nec.comAbstractIn this paper, we propose a topic detectionmethod using a dialogue history forselecting a scene in the automaticinterpretation system (Ikeda et al, 2002).The method uses a k-nearest neighbormethod for the algorithm, automaticallyclusters target topics into smaller topicsgrouped by similarity, and incorporatesdialogue history weighted in terms of timeto detect and track topics on spokenphrases.
From the evaluation ofdetection performance using test corpuscomprised of realistic spoken dialogue,the method has shown to perform betterwith clustering incorporated, andcombined with time-weighted dialoguehistory of three sentences, gives detectionaccuracy of 77.0%.1 IntroductionIn recent years, speech-to-speech translationsystems have been developed that integrate threecomponents: speech recognition, machinetranslation, and speech synthesis (Watanabe etal., 2000).
However, these systems cannotguarantee accurate translation because theindividual components do not always providecorrect results.
To overcome this restriction,we proposed a method to use parallel text basedtranslation for supporting free-style sentencetranslation.
In addition, we built a prototypeautomatic interpretation system for Japaneseoverseas travelers (Ikeda et al, 2002).
Withthis system, the user searches for an appropriatesentence in source language from the registeredparallel text by using the criteria of an utterance,a scene, and a situation, and then uses the targetlanguage sentence for a translation.Although parallel text based translationprovides guaranteed translation results, it hastwo problems as the user searches for thesentence.
One is difficulty in searching anappropriate sentence from user?s short utterance,which is often heard in travel conversation.Short phrases provide only a few keywords andmake the search result too broad.
Specifyingthe exact scene and action helps narrow downthe result, but the task may cause user frustrationin having to select the right option from the vastcategories of scenes and actions.The other problem is existence of nonadaptivesentences that may be inappropriate in some ofthe scenes.
Users usually select sentencesaccording to the scenes so they can excludethose inapplicable sentences, but some newusers may accidentally select those nonadaptivesentences by failing to specify a scene.Here, we propose a method to detect a topicfor each utterance.
We define a topic ascorresponding to a scene that is a place or asituation in which the user converses.
Theproposed method is based on the k-nearestneighbor method, which is improved fordialogue utterances by clustering training dataand using dialogue history.
We use thedetected topic for specifying a scene condition inparallel text based translation, and thereby solvethe two problems described above.Detecting topics also helps improve accuracyof the automatic interpretation system bydisambiguating polysemy.
Some words shouldbe translated into different words according tothe scene and context selection.
Topicdetection can enhance speech recognitionaccuracy by selecting the correct wordAssociation for Computational Linguistics.Algorithms and Systems, Philadelphia, July 2002, pp.
9-14.Proceedings of the Workshop on Speech-to-Speech Translation:dictionary and resources, which are organizedaccording to the topic.The remainder of this paper is organized asfollows.
Section 2 describes the constraints indetecting a topic from dialogue utterances.Section 3 describes our topic detection algorithmto overcome these constraints.
Section 4explains the evaluation of our method by using atravel conversation corpus and Section 5presents the evaluation result.
Section 6discusses the effect of our method from acomparison of the results on typical dialoguedata and on real situation dialogue data.
Weconclude in Section 7 with some final remarksand mention of future work.2 Topic detectionAmong conventional topic detection methods,one uses compound words that features certaintopic as trigger information for detecting a topic(Hatori et al, 2000), and another usesdomain-dependant dictionaries and thesaurusesto construct knowledge applicable to a certaintopic (Tsunoda et al, 1996).
In the formermethod, a scene-dependant dictionary providesthe knowledge relevant to the scene andcompound words in the dictionary are used fordetecting a topic.
In the latter method, wordsappearing in a scene are defined as theknowledge relevant to the scene andsuperordinate/subordinate relation andsynonyms provided by thesauruses are used toenhance the robustness.These conventional methods are suitable forwritten texts but not for dialogue utterances in aspeech translation system.
The following twomajor constraints make the topic detection fordialogue utterances more difficult.
(1) Constraint due to single sentence process- Sentences in a dialogue are usuallyshort with few keywords.- In a dialogue, the frequency values ofthe word in a sentence are mostly one,making it difficult to apply a statisticalmethod.
(2) Constraint due to the nature of spokendialogue- In a dialogue, one topic is sometimesexpressed with two or more sentences.- The words appearing in a sentence aresometimes replaced by anaphora oromitted by ellipsis in the next sentence.- Topics frequently change in a dialogue.On the other hand, a speech translation systemrequires the following:- Topic detection for each utterance in adialogue;- Prompt topic detection in real timeprocessing;- Dynamic tracking of topic transition.To make topic detection adaptive to thespeech translation system, we propose a methodapplicable to one utterance in a dialogue as aninput, which can be used for tracking the topictransitions dynamically and outputting mostappropriate topic for the latest utterance.
Thek-nearest neighbor method (Yang, 1994) is usedwith the clustering method linked with thedialogue history as a topic detection algorithmfor dialogue utterance.
The k-nearest neighbormethod is known to have high precisionperformance with less restriction in the field ofdocument categorization.
This method isfrequently used as a baseline in the field and alsoapplied to topic detection for story but not for asingle sentence (Yang et al, 1999).
This paperincorporates two new methods to the k-nearestneighbor method to overcome two constraintsmentioned above.To overcome the first constraint, we cluster aset of sentences in training data into subsets(called subtopics) based on similarity betweenthe sentences.
A topic is detected bycalculating the relevance between the inputsentence and these subtopics.
Clusteringsentences on the same subtopic increasesnumber of characteristic words to be comparedwith input sentence in calculation.To overcome the second constraint, we groupan input sentence with other sentences in thedialogue history.
A topic is detected bycalculating the relevance between this group andeach possible topic.
Grouping the inputsentence with the preceding sentences increasesnumber of characteristic words to be comparedwith topics in calculation.
We consider theorder of the sentences in the dialogue incalculating the relevance to avoid the influenceof topic change in the dialogue.3 Topic detection algorithmThis section explains three methods used inthe proposed topic detection algorithm: 1)k-nearest neighbor method, 2) the clusteringmethod using TF-IDF, and 3) the application ofthe dialogue history.3.1 k-nearest neighbor methodWe denote the character vector for a givensentence in the training data as Dj, and that for agiven input sentence as X.
Each vector has aTF-IDF value of the word in the sentence as itselement value (Salton 1989).The similarity between the input sentence Xand the training data Dj is calculated by takingthe inner product of the character vectors.The conditional probability of topic Cl beingrelated to the training data Dj is calculated as:?
?jljl DCDCPrthe torelated beingtopicsofnumber  The1)|( =The relevance score between the input sentenceX and each topic Cl is calculated as the sum ofsimilarity for k sentences taken from the trainingdata in descending order of similarity.??
?=}sentence  ranking  k  top{)|(),()|(jDjljl DCPrDXSimXCRel3.2 Topics clustering methodThis method clusters topics into smallersubtopics.
The word ?topic?
used in thismethod consists of several subtopicsrepresenting detailed situations.
The topic?Hotel?
consists of subtopics such as ?CheckingIn?
and ?Room Service?.
Sentences in trainingdata categorized under the same topic are furthergrouped into subtopics based on their similarity.Calculating the relevance between the test datainput and these subsets of training data providesmore keywords in detecting topics.
Ourmethod to create the subtopics identifies akeyword in a sentence set, and then recursivelydivides the set into two smaller subsets, one thatincludes the keyword and one that does not.TF-IDF Clustering Method(1) Find the word that has the highest TF-IDFvalue among the words in the sentenceset;(2) Divide the sentence set into two subsets;one that contains the word obtained instep (1) and one that does not;(3) Repeat steps (1) and (2) recursively untilTF-IDF value reaches the threshold.Subtopics created by this method consist ofkeywords featuring each subtopic and theirrelated words.3.3 Application of the dialogue historyThe proposed method applies the dialoghistory in topic detection.
The methodinterprets a current input sentence and thesentences prior to the current input as a dialoguehistory subset, and detects topics by calculatingthe relevance score between the dialogue historysubset and the each topic.
The methodincreases number of keywords in the input forcalculation.
We assign a weight to eachsentence in the dialogue history subset to controlthe effect of time-sequence in sentences.The relevance score combined with the dialoghistory is calculated as:)Xr|C(lRer...)Xr|C(lRer)X|C(lRe)Xr,...,Xr,X|C(lRenlnllnl??
?+++=111Here the similarity is calculated with the inputsentence X and the sentence in the dialog historysubset Xri, taking ?
and ?ri as the weights for theinput sentences and the sentences in the dialoguehistory, respectively.4 EvaluationTo evaluate the proposed method, weprepared training data and test data from a travelconversation corpus.
We also prepared three22 || || || ||) , (ji ij ij D Xd xD X Sim?
?=?types of clusters with different thresholds andtwo types of dialogue history with differentweight values.4.1 Training dataIn the evaluation, we used approximately25,000 sentences from our original travelconversation corpus as our training data.
Thesentences are manually classified into fourtopics: 1) Hotel, 2) Restaurant, 3) Shopping, and4) Others.
The topic ?Others?
consists ofsentences not categorized into the remainingthree.
Topics such as ?Transportation?
or?Illnesses and injuries?
are placed into this?Others?
in this evaluation.4.2 Test dataWe prepared two sets of test data.
One setconsists of 62 typical travel dialoguescomprising 896 sentences (hereafter called?typical dialogue data?).
The other set consistsof 45 dialogues comprising 498 sentences,which may include irregular expressions butclosely representing daily spoken language(hereafter called ?real situation dialogue data?
).Sentences in ?typical dialogue data?
are oftenheard in travel planning and travelling situations,and form a variety of initiating dialogues as thetravel conversation unfolds.
The data includeswords and phrases often used in the topics listedabove, and each sentence is short with littleredundancy.
On the other hand, ?real situationdialogue data?
consists of spoken dialoguephrases which are likely to appear inuser-specific situations in the travel domain.Some phrases may be typically used, whileothers may consist of colloquial expressions andwords and phrases that are redundant.
Some ofthe words may not appear in the training data.4.3 Clustering the topicsWe applied the clustering with theaforementioned method to 8,457 sentences fromtraining data which are categorized into one ormore of the three topics: 1) Hotel, 2) Restaurant,and 3) Shopping.
Clusters are created on threedifferent thresholds: 8,409 clusters (small-sizedcluster), 3,845 clusters (medium-sized cluster)and 2,203 clusters (large-sized cluster).
Incarrying out clustering, we set one sentence asone cluster if the sentence does not contain aword whose TF-IDF value is not equal to orgreater than the threshold.
We excluded datathat falls only under the topic ?Others?
and datathat falls under all four topics, which areconsidered to be general conversation.Variations of these topics produce 13 probablecombinations.The number of clusters is smallest (13) whenwe set one topic as one cluster and largest(8,457) when we set one sentence as one cluster.4.4 Use of the dialogue historyTo evaluate the effect of the dialogue history,we use an input sentence, the most precedingand the next preceding sentence (hereafter?sentence 0?, ?sentence -1?, and ?sentence -2?
)as a dialogue history.
Two types of sentenceweights are applied to these three sentences, oneof equal weights and one of weights based on atime series.
These sets are:0.33) 0.33, (0.33,2)- sentence 1,- sentence 0, (sentence=0.2) 0.3, (0.5,2)- sentence 1,- sentence 0, (sentence=5 ResultsWe performed the detection test described in4.3 on 13 types of topic combinations usingtypical dialogue data and real situation dialoguedata.5.1 Test results on typical dialogue dataFigure 1 shows the results of topic detectionon typical dialogue data for a varying number ofclusters.
The figure shows that the accuracy ishighest when one sentence is set as one cluster(one sentence per cluster) in each topic, andlowest when one whole topic is set as onecluster.5.2 Test result on real situation dialoguedataFigure 2 shows the results of topic detectionon real situation dialogue data for a varyingnumber of clusters.
The figure shows that theaccuracy of the medium cluster is slightly betterthan that for one sentence per cluster.
Thisindicates that sentences grouped in terms ofsimilarity heighten the accuracy of similaritycalculation between input sentences and thetraining data.5.3 Results of dialogue historyapplicationWe evaluated the effect of the dialoguehistory for typical dialogue test data, andcompared the case of one sentence per clusterwith the case of medium cluster.
Using onlythe input sentence, the topic detection accuracywas 59.2% for the former and 56.0% for thelatter.
Using three sentences from the dialoguehistory, the respective figures were 72.0% and70.0% with equal weights, 76.7% and 77.0%with time series weights.6 DiscussionLooking at the results on the typical dialoguedata, it can be argued that theone-sentence-per-cluster case shows the highestaccuracy because the data is a typical dialogueand each sentence is short, so that feature wordsin the input sentences and those of the learningdata are likely to match.
On the other hand, itcan be argued that the one-topic-per-cluster caseshows the lowest accuracy because featurewords become less effective when so manysubtopics are in one cluster.For example, let us look at the sentence in thelearning data, ?Is it all right to pick it up withmy hand??
This sentence can be used whendeciding what to buy, and so is categorizedunder the topic ?Shopping?.
When a cluster isone sentence, the result will likely besatisfactory if you input the sentence, ?Is it allright to pick it up with my hand??
because theinput sentence is similar to the cluster.However, when a cluster is one topic, thissentence might be categorized under the topic?Others?, along with sentences used to expressphysical conditions such as ?My hand hurts?
or?I am all right?.
Therefore, it can be concludedthat it is better to divide a large topic intosmaller groups or even into single sentences.Looking at the results on real situationdialogue data, we find the ratio of correctanswers is almost the same for theone-sentence-per-cluster and the medium-clustercases, but the actual sentences correctly detectedtopics differed significantly between them.
Inthe former case, topics are identified correctlywhen there are strong feature words, while in thelatter case, it works well when there is no strongfeature word but the topics can be determined bysets of words.
From this fact, we can concludethat typical input sentences can be comparedeasily with the one-sentence-per-cluster case,and real situation input sentences can beFigure 2: The result on real situation test dataFigure 1: The result on typical test data0102030405060one topicper clusterlargeclustermedium clustersmallclusterone sentence per clusternumber of clusteraccuracyrate424446485052545658one topicper clusterlargeclustermedium clustersmallclusterone sentence per clusternumber of clusteraccuracyratecompared with the medium-cluster case eventhough the sentences are different from those intypical dialogue in terms of content andexpressions.
We find that with typical dialoguedata, the accuracy level is almost the same forthe one-sentence-per-cluster and themedium-cluster cases, but with the real situationdialogue data, the accuracy level is slightlyimproved.
Therefore, it might be possible toimprove the practicality of topic detection bycollecting a large amount of data, dividing thedata into typical and real situation dialogues, andsetting the appropriate clusters to each type.7 ConclusionsIn this paper, we proposed a topic detectionmethod using a dialogue history to select a scenefor the automatic interpretation system.
Weinvestigated its limitation in dialogue utterancesand provided solutions by clustering trainingdata and utilizing dialogue history.
Ourmethod showed topic detection accuracy of atleast 50% for both typical and real situationdialogues in 13 topic combinations.
For typicaldialogues, we found that the best results wereobtained when one sentence is used for onecluster, and for real situation dialogues, wefound slightly better results were obtained whenclustering was introduced.
Therefore, it can beargued that the topic detection accuracy isimproved for both typical and real situationsentences if an appropriate size cluster isintroduced.We plan to use our topic detection techniquefor specifying a scene condition of parallel textbased translation in our automatic interpretationsystem.
Detecting topics also helps improveaccuracy of the automatic interpretation systemby disambiguating polysemy.
Topic detectioncan enhance speech recognition accuracy byselecting the correct word dictionary andresources, which are organized according to thetopic.Our method is also applicable in determiningtime series behavior such as topic transition.Our future studies will focus on linking thedialogue history and clustering more closely toimprove the topic detection accuracy.ReferencesH.
Hatori, Y. Kamiyama (2000) Web translationby feeding back information for judgingcategory, Information Processing Society ofJapan 63rd.
Annual Meeting, Vol.
2, pp.253-254.T.
Ikeda, S. Ando, K. Satoh, A. Okumura, T.Watanabe (2002) Automatic InterpretationSystem Integrating Free-style  SentenceTranslation and Parallel Text Based Translation,ACL-02 Workshop on Speech-to-speechTranslation (to appear).G.
Salton (1989) The vector space model,automatic text processing ?
thetransformation, analysis, and retrieval ofinformation by computer, Addison-WesleyPublishing Company Inc., pp.312-325.T.
Tsunoda and H. Tanaka (1996) Evaluation ofScene Information as Context for English NounDisambiguation, Natural Language Processing,Vol.3 No.1, pp.
3-27.T.
Watanabe, A. Okumura, S. Sakai, K.Yamabana, S. Doi, K. Hanazawa (2000) AnAutomatic Interpretation System for TravelConversation, The Proceeding of the 6thInternational Conference on Spoken LanguageProcessing Vol.
4, pp.
444-447.Y.
Yang (1994) Expert Network, Effective andEfficient Learning from Human Decisions inText Categorization and Retrieval, Proceedingsof the 17th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR?94) 1994:11-21.Y.
Yang, J.G.
Carbonell, R. Brown, T. Pierce, B.T.
Archibald, and X. Liu (1999) Learningapproaches for detecting and tracking newsevents, IEEE Intelligent Systems, 14(4), pp.32-43.
