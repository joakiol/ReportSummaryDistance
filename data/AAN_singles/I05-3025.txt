A Maximum Entropy Approach to Chinese Word SegmentationJin Kiat Low 1 and Hwee Tou Ng 1,2 and Wenyuan Guo 21.
Department of Computer Science, National University of Singapore,3 Science Drive 2, Singapore 1175432.
Singapore-MIT Alliance, E4-04-10, 4 Engineering Drive 3, Singapore 117576{lowjinki, nght, guowy}@comp.nus.edu.sgAbstractWe participated in the Second Inter-national Chinese Word SegmentationBakeoff.
Specifically, we evaluatedour Chinese word segmenter in theopen track, on all four corpora, namelyAcademia Sinica (AS), City Universityof Hong Kong (CITYU), Microsoft Re-search (MSR), and Peking University(PKU).
Based on a maximum entropyapproach, our word segmenter achievedthe highest F measure for AS, CITYU,and PKU, and the second highest forMSR.
We found that the use of an ex-ternal dictionary and additional trainingcorpora of different segmentation stan-dards helped to further improve seg-mentation accuracy.1 Chinese Word SegmenterThe Chinese word segmenter we built is similarto the maximum entropy word segmenter we em-ployed in our previous work (Ng and Low, 2004).Our word segmenter uses a maximum entropyframework (Ratnaparkhi, 1998; Xue and Shen,2003) and is trained on manually segmented sen-tences.
It classifies each Chinese character giventhe features derived from its surrounding context.Each Chinese character can be assigned one offour possible boundary tags: s for a character thatoccurs as a single-character word, b for a charac-ter that begins a multi-character (i.e., two or morecharacters) word, e for a character that ends amulti-character word, and m for a character that isneither the first nor last in a multi-character word.Our implementation used the opennlp maximumentropy package v2.1.0 from sourceforge.11.1 Basic FeaturesThe basic features of our word segmenter aresimilar to our previous work (Ng and Low, 2004):(a) Cn(n = ?2,?1, 0, 1, 2)(b) CnCn+1(n = ?2,?1, 0, 1)(c) C?1C1(d) Pu(C0)(e) T (C?2)T (C?1)T (C0)T (C1)T (C2)In the above feature templates, C refers to aChinese character.
Templates (a) ?
(c) refer to acontext of five characters (the current characterand two characters to its left and right).
C0denotes the current character, Cn(C?n ) denotesthe character n positions to the right (left) of thecurrent character.
For example, given the charac-ter sequence ?c????
?, when consideringthe character C0 ??
?, C?2 denotes ?c?, C1C2denotes ???
?, etc.
The punctuation feature,Pu(C0), checks whether C0 is a punctuationsymbol (such as ??
?, ??
?, ?,?).
For the type fea-ture (e), four type classes are defined: numbersrepresent class 1, dates (??
?, ??
?, ?#?, theChinese characters for ?day?, ?month?, ?year?,respectively) represent class 2, English lettersrepresent class 3, and other characters representclass 4.
For example, when considering thecharacter ?#?
in the character sequence ?
?#SR?, the feature T (C?2) .
.
.
T (C2) = 112431http://maxent.sourceforge.net/161will be set to 1 (???
is the Chinese character for?9?
and ??
is the Chinese character for ?0?
).Besides these basic features, we also made useof character normalization.
We note that char-acters like punctuation symbols and Arabic dig-its have different character codes in the ASCII,GB, and BIG5 encoding standard, although theymean the same thing.
For example, comma ?,?is represented as the hexadecimal value 0x2c inASCII, but as the hexadecimal value 0xa3ac inGB.
In our segmenter, these different charactercodes are normalized and replaced by the corre-sponding character code in ASCII.
Also, all Ara-bic digits are replaced by the ASCII digit ?0?
todenote any digit.
Incorporating character normal-ization enables our segmenter to be more robustagainst the use of different encodings to representthe same character.For all the experiments that we conducted,training was done with a feature cutoff of 2 and100 iterations, except for the AS corpus whichhad a feature cutoff of 3.A major difficulty faced by a Chinese wordsegmenter is the presence of out-of-vocabulary(OOV) words.
Segmenting a text with many OOVwords tends to result in lower accuracy.
We ad-dress the problem of OOV words in two ways:using an external dictionary containing a list ofpredefined words, and using additional trainingcorpora which are not segmented according to thesame segmentation standard.1.2 External DictionaryIf a sequence of characters in a sentence matchesa word in an existing dictionary, it may be aclue that the sequence of characters should besegmented as one word.
We used an onlinedictionary from Peking University downloadablefrom the Internet2, consisting of about 108,000words of length one to four characters.
If there issome sequence of neighboring characters aroundC0 in the sentence that matches a word in thisdictionary, then we greedily choose the longestsuch matching word W in the dictionary.
Let t0be the boundary tag of C0 in W , L the number ofcharacters in W , and C1(C?1) be the character2http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Information%20Processing/Source Code/Chapter 8/Lexicon full 2000.zipimmediately following (preceding) C0 in thesentence.
We then add the following featuresderived from the dictionary:(f) Lt0(g) Cnt0(n = ?1, 0, 1)For example, consider the sentence ?c????.
.
.
?.
When processing the current characterC0 ??
?, we will attempt to match the followingcandidate sequences ??
?, ?c?
?, ???
?, ?c??
?, ????
?, ?c???
?, and ?????
?against existing words in our dictionary.
Supposeboth ????
and ?c???
are found in thedictionary.
Then the longest matching word Wchosen is ?c??
?, t0 is m, L is 3, C?1 is ?c?,and C1 is ??
?.1.3 Additional Training CorporaThe presence of different standards in Chineseword segmentation limits the amount of trainingcorpora available for the community, due to dif-ferent organizations preparing training corpora intheir own standards.
Indeed, if one uniform seg-mentation standard were adopted, more trainingdata would have been available, and the OOVproblem could be significantly reduced.We observed that although different segmenta-tion standards exist, the differences are limited,and many words are still segmented in the sameway across two different segmentation standards.As such, in our work, we attempt to incorporatecorpora from other segmentation standards as ad-ditional training data, to help reduce the OOVproblem.Specifically, the steps taken are:1.
Perform training with maximum entropymodeling using the original training corpusD0 annotated in a given segmentation stan-dard.2.
Use the trained word segmenter to segmentanother corpus D annotated in a differentsegmentation standard.3.
Suppose a Chinese character C in D is as-signed a boundary tag t by the word seg-menter with probability p. If t is identical tothe boundary tag of C in the gold-standard162annotated corpus D, and p is less than somethreshold ?, then C (with its surroundingcontext in D) is used as additional trainingdata.4.
Add all such characters C as additional train-ing data to the original training corpus D0,and train a new word segmenter using the en-larged training data.5.
Evaluate the accuracy of the new word seg-menter on the same test data annotated in theoriginal segmentation standard of D0.For the current bakeoff, when training a wordsegmenter on a particular training corpus, the ad-ditional training corpora are all the three corporain the other segmentation standards.
For example,when training a word segmenter for the AS cor-pus, the additional training corpora are CITYU,MSR, and PKU.
The necessary character encod-ing conversion between GB and BIG5 is per-formed, and the probability threshold ?
is set to0.8.
We found from our experiments that setting?
to a higher value did not further improve seg-mentation accuracy, but would instead increasethe training set size and incur longer training time.2 TestingDuring testing, the probability of a boundary tagsequence assignment t1 .
.
.
tn given a charactersequence C1 .
.
.
Cn is determined by using themaximum entropy classifier to compute the prob-ability that a boundary tag ti is assigned to eachindividual character Ci.
If we were to just as-sign each character the boundary tag with thehighest probability, it is possible that the clas-sifier produces a sequence of invalid tags (e.g.,m followed by s).
To eliminate such possibil-ities, we implemented a dynamic programmingalgorithm which considers only valid boundarytag sequences given an input character sequence.At each character position i, the algorithm con-siders each last word candidate ending at posi-tion i and consisting of K characters in length(K = 1, .
.
.
, 20 in our experiments).
To deter-mine the boundary tag assignment to the last wordW with K characters, the first character of W isassigned boundary tag b, the last character of Wis assigned tag e, and the intervening charactersCorpus R P F ROOV RIVAS 0.962 0.950 0.956 0.684 0.975CITYU 0.967 0.956 0.962 0.806 0.980MSR 0.969 0.968 0.968 0.736 0.975PKU 0.968 0.969 0.969 0.838 0.976Table 1: Our official SIGHAN bakeoff resultsare assigned tag m. (If W is a single-characterword, then the single character is assigned tag s).In this way, the dynamic programming algorithmonly considers valid tag sequences.After word segmentation is done by the maxi-mum entropy classifier, a post-processing step isapplied to correct inconsistently segmented wordsmade up of 3 or more characters.
A word W isdefined to be inconsistently segmented if the con-catenation of 2 to 6 consecutive words elsewherein the segmented output document matches W .
Inthe post-processing step, the segmentation of thecharacters of these consecutive words is changedso that they are segmented as a single word.
Toillustrate, if the concatenation of 2 consecutivewords ?&??
and ??n?
in the segmented out-put document matches another word ?&?
?n?,then the 2 consecutive words ?&??
and ?
?n?will be re-segmented as a single word ?&?
?n?.3 Evaluation ResultsWe evaluated our Chinese word segmenter inthe open track, on all 4 corpora, namelyAcademia Sinica (AS), City University of HongKong (CITYU), Microsoft Research (MSR), andPeking University (PKU).
Table 1 shows our of-ficial SIGHAN bakeoff results.
The columns R,P, and F show the recall, precision, and F mea-sure, respectively.
The columns ROOV and RIVshow the recall on out-of-vocabulary words andin-vocabulary words, respectively.
Our Chineseword segmenter which participated in the bakeoffwas trained with the basic features (Section 1.1),and made use of the external dictionary (Sec-tion 1.2) and additional training corpora (Sec-tion 1.3).
Our word segmenter achieved the high-est F measure for AS, CITYU, and PKU, and thesecond highest for MSR.After the release of the official bakeoff results,163Corpus V1 V2 V3 V4AS 0.953 0.955 0.956 0.956CITYU 0.950 0.960 0.961 0.962MSR 0.960 0.968 0.963 0.968PKU 0.948 0.965 0.956 0.969Table 2: Word segmentation accuracy (F mea-sure) of different versions of our word segmenterwe ran a series of experiments to determine thecontribution of each component of our word seg-menter, using the official scorer and test sets withgold-standard segmentations.
Version V1 usedonly the basic features (Section 1.1); Version V2used the basic features and additional features de-rived from our external dictionary (Section 1.2);Version V3 used the basic features but with ad-ditional training corpora (Section 1.3); and Ver-sion V4 is our official submitted version combin-ing basic features, external dictionary, and addi-tional training corpora.
Table 2 shows the wordsegmentation accuracy (F measure) of the differ-ent versions of our word segmenter, when testedon the official test sets of the four corpora.
Theresults indicate that the use of external dictionaryincreases segmentation accuracy.
Similarly, theuse of additional training corpora of different seg-mentation standards also increases segmentationaccuracy.4 ConclusionUsing a maximum entropy approach, our Chi-nese word segmenter achieves state-of-the-art ac-curacy, when evaluated on all four corpora in theopen track of the Second International ChineseWord Segmentation Bakeoff.
The use of an exter-nal dictionary and additional training corpora ofdifferent segmentation standards helps to furtherimprove segmentation accuracy.AcknowledgementsThis research is partially supported by a researchgrant R252-000-125-112 from National Univer-sity of Singapore Academic Research Fund, aswell as the Singapore-MIT Alliance.ReferencesHwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2004), pages277?284.Adwait Ratnaparkhi.
1998.
Maximum Entropy Mod-els for Natural Language Ambiguity Resolution.Ph.D.
thesis, University of Pennsylvania.Nianwen Xue and Libin Shen.
2003.
Chinese wordsegmentation as LMR tagging.
In Proceedings ofthe Second SIGHAN Workshop on Chinese Lan-guage Processing, pages 176?179.164
