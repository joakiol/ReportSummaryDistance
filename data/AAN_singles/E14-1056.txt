Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530?539,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMachine Reading Tea Leaves: Automatically Evaluating Topic Coherenceand Topic Model QualityJey Han LauDept of PhilosophyKing?s College Londonjeyhan.lau@gmail.comDavid NewmanGooglednewman@google.comTimothy BaldwinDept of Computing andInformation SystemsThe University of Melbournetb@ldwin.netAbstractTopic models based on latent Dirichlet al-location and related methods are used in arange of user-focused tasks including doc-ument navigation and trend analysis, butevaluation of the intrinsic quality of thetopic model and topics remains an openresearch area.
In this work, we explorethe two tasks of automatic evaluation ofsingle topics and automatic evaluation ofwhole topic models, and provide recom-mendations on the best strategy for per-forming the two tasks, in addition to pro-viding an open-source toolkit for topic andtopic model evaluation.1 IntroductionTopic modelling based on Latent Dirichlet Alloca-tion (LDA: Blei et al.
(2003)) and related methodsis increasingly being used in user-focused tasks, incontexts such as the evaluation of scientific impact(McCallum et al., 2006; Hall et al., 2008), trendanalysis (Bolelli et al., 2009; Lau et al., 2012a)and document search (Wang et al., 2007).
TheLDA model is based on the assumption that doc-ument collections have latent topics, in the formof a multinomial distribution of words, which istypically presented to users via its top-N highest-probability words.
In NLP, topic models are gener-ally used as a means of preprocessing a documentcollection, and the topics and per-document topicallocations are fed into downstream applicationssuch as document summarisation (Haghighi andVanderwende, 2009), novel word sense detectionmethods (Lau et al., 2012b) and machine transla-tion (Zhao and Xing, 2007).
In fields such as thedigital humanities, on the other hand, human usersinteract directly with the output of topic models.
Itis this context of topic modelling for direct humanconsumption that we target in this paper.The topics produced by topic models have avarying degree of human-interpretability.
To il-lustrate this, we present two topics automaticallylearnt from a collection of news articles:1.
?farmers, farm, food, rice, agriculture?2.
?stories, undated, receive, scheduled, clients?The first topic is clearly related to agriculture.The subject of the second topic, however, is lessclear, and may confuse users if presented to themas part of a larger topic model.
Measuring thehuman-interpretability of topics and the overalltopic model is the core topic of this paper.Various methodologies have been proposed formeasuring the semantic interpretability of topics.In Chang et al.
(2009), the authors proposed anindirect approach based on word intrusion, where?intruder words?
are randomly injected into topicsand human users are asked to identify the intruderwords.
The word intrusion task builds on the as-sumption that the intruder words are more iden-tifiable in coherent topics than in incoherent top-ics, and thus the interpretability of a topic can beestimated by measuring how readily the intruderwords can be manually identified by annotators.Since its inception, the method of Chang etal.
(2009) has been used variously as a meansof assessing topic models (Paul and Girju, 2010;Reisinger et al., 2010; Hall et al., 2012).
Despiteits wide acceptance, the method relies on manualannotation and has never been automated.
This isone of the primary contributions of this work: thedemonstration that we can automate the method ofChang et al.
(2009) at near-human levels of accu-racy, as a result of which we can perform auto-matic evaluation of the human-interpretability oftopics, as well as topic models.There has been prior work to directly estimatethe human-interpretability of topics through au-tomatic means.
For example, Newman et al.530(2010) introduced the notion of topic ?coher-ence?, and proposed an automatic method for es-timating topic coherence based on pairwise point-wise mutual information (PMI) between the topicwords.
Mimno et al.
(2011) similarly introduceda methodology for computing coherence, replac-ing PMI with log conditional probability.
Musatet al.
(2011) incorporated the WordNet hierarchyto capture the relevance of topics, and in Aletrasand Stevenson (2013a), the authors proposed theuse of distributional similarity for computing thepairwise association of the topic words.
One ap-plication of these methods has been to remove in-coherent topics before generating labels for topics(Lau et al., 2011; Aletras and Stevenson, 2013b).Ultimately, all these methodologies, and alsothe word intrusion approach, attempt to assess thesame quality: the human-interpretability of top-ics.
The relationship between these methodolo-gies, however, is poorly understood, and there isno consensus on what is the best approach forcomputing the semantic interpretability of topicmodels.
This is a second contribution of this pa-per: we perform a systematic empirical compar-ison of the different methods and find apprecia-ble differences between them.
We further go on topropose an improved formulation of Newman etal.
(2010) based on normalised PMI.
Finally, werelease a toolkit which implements the topic inter-pretability measures described in this paper.2 Related WorkChang et al.
(2009) challenged the conventionalwisdom that held-out likelihood ?
often com-puted as the perplexity of test data or unseen doc-uments ?
is the only way to evaluate topic mod-els.
To measure the human-interpretability of top-ics, the authors proposed a word intrusion taskand conducted experiments using three topic mod-els: Latent Dirichlet Allocation (LDA: Blei et al.
(2003)), Probabilistic Latent Semantic Indexing(PLSI: Hofmann (1999)) and the Correlated TopicModel (CTM: Blei and Lafferty (2005)).
Contraryto expectation, they found that perplexity corre-lates negatively with topic interpretability.In the word intrusion task, each topic is pre-sented as a list of six words ?
the five most proba-ble topic words and a randomly-selected ?intruderword?, which has low probability in the topic ofinterest, but high probability in other topics ?and human users are asked to identify the intruderword that does not belong to the topic in question.Newman et al.
(2010) capture topic inter-pretability using a more direct approach, by askinghuman users to rate topics (represented by theirtop-10 topic words) on a 3-point scale based onhow coherent the topic words are (i.e.
their ob-served coherence).
They proposed several ways ofautomating the estimation of the observed coher-ence, and ultimately found that a simple methodbased on PMI term co-occurrence within a slidingcontext window over English Wikipedia producesthe consistently best result, nearing levels of inter-annotator agreement over topics learnt from twodistinct document collections.Mimno et al.
(2011) proposed a closely-relatedmethod for evaluating semantic coherence, replac-ing PMI with log conditional probability.
Ratherthan using Wikipedia for sampling the word co-occurrence counts, Mimno et al.
(2011) used thetopic-modelled documents, and found that theirmeasure correlates well with human judgementsof observed coherence (where topics were ratedin the same manner as Newman et al.
(2010),based on a 3-point ordinal scale).
To incorpo-rate the evaluation of semantic coherence into thetopic model, the authors proposed to record wordsthat co-occur together frequently, and update thecounts of all associated words before and after thesampling of a new topic assignment in the Gibbssampler.
This variant of topic model was shown toproduce more coherent topics than LDA based onthe log conditional probability coherence measure.Aletras and Stevenson (2013a) introduced dis-tributional semantic similarity methods for com-puting coherence, calculating the distributionalsimilarity between semantic vectors for the top-Ntopic words using a range of distributional similar-ity measures such as cosine similarity and the Dicecoefficient.
To construct the semantic vector spacefor the topic words, they used English Wikipediaas the reference corpus, and collected words thatco-occur in a window of ?5 words.
They showedthat their method correlates well with the observedcoherence rated by human judges.3 DatasetAs one of the primary foci of this paper is the au-tomation of the intruder word task of Chang etal.
(2009), our primary dataset is that used in theoriginal paper by Chang et al.
(2009), which pro-vides topics and human annotations for a range of531domains and topic model types.
In the dataset,two text collections were used: (1) 10,000 articlesfrom English Wikipedia (WIKI); and (2) 8,447 arti-cles from the New York Times dating from 1987 to2007 (NEWS).
For each document collection, top-ics were generated by three topic modelling meth-ods: LDA, PLSI and CTM (see Section 2).
Foreach topic model, three settings of T (the num-ber of topics) were used: T = 50, T = 100and T = 150.
In total, there were 9 topic mod-els (3 models ?
3 T ) and 900 topics (3 models ?
(50 + 100 + 150)) for each dataset.1For some of topic interpretability estimationmethods, we require a reference corpus to sam-ple lexical probabilities.
We use two referencecorpora: (1) NEWS-FULL, which contains 1.2 mil-lion New York Times articles from 1994 to 2004(from the English Gigaword); and (2) WIKI-FULL,which contains 3.3 million English Wikipedia ar-ticles (retrieved November 28th 2009).2The ratio-nale for choosing the New York Times and EnglishWikipedia as the reference corpora is to ensure do-main consistency with the word intrusion dataset;the full collections are used to more robustly esti-mate lexical probabilities.4 Human-Interpretability at the ModelLevelIn this section, we evaluate measures for estimat-ing human-interpretability at the model level.
Thatis, for a measure ?
human-judged or automated?
we first aggregate its coherence/interpretabilityscores for all topics from a given topic model toobtain the topic model?s average coherence score.We then calculate the Pearson correlation coeffi-cients between the two measures using the topicmodels?
average coherence scores.
In summary,the correlation is computed over nine sets of top-ics (3 topic modellers ?
3 settings of T ) for eachof WIKI and NEWS.4.1 Indirect Approach: Word IntrusionThe word intrusion task measures topic inter-pretability indirectly, by computing the fractionof annotators who successfully identify the in-truder word.
A limitation of the word intrusion1In the WIKI topics there were corrupted symbols in thetopic words for 24 topics.
We removed these topics, reducingthe total number of topics to 876.2For both corpora we perform tokenisation and POS tag-ging using OpenNLP and lemmatisation using Morpha (Min-nen et al., 2001).task is that it requires human annotations, there-fore preventing large-scale evaluation.
We beginby proposing a methodology to fully automate theword intrusion task.Lau et al.
(2010) proposed a methodology thatlearns the most representative or best topic wordthat summarises the semantics of the topic.
Ob-serving that the word intrusion task ?
the taskof detecting the least representative word ?
isthe converse of the best topic word selection task,we adapt their methodology to automatically iden-tify the intruder word for the word intrusion task,based on the knowledge that there is a unique in-truder word per topic.The methodology works as follows: given a setof topics (including intruder words), we computethe word association features for each of the top-N topic words of a topic,3and combine the fea-tures in a ranking support vector regression model(SVMrank: Joachims (2006)) to learn the intruderwords.
Following Lau et al.
(2010), we use threeword association measures:PMI(wi) =N?1?jlogP (wi, wj)P (wi)P (wj)CP1(wi) =N?1?jP (wi, wj)P (wj)CP2(wi) =N?1?jP (wi, wj)P (wi)We additionally experiment with normalisedpointwise mutual information (NPMI: Bouma(2009)):NPMI(wi) =N?1?jlogP (wi,wj)P (wi)P (wj)?
logP (wi, wj)In the dataset of Chang et al.
(2009) (see Sec-tion 3), each topic was presented to 8 annota-tors, with small variations in the displayed topicwords (including the intruder word) for each an-notator.
That is, each topic has essentially 8 subtlydifferent representations.
To measure topic inter-pretability, the authors defined ?model precision?
:the relative success of human annotators at identi-fying the intruder word, across all representationsof the different topics.
The model precision scoresproduced by human judges are henceforth referredto as WI-Human, and the scores produced by our3N is the number of topic words displayed to the humanusers in the word intrusion task, including the intruder word.532Topic Ref.
Pearson?s r with WI-HumanDomain Corpus WI-Auto-PMI WI-Auto-NPMIWIKIWIKI-FULL 0.947 0.936NEWS-FULL 0.801 0.835NEWSNEWS-FULL 0.913 0.831WIKI-FULL 0.811 0.750Table 1: Pearson correlation of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the model level.automated method for the PMI and NPMI vari-ants as WI-Auto-PMI and WI-Auto-NPMI respec-tively.4The Pearson correlation coefficients betweenWI-Human and WI-Auto-PMI/WI-Auto-NPMI atthe model level are presented in Table 1.
Notethat our two reference corpora are used to inde-pendently sample the lexical probabilities for theword association features.We see very strong correlation for in-domainpairings (i.e.
WIKI+WIKI-FULL and NEWS+NEWS-FULL), achieving r > 0.9 in most cases for bothWI-Auto-PMI or WI-Auto-NPMI, demonstratingthe effectiveness of our methodology at automat-ing the word intrusion task for estimating human-interpretability at the model level.
Overall, WI-Auto-PMI outperforms WI-Auto-NPMI.Note that although our proposed methodologyis supervised, as intruder words are syntheticallygenerated and no annotation is needed for the su-pervised learning, the whole process of computingtopic coherence via word intrusion is fully auto-matic, without the need for hand-labelled trainingdata.4.2 Direct Approach: Observed CoherenceNewman et al.
(2010) defined topic interpretabil-ity based on a more direct approach, by asking hu-man judges to rate topics based on the observedcoherence of the top-N topic words, and variousmethodologies have since been proposed to auto-mate the computation of the observed coherence.In this section, we present all these methods andcompare them.The word intrusion dataset is not annotated withhuman ratings of observed coherence.
To cre-ate gold-standard coherence judgements, we usedAmazon Mechanical Turk:5we presented the top-ics (with intruder words removed) to the Turkersand asked them to rate the topics using on a 3-point4Note that both variants use CP1 and CP2 features, i.e.WI-Auto-PMI uses PMI+CP1+C2 while WI-Auto-NPMIuses NPMI+CP1+C2 features.5https://www.mturk.com/mturk/ordinal scale, following Newman et al.
(2010).
Intotal, we collected six to fourteen annotations pertopic (an average of 8.4 annotations per topic).The observed coherence of a topic is computedas the arithmetic mean of the annotators?
ratings,once again following Newman et al.
(2010).
Thehuman-judged observed topic coherence is hence-forth referred to as OC-Human.For the automated methods, we experimentedwith the following methods for estimating thehuman-interpretability of a topic t:1.
OC-Auto-PMI: Pairwise PMI of top-Ntopic words (Newman et al., 2010):OC-Auto-PMI(t) =N?j=2j?1?i=1logP (wj, wi)P (wi)P (wj)2.
OC-Auto-NPMI: NPMI variant of OC-Auto-PMI:OC-Auto-NPMI(t) =N?j=2j?1?i=1logP (wj,wi)P (wi)P (wj)?
logP (wi, wj)3.
OC-Auto-LCP: Pairwise log conditionalprobability of top-N topic words (Mimno etal., 2011):6OC-Auto-LCP(t) =N?j=2j?1?i=1logP (wj, wi)P (wi)4.
OC-Auto-DS: Pairwise distributional simi-larity of the top-N topic words, as describedin Aletras and Stevenson (2013a).For OC-Auto-PMI, OC-Auto-NPMI and OC-Auto-LCP, all topics are lemmatised and intruderwords are removed before coherence is com-puted.7In-domain and cross-domain pairings of6Although the original method uses the topic-modelleddocument collection and document co-occurrence for sam-pling word counts, for a fairer comparison we use log condi-tional probability only as a replacement to the PMI compo-nent of the coherence computation (i.e.
words are still sam-pled using a reference corpus and a sliding window).
For ad-ditional evidence that the original method performs at a sub-par level, see Lau et al.
(2013) and Aletras and Stevenson(2013a).7We once again use Morpha to do the lemmatisation, anddetermine POS via the majority POS for a given word, aggre-gated over all its occurrences in English Wikipedia.533Topic Ref.
Pearson?s r with OC-HumanDomain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DSWIKIWIKI-FULL 0.490 0.903 0.9590.859NEWS-FULL 0.696 0.844 0.913NEWSNEWS-FULL 0.965 0.979 0.8870.941WIKI-FULL 0.931 0.964 0.872Table 2: Pearson correlation of OC-Human and the automated methods ?
OC-Auto-PMI, OC-Auto-NPMI, OC-Auto-LCP and OC-Auto-DS ?
at the model level.the topic domain and reference corpus are experi-mented with for these measures.For OC-Auto-DS, all topics are lemmatised, in-truder words are removed and English Wikipediais used to generate the vector space for the topicwords.
The size of the context window is set to?5 word (i.e.
5 words to either side of the tar-get word).
We use PMI to weight the vectors,cosine similarity for measuring the distributionalsimilarity between the top-N topic words, and the?Topic Word Space?
approach to reduce the di-mensionality of the vector space.
A complete de-scription of the parameters can be found in Aletrasand Stevenson (2013a).
Note that cross-domainpairings of the topic domain and reference corpusare not tested: in line with the original paper, weuse only English Wikipedia to generate the vectorspace before distributional similarity.We present the Pearson correlation coefficientof OC-Human and the four automated methods atthe model level in Table 2.
For OC-Auto-NPMI,OC-Auto-LCP and OC-Auto-DS, we see that theycorrelate strongly with the human-judged coher-ence.
Overall, OC-Auto-NPMI has the best per-formance among the methods, and in-domain pair-ings generally produce the best results for OC-Auto-NPMI and OC-Auto-LCP.
The results arecomparable to those for the automated intruderword detection method in Section 4.1.The non-normalised variant OC-Auto-PMI cor-relates well for NEWS but performs poorly for WIKI,producing a correlation of only 0.490 for the in-domain pairing.
We revisit this in Section 6, andprovide a qualitative analysis to explain the dis-crepancy in results between OC-Auto-PMI andOC-Auto-NPMI.4.3 Word Intrusion vs.
Observed CoherenceIn the previous sections, we showed for both thedirect and indirect approaches that the automatedmethods correlate strongly with the manually-annotated human-interpretability of topics at themodel level (with the exception of OC-Auto-PMI).One question that remains unanswered, however,is whether word intrusion measures topic inter-pretability differently to observed coherence.
Thisis the focus of this section.From the results in Table 3 for the intruderword model vs. observed coherence, we see astrong correlation between WI-Human and OC-Human.
This observation is insightful: it showsthat the topic interpretability estimated by the twoapproaches is almost identical at the model level.Between WI-Human and the observed coher-ence methods automated methods, overall we seea strong correlation for the OC-Auto-NPMI, OC-Auto-LCP and OC-Auto-DS methods.
OC-Auto-PMI once again performs poorly over WIKI, butthis is unsurprising given its previous results (i.e.its poor correlation with OC-Human).
In-domainpairings tend to perform better, and the per-formance of OC-Auto-NPMI, OC-Auto-LCP andOC-Auto-DS is comparable, with no one clearlybest method.5 Human-Interpretability at the TopicLevelIn this section, we evaluate the various methodsat the topic level.
We group together all topicsfor each dataset (without distinguishing the topicmodels that produce them) and calculate the cor-relation of one measure against another.
That is,the correlation coefficient is computed for 900 top-ics/data points in the case of each of WIKI andNEWS.5.1 Indirect Approach: Word IntrusionIn Section 4.1, we proposed a novel methodol-ogy to automate the word intrusion task (WI-Auto-PMI and WI-Auto-NPMI).
We now evaluate itsperformance at the topic level, and present itscorrelation with the human gold standard (WI-Human) in Table 4.The correlation of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the topic level is consid-erably worse, compared to its results at the model534Topic Ref.
Pearson?s r with WI-HumanDomain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DSWIKIWIKI-FULL0.9000.638 0.927 0.9110.907NEWS-FULL 0.614 0.757 0.821NEWSNEWS-FULL0.9150.865 0.866 0.8670.925WIKI-FULL 0.838 0.874 0.893Table 3: Word intrusion vs. observed coherence: Pearson correlation coefficient at the model level.Topic Ref.
Pearson?s r with WI-Human HumanDomain Corpus WI-Auto-PMI WI-Auto-NPMI AgreementWIKIWIKI-FULL 0.554 0.5730.735NEWS-FULL 0.622 0.592NEWSNEWS-FULL 0.602 0.6120.770WIKI-FULL 0.638 0.648Table 4: Pearson correlation coefficient of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the topiclevel.level (Table 1).
The performance between WI-Auto-PMI and WI-Auto-NPMI is not very differ-ent, and the cross-domain pairing slightly outper-forms the in-domain pairing.To better understand the difficulty of the task,we compute the agreement between human anno-tators by calculating the Pearson correlation co-efficient of model precisions produced by ran-domised sub-group pairs in the topics.8That is, foreach topic, we randomly split the annotations intotwo sub-groups, and compute the Pearson correla-tion coefficient of the model precisions producedby the first sub-group and that of the second sub-group.The original dataset has 8 annotations per topic.Splitting the annotations into two sub-groups re-duces the number of annotations to 4 per group,which is not ideal for computing model precision.We thus chose to expand the number of annota-tions by sampling 300 random topics from eachdomain (for a total of 600 topics) and followingthe same process as Chang et al.
(2009) to get in-truder word annotations using Amazon Mechani-cal Turk.
On average, we obtained 11.7 additionalannotations per topic for these 600 topics.
The hu-man agreement scores (i.e.
the Pearson correlationcoefficient of randomised sub-group pairs) for thesampled 600 topics are presented in the last col-umn of Table 4.The sub-group correlation is around r = 0.75for the topics from both datasets.
As such, esti-mating topic interpretability at the topic level is amuch harder task than model-level evaluation.
Ourautomated methods perform at a highly credible8To counter for the fact that annotators labelled varyingnumbers of topics.r = 0.6, but there is certainly room for improve-ment.
Note that the correlation values reported inNewman et al.
(2010) are markedly higher thanours, as they evaluated based on Spearman rankcorrelation, which isn?t attuned to the relative dif-ferences in coherence values and returns highervalues for the task.5.2 Direct Approach: Observed CoherenceWe repeat the experiments of observed coherencein Section 4.2, and evaluate the correlation ofthe automated methods (OC-Auto-PMI, OC-Auto-NPMI, OC-Auto-LCP and OC-Auto-DS) on thehuman gold standard (OC-Human) at the topiclevel.
Results are summarised in Table 5.OC-Auto-PMI performs poorly at the topiclevel in the WIKI domain, similar to what wasseen at the model level in Section 4.2.
Over-all, both OC-Auto-NPMI and OC-Auto-DS are themost consistent methods.
OC-Auto-LCP performsmarkedly worse than these two methods.To get a better understanding of how well hu-man annotators perform at the task, we computethe one-vs-rest Pearson correlation coefficient us-ing the gold standard annotations.
That is, foreach topic, we single out each rating/annotationand compare it to the average of all other rat-ings/annotations.
The one-vs-rest correlation re-sult is displayed in the last column (titled ?Hu-man Agreement?)
in Table 5.
The best auto-mated methods surpass the single-annotator per-formance, indicating that they are able to per-form the task as well as human annotators (unlikethe topic-level results for the word intrusion taskwhere humans were markedly better at the taskthan the automated methods).535Topic Ref.
Pearson?s r with OC-Human HumanDomain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS AgreementWIKIWIKI-FULL 0.533 0.638 0.5790.682 0.624NEWS-FULL 0.582 0.667 0.496NEWSNEWS-FULL 0.719 0.741 0.4710.682 0.634WIKI-FULL 0.671 0.722 0.452Table 5: Pearson correlation of OC-Human and the automated methods at the topic level.Topic Ref.
Pearson?s r with WI-HumanDomain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DSWIKIWIKI-FULL0.6650.472 0.557 0.5470.639NEWS-FULL 0.504 0.571 0.455NEWSNEWS-FULL0.6410.629 0.634 0.4070.649WIKI-FULL 0.604 0.633 0.390Table 6: Word intrusion vs. observed coherence: pearson correlation results at the topic level.5.3 Word Intrusion vs.
Observed CoherenceIn this section, we bring together the indirect ap-proach of word intrusion and the direct approachof observed coherence, and evaluate them againsteach other at the topic level.
Results are sum-marised in Table 6.We see that the correlation between the humanratings of intruder words and observed coherenceis only modest, implying that there are topic-leveldifferences in the output of the two approaches.
InSection 6, we provide a qualitative analysis andexplanation as to what constitutes the differencesbetween the approaches.For the automated methods, OC-Auto-DS hasthe best performance, with OC-Auto-NPMI per-forming relatively well (in particularly in the NEWSdomain).6 DiscussionNormalised PMI (NPMI) was first introduced byBouma (2009) as a means of reducing the bias forPMI towards words of lower frequency, in additionto providing a standardised range of [?1, 1] for thecalculated values.We introduced NPMI to the automated meth-ods of word intrusion (WI-Auto-NPMI) and ob-served coherence (OC-Auto-NPMI) to explore itssuitability for the task.
For the latter, we sawthat NPMI achieves markedly higher correlationthan OC-Human (in particular, at the model level).To better understand the impact of normalisation,we inspected a list of WIKI topics that have simi-lar scores for OC-Human and OC-Auto-NPMI butvery different OC-Auto-PMI scores.
A sample ofthese topics is presented in Table 7.
WIKI-FULLis used as the reference corpus for computing thescores.
Note that the presented OC-Auto-NPMI*and OC-Auto-PMI* scores are post-normalised tothe range [0, 1] for ease of interpretation.
To givea sense of how readily these topic words occur inthe reference corpus, we additionally display thefrequency of the first topic word in the referencecorpus (last column).All topics presented have an OC-Human scoreof 3.0 (i.e.
these topics are rated as being very co-herent by human judges) and similar OC-Auto-NPMI values.
Their OC-Auto-PMI scores, how-ever, are very different between the top-3 andbottom-3 topics.
The bias of PMI towards lowerfrequency words is clear: topic words that occurfrequently in the corpus receive a lower OC-Auto-PMI score compared to those that occur less fre-quently, even though the human-judged observedcoherence is the same.
OC-Auto-NPMI on theother hand, correctly estimates the coherence.We observed, however, that the impact of nor-malising PMI is less in the word intrusion task.One possible explanation is that for the automatedmethods WI-Auto-PMI and WI-Auto-NPMI, thePMI/NPMI scores are used indirectly as a featureto a machine learning framework, and the biascould be reduced/compensated by other features.On the subject of the difference between ob-served coherence and word intrusion in estimat-ing topic interpretability, we observed that WI-Human and OC-Human correlate only moderately(r ?
0.6) at the topic level (Table 6).
To betterunderstand this effect, we manually analysed top-ics that have differing WI-Human and OC-Humanscores.
A sample of topics with high divergencein estimated coherence score is given in Table 8.As before, the presented the OC-Human* and WI-536TopicOC- OC- OC- WordHuman Auto-NPMI* Auto-PMI* Countcell hormone insulin muscle receptor 3.0 0.59 0.61 #(cell) = 1.1Melectron laser magnetic voltage wavelength 3.0 0.52 0.54 #(electron) = 0.3Mmagnetic neutrino particle quantum universe 3.0 0.55 0.55 #(magnetic) = 0.4Malbum band music release song 3.0 0.56 0.37 #(album) = 12.5Mcollege education school student university 3.0 0.57 0.38 #(college) = 9.8Mcity county district population town 3.0 0.52 0.34 #(city) = 22.0MTable 7: A list of WIKI topics to illustrate the impact of NPMI.Topic # Topic OC-Human* WI-Human*1 business company corporation cluster loch shareholder 0.94 0.252 song actor clown play role theatre 1.00 0.503 census ethnic female male population village 0.92 0.254 composer singer jazz music opera piano 1.00 0.635 choice count give i.e.
simply unionist 0.14 1.006 digital clown friend love mother wife 0.17 1.00Table 8: A list of WIKI topics to illustrate the difference between observed coherence and word intrusion.Boxes denote human chosen intruder words, and boldface denotes true intruder words.Human* scores in the table are post-normalised tothe range [0, 1] for ease of comparison.In general, there are two reasons for topics tohave high OC-Human and low WI-Human scores.First, if a topic has an outlier word that is mildlyrelated to the topic, users tend to choose this wordas the intruder word in the word intrusion task,yielding a low WI-Human score.
If they are askedto rate the observed coherence, however, the singleoutlier word often does not affect its overall coher-ence, resulting in a high OC-Human score.
This isobserved in topics 1 and 2 in Table 8, where lochand clown are chosen by annotators in the word in-trusion task, as they detract from the semantics ofthe topic.
This results in low WI-Human scores,but high observed coherence scores (OC-Human).The second reason is the random selection ofintruder words related to the original topic.
Wesee this in topics 3 and 4, where related intruderwords (village and singer) were selected.For topics with low OC-Human and high WI-Human scores, the true intruder words are oftenvery different to the domain/focus of other topicwords.
As such, annotators are consistently ableto single them out to yield high WI-Human scores,even though the topic as a whole is not coherent.Topics 5 and 6 in Table 8 exhibit this.All topic evaluation measures described in thispaper are implemented in an open-source toolkit.99https://github.com/jhlau/topic_interpretability7 ConclusionIn this paper, we examined various methodologiesthat estimate the semantic interpretability of top-ics, at two levels: the model level and the topiclevel.
We looked first at the word intrusion taskproposed by Chang et al.
(2009), and proposeda method that fully automates the task.
Next weturned to observed coherence, a more direct ap-proach to estimate topic interpretability.
At themodel level, results were very positive for both theword intrusion and observed coherence methods.At the topic level, however, the results were moremixed.
For observed coherence, our best methods(OC-Auto-NPMI and OC-Auto-DS) were able toemulate human performance.
For word intrusion,the automated methods were slightly below humanperformance, with some room for improvement.We finally observed that there are systematic dif-ferences in the topic-level scores derived from thetwo task formulations.AcknowledgementsThis work was supported in part by the AustralianResearch Council, and for author JHL, also partlyfunded by grant ES/J022969/1 from the Economicand Social Research Council of the UK.
The au-thors acknowledge the generosity of Nikos Ale-tras and Mark Stevenson in providing their codefor OC-Auto-DS, and Jordan Boyd-Graber in pro-viding the data used in Chang et al.
(2009).537ReferencesN.
Aletras and M. Stevenson.
2013a.
Evaluatingtopic coherence using distributional semantics.
InProceedings of the Tenth International Workshop onComputational Semantics (IWCS-10), pages 13?22,Potsdam, Germany.N.
Aletras and M. Stevenson.
2013b.
Representingtopics using images.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (NAACL HLT 2013), pages158?167, Atlanta, USA.D.
Blei and J. Lafferty.
2005.
Correlated topic mod-els.
In Advances in Neural Information ProcessingSystems 17 (NIPS-05), pages 147?154, Vancouver,Canada.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet allocation.
Journal of Machine LearningResearch, 3:993?1022.L.
Bolelli, S?.
Ertekin, and C.L.
Giles.
2009.
Topicand trend detection in text collections using LatentDirichlet Allocation.
In Proceedings of ECIR 2009,pages 776?780, Toulouse, France.G.
Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
In Proceed-ings of the Biennial GSCL Conference, pages 31?40,Potsdam, Germany.J.
Chang, J. Boyd-Graber, S. Gerrish, C. Wang, andD.
Blei.
2009.
Reading tea leaves: How humansinterpret topic models.
In Advances in Neural In-formation Processing Systems 21 (NIPS-09), pages288?296, Vancouver, Canada.A.
Haghighi and L. Vanderwende.
2009.
Exploringcontent models for multi-document summarization.In Proceedings of the North American Chapter of theAssociation for Computational Linguistics ?
HumanLanguage Technologies 2009 (NAACL HLT 2009),pages 362?370.D.
Hall, D. Jurafsky, and C.D.
Manning.
2008.
Study-ing the history of ideas using topic models.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2008), pages 363?371, Honolulu, USA.M.
Hall, P. Clough, and M. Stevenson.
2012.
Evalu-ating the use of clustering for automatically organ-ising digital library collections.
In Proceedings ofthe Second International Conference on Theory andPractice of Digital Libraries, pages 323?334, Pa-phos, Cyprus.T.
Hofmann.
1999.
Probabilistic latent semantic in-dexing.
In Proceedings of 22nd International ACM-SIGIR Conference on Research and Developmentin Information Retrieval (SIGIR?99), pages 50?57,Berkeley, USA.T.
Joachims.
2006.
Training linear SVMs in lineartime.
In Proceedings of the 12th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining (KDD 2006), Philadelphia, USA.J.H.
Lau, D. Newman, S. Karimi, and T. Baldwin.2010.
Best topic word selection for topic labelling.In Proceedings of the 23rd International Confer-ence on Computational Linguistics (COLING 2010),Posters Volume, pages 605?613, Beijing, China.J.H.
Lau, K. Grieser, D. Newman, and T. Baldwin.2011.
Automatic labelling of topic models.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL HLT 2011), pages 1536?1545, Portland, USA.J.H.
Lau, N. Collier, and T. Baldwin.
2012a.
On-line trend analysis with topic models: #twittertrends detection topic model online.
In Proceedingsof the 24th International Conference on Compu-tational Linguistics (COLING 2012), pages 1519?1534, Mumbai, India.J.H.
Lau, P. Cook, D. McCarthy, D. Newman, andT.
Baldwin.
2012b.
Word sense induction for novelsense detection.
In Proceedings of the 13th Con-ference of the EACL (EACL 2012), pages 591?601,Avignon, France.J.H.
Lau, T. Baldwin, and D. Newman.
2013.
Oncollocations and topic models.
ACM Transactionson Speech and Language Processing, 10(3):10:1?10:14.A McCallum, G.S.
Mann, and D Mimno.
2006.
Bib-liometric impact measures leveraging topic analysis.In Proceedings of the 6th ACM/IEEE-CS Joint Con-ference on Digital Libraries 2006 (JCDL?06), pages65?74, Chapel Hill, USA.D.
Mimno, H. Wallach, E. Talley, M. Leenders, andA.
McCallum.
2011.
Optimizing semantic coher-ence in topic models.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2011), pages 262?272,Edinburgh, UK.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.C.
Musat, J. Velcin, S. Trausan-Matu, and M.A.
Rizoiu.2011.
Improving topic evaluation using concep-tual knowledge.
In Proceedings of the 22nd Inter-national Joint Conference on Artificial Intelligence(IJCAI-2011), pages 1866?1871, Barcelona, Spain.D.
Newman, J.H.
Lau, K. Grieser, and T. Baldwin.2010.
Automatic evaluation of topic coherence.In Proceedings of Human Language Technologies:The 11th Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL HLT 2010), pages 100?108, LosAngeles, USA.538M.
Paul and R. Girju.
2010.
A two-dimensional topic-aspect model for discovering multi-faceted topics.In Proceedings of the 24th Annual Conference onArtificial Intelligence (AAAI-10), Atlanta, USA.J.
Reisinger, A.
Waters, B. Silverthorn, and R.J.Mooney.
2010.
Spherical topic models.
In Proceed-ings of the 27th International Conference on Ma-chine Learning (ICML 2010), pages 903?910, Haifa,Israel.X.
Wang, A. McCallum, and X. Wei.
2007.
Topicaln-grams: Phrase and topic discovery, with an ap-plication to information retrieval.
In Proceedingsof the Seventh IEEE International Conference onData Mining (ICDM 2007), pages 697?702, Omaha,USA.B.
Zhao and E.P.
Xing.
2007.
HM-BiTAM: Bilin-gual topic exploration, word alignment, and transla-tion.
In Advances in Neural Information ProcessingSystems (NIPS 2007), pages 1689?1696, Vancouver,Canada.539
