Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 100?105,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLanguage Models for Image Captioning: The Quirks and What WorksJacob DevlinF, Hao Cheng?, Hao Fang?, Saurabh Gupta?,Li Deng, Xiaodong HeF, Geoffrey ZweigF, Margaret MitchellFMicrosoft ResearchF Corresponding authors: {jdevlin,xiaohe,gzweig,memitc}@microsoft.com?
University of Washington?
University of California at BerkeleyAbstractTwo recent approaches have achievedstate-of-the-art results in image caption-ing.
The first uses a pipelined processwhere a set of candidate words is gen-erated by a convolutional neural network(CNN) trained on images, and then a max-imum entropy (ME) language model isused to arrange these words into a coherentsentence.
The second uses the penultimateactivation layer of the CNN as input to arecurrent neural network (RNN) that thengenerates the caption sequence.
In this pa-per, we compare the merits of these dif-ferent language modeling approaches forthe first time by using the same state-of-the-art CNN as input.
We examine is-sues in the different approaches, includ-ing linguistic irregularities, caption repe-tition, and data set overlap.
By combiningkey aspects of the ME and RNN methods,we achieve a new record performance overpreviously published results on the bench-mark COCO dataset.
However, the gainswe see in BLEU do not translate to humanjudgments.1 IntroductionRecent progress in automatic image captioninghas shown that an image-conditioned languagemodel can be very effective at generating captions.Two leading approaches have been explored forthis task.
The first decomposes the problem intoan initial step that uses a convolutional neural net-work to predict a bag of words that are likely tobe present in a caption; then in a second step, amaximum entropy language model (ME LM) isused to generate a sentence that covers a mini-mum number of the detected words (Fang et al,2015).
The second approach uses the activationsfrom final hidden layer of an object detection CNNas the input to a recurrent neural network lan-guage model (RNN LM).
This is referred to as aMultimodal Recurrent Neural Network (MRNN)(Karpathy and Fei-Fei, 2015; Mao et al, 2015;Chen and Zitnick, 2015).
Similar in spirit is thethe log-bilinear (LBL) LM of Kiros et al (2014).In this paper, we study the relative merits ofthese approaches.
By using an identical state-of-the-art CNN as the input to RNN-based and ME-based models, we are able to empirically com-pare the strengths and weaknesses of the lan-guage modeling components.
We find that theapproach of directly generating the text with anMRNN1outperforms the ME LM when measuredby BLEU on the COCO dataset (Lin et al, 2014),2but this recurrent model tends to reproduce cap-tions in the training set.
In fact, a simple k-nearestneighbor approach, which is common in earlier re-lated work (Farhadi et al, 2010; Mason and Char-niak, 2014), performs similarly to the MRNN.
Incontrast, the ME LM generates the most novelcaptions, and does the best at captioning imagesfor which there is no close match in the trainingdata.
With a Deep Multimodal Similarity Model(DMSM) incorporated,3the ME LM significantlyoutperforms other methods according to humanjudgments.
In sum, the contributions of this pa-per are as follows:1.
We compare the use of discrete detectionsand continuous valued CNN activations asthe conditioning information for languagemodels trained to generate image captions.2.
We show that a simple k-nearest neighbor re-trieval method performs at near state-of-the-art for this task and dataset.3.
We demonstrate that a state-of-the-art1In our case, a gated recurrent neural network (GRNN) isused (Cho et al, 2014), similar to an LSTM.2This is the largest image captioning dataset to date.3As described by Fang et al (2015).100MRNN-based approach tends to reconstructpreviously seen captions; in contrast, thetwo stage ME LM approach achieves similaror better performance while generatingrelatively novel captions.4.
We advance the state-of-the-art BLEU scoreson the COCO dataset.5.
We present human evaluation results on thesystems with the best performance as mea-sured by automatic metrics.6.
We explore several issues with the statisticalmodels and the underlying COCO dataset, in-cluding linguistic irregularities, caption repe-tition, and data set overlap.2 ModelsAll language models compared here are trainedusing output from the same state-of-the-art CNN.The CNN used is the 16-layer variant of VGGNet(Simonyan and Zisserman, 2014) which was ini-tially trained for the ILSVRC2014 classificationtask (Russakovsky et al, 2015), and then fine-tuned on the Microsoft COCO data set (Fang etal., 2015; Lin et al, 2014).2.1 Detector Conditioned ModelsWe study the effect of leveraging an explicit de-tection step to find key objects/attributes in imagesbefore generation, examining both an ME LM ap-proach as reported in previous work (Fang et al,2015), and a novel LSTM approach introducedhere.
Both use a CNN trained to output a bag ofwords indicating the words that are likely to ap-pear in a caption, and both use a beam search tofind a top-scoring sentence that contains a subsetof the words.
This set of words is dynamically ad-justed to remove words as they are mentioned.We refer the reader to Fang et al (2015) for afull description of their ME LM approach, whose500-best outputs we analyze here.4We also in-clude the output from their ME LM that leveragesscores from a Deep Multimodal Similarity Model(DMSM) during n-best re-ranking.
Briefly, theDMSM is a non-generative neural network modelwhich projects both the image pixels and captiontext into a comparable vector space, and scorestheir similarity.In the LSTM approach, similar to the ME LMapproach, we maintain a set of likely wordsD that4We will refer to this system as D-ME.have not yet been mentioned in the caption un-der construction.
This set is initialized to all thewords predicted by the CNN above some thresh-old ?.5The words already mentioned in thesentence history h are then removed to producea set of conditioning words D \ {h}.
We in-corporate this information within the LSTM byadding an additional input encoded to representthe remaining visual attributes D \ {h} as a con-tinuous valued auxiliary feature vector (Mikolovand Zweig, 2012).
This is encoded as f(sh?1+?v?D\{h}gv+ Uqh,D), where sh?1and gvarerespectively the continuous-space representationsfor last word h?1and detector v ?
D \ {h}, U islearned matrix for recurrent histories, and f(?)
isthe sigmoid transformation.2.2 Multimodal Recurrent Neural NetworkIn this section, we explore a model directly con-ditioned on the CNN activations rather than a setof word detections.
Our implementation is verysimilar to captioning models described in Karpa-thy and Fei-Fei (2015), Vinyals et al (2014), Maoet al (2015), and Donahue et al (2014).
Thisjoint vision-language RNN is referred to as a Mul-timodal Recurrent Neural Network (MRNN).In this model, we feed each image into ourCNN and retrieve the 4096-dimensional final hid-den layer, denoted as fc7.
The fc7 vector isthen fed into a hidden layer H to obtain a 500-dimensional representation that serves as the ini-tial hidden state to a gated recurrent neural net-work (GRNN) (Cho et al, 2014).
The GRNNis trained jointly with H to produce the captionone word at a time, conditioned on the previousword and the previous recurrent state.
For decod-ing, we perform a beam search of size 10 to emittokens until an END token is produced.
We usea 500-dimensional GRNN hidden layer and 200-dimensional word embeddings.2.3 k-Nearest Neighbor ModelBoth Donahue et al (2015) and Karpathy and Fei-Fei (2015) present a 1-nearest neighbor baseline.As a first step, we replicated these results using thecosine similarity of the fc7 layer between eachtest set image t and training image r. We randomlyemit one caption from t?s most similar training im-age as the caption of t. As reported in previousresults, performance is quite poor, with a BLEU5In all experiments in this paper, ?=0.5.101Figure 1: Example of the set of candidate captions for animage, the highest scoring m captions (green) and the con-sensus caption (orange).
This is a real example visualized intwo dimensions.score of 11.2%.However, we explore the idea that we may beable to find an optimal k-nearest neighbor consen-sus caption.
We first select the k = 90 nearesttraining images of a test image t as above.
We de-note the union of training captions in this set asC = c1, ..., c5k.6For each caption ci, we com-pute the n-gram overlap F-score between ciandeach other caption in C. We define the consen-sus caption c?to be caption with the highest meann-gram overlap with the other captions in C. Wehave found it is better to only compute this averageamong ci?s m = 125 most similar captions, ratherthan all of C. The hyperparameters k and m wereobtained by a grid search on the validation set.A visual example of the consensus caption isgiven in Figure 1.
Intuitively, we are choosinga single caption that may describe many differentimages that are similar to t, rather than a captionthat describes the single image that is most similarto t. We believe that this is a reasonable approachto take for a retrieval-based method for captioning,as it helps ensure incorrect information is not men-tioned.
Further details on retrieval-based methodsare available in, e.g., (Ordonez et al, 2011; Ho-dosh et al, 2013).3 Experimental Results3.1 The Microsoft COCO DatasetWe work with the Microsoft COCO dataset (Linet al, 2014), with 82,783 training images, andthe validation set split into 20,243 validation im-ages and 20,244 testval images.
Most images con-tain multiple objects and significant contextual in-formation, and each image comes with 5 human-6Each training image has 5 captions.LM PPLX BLEU METEORD-ME?18.1 23.6 22.8D-LSTM 14.3 22.4 22.6MRNN 13.2 25.7 22.6k-Nearest Neighbor - 26.0 22.51-Nearest Neighbor - 11.2 17.3Table 1: Model performance on testval.
?
: From (Fang et al,2015).D-ME+DMSM a plate with a sandwich and a cup of coffeeMRNN a close up of a plate of foodD-ME+DMSM+MRNN a plate of food and a cup of coffeek-NN a cup of coffee on a plate with a spoonD-ME+DMSM a black bear walking across a lush green forestMRNN a couple of bears walking across a dirt roadD-ME+DMSM+MRNN a black bear walking through a wooded areak-NN a black bear that is walking in the woodsD-ME+DMSM a gray and white cat sitting on top of itMRNN a cat sitting in front of a mirrorD-ME+DMSM+MRNN a close up of a cat looking at the camerak-NN a cat sitting on top of a wooden tableTable 2: Example generated captions.annotated captions.
The images create a challeng-ing testbed for image captioning and are widelyused in recent automatic image captioning work.3.2 MetricsThe quality of generated captions is measured au-tomatically using BLEU (Papineni et al, 2002)and METEOR (Denkowski and Lavie, 2014).BLEU roughly measures the fraction of N -grams(up to 4 grams) that are in common between a hy-pothesis and one or more references, and penalizesshort hypotheses by a brevity penalty term.7ME-TEOR (Denkowski and Lavie, 2014) measures un-igram precision and recall, extending exact wordmatches to include similar words based on Word-Net synonyms and stemmed tokens.
We also re-port the perplexity (PPLX) of studied detection-conditioned LMs.
The PPLX is in many waysthe natural measure of a statistical LM, but can beloosely correlated with BLEU (Auli et al, 2013).3.3 Model ComparisonIn Table 1, we summarize the generation perfor-mance of our different models.
The discrete de-tection based models are prefixed with ?D?.
Someexample generated results are show in Table 2.We see that the detection-conditioned LSTMLM produces much lower PPLX than thedetection-conditioned ME LM, but its BLEUscore is no better.
The MRNN has the lowestPPLX, and highest BLEU among all LMs stud-7We use the length of the reference that is closest to thelength of the hypothesis to compute the brevity penalty.102Re-Ranking Features BLEU METEORD-ME?23.6 22.8+ DMSM?25.7 23.6+ MRNN 26.8 23.3+ DMSM + MRNN 27.3 23.6Table 3: Model performance on testval after re-ranking.?
: previously reported and reconfirmed BLEU scores from(Fang et al, 2015).
+DMSM had resulted in the highest scoreyet reported.ied in our experiments.
It significantly improvesBLEU by 2.1 absolutely over the D-ME LM base-line.
METEOR is similar across all three LM-based methods.Perhaps most surprisingly, the k-nearest neigh-bor algorithm achieves a higher BLEU score thanall other models.
However, as we will demonstratein Section 3.5, the generated captions perform sig-nificantly better than the nearest neighbor captionsin terms of human quality judgements.3.4 n-best Re-RankingIn addition to comparing the ME-based and RNN-based LMs independently, we explore whethercombining these models results in an additive im-provement.
To this end, we use the 500-best listfrom the D-ME and add a score for each hypoth-esis from the MRNN.8We then re-rank the hy-potheses using MERT (Och, 2003).
As in previouswork (Fang et al, 2015), model weights were opti-mized to maximize BLEU score on the validationset.
We further extend this combination approachto the D-ME model with DMSM scores includedduring re-ranking (Fang et al, 2015).Results are show in Table 3.
We find that com-bining the D-ME, DMSM, and MRNN achieves a1.6 BLEU improvement over the D-ME+DMSM.3.5 Human EvaluationBecause automatic metrics do not always corre-late with human judgments (Callison-Burch et al,2006; Hodosh et al, 2013), we also performed hu-man evaluations using the same procedure as inFang et al (2015).
Here, human judges were pre-sented with an image, a system generated caption,and a human generated caption, and were askedwhich caption was ?better?.9For each condition,5 judgments were obtained for 1000 images fromthe testval set.8The MRNN does not produce a diverse n-best list.9The captions were randomized and the users were notinformed which was which.Results are shown in Table 4.
The D-ME+DMSM outperforms the MRNN by 5 per-centage points for the ?Better Or Equal to Hu-man?
judgment, despite both systems achievingthe same BLEU score.
The k-Nearest Neighborsystem performs 1.4 percentage points worse thanthe MRNN, despite achieving a slightly higherBLEU score.
Finally, the combined model doesnot outperform the D-ME+DMSM in terms of hu-man judgments despite a 1.6 BLEU improvement.Although we cannot pinpoint the exact reasonfor this mismatch between automated scores andhuman evaluation, a more detailed analysis of thedifference between systems is performed in Sec-tions 4 and 5.Human JudgementsBetter BetterApproach or Equal BLEUD-ME+DMSM 7.8% 34.0% 25.7MRNN 8.8% 29.0% 25.7D-ME+DMSM+MRNN 5.7% 34.2% 27.3k-Nearest Neighbor 5.5% 27.6% 26.0Table 4: Results when comparing produced captions to thosewritten by humans, as judged by humans.
These are the per-cent of captions judged to be ?better than?
or ?better than orequal to?
a caption written by a human.4 Language AnalysisExamples of common mistakes we observe on thetestval set are shown in Table 5.
The D-ME systemhas difficulty with anaphora, particularly withinthe phrase ?on top of it?, as shown in examples(1), (2), and (3).
This is likely due to the fact that ismaintains a local context window.
In contrast, theMRNN approach tends to generate such anaphoricrelationships correctly.However, the D-ME LM maintains an explicitcoverage state vector tracking which attributeshave already been emitted.
The MRNN implicitlymaintains the full state using its recurrent layer,which sometimes results in multiple emission mis-takes, where the same attribute is emitted morethan once.
This is particularly evident when coor-dination (?and?)
is present (examples (4) and (5)).4.1 Repeated CaptionsAll of our models produce a large number of cap-tions seen in the training and repeated for differ-ent images in the test set, as shown in Table 6(also observed by Vinyals et al (2014) for theirLSTM-based model).
There are at least two po-tential causes for this repetition.103D-ME+DMSM MRNN1 a slice of pizza sitting on top of it a bed with a red blanket on top of it2 a black and white bird perched ontop of ita birthday cake with candles on topof it3 a little boy that is brushing histeeth with a toothbrush in hermoutha little girl brushing her teeth with atoothbrush4 a large bed sitting in a bedroom a bedroom with a bed and a bed5 a man wearing a bow tie a man wearing a tie and a tieTable 5: Example errors in the two basic approaches.System Unique Seen InCaptions TrainingHuman 99.4% 4.8%D-ME+DMSM 47.0% 30.0%MRNN 33.1% 60.3%D-ME+DMSM+MRNN 28.5% 61.3%k-Nearest Neighbor 36.6% 100%Table 6: Percentage unique (Unique Captions) and novel(Seen In Training) captions for testval images.
For example,28.5% unique means 5,776 unique strings were generated forall 20,244 images.First, the systems often produce generic cap-tions such as ?a close up of a plate of food?, whichmay be applied to many publicly available im-ages.
This may suggest a deeper issue in the train-ing and evaluation of our models, which warrantsmore discussion in future work.
Second, althoughthe COCO dataset and evaluation server10has en-couraged rapid progress in image captioning, theremay be a lack of diversity in the data.
We also notethat although caption duplication is an issue in allsystems, it is a greater issue in the MRNN than theD-ME+DMSM.5 Image DiversityThe strong performance of the k-nearest neighboralgorithm and the large number of repeated cap-tions produced by the systems here suggest a lackof diversity in the training and test data.11We believe that one reason to work on imagecaptioning is to be able to caption compositionallynovel images, where the individual components ofthe image may be seen in the training, but the en-tire composition is often not.In order to evaluate results for only compo-sitionally novel images, we bin the test imagesbased on visual overlap with the training data.For each test image, we compute the fc7 cosinesimilarity with each training image, and the meanvalue of the 50 closest images.
We then computeBLEU on the 20% least overlapping and 20% most10http://mscoco.org/dataset/11This is partially an artifact of the manner in which theMicrosoft COCO data set was constructed, since each imagewas chosen to be in one of 80 pre-defined object categories.Condition Train/Test Visual OverlapBLEUWhole 20% 20%Set Least MostD-ME+DMSM 25.7 20.9 29.9MRNN 25.7 18.8 32.0D-ME+DMSM+MRNN 27.3 21.7 32.0k-Nearest Neighbor 26.0 18.4 33.2Table 7: Performance for different portions of testval, basedon visual overlap with the training.overlapping subsets.Results are shown in Table 7.
The D-ME+DMSM outperforms the k-nearest neighborapproach by 2.5 BLEU on the ?20% Least?
set,even though performance on the whole set is com-parable.
Additionally, the D-ME+DMSM out-performs the MRNN by 2.1 BLEU on the ?20%Least?
set, but performs 2.1 BLEU worse onthe ?20% Most?
set.
This is evidence that D-ME+DMSM generalizes better on novel imagesthan the MRNN; this is further supported by therelatively low percentage of captions it gener-ates seen in the training data (Table 6) while stillachieving reasonable captioning performance.
Wehypothesize that these are the main reasons forthe strong human evaluation results of the D-ME+DMSM shown in Section 3.5.6 ConclusionWe have shown that a gated RNN conditioned di-rectly on CNN activations (an MRNN) achievesbetter BLEU performance than an ME LM orLSTM conditioned on a set of discrete activations;and a similar BLEU performance to an ME LMcombined with a DMSM.
However, the ME LM+ DMSM method significantly outperforms theMRNN in terms of human quality judgments.
Wehypothesize that this is partially due to the lack ofnovelty in the captions produced by the MRNN.In fact, a k-nearest neighbor retrieval algorithmintroduced in this paper performs similarly to theMRNN in terms of both automatic metrics and hu-man judgements.When we use the MRNN system alongside theDMSM to provide additional scores in MERT re-ranking of the n-best produced by the image-conditioned ME LM, we advance by 1.6 BLEUpoints on the best previously published results onthe COCO dataset.
Unfortunately, this improve-ment in BLEU does not translate to improved hu-man quality judgments.104ReferencesMichael Auli, Michel Galley, Chris Quirk, and Ge-offrey Zweig.
2013.
Joint language and transla-tion modeling with recurrent neural networks.
InProc.
Conf.
Empirical Methods Natural LanguageProcess.
(EMNLP), pages 1044?1054.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluation the role of bleu inmachine translation research.
In EACL, volume 6,pages 249?256.Xinlei Chen and C. Lawrence Zitnick.
2015.
Mind?seye: A recurrent visual representation for image cap-tion generation.
In Proc.
Conf.
Comput.
Vision andPattern Recognition (CVPR).Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representationsusing RNN encoder-decoder for statistical machinetranslation.
CoRR.Michael Denkowski and Alon Lavie.
2014.
Meteoruniversal: language specific translation evaluationfor any target language.
In Proc.
EACL 2014 Work-shop Statistical Machine Translation.Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2014.
Long-termrecurrent convolutional networks for visual recogni-tion and description.
arXiv:1411.4389 [cs.CV].Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2015.
Long-termrecurrent convolutional networks for visual recogni-tion and description.
In Proc.
Conf.
Comput.
Visionand Pattern Recognition (CVPR).Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Sri-vastava, Li Deng, Piotr Doll?a, Margaret Mitchell,John C. Platt, C. Lawrence Zitnick, and GeoffreyZweig.
2015.
From captionons to visual conceptsand back.
In Proc.
Conf.
Comput.
Vision and Pat-tern Recognition (CVPR).Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Every pic-ture tells a story: generating sentences from images.In Proc.
European Conf.
Comput.
Vision (ECCV),pages 15?29.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:data models and evaluation metrics.
J.
Artificial In-tell.
Research, pages 853?899.Andrej Karpathy and Li Fei-Fei.
2015.
Deep visual-semantic alignments for generating image descrip-tions.
In Proc.
Conf.
Comput.
Vision and PatternRecognition (CVPR).Ryan Kiros, Ruslan Salakhutdinov, and Richard Zemel.2014.
Multimodal neural language models.
In Proc.Int.
Conf.
Machine Learning (ICML).Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Doll?ar,and C. Lawrence Zitnick.
2014.
Microsoft COCO:Common objects in context.
arXiv:1405.0312[cs.CV].Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, andAlan L. Yuille.
2015.
Deep captioning with multi-modal recurrent neural networks (m-RNN).
In Proc.Int.
Conf.
Learning Representations (ICLR).Rebecca Mason and Eugene Charniak.
2014.
Domain-specific image captioning.
In CoNLL.Tomas Mikolov and Geoffrey Zweig.
2012.
Contextdependent recurrent neural network language model.In SLT, pages 234?239.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL, ACL ?03.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2Text: Describing images using 1 millioncaptioned photogrphs.
In Proc.
Annu.
Conf.
NeuralInform.
Process.
Syst.
(NIPS).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
Assoc.for Computational Linguistics (ACL), pages 311?318.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael Bernstein,Alexander C. Berg, and Li Fei-Fei.
2015.
ImageNetLarge Scale Visual Recognition Challenge.
Interna-tional Journal of Computer Vision (IJCV).Karen Simonyan and Andrew Zisserman.
2014.
Verydeep convolutional networks for large-scale imagerecognition.
arXiv preprint.Oriol Vinyals, Alexander Toshev, Samy Bengio, andDumitru Erhan.
2014.
Show and tell: a neural im-age caption generator.
In Proc.
Conf.
Comput.
Vi-sion and Pattern Recognition (CVPR).105
