Coling 2010: Poster Volume, pages 775?781,Beijing, August 2010Connective-Based Measuring of the Inter-Annotator Agreementin the Annotation of Discourse in PDTJi??
M?rovsk?, Lucie Mladov?, ?
?rka Zik?nov?Charles University in PragueInstitute of Formal and applied Linguistics{mirovsky,mladova,zikanova}@ufal.mff.cuni.czAbstractWe present  several  ways of measuringthe inter-annotator agreement in the on-going annotation of semantic inter-sen-tential discourse relations in the PragueDependency  Treebank  (PDT).
Twoways have been employed to overcomelimitations of measuring the agreementon  the  exact  location  of  the  start/endpoints of the relations.
Both methods ?skipping one tree  level  in the start/endnodes,  and  the  connective-based  mea-sure ?
are focused on a recognition ofthe existence and of the type of the rela-tions, rather than on fixing the exact po-sitions of the start/end points of the con-necting arrows.1 Introduction1.1 Prague Dependency Treebank 2.0The  Prague  Dependency  Treebank  2.0  (PDT2.0; Haji?
et al, 2006) is a manually annotatedcorpus of Czech.
It belongs to the most complexand elaborate linguistically annotated treebanksin the world.
The texts  are annotated on threelayers  of  language description:  morphological,analytical (which expresses the surface syntacticstructure),  and  tectogrammatical  (which  ex-presses the deep syntactic structure).
On the tec-togrammatical layer, the data consist of almost50 thousand sentences.For the upcoming release of PDT, many addi-tional features are planned, coming as results ofseveral  projects.
Annotation  of  semantic  in-ter-sentential  discourse  relations  is  one  of  theplanned additions.To ensure the highest possible quality of theannotated data, it would be best if several anno-tators annotated the whole data in parallel.
Aftersolving discrepancies in the annotations of theindividual   annotators,  we  would  get  a  high--quality annotation.
This approach is sometimesemployed, but most of the times, the availableresources prohibit it (which is also the case ofthe discourse annotation project).
Manual anno-tation of data is a very expensive and time con-suming task.
To overcome the restriction of lim-ited resources, each part of the data is annotatedby one annotator only, with the exception of asmall  overlap  for  studying  and measuring  theinter-annotator (dis-)agreement.1.2 Inter-Annotator Agreement in Compu-tational LinguisticsMeasuring  the  inter-annotator  agreement  haslong been studied (not  only) in computationallinguistics.
It is a complex field of research anddifferent domains require different approaches.Classical  measures  recall,  precision and  F-measure offer the most straightforward and in-tuitively interpretable results.
Since they do takeinto account neither the contribution of chancein agreement, nor different importance of differ-ent  types  of disagreement,  etc.,  other  more orless elaborate coefficients for measuring the in-ter-annotator  agreement  have  been  developed.Cohen's ?
(Cohen, 1960) is suitable for classifi-cation tasks and tries to measure the agreement?above  chance?.
Krippendorff's  ?
(Krippen-dorff,  1980) can be used if we need to distin-guish various  levels  of  disagreement.
RebeccaPassonneau (2004) offered a solution for mea-suring agreement between sets of elements (likewords in coreferential chains).
Variants of thesecoefficients  can be used  for  measuring  agree-ment among more than two annotators.
A com-prehensive overview of methods for measuringthe inter-annotator agreement in various areas of775computational linguistics was given in Artsteinand Poesio (2008).For measuring the inter-annotator agreementin  the  annotation  of  semantic  inter-sententialdiscourse relations in PDT, we have chosen twomeasures.
The  relations  do  not  form  naturalchains  (unlike  e.g.
textual  and  grammaticalcoreference)  and  a  simple  F1-measure is  wellsuited for the agreement on existence of the re-lations.
For the agreement on types of the rela-tions, which is a typical classification task, weuse Cohen's ?.Our  research  has  then  been  focused  not  on?how to measure?
the agreement (which coeffi-cient to use),  but rather on ?what to measure?
(which phenomena), which is the topic of thispaper.2 Annotated PhenomenaSince the Prague Dependency Treebank 2.0 al-ready contains three layers of linguistic annota-tion, two of which (the analytical layer ?
surfacesyntax, and the tectogrammatical layer ?
under-lying syntax and semantics) are tree representa-tions,  we  took  advantage  of  these  existinganalyses  and  carry  out  the  annotation  of  dis-course phenomena directly on the trees (the tec-togrammatical layer).
It means that we capturethe  discourse  relation  between  any  two(sub)trees in the document by drawing a link (anarrow)  between  the  highest  nodes  in  the(sub)trees, see Figure 1.Figure 1.
A discourse arrow between two nodesrepresents a discourse relation between twotrees ?
subtrees of the nodes.Discourse relations we annotate are in princi-ple  semantic  relations  that  apply between twoabstract  objects  (Asher,  1993)  (i.e.
discourseunits or text spans) and help make the text a co-herent whole.
These relations are often signaledby the presence of a discourse connective, i.e.expressions  as  ?ale?,  a?koliv?,  ?tedy?,?ov?em?
(in  English  ?but?, ?although?,?then?, ?however?
etc.
In the first phase of theproject,  we  only  annotate  relations  (link  the(sub)trees) where such a connective is present.Every  relation  gets  assigned  two  importantattributes:  first,  the  discourse  connective  thatanchors the relation, and, second, the semantictype of the relation.
For assigning semantic rela-tions in the discourse, we developed a set of 22discourse-semantic tags (Mladov?
et al, 2009).It is inspired partly by the set of semantic labelsused for the annotation of the tectogrammaticallayer in  PDT 2.0, relations within the sentence(the  tectogrammatical  syntactico-semantic  la-bels  called  functors,  Mikulov?
et  al.,  2005)  ?since some of the semantic relations apply alsointra-sententially, like causal or contrastive rela-tions; and partly by the set of semantic tags inthe Penn Discourse Treebank 2.0 (Prasad et al,2008), a discourse annotation project for Eng-lish with similar aims.Hence,  there  are  three  important  issues  forthe  inter-annotator  measurement  on  the  dis-course  level  of  annotation  in  PDT:  the  agree-ment on the start  and target  nodes of the dis-course  relation  (and  so  the  extent  of  the  dis-course  arguments),  the  agreement  on  the  dis-course connective assigned to the relation, and,last but not least, the agreement on the semantictype of the relation.3 Measuring  the  Inter-AnnotatorAgreement in the Annotation of Dis-course in PDT 2.03.1 Simple (Strict) ApproachThe basic method we use for measuring the in-ter-annotator  agreement  requires  a  perfectmatch  in  the  start  and end points  of  the  rela-tions.
We  calculate  recall and  precision be-tween the two annotators.
Since these measuresare not symmetric in respect to the annotators,we use their combination ?
F1-measure ?
whichis symmetric.
At each node, we compare target776nodes of the discourse relations created by thetwo annotators.
We consider two relations to bein agreement strictly only if they share both thestart node and the target node.A second number  we measure  is  an agree-ment on the relation and the type.
For consider-ing two relations to be in agreement, we requirethat they share their start and target nodes, andalso have attached the same type.Similarly,  we measure an agreement on therelation and the connective,  and an agreementon the relation, the type and the connective.Attaching a type to a relation can be under-stood as a classification task.
We calculate twonumbers ?
simple ratio agreement and Cohen's?
?
on  the  types  attached  to  those  relationswhere the annotators agreed on the start and thetarget  nodes.
Cohen's  ?
shows  the  level  ofagreement on the types above chance.For  completeness,  we also  calculate  simpleratio agreement on the connectives attached tothose relations the annotators agreed on.Table 1 shows results of these measurementson two hundred sentences annotated in parallelby two annotators.1measure valueF1-measure on relations 0.43F1-measure on relations + types 0.34F1-measure on relations + connectives 0.41F1-measure on rel.
+ types + connect.
0.32agreement on types 0.8agreement on connectives 0.95Cohen's ?
on types 0.74Table 1.
The inter-annotator agreement for astrict match.3.2 Skipping a Tree LevelRequiring a perfect agreement on the start nodeand the  target  node  of  the  discourse  relationsturns out to be too strict for a fair evaluation of1 The annotators did not know which part of the data willbe used for the measurement.
The agreement was mea-sured on 200 sentences (6 documents).
PDT 2.0 containsdata from three sources.
The proportion of the sentencesselected for the measurement reflected the total proportionof these data sources in the whole treebank.the inter-annotator agreement.
It often happensthat the annotators recognize the same discourserelation in the data but they disagree either inthe start node or the target node of the relation.In  Zik?nov?
et  al.
(2010),  we  elaborate  ontypical cases of this type of disagreement andshow that in many times, the difference in thestart node or the target node is only one level inthe  tree.
We  have  also  shown that  these  dis-agreements usually depend on a subtle and notcrucial  difference  in  the  interpretation  of  thetext.Figure 2 shows an example of a disagreementcaused  by  a  one-level  difference  in  the  targetnode of a relation.
The two trees (a cut of them)represent these two sentences:?V?m, ?e se n?s Rus?
boj?te, ?e n?s nem?ter?di, ?e n?mi trochu pohrd?te.
Ale Rusko nen?jenom ?irinovskij, Rusko nen?
jenom vra?d?n?
v?e?ensku.?
(In English: ?I know that you are afraid of usRussians, that you dislike us, that you despiseus a little.
But Russia is not only Zhirinovsky,Russia is not only murdering in Chechnya.?
)Figure 2.
Disagreement in the target node.Both annotators recognized the discourse re-lation between the two sentences, both selectedthe same type (opposition), and both marked thesame connective (?Ale?, in English ?But?).
Thedisagreement in the target node is caused by thefact that one annotator has connected the secondsentence with ?knowing that something is goingon?,  while  the  other  has  connected  it  directlywith the expression?something is going on?.We have shown in Zik?nov?
et al (2010) thatallowing for skipping one tree level either at thestart  node  or  the  target  node  of  the  relationsleads to an improvement in the inter-annotator777agreement  (F1-measure on  the  relations)  ofabout 10%.
To be exact, by allowing to skip onetree  level  we mean:  if  node A is  a  parent  ofnode  B,  then  we  consider  arrows  A?C andB?C to  be  in  agreement,  as  well  as  arrowsD?A and D?B.
Table 2 shows present resultsof this type of measurement, performed on thesame data as Table 1.measure valueF1-measure on relations 0.54F1-measure on relations + types 0.43F1-measure on relations + connectives 0.49F1-measure on rel.
+ types + connect.
0.39agreement on types 0.8agreement on connectives 0.92Cohen's ?
on types 0.73Table 2.
The inter-annotator agreement withone-level skipping.The results  seem to be consistent,  since theimprovement  here  is  similar  to  the  previouslypublished test.
The F1-measure on the relationsimproved from 0.43 to 0.54.
On the other hand(and also  consistently  with  the  previous  test),simple  ratio  agreement  on  types  (or  connec-tives) and Cohen's  ?
on types, all measured onthose arrows the annotators  agreed on, do notchange (more or less) after skipping one level isallowed.
For these three measures, skipping onelevel only adds more data to evaluate and doesnot change conditions of the evaluation.3.3 Connective-Based ApproachFurther studies of discrepancies in parallel an-notations show that skipping one level does notcover all ?less severe?
cases of disagreement.Figure 3 presents an example of a disagree-ment in the start node of a relation with a two-level distance between the nodes.
The two trees(a cut of them) represent these two sentences:?Racion?ln?
kalkulace  vlastn?k?
n?jemn?chbyt?
je proto povede k jedin?mu z?v?ru: jak?ko-liv investice do oprav a modernizace n?jemn?hobytov?ho fondu jsou a budou ztr?tov?.
Proto jedal??
ch?tr?n?
n?jemn?ch dom?
neodvratn?.?
(In  English:  A  rational  calculation  of  theowners of the apartments will lead them to theonly conclusion: any investment in repairs andrenovation  of  the  rental  housing  resources  isand will be loss-making.
Therefore, further di-lapidation  of  the  apartment  buildings  is  in-evitable.?
)Figure 3.
Two-level disagreement in the startnodesThe difference between the annotators is thatone of  them started  the  relation  at  the  phrase?will  lead to  the  only  conclusion:  any invest-ment ... is and will be ...?, while the other start-ed the relation directly  at the  phrase  ?any in-vestment ?
is and will be ...?.However, both the annotators admittedly rec-ognized the existence of the discourse relation,they also selected the same type (reason), andmarked the same connective (?Proto?, in Eng-lish ?Therefore?
).Figure 4 shows an example of a disagreementcaused by a different selection of nodes and bythe opposite direction of the arrows.
The treesrepresent these sentences: ?To je jasn?, ?e bychbyl  rad?i,  kdyby  tady  dosud  st?l  z?mek  a  netohle  monstrum.
Ale  pro?
o  tom  st?leuva?ovat??
(In English:  It is clear that I would prefer ifthere still was a castle here and not this mon-ster.
But why keep thinking about it forever??
)778Figure 4.
Disagreement in the nodes and in thedirection of the arrows.This time, both annotators recognized a pres-ence  of  a  discourse  relation  and  marked  thesame  connective  (?Ale?,  in  English  ?But?
).They did not agree on the start/end nodes andon the type of the relation (opposition vs. con-cession).Figure 5 shows another type of ?slight?
dis-agreement.
This time, the annotators agreed oneverything but  the  range of the  relation.
Theyagreed both on the type (reason) and the con-nective (?tak?,  in English  ?Thus?).
The threetrees (again a cut of them) represent these threesentences:?Podle  ?
?fa  kancel?
?e  p?edstavenstva  a.  s.?koda Zde?ka Lavi?ky jsou v?ak v ?
?jnu schop-ny  fungovat  prakticky  v?echny  z?vody  bezv??n?j?
?ho omezen?.
To je v rozporu s tvrzen?mveden?
koncernu  z  minul?ho  t?dne,  ve  kter?m?kodov?ck?
management tvrdil, ?e se odst?vkadotkne v?t?iny provoz?
a z?vod?
?kody Plze?,kter?
m?
v  sou?asnosti  28000  zam?stnanc?.Vznik?
tak  podez?en?,  ?e  se  veden?
koncernusna?ilo vyvinout tlak na vl?du a donutit ji k za-placen?
dluh?.?
(In English:  ?According to Zden?k Lavi?ka,the  chief  of  the  board  of  directors  of  ?kodacorp., virtually all factories are able to operatein October without  serious limitations.
It  con-tradicts the statement of the syndicate adminis-tration from the last  week,  in which the man-agement  of  ?koda  claimed  that  the  downtimewould affect most of the plants and factories of?koda Plze?,  which  presently  has  28,000 em-ployees.
Thus a suspicion arises that the syndi-cate  administration tried to  exert  pressure onthe government and force it to pay the debts.?
)Figure 5.
Disagreement in the range of the dis-course relation.The difference between the annotators  is  inthe range of the start part of the arrows.
One ofthe annotators marked the two first sentences asa  start  point  of  the  relation,  while  the  othermarked  the  second sentence  as  the  start  pointonly.
They agreed on the target point of the rela-tion being the third sentence.Inspired by these examples, we designed an-other ?
a connective-based ?
measure for evalu-ating the inter-annotator agreement  of the dis-course relations.
It seems that although the an-notators  sometimes  fail  to  mark  the  samestart/target nodes, or to select the same type orthe  same  range  of  the  relations,  they  usuallyagree on the connective.
This idea is also sup-ported by high levels of the simple ratio agree-ment on connectives measured on relations theannotators agreed on from Tables 1 and 2 (0.95and 0.91).
These numbers  show that  once theannotators agree on a relation,  they almost al-ways agree also on the connective.2The connective-based measure considers theannotators to be in agreement on recognizing adiscourse relation if they agree on recognizingthe same connective (please note that we onlyannotate discourse relations with explicitly ex-pressed connectives).Table 3 shows results of the evaluation of theinter-annotator agreement, performed using theconnective-based measure, on the same data asTables 1 and 2.2 This is only an interpretation of the numbers, not adescription of the annotation process; in fact, the an-notators usually first find a connective and thensearch for the arguments of the discourse relation.779measure valueF1-measure on relations 0.86F1-measure on relations + types 0.56F1-measure on rel.
+ start/end nodes 0.43F1-measure on rel.
+ types + nodes 0.34agreement on types 0.65agreement on start/end nodes 0.50Cohen's ?
on types 0.56Table 3.
The inter-annotator agreement evaluat-ed with the connective-based measure.This time (compared with Tables 1 and 2, i.e.the simple strict measure and the one-level skip-ping measure),  the agreement (F1-measure) onrelations  is  much higher  ?
0.86 (vs.  0.43 and0.54).
On the other hand, simple ratio agreement(and Cohen's  ?)
measured  on  relations  recog-nized by both annotators are lower than in Ta-bles  1  and  2.
Although  the  annotators  mighthave recognized the same discourse relation, a(possibly small) difference in the interpretationof  the  text  caused  sometimes  not  only  a  dis-agreement  in  the  positions  of  the  start/endnodes, but also in the type of the relation.The  simple  ratio  agreement  on  types  fromTable 3 (0.65) is probably the closest measureto  the  way  of  measuring  the  inter-annotatoragreement on subtypes in the annotation of dis-course relations in the Penn Discourse Treebank2.0,  reported  in  Prasad  et  al.
(2008).
Theiragreement was 0.8.4 ConclusionWe have presented several ways of measuringthe inter-annotator agreement in the project  ofannotating  the  semantic  inter-sentential  dis-course relations with explicitly  expressed con-nectives  in  the  Prague  Dependency  Treebank.We have shown examples from parallel annota-tions that substantiate the importance of the al-ternative  approaches  to  the  evaluation  of  theagreement.Skipping a tree level in the start node or theend node of the relations helps to recognize fac-tual  agreement  in some cases  where the  strictapproach detects disagreement.
We have shownthat it is still too strict and that there are caseswhich we would like to  classify as agreementbut the measure does not recognize them.The  connective-based  measure  seems  to  bethe closest one to what we would like to consid-er a criterion of agreement.
It disregards the ac-tual nodes that are connected with a discourserelation, and even disregards the direction of therelation.
In this sense, it is the most benevolentof the three measures.It does not mean that the simple strict mea-sure or skipping a tree level are inferior or obso-lete ways of measuring the agreement.
All themeasures  focus  on  different  aspects  of  theagreement  and  they  are  all  important  in  theprocess  of annotating the corpus,  studying theparallel annotations and improving the annota-tion instructions.
We may agree on the fact thaton this level of language description, it is veryhard to  achieve perfect  agreement  (Lee at  al.,2006), yet we should never cease the effort tofurther specify and clarify the ways of annota-tion, in order to catch the same  linguistic phe-nomena in the same way, and thus provide sys-tematic and coherent linguistic data.AcknowledgmentsWe gratefully  acknowledge  support  from  theCzech  Ministry  of  Education  (grant  MSM-0021620838),  the  Grant  Agency of the  CzechRepublic  (grants  405/09/0729  andP406/2010/0875),  the Czech  Science  Founda-tion (grant 201/09/H057), and the Grant Agencyof  Charles  University  in  Prague  (GAUK103609).ReferencesArtstein R. and M. Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
ComputationalLinguistics 34/4, pp.
555?596.Asher,  N.  1993.
Reference  to  Abstract  Objects  inDiscourse.
Kluwer Academic PublishersCohen, J.
1960.
A coefficient of agreement for nomi-nal  scales.
Educational  and  Psychological  Mea-surement, 20(1), pp.
37?46.Haji?, J., Panevov?, J., Haji?ov?, E., Sgall, P., Pajas,P.,  ?t?p?nek,  J.,  Havelka,  J.,  Mikulov?,  M.,?abokrtsk?,  Z.,  and  M.
?ev??kov?-Raz?mov?.2006.
Prague  Dependency  Treebank  2.0.
CD-ROM,  LDC2006T01,  Linguistic  Data  Consor-tium, Philadelphia, USA.780Krippendorff, K. 1980.
Content Analysis: An Intro-duction  to  Its  Methodology.
Chapter  12.
Sage,Beverly Hills, CA, USA.Lee,  A.,  Prasad,  R.,  Joshi,  A.,  Dinesh,  N.,  and B.Weber.
2006.
Complexity of dependencies in dis-course: Are dependencies in discourse more com-plex than in syntax?
J. Haji?
and J. Nivre, (eds.
).Proceedings  of  the  5th Workshop on Treebanksand  Linguistic  Theories  (TLT  2006).
Prague,Czech Republic, pp.
79?90.Mikulov?,  M.  et  al.
2005:  Annotation  on  the  tec-togrammatical  layer  in  the  Prague DependencyTreebank.
Annotation  manual.
Avaiable  fromhttp://ufal.mff.cuni.cz/pdt2.0/doc/manuals/en/t-layer/pdf/t-man-en.pdfMladov?
L., Zik?nov?, ?., Bed?ichov?, Z., and E.Haji?ov?.
2009.
Towards a Discourse Corpus ofCzech.
Proceedings of the Corpus LinguisticsConference, Liverpool, Great Britain, in press(online proceedings: http://ucrel.lancs.ac.uk/publi-cations/cl2009/).Passonneau,  R.  2004.
Computing  Reliability  forCoreference.
Proceedings  of  LREC, vol.
4,  Lis-bon,  Portugal, pp.
1503?1506.Prasad, R., Dinesh N., Lee A., Miltsakaki, E., Robal-do, L., Joshi, A., Webber, B.
2008.
The Penn Dis-course Treebank 2.0.
Proceedings of the 6th Inter-national Conference on  Language Resources andEvaluation (LREC 2008), Morocco.Zik?nov?
?.,  Mladov?
L.,  M?rovsk?
J.,  and  P.J?nov?.
2010.
Typical Cases of Annotators?
Dis-agreement  in  Discourse  Annotations  in  PragueDependency  Treebank.
LREC  2010,  Malta,  inpress.781
