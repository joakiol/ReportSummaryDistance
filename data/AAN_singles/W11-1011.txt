Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 98?106,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsAutomatic Category Label Coarsening for Syntax-Based MachineTranslationGreg Hanneman and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{ghannema, alavie}@cs.cmu.eduAbstractWe consider SCFG-basedMT systems that getsyntactic category labels from parsing boththe source and target sides of parallel train-ing data.
The resulting joint nonterminals of-ten lead to needlessly large label sets that arenot optimized for an MT scenario.
This pa-per presents a method of iteratively coarseninga label set for a particular language pair andtraining corpus.
We apply this label collaps-ing on Chinese?English and French?Englishgrammars, obtaining test-set improvements ofup to 2.8 BLEU, 5.2 TER, and 0.9 METEORon Chinese?English translation.
An analysisof label collapsing?s effect on the grammarand the decoding process is also given.1 IntroductionA common modeling choice among syntax-basedstatistical machine translation systems is the use ofsynchronous context-free grammar (SCFG), where asource-language string and a target-language stringare produced simultaneously by applying a series ofre-write rules.
Given a parallel corpus that has beenstatistically word-aligned and annotated with con-stituency structure on one or both sides, SCFG mod-els for MT can be learned via a variety of methods.Parsing may be applied on the source side (Liu et al,2006), on the target side (Galley et al, 2004), or onboth sides of the parallel corpus (Lavie et al, 2008;Zhechev and Way, 2008).In any of these cases, using the raw label set fromsource- and/or target-side parsers can be undesir-able.
Label sets used in statistical parsers are usu-ally inherited directly from monolingual treebankprojects, where the inventory of category labels wasdesigned by independent teams of human linguists.These labels sets are not necessarily ideal for sta-tistical parsing, let alne for bilingual syntax-basedtranslation models.
Further, the side(s) on whichsyntax is represented defines the nonterminal labelspace used by the resulting SCFG.
A pair of alignedadjectives, for example, may be labeled ADJ if onlysource-side syntax is used, JJ if only target-side syn-tax is used, or ADJ::JJ if syntax from both sidesis used in the grammar.
Beyond such differences,however, most existing SCFG-based MT systemsdo not further modify the nonterminal label set inuse.
Those that do require either specialized de-coders or complicated parameter tuning, or the la-bel set may be unsatisfactory from a computationalpoint of view (Section 2).We believe that representing both source-side andtarget-side syntax is important.
Even assuming twomonolingually perfect label sets for the source andtarget languages, using label information from onlyone side ignores any meaningful constraints ex-pressed in the labels of the other.
On the other hand,using the default node labels from both sides gener-ates a joint nonterminal set of thousands of uniquelabels, not all of which may be useful.
Our real pref-erence is to use a joint nonterminal set adapted toour particular language pair or translation task.In this paper, we present the first step towardsa tailored label set: collapsing syntactic categoriesto remove the most redundant labels and shrink theoverall source?target nonterminal set.1 There are1The complementary operation, splitting existing labels, isbeyond the scope of this paper and is left for future work.98two problems with an overly large label set:First, it encourages labeling ambiguity amongrules, a well-known practical problem in SCFG-based MT.
Most simply, the same right-hand sidemay be observed in rule extraction with a variety ofleft-hand-side labels, each leading to a unique rulein the grammar.
The grammar may further containmany rules with the same structure and reorderingpattern that differ only with respect to the actual la-bels in use.
Together, these properties can cause anSCFG-based MT system to process a large numberof alternative syntactic derivations that use differentrules but produce identical output strings.
Limitingthe possible number of variant labelings cuts downon ambiguous derivations.Second, a large label set leads to rule sparsity.
Arule whose right-hand side can only apply on a verytightly specified set of labels is unlikely to be es-timated reliably from a parallel corpus or to applyin all needed cases at test time.
However, a coarserversion of its application constraints may be morefrequently observed in training data and more likelyto apply on test data.We therefore introduce a method for automati-cally clustering and collapsing category labels, oneither one or both sides of SCFG rules, for any lan-guage pair and choice of statistical parsers (Section3).
Turning to alignments between source and tar-get parse nodes as an additional source of informa-tion, we calculate a distance metric between anytwo labels in one language based on the differencein alignment probabilities to labels in the other lan-guage.
We then apply a greedy label collapsing al-gorithm that repeatedly merges the two labels withthe closest distance until some stopping criterion isreached.
The resulting coarsened labels are used inthe SCFG rules of a syntactic machine translationsystem in place of the original labels.In experiments on Chinese?English translation(Section 4), we find significantly improved perfor-mance of up to 2.8 BLEU points, 5.2 TER points,and 0.9 METEOR points by applying varying de-grees of label collapsing to a baseline syntax-basedMT system (Section 5).
In our analysis of the results(Section 6), we find that the largest immediate effectof coarsening the label set is to reduce the number offully abstract hierarchical SCFG rules present in thegrammar.
These rules?
increased permissiveness, inturn, directs the decoder?s search into a largely dis-joint realm from the search space explored by thebaseline system.
A full summary and ideas for fu-ture work are given in Section 7.2 Related WorkOne example of modifying the SCFG nonterminalset is seen in the Syntax-Augmented MT (SAMT)system of Zollmann and Venugopal (2006).
InSAMT rule extraction, rules whose left-hand sidescorrespond exactly to a target-side parse node t re-tain that label in the grammar.
Additional nontermi-nal labels of the form t1+ t2 are created for rulesspanning two adjacent parse nodes, while catego-rial grammar?style nonterminals t1/t2 and t1\t2 areused for rules spanning a partial t1 node that is miss-ing a t2 node to its right or left.These compound nonterminals in practice lead toa very large label set.
Probability estimates for ruleswith the same structure up to labeling can be com-bined with the use of a preference grammar (Venu-gopal et al, 2009), which replaces the variant label-ings with a single SCFG rule using generic ?X?
la-bels.
The generic rule?s ?preference?
over possiblelabelings is stored as a probability distribution insidethe rule for use at decoding time.
Preference gram-mars thus reduce the label set size to one for the pur-poses of some feature calculations ?
which avoidsthe fragmentation of rule scores due to labeling am-biguity ?
but the original labels persist for specify-ing which rules may combine with which others.Chiang (2010) extended SAMT-style labels toboth source- and target-side parses, also introducinga mechanism by which SCFG rules may apply at runtime even if their labels do not match.
Under Chi-ang?s soft matching constraint, a rule headed by a la-bel A::Z may still plug into a substitution site labeledB::Y by paying additional model costs substB?Aand substY?Z .
This is an on-the-fly method ofcoarsening the effective label set on a case-by-casebasis.
Unfortunately, it also requires tuning a sep-arate decoder feature for each pair of source-sideand each pair of target-side labels.
This tuning canbecome prohibitively complex when working withstandard parser label sets, which typically containbetween 30 and 70 labels on each side.99JJ JJR JJSFigure 1: Alignment distributions over French labels for the English adjective labels JJ, JJR, and JJS.3 Label Collapsing AlgorithmWe begin with an initial set of SCFG rules extractedfrom a parallel parsed corpus, where S denotes theset of labels used on the source side and T denotesthe set of labels used on the target side.
Each rule hasa left-hand side of the form s :: t, where s ?
S andt ?
T , meaning that a node labeled s was aligned toa node labeled t in a parallel sentence.
From the left-hand sides of all extracted rule instances, we com-pute label alignment distribution P (s | t) by simplecounting and normalizing:P (s | t) =#(s :: t)#(t)(1)We use an analogous equation to calculate P (t | s).For two target-language labels t1 and t2, we havean equally simple metric of alignment distributiondifference d: the total of the absolute differences inlikelihood for each aligned source-language label.d(t1, t2) =?s?S|P (s | t1) ?
P (s | t2)| (2)Again, the calculation for d(s1, s2) is analogous.If t1 and t2 are plotted as points in |S|-dimensional space such that each point?s position indimension s is equal to P (s | t), then this metric isequivalent to the L1 distance between t1 and t2.Sample alignment distributions into French forthree English adjective labels are shown in Figure1.
Bars in the chart represent alignment probabili-ties between French and English according to Equa-tion 1, with the various French labels as s and JJ,JJR, or JJS as t. To compute an L1 alignment dis-tribution difference between a pair of English ad-jective tags, we sum the absolute differences in barheights for each column of two graphs, as in Equa-tion 2.
It is already visually clear from Figure 1that all three English labels are somewhat relatedin terms of distribution, but it appears that JJR andJJS are more closely related to each other than eitheris to JJ.
This is reflected in the actual L1 distances:d(JJ, JJR) = 0.9941 and d(JJ, JJS) = 0.8730, butd(JJR, JJS) = 0.3996.Given the above method for computing an align-ment distribution difference for any pair of labels,we develop an iterative greedy method for label col-lapsing.
At each step, we compute the L1 distancebetween all pairs of labels, then collapse the pairwith the smallest distance into a single label.
ThenL1 distances are recomputed over the new, smallerlabel set, and again the label pair with the smallestdistance is collapsed.
This process continues untilsome stopping criterion is reached.
Label pairs be-ing considered for collapsing may be only source-side labels, only target-side labels, or both.
In gen-eral, we choose to allow label collapsing to apply oneither side during each iteration of our algorithm.In the limit, label collapsing can be applied it-eratively until all syntactic categories on both thesource and target sides have been collapsed into asingle label.
In Section 5, we explore several earlierand more meaningful stopping points.4 Experimental SetupExperiments are conducted on Chinese-to-Englishtranslation using approximately 300,000 sentencepairs from the FBIS corpus.
To obtain parse treesover both sides of each parallel corpus, we usedthe English and Chinese grammars of the Berkeley100parser (Petrov and Klein, 2007).Given a parsed and word-aligned parallel sen-tence, we extract SCFG rules from it following theprocedure of Lavie et al (2008).
The method firstidentifies node alignments between the two parsetrees according to support from the word alignments.A node in the source parse tree will be aligned toa node in the target parse tree if all the words inthe yield of the source node are either all aligned towords within the yield of the target node or have noalignments at all.
Then SCFG rules can be extractedfrom adjacent levels of aligned nodes, which spec-ify points at which the tree pair can be decomposedinto minimal SCFG rules.
In addition to producinga minimal rule, each decomposition point also pro-duces a phrase pair rule with the node pair?s yieldsas the right-hand side, as long as the length of theyield is less than a specified threshold.Following grammar extraction, labels are option-ally clustered and collapsed according to the algo-rithm in Section 3.
The grammar is re-written withthe modified nonterminals, then scored as usual ac-cording to our translation model features.
Featureweights themselves are learned via minimum errorrate training as implemented in Z-MERT (Zaidan,2009) with the BLEU metric (Papineni et al, 2002).Decoding is carried out with Joshua (Li et al, 2009),an open-source platform for SCFG-based MT.Due to engineering limitations in decoding witha large grammar, we apply three additional error-correction and filtering steps to every system.
First,we observed that the syntactic parsers were mostlikely to make labeling errors for cardinal numbersin English and punctuation marks in all languages.We thus post-process the parses of our training datato tag all English cardinal numbers as CD and tooverwrite the labels of various punctuation markswith the correct labels as defined by each language?slabel set.
Second, after rule extraction, we com-pute the distribution of left-hand-side labels for eachunique labeled right-hand side in the grammar, andwe remove the labels in the least frequent 10% of thedistribution.
This puts a general-purpose limit on la-beling ambiguity.
Third, we filter and prune the finalscored grammar to each individual development andtest set before decoding: all matching phrase pairsare retained, along with the most frequent 10,000 hi-erarchical grammar rules.5 Experiments and ResultsIn our first set of experiments, we sought to explorethe effect of increasing degrees of label collapsingon a baseline system and to determine a reasonablestopping point.
Starting with the baseline grammar,we ran the label collapsing algorithm of Section 3until all the constituent labels on each side had beencollapsed into a single category.
We next examinedthe L1 distances between the label pairs that hadbeen merged in each iteration of the algorithm.
Thisdata is shown in Figure 2 as a plot of L1 distanceversus iteration number.
The distances between thesuccessive labels merged in the first 29 iterations ofthe algorithm are nearly monotonically increasing,followed by a much larger discontinuity at iteration30.
Similar patterns emerge for iterations 30 to 45and for iterations 46 to 60.
The next regions of thegraph, from iterations 61 to 81 and from iterations82 to 99, show an increasing prevalence of disconti-nuities.
Finally, from iterations 100 to 123, the suc-cessive L1 distances entirely alternate between veryhigh and very low values.Discontinuities are merely the result of a labelpair in one language suddenly scoring much loweron the distribution difference metric than previously,thanks to some change that has occurred in the la-bel set of the other language.
Looking back to Fig-ure 1, for example, we could bring the distributionsfor JJ and JJS much closer together by merging Aand ADV on the French side.
Although such suddendrops in distribution difference value are expected,they may provide an indication of when the labelcollapsing algorithm has progressed too far, sincewe have so reduced the label set that categories pre-viously very different have become much less dis-tinguishable.
On the other hand, further reduction ofthe label set may have a variety of pratical benefits.We tested this trade-off empirically by buildingfive Chinese?English MT systems, each exhibitingan increasing degree of label collapsing compared tothe original label set, which serves as our baseline.The degree of label collapsing in each of the fivesystems corresponds to one of the major discontinu-ity features highlighted in the right-hand side Figure2.
The systems were tuned on the NIST MT 2006data set, and we evaluated performance on the NISTMT 2003 and 2008 sets.
(All data sets have four101Iter.
L1 Dist.29 0.364645 0.560760 0.615581 0.866599 1.1303Figure 2: Observed L1 distance values for the labels merged in each iteration of our algorithm on a Chinese?EnglishSCFG.
We divide the graph into six distinct regions using the cutoffs at right.Chinese?English MT 2003 Test Set MT 2008 Test SetSystem METEOR BLEU TER METEOR BLEU TERBaseline 54.35 24.39 68.01 45.68 18.27 69.18Collapsed, 29 iterations 55.24 27.03 63.77 46.25 19.78 65.88Collapsed, 45 iterations 54.65 26.69 62.76 46.02 19.60 64.88Collapsed, 60 iterations 55.11 27.23 63.06 46.30 20.19 65.18Collapsed, 81 iterations 54.87 26.87 64.92 45.70 20.48 66.75Collapsed, 99 iterations 54.86 26.16 64.17 45.87 19.52 65.61Table 1: Results of applying increasing degrees of label collapsing on our Chinese?English baseline system.
Boldfigures indicate the best score in each column.references.)
Table 1 reports automatic metric resultsfor version 1.0 of METEOR (Lavie and Denkowski,2009) using the default settings, uncased IBM-styleBLEU (Papineni et al, 2002), and uncased TER ver-sion 0.7 (Snover et al, 2006).No matter the degree of label collapsing, we findsignificant improvements in BLEU and TER scoreson both test sets.
On the MT 2003 set, label-collapsed systems score 1.77 to 2.84 BLEU pointsand 3.09 to 5.25 TER points better than the baseline.OnMT 2008, improvements range from 1.25 to 2.21points on BLEU and from 2.43 to 4.30 points onTER.
Improvements on both sets according to ME-TEOR, though smaller, are still noticable (up to 0.89points).
In the case of BLEU, we verified the sig-nificance of the improvements by conducting pairedbootstrap resampling (Koehn, 2004) on theMT 2003output.
With n = 1000 and p < 0.05, all five label-collapsed systems were statistically significant im-provements over the baseline, and all other collapsedsystems were significant improvements over the 99-iteration system.Thus, though the system that provides the highestscore changes across metrics and test sets, the over-all pattern of scores suggests that over-collapsing la-bels may start to weaken results.
A more moderatestopping point is thus preferable, but beyond that wesuspect the best result is determined more by the testset, automatic metric choice, and MERT instabilitythan systematic changes in the label set.6 AnalysisTable 1 showed a strong practical benefit to runningthe label collapsing algorithm.
In this section, we102seek to further understand where this benefit comesfrom, tracing the effects of label collapsing via itsmodification of labels themselves, the differences inthe resulting grammars, and collapsing?s effect ondecoding and output.6.1 Labels Selected for CollapsingOur first concern is for the size of the grammar?soverall nonterminal set.
The baseline system uses atotal of 55 labels on the Chinese side and 71 on theEnglish side, leading to an observed joint nontermi-nal set of 1556 unique labels.
After 29 iterationsof label collapsing, this is reduced to 46 Chinese,51 English, and 1035 joint labels ?
a reduction of33%.
In the grammar of our most collapsed gram-mar variant (99 iterations), the nonterminal set is re-duced to 14 English and 14 Chinese labels, for a to-tal of 106 joint labels and a reduction of 93% fromthe baseline grammar.
This demonstrates one facetof our introductory claim from Section 1: since wehave improved translation results by removing thevast majority of our grammar nonterminals, most ofthe initial joint Chinese?English syntactic categorieswere not necessary for Chinese?English translation.We identify three broad trends in the sets of labelsthat are collapsed:?
Full Subtype Collapsing.
The Chinese-sideparses include six phrase-level tags for varioustypes of verb compounds.
As label collapsingprogresses, these labels are all combined witheach other at relatively low L1 distances.?
Partial Subtype Collapsing.
In English, threeof the four noun labels (NN, NNS, and NNPS)form a cohesive cluster early on in Chinese?English collapsing.
However, the fourth tag(NNP, for singular proper nouns) remains sep-arate, then later joins a cluster for moreadjective-like labels.?
Combination by Syntactic Function.
InFrench?English label collapsing (see below),we find the creation of a combined label inEnglish for reduced relative clauses (RRC),adjective phrases headed by a wh-adjective(WHADJP), and interjections (INTJ).
Eventhough these tags are unrelated in surface form,at some level they all represent parenthetical in-sertions or explanatory phrases.The formulation of the L1 distance metric in Sec-tion 3 means that our label collapsing algorithm willnaturally produce different label clusters for differ-ent input grammars ?
any change in the Viterbiword alignments, underlying parallel corpus, initiallabel set, or choice of automatic parser will neces-sarily change the label alignment distributions onwhich the collapsing algorithm is based.
In par-ticular, the label clusters formed in one languageare likely to be markedly different depending onwhich other language it is paired with.
We exam-ine these differences in more detail for the case ofEnglish when paired with either Chinese or withFrench.
Our 29-iteration run of label collapsing forChinese?English merged labels on the English side19 times.
For an exact comparison, we run iterationsof label collapsing on a large-scale French?Englishgrammar, extracted in the same way as the Chinese?English grammar, until the same number of English-side merges have been carried out, then examine theresults.Table 2 shows the English label clusters cre-ated from the Chinese?English and French?Englishgrammars, arranged by broad syntactic categories.The differences in English label clusters hint at dif-ferences in the source-side label sets, as well asstructural divergences relevant for translating Chi-nese versus French into English.For example, Table 2 shows partial subtype col-lapsing of the English verb tags when paired withFrench.
The French Berkeley parser has a single tag,V, to represent all verbs, and most English verb tagsas well as the tag for modals very consistently alignto it.
The exception is VBG, for present-progressiveor gerundive verb forms, which is more easily con-flatable in French?English translation with a noun oran adjective.
In translation from Chinese, however,it is VBG that is combined early on with a smallerselection of English verb labels that correspond moststrongly to a basic Chinese verb.
Other English verbtags are more likely to align to Chinese copulas, ex-istential verbs, and nouns; they are not combinedwith the group for more ?typical?
verbs until itera-tion 67.
The adverb series presents another exampleof translational divergence between language pairs.103Cluster Chinese?English French?EnglishNouns NN NNS NNPS # NN NNS $Verbs VB VBG VBN VB VBD VBN VBP VBZ MDAdverbs RB RBR RBR RBSPunctuation LRB RRB ?
?
, .
?
?Prepositions IN TO SYMDeterminers DT PRP$Noun phrases NP NX QP UCP NAC NP WHNP NX WHADVP NACAdjective phrases ADJP WHADJPAdverb phrases ADVP WHADVPPrepositional phrases PP WHPPSentences S SINV SBARQ FRAG S SQ SBARQTable 2: English-side label clusters created after partial label collapsing of a Chinese?English and a French?Englishgrammar.
In each case, the algorithm has been run until merges have occurred 19 times on the English side.6.2 Effect on the GrammarWith a smaller label set, we also expect a reduc-tion in the overall size of our various label-collapsedgrammars as labeling ambiguity is removed.
In theaggregate, however, even 99 iterations of Chinese?English label collapsing has a minimal effect onthe total number of unique rules in the resultingSCFG.
A clearer picture emerges when we sepa-rate rules according to their form.
Figure 3 parti-tions the grammar into three parts: one for phrasepairs, where the rules?
right-hand sides are made upentirely of terminals (?P-type?
rules); one for hier-archical rules whose right-hand sides are made upentirely of nonterminals (abstract or ?A-type?
rules);and one for hierarchical rules whose right-hand sidesinclude a mix of terminals and nonterminals (re-maining grammar or ?G-type?
rules).This separation reveals two interesting facts.First, although the size of the label set continuesto shrink considerably between iterations 29 and 81,the number of unique rules in the grammar remainsrelatively unchanged.
Second, the reduction in thesize of the grammar is largely due to a reduction inthe number of fully abstract grammar rules, ratherthan phrase pairs or partially lexicalized grammarrules.
From these observations, we infer that the ma-jor practical benefit of label collapsing is a reductionin rule sparsity rather than a reduction in left-hand-side labeling ambiguity.
Many highly ambiguousrules have had their possible left-hand-side labels ef-fectively pruned down by the pre-processing stepswe described in Section 4, which in preliminary ex-Figure 3: The effect of label collapsing on the number ofunique phrase pairs, partially lexicalized grammar rules,and fully abstract grammar rules.periments had a larger effect on the overall size ofthe grammar than label collapsing.
As a more com-plementary technique, increasing the applicability ofthe fully abstract rules via label collapsing is impor-tant for performance.
Such rules make up 49% to59% of the hierarchical rules retained at decodingtime, and they account for 76% to 87% of the ruleapplication instances on the MT 2003 test set.6.3 Effect on Decoding and OutputInterestingly, the label collapsing algorithm doesnot owe its success at decoding time to a signif-icant increase in the number of rule applications.Among our systems, both the 45-iteration and the10460-iteration collapsed versions scored highly ac-cording to automatic metrics.
Nevertheless, the 45-iteration system used 32% and 38% more rule appli-cations than the baseline on the MT 2003 and MT2008 test sets, respectively, while the 60-iterationsystem used 15% and 11% fewer.
The number ofunique rule types and the number of reordering rulesapplied on a test set may also go up or down.Instead, the practical effect of making the gram-mar more permissive seems to be a significantchange in the search space explored during decod-ing.
This can be seen superficially via an exam-ination of output n-best lists.
On both test setscombined (2276 sentences), the 60-iteration label-collapsed system?s top-best output appears in thebaseline?s 100-best list in only 81 sentences.
Whenit does appear in the baseline, the improved system?stranslation is ranked fairly highly ?
always 30thplace or higher.
Conversely, the baseline?s top-bestoutput tends to be ranked lower in the improved sys-tem?s n-best list: among the 114 times it appears, itis placed as low as 87th.We ran a small follow-up analysis on the transla-tion fragments explored during decoding.
Using amodified version of the Joshua decoder, we dumpedlists of hypergraph entries that were explored bycube pruning during Joshua?s lazy generation of a100-best list.
These entries represent the decoder?sapproximative search through the larger space oftranslations licenced by the grammar for each testsentence.
We then compared the hypergraph entries,excluding glue rules, produced on the first 100 sen-tences of the MT 2003 test set by both the baselineand the 60-iteration label-collapsed system.A full 90% of the entries produced by the label-collapsed system had no analogue in the baselinesystem.
The average length of the entries that domatch is 2.3 source words, compared with an aver-age of 6.2 words for the non-matched entries.
Webelieve that the increased permissiveness of the hi-erarchical grammar rules is again the root cause ofthese results.
Low-level constituents are more likelyto be matched in both the baseline and the label-collapsed system, but different applications of thegrammar rules, perhaps combined with retuned fea-ture weights, leads the search for larger translationfragments into new areas.7 Conclusions and Future WorkThis paper has presented a language-specific methodfor automatically coarsening the label set used inan SCFG-based MT system.
Our motivation forcollapsing labels comes from the intuition that thefull cross-product of joint source?target labels, asproduced by statistical parsers, is too large and notspecifically created for bilingual MT modeling.
Thegreedy collapsing algorithm we developed is basedon iterative merging of the two single-language la-bels whose alignment distributions are most similaraccording to a simple L1 distance metric.In applying varying degrees of label collapsing toa baseline MT system, we found significantly im-proved automatic metric results even when the sizeof the joint label set had been reduced by 93%.
Thebest results, however, were obtained with more mod-erate coarsening.
The coarser labels that our methodproduces are syntactically meaningful and representspecific cross-language behaviors of the languagepair involved.
At the grammar level, label collaps-ing primarily caused a reduction in the number ofrules whose right-hand sides are made up entirely ofnonterminals.
The coarser labels made the grammarmore permissive, cutting down on the problem ofrule sparsity.
Labeling ambiguity, on the other hand,was more effectively addressed by pre-processingwe applied to the grammar beforehand.
At run time,the more permissive collapsed grammar allowed thedecoder to search a markedly different region of theallowable translation space than in the baseline sys-tem, generally leading to improved output.One shortcoming of our current algorithm is thatit is based entirely on label alignment distributionwithout regard to the different contexts in which la-bels occur.
It thus cannot distinguish between twolabels that align similarly but appear in very differentrules.
For example, singular common nouns (NN)and plural proper nouns (NNPS) in English bothmost frequently align to French nouns (N) and arethus strong candidates for label collapsing under ouralgorithm.
However, when building noun phrases,an N::NNPS will more likely require a rule to deletea French-side determiner, while an N::NN will typ-ically require a determiner in both French and En-glish.
Thus, collapsing NN and NNPS may lead toadditional ambiguity or incorrect choices when ap-105plying larger rules.Another dimension to be explored is the trade-offbetween greedy collapsing and other methods thatcluster all labels at once.
K-means clustering couldbe a reasonable contrast in this respect; its down-side would be that all labels in one language mustbe assigned to clusters without knowledge of whatclusters are being formed in the other language.Finally, label collapsing is only the first step in abroader exploration of SCFG labeling for MT.
Wealso plan to investigate methods for refining exist-ing category labels in order to find finer-grained sub-types that are useful for translating a particular lan-guage pair.
By running label collapsing and refiningtogether, our end goal is to be able to adapt standardparser labels to individual translation scenarios.AcknowledgmentsThis research was supported in part by U.S. Na-tional Science Foundation grants IIS-0713402 andIIS-0915327 and by the DARPA GALE program.Thanks to Chris Dyer for providing the word-aligned and preprocessed FBIS corpus we used inour Chinese?English experiments, and to Jon Clarkfor suggesting and setting up the hypergraph com-parison analysis.
We also thank Yahoo!
for the useof the M45 research computing cluster, where weran many steps of our experimental pipeline.ReferencesDavid Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452, Uppsala, Sweden, July.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InHLT-NAACL 2004: Main Proceedings, pages 273?280, Boston, MA, May.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395, Barcelona, Spain, July.Alon Lavie and Michael J. Denkowski.
2009.
TheMETEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proceedings of the Second ACL Work-shop on Syntax and Structure in Statistical Transla-tion, pages 87?95, Columbus, OH, June.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren N.G.
Thornton, Jonathan Weese, and Omar F.Zaidan.
2009.
Joshua: An open source toolkit forparsing-based machine translation.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, pages 135?139, Athens, Greece, March.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the ACL, pages 609?616, Sydney, Aus-tralia, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eva-lution of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA,July.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411, Rochester, NY, April.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProceedings of the Seventh Conference of the Associ-ation for Machine Translation in the Americas, pages223?231, Cambridge, MA, August.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars: Soft-ening syntactic constraints to improve statistical ma-chine translation.
In Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the ACL, pages 236?244, Boulder, CO,June.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Ventsislav Zhechev and Andy Way.
2008.
Automaticgeneration of parallel treebanks.
In Proceedings of the22nd International Conference on Computational Lin-guistics, pages 1105?1112, Manchester, England, Au-gust.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, pages 138?141, New York, NY, June.106
