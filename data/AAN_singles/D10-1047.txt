Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 482?491,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsMulti-document summarization using A* search and discriminative trainingAhmet Aker Trevor CohnDepartment of Computer ScienceUniversity of Sheffield, Sheffield, S1 4DP, UK{a.aker, t.cohn, r.gaizauskas}@dcs.shef.ac.ukRobert GaizauskasAbstractIn this paper we address two key challengesfor extractive multi-document summarization:the search problem of finding the best scoringsummary and the training problem of learn-ing the best model parameters.
We propose anA* search algorithm to find the best extractivesummary up to a given length, which is bothoptimal and efficient to run.
Further, we pro-pose a discriminative training algorithm whichdirectly maximises the quality of the best sum-mary, rather than assuming a sentence-leveldecomposition as in earlier work.
Our ap-proach leads to significantly better results thanearlier techniques across a number of evalua-tion metrics.1 IntroductionMulti-document summarization aims to presentmultiple documents in form of a short summary.This short summary can be used as a replacementfor the original documents to reduce, for instance,the time a reader would spend if she were to readthe original documents.
Following dominant trendsin summarization research (Mani, 2001), we focussolely on extractive summarization which simplifiesthe summarization task to the problem of identify-ing a subset of units from the document collection(here sentences) which are concatenated to form thesummary.Most multi-document summarization systems de-fine a model which assigns a score to a candidatesummary based on the features of the sentences in-cluded in the summary.
The research challenges arethen twofold: 1) the search problem of finding thebest scoring summary for a given document set, and2) the training problem of learning the model pa-rameters to best describe a training set consisting ofpairs of document sets with model or reference sum-maries ?
typically human authored extractive or ab-stractive summaries.Search is typically performed by a greedy al-gorithm which selects each sentence in decreasingorder of model score until the desired summarylength is reached (see, e.g., Saggion (2005)) or us-ing heuristic strategies based on position in docu-ment or lexical clues (Edmundson, 1969; Brandowet al, 1995; Hearst, 1997; Ouyang et al, 2010).1We show in this paper that the search problem canbe solved optimally and efficiently using A* search(Russell et al, 1995).
Assuming the model only usesfeatures local to each sentence in the summary, ouralgorithm finds the best scoring extractive summaryup to a given length in words.Framing summarization as search suggests thatmany of the popular training techniques are max-imising the wrong objective.
These approaches traina classifier, regression or ranking model to distin-guish between good and bad sentences under anevaluation metric, e.g., ROUGE (Lin, 2004).
Themodel is then used during search to find a summarycomposed of high scoring (?good?)
sentences (seefor a review Ouyang et al (2010)).
However, thereis a disconnect between the model used for trainingand the model used for prediction.
In this paper wepresent a solution to this disconnect in the form ofa training algorithm that optimises the full predic-tion model directly with the search algorithm intact.The training algorithm learns parameters such that1Genetic algorithms have also been devised for solving thesearch problem (see, e.g., Riedhammer et al (2008)), howeverthese approaches do not guarantee optimality, nor are they effi-cient enough to be practicable for large datasets.482the best scoring whole summary under the modelhas a high score under the evaluation metric.
Wedemonstrate that this leads to significantly better testperformance than a competitive baseline, to the tuneof 3% absolute increase for ROUGE-1, -2 and -SU4.The paper is structured as follows.
Section 2presents the summarization model.
Next in sec-tion 3 we present an A* search algorithm for findingthe best scoring (argmax) summary under the modelwith a constraint on the maximum summary length.We show that this algorithm performs search effi-ciently, even for very large document sets composedof many sentences.
The second contribution of thepaper is a new training method which directly opti-mises the summarization system, and is presented insection 4.
This uses the minimum error-rate training(MERT) technique from machine translation (Och,2003) to optimise the summariser?s output to an ar-bitrary evaluation metric.
Section 5 describes ourexperimental setup and section 6 the results.
Finallywe conclude in section 7.2 Summarization ModelExtractive multi-document summarization aims tofind the most important sentences from a set of doc-uments, which are then collated and presented tothe user in form of a short summary.
Followingthe predominant approach to data-driven summari-sation, we define a linear model which scores sum-maries as the weighted sum of their features,s(y|x) = ?
(x,y) ?
?
, (1)where x is the document set, composed of k sen-tences, y ?
{1 .
.
.
k} are the set of selected sen-tence indices, ?
(?, ?)
is a feature function which re-turns a vector of features for the candidate summaryand ?
are the model parameters.
We further assumethat the features decompose with the sentences inthe summary, ?
(x,y) =?i?y ?
(xi), and there-fore the scoring function also decomposes along thesame lines,s(y|x) =?i?y?
(xi) ?
?
.
(2)While this assumption greatly simplifies inference, itdoes constrain the representative power of the modelby disallowing global features, e.g., those whichmeasure duplication in the summary.2 Under thismodel, the search problem is to solvey?
= arg maxys(y|x) , (3)for which we develop a best-first algorithm using A*search, as described in section 3.
The training chal-lenge is to find the parameters, ?, to best model thetraining set.
This is achieved by finding ?
such thaty?
is similar to the gold standard summary accord-ing to an automatic evaluation metric, as describedin section 4.3 A* SearchThe prediction problem is to find the best scoringextractive summary (see Equation 3) up to a givenlength, L. At first glance, this appears to be a sim-ple problem that might be solved efficiently with agreedy algorithm, say by taking the sentences in or-der of decreasing score and stopping just before thesummary exceeds the length threshold.
However,the greedy algorithm cannot be guaranteed to findthe best summary; to do so requires arbitrary back-tracking to revise previous incorrect decisions.The problem of constructing the summary can beconsidered a search problem in which we start withan empty summary and incrementally enlarge thesummary by concatenating a sentence from our doc-ument set.
The search graph starts with an emptysummary (the starting state) and each outgoing edgeadds a sentence to produce a subsequent state, andis assigned a score under the model.
A goal state isany state with no more words than the given thresh-old.
The summarisation problem is then equivalentto finding the best scoring path (summed over theedge scores) between the start state and a goal state.The novel insight in our work is to use A* search(Russell et al, 1995) to solve the prediction prob-lem.
A* is a best-first search algorithm which canefficiently find the best scoring path or the n-bestpaths (unlike the greedy algorithm which is not op-timal, and the backtracking variant which is not ef-ficient).
The search procedure requires a scoringfunction for each state, here s(y|x) from (2), and2Our approach could be adapted to support global features,which would require changes to the heuristic for A* search tobound the score obtainable from the global features.
This mayincur an additional computational cost over a purely local fea-ture model and perhaps also necessitate using beam search.483a heuristic function which estimates the additionalscore to get from a given state to a goal state.
Forthe search to be optimal ?
guaranteed to find the bestscoring path as the first solution ?
the heuristic mustbe admissible, meaning that it bounds from abovethe score for reaching a goal state.
We present threedifferent admissible heuristics later in this section,which bound the score with differing tightness andconsequently different search cost.Algorithm 1 presents A* search for our extractivesummarisation model.
Given a set of sentences tosummary, a scoring and a heuristic function, it findsthe best scoring summary.
This is achieved by build-ing the search graph incrementally, and storing eachfrontier state in a priority queue (line 1) which issorted by the sum of the state?s score and its heuris-tic.
These states are popped off the queue (line 3)and expanded by adding a sentence, which is thenadded to the schedule (lines 8?14).
We designatespecial finishing states using a boolean variable (thelast entry in the tuple in lines 1, 7 and 12).
Fin-ishing states (with value T) denote ceasing to ex-pand the summary, and consequently their scoresdo not include the heuristic component.
When-ever one of these states is popped in line 2, weknow that it outscores all competing hypotheses andtherefore represents the optimal summary (becausethe heuristic is guaranteed to never underestimatethe cost to a goal state from an unfinished state).3Note that in algorithm 1 we create the summaryby building a list of sentence indices in sorted or-der to avoid spurious ambiguity which would un-necessarily expand the search space.
The functionlength(y,x) =?n?y length(xn) returns the lengthof sentences specified.We now return to the problem of defining theheuristic function, h(y;x, l) which provides an up-per bound on the additional score achievable inreaching a goal state from state y.
We present threedifferent variants of increasing fidelity, that is, thatbound the cost to a goal state more tightly.
Algo-rithm 2 is the simplest, which simply finds the max-imum score per word from the set of unused sen-3To improve the efficiency of Algorithm 1 we make a smallmodification to avoid expanding every possible edge in step 8,of which there are O(k) options.
Instead we expand a smallnumber (here, 3) at a time and defer the remaining items untillater by inserting a special node into the schedule.
These specialnodes are represented using a third ?to-be-continued?
state intothe done flag.Algorithm 1 A* search for extractive summarization.Require: set of sentences, x = x1, .
.
.
, xkRequire: scoring function s(?
)Require: heuristic function h(?
)Require: summary length limit L1: schedule = [(0, ?, F)] {priority queue of triples}{(A* score, sentence indices, done flag)}2: while schedule 6= [] do3: v,y, f ?
pop(schedule)4: if f = T then5: return y {success}6: else7: push(schedule, (s(y|x),y,T))8: for y ?
[max(y) + 1, k] do9: y?
?
y ?
y10: if length(y?,x) ?
L then11: v?
?
s(y?|x) + h(y?
;x, l)12: push(schedule, (v?,y?, F))13: end if14: end for15: end if16: end whiletences and then extrapolates this out over the re-maining words available to the length threshold.
Inthe algorithm, we use the shorthand sn = ?
(xn) ?
?for sentence n?s score, ln = length(xn) for its lengthand ly =?n?y ln for the total length of the currentstate (unfinished summary).Algorithm 2 Uniform heuristic, h1(y;x, L)Require: x sorted in order of score/length1: n?
max(y) + 12: return (L?
ly)max(snln, 0)The h1 heuristic is overly simple in that it assumeswe can ?reuse?
a high scoring short sentence manytimes despite this being disallowed by the model.For this reason we develop an improved bound, h2,in Algorithm 3.
This incrementally adds each sen-tence in order of its score-per-word until the lengthlimit is reached.
If the limit is to be exceeded,the heuristic scales down the final sentence?s scorebased on the fraction of words than can be used toreach the limit.The fractional usage of the final sentence in h2could be considered overly optimistic, especiallywhen the state has length just shy of the limit L. Ifthe next best ranked sentence is a long one, then itwill be used in the heuristic to over-estimate of thestate.
This is complicated to correct, and doing soexactly would require full backtracking which is in-tractable and would obviate the entire point of usingA* search.
Instead we use a subtle modification inh3 (Alg.
4) which is equivalent to h2 except in the484Algorithm 3 Aggregated heuristic, h2(y;x, L)Require: x sorted in order of score/length1: v ?
02: l?
?
ly3: for n ?
[max(y) + 1, k] do4: if sn ?
0 then5: return v6: end if7: if l?
+ ln ?
L then8: l?
?
l?
+ ln9: v ?
v + sn10: else11: return v + lnL?l?
sn12: end if13: end for14: return vinstance where the next best score/word sentence istoo long, where it skips over these sentences untilit finds the best scoring sentence that does fit.
Thishelps to address the overestimate of h2 and shouldtherefore lead to a smaller search graph and fasterruntime due to its early elimination of dead-ends.Algorithm 4 Agg.+final heuristic, h3(y;x, L)Require: x sorted in order of score/length1: n?
max(y) + 12: if n ?
k ?
sn > 0 then3: if ly + ln ?
L then4: return h2(y;x, L)5: else6: form ?
[n+ 1, k] do7: if ly + lm ?
L then8: return smL?lylm9: end if10: end for11: end if12: end if13: return 0The search process is illustrated in figure 1.
Whena node is visited in the search, if it satisfied thelength constraint then the all its child nodes areadded to the schedule.
These nodes are scored withthe score for the summary thus far plus a heuristicterm.
For example, the value of 4+1.5=5.5 for the{1} node arises from a score of 4 plus a heuristic of(7?
5) ?
34 = 1.5, reflecting the additional score thatwould arise if it were to use half of the next sentenceto finish the summary.
Note that in finding the besttwo summaries the search process did not need toinstantiate the full search graph.To test the efficacy of A* search with each of thedifferent heuristic functions, we now present empir-ical runtime results.
We used the training data asdescribed in Section 5.2 and for each document setstart(4+1.5,{1},F)+1 (3+2,{2},F)+2 (2+2,{3},F)+3(1+0,{4},F)+4(0,{},T)finish(7+0,{1,2},F)+2 (6+0,{1,3},F)+3(5+0,{1,4},F)+4(5+0,{2,3},F)+3(4+0,{2,4},F)+4(5,{1,4},T)finish(6+0,{2,3,4},F)+4(5,{2,3},T)finishFigure 1: Example of the A* search graph created to findthe two top scoring summaries of length ?
7 when sum-marising four sentences with scores of 4, 3, 2 and 1 re-spectively and lengths of 5, 4, 3 and 1 respectively.
Theh1 heuristic was used and the score and heuristic scoresare shown separately for clarity.
Bold nodes were visitedwhile dashed nodes were visited but found to exceed thelength constraint.generated the 100-best summaries with word limitL = 200.
Figure 2 shows the number of nodesand edges visited by A* search, reflecting the spaceand time cost of the algorithm, as a function of thenumber of sentences in the document set being sum-marised.
All three heuristics shown an empiricalincrease in complexity that is roughly linear in thedocument size, although there are some notable out-liers, particularly for the uniform heuristic.
Surpris-ingly the aggregated heuristic, h2, is not consider-ably more efficient than the uniform heuristic h1,despite bounding the cost more precisely.
However,the aggregated+final heuristic, h3, consistently out-performs the other two methods.
For this reason wehave used h3 in all subsequent experimentation.4 TrainingWe frame the training problem as one of findingmodel parameters, ?, such that the predicted out-put, y?
closely matches the gold standard, r.4 Thequality of the match is measured using an automaticevaluation metric.
We adopt the standard machinelearning terminology of loss functions, which mea-sure the degree of error in the prediction, ?
(y?, r).In our case the accuracy is measured by the ROUGE4The gold standard is typically an abstractive summary, andas such it is usually impossible for an extractive summarizer tomatch it exactly.485llllllll lllllllllllllllllllllll llllllllllllllllllllllllllllllll llllllllllllllllll llll llllllllll l llll ll llllllllll llll lllllllllllllllllllll lllllll lllllllllllllllll5 10 20 50 100 200 500 1000 20001e+021e+031e+041e+051e+06sentences in document settotal edges and nodesl uniformaggregatedaggregated+finalFigure 2: Efficiency of A* search search is roughly linearin the number of sentences in the document set.
The yaxis measures the search graph size in terms of the num-ber of edges in the schedule and the number of nodesvisited.
Measured with the final parameters after trainingto optimise ROUGE-2 with the three different heuristicsand expanding five nodes in each step.score, R, and the loss is simply 1 - R. The trainingproblem is to solve??
= arg min??
(y?, r) , (4)where with a slight abuse of notation, y?
and r aretaken to range over the corpus of many document-sets and summaries.To optimise the weights we use the minimum er-ror rate training (MERT) technique (Och, 2003), asused for training statistical machine translation sys-tems.
This approach is a first order optimizationmethod using Powell search to find the parameterswhich minimise the loss on the training data.
MERTrequires n-best lists which it uses to approximatethe full space of possible outcomes.
We use theA* search algorithm to construct these n-best lists,5and use MERT to optimise the ROUGE score on thetraining set for the R-1, R-2 and R-SU4 variants ofthe metric.5We used n = 100 in our experiments.5 Experimental settingsIn this section we describe the features for which welearn weights.
We also describe the input data usedin training and testing.5.1 Summarization systemThe summarizer we use is an extractive, query-basedmulti-document summarization system.
It is giventwo inputs: a query (place name) associated with animage and a set of documents.
The summarizer usesthe following features, as reported in previous work(Edmundson, 1969; Brandow et al, 1995; Radev etal., 2001; Conroy et al, 2005; Aker and Gaizauskas,2009; Aker and Gaizauskas, 2010a):?
querySimilarity: Sentence similarity to thequery (cosine similarity over the vector repre-sentation of the sentence and the query).?
centroidSimilarity: Sentence similarity to thecentroid.
The centroid is composed of the 100most frequently occurring non stop words inthe document collection (cosine similarity overthe vector representation of the sentence andthe centroid).
For each word/term in the vec-tor we store a value which is the product ofthe term frequency in the document and the in-verse document frequency, a measurement ofthe term?s distribution over the set of docu-ments (Salton and Buckley, 1988).?
sentencePosition: Position of the sentencewithin its document.
The first sentence in thedocument gets the score 1 and the last one gets1n where n is the number of sentences in thedocument.?
inFirst5: Binary feature indicating whether thesentence occurs is one of the first 5 sentencesof the document.?
isStarter: A sentence gets a binary score if itstarts with the query term (e.g.
WestminsterAbbey, The Westminster Abbey, The Westmin-ster or The Abbey) or with the object type, e.g.The church.
We also allow gaps (up to fourwords) between the and the query/object typeto capture cases such as The most magnificentabbey, etc.?
LMProb: The probability of the sentence un-der a unigram language model.
We traineda separate language model on Wikipedia arti-cles about locations for each object type, e.g.,486church, bridge, etc.
When we generate a sum-mary about a location of type church, for in-stance, then we apply the church languagemodel on the related input documents relatedto the location.6?
sentenceCount: Each sentence gets assigned avalue of 1.
This feature is used to learn whethersummaries with many sentences are better thansummaries with few sentences or vice versa.?
wordCount: Number of words in the summary,to decide whether the model should favor longsummaries or short ones.5.2 DataFor training and testing we use the freely avail-able image description corpus described in Aker andGaizauskas (2010b).
The corpus is based around289 images of static located objects (e.g EiffelTower, Mont Blanc) each with a manually assignedplace name and object type category (e.g.
church,mountain).
For each place name there are up tofour model summaries that were created manuallyafter reading existing image descriptions taken fromthe VirtualTourist travel community web-site.
Eachsummary contains a minimum of 190 and a maxi-mum of 210 words.
We divide this set of 289 placenames into training and testing sets.
Both sets aredescribed in the following subsections.Training We use 184 place names from the 289set for training feature weights.
For each train-ing place name we gather all descriptions associ-ated with it from VirtualTourist.
We compute foreach sentence in each description a ROUGE scoreby comparing the sentence to those included in themodel summaries for that particular place name andretaining the highest score.
Table 1 gives some de-tails about this training data.We use ROUGE as a metric to maximize be-cause it is also used in DUC7 and TAC.8 How-ever, it should be noted that any automatic metriccould be used instead of ROUGE.
In particular weuse ROUGE 1 (R-1), ROUGE 2 (R-2) and ROUGESU4 (R-SU4).
R-1 and R-2 compute the number6For our training and testing sets we manually assigned eachlocation to its corresponding object type (Aker and Gaizauskas,2009).7http://duc.nist.gov/8http://www.nist.gov/tac/Max Min AvgSentences/place 1724 3 260Words/sentence 37 3 17Table 1: The training input data contains 184 placenames with 42333 sentences in total.
The numbers inthe columns give detail about the number of sentencesfor each place and the lengths of the sentences.Max Min AvgDocuments/place 20 5 12Sentences/place 1716 15 132Sentences/document 275 1 10Words/sentence 211 1 20Table 2: In domain test data.
The numbers in the columnsgive detail about the number of documents (descriptions)for each place, number of sentences for each place anddocument (description) and the lengths of the sentences.of uni-gram and bi-gram overlaps, respectively, be-tween the automatic and model summaries.
R-SU4allows bi-grams to be composed of non-contiguouswords, with a maximum of four words between thebi-grams.Testing For testing purposes we use the rest ofthe place names (105) from the 289 place nameset.
For each place name we use a set of inputdocuments, generate a summary from these docu-ments using our summarizer and compare the resultsagainst model summaries of that place name usingROUGE.
We experimented with two different inputdocument types: out of domain and in domain.The in domain documents are the VirtualTouristoriginal image descriptions from which the modelsummaries were derived.
As with the training setwe take all place name descriptions for a particularplace and use them as input documents to our sum-marizer.
Table 2 summarizes these input documents.The out of domain documents are retrieved fromthe web.
Compared to the in domain documentsthese documents should more challenging to sum-marize because they will contain different kindsof documents to those seen in training.
For eachplace name we retrieved the top ten related web-documents using the Yahoo!
search engine with theplace name as a query.
The text from these docu-ments is extracted using an HTML parser and passedto the summarizer.
Table 3 gives an overview of thisdata.487Max Min AvgSentences/place 1773 55 328Sentence/document 874 1 32Words/sentence 236 1 21Table 3: Out of domain test data.
The numbers in thecolumns give detail about the number of sentences foreach place and document and the lengths of the sentences.6 ResultsTo evaluate our approach we used two different as-sessment methods: ROUGE (Lin, 2004) and manualreadability.
In the following we present the resultsof each assessment.6.1 Automatic Evaluation using ROUGEWe report results for training and testing.
Inboth training and testing we distinguish betweenthree different summaries: wordLimit, sentence-Limit and regression.
WordLimit and sentenceLimitsummaries are the ones generated using the modeltrained by MERT.
As described in section 4 wetrained the summariser using the A* search decoderto maximise the ROUGE score of the best scoringsummaries.
We used the heuristic function h3 inA* search because it is the best performing heuris-tic, and 100-best lists.
To experiment with differ-ent summary length conditions we differentiate be-tween summaries with a word limit (wordLimit, setto 200 words) and summaries containing N numberof sentences (sentenceLimit) as stop condition in A*search.
We set N so that in both wordLimit and sen-tenceLimit summaries we obtain more or less thesame number of words (because our training datacontains on average 17 words for each word we setN to 12, 12*17=194).
However, this is only the casein the training.
In the testing for both wordLimit andsentenceLimit we generate summaries with the sameword limit constraint which allows us to have a faircomparison between the ROUGE recall scores.The regression summaries are our baseline.
Inthese summaries the sentences are ranked based onthe weighted features produced by Support Vec-tor Regression (SVR).9 Ouyang et al (2010) usemulti-document summarization and linear regres-sion methods to rank sentences in the documents.As regression model they used SVR and showed9We use the term regression to refer to SVR.Type metric R-1 R-2 R-SU4wordLimitR-1 0.5792 0.3176 0.3580R-2 0.5656 0.3208 0.3510R-SU4 0.5688 0.3197 0.3585sentenceLimitR-1 0.5915 0.3507 0.3881R-2 0.5783 0.3601 0.3890R-SU4 0.5870 0.3546 0.3929regressionR-1 0.4993 0.1946 0.2448R-2 0.4833 0.1949 0.2413R-SU4 0.5009 0.2031 0.2562Table 4: ROUGE scores obtained on the training data.that it out-performed classification and Learning ToRank methods on the DUC 2005 to 2007 data.
Forcomparison purpose we use SVR as a baseline sys-tem for learning feature weights.
It should be notedthat these weights are learned based on single sen-tences.
However, to have a fair comparison betweenall our summary types we use these weights to gen-erate summaries using the A* search with the wordlimit as constraint.
We do this for reporting both fortraining and testing results.The results for training are shown in Table 4.
Thetable shows ROUGE recall numbers obtained bycomparing model summaries against automaticallygenerated summaries on the training data.
Becausein training we used three different metrics (R-1, R-2,R-SU4) to train weights we report results for each ofthese three different ROUGE metrics.In Table 4 we can see that the scores for wordLimitand sentenceLimit type summaries are always atmaximum on the metric they were trained on (thiscan be observed by following the main diagonal ofthe result matrix).
This confirms that MERT is max-imizing the metric for which it was trained.
How-ever, this is not the case for regression results.
Thescores obtained with R-SU4 metric trained weightsachieve higher scores on R-1 and R-2 compared tothe scores obtained using weights trained on thosemetrics.
This is most likely due to SVR beingtrained on sentences rather than over entire sum-maries, and thereby not adequately optimising themetric used for evaluation.The results for testing are shown in Tables 5 and6.
As with the training setting we report ROUGE re-call scores.
We use the testing data described in sec-tion 5.2 for this setting.
However, because we havetwo different input document sets we report sepa-rate results for each of these (Table 5 shows resultfor in domain data and Table 6 shows result for out488Type metric R-1 R-2 R-SU4wordLimitR-1 0.3733 0.0842 0.1399R-2 0.3731 0.0842 0.1402R-SU4 0.3627 0.0794 0.1340sentenceLimitR-1 0.3664 0.0774 0.1321R-2 0.3559 0.0717 0.1251R-SU4 0.3629 0.0778 0.1312regressionR-1 0.3431 0.0669 0.1229R-2 0.2934 0.0560 0.1043R-SU4 0.3417 0.0668 0.1226Table 5: ROUGE scores obtained on the testing data.
Theautomated summaries are generated using the in domaininput documents.Type metric R-1 R-2 R-SU4wordLimitR-1 0.3758 0.0882 0.1421R-2 0.3755 0.0895 0.1423R-SU4 0.369 0.0812 0.137sentenceLimitR-1 0.3541 0.0693 0.1226R-2 0.3426 0.0638 0.1157R-SU4 0.3573 0.073 0.1251regressionR-1 0.3392 0.0611 0.1179R-2 0.3422 0.0606 0.1164R-SU4 0.3413 0.0606 0.1176Table 6: ROUGE scores obtained on the testing data.
Theautomated summaries are generated using the out of do-main input documents.of domain data).
Again as with the training settingwe report results for the different metrics (R-1, R-2,R-SU4) separately.From Table 5 we can see that the wordLimit sum-maries score highest compared to the other two typesof summaries.
This is different from the train-ing results where sentenceLimit summary type sum-maries are the top scoring ones.
As mentioned ear-lier the sentenceLimit summaries contain exactly 12sentences, where on average each sentence in thetraining data has 17 words.
We picked 12 sen-tences to achieve roughly the same word limit con-straint (12 ?
17 = 204) so they can be comparedto the wordLimit and regression type summaries.However, these sentenceLimit summaries have anaverage of 221 words, which explains the higherROUGE recall scores seen in training compared totesting (where a 200 word limit was imposed).The wordLimit summaries are significantly betterthan the scores from the other summary types ir-respective of the evaluation metric.10 It should be10Significance is reported at level p < 0.001.
We usedWilcoxson signed ranked test to perform significance.noted that these summaries are the only ones wherethe training and testing had the same condition inA* search concerning the summary word limit con-straint.
The scores in sentenceLimit type summariesare significantly lower than wordLimit summaries,despite using MERT to learn the weights.
Thisshows that training the true model is critical forgetting good accuracy.
The regression type sum-maries achieved the worst ROUGE metric scores.The weights used to generate these summaries weretrained on single sentences using SVR.
These resultsindicate that if the goal is to generate high scoringsummaries under a length limit in testing, then thesame constraint should also be used in training.From Table 5 and 6 we can see that the summariesobtained from VirtualTourist captions (in domaindata) score roughly the same as the summaries gen-erated using web-documents (out of domain data) asinput.
A possible explanation is that in many casesthe VirtualTourist original captions contain text fromWikipedia articles, which are also returned as resultsfrom the web search.
Therefore the web-documentsets included similar content to the VirtualTouristcaptions.6.2 Manual EvaluationWe also evaluated our summaries using a readabil-ity assessment as in DUC and TAC.
DUC and TACmanually assess the quality of automatically gener-ated summaries by asking human subjects to scoreeach summary using five criteria ?
grammaticality,redundancy, clarity, focus and coherence criteria.Each criterion is scored on a five point scale withhigh scores indicating a better result (Dang, 2005).For this evaluation we used the best scoring sum-maries from the wordLimit summary type (R-1, R-2and R-SU4) generated using web-documents (out ofdomain documents) as input.
We also evaluate theregression summary types generated using the sameinput documents to investigate the correlation be-tween high and low ROUGE metric scores to man-ual evaluation ones.
From the regression summarytype we only use summaries under the R2 and RSU4trained models.In total we evaluated five different summary types(three from wordLimit and two from regression).For each type we randomly selected 30 place namesand asked three people to assess the summaries forthese place names.
Each person was shown all 150489Criterion wordLimit regressionR1 R2 RSU4 R2 RSU4clarity 4.03 3.92 3.99 3.00 2.92coherence 3.31 3.06 2.99 2.12 1.88focus 3.79 3.56 3.54 2.44 2.29grammaticality 4.21 4.13 4.13 3.93 3.87redundancy 4.19 4.33 4.41 4.47 4.44Table 7: Manual evaluation results for the wordLimit (R1,R2, RSU4) and regression (R2, RSU4) summary types.The numbers in the columns are the average scores.summaries (30 from each summary type) in a ran-dom way and was asked to assess them according tothe DUC and TAC manual assessment scheme.
Theresults are shown in Table 7.11From Table 7 we can see that overall thewordLimit type summaries perform better than theregression ones.
For each metric in regression sum-mary types (R-2 and R-SU4) we compute the sig-nificance of the difference with the same metricsin wordLimit summary types.12 The results for theclarity, coherence and focus criteria in wordLimitsummaries are significantly better than in regressionones (p<0.001) irrespective of the training metric.These results concur with the automatic evaluationresults as described in section 6.1.
However, thisis not the case for the grammaticality and redun-dancy criteria.
Although in regression type sum-maries the scores for the grammaticality criterionare lower than those in wordLimit summaries thedifference is not significant.
Furthermore, we cansee that the redundancy scores for regression sum-maries are slightly higher than those for wordLimitsummaries.One reason for these differences might be theway we trained feature weights for wordLimit andregression summaries.
As mentioned above, fea-ture weights for wordLimit summaries are trainedusing summaries with a specific word limit con-straint, whereas the weights for the regression sum-maries are learned using single sentences.
Maxi-mizing the ROUGE metrics using ?final or output11We computed the agreement between the users using intraclass correlation with Cronbach?s Alpha where the correlationcoefficient ranges between 0 and 1.
Numbers close to 1 indicatehigh correlation and numbers close to 0 indicate low correlation.For the clarity criterion the assessors?
correlation coefficient is0.547, for coherence 0.687, for focus 0.688, for grammaticality0.232 and for redundancy 0.453.12We compute significance test for the manual evaluation re-sults using ?
square.like summaries?
will lead to a higher content agree-ment between the training and the model summarieswhereas this is not guaranteed with single sentences.With single sentences we have only a guarantee forhigh content overlap between single training andmodel sentences.
However, when these sentencesare combined into summaries it is not guaranteedthat these summaries will also have high contentoverlap with the entire model ones.
Therefore webelieve if there is a high content agreement betweenthe training and model summaries this could lead tomore readable summaries.
However, as we can seefrom Table 7 this hypothesis does not hold for allcriteria.
In case of the redundancy criterion we havecompared to wordLimit summary type high scoresin regression summaries although wordLimit sum-maries are significantly better than regression oneswhen it concerns the ROUGE scores.
Thus it islikely that by aggressively optimising the ROUGEmetric the model learns to game the metric, whichdoes not penalise redundancy in the summaries.As such it may no longer possible to extrapolatetrends from earlier correlation studies against humanjudgements (Lin, 2004).To minimize redundancy in summaries it is nec-essary to also take into consideration global featuresaddressing the linguistic aspects of the summaries.Furthermore, instead of ROUGE recall scores whichdo not take the repetition of information into consid-eration, ROUGE precision scores could be used as ametric in order to minimize the redundant content inthe summaries.7 ConclusionIn this paper we have proposed an A* search ap-proach for generating a summary from a ranked listof sentences and learning feature weights for a fea-ture based extractive multi-document summariza-tion system.
We developed an algorithm to learnoptimize an arbitrary metric and showed that ourapproach significantly outperforms state of the arttechniques.
Furthermore, we highlighted the impor-tance of uniformity in training and testing and ar-gued that if the goal is to generate high scoring sum-maries under a length limit in testing, then the sameconstraint should also be used in training.In this paper we experimented with sentence-localfeatures.
In the future we plan to expand this fea-ture set with global features, especially ones mea-490suring lexical diversity in the summaries to reducethe redundancy in them.
We will investigate vari-ous ways of incorporating these global features intoour A* search.
However this will incur an additionalcomputational cost over a purely local feature modeland therefore may necessitate using an approximatebeam search.
We also plan to investigate using othermetrics in training in order to reduce redundant in-formation in the summaries.
Finally, we have madeour summarizer publicly available as open-sourcesoftware.13ReferencesA.
Aker and R. Gaizauskas.
2009.
Summary Gener-ation for Toponym-Referenced Images using ObjectType Language Models.
International Conferenceon Recent Advances in Natural Language Processing(RANLP) September 14-16, 2009, Borovets, Bulgaria.A.
Aker and R. Gaizauskas.
2010a.
Generating im-age descriptions using dependency relational patterns.Proc.
of the ACL 2010, Upsala, Sweden.A.
Aker and R. Gaizauskas.
2010b.
Model Summariesfor Location-related Images.
In Proc.
of the LREC-2010 Conference.R.
Brandow, K. Mitze, and L.F. Rau.
1995.
Automaticcondensation of electronic publications by sentenceselection* 1.
Information Processing & Management,31(5):675?685.J.M.
Conroy, J.D.
Schlesinger, and J.G.
Stewart.
2005.CLASSY query-based multi-document summariza-tion.
Proc.
of the 2005 Document UnderstandingWorkshop, Boston.H.T.
Dang.
2005.
Overview of DUC 2005.
DUC 05Workshop at HLT/EMNLP.H.
Edmundson, P. 1969.
New Methods in AutomaticExtracting.
Journal of the Association for ComputingMachinery, 16:264?285.M.A.
Hearst.
1997.
TextTiling: segmenting text intomulti-paragraph subtopic passages.
Computationallinguistics, 23(1):33?64.C-Y.
Lin.
2004.
Rouge: A package for automatic evalua-tion of summaries.
Text Summarization Branches Out:Proc.
of the ACL-04 Workshop, pages 74?81.I.
Mani.
2001.
Automatic Summarization.
John Ben-jamins Publishing Company.F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
Proc.
of the 41st Annual Meetingon Association for Computational Linguistics-Volume1, page 167.13Available from http://www.dcs.shef.ac.uk/?tcohn/a-starY.
Ouyang, W. Li, S. Li, and Q. Lu.
2010.
Applyingregression models to query-focused multi-documentsummarization.
Information Processing & Manage-ment.D.R.
Radev, S. Blair-Goldensohn, and Z. Zhang.
2001.Experiments in single and multi-document summa-rization using MEAD.
Document Understanding Con-ference.K.
Riedhammer, D. Gillick, B. Favre, and D. Hakkani-T?ur.
2008.
Packing the meeting summarization knap-sack.
Proc.
Interspeech, Brisbane, Australia.S.J.
Russell, P. Norvig, J.F.
Canny, J. Malik, and D.D.Edwards.
1995.
Artificial intelligence: a modern ap-proach.
Prentice hall Englewood Cliffs, NJ.H.
Saggion.
2005.
Topic-based Summarization atDUC 2005.
Document Understanding Conference(DUC05).G.
Salton and C. Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
Information Pro-cessing and Management: an International Journal,24(5):513?523.491
