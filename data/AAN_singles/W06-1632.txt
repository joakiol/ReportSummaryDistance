Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 267?274,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Linguistically Motivated Featuresfor Paragraph Boundary IdentificationKatja Filippova and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlpAbstractIn this paper we propose a machine-learning approach to paragraph boundaryidentification which utilizes linguisticallymotivated features.
We investigate the re-lation between paragraph boundaries anddiscourse cues, pronominalization and in-formation structure.
We test our algorithmon German data and report improvementsover three baselines including a reimple-mentation of Sporleder & Lapata?s (2006)work on paragraph segmentation.
Ananalysis of the features?
contribution sug-gests an interpretation of what paragraphboundaries indicate and what they dependon.1 IntroductionOur work is concerned with multi-document sum-marization, namely with the merging of multipledocuments about the same topic taken from theweb.
We view summarization as extraction of im-portant sentences from the text.
As a consequenceof the merging process the layout of the documentsis lost.
In order to create the layout of the out-put, the document structure (Power et al, 2003)has to be regenerated.
One aspect of this struc-ture is of particular importance for our work: theparagraph structure.
In web documents paragraphboundaries are used to anchor figures and illustra-tions, so that the figures are always aligned withthe same paragraph even when the font size or thewindow size is changed.
Since we want to includefigures in the generated summaries, paragraph seg-mentation is an important subtask in our applica-tion.Besides multi-document summarization of webdocuments, paragraph boundary identification(PBI) could be useful for a number of different ap-plications, such as producing the layout for tran-scripts provided by speech recognizers and opti-cal character recognition systems, and determin-ing the layout of documents generated for outputdevices with different screen size.Though related to the task of topic segmenta-tion which stimulated a large number of studies(Hearst, 1997; Choi, 2000; Galley et al, 2003,inter alia), paragraph segmentation has not beenthoroughly investigated so far.
We explain this bythe fact that paragraphs are considered a stylisticphenomenon and that there is no unanimous opin-ion on what the function of the paragraph is.
Someauthors (Irmscher (1972) as cited by Stark (1988))suggest that paragraph structure is arbitrary andcan not be determined based solely on the prop-erties of the text.
Still, psycholinguistic studiesreport that humans agree, at least to some extent,on placing boundaries between paragraphs.
Thesestudies also note that paragraph boundaries are in-formative and make the reader perceive paragraph-initial sentences as being important (Stark, 1988).In contrast to topic segmentation, paragraph seg-mentation has the advantage that large amounts ofannotated data are readily availabe for supervisedlearning.In this paper we describe our approach to para-graph segmentation.
Previous work (Sporleder &Lapata, 2004; 2006) mainly focused on superficialand easily obtainable surface features like punctu-ation, quotes, distance and words in the sentence.Their approach was claimed to be domain- andlanguage-independent.
Our hypothesis, however,is that linguistically motivated features, which wecompute automatically, provide a better paragraphsegmentation than Sporleder & Lapata?s surfaceones, though our approach may loose some of the267domain-independence.
We test our hypothesis ona corpus of biographies downloaded from the Ger-man Wikipedia1.
The results we report in this pa-per indicate that linguistically motivated featuresoutperform surface features significantly.
It turnedout that pronominalization and information struc-ture contribute to the determination of paragraphboundaries while discourse cues have a negativeeffect.The paper is organized as follows: First, we de-scribe related work in Section 2, then in Section3 our data is introduced.
The baselines, the ma-chine learners, the features and the experimentalsetup are given in Section 4.
Section 5 reports anddiscusses the results.2 Related WorkCompared to other text segmentation tasks, e.g.topic segmentation, PBI has received relatively lit-tle attention.
We are aware of three studies whichapproach the problem from different perspectives.Bolshakov & Gelbukh (2001) assume that split-ting text into paragraphs is determined by text co-hesion: The link between a paragraph initial sen-tence and the preceding context is weaker than thelinks between sentences within a paragraph.
Theyevaluate text cohesion using a database of collo-cations and semantic links and insert paragraphboundaries where the cohesion is low.The algorithm of Sporleder & Lapata (2004,2006) uses surface, syntactic and language modelfeatures and is applied to three different languagesand three domains (fiction, news, parliament).This study is of particular interest to us since oneof the languages the algorithm is tested on is Ger-man.
They investigate the impact of different fea-tures and data size, and report results significantlybetter than a simple baseline.
However, their re-sults vary considerably between the languages andthe domains.
Also, the features determined impor-tant is different for each setting.
So, it may bepossible that Sporleder & Lapata do not provideconclusive results.Genzel (2005) considers lexical and syntacticfeatures and reports accuracy obtained from En-glish fiction data as well as from the WSJ corpus.He points out that lexical coherence and structuralfeatures turn out to be the most useful for his algo-rithm.
Unfortunately, the only evaluation measurehe provides is accuracy which, for the PBI task,1http://de.wikipedia.orgdoes not describe the performance of a system suf-ficiently.In comparison to the mentioned studies, ourgoal is to examine the influence of cohesive fea-tures on the choice of paragraph boundary inser-tion.
Unlike Bolshakov & Gelbukh (2001), whohave similar motivation but measure cohesion bycollocations, we explore the role of discourse cues,pronominalization and information structure.The task of topic segmentation is closely relatedto the task of paragraph segmentation.
If thereis a topic boundary, it is very likely that it coin-cides with a paragraph boundary.
However, thereverse is not true and one topic can extend overseveral paragraphs.
So, if determined reliably,topic boundaries could be used as high precision,low recall predictors for paragraph boundaries.Still, there is an important difference: While workon topic segmentation mainly depends on contentwords (Hearst, 1997) and relations between themwhich are computed using lexical chains (Galleyet al, 2003), paragraph segmentation as a stylisticphenomenon may depend equally likely on func-tion words.
Hence, paragraph segmentation isa task which encompasses the traditional bordersbetween content and style.3 DataThe data we used is a collection of biographiesfrom the German version of Wikipedia.
We se-lected all biographies under the Wikipedia cate-gories of physicists, chemists, mathematicians andbiologists and obtained 970 texts with an averagelength of 20 sentences and 413,776 tokens in total.Although our corpus is substantially smallerthan the German corpora of Sporleder & Lapata(2006), it should be big enough for a fair com-parison between their algorithm and the algorithmproposed here.
Having investigated the effect ofthe training size, Sporleder & Lapata (2006) cameto the conclusion that their system performs wellbeing trained on a small data set.
In particular,the learning curve for German shows an improve-ment of only about 2% when the amount of train-ing data is increased from 20%, which in case ofGerman fiction approximately equals 370,000 to-kens, to 100%.Fully automatic preprocessing in our systemcomprises the following stages: First, a list of peo-ple of a certain Wikipedia category is taken andfor every person an article is extracted The text268training development testtokens 347,763 39,228 19,943sentences 15,583 1,823 922paragraphs 5,323 654 362Table 1: Number of tokens and sentences per setis purged from Wiki tags and comments, the in-formation on subtitles and paragraph structure ispreserved.
Second, sentence boundaries are iden-tified with a Perl CPAN module2 whose perfor-mance we improved by extending the list of abbre-viations and modifying the output format.
Next,the sentences are split into tokens.
The TnT tag-ger (Brants, 2000) and the TreeTagger (Schmid,1997) are used for tagging and lemmatizing.
Fi-nally, the texts are parsed with the CDG depen-dency parser (Foth & Menzel, 2006).
Thus, thetext is split on three levels: paragraphs, sentencesand tokens, and morphological and syntactic in-formation is provided.A publicly available list of about 300 discourseconnectives was downloaded from the Internet siteof the Institute for the German Language3 (Insti-tut fu?r Deutsche Sprache, Mannheim) and slightlyextended.
These are identified in the text and an-notated automatically as well.
Named entities areclassified according to their type using informa-tion from Wikipedia: person, location, organiza-tion or undefined.
Given the peculiarity of our cor-pus, we are able to identify all mentions of the bi-ographee in the text by simple string matching.
Wealso annotate different types of referring expres-sions (first, last, full name) and resolve anaphoraby linking personal pronouns to the biographeeprovided that they match in number and gender.The annotated corpus is split into training(85%), development (10%) and testing (5%) sets.Distribution of data among the three sets is pre-sented in Table 1.
Sentences which serve as sub-titles in a text are filtered out because they makeidentifying a paragraph boundary for the follow-ing sentence trivial.4 Experiments4.1 Machine LearnersThe PBI task was reformulated as a binary classifi-cation problem: every training instance represent-2http://search.cpan.org/?holsten/Lingua-DE-Sentence-0.07/Sentence.pm3http://hypermedia.ids-mannheim.de/index.htmling a sentence was classified either as paragraph-initial or not.We used two machine learners: BoosTexter(Schapire & Singer, 2000) and TiMBL (Daele-mans et al, 2004).
BoosTexter was developedfor text categorization, and combines simple rules(decision stumps) in a boosting manner.
Sporleder& Lapata used this learner because it has the abil-ity to combine many only moderately accuratehypotheses.
TiMBL is a memory-based learnerwhich classifies every test instance by finding themost similar examples in the training set, hence itdoes not abstract from the data and is well suitedto handle features with many values, e.g.
the listof discourse cues.
For both classifiers, all experi-ments were run with the default settings.4.2 BaselinesWe compared the performance of our algorithmagainst three baselines.
The first one (distance)trivially inserts a paragraph break after each thirdsentence, which is the average number of sen-tences in a paragraph.
The second baseline (Gal-ley) hypothesizes that paragraph breaks coincidewith topic boundaries and utilizes Galley et al?s(2003) topic boundary identification tool LCseg.The third baseline (Sporleder) is a reimplementa-tion of Sporleder & Lapata?s 2006 algorithm withthe following features:Word and Sentence Distances from the currentsentence to the previous paragraph break;Sentence Length and Relative Position (relPos)of the sentence in a text;Quotes encodes whether this and the previoussentences contain a quotation, and whetherthe quotation is continued in the current sen-tence or not;Final Punctuation of the previous sentence;Words ?
the first (word1), the first two (word2),the first three and all words from the sen-tence;Parsed has positive value in case the sentence isparsed, negative otherwise;Number of S, VP, NP and PP nodes in the sen-tence;Signature is the sequence of PoS tags with andwithout punctuation;269Children of Top-Level Nodes are two featuresrepresenting the sequence of syntactic labelsof the children of the root of the parse treeand the children of the highest S-node;Branching Factor features express the averagenumber of children of S, VP, NP and PPnodes in the parse;Tree Depth is the average length of the path fromthe root to the leaves;Per-word Entropy is a feature based on Gen-zel & Charniak?s (2003) observation thatparagraph-initial sentences have lower en-tropy than non-initial ones;Sentence Probability according to a languagemodel computed from the training data;Character-level n-gram models are built usingthe CMU toolkit (Clarkson & Rosenfeld,1997).Since the parser we used produces dependencytrees as an output, we could not distinguish be-tween such features as children of the root of thetree and children of the top-level S-node.
Apartfrom this minor change, we reimplemented the al-gorithm in every detail.4.3 Our FeaturesFor our algorithm we first selected the features ofSporleder & Lapata?s (2006) system which per-formed best on the development set.
These arerelative position, the first and the first two words(relPos, word1, word2).
Quote and final punctu-ation features, which were particularly helpful inSporleder & Lapata?s experiments on the Germanfiction data, turned out to be superfluous given theinfrequency of quotations and the prevalent use ofthe period as sentence delimiter in our data.We experimented with text cohesion features as-suming that the paragraph structure crucially de-pends on cohesion and that paragraph breaks arelikely to occur between sentences where cohesivelinks are weak.
In order to estimate the degree ofcohesion, we looked at lexical cohesion, pronom-inalization, discourse cues and information struc-ture.4.3.1 Lexical CohesionnounOver, verbOver: Similar to Sporleder &Lapata (2006), we introduced an overlap fea-ture, but measured the degree of overlap asa number of common noun and verb lem-mas between two adjacent sentences.
We pre-ferred lemmas over words in order to matchall possible forms of the same word in Ger-man.LCseg: Apart from the overlap, a boolean featurebased on LCseg (Galley et al, 2003) markedwhether the tool suggests that a new topic be-gins with the current sentence.
This feature,relying on lexical chains, was supposed toprovide more fine-grained information on thedegree of similarity between two sentences.4.3.2 PronominalizationAs Stark (1988) points out, humans tend to in-terpret over-reference as a clue for the beginningof a new paragraph: In a sentence, if a non-pronominal reference is preferred over a pronom-inal one where the pronoun would be admissi-ble, humans are likely to mark this sentence as aparagraph-initial one.
In order to check whetherover-reference indeed correlates with paragraph-initial sentences, we described the way the bi-ographee is referred to in the current and the pre-vious sentences.prevSPerson, currSPerson: This feature4 withthe values NA, biographee, other indicateswhether there is a reference to the biographeeor some other person in the sentence.prevSRE, currSRE: This feature describes thebiographee?s referring expression and hasthree possible values: NA, name, pronoun.Although our annotation distinguishes betweenfirst, last and full names, we found out that, forthe PBI task, the distinction is spurious and unify-ing these three under the same category improvesthe results.REchange: Since our classifiers assume featureindependence and can not infer the informa-tion on the change in referring expression, weexplicitly encoded that information by merg-ing the values of the previous feature for thecurrent and the preceding sentences into one,which has nine possible values (name-name,NA-name, pronoun-name, etc.
).4Prefixes prevS-, currS- stand for the previous and thecurrent sentences respectively.2704.3.3 Discourse CuesThe intuition behind these features is that cuewords and phrases are used to signal the relationbetween the current sentence and the precedingsentence or context (Mann & Thompson, 1988).Such connectives as endlich (finally), abgesehendavon (apart from that), danach (afterwards) ex-plicitly mark a certain relation between the sen-tence they occur in and the preceding context.
Wehypothesize that the relations which hold acrossparagraph boundaries should differ from thosewhich hold within paragraphs and that the same istrue for the discourse cues.
Absence of a connec-tive is supposed to be informative as well, beingmore typical for paragraph-initial sentences.Three features describe the connective of thecurrent sentence.
Another three features describethe one from the preceding sentence.prevSCue, currSCue: This feature is the con-nective itself (NA in case of none).prevSCueClass, currSCueClass: This featurerepresents the semantic class of the cue wordor phrase as assigned by the IDS Mannheim.There are 25 values, including NA in caseof no connective, altogether, with the mostfrequent values being temporal, concessive,conclusive, etc.prevSProCue, currSProCue: The third binaryfeature marks whether the connective isproadverbial or not (NA if there is no connec-tive).
Being anaphors, proadverbials, such asdeswegen (because of that), daru?ber (aboutthat) explicitly link a sentence to the preced-ing one(s).4.3.4 Information StructureInformation structure, which is in German to alarge extent expressed by word order, providesadditional clues to the degree of connectednessbetween two sentences.
In respect to the PBItask, Stark (1988) reports that paragraph-initialsentences are often theme-marking which meansthat the subject of such sentences is not the firstelement.
Given the lower frequency of paragraph-initial sentences, this feature can not be consideredreliable, but in combination with others it providesan additional clue.
In German, the first elementbest corresponds to the prefield (Vorfeld) ?
nor-mally, the single constituent placed before the fi-nite verb in the main clause.currSVF encodes whether the constituent inthe prefield is a NP, PP, ADV, CARD, orSub.Clause.
Values different from NP un-ambiguously represent theme-marking sen-tences, whereas the NP value may stand forboth: theme-marking as well as not theme-marking sentence.4.4 DiscussionNote, that we did not exclude text-initial sentencesfrom the study because the encoding we used doesnot make such cases trivial for classification.
Al-though some of the features refer to the previoussentence, none of them has to be necessarily re-alized and therefore none of them explicitly indi-cates the absence of the preceding sentence.
Forexample, the label NA appears in cases where thereis no discourse cue in the preceding sentence aswell as in cases where there is no preceding sen-tence.
The same holds for all other features pre-fixed with prevS-.Another point concerns the use ofpronominalization-based features.
Sporleder& Lapata (2006) waive using such features be-cause they consider pronominalization dependenton the paragraph structure and not the otherway round.
At the same time they mentionspeech and optical character recognition tasksas possible application domains for the PBI.There, pronouns are already given and neednot be regenerated, hence for such applicationsfeatures which utilize pronouns are absolutelyappropriate.
Unlike the recognition tasks, formulti-document summarization both decisionshave to be made, and the order of the two tasksis not self-evident.
The best decision wouldprobably be to decide simultaneously on bothusing optimization methods (Roth & Yih, 2004;Marciniak & Strube, 2005).
Generating pronounsbefore inserting boundaries seems as reasonableas doing it the other way round.4.5 Feature SelectionWe determine the relevant feature set and evaluatewhich features from this set contribute most to theperformance of the system by the following pro-cedures.First, we follow an iterative algorithm similarto the wrapper approach for feature selection (Ko-havi & John, 1997) using the development dataand TiMBL.
The feature subset selection algo-rithm performs a hill-climbing search along the271Feature set F-measureall 58.85%?prevSCue 0.78%?currSCue 0.32%?currSCueClass 0.38%?prevSCueClass 0.37%?prevSProCue 1.02%best 61.72%Table 2: Removed featuresFeature set F-measurerelPos, word1, word2 48.06%+currSRE +10.50%+currSVF +0.49%+currSPerson +0.57%+prevSPerson +1.32%best 60.94%Table 3: Best featuresfeature space.
We start with a model based on allavailable features.
Then we train models obtainedby removing one feature at a time.
We choose theworst performing feature, namely the one whoseremoval gives the largest improvement based onthe F-measure, and remove it from the model.
Wethen train classifiers removing each of the remain-ing features separately from the enhanced model.The process is iteratively run as long as significantimprovement is observed.To measure the contribution of the relevant fea-tures we start with the three best features fromSporleder & Lapata (2006) (see Section 4.3) andtrain TiMBL combining the current feature setwith each feature in turn.
We then choose the bestperforming feature based on the F-measure andadd it to the model.
We iterate the process untilall features are added to the three-feature system.Thus, we optimize the default setting and obtainthe information on what the paragraph structurecrucially depends.5 ResultsHaving trained our algorithm on the developmentdata, we then determined the optimal feature com-bination and finally evaluated the performance onthe previously unseen test data.Table 2 and Table 3 present the ranking of theleast and of the most beneficial features respec-tively.
Somewhat surprising to us, Table 2 showsthat basically all features capturing information ondiscourse cues actually worsened the performanceof the classifier.
The bad performance of theprevSCue and currSCue features may be causedby their extreme sparseness.
To test these fea-tures reasonably, we plan to increase the data setsize by an order of magnitude.
Then, at least, itshould be possible to determine which discoursecues, if any, are correlated with paragraph bound-aries.
The bad performance of the prevSCueClassand currSCueClass features may be caused by thecategorization provided by the IDS.
This questionalso requires further investigation, maybe with adifferent categorization.Table 3 also provides interesting insights in thefeature set.
First, with only the three featuresrelPos, word1 and word2 the baseline performsalmost as well as the full feature set used bySporleder & Lapata.
Then, as expected, currSREprovides the largest gain in performance, fol-lowed by currSVF, currSPerson and prevSPerson.This result confirms our hypothesis that linguisti-cally motivated features capturing information onpronominalization and information structure playan important role in determining paragraph seg-mentation.The results of our system and the baselinesfor different classifiers (BT stands for BoosTex-ter and Ti for TiMBL) are summarized in Table4.
Accuracy is calculated by dividing the num-ber of matches over the total number of test in-stances.
Precision, recall and F-measure are ob-tained by considering true positives, false positivesand false negatives.
The latter metric, WindowDiff(Pevzner & Hearst, 2002), is supposed to over-come the disadvantage of the F-measure which pe-nalizes near misses as harsh as more serious mis-takes.
The value of WindowDiff varies between 0and 1, where a lesser count corresponds to betterperformance.The significance of our results was computedusing the    test.
All results are significantlybetter (on the      level or below) thanboth baselines and the reimplemented version ofSporleder & Lapata?s (2006) algorithm whose per-formance on our data is comparable to what theauthors reported on their corpus of German fic-tion.
Interestingly, TiMBL does much better thanBoosTexter on Sporleder & Lapata?s feature set.Apparently, Sporleder & Lapata?s presupposition,that they would rely on many weak hypotheses,272Accuracy Precision Recall F-measure WindowDiffdistance 52.16 37.98 31.88 34.66 .426Galley 56.83 43.04 26.15 32.54 .416developmentSporleder BT 71.96 80.15 30.46 44.15 .327Sporleder Ti 62.36 48.65 62.89 54.86 .338all BT 74.93 72.10 50.67 59.52 .286all Ti 70.54 59.81 57.91 58.85 .302best Ti 73.39 64.73 58.97 61.72 .280testSporleder BT 68.76 80.15 28.61 42.16 .341Sporleder Ti 60.62 50.46 59.67 54.68 .345all BT 72.12 71.31 50.13 58.88 .286all Ti 67.13 59.14 56.40 57.74 .303best Ti 68.00 60.46 56.67 58.50 .302Table 4: Results for the development and test sets with the two classifiersdoes not hold.
This is also confirmed by the resultsreported in Table 3 where only three of their fea-tures perform surprisingly strong.
In contrast, onour feature set TiMBL and BoosTexter perform al-most equally.
However, BoosTexter achieves in allcases a much higher precision which is preferableover the higher recall provided by TiMBL.6 ConclusionIn this paper, we proposed a novel approach toparagraph boundary identification based on lin-guistic features such as pronominalization, dis-course cues and information structure.
The resultsare significantly higher than all baselines and areimplementation of Sporleder & Lapata?s (2006)system and achieve an F-measure of about 59%.We investigated to what extent the paragraphstructure is determined by each of the three fac-tors and came to the conclusion that it cruciallydepends on the use of pronouns and informationstructure.
Surprisingly, discourse cues did not turnout to be useful for this task and even negativelyaffected the results which we explain by the ex-tremely sparseness of the cues in our data.It turned out that the best results could beachieved by a combination of surface features (rel-Pos, word1, word2) and features capturing textcohesion.
This indicates that paragraph bound-ary identification requires features usually used forstyle analysis and ones describing cohesive rela-tions.
Therefore, paragraph boundary identifica-tion is in fact a task which crosses the borders be-tween content and style.An obvious limitation of our study is that wetrained and tested the algorithm on one-genre do-main where pronouns are used extensively.
Ex-perimenting with different genres should shedlight on whether our features are in fact domain-dependent.
In the future, we also want to ex-periment with a larger data set for determiningwhether discourse cues really do not correlate withparagraph boundaries.
Then, we will move ontowards multi-document summarization, the ap-plication which motivates the research describedhere.Acknowledments: This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by aKTF grant (09.009.2004).
We would also liketo thank the three anonymous reviewers for theircomments.ReferencesBolshakov, Igor A.
& Alexander Gelbukh (2001).Text segmentation into paragraph based on localtext cohesion.
In Text, Speech and Dialogue, pp.158?166.Brants, Thorsten (2000).
TnT ?
A statistical Part-of-Speech tagger.
In Proceedings of the 6thConference on Applied Natural Language Pro-cessing, Seattle, Wash., 29 April ?
4 May 2000,pp.
224?231.Choi, Freddy Y. Y.
(2000).
Advances in domainindependent linear text segmentation.
In Pro-273ceedings of the 1st Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics, Seattle, Wash., 29 April?
3 May, 2000, pp.
26?33.Clarkson, Philip & Roni Rosenfeld (1997).
Sta-tistical language modeling.
In Proceedingsof ESCA, EuroSpeech?97.
Rhodes, pp.
2707?2710.Daelemans, Walter, Jakub Zavrel, Ko van derSloot & Antal van den Bosch (2004).
TiMBL:Tilburg Memory Based Learner, version 5.1,Reference Guide.
Technical Report ILK 04-02:ILK Tilburg.Foth, Kilian & Wolfgang Menzel (2006).
Robustparsing: More with less.
In Proceedings of the11th Conference of the European Chapter ofthe Association for Computational Linguistics,Trento, Italy, 3?7 April 2006, pp.
25?32.Galley, Michel, Kathleen R. McKeown, EricFosler-Lussier & Hongyan Jing (2003).
Dis-course segmentation of multi-party conversa-tion.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguis-tics, Sapporo, Japan, 7?12 July 2003, pp.
562?569.Genzel, Dmitriy (2005).
A paragraph bound-ary detection system.
In Proceedings of theSixth International Conference on IntelligentText Processing and Computational Linguistics,Mexico City, Mexico.Genzel, Dmitriy & Eugene Charniak (2003).
Vari-ation of entropy and parse trees of sentences asa function of the sentence number.
In Proceed-ings of the 2003 Conference on Empirical Meth-ods in Natural Language Processing, Sapporo,Japan, 11?12 July 2003, pp.
65?72.Hearst, Marti A.
(1997).
TextTiling: Segment-ing text into multi-paragraph subtopic passages.Computational Linguistics, 23(1):33?64.Irmscher, William F. (1972).
The Holt Guide toEnglish.
New-York: Holt, Rinehart Winston.Kohavi, Ron & George H. John (1997).
Wrap-pers for feature subset selection.
Artificial In-telligence Journal, 97(1-2):273?324.Mann, William C. & Sandra A. Thompson (1988).Rhetorical structure theory.
Toward a functionaltheory of text organization.
Text, 8(3):243?281.Marciniak, Tomacz & Michael Strube (2005).
Be-yond the pipeline: Discrete optimization inNLP.
In Proceedings of the 9th Conferenceon Computational Natural Language Learning,Ann Arbor, Mich., USA, 29?30 June 2005, pp.136?145.Pevzner, Lev & Marti Hearst (2002).
A critiqueand improvement of an evaluation metric fortext segmentation.
Computational Linguistics,28(1):19?36.Power, Richard, Donia Scott & Nadjet Bouayad-Agha (2003).
Document structure.
Computa-tional Linguistics, 29(2):211?260.Roth, Dan & Wen-tau Yih (2004).
A linear pro-gramming formulation for global inference innatural language tasks.
In Proceedings of the8th Conference on Computational Natural Lan-guage Learning, Boston, Mass., USA, 6?7 May2004, pp.
1?8.Schapire, Robert E. & Yoram Singer (2000).BoosTexter: A boosting-based system fortext categorization.
Machine Learning,39(2/3):135?168.Schmid, Helmut (1997).
Probabilistic part-of-speech tagging using decision trees.
In DanielJones & Harold Somers (Eds.
), New Methodsin Language Processing, pp.
154?164.
London,UK: UCL Press.Sporleder, Caroline & Mirella Lapata (2004).
Au-tomatic paragraph identification: A study acrosslanguages and domains.
In Proceedings of the2004 Conference on Empirical Methods in Nat-ural Language Processing, Barcelona, Spain,25?26 July 2004, pp.
72?79.Sporleder, Caroline & Mirella Lapata (2006).Broad coverage paragraph segmentation acrosslanguages and domains.
ACM Transactions inSpeech and Language Processing.
To appear.Stark, Heather (1988).
What do paragraph mark-ings do?
Discourse Processes, (11):275?303.274
