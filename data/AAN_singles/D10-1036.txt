Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366?376,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAutomatic Keyphrase Extraction via Topic DecompositionZhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong SunDepartment of Computer Science and TechnologyState Key Lab on Intelligent Technology and SystemsNational Lab for Information Science and TechnologyTsinghua University, Beijing 100084, China{lzy.thu, harrywy, yabin.zheng}@gmail.com,sms@tsinghua.edu.cnAbstractExisting graph-based ranking methods forkeyphrase extraction compute a single impor-tance score for each word via a single ran-dom walk.
Motivated by the fact that bothdocuments and words can be represented bya mixture of semantic topics, we propose todecompose traditional random walk into mul-tiple random walks specific to various topics.We thus build a Topical PageRank (TPR) onword graph to measure word importance withrespect to different topics.
After that, giventhe topic distribution of the document, we fur-ther calculate the ranking scores of words andextract the top ranked ones as keyphrases.
Ex-perimental results show that TPR outperformsstate-of-the-art keyphrase extraction methodson two datasets under various evaluation met-rics.1 IntroductionKeyphrases are defined as a set of terms in a doc-ument that give a brief summary of its content forreaders.
Automatic keyphrase extraction is widelyused in information retrieval and digital library (Tur-ney, 2000; Nguyen and Kan, 2007).
Keyphrase ex-traction is also an essential step in various tasks ofnatural language processing such as document cate-gorization, clustering and summarization (Manningand Schutze, 2000).There are two principled approaches to extractingkeyphrases: supervised and unsupervised.
The su-pervised approach (Turney, 1999) regards keyphraseextraction as a classification task, in which a modelis trained to determine whether a candidate phraseis a keyphrase.
Supervised methods require a doc-ument set with human-assigned keyphrases as train-ing set.
In Web era, articles increase exponentiallyand change dynamically, which demands keyphraseextraction to be efficient and adaptable.
However,since human labeling is time consuming, it is im-practical to label training set from time to time.We thus focus on the unsupervised approach in thisstudy.In the unsupervised approach, graph-based rank-ing methods are state-of-the-art (Mihalcea and Ta-rau, 2004).
These methods first build a word graphaccording to word co-occurrences within the docu-ment, and then use random walk techniques (e.g.,PageRank) to measure word importance.
After that,top ranked words are selected as keyphrases.Existing graph-based methods maintain a singleimportance score for each word.
However, a docu-ment (e.g., news article or research article) is usu-ally composed of multiple semantic topics.
Takingthis paper for example, it refers to two major top-ics, ?keyphrase extraction?
and ?random walk?.
Aswords are used to express various meanings corre-sponding to different semantic topics, a word willplay different importance roles in different topicsof the document.
For example, the words ?phrase?and ?extraction?
will be ranked to be more impor-tant in topic ?keyphrase extraction?, while the words?graph?
and ?PageRank?
will be more important intopic ?random walk?.
Since they do not take topicsinto account, graph-based methods may suffer fromthe following two problems:1.
Good keyphrases should be relevant to the ma-jor topics of the given document.
In graph-based methods, the words that are strongly con-nected with other words tend to be ranked high,366which do not necessarily guarantee they are rel-evant to major topics of the document.2.
An appropriate set of keyphrases should alsohave a good coverage of the document?s ma-jor topics.
In graph-based methods, the ex-tracted keyphrases may fall into a single topicof the document and fail to cover other substan-tial topics of the document.To address the problem, it is intuitive to considerthe topics of words and document in random walkfor keyphrase extraction.
In this paper, we pro-pose to decompose traditional PageRank into multi-ple PageRanks specific to various topics and obtainthe importance scores of words under different top-ics.
After that, with the help of the document topics,we can further extract keyphrases that are relevantto the document and at the same time have a goodcoverage of the document?s major topics.
We callthe topic-decomposed PageRank as Topical PageR-ank (TPR).In experiments we find that TPR can extractkeyphrases with high relevance and good cover-age, which outperforms other baseline methods un-der various evaluation metrics on two datasets.
Wealso investigate the performance of TPR with dif-ferent parameter values and demonstrate its robust-ness.
Moreover, TPR is unsupervised and language-independent, which is applicable in Web era withenormous information.TPR for keyphrase extraction is a two-stage pro-cess:1.
Build a topic interpreter to acquire the topics ofwords and documents.2.
Perform TPR to extract keyphrases for docu-ments.We will introduce the two stages in Section 2 andSection 3.2 Building Topic InterpretersTo run TPR on a word graph, we have to acquiretopic distributions of words.
There are roughly twoapproaches that can provide topics of words: (1) Usemanually annotated knowledge bases, e.g., Word-Net (Miller et al, 1990); (2) Use unsupervised ma-chine learning techniques to obtain word topics froma large-scale document collection.
Since the vocab-ulary in WordNet cannot cover many words in mod-ern news and research articles, we employ the sec-ond approach to build topic interpreters for TPR.In machine learning, various methods have beenproposed to infer latent topics of words and docu-ments.
These methods, known as latent topic mod-els, derive latent topics from a large-scale documentcollection according to word occurrence informa-tion.
Latent Dirichlet Allocation (LDA) (Blei et al,2003) is a representative of topic models.
Com-pared to Latent Semantic Analysis (LSA) (Landaueret al, 1998) and probabilistic LSA (pLSA) (Hof-mann, 1999), LDA has more feasibility for inferenceand can reduce the risk of over-fitting.In LDA, each word w of a document d is regardedto be generated by first sampling a topic z from d?stopic distribution ?
(d), and then sampling a wordfrom the distribution over words ?
(z) that charac-terizes topic z.
In LDA, ?
(d) and ?
(z) are drawnfrom conjugate Dirichlet priors ?
and ?, separately.Therefore, ?
and ?
are integrated out and the prob-ability of word w given document d and priors isrepresented as follows:pr(w|d, ?, ?)
=K?z=1pr(w|z, ?
)pr(z|d, ?
), (1)where K is the number of topics.Using LDA, we can obtain the topic distributionof each word w, namely pr(z|w) for topic z ?
K.The word topic distributions will be used in TPR.Moreover, using the obtained word topic distribu-tions, we can infer the topic distribution of a newdocument (Blei et al, 2003), namely pr(z|d) foreach topic z ?
K, which will be used for rankingkeyphrases.3 Topical PageRank for KeyphraseExtractionAfter building a topic interpreter to acquire thetopics of words and documents, we can performkeyphrase extraction for documents via TPR.
Givena document d, the process of keyphrase extractionusing TPR consists of the following four steps whichis also illustrated in Fig.
1:1.
Construct a word graph for d according to wordco-occurrences within d.367Figure 1: Topical PageRank for Keyphrase Extraction.2.
Perform TPR to calculate the importancescores for each word with respect to differenttopics.3.
Using the topic-specific importance scores ofwords, rank candidate keyphrases respect toeach topic separately.4.
Given the topics of document d, integrate thetopic-specific rankings of candidate keyphrasesinto a final ranking, and the top ranked ones areselected as keyphrases.3.1 Constructing Word GraphWe construct a word graph according to word co-occurrences within the given document, which ex-presses the cohesion relationship between wordsin the context of document.
The document is re-garded as a word sequence, and the link weights be-tween words is simply set to the co-occurrence countwithin a sliding window with maximum W words inthe word sequence.It was reported in (Mihalcea and Tarau, 2004)the graph direction does not influence the perfor-mance of keyphrase extraction very much.
In thispaper we simply construct word graphs with direc-tions.
The link directions are determined as follows.When sliding a W -width window, at each position,we add links from the first word pointing to otherwords within the window.
Since keyphrases are usu-ally noun phrases, we only add adjectives and nounsin word graph.3.2 Topical PageRankBefore introducing TPR, we first give some formalnotations.
We denote G= (V,E) as the graph of adocument, with vertex set V = {w1, w2, ?
?
?
, wN}and link set (wi, wj) ?
E if there is a link fromwi to wj .
In a word graph, each vertex representsa word, and each link indicates the relatedness be-tween words.
We denote the weight of link (wi, wj)as e(wi, wj), and the out-degree of vertex wi asO(wi)=?j:wi?wj e(wi, wj).Topical PageRank is based on PageRank (Page etal., 1998).
PageRank is a well known ranking al-gorithm that uses link information to assign globalimportance scores to web pages.
The basic idea ofPageRank is that a vertex is important if there areother important vertices pointing to it.
This can beregarded as voting or recommendation among ver-tices.
In PageRank, the score R(wi) of word wi isdefined asR(wi) = ?
?j:wj?wie(wj , wi)O(wj)R(wj) + (1?
?
)1|V | ,(2)where ?
is a damping factor range from 0 to 1, and|V | is the number of vertices.
The damping fac-tor indicates that each vertex has a probability of(1 ?
?)
to perform random jump to another vertexwithin this graph.
PageRank scores are obtained byrunning Eq.
(2) iteratively until convergence.
Thesecond term in Eq.
(2) can be regarded as a smooth-ing factor to make the graph fulfill the property ofbeing aperiodic and irreducible, so as to guaranteethat PageRank converges to a unique stationary dis-368tribution.
In PageRank, the second term is set to bethe same value 1|V | for all vertices within the graph,which indicates there are equal probabilities of ran-dom jump to all vertices.In fact, the second term of PageRank in Eq.
(2)can be set to be non-uniformed.
Suppose we as-sign larger probabilities to some vertices, the finalPageRank scores will prefer these vertices.
We callthis Biased PageRank.The idea of Topical PageRank (TPR) is to runBiased PageRank for each topic separately.
Eachtopic-specific PageRank prefers those words withhigh relevance to the corresponding topic.
Andthe preferences are represented using random jumpprobabilities of words.Formally, in the PageRank of a specific topicz, we will assign a topic-specific preference valuepz(w) to each word w as its random jump proba-bility with?w?V pz(w) = 1.
The words that aremore relevant to topic z will be assigned larger prob-abilities when performing the PageRank.
For topicz, the topic-specific PageRank scores are defined asfollows:Rz(wi) = ?
?j:wj?wie(wj , wi)O(wj)Rz(wj)+(1??)pz(wi).
(3)In Fig.
1, we show an example with two topics.
Inthis figure, we use the size of circles to indicate howrelevant the word is to the topic.
In the PageRanksof the two topics, high preference values will be as-signed to different words with respect to the topic.Finally, the words will get different PageRank val-ues in the two PageRanks.The setting of preference values pz(w) will havea great influence to TPR.
In this paper we use threemeasures to set preference values for TPR:?
pz(w) = pr(w|z), is the probability that wordw occurs given topic z.
This indicates howmuch that topic z focuses on word w.?
pz(w) = pr(z|w), is the probability of topic zgiven word w. This indicates how much thatword w focuses on topic z.?
pz(w) = pr(w|z) ?
pr(z|w), is the product ofhub and authority values.
This measure is in-spired by the work in (Cohn and Chang, 2000).Both PageRank and TPR are all iterative algo-rithms.
We terminate the algorithms when the num-ber of iterations reaches 100 or the difference of eachvertex between two neighbor iterations is less than0.001.3.3 Extract Keyphrases Using Ranking ScoresAfter obtaining word ranking scores using TPR, webegin to rank candidate keyphrases.
As reported in(Hulth, 2003), most manually assigned keyphrasesturn out to be noun phrases.
We thus select nounphrases from a document as candidate keyphrasesfor ranking.The candidate keyphrases of a document is ob-tained as follows.
The document is first tokenized.After that, we annotate the document with part-of-speech (POS) tags 1.
Third, we extract nounphrases with pattern (adjective)*(noun)+,which represents zero or more adjectives followedby one or more nouns.
We regard these noun phrasesas candidate keyphrases.After identifying candidate keyphrases, we rankthem using the ranking scores obtained by TPR.In PageRank for keyphrase extraction, the rankingscore of a candidate keyphrase p is computed bysumming up the ranking scores of all words withinthe phrase: R(p)=?wi?p R(wi) (Mihalcea and Ta-rau, 2004; Wan and Xiao, 2008a; Wan and Xiao,2008b).
Then candidate keyphrases are ranked indescending order of ranking scores.
The top M can-didates are selected as keyphrases.In TPR for keyphrase extraction, we first com-pute the ranking scores of candidate keyphrases sep-arately for each topic.
That is for each topic z wecomputeRz(p) =?wi?pRz(wi).
(4)By considering the topic distribution of document,We further integrate topic-specific rankings of can-didate keyphrases into a final ranking and extracttop-ranked ones as the keyphrases of the document.Denote the topic distribution of the document das pr(z|d) for each topic z.
For each candidatekeyphrase p, we compute its final ranking score as1In experiments we use Stanford POS Tagger from http://nlp.stanford.edu/software/tagger.shtmlwith English tagging model left3words-distsim-wsj.369follows:R(p) =K?z=1Rz(p)?
pr(z|d).
(5)After ranking candidate phrases in descending orderof their integrated ranking scores, we select the topM as the keyphrases of document d.4 Experiments4.1 DatasetsTo evaluate the performance of TPR for keyphraseextraction, we carry out experiments on twodatasets.One dataset was built by Wan and Xiao 2 whichwas used in (Wan and Xiao, 2008b).
This datasetcontains 308 news articles in DUC2001 (Over et al,2001) with 2, 488 manually annotated keyphrases.There are at most 10 keyphrases for each document.In experiments we refer to this dataset as NEWS.The other dataset was built by Hulth 3 which wasused in (Hulth, 2003).
This dataset contains 2, 000abstracts of research articles and 19, 254 manuallyannotated keyphrases.
In experiments we refer tothis dataset as RESEARCH.Since neither NEWS nor RESEARCH itself islarge enough to learn efficient topics, we use theWikipedia snapshot at March 2008 4 to build topicinterpreters with LDA.
After removing non-articlepages and the articles shorter than 100 words, wecollected 2, 122, 618 articles.
After tokenization,stop word removal and word stemming, we build thevocabulary by selecting 20, 000 words according totheir document frequency.
We learn LDA models bytaking each Wikipedia article as a document.
In ex-periments we learned several models with differentnumbers of topics, from 50 to 1, 500 respectively.For the words absent in topic models, we simply setthe topic distribution of the word as uniform distri-bution.4.2 Evaluation MetricsFor evaluation, the words in both standard and ex-tracted keyphrases are reduced to base forms using2http://wanxiaojun1979.googlepages.com.3It was obtained from the author.4http://en.wikipedia.org/wiki/Wikipedia_database.Porter Stemmer 5 for comparison.
In experimentswe select three evaluation metrics.The first metric is precision/recall/F-measure rep-resented as follows,p = ccorrectcextract, r = ccorrectcstandard, f = 2prp+ r , (6)where ccorrect is the total number of correctkeyphrases extracted by a method, cextract the to-tal number of automatic extracted keyphrases, andcstandard the total number of human-labeled stan-dard keyphrases.We note that the ranking order of extractedkeyphrases also indicates the method performance.An extraction method will be better than another oneif it can rank correct keyphrases higher.
However,precision/recall/F-measure does not take the orderof extracted keyphrases into account.
To address theproblem, we select the following two additional met-rics.One metric is binary preference measure(Bpref) (Buckley and Voorhees, 2004).
Bpref isdesirable to evaluate the performance consideringthe order in which the extracted keyphrases areranked.
For a document, if there are R correctkeyphrases within M extracted keyphrases by amethod, in which r is a correct keyphrase and n isan incorrect keyphrase, Bpref is defined as follows,Bpref = 1R?r?R1?
|n ranked higher than r|M .
(7)The other metric is mean reciprocal rank(MRR) (Voorhees, 2000) which is used to evaluatehow the first correct keyphrase for each document isranked.
For a document d, rankd is denoted as therank of the first correct keyphrase with all extractedkeyphrases, MRR is defined as follows,MRR =1|D|?d?D1rankd, (8)where D is the document set for keyphrase extrac-tion.Note that although the evaluation scores of mostkeyphrase extractors are still lower compared to5http://tartarus.org/?martin/PorterStemmer.370other NLP-tasks, it does not indicate the perfor-mance is poor because even different annotators mayassign different keyphrases to the same document.4.3 Influences of Parameters to TPRThere are four parameters in TPR that may influencethe performance of keyphrase extraction including:(1) window size W for constructing word graph, (2)the number of topics K learned by LDA, (3) dif-ferent settings of preference values pz(w), and (4)damping factor ?
of TPR.In this section, we look into the influences of theseparameters to TPR for keyphrase extraction.
Exceptthe parameter under investigation, we set parametersto the following values: W =10, K=1, 000, ?=0.3and pz(w) = pr(z|w), which are the settings whenTPR achieves the best (or near best) performance onboth NEWS and RESEARCH.
In the following tables,we use ?Pre.
?, ?Rec.?
and ?F.?
as the abbreviationsof precision, recall and F-measure.4.3.1 Window Size WIn experiments on NEWS, we find that the perfor-mance of TPR is stable when W ranges from 5 to 20as shown in Table 1.
This observation is consistentwith the findings reported in (Wan and Xiao, 2008b).Size Pre.
Rec.
F. Bpref MRR5 0.280 0.345 0.309 0.213 0.63610 0.282 0.348 0.312 0.214 0.63815 0.282 0.347 0.311 0.214 0.64620 0.284 0.350 0.313 0.215 0.644Table 1: Influence of window size W when the num-ber of keyphrases M=10 on NEWS.Similarly, when W ranges from 2 to 10, the per-formance on RESEARCH does not change much.However, the performance on NEWS will becomepoor when W = 20.
This is because the abstractsin RESEARCH (there are 121 words per abstract onaverage) are much shorter than the news articlesin NEWS (there are 704 words per article on av-erage).
If the window size W is set too large onRESEARCH, the graph will become full-connectedand the weights of links will tend to be equal, whichcannot capture the local structure information of ab-stracts for keyphrase extraction.4.3.2 The Number of Topics KWe demonstrate the influence of the number oftopics K of LDA models in Table 2.
Table 2 showsthe results when K ranges from 50 to 1, 500 andM =10 on NEWS.
We observe that the performancedoes not change much as the number of topicsvaries until the number is much smaller (K = 50).The influence is similar on RESEARCH which indi-cates that LDA is appropriate for obtaining topics ofwords and documents for TPR to extract keyphrases.K Pre.
Rec.
F. Bpref MRR50 0.268 0.330 0.296 0.204 0.632100 0.276 0.340 0.304 0.208 0.632500 0.284 0.350 0.313 0.215 0.6481000 0.282 0.348 0.312 0.214 0.6381500 0.282 0.348 0.311 0.214 0.631Table 2: Influence of the number of topics K whenthe number of keyphrases M=10 on NEWS.4.3.3 Damping Factor ?Damping factor ?
of TPR reconciles the influ-ences of graph walks (the first term in Eq.
(3)) andpreference values (the second term in Eq.
(3)) to thetopic-specific PageRank scores.
We demonstratethe influence of ?
on NEWS in Fig.
2.
This fig-ure shows the precision/recall/F-measure when ?
=0.1, 0.3, 0.5, 0.7, 0.9 and M ranges from 1 to 20.From this figure we find that, when ?
is set from 0.2to 0.7, the performance is consistently good.
Thevalues of Bpref and MRR also keep stable with thevariations of ?.4.3.4 Preference ValuesFinally, we explore the influences of different set-tings of preference values for TPR in Eq.(3).
In Ta-ble 3 we show the influence when the number ofkeyphrases M = 10 on NEWS.
From the table, weobserve that pr(z|w) performs the best.
The similarobservation is also got on RESEARCH.In keyphrase extraction task, it is required to findthe keyphrases that can appropriately represent thetopics of the document.
It thus does not want to ex-tract those phrases that may appear in multiple top-ics like common words.
The measure pr(w|z) as-signs preference values according to how frequentlythat words appear in the given topic.
Therefore, the3711 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.20.250.30.350.40.450.5Keyphrase NumberPrecision?=0.1?=0.3?=0.5?=0.7?=0.9(a) Precision1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.050.10.150.20.250.30.350.40.450.5Keyphrase NumberRecall?=0.1?=0.3?=0.5?=0.7?=0.9(b) Recall1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.10.120.140.160.180.20.220.240.260.280.3Keyphrase NumberF?measure?=0.1?=0.3?=0.5?=0.7?=0.9(c) F-measureFigure 2: Precision, recall and F-measure of TPR with ?=0.1, 0.3, 0.5, 0.7 and 0.9 when M ranges from 1to 20 on NEWS.common words will always be assigned to a rela-tively large value in each topic-specific PageRankand finally obtain a high rank.
pr(w|z) is thus not agood setting of preference values in TPR.
In the con-trast, pr(z|w) prefers those words that are focusedon the given topic.
Using pr(z|w) to set preferencevalues for TPR, we will tend to extract topic-focusedphrases as keyphrases.Pref Pre.
Rec.
F. Bpref MRRpr(w|z) 0.256 0.316 0.283 0.192 0.584pr(z|w) 0.282 0.348 0.312 0.214 0.638prod 0.259 0.320 0.286 0.193 0.587Table 3: Influence of three preference value settingswhen the number of keyphrases M=10 on NEWS.4.4 Comparing with Baseline MethodsAfter we explore the influences of parameters toTPR, we obtain the best results on both NEWS andRESEARCH.
We further select three baseline meth-ods, i.e., TFIDF, PageRank and LDA, to comparewith TPR.The TFIDF computes the ranking scores of wordsbased on words?
tfidf values in the document,namely R(w) = tfw ?
log(idfw).
While in PageR-ank (i.e., TextRank), the ranking scores of words areobtained using Eq.(2).
The two baselines do not usetopic information of either words or documents.
TheLDA computes the ranking score for each word us-ing the topical similarity between the word and thedocument.
Given the topics of the document d anda word w, We have used various methods to com-pute similarity including cosine similarity, predic-tive likelihood and KL-divergence (Heinrich, 2005),among which cosine similarity performs the best onboth datasets.
Therefore, we only show the results ofthe LDA baseline calculated using cosine similarity.In Tables 4 and 5 we show the compar-ing results of the four methods on both NEWSand RESEARCH.
Since the average number ofmanual-labeled keyphrases on NEWS is larger thanRESEARCH, we set M = 10 for NEWS and M =5 for RESEARCH.
The parameter settings on bothNEWS and RESEARCH have been stated in Section4.3.Method Pre.
Rec.
F. Bpref MRRTFIDF 0.239 0.295 0.264 0.179 0.576PageRank 0.242 0.299 0.267 0.184 0.564LDA 0.259 0.320 0.286 0.194 0.518TPR 0.282 0.348 0.312 0.214 0.638Table 4: Comparing results on NEWS when the num-ber of keyphrases M=10.Method Pre.
Rec.
F. Bpref MRRTFIDF 0.333 0.173 0.227 0.255 0.565PageRank 0.330 0.171 0.225 0.263 0.575LDA 0.332 0.172 0.227 0.254 0.548TPR 0.354 0.183 0.242 0.274 0.583Table 5: Comparing results on RESEARCHwhen thenumber of keyphrases M=5.From the two tables, we have the following obser-vations.3720.2 0.25 0.3 0.35 0.4 0.45 0.500.050.10.150.20.250.30.350.40.450.5PrecisionRecallTFIDFPageRankLDATPRFigure 3: Precision-recall results on NEWS when Mranges from 1 to 20.First, TPR outperform all baselines on bothdatasets.
The improvements are all statistically sig-nificant tested with bootstrap re-sampling with 95%confidence.
This indicates the robustness and effec-tiveness of TPR.Second, LDA performs equal or better thanTFIDF and PageRank under precision/recall/F-measure.
However, the performance of LDA un-der MRR is much worse than TFIDF and PageR-ank, which indicates LDA fails to correctly extractthe first keyphrase earlier than other methods.
Thereason is: (1) LDA does not consider the local struc-ture information of document as PageRank, and (2)LDA also does not consider the frequency infor-mation of words within the document.
In the con-trast, TPR enjoys the advantages of both LDA andTFIDF/PageRank, by using the external topic infor-mation like LDA and internal document structurelike TFIDF/PageRank.Moreover, in Figures 3 and 4 we show theprecision-recall relations of four methods on NEWSand RESEARCH.
Each point on the precision-recallcurve is evaluated on different numbers of extractedkeyphrases M .
The closer the curve to the upperright, the better the overall performance.
The resultsagain illustrate the superiority of TPR.4.5 Extracting ExampleAt the end, in Table 6 we show an example ofextracted keyphrases using TPR from a news arti-cle with title ?Arafat Says U.S.
Threatening to KillPLO Officials?
(The article number in DUC2001is AP880510-0178).
Here we only show the top10 keyphrases, and the correctly extracted ones0.3 0.32 0.34 0.36 0.38 0.4 0.4200.050.10.150.20.250.3PrecisionRecallTFIDFPageRankLDATPRFigure 4: Precision-recall results on RESEARCHwhen M ranges from 1 to 10.are marked with ?(+)?.
We also mark the num-ber of correctly extracted keyphrases after methodname like ?(+7)?
after TPR.
We also illustrate thetop 3 topics of the document with their topic-specific keyphrases.
It is obvious that the top topics,on ?Palestine?, ?Israel?
and ?terrorism?
separately,have a good coverage on the discussion objects ofthis article, which also demonstrate a good diversitywith each other.
By integrating these topic-specifickeyphrases considering the proportions of these top-ics, we obtain the best performance of keyphrase ex-traction using TPR.In Table 7 we also show the extracted keyphrasesof baselines from the same news article.
For TFIDF,it only considered the frequency properties of words,and thus highly ranked the phrases with ?PLO?which appeared about 16 times in this article, andfailed to extract the keyphrases on topic ?Israel?.LDA only measured the importance of words usingdocument topics without considering the frequencyinformation of words and thus missed keyphraseswith high-frequency words.
For example, LDAfailed to extract keyphrase ?political assassination?,in which the word ?assassination?
occurred 8 timesin this article.5 Related WorkIn this paper we proposed TPR for keyphrase ex-traction.
A pioneering achievement in keyphrase ex-traction was carried out in (Turney, 1999) which re-garded keyphrase extraction as a classification task.Generally, the supervised methods need manuallyannotated training set which is time-consuming andin this paper we focus on unsupervised method.373TPR (+7)PLO leader Yasser Arafat(+), Abu Jihad, KhalilWazir(+), slaying Wazir, political assassina-tion(+), Palestinian guerrillas(+), particularyPalestinian circles, Israeli officials(+), Israelisquad(+), terrorist attacks(+)TPR, Rank 1 Topic on ?Palestine?PLO leader Yasser Arafat(+), United States(+),State Department spokesman Charles Redman,Abu Jihad, U.S. government document, PalestineLiberation Organization leader, political assassi-nation(+), Israeli officials(+), alleged documentTPR, Rank 2 Topic on ?Israel?PLO leader Yasser Arafat(+), United States(+),Palestine Liberation Organization leader, Israeliofficials(+), U.S. government document, allegeddocument, Arab government, slaying Wazir, StateDepartment spokesman Charles Redman, KhalilWazir(+)TPR, Rank 3 Topic on ?terrorism?terrorist attacks(+), PLO leader Yasser Arafat(+),Abu Jihad, United States(+), alleged docu-ment, U.S. government document, Palestine Lib-eration Organization leader, State Departmentspokesman Charles Redman, political assassina-tion(+), full cooperationTable 6: Extracted keyphrases by TPR.Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becomingthe most widely used unsupervised approach forkeyphrase extraction.
Litvak and Last (2008) ap-plied HITS algorithm on the word graph of a docu-ment for keyphrase extraction.
Although HITS itselfworked the similar performance to PageRank, weplan to explore the integration of topics and HITS infuture work.
Wan (2008b; 2008a) used a small num-ber of nearest neighbor documents to provide moreknowledge for keyphrase extraction.
Some meth-ods used clustering techniques on word graphs forkeyphrase extraction (Grineva et al, 2009; Liu etal., 2009).
The clustering-based method performedwell on short abstracts (with F-measure 0.382 onRESEARCH) but poorly on long articles (NEWS withF-measure score 0.216) due to two non-trivial is-sues: (1) how to determine the number of clus-TFIDF (+5)PLO leader Yasser Arafat(+), PLO attacks, PLOoffices, PLO officials(+), PLO leaders, Abu Ji-had, terrorist attacks(+), Khalil Wazir(+), slayingwazir, political assassination(+)PageRank (+3)PLO leader Yasser Arafat(+), PLO officials(+),PLO attacks, United States(+), PLO offices, PLOleaders, State Department spokesman CharlesRedman, U.S. government document, allegeddocument, Abu JihadLDA (+5)PLO leader Yasser Arafat(+), Palestine LiberationOrganization leader, Khalil Wazir(+), Palestinianguerrillas(+), Abu Jihad, Israeli officials(+), par-ticulary Palestinian circles, Arab government,State Department spokesman Charles Redman,Israeli squad(+)Table 7: Extracted keyphrases by baselines.ters, and (2) how to weight each cluster and selectkeyphrases from the clusters.
In this paper we fo-cus on improving graph-based methods via topic de-composition, we thus only compare with PageRankas well as TFIDF and LDA and do not compare withclustering-based methods in details.In recent years, two algorithms were proposed torank web pages by incorporating topic informationof web pages within PageRank (Haveliwala, 2002;Nie et al, 2006).
The method in (Haveliwala, 2002),is similar to TPR which also decompose PageRankinto various topics.
However, the method in (Haveli-wala, 2002) only considered to set the preferencevalues using pr(w|z) (In the context of (Haveliwala,2002), w indicates Web pages).
In Section 4.3.4 wehave shown that the setting of using pr(z|w) is muchbetter than pr(w|z).Nie et al (2006) proposed a more complicatedranking method.
In this method, topical PageRanksare performed together.
The basic idea of (Nie et al,2006) is, when surfing following a graph link fromvertex wi to wj , the ranking score on topic z of wiwill have a higher probability to pass to the sametopic of wj and have a lower probability to pass toa different topic of wj .
When the inter-topic jumpprobability is 0, this method is identical to (Haveli-374wala, 2002).
We implemented the method and foundthat the random jumps between topics did not helpimprove the performance for keyphrase extraction,and did not demonstrate the results of this method.6 Conclusion and Future WorkIn this paper we propose a new graph-based frame-work, Topical PageRank, which incorporates topicinformation within random walk for keyphrase ex-traction.
Experiments on two datasets show thatTPR achieves better performance than other base-line methods.
We also investigate the influence ofvarious parameters on TPR, which indicates the ef-fectiveness and robustness of the new method.We consider the following research directions asfuture work.1.
In this paper we obtained latent topics us-ing LDA learned from Wikipedia.
We de-sign to obtain topics using other machine learn-ing methods and from other knowledge bases,and investigate the influence to performance ofkeyphrase extraction.2.
In this paper we integrated topic informationin PageRank.
We plan to consider topic infor-mation in other graph-based ranking algorithmssuch as HITS (Kleinberg, 1999).3.
In this paper we used Wikipedia to trainLDA by assuming Wikipedia is an exten-sive snapshot of human knowledge which cancover most topics talked about in NEWS andRESEARCH.
In fact, the learned topics arehighly dependent on the learning corpus.
Wewill investigate the influence of corpus selec-tion in training LDA for keyphrase extractionusing TPR.AcknowledgmentsThis work is supported by the National Natu-ral Science Foundation of China under Grant No.60873174.
The authors would like to thank AnetteHulth and Xiaojun Wan for kindly sharing theirdatasets.
The authors would also thank Xiance Si,Tom Chao Zhou, Peng Li for their insightful sug-gestions and comments.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022, January.C.
Buckley and E.M. Voorhees.
2004.
Retrieval evalu-ation with incomplete information.
In Proceedings ofSIGIR, pages 25?32.David Cohn and Huan Chang.
2000.
Learning to prob-abilistically identify authoritative documents.
In Pro-ceedings of ICML, pages 167?174.M.
Grineva, M. Grinev, and D. Lizorkin.
2009.
Extract-ing key terms from noisy and multi-theme documents.In Proceedings of WWW, pages 661?670.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.
InProceedings of WWW, pages 517?526.G.
Heinrich.
2005.
Parameter estimation for text anal-ysis.
Web: http://www.
arbylon.
net/publications/text-est.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of SIGIR, pages 50?57.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Proceed-ings of EMNLP, pages 216?223.J.M.
Kleinberg.
1999.
Authoritative sources in a hyper-linked environment.
Journal of the ACM, 46(5):604?632.T.K.
Landauer, P.W.
Foltz, and D. Laham.
1998.
An in-troduction to latent semantic analysis.
Discourse Pro-cesses, 25:259?284.Marina Litvak and Mark Last.
2008.
Graph-based key-word extraction for single-document summarization.In Proceedings of the workshop Multi-source Mul-tilingual Information Extraction and Summarization,pages 17?24.Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.2009.
Clustering to find exemplar terms for keyphraseextraction.
In Proceedings of EMNLP, pages 257?266.C.D.
Manning and H. Schutze.
2000.
Foundations ofstatistical natural language processing.
MIT Press.Rada Mihalcea and Paul Tarau.
2004.
Textrank: Bring-ing order into texts.
In Proceedings of EMNLP, pages404?411.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.WordNet: An on-line lexical database.
InternationalJournal of Lexicography, 3:235?244.Thuy Nguyen and Min-Yen Kan. 2007.
Keyphrase ex-traction in scientific publications.
In Proceedings ofthe 10th International Conference on Asian Digital Li-braries, pages 317?326.375Lan Nie, Brian D. Davison, and Xiaoguang Qi.
2006.Topical link analysis for web search.
In Proceedingsof SIGIR, pages 91?98.P.
Over, W. Liggett, H. Gilbert, A. Sakharov, andM.
Thatcher.
2001.
Introduction to duc-2001: An in-trinsic evaluation of generic news text summarizationsystems.
In Proceedings of DUC2001.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The pagerank citation ranking: Bringing order to theweb.
Technical report, Stanford Digital Library Tech-nologies Project, 1998.Peter D. Turney.
1999.
Learning to extract keyphrasesfrom text.
National Research Council Canada, In-stitute for Information Technology, Technical ReportERB-1057.Peter D. Turney.
2000.
Learning algorithmsfor keyphrase extraction.
Information Retrieval,2(4):303?336.E.M.
Voorhees.
2000.
The trec-8 question answeringtrack report.
In Proceedings of TREC, pages 77?82.Xiaojun Wan and Jianguo Xiao.
2008a.
Collabrank:Towards a collaborative approach to single-documentkeyphrase extraction.
In Proceedings of COLING,pages 969?976.Xiaojun Wan and Jianguo Xiao.
2008b.
Single documentkeyphrase extraction using neighborhood knowledge.In Proceedings of AAAI, pages 855?860.376
