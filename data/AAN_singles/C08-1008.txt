Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 57?64Manchester, August 2008Weakly supervised supertagging with grammar-informed initializationJason BaldridgeDepartment of LinguisticsThe University of Texas at Austinjbaldrid@mail.utexas.eduAbstractMuch previous work has investigated weaksupervision with HMMs and tag dictionar-ies for part-of-speech tagging, but therehave been no similar investigations for theharder problem of supertagging.
Here, Ishow that weak supervision for supertag-ging does work, but that it is subject tosevere performance degradation when thetag dictionary is highly ambiguous.
I showthat lexical category complexity and infor-mation about how supertags may combinesyntactically can be used to initialize thetransition distributions of a first-order Hid-den Markov Model for weakly supervisedlearning.
This initialization proves moreeffective than starting with uniform tran-sitions, especially when the tag dictionaryis highly ambiguous.1 IntroductionSupertagging involves assigning words lexical en-tries based on a lexicalized grammatical theory,such as Combinatory Categorial Grammar (CCG)(Steedman, 2000) Tree-adjoining Grammar (Joshi,1988), or Head-driven Phrase Structure Grammar(Pollard and Sag, 1994).
Supertag sets are largerthan part-of-speech (POS) tag sets and their ele-ments are generally far more articulated.
For ex-ample, the English verb join has the POS VB andthe CCG category ((Sb\NP)/PP)/NP in CCG-bank (Hockenmaier and Steedman, 2007).
Thiscategory indicates that join requires a noun phrasec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.to its left, another to its right, and a prepositionalphrase to the right of that.Supertags convey such detailed syntactic sub-categorization information that supertag disam-biguation is referred to as almost parsing (Banga-lore and Joshi, 1999).
Standard sequence predic-tion models are highly effective for supertagging,including Hidden Markov Models (Bangalore andJoshi, 1999; Nielsen, 2002), Maximum EntropyMarkov Models (Clark, 2002; Hockenmaier et al,2004; Clark and Curran, 2007), and ConditionalRandom Fields (Blunsom and Baldwin, 2006).The original motivation for supertags?parse pre-filtering for lexicalized grammars?of Bangaloreand Joshi (1999) has been realized to good effect:the supertagger of Clark and Curran (2007) pro-vides staged n-best lists of multi-tags that dramat-ically improve parsing speed and coverage withoutmuch loss in accuracy.
Espinosa et al (2008) haveshown that hypertagging (predicting the supertagassociated with a logical form) can improve bothspeed and accuracy of wide-coverage sentence re-alization with CCG.
Supertags have gained fur-ther relevance as they are increasingly used as fea-tures for other tasks, including machine translation(Birch et al, 2007; Hassan et al, 2007).Supertaggers typically rely on a significantamount of carefully annotated sentences.
As withmany problems, there is pressing need to findstrategies for reducing the amount of supervisionrequired for producing accurate supertaggers, butas yet, no one has explored the use of weak super-vision for the task.
In particular, there are many di-alog systems which rely on hand-crafted lexiconsthat both provide a starting point for bootstrappinga supertagger and which could benefit greatly fromsupertag pre-parse filter.
For example, the dialogsystem used by Kruijff et al (2007) uses a hand-57crafted CCG grammar for OpenCCG (White andBaldridge, 2003).
It is important to stress that thereare many such uses of CCG and related frame-works which do not rely on first annotating (even asmall number of) sentences in a corpus: these de-fine a lexicon that maps from words to categories(supertags) for a particular domain/application.This scenario is a natural fit for learning taggersfrom tag dictionaries using hidden Markov mod-els with Expectation-Maximization (EM).
Here,I investigate such weakly supervised learning forsupertagging and demonstrate the importance ofproper initialization of the tag transition distribu-tions of the HMM.
In particular, such initializa-tion can be done using inherent properties of theCCG formalism itself regarding how categories1may combine.
Informed initialization should helpwith supertagging for two reasons.
First, cate-gories have structure?lacking in POS tags?waitingto be exploited.
For example, it is far more likelya priori to see the category sequence (S\NP)/NPNP/N than the sequence S/S NP\NP.
Given thecategories for a word, this information can be usedto influence our expectations about categories foradjacent words.
Second, this kind of informationtruly matters for the task: a key aspect of supertag-ging that differentiates it from POS tagging is thatthe contextual information is much more importantfor the former.
Lexical probabilities handle mostof the ambiguity for POS tagging, but supertagsare inherently about context and, furthermore, lex-ical ambiguity is much greater for supertagging,making lexical probabilities less effective.I start by defining a distribution over lexicalcategories and then use this distribution as partof creating a CCG-informed transition distributionthat appropriately breaks the symmetry of uniformHMM initialization.
After describing how thesecomponents are included in the HMM, I describeexperiments with CCGbank varying the ambiguityof the lexicon provided.
I show that using knowl-edge about the formalism consistently improvesperformance, and is especially important as cate-gorial ambiguity increases.2 Lexical category distributionThe categories of CCG are an inductively definedset containing elements that are either atomic ele-ments or (curried) functions specifying the canon-1For the rest of the paper, I will refer to categories rathersupertags, but will still refer to the task as supertagging.ical linear direction in which they seek their argu-ments.
Some example entries from CCGbank are:the := NPnb/Nof := (NP\NP)/NPof := ((S\NP)\(S\NP)/NPwere := (Sdcl\NP)/(Spss\NP)buy := (Sdcl\NP)/NPbuy := ((((Sb\NP)/PP)/PP)/(Sadj\NP))/NPWords can be associated with multiple categories;the distribution over these categories is typicallyquite skewed.
For example, the first entry for buyoccurs 33 times in CCGbank, compared with justonce for the second.
That the simpler category ismore prevalent is unsurprising: a general strategywhen creating CCG lexicons is to use simpler cate-gories whenever possible.
This points to the possi-bility of defining distributions over CCG lexiconsbased on measures of the complexity of categories.I use a simple distribution here: given a lexicon L,the probability of a category i is inversely propor-tional to its complexity:?i=1complexity(ci)?j?L1complexity(cj)(1)Here, a very simple complexity measure isassumed: the number of subcategories (to-kens) contained in a category.2For example,((S\NP)\(S\NP)/NP contains 9: S (twice), NP(thrice), S\NP (twice), (S\NP)\(S\NP), and((S\NP)\(S\NP)/NP.The tag transition distribution defined in the nextsection uses ?
to bias transitions toward simplercategories, e.g., preferring the first category forbuy over the second.
Performance when using ?is compared to using a uniform distribution.Other distributions could be given, e.g., onewhich gives more mass to adjunct categories suchas (S\NP)\(S\NP) than to ones which are oth-erwise similar but do not display such symmetry,like (S/NP)\(NP\S).
However, the most impor-tant thing for present purposes is that simpler cate-gories are more likely than more complex ones.This distribution imposes no internal struc-ture on the likelihood of a lexicon.
As faras ?
is concerned, lexicons can as well havethe category (S\NP)\NP for transitive verbs and((S/NP)/NP)/NP for ditransitive verbs, eventhough this is a highly unlikely pattern since we2This worked better than using category arity or numberof unique subcategory types.58Vinken will join the board as non?executive directorS/(S\NP) (S\NP)/(S\NP) ((S\NP)/PP)/NP NP/N N PP/NP NP/N N> >NP NP> >(S\NP)/PP PP>S\NP>S\NP>SFigure 1: Normal form CCG derivation, using only application rules.would expect both types of verbs to seek their ar-guments in the same direction.
Languages alsotend to prefer lexicons with one or the other slashdirection predominating (Villavicencio, 2002).
Inthe future, it would be interesting to considerBayesian approaches that could encode more com-plex structure and assign priors over distributionsover lexicons, building on these observations.An aspect of CCGbank that relevant for ?iisthat some categories actually are not true cate-gories.
For example, many punctuation ?cate-gories?
are given as LRB, ., :, etc.
In mostgrammars, the category of ?.?
is usually assumedto be S\S.
The grammatical behavior of suchpseudo-categories is handled via special rules inthe parsers of Hockenmaier and Steedman (2007)and Clark and Curran (2007).
I relabeled three ofthese: , to NP\NP, .
to S\S and ; to (S\S)/S.
Asingle best change was not clear for others such asLRB and :, so they were left as is.3 Category transition distributionCCG analyses of sentences are built up from lex-ical categories combining to form derived cate-gories, until an entire sentence is reduced to a sin-gle derived category with corresponding depen-dencies.
One of CCG?s most interesting linguis-tic properties is it allows alternative constituents.Consider the derivations in Figures 1 and 2, whichshow a normal form derivation (Eisner, 1996) andfully incremental derivation, respectively.
Bothproduce the same dependencies, guaranteed by thesemantic consistency of CCG?s rules (Steedman,2000).
This property of CCG of supporting mul-tiple derivations of the same analysis has beentermed spurious ambiguity.
However, the extraconstituents are anything but spurious: they areimplicated in a range of CCG (along with otherforms of categorial grammar) linguistic analyses,including coordination, long-distance extraction,intonation, and incremental processing.This all boils down to associativity: just as(1 + (4 + 2)) = ((1 + 4) + 2) = 7, CCG ensuresthat (Ed?
(saw?Ted)) = ((Ed?saw)?Ted) = S Suchmultiple derivations arise when adjacent categoriescan combine through either application or compo-sition.
Thus, we would expect that the lexical cat-egories needed to analyze an entire sentence willmore often than not be able to combine with theirimmediate neighbors.
For example, six of sevenpairs of adjacent lexical categories in the sentencein Figure 1 can combine.
Only N PP/NP of boardas cannot.3This observation can be used in different waysby different models for CCG supertagging.
Forexample, discriminative tagging models could in-clude features that capture whether or not the cur-rent supertag can combine with the previous oneand possibly via which CCG rule.
Here, I showhow it can be used to provide a non-uniform start-ing point for the transition distributions ?j|iin afirst-order Hidden Markov Model.
This is similarto how Grenager et al (2005) use diagonal initial-ization in an HMM for field segmentation to en-courage the model to remain in the same state (andthus predict the same label for adjacent words).For CCG supertagging, the initialization shoulddiscourage diagonalization and establish a prefer-ence for some transitions over others.There are many ways to define such a startingpoint.
The simplest would be to reserve a smallpart of the mass spread uniformly over categorypairs which cannot combine and then spread therest of the mass uniformly over those which can.However, we can provide a more refined distri-bution, ?j|i, by incorporating the lexical categorydistribution ?idefined in the previous section toweight these transitions according to this furtherinformation.
In a similar manner to Grenager et al(2005), I define ?
as follows:3I make the standard assumption that type-raising is per-formed in the lexicon, so the possibility of combining thesethrough type-raising plus composition is not available.59Vinken will join the board as non?executive directorS/(S\NP) (S\NP)/(S\NP) ((S\NP)/PP)/NP NP/N N PP/NP NP/N N>BS/(S\NP)>B2(S/PP)/NP>B(S/PP)/N>S/PP>BS/NP>BS/N>SFigure 2: Incremental CCG derivation, using both application and composition (B) rules.
?j|i= (1??
)?j+ ?
?
?
(i, j) ??j?k?L|?
(i,k)?k(2)where ?
(i, j) is an indicator function that returns1 if categories ciand cjcan combine when ciim-mediately precedes cj, ?
is a global parameter thatspecifying the total probability of transitions thatare combinable from i.
Each j receives a propor-tion of ?
according to its lexical prior probabilityover the sum of the lexical prior probabilities forall categories that combine with i.
For the experi-ments in this paper, ?
was set to .95.
For the mod-els referred to as ?U and ?U-EM in section 5, theuniform lexical probability 1/|C| is used for ?i.For ?
(i, j), I use the standard rules assumedfor CCGbank parsers: forward and backwardapplication (>, <), order-preserving com-position (>B, <B), and backward crossedcomposition (<B?)
for S-rooted categories.Thus, ?
(NP,S\NP)=1, ?(S/NP,NP/N)=1,?
((S\NP)/NP, (S\NP)\(S\NP))=1 and?(S/NP,NP\NP)=0.
For application, left-ward and rightward arguments are handledseparately by assuming that it would be possi-ble to consume all preceding arguments of thefirst category and all following arguments ofthe second.
So, ?
((S/NP)\S,NP/N)=1 and?
(NP, (S\NP)/NP)=1.
Unification on categoriesis standard (so ?
(NP[nb],S\NP)=1), except thatN unifies with NP only when N is the argument:?
(N,S\NP)=1, but ?(NP/N,NP)=0.
This isto deal with the fact that CCGbank representsmany words with N (e.g., Mr.|N/N Vinken|Nis|(S[dcl]\NP)/NP) and assumes that a parser willinclude the unary type changing rule N?NP.The HMM also has initial and final probabili-ties; distributions can be defined based on whichcategories are likely to start or end a sentence.
Forthis, I assume only that categories which seek ar-guments to the left (e.g., S\NP) are less likelyat the beginning of a sentence and those whichseek rightward arguments are less likely at the end.The initializations for these are defined similarlyto the transition distribution, substituting functionsnoLeftArgs(i) and noRightArgs(i) for ?
(i, j).4 ModelA first-order Hidden Markov Model (bitag HMM)is used for bootstrapping a supertagger from a lex-icon.
See Rabiner (1989) for an extensive intro-duction to and discussion of HMMs.
There areseveral reasons why this is an attractive taggingmodel here.
First, though extra context in theform of tritag transition distributions or other tech-niques can improve supervised POS tagging accu-racy, the accuracy of bitag HMMs is not far behind.The goal here is to investigate the relative gainsof using CCG-based information in weakly super-vised HMM learning.
Second, the expectation-maximization algorithm for bitag HMMs is effi-cient and has been shown to be quite effective foracquiring accurate POS taggers given only a lex-icon (tag dictionary) and certain favorable condi-tions (Banko andMoore, 2004).
Third, the model?ssimplicity makes it straightforward to test the ideaof CCG-initialization on tag transitions.Dirichlet priors can be used to bias HMMs to-ward more skewed distributions (Goldwater andGriffiths, 2007; Johnson, 2007), which is espe-cially useful in the weakly supervised setting con-sidered here.
Following Johnson (2007), I use vari-ational Bayes EM (Beal, 2003) during the M-stepfor the transition distribution:?l+1j|i=f(E[ni,j] + ?i)f(E[ni] + |C | ?
?i)(3)f(v) = exp(?
(v)) (4)60?
(v) ={g(v ?12) if v > 7?
(v + 1) ?1vo.w.
(5)g(x) ?
log(x) +124x2?7960x4+318064x6?12730720x8(6)where V is the set of word types, ?
is the digammafunction (which is approximated by g), and ?iisthe hyperparameter of the Dirichlet priors.
In allexperiments, the ?iparameters were set symmet-rically to .005.For experiments using the transition prior ?j|i,the initial expectations of the model were set asE[ni,j] = |Ei| ?
?j|iand E[ni] = |Ei|, where Eiis the set of emissions for category ci.
The uni-form probability1|C |was used in place of ?j|iforstandard HMM initialization.The emission distributions use standard EM ex-pectations with more mass reserved for unknownsfor tags with more emissions as follows:4?l+1k|i=E[ni,k] + |Ei| ?1|V|E[ni] + |Ei|(7)The Viterbi algorithm is used for decoding.5 ExperimentsCCGbank (Hockenmaier and Steedman, 2007) is atranslation of phrase structure analyses of the PennTreebank into CCG analyses.
Here, I consideronly the lexical category annotations and ignorederivations.
The standard split used for weakly su-pervised HMM tagging experiments (Banko andMoore, 2004; Wang and Schuurmans, 2005) isused: sections 0-18 for training (train), 19-21 fordevelopment (dev), and 22-24 for testing (test).
Allparameters and models were developed using dev.The test set was used only once to obtain the per-formance figures reported here.Counts for word types, word tokens and sen-tences for each data set are given in Table 1.
Intrain, there are 1241 distinct categories, the am-biguity per word type is 1.69, and the maximumnumber of categories for a single word type is 126.This is much greater than for POS tags in CCG-bank, for which there are 48 POS tags with an av-4I also experimented with a Dirichlet prior on the emis-sions, but it performed worse.
Using a symmetric priorwas actually detrimental, while performance within a percentof those achieved with the above update was achieved withDirichlet hyperparameters set relative to |Ei|/|V|.Dataset Types Tokens Sentencestrain 43063 893k 38,015dev 14961 128k 5484test 13898 127k 5435Table 1: Basic statistics for the datasets.erage ambiguity of 1.17 per word and a maximumof 7 tags in train.5The set of supertags was not reduced: any cat-egory found in the data used to initialize a lexi-con was considered.
This is one of the advan-tages of the HMM over using discriminative mod-els, where typically only supertags seen at least 10times in the training material are utilized for effi-ciency (Clark and Curran, 2007).
Ignoring somesupertags makes sense when building supervisedsupertaggers for pre-parse filtering, but not forlearning from lexicons, where we cannot assumewe have such frequencies.For supervised training with the HMM on train,the performance is 87.6%.
This compares to91.4% for the C&C supertagger.
The accuracy ofthe HMM, though quite a bit lower than that ofC&C, is still quite good, indicating that it is an ad-equate model for the task.
Note also that it usesonly the words themselves and does not rely onPOS tags.
The performance of the C&C taggerwas obtained by training the C&C POS tagger onthe given dataset and tagging the evaluation mate-rial with it.
Finally, the HMM trains in just a fewseconds as opposed to over an hour.6Five different weakly supervised scenarios areevaluated: (1) standard EM with 50 iterations(EM), (2) ?
initialization with uniform lexicalprobabilities w/o EM (?U), (3) ?
with ?
proba-bilities w/o EM (??
), (4) ?
with uniform lexicalprobabilities and 10 EM iterations, and (5) ?
with?
and 10 EM iterations.7These scenarios com-pare the effectiveness of standard EM with the useof grammar informed transitions; these in turn areof two varieties ?
one using a uniform lexical prioror one that is biased in favor of less complex cate-gories according to ?.As Banko and Moore (2004) discovered when5Note that the POS tag information is not used in theseexperiments, except for by the C&C tagger.6It should be stressed that the goal of this paper is not tocompete on supervised performance with C&C; instead, thiscomparison shows that the HMM supervised performance isreasonable and is thus relevant for bootstrapping.7The number of iterations for standard and grammar in-formed iteration were determined by performance on dev.61reimplementing several previous HMMs for POStagging, the lexicons had been limited to containonly tags occurring above a particular frequency.For POS tagging, this keeps a cleaner lexicon thatavoids errors in annotated data (such as the taggedas VB) and rare tags (such as a tagged as SYM).When learning from a lexicon alone, such elementsreceive the same weight as their other (correct ormore fundamental) tags in initializing the HMM.The problem of rare tags turns out to be very im-portant for weakly supervised CCG supertagging.8To consider the effect of the CCG-based initial-ization for lexicons with differing ambiguity, I usetag cutoffs that remove any lexical entry containinga category that appears with a particular word lessthan X% of the time (Banko and Moore, 2004),as well as using no cutoffs at all.
Recall that thegoal of these experiments is to investigate the rela-tive difference in performance between using thegrammar-based initialization or not, given some(possibly hand-crafted) lexicon.
Lexicon cutoffsactually constitute a strong source of supervisionbecause they use tag frequencies (which would notbe known for a hand-crafted lexicon), so it shouldbe stressed that they are used here only so that thisrelative performance can be measured for differentambiguity levels.Table 2 provides accuracy for ambiguouswords(and not including punctuation) for the five scenar-ios, varying the cutoff to measure the effect of pro-gressively allowing more lexical ambiguity (andmuch rarer categories).
The number of ambiguous,non-punctuation tokens is 101,167.The first thing to note is performance given onlythe lexicon and the ?U or ??
initialization ofthe transitions.
These correspond to taggers whichhave only been given the lexicon and have not uti-lized any data to improve their estimates of thetransition and emission probabilities.
Interestingly,both do quite well with a clean lexicon: see thecolumns under ?U and ??.
These indicate thatinitializing the transitions based on whether cate-gories can combine does indeed appropriately cap-ture key aspects of category transitions.
Further-more, using the lexical category distribution (??
)to create the transition initialization provides a bet-ter starting point than the uniform one (?U), espe-8CCGbank actually corrects many errors in the Penn Tree-bank, and does not suffer as much from mistagged examples.However, there were two instances of an ill-formed category((S[b]\NP)/NP)/ in wsj 0595 for the words own and keep.These were corrected to (S[b]\NP)/NP.Cutoff EM ?U ??
?U-EM ?
?-EM.1 77.4 73.1 74.7 80.0 79.6.05 69.1 70.6 72.5 79.2 79.2.01 60.2 62.2 65.0 75.4 76.7.005 52.2 57.8 59.0 72.5 73.8.001 41.3 45.5 48.2 63.0 67.6None 33.0 33.9 37.8 52.9 56.1Table 2: Performance on ambiguous word typesof the HMM with standard EM (uniform startingtransitions), just the initial ?
transitions (?U and??
), and EM initialized with ?U and ?
?, forlexicons with varied cutoffs.
Note also that thesescores do not include punctuation.cially as lexical ambiguity increases.Next, note that both ?U-EM and ?
?-EM beatthe randomly initialized EM for all cutoff levels.For the 10% tag cutoff (the first row), there is anabsolute difference of over 2% for both.9As theambiguity increases, the grammar-informed ini-tialization has a much stronger effect.
In the ex-treme case of using no cutoff at all (the None rowof Table 2), ?U-EM and?
?-EM beat EM by 19.9%and 23.1%, respectively.
Finally, using the lexicalcategory distribution ?
instead of a uniform oneis much more effective when there is more lexi-cal ambiguity (e.g., compare the .01 through Nonerows of the ?U-EM and ?
?-EM columns), buthas a negligible effect with less ambiguity (rows.05 and .01).
This demonstrates that the grammar-based initialization can be effectively exploited ?
itis in fact crucial for improving performance whenwe are given much more ambiguous lexicons.The majority of errors with ?
?-EM involvemarking adjectives (N/N) as nouns (N) or viceversa, and assigning the wrong prepositional cat-egory (usually the simpler noun phrase post-modifier (NP\NP)/NP instead of the verb phrasemodifier ((S\NP)\(S\NP))/NP.
Both of thesekinds of errors, and others, could potentially becorrected if the categories proposed by the taggerwere further filtered by an attempt to parse eachsentence with the categories.6 Related workThe idea of using knowledge from the formalismfor constraining supertagging originates with Ban-9For comparison with the performance of 87.6% for thefully supervised HMM on all tokens, ?-EM achieves 82.1%and 58.9% using a cutoff of .1 or no cutoff, respectively.62galore and Joshi (1999).
They used constraintsbased on how elementary trees of Tree-AdjoiningGrammar could or could not combine as filters toblock out tags that do not fit in certain locationsin the string.
My approach is different is sev-eral ways.
First, they dealt with fully supervisedsupertagging; here I show that using this knowl-edge is important for weakly supervised supertag-ging where we are given only a tag dictionary (lex-icon).
Second, my approach encodes grammar-based cues only as an initial bias, so categories arenever explicitly filtered.
Finally, I use CCG ratherthan TAG, which makes it possible to exploit amuch higher degree of associativity in derivations.This in turn makes it easier to utilize prior knowl-edge about adjacent contexts ?
precisely what isneeded for using the grammar to influence the tran-sition probabilities of a bigram HMM.On the other hand, Bangalore and Joshi (1999)use constraints that act at greater distances thanI have considered here.
For example, if onewishes to provide a word with the category((S\NP)/PP)/NP, then there should be a wordwith a category which results in a PP two or morewords to its right ?
this is something which thebigram transitions considered here cannot capture.An interesting way to extend the present approachwould be to enforce such patterns as posterior con-straints during EM (Graca et al, 2007).Recent work considers a damaged tag dictionaryby assuming that tags are known only for wordsthat occur more than once or twice (Toutanovaand Johnson, 2007).
A very interesting aspect ofthis work is that they explicitly model ambiguityclasses to exploit commonality in the lexicon be-tween different word forms, which could be evenmore useful for supertagging.In a grammar development context, it is oftenthe case that only some of the categories for a wordhave been assigned.
This is the scenario consid-ered by Haghighi and Klein (2006) for POS tag-ging: how to construct an accurate tagger givena set of tags and a few example words for eachof those tags.
They use distributional similarity ofwords to define features for tagging that effectivelyallow such prototype words to stand in for others.This idea could be used with my approach as well;the most obvious way would be to use prototypewords to suggest extra categories (beyond the tagdictionary) for known words and a reduced set ofcategories for unknown words.Other work aims to do truly unsupervised learn-ing of taggers, such as Goldwater and Griffiths(2007) and Johnson (2007).
No tag dictionariesare assumed, and the models are parametrized withDirichlet priors.
The states of these models implic-itly represent tags; however, it actually is not clearwhat the states in such models truly represent: theyare (probably interesting) clusters that may or maynot correspond to what we normally think of asparts-of-speech.
POS tags are relatively inert, pas-sive elements in a grammar, whereas CCG cate-gories are the very drivers of grammatical analy-sis.
That is, syntax is projected, quite locally, bylexical categories.
It would thus be interesting toconsider the induction of categories with grammar-based priors with such models.7 ConclusionI have shown that weakly supervised learning canindeed be used to induce supertaggers from a lex-icon mapping words to their possible categories,but that the extra ambiguity in the supertaggingtask over that of POS tagging makes performancemuch more sensitive to rare categories that occurin larger, more ambiguous lexicons.
However, Ihave also shown that the CCG formalism itself canprovide the basis for useful distributions over lexi-cal categories and tag transitions in a bitag HMM.By using these distributions to initialize the HMM,it is possible to improve performance regardless ofthe underlying ambiguity.
This is especially im-portant for reducing error when the lexicon usedfor bootstrapping is highly ambiguous and con-tains very rare categories.AcknowledgmentsThanks to the UT Austin Natural Language Learn-ing group and three anonymous reviewers for use-ful comments and feedback.
This work was sup-ported by NSF grant BCS-0651988 and a FacultyResearch Assignment from UT Austin.ReferencesBangalore, Srinivas and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, 25(2):237?265.Banko, Michele and Robert C. Moore.
2004.
Part-of-speech tagging in context.
In Proceedings of COL-ING.63Beal, Matthew.
2003.
Variational Algorithms for Ap-proximate Inference.
Ph.D. thesis, University ofCambridge.Birch, Alexandra, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the 2nd Workshop onStatistical Machine Translation.Blunsom, Phil and Timothy Baldwin.
2006.
Multi-lingual deep lexical acquisition for HPSGs via su-pertagging.
In Proceedings of EMNLP 06, pages164?171.Clark, Stephen and James Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4).Clark, Stephen.
2002.
Supertagging for combina-tory categorial grammar.
In Proceedings of TAG+6,pages 19?24, Venice, Italy.Eisner, Jason.
1996.
Efficient normal-form parsing forcombinatory categorial grammars.
In Proceedings ofthe 35th ACL.Espinosa, Dominic, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proceedings of ACL-08: HLT,pages 183?191, Columbus, Ohio, June.Goldwater, Sharon and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the 45th ACL.Graca, Joao, Kuzman Ganchev, and Ben Taskar.
2007.Expectation maximization, posterior constraints, andstatistical alignment.
In Proceedings of NIPS07.Grenager, Trond, Dan Klein, and Christopher D. Man-ning.
2005.
Unsupervised learning of field segmen-tation models for information extraction.
In Pro-ceedings of the 43rd ACL, pages 371?378.Haghighi, Aria and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofHLT-NAACL 2006.Hassan, Hany, Khalil Sima?an, and Andy Way.
2007.Supertagged phrase-based statistical machine trans-lation.
In Proceedings of the 45th ACL.Hockenmaier, Julia and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Hockenmaier, Julia, Gann Bierner, and JasonBaldridge.
2004.
Extending the coverage of aCCG system.
Research in Language and Computa-tion, 2:165?208.Johnson, Mark.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of the EMNLP-CoNLL 2007.Joshi, Aravind.
1988.
Tree Adjoining Grammars.
InDowty, David, Lauri Karttunen, and Arnold Zwicky,editors, Natural Language Parsing, pages 206?250.Cambridge University Press, Cambridge.Kruijff, Geert-Jan M., Hendrik Zender, Patric Jensfelt,and Henrik I. Christensen.
2007.
Situated dialogueand spacial organization: What, where,...and why?International Journal of Advanced Robotic Systems,4(1):125?138.Nielsen, Leif.
2002.
Supertagging with combinatorycategorial grammar.
In Proceedings of the SeventhESSLLI Student Session, pages 209?220.Pollard, Carl and Ivan Sag.
1994.
Head DrivenPhrase Structure Grammar.
CSLI/Chicago Univer-sity Press, Chicago.Rabiner, Lawrence.
1989.
A tutorial on HiddenMarkov Models and selected applications in speechrecognition.
Proceedings of the IEEE, 77(2):257?286.Steedman, Mark.
2000.
The Syntactic Process.
TheMIT Press, Cambridge Mass.Toutanova, Kristina and Mark Johnson.
2007.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In Proceedings of NIPS 20.Villavicencio, Aline.
2002.
The Acquisition ofa Unification-Based Generalised Categorial Gram-mar.
Ph.D. thesis, University of Cambridge.Wang, Qin Iris and Dal Schuurmans.
2005.
Improvedestimation for unsupervised part-of-speech tagging.In EEE International Conference on Natural Lan-guage Processing and Knowledge Engineering.White, Michael and Jason Baldridge.
2003.
Adaptingchart realization to CCG.
In Proceedings of ENLG.64
