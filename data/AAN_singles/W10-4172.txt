K-means and Graph-based Approaches for Chinese Word SenseInduction TaskLisha Wang       Yanzhao Dou       Xiaoling Sun       Hongfei LinComputer Science DepartmentDalian University of Technology{lisawang0110,yanzhaodou}@gmail.comxlsun@mail.dlut.edu.cn hflin@dlut.edu.cnAbstractThis paper details our experimentscarried out at Word Sense Induction task.For the foreign language (especiallyEnglish), there have been many studiesof word sense induction (WSI), and theapproaches and the techniques are moreand more mature.
However, the study ofChinese WSI is just getting started, andthere has not been a better way to solvethe problems encountered.
WSI can bedivided into two categories: supervisedmanner and unsupervised manner.
But inthe light of the high cost of supervisedmanner, we introduce novel solutions toautomatic and unsupervised WSI.
In thispaper, we propose two different systems.The first one is called K-means-basedChinese word sense induction in anunsupervised manner while the secondone is graph-based Chinese word senseinduction.
In the experiments, the firstsystem has achieved a 0.7729 Fscore onaverage while the second one hasachieved a 0.6067 Fscore.1 IntroductionNo matter in which kind of language, ambiguousterms always exist, Chinese is also notexceptional.
According to statistics, although thepercent of ambiguous terms in Chinesedictionary is only about 14.8%, the frequency ofthem is up to 42% in Chinese corpora.
Thisphenomenon shows that the number ofambiguous terms is small in natural language,but their frequency is extremely high.
Therefore,the key step in natural language processing(NLP) is to identify the specific meaning of agiven target word according to its context.
Inthis task, the input to a WSI algorithm is thesentences including the same ambiguous term,and our task is to cluster these sentences intodifferent categories according to the meanings ofthis ambiguous term in every sentence.
Thestudy of WSI is earlier abroad and there hasbeen a set of well-developed theories by now.However, the start of studying Chinese WSI islater and we need to find a better and appropriateway for Chinese WSI.
In this paper, we developtwo different systems.
The first one is based onK-means algorithm which optimizes the initialcenters and a Chinese thesaurus - TongYiCiCiLin is used to solve the problem of sparsenessof a sentence?s vector.
The second one is acombination approach of graph-based clusteringand K-means algorithm.
We choose ChineseWhisper as the graph-based clustering approach.2 K-means-based Chinese WSI in anUnsupervised MannerSince the number of total meanings of anambiguous term has been given in this task, ourgoal is to cluster those sentences which containthe same ambiguous term in an unsupervisedmanner.
In this condition our primary problem isthe selection of a suitable clustering method.Clustering algorithms are generally dividedinto two categories, namely partitioningclustering algorithm and hierarchical clusteringalgorithm.
Partitioning clustering algorithm isusually selected when the number of finalclusters is known.
Consequently, we need toinput a parameter K as the number.
Typicalpartitioning clustering algorithm contains K-means, K-medoids, CLARANS and so on.Among them, K-means clustering algorithm iswidely used and relatively simple.
Hierarchicalclustering algorithms are not required to inputany parameters, which is their advantagecompared to partitioning clustering algorithms.Typical hierarchical clustering algorithmscontain BIRCH algorithm, DBSCAN algorithm,CURE algorithm and so on.Considering the characters of WSI (e.g.
thetotal number of a target word?s sense has beengiven in advance), we should select partitioningclustering algorithm.
In addition, considering thequality, the performance, and the degree ofdifficulty while being implemented among allkinds of partitioning clustering algorithms, wefinally decide to use k-means algorithm, but wehave improved it in order to obtain betterclustering performance.2.1 Traditional K-means AlgorithmThe process of traditional K-means algorithm isas follows:Input: the number of clusters (k) and n-dataobjects.Output: k-clusters.
The clusters should satisfythe following requirements: the objects in thesame cluster have higher similarity, while theobjects in different clusters have lower similarity.The process steps:(1) Choose k-objects randomly as initial clustercenters;(2) Repeat;(3) Compute each object?s distance to eachcluster?s center, then object is assigned to themost similar cluster;(4) Update the center of each cluster;(5) Until the changes of all clusters?
centers aresmaller than a given threshold.2.2 The Advantages and Disadvantages ofTraditional K-means AlgorithmThe greatest advantage of traditional K-meansalgorithm is comparatively simple.
In addition,its implementation is quick, effective and doesnot need a high cost.
However, from the ideaand processes as illustrated, we can see that thetraditional K-means algorithm has twodisadvantages: (1) an over-reliance on theselection of initial points.
If the selection isimproper (e.g.
just select some points in thesame cluster as the initial points), the result willbe poor.
(2) the clustering results are sensitive to"noise" and isolated points.
Small amounts ofsuch data can greatly decrease the precision.2.3 Maximum Distance-based Selection ofthe Initial CentersGiven the above considerations, this paperintroduces a maximum distance-based selectionof the initial centers.The selection of initial centers has a greatimpact on the result in traditional K-meansclustering algorithm.
If the selection is moreappropriate, then the result will be morereasonable, while the convergence rate will befaster.
So we hope that the initial centers shouldbe dispersed as far as possible, not be placed in aparticular one or limited several clusters.
Thebest selection should be that K-initial pointsbelong to K-different clusters.
In order toachieve this goal, we use the maximum distance.Specific method is processed as follows: Firstly,select an arbitrary point as the first cluster?scenter from the n-data objects, and then calculateits distance to the remaining (n-1) data objects,to find out the farthest point away from it as thesecond cluster?s initial center.
Secondly,calculate the distances of the remaining (n-2)data objects to both the clusters?
center, computethe average of the two values, and then select thepoint with the maximum average value as theinitial cluster center of the third.
We repeat thisprocess until find out K-initial points.From Figure 1 we can see that the result ofimproved algorithm is much better thantraditional K-means algorithm.0.7250.730.7350.74Traditional K-meansImproved K-meansFigure 1: The results of traditional K-meansalgorithm and improved K-means algorithm.2.4 The Context of the Target WordsDuring the process of WSI, we believe that thespecific meaning of an ambiguous term isdetermined by its context, that is to say, thosetarget words with similar context should havesimilar meaning in theory.
So the first step wehave to do is to establish all sentences?
contextaround a target word (we have carried outChinese word segmentation and stop wordfiltering to these sentences).
As the K-meansalgorithm can only handle numerical data, wechange the context into numerical format andthen represent it using VSM.
But how todetermine the window size of the context isnecessary to be further discussed.In this paper, we use the information gainproposed by Lu et al to achieve the goal ofdetermining the window size.
We count out3000 high frequency words from the given testset in this task, every word as a class, and thencalculate the statistical uncertainty of the wholesystem (entropy), namely H (D) in equation (3);The next step is to calculate the uncertainty ofthe whole system on the premise of knowingrelative position, namely the ?v?VpP(v)?H(D|v)in equation (3); Difference between the twovalues is just the amount of information on theentire system provided by this relative position.The amount of information (i.e.
information gain)is the weight of this position in the whole system.In this way we can determine the windows sizeby the weight.IGp=H(D)?
?v?VpP(v)?H (D|v)      (1)whereH(D)=?
?d?DP(d)?log2P(d)          (2)P(d)=?i idfredfre)()(         (3)?i idfre )(  is the sum of frequency of the3,000 high frequency words appearing in thecorpus; )(dfre is the occurrence frequency ofterm d in the corpus.We first separately select eight words beforeand after the target word in a sentence toconstitute the context, expressed as thefollowing form:<wd?8, wd?7, wd?6, wd?5, wd?4, wd?3, wd?2, wd?1, focus-word,wd+1, wd+2, wd+3, wd+4, wd+5, wd+6, wd+7, wd+8>Table 1 Information gain of every position ofcontextLeft context Right contextPosition InformationgainPosition Informationgainwd?1 3.979 875 wd+1 4.005 737wd?2 2.800 943 wd+2 2.931 834wd?3 2.183 287 wd+3 2.287 020wd?4 1.709 504 wd+4 1.810 530wd?5 1.361 637 wd+5 1.437 952wd?6 1.074 606 wd+6 1.137 979wd?7 0.304 546 wd+7 0.821 330wd?8 0.298 992 wd+8 0.419 472The amount of information provided by eachposition is presented in Table 1.
According tothe information gain in this table we can draw aconclusion: the closer a term to the target word,the more greatly it contributes to its meaning,and the ability to describe the target word?smeaning decreases with the term?s distanceincreasing to the focus-word.
Because thosewords whose distance to the target word is morethan 6 words contribute less to the meaning ofthe target word, we separately select at most 6words before and after the target word as context.2.5 Sparsity ProblemFor those sentences containing the same targetword we can respectively establish their context,and then merge the same words in those contextto form a n-dimension space .Then we establishthe vector model for each sentence.
We haveexperimented with two different methods torepresent weight in the vector: one is TF*IDFwhich is conventional and widely used inpractice and the other one is Boolean.
However,from Figure 2 we can see that the result ofBoolean method is better.
Analyzing the reasons,we can infer that the decisive role of a word tothe target word is relevant whether the wordappears or not, and has nothing to do with thetimes of appearance.
Consequently, we selectBoolean method to represent weight in thevector: if a word in the space appears in thissentence, the weight of this position insentence?s vector is 1, otherwise is 0.Now we find a problem which should besolved: vector sparsity problem.
In a fewhundreds dimension vector space, a sentencecontains only several limited words, thus thevector is highly sparse.
As we analyzed, thereare two main causes: 1).
The length of asentence is too short, so the number of wordscontained by it is few.
2).
When merging thosewords in the context of a target word, we don?ttake into account the semantic similaritybetween them.
We know that if the vector is toosparse, the result will have large errors, even twosentences which should have belonged to thesame class are divided into different clusters.We can not solve the problem caused by thefirst factor, but we can improve the second one.In this paper we introduce TongYiCi CiLin fromHIT to compress the vector?s dimension.0.710.7150.720.7250.730.7350.740.745TF*IDF BooleanFigure 2: The results of two different methods torepresent weight in the vector.
Here we haveselected improved K-means algorithm tooptimize the initial centers.2.6 ExperimentsThe whole process of experiment is as follows:(1) Segment all sentences and filter stop-wordsfor a given data set;(2) Extract respectively six words before andafter the focus-word from those sentencescontaining the same target words, and thenuse TongYiCi CiLin to merge these wordsinto a lower n-dimension space;(3) Establish the vector model for each sentencein this space;(4) Cluster those sentences containing the sametarget words with maximum distance-basedK-means algorithm proposed in this paper.This experimental method is based on thefollowing assumption: the similarity of targetwords?
context determines the similarity of theirmeanings.
In the framework of this assumption,we construct the context vector of each sentence,and then cluster those sentences containing thesame target word.In the experimental result, we have achieved0.7729 Fscore on 100 ambiguous words.3 Graph-based Chinese Word SenseInductionIn this system, we use a combination of graph-based clustering and K-means algorithm.
At firstwe use Chinese Whisper to cluster the words inthe corpus and the clustering result can beconsidered as an artificial synonyms dictionary.Secondly we construct corpus vectors usingdifferent methods, and now the vector dimensionis decreased to the number of clusters.
At last wecluster the vectors with the help of K-meansalgorithm.3.1 Chinese Whisper MethodMany researches on WSI are based on word co-occurrence.
The approach proposed by ChrisBiemann has a wide range of applications,including language separation, acquisition ofword class, word sense induction and so on.Chinese Whisper, which comes from a gamecalled ?Chinese Whisper?, is a method used forgraph clustering and its process is as follows:(1) All nodes belong to different classes at thebeginning;(2) The nodes are processed for a small numberof iterations and inherit the strongest class inthe local neighborhoods.
The sum of edgeweights is maximal in this class.
(3) While updating a vertex i, each class, e.g.
cl,receives a score equal to the weight of edge(i, j), here j has been assigned to cl.
Themaximum score determines the strongestclass.
If there are more than 2 strongestclasses, only one is chosen randomly.
(4) While clustering, there are two importantparameters to select: convergence constantand the iterations.
From this we can see thatthis method has a great flexibility onparameter selection, and its clustering resultis totally determined by the parameters.In Chris Biemann?s paper, using ChineseWhisper, his experiment about WSI based onBritish National Corpus (BNC) achieved 92.2%precision in adjective, 90% precision in noun,and 77.6% precision in verbs.
Ioannis P.Klapaftis and Suresh Manandhar use ChineseWhisper method for clustering and theirexperiment based on BNC achieved 81.1%FScore after trying 72 different parameters.3.2 Graph ConstructionWhen we construct the graph, every word isconsidered as a node in the graph and the weightof edge eij is measured by co-cocurence times ofword i and word j.
However, if we just use thismethod to construct the graph, the graph is verysparse.
We use some methods proposed by IPKlapaftis to add new edges:(1) Associate a vertex vector VCi containing thevertices, which share an edge with vertex iin the graph.
(2) Calculate the similarity between each vertexvector VCi and each vertex vector VCj, herewe use Jaccard similarity coefficient (JC) asa similarity measure:| |( , )| |i ji ji jVC VCJC VC VCVC VC= ??
(4)Two nodes ci and cj are mutually similar if ciis the most similar node to cj and the otherway round.
(3) Two mutually similar nodes ci and cj areclustered with the result that an occurrenceof a node ck with one of ci, cj is also countedas an occurrence with the other node.3.3 ExperimentsK-means algorithm has a good performance forsmall corpus, but when the corpus size is too big,vector dimension will increase rapidly.
So Atfirst we use Chinese Whisper to cluster thewords in the corpus after preprocessing, such assplitting the sentences, filtering stopwords andselecting context.
Secondly we construct corpusvectors with VSM, and now the vectordimension is decreased to the number of clusters.At last we cluster the vectors using K-meansalgorithm analogous to the first system.The choice of parameters is an importantfactor in Chinese Whisper and differentparameters will result in different clusters.
Inthis experiment we use batch process method inorder to select the best parameters on training set.We select a group of parameters: convergenceconstant is from 0 to 1 and the step length is 0.1;iterations is from 1 to 30 and the step length is 1,which depends on the size of corpus.
Theprocess of experiment is as follows:(1) Get a pair of parameters from the parametergroup, cluster the corpus using ChineseWhisper, and then remove this pair ofparameter from the parameter group.
(2) Construct vectors using the result of step (1).
(3) Cluster the vectors using K-means.
(4) The results are as the following two tables.From table 2 and table 3 we can see that ifwe use JC method to add new edges, theprecision has a great improvement.In the experimental result, we have achieved0.6067 Fscore on 100 ambiguous words with theparameters: 0.8 and 12.Table 2 Experimental results without using JCmethodconvergeconstanceiterations precision (Boolean)0.1 11 0.61190.1 15 0.61750.3 15 0.62100.5 15 0.6188Table 3 Experimental results using JC methodconvergeconstanceiterations precision(Boolean)0.6 17 0.62110.6 15 0.62510.7 11 0.62610.7 15 0.62870.8 12 0.63910.9 14 0.61921.0 16 0.63891.0 15 0.63004 ConclusionIn this paper, we propose two different systemsfor the task of Chinese WSI.The result of the first system which is basedon an improved K-means algorithm shows theproposed idea is feasible, and the precision isguaranteed.
However, some problems still existand need further to be resolved:(1) The extended particle size of a word?ssynonym while using TongYiCi CiLin.
Ifparticle size is too large, the "noise" affectsthe accuracy of the result; If particle size istoo small, time complexity of the algorithmwill increase drastically.
(2) The selection of initial centers in K-meansalgorithm remains to be further optimized.In addition to avoid the selected initialcenters placing in one or several clusters,the problem of "noise" and isolated dataneed to be considered.
(3) The instability of this method.
While wehave got better results on most ofambiguous terms, but for those words withvery many meanings, the induction effect isnot so good.
The reasons should be furtheranalyzed and the solutions should be foundout.The result of the second system which isbased on graph clustering shows that thismethod has a good performance in decreasingvector dimension.
However, the number ofclusters is too small, which made theperformance of K-means algorithm poor.Chinese Whisper has a good performance inWSI, but this is the first time to combine it withK-means together, thus there are lots ofproblems to be solved.
As we have investigated,some methods can be used to improve theperformance in the future work:(1) Use a pair of words as a vertex of the graphinstead of using a single word.
(2) Instead of using co-occurrence times as theweight of an edge, we can use conditionalprobability.
(3) Constrain words pair which can filter outsome ?noise?, i.e.
only use those wordswhose co-occurrence times is greater than agiven value threshold.AcknowledgmentsThis work is supported by grant from the NaturalScience Foundation of China (No.60673039 and60973068), the National High Tech Researchand Development Plan of China(No.2006AA01Z151), National Social ScienceFoundation of China (No.08BTQ025), theProject Sponsored by the Scientific ResearchFoundation for the Returned Overseas ChineseScholars, State Education Ministry and TheResearch Fund for the Doctoral Program ofHigher Education (No.20090041110002).ReferencesLu Song, Bai Shuo, and Huang Xiong.
2002.
AnUnsupervised Approach to Word SenseDisambiguation Based on Sense-Words in VectorSpace Model.
Journal of Software, 13(06): 1082-1089.Stefan Bordag.
2004.
Word Sense Induction: Triplet-Based Clustering and Automatic Evaluation.
In:Proceedings of HLT-NAACL, Workshop onComputational Lexical Semantics, pages 137-144,Boston, Massachusetts.Ioannis P.Klapaftis and Suresh Manandhar.
2008.Word Sense Induction Using Graphs ofCollocations.
In: Proceedings of the 2008conference on ECAI 2008: 18th EuropeanConference on Artificial Intelligence, Frontiers inArtificial Intelligence and Applications, pages 298-302, United Kingdom.Chris Biemann.
2006.
Chinese Whispers - anEfficient Graph Clustering Algorithm and itsApplication to Natural Language ProcessingProblems.
In: Proceedings of the HLT-NAACL2006 Workshop on Textgraphs, New York, USA.
